Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月23日 16:38
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed 21 Feb 24 19:00:00 GMT  to  Thu 22 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.14083
Date: Wed, 21 Feb 2024 19:17:28 GMT   (758kb,D)

Title: Beyond A*: Better Planning with Transformers via Search Dynamics
  Bootstrapping
Authors: Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat,
  Yuandong Tian
Categories: cs.AI
\\
  While Transformers have enabled tremendous progress in various application
settings, such architectures still lag behind traditional symbolic planners for
solving complex decision making tasks. In this work, we demonstrate how to
train Transformers to solve complex planning tasks and present Searchformer, a
Transformer model that optimally solves previously unseen Sokoban puzzles 93.7%
of the time, while using up to 26.8% fewer search steps than standard $A^*$
search. Searchformer is an encoder-decoder Transformer model trained to predict
the search dynamics of $A^*$. This model is then fine-tuned via expert
iterations to perform fewer search steps than $A^*$ search while still
generating an optimal plan. In our training method, $A^*$'s search dynamics are
expressed as a token sequence outlining when task states are added and removed
into the search tree during symbolic planning. In our ablation studies on maze
navigation, we find that Searchformer significantly outperforms baselines that
predict the optimal plan directly with a 5-10$\times$ smaller model size and a
10$\times$ smaller training dataset. We also demonstrate how Searchformer
scales to larger and more complex decision making tasks like Sokoban with
improved percentage of solved tasks and shortened search dynamics.
\\ ( https://arxiv.org/abs/2402.14083 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14090
Date: Wed, 21 Feb 2024 19:29:14 GMT   (1331kb,D)

Title: Social Environment Design
Authors: Edwin Zhang, Sadie Zhao, Tonghan Wang, Safwan Hossain, Henry
  Gasztowtt, Stephan Zheng, David C. Parkes, Milind Tambe, Yiling Chen
Categories: cs.AI econ.GN q-fin.EC stat.ML
Comments: Code at https://github.com/ezhang7423/social-environment-design
\\
  Artificial Intelligence (AI) holds promise as a technology that can be used
to improve government and economic policy-making. This paper proposes a new
research agenda towards this end by introducing Social Environment Design, a
general framework for the use of AI for automated policy-making that connects
with the Reinforcement Learning, EconCS, and Computational Social Choice
communities. The framework seeks to capture general economic environments,
includes voting on policy objectives, and gives a direction for the systematic
analysis of government and economic policy through AI simulation. We highlight
key open problems for future research in AI-based policy-making. By solving
these challenges, we hope to achieve various social welfare objectives, thereby
promoting more ethical and responsible decision making.
\\ ( https://arxiv.org/abs/2402.14090 ,  1331kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14244
Date: Thu, 22 Feb 2024 03:11:09 GMT   (5810kb,D)

Title: MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback
  and Dynamic Distance Constraint
Authors: Xinglin Zhou, Yifu Yuan, Shaofu Yang, Jianye Hao
Categories: cs.AI cs.HC cs.LG
\\
  Hierarchical reinforcement learning (HRL) provides a promising solution for
complex tasks with sparse rewards of intelligent agents, which uses a
hierarchical framework that divides tasks into subgoals and completes them
sequentially. However, current methods struggle to find suitable subgoals for
ensuring a stable learning process. Without additional guidance, it is
impractical to rely solely on exploration or heuristics methods to determine
subgoals in a large goal space. To address the issue, We propose a general
hierarchical reinforcement learning framework incorporating human feedback and
dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating
human feedback into high-level policy learning, to find better subgoals. As for
low-level policy, MENTOR designs a dual policy for exploration-exploitation
decoupling respectively to stabilize the training. Furthermore, although humans
can simply break down tasks into subgoals to guide the right learning
direction, subgoals that are too difficult or too easy can still hinder
downstream learning efficiency. We propose the Dynamic Distance Constraint
(DDC) mechanism dynamically adjusting the space of optional subgoals. Thus
MENTOR can generate subgoals matching the low-level policy learning process
from easy to hard. Extensive experiments demonstrate that MENTOR uses a small
amount of human feedback to achieve significant improvement in complex tasks
with sparse rewards.
\\ ( https://arxiv.org/abs/2402.14244 ,  5810kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14424
Date: Thu, 22 Feb 2024 10:12:16 GMT   (14102kb,D)

Title: Automating Psychological Hypothesis Generation with AI: Large Language
  Models Meet Causal Graph
Authors: Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng
Categories: cs.AI cs.CY
DOI: 10.31234/osf.io/7ck9m
\\
  Leveraging the synergy between causal knowledge graphs and a large language
model (LLM), our study introduces a groundbreaking approach for computational
hypothesis generation in psychology. We analyzed 43,312 psychology articles
using a LLM to extract causal relation pairs. This analysis produced a
specialized causal graph for psychology. Applying link prediction algorithms,
we generated 130 potential psychological hypotheses focusing on `well-being',
then compared them against research ideas conceived by doctoral scholars and
those produced solely by the LLM. Interestingly, our combined approach of a LLM
and causal graphs mirrored the expert-level insights in terms of novelty,
clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) =
4.32, p<0.001, respectively). This alignment was further corroborated using
deep semantic analysis. Our results show that combining LLM with machine
learning techniques such as causal knowledge graphs can revolutionize automated
discovery in psychology, extracting novel insights from the extensive
literature. This work stands at the crossroads of psychology and artificial
intelligence, championing a new enriched paradigm for data-driven hypothesis
generation in psychological research.
\\ ( https://arxiv.org/abs/2402.14424 ,  14102kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14460
Date: Thu, 22 Feb 2024 11:38:43 GMT   (28kb)

Title: Reframing the Expected Free Energy: Four Formulations and a Unification
Authors: Th\'eophile Champion, Howard Bowman, Dimitrije Markovi\'c, Marek
  Grze\'s
Categories: cs.AI
Comments: 17 pages, 2 figures
\\
  Active inference is a leading theory of perception, learning and decision
making, which can be applied to neuroscience, robotics, psychology, and machine
learning. Active inference is based on the expected free energy, which is
mostly justified by the intuitive plausibility of its formulations, e.g., the
risk plus ambiguity and information gain / pragmatic value formulations. This
paper seek to formalize the problem of deriving these formulations from a
single root expected free energy definition, i.e., the unification problem.
Then, we study two settings, each one having its own root expected free energy
definition. In the first setting, no justification for the expected free energy
has been proposed to date, but all the formulations can be recovered from it.
However, in this setting, the agent cannot have arbitrary prior preferences
over observations. Indeed, only a limited class of prior preferences over
observations is compatible with the likelihood mapping of the generative model.
In the second setting, a justification of the root expected free energy
definition is known, but this setting only accounts for two formulations, i.e.,
the risk over states plus ambiguity and entropy plus expected energy
formulations.
\\ ( https://arxiv.org/abs/2402.14460 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14580
Date: Thu, 8 Feb 2024 07:24:45 GMT   (264kb,D)

Title: Savvy: Trustworthy Autonomous Vehicles Architecture
Authors: Ali Shoker, Rehana Yasmin, Paulo Esteves-Verissimo
Categories: cs.AI cs.SY eess.SY
\\
  The increasing interest in Autonomous Vehicles (AV) is notable due to
business, safety, and performance reasons. While there is salient success in
recent AV architectures, hinging on the advancements in AI models, there is a
growing number of fatal incidents that impedes full AVs from going mainstream.
This calls for the need to revisit the fundamentals of building safety-critical
AV architectures. However, this direction should not deter leveraging the power
of AI. To this end, we propose Savvy, a new trustworthy intelligent AV
architecture that achieves the best of both worlds. Savvy makes a clear
separation between the control plane and the data plane to guarantee the
safety-first principles. The former assume control to ensure safety using
design-time defined rules, while launching the latter for optimizing decisions
as much as possible within safety time-bounds. This is achieved through guided
Time-aware predictive quality degradation (TPQD): using dynamic ML models that
can be tuned to provide either richer or faster outputs based on the available
safety time bounds. For instance, Savvy allows to safely identify an elephant
as an obstacle (a mere object) the earliest possible, rather than optimally
recognizing it as an elephant when it is too late. This position paper presents
the Savvy's motivations and concept, whereas empirical evaluation is a work in
progress.
\\ ( https://arxiv.org/abs/2402.14580 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14596
Date: Wed, 7 Feb 2024 05:22:10 GMT   (1072kb)

Title: The Role of LLMs in Sustainable Smart Cities: Applications, Challenges,
  and Future Directions
Authors: Amin Ullah, Guilin Qi, Saddam Hussain, Irfan Ullah, Zafar Ali
Categories: cs.AI
\\
  Smart cities stand as pivotal components in the ongoing pursuit of elevating
urban living standards, facilitating the rapid expansion of urban areas while
efficiently managing resources through sustainable and scalable innovations. In
this regard, as emerging technologies like Artificial Intelligence (AI), the
Internet of Things (IoT), big data analytics, and fog and edge computing have
become increasingly prevalent, smart city applications grapple with various
challenges, including the potential for unauthorized disclosure of confidential
and sensitive data. The seamless integration of emerging technologies has
played a vital role in sustaining the dynamic pace of their development. This
paper explores the substantial potential and applications of Deep Learning
(DL), Federated Learning (FL), IoT, Blockchain, Natural Language Processing
(NLP), and large language models (LLMs) in optimizing ICT processes within
smart cities. We aim to spotlight the vast potential of these technologies as
foundational elements that technically strengthen the realization and
advancement of smart cities, underscoring their significance in driving
innovation within this transformative urban milieu. Our discourse culminates
with an exploration of the formidable challenges that DL, FL, IoT, Blockchain,
NLP, and LLMs face within these contexts, and we offer insights into potential
future directions.
\\ ( https://arxiv.org/abs/2402.14596 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14600
Date: Sun, 4 Feb 2024 05:46:28 GMT   (8470kb,D)

Title: Diffusion Model-Based Multiobjective Optimization for Gasoline Blending
  Scheduling
Authors: Wenxuan Fang and Wei Du and Renchu He and Yang Tang and Yaochu Jin and
  Gary G. Yen
Categories: cs.AI
\\
  Gasoline blending scheduling uses resource allocation and operation
sequencing to meet a refinery's production requirements. The presence of
nonlinearity, integer constraints, and a large number of decision variables
adds complexity to this problem, posing challenges for traditional and
evolutionary algorithms. This paper introduces a novel multiobjective
optimization approach driven by a diffusion model (named DMO), which is
designed specifically for gasoline blending scheduling. To address integer
constraints and generate feasible schedules, the diffusion model creates
multiple intermediate distributions between Gaussian noise and the feasible
domain. Through iterative processes, the solutions transition from Gaussian
noise to feasible schedules while optimizing the objectives using the gradient
descent method. DMO achieves simultaneous objective optimization and constraint
adherence. Comparative tests are conducted to evaluate DMO's performance across
various scales. The experimental results demonstrate that DMO surpasses
state-of-the-art multiobjective evolutionary algorithms in terms of efficiency
when solving gasoline blending scheduling problems.
\\ ( https://arxiv.org/abs/2402.14600 ,  8470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14744
Date: Thu, 22 Feb 2024 18:03:14 GMT   (5062kb,D)

Title: Large Language Models as Urban Residents: An LLM Agent Framework for
  Personal Mobility Generation
Authors: Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka,
  Ryosuke Shibasaki, Chuan Xiao
Categories: cs.AI cs.CL cs.CY cs.LG
Comments: Source codes will be released soon
\\
  This paper introduces a novel approach using Large Language Models (LLMs)
integrated into an agent framework for flexible and efficient personal mobility
generation. LLMs overcome the limitations of previous models by efficiently
processing semantic data and offering versatility in modeling various tasks.
Our approach addresses the critical need to align LLMs with real-world urban
mobility data, focusing on three research questions: aligning LLMs with rich
activity data, developing reliable activity generation strategies, and
exploring LLM applications in urban mobility. The key technical contribution is
a novel LLM agent framework that accounts for individual activity patterns and
motivations, including a self-consistency approach to align LLMs with
real-world activity data and a retrieval-augmented strategy for interpretable
activity generation. In experimental studies, comprehensive validation is
performed using real-world data. This research marks the pioneering work of
designing an LLM agent framework for activity generation based on real-world
human activity data, offering a promising tool for urban mobility analysis.
\\ ( https://arxiv.org/abs/2402.14744 ,  5062kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14757
Date: Thu, 22 Feb 2024 18:19:45 GMT   (2325kb,D)

Title: SHM-Traffic: DRL and Transfer learning based UAV Control for Structural
  Health Monitoring of Bridges with Traffic
Authors: Divija Swetha Gadiraju, Saeed Eftekhar Azam and Deepak Khazanchi
Categories: cs.AI
\\
  This work focuses on using advanced techniques for structural health
monitoring (SHM) for bridges with Traffic. We propose an approach using deep
reinforcement learning (DRL)-based control for Unmanned Aerial Vehicle (UAV).
Our approach conducts a concrete bridge deck survey while traffic is ongoing
and detects cracks. The UAV performs the crack detection, and the location of
cracks is initially unknown. We use two edge detection techniques. First, we
use canny edge detection for crack detection. We also use a Convolutional
Neural Network (CNN) for crack detection and compare it with canny edge
detection. Transfer learning is applied using CNN with pre-trained weights
obtained from a crack image dataset. This enables the model to adapt and
improve its performance in identifying and localizing cracks. Proximal Policy
Optimization (PPO) is applied for UAV control and bridge surveys. The
experimentation across various scenarios is performed to evaluate the
performance of the proposed methodology. Key metrics such as task completion
time and reward convergence are observed to gauge the effectiveness of the
approach. We observe that the Canny edge detector offers up to 40\% lower task
completion time, while the CNN excels in up to 12\% better damage detection and
1.8 times better rewards.
\\ ( https://arxiv.org/abs/2402.14757 ,  2325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14052
Date: Wed, 21 Feb 2024 18:57:54 GMT   (7886kb,D)

Title: On Leveraging Encoder-only Pre-trained Language Models for Effective
  Keyphrase Generation
Authors: Di Wu, Wasi Uddin Ahmad, Kai-Wei Chang
Categories: cs.CL
Comments: LREC-COLING 2024 camera ready. arXiv admin note: text overlap with
  arXiv:2212.10233
\\
  This study addresses the application of encoder-only Pre-trained Language
Models (PLMs) in keyphrase generation (KPG) amidst the broader availability of
domain-tailored encoder-only models compared to encoder-decoder models. We
investigate three core inquiries: (1) the efficacy of encoder-only PLMs in KPG,
(2) optimal architectural decisions for employing encoder-only PLMs in KPG, and
(3) a performance comparison between in-domain encoder-only and encoder-decoder
PLMs across varied resource settings. Our findings, derived from extensive
experimentation in two domains reveal that with encoder-only PLMs, although KPE
with Conditional Random Fields slightly excels in identifying present
keyphrases, the KPG formulation renders a broader spectrum of keyphrase
predictions. Additionally, prefix-LM fine-tuning of encoder-only PLMs emerges
as a strong and data-efficient strategy for KPG, outperforming general-domain
seq2seq PLMs. We also identify a favorable parameter allocation towards model
depth rather than width when employing encoder-decoder architectures
initialized with encoder-only PLMs. The study sheds light on the potential of
utilizing encoder-only PLMs for advancing KPG systems and provides a groundwork
for future KPG methods. Our code and pre-trained checkpoints are released at
https://github.com/uclanlp/DeepKPG.
\\ ( https://arxiv.org/abs/2402.14052 ,  7886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14073
Date: Wed, 21 Feb 2024 19:01:03 GMT   (760kb,D)

Title: Improving Language Understanding from Screenshots
Authors: Tianyu Gao, Zirui Wang, Adithya Bhaskar, Danqi Chen
Categories: cs.CL cs.LG
Comments: Our model and code are available at
  https://github.com/princeton-nlp/PTP
\\
  An emerging family of language models (LMs), capable of processing both text
and images within a single visual view, has the promise to unlock complex tasks
such as chart understanding and UI navigation. We refer to these models as
screenshot language models. Despite their appeal, existing screenshot LMs
substantially lag behind text-only models on language understanding tasks. To
close this gap, we adopt a simplified setting where the model inputs are
plain-text-rendered screenshots, and we focus on improving the text ability of
screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective,
which masks and recovers both image patches of screenshots and text within
screenshots. We also conduct extensive ablation studies on masking rates and
patch sizes, as well as designs for improving training stability. Our
pre-trained model, while solely taking visual inputs, achieves comparable
performance with BERT on 6 out of 8 GLUE tasks (within 2%) and improves up to
8% over prior work. Additionally, we extend PTP to train autoregressive
screenshot LMs and demonstrate its effectiveness--our models can significantly
reduce perplexity by utilizing the screenshot context. Together, we hope our
findings can inspire future research on developing powerful screenshot LMs and
extending their reach to broader applications.
\\ ( https://arxiv.org/abs/2402.14073 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14086
Date: Wed, 21 Feb 2024 19:20:06 GMT   (8208kb,D)

Title: LexC-Gen: Generating Data for Extremely Low-Resource Languages with
  Large Language Models and Bilingual Lexicons
Authors: Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach
Categories: cs.CL cs.AI cs.LG
\\
  Data scarcity in low-resource languages can be addressed with word-to-word
translations from labeled task data in high-resource languages using bilingual
lexicons. However, bilingual lexicons often have limited lexical overlap with
task data, which results in poor translation coverage and lexicon utilization.
We propose lexicon-conditioned data generation (LexC-Gen), a method that
generates low-resource-language classification task data at scale.
Specifically, LexC-Gen first uses high-resource-language words from bilingual
lexicons to generate lexicon-compatible task data, and then it translates them
into low-resource languages with bilingual lexicons via word translation.
Across 17 extremely low-resource languages, LexC-Gen generated data is
competitive with expert-translated gold data, and yields on average 5.6 and 8.9
points improvement over existing lexicon-based word translation methods on
sentiment analysis and topic classification tasks respectively. We show that
conditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen
is also practical -- it only needs a single GPU to generate data at scale. It
works well with open-access LLMs, and its cost is one-fifth of the cost of
GPT4-based multilingual data generation.
\\ ( https://arxiv.org/abs/2402.14086 ,  8208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14101
Date: Wed, 21 Feb 2024 19:53:36 GMT   (2964kb,D)

Title: Cost-Efficient Subjective Task Annotation and Modeling through Few-Shot
  Annotator Adaptation
Authors: Preni Golazizian, Ali Omrani, Alireza S. Ziabari, Morteza Dehghani
Categories: cs.CL
\\
  In subjective NLP tasks, where a single ground truth does not exist, the
inclusion of diverse annotators becomes crucial as their unique perspectives
significantly influence the annotations. In realistic scenarios, the annotation
budget often becomes the main determinant of the number of perspectives (i.e.,
annotators) included in the data and subsequent modeling. We introduce a novel
framework for annotation collection and modeling in subjective tasks that aims
to minimize the annotation budget while maximizing the predictive performance
for each annotator. Our framework has a two-stage design: first, we rely on a
small set of annotators to build a multitask model, and second, we augment the
model for a new perspective by strategically annotating a few samples per
annotator. To test our framework at scale, we introduce and release a unique
dataset, Moral Foundations Subjective Corpus, of 2000 Reddit posts annotated by
24 annotators for moral sentiment. We demonstrate that our framework surpasses
the previous SOTA in capturing the annotators' individual perspectives with as
little as 25% of the original annotation budget on two datasets. Furthermore,
our framework results in more equitable models, reducing the performance
disparity among annotators.
\\ ( https://arxiv.org/abs/2402.14101 ,  2964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14116
Date: Wed, 21 Feb 2024 20:30:45 GMT   (8421kb,D)

Title: FanOutQA: Multi-Hop, Multi-Document Question Answering for Large
  Language Models
Authors: Andrew Zhu and Alyssa Hwang and Liam Dugan and Chris Callison-Burch
Categories: cs.CL cs.AI
Comments: 18 pages, 2 figures. In review at ACL 2024
\\
  One type of question that is commonly found in day-to-day scenarios is
``fan-out'' questions, complex multi-hop, multi-document reasoning questions
that require finding information about a large number of entities. However,
there exist few resources to evaluate this type of question-answering
capability among large language models. To evaluate complex reasoning in LLMs
more fully, we present FanOutQA, a high-quality dataset of fan-out
question-answer pairs and human-annotated decompositions with English Wikipedia
as the knowledge base. We formulate three benchmark settings across our dataset
and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,
finding that contemporary models still have room to improve reasoning over
inter-document dependencies in a long context. We provide our dataset and
open-source tools to run models to encourage evaluation at https://fanoutqa.com
\\ ( https://arxiv.org/abs/2402.14116 ,  8421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14146
Date: Wed, 21 Feb 2024 22:02:37 GMT   (8339kb,D)

Title: Reinforcement Learning with Dynamic Multi-Reward Weighting for
  Multi-Style Controllable Generation
Authors: Karin de Langis, Ryan Koo, Dongyeop Kang
Categories: cs.CL
\\
  Style is an integral component of text that expresses a diverse set of
information, including interpersonal dynamics (e.g. formality) and the author's
emotions or attitudes (e.g. disgust). Humans often employ multiple styles
simultaneously. An open question is how large language models can be explicitly
controlled so that they weave together target styles when generating text: for
example, to produce text that is both negative and non-toxic. Previous work
investigates the controlled generation of a single style, or else controlled
generation of a style and other attributes. In this paper, we expand this into
controlling multiple styles simultaneously. Specifically, we investigate
various formulations of multiple style rewards for a reinforcement learning
(RL) approach to controlled multi-style generation. These reward formulations
include calibrated outputs from discriminators and dynamic weighting by
discriminator gradient magnitudes. We find that dynamic weighting generally
outperforms static weighting approaches, and we explore its effectiveness in 2-
and 3-style control, even compared to strong baselines like plug-and-play
model. All code and data for RL pipelines with multiple style attributes will
be publicly available.
\\ ( https://arxiv.org/abs/2402.14146 ,  8339kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14154
Date: Wed, 21 Feb 2024 22:27:40 GMT   (2324kb,D)

Title: MM-Soc: Benchmarking Multimodal Large Language Models in Social Media
  Platforms
Authors: Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar
Categories: cs.CL cs.CV cs.CY
Comments: 18 pages, 6 figures
\\
  Social media platforms are hubs for multimodal information exchange,
encompassing text, images, and videos, making it challenging for machines to
comprehend the information or emotions associated with interactions in online
spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising
solution to address these challenges, yet struggle with accurately interpreting
human emotions and complex contents like misinformation. This paper introduces
MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of
multimodal social media content. MM-Soc compiles prominent multimodal datasets
and incorporates a novel large-scale YouTube tagging dataset, targeting a range
of tasks from misinformation detection, hate speech detection, and social
context generation. Through our exhaustive evaluation on ten size-variants of
four open-source MLLMs, we have identified significant performance disparities,
highlighting the need for advancements in models' social understanding
capabilities. Our analysis reveals that, in a zero-shot setting, various types
of MLLMs generally exhibit difficulties in handling social media tasks.
However, MLLMs demonstrate performance improvements post fine-tuning,
suggesting potential pathways for improvement.
\\ ( https://arxiv.org/abs/2402.14154 ,  2324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14155
Date: Wed, 21 Feb 2024 22:30:57 GMT   (1427kb,D)

Title: Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for
  Intent Recognition?
Authors: Amogh Mannekote, Xiaoyi Tian, Kristy Elizabeth Boyer, Bonnie J. Dorr
Categories: cs.CL cs.AI
\\
  Task-oriented dialogue systems are expected to handle a constantly expanding
set of intents and domains even after they have been deployed to support more
and more functionalities. To live up to this expectation, it becomes critical
to mitigate the catastrophic forgetting problem (CF) that occurs in continual
learning (CL) settings for a task such as intent recognition. While existing
dialogue systems research has explored replay-based and regularization-based
methods to this end, the effect of domain ordering on the CL performance of
intent recognition models remains unexplored. If understood well, domain
ordering has the potential to be an orthogonal technique that can be leveraged
alongside existing techniques such as experience replay. Our work fills this
gap by comparing the impact of three domain-ordering strategies (min-sum path,
max-sum path, random) on the CL performance of a generative intent recognition
model. Our findings reveal that the min-sum path strategy outperforms the
others in reducing catastrophic forgetting when training on the 220M T5-Base
model. However, this advantage diminishes with the larger 770M T5-Large model.
These results underscores the potential of domain ordering as a complementary
strategy for mitigating catastrophic forgetting in continually learning intent
recognition models, particularly in resource-constrained scenarios.
\\ ( https://arxiv.org/abs/2402.14155 ,  1427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14158
Date: Wed, 21 Feb 2024 22:41:38 GMT   (455kb,D)

Title: TOOLVERIFIER: Generalization to New Tools via Self-Verification
Authors: Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria
  Lomeli, Jingbo Shang, Jane Dwivedi-Yu
Categories: cs.CL
\\
  Teaching language models to use tools is an important milestone towards
building general assistants, but remains an open problem. While there has been
significant progress on learning to use specific tools via fine-tuning,
language models still struggle with learning how to robustly use new tools from
only a few demonstrations. In this work we introduce a self-verification method
which distinguishes between close candidates by self-asking contrastive
questions during (1) tool selection; and (2) parameter generation. We construct
synthetic, high-quality, self-generated data for this goal using Llama-2 70B,
which we intend to release publicly. Extensive experiments on 4 tasks from the
ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average
improvement of 22% over few-shot baselines, even in scenarios where the
distinctions between candidate tools are finely nuanced.
\\ ( https://arxiv.org/abs/2402.14158 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14179
Date: Wed, 21 Feb 2024 23:43:04 GMT   (289kb,D)

Title: Bangla AI: A Framework for Machine Translation Utilizing Large Language
  Models for Ethnic Media
Authors: MD Ashraful Goni, Fahad Mostafa, Kerk F. Kee
Categories: cs.CL cs.AI
Comments: 7 Pages, 1 figure
\\
  Ethnic media, which caters to diaspora communities in host nations, serves as
a vital platform for these communities to both produce content and access
information. Rather than utilizing the language of the host nation, ethnic
media delivers news in the language of the immigrant community. For instance,
in the USA, Bangla ethnic media presents news in Bangla rather than English.
This research delves into the prospective integration of large language models
(LLM) and multi-lingual machine translations (MMT) within the ethnic media
industry. It centers on the transformative potential of using LLM in MMT in
various facets of news translation, searching, and categorization. The paper
outlines a theoretical framework elucidating the integration of LLM and MMT
into the news searching and translation processes for ethnic media.
Additionally, it briefly addresses the potential ethical challenges associated
with the incorporation of LLM and MMT in news translation procedures.
\\ ( https://arxiv.org/abs/2402.14179 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14195
Date: Thu, 22 Feb 2024 00:41:23 GMT   (4390kb,D)

Title: Learning to Reduce: Optimal Representations of Structured Data in
  Prompting Large Language Models
Authors: Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, Xiang Chen
Categories: cs.CL
Comments: 5 pages
\\
  Large Language Models (LLMs) have been widely used as general-purpose AI
agents showing comparable performance on many downstream tasks. However,
existing work shows that it is challenging for LLMs to integrate structured
data (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand
long text data or select the most relevant evidence prior to inference, and
both approaches are not trivial.
  In this paper, we propose a framework, Learning to Reduce, that fine-tunes a
language model to generate a reduced version of an input context, given a task
description and context input. The model learns to reduce the input context
using On-Policy Reinforcement Learning and aims to improve the reasoning
performance of a fixed LLM. Experimental results illustrate that our model not
only achieves comparable accuracies in selecting the relevant evidence from an
input context, but also shows generalizability on different datasets. We
further show that our model helps improve the LLM's performance on downstream
tasks especially when the context is long.
\\ ( https://arxiv.org/abs/2402.14195 ,  4390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14200
Date: Thu, 22 Feb 2024 01:02:37 GMT   (8247kb,D)

Title: Towards Understanding Counseling Conversations: Domain Knowledge and
  Large Language Models
Authors: Younghun Lee, Dan Goldwasser, Laura Schwab Reese
Categories: cs.CL
Comments: Findings of EACL 2024, 10 pages
\\
  Understanding the dynamics of counseling conversations is an important task,
yet it is a challenging NLP problem regardless of the recent advance of
Transformer-based pre-trained language models. This paper proposes a systematic
approach to examine the efficacy of domain knowledge and large language models
(LLMs) in better representing conversations between a crisis counselor and a
help seeker. We empirically show that state-of-the-art language models such as
Transformer-based models and GPT models fail to predict the conversation
outcome. To provide richer context to conversations, we incorporate
human-annotated domain knowledge and LLM-generated features; simple integration
of domain knowledge and LLM features improves the model performance by
approximately 15%. We argue that both domain knowledge and LLM-generated
features can be exploited to better characterize counseling conversations when
they are used as an additional context to conversations.
\\ ( https://arxiv.org/abs/2402.14200 ,  8247kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14207
Date: Thu, 22 Feb 2024 01:20:17 GMT   (8683kb,D)

Title: Assisting in Writing Wikipedia-like Articles From Scratch with Large
  Language Models
Authors: Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab,
  Monica S. Lam
Categories: cs.CL cs.AI
Comments: Preprint, under review. 27 pages
\\
  We study how to apply large language models to write grounded and organized
long-form articles from scratch, with comparable breadth and depth to Wikipedia
pages. This underexplored problem poses new challenges at the pre-writing
stage, including how to research the topic and prepare an outline prior to
writing. We propose STORM, a writing system for the Synthesis of Topic Outlines
through Retrieval and Multi-perspective Question Asking. STORM models the
pre-writing stage by (1) discovering diverse perspectives in researching the
given topic, (2) simulating conversations where writers carrying different
perspectives pose questions to a topic expert grounded on trusted Internet
sources, (3) curating the collected information to create an outline.
  For evaluation, we curate FreshWiki, a dataset of recent high-quality
Wikipedia articles, and formulate outline assessments to evaluate the
pre-writing stage. We further gather feedback from experienced Wikipedia
editors. Compared to articles generated by an outline-driven
retrieval-augmented baseline, more of STORM's articles are deemed to be
organized (by a 25% absolute increase) and broad in coverage (by 10%). The
expert feedback also helps identify new challenges for generating grounded long
articles, such as source bias transfer and over-association of unrelated facts.
\\ ( https://arxiv.org/abs/2402.14207 ,  8683kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14208
Date: Thu, 22 Feb 2024 01:20:51 GMT   (2191kb,D)

Title: Content Conditional Debiasing for Fair Text Embedding
Authors: Wenlong Deng, Blair Chen, Xiaoxiao Li, Christos Thrampoulidis
Categories: cs.CL cs.AI cs.CY cs.LG
\\
  Mitigating biases in machine learning models has gained increasing attention
in Natural Language Processing (NLP). Yet, only a few studies focus on fair
text embeddings, which are crucial yet challenging for real-world applications.
In this paper, we propose a novel method for learning fair text embeddings. We
achieve fairness while maintaining utility trade-off by ensuring conditional
independence between sensitive attributes and text embeddings conditioned on
the content. Specifically, we enforce that embeddings of texts with different
sensitive attributes but identical content maintain the same distance toward
the embedding of their corresponding neutral text. Furthermore, we address the
issue of lacking proper training data by using Large Language Models (LLMs) to
augment texts into different sensitive groups. Our extensive evaluations
demonstrate that our approach effectively improves fairness while preserving
the utility of embeddings, representing a pioneering effort in achieving
conditional independence for fair text embeddings.
\\ ( https://arxiv.org/abs/2402.14208 ,  2191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14224
Date: Thu, 22 Feb 2024 02:07:21 GMT   (9913kb,D)

Title: Framing in the Presence of Supporting Data: A Case Study in U.S.
  Economic News
Authors: Alexandria Leto, Elliot Pickens, Coen D. Needell, David Rothschild,
  Maria Leonor Pacheco
Categories: cs.CL
Comments: total pages: 19; main body pages: 8; total figures: 19 submitted to
  Association for Computational Linguistics (ACL 2024), February 15th deadline
\\
  The mainstream media has much leeway in what it chooses to cover and how it
covers it. These choices have real-world consequences on what people know and
their subsequent behaviors. However, the lack of objective measures to evaluate
editorial choices makes research in this area particularly difficult. In this
paper, we argue that there are newsworthy topics where objective measures exist
in the form of supporting data and propose a computational framework to analyze
editorial choices in this setup. We focus on the economy because the reporting
of economic indicators presents us with a relatively easy way to determine both
the selection and framing of various publications. Their values provide a
ground truth of how the economy is doing relative to how the publications
choose to cover it. To do this, we define frame prediction as a set of
interdependent tasks. At the article level, we learn to identify the reported
stance towards the general state of the economy. Then, for every numerical
quantity reported in the article, we learn to identify whether it corresponds
to an economic indicator and whether it is being reported in a positive or
negative way. To perform our analysis, we track six American publishers and
each article that appeared in the top 10 slots of their landing page between
2015 and 2023.
\\ ( https://arxiv.org/abs/2402.14224 ,  9913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14258
Date: Thu, 22 Feb 2024 03:46:02 GMT   (825kb,D)

Title: Eagle: Ethical Dataset Given from Real Interactions
Authors: Masahiro Kaneko, Danushka Bollegala, Timothy Baldwin
Categories: cs.CL
\\
  Recent studies have demonstrated that large language models (LLMs) have
ethical-related problems such as social biases, lack of moral reasoning, and
generation of offensive content. The existing evaluation metrics and methods to
address these ethical challenges use datasets intentionally created by
instructing humans to create instances including ethical problems. Therefore,
the data does not reflect prompts that users actually provide when utilizing
LLM services in everyday contexts. This may not lead to the development of safe
LLMs that can address ethical challenges arising in real-world applications. In
this paper, we create Eagle datasets extracted from real interactions between
ChatGPT and users that exhibit social biases, toxicity, and immoral problems.
Our experiments show that Eagle captures complementary aspects, not covered by
existing datasets proposed for evaluation and mitigation of such ethical
challenges. Our code is publicly available at
https://huggingface.co/datasets/MasahiroKaneko/eagle.
\\ ( https://arxiv.org/abs/2402.14258 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14259
Date: Thu, 22 Feb 2024 03:46:08 GMT   (1658kb,D)

Title: Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form
  Medical Question Answering Applications and Beyond
Authors: Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen,
  Huaxiu Yao, Yue Zhang, Ren Wang, Kaidi Xu, Xiaoshuang Shi
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages
\\
  Uncertainty estimation plays a pivotal role in ensuring the reliability of
safety-critical human-AI interaction systems, particularly in the medical
domain. However, a general method for quantifying the uncertainty of free-form
answers has yet to be established in open-ended medical question-answering (QA)
tasks, where irrelevant words and sequences with limited semantic information
can be the primary source of uncertainty due to the presence of generative
inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which
calibrates the uncertainty proportion at both the word and sequence levels
according to the semantic relevance, with greater emphasis placed on keywords
and more relevant sequences when performing uncertainty quantification. We
compare WSE with 6 baseline methods on 5 free-form medical QA datasets,
utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE
exhibits superior performance on accurate uncertainty measurement under two
standard criteria for correctness evaluation (e.g., WSE outperforms existing
state-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, in
terms of the potential for real-world medical QA applications, we achieve a
significant enhancement in the performance of LLMs when employing sequences
with lower uncertainty, identified by WSE, as final answers (e.g., +6.36%
accuracy improvement on the COVID-QA dataset), without requiring any additional
task-specific fine-tuning or architectural modifications.
\\ ( https://arxiv.org/abs/2402.14259 ,  1658kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14268
Date: Thu, 22 Feb 2024 04:07:00 GMT   (4033kb,D)

Title: Can Large Language Models Detect Misinformation in Scientific News
  Reporting?
Authors: Yupeng Cao, Aishwarya Muralidharan Nair, Elyon Eyimife, Nastaran
  Jamalipour Soofi, K.P. Subbalakshmi, John R. Wullert II, Chumki Basu, David
  Shallcross
Categories: cs.CL cs.AI cs.SI
\\
  Scientific facts are often spun in the popular press with the intent to
influence public opinion and action, as was evidenced during the COVID-19
pandemic. Automatic detection of misinformation in the scientific domain is
challenging because of the distinct styles of writing in these two media types
and is still in its nascence. Most research on the validity of scientific
reporting treats this problem as a claim verification challenge. In doing so,
significant expert human effort is required to generate appropriate claims. Our
solution bypasses this step and addresses a more real-world scenario where such
explicit, labeled claims may not be available. The central research question of
this paper is whether it is possible to use large language models (LLMs) to
detect misinformation in scientific reporting. To this end, we first present a
new labeled dataset SciNews, containing 2.4k scientific news stories drawn from
trusted and untrustworthy sources, paired with related abstracts from the
CORD-19 database. Our dataset includes both human-written and LLM-generated
news articles, making it more comprehensive in terms of capturing the growing
trend of using LLMs to generate popular press articles. Then, we identify
dimensions of scientific validity in science news articles and explore how this
can be integrated into the automated detection of scientific misinformation. We
propose several baseline architectures using LLMs to automatically detect false
representations of scientific findings in the popular press. For each of these
architectures, we use several prompt engineering strategies including
zero-shot, few-shot, and chain-of-thought prompting. We also test these
architectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,
Llama2-13B.
\\ ( https://arxiv.org/abs/2402.14268 ,  4033kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14272
Date: Thu, 22 Feb 2024 04:14:10 GMT   (8326kb,D)

Title: Qsnail: A Questionnaire Dataset for Sequential Question Generation
Authors: Yan Lei, Liang Pang, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng
Categories: cs.CL
Comments: Accepted to the LREC-COLING 2024
\\
  The questionnaire is a professional research methodology used for both
qualitative and quantitative analysis of human opinions, preferences,
attitudes, and behaviors. However, designing and evaluating questionnaires
demands significant effort due to their intricate and complex structure.
Questionnaires entail a series of questions that must conform to intricate
constraints involving the questions, options, and overall structure.
Specifically, the questions should be relevant and specific to the given
research topic and intent. The options should be tailored to the questions,
ensuring they are mutually exclusive, completed, and ordered sensibly.
Moreover, the sequence of questions should follow a logical order, grouping
similar topics together. As a result, automatically generating questionnaires
presents a significant challenge and this area has received limited attention
primarily due to the scarcity of high-quality datasets. To address these
issues, we present Qsnail, the first dataset specifically constructed for the
questionnaire generation task, which comprises 13,168 human-written
questionnaires gathered from online platforms. We further conduct experiments
on Qsnail, and the results reveal that retrieval models and traditional
generative models do not fully align with the given research topic and intents.
Large language models, while more closely related to the research topic and
intents, exhibit significant limitations in terms of diversity and specificity.
Despite enhancements through the chain-of-thought prompt and finetuning,
questionnaires generated by language models still fall short of human-written
questionnaires. Therefore, questionnaire generation is challenging and needs to
be further explored. The dataset is available at:
https://github.com/LeiyanGithub/qsnail.
\\ ( https://arxiv.org/abs/2402.14272 ,  8326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14273
Date: Thu, 22 Feb 2024 04:20:14 GMT   (8066kb,D)

Title: Can Language Models Act as Knowledge Bases at Scale?
Authors: Qiyuan He and Yizhong Wang and Wenya Wang
Categories: cs.CL
\\
  Large language models (LLMs) have demonstrated remarkable proficiency in
understanding and generating responses to complex queries through large-scale
pre-training. However, the efficacy of these models in memorizing and reasoning
among large-scale structured knowledge, especially world knowledge that
explicitly covers abundant factual information remains questionable. Addressing
this gap, our research investigates whether LLMs can effectively store, recall,
and reason with knowledge on a large scale comparable to latest knowledge bases
(KBs) such as Wikidata. Specifically, we focus on three crucial aspects to
study the viability: (1) the efficiency of LLMs with different sizes in
memorizing the exact knowledge in the large-scale KB; (2) the flexibility of
recalling the memorized knowledge in response to natural language queries; (3)
the capability to infer new knowledge through reasoning. Our findings indicate
that while LLMs hold promise as large-scale KBs capable of retrieving and
responding with flexibility, enhancements in their reasoning capabilities are
necessary to fully realize their potential.
\\ ( https://arxiv.org/abs/2402.14273 ,  8066kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14277
Date: Thu, 22 Feb 2024 04:36:14 GMT   (93kb,D)

Title: GATE X-E : A Challenge Set for Gender-Fair Translations from
  Weakly-Gendered Languages
Authors: Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary
Categories: cs.CL cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2311.08836
\\
  Neural Machine Translation (NMT) continues to improve in quality and
adoption, yet the inadvertent perpetuation of gender bias remains a significant
concern. Despite numerous studies on gender bias in translations into English
from weakly gendered-languages, there are no benchmarks for evaluating this
phenomenon or for assessing mitigation strategies. To address this gap, we
introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus,
that consists of human translations from Turkish, Hungarian, Finnish, and
Persian into English. Each translation is accompanied by feminine, masculine,
and neutral variants. The dataset, which contains between 1250 and 1850
instances for each of the four language pairs, features natural sentences with
a wide range of sentence lengths and domains, challenging translation rewriters
on various linguistic phenomena. Additionally, we present a translation gender
rewriting solution built with GPT-4 and use GATE X-E to evaluate it. We open
source our contributions to encourage further research on gender debiasing.
\\ ( https://arxiv.org/abs/2402.14277 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14279
Date: Thu, 22 Feb 2024 04:41:52 GMT   (917kb,D)

Title: Mitigating the Linguistic Gap with Phonemic Representations for Robust
  Multilingual Language Understanding
Authors: Haeji Jung, Changdae Oh, Jooeon Kang, Jimin Sohn, Kyungwoo Song,
  Jinkyu Kim, David R. Mortensen
Categories: cs.CL cs.AI
\\
  Approaches to improving multilingual language understanding often require
multiple languages during the training phase, rely on complicated training
techniques, and -- importantly -- struggle with significant performance gaps
between high-resource and low-resource languages. We hypothesize that the
performance gaps between languages are affected by linguistic gaps between
those languages and provide a novel solution for robust multilingual language
modeling by employing phonemic representations (specifically, using phonemes as
input tokens to LMs rather than subwords). We present quantitative evidence
from three cross-lingual tasks that demonstrate the effectiveness of phonemic
representation, which is further justified by a theoretical analysis of the
cross-lingual performance gap.
\\ ( https://arxiv.org/abs/2402.14279 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14290
Date: Thu, 22 Feb 2024 05:07:31 GMT   (726kb,D)

Title: CEV-LM: Controlled Edit Vector Language Model for Shaping Natural
  Language Generations
Authors: Samraj Moorjani, Adit Krishnan, Hari Sundaram
Categories: cs.CL cs.LG
Comments: 16 pages, 3 figures, accepted into EACL 2024
\\
  As large-scale language models become the standard for text generation, there
is a greater need to tailor the generations to be more or less concise,
targeted, and informative, depending on the audience/application. Existing
control approaches primarily adjust the semantic (e.g., emotion, topics),
structural (e.g., syntax tree, parts-of-speech), and lexical (e.g.,
keyword/phrase inclusion) properties of text, but are insufficient to
accomplish complex objectives such as pacing which control the complexity and
readability of the text. In this paper, we introduce CEV-LM - a lightweight,
semi-autoregressive language model that utilizes constrained edit vectors to
control three complementary metrics (speed, volume, and circuitousness) that
quantify the shape of text (e.g., pacing of content). We study an extensive set
of state-of-the-art CTG models and find that CEV-LM provides significantly more
targeted and precise control of these three metrics while preserving semantic
content, using less training data, and containing fewer parameters.
\\ ( https://arxiv.org/abs/2402.14290 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14293
Date: Thu, 22 Feb 2024 05:15:27 GMT   (472kb,D)

Title: Leveraging Large Language Models for Concept Graph Recovery and Question
  Answering in NLP Education
Authors: Rui Yang, Boming Yang, Sixun Ouyang, Tianwei She, Aosong Feng, Yuang
  Jiang, Freddy Lecue, Jinghui Lu, Irene Li
Categories: cs.CL
\\
  In the domain of Natural Language Processing (NLP), Large Language Models
(LLMs) have demonstrated promise in text-generation tasks. However, their
educational applications, particularly for domain-specific queries, remain
underexplored. This study investigates LLMs' capabilities in educational
scenarios, focusing on concept graph recovery and question-answering (QA). We
assess LLMs' zero-shot performance in creating domain-specific concept graphs
and introduce TutorQA, a new expert-verified NLP-focused benchmark for
scientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA
pairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating
concept graphs with LLMs for answering diverse questions. Our results indicate
that LLMs' zero-shot concept graph recovery is competitive with supervised
methods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs
achieve up to 26% F1 score enhancement. Moreover, human evaluation and analysis
show that CGLLM generates answers with more fine-grained concepts.
\\ ( https://arxiv.org/abs/2402.14293 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14296
Date: Thu, 22 Feb 2024 05:17:49 GMT   (8029kb,D)

Title: Mitigating Biases of Large Language Models in Stance Detection with
  Calibration
Authors: Ang Li, Jingqian Zhao, Bin Liang, Lin Gui, Hui Wang, Xi Zeng, Kam-Fai
  Wong and Ruifeng Xu
Categories: cs.CL
\\
  Large language models (LLMs) have achieved remarkable progress in many
natural language processing tasks. However, our experiment reveals that, in
stance detection tasks, LLMs may generate biased stances due to spurious
sentiment-stance correlation and preference towards certain individuals and
topics, thus harming their performance. Therefore, in this paper, we propose to
Mitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In
which, a novel gated calibration network is devised to mitigate the biases on
the stance reasoning results from LLMs. Further, to make the calibration more
accurate and generalizable, we construct counterfactual augmented data to
rectify stance biases. Experimental results on in-target and zero-shot stance
detection tasks show that the proposed MB-Cal can effectively mitigate biases
of LLMs, achieving state-of-the-art results.
\\ ( https://arxiv.org/abs/2402.14296 ,  8029kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14298
Date: Thu, 22 Feb 2024 05:24:19 GMT   (3485kb,D)

Title: Multi-modal Stance Detection: New Datasets and Model
Authors: Bin Liang, Ang Li, Jingqian Zhao, Lin Gui, Min Yang, Yue Yu, Kam-Fai
  Wong and Ruifeng Xu
Categories: cs.CL
\\
  Stance detection is a challenging task that aims to identify public opinion
from social media platforms with respect to specific targets. Previous work on
stance detection largely focused on pure texts. In this paper, we study
multi-modal stance detection for tweets consisting of texts and images, which
are prevalent in today's fast-growing social media platforms where people often
post multi-modal messages. To this end, we create five new multi-modal stance
detection datasets of different domains based on Twitter, in which each example
consists of a text and an image. In addition, we propose a simple yet effective
Targeted Multi-modal Prompt Tuning framework (TMPT), where target information
is leveraged to learn multi-modal stance features from textual and visual
modalities. Experimental results on our three benchmark datasets show that the
proposed TMPT achieves state-of-the-art performance in multi-modal stance
detection.
\\ ( https://arxiv.org/abs/2402.14298 ,  3485kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14310
Date: Thu, 22 Feb 2024 05:58:03 GMT   (305kb,D)

Title: Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize
  Encoded Knowledge
Authors: Jinlan Fu, Shenzhen Huangfu, Hang Yan, See-Kiong Ng, Xipeng Qiu
Categories: cs.CL
Comments: 18 pages
\\
  Large Language Models (LLMs) have recently showcased remarkable
generalizability in various domains. Despite their extensive knowledge, LLMs
still face challenges in efficiently utilizing encoded knowledge to develop
accurate and logical reasoning processes. To mitigate this problem, we
introduced Hint-before-Solving Prompting (HSP), which guides the model to
generate hints (e.g., specific knowledge or key ideas) for solving the problem
and then generate solutions containing intermediate reasoning steps. Since HSP
is orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied
HSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results
of extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs
demonstrate that HSP can effectively improve the accuracy of reasoning tasks:
(1) By applying high-quality hint-enhanced HSP to CoT prompting,
Llama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free
LLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned
Llemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We
make our code and dataset publicly available at
\url{https://github.com/jinlanfu/HSP}.
\\ ( https://arxiv.org/abs/2402.14310 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14318
Date: Thu, 22 Feb 2024 06:21:41 GMT   (77kb,D)

Title: Assessing generalization capability of text ranking models in Polish
Authors: S{\l}awomir Dadas, Ma{\l}gorzata Gr\k{e}bowiec
Categories: cs.CL
\\
  Retrieval-augmented generation (RAG) is becoming an increasingly popular
technique for integrating internal knowledge bases with large language models.
In a typical RAG pipeline, three models are used, responsible for the
retrieval, reranking, and generation stages. In this article, we focus on the
reranking problem for the Polish language, examining the performance of
rerankers and comparing their results with available retrieval models. We
conduct a comprehensive evaluation of existing models and those trained by us,
utilizing a benchmark of 41 diverse information retrieval tasks for the Polish
language. The results of our experiments show that most models struggle with
out-of-domain generalization. However, a combination of effective optimization
method and a large training dataset allows for building rerankers that are both
compact in size and capable of generalization. The best of our models
establishes a new state-of-the-art for reranking in the Polish language,
outperforming existing models with up to 30 times more parameters.
\\ ( https://arxiv.org/abs/2402.14318 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14320
Date: Thu, 22 Feb 2024 06:23:37 GMT   (105kb,D)

Title: Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve
  Knowledge Base Question Answering
Authors: Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting
  Zhuang
Categories: cs.CL cs.AI
Comments: 8 pages
ACM-class: I.2.7
\\
  Recent progress with LLM-based agents has shown promising results across
various tasks. However, their use in answering questions from knowledge bases
remains largely unexplored. Implementing a KBQA system using traditional
methods is challenging due to the shortage of task-specific training data and
the complexity of creating task-focused model structures. In this paper, we
present Triad, a unified framework that utilizes an LLM-based agent with three
roles for KBQA tasks. The agent is assigned three roles to tackle different
KBQA subtasks: agent as a generalist for mastering various subtasks, as a
decision maker for the selection of candidates, and as an advisor for answering
questions with knowledge. Our KBQA framework is executed in four phases,
involving the collaboration of the agent's multiple roles. We evaluated the
performance of our framework using three benchmark datasets, and the results
show that our framework outperforms state-of-the-art systems on the LC-QuAD and
YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.
\\ ( https://arxiv.org/abs/2402.14320 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14328
Date: Thu, 22 Feb 2024 06:47:56 GMT   (8893kb,D)

Title: Understanding and Patching Compositional Reasoning in LLMs
Authors: Zhaoyi Li, Gangwei Jiang, Hong Xie, Linqi Song, Defu Lian, Ying Wei
Categories: cs.CL
Comments: Work In Progress
\\
  LLMs have marked a revolutonary shift, yet they falter when faced with
compositional reasoning tasks. Our research embarks on a quest to uncover the
root causes of compositional reasoning failures of LLMs, uncovering that most
of them stem from the improperly generated or leveraged implicit reasoning
results. Inspired by our empirical findings, we resort to Logit Lens and an
intervention experiment to dissect the inner hidden states of LLMs. This deep
dive reveals that implicit reasoning results indeed surface within middle
layers and play a causative role in shaping the final explicit reasoning
results. Our exploration further locates multi-head self-attention (MHSA)
modules within these layers, which emerge as the linchpins in accurate
generation and leveraing of implicit reasoning results. Grounded on the above
findings, we develop CREME, a lightweight method to patch errors in
compositional reasoning via editing the located MHSA modules. Our empirical
evidence stands testament to CREME's effectiveness, paving the way for
autonomously and continuously enhancing compositional reasoning capabilities in
language models.
\\ ( https://arxiv.org/abs/2402.14328 ,  8893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14334
Date: Thu, 22 Feb 2024 06:59:50 GMT   (1175kb,D)

Title: INSTRUCTIR: A Benchmark for Instruction Following of Information
  Retrieval Models
Authors: Hanseok Oh, Hyunji Lee, Seonghyeon Ye, Haebin Shin, Hansol Jang,
  Changwook Jun, Minjoon Seo
Categories: cs.CL
\\
  Despite the critical need to align search targets with users' intention,
retrievers often only prioritize query information without delving into the
users' intended search context. Enhancing the capability of retrievers to
understand intentions and preferences of users, akin to language model
instructions, has the potential to yield more aligned search targets. Prior
studies restrict the application of instructions in information retrieval to a
task description format, neglecting the broader context of diverse and evolving
search scenarios. Furthermore, the prevailing benchmarks utilized for
evaluation lack explicit tailoring to assess instruction-following ability,
thereby hindering progress in this field. In response to these limitations, we
propose a novel benchmark,INSTRUCTIR, specifically designed to evaluate
instruction-following ability in information retrieval tasks. Our approach
focuses on user-aligned instructions tailored to each query instance,
reflecting the diverse characteristics inherent in real-world search scenarios.
Through experimental analysis, we observe that retrievers fine-tuned to follow
task-style instructions, such as INSTRUCTOR, can underperform compared to their
non-instruction-tuned counterparts. This underscores potential overfitting
issues inherent in constructing retrievers trained on existing
instruction-aware retrieval datasets.
\\ ( https://arxiv.org/abs/2402.14334 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14337
Date: Thu, 22 Feb 2024 07:12:34 GMT   (658kb,D)

Title: AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales
Authors: Hazel Kim
Categories: cs.CL cs.LG
\\
  Rationales behind answers not only explain model decisions but boost language
models to reason well on complex reasoning tasks. However, obtaining impeccable
rationales is often impossible. Besides, it is non-trivial to estimate the
degree to which the rationales are faithful enough to encourage model
performance. Thus, such reasoning tasks often compel models to output correct
answers under undesirable rationales and are sub-optimal compared to what the
models are fully capable of. In this work, we propose how to deal with
imperfect rationales causing aleatoric uncertainty. We first define the
ambiguous rationales with entropy scores of given rationales, using model prior
beliefs as informativeness. We then guide models to select one of two different
reasoning models according to the ambiguity of rationales. We empirically argue
that our proposed method produces robust performance superiority against the
adversarial quality of rationales and low-resource settings.
\\ ( https://arxiv.org/abs/2402.14337 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14355
Date: Thu, 22 Feb 2024 07:55:26 GMT   (362kb,D)

Title: Rule or Story, Which is a Better Commonsense Expression for Talking with
  Large Language Models?
Authors: Ning Bian, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, Le Sun
Categories: cs.CL
\\
  Building machines with commonsense has been a longstanding challenge in NLP
due to the reporting bias of commonsense rules and the exposure bias of
rule-based commonsense reasoning. In contrast, humans convey and pass down
commonsense implicitly through stories. This paper investigates the inherent
commonsense ability of large language models (LLMs) expressed through
storytelling. We systematically investigate and compare stories and rules for
retrieving and leveraging commonsense in LLMs. Experimental results on 28
commonsense QA datasets show that stories outperform rules as the expression
for retrieving commonsense from LLMs, exhibiting higher generation confidence
and commonsense accuracy. Moreover, stories are the more effective commonsense
expression for answering questions regarding daily events, while rules are more
effective for scientific questions. This aligns with the reporting bias of
commonsense in text corpora. We further show that the correctness and relevance
of commonsense stories can be further improved via iterative self-supervised
fine-tuning. These findings emphasize the importance of using appropriate
language to express, retrieve, and leverage commonsense for LLMs, highlighting
a promising direction for better exploiting their commonsense abilities.
\\ ( https://arxiv.org/abs/2402.14355 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14359
Date: Thu, 22 Feb 2024 07:58:29 GMT   (1134kb,D)

Title: Rethinking Scientific Summarization Evaluation: Grounding Explainable
  Metrics on Facet-aware Benchmark
Authors: Xiuying Chen, Tairan Wang, Qingqing Zhu, Taicheng Guo, Shen Gao,
  Zhiyong Lu, Xin Gao, Xiangliang Zhang
Categories: cs.CL
Comments: 14pages
\\
  The summarization capabilities of pretrained and large language models (LLMs)
have been widely validated in general areas, but their use in scientific
corpus, which involves complex sentences and specialized knowledge, has been
less assessed. This paper presents conceptual and experimental analyses of
scientific summarization, highlighting the inadequacies of traditional
evaluation methods, such as $n$-gram, embedding comparison, and QA,
particularly in providing explanations, grasping scientific concepts, or
identifying key content. Subsequently, we introduce the Facet-aware Metric
(FM), employing LLMs for advanced semantic matching to evaluate summaries based
on different aspects. This facet-aware approach offers a thorough evaluation of
abstracts by decomposing the evaluation task into simpler subtasks.Recognizing
the absence of an evaluation benchmark in this domain, we curate a Facet-based
scientific summarization Dataset (FD) with facet-level annotations. Our
findings confirm that FM offers a more logical approach to evaluating
scientific summaries. In addition, fine-tuned smaller models can compete with
LLMs in scientific contexts, while LLMs have limitations in learning from
in-context information in scientific domains. This suggests an area for future
enhancement of LLMs.
\\ ( https://arxiv.org/abs/2402.14359 ,  1134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14373
Date: Thu, 22 Feb 2024 08:26:56 GMT   (7494kb,D)

Title: Small Language Model Is a Good Guide for Large Language Model in Chinese
  Entity Relation Extraction
Authors: Xuemei Tang and Jun Wang and Qi Su
Categories: cs.CL
Comments: 12 pages, 5 tables, 3 figures
\\
  Recently, large language models (LLMs) have been successful in relational
extraction (RE) tasks, especially in the few-shot learning. An important
problem in the field of RE is long-tailed data, while not much attention is
currently paid to this problem using LLM approaches. Therefore, in this paper,
we propose SLCoLM, a model collaboration framework, to mitigate the data
long-tail problem. In our framework, We use the
``\textit{Training-Guide-Predict}'' strategy to combine the strengths of
pre-trained language models (PLMs) and LLMs, where a task-specific PLM
framework acts as a tutor, transfers task knowledge to the LLM, and guides the
LLM in performing RE tasks. Our experiments on a RE dataset rich in relation
types show that the approach in this paper facilitates RE of long-tail relation
types.
\\ ( https://arxiv.org/abs/2402.14373 ,  7494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14379
Date: Thu, 22 Feb 2024 08:48:21 GMT   (320kb,D)

Title: Novi jezi\v{c}ki modeli za srpski jezik
Authors: Mihailo \v{S}kori\'c
Categories: cs.CL
Comments: in Serbian language
\\
  The paper will briefly present the development history of transformer-based
language models for the Serbian language. Several new models for text
generation and vectorization, trained on the resources of the Society for
Language Resources and Technologies, will also be presented. Ten selected
vectorization models for Serbian, including two new ones, will be compared on
four natural language processing tasks. Paper will analyze which models are the
best for each selected task, how does their size and the size of their training
sets affect the performance on those tasks, and what is the optimal setting to
train the best language models for the Serbian language.
\\ ( https://arxiv.org/abs/2402.14379 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14382
Date: Thu, 22 Feb 2024 08:51:39 GMT   (6308kb,D)

Title: Enhancing Temporal Knowledge Graph Forecasting with Large Language
  Models via Chain-of-History Reasoning
Authors: Yuwei Xia, Ding Wang, Qiang Liu, Liang Wang, Shu Wu, Xiaoyu Zhang
Categories: cs.CL
\\
  Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based
on given histories. Most recent graph-based models excel at capturing
structural information within TKGs but lack semantic comprehension abilities.
Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has
emerged. However, the existing LLM-based model exhibits three shortcomings: (1)
It only focuses on the first-order history for prediction while ignoring
high-order historical information, resulting in the provided information for
LLMs being extremely limited. (2) LLMs struggle with optimal reasoning
performance under heavy historical information loads. (3) For TKG prediction,
the temporal reasoning capability of LLM alone is limited. To address the first
two challenges, we propose Chain-of-History (CoH) reasoning which explores
high-order histories step-by-step, achieving effective utilization of
high-order historical information for LLMs on TKG prediction. To address the
third issue, we design CoH as a paly-and-plug module to enhance the performance
of graph-based models for TKG prediction. Extensive experiments on three
datasets and backbones demonstrate the effectiveness of CoH.
\\ ( https://arxiv.org/abs/2402.14382 ,  6308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14404
Date: Thu, 22 Feb 2024 09:45:26 GMT   (8435kb,D)

Title: On the Tip of the Tongue: Analyzing Conceptual Representation in Large
  Language Models with Reverse-Dictionary Probe
Authors: Ningyu Xu, Qi Zhang, Menghan Zhang, Peng Qian, Xuanjing Huang
Categories: cs.CL cs.AI
Comments: 21 pages, 13 figures
\\
  Probing and enhancing large language models' reasoning capacity remains a
crucial open question. Here we re-purpose the reverse dictionary task as a case
study to probe LLMs' capacity for conceptual inference. We use in-context
learning to guide the models to generate the term for an object concept implied
in a linguistic description. Models robustly achieve high accuracy in this
task, and their representation space encodes information about object
categories and fine-grained features. Further experiments suggest that the
conceptual inference ability as probed by the reverse-dictionary task predicts
model's general reasoning performance across multiple benchmarks, despite
similar syntactic generalization behaviors across models. Explorative analyses
suggest that prompting LLMs with description$\Rightarrow$word examples may
induce generalization beyond surface-level differences in task construals and
facilitate models on broader commonsense reasoning problems.
\\ ( https://arxiv.org/abs/2402.14404 ,  8435kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14408
Date: Thu, 22 Feb 2024 09:49:26 GMT   (830kb,D)

Title: Transferring BERT Capabilities from High-Resource to Low-Resource
  Languages Using Vocabulary Matching
Authors: Piotr Rybak
Categories: cs.CL
\\
  Pre-trained language models have revolutionized the natural language
understanding landscape, most notably BERT (Bidirectional Encoder
Representations from Transformers). However, a significant challenge remains
for low-resource languages, where limited data hinders the effective training
of such models. This work presents a novel approach to bridge this gap by
transferring BERT capabilities from high-resource to low-resource languages
using vocabulary matching. We conduct experiments on the Silesian and Kashubian
languages and demonstrate the effectiveness of our approach to improve the
performance of BERT models even when the target language has minimal training
data. Our results highlight the potential of the proposed technique to
effectively train BERT models for low-resource languages, thus democratizing
access to advanced language understanding models.
\\ ( https://arxiv.org/abs/2402.14408 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14409
Date: Thu, 22 Feb 2024 09:51:08 GMT   (1443kb,D)

Title: Tug-of-War Between Knowledge: Exploring and Resolving Knowledge
  Conflicts in Retrieval-Augmented Language Models
Authors: Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin
  Xu, Qiuxia Li, Jun Zhao
Categories: cs.CL cs.AI cs.IR
Comments: Accepted at LREC-COLING 2024
\\
  Retrieval-augmented language models (RALMs) have demonstrated significant
potential in refining and expanding their internal memory by retrieving
evidence from external sources. However, RALMs will inevitably encounter
knowledge conflicts when integrating their internal memory with external
sources. Knowledge conflicts can ensnare RALMs in a tug-of-war between
knowledge, limiting their practical applicability. In this paper, we focus on
exploring and resolving knowledge conflicts in RALMs. First, we present an
evaluation framework for assessing knowledge conflicts across various
dimensions. Then, we investigate the behavior and preference of RALMs from the
following two perspectives: (1) Conflicts between internal memory and external
sources: We find that stronger RALMs emerge with the Dunning-Kruger effect,
persistently favoring their faulty internal memory even when correct evidence
is provided. Besides, RALMs exhibit an availability bias towards common
knowledge; (2) Conflicts between truthful, irrelevant and misleading evidence:
We reveal that RALMs follow the principle of majority rule, leaning towards
placing trust in evidence that appears more frequently. Moreover, we find that
RALMs exhibit confirmation bias, and are more willing to choose evidence that
is consistent with their internal memory. To solve the challenge of knowledge
conflicts, we propose a method called Conflict-Disentangle Contrastive Decoding
(CD2) to better calibrate the model's confidence. Experimental results
demonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.
\\ ( https://arxiv.org/abs/2402.14409 ,  1443kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14411
Date: Thu, 22 Feb 2024 09:56:51 GMT   (637kb,D)

Title: J-UniMorph: Japanese Morphological Annotation through the Universal
  Feature Schema
Authors: Kosuke Matsuzaki, Masaya Taniguchi, Kentaro Inui, Keisuke Sakaguchi
Categories: cs.CL
Comments: 14 pages, 4 figures
\\
  We introduce a Japanese Morphology dataset, J-UniMorph, developed based on
the UniMorph feature schema. This dataset addresses the unique and rich verb
forms characteristic of the language's agglutinative nature. J-UniMorph
distinguishes itself from the existing Japanese subset of UniMorph, which is
automatically extracted from Wiktionary. On average, the Wiktionary Edition
features around 12 inflected forms for each word and is primarily dominated by
denominal verbs (i.e., [noun] +suru (do-PRS)). Morphologically, this form is
equivalent to the verb suru (do). In contrast, J-UniMorph explores a much
broader and more frequently used range of verb forms, offering 118 inflected
forms for each word on average. It includes honorifics, a range of politeness
levels, and other linguistic nuances, emphasizing the distinctive
characteristics of the Japanese language. This paper presents detailed
statistics and characteristics of J-UniMorph, comparing it with the Wiktionary
Edition. We release J-UniMorph and its interactive visualizer publicly
available, aiming to support cross-linguistic research and various
applications.
\\ ( https://arxiv.org/abs/2402.14411 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14428
Date: Thu, 22 Feb 2024 10:17:57 GMT   (1545kb,D)

Title: KoCoSa: Korean Context-aware Sarcasm Detection Dataset
Authors: Yumin Kim, Heejae Suh, Mingi Kim, Dongyeon Won and Hwanhee Lee
Categories: cs.CL cs.AI
\\
  Sarcasm is a way of verbal irony where someone says the opposite of what they
mean, often to ridicule a person, situation, or idea. It is often difficult to
detect sarcasm in the dialogue since detecting sarcasm should reflect the
context (i.e., dialogue history). In this paper, we introduce a new dataset for
the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware
Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and
the labels for this task on the last response. To build the dataset, we propose
an efficient sarcasm detection dataset generation pipeline: 1) generating new
sarcastic dialogues from source dialogues with large language models, 2)
automatic and manual filtering of abnormal and toxic dialogues, and 3) human
annotation for the sarcasm detection task. We also provide a simple but
effective baseline for the Korean sarcasm detection task trained on our
dataset. Experimental results on the dataset show that our baseline system
outperforms strong baselines like large language models, such as GPT-3.5, in
the Korean sarcasm detection task. We show that the sarcasm detection task
relies deeply on the existence of sufficient context. We will release the
dataset at https://anonymous.4open.science/r/KoCoSa-2372.
\\ ( https://arxiv.org/abs/2402.14428 ,  1545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14433
Date: Thu, 22 Feb 2024 10:25:14 GMT   (3764kb,D)

Title: A Language Model's Guide Through Latent Space
Authors: Dimitri von R\"utte, Sotiris Anagnostidis, Gregor Bachmann, Thomas
  Hofmann
Categories: cs.CL cs.AI
ACM-class: I.2
\\
  Concept guidance has emerged as a cheap and simple way to control the
behavior of language models by probing their hidden representations for concept
vectors and using them to perturb activations at inference time. While the
focus of previous work has largely been on truthfulness, in this paper we
extend this framework to a richer set of concepts such as appropriateness,
humor, creativity and quality, and explore to what degree current detection and
guidance strategies work in these challenging settings. To facilitate
evaluation, we develop a novel metric for concept guidance that takes into
account both the success of concept elicitation as well as the potential
degradation in fluency of the guided model. Our extensive experiments reveal
that while some concepts such as truthfulness more easily allow for guidance
with current techniques, novel concepts such as appropriateness or humor either
remain difficult to elicit, need extensive tuning to work, or even experience
confusion. Moreover, we find that probes with optimal detection accuracies do
not necessarily make for the optimal guides, contradicting previous
observations for truthfulness. Our work warrants a deeper investigation into
the interplay between detectability, guidability, and the nature of the
concept, and we hope that our rich experimental test-bed for guidance research
inspires stronger follow-up approaches.
\\ ( https://arxiv.org/abs/2402.14433 ,  3764kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14453
Date: Thu, 22 Feb 2024 11:16:23 GMT   (2328kb,D)

Title: Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?
Authors: Seiji Gobara, Hidetaka Kamigaito and Taro Watanabe
Categories: cs.CL
Comments: 17pages
\\
  Education that suits the individual learning level is necessary to improve
students' understanding. The first step in achieving this purpose by using
large language models (LLMs) is to adjust the textual difficulty of the
response to students. This work analyzes how LLMs can implicitly adjust text
difficulty between user input and its generated text. To conduct the
experiments, we created a new dataset from Stack-Overflow to explore the
performance of question-answering-based conversation. Experimental results on
the Stack-Overflow dataset and the TSCC dataset, including multi-turn
conversation show that LLMs can implicitly handle text difficulty between user
input and its generated response. We also observed that some LLMs can surpass
humans in handling text difficulty and the importance of instruction-tuning.
\\ ( https://arxiv.org/abs/2402.14453 ,  2328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14457
Date: Thu, 22 Feb 2024 11:24:45 GMT   (757kb)

Title: Annotation and Classification of Relevant Clauses in
  Terms-and-Conditions Contracts
Authors: Pietro Giovanni Bizzaro, Elena Della Valentina, Maurizio Napolitano,
  Nadia Mana and Massimo Zancanaro
Categories: cs.CL
Comments: Pre-review version of the paper accepted to the 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING) 2024
\\
  In this paper, we propose a new annotation scheme to classify different types
of clauses in Terms-and-Conditions contracts with the ultimate goal of
supporting legal experts to quickly identify and assess problematic issues in
this type of legal documents. To this end, we built a small corpus of
Terms-and-Conditions contracts and finalized an annotation scheme of 14
categories, eventually reaching an inter-annotator agreement of 0.92. Then, for
11 of them, we experimented with binary classification tasks using few-shot
prompting with a multilingual T5 and two fine-tuned versions of two BERT-based
LLMs for Italian. Our experiments showed the feasibility of automatic
classification of our categories by reaching accuracies ranging from .79 to .95
on validation tasks.
\\ ( https://arxiv.org/abs/2402.14457 ,  757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14458
Date: Thu, 22 Feb 2024 11:31:50 GMT   (90kb,D)

Title: NLAS-multi: A Multilingual Corpus of Automatically Generated Natural
  Language Argumentation Schemes
Authors: Ramon Ruiz-Dolz, Joaquin Taverner, John Lawrence and Chris Reed
Categories: cs.CL cs.AI
\\
  Some of the major limitations identified in the areas of argument mining,
argument generation, and natural language argument analysis are related to the
complexity of annotating argumentatively rich data, the limited size of these
corpora, and the constraints that represent the different languages and domains
in which these data is annotated. To address these limitations, in this paper
we present the following contributions: (i) an effective methodology for the
automatic generation of natural language arguments in different topics and
languages, (ii) the largest publicly available corpus of natural language
argumentation schemes, and (iii) a set of solid baselines and fine-tuned models
for the automatic identification of argumentation schemes.
\\ ( https://arxiv.org/abs/2402.14458 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14484
Date: Thu, 22 Feb 2024 12:19:04 GMT   (711kb,D)

Title: Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation
  and Analysis
Authors: Takehiro Takayanagi and Masahiro Suzuki and Ryotaro Kobayashi and
  Hiroki Sakaji and Kiyoshi Izumi
Categories: cs.CL
\\
  Causality is fundamental in human cognition and has drawn attention in
diverse research fields. With growing volumes of textual data, discerning
causalities within text data is crucial, and causal text mining plays a pivotal
role in extracting meaningful patterns. This study conducts comprehensive
evaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce
a benchmark that extends beyond general English datasets, including
domain-specific and non-English datasets. We also provide an evaluation
framework to ensure fair comparisons between ChatGPT and previous approaches.
Finally, our analysis outlines the limitations and future challenges in
employing ChatGPT for causal text mining. Specifically, our analysis reveals
that ChatGPT serves as a good starting point for various datasets. However,
when equipped with a sufficient amount of training data, previous models still
surpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency
to falsely recognize non-causal sequences as causal sequences. These issues
become even more pronounced with advanced versions of the model, such as GPT-4.
In addition, we highlight the constraints of ChatGPT in handling complex
causality types, including both intra/inter-sentential and implicit causality.
The model also faces challenges with effectively leveraging in-context learning
and domain adaptation. Our code is available on
\url{https://github.com/retarfi/gemcausal}
\\ ( https://arxiv.org/abs/2402.14484 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14488
Date: Thu, 22 Feb 2024 12:26:07 GMT   (150kb,D)

Title: Does the Generator Mind its Contexts? An Analysis of Generative Model
  Faithfulness under Context Transfer
Authors: Xinshuo Hu and Baotian Hu and Dongfang Li and Xiaoguang Li and Lifeng
  Shang
Categories: cs.CL
Comments: LREC-Coling 2024
\\
  The present study introduces the knowledge-augmented generator, which is
specifically designed to produce information that remains grounded in
contextual knowledge, regardless of alterations in the context. Previous
research has predominantly focused on examining hallucinations stemming from
static input, such as in the domains of summarization or machine translation.
However, our investigation delves into the faithfulness of generative question
answering in the presence of dynamic knowledge. Our objective is to explore the
existence of hallucinations arising from parametric memory when contextual
knowledge undergoes changes, while also analyzing the underlying causes for
their occurrence. In order to efficiently address this issue, we propose a
straightforward yet effective measure for detecting such hallucinations.
Intriguingly, our investigation uncovers that all models exhibit a tendency to
generate previous answers as hallucinations. To gain deeper insights into the
underlying causes of this phenomenon, we conduct a series of experiments that
verify the critical role played by context in hallucination, both during
training and testing, from various perspectives.
\\ ( https://arxiv.org/abs/2402.14488 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14492
Date: Thu, 22 Feb 2024 12:35:50 GMT   (1300kb,D)

Title: INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction
  Fine-tuning
Authors: Wei Han, Hui Chen, Soujanya Poria
Categories: cs.CL cs.AI
Comments: 23 pages, 7 figures
\\
  Fine-tuning large language models (LLMs) on multi-task instruction-following
data has been proven to be a powerful learning paradigm for improving their
zero-shot capabilities on new tasks. Recent works about high-quality
instruction-following data generation and selection require amounts of human
labor to conceive model-understandable instructions for the given tasks and
carefully filter the LLM-generated data. In this work, we introduce an
automatic instruction augmentation method named INSTRAUG in multimodal tasks.
It starts from a handful of basic and straightforward meta instructions but can
expand an instruction-following dataset by 30 times. Results on two popular
multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show
that INSTRAUG can significantly improve the alignment of multimodal large
language models (MLLMs) across 12 multimodal tasks, which is even equivalent to
the benefits of scaling up training data multiple times.
\\ ( https://arxiv.org/abs/2402.14492 ,  1300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14494
Date: Thu, 22 Feb 2024 12:39:50 GMT   (2211kb,D)

Title: Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment
  Pre-training for Noisy Slot Filling Task
Authors: Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song,
  Daichi Guo, Weiran Xu
Categories: cs.CL
\\
  In a realistic dialogue system, the input information from users is often
subject to various types of input perturbations, which affects the slot-filling
task. Although rule-based data augmentation methods have achieved satisfactory
results, they fail to exhibit the desired generalization when faced with
unknown noise disturbances. In this study, we address the challenges posed by
input perturbations in slot filling by proposing Noise-BERT, a unified
Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework
incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and
Sentence Noisiness Discrimination, aiming to guide the pre-trained language
model in capturing accurate slot information and noise distribution. During
fine-tuning, we employ a contrastive learning loss to enhance the semantic
representation of entities and labels. Additionally, we introduce an
adversarial attack training strategy to improve the model's robustness.
Experimental results demonstrate the superiority of our proposed approach over
state-of-the-art models, and further analysis confirms its effectiveness and
generalization ability.
\\ ( https://arxiv.org/abs/2402.14494 ,  2211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14499
Date: Thu, 22 Feb 2024 12:47:33 GMT   (7355kb,D)

Title: "My Answer is C": First-Token Probabilities Do Not Match Text Answers in
  Instruction-Tuned Language Models
Authors: Xinpeng Wang, Bolei Ma, Chengzhi Hu, Leon Weber-Genzel, Paul
  R\"ottger, Frauke Kreuter, Dirk Hovy, Barbara Plank
Categories: cs.CL
\\
  The open-ended nature of language generation makes the evaluation of
autoregressive large language models (LLMs) challenging. One common evaluation
approach uses multiple-choice questions (MCQ) to limit the response space. The
model is then evaluated by ranking the candidate answers by the log probability
of the first token prediction. However, first-tokens may not consistently
reflect the final response output, due to model's diverse response styles such
as starting with "Sure" or refusing to answer. Consequently, MCQ evaluation is
not indicative of model behaviour when interacting with users. But by how much?
We evaluate how aligned first-token evaluation is with the text output along
several dimensions, namely final option choice, refusal rate, choice
distribution and robustness under prompt perturbation. Our results show that
the two approaches are severely misaligned on all dimensions, reaching mismatch
rates over 60%. Models heavily fine-tuned on conversational or safety data are
especially impacted. Crucially, models remain misaligned even when we
increasingly constrain prompts, i.e., force them to start with an option letter
or example template. Our findings i) underscore the importance of inspecting
the text output, too and ii) caution against relying solely on first-token
evaluation.
\\ ( https://arxiv.org/abs/2402.14499 ,  7355kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14521
Date: Thu, 22 Feb 2024 13:12:05 GMT   (1228kb,D)

Title: Malaysian English News Decoded: A Linguistic Resource for Named Entity
  and Relation Extraction
Authors: Mohan Raj Chanthran, Lay-Ki Soon, Huey Fang Ong, Bhawani Selvaretnam
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Standard English and Malaysian English exhibit notable differences, posing
challenges for natural language processing (NLP) tasks on Malaysian English.
Unfortunately, most of the existing datasets are mainly based on standard
English and therefore inadequate for improving NLP tasks in Malaysian English.
An experiment using state-of-the-art Named Entity Recognition (NER) solutions
on Malaysian English news articles highlights that they cannot handle
morphosyntactic variations in Malaysian English. To the best of our knowledge,
there is no annotated dataset available to improvise the model. To address
these issues, we constructed a Malaysian English News (MEN) dataset, which
contains 200 news articles that are manually annotated with entities and
relations. We then fine-tuned the spaCy NER tool and validated that having a
dataset tailor-made for Malaysian English could improve the performance of NER
in Malaysian English significantly. This paper presents our effort in the data
acquisition, annotation methodology, and thorough analysis of the annotated
dataset. To validate the quality of the annotation, inter-annotator agreement
was used, followed by adjudication of disagreements by a subject matter expert.
Upon completion of these tasks, we managed to develop a dataset with 6,061
entities and 3,268 relation instances. Finally, we discuss on spaCy fine-tuning
setup and analysis on the NER performance. This unique dataset will contribute
significantly to the advancement of NLP research in Malaysian English, allowing
researchers to accelerate their progress, particularly in NER and relation
extraction. The dataset and annotation guideline has been published on Github.
\\ ( https://arxiv.org/abs/2402.14521 ,  1228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14522
Date: Thu, 22 Feb 2024 13:13:31 GMT   (1963kb,D)

Title: Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap
  for Prompt-Based Large Language Models and Beyond
Authors: Xinyu Wang, Hainiu Xu, Lin Gui, Yulan He
Categories: cs.CL cs.LG
\\
  Task embedding, a meta-learning technique that captures task-specific
information, has become prevalent, especially in areas such as multi-task
learning, model editing, and interpretability. However, it faces challenges
with the emergence of prompt-guided Large Language Models (LLMs) operating in a
gradientfree manner. Existing task embedding methods rely on fine-tuned,
task-specific language models, which hinders the adaptability of task
embeddings across diverse models, especially prompt-based LLMs. To unleash the
power of task embedding in the era of LLMs, we propose a framework for unified
task embeddings (FUTE), harmonizing task embeddings from various models,
including smaller language models and LLMs with varied prompts, within a single
vector space. Such uniformity enables the comparison and analysis of
similarities amongst different models, extending the scope and utility of
existing task embedding methods in addressing multi-model scenarios, whilst
maintaining their performance to be comparable to architecture-specific
methods.
\\ ( https://arxiv.org/abs/2402.14522 ,  1963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14523
Date: Thu, 22 Feb 2024 13:15:49 GMT   (32635kb,D)

Title: Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding
  Decomposition
Authors: Rendi Chevi, Alham Fikri Aji
Categories: cs.CL cs.SD eess.AS
Comments: Project Page: https://rendchevi.github.io/daisy-tts
\\
  We often verbally express emotions in a multifaceted manner, they may vary in
their intensities and may be expressed not just as a single but as a mixture of
emotions. This wide spectrum of emotions is well-studied in the structural
model of emotions, which represents variety of emotions as derivative products
of primary emotions with varying degrees of intensity. In this paper, we
propose an emotional text-to-speech design to simulate a wider spectrum of
emotions grounded on the structural model. Our proposed design, Daisy-TTS,
incorporates a prosody encoder to learn emotionally-separable prosody embedding
as a proxy for emotion. This emotion representation allows the model to
simulate: (1) Primary emotions, as learned from the training samples, (2)
Secondary emotions, as a mixture of primary emotions, (3) Intensity-level, by
scaling the emotion embedding, and (4) Emotions polarity, by negating the
emotion embedding. Through a series of perceptual evaluations, Daisy-TTS
demonstrated overall higher emotional speech naturalness and emotion
perceiveability compared to the baseline.
\\ ( https://arxiv.org/abs/2402.14523 ,  32635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14526
Date: Thu, 22 Feb 2024 13:20:53 GMT   (7013kb,D)

Title: Balanced Data Sampling for Language Model Training with Clustering
Authors: Yunfan Shao, Linyang Li, Zhaoye Fei, Hang Yan, Dahua Lin, Xipeng Qiu
Categories: cs.CL cs.AI
\\
  Data plays a fundamental role in the training of Large Language Models
(LLMs). While attention has been paid to the collection and composition of
datasets, determining the data sampling strategy in training remains an open
question. Most LLMs are trained with a simple strategy, random sampling.
However, this sampling strategy ignores the unbalanced nature of training data
distribution, which can be sub-optimal. In this paper, we propose ClusterClip
Sampling to balance the text distribution of training data for better model
training. Specifically, ClusterClip Sampling utilizes data clustering to
reflect the data distribution of the training set and balances the common
samples and rare samples during training based on the cluster results. A
repetition clip operation is introduced to mitigate the overfitting issue led
by samples from certain clusters. Extensive experiments validate the
effectiveness of ClusterClip Sampling, which outperforms random sampling and
other cluster-based sampling variants under various training datasets and large
language models.
\\ ( https://arxiv.org/abs/2402.14526 ,  7013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14531
Date: Thu, 22 Feb 2024 13:24:10 GMT   (2796kb)

Title: Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt
  Politeness on LLM Performance
Authors: Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine
Categories: cs.CL
\\
  We investigate the impact of politeness levels in prompts on the performance
of large language models (LLMs). Polite language in human communications often
garners more compliance and effectiveness, while rudeness can cause aversion,
impacting response quality. We consider that LLMs mirror human communication
traits, suggesting they align with human cultural norms. We assess the impact
of politeness in prompts on LLMs across English, Chinese, and Japanese tasks.
We observed that impolite prompts often result in poor performance, but overly
polite language does not guarantee better outcomes. The best politeness level
is different according to the language. This phenomenon suggests that LLMs not
only reflect human behavior but are also influenced by language, particularly
in different cultural contexts. Our findings highlight the need to factor in
politeness for cross-cultural natural language processing and LLM usage.
\\ ( https://arxiv.org/abs/2402.14531 ,  2796kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14533
Date: Thu, 22 Feb 2024 13:25:17 GMT   (156kb,D)

Title: Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for
  GPT-3.5, GPT-4 and Bard
Authors: Ariel Rosenfeld, Teddy Lazebnik
Categories: cs.CL
\\
  Large Language Models (LLMs) are capable of generating text that is similar
to or surpasses human quality. However, it is unclear whether LLMs tend to
exhibit distinctive linguistic styles akin to how human authors do. Through a
comprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech
(POS) distribution, dependency distribution, and sentiment of texts generated
by three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse
inputs. The results point to significant linguistic variations which, in turn,
enable us to attribute a given text to its LLM origin with a favorable 88\%
accuracy using a simple off-the-shelf classification model. Theoretical and
practical implications of this intriguing finding are discussed.
\\ ( https://arxiv.org/abs/2402.14533 ,  156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14536
Date: Thu, 22 Feb 2024 13:26:56 GMT   (1399kb,D)

Title: Domain Generalization via Causal Adjustment for Cross-Domain Sentiment
  Analysis
Authors: Siyin Wang, Jie Zhou, Qin Chen, Qi Zhang, Tao Gui, Xuanjing Huang
Categories: cs.CL
\\
  Domain adaption has been widely adapted for cross-domain sentiment analysis
to transfer knowledge from the source domain to the target domain. Whereas,
most methods are proposed under the assumption that the target (test) domain is
known, making them fail to generalize well on unknown test data that is not
always available in practice. In this paper, we focus on the problem of domain
generalization for cross-domain sentiment analysis. Specifically, we propose a
backdoor adjustment-based causal model to disentangle the domain-specific and
domain-invariant representations that play essential roles in tackling domain
shift. First, we rethink the cross-domain sentiment analysis task in a causal
view to model the causal-and-effect relationships among different variables.
Then, to learn an invariant feature representation, we remove the effect of
domain confounders (e.g., domain knowledge) using the backdoor adjustment. A
series of experiments over many homologous and diverse datasets show the great
performance and robustness of our model by comparing it with the
state-of-the-art domain generalization baselines.
\\ ( https://arxiv.org/abs/2402.14536 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14545
Date: Thu, 22 Feb 2024 13:33:13 GMT   (2363kb,D)

Title: Less is More: Mitigating Multimodal Hallucination from an EOS Decision
  Perspective
Authors: Zihao Yue, Liang Zhang, Qin Jin
Categories: cs.CL cs.CV
\\
  Large Multimodal Models (LMMs) often suffer from multimodal hallucinations,
wherein they may create content that is not present in the visual inputs. In
this paper, we explore a new angle of this issue: overly detailed training data
hinders the model's ability to timely terminate generation, leading to
continued outputs beyond visual perception limits. By investigating how the
model decides to terminate generation with EOS, the special end-of-sentence
token, we find that the model assesses the completeness of the entire sequence
by comparing the generated text with the image. This observation suggests that
the model possesses an inherent potential of making proper EOS decisions based
on its visual perception to avoid overly lengthy outputs. To take advantage of
such potential, we explore two methods to mitigate multimodal hallucinations: a
training objective that enables the model to reduce hallucinations by learning
from regular instruction data, and a data filtering strategy to prevent harmful
training data from exacerbating model hallucinations. Both methods
significantly improve the hallucination performance of LMMs, without requiring
any additional data or knowledge.
\\ ( https://arxiv.org/abs/2402.14545 ,  2363kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14558
Date: Thu, 22 Feb 2024 13:52:02 GMT   (130kb,D)

Title: LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A
  Survey
Authors: Ashok Urlana, Charaka Vinayak Kumar, Ajeet Kumar Singh, Bala
  Mallikarjunarao Garlapati, Srinivasa Rao Chalamala, Rahul Mishra
Categories: cs.CL
Comments: 25 pages, 7 figures
\\
  Large language models (LLMs) have become the secret ingredient driving
numerous industrial applications, showcasing their remarkable versatility
across a diverse spectrum of tasks. From natural language processing and
sentiment analysis to content generation and personalized recommendations,
their unparalleled adaptability has facilitated widespread adoption across
industries. This transformative shift driven by LLMs underscores the need to
explore the underlying associated challenges and avenues for enhancement in
their utilization. In this paper, our objective is to unravel and evaluate the
obstacles and opportunities inherent in leveraging LLMs within an industrial
context. To this end, we conduct a survey involving a group of industry
practitioners, develop four research questions derived from the insights
gathered, and examine 68 industry papers to address these questions and derive
meaningful conclusions.
\\ ( https://arxiv.org/abs/2402.14558 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14568
Date: Thu, 22 Feb 2024 14:19:56 GMT   (8078kb,D)

Title: LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named
  Entity Recognition
Authors: Junjie Ye, Nuo Xu, Yikun Wang, Jie Zhou, Qi Zhang, Tao Gui, Xuanjing
  Huang
Categories: cs.CL
\\
  Despite the impressive capabilities of large language models (LLMs), their
performance on information extraction tasks is still not entirely satisfactory.
However, their remarkable rewriting capabilities and extensive world knowledge
offer valuable insights to improve these tasks. In this paper, we propose
$LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot
NER task. To overcome the limitations of existing data augmentation methods
that compromise semantic integrity and address the uncertainty inherent in
LLM-generated text, we leverage the distinctive characteristics of the NER task
by augmenting the original data at both the contextual and entity levels. Our
approach involves employing 14 contextual rewriting strategies, designing
entity replacements of the same type, and incorporating noise injection to
enhance robustness. Extensive experiments demonstrate the effectiveness of our
approach in enhancing NER model performance with limited data. Furthermore,
additional analyses provide further evidence supporting the assertion that the
quality of the data we generate surpasses that of other existing methods.
\\ ( https://arxiv.org/abs/2402.14568 ,  8078kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14614
Date: Thu, 22 Feb 2024 15:03:25 GMT   (7974kb,D)

Title: Two Counterexamples to \textit{Tokenization and the Noiseless Channel}
Authors: Marco Cognetta and Vil\'em Zouhar and Sangwhan Moon and Naoaki Okazaki
Categories: cs.CL
Comments: 9 pages, 2 figures, to appear in LREC-COLING 2024
\\
  In \textit{Tokenization and the Noiseless Channel}
\cite{zouhar-etal-2023-tokenization}, R\'enyi efficiency is suggested as an
intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer
which leads to the highest R\'enyi efficiency of the unigram distribution
should be chosen. The R\'enyi efficiency is thus treated as a predictor of
downstream performance (e.g., predicting BLEU for a machine translation task),
without the expensive step of training multiple models with different
tokenizers. Although useful, the predictive power of this metric is not
perfect, and the authors note there are additional qualities of a good
tokenization scheme that R\'enyi efficiency alone cannot capture.
  We describe two variants of BPE tokenization which can arbitrarily increase
R\'enyi efficiency while decreasing the downstream model performance. These
counterexamples expose cases where R\'enyi efficiency fails as an intrinsic
tokenization metric and thus give insight for building more accurate
predictors.
\\ ( https://arxiv.org/abs/2402.14614 ,  7974kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14616
Date: Thu, 22 Feb 2024 15:04:24 GMT   (504kb,D)

Title: The Impact of Word Splitting on the Semantic Content of Contextualized
  Word Representations
Authors: Aina Gar\'i Soler, Matthieu Labeau and Chlo\'e Clavel
Categories: cs.CL
Comments: Accepted to TACL
\\
  When deriving contextualized word representations from language models, a
decision needs to be made on how to obtain one for out-of-vocabulary (OOV)
words that are segmented into subwords. What is the best way to represent these
words with a single vector, and are these representations of worse quality than
those of in-vocabulary words? We carry out an intrinsic evaluation of
embeddings from different models on semantic similarity tasks involving OOV
words. Our analysis reveals, among other interesting findings, that the quality
of representations of words that are split is often, but not always, worse than
that of the embeddings of known words. Their similarity values, however, must
be interpreted with caution.
\\ ( https://arxiv.org/abs/2402.14616 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14652
Date: Thu, 22 Feb 2024 16:04:03 GMT   (4775kb,D)

Title: Cleaner Pretraining Corpus Curation with Neural Web Scraping
Authors: Zhipeng Xu, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Chenyan Xiong, Ge Yu
Categories: cs.CL
\\
  The web contains large-scale, diverse, and abundant information to satisfy
the information-seeking needs of humans. Through meticulous data collection,
preprocessing, and curation, webpages can be used as a fundamental data
resource for language model pretraining. However, when confronted with the
progressively revolutionized and intricate nature of webpages,
rule-based/feature-based web scrapers are becoming increasingly inadequate.
This paper presents a simple, fast, and effective Neural web Scraper
(NeuScraper) to help extract primary and clean text contents from webpages.
Experimental results show that NeuScraper surpasses the baseline scrapers by
achieving more than a 20% improvement, demonstrating its potential in
extracting higher-quality data to facilitate the language model pretraining.
All of the code is available at https://github.com/OpenMatch/NeuScraper.
\\ ( https://arxiv.org/abs/2402.14652 ,  4775kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14660
Date: Thu, 22 Feb 2024 16:06:49 GMT   (2292kb,D)

Title: ConceptMath: A Bilingual Concept-wise Benchmark for Measuring
  Mathematical Reasoning of Large Language Models
Authors: Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing
  Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang,
  Wenbo Su, Bo Zheng
Categories: cs.CL cs.AI
Comments: The benchmark dataset will be released soon
\\
  This paper introduces ConceptMath, a bilingual (English and Chinese),
fine-grained benchmark that evaluates concept-wise mathematical reasoning of
Large Language Models (LLMs). Unlike traditional benchmarks that evaluate
general mathematical reasoning with an average accuracy, ConceptMath
systematically organizes math problems under a hierarchy of math concepts, so
that mathematical reasoning can be evaluated at different granularity with
concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range
of LLMs, and we observe existing LLMs, though achieving high average accuracies
on traditional benchmarks, exhibit significant performance variations across
different math concepts and may even fail catastrophically on the most basic
ones. Besides, we also introduce an efficient fine-tuning strategy to enhance
the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the
developers to understand the fine-grained mathematical abilities of their
models and facilitate the growth of foundation models.
\\ ( https://arxiv.org/abs/2402.14660 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14672
Date: Thu, 22 Feb 2024 16:18:07 GMT   (9299kb,D)

Title: Middleware for LLMs: Tools Are Instrumental for Language Agents in
  Complex Environments
Authors: Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth
  Srinivasa, Hugo Latapie, Yu Su
Categories: cs.CL cs.AI
Comments: 16 pages, 8 figures, 4 tables
ACM-class: I.2.7
\\
  The applications of large language models (LLMs) have expanded well beyond
the confines of text processing, signaling a new era where LLMs are envisioned
as generalist language agents capable of operating within complex real-world
environments. These environments are often highly expansive, making it
impossible for the LLM to process them within its short-term memory. Motivated
by recent research on extending the capabilities of LLMs with tools, this paper
investigates the intriguing potential of tools to augment LLMs in handling such
complexity. To this end, we design customized tools to aid in the proactive
exploration within these massive environments. Such tools can serve as a
middleware layer shielding the LLM from environmental complexity. In two
representative complex environments -- knowledge bases (KBs) and databases --
we demonstrate the significant potential of augmenting language agents with
tools in complex environments. Notably, equipped with these tools, GPT-4
achieves 2.8X the performance of the best baseline in tasks requiring access to
database content and 2.2X in KB tasks. Our findings illuminate the path for
advancing language agents in complex real-world applications.
\\ ( https://arxiv.org/abs/2402.14672 ,  9299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14679
Date: Thu, 22 Feb 2024 16:32:08 GMT   (129kb,D)

Title: Is Cognition and Action Consistent or Not: Investigating Large Language
  Model's Personality
Authors: Yiming Ai, Zhiwei He, Ziyin Zhang, Wenhong Zhu, Hongkun Hao, Kai Yu,
  Lingjun Chen and Rui Wang
Categories: cs.CL cs.CY
\\
  In this study, we investigate the reliability of Large Language Models (LLMs)
in professing human-like personality traits through responses to personality
questionnaires. Our goal is to evaluate the consistency between LLMs' professed
personality inclinations and their actual "behavior", examining the extent to
which these models can emulate human-like personality patterns. Through a
comprehensive analysis of LLM outputs against established human benchmarks, we
seek to understand the cognition-action divergence in LLMs and propose
hypotheses for the observed results based on psychological theories and
metrics.
\\ ( https://arxiv.org/abs/2402.14679 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14690
Date: Thu, 22 Feb 2024 16:45:32 GMT   (3157kb,D)

Title: UFO: a Unified and Flexible Framework for Evaluating Factuality of Large
  Language Models
Authors: Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Ji-rong Wen
Categories: cs.CL
Comments: under review
\\
  Large language models (LLMs) may generate text that lacks consistency with
human knowledge, leading to factual inaccuracies or \textit{hallucination}.
Existing research for evaluating the factuality of LLMs involves extracting
fact claims using an LLM and verifying them against a predefined fact source.
However, these evaluation metrics are task-specific, and not scalable, and the
substitutability of fact sources in different tasks is under-explored. To
address these challenges, we categorize four available fact sources:
human-written evidence, reference documents, search engine results, and LLM
knowledge, along with five text generation tasks containing six representative
datasets. Then, we propose \texttt{UFO}, an LLM-based unified and flexible
evaluation framework to verify facts against plug-and-play fact sources. We
implement five evaluation scenarios based on this framework. Experimental
results show that for most QA tasks, human-written evidence and reference
documents are crucial, and they can substitute for each other in
retrieval-augmented QA tasks. In news fact generation tasks, search engine
results and LLM knowledge are essential. Our dataset and code are available at
\url{https://github.com/WaldenRUC/UFO}.
\\ ( https://arxiv.org/abs/2402.14690 ,  3157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14700
Date: Thu, 22 Feb 2024 16:56:13 GMT   (2348kb,D)

Title: Unveiling Linguistic Regions in Large Language Models
Authors: Zhihao Zhang, Jun Zhao, Qi Zhang, Tao Gui, Xuanjing Huang
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated considerable cross-lingual
alignment and generalization ability. Current research primarily focuses on
improving LLMs' cross-lingual generalization capabilities. However, there is
still a lack of research on the intrinsic mechanisms of how LLMs achieve
cross-lingual alignment. From the perspective of region partitioning, this
paper conducts several investigations on the linguistic competence of LLMs. We
discover a core region in LLMs that corresponds to linguistic competence,
accounting for approximately 1% of the total model parameters. Removing this
core region by setting parameters to zero results in a significant performance
decrease across 30 different languages. Furthermore, this core region exhibits
significant dimensional dependency, perturbations to even a single parameter on
specific dimensions leading to a loss of linguistic competence. Moreover, we
discover that distinct regions exist for different monolingual families, and
disruption to these specific regions substantially reduces the LLMs'
proficiency in those corresponding languages. Our research also indicates that
freezing the core linguistic region during further pre-training can mitigate
the issue of catastrophic forgetting (CF), a common occurrence observed during
further pre-training of LLMs. Overall, exploring the LLMs' functional regions
provides insights into the foundation of their intelligence.
\\ ( https://arxiv.org/abs/2402.14700 ,  2348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14701
Date: Thu, 22 Feb 2024 16:56:44 GMT   (8334kb,D)

Title: COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies
  with Language Modeling
Authors: Baihan Lin, Djallel Bouneffouf, Yulia Landa, Rachel Jespersen, Cheryl
  Corcoran, Guillermo Cecchi
Categories: cs.CL cs.AI cs.HC cs.LG q-bio.NC
Comments: This work extends our research series in computational psychiatry
  (e.g auto annotation in arXiv:2204.05522, topic extraction in
  arXiv:2204.10189, and diagnosis in arXiv:2210.15603) with the introduction of
  LLMs to complete the full cycle of interpreting and understanding
  psychotherapy strategies as a comprehensive analytical framework
\\
  The therapeutic working alliance is a critical factor in predicting the
success of psychotherapy treatment. Traditionally, working alliance assessment
relies on questionnaires completed by both therapists and patients. In this
paper, we present COMPASS, a novel framework to directly infer the therapeutic
working alliance from the natural language used in psychotherapy sessions. Our
approach utilizes advanced large language models to analyze transcripts of
psychotherapy sessions and compare them with distributed representations of
statements in the working alliance inventory. Analyzing a dataset of over 950
sessions covering diverse psychiatric conditions, we demonstrate the
effectiveness of our method in microscopically mapping patient-therapist
alignment trajectories and providing interpretability for clinical psychiatry
and in identifying emerging patterns related to the condition being treated. By
employing various neural topic modeling techniques in combination with
generative language prompting, we analyze the topical characteristics of
different psychiatric conditions and incorporate temporal modeling to capture
the evolution of topics at a turn-level resolution. This combined framework
enhances the understanding of therapeutic interactions, enabling timely
feedback for therapists regarding conversation quality and providing
interpretable insights to improve the effectiveness of psychotherapy.
\\ ( https://arxiv.org/abs/2402.14701 ,  8334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14702
Date: Thu, 22 Feb 2024 16:59:09 GMT   (7128kb,D)

Title: InfFeed: Influence Functions as a Feedback to Improve the Performance of
  Subjective Tasks
Authors: Somnath Banerjee, Maulindu Sarkar, Punyajoy Saha, Binny Mathew,
  Animesh Mukherjee
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 (Long Paper)
\\
  Recently, influence functions present an apparatus for achieving
explainability for deep neural models by quantifying the perturbation of
individual train instances that might impact a test prediction. Our objectives
in this paper are twofold. First we incorporate influence functions as a
feedback into the model to improve its performance. Second, in a dataset
extension exercise, using influence functions to automatically identify data
points that have been initially `silver' annotated by some existing method and
need to be cross-checked (and corrected) by annotators to improve the model
performance. To meet these objectives, in this paper, we introduce InfFeed,
which uses influence functions to compute the influential instances for a
target instance. Toward the first objective, we adjust the label of the target
instance based on its influencer(s) label. In doing this, InfFeed outperforms
the state-of-the-art baselines (including LLMs) by a maximum macro F1-score
margin of almost 4% for hate speech classification, 3.5% for stance
classification, and 3% for irony and 2% for sarcasm detection. Toward the
second objective we show that manually re-annotating only those silver
annotated data points in the extension set that have a negative influence can
immensely improve the model performance bringing it very close to the scenario
where all the data points in the extension set have gold labels. This allows
for huge reduction of the number of data points that need to be manually
annotated since out of the silver annotated extension dataset, the influence
function scheme picks up ~1/1000 points that need manual correction.
\\ ( https://arxiv.org/abs/2402.14702 ,  7128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14704
Date: Thu, 22 Feb 2024 17:04:30 GMT   (481kb,D)

Title: An LLM-Enhanced Adversarial Editing System for Lexical Simplification
Authors: Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu
Categories: cs.CL
\\
  Lexical Simplification (LS) aims to simplify text at the lexical level.
Existing methods rely heavily on annotated data, making it challenging to apply
in low-resource scenarios. In this paper, we propose a novel LS method without
parallel corpora. This method employs an Adversarial Editing System with
guidance from a confusion loss and an invariance loss to predict lexical edits
in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced
loss to enable the distillation of knowledge from Large Language Models (LLMs)
into a small-size LS system. From that, complex words within sentences are
masked and a Difficulty-aware Filling module is crafted to replace masked
positions with simpler words. At last, extensive experimental results and
analyses on three benchmark LS datasets demonstrate the effectiveness of our
proposed method.
\\ ( https://arxiv.org/abs/2402.14704 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14710
Date: Thu, 22 Feb 2024 17:11:38 GMT   (1480kb,D)

Title: IEPile: Unearthing Large-Scale Schema-Based Information Extraction
  Corpus
Authors: Honghao Gui, Hongbin Ye, Lin Yuan, Ningyu Zhang, Mengshu Sun, Lei
  Liang, Huajun Chen
Categories: cs.CL cs.AI cs.DB cs.IR cs.LG
Comments: Ongoing work; 18 pages; Github: https://github.com/zjunlp/IEPile
\\
  Large Language Models (LLMs) demonstrate remarkable potential across various
domains; however, they exhibit a significant performance gap in Information
Extraction (IE). Note that high-quality instruction data is the vital key for
enhancing the specific capabilities of LLMs, while current IE datasets tend to
be small in scale, fragmented, and lack standardized schema. To this end, we
introduce IEPile, a comprehensive bilingual (English and Chinese) IE
instruction corpus, which contains approximately 0.32B tokens. We construct
IEPile by collecting and cleaning 33 existing IE datasets, and introduce
schema-based instruction generation to unearth a large-scale corpus.
Experimental results on LLaMA and Baichuan demonstrate that using IEPile can
enhance the performance of LLMs for IE, especially the zero-shot
generalization. We open-source the resource and pre-trained models, hoping to
provide valuable support to the NLP community.
\\ ( https://arxiv.org/abs/2402.14710 ,  1480kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14714
Date: Thu, 22 Feb 2024 17:12:39 GMT   (1037kb,D)

Title: Efficient and Effective Vocabulary Expansion Towards Multilingual Large
  Language Models
Authors: Seungduk Kim, Seungtaek Choi, Myeongho Jeong
Categories: cs.CL cs.AI
\\
  This report introduces \texttt{EEVE-Korean-v1.0}, a Korean adaptation of
large language models that exhibit remarkable capabilities across English and
Korean text understanding. Building on recent highly capable but
English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts
are inefficiently processed with English-centric tokenizers, we present an
efficient and effective vocabulary expansion (EEVE) method, which encompasses
parameter freezing and subword initialization. In contrast to previous efforts
that believe new embeddings require trillions of training tokens, we show that
our method can significantly boost non-English proficiency within just 2
billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM
Leaderboard, as of January 2024, our model \texttt{EEVE-Korean-10.8B-v1.0}
ranks as the leading Korean pre-trained model in the open-source community,
according to Hugging Face's leaderboard. We open-source our models on
Huggingface to empower the open research community in various languages.
\\ ( https://arxiv.org/abs/2402.14714 ,  1037kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14743
Date: Thu, 22 Feb 2024 17:58:50 GMT   (1073kb,D)

Title: Dependency Annotation of Ottoman Turkish with Multilingual BERT
Authors: \c{S}aziye Bet\"ul \"Ozate\c{s}, Tar{\i}k Emre T{\i}ra\c{s}, Efe Eren
  Gen\c{c}, Esma Fat{\i}ma Bilgin Ta\c{s}demir
Categories: cs.CL
Comments: 9 pages, 5 figures. Accepted to LAW-XVIII
\\
  This study introduces a pretrained large language model-based annotation
methodology for the first dependency treebank in Ottoman Turkish. Our
experimental results show that, iteratively, i) pseudo-annotating data using a
multilingual BERT-based parsing model, ii) manually correcting the
pseudo-annotations, and iii) fine-tuning the parsing model with the corrected
annotations, we speed up and simplify the challenging dependency annotation
process. The resulting treebank, that will be a part of the Universal
Dependencies (UD) project, will facilitate automated analysis of Ottoman
Turkish documents, unlocking the linguistic richness embedded in this
historical heritage.
\\ ( https://arxiv.org/abs/2402.14743 ,  1073kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14746
Date: Thu, 22 Feb 2024 18:06:19 GMT   (184kb)

Title: Scaling Efficient LLMs
Authors: B.N. Kausik
Categories: cs.CL cs.LG
\\
  Trained LLMs are typically sparse in that most of the parameters are zero,
raising questions on efficiency. In response, we inquire into efficient LLMs,
i.e. those with the fewest parameters that achieve the desired accuracy on a
training corpus. Specifically, we compare theoretical and empirical estimates
for training loss at current scale to obtain upper and lower bounds on the
number of unique sequences in a natural training corpus as a function of its
size. Our result implies (1) to double the number of skills represented in a
training corpus, the corpus must scale roughly between three and five fold (2)
for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural
training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of
an LLM is smaller than the number of unique sequences in the training corpus,
scaling up can uncover emergent skills.
\\ ( https://arxiv.org/abs/2402.14746 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14762
Date: Thu, 22 Feb 2024 18:21:59 GMT   (1258kb,D)

Title: MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language
  Models in Multi-Turn Dialogues
Authors: Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou,
  Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang
Categories: cs.CL cs.AI
Comments: The first three authors contribute equally, 32 pages, repo at
  https://github.com/mtbench101/mt-bench-101
\\
  The advent of Large Language Models (LLMs) has drastically enhanced dialogue
systems. However, comprehensively evaluating the dialogue abilities of LLMs
remains a challenge. Previous benchmarks have primarily focused on single-turn
dialogues or provided coarse-grained and incomplete assessments of multi-turn
dialogues, overlooking the complexity and fine-grained nuances of real-life
dialogues. To address this issue, we introduce MT-Bench-101, specifically
designed to evaluate the fine-grained abilities of LLMs in multi-turn
dialogues. By conducting a detailed analysis of real multi-turn dialogue data,
we construct a three-tier hierarchical ability taxonomy comprising 4208 turns
across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21
popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both
ability and task perspectives and observing differing trends in LLMs
performance across dialogue turns within various tasks. Further analysis
indicates that neither utilizing common alignment techniques nor chat-specific
designs has led to obvious enhancements in the multi-turn abilities of LLMs.
Extensive case studies suggest that our designed tasks accurately assess the
corresponding multi-turn abilities.
\\ ( https://arxiv.org/abs/2402.14762 ,  1258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14776
Date: Thu, 22 Feb 2024 18:35:05 GMT   (6881kb,D)

Title: 2D Matryoshka Sentence Embeddings
Authors: Xianming Li, Zongxi Li, Jing Li, Haoran Xie, Qing Li
Categories: cs.CL cs.LG
\\
  Common approaches rely on fixed-length embedding vectors from language models
as sentence embeddings for downstream tasks such as semantic textual similarity
(STS). Such methods are limited in their flexibility due to unknown
computational constraints and budgets across various applications. Matryoshka
Representation Learning (MRL) (Kusupati et al., 2022) encodes information at
finer granularities, i.e., with lower embedding dimensions, to adaptively
accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller
embedding size, leading to speedups in downstream tasks. Despite its improved
efficiency, MRL still requires traversing all Transformer layers before
obtaining the embedding, which remains the dominant factor in time and memory
consumption. This prompts consideration of whether the fixed number of
Transformer layers affects representation quality and whether using
intermediate layers for sentence representation is feasible. In this paper, we
introduce a novel sentence embedding model called Two-dimensional Matryoshka
Sentence Embedding (2DMSE). It supports elastic settings for both embedding
sizes and Transformer layers, offering greater flexibility and efficiency than
MRL. We conduct extensive experiments on STS tasks and downstream applications.
The experimental results demonstrate the effectiveness of our proposed model in
dynamically supporting different embedding sizes and Transformer layers,
allowing it to be highly adaptable to various scenarios.
\\ ( https://arxiv.org/abs/2402.14776 ,  6881kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14778
Date: Thu, 22 Feb 2024 18:37:33 GMT   (9137kb,D)

Title: Zero-shot cross-lingual transfer in instruction tuning of large language
  model
Authors: Nadezhda Chirkova, Vassilina Nikoulina
Categories: cs.CL cs.AI
\\
  Instruction tuning (IT) is widely used to teach pretrained large language
models (LLMs) to follow arbitrary instructions, but is under-studied in
multilingual settings. In this work, we conduct a systematic study of zero-shot
cross-lingual transfer in IT, when an LLM is instruction-tuned on English-only
data and then tested on user prompts in other languages. We investigate the
influence of model configuration choices and devise a multi-facet evaluation
strategy for multilingual instruction following. We find that cross-lingual
transfer does happen successfully in IT even if all stages of model training
are English-centric, but only if multiliguality is taken into account in
hyperparameter tuning and with large enough IT data. English-trained LLMs are
capable of generating correct-language, comprehensive and helpful responses in
the other languages, but suffer from low factuality and may occasionally have
fluency errors.
\\ ( https://arxiv.org/abs/2402.14778 ,  9137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14798
Date: Thu, 22 Feb 2024 18:55:17 GMT   (592kb,D)

Title: Enhancing Systematic Decompositional Natural Language Inference Using
  Informal Logic
Authors: Nathaniel Weir, Kate Sanders, Orion Weller, Shreya Sharma, Dongwei
  Jiang, Zhengping Zhang, Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Jansen,
  Peter Clark, Benjamin Van Durme
Categories: cs.CL cs.AI
\\
  Contemporary language models enable new opportunities for structured
reasoning with text, such as the construction and evaluation of intuitive,
proof-like textual entailment trees without relying on brittle formal logic.
However, progress in this direction has been hampered by a long-standing lack
of a clear protocol for determining what valid compositional entailment is.
This absence causes noisy datasets and limited performance gains by modern
neuro-symbolic engines. To address these problems, we formulate a consistent
and theoretically grounded approach to annotating decompositional entailment
datasets, and evaluate its impact on LLM-based textual inference. We find that
our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment),
has a substantially higher internal consistency (+9%) than prior
decompositional entailment datasets, suggesting that RDTE is a significant step
forward in the long-standing problem of forming a clear protocol for discerning
entailment. We also find that training an RDTE-oriented entailment classifier
via knowledge distillation and employing it in a modern neuro-symbolic
reasoning engine significantly improves results (both accuracy and proof
quality) over other entailment classifier baselines, illustrating the practical
benefit of this advance for textual inference.
\\ ( https://arxiv.org/abs/2402.14798 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14800
Date: Thu, 22 Feb 2024 18:56:07 GMT   (262kb,D)

Title: Not All Experts are Equal: Efficient Expert Pruning and Skipping for
  Mixture-of-Experts Large Language Models
Authors: Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang,
  Junchi Yan, Hongsheng Li
Categories: cs.CL cs.AI cs.LG
Comments: Mixture-of-Experts Large Language Models
\\
  A pivotal advancement in the progress of large language models (LLMs) is the
emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,
MoE LLMs can achieve higher performance with fewer parameters, but it is still
hard to deploy them due to their immense parameter sizes. Different from
previous weight pruning methods that rely on specifically designed hardware,
this paper mainly aims to enhance the deployment efficiency of MoE LLMs by
introducing plug-and-play expert-level sparsification techniques. Specifically,
we propose, for the first time to our best knowledge, post-training approaches
for task-agnostic and task-specific expert pruning and skipping of MoE LLMs,
tailored to improve deployment efficiency while maintaining model performance
across a wide range of tasks. Extensive experiments show that our proposed
methods can simultaneously reduce model sizes and increase the inference speed,
while maintaining satisfactory performance. Data and code will be available at
https://github.com/Lucky-Lance/Expert_Sparsity.
\\ ( https://arxiv.org/abs/2402.14800 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14805
Date: Thu, 22 Feb 2024 18:57:20 GMT   (1577kb,D)

Title: Identifying Multiple Personalities in Large Language Models with
  External Evaluation
Authors: Xiaoyang Song, Yuta Adachi, Jessie Feng, Mouwei Lin, Linhao Yu, Frank
  Li, Akshat Gupta, Gopala Anumanchipalli, Simerjot Kaur
Categories: cs.CL cs.AI
\\
  As Large Language Models (LLMs) are integrated with human daily applications
rapidly, many societal and ethical concerns are raised regarding the behavior
of LLMs. One of the ways to comprehend LLMs' behavior is to analyze their
personalities. Many recent studies quantify LLMs' personalities using
self-assessment tests that are created for humans. Yet many critiques question
the applicability and reliability of these self-assessment tests when applied
to LLMs. In this paper, we investigate LLM personalities using an alternate
personality measurement method, which we refer to as the external evaluation
method, where instead of prompting LLMs with multiple-choice questions in the
Likert scale, we evaluate LLMs' personalities by analyzing their responses
toward open-ended situational questions using an external machine learning
model. We first fine-tuned a Llama2-7B model as the MBTI personality predictor
that outperforms the state-of-the-art models as the tool to analyze LLMs'
responses. Then, we prompt the LLMs with situational questions and ask them to
generate Twitter posts and comments, respectively, in order to assess their
personalities when playing two different roles. Using the external personality
evaluation method, we identify that the obtained personality types for LLMs are
significantly different when generating posts versus comments, whereas humans
show a consistent personality profile in these two different situations. This
shows that LLMs can exhibit different personalities based on different
scenarios, thus highlighting a fundamental difference between personality in
LLMs and humans. With our work, we call for a re-evaluation of personality
definition and measurement in LLMs.
\\ ( https://arxiv.org/abs/2402.14805 ,  1577kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14808
Date: Thu, 22 Feb 2024 18:58:28 GMT   (21563kb,D)

Title: RelayAttention for Efficient Large Language Model Serving with Long
  System Prompts
Authors: Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. Lau
Categories: cs.CL
\\
  Practical large language model (LLM) services may involve a long system
prompt, which specifies the instructions, examples, and knowledge documents of
the task and is reused across numerous requests. However, the long system
prompt causes throughput/latency bottlenecks as the cost of generating the next
token grows w.r.t. the sequence length. This paper aims to improve the
efficiency of LLM services that involve long system prompts. Our key
observation is that handling these system prompts requires heavily redundant
memory accesses in existing causal attention computation algorithms.
Specifically, for batched requests, the cached hidden states (i.e., key-value
pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM
multiple times, each corresponding to an individual request. To eliminate such
a redundancy, we propose RelayAttention, an attention algorithm that allows
reading these hidden states from DRAM exactly once for a batch of input tokens.
RelayAttention is a free lunch: it maintains the generation quality while
requiring no model retraining, as it is based on a mathematical reformulation
of causal attention.
\\ ( https://arxiv.org/abs/2402.14808 ,  21563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14809
Date: Thu, 22 Feb 2024 18:59:02 GMT   (1194kb,D)

Title: CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
Authors: Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu
  Yang
Categories: cs.CL cs.AI cs.LG
\\
  The ability of Large Language Models (LLMs) to critique and refine their
reasoning is crucial for their application in evaluation, feedback provision,
and self-improvement. This paper introduces CriticBench, a comprehensive
benchmark designed to assess LLMs' abilities to critique and rectify their
reasoning across a variety of tasks. CriticBench encompasses five reasoning
domains: mathematical, commonsense, symbolic, coding, and algorithmic. It
compiles 15 datasets and incorporates responses from three LLM families.
Utilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in
generation, critique, and correction reasoning, i.e., GQC reasoning. Our
findings reveal: (1) a linear relationship in GQC capabilities, with
critique-focused training markedly enhancing performance; (2) a task-dependent
variation in correction effectiveness, with logic-oriented tasks being more
amenable to correction; (3) GQC knowledge inconsistencies that decrease as
model size increases; and (4) an intriguing inter-model critiquing dynamic,
where stronger models are better at critiquing weaker ones, while weaker models
can surprisingly surpass stronger ones in their self-critique. We hope these
insights into the nuanced critique-correct reasoning of LLMs will foster
further research in LLM critique and self-improvement.
\\ ( https://arxiv.org/abs/2402.14809 ,  1194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14811
Date: Thu, 22 Feb 2024 18:59:24 GMT   (2381kb,D)

Title: Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity
  Tracking
Authors: Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, David
  Bau
Categories: cs.CL cs.LG
Comments: ICLR 2024. 26 pages, 13 figures. Code and data at
  https://finetuning.baulab.info/
\\
  Fine-tuning on generalized tasks such as instruction following, code
generation, and mathematics has been shown to enhance language models'
performance on a range of tasks. Nevertheless, explanations of how such
fine-tuning influences the internal computations in these models remain
elusive. We study how fine-tuning affects the internal mechanisms implemented
in language models. As a case study, we explore the property of entity
tracking, a crucial facet of language comprehension, where models fine-tuned on
mathematics have substantial performance gains. We identify the mechanism that
enables entity tracking and show that (i) in both the original model and its
fine-tuned versions primarily the same circuit implements entity tracking. In
fact, the entity tracking circuit of the original model on the fine-tuned
versions performs better than the full original model. (ii) The circuits of all
the models implement roughly the same functionality: Entity tracking is
performed by tracking the position of the correct entity in both the original
model and its fine-tuned versions. (iii) Performance boost in the fine-tuned
models is primarily attributed to its improved ability to handle the augmented
positional information. To uncover these findings, we employ: Patch Patching,
DCM, which automatically detects model components responsible for specific
semantics, and CMAP, a new approach for patching activations across models to
reveal improved mechanisms. Our findings suggest that fine-tuning enhances,
rather than fundamentally alters, the mechanistic operation of the model.
\\ ( https://arxiv.org/abs/2402.14811 ,  2381kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14818
Date: Thu, 22 Feb 2024 18:59:58 GMT   (25805kb,D)

Title: PALO: A Polyglot Large Multimodal Model for 5B People
Authors: Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan,
  Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan
Categories: cs.CL cs.CV
Comments: Technical Report of PALO
\\
  In pursuit of more inclusive Vision-Language Models (VLMs), this study
introduces a Large Multilingual Multimodal Model called \textsc{Palo}.
\textsc{Palo} offers visual reasoning capabilities in 10 major languages,
including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian,
Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world
population). Our approach involves a semi-automated translation approach to
adapt the multimodal instruction dataset from English to the target languages
using a fine-tuned Large Language Model, thereby ensuring high linguistic
fidelity while allowing scalability due to minimal manual effort. The
incorporation of diverse instruction sets helps us boost overall performance
across multiple languages especially those that are underrepresented like
Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three
scales (1.7B, 7B and 13B parameters) to show the generalization and scalability
where we observe substantial improvements compared to strong baselines. We also
propose the first multilingual multimodal benchmark for the forthcoming
approaches to evaluate their vision-language reasoning capabilities across
languages. Code: https://github.com/mbzuai-oryx/PALO.
\\ ( https://arxiv.org/abs/2402.14818 ,  25805kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14027
Date: Sat, 17 Feb 2024 22:26:56 GMT   (1632kb)

Title: Learning causation event conjunction sequences
Authors: Thomas E. Portegys
Categories: cs.LG
\\
  This is an examination of some methods that learn causations in event
sequences. A causation is defined as a conjunction of one or more cause events
occurring in an arbitrary order, with possible intervening non-causal events,
that lead to an effect. The methods include recurrent and non-recurrent
artificial neural networks (ANNs), as well as a histogram-based algorithm. An
attention recurrent ANN performed the best of the ANNs, while the histogram
algorithm was significantly superior to all the ANNs.
\\ ( https://arxiv.org/abs/2402.14027 ,  1632kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14029
Date: Tue, 20 Feb 2024 03:14:45 GMT   (288kb,D)

Title: Partial Search in a Frozen Network is Enough to Find a Strong Lottery
  Ticket
Authors: Hikari Otsuka, Daiki Chijiwa, \'Angel L\'opez Garc\'ia-Arias, Yasuyuki
  Okoshi, Kazushi Kawamura, Thiem Van Chu, Daichi Fujiki, Susumu Takeuchi,
  Masato Motomura
Categories: cs.LG cs.AI
Comments: 13 pages; comments are welcome
\\
  Randomly initialized dense networks contain subnetworks that achieve high
accuracy without weight learning -- strong lottery tickets (SLTs). Recently,
Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs
can also be found within a randomly pruned source network, thus reducing the
SLT search space. However, this limits the search to SLTs that are even sparser
than the source, leading to worse accuracy due to unintentionally high
sparsity. This paper proposes a method that reduces the SLT search space by an
arbitrary ratio that is independent of the desired SLT sparsity. A random
subset of the initial weights is excluded from the search space by freezing it
-- i.e., by either permanently pruning them or locking them as a fixed part of
the SLT. Indeed, the SLT existence in such a reduced search space is
theoretically guaranteed by our subset-sum approximation with randomly frozen
variables. In addition to reducing search space, the random freezing pattern
can also be exploited to reduce model size in inference. Furthermore,
experimental results show that the proposed method finds SLTs with better
accuracy and model size trade-off than the SLTs obtained from dense or randomly
pruned source networks. In particular, the SLT found in a frozen graph neural
network achieves higher accuracy than its weight trained counterpart while
reducing model size by $40.3\times$.
\\ ( https://arxiv.org/abs/2402.14029 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14033
Date: Wed, 21 Feb 2024 03:04:34 GMT   (6774kb,D)

Title: VN Network: Embedding Newly Emerging Entities with Virtual Neighbors
Authors: Yongquan He and Zihan Wang and Peng Zhang and Zhaopeng Tu and Zhaochun
  Ren
Categories: cs.LG cs.AI
Comments: 10 pages, 5 figures
ACM-class: I.2.4; I.2.6
Journal-ref: CIKM (2020) 505-514
DOI: 10.1145/3340531.3411865
\\
  Embedding entities and relations into continuous vector spaces has attracted
a surge of interest in recent years. Most embedding methods assume that all
test entities are available during training, which makes it time-consuming to
retrain embeddings for newly emerging entities. To address this issue, recent
works apply the graph neural network on the existing neighbors of the unseen
entities. In this paper, we propose a novel framework, namely Virtual Neighbor
(VN) network, to address three key challenges. Firstly, to reduce the neighbor
sparsity problem, we introduce the concept of the virtual neighbors inferred by
rules. And we assign soft labels to these neighbors by solving a
rule-constrained problem, rather than simply regarding them as unquestionably
true. Secondly, many existing methods only use one-hop or two-hop neighbors for
aggregation and ignore the distant information that may be helpful. Instead, we
identify both logic and symmetric path rules to capture complex patterns.
Finally, instead of one-time injection of rules, we employ an iterative
learning scheme between the embedding method and virtual neighbor prediction to
capture the interactions within. Experimental results on two knowledge graph
completion tasks demonstrate that our VN network significantly outperforms
state-of-the-art baselines. Furthermore, results on Subject/Object-R show that
our proposed VN network is highly robust to the neighbor sparsity problem.
\\ ( https://arxiv.org/abs/2402.14033 ,  6774kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14035
Date: Wed, 21 Feb 2024 04:33:26 GMT   (258kb,D)

Title: Wisdom of Committee: Distilling from Foundation Model to
  SpecializedApplication Model
Authors: Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali
  Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao
Categories: cs.LG cs.AI
\\
  Recent advancements in foundation models have yielded impressive performance
across a wide range of tasks. Meanwhile, for specific applications,
practitioners have been developing specialized application models. To enjoy the
benefits of both kinds of models, one natural path is to transfer the knowledge
in foundation models into specialized application models, which are generally
more efficient for serving. Techniques from knowledge distillation may be
applied here, where the application model learns to mimic the foundation model.
However, specialized application models and foundation models have substantial
gaps in capacity, employing distinct architectures, using different input
features from different modalities, and being optimized on different
distributions. These differences in model characteristics lead to significant
challenges for distillation methods. In this work, we propose creating a
teaching committee comprising both foundation model teachers and complementary
teachers. Complementary teachers possess model characteristics akin to the
student's, aiming to bridge the gap between the foundation model and
specialized application models for a smoother knowledge transfer. Further, to
accommodate the dissimilarity among the teachers in the committee, we introduce
DiverseDistill, which allows the student to understand the expertise of each
teacher and extract task knowledge. Our evaluations demonstrate that adding
complementary teachers enhances student performance. Finally, DiverseDistill
consistently outperforms baseline distillation methods, regardless of the
teacher choices, resulting in significantly improved student performance.
\\ ( https://arxiv.org/abs/2402.14035 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14039
Date: Wed, 21 Feb 2024 06:39:04 GMT   (1154kb)

Title: Specialty detection in the context of telemedicine in a highly
  imbalanced multi-class distribution
Authors: Alaa Alomari, Hossam Faris, Pedro A. Castillo
Categories: cs.LG cs.AI
\\
  The Covid-19 pandemic has led to an increase in the awareness of and demand
for telemedicine services, resulting in a need for automating the process and
relying on machine learning (ML) to reduce the operational load. This research
proposes a specialty detection classifier based on a machine learning model to
automate the process of detecting the correct specialty for each question and
routing it to the correct doctor. The study focuses on handling multiclass and
highly imbalanced datasets for Arabic medical questions, comparing some
oversampling techniques, developing a Deep Neural Network (DNN) model for
specialty detection, and exploring the hidden business areas that rely on
specialty detection such as customizing and personalizing the consultation flow
for different specialties. The proposed module is deployed in both synchronous
and asynchronous medical consultations to provide more real-time
classification, minimize the doctor effort in addressing the correct specialty,
and give the system more flexibility in customizing the medical consultation
flow. The evaluation and assessment are based on accuracy, precision, recall,
and F1-score. The experimental results suggest that combining multiple
techniques, such as SMOTE and reweighing with keyword identification, is
necessary to achieve improved performance in detecting rare classes in
imbalanced multiclass datasets. By using these techniques, specialty detection
models can more accurately detect rare classes in real-world scenarios where
imbalanced data is common.
\\ ( https://arxiv.org/abs/2402.14039 ,  1154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14041
Date: Wed, 21 Feb 2024 10:16:57 GMT   (4154kb)

Title: E2USD: Efficient-yet-effective Unsupervised State Detection for
  Multivariate Time Series
Authors: Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S.
  Jensen
Categories: cs.LG cs.AI cs.DB
Comments: accepted by The Web Conference 2024 (WWW 2024)
\\
  We propose E2USD that enables efficient-yet-accurate unsupervised MTS state
detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor
(FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together
encode input MTSs at low computational overhead. Additionally, we propose a
False Negative Cancellation Contrastive Learning method (FNCCLearning) to
counteract the effects of false negatives and to achieve more cluster-friendly
embedding spaces. To reduce computational overhead further in streaming
settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive
experiments with six baselines and six datasets offer evidence that E2USD is
capable of SOTA accuracy at significantly reduced computational overhead. Our
code is available at https://github.com/AI4CTS/E2Usd.
\\ ( https://arxiv.org/abs/2402.14041 ,  4154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14042
Date: Wed, 21 Feb 2024 10:24:34 GMT   (3565kb,D)

Title: Protect and Extend -- Using GANs for Synthetic Data Generation of
  Time-Series Medical Records
Authors: Navid Ashrafi, Vera Schmitt, Robert P. Spang, Sebastian M\"oller,
  Jan-Niklas Voigt-Antons
Categories: cs.LG cs.AI cs.CR
\\
  Preservation of private user data is of paramount importance for high Quality
of Experience (QoE) and acceptability, particularly with services treating
sensitive data, such as IT-based health services. Whereas anonymization
techniques were shown to be prone to data re-identification, synthetic data
generation has gradually replaced anonymization since it is relatively less
time and resource-consuming and more robust to data leakage. Generative
Adversarial Networks (GANs) have been used for generating synthetic datasets,
especially GAN frameworks adhering to the differential privacy phenomena. This
research compares state-of-the-art GAN-based models for synthetic data
generation to generate time-series synthetic medical records of dementia
patients which can be distributed without privacy concerns. Predictive
modeling, autocorrelation, and distribution analysis are used to assess the
Quality of Generating (QoG) of the generated data. The privacy preservation of
the respective models is assessed by applying membership inference attacks to
determine potential data leakage risks. Our experiments indicate the
superiority of the privacy-preserving GAN (PPGAN) model over other models
regarding privacy preservation while maintaining an acceptable level of QoG.
The presented results can support better data protection for medical use cases
in the future.
\\ ( https://arxiv.org/abs/2402.14042 ,  3565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14047
Date: Wed, 21 Feb 2024 15:51:01 GMT   (1216kb,D)

Title: Simple and Effective Transfer Learning for Neuro-Symbolic Integration
Authors: Alessandro Daniele, Tommaso Campari, Sagar Malhotra and Luciano
  Serafini
Categories: cs.LG cs.AI
Comments: Under Review
\\
  Deep Learning (DL) techniques have achieved remarkable successes in recent
years. However, their ability to generalize and execute reasoning tasks remains
a challenge. A potential solution to this issue is Neuro-Symbolic Integration
(NeSy), where neural approaches are combined with symbolic reasoning. Most of
these methods exploit a neural network to map perceptions to symbols and a
logical reasoner to predict the output of the downstream task. These methods
exhibit superior generalization capacity compared to fully neural
architectures. However, they suffer from several issues, including slow
convergence, learning difficulties with complex perception tasks, and
convergence to local minima. This paper proposes a simple yet effective method
to ameliorate these problems. The key idea involves pretraining a neural model
on the downstream task. Then, a NeSy model is trained on the same task via
transfer learning, where the weights of the perceptual part are injected from
the pretrained network. The key observation of our work is that the neural
network fails to generalize only at the level of the symbolic part while being
perfectly capable of learning the mapping from perceptions to symbols. We have
tested our training strategy on various SOTA NeSy methods and datasets,
demonstrating consistent improvements in the aforementioned problems.
\\ ( https://arxiv.org/abs/2402.14047 ,  1216kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14048
Date: Wed, 21 Feb 2024 16:38:14 GMT   (2410kb,D)

Title: PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial
  Optimization
Authors: Andr\'e Hottung, Mridul Mahajan, Kevin Tierney
Categories: cs.LG cs.AI
\\
  Reinforcement learning-based methods for constructing solutions to
combinatorial optimization problems are rapidly approaching the performance of
human-designed algorithms. To further narrow the gap, learning-based approaches
must efficiently explore the solution space during the search process. Recent
approaches artificially increase exploration by enforcing diverse solution
generation through handcrafted rules, however, these rules can impair solution
quality and are difficult to design for more complex problems. In this paper,
we introduce PolyNet, an approach for improving exploration of the solution
space by learning complementary solution strategies. In contrast to other
works, PolyNet uses only a single-decoder and a training schema that does not
enforce diverse solution generation through handcrafted rules. We evaluate
PolyNet on four combinatorial optimization problems and observe that the
implicit diversity mechanism allows PolyNet to find better solutions than
approaches the explicitly enforce diverse solution generation.
\\ ( https://arxiv.org/abs/2402.14048 ,  2410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14049
Date: Wed, 21 Feb 2024 18:25:04 GMT   (39951kb,D)

Title: Generative Adversarial Models for Extreme Downscaling of Climate
  Datasets
Authors: Guiye Li and Guofeng Cao
Categories: cs.LG cs.AI physics.ao-ph
\\
  Addressing the challenges of climate change requires accurate and
high-resolution mapping of climate and weather variables. However, many
existing climate datasets, such as the gridded outputs of the state-of-the-art
numerical climate models (e.g., general circulation models), are only available
at very coarse spatial resolutions due to the model complexity and extremely
high computational demand. Deep-learning-based methods, particularly generative
adversarial networks (GANs) and their variants, have proved effective for
refining natural images, and have shown great promise in improving scientific
datasets. In this paper, we describe a conditional GAN-based geospatial
downscaling method for extreme downscaling of gridded climate datasets.
Compared to most existing methods, the method can generate high-resolution
accurate climate datasets from very low-resolution inputs. More importantly,
the method explicitly considers the uncertainty inherent to the downscaling
process that tends to be ignored in existing methods. Given an input, the
method can produce a multitude of plausible high-resolution samples instead of
one single deterministic result. These samples allow for an empirical
exploration and inferences of model uncertainty and robustness. With a case
study of gridded climate datasets (wind velocity and solar irradiance), we
demonstrate the performances of the framework in downscaling tasks with very
high scaling factors (up to $64\times$) and highlight the advantages of the
framework with a comprehensive comparison with commonly used downscaling
methods, including area-to-point (ATP) kriging, deep image prior (DIP),
enhanced deep super-resolution network (EDSR), enhanced super-resolution
generative adversarial networks (ESRGAN), and physics-informed
resolution-enhancing GAN (PhIRE GAN).
\\ ( https://arxiv.org/abs/2402.14049 ,  39951kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14080
Date: Wed, 21 Feb 2024 19:09:53 GMT   (292kb,D)

Title: Efficient Normalized Conformal Prediction and Uncertainty Quantification
  for Anti-Cancer Drug Sensitivity Prediction with Deep Regression Forests
Authors: Daniel Nolte, Souparno Ghosh, Ranadip Pal
Categories: cs.LG cs.AI stat.ML
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Deep learning models are being adopted and applied on various critical
decision-making tasks, yet they are trained to provide point predictions
without providing degrees of confidence. The trustworthiness of deep learning
models can be increased if paired with uncertainty estimations. Conformal
Prediction has emerged as a promising method to pair machine learning models
with prediction intervals, allowing for a view of the model's uncertainty.
However, popular uncertainty estimation methods for conformal prediction fail
to provide heteroskedastic intervals that are equally accurate for all samples.
In this paper, we propose a method to estimate the uncertainty of each sample
by calculating the variance obtained from a Deep Regression Forest. We show
that the deep regression forest variance improves the efficiency and coverage
of normalized inductive conformal prediction on a drug response prediction
task.
\\ ( https://arxiv.org/abs/2402.14080 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14081
Date: Wed, 21 Feb 2024 19:10:08 GMT   (635kb,D)

Title: Robust Learning of Noisy Time Series Collections Using Stochastic
  Process Models with Motion Codes
Authors: Chandrajit Bajaj, Minh Nguyen
Categories: cs.LG cs.AI stat.ML
Comments: 17 pages, 2 figures, 3 tables
\\
  While time series classification and forecasting problems have been
extensively studied, the cases of noisy time series data with arbitrary time
sequence lengths have remained challenging. Each time series instance can be
thought of as a sample realization of a noisy dynamical model, which is
characterized by a continuous stochastic process. For many applications, the
data are mixed and consist of several types of noisy time series sequences
modeled by multiple stochastic processes, making the forecasting and
classification tasks even more challenging. Instead of regressing data naively
and individually to each time series type, we take a latent variable model
approach using a mixtured Gaussian processes with learned spectral kernels.
More specifically, we auto-assign each type of noisy time series data a
signature vector called its motion code. Then, conditioned on each assigned
motion code, we infer a sparse approximation of the corresponding time series
using the concept of the most informative timestamps. Our unmixing
classification approach involves maximizing the likelihood across all the mixed
noisy time series sequences of varying lengths. This stochastic approach allows
us to learn not only within a single type of noisy time series data but also
across many underlying stochastic processes, giving us a way to learn multiple
dynamical models in an integrated and robust manner. The different learned
latent stochastic models allow us to generate specific sub-type forecasting. We
provide several quantitative comparisons demonstrating the performance of our
approach.
\\ ( https://arxiv.org/abs/2402.14081 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14098
Date: Wed, 21 Feb 2024 19:48:11 GMT   (8775kb,D)

Title: Intriguing Properties of Modern GANs
Authors: Roy Friedman and Yair Weiss
Categories: cs.LG cs.CV
\\
  Modern GANs achieve remarkable performance in terms of generating realistic
and diverse samples. This has led many to believe that ``GANs capture the
training data manifold''. In this work we show that this interpretation is
wrong. We empirically show that the manifold learned by modern GANs does not
fit the training distribution: specifically the manifold does not pass through
the training examples and passes closer to out-of-distribution images than to
in-distribution images. We also investigate the distribution over images
implied by the prior over the latent codes and study whether modern GANs learn
a density that approximates the training distribution. Surprisingly, we find
that the learned density is very far from the data distribution and that GANs
tend to assign higher density to out-of-distribution images. Finally, we
demonstrate that the set of images used to train modern GANs are often not part
of the typical set described by the GANs' distribution.
\\ ( https://arxiv.org/abs/2402.14098 ,  8775kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14103
Date: Wed, 21 Feb 2024 19:55:01 GMT   (45kb)

Title: Computational-Statistical Gaps for Improper Learning in Sparse Linear
  Regression
Authors: Rares-Darius Buhai, Jingqiu Ding, Stefan Tiegel
Categories: cs.LG cs.CC math.ST stat.ML stat.TH
Comments: 23 pages
\\
  We study computational-statistical gaps for improper learning in sparse
linear regression. More specifically, given $n$ samples from a $k$-sparse
linear model in dimension $d$, we ask what is the minimum sample complexity to
efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense
estimate for the regression vector that achieves non-trivial prediction error
on the $n$ samples. Information-theoretically this can be achieved using
$\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature,
there is no polynomial-time algorithm known to achieve the same guarantees
using less than $\Theta(d)$ samples without additional restrictions on the
model. Similarly, existing hardness results are either restricted to the proper
setting, in which the estimate must be sparse as well, or only apply to
specific algorithms.
  We give evidence that efficient algorithms for this task require at least
(roughly) $\Omega(k^2)$ samples. In particular, we show that an improper
learning algorithm for sparse linear regression can be used to solve sparse PCA
problems (with a negative spike) in their Wishart form, in regimes in which
efficient algorithms are widely believed to require at least $\Omega(k^2)$
samples. We complement our reduction with low-degree and statistical query
lower bounds for the sparse PCA problems from which we reduce.
  Our hardness results apply to the (correlated) random design setting in which
the covariates are drawn i.i.d. from a mean-zero Gaussian distribution with
unknown covariance.
\\ ( https://arxiv.org/abs/2402.14103 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14123
Date: Wed, 21 Feb 2024 20:43:49 GMT   (17700kb,D)

Title: DeiSAM: Segment Anything with Deictic Prompting
Authors: Hikaru Shindo, Manuel Brack, Gopika Sudhakaran, Devendra Singh Dhami,
  Patrick Schramowski, Kristian Kersting
Categories: cs.LG cs.AI cs.CV
Comments: Preprint
\\
  Large-scale, pre-trained neural networks have demonstrated strong
capabilities in various tasks, including zero-shot image segmentation. To
identify concrete objects in complex scenes, humans instinctively rely on
deictic descriptions in natural language, i.e., referring to something
depending on the context such as "The object that is on the desk and behind the
cup.". However, deep learning approaches cannot reliably interpret such deictic
representations due to their lack of reasoning capabilities in complex
scenarios. To remedy this issue, we propose DeiSAM -- a combination of large
pre-trained neural networks with differentiable logic reasoners -- for deictic
promptable segmentation. Given a complex, textual segmentation description,
DeiSAM leverages Large Language Models (LLMs) to generate first-order logic
rules and performs differentiable forward reasoning on generated scene graphs.
Subsequently, DeiSAM segments objects by matching them to the logically
inferred image regions. As part of our evaluation, we propose the Deictic
Visual Genome (DeiVG) dataset, containing paired visual input and complex,
deictic textual prompts. Our empirical results demonstrate that DeiSAM is a
substantial improvement over purely data-driven baselines for deictic
promptable segmentation.
\\ ( https://arxiv.org/abs/2402.14123 ,  17700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14139
Date: Wed, 21 Feb 2024 21:33:07 GMT   (1912kb,D)

Title: NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning
Authors: Dhananjay Saikumar and Blesson Varghese
Categories: cs.LG
Comments: Accepted to EuroSys 2024
\\
  Efficient on-device convolutional neural network (CNN) training in
resource-constrained mobile and edge environments is an open challenge.
Backpropagation is the standard approach adopted, but it is GPU memory
intensive due to its strong inter-layer dependencies that demand intermediate
activations across the entire CNN model to be retained in GPU memory. This
necessitates smaller batch sizes to make training possible within the available
GPU memory budget, but in turn, results in a substantially high and impractical
training time. We introduce NeuroFlux, a novel CNN training system tailored for
memory-constrained scenarios. We develop two novel opportunities: firstly,
adaptive auxiliary networks that employ a variable number of filters to reduce
GPU memory usage, and secondly, block-specific adaptive batch sizes, which not
only cater to the GPU memory constraints but also accelerate the training
process. NeuroFlux segments the CNNs into blocks based on GPU memory usage and
further attaches an auxiliary network to each layer in these blocks. This
disrupts the typical layer dependencies under a new training paradigm -
'adaptive local learning'. Moreover, NeuroFlux adeptly caches intermediate
activations, eliminating redundant forward passes over previously trained
blocks, further accelerating the training process. The results are twofold when
compared to Backpropagation: on various hardware platforms, NeuroFlux
demonstrates training speed-ups of 2.3$\times$ to 6.1$\times$ under stringent
GPU memory budgets, and NeuroFlux generates streamlined models that have
10.9$\times$ to 29.4$\times$ fewer parameters without sacrificing accuracy.
\\ ( https://arxiv.org/abs/2402.14139 ,  1912kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14160
Date: Wed, 21 Feb 2024 22:57:49 GMT   (5160kb,D)

Title: Recursive Speculative Decoding: Accelerating LLM Inference via Sampling
  Without Replacement
Authors: Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee,
  Christopher Lott
Categories: cs.LG cs.AI
Comments: 82 pages, 9 figures, 54 tables
\\
  Speculative decoding is an inference-acceleration method for large language
models (LLMs) where a small language model generates a draft-token sequence
which is further verified by the target LLM in parallel. Recent works have
advanced this method by establishing a draft-token tree, achieving superior
performance over a single-sequence speculative decoding. However, those works
independently generate tokens at each level of the tree, not leveraging the
tree's entire diversifiability. Besides, their empirical superiority has been
shown for fixed length of sequences, implicitly granting more computational
resource to LLM for the tree-based methods. None of the existing works has
conducted empirical studies with fixed target computational budgets despite its
importance to resource-bounded devices. We present Recursive Speculative
Decoding (RSD), a novel tree-based method that samples draft tokens without
replacement and maximizes the diversity of the tree. During RSD's drafting, the
tree is built by either Gumbel-Top-$k$ trick that draws tokens without
replacement in parallel or Stochastic Beam Search that samples sequences
without replacement while early-truncating unlikely draft sequences and
reducing the computational cost of LLM. We empirically evaluate RSD with Llama
2 and OPT models, showing that RSD outperforms the baseline methods,
consistently for fixed draft sequence length and in most cases for fixed
computational budgets at LLM.
\\ ( https://arxiv.org/abs/2402.14160 ,  5160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14169
Date: Wed, 21 Feb 2024 23:18:03 GMT   (4187kb,D)

Title: A Temporal Bias Correction using a Machine Learning Attention model
Authors: Omer Nivron, Damon J. Wischik
Categories: cs.LG physics.ao-ph
Comments: 19 pages, 15 figures
\\
  Climate models are biased with respect to real world observations and usually
need to be calibrated prior to impact studies. The suite of statistical methods
that enable such calibrations is called bias correction (BC). However, current
BC methods struggle to adjust for temporal biases, because they disregard the
dependence between consecutive time-points. As a result, climate statistics
with long-range temporal properties, such as heatwave duration and frequency,
cannot be corrected accurately, making it more difficult to produce reliable
impact studies on such climate statistics. In this paper, we offer a novel BC
methodology to correct for temporal biases. This is made possible by i)
re-thinking BC as a probability model rather than an algorithmic procedure, and
ii) adapting state-of-the-art machine-learning (ML) probabilistic attention
models to fit the BC task. With a case study of heatwave duration statistics in
Abuja, Nigeria, and Tokyo, Japan, we show striking results compared to current
climate model outputs and alternative BC methods.
\\ ( https://arxiv.org/abs/2402.14169 ,  4187kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14180
Date: Wed, 21 Feb 2024 23:45:57 GMT   (534kb,D)

Title: Linear Transformers are Versatile In-Context Learners
Authors: Max Vladymyrov, Johannes von Oswald, Mark Sandler, Rong Ge
Categories: cs.LG
\\
  Recent research has demonstrated that transformers, particularly linear
attention models, implicitly execute gradient-descent-like algorithms on data
provided in-context during their forward inference step. However, their
capability in handling more complex problems remains unexplored. In this paper,
we prove that any linear transformer maintains an implicit linear model and can
be interpreted as performing a variant of preconditioned gradient descent. We
also investigate the use of linear transformers in a challenging scenario where
the training data is corrupted with different levels of noise. Remarkably, we
demonstrate that for this problem linear transformers discover an intricate and
highly effective optimization algorithm, surpassing or matching in performance
many reasonable baselines. We reverse-engineer this algorithm and show that it
is a novel approach incorporating momentum and adaptive rescaling based on
noise levels. Our findings show that even linear transformers possess the
surprising ability to discover sophisticated optimization strategies.
\\ ( https://arxiv.org/abs/2402.14180 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14184
Date: Thu, 22 Feb 2024 00:04:21 GMT   (1367kb,D)

Title: Diversity-Aware Ensembling of Language Models Based on Topological Data
  Analysis
Authors: Polina Proskura, Alexey Zaytsev
Categories: cs.LG cs.CL
\\
  Ensembles are important tools for improving the performance of machine
learning models. In cases related to natural language processing, ensembles
boost the performance of a method due to multiple large models available in
open source. However, existing approaches mostly rely on simple averaging of
predictions by ensembles with equal weights for each model, ignoring
differences in the quality and conformity of models. We propose to estimate
weights for ensembles of NLP models using not only knowledge of their
individual performance but also their similarity to each other. By adopting
distance measures based on Topological Data Analysis (TDA), we improve our
ensemble. The quality improves for both text classification accuracy and
relevant uncertainty estimation.
\\ ( https://arxiv.org/abs/2402.14184 ,  1367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14194
Date: Thu, 22 Feb 2024 00:38:43 GMT   (5541kb,D)

Title: BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human
  Racing Gameplay
Authors: Catherine Weaver, Chen Tang, Ce Hao, Kenta Kawamoto, Masayoshi
  Tomizuka, Wei Zhan
Categories: cs.LG cs.RO
Comments: Preprint
\\
  Imitation learning learns a policy from demonstrations without requiring
hand-designed reward functions. In many robotic tasks, such as autonomous
racing, imitated policies must model complex environment dynamics and human
decision-making. Sequence modeling is highly effective in capturing intricate
patterns of motion sequences but struggles to adapt to new environments or
distribution shifts that are common in real-world robotics tasks. In contrast,
Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles
with sample inefficiency and handling complex motion patterns. Thus, we propose
BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a
Behavior Transformer (BeT) policy from human demonstrations with online AIL.
BeTAIL adds an AIL residual policy to the BeT policy to model the sequential
decision-making process of human experts and correct for out-of-distribution
states or shifts in environment dynamics. We test BeTAIL on three challenges
with expert-level demonstrations of real human gameplay in Gran Turismo Sport.
Our proposed residual BeTAIL reduces environment interactions and improves
racing performance and stability, even when the BeT is pretrained on different
tracks than downstream learning. Videos and code available at:
https://sites.google.com/berkeley.edu/BeTAIL/home.
\\ ( https://arxiv.org/abs/2402.14194 ,  5541kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14202
Date: Thu, 22 Feb 2024 01:07:48 GMT   (123kb,D)

Title: Comparing Graph Transformers via Positional Encodings
Authors: Mitchell Black, Zhengchao Wan, Gal Mishne, Amir Nayyeri, and Yusu Wang
Categories: cs.LG
\\
  The distinguishing power of graph transformers is closely tied to the choice
of positional encoding: features used to augment the base transformer with
information about the graph. There are two primary types of positional
encoding: absolute positional encodings (APEs) and relative positional
encodings (RPEs). APEs assign features to each node and are given as input to
the transformer. RPEs instead assign a feature to each pair of nodes, e.g.,
graph distance, and are used to augment the attention block. A priori, it is
unclear which method is better for maximizing the power of the resulting graph
transformer. In this paper, we aim to understand the relationship between these
different types of positional encodings. Interestingly, we show that graph
transformers using APEs and RPEs are equivalent in terms of distinguishing
power. In particular, we demonstrate how to interchange APEs and RPEs while
maintaining their distinguishing power in terms of graph transformers. Based on
our theoretical results, we provide a study on several APEs and RPEs (including
the resistance distance and the recently introduced stable and expressive
positional encoding (SPE)) and compare their distinguishing power in terms of
transformers. We believe our work will help navigate the huge number of choices
of positional encoding and will provide guidance on the future design of
positional encodings for graph transformers.
\\ ( https://arxiv.org/abs/2402.14202 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14212
Date: Thu, 22 Feb 2024 01:33:31 GMT   (442kb,D)

Title: Moonwalk: Inverse-Forward Differentiation
Authors: Dmitrii Krylov, Armin Karamzade, Roy Fox
Categories: cs.LG cs.AI
\\
  Backpropagation, while effective for gradient computation, falls short in
addressing memory consumption, limiting scalability. This work explores
forward-mode gradient computation as an alternative in invertible networks,
showing its potential to reduce the memory footprint without substantial
drawbacks. We introduce a novel technique based on a vector-inverse-Jacobian
product that accelerates the computation of forward gradients while retaining
the advantages of memory reduction and preserving the fidelity of true
gradients. Our method, Moonwalk, has a time complexity linear in the depth of
the network, unlike the quadratic time complexity of na\"ive forward, and
empirically reduces computation time by several orders of magnitude without
allocating more memory. We further accelerate Moonwalk by combining it with
reverse-mode differentiation to achieve time complexity comparable with
backpropagation while maintaining a much smaller memory footprint. Finally, we
showcase the robustness of our method across several architecture choices.
Moonwalk is the first forward-based method to compute true gradients in
invertible networks in computation time comparable to backpropagation and using
significantly less memory.
\\ ( https://arxiv.org/abs/2402.14212 ,  442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14220
Date: Thu, 22 Feb 2024 01:53:56 GMT   (7532kb,D)

Title: Estimating Unknown Population Sizes Using the Hypergeometric
  Distribution
Authors: Liam Hodgson and Danilo Bzdok
Categories: cs.LG stat.ME stat.ML
\\
  The multivariate hypergeometric distribution describes sampling without
replacement from a discrete population of elements divided into multiple
categories. Addressing a gap in the literature, we tackle the challenge of
estimating discrete distributions when both the total population size and the
sizes of its constituent categories are unknown. Here, we propose a novel
solution using the hypergeometric likelihood to solve this estimation
challenge, even in the presence of severe under-sampling. We develop our
approach to account for a data generating process where the ground-truth is a
mixture of distributions conditional on a continuous latent variable, such as
with collaborative filtering, using the variational autoencoder framework.
Empirical data simulation demonstrates that our method outperforms other
likelihood functions used to model count data, both in terms of accuracy of
population size estimate and in its ability to learn an informative latent
space. We demonstrate our method's versatility through applications in NLP, by
inferring and estimating the complexity of latent vocabularies in text
excerpts, and in biology, by accurately recovering the true number of gene
transcripts from sparse single-cell genomics data.
\\ ( https://arxiv.org/abs/2402.14220 ,  7532kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14227
Date: Thu, 22 Feb 2024 02:17:50 GMT   (87kb)

Title: Quaternion recurrent neural network with real-time recurrent learning
  and maximum correntropy criterion
Authors: Pauline Bourigault, Dongpo Xu, Danilo P. Mandic
Categories: cs.LG
\\
  We develop a robust quaternion recurrent neural network (QRNN) for real-time
processing of 3D and 4D data with outliers. This is achieved by combining the
real-time recurrent learning (RTRL) algorithm and the maximum correntropy
criterion (MCC) as a loss function. While both the mean square error and
maximum correntropy criterion are viable cost functions, it is shown that the
non-quadratic maximum correntropy loss function is less sensitive to outliers,
making it suitable for applications with multidimensional noisy or uncertain
data. Both algorithms are derived based on the novel generalised HR (GHR)
calculus, which allows for the differentiation of real functions of quaternion
variables and offers the product and chain rules, thus enabling elegant and
compact derivations. Simulation results in the context of motion prediction of
chest internal markers for lung cancer radiotherapy, which includes regular and
irregular breathing sequences, support the analysis.
\\ ( https://arxiv.org/abs/2402.14227 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14228
Date: Thu, 22 Feb 2024 02:20:08 GMT   (250kb,D)

Title: COPR: Continual Human Preference Learning via Optimal Policy
  Regularization
Authors: Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui
  Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu
Categories: cs.LG cs.AI
\\
  Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to
improve the alignment of Large Language Models (LLMs) with human preferences.
Given the evolving nature of human preferences, continual alignment becomes
more crucial and practical in comparison to traditional static alignment.
Nevertheless, making RLHF compatible with Continual Learning (CL) is
challenging due to its complex process. Meanwhile, directly learning new human
preferences may lead to Catastrophic Forgetting (CF) of historical preferences,
resulting in helpless or harmful outputs. To overcome these challenges, we
propose the Continual Optimal Policy Regularization (COPR) method, which draws
inspiration from the optimal policy theory. COPR utilizes a sampling
distribution as a demonstration and regularization constraints for CL. It
adopts the Lagrangian Duality (LD) method to dynamically regularize the current
policy based on the historically optimal policy, which prevents CF and avoids
over-emphasizing unbalanced objectives. We also provide formal proof for the
learnability of COPR. The experimental results show that COPR outperforms
strong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4
evaluations and human assessment. Furthermore, we validate the robustness of
COPR under various CL settings, including different backbones, replay memory
sizes, and learning orders.
\\ ( https://arxiv.org/abs/2402.14228 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14236
Date: Thu, 22 Feb 2024 02:36:14 GMT   (893kb,D)

Title: Automated Design and Optimization of Distributed Filtering Circuits via
  Reinforcement Learning
Authors: Peng Gao, Tao Yu, Fei Wang, Ru-Yue Yuan
Categories: cs.LG cs.AI cs.AR
Comments: 13 pages, 7 figures, 4 tables
\\
  Designing distributed filtering circuits (DFCs) is complex and
time-consuming, with the circuit performance relying heavily on the expertise
and experience of electronics engineers. However, manual design methods tend to
have exceedingly low-efficiency. This study proposes a novel end-to-end
automated method for fabricating circuits to improve the design of DFCs. The
proposed method harnesses reinforcement learning (RL) algorithms, eliminating
the dependence on the design experience of engineers. Thus, it significantly
reduces the subjectivity and constraints associated with circuit design. The
experimental findings demonstrate clear improvements in both design efficiency
and quality when comparing the proposed method with traditional engineer-driven
methods. In particular, the proposed method achieves superior performance when
designing complex or rapidly evolving DFCs. Furthermore, compared to existing
circuit automation design techniques, the proposed method demonstrates superior
design efficiency, highlighting the substantial potential of RL in circuit
design automation.
\\ ( https://arxiv.org/abs/2402.14236 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14246
Date: Thu, 22 Feb 2024 03:15:13 GMT   (2847kb,D)

Title: Reconstruction-Based Anomaly Localization via Knowledge-Informed
  Self-Training
Authors: Cheng Qian, Xiaoxian Lao, Chunguang Li
Categories: cs.LG cs.CV
\\
  Anomaly localization, which involves localizing anomalous regions within
images, is a significant industrial task. Reconstruction-based methods are
widely adopted for anomaly localization because of their low complexity and
high interpretability. Most existing reconstruction-based methods only use
normal samples to construct model. If anomalous samples are appropriately
utilized in the process of anomaly localization, the localization performance
can be improved. However, usually only weakly labeled anomalous samples are
available, which limits the improvement. In many cases, we can obtain some
knowledge of anomalies summarized by domain experts. Taking advantage of such
knowledge can help us better utilize the anomalous samples and thus further
improve the localization performance. In this paper, we propose a novel
reconstruction-based method named knowledge-informed self-training (KIST) which
integrates knowledge into reconstruction model through self-training.
Specifically, KIST utilizes weakly labeled anomalous samples in addition to the
normal ones and exploits knowledge to yield pixel-level pseudo-labels of the
anomalous samples. Based on the pseudo labels, a novel loss which promotes the
reconstruction of normal pixels while suppressing the reconstruction of
anomalous pixels is used. We conduct experiments on different datasets and
demonstrate the advantages of KIST over the existing reconstruction-based
methods.
\\ ( https://arxiv.org/abs/2402.14246 ,  2847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14254
Date: Thu, 22 Feb 2024 03:41:05 GMT   (3210kb,D)

Title: A hierarchical decomposition for explaining ML performance discrepancies
Authors: Jean Feng, Harvineet Singh, Fan Xia, Adarsh Subbaswamy, Alexej
  Gossmann
Categories: cs.LG stat.ML
Comments: 11 pages, 5 figures in main body; 14 pages and 2 figures in
  appendices
\\
  Machine learning (ML) algorithms can often differ in performance across
domains. Understanding $\textit{why}$ their performance differs is crucial for
determining what types of interventions (e.g., algorithmic or operational) are
most effective at closing the performance gaps. Existing methods focus on
$\textit{aggregate decompositions}$ of the total performance gap into the
impact of a shift in the distribution of features $p(X)$ versus the impact of a
shift in the conditional distribution of the outcome $p(Y|X)$; however, such
coarse explanations offer only a few options for how one can close the
performance gap. $\textit{Detailed variable-level decompositions}$ that
quantify the importance of each variable to each term in the aggregate
decomposition can provide a much deeper understanding and suggest much more
targeted interventions. However, existing methods assume knowledge of the full
causal graph or make strong parametric assumptions. We introduce a
nonparametric hierarchical framework that provides both aggregate and detailed
decompositions for explaining why the performance of an ML algorithm differs
across domains, without requiring causal knowledge. We derive debiased,
computationally-efficient estimators, and statistical inference procedures for
asymptotically valid confidence intervals.
\\ ( https://arxiv.org/abs/2402.14254 ,  3210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14270
Date: Thu, 22 Feb 2024 04:10:57 GMT   (313kb,D)

Title: Take the Bull by the Horns: Hard Sample-Reweighted Continual Training
  Improves LLM Generalization
Authors: Xuxi Chen, Zhendong Wang, Daouda Sow, Junjie Yang, Tianlong Chen,
  Yingbin Liang, Mingyuan Zhou, Zhangyang Wang
Categories: cs.LG
Comments: Preprint
\\
  In the rapidly advancing arena of large language models (LLMs), a key
challenge is to enhance their capabilities amid a looming shortage of
high-quality training data. Our study starts from an empirical strategy for the
light continual training of LLMs using their original pre-training data sets,
with a specific focus on selective retention of samples that incur moderately
high losses. These samples are deemed informative and beneficial for model
refinement, contrasting with the highest-loss samples, which would be discarded
due to their correlation with data noise and complexity. We then formalize this
strategy into a principled framework of Instance-Reweighted Distributionally
Robust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the
training focus on informative samples through an instance reweighting
mechanism, streamlined by a closed-form solution for straightforward
integration into established training protocols. Through rigorous
experimentation with various models and datasets, our findings indicate that
our sample-targeted methods significantly improve LLM performance across
multiple benchmarks, in both continual pre-training and instruction tuning
scenarios. Our codes are available at
https://github.com/VITA-Group/HardFocusTraining.
\\ ( https://arxiv.org/abs/2402.14270 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14289
Date: Thu, 22 Feb 2024 05:05:30 GMT   (2308kb,D)

Title: TinyLLaVA: A Framework of Small-scale Large Multimodal Models
Authors: Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji
  Wu, Lei Huang
Categories: cs.LG cs.CL
Comments: Our model weights and codes will be made public at
  https://github.com/DLCV-BUAA/TinyLLaVABench
\\
  We present the TinyLLaVA framework that provides a unified perspective in
designing and analyzing the small-scale Large Multimodal Models (LMMs). We
empirically study the effects of different vision encoders, connection modules,
language models, training data and training recipes. Our extensive experiments
showed that better quality of data combined with better training recipes,
smaller LMMs can consistently achieve on-par performances compared to bigger
LMMs. Under our framework, we train a family of small-scale LMMs. Our best
model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B
models such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as
baselines for future research in terms of data scaling, training setups and
model selections. Our model weights and codes will be made public.
\\ ( https://arxiv.org/abs/2402.14289 ,  2308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14294
Date: Thu, 22 Feb 2024 05:16:04 GMT   (127kb,D)

Title: High-arity PAC learning via exchangeability
Authors: Leonardo N. Coregliano and Maryanthe Malliaris
Categories: cs.LG math.LO math.ST stat.TH
Comments: 145 pages, 1 figure
MSC-class: Primary: 68Q32. Secondary: 60F05, 60F15, 03C99
\\
  We develop a theory of high-arity PAC learning, which is statistical learning
in the presence of "structured correlation". In this theory, hypotheses are
either graphs, hypergraphs or, more generally, structures in finite relational
languages, and i.i.d. sampling is replaced by sampling an induced substructure,
producing an exchangeable distribution. We prove a high-arity version of the
fundamental theorem of statistical learning by characterizing high-arity
(agnostic) PAC learnability in terms of finiteness of a purely combinatorial
dimension and in terms of an appropriate version of uniform convergence.
\\ ( https://arxiv.org/abs/2402.14294 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14332
Date: Thu, 22 Feb 2024 06:53:35 GMT   (6301kb,D)

Title: From Large to Small Datasets: Size Generalization for Clustering
  Algorithm Selection
Authors: Vaggos Chatziafratis, Ishani Karmarkar, and Ellen Vitercik
Categories: cs.LG stat.ML
\\
  In clustering algorithm selection, we are given a massive dataset and must
efficiently select which clustering algorithm to use. We study this problem in
a semi-supervised setting, with an unknown ground-truth clustering that we can
only access through expensive oracle queries. Ideally, the clustering
algorithm's output will be structurally close to the ground truth. We approach
this problem by introducing a notion of size generalization for clustering
algorithm accuracy. We identify conditions under which we can (1) subsample the
massive clustering instance, (2) evaluate a set of candidate algorithms on the
smaller instance, and (3) guarantee that the algorithm with the best accuracy
on the small instance will have the best accuracy on the original big instance.
We provide theoretical size generalization guarantees for three classic
clustering algorithms: single-linkage, k-means++, and (a smoothed variant of)
Gonzalez's k-centers heuristic. We validate our theoretical analysis with
empirical results, observing that on real-world clustering instances, we can
use a subsample of as little as 5% of the data to identify which algorithm is
best on the full dataset.
\\ ( https://arxiv.org/abs/2402.14332 ,  6301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14335
Date: Thu, 22 Feb 2024 07:07:16 GMT   (3977kb,D)

Title: HyperFast: Instant Classification for Tabular Data
Authors: David Bonet, Daniel Mas Montserrat, Xavier Gir\'o-i-Nieto, Alexander
  G. Ioannidis
Categories: cs.LG cs.AI stat.ML
Comments: 21 pages, 9 figures, AAAI 2024
\\
  Training deep learning models and performing hyperparameter tuning can be
computationally demanding and time-consuming. Meanwhile, traditional machine
learning methods like gradient-boosting algorithms remain the preferred choice
for most tabular data applications, while neural network alternatives require
extensive hyperparameter tuning or work only in toy datasets under limited
settings. In this paper, we introduce HyperFast, a meta-trained hypernetwork
designed for instant classification of tabular data in a single forward pass.
HyperFast generates a task-specific neural network tailored to an unseen
dataset that can be directly used for classification inference, removing the
need for training a model. We report extensive experiments with OpenML and
genomic data, comparing HyperFast to competing tabular data neural networks,
traditional ML methods, AutoML systems, and boosting machines. HyperFast shows
highly competitive results, while being significantly faster. Additionally, our
approach demonstrates robust adaptability across a variety of classification
tasks with little to no fine-tuning, positioning HyperFast as a strong solution
for numerous applications and rapid model deployment. HyperFast introduces a
promising paradigm for fast classification, with the potential to substantially
decrease the computational burden of deep learning. Our code, which offers a
scikit-learn-like interface, along with the trained HyperFast model, can be
found at https://github.com/AI-sandbox/HyperFast.
\\ ( https://arxiv.org/abs/2402.14335 ,  3977kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14346
Date: Thu, 22 Feb 2024 07:24:26 GMT   (2041kb,D)

Title: Dependable Distributed Training of Compressed Machine Learning Models
Authors: Francesco Malandrino and Giuseppe Di Giacomo and Marco Levorato and
  Carla Fabiana Chiasserini
Categories: cs.LG cs.AI
Journal-ref: IEEE WoWMoM 2024
\\
  The existing work on the distributed training of machine learning (ML) models
has consistently overlooked the distribution of the achieved learning quality,
focusing instead on its average value. This leads to a poor dependability}of
the resulting ML models, whose performance may be much worse than expected. We
fill this gap by proposing DepL, a framework for dependable learning
orchestration, able to make high-quality, efficient decisions on (i) the data
to leverage for learning, (ii) the models to use and when to switch among them,
and (iii) the clusters of nodes, and the resources thereof, to exploit. For
concreteness, we consider as possible available models a full DNN and its
compressed versions. Unlike previous studies, DepL guarantees that a target
learning quality is reached with a target probability, while keeping the
training cost at a minimum. We prove that DepL has constant competitive ratio
and polynomial complexity, and show that it outperforms the state-of-the-art by
over 27% and closely matches the optimum.
\\ ( https://arxiv.org/abs/2402.14346 ,  2041kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14361
Date: Thu, 22 Feb 2024 08:01:01 GMT   (2765kb,D)

Title: OpenTab: Advancing Large Language Models as Open-domain Table Reasoners
Authors: Kezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan,
  Chuan Lei, Christos Faloutsos, Huzefa Rangwala, George Karypis
Categories: cs.LG
Comments: Accepted by ICLR 2024
\\
  Large Language Models (LLMs) trained on large volumes of data excel at
various natural language tasks, but they cannot handle tasks requiring
knowledge that has not been trained on previously. One solution is to use a
retriever that fetches relevant information to expand LLM's knowledge scope.
However, existing textual-oriented retrieval-based LLMs are not ideal on
structured table data due to diversified data modalities and large table sizes.
In this work, we propose OpenTab, an open-domain table reasoning framework
powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant
tables and then generates SQL programs to parse the retrieved tables
efficiently. Utilizing the intermediate data derived from the SQL executions,
it conducts grounded inference to produce accurate response. Extensive
experimental evaluation shows that OpenTab significantly outperforms baselines
in both open- and closed-domain settings, achieving up to 21.5% higher
accuracy. We further run ablation studies to validate the efficacy of our
proposed designs of the system.
\\ ( https://arxiv.org/abs/2402.14361 ,  2765kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14367
Date: Thu, 22 Feb 2024 08:11:22 GMT   (5494kb,D)

Title: Representation Learning for Frequent Subgraph Mining
Authors: Rex Ying, Tianyu Fu, Andrew Wang, Jiaxuan You, Yu Wang, Jure Leskovec
Categories: cs.LG cs.SI
Comments: Oral Presentation in The Graph Representation Learning and Beyond
  (GRL+) Workshop from The 37th International Conference on Ma- chine Learning,
  2020
\\
  Identifying frequent subgraphs, also called network motifs, is crucial in
analyzing and predicting properties of real-world networks. However, finding
large commonly-occurring motifs remains a challenging problem not only due to
its NP-hard subroutine of subgraph counting, but also the exponential growth of
the number of possible subgraphs patterns. Here we present Subgraph Pattern
Miner (SPMiner), a novel neural approach for approximately finding frequent
subgraphs in a large target graph. SPMiner combines graph neural networks,
order embedding space, and an efficient search strategy to identify network
subgraph patterns that appear most frequently in the target graph. SPMiner
first decomposes the target graph into many overlapping subgraphs and then
encodes each subgraph into an order embedding space. SPMiner then uses a
monotonic walk in the order embedding space to identify frequent motifs.
Compared to existing approaches and possible neural alternatives, SPMiner is
more accurate, faster, and more scalable. For 5- and 6-node motifs, we show
that SPMiner can almost perfectly identify the most frequent motifs while being
100x faster than exact enumeration methods. In addition, SPMiner can also
reliably identify frequent 10-node motifs, which is well beyond the size limit
of exact enumeration approaches. And last, we show that SPMiner can find large
up to 20 node motifs with 10-100x higher frequency than those found by current
approximate methods.
\\ ( https://arxiv.org/abs/2402.14367 ,  5494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14384
Date: Thu, 22 Feb 2024 08:54:57 GMT   (859kb,D)

Title: Generative Adversarial Network with Soft-Dynamic Time Warping and
  Parallel Reconstruction for Energy Time Series Anomaly Detection
Authors: Hardik Prabhu, Jayaraman Valadi, and Pandarasamy Arjunan
Categories: cs.LG
Comments: Accepted at AI4TS Workshop AAAI 2024
\\
  In this paper, we employ a 1D deep convolutional generative adversarial
network (DCGAN) for sequential anomaly detection in energy time series data.
Anomaly detection involves gradient descent to reconstruct energy
sub-sequences, identifying the noise vector that closely generates them through
the generator network. Soft-DTW is used as a differentiable alternative for the
reconstruction loss and is found to be superior to Euclidean distance.
Combining reconstruction loss and the latent space's prior probability
distribution serves as the anomaly score. Our novel method accelerates
detection by parallel computation of reconstruction of multiple points and
shows promise in identifying anomalous energy consumption in buildings, as
evidenced by performing experiments on hourly energy time series from 15
buildings.
\\ ( https://arxiv.org/abs/2402.14384 ,  859kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14385
Date: Thu, 22 Feb 2024 08:55:21 GMT   (368kb,D)

Title: WindDragon: Enhancing wind power forecasting with Automated Deep
  Learning
Authors: Julie Keisler (EDF R\&D OSIRIS, EDF R\&D), Etienne Le Naour (ISIR)
Categories: cs.LG physics.ao-ph stat.ML
\\
  Achieving net zero carbon emissions by 2050 requires the integration of
increasing amounts of wind power into power grids. This energy source poses a
challenge to system operators due to its variability and uncertainty.
Therefore, accurate forecasting of wind power is critical for grid operation
and system balancing. This paper presents an innovative approach to short-term
(1 to 6 hour horizon) windpower forecasting at a national level. The method
leverages Automated Deep Learning combined with Numerical Weather Predictions
wind speed maps to accurately forecast wind power.
\\ ( https://arxiv.org/abs/2402.14385 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14389
Date: Thu, 22 Feb 2024 09:01:42 GMT   (512kb,D)

Title: Securing Transactions: A Hybrid Dependable Ensemble Machine Learning
  Model using IHT-LR and Grid Search
Authors: Md. Alamin Talukder, Rakib Hossen, Md Ashraf Uddin, Mohammed Nasir
  Uddin and Uzzal Kumar Acharjee
Categories: cs.LG q-fin.GN
Comments: Q1, Scopus, ISI, ESCI, IF: 4.8 (Accepted on Jan 19, 2024 -
  Cybersecurity, Springer Open Journal)
\\
  Financial institutions and businesses face an ongoing challenge from
fraudulent transactions, prompting the need for effective detection methods.
Detecting credit card fraud is crucial for identifying and preventing
unauthorized transactions.Timely detection of fraud enables investigators to
take swift actions to mitigate further losses. However, the investigation
process is often time-consuming, limiting the number of alerts that can be
thoroughly examined each day. Therefore, the primary objective of a fraud
detection model is to provide accurate alerts while minimizing false alarms and
missed fraud cases. In this paper, we introduce a state-of-the-art hybrid
ensemble (ENS) dependable Machine learning (ML) model that intelligently
combines multiple algorithms with proper weighted optimization using Grid
search, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor
(KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To
address the data imbalance issue, we employ the Instant Hardness Threshold
(IHT) technique in conjunction with Logistic Regression (LR), surpassing
conventional approaches. Our experiments are conducted on a publicly available
credit card dataset comprising 284,807 transactions. The proposed model
achieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a
perfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid
ensemble model outperforms existing works, establishing a new benchmark for
detecting fraudulent transactions in high-frequency scenarios. The results
highlight the effectiveness and reliability of our approach, demonstrating
superior performance metrics and showcasing its exceptional potential for
real-world fraud detection applications.
\\ ( https://arxiv.org/abs/2402.14389 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14391
Date: Thu, 22 Feb 2024 09:04:41 GMT   (1118kb,D)

Title: MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction
  Prediction via Microenvironment-Aware Protein Embedding
Authors: Lirong Wu, Yijun Tian, Yufei Huang, Siyuan Li, Haitao Lin, Nitesh V
  Chawla, Stan Z. Li
Categories: cs.LG q-bio.BM
\\
  Protein-Protein Interactions (PPIs) are fundamental in various biological
processes and play a key role in life activities. The growing demand and cost
of experimental PPI assays require computational methods for efficient PPI
prediction. While existing methods rely heavily on protein sequence for PPI
prediction, it is the protein structure that is the key to determine the
interactions. To take both protein modalities into account, we define the
microenvironment of an amino acid residue by its sequence and structural
contexts, which describe the surrounding chemical properties and geometric
features. In addition, microenvironments defined in previous work are largely
based on experimentally assayed physicochemical properties, for which the
"vocabulary" is usually extremely small. This makes it difficult to cover the
diversity and complexity of microenvironments. In this paper, we propose
Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which
encodes microenvironments into chemically meaningful discrete codes via a
sufficiently large microenvironment "vocabulary" (i.e., codebook). Moreover, we
propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM),
to capture the dependencies between different microenvironments by randomly
masking the codebook and reconstructing the input. With the learned
microenvironment codebook, we can reuse it as an off-the-shelf tool to
efficiently and effectively encode proteins of different sizes and functions
for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can
scale to PPI prediction with millions of PPIs with superior trade-offs between
effectiveness and computational efficiency than the state-of-the-art
competitors.
\\ ( https://arxiv.org/abs/2402.14391 ,  1118kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14393
Date: Thu, 22 Feb 2024 09:08:36 GMT   (3383kb,D)

Title: Graph Parsing Networks
Authors: Yunchong Song, Siyuan Huang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin
Categories: cs.LG
Comments: Published as a conference paper at ICLR 2024
\\
  Graph pooling compresses graph information into a compact representation.
State-of-the-art graph pooling methods follow a hierarchical approach, which
reduces the graph size step-by-step. These methods must balance memory
efficiency with preserving node information, depending on whether they use node
dropping or node clustering. Additionally, fixed pooling ratios or numbers of
pooling layers are predefined for all graphs, which prevents personalized
pooling structures from being captured for each individual graph. In this work,
inspired by bottom-up grammar induction, we propose an efficient graph parsing
algorithm to infer the pooling structure, which then drives graph pooling. The
resulting Graph Parsing Network (GPN) adaptively learns personalized pooling
structure for each individual graph. GPN benefits from the discrete assignments
generated by the graph parsing algorithm, allowing good memory efficiency while
preserving node information intact. Experimental results on standard benchmarks
demonstrate that GPN outperforms state-of-the-art graph pooling methods in
graph classification tasks while being able to achieve competitive performance
in node classification tasks. We also conduct a graph reconstruction task to
show GPN's ability to preserve node information and measure both memory and
time efficiency through relevant tests.
\\ ( https://arxiv.org/abs/2402.14393 ,  3383kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14402
Date: Thu, 22 Feb 2024 09:43:25 GMT   (2384kb,D)

Title: Global Safe Sequential Learning via Efficient Knowledge Transfer
Authors: Cen-You Li, Olaf Duennbier, Marc Toussaint, Barbara Rakitsch,
  Christoph Zimmer
Categories: cs.LG stat.ML
\\
  Sequential learning methods such as active learning and Bayesian optimization
select the most informative data to learn about a task. In many medical or
engineering applications, the data selection is constrained by a priori unknown
safety conditions. A promissing line of safe learning methods utilize Gaussian
processes (GPs) to model the safety probability and perform data selection in
areas with high safety confidence. However, accurate safety modeling requires
prior knowledge or consumes data. In addition, the safety confidence centers
around the given observations which leads to local exploration. As transferable
source knowledge is often available in safety critical experiments, we propose
to consider transfer safe sequential learning to accelerate the learning of
safety. We further consider a pre-computation of source components to reduce
the additional computational load that is introduced by incorporating source
data. In this paper, we theoretically analyze the maximum explorable safe
regions of conventional safe learning methods. Furthermore, we empirically
demonstrate that our approach 1) learns a task with lower data consumption, 2)
globally explores multiple disjoint safe regions under guidance of the source
knowledge, and 3) operates with computation comparable to conventional safe
learning methods.
\\ ( https://arxiv.org/abs/2402.14402 ,  2384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14407
Date: Thu, 22 Feb 2024 09:48:47 GMT   (2182kb,D)

Title: Large-Scale Actionless Video Pre-Training via Discrete Diffusion for
  Efficient Policy Learning
Authors: Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li
Categories: cs.LG cs.CV cs.RO
Comments: 21 pages
\\
  Learning a generalist embodied agent capable of completing multiple tasks
poses challenges, primarily stemming from the scarcity of action-labeled
robotic datasets. In contrast, a vast amount of human videos exist, capturing
intricate tasks and interactions with the physical world. Promising prospects
arise for utilizing actionless human videos for pre-training and transferring
the knowledge to facilitate robot policy learning through limited robot
demonstrations. In this paper, we introduce a novel framework that leverages a
unified discrete diffusion to combine generative pre-training on human videos
and policy fine-tuning on a small number of action-labeled robot videos. We
start by compressing both human and robot videos into unified video tokens. In
the pre-training stage, we employ a discrete diffusion model with a
mask-and-replace diffusion strategy to predict future video tokens in the
latent space. In the fine-tuning stage, we harness the imagined future videos
to guide low-level action learning trained on a limited set of robot data.
Experiments demonstrate that our method generates high-fidelity future videos
for planning and enhances the fine-tuned policies compared to previous
state-of-the-art approaches with superior generalization ability. Our project
website is available at https://video-diff.github.io/.
\\ ( https://arxiv.org/abs/2402.14407 ,  2182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14427
Date: Thu, 22 Feb 2024 10:14:59 GMT   (1461kb,D)

Title: Text me the data: Generating Ground Pressure Sequence from Textual
  Descriptions for HAR
Authors: Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Lars Krupp, Vitor Fortes
  Rey, Paul Lukowicz
Categories: cs.LG cs.CL eess.SP
Comments: PerCom2024WiP
\\
  In human activity recognition (HAR), the availability of substantial ground
truth is necessary for training efficient models. However, acquiring ground
pressure data through physical sensors itself can be cost-prohibitive,
time-consuming. To address this critical need, we introduce Text-to-Pressure
(T2P), a framework designed to generate extensive ground pressure sequences
from textual descriptions of human activities using deep learning techniques.
We show that the combination of vector quantization of sensor data along with
simple text conditioned auto regressive strategy allows us to obtain
high-quality generated pressure sequences from textual descriptions with the
help of discrete latent correlation between text and pressure maps. We achieved
comparable performance on the consistency between text and generated motion
with an R squared value of 0.722, Masked R squared value of 0.892, and FID
score of 1.83. Additionally, we trained a HAR model with the the synthesized
data and evaluated it on pressure dynamics collected by a real pressure sensor
which is on par with a model trained on only real data. Combining both real and
synthesized training data increases the overall macro F1 score by 5.9 percent.
\\ ( https://arxiv.org/abs/2402.14427 ,  1461kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14430
Date: Thu, 22 Feb 2024 10:19:34 GMT   (1220kb,D)

Title: Robust Training of Federated Models with Extremely Label Deficiency
Authors: Yonggang Zhang, Zhiqin Yang, Xinmei Tian, Nannan Wang, Tongliang Liu,
  Bo Han
Categories: cs.LG
Comments: ICLR 2024, 22 pages
\\
  Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm
for collaboratively training machine learning models using distributed data
with label deficiency. Advanced FSSL methods predominantly focus on training a
single model on each client. However, this approach could lead to a discrepancy
between the objective functions of labeled and unlabeled data, resulting in
gradient conflicts. To alleviate gradient conflict, we propose a novel
twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by
providing insights from different perspectives of labeled and unlabeled data.
In particular, Twin-sight concurrently trains a supervised model with a
supervised objective function while training an unsupervised model using an
unsupervised objective function. To enhance the synergy between these two
models, Twin-sight introduces a neighbourhood-preserving constraint, which
encourages the preservation of the neighbourhood relationship among data
features extracted by both models. Our comprehensive experiments on four
benchmark datasets provide substantial evidence that Twin-sight can
significantly outperform state-of-the-art methods across various experimental
settings, demonstrating the efficacy of the proposed Twin-sight.
\\ ( https://arxiv.org/abs/2402.14430 ,  1220kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14474
Date: Thu, 22 Feb 2024 12:04:15 GMT   (427kb,D)

Title: Data Science with LLMs and Interpretable Models
Authors: Sebastian Bordt, Ben Lengerich, Harsha Nori, Rich Caruana
Categories: cs.LG cs.CL
Comments: XAI4Sci Workshop at AAAI-24
\\
  Recent years have seen important advances in the building of interpretable
models, machine learning models that are designed to be easily understood by
humans. In this work, we show that large language models (LLMs) are remarkably
good at working with interpretable models, too. In particular, we show that
LLMs can describe, interpret, and debug Generalized Additive Models (GAMs).
Combining the flexibility of LLMs with the breadth of statistical patterns
accurately described by GAMs enables dataset summarization, question answering,
and model critique. LLMs can also improve the interaction between domain
experts and interpretable models, and generate hypotheses about the underlying
phenomenon. We release \url{https://github.com/interpretml/TalkToEBM} as an
open-source LLM-GAM interface.
\\ ( https://arxiv.org/abs/2402.14474 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14475
Date: Thu, 22 Feb 2024 12:09:52 GMT   (2322kb,D)

Title: DynGMA: a robust approach for learning stochastic differential equations
  from data
Authors: Aiqing Zhu and Qianxiao Li
Categories: cs.LG cs.NA math.NA physics.comp-ph
\\
  Learning unknown stochastic differential equations (SDEs) from observed data
is a significant and challenging task with applications in various fields.
Current approaches often use neural networks to represent drift and diffusion
functions, and construct likelihood-based loss by approximating the transition
density to train these networks. However, these methods often rely on one-step
stochastic numerical schemes, necessitating data with sufficiently high time
resolution. In this paper, we introduce novel approximations to the transition
density of the parameterized SDE: a Gaussian density approximation inspired by
the random perturbation theory of dynamical systems, and its extension, the
dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust
density approximation, our method exhibits superior accuracy compared to
baseline methods in learning the fully unknown drift and diffusion functions
and computing the invariant distribution from trajectory data. And it is
capable of handling trajectory data with low time resolution and variable, even
uncontrollable, time step sizes, such as data generated from Gillespie's
stochastic simulations. We then conduct several experiments across various
scenarios to verify the advantages and robustness of the proposed method.
\\ ( https://arxiv.org/abs/2402.14475 ,  2322kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14481
Date: Thu, 22 Feb 2024 12:13:58 GMT   (2732kb,D)

Title: Towards Automated Causal Discovery: a case study on 5G telecommunication
  data
Authors: Konstantina Biza, Antonios Ntroumpogiannis, Sofia Triantafillou,
  Ioannis Tsamardinos
Categories: cs.LG stat.ME
Comments: 14 pages, 9 figures
\\
  We introduce the concept of Automated Causal Discovery (AutoCD), defined as
any system that aims to fully automate the application of causal discovery and
causal reasoning methods. AutoCD's goal is to deliver all causal information
that an expert human analyst would and answer a user's causal queries. We
describe the architecture of such a platform, and illustrate its performance on
synthetic data sets. As a case study, we apply it on temporal telecommunication
data. The system is general and can be applied to a plethora of causal
discovery problems.
\\ ( https://arxiv.org/abs/2402.14481 ,  2732kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14482
Date: Thu, 22 Feb 2024 12:15:05 GMT   (18137kb,D)

Title: SpanSeq: Similarity-based sequence data splitting method for improved
  development and assessment of deep learning projects
Authors: Alfred Ferrer Florensa, Jose Juan Almagro Armenteros, Henrik Nielsen,
  Frank M{\o}ller Aarestrup, Philip Thomas Lanken Conradsen Clausen
Categories: cs.LG q-bio.QM
\\
  The use of deep learning models in computational biology has increased
massively in recent years, and is expected to do so further with the current
advances in fields like Natural Language Processing. These models, although
able to draw complex relations between input and target, are also largely
inclined to learn noisy deviations from the pool of data used during their
development. In order to assess their performance on unseen data (their
capacity to generalize), it is common to randomly split the available data in
development (train/validation) and test sets. This procedure, although
standard, has lately been shown to produce dubious assessments of
generalization due to the existing similarity between samples in the databases
used. In this work, we present SpanSeq, a database partition method for machine
learning that can scale to most biological sequences (genes, proteins and
genomes) in order to avoid data leakage between sets. We also explore the
effect of not restraining similarity between sets by reproducing the
development of the state-of-the-art model DeepLoc, not only confirming the
consequences of randomly splitting databases on the model assessment, but
expanding those repercussions to the model development. SpanSeq is available
for downloading and installing at
https://github.com/genomicepidemiology/SpanSeq.
\\ ( https://arxiv.org/abs/2402.14482 ,  18137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14490
Date: Thu, 22 Feb 2024 12:27:38 GMT   (3964kb,D)

Title: Imbalanced Data Clustering using Equilibrium K-Means
Authors: Yudong He
Categories: cs.LG stat.ML
\\
  Imbalanced data, characterized by an unequal distribution of data points
across different clusters, poses a challenge for traditional hard and fuzzy
clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and
fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium
K-means (EKM), a novel and simple K-means-type algorithm that alternates
between just two steps, yielding significantly improved clustering results for
imbalanced data by reducing the tendency of centroids to crowd together in the
center of large clusters. We also present a unifying perspective for HKM, FKM,
and EKM, showing they are essentially gradient descent algorithms with an
explicit relationship to Newton's method. EKM has the same time and space
complexity as FKM but offers a clearer physical meaning for its membership
definition. We illustrate the performance of EKM on two synthetic and ten real
datasets, comparing it to various clustering algorithms, including HKM, FKM,
maximum-entropy fuzzy clustering, two FKM variations designed for imbalanced
data, and the Gaussian mixture model. The results demonstrate that EKM performs
competitively on balanced data while significantly outperforming other
techniques on imbalanced data. For high-dimensional data clustering, we
demonstrate that a more discriminative representation can be obtained by
mapping high-dimensional data via deep neural networks into a low-dimensional,
EKM-friendly space. Deep clustering with EKM improves clustering accuracy by
35% on an imbalanced dataset derived from MNIST compared to deep clustering
based on HKM.
\\ ( https://arxiv.org/abs/2402.14490 ,  3964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14527
Date: Thu, 22 Feb 2024 13:21:26 GMT   (2540kb,D)

Title: Federated Learning on Transcriptomic Data: Model Quality and Performance
  Trade-Offs
Authors: Anika Hannemann, Jan Ewald, Leo Seeger, Erik Buchmann
Categories: cs.LG cs.CR q-bio.GN
\\
  Machine learning on large-scale genomic or transcriptomic data is important
for many novel health applications. For example, precision medicine tailors
medical treatments to patients on the basis of individual biomarkers, cellular
and molecular states, etc. However, the data required is sensitive, voluminous,
heterogeneous, and typically distributed across locations where dedicated
machine learning hardware is not available. Due to privacy and regulatory
reasons, it is also problematic to aggregate all data at a trusted third
party.Federated learning is a promising solution to this dilemma, because it
enables decentralized, collaborative machine learning without exchanging raw
data. In this paper, we perform comparative experiments with the federated
learning frameworks TensorFlow Federated and Flower. Our test case is the
training of disease prognosis and cell type classification models. We train the
models with distributed transcriptomic data, considering both data
heterogeneity and architectural heterogeneity. We measure model quality,
robustness against privacy-enhancing noise, computational performance and
resource overhead. Each of the federated learning frameworks has different
strengths. However, our experiments confirm that both frameworks can readily
build models on transcriptomic data, without transferring personal raw data to
a third party with abundant computational resources.
\\ ( https://arxiv.org/abs/2402.14527 ,  2540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14528
Date: Thu, 22 Feb 2024 13:22:06 GMT   (20799kb,D)

Title: ACE : Off-Policy Actor-Critic with Causality-Aware Entropy
  Regularization
Authors: Tianying Ji, Yongyuan Liang, Yan Zeng, Yu Luo, Guowei Xu, Jiawei Guo,
  Ruijie Zheng, Furong Huang, Fuchun Sun, Huazhe Xu
Categories: cs.LG cs.AI
ACM-class: I.2
\\
  The varying significance of distinct primitive behaviors during the policy
learning process has been overlooked by prior model-free RL algorithms.
Leveraging this insight, we explore the causal relationship between different
action dimensions and rewards to evaluate the significance of various primitive
behaviors during training. We introduce a causality-aware entropy term that
effectively identifies and prioritizes actions with high potential impacts for
efficient exploration. Furthermore, to prevent excessive focus on specific
primitive behaviors, we analyze the gradient dormancy phenomenon and introduce
a dormancy-guided reset mechanism to further enhance the efficacy of our
method. Our proposed algorithm, ACE: Off-policy Actor-critic with
Causality-aware Entropy regularization, demonstrates a substantial performance
advantage across 29 diverse continuous control tasks spanning 7 domains
compared to model-free RL baselines, which underscores the effectiveness,
versatility, and efficient sample efficiency of our approach. Benchmark results
and videos are available at https://ace-rl.github.io/.
\\ ( https://arxiv.org/abs/2402.14528 ,  20799kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14532
Date: Thu, 22 Feb 2024 13:24:43 GMT   (493kb,D)

Title: A Framework for Variational Inference of Lightweight Bayesian Neural
  Networks with Heteroscedastic Uncertainties
Authors: David J. Schodt, Ryan Brown, Michael Merritt, Samuel Park, Delsin
  Menolascino, Mark A. Peot
Categories: cs.LG stat.ML
\\
  Obtaining heteroscedastic predictive uncertainties from a Bayesian Neural
Network (BNN) is vital to many applications. Often, heteroscedastic aleatoric
uncertainties are learned as outputs of the BNN in addition to the predictive
means, however doing so may necessitate adding more learnable parameters to the
network. In this work, we demonstrate that both the heteroscedastic aleatoric
and epistemic variance can be embedded into the variances of learned BNN
parameters, improving predictive performance for lightweight networks. By
complementing this approach with a moment propagation approach to inference, we
introduce a relatively simple framework for sampling-free variational inference
suitable for lightweight BNNs.
\\ ( https://arxiv.org/abs/2402.14532 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14547
Date: Thu, 22 Feb 2024 13:36:53 GMT   (1889kb,D)

Title: OmniPred: Language Models as Universal Regressors
Authors: Xingyou Song, Oscar Li, Chansoo Lee, Bangding (Jeffrey) Yang, Daiyi
  Peng, Sagi Perel, Yutian Chen
Categories: cs.LG cs.AI cs.CL cs.DB
Comments: 24 pages, 10 figures
\\
  Over the broad landscape of experimental design, regression has been a
powerful tool to accurately predict the outcome metrics of a system or model
given a set of parameters, but has been traditionally restricted to methods
which are only applicable to a specific task. In this paper, we propose
OmniPred, a framework for training language models as universal end-to-end
regressors over $(x,y)$ evaluation data from diverse real world experiments.
Using data sourced from Google Vizier, one of the largest blackbox optimization
databases in the world, our extensive experiments demonstrate that through only
textual representations of mathematical parameters and values, language models
are capable of very precise numerical regression, and if given the opportunity
to train over multiple tasks, can significantly outperform traditional
regression models.
\\ ( https://arxiv.org/abs/2402.14547 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14585
Date: Thu, 22 Feb 2024 14:38:52 GMT   (17931kb,D)

Title: Bandits with Abstention under Expert Advice
Authors: Stephen Pasteris, Alberto Rumi, Maximilian Thiessen, Shota Saito,
  Atsushi Miyauchi, Fabio Vitale, Mark Herbster
Categories: cs.LG stat.ML
\\
  We study the classic problem of prediction with expert advice under bandit
feedback. Our model assumes that one action, corresponding to the learner's
abstention from play, has no reward or loss on every trial. We propose the CBA
algorithm, which exploits this assumption to obtain reward bounds that can
significantly improve those of the classical Exp4 algorithm. We can view our
problem as the aggregation of confidence-rated predictors when the learner has
the option of abstention from play. Importantly, we are the first to achieve
bounds on the expected cumulative reward for general confidence-rated
predictors. In the special case of specialists we achieve a novel reward bound,
significantly improving previous bounds of SpecialistExp (treating abstention
as another action). As an example application, we discuss learning unions of
balls in a finite metric space. In this contextual setting, we devise an
efficient implementation of CBA, reducing the runtime from quadratic to almost
linear in the number of contexts. Preliminary experiments show that CBA
improves over existing bandit algorithms.
\\ ( https://arxiv.org/abs/2402.14585 ,  17931kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14609
Date: Thu, 22 Feb 2024 14:57:44 GMT   (463kb,D)

Title: Federated Complex Qeury Answering
Authors: Qi Hu, Weifeng Jiang, Haoran Li, Zihao Wang, Jiaxin Bai, Qianren Mao,
  Yangqiu Song, Lixin Fan, Jianxin Li
Categories: cs.LG cs.AI cs.CR cs.DB
\\
  Complex logical query answering is a challenging task in knowledge graphs
(KGs) that has been widely studied. The ability to perform complex logical
reasoning is essential and supports various graph reasoning-based downstream
tasks, such as search engines. Recent approaches are proposed to represent KG
entities and logical queries into embedding vectors and find answers to logical
queries from the KGs. However, existing proposed methods mainly focus on
querying a single KG and cannot be applied to multiple graphs. In addition,
directly sharing KGs with sensitive information may incur privacy risks, making
it impractical to share and construct an aggregated KG for reasoning to
retrieve query answers. Thus, it remains unknown how to answer queries on
multi-source KGs. An entity can be involved in various knowledge graphs and
reasoning on multiple KGs and answering complex queries on multi-source KGs is
important in discovering knowledge cross graphs. Fortunately, federated
learning is utilized in knowledge graphs to collaboratively learn
representations with privacy preserved. Federated knowledge graph embeddings
enrich the relations in knowledge graphs to improve the representation quality.
However, these methods only focus on one-hop relations and cannot perform
complex reasoning tasks. In this paper, we apply federated learning to complex
query-answering tasks to reason over multi-source knowledge graphs while
preserving privacy. We propose a Federated Complex Query Answering framework
(FedCQA), to reason over multi-source KGs avoiding sensitive raw data
transmission to protect privacy. We conduct extensive experiments on three
real-world datasets and evaluate retrieval performance on various types of
complex queries.
\\ ( https://arxiv.org/abs/2402.14609 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14621
Date: Thu, 22 Feb 2024 15:09:13 GMT   (85kb,D)

Title: latrend: A Framework for Clustering Longitudinal Data
Authors: Niek Den Teuling, Steffen Pauws, Edwin van den Heuvel
Categories: cs.LG stat.ML
Comments: 25 pages, 4 figures
\\
  Clustering of longitudinal data is used to explore common trends among
subjects over time for a numeric measurement of interest. Various R packages
have been introduced throughout the years for identifying clusters of
longitudinal patterns, summarizing the variability in trajectories between
subject in terms of one or more trends. We introduce the R package "latrend" as
a framework for the unified application of methods for longitudinal clustering,
enabling comparisons between methods with minimal coding. The package also
serves as an interface to commonly used packages for clustering longitudinal
data, including "dtwclust", "flexmix", "kml", "lcmm", "mclust", "mixAK", and
"mixtools". This enables researchers to easily compare different approaches,
implementations, and method specifications. Furthermore, researchers can build
upon the standard tools provided by the framework to quickly implement new
cluster methods, enabling rapid prototyping. We demonstrate the functionality
and application of the latrend package on a synthetic dataset based on the
therapy adherence patterns of patients with sleep apnea.
\\ ( https://arxiv.org/abs/2402.14621 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14645
Date: Thu, 22 Feb 2024 15:45:27 GMT   (49kb)

Title: Sparse Linear Regression and Lattice Problems
Authors: Aparna Gupte, Neekon Vafa, Vinod Vaikuntanathan
Categories: cs.LG stat.ML
\\
  Sparse linear regression (SLR) is a well-studied problem in statistics where
one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector
$y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is,
$\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find
a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean
squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$.
While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig
selector solve SLR when the design matrix is well-conditioned, no general
algorithm is known, nor is there any formal evidence of hardness in an
average-case setting with respect to all efficient algorithms.
  We give evidence of average-case hardness of SLR w.r.t. all efficient
algorithms assuming the worst-case hardness of lattice problems. Specifically,
we give an instance-by-instance reduction from a variant of the bounded
distance decoding (BDD) problem on lattices to SLR, where the condition number
of the lattice basis that defines the BDD instance is directly related to the
restricted eigenvalue condition of the design matrix, which characterizes some
of the classical statistical-computational gaps for sparse linear regression.
Also, by appealing to worst-case to average-case reductions from the world of
lattices, this shows hardness for a distribution of SLR instances; while the
design matrices are ill-conditioned, the resulting SLR instances are in the
identifiable regime.
  Furthermore, for well-conditioned (essentially) isotropic Gaussian design
matrices, where Lasso is known to behave well in the identifiable regime, we
show hardness of outputting any good solution in the unidentifiable regime
where there are many solutions, assuming the worst-case hardness of standard
and well-studied lattice problems.
\\ ( https://arxiv.org/abs/2402.14645 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14646
Date: Thu, 22 Feb 2024 15:45:31 GMT   (517kb,D)

Title: CoLoRA: Continuous low-rank adaptation for reduced implicit neural
  modeling of parameterized partial differential equations
Authors: Jules Berman and Benjamin Peherstorfer
Categories: cs.LG cs.NA math.NA stat.ML
\\
  This work introduces reduced models based on Continuous Low Rank Adaptation
(CoLoRA) that pre-train neural networks for a given partial differential
equation and then continuously adapt low-rank weights in time to rapidly
predict the evolution of solution fields at new physics parameters and new
initial conditions. The adaptation can be either purely data-driven or via an
equation-driven variational approach that provides Galerkin-optimal
approximations. Because CoLoRA approximates solution fields locally in time,
the rank of the weights can be kept small, which means that only few training
trajectories are required offline so that CoLoRA is well suited for data-scarce
regimes. Predictions with CoLoRA are orders of magnitude faster than with
classical methods and their accuracy and parameter efficiency is higher
compared to other neural network approaches.
\\ ( https://arxiv.org/abs/2402.14646 ,  517kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14648
Date: Thu, 22 Feb 2024 15:53:46 GMT   (2007kb,D)

Title: Rethinking Invariance Regularization in Adversarial Training to Improve
  Robustness-Accuracy Trade-off
Authors: Futa Waseda, Isao Echizen
Categories: cs.LG cs.AI
Comments: 14pages
\\
  Although adversarial training has been the state-of-the-art approach to
defend against adversarial examples (AEs), they suffer from a
robustness-accuracy trade-off. In this work, we revisit representation-based
invariance regularization to learn discriminative yet adversarially invariant
representations, aiming to mitigate this trade-off. We empirically identify two
key issues hindering invariance regularization: (1) a "gradient conflict"
between invariance loss and classification objectives, indicating the existence
of "collapsing solutions," and (2) the mixture distribution problem arising
from diverged distributions of clean and adversarial inputs. To address these
issues, we propose Asymmetrically Representation-regularized Adversarial
Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor
in the invariance loss to avoid "collapsing solutions," inspired by a recent
non-contrastive self-supervised learning approach, and a split-BatchNorm (BN)
structure to resolve the mixture distribution problem. Our method significantly
improves the robustness-accuracy trade-off by learning adversarially invariant
representations without sacrificing discriminative power. Furthermore, we
discuss the relevance of our findings to knowledge-distillation-based defense
methods, contributing to a deeper understanding of their relative successes.
\\ ( https://arxiv.org/abs/2402.14648 ,  2007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14664
Date: Thu, 22 Feb 2024 16:09:45 GMT   (2432kb,D)

Title: Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
Authors: Imad Aouali, Victor-Emmanuel Brunel, David Rohde, Anna Korba
Categories: cs.LG cs.AI stat.ML
Comments: 23 pages, 5 figures
\\
  In interactive systems, actions are often correlated, presenting an
opportunity for more sample-efficient off-policy evaluation (OPE) and learning
(OPL) in large action spaces. We introduce a unified Bayesian framework to
capture these correlations through structured and informative priors. In this
framework, we propose sDM, a generic Bayesian approach designed for OPE and
OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM
leverages action correlations without compromising computational efficiency.
Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics
that assess the average performance of algorithms across multiple problem
instances, deviating from the conventional worst-case assessments. We analyze
sDM in OPE and OPL, highlighting the benefits of leveraging action
correlations. Empirical evidence showcases the strong performance of sDM.
\\ ( https://arxiv.org/abs/2402.14664 ,  2432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14688
Date: Thu, 22 Feb 2024 16:43:16 GMT   (523kb,D)

Title: Q-Probe: A Lightweight Approach to Reward Maximization for Language
  Models
Authors: Kenneth Li, Samy Jelassi, Hugh Zhang, Sham Kakade, Martin Wattenberg,
  David Brandfonbrener
Categories: cs.LG
\\
  We present an approach called Q-probing to adapt a pre-trained language model
to maximize a task-specific reward function. At a high level, Q-probing sits
between heavier approaches such as finetuning and lighter approaches such as
few shot prompting, but can also be combined with either. The idea is to learn
a simple linear function on a model's embedding space that can be used to
reweight candidate completions. We theoretically show that this sampling
procedure is equivalent to a KL-constrained maximization of the Q-probe as the
number of samples increases. To train the Q-probes we consider either reward
modeling or a class of novel direct policy learning objectives based on
importance weighted policy gradients. With this technique, we see gains in
domains with ground-truth rewards (code generation) as well as implicit rewards
defined by preference data, even outperforming finetuning in data-limited
regimes. Moreover, a Q-probe can be trained on top of an API since it only
assumes access to sampling and embeddings. Code:
https://github.com/likenneth/q_probe .
\\ ( https://arxiv.org/abs/2402.14688 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14698
Date: Thu, 22 Feb 2024 16:50:32 GMT   (4520kb,D)

Title: Big data analytics to classify earthwork-related locations: A Chengdu
  study
Authors: Lei Yu, Ke Han
Categories: cs.LG cs.AI
Comments: 11 pages, 7 figures
\\
  Air pollution has significantly intensified, leading to severe health
consequences worldwide. Earthwork-related locations (ERLs) constitute
significant sources of urban dust pollution. The effective management of ERLs
has long posed challenges for governmental and environmental agencies,
primarily due to their classification under different regulatory authorities,
information barriers, delays in data updating, and a lack of dust suppression
measures for various sources of dust pollution. To address these challenges, we
classified urban dust pollution sources using dump truck trajectory, urban
point of interest (POI), and land cover data. We compared several prediction
models and investigated the relationship between features and dust pollution
sources using real data. The results demonstrate that high-accuracy
classification can be achieved with a limited number of features. This method
was successfully implemented in the system called Alpha MAPS in Chengdu to
provide decision support for urban pollution control.
\\ ( https://arxiv.org/abs/2402.14698 ,  4520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14703
Date: Thu, 22 Feb 2024 17:00:50 GMT   (69kb)

Title: On the Curses of Future and History in Future-dependent Value Functions
  for Off-policy Evaluation
Authors: Yuheng Zhang, Nan Jiang
Categories: cs.LG cs.AI stat.ML
\\
  We study off-policy evaluation (OPE) in partially observable environments
with complex observations, with the goal of developing estimators whose
guarantee avoids exponential dependence on the horizon. While such estimators
exist for MDPs and POMDPs can be converted to history-based MDPs, their
estimation errors depend on the state-density ratio for MDPs which becomes
history ratios after conversion, an exponential object. Recently, Uehara et al.
(2022) proposed future-dependent value functions as a promising framework to
address this issue, where the guarantee for memoryless policies depends on the
density ratio over the latent state space. However, it also depends on the
boundedness of the future-dependent value function and other related
quantities, which we show could be exponential-in-length and thus erasing the
advantage of the method. In this paper, we discover novel coverage assumptions
tailored to the structure of POMDPs, such as outcome coverage and belief
coverage. These assumptions not only enable polynomial bounds on the
aforementioned quantities, but also lead to the discovery of new algorithms
with complementary properties.
\\ ( https://arxiv.org/abs/2402.14703 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14708
Date: Thu, 22 Feb 2024 17:08:09 GMT   (630kb,D)

Title: CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph
  Neural Networks
Authors: Yifan Duan, Guibin Zhang, Shilong Wang, Xiaojiang Peng, Wang Ziqi,
  Junyuan Mao, Hao Wu, Xinke Jiang, Kun Wang
Categories: cs.LG cs.AI q-fin.ST
\\
  Credit card fraud poses a significant threat to the economy. While Graph
Neural Network (GNN)-based fraud detection methods perform well, they often
overlook the causal effect of a node's local structure on predictions. This
paper introduces a novel method for credit card fraud detection, the
\textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal
\textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork
(CaT-GNN), which leverages causal invariant learning to reveal inherent
correlations within transaction data. By decomposing the problem into discovery
and intervention phases, CaT-GNN identifies causal nodes within the transaction
graph and applies a causal mixup strategy to enhance the model's robustness and
interpretability. CaT-GNN consists of two key components: Causal-Inspector and
Causal-Intervener. The Causal-Inspector utilizes attention weights in the
temporal attention mechanism to identify causal and environment nodes without
introducing additional parameters. Subsequently, the Causal-Intervener performs
a causal mixup enhancement on environment nodes based on the set of nodes.
Evaluated on three datasets, including a private financial dataset and two
public datasets, CaT-GNN demonstrates superior performance over existing
state-of-the-art methods. Our findings highlight the potential of integrating
causal reasoning with graph neural networks to improve fraud detection
capabilities in financial transactions.
\\ ( https://arxiv.org/abs/2402.14708 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14726
Date: Thu, 22 Feb 2024 17:33:49 GMT   (545kb,D)

Title: Incorporating Expert Rules into Neural Networks in the Framework of
  Concept-Based Learning
Authors: Andrei V. Konstantinov and Lev V. Utkin
Categories: cs.LG cs.AI stat.ML
\\
  A problem of incorporating the expert rules into machine learning models for
extending the concept-based learning is formulated in the paper. It is proposed
how to combine logical rules and neural networks predicting the concept
probabilities. The first idea behind the combination is to form constraints for
a joint probability distribution over all combinations of concept values to
satisfy the expert rules. The second idea is to represent a feasible set of
probability distributions in the form of a convex polytope and to use its
vertices or faces. We provide several approaches for solving the stated problem
and for training neural networks which guarantee that the output probabilities
of concepts would not violate the expert rules. The solution of the problem can
be viewed as a way for combining the inductive and deductive learning. Expert
rules are used in a broader sense when any logical function that connects
concepts and class labels or just concepts with each other can be regarded as a
rule. This feature significantly expands the class of the proposed results.
Numerical examples illustrate the approaches. The code of proposed algorithms
is publicly available.
\\ ( https://arxiv.org/abs/2402.14726 ,  545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14730
Date: Thu, 22 Feb 2024 17:42:15 GMT   (3553kb,D)

Title: Clifford-Steerable Convolutional Neural Networks
Authors: Maksim Zhdanov, David Ruhe, Maurice Weiler, Ana Lucic, Johannes
  Brandstetter, Patrick Forr\'e
Categories: cs.LG cs.AI
\\
  We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a
novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector
fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance,
$\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on
Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit
parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group
equivariant neural networks. We significantly and consistently outperform
baseline methods on fluid dynamics as well as relativistic electrodynamics
forecasting tasks.
\\ ( https://arxiv.org/abs/2402.14730 ,  3553kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14735
Date: Thu, 22 Feb 2024 17:47:03 GMT   (1334kb,D)

Title: How Transformers Learn Causal Structure with Gradient Descent
Authors: Eshaan Nichani, Alex Damian, Jason D. Lee
Categories: cs.LG cs.IT math.IT stat.ML
\\
  The incredible success of transformers on sequence modeling tasks can be
largely attributed to the self-attention mechanism, which allows information to
be transferred between different parts of a sequence. Self-attention allows
transformers to encode causal structure which makes them particularly suitable
for sequence modeling. However, the process by which transformers learn such
causal structure via gradient-based training algorithms remains poorly
understood. To better understand this process, we introduce an in-context
learning task that requires learning latent causal structure. We prove that
gradient descent on a simplified two-layer transformer learns to solve this
task by encoding the latent causal graph in the first attention layer. The key
insight of our proof is that the gradient of the attention matrix encodes the
mutual information between tokens. As a consequence of the data processing
inequality, the largest entries of this gradient correspond to edges in the
latent causal graph. As a special case, when the sequences are generated from
in-context Markov chains, we prove that transformers learn an induction head
(Olsson et al., 2022). We confirm our theoretical findings by showing that
transformers trained on our in-context learning task are able to recover a wide
variety of causal structures.
\\ ( https://arxiv.org/abs/2402.14735 ,  1334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14740
Date: Thu, 22 Feb 2024 17:52:34 GMT   (204kb,D)

Title: Back to Basics: Revisiting REINFORCE Style Optimization for Learning
  from Human Feedback in LLMs
Authors: Arash Ahmadian, Chris Cremer, Matthias Gall\'e, Marzieh Fadaee, Julia
  Kreutzer, Ahmet \"Ust\"un, Sara Hooker
Categories: cs.LG
Comments: 27 pages, 7 figures, 2 tables
ACM-class: I.2.7
\\
  AI alignment in the shape of Reinforcement Learning from Human Feedback
(RLHF) is increasingly treated as a crucial ingredient for high performance
large language models. \textsc{Proximal Policy Optimization} (PPO) has been
positioned by recent literature as the canonical method for the RL part of
RLHF. However, it involves both high computational cost and sensitive
hyperparameter tuning. We posit that most of the motivational principles that
led to the development of PPO are less of a practical concern in RLHF and
advocate for a less computationally expensive method that preserves and even
increases performance. We revisit the \textit{formulation} of alignment from
human preferences in the context of RL. Keeping simplicity as a guiding
principle, we show that many components of PPO are unnecessary in an RLHF
context and that far simpler REINFORCE-style optimization variants outperform
both PPO and newly proposed "RL-free" methods such as DPO and RAFT. Our work
suggests that careful adaptation to LLMs alignment characteristics enables
benefiting from online RL optimization at low cost.
\\ ( https://arxiv.org/abs/2402.14740 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14753
Date: Thu, 22 Feb 2024 18:12:48 GMT   (13944kb,D)

Title: Prompting a Pretrained Transformer Can Be a Universal Approximator
Authors: Aleksandar Petrov, Philip H.S. Torr, Adel Bibi
Categories: cs.LG cs.AI math.FA
\\
  Despite the widespread adoption of prompting, prompt tuning and prefix-tuning
of transformer models, our theoretical understanding of these fine-tuning
methods remains limited. A key question is whether one can arbitrarily modify
the behavior of pretrained model by prompting or prefix-tuning it. Formally,
whether prompting and prefix-tuning a pretrained model can universally
approximate sequence-to-sequence functions. This paper answers in the
affirmative and demonstrates that much smaller pretrained models than
previously thought can be universal approximators when prefixed. In fact, the
attention mechanism is uniquely suited for universal approximation with
prefix-tuning a single attention head being sufficient to approximate any
continuous function. Moreover, any sequence-to-sequence function can be
approximated by prefixing a transformer with depth linear in the sequence
length. Beyond these density-type results, we also offer Jackson-type bounds on
the length of the prefix needed to approximate a function to a desired
precision.
\\ ( https://arxiv.org/abs/2402.14753 ,  13944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14759
Date: Thu, 22 Feb 2024 18:20:25 GMT   (58kb)

Title: Generalising realisability in statistical learning theory under
  epistemic uncertainty
Authors: Fabio Cuzzolin
Categories: cs.LG cs.AI math.ST stat.TH
Comments: arXiv admin note: text overlap with arXiv:2401.09435
\\
  The purpose of this paper is to look into how central notions in statistical
learning theory, such as realisability, generalise under the assumption that
train and test distribution are issued from the same credal set, i.e., a convex
set of probability distributions. This can be considered as a first step
towards a more general treatment of statistical learning under epistemic
uncertainty.
\\ ( https://arxiv.org/abs/2402.14759 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14760
Date: Thu, 22 Feb 2024 18:20:33 GMT   (1441kb,D)

Title: Generalizing Reward Modeling for Out-of-Distribution Preference Learning
Authors: Chen Jia
Categories: cs.LG cs.CL
Comments: 25 pages, 4 figures
\\
  Preference learning (PL) with large language models (LLMs) aims to align the
LLMs' generations with human preferences. Previous work on reinforcement
learning from human feedback (RLHF) has demonstrated promising results in
in-distribution PL. However, due to the difficulty of obtaining human feedback,
discretely training reward models for every encountered distribution is
challenging. Thus, out-of-distribution (OOD) PL is practically useful for
enhancing the generalization ability of LLMs with limited preference feedback.
This work addresses OOD PL by optimizing a general reward model through a
meta-learning approach. During meta-training, a bilevel optimization algorithm
is utilized to learn a reward model capable of guiding policy learning to align
with human preferences across various distributions. When encountering a test
distribution, the meta-test procedure conducts regularized policy optimization
using the learned reward model for PL. We theoretically demonstrate the
convergence rate of the bilevel optimization algorithm under reasonable
assumptions. Additionally, we conduct experiments on two text generation tasks
across 20 held-out domains and outperform a variety of strong baselines across
various evaluation metrics.
\\ ( https://arxiv.org/abs/2402.14760 ,  1441kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14781
Date: Thu, 22 Feb 2024 18:39:24 GMT   (50kb)

Title: Rao-Blackwellising Bayesian Causal Inference
Authors: Christian Toth, Christian Knoll, Franz Pernkopf, Robert Peharz
Categories: cs.LG cs.AI stat.ME stat.ML
Comments: 8 pages + references + appendices (19 pages total)
\\
  Bayesian causal inference, i.e., inferring a posterior over causal models for
the use in downstream causal reasoning tasks, poses a hard computational
inference problem that is little explored in literature. In this work, we
combine techniques from order-based MCMC structure learning with recent
advances in gradient-based graph learning into an effective Bayesian causal
inference framework. Specifically, we decompose the problem of inferring the
causal structure into (i) inferring a topological order over variables and (ii)
inferring the parent sets for each variable. When limiting the number of
parents per variable, we can exactly marginalise over the parent sets in
polynomial time. We further use Gaussian processes to model the unknown causal
mechanisms, which also allows their exact marginalisation. This introduces a
Rao-Blackwellization scheme, where all components are eliminated from the
model, except for the causal order, for which we learn a distribution via
gradient-based optimisation. The combination of Rao-Blackwellization with our
sequential inference procedure for causal orders yields state-of-the-art on
linear and non-linear additive noise benchmarks with scale-free and Erdos-Renyi
graph structures.
\\ ( https://arxiv.org/abs/2402.14781 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14789
Date: Thu, 22 Feb 2024 18:46:22 GMT   (714kb,D)

Title: Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised
  Learning
Authors: Johnathan Xie, Yoonho Lee, Annie S. Chen, Chelsea Finn
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\
  Self-supervised learning excels in learning representations from large
amounts of unlabeled data, demonstrating success across multiple data
modalities. Yet, extending self-supervised learning to new modalities is
non-trivial because the specifics of existing methods are tailored to each
domain, such as domain-specific augmentations which reflect the invariances in
the target task. While masked modeling is promising as a domain-agnostic
framework for self-supervised learning because it does not rely on input
augmentations, its mask sampling procedure remains domain-specific. We present
Self-guided Masked Autoencoders (SMA), a fully domain-agnostic masked modeling
method. SMA trains an attention based model using a masked modeling objective,
by learning masks to sample without any domain-specific assumptions. We
evaluate SMA on three self-supervised learning benchmarks in protein biology,
chemical property prediction, and particle physics. We find SMA is capable of
learning representations without domain-specific knowledge and achieves
state-of-the-art performance on these three benchmarks.
\\ ( https://arxiv.org/abs/2402.14789 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14802
Date: Thu, 22 Feb 2024 18:56:31 GMT   (188kb,D)

Title: Link Prediction under Heterophily: A Physics-Inspired Graph Neural
  Network Approach
Authors: Andrea Giuseppe Di Francesco, Francesco Caso, Maria Sofia Bucarelli
  and Fabrizio Silvestri
Categories: cs.LG cs.IR cs.SI
Comments: 7 pages, 1 figure
\\
  In the past years, Graph Neural Networks (GNNs) have become the `de facto'
standard in various deep learning domains, thanks to their flexibility in
modeling real-world phenomena represented as graphs. However, the
message-passing mechanism of GNNs faces challenges in learnability and
expressivity, hindering high performance on heterophilic graphs, where adjacent
nodes frequently have different labels. Most existing solutions addressing
these challenges are primarily confined to specific benchmarks focused on node
classification tasks. This narrow focus restricts the potential impact that
link prediction under heterophily could offer in several applications,
including recommender systems. For example, in social networks, two users may
be connected for some latent reason, making it challenging to predict such
connections in advance. Physics-Inspired GNNs such as GRAFF provided a
significant contribution to enhance node classification performance under
heterophily, thanks to the adoption of physics biases in the message-passing.
Drawing inspiration from these findings, we advocate that the methodology
employed by GRAFF can improve link prediction performance as well. To further
explore this hypothesis, we introduce GRAFF-LP, an extension of GRAFF to link
prediction. We evaluate its efficacy within a recent collection of heterophilic
graphs, establishing a new benchmark for link prediction under heterophily. Our
approach surpasses previous methods, in most of the datasets, showcasing a
strong flexibility in different contexts, and achieving relative AUROC
improvements of up to 26.7%.
\\ ( https://arxiv.org/abs/2402.14802 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14806
Date: Thu, 22 Feb 2024 18:58:05 GMT   (4619kb,D)

Title: Difference Learning for Air Quality Forecasting Transport Emulation
Authors: Reed River Chen, Christopher Ribaudo, Jennifer Sleeman, Chace
  Ashcraft, Collin Kofroth, Marisa Hughes, Ivanka Stajner, Kevin Viner, Kai
  Wang
Categories: cs.LG physics.ao-ph
\\
  Human health is negatively impacted by poor air quality including increased
risk for respiratory and cardiovascular disease. Due to a recent increase in
extreme air quality events, both globally and locally in the United States,
finer resolution air quality forecasting guidance is needed to effectively
adapt to these events. The National Oceanic and Atmospheric Administration
provides air quality forecasting guidance for the Continental United States.
Their air quality forecasting model is based on a 15 km spatial resolution;
however, the goal is to reach a three km spatial resolution. This is currently
not feasible due in part to prohibitive computational requirements for modeling
the transport of chemical species. In this work, we describe a deep learning
transport emulator that is able to reduce computations while maintaining skill
comparable with the existing numerical model. We show how this method maintains
skill in the presence of extreme air quality events, making it a potential
candidate for operational use. We also explore evaluating how well this model
maintains the physical properties of the modeled transport for a given set of
species.
\\ ( https://arxiv.org/abs/2402.14806 ,  4619kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.14021 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:30:34 GMT   (32kb)

Title: Betting on what is neither verifiable nor falsifiable
Authors: Abhimanyu Pallavi Sudhir, Long Tran-Thanh
Categories: cs.GT cs.AI cs.LO
Comments: 15 pages, 4 figures
MSC-class: 91B26 (Primary), 03F03 (Secondary)
ACM-class: F.4.1; I.2.11
\\
  Prediction markets are useful for estimating probabilities of claims whose
truth will be revealed at some fixed time -- this includes questions about the
values of real-world events (i.e. statistical uncertainty), and questions about
the values of primitive recursive functions (i.e. logical or algorithmic
uncertainty). However, they cannot be directly applied to questions without a
fixed resolution criterion, and real-world applications of prediction markets
to such questions often amount to predicting not whether a sentence is true,
but whether it will be proven. Such questions could be represented by countable
unions or intersections of more basic events, or as First-Order-Logic sentences
on the Arithmetical Hierarchy (or even beyond FOL, as hyperarithmetical
sentences). In this paper, we propose an approach to betting on such events via
options, or equivalently as bets on the outcome of a
"verification-falsification game". Our work thus acts as an alternative to the
existing framework of Garrabrant induction for logical uncertainty, and relates
to the stance known as constructivism in the philosophy of mathematics;
furthermore it has broader implications for philosophy and mathematical logic.
\\ ( https://arxiv.org/abs/2402.14021 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14034 (*cross-listing*)
Date: Wed, 21 Feb 2024 04:11:28 GMT   (5905kb,D)

Title: AgentScope: A Flexible yet Robust Multi-Agent Platform
Authors: Dawei Gao, Zitao Li, Weirui Kuang, Xuchen Pan, Daoyuan Chen, Zhijian
  Ma, Bingchen Qian, Liuyi Yao, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li,
  Bolin Ding, Jingren Zhou
Categories: cs.MA cs.AI
Comments: We have released code on https://github.com/modelscope/agentscope
\\
  With the rapid advancement of Large Language Models (LLMs), significant
progress has been made in multi-agent applications. However, the complexities
in coordinating agents' cooperation and LLMs' erratic performance pose notable
challenges in developing robust and efficient multi-agent applications. To
tackle these challenges, we propose AgentScope, a developer-centric multi-agent
platform with message exchange as its core communication mechanism. Together
with abundant syntactic tools, built-in resources, and user-friendly
interactions, our communication mechanism significantly reduces the barriers to
both development and understanding. Towards robust and flexible multi-agent
application, AgentScope provides both built-in and customizable fault tolerance
mechanisms while it is also armed with system-level supports for multi-modal
data generation, storage and transmission. Additionally, we design an
actor-based distribution framework, enabling easy conversion between local and
distributed deployments and automatic parallel optimization without extra
effort. With these features, AgentScope empowers developers to build
applications that fully realize the potential of intelligent agents. We have
released AgentScope at https://github.com/modelscope/agentscope, and hope
AgentScope invites wider participation and innovation in this fast-moving
field.
\\ ( https://arxiv.org/abs/2402.14034 ,  5905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14037 (*cross-listing*)
Date: Wed, 21 Feb 2024 06:25:50 GMT   (1024kb)

Title: An Effective Networks Intrusion Detection Approach Based on Hybrid
  Harris Hawks and Multi-Layer Perceptron
Authors: Moutaz Alazab, Ruba Abu Khurma, Pedro A. Castillo, Bilal Abu-Salih,
  Alejandro Martin, David Camacho
Categories: cs.NE cs.AI
\\
  This paper proposes an Intrusion Detection System (IDS) employing the Harris
Hawks Optimization algorithm (HHO) to optimize Multilayer Perceptron learning
by optimizing bias and weight parameters. HHO-MLP aims to select optimal
parameters in its learning process to minimize intrusion detection errors in
networks. HHO-MLP has been implemented using EvoloPy NN framework, an
open-source Python tool specialized for training MLPs using evolutionary
algorithms. For purposes of comparing the HHO model against other evolutionary
methodologies currently available, specificity and sensitivity measures,
accuracy measures, and mse and rmse measures have been calculated using KDD
datasets. Experiments have demonstrated the HHO MLP method is effective at
identifying malicious patterns. HHO-MLP has been tested against evolutionary
algorithms like Butterfly Optimization Algorithm (BOA), Grasshopper
Optimization Algorithms (GOA), and Black Widow Optimizations (BOW), with
validation by Random Forest (RF), XG-Boost. HHO-MLP showed superior performance
by attaining top scores with accuracy rate of 93.17%, sensitivity level of
89.25%, and specificity percentage of 95.41%.
\\ ( https://arxiv.org/abs/2402.14037 ,  1024kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14044 (*cross-listing*)
Date: Wed, 21 Feb 2024 11:28:00 GMT   (3924kb)

Title: A new approach for solving global optimization and engineering problems
  based on modified Sea Horse Optimizer
Authors: Fatma A. Hashim, Reham R. Mostafa, Ruba Abu Khurma, Raneem Qaddoura
  and P.A. Castillo
Categories: cs.NE cs.AI
\\
  Sea Horse Optimizer (SHO) is a noteworthy metaheuristic algorithm that
emulates various intelligent behaviors exhibited by sea horses, encompassing
feeding patterns, male reproductive strategies, and intricate movement
patterns. To mimic the nuanced locomotion of sea horses, SHO integrates the
logarithmic helical equation and Levy flight, effectively incorporating both
random movements with substantial step sizes and refined local exploitation.
Additionally, the utilization of Brownian motion facilitates a more
comprehensive exploration of the search space. This study introduces a robust
and high-performance variant of the SHO algorithm named mSHO. The enhancement
primarily focuses on bolstering SHO's exploitation capabilities by replacing
its original method with an innovative local search strategy encompassing three
distinct steps: a neighborhood-based local search, a global non-neighbor-based
search, and a method involving circumnavigation of the existing search region.
These techniques improve mSHO algorithm's search capabilities, allowing it to
navigate the search space and converge toward optimal solutions efficiently.
The comprehensive results distinctly establish the supremacy and efficiency of
the mSHO method as an exemplary tool for tackling an array of optimization
quandaries. The results show that the proposed mSHO algorithm has a total rank
of 1 for CEC'2020 test functions. In contrast, the mSHO achieved the best value
for the engineering problems, recording a value of 0.012665, 2993.634, 0.01266,
1.724967, 263.8915, 0.032255, 58507.14, 1.339956, and 0.23524 for the pressure
vessel design, speed reducer design, tension/compression spring, welded beam
design, three-bar truss engineering design, industrial refrigeration system,
multi-Product batch plant, cantilever beam problem, multiple disc clutch brake
problems, respectively.
\\ ( https://arxiv.org/abs/2402.14044 ,  3924kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14095 (*cross-listing*)
Date: Wed, 21 Feb 2024 19:45:05 GMT   (3906kb,D)

Title: Zero-shot generalization across architectures for visual classification
Authors: Evan Gerrtiz, Luciano Dyballa, Steven W. Zucker
Categories: cs.CV cs.AI cs.LG
Comments: Accepted as a Tiny Paper at ICLR 24
\\
  Generalization to unseen data is a key desideratum for deep networks, but its
relation to classification accuracy is unclear. Using a minimalist vision
dataset and a measure of generalizability, we show that popular networks, from
deep convolutional networks (CNNs) to transformers, vary in their power to
extrapolate to unseen classes both across layers and across architectures.
Accuracy is not a good predictor of generalizability, and generalization varies
non-monotonically with layer depth. Code is available at
https://github.com/dyballa/zero-shot-generalization.
\\ ( https://arxiv.org/abs/2402.14095 ,  3906kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14096 (*cross-listing*)
Date: Wed, 21 Feb 2024 19:45:06 GMT   (7896kb,D)

Title: EyeTrans: Merging Human and Machine Attention for Neural Code
  Summarization
Authors: Yifan Zhang, Jiliang Li, Zachary Karas, Aakash Bansal, Toby Jia-Jun
  Li, Collin McMillan, Kevin Leach, Yu Huang
Categories: cs.SE cs.AI cs.HC
DOI: 10.1145/3643732
\\
  Neural code summarization leverages deep learning models to automatically
generate brief natural language summaries of code snippets. The development of
Transformer models has led to extensive use of attention during model design.
While existing work has primarily and almost exclusively focused on static
properties of source code and related structural representations like the
Abstract Syntax Tree (AST), few studies have considered human attention, that
is, where programmers focus while examining and comprehending code. In this
paper, we develop a method for incorporating human attention into machine
attention to enhance neural code summarization. To facilitate this
incorporation and vindicate this hypothesis, we introduce EyeTrans, which
consists of three steps: (1) we conduct an extensive eye-tracking human study
to collect and pre-analyze data for model training, (2) we devise a
data-centric approach to integrate human attention with machine attention in
the Transformer architecture, and (3) we conduct comprehensive experiments on
two code summarization tasks to demonstrate the effectiveness of incorporating
human attention into Transformers. Integrating human attention leads to an
improvement of up to 29.91% in Functional Summarization and up to 6.39% in
General Code Summarization performance, demonstrating the substantial benefits
of this combination. We further explore performance in terms of robustness and
efficiency by creating challenging summarization scenarios in which EyeTrans
exhibits interesting properties. We also visualize the attention map to depict
the simplifying effect of machine attention in the Transformer by incorporating
human attention. This work has the potential to propel AI research in software
engineering by introducing more human-centered approaches and data.
\\ ( https://arxiv.org/abs/2402.14096 ,  7896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14118 (*cross-listing*)
Date: Wed, 21 Feb 2024 20:36:08 GMT   (823kb,D)

Title: Masked Matrix Multiplication for Emergent Sparsity
Authors: Brian Wheatman, Meghana Madhyastha, and Randal Burns
Categories: cs.DS cs.AI
\\
  Artificial intelligence workloads, especially transformer models, exhibit
emergent sparsity in which computations perform selective sparse access to
dense data. The workloads are inefficient on hardware designed for dense
computations and do not map well onto sparse data representations. We build a
vectorized and parallel matrix-multiplication system A X B = C that eliminates
unnecessary computations and avoids branches based on a runtime evaluation of
sparsity. We use a combination of dynamic code lookup to adapt to the specific
sparsity encoded in the B matrix and preprocessing of sparsity maps of the A
and B matrices to compute conditional branches once for the whole computation.
For a wide range of sparsity, from 60% to 95% zeros, our implementation
performs fewer instructions and increases performance when compared with Intel
MKL's dense or sparse matrix multiply routines. Benefits can be as large as 2
times speedup and 4 times fewer instructions.
\\ ( https://arxiv.org/abs/2402.14118 ,  823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14143 (*cross-listing*)
Date: Wed, 21 Feb 2024 21:55:29 GMT   (4467kb,D)

Title: SecurePose: Automated Face Blurring and Human Movement Kinematics
  Extraction from Videos Recorded in Clinical Settings
Authors: Rishabh Bajpai and Bhooma Aravamuthan
Categories: cs.CV cs.AI
\\
  Movement disorders are typically diagnosed by consensus-based expert
evaluation of clinically acquired patient videos. However, such broad sharing
of patient videos poses risks to patient privacy. Face blurring can be used to
de-identify videos, but this process is often manual and time-consuming.
Available automated face blurring techniques are subject to either excessive,
inconsistent, or insufficient facial blurring - all of which can be disastrous
for video assessment and patient privacy. Furthermore, assessing movement
disorders in these videos is often subjective. The extraction of quantifiable
kinematic features can help inform movement disorder assessment in these
videos, but existing methods to do this are prone to errors if using
pre-blurred videos. We have developed an open-source software called SecurePose
that can both achieve reliable face blurring and automated kinematic extraction
in patient videos recorded in a clinic setting using an iPad. SecurePose,
extracts kinematics using a pose estimation method (OpenPose), tracks and
uniquely identifies all individuals in the video, identifies the patient, and
performs face blurring. The software was validated on gait videos recorded in
outpatient clinic visits of 116 children with cerebral palsy. The validation
involved assessing intermediate steps of kinematics extraction and face
blurring with manual blurring (ground truth). Moreover, when SecurePose was
compared with six selected existing methods, it outperformed other methods in
automated face detection and achieved ceiling accuracy in 91.08% less time than
a robust manual face blurring method. Furthermore, ten experienced researchers
found SecurePose easy to learn and use, as evidenced by the System Usability
Scale. The results of this work validated the performance and usability of
SecurePose on clinically recorded gait videos for face blurring and kinematics
extraction.
\\ ( https://arxiv.org/abs/2402.14143 ,  4467kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14147 (*cross-listing*)
Date: Wed, 21 Feb 2024 22:10:21 GMT   (4844kb,D)

Title: Wikibench: Community-Driven Data Curation for AI Evaluation on Wikipedia
Authors: Tzu-Sheng Kuo, Aaron Halfaker, Zirui Cheng, Jiwoo Kim, Meng-Hsin Wu,
  Tongshuang Wu, Kenneth Holstein, Haiyi Zhu
Categories: cs.HC cs.AI
Journal-ref: Proceedings of the 2024 CHI Conference on Human Factors in
  Computing Systems (CHI '24)
DOI: 10.1145/3613904.3642278
\\
  AI tools are increasingly deployed in community contexts. However, datasets
used to evaluate AI are typically created by developers and annotators outside
a given community, which can yield misleading conclusions about AI performance.
How might we empower communities to drive the intentional design and curation
of evaluation datasets for AI that impacts them? We investigate this question
on Wikipedia, an online community with multiple AI-based content moderation
tools deployed. We introduce Wikibench, a system that enables communities to
collaboratively curate AI evaluation datasets, while navigating ambiguities and
differences in perspective through discussion. A field study on Wikipedia shows
that datasets curated using Wikibench can effectively capture community
consensus, disagreement, and uncertainty. Furthermore, study participants used
Wikibench to shape the overall data curation process, including refining label
definitions, determining data inclusion criteria, and authoring data
statements. Based on our findings, we propose future directions for systems
that support community-driven data curation.
\\ ( https://arxiv.org/abs/2402.14147 ,  4844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14151 (*cross-listing*)
Date: Wed, 21 Feb 2024 22:22:30 GMT   (21533kb,D)

Title: BIRCO: A Benchmark of Information Retrieval Tasks with Complex
  Objectives
Authors: Xiaoyue Wang, Jianyou Wang, Weili Cao, Kaicheng Wang, Ramamohan
  Paturi, Leon Bergen
Categories: cs.IR cs.AI cs.CL cs.LG
\\
  We present the Benchmark of Information Retrieval (IR) tasks with Complex
Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve
documents given multi-faceted user objectives. The benchmark's complexity and
compact size make it suitable for evaluating large language model (LLM)-based
information retrieval systems. We present a modular framework for investigating
factors that may influence LLM performance on retrieval tasks, and identify a
simple baseline model which matches or outperforms existing approaches and more
complex alternatives. No approach achieves satisfactory performance on all
benchmark tasks, suggesting that stronger models and new retrieval protocols
are necessary to address complex user needs.
\\ ( https://arxiv.org/abs/2402.14151 ,  21533kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14162 (*cross-listing*)
Date: Wed, 21 Feb 2024 23:01:38 GMT   (1562kb,D)

Title: On Large Visual Language Models for Medical Imaging Analysis: An
  Empirical Study
Authors: Minh-Hao Van, Prateek Verma, Xintao Wu
Categories: cs.CV cs.AI
\\
  Recently, large language models (LLMs) have taken the spotlight in natural
language processing. Further, integrating LLMs with vision enables the users to
explore emergent abilities with multimodal data. Visual language models (VLMs),
such as LLaVA, Flamingo, or CLIP, have demonstrated impressive performance on
various visio-linguistic tasks. Consequently, there are enormous applications
of large models that could be potentially used in the biomedical imaging field.
Along that direction, there is a lack of related work to show the ability of
large models to diagnose the diseases. In this work, we study the zero-shot and
few-shot robustness of VLMs on the medical imaging analysis tasks. Our
comprehensive experiments demonstrate the effectiveness of VLMs in analyzing
biomedical images such as brain MRIs, microscopic images of blood cells, and
chest X-rays.
\\ ( https://arxiv.org/abs/2402.14162 ,  1562kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14174 (*cross-listing*)
Date: Wed, 21 Feb 2024 23:22:32 GMT   (23875kb,D)

Title: Blending Data-Driven Priors in Dynamic Games
Authors: Justin Lidard, Haimin Hu, Asher Hancock, Zixu Zhang, Albert Gim\'o
  Contreras, Vikash Modi, Jonathan DeCastro, Deepak Gopinath, Guy Rosman, Naomi
  Leonard, Mar\'ia Santos, Jaime Fern\'andez Fisac
Categories: cs.RO cs.AI cs.SY eess.SY math.OC
Comments: 19 pages, 11 figures
\\
  As intelligent robots like autonomous vehicles become increasingly deployed
in the presence of people, the extent to which these systems should leverage
model-based game-theoretic planners versus data-driven policies for safe,
interaction-aware motion planning remains an open question. Existing dynamic
game formulations assume all agents are task-driven and behave optimally.
However, in reality, humans tend to deviate from the decisions prescribed by
these models, and their behavior is better approximated under a noisy-rational
paradigm. In this work, we investigate a principled methodology to blend a
data-driven reference policy with an optimization-based game-theoretic policy.
We formulate KLGame, a type of non-cooperative dynamic game with
Kullback-Leibler (KL) regularization with respect to a general, stochastic, and
possibly multi-modal reference policy. Our method incorporates, for each
decision maker, a tunable parameter that permits modulation between task-driven
and data-driven behaviors. We propose an efficient algorithm for computing
multimodal approximate feedback Nash equilibrium strategies of KLGame in real
time. Through a series of simulated and real-world autonomous driving
scenarios, we demonstrate that KLGame policies can more effectively incorporate
guidance from the reference policy and account for noisily-rational human
behaviors versus non-regularized baselines.
\\ ( https://arxiv.org/abs/2402.14174 ,  23875kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14182 (*cross-listing*)
Date: Thu, 22 Feb 2024 00:01:02 GMT   (56kb,D)

Title: Do Machines and Humans Focus on Similar Code? Exploring Explainability
  of Large Language Models in Code Summarization
Authors: Jiliang Li, Yifan Zhang, Zachary Karas, Collin McMillan, Kevin Leach,
  Yu Huang
Categories: cs.SE cs.AI
DOI: 10.1145/3643916.3644434
\\
  Recent language models have demonstrated proficiency in summarizing source
code. However, as in many other domains of machine learning, language models of
code lack sufficient explainability. Informally, we lack a formulaic or
intuitive understanding of what and how models learn from code. Explainability
of language models can be partially provided if, as the models learn to produce
higher-quality code summaries, they also align in deeming the same code parts
important as those identified by human programmers. In this paper, we report
negative results from our investigation of explainability of language models in
code summarization through the lens of human comprehension. We measure human
focus on code using eye-tracking metrics such as fixation counts and duration
in code summarization tasks. To approximate language model focus, we employ a
state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP
(SHapley Additive exPlanations), to identify which code tokens influence that
generation of summaries. Using these settings, we find no statistically
significant relationship between language models' focus and human programmers'
attention. Furthermore, alignment between model and human foci in this setting
does not seem to dictate the quality of the LLM-generated summaries. Our study
highlights an inability to align human focus with SHAP-based model focus
measures. This result calls for future investigation of multiple open questions
for explainable language models for code summarization and software engineering
tasks in general, including the training mechanisms of language models for
code, whether there is an alignment between human and model attention on code,
whether human attention can improve the development of language models, and
what other model focus measures are appropriate for improving explainability.
\\ ( https://arxiv.org/abs/2402.14182 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14187 (*cross-listing*)
Date: Thu, 22 Feb 2024 00:24:44 GMT   (7550kb,D)

Title: From Adoption to Adaption: Tracing the Diffusion of New Emojis on
  Twitter
Authors: Yuhang Zhou, Xuan Lu, Wei Ai
Categories: cs.CY cs.AI cs.CL cs.HC
Comments: 13 pages, 3 page appendix
\\
  In the rapidly evolving landscape of social media, the introduction of new
emojis in Unicode release versions presents a structured opportunity to explore
digital language evolution. Analyzing a large dataset of sampled English
tweets, we examine how newly released emojis gain traction and evolve in
meaning. We find that community size of early adopters and emoji semantics are
crucial in determining their popularity. Certain emojis experienced notable
shifts in the meanings and sentiment associations during the diffusion process.
Additionally, we propose a novel framework utilizing language models to extract
words and pre-existing emojis with semantically similar contexts, which
enhances interpretation of new emojis. The framework demonstrates its
effectiveness in improving sentiment classification performance by substituting
unknown new emojis with familiar ones. This study offers a new perspective in
understanding how new language units are adopted, adapted, and integrated into
the fabric of online communication.
\\ ( https://arxiv.org/abs/2402.14187 ,  7550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14230 (*cross-listing*)
Date: Thu, 22 Feb 2024 02:21:59 GMT   (8246kb,D)

Title: MerRec: A Large-scale Multipurpose Mercari Dataset for
  Consumer-to-Consumer Recommendation Systems
Authors: Lichi Li, Zainul Abi Din, Zhen Tan, Sam London, Tianlong Chen, Ajay
  Daptardar
Categories: cs.IR cs.AI
\\
  In the evolving e-commerce field, recommendation systems crucially shape user
experience and engagement. The rise of Consumer-to-Consumer (C2C)
recommendation systems, noted for their flexibility and ease of access for
customer vendors, marks a significant trend. However, the academic focus
remains largely on Business-to-Consumer (B2C) models, leaving a gap filled by
the limited C2C recommendation datasets that lack in item attributes, user
diversity, and scale. The intricacy of C2C recommendation systems is further
accentuated by the dual roles users assume as both sellers and buyers,
introducing a spectrum of less uniform and varied inputs. Addressing this, we
introduce MerRec, the first large-scale dataset specifically for C2C
recommendations, sourced from the Mercari e-commerce platform, covering
millions of users and products over 6 months in 2023. MerRec not only includes
standard features such as user_id, item_id, and session_id, but also unique
elements like timestamped action types, product taxonomy, and textual product
attributes, offering a comprehensive dataset for research. This dataset,
extensively evaluated across six recommendation tasks, establishes a new
benchmark for the development of advanced recommendation algorithms in
real-world scenarios, bridging the gap between academia and industry and
propelling the study of C2C recommendations.
\\ ( https://arxiv.org/abs/2402.14230 ,  8246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14241 (*cross-listing*)
Date: Thu, 22 Feb 2024 02:54:43 GMT   (6194kb)

Title: A Self-supervised Pressure Map human keypoint Detection Approch:
  Optimizing Generalization and Computational Efficiency Across Datasets
Authors: Chengzhang Yu and Xianjun Yang and Wenxia Bao and Shaonan Wang and
  Zhiming Yao
Categories: cs.CV cs.AI
Comments: 5pages, 6figures
\\
  In environments where RGB images are inadequate, pressure maps is a viable
alternative, garnering scholarly attention. This study introduces a novel
self-supervised pressure map keypoint detection (SPMKD) method, addressing the
current gap in specialized designs for human keypoint extraction from pressure
maps. Central to our contribution is the Encoder-Fuser-Decoder (EFD) model,
which is a robust framework that integrates a lightweight encoder for precise
human keypoint detection, a fuser for efficient gradient propagation, and a
decoder that transforms human keypoints into reconstructed pressure maps. This
structure is further enhanced by the Classification-to-Regression Weight
Transfer (CRWT) method, which fine-tunes accuracy through initial
classification task training. This innovation not only enhances human keypoint
generalization without manual annotations but also showcases remarkable
efficiency and generalization, evidenced by a reduction to only $5.96\%$ in
FLOPs and $1.11\%$ in parameter count compared to the baseline methods.
\\ ( https://arxiv.org/abs/2402.14241 ,  6194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14245 (*cross-listing*)
Date: Thu, 22 Feb 2024 03:14:03 GMT   (3630kb,D)

Title: Enhancing Robotic Manipulation with AI Feedback from Multimodal Large
  Language Models
Authors: Jinyi Liu, Yifu Yuan, Jianye Hao, Fei Ni, Lingzhi Fu, Yibin Chen, Yan
  Zheng
Categories: cs.RO cs.AI cs.LG
Comments: Presented at AAAI 2024 RL+LLMs Workshop
\\
  Recently, there has been considerable attention towards leveraging large
language models (LLMs) to enhance decision-making processes. However, aligning
the natural language text instructions generated by LLMs with the vectorized
operations required for execution presents a significant challenge, often
necessitating task-specific details. To circumvent the need for such
task-specific granularity, inspired by preference-based policy learning
approaches, we investigate the utilization of multimodal LLMs to provide
automated preference feedback solely from image inputs to guide
decision-making. In this study, we train a multimodal LLM, termed CriticGPT,
capable of understanding trajectory videos in robot manipulation tasks, serving
as a critic to offer analysis and preference feedback. Subsequently, we
validate the effectiveness of preference labels generated by CriticGPT from a
reward modeling perspective. Experimental evaluation of the algorithm's
preference accuracy demonstrates its effective generalization ability to new
tasks. Furthermore, performance on Meta-World tasks reveals that CriticGPT's
reward model efficiently guides policy learning, surpassing rewards based on
state-of-the-art pre-trained representation models.
\\ ( https://arxiv.org/abs/2402.14245 ,  3630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14261 (*cross-listing*)
Date: Thu, 22 Feb 2024 03:51:34 GMT   (1773kb,D)

Title: Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming
Authors: Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller,
  Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, Michele
  Tufano
Categories: cs.SE cs.AI
\\
  The integration of Large Language Models (LLMs) into Development Environments
(IDEs) has become a focal point in modern software development. LLMs such as
OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment
developer productivity by serving as intelligent, chat-driven programming
assistants. However, utilizing LLMs out of the box is unlikely to be optimal
for any given scenario. Rather, each system requires the LLM to be honed to its
set of heuristics to ensure the best performance. In this paper, we introduce
the Copilot evaluation harness: a set of data and tools for evaluating
LLM-guided IDE interactions, covering various programming scenarios and
languages. We propose our metrics as a more robust and information-dense
evaluation than previous state of the art evaluation systems. We design and
compute both static and execution based success metrics for scenarios
encompassing a wide range of developer tasks, including code generation from
natural language (generate), documentation generation from code (doc), test
case generation (test), bug-fixing (fix), and workspace understanding and query
resolution (workspace). These success metrics are designed to evaluate the
performance of LLMs within a given IDE and its respective parameter space. Our
learnings from evaluating three common LLMs using these metrics can inform the
development and validation of future scenarios in LLM guided IDEs.
\\ ( https://arxiv.org/abs/2402.14261 ,  1773kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14299 (*cross-listing*)
Date: Thu, 22 Feb 2024 05:32:27 GMT   (6766kb,D)

Title: We Choose to Go to Space: Agent-driven Human and Multi-Robot
  Collaboration in Microgravity
Authors: Miao Xin, Zhongrui You, Zihan Zhang, Taoran Jiang, Tingjia Xu, Haotian
  Liang, Guojing Ge, Yuchen Ji, Shentong Mo, Jian Cheng
Categories: cs.RO cs.AI
\\
  We present SpaceAgents-1, a system for learning human and multi-robot
collaboration (HMRC) strategies under microgravity conditions. Future space
exploration requires humans to work together with robots. However, acquiring
proficient robot skills and adept collaboration under microgravity conditions
poses significant challenges within ground laboratories. To address this issue,
we develop a microgravity simulation environment and present three typical
configurations of intra-cabin robots. We propose a hierarchical heterogeneous
multi-agent collaboration architecture: guided by foundation models, a
Decision-Making Agent serves as a task planner for human-robot collaboration,
while individual Skill-Expert Agents manage the embodied control of robots.
This mechanism empowers the SpaceAgents-1 system to execute a range of
intricate long-horizon HMRC tasks.
\\ ( https://arxiv.org/abs/2402.14299 ,  6766kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14304 (*cross-listing*)
Date: Thu, 22 Feb 2024 05:45:17 GMT   (5310kb,D)

Title: Vision-Language Navigation with Embodied Intelligence: A Survey
Authors: Peng Gao, Peng Wang, Feng Gao, Fei Wang, Ruyue Yuan
Categories: cs.RO cs.AI cs.CV
Comments: 31 pages, 182 references
\\
  As a long-term vision in the field of artificial intelligence, the core goal
of embodied intelligence is to improve the perception, understanding, and
interaction capabilities of agents and the environment. Vision-language
navigation (VLN), as a critical research path to achieve embodied intelligence,
focuses on exploring how agents use natural language to communicate effectively
with humans, receive and understand instructions, and ultimately rely on visual
information to achieve accurate navigation. VLN integrates artificial
intelligence, natural language processing, computer vision, and robotics. This
field faces technical challenges but shows potential for application such as
human-computer interaction. However, due to the complex process involved from
language understanding to action execution, VLN faces the problem of aligning
visual information and language instructions, improving generalization ability,
and many other challenges. This survey systematically reviews the research
progress of VLN and details the research direction of VLN with embodied
intelligence. After a detailed summary of its system architecture and research
based on methods and commonly used benchmark datasets, we comprehensively
analyze the problems and challenges faced by current research and explore the
future development direction of this field, aiming to provide a practical
reference for researchers.
\\ ( https://arxiv.org/abs/2402.14304 ,  5310kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14323 (*cross-listing*)
Date: Thu, 22 Feb 2024 06:34:50 GMT   (1470kb,D)

Title: REPOFUSE: Repository-Level Code Completion with Fused Dual Context
Authors: Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, wei
  jiang, Hongwei Chen, Chengpeng Wang, Gang Fan
Categories: cs.SE cs.AI
\\
  The success of language models in code assistance has spurred the proposal of
repository-level code completion as a means to enhance prediction accuracy,
utilizing the context from the entire codebase. However, this amplified context
can inadvertently increase inference latency, potentially undermining the
developer experience and deterring tool adoption-a challenge we termed the
Context-Latency Conundrum. This paper introduces RepoGenix, a pioneering
solution designed to enhance repository-level code completion without the
latency trade-off. RepoGenix uniquely fuses two types of contexts: the analogy
context, rooted in code analogies, and the rationale context, which encompasses
in-depth semantic relationships. We propose a novel rank truncated generation
(RTG) technique that efficiently condenses these contexts into prompts with
restricted size. This enables RepoGenix to deliver precise code completions
while maintaining inference efficiency. Through testing with the CrossCodeEval
suite, RepoGenix has demonstrated a significant leap over existing models,
achieving a 40.90% to 59.75% increase in exact match (EM) accuracy for code
completions and a 26.8% enhancement in inference speed. Beyond experimental
validation, RepoGenix has been integrated into the workflow of a large
enterprise, where it actively supports various coding tasks.
\\ ( https://arxiv.org/abs/2402.14323 ,  1470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14398 (*cross-listing*)
Date: Thu, 22 Feb 2024 09:28:47 GMT   (44353kb,D)

Title: Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion
  and Image Attribute Editing
Authors: Hao Li, Mengqi Huang, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao
Categories: cs.CV cs.AI
Comments: 18 pages, 18 figures, published to AAAI24
\\
  GAN-based image attribute editing firstly leverages GAN Inversion to project
real images into the latent space of GAN and then manipulates corresponding
latent codes. Recent inversion methods mainly utilize additional high-bit
features to improve image details preservation, as low-bit codes cannot
faithfully reconstruct source images, leading to the loss of details. However,
during editing, existing works fail to accurately complement the lost details
and suffer from poor editability. The main reason is they inject all the lost
details indiscriminately at one time, which inherently induces the position and
quantity of details to overfit source images, resulting in inconsistent content
and artifacts in edited images. This work argues that details should be
gradually injected into both the reconstruction and editing process in a
multi-stage coarse-to-fine manner for better detail preservation and high
editability. Therefore, a novel dual-stream framework is proposed to accurately
complement details at each stage. The Reconstruction Stream is employed to
embed coarse-to-fine lost details into residual features and then adaptively
add them to the GAN generator. In the Editing Stream, residual features are
accurately aligned by our Selective Attention mechanism and then injected into
the editing process in a multi-stage manner. Extensive experiments have shown
the superiority of our framework in both reconstruction accuracy and editing
quality compared with existing methods.
\\ ( https://arxiv.org/abs/2402.14398 ,  44353kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14399 (*cross-listing*)
Date: Thu, 22 Feb 2024 09:32:34 GMT   (4785kb,D)

Title: Ensure Timeliness and Accuracy: A Novel Sliding Window Data Stream
  Paradigm for Live Streaming Recommendation
Authors: Fengqi Liang, Baigong Zheng, Liqin Zhao, Guorui Zhou, Qian Wang, Yanan
  Niu
Categories: cs.IR cs.AI
\\
  Live streaming recommender system is specifically designed to recommend
real-time live streaming of interest to users. Due to the dynamic changes of
live content, improving the timeliness of the live streaming recommender system
is a critical problem. Intuitively, the timeliness of the data determines the
upper bound of the timeliness that models can learn. However, none of the
previous works addresses the timeliness problem of the live streaming
recommender system from the perspective of data stream design. Employing the
conventional fixed window data stream paradigm introduces a trade-off dilemma
between labeling accuracy and timeliness. In this paper, we propose a new data
stream design paradigm, dubbed Sliver, that addresses the timeliness and
accuracy problem of labels by reducing the window size and implementing a
sliding window correspondingly. Meanwhile, we propose a time-sensitive re-reco
strategy reducing the latency between request and impression to improve the
timeliness of the recommendation service and features by periodically
requesting the recommendation service. To demonstrate the effectiveness of our
approach, we conduct offline experiments on a multi-task live streaming dataset
with labeling timestamps collected from the Kuaishou live streaming platform.
Experimental results demonstrate that Sliver outperforms two fixed-window data
streams with varying window sizes across all targets in four typical multi-task
recommendation models. Furthermore, we deployed Sliver on the Kuaishou live
streaming platform. Results of the online A/B test show a significant
improvement in click-through rate (CTR), and new follow number (NFN), further
validating the effectiveness of Sliver.
\\ ( https://arxiv.org/abs/2402.14399 ,  4785kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14418 (*cross-listing*)
Date: Thu, 22 Feb 2024 10:04:17 GMT   (1128kb,D)

Title: Uncertainty-Aware Evaluation for Vision-Language Models
Authors: Vasily Kostumov, Bulat Nutfullin, Oleg Pilipenko, Eugene Ilyushin
Categories: cs.CV cs.AI
\\
  Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in
popularity recently due to their impressive performance in several
vision-language tasks. Current evaluation methods, however, overlook an
essential component: uncertainty, which is crucial for a comprehensive
assessment of VLMs. Addressing this oversight, we present a benchmark
incorporating uncertainty quantification into evaluating VLMs.
  Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question
Answering (VQA) task. We examine models on 5 datasets that evaluate various
vision-language capabilities.
  Using conformal prediction as an uncertainty estimation approach, we
demonstrate that the models' uncertainty is not aligned with their accuracy.
Specifically, we show that models with the highest accuracy may also have the
highest uncertainty, which confirms the importance of measuring it for VLMs.
Our empirical findings also reveal a correlation between model uncertainty and
its language model part.
\\ ( https://arxiv.org/abs/2402.14418 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14473 (*cross-listing*)
Date: Thu, 22 Feb 2024 12:03:21 GMT   (1386kb,D)

Title: Personalized Behavior-Aware Transformer for Multi-Behavior Sequential
  Recommendation
Authors: Jiajie Su, Chaochao Chen, Zibin Lin, Xi Li, Weiming Liu, and Xiaolin
  Zheng
Categories: cs.IR cs.AI
Journal-ref: Proceedings of the 31st ACM International Conference on
  Multimedia. 2023: 6321-6331
\\
  Sequential Recommendation (SR) captures users' dynamic preferences by
modeling how users transit among items. However, SR models that utilize only
single type of behavior interaction data encounter performance degradation when
the sequences are short. To tackle this problem, we focus on Multi-Behavior
Sequential Recommendation (MBSR) in this paper, which aims to leverage
time-evolving heterogeneous behavioral dependencies for better exploring users'
potential intents on the target behavior. Solving MBSR is challenging. On the
one hand, users exhibit diverse multi-behavior patterns due to personal
characteristics. On the other hand, there exists comprehensive co-influence
between behavior correlations and item collaborations, the intensity of which
is deeply affected by temporal factors. To tackle these challenges, we propose
a Personalized Behavior-Aware Transformer framework (PBAT) for MBSR problem,
which models personalized patterns and multifaceted sequential collaborations
in a novel way to boost recommendation performance. First, PBAT develops a
personalized behavior pattern generator in the representation layer, which
extracts dynamic and discriminative behavior patterns for sequential learning.
Second, PBAT reforms the self-attention layer with a behavior-aware
collaboration extractor, which introduces a fused behavior-aware attention
mechanism for incorporating both behavioral and temporal impacts into
collaborative transitions. We conduct experiments on three benchmark datasets
and the results demonstrate the effectiveness and interpretability of our
framework. Our implementation code is released at
https://github.com/TiliaceaeSU/PBAT.
\\ ( https://arxiv.org/abs/2402.14473 ,  1386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14486 (*cross-listing*)
Date: Thu, 22 Feb 2024 12:19:19 GMT   (35kb)

Title: Are Bounded Contracts Learnable and Approximately Optimal?
Authors: Yurong Chen, Zhaohua Chen, Xiaotie Deng, and Zhiyi Huang
Categories: cs.GT cs.AI cs.LG econ.TH
\\
  This paper considers the hidden-action model of the principal-agent problem,
in which a principal incentivizes an agent to work on a project using a
contract. We investigate whether contracts with bounded payments are learnable
and approximately optimal. Our main results are two learning algorithms that
can find a nearly optimal bounded contract using a polynomial number of
queries, under two standard assumptions in the literature: a costlier action
for the agent leads to a better outcome distribution for the principal, and the
agent's cost/effort has diminishing returns. Our polynomial query complexity
upper bound shows that standard assumptions are sufficient for achieving an
exponential improvement upon the known lower bound for general instances.
Unlike the existing algorithms, which relied on discretizing the contract
space, our algorithms directly learn the underlying outcome distributions. As
for the approximate optimality of bounded contracts, we find that they could be
far from optimal in terms of multiplicative or additive approximation, but
satisfy a notion of mixed approximation.
\\ ( https://arxiv.org/abs/2402.14486 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14498 (*cross-listing*)
Date: Thu, 22 Feb 2024 12:47:04 GMT   (6366kb,D)

Title: A Collision-Aware Cable Grasping Method in Cluttered Environment
Authors: Lei Zhang, Kaixin Bai, Qiang Li, Zhaopeng Chen, Jianwei Zhang
Categories: cs.RO cs.AI
Comments: 7 pages
\\
  We introduce a Cable Grasping-Convolutional Neural Network designed to
facilitate robust cable grasping in cluttered environments. Utilizing physics
simulations, we generate an extensive dataset that mimics the intricacies of
cable grasping, factoring in potential collisions between cables and robotic
grippers. We employ the Approximate Convex Decomposition technique to dissect
the non-convex cable model, with grasp quality autonomously labeled based on
simulated grasping attempts. The CG-CNN is refined using this simulated dataset
and enhanced through domain randomization techniques. Subsequently, the trained
model predicts grasp quality, guiding the optimal grasp pose to the robot
controller for execution. Grasping efficacy is assessed across both synthetic
and real-world settings. Given our model implicit collision sensitivity, we
achieved commendable success rates of 92.3% for known cables and 88.4% for
unknown cables, surpassing contemporary state-of-the-art approaches.
Supplementary materials can be found at
https://leizhang-public.github.io/cg-cnn/ .
\\ ( https://arxiv.org/abs/2402.14498 ,  6366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14505 (*cross-listing*)
Date: Thu, 22 Feb 2024 12:55:01 GMT   (8452kb,D)

Title: Towards Seamless Adaptation of Pre-trained Models for Visual Place
  Recognition
Authors: Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun
  Yuan
Categories: cs.CV cs.AI
Comments: ICLR2024
\\
  Recent studies show that vision models pre-trained in generic visual learning
tasks with large-scale data can provide useful feature representations for a
wide range of visual perception problems. However, few attempts have been made
to exploit pre-trained foundation models in visual place recognition (VPR). Due
to the inherent difference in training objectives and data between the tasks of
model pre-training and VPR, how to bridge the gap and fully unleash the
capability of pre-trained models for VPR is still a key issue to address. To
this end, we propose a novel method to realize seamless adaptation of
pre-trained models for VPR. Specifically, to obtain both global and local
features that focus on salient landmarks for discriminating places, we design a
hybrid adaptation method to achieve both global and local adaptation
efficiently, in which only lightweight adapters are tuned without adjusting the
pre-trained model. Besides, to guide effective adaptation, we propose a mutual
nearest neighbor local feature loss, which ensures proper dense local features
are produced for local matching and avoids time-consuming spatial verification
in re-ranking. Experimental results show that our method outperforms the
state-of-the-art methods with less training data and training time, and uses
about only 3% retrieval runtime of the two-stage VPR methods with RANSAC-based
spatial verification. It ranks 1st on the MSLS challenge leaderboard (at the
time of submission). The code is released at
https://github.com/Lu-Feng/SelaVPR.
\\ ( https://arxiv.org/abs/2402.14505 ,  8452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14551 (*cross-listing*)
Date: Thu, 22 Feb 2024 13:45:01 GMT   (6841kb,D)

Title: CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for
  Optimized Learning Fusion
Authors: Zijun Long and George Killick and Lipeng Zhuang and Gerardo
  Aragon-Camarasa and Zaiqiao Meng and Richard Mccreadie
Categories: cs.CV cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2308.14893
\\
  State-of-the-art pre-trained image models predominantly adopt a two-stage
approach: initial unsupervised pre-training on large-scale datasets followed by
task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been
demonstrated that CE can compromise model generalization and stability. While
recent works employing contrastive learning address some of these limitations
by enhancing the quality of embeddings and producing better decision
boundaries, they often overlook the importance of hard negative mining and rely
on resource intensive and slow training using large sample batches. To counter
these issues, we introduce a novel approach named CLCE, which integrates
Label-Aware Contrastive Learning with CE. Our approach not only maintains the
strengths of both loss functions but also leverages hard negative mining in a
synergistic way to enhance performance. Experimental results demonstrate that
CLCE significantly outperforms CE in Top-1 accuracy across twelve benchmarks,
achieving gains of up to 3.52% in few-shot learning scenarios and 3.41% in
transfer learning settings with the BEiT-3 model. Importantly, our proposed
CLCE approach effectively mitigates the dependency of contrastive learning on
large batch sizes such as 4096 samples per batch, a limitation that has
previously constrained the application of contrastive learning in
budget-limited hardware environments.
\\ ( https://arxiv.org/abs/2402.14551 ,  6841kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14569 (*cross-listing*)
Date: Thu, 22 Feb 2024 14:20:07 GMT   (2127kb,D)

Title: Transformable Gaussian Reward Function for Socially-Aware Navigation
  with Deep Reinforcement Learning
Authors: Jinyeob Kim, Sumin Kang, Sungwoo Yang, Beomjoon Kim, Jargalbaatar
  Yura, Donghan Kim
Categories: cs.RO cs.AI
Comments: 22 pages, 9 figures
\\
  Robot navigation has transitioned from prioritizing obstacle avoidance to
adopting socially aware navigation strategies that accommodate human presence.
As a result, the recognition of socially aware navigation within dynamic
human-centric environments has gained prominence in the field of robotics.
Although reinforcement learning technique has fostered the advancement of
socially aware navigation, defining appropriate reward functions, especially in
congested environments, has posed a significant challenge. These rewards,
crucial in guiding robot actions, demand intricate human-crafted design due to
their complex nature and inability to be automatically set. The multitude of
manually designed rewards poses issues with hyperparameter redundancy,
imbalance, and inadequate representation of unique object characteristics. To
address these challenges, we introduce a transformable gaussian reward function
(TGRF). The TGRF significantly reduces the burden of hyperparameter tuning,
displays adaptability across various reward functions, and demonstrates
accelerated learning rates, particularly excelling in crowded environments
utilizing deep reinforcement learning (DRL). We introduce and validate TGRF
through sections highlighting its conceptual background, characteristics,
experiments, and real-world application, paving the way for a more effective
and adaptable approach in robotics.The complete source code is available on
https://github.com/JinnnK/TGRF
\\ ( https://arxiv.org/abs/2402.14569 ,  2127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14582 (*cross-listing*)
Date: Thu, 8 Feb 2024 13:51:13 GMT   (8559kb,D)

Title: Enhancement of High-definition Map Update Service Through Coverage-aware
  and Reinforcement Learning
Authors: Jeffrey Redondo, Zhenhui Yuan, Nauman Aslam
Categories: cs.NI cs.AI cs.LG
\\
  High-definition (HD) Map systems will play a pivotal role in advancing
autonomous driving to a higher level, thanks to the significant improvement
over traditional two-dimensional (2D) maps. Creating an HD Map requires a huge
amount of on-road and off-road data. Typically, these raw datasets are
collected and uploaded to cloud-based HD map service providers through
vehicular networks. Nevertheless, there are challenges in transmitting the raw
data over vehicular wireless channels due to the dynamic topology. As the
number of vehicles increases, there is a detrimental impact on service quality,
which acts as a barrier to a real-time HD Map system for collaborative driving
in Autonomous Vehicles (AV). In this paper, to overcome network congestion, a
Q-learning coverage-time-awareness algorithm is presented to optimize the
quality of service for vehicular networks and HD map updates. The algorithm is
evaluated in an environment that imitates a dynamic scenario where vehicles
enter and leave. Results showed an improvement in latency for HD map data of
$75\%$, $73\%$, and $10\%$ compared with IEEE802.11p without Quality of Service
(QoS), IEEE802.11 with QoS, and IEEE802.11p with new access category (AC) for
HD map, respectively.
\\ ( https://arxiv.org/abs/2402.14582 ,  8559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14594 (*cross-listing*)
Date: Sun, 4 Feb 2024 20:42:30 GMT   (850kb,D)

Title: Improving Assessment of Tutoring Practices using Retrieval-Augmented
  Generation
Authors: Zifei (FeiFei) Han, Jionghao Lin, Ashish Gurung, Danielle R. Thomas,
  Eason Chen, Conrad Borchers, Shivang Gupta, Kenneth R. Koedinger
Categories: cs.CY cs.AI cs.CL cs.HC cs.IR
Comments: 11 page Workshop paper, AAAI2024 Workshop on AI for Education -
  Bridging Innovation and Responsibility, Large Language Model, Personalized
  Tutor Training, Automatic Assessment
\\
  One-on-one tutoring is an effective instructional method for enhancing
learning, yet its efficacy hinges on tutor competencies. Novice math tutors
often prioritize content-specific guidance, neglecting aspects such as
social-emotional learning. Social-emotional learning promotes equity and
inclusion and nurturing relationships with students, which is crucial for
holistic student development. Assessing the competencies of tutors accurately
and efficiently can drive the development of tailored tutor training programs.
However, evaluating novice tutor ability during real-time tutoring remains
challenging as it typically requires experts-in-the-loop. To address this
challenge, this preliminary study aims to harness Generative Pre-trained
Transformers (GPT), such as GPT-3.5 and GPT-4 models, to automatically assess
tutors' ability of using social-emotional tutoring strategies. Moreover, this
study also reports on the financial dimensions and considerations of employing
these models in real-time and at scale for automated assessment. The current
study examined four prompting strategies: two basic Zero-shot prompt
strategies, Tree of Thought prompt, and Retrieval-Augmented Generator (RAG)
based prompt. The results indicate that the RAG prompt demonstrated more
accurate performance (assessed by the level of hallucination and correctness in
the generated assessment texts) and lower financial costs than the other
strategies evaluated. These findings inform the development of personalized
tutor training interventions to enhance the the educational effectiveness of
tutored learning.
\\ ( https://arxiv.org/abs/2402.14594 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14601 (*cross-listing*)
Date: Fri, 2 Feb 2024 23:54:51 GMT   (1244kb,D)

Title: Bringing Generative AI to Adaptive Learning in Education
Authors: Hang Li, Tianlong Xu, Chaoli Zhang, Eason Chen, Jing Liang, Xing Fan,
  Haoyang Li, Jiliang Tang, Qingsong Wen
Categories: cs.CY cs.AI cs.HC cs.LG
Comments: 14 pages, 5 figures
\\
  The recent surge in generative AI technologies, such as large language models
and diffusion models, have boosted the development of AI applications in
various domains, including science, finance, and education. Concurrently,
adaptive learning, a concept that has gained substantial interest in the
educational sphere, has proven its efficacy in enhancing students' learning
efficiency. In this position paper, we aim to shed light on the intersectional
studies of these two methods, which combine generative AI with adaptive
learning concepts. By presenting discussions about the benefits, challenges,
and potentials in this field, we argue that this union will contribute
significantly to the development of the next stage learning format in
education.
\\ ( https://arxiv.org/abs/2402.14601 ,  1244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14622 (*cross-listing*)
Date: Thu, 22 Feb 2024 15:10:45 GMT   (3887kb,D)

Title: From Keywords to Structured Summaries: Streamlining Scholarly Knowledge
  Access
Authors: Mahsa Shamsabadi and Jennifer D'Souza
Categories: cs.IR cs.AI cs.CL cs.DL
Comments: 6 pages, 1 figure
\\
  This short paper highlights the growing importance of information retrieval
(IR) engines in the scientific community, addressing the inefficiency of
traditional keyword-based search engines due to the rising volume of
publications. The proposed solution involves structured records, underpinning
advanced information technology (IT) tools, including visualization dashboards,
to revolutionize how researchers access and filter articles, replacing the
traditional text-heavy approach. This vision is exemplified through a proof of
concept centered on the ``reproductive number estimate of infectious diseases''
research theme, using a fine-tuned large language model (LLM) to automate the
creation of structured records to populate a backend database that now goes
beyond keywords. The result is a next-generation IR method accessible at
https://orkg.org/usecases/r0-estimates.
\\ ( https://arxiv.org/abs/2402.14622 ,  3887kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14623 (*cross-listing*)
Date: Thu, 22 Feb 2024 15:12:00 GMT   (10196kb,D)

Title: RoboScript: Code Generation for Free-Form Manipulation Tasks across Real
  and Simulation
Authors: Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng
  Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, Yu Qiao, Huazhe
  Xu, Mingyu Ding, Ping Luo
Categories: cs.RO cs.AI cs.CL cs.CV
Comments: 10 pages of main paper, 4 pages of appendix; 10 figures in main
  paper, 3 figures in appendix
ACM-class: I.2.7; I.2.8; I.2.9; I.2.10
\\
  Rapid progress in high-level task planning and code generation for open-world
robot manipulation has been witnessed in Embodied AI. However, previous studies
put much effort into general common sense reasoning and task planning
capabilities of large-scale language or multi-modal models, relatively little
effort on ensuring the deployability of generated code on real robots, and
other fundamental components of autonomous robot systems including robot
perception, motion planning, and control. To bridge this ``ideal-to-real'' gap,
this paper presents \textbf{RobotScript}, a platform for 1) a deployable robot
manipulation pipeline powered by code generation; and 2) a code generation
benchmark for robot manipulation tasks in free-form natural language. The
RobotScript platform addresses this gap by emphasizing the unified interface
with both simulation and real robots, based on abstraction from the Robot
Operating System (ROS), ensuring syntax compliance and simulation validation
with Gazebo. We demonstrate the adaptability of our code generation framework
across multiple robot embodiments, including the Franka and UR5 robot arms, and
multiple grippers. Additionally, our benchmark assesses reasoning abilities for
physical space and constraints, highlighting the differences between GPT-3.5,
GPT-4, and Gemini in handling complex physical interactions. Finally, we
present a thorough evaluation on the whole system, exploring how each module in
the pipeline: code generation, perception, motion planning, and even object
geometric properties, impact the overall performance of the system.
\\ ( https://arxiv.org/abs/2402.14623 ,  10196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14658 (*cross-listing*)
Date: Thu, 22 Feb 2024 16:06:23 GMT   (2930kb,D)

Title: OpenCodeInterpreter: Integrating Code Generation with Execution and
  Refinement
Authors: Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin,
  Jie Fu, Wenhu Chen, and Xiang Yue
Categories: cs.SE cs.AI cs.CL
\\
  The introduction of large language models has significantly advanced code
generation. However, open-source models often lack the execution capabilities
and iterative refinement of advanced systems like the GPT-4 Code Interpreter.
To address this, we introduce OpenCodeInterpreter, a family of open-source code
systems designed for generating, executing, and iteratively refining code.
Supported by Code-Feedback, a dataset featuring 68K multi-turn interactions,
OpenCodeInterpreter integrates execution and human feedback for dynamic code
refinement. Our comprehensive evaluation of OpenCodeInterpreter across key
benchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus
reveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves
an accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and
MBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6)
with synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap
between open-source code generation models and proprietary systems like GPT-4
Code Interpreter.
\\ ( https://arxiv.org/abs/2402.14658 ,  2930kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14683 (*cross-listing*)
Date: Thu, 22 Feb 2024 16:40:33 GMT   (43089kb,D)

Title: Visual Hallucinations of Multi-modal Large Language Models
Authors: Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong
Categories: cs.CV cs.AI cs.LG
\\
  Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines
incorrect details about an image in visual question answering. Existing studies
find VH instances only in existing image datasets, which results in biased
understanding of MLLMs' performance under VH due to limited diversity of such
VH instances. In this work, we propose a tool called VHTest to generate a
diverse set of VH instances. Specifically, VHTest finds some initial VH
instances in existing image datasets (e.g., COCO), generates a text description
for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to
generate VH images based on the text descriptions. We collect a benchmark
dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that
existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a
large fraction of the instances in our benchmark. Moreover, we find that
fine-tuning an MLLM using our benchmark dataset reduces its likelihood to
hallucinate without sacrificing its performance on other benchmarks. Our
benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.
\\ ( https://arxiv.org/abs/2402.14683 ,  43089kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14728 (*cross-listing*)
Date: Thu, 22 Feb 2024 17:35:29 GMT   (35kb)

Title: The European Commitment to Human-Centered Technology: The Integral Role
  of HCI in the EU AI Act's Success
Authors: Andr\'e Calero Valdez, Moreen Heine, Thomas Franke, Nicole Jochems,
  Hans-Christian Jetter, Tim Schrills
Categories: cs.HC cs.AI
Comments: 15 pages, 0 figures
\\
  The evolution of AI is set to profoundly reshape the future. The European
Union, recognizing this impending prominence, has enacted the AI Act,
regulating market access for AI-based systems. A salient feature of the Act is
to guard democratic and humanistic values by focusing regulation on
transparency, explainability, and the human ability to understand and control
AI systems. Hereby, the EU AI Act does not merely specify technological
requirements for AI systems. The EU issues a democratic call for human-centered
AI systems and, in turn, an interdisciplinary research agenda for
human-centered innovation in AI development. Without robust methods to assess
AI systems and their effect on individuals and society, the EU AI Act may lead
to repeating the mistakes of the General Data Protection Regulation of the EU
and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more
confusion than lending guidance. Moreover, determined research activities in
Human-AI interaction will be pivotal for both regulatory compliance and the
advancement of AI in a manner that is both ethical and effective. Such an
approach will ensure that AI development aligns with human values and needs,
fostering a technology landscape that is innovative, responsible, and an
integral part of our society.
\\ ( https://arxiv.org/abs/2402.14728 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14758 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:20:22 GMT   (6977kb,D)

Title: Batch and match: black-box variational inference with a score-based
  divergence
Authors: Diana Cai, Chirag Modi, Loucas Pillaud-Vivien, Charles C. Margossian,
  Robert M. Gower, David M. Blei, Lawrence K. Saul
Categories: stat.ML cs.AI cs.LG stat.CO
Comments: 46 pages, 11 figures
\\
  Most leading implementations of black-box variational inference (BBVI) are
based on optimizing a stochastic evidence lower bound (ELBO). But such
approaches to BBVI often converge slowly due to the high variance of their
gradient estimates. In this work, we propose batch and match (BaM), an
alternative approach to BBVI based on a score-based divergence. Notably, this
score-based divergence can be optimized by a closed-form proximal update for
Gaussian variational families with full covariance matrices. We analyze the
convergence of BaM when the target distribution is Gaussian, and we prove that
in the limit of infinite batch size the variational parameter updates converge
exponentially quickly to the target mean and covariance. We also evaluate the
performance of BaM on Gaussian and non-Gaussian target distributions that arise
from posterior inference in hierarchical and deep generative models. In these
experiments, we find that BaM typically converges in fewer (and sometimes
significantly fewer) gradient evaluations than leading implementations of BBVI
based on ELBO maximization.
\\ ( https://arxiv.org/abs/2402.14758 ,  6977kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14797 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:55:08 GMT   (45131kb,D)

Title: Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video
  Synthesis
Authors: Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina
  Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci,
  Jian Ren, Sergey Tulyakov
Categories: cs.CV cs.AI
\\
  Contemporary models for generating images show remarkable quality and
versatility. Swayed by these advantages, the research community repurposes them
to generate videos. Since video content is highly redundant, we argue that
naively bringing advances of image models to the video generation domain
reduces motion fidelity, visual quality and impairs scalability. In this work,
we build Snap Video, a video-first model that systematically addresses these
challenges. To do that, we first extend the EDM framework to take into account
spatially and temporally redundant pixels and naturally support video
generation. Second, we show that a U-Net - a workhorse behind image generation
- scales poorly when generating videos, requiring significant computational
overhead. Hence, we propose a new transformer-based architecture that trains
3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us
to efficiently train a text-to-video model with billions of parameters for the
first time, reach state-of-the-art results on a number of benchmarks, and
generate videos with substantially higher quality, temporal consistency, and
motion complexity. The user studies showed that our model was favored by a
large margin over the most recent methods. See our website at
https://snap-research.github.io/snapvideo/.
\\ ( https://arxiv.org/abs/2402.14797 ,  45131kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14804 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:56:38 GMT   (19910kb,D)

Title: Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset
Authors: Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, Hongsheng Li
Categories: cs.CV cs.AI cs.CL cs.LG math.HO
\\
  Recent advancements in Large Multimodal Models (LMMs) have shown promising
results in mathematical reasoning within visual contexts, with models
approaching human-level performance on existing benchmarks such as MathVista.
However, we observe significant limitations in the diversity of questions and
breadth of subjects covered by these benchmarks. To address this issue, we
present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of
3,040 high-quality mathematical problems with visual contexts sourced from real
math competitions. Spanning 16 distinct mathematical disciplines and graded
across 5 levels of difficulty, our dataset provides a comprehensive and diverse
set of challenges for evaluating the mathematical reasoning abilities of LMMs.
Through extensive experimentation, we unveil a notable performance gap between
current LMMs and human performance on MATH-V, underscoring the imperative for
further advancements in LMMs. Moreover, our detailed categorization allows for
a thorough error analysis of LMMs, offering valuable insights to guide future
research and development. The project is available at
https://mathvision-cuhk.github.io
\\ ( https://arxiv.org/abs/2402.14804 ,  19910kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14807 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:58:27 GMT   (857kb,D)

Title: A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit
  Tasks in Public Health
Authors: Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj
  Nagaraj, Milind Tambe
Categories: cs.MA cs.AI cs.LG
\\
  Efforts to reduce maternal mortality rate, a key UN Sustainable Development
target (SDG Target 3.1), rely largely on preventative care programs to spread
critical health information to high-risk populations. These programs face two
important challenges: efficiently allocating limited health resources to large
beneficiary populations, and adapting to evolving policy priorities. While
prior works in restless multi-armed bandit (RMAB) demonstrated success in
public health allocation tasks, they lack flexibility to adapt to evolving
policy priorities. Concurrently, Large Language Models (LLMs) have emerged as
adept, automated planners in various domains, including robotic control and
navigation. In this paper, we propose DLM: a Decision Language Model for RMABs.
To enable dynamic fine-tuning of RMAB policies for challenging public health
settings using human-language commands, we propose using LLMs as automated
planners to (1) interpret human policy preference prompts, (2) propose code
reward functions for a multi-agent RL environment for RMABs, and (3) iterate on
the generated reward using feedback from RMAB simulations to effectively adapt
policy outcomes. In collaboration with ARMMAN, an India-based public health
organization promoting preventative care for pregnant mothers, we conduct a
simulation study, showing DLM can dynamically shape policy outcomes using only
human language commands as input.
\\ ( https://arxiv.org/abs/2402.14807 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14810 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:59:21 GMT   (17885kb,D)

Title: GeneOH Diffusion: Towards Generalizable Hand-Object Interaction
  Denoising via Denoising Diffusion
Authors: Xueyi Liu, Li Yi
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Accepted to ICLR 2024. Project website:
  https://meowuu7.github.io/GeneOH-Diffusion/; Huggingface Demo:
  https://huggingface.co/spaces/xymeow7/gene-hoi-denoising; Code:
  https://github.com/Meowuu7/GeneOH-Diffusion
Journal-ref: ICLR 2024
\\
  In this work, we tackle the challenging problem of denoising hand-object
interactions (HOI). Given an erroneous interaction sequence, the objective is
to refine the incorrect hand trajectory to remove interaction artifacts for a
perceptually realistic sequence. This challenge involves intricate interaction
noise, including unnatural hand poses and incorrect hand-object relations,
alongside the necessity for robust generalization to new interactions and
diverse noise patterns. We tackle those challenges through a novel approach,
GeneOH Diffusion, incorporating two key designs: an innovative contact-centric
HOI representation named GeneOH and a new domain-generalizable denoising
scheme. The contact-centric representation GeneOH informatively parameterizes
the HOI process, facilitating enhanced generalization across various HOI
scenarios. The new denoising scheme consists of a canonical denoising model
trained to project noisy data samples from a whitened noise space to a clean
data manifold and a "denoising via diffusion" strategy which can handle input
trajectories with various noise patterns by first diffusing them to align with
the whitened noise space and cleaning via the canonical denoiser. Extensive
experiments on four benchmarks with significant domain variations demonstrate
the superior effectiveness of our method. GeneOH Diffusion also shows promise
for various downstream applications. Project website:
https://meowuu7.github.io/GeneOH-Diffusion/.
\\ ( https://arxiv.org/abs/2402.14810 ,  17885kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14812 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:59:24 GMT   (6465kb,D)

Title: WeakSAM: Segment Anything Meets Weakly-supervised Instance-level
  Recognition
Authors: Lianghui Zhu, Junwei Zhou, Yan Liu, Xin Hao, Wenyu Liu, Xinggang Wang
Categories: cs.CV cs.AI
Comments: Code is available at https://github.com/hustvl/WeakSAM
\\
  Weakly supervised visual recognition using inexact supervision is a critical
yet challenging learning problem. It significantly reduces human labeling costs
and traditionally relies on multi-instance learning and pseudo-labeling. This
paper introduces WeakSAM and solves the weakly-supervised object detection
(WSOD) and segmentation by utilizing the pre-learned world knowledge contained
in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM
addresses two critical limitations in traditional WSOD retraining, i.e., pseudo
ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT
generation and Region of Interest (RoI) drop regularization. It also addresses
the SAM's problems of requiring prompts and category unawareness for automatic
object detection and segmentation. Our results indicate that WeakSAM
significantly surpasses previous state-of-the-art methods in WSOD and WSIS
benchmarks with large margins, i.e. average improvements of 7.4% and 8.5%,
respectively. The code is available at \url{https://github.com/hustvl/WeakSAM}.
\\ ( https://arxiv.org/abs/2402.14812 ,  6465kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14815 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:59:53 GMT   (3603kb)

Title: Demographic Bias of Expert-Level Vision-Language Foundation Models in
  Medical Imaging
Authors: Yuzhe Yang, Yujia Liu, Xin Liu, Avanti Gulhane, Domenico Mastrodicasa,
  Wei Wu, Edward J Wang, Dushyant W Sahani, Shwetak Patel
Categories: cs.CY cs.AI cs.CV cs.LG
Comments: Code and data are available at
  https://github.com/YyzHarry/vlm-fairness
\\
  Advances in artificial intelligence (AI) have achieved expert-level
performance in medical imaging applications. Notably, self-supervised
vision-language foundation models can detect a broad spectrum of pathologies
without relying on explicit training annotations. However, it is crucial to
ensure that these AI models do not mirror or amplify human biases, thereby
disadvantaging historically marginalized groups such as females or Black
patients. The manifestation of such biases could systematically delay essential
medical care for certain patient subgroups. In this study, we investigate the
algorithmic fairness of state-of-the-art vision-language foundation models in
chest X-ray diagnosis across five globally-sourced datasets. Our findings
reveal that compared to board-certified radiologists, these foundation models
consistently underdiagnose marginalized groups, with even higher rates seen in
intersectional subgroups, such as Black female patients. Such demographic
biases present over a wide range of pathologies and demographic attributes.
Further analysis of the model embedding uncovers its significant encoding of
demographic information. Deploying AI systems with these biases in medical
imaging can intensify pre-existing care disparities, posing potential
challenges to equitable healthcare access and raising ethical questions about
their clinical application.
\\ ( https://arxiv.org/abs/2402.14815 ,  3603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13607 (*cross-listing*)
Date: Wed, 21 Feb 2024 08:21:12 GMT   (2054kb,D)

Title: CODIS: Benchmarking Context-Dependent Visual Comprehension for
  Multimodal Large Language Models
Authors: Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li,
  Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong
  Sun, Yang Liu
Categories: cs.CV cs.CL
\\
  Multimodal large language models (MLLMs) have demonstrated promising results
in a variety of tasks that combine vision and language. As these models become
more integral to research and applications, conducting comprehensive
evaluations of their capabilities has grown increasingly important. However,
most existing benchmarks fail to consider that, in certain situations, images
need to be interpreted within a broader context. In this work, we introduce a
new benchmark, named as CODIS, designed to assess the ability of models to use
context provided in free-form text to enhance visual comprehension. Our
findings indicate that MLLMs consistently fall short of human performance on
this benchmark. Further analysis confirms that these models struggle to
effectively extract and utilize contextual information to improve their
understanding of images. This underscores the pressing need to enhance the
ability of MLLMs to comprehend visuals in a context-dependent manner. View our
project website at https://thunlp-mt.github.io/CODIS.
\\ ( https://arxiv.org/abs/2402.13607 ,  2054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14129 (*cross-listing*)
Date: Wed, 21 Feb 2024 20:53:29 GMT   (208kb,D)

Title: Combining Language and Graph Models for Semi-structured Information
  Extraction on the Web
Authors: Zhi Hong, Kyle Chard and Ian Foster
Categories: cs.IR cs.CL
Comments: 7 pages, 2 figures
\\
  Relation extraction is an efficient way of mining the extraordinary wealth of
human knowledge on the Web. Existing methods rely on domain-specific training
data or produce noisy outputs. We focus here on extracting targeted relations
from semi-structured web pages given only a short description of the relation.
We present GraphScholarBERT, an open-domain information extraction method based
on a joint graph and language model structure. GraphScholarBERT can generalize
to previously unseen domains without additional data or training and produces
only clean extraction results matched to the search keyword. Experiments show
that GraphScholarBERT can improve extraction F1 scores by as much as 34.8\%
compared to previous work in a zero-shot domain and zero-shot website setting.
\\ ( https://arxiv.org/abs/2402.14129 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14327 (*cross-listing*)
Date: Thu, 22 Feb 2024 06:47:44 GMT   (1209kb,D)

Title: Subobject-level Image Tokenization
Authors: Delong Chen, Samuel Cahyawijaya, Jianfeng Liu, Baoyuan Wang, Pascale
  Fung
Categories: cs.CV cs.CL
Comments: Work in progress
\\
  Transformer-based vision models typically tokenize images into fixed-size
square patches as input units, which lacks the adaptability to image content
and overlooks the inherent pixel grouping structure. Inspired by the subword
tokenization widely adopted in language models, we propose an image tokenizer
at a subobject level, where the subobjects are represented by semantically
meaningful image segments obtained by segmentation models (e.g., segment
anything models). To implement a learning system based on subobject
tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to
compress subobject segments of varying sizes and shapes into compact embedding
vectors, then fed the subobject embeddings into a large language model for
vision language learning. Empirical results demonstrated that our
subobject-level tokenization significantly facilitates efficient learning of
translating images into object and attribute descriptions compared to the
traditional patch-level tokenization. Codes and models will be open-sourced at
https://github.com/ChenDelong1999/subobjects.
\\ ( https://arxiv.org/abs/2402.14327 ,  1209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14579 (*cross-listing*)
Date: Thu, 8 Feb 2024 13:21:44 GMT   (3355kb,D)

Title: Text Role Classification in Scientific Charts Using Multimodal
  Transformers
Authors: Hye Jin Kim, Nicolas Lell, Ansgar Scherp
Categories: cs.CV cs.CL cs.LG
\\
  Text role classification involves classifying the semantic role of textual
elements within scientific charts. For this task, we propose to finetune two
pretrained multimodal document layout analysis models, LayoutLMv3 and UDOP, on
chart datasets. The transformers utilize the three modalities of text, image,
and layout as input. We further investigate whether data augmentation and
balancing methods help the performance of the models. The models are evaluated
on various chart datasets, and results show that LayoutLMv3 outperforms UDOP in
all experiments. LayoutLMv3 achieves the highest F1-macro score of 82.87 on the
ICPR22 test dataset, beating the best-performing model from the ICPR22
CHART-Infographics challenge. Moreover, the robustness of the models is tested
on a synthetic noisy dataset ICPR22-N. Finally, the generalizability of the
models is evaluated on three chart datasets, CHIME-R, DeGruyter, and EconBiz,
for which we added labels for the text roles. Findings indicate that even in
cases where there is limited training data, transformers can be used with the
help of data augmentation and balancing methods. The source code and datasets
are available on GitHub under
https://github.com/hjkimk/text-role-classification
\\ ( https://arxiv.org/abs/2402.14579 ,  3355kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14590 (*cross-listing*)
Date: Wed, 7 Feb 2024 23:47:02 GMT   (625kb,D)

Title: Scaling Up LLM Reviews for Google Ads Content Moderation
Authors: Wei Qiao, Tushar Dogra, Otilia Stretcu, Yu-Han Lyu, Tiantian Fang,
  Dongjin Kwon, Chun-Ta Lu, Enming Luo, Yuan Wang, Chih-Chun Chia, Ariel
  Fuxman, Fangzhou Wang, Ranjay Krishna, Mehmet Tek
Categories: cs.IR cs.CL cs.LG
DOI: 10.1145/3616855.3635736
\\
  Large language models (LLMs) are powerful tools for content moderation, but
their inference costs and latency make them prohibitive for casual use on large
datasets, such as the Google Ads repository. This study proposes a method for
scaling up LLM reviews for content moderation in Google Ads. First, we use
heuristics to select candidates via filtering and duplicate removal, and create
clusters of ads for which we select one representative ad per cluster. We then
use LLMs to review only the representative ads. Finally, we propagate the LLM
decisions for the representative ads back to their clusters. This method
reduces the number of reviews by more than 3 orders of magnitude while
achieving a 2x recall compared to a baseline non-LLM model. The success of this
approach is a strong function of the representations used in clustering and
label propagation; we found that cross-modal similarity representations yield
better results than uni-modal representations.
\\ ( https://arxiv.org/abs/2402.14590 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13794 (*cross-listing*)
Date: Wed, 21 Feb 2024 13:24:14 GMT   (39kb)

Title: Revisiting Convergence of AdaGrad with Relaxed Assumptions
Authors: Yusu Hong and Junhong Lin
Categories: math.OC cs.LG stat.ML
\\
  In this study, we revisit the convergence of AdaGrad with momentum (covering
AdaGrad as a special case) on non-convex smooth optimization problems. We
consider a general noise model where the noise magnitude is controlled by the
function value gap together with the gradient magnitude. This model encompasses
a broad range of noises including bounded noise, sub-Gaussian noise, affine
variance noise and the expected smoothness, and it has been shown to be more
realistic in many practical applications. Our analysis yields a probabilistic
convergence rate which, under the general noise, could reach at
(\tilde{\mathcal{O}}(1/\sqrt{T})). This rate does not rely on prior knowledge
of problem-parameters and could accelerate to (\tilde{\mathcal{O}}(1/T)) where
(T) denotes the total number iterations, when the noise parameters related to
the function value gap and noise level are sufficiently small. The convergence
rate thus matches the lower rate for stochastic first-order methods over
non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. We
further derive a convergence bound for AdaGrad with mometum, considering the
generalized smoothness where the local smoothness is controlled by a
first-order function of the gradient norm.
\\ ( https://arxiv.org/abs/2402.13794 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14022 (*cross-listing*)
Date: Thu, 1 Feb 2024 13:46:36 GMT   (5245kb,D)

Title: Statistical validation of a deep learning algorithm for dental anomaly
  detection in intraoral radiographs using paired data
Authors: Pieter Van Leemput, Johannes Keustermans, Wouter Mollemans
Categories: eess.IV cs.CV cs.LG stat.AP
Comments: 10 pages, 2 figures, 10 tables
MSC-class: 62P10 (Primary) 62F03, 68U10, 92C55, 94A08 (Secondary)
\\
  This article describes the clinical validation study setup, statistical
analysis and results for a deep learning algorithm which detects dental
anomalies in intraoral radiographic images, more specifically caries, apical
lesions, root canal treatment defects, marginal defects at crown restorations,
periodontal bone loss and calculus. The study compares the detection
performance of dentists using the deep learning algorithm to the prior
performance of these dentists evaluating the images without algorithmic
assistance. Calculating the marginal profit and loss of performance from the
annotated paired image data allows for a quantification of the hypothesized
change in sensitivity and specificity. The statistical significance of these
results is extensively proven using both McNemar's test and the binomial
hypothesis test. The average sensitivity increases from $60.7\%$ to $85.9\%$,
while the average specificity slightly decreases from $94.5\%$ to $92.7\%$. We
prove that the increase of the area under the localization ROC curve (AUC) is
significant (from $0.60$ to $0.86$ on average), while the average AUC is
bounded by the $95\%$ confidence intervals ${[}0.54, 0.65{]}$ and ${[}0.82,
0.90{]}$. When using the deep learning algorithm for diagnostic guidance, the
dentist can be $95\%$ confident that the average true population sensitivity is
bounded by the range $79.6\%$ to $91.9\%$. The proposed paired data setup and
statistical analysis can be used as a blueprint to thoroughly test the effect
of a modality change, like a deep learning based detection and/or segmentation,
on radiographic images.
\\ ( https://arxiv.org/abs/2402.14022 ,  5245kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14031 (*cross-listing*)
Date: Tue, 20 Feb 2024 11:34:19 GMT   (964kb)

Title: Autoencoder with Ordered Variance for Nonlinear Model Identification
Authors: Midhun T. Augustine, Parag Patil, Mani Bhushan, Sharad Bhartiya
Categories: eess.SY cs.LG cs.SY
Comments: 14 pages, 8 figures
\\
  This paper presents a novel autoencoder with ordered variance (AEO) in which
the loss function is modified with a variance regularization term to enforce
order in the latent space. Further, the autoencoder is modified using ResNets,
which results in a ResNet AEO (RAEO). The paper also illustrates the
effectiveness of AEO and RAEO in extracting nonlinear relationships among input
variables in an unsupervised setting.
\\ ( https://arxiv.org/abs/2402.14031 ,  964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14045 (*cross-listing*)
Date: Wed, 21 Feb 2024 13:06:48 GMT   (2169kb,D)

Title: Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical
  Imaging: A Systematic Literature Review and Future Directions
Authors: Sisipho Hamlomo, Marcellin Atemkeng, Yusuf Brima, Chuneeta Nunhokee
  and Jeremy Baxter
Categories: eess.IV cs.CV cs.LG
\\
  The large volume and complexity of medical imaging datasets are bottlenecks
for storage, transmission, and processing. To tackle these challenges, the
application of low-rank matrix approximation (LRMA) and its derivative, local
LRMA (LLRMA) has demonstrated potential.
  This paper conducts a systematic literature review to showcase works applying
LRMA and LLRMA in medical imaging. A detailed analysis of the literature
identifies LRMA and LLRMA methods applied to various imaging modalities. This
paper addresses the challenges and limitations associated with existing LRMA
and LLRMA methods.
  We note a significant shift towards a preference for LLRMA in the medical
imaging field since 2015, demonstrating its potential and effectiveness in
capturing complex structures in medical data compared to LRMA. Acknowledging
the limitations of shallow similarity methods used with LLRMA, we suggest
advanced semantic image segmentation for similarity measure, explaining in
detail how it can measure similar patches and their feasibility.
  We note that LRMA and LLRMA are mainly applied to unstructured medical data,
and we propose extending their application to different medical data types,
including structured and semi-structured. This paper also discusses how LRMA
and LLRMA can be applied to regular data with missing entries and the impact of
inaccuracies in predicting missing values and their effects. We discuss the
impact of patch size and propose the use of random search (RS) to determine the
optimal patch size. To enhance feasibility, a hybrid approach using Bayesian
optimization and RS is proposed, which could improve the application of LRMA
and LLRMA in medical imaging.
\\ ( https://arxiv.org/abs/2402.14045 ,  2169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14102 (*cross-listing*)
Date: Wed, 21 Feb 2024 19:54:25 GMT   (10680kb,D)

Title: Learning dynamic representations of the functional connectome in
  neurobiological networks
Authors: Luciano Dyballa, Samuel Lang, Alexandra Haslund-Gourley, Eviatar
  Yemini, Steven W. Zucker
Categories: q-bio.NC cs.LG cs.SI
Comments: Accepted at ICLR 24
\\
  The static synaptic connectivity of neuronal circuits stands in direct
contrast to the dynamics of their function. As in changing community
interactions, different neurons can participate actively in various
combinations to effect behaviors at different times. We introduce an
unsupervised approach to learn the dynamic affinities between neurons in live,
behaving animals, and to reveal which communities form among neurons at
different times. The inference occurs in two major steps. First, pairwise
non-linear affinities between neuronal traces from brain-wide calcium activity
are organized by non-negative tensor factorization (NTF). Each factor specifies
which groups of neurons are most likely interacting for an inferred interval in
time, and for which animals. Finally, a generative model that allows for
weighted community detection is applied to the functional motifs produced by
NTF to reveal a dynamic functional connectome. Since time codes the different
experimental variables (e.g., application of chemical stimuli), this provides
an atlas of neural motifs active during separate stages of an experiment (e.g.,
stimulus application or spontaneous behaviors). Results from our analysis are
experimentally validated, confirming that our method is able to robustly
predict causal interactions between neurons to generate behavior. Code is
available at https://github.com/dyballa/dynamic-connectomes.
\\ ( https://arxiv.org/abs/2402.14102 ,  10680kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14131 (*cross-listing*)
Date: Wed, 21 Feb 2024 21:10:12 GMT   (8537kb,D)

Title: Random forests for detecting weak signals and extracting physical
  information: a case study of magnetic navigation
Authors: Mohammadamin Moradi, Zheng-Meng Zhai, Aaron Nielsen, Ying-Cheng Lai
Categories: eess.SP cs.LG physics.data-an
Comments: 12 pages, 11 figures
\\
  It was recently demonstrated that two machine-learning architectures,
reservoir computing and time-delayed feed-forward neural networks, can be
exploited for detecting the Earth's anomaly magnetic field immersed in
overwhelming complex signals for magnetic navigation in a GPS-denied
environment. The accuracy of the detected anomaly field corresponds to a
positioning accuracy in the range of 10 to 40 meters. To increase the accuracy
and reduce the uncertainty of weak signal detection as well as to directly
obtain the position information, we exploit the machine-learning model of
random forests that combines the output of multiple decision trees to give
optimal values of the physical quantities of interest. In particular, from
time-series data gathered from the cockpit of a flying airplane during various
maneuvering stages, where strong background complex signals are caused by other
elements of the Earth's magnetic field and the fields produced by the
electronic systems in the cockpit, we demonstrate that the random-forest
algorithm performs remarkably well in detecting the weak anomaly field and in
filtering the position of the aircraft. With the aid of the conventional
inertial navigation system, the positioning error can be reduced to less than
10 meters. We also find that, contrary to the conventional wisdom, the classic
Tolles-Lawson model for calibrating and removing the magnetic field generated
by the body of the aircraft is not necessary and may even be detrimental for
the success of the random-forest method.
\\ ( https://arxiv.org/abs/2402.14131 ,  8537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14136 (*cross-listing*)
Date: Wed, 21 Feb 2024 21:24:57 GMT   (7351kb,D)

Title: GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal
  Sensors
Authors: Ho Lyun Jeong, Ziqi Wang, Colin Samplawski, Jason Wu, Shiwei Fang,
  Lance M. Kaplan, Deepak Ganesan, Benjamin Marlin, Mani Srivastava
Categories: cs.RO cs.LG eess.SP
\\
  Constantly locating moving objects, i.e., geospatial tracking, is essential
for autonomous building infrastructure. Accurate and robust geospatial tracking
often leverages multimodal sensor fusion algorithms, which require large
datasets with time-aligned, synchronized data from various sensor types.
However, such datasets are not readily available. Hence, we propose GDTM, a
nine-hour dataset for multimodal object tracking with distributed multimodal
sensors and reconfigurable sensor node placements. Our dataset enables the
exploration of several research problems, such as optimizing architectures for
processing multimodal data, and investigating models' robustness to adverse
sensing conditions and sensor placement variances. A GitHub repository
containing the code, sample data, and checkpoints of this work is available at
https://github.com/nesl/GDTM.
\\ ( https://arxiv.org/abs/2402.14136 ,  7351kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14145 (*cross-listing*)
Date: Wed, 21 Feb 2024 22:01:10 GMT   (2450kb,D)

Title: Multiply Robust Estimation for Local Distribution Shifts with Multiple
  Domains
Authors: Steven Wilkins-Reeves, Xu Chen, Qi Ma, Christine Agarwal, Aude
  Hofleitner
Categories: stat.ML cs.LG stat.ME
Comments: 9 pages, 4 figures
\\
  Distribution shifts are ubiquitous in real-world machine learning
applications, posing a challenge to the generalization of models trained on one
data distribution to another. We focus on scenarios where data distributions
vary across multiple segments of the entire population and only make local
assumptions about the differences between training and test (deployment)
distributions within each segment. We propose a two-stage multiply robust
estimation method to improve model performance on each individual segment for
tabular data analysis. The method involves fitting a linear combination of the
based models, learned using clusters of training data from multiple segments,
followed by a refinement step for each segment. Our method is designed to be
implemented with commonly used off-the-shelf machine learning models. We
establish theoretical guarantees on the generalization bound of the method on
the test risk. With extensive experiments on synthetic and real datasets, we
demonstrate that the proposed method substantially improves over existing
alternatives in prediction accuracy and robustness on both regression and
classification tasks. We also assess its effectiveness on a user city
prediction dataset from a large technology company.
\\ ( https://arxiv.org/abs/2402.14145 ,  2450kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14148 (*cross-listing*)
Date: Wed, 21 Feb 2024 22:11:01 GMT   (2312kb,D)

Title: Neural Networks and Friction: Slide, Hold, Learn
Authors: Joaquin Garcia-Suarez
Categories: physics.geo-ph cs.LG
Comments: 10 paged, 10 figures, 2 tables
\\
  In this study, it is demonstrated that Recurrent Neural Networks (RNNs),
specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess
the capability to learn the complex dynamics of rate-and-state friction laws
from synthetic data. The data employed for training the network is generated
through the application of traditional rate-and-state friction equations
coupled with the aging law for state evolution. A novel aspect of our approach
is the formulation of a loss function that explicitly accounts for initial
conditions, the direct effect, and the evolution of state variables during
training. It is found that the RNN, with its GRU architecture, effectively
learns to predict changes in the friction coefficient resulting from velocity
jumps, thereby showcasing the potential of machine learning models in
understanding and simulating the physics of frictional processes.
\\ ( https://arxiv.org/abs/2402.14148 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14167 (*cross-listing*)
Date: Wed, 21 Feb 2024 23:08:54 GMT   (22997kb,D)

Title: T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with
  Trajectory Stitching
Authors: Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei
  Xiao, Jianfei Cai, Anima Anandkumar
Categories: cs.CV cs.LG
\\
  Sampling from diffusion probabilistic models (DPMs) is often expensive for
high-quality image generation and typically requires many steps with a large
model. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a
simple yet efficient technique to improve the sampling efficiency with little
or no generation degradation. Instead of solely using a large DPM for the
entire sampling trajectory, T-Stitch first leverages a smaller DPM in the
initial steps as a cheap drop-in replacement of the larger DPM and switches to
the larger DPM at a later stage. Our key insight is that different diffusion
models learn similar encodings under the same training data distribution and
smaller models are capable of generating good global structures in the early
steps. Extensive experiments demonstrate that T-Stitch is training-free,
generally applicable for different architectures, and complements most existing
fast sampling techniques with flexible speed and quality trade-offs. On DiT-XL,
for example, 40% of the early timesteps can be safely replaced with a 10x
faster DiT-S without performance drop on class-conditional ImageNet generation.
We further show that our method can also be used as a drop-in technique to not
only accelerate the popular pretrained stable diffusion (SD) models but also
improve the prompt alignment of stylized SD models from the public model zoo.
Code is released at https://github.com/NVlabs/T-Stitch
\\ ( https://arxiv.org/abs/2402.14167 ,  22997kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14205 (*cross-listing*)
Date: Thu, 22 Feb 2024 01:18:55 GMT   (5351kb,D)

Title: Compression Robust Synthetic Speech Detection Using Patched Spectrogram
  Transformer
Authors: Amit Kumar Singh Yadav, Ziyue Xiang, Kratika Bhagtani, Paolo
  Bestagini, Stefano Tubaro, Edward J. Delp
Categories: cs.SD cs.CV cs.LG eess.AS eess.SP
Comments: Accepted as long oral paper at ICMLA 2023
\\
  Many deep learning synthetic speech generation tools are readily available.
The use of synthetic speech has caused financial fraud, impersonation of
people, and misinformation to spread. For this reason forensic methods that can
detect synthetic speech have been proposed. Existing methods often overfit on
one dataset and their performance reduces substantially in practical scenarios
such as detecting synthetic speech shared on social platforms. In this paper we
propose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a
synthetic speech detector that converts a time domain speech signal to a
mel-spectrogram and processes it in patches using a transformer neural network.
We evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our
experiments show that PS3DT performs well on ASVspoof2019 dataset compared to
other approaches using spectrogram for synthetic speech detection. We also
investigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT
generalizes well than several existing methods on detecting synthetic speech
from an out-of-distribution dataset. We also evaluate robustness of PS3DT to
detect telephone quality synthetic speech and synthetic speech shared on social
platforms (compressed speech). PS3DT is robust to compression and can detect
telephone quality synthetic speech better than several existing methods.
\\ ( https://arxiv.org/abs/2402.14205 ,  5351kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14213 (*cross-listing*)
Date: Thu, 22 Feb 2024 01:42:12 GMT   (12361kb)

Title: Contrastive Learning of Shared Spatiotemporal EEG Representations Across
  Individuals for Naturalistic Neuroscience
Authors: Xinke Shen, Lingyi Tao, Xuyang Chen, Sen Song, Quanying Liu, Dan Zhang
Categories: q-bio.NC cs.LG eess.SP
Comments: 52 pages, 14 figures
\\
  Neural representations induced by naturalistic stimuli offer insights into
how humans respond to peripheral stimuli in daily life. The key to
understanding the general neural mechanisms underlying naturalistic stimuli
processing involves aligning neural activities across individuals and
extracting inter-subject shared neural representations. Targeting the
Electroencephalogram (EEG) technique, known for its rich spatial and temporal
information, this study presents a general framework for Contrastive Learning
of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER).
Harnessing the representational capabilities of contrastive learning, CL-SSTER
utilizes a neural network to maximize the similarity of EEG representations
across individuals for identical stimuli, contrasting with those for varied
stimuli. The network employed spatial and temporal convolutions to
simultaneously learn the spatial and temporal patterns inherent in EEG. The
versatility of CL-SSTER was demonstrated on three EEG datasets, including a
synthetic dataset, a speech audio EEG dataset, and an emotional video EEG
dataset. CL-SSTER attained the highest inter-subject correlation (ISC) values
compared to the state-of-the-art ISC methods. The latent representations
generated by CL-SSTER exhibited reliable spatiotemporal EEG patterns, which can
be explained by specific aspects of the stimuli. CL-SSTER serves as an
interpretable and scalable foundational framework for the identification of
inter-subject shared neural representations in the realm of naturalistic
neuroscience.
\\ ( https://arxiv.org/abs/2402.14213 ,  12361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14229 (*cross-listing*)
Date: Thu, 22 Feb 2024 02:20:24 GMT   (46kb)

Title: Sample-Efficient Linear Regression with Self-Selection Bias
Authors: Jason Gaitonde and Elchanan Mossel
Categories: math.ST cs.DS cs.LG stat.TH
Comments: 40 pages
\\
  We consider the problem of linear regression with self-selection bias in the
unknown-index setting, as introduced in recent work by Cherapanamjeri,
Daskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$
i.i.d. samples $(\mathbf{x}_{\ell},z_{\ell})_{\ell=1}^m$ where
$z_{\ell}=\max_{i\in [k]}\{\mathbf{x}_{\ell}^T\mathbf{w}_i+\eta_{i,\ell}\}$,
but the maximizing index $i_{\ell}$ is unobserved. Here, the
$\mathbf{x}_{\ell}$ are assumed to be $\mathcal{N}(0,I_n)$ and the noise
distribution $\mathbf{\eta}_{\ell}\sim \mathcal{D}$ is centered and independent
of $\mathbf{x}_{\ell}$. We provide a novel and near optimally sample-efficient
(in terms of $k$) algorithm to recover $\mathbf{w}_1,\ldots,\mathbf{w}_k\in
\mathbb{R}^n$ up to additive $\ell_2$-error $\varepsilon$ with polynomial
sample complexity $\tilde{O}(n)\cdot \mathsf{poly}(k,1/\varepsilon)$ and
significantly improved time complexity
$\mathsf{poly}(n,k,1/\varepsilon)+O(\log(k)/\varepsilon)^{O(k)}$. When
$k=O(1)$, our algorithm runs in $\mathsf{poly}(n,1/\varepsilon)$ time,
generalizing the polynomial guarantee of an explicit moment matching algorithm
of Cherapanamjeri, et al. for $k=2$ and when it is known that
$\mathcal{D}=\mathcal{N}(0,I_k)$. Our algorithm succeeds under significantly
relaxed noise assumptions, and therefore also succeeds in the related setting
of max-linear regression where the added noise is taken outside the maximum.
For this problem, our algorithm is efficient in a much larger range of $k$ than
the state-of-the-art due to Ghosh, Pananjady, Guntuboyina, and Ramchandran
[IEEE Trans. Inf. Theory 2022] for not too small $\varepsilon$, and leads to
improved algorithms for any $\varepsilon$ by providing a warm start for
existing local convergence methods.
\\ ( https://arxiv.org/abs/2402.14229 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14264 (*cross-listing*)
Date: Thu, 22 Feb 2024 04:03:32 GMT   (120kb)

Title: Structure-agnostic Optimality of Doubly Robust Learning for Treatment
  Effect Estimation
Authors: Jikai Jin and Vasilis Syrgkanis
Categories: stat.ML cs.LG econ.EM math.ST stat.ME stat.TH
Comments: 30 pages
\\
  Average treatment effect estimation is the most central problem in causal
inference with application to numerous disciplines. While many estimation
strategies have been proposed in the literature, recently also incorporating
generic machine learning estimators, the statistical optimality of these
methods has still remained an open area of investigation. In this paper, we
adopt the recently introduced structure-agnostic framework of statistical lower
bounds, which poses no structural properties on the nuisance functions other
than access to black-box estimators that attain small errors; which is
particularly appealing when one is only willing to consider estimation
strategies that use non-parametric regression and classification oracles as a
black-box sub-process. Within this framework, we prove the statistical
optimality of the celebrated and widely used doubly robust estimators for both
the Average Treatment Effect (ATE) and the Average Treatment Effect on the
Treated (ATTE), as well as weighted variants of the former, which arise in
policy evaluation.
\\ ( https://arxiv.org/abs/2402.14264 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14280 (*cross-listing*)
Date: Thu, 22 Feb 2024 04:41:56 GMT   (2363kb,D)

Title: Secure Navigation using Landmark-based Localization in a GPS-denied
  Environment
Authors: Ganesh Sapkota, Sanjay Madria
Categories: cs.RO cs.CV cs.LG
Comments: 10 pages,12 figures
\\
  In modern battlefield scenarios, the reliance on GPS for navigation can be a
critical vulnerability. Adversaries often employ tactics to deny or deceive GPS
signals, necessitating alternative methods for the localization and navigation
of mobile troops. Range-free localization methods such as DV-HOP rely on
radio-based anchors and their average hop distance which suffers from accuracy
and stability in a dynamic and sparse network topology. Vision-based approaches
like SLAM and Visual Odometry use sensor fusion techniques for map generation
and pose estimation that are more sophisticated and computationally expensive.
This paper proposes a novel framework that integrates landmark-based
localization (LanBLoc) with an Extended Kalman Filter (EKF) to predict the
future state of moving entities along the battlefield. Our framework utilizes
safe trajectory information generated by the troop control center by
considering identifiable landmarks and pre-defined hazard maps. It performs
point inclusion tests on the convex hull of the trajectory segments to ensure
the safety and survivability of a moving entity and determines the next point
forward decisions. We present a simulated battlefield scenario for two
different approaches (with EKF and without EKF) that guide a moving entity
through an obstacle and hazard-free path. Using the proposed method, we
observed a percent error of 6.51% lengthwise in safe trajectory estimation with
an Average Displacement Error (ADE) of 2.97m and a Final Displacement Error
(FDE) of 3.27m. The results demonstrate that our approach not only ensures the
safety of the mobile units by keeping them within the secure trajectory but
also enhances operational effectiveness by adapting to the evolving threat
landscape.
\\ ( https://arxiv.org/abs/2402.14280 ,  2363kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14285 (*cross-listing*)
Date: Thu, 22 Feb 2024 04:55:58 GMT   (1758kb,D)

Title: Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
Authors: Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang,
  Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, Yisong Yue
Categories: cs.SD cs.LG eess.AS
\\
  We study the problem of symbolic music generation (e.g., generating piano
rolls), with a technical focus on non-differentiable rule guidance. Musical
rules are often expressed in symbolic form on note characteristics, such as
note density or chord progression, many of which are non-differentiable which
pose a challenge when using them for guided diffusion. We propose Stochastic
Control Guidance (SCG), a novel guidance method that only requires forward
evaluation of rule functions that can work with pre-trained diffusion models in
a plug-and-play way, thus achieving training-free guidance for
non-differentiable rules for the first time. Additionally, we introduce a
latent diffusion architecture for symbolic music generation with high time
resolution, which can be composed with SCG in a plug-and-play fashion. Compared
to standard strong baselines in symbolic music generation, this framework
demonstrates marked advancements in music quality and rule-based
controllability, outperforming current state-of-the-art generators in a variety
of settings. For detailed demonstrations, please visit our project site:
https://scg-rule-guided-music.github.io/.
\\ ( https://arxiv.org/abs/2402.14285 ,  1758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14301 (*cross-listing*)
Date: Thu, 22 Feb 2024 05:41:24 GMT   (1191kb,D)

Title: GenSERP: Large Language Models for Whole Page Presentation
Authors: Zhenning Zhang, Yunan Zhang, Suyu Ge, Guangwei Weng, Mridu Narang, Xia
  Song, Saurabh Tiwary
Categories: cs.IR cs.LG
\\
  The advent of large language models (LLMs) brings an opportunity to minimize
the effort in search engine result page (SERP) organization. In this paper, we
propose GenSERP, a framework that leverages LLMs with vision in a few-shot
setting to dynamically organize intermediate search results, including
generated chat answers, website snippets, multimedia data, knowledge panels
into a coherent SERP layout based on a user's query. Our approach has three
main stages: (1) An information gathering phase where the LLM continuously
orchestrates API tools to retrieve different types of items, and proposes
candidate layouts based on the retrieved items, until it's confident enough to
generate the final result. (2) An answer generation phase where the LLM
populates the layouts with the retrieved content. In this phase, the LLM
adaptively optimize the ranking of items and UX configurations of the SERP.
Consequently, it assigns a location on the page to each item, along with the UX
display details. (3) A scoring phase where an LLM with vision scores all the
generated SERPs based on how likely it can satisfy the user. It then send the
one with highest score to rendering. GenSERP features two generation paradigms.
First, coarse-to-fine, which allow it to approach optimal layout in a more
manageable way, (2) beam search, which give it a better chance to hit the
optimal solution compared to greedy decoding. Offline experimental results on
real-world data demonstrate how LLMs can contextually organize heterogeneous
search results on-the-fly and provide a promising user experience.
\\ ( https://arxiv.org/abs/2402.14301 ,  1191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14305 (*cross-listing*)
Date: Thu, 22 Feb 2024 05:48:54 GMT   (449kb,D)

Title: Towards Efficient Pareto-optimal Utility-Fairness between Groups in
  Repeated Rankings
Authors: Phuong Dinh Mai, Duc-Trong Le, Tuan-Anh Hoang, Dung D. Le
Categories: cs.IR cs.LG
\\
  In this paper, we tackle the problem of computing a sequence of rankings with
the guarantee of the Pareto-optimal balance between (1) maximizing the utility
of the consumers and (2) minimizing unfairness between producers of the items.
Such a multi-objective optimization problem is typically solved using a
combination of a scalarization method and linear programming on bi-stochastic
matrices, representing the distribution of possible rankings of items. However,
the above-mentioned approach relies on Birkhoff-von Neumann (BvN)
decomposition, of which the computational complexity is $\mathcal{O}(n^5)$ with
$n$ being the number of items, making it impractical for large-scale systems.
To address this drawback, we introduce a novel approach to the above problem by
using the Expohedron - a permutahedron whose points represent all achievable
exposures of items. On the Expohedron, we profile the Pareto curve which
captures the trade-off between group fairness and user utility by identifying a
finite number of Pareto optimal solutions. We further propose an efficient
method by relaxing our optimization problem on the Expohedron's circumscribed
$n$-sphere, which significantly improve the running time. Moreover, the
approximate Pareto curve is asymptotically close to the real Pareto optimal
curve as the number of substantial solutions increases. Our methods are
applicable with different ranking merits that are non-decreasing functions of
item relevance. The effectiveness of our methods are validated through
experiments on both synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2402.14305 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14307 (*cross-listing*)
Date: Thu, 22 Feb 2024 05:52:55 GMT   (6006kb,D)

Title: An FPGA-Based Accelerator Enabling Efficient Support for CNNs with
  Arbitrary Kernel Sizes
Authors: Miaoxin Wang, Xiao Wu, Jun Lin, Zhongfeng Wang
Categories: cs.AR cs.LG
Comments: 5 pages,4 figures. This work has been accepted by 2024 lEEE
  International Symposium on Circuits and Systems (lSCAS 2024) as a regular
  paper
\\
  Convolutional neural networks (CNNs) with large kernels, drawing inspiration
from the key operations of vision transformers (ViTs), have demonstrated
impressive performance in various vision-based applications. To address the
issue of computational efficiency degradation in existing designs for
supporting large-kernel convolutions, an FPGA-based inference accelerator is
proposed for the efficient deployment of CNNs with arbitrary kernel sizes.
Firstly, a Z-flow method is presented to optimize the computing data flow by
maximizing data reuse opportunity. Besides, the proposed design, incorporating
the kernel-segmentation (Kseg) scheme, enables extended support for
large-kernel convolutions, significantly reducing the storage requirements for
overlapped data. Moreover, based on the analysis of typical block structures in
emerging CNNs, vertical-fused (VF) and horizontal-fused (HF) methods are
developed to optimize CNN deployments from both computation and transmission
perspectives. The proposed hardware accelerator, evaluated on Intel Arria 10
FPGA, achieves up to 3.91 times better DSP efficiency than prior art on the
same network. Particularly, it demonstrates efficient support for large-kernel
CNNs, achieving throughputs of 169.68 GOPS and 244.55 GOPS for RepLKNet-31 and
PyConvResNet-50, respectively, both of which are implemented on hardware for
the first time.
\\ ( https://arxiv.org/abs/2402.14307 ,  6006kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14315 (*cross-listing*)
Date: Thu, 22 Feb 2024 06:17:11 GMT   (3074kb,D)

Title: Structure-Based Drug Design via 3D Molecular Generative Pre-training and
  Sampling
Authors: Yuwei Yang, Siqi Ouyang, Xueyu Hu, Meihua Dang, Mingyue Zheng, Hao
  Zhou, Lei Li
Categories: q-bio.BM cs.LG
\\
  Structure-based drug design aims at generating high affinity ligands with
prior knowledge of 3D target structures. Existing methods either use
conditional generative model to learn the distribution of 3D ligands given
target binding sites, or iteratively modify molecules to optimize a
structure-based activity estimator. The former is highly constrained by data
quantity and quality, which leaves optimization-based approaches more promising
in practical scenario. However, existing optimization-based approaches choose
to edit molecules in 2D space, and use molecular docking to estimate the
activity using docking predicted 3D target-ligand complexes. The misalignment
between the action space and the objective hinders the performance of these
models, especially for those employ deep learning for acceleration. In this
work, we propose MolEdit3D to combine 3D molecular generation with optimization
frameworks. We develop a novel 3D graph editing model to generate molecules
using fragments, and pre-train this model on abundant 3D ligands for learning
target-independent properties. Then we employ a target-guided self-learning
strategy to improve target-related properties using self-sampled molecules.
MolEdit3D achieves state-of-the-art performance on majority of the evaluation
metrics, and demonstrate strong capability of capturing both target-dependent
and -independent properties.
\\ ( https://arxiv.org/abs/2402.14315 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14349 (*cross-listing*)
Date: Thu, 22 Feb 2024 07:39:41 GMT   (3369kb)

Title: Uncertainty-driven and Adversarial Calibration Learning for Epicardial
  Adipose Tissue Segmentation
Authors: Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang
  Tang, Qiuyu Wang, Chunquan Li
Categories: eess.IV cs.CV cs.LG
Comments: 13 pages,7 figuers
\\
  Epicardial adipose tissue (EAT) is a type of visceral fat that can secrete
large amounts of adipokines to affect the myocardium and coronary arteries. EAT
volume and density can be used as independent risk markers measurement of
volume by noninvasive magnetic resonance images is the best method of assessing
EAT. However, segmenting EAT is challenging due to the low contrast between EAT
and pericardial effusion and the presence of motion artifacts. we propose a
novel feature latent space multilevel supervision network (SPDNet) with
uncertainty-driven and adversarial calibration learning to enhance segmentation
for more accurate EAT volume estimation. The network first addresses the
blurring of EAT edges due to the medical images in the open medical
environments with low quality or out-of-distribution by modeling the
uncertainty as a Gaussian distribution in the feature latent space, which using
its Bayesian estimation as a regularization constraint to optimize SwinUNETR.
Second, an adversarial training strategy is introduced to calibrate the
segmentation feature map and consider the multi-scale feature differences
between the uncertainty-guided predictive segmentation and the ground truth
segmentation, synthesizing the multi-scale adversarial loss directly improves
the ability to discriminate the similarity between organizations. Experiments
on both the cardiac public MRI dataset (ACDC) and the real-world clinical
cohort EAT dataset show that the proposed network outperforms mainstream
models, validating that uncertainty-driven and adversarial calibration learning
can be used to provide additional information for modeling multi-scale
ambiguities.
\\ ( https://arxiv.org/abs/2402.14349 ,  3369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14396 (*cross-listing*)
Date: Thu, 22 Feb 2024 09:20:54 GMT   (1751kb,D)

Title: Quantum Circuit Optimization with AlphaTensor
Authors: Francisco J. R. Ruiz, Tuomas Laakkonen, Johannes Bausch, Matej Balog,
  Mohammadamin Barekatain, Francisco J. H. Heras, Alexander Novikov, Nathan
  Fitzpatrick, Bernardino Romera-Paredes, John van de Wetering, Alhussein
  Fawzi, Konstantinos Meichanetzidis, Pushmeet Kohli
Categories: quant-ph cs.LG
Comments: 25 pages main paper + 19 pages appendix
\\
  A key challenge in realizing fault-tolerant quantum computers is circuit
optimization. Focusing on the most expensive gates in fault-tolerant quantum
computation (namely, the T gates), we address the problem of T-count
optimization, i.e., minimizing the number of T gates that are needed to
implement a given circuit. To achieve this, we develop AlphaTensor-Quantum, a
method based on deep reinforcement learning that exploits the relationship
between optimizing T-count and tensor decomposition. Unlike existing methods
for T-count optimization, AlphaTensor-Quantum can incorporate domain-specific
knowledge about quantum computation and leverage gadgets, which significantly
reduces the T-count of the optimized circuits. AlphaTensor-Quantum outperforms
the existing methods for T-count optimization on a set of arithmetic benchmarks
(even when compared without making use of gadgets). Remarkably, it discovers an
efficient algorithm akin to Karatsuba's method for multiplication in finite
fields. AlphaTensor-Quantum also finds the best human-designed solutions for
relevant arithmetic computations used in Shor's algorithm and for quantum
chemistry simulation, thus demonstrating it can save hundreds of hours of
research by optimizing relevant quantum circuits in a fully automated way.
\\ ( https://arxiv.org/abs/2402.14396 ,  1751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14397 (*cross-listing*)
Date: Thu, 22 Feb 2024 09:26:16 GMT   (1701kb,D)

Title: Closed-Form Bounds for DP-SGD against Record-level Inference
Authors: Giovanni Cherubin, Boris K\"opf, Andrew Paverd, Shruti Tople, Lukas
  Wutschitz, Santiago Zanella-B\'eguelin
Categories: cs.CR cs.LG
\\
  Machine learning models trained with differentially-private (DP) algorithms
such as DP-SGD enjoy resilience against a wide range of privacy attacks.
Although it is possible to derive bounds for some attacks based solely on an
$(\varepsilon,\delta)$-DP guarantee, meaningful bounds require a small enough
privacy budget (i.e., injecting a large amount of noise), which results in a
large loss in utility. This paper presents a new approach to evaluate the
privacy of machine learning models against specific record-level threats, such
as membership and attribute inference, without the indirection through DP. We
focus on the popular DP-SGD algorithm, and derive simple closed-form bounds.
Our proofs model DP-SGD as an information theoretic channel whose inputs are
the secrets that an attacker wants to infer (e.g., membership of a data record)
and whose outputs are the intermediate model parameters produced by iterative
optimization. We obtain bounds for membership inference that match
state-of-the-art techniques, whilst being orders of magnitude faster to
compute. Additionally, we present a novel data-dependent bound against
attribute inference. Our results provide a direct, interpretable, and practical
way to evaluate the privacy of trained models against specific inference
threats without sacrificing utility.
\\ ( https://arxiv.org/abs/2402.14397 ,  1701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14400 (*cross-listing*)
Date: Thu, 22 Feb 2024 09:34:48 GMT   (357kb,D)

Title: Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks
Authors: Daniel Holmberg, Manu Airaksinen, Viviana Marchi, Andrea Guzzetta,
  Anna Kivi, Leena Haataja, Sampsa Vanhatalo, Teemu Roos
Categories: cs.CV cs.LG eess.IV
Comments: 10 pages, 3 figures. Code repository available via
  https://github.com/deinal/infant-aagcn
MSC-class: 68T06
ACM-class: I.2; I.4; J.3
\\
  Reliable methods for the neurodevelopmental assessment of infants are
essential for early detection of medical issues that may need prompt
interventions. Spontaneous motor activity, or `kinetics', is shown to provide a
powerful surrogate measure of upcoming neurodevelopment. However, its
assessment is by and large qualitative and subjective, focusing on visually
identified, age-specific gestures. Here, we follow an alternative approach,
predicting infants' neurodevelopmental maturation based on data-driven
evaluation of individual motor patterns. We utilize 3D video recordings of
infants processed with pose-estimation to extract spatio-temporal series of
anatomical landmarks, and apply adaptive graph convolutional networks to
predict the actual age. We show that our data-driven approach achieves
improvement over traditional machine learning baselines based on manually
engineered features.
\\ ( https://arxiv.org/abs/2402.14400 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14401 (*cross-listing*)
Date: Thu, 22 Feb 2024 09:39:46 GMT   (9071kb,D)

Title: Diffusion Model Based Visual Compensation Guidance and Visual Difference
  Analysis for No-Reference Image Quality Assessment
Authors: Zhaoyang Wang, Bo Hu, Mingyang Zhang, Jie Li, Leida Li, Maoguo Gong,
  Xinbo Gao
Categories: cs.CV cs.LG eess.IV
\\
  Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA)
methods still suffer from finding a balance between learning feature
information at the pixel level of the image and capturing high-level feature
information and the efficient utilization of the obtained high-level feature
information remains a challenge. As a novel class of state-of-the-art (SOTA)
generative model, the diffusion model exhibits the capability to model
intricate relationships, enabling a comprehensive understanding of images and
possessing a better learning of both high-level and low-level visual features.
In view of these, we pioneer the exploration of the diffusion model into the
domain of NR-IQA. Firstly, we devise a new diffusion restoration network that
leverages the produced enhanced image and noise-containing images,
incorporating nonlinear features obtained during the denoising process of the
diffusion model, as high-level visual information. Secondly, two visual
evaluation branches are designed to comprehensively analyze the obtained
high-level feature information. These include the visual compensation guidance
branch, grounded in the transformer architecture and noise embedding strategy,
and the visual difference analysis branch, built on the ResNet architecture and
the residual transposed attention block. Extensive experiments are conducted on
seven public NR-IQA datasets, and the results demonstrate that the proposed
model outperforms SOTA methods for NR-IQA.
\\ ( https://arxiv.org/abs/2402.14401 ,  9071kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14434 (*cross-listing*)
Date: Thu, 22 Feb 2024 10:26:46 GMT   (568kb,D)

Title: Parallelized Midpoint Randomization for Langevin Monte Carlo
Authors: Lu Yu, Arnak Dalalyana
Categories: math.ST cs.LG math.PR stat.CO stat.TH
Comments: arXiv admin note: text overlap with arXiv:2306.08494
\\
  We explore the sampling problem within the framework where parallel
evaluations of the gradient of the log-density are feasible. Our investigation
focuses on target distributions characterized by smooth and strongly
log-concave densities. We revisit the parallelized randomized midpoint method
and employ proof techniques recently developed for analyzing its purely
sequential version. Leveraging these techniques, we derive upper bounds on the
Wasserstein distance between the sampling and target densities. These bounds
quantify the runtime improvement achieved by utilizing parallel processing
units, which can be considerable.
\\ ( https://arxiv.org/abs/2402.14434 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14446 (*cross-listing*)
Date: Thu, 22 Feb 2024 11:06:07 GMT   (2963kb,D)

Title: Model-Based Reinforcement Learning Control of Reaction-Diffusion
  Problems
Authors: Christina Schenk, Aditya Vasudevan, Maciej Haranczyk, Ignacio Romero
Categories: math.OC cs.LG cs.SY eess.SY math-ph math.DS math.MP
Comments: 23 pages, 12 figures
\\
  Mathematical and computational tools have proven to be reliable in
decision-making processes. In recent times, in particular, machine
learning-based methods are becoming increasingly popular as advanced support
tools. When dealing with control problems, reinforcement learning has been
applied to decision-making in several applications, most notably in games. The
success of these methods in finding solutions to complex problems motivates the
exploration of new areas where they can be employed to overcome current
difficulties. In this paper, we explore the use of automatic control strategies
to initial boundary value problems in thermal and disease transport.
Specifically, in this work, we adapt an existing reinforcement learning
algorithm using a stochastic policy gradient method and we introduce two novel
reward functions to drive the flow of the transported field. The new
model-based framework exploits the interactions between a reaction-diffusion
model and the modified agent. The results show that certain controls can be
implemented successfully in these applications, although model simplifications
had to be assumed.
\\ ( https://arxiv.org/abs/2402.14446 ,  2963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14459 (*cross-listing*)
Date: Thu, 22 Feb 2024 11:35:52 GMT   (8999kb)

Title: Machine Learning Reveals Large-scale Impact of Posidonia Oceanica on
  Mediterranean Sea Water
Authors: Celio Trois, Luciana Didonet Del Fabro, Vladimir A. Baulin
Categories: physics.ao-ph cs.LG
Comments: arXiv admin note: text overlap with arXiv:2204.13587 by other authors
\\
  Posidonia oceanica is a protected endemic seagrass of Mediterranean sea that
fosters biodiversity, stores carbon, releases oxygen, and provides habitat to
numerous sea organisms. Leveraging augmented research, we collected a
comprehensive dataset of 174 features compiled from diverse data sources.
Through machine learning analysis, we discovered the existence of a robust
correlation between the exact location of P. oceanica and water biogeochemical
properties. The model's feature importance, showed that carbon-related
variables as net biomass production and downward surface mass flux of carbon
dioxide have their values altered in the areas with P. oceanica, which in turn
can be used for indirect location of P. oceanica meadows. The study provides
the evidence of the plant's ability to exert a global impact on the environment
and underscores the crucial role of this plant in sea ecosystems, emphasizing
the need for its conservation and management.
\\ ( https://arxiv.org/abs/2402.14459 ,  8999kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14469 (*cross-listing*)
Date: Thu, 22 Feb 2024 11:56:44 GMT   (9433kb,D)

Title: Reimagining Anomalies: What If Anomalies Were Normal?
Authors: Philipp Liznerski, Saurabh Varshneya, Ece Calikus, Sophie Fellenz, and
  Marius Kloft
Categories: cs.CV cs.LG stat.ML
Comments: 30 pages; preprint
\\
  Deep learning-based methods have achieved a breakthrough in image anomaly
detection, but their complexity introduces a considerable challenge to
understanding why an instance is predicted to be anomalous. We introduce a
novel explanation method that generates multiple counterfactual examples for
each anomaly, capturing diverse concepts of anomalousness. A counterfactual
example is a modification of the anomaly that is perceived as normal by the
anomaly detector. The method provides a high-level semantic explanation of the
mechanism that triggered the anomaly detector, allowing users to explore
"what-if scenarios." Qualitative and quantitative analyses across various image
datasets show that the method applied to state-of-the-art anomaly detectors can
achieve high-quality semantic explanations of detectors.
\\ ( https://arxiv.org/abs/2402.14469 ,  9433kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14489 (*cross-listing*)
Date: Thu, 22 Feb 2024 12:27:35 GMT   (3372kb,D)

Title: A Class of Topological Pseudodistances for Fast Comparison of
  Persistence Diagrams
Authors: Rolando Kindelan Nu\~nez, Mircea Petrache, Mauricio Cerda, Nancy
  Hitschfeld
Categories: cs.CG cs.CV cs.LG math.AT
Comments: Accepted for presentation and poster on the 38th Annual AAAI
  Conference on Artificial Intelligence (AAAI24)
MSC-class: 62R40, 55N31, 68T09, 62R07, 68T10
ACM-class: I.2; I.3.5; I.5.1; I.5.2; I.5.3; I.5.4
\\
  Persistence diagrams (PD)s play a central role in topological data analysis,
and are used in an ever increasing variety of applications. The comparison of
PD data requires computing comparison metrics among large sets of PDs, with
metrics which are accurate, theoretically sound, and fast to compute.
Especially for denser multi-dimensional PDs, such comparison metrics are
lacking. While on the one hand, Wasserstein-type distances have high accuracy
and theoretical guarantees, they incur high computational cost. On the other
hand, distances between vectorizations such as Persistence Statistics (PS)s
have lower computational cost, but lack the accuracy guarantees and in general
they are not guaranteed to distinguish PDs (i.e. the two PS vectors of
different PDs may be equal). In this work we introduce a class of
pseudodistances called Extended Topological Pseudodistances (ETD)s, which have
tunable complexity, and can approximate Sliced and classical Wasserstein
distances at the high-complexity extreme, while being computationally lighter
and close to Persistence Statistics at the lower complexity extreme, and thus
allow users to interpolate between the two metrics. We build theoretical
comparisons to show how to fit our new distances at an intermediate level
between persistence vectorizations and Wasserstein distances. We also
experimentally verify that ETDs outperform PSs in terms of accuracy and
outperform Wasserstein and Sliced Wasserstein distances in terms of
computational complexity.
\\ ( https://arxiv.org/abs/2402.14489 ,  3372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14515 (*cross-listing*)
Date: Thu, 22 Feb 2024 13:04:50 GMT   (34kb,D)

Title: Spectral invariance and maximality properties of the frequency spectrum
  of quantum neural networks
Authors: Patrick Holzer, Ivica Turkalj
Categories: quant-ph cs.LG stat.ML
\\
  Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine
Learning due to their close connection to Variational Quantum Circuits, making
them a promising candidate for practical applications on Noisy
Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite
Fourier series, where the set of frequencies is called the frequency spectrum.
We analyse this frequency spectrum and prove, for a large class of models,
various maximality results. Furthermore, we prove that under some mild
conditions there exists a bijection between classes of models with the same
area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the
number of qubits and $L$ the number of layers, which we consequently call
spectral invariance under area-preserving transformations. With this we explain
the symmetry in $R$ and $L$ in the results often observed in the literature and
show that the maximal frequency spectrum depends only on the area $A = RL$ and
not on the individual values of $R$ and $L$. Moreover, we extend existing
results and specify the maximum possible frequency spectrum of a QNN with
arbitrarily many layers as a function of the spectrum of its generators. If the
generators of the QNN can be further decomposed into 2-dimensional
sub-generators, then this specification follows from elementary
number-theoretical considerations. In the case of arbitrary dimensional
generators, we extend existing results based on the so-called Golomb ruler and
introduce a second novel approach based on a variation of the turnpike problem,
which we call the relaxed turnpike problem.
\\ ( https://arxiv.org/abs/2402.14515 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14525 (*cross-listing*)
Date: Thu, 22 Feb 2024 13:19:02 GMT   (8903kb,D)

Title: Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers
Authors: Yasemin G\"oksu, Antonio De Almeida Correia, Vignesh Prasad, Alap
  Kshirsagar, Dorothea Koert, Jan Peters, Georgia Chalvatzaki
Categories: cs.RO cs.HC cs.LG
Comments: Accepted as a Late Breaking Report in The ACM/IEEE International
  Conference on Human Robot Interaction (HRI) 2024
DOI: 10.1145/3610978.3640670
\\
  Bimanual handovers are crucial for transferring large, deformable or delicate
objects. This paper proposes a framework for generating kinematically
constrained human-like bimanual robot motions to ensure seamless and natural
robot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to
reactively generate suitable response trajectories for a robot based on the
observed human partner's motion. The trajectories are adapted with task space
constraints to ensure accurate handovers. Results from a pilot study show that
our approach is perceived as more human--like compared to a baseline Inverse
Kinematics approach.
\\ ( https://arxiv.org/abs/2402.14525 ,  8903kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14576 (*cross-listing*)
Date: Thu, 8 Feb 2024 17:17:46 GMT   (1278kb,D)

Title: Edge Caching Based on Deep Reinforcement Learning and Transfer Learning
Authors: Farnaz Niknia, Ping Wang, Zixu Wang, Aakash Agarwal and Adib S. Rezaei
Categories: cs.NI cs.LG cs.SY eess.SY
\\
  This paper addresses the escalating challenge of redundant data transmission
in networks. The surge in traffic has strained backhaul links and backbone
networks, prompting the exploration of caching solutions at the edge router.
Existing work primarily relies on Markov Decision Processes (MDP) for caching
issues, assuming fixed-time interval decisions; however, real-world scenarios
involve random request arrivals, and despite the critical role of various file
characteristics in determining an optimal caching policy, none of the related
existing work considers all these file characteristics in forming a caching
policy. In this paper, first, we formulate the caching problem using a
semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature
of real-world scenarios allowing for caching decisions at random times upon
file requests. Then, we propose a double deep Q-learning-based caching approach
that comprehensively accounts for file features such as lifetime, size, and
importance. Simulation results demonstrate the superior performance of our
approach compared to a recent Deep Reinforcement Learning-based method.
Furthermore, we extend our work to include a Transfer Learning (TL) approach to
account for changes in file request rates in the SMDP framework. The proposed
TL approach exhibits fast convergence, even in scenarios with increased
differences in request rates between source and target domains, presenting a
promising solution to the dynamic challenges of caching in real-world
environments.
\\ ( https://arxiv.org/abs/2402.14576 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14578 (*cross-listing*)
Date: Thu, 22 Feb 2024 14:33:54 GMT   (1085kb,D)

Title: Multivariate Online Linear Regression for Hierarchical Forecasting
Authors: Massil Hihat, Guillaume Garrigos, Adeline Fermanian, Simon Bussy
Categories: stat.ML cs.LG math.OC
\\
  In this paper, we consider a deterministic online linear regression model
where we allow the responses to be multivariate. To address this problem, we
introduce MultiVAW, a method that extends the well-known Vovk-Azoury-Warmuth
algorithm to the multivariate setting, and show that it also enjoys logarithmic
regret in time. We apply our results to the online hierarchical forecasting
problem and recover an algorithm from this literature as a special case,
allowing us to relax the hypotheses usually made for its analysis.
\\ ( https://arxiv.org/abs/2402.14578 ,  1085kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14589 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:36:19 GMT   (325kb)

Title: Avoiding an AI-imposed Taylor's Version of all music history
Authors: Nick Collins and Mick Grierson
Categories: cs.CY cs.LG cs.SD eess.AS
\\
  As future musical AIs adhere closely to human music, they may form their own
attachments to particular human artists in their databases, and these biases
may in the worst case lead to potential existential threats to all musical
history. AI super fans may act to corrupt the historical record and extant
recordings in favour of their own preferences, and preservation of the
diversity of world music culture may become even more of a pressing issue than
the imposition of 12 tone equal temperament or other Western homogenisations.
We discuss the technical capability of AI cover software and produce Taylor's
Versions of famous tracks from Western pop history as provocative examples; the
quality of these productions does not affect the overall argument (which might
even see a future AI try to impose the sound of paperclips onto all existing
audio files, let alone Taylor Swift). We discuss some potential defenses
against the danger of future musical monopolies, whilst analysing the
feasibility of a maximal 'Taylor Swiftication' of the complete musical record.
\\ ( https://arxiv.org/abs/2402.14589 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14597 (*cross-listing*)
Date: Sun, 4 Feb 2024 11:56:49 GMT   (3437kb)

Title: Learning Style Identification Using Semi-Supervised Self-Taught Labeling
Authors: Hani Y. Ayyoub and Omar S. Al-Kadi
Categories: cs.CY cs.CV cs.LG
Comments: 14 pages, 12 figures, journal paper in IEEE Transactions on Learning
  Technologies
DOI: 10.1109/TLT.2024.3358864
\\
  Education is a dynamic field that must be adaptable to sudden changes and
disruptions caused by events like pandemics, war, and natural disasters related
to climate change. When these events occur, traditional classrooms with
traditional or blended delivery can shift to fully online learning, which
requires an efficient learning environment that meets students' needs. While
learning management systems support teachers' productivity and creativity, they
typically provide the same content to all learners in a course, ignoring their
unique learning styles. To address this issue, we propose a semi-supervised
machine learning approach that detects students' learning styles using a data
mining technique. We use the commonly used Felder Silverman learning style
model and demonstrate that our semi-supervised method can produce reliable
classification models with few labeled data. We evaluate our approach on two
different courses and achieve an accuracy of 88.83% and 77.35%, respectively.
Our work shows that educational data mining and semi-supervised machine
learning techniques can identify different learning styles and create a
personalized learning environment.
\\ ( https://arxiv.org/abs/2402.14597 ,  3437kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14598 (*cross-listing*)
Date: Sun, 4 Feb 2024 09:58:17 GMT   (21648kb,D)

Title: Brain-inspired Distributed Memorization Learning for Efficient
  Feature-free Unsupervised Domain Adaptation
Authors: Jianming Lv, Depin Liang, Zequan Liang, Yaobin Zhang, Sijun Xia
Categories: cs.NE cs.LG
Comments: 15 pages,15 figures
\\
  Compared with gradient based artificial neural networks, biological neural
networks usually show a more powerful generalization ability to quickly adapt
to unknown environments without using any gradient back-propagation procedure.
Inspired by the distributed memory mechanism of human brains, we propose a
novel gradient-free Distributed Memorization Learning mechanism, namely DML, to
support quick domain adaptation of transferred models. In particular, DML
adopts randomly connected neurons to memorize the association of input signals,
which are propagated as impulses, and makes the final decision by associating
the distributed memories based on their confidence. More importantly, DML is
able to perform reinforced memorization based on unlabeled data to quickly
adapt to a new domain without heavy fine-tuning of deep features, which makes
it very suitable for deploying on edge devices. Experiments based on four
cross-domain real-world datasets show that DML can achieve superior performance
of real-time domain adaptation compared with traditional gradient based MLP
with more than 10% improvement of accuracy while reducing 87% of the timing
cost of optimization.
\\ ( https://arxiv.org/abs/2402.14598 ,  21648kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14603 (*cross-listing*)
Date: Fri, 2 Feb 2024 12:57:21 GMT   (6015kb,D)

Title: Balanced Resonate-and-Fire Neurons
Authors: Saya Higuchi, Sebastian Kairat, Sander M. Bohte. Sebastian Otte
Categories: cs.NE cs.LG
\\
  The resonate-and-fire (RF) neuron, introduced over two decades ago, is a
simple, efficient, yet biologically plausible spiking neuron model, which can
extract frequency patterns within the time domain due to its resonating
membrane dynamics. However, previous RF formulations suffer from intrinsic
shortcomings that limit effective learning and prevent exploiting the
principled advantage of RF neurons. Here, we introduce the balanced RF (BRF)
neuron, which alleviates some of the intrinsic limitations of vanilla RF
neurons and demonstrates its effectiveness within recurrent spiking neural
networks (RSNNs) on various sequence learning tasks. We show that networks of
BRF neurons achieve overall higher task performance, produce only a fraction of
the spikes, and require significantly fewer parameters as compared to modern
RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable
training convergence, even when bridging many hundreds of time steps during
backpropagation through time (BPTT). These results underscore that our BRF-RSNN
is a strong candidate for future large-scale RSNN architectures, further lines
of research in SNN methodology, and more efficient hardware implementations.
\\ ( https://arxiv.org/abs/2402.14603 ,  6015kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14684 (*cross-listing*)
Date: Thu, 22 Feb 2024 16:40:55 GMT   (3248kb,D)

Title: Adaptive time series forecasting with markovian variance switching
Authors: Baptiste Ab\'el\`es, Joseph de Vilmarest, Olivier Wintemberger
Categories: stat.ML cs.LG math.PR
\\
  Adaptive time series forecasting is essential for prediction under regime
changes. Several classical methods assume linear Gaussian state space model
(LGSSM) with variances constant in time. However, there are many real-world
processes that cannot be captured by such models. We consider a state-space
model with Markov switching variances. Such dynamical systems are usually
intractable because of their computational complexity increasing exponentially
with time; Variational Bayes (VB) techniques have been applied to this problem.
In this paper, we propose a new way of estimating variances based on online
learning theory; we adapt expert aggregation methods to learn the variances
over time. We apply the proposed method to synthetic data and to the problem of
electricity load forecasting. We show that this method is robust to
misspecification and outperforms traditional expert aggregation.
\\ ( https://arxiv.org/abs/2402.14684 ,  3248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14692 (*cross-listing*)
Date: Thu, 22 Feb 2024 16:47:15 GMT   (626kb,D)

Title: PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a
  Diffusion Probabilistic Model
Authors: Yukiya Hono, Kei Hashimoto, Yoshihiko Nankaku, Keiichi Tokuda
Categories: eess.AS cs.LG cs.SD eess.SP
Comments: 5 pages, 4 figures, To appear in ICASSP 2024. Audio samples:
  https://www.sp.nitech.ac.jp/~hono/demos/icassp2024/
\\
  This paper presents a neural vocoder based on a denoising diffusion
probabilistic model (DDPM) incorporating explicit periodic signals as auxiliary
conditioning signals. Recently, DDPM-based neural vocoders have gained
prominence as non-autoregressive models that can generate high-quality
waveforms. The neural vocoders based on DDPM have the advantage of training
with a simple time-domain loss. In practical applications, such as singing
voice synthesis, there is a demand for neural vocoders to generate
high-fidelity speech waveforms with flexible pitch control. However,
conventional DDPM-based neural vocoders struggle to generate speech waveforms
under such conditions. Our proposed model aims to accurately capture the
periodic structure of speech waveforms by incorporating explicit periodic
signals. Experimental results show that our model improves sound quality and
provides better pitch control than conventional DDPM-based neural vocoders.
\\ ( https://arxiv.org/abs/2402.14692 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14694 (*cross-listing*)
Date: Thu, 22 Feb 2024 16:48:17 GMT   (1069kb,D)

Title: A Quick Introduction to Quantum Machine Learning for Non-Practitioners
Authors: Ethan N. Evans, Dominic Byrne, and Matthew G. Cook
Categories: quant-ph cs.ET cs.LG
Comments: Published as a DTIC report under the title "Quantum Computing for
  Machine Learning - An Introduction". Distribution Statement A: Approved for
  Public Release. Distribution is Unlimited
\\
  This paper provides an introduction to quantum machine learning, exploring
the potential benefits of using quantum computing principles and algorithms
that may improve upon classical machine learning approaches. Quantum computing
utilizes particles governed by quantum mechanics for computational purposes,
leveraging properties like superposition and entanglement for information
representation and manipulation. Quantum machine learning applies these
principles to enhance classical machine learning models, potentially reducing
network size and training time on quantum hardware. The paper covers basic
quantum mechanics principles, including superposition, phase space, and
entanglement, and introduces the concept of quantum gates that exploit these
properties. It also reviews classical deep learning concepts, such as
artificial neural networks, gradient descent, and backpropagation, before
delving into trainable quantum circuits as neural networks. An example problem
demonstrates the potential advantages of quantum neural networks, and the
appendices provide detailed derivations. The paper aims to help researchers new
to quantum mechanics and machine learning develop their expertise more
efficiently.
\\ ( https://arxiv.org/abs/2402.14694 ,  1069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14777 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:37:33 GMT   (6933kb,D)

Title: Causal Imputation for Counterfactual SCMs: Bridging Graphs and Latent
  Factor Models
Authors: Alvaro Ribot, Chandler Squires, Caroline Uhler
Categories: stat.ML cs.LG
Comments: 35 pages, 17 figures
\\
  We consider the task of causal imputation, where we aim to predict the
outcomes of some set of actions across a wide range of possible contexts. As a
running example, we consider predicting how different drugs affect cells from
different cell types. We study the index-only setting, where the actions and
contexts are categorical variables with a finite number of possible values.
Even in this simple setting, a practical challenge arises, since often only a
small subset of possible action-context pairs have been studied. Thus, models
must extrapolate to novel action-context pairs, which can be framed as a form
of matrix completion with rows indexed by actions, columns indexed by contexts,
and matrix entries corresponding to outcomes. We introduce a novel SCM-based
model class, where the outcome is expressed as a counterfactual, actions are
expressed as interventions on an instrumental variable, and contexts are
defined based on the initial state of the system. We show that, under a
linearity assumption, this setup induces a latent factor model over the matrix
of outcomes, with an additional fixed effect term. To perform causal prediction
based on this model class, we introduce simple extension to the Synthetic
Interventions estimator (Agarwal et al., 2020). We evaluate several matrix
completion approaches on the PRISM drug repurposing dataset, showing that our
method outperforms all other considered matrix completion approaches.
\\ ( https://arxiv.org/abs/2402.14777 ,  6933kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14792 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:50:18 GMT   (26762kb,D)

Title: Consolidating Attention Features for Multi-view Image Editing
Authors: Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la
  Torre
Categories: cs.CV cs.GR cs.LG
Comments: Project Page at
  https://qnerf-consolidation.github.io/qnerf-consolidation/
\\
  Large-scale text-to-image models enable a wide range of image editing
techniques, using text prompts or even spatial controls. However, applying
these editing methods to multi-view images depicting a single scene leads to
3D-inconsistent results. In this work, we focus on spatial control-based
geometric manipulations and introduce a method to consolidate the editing
process across various views. We build on two insights: (1) maintaining
consistent features throughout the generative process helps attain consistency
in multi-view editing, and (2) the queries in self-attention layers
significantly influence the image structure. Hence, we propose to improve the
geometric consistency of the edited images by enforcing the consistency of the
queries. To do so, we introduce QNeRF, a neural radiance field trained on the
internal query features of the edited images. Once trained, QNeRF can render
3D-consistent queries, which are then softly injected back into the
self-attention layers during generation, greatly improving multi-view
consistency. We refine the process through a progressive, iterative method that
better consolidates queries across the diffusion timesteps. We compare our
method to a range of existing techniques and demonstrate that it can achieve
better multi-view consistency and higher fidelity to the input scene. These
advantages allow us to train NeRFs with fewer visual artifacts, that are better
aligned with the target geometry.
\\ ( https://arxiv.org/abs/2402.14792 ,  26762kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14817 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:59:56 GMT   (8468kb,D)

Title: Cameras as Rays: Pose Estimation via Ray Diffusion
Authors: Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan,
  Shubham Tulsiani
Categories: cs.CV cs.LG
Comments: To appear in ICLR 2024 (oral). Project webpage:
  https://jasonyzhang.com/RayDiffusion
\\
  Estimating camera poses is a fundamental task for 3D reconstruction and
remains challenging given sparse views (<10). In contrast to existing
approaches that pursue top-down prediction of global parametrizations of camera
extrinsics, we propose a distributed representation of camera pose that treats
a camera as a bundle of rays. This representation allows for a tight coupling
with spatial image features improving pose precision. We observe that this
representation is naturally suited for set-level level transformers and develop
a regression-based approach that maps image patches to corresponding rays. To
capture the inherent uncertainties in sparse-view pose inference, we adapt this
approach to learn a denoising diffusion model which allows us to sample
plausible modes while improving performance. Our proposed methods, both
regression- and diffusion-based, demonstrate state-of-the-art performance on
camera pose estimation on CO3D while generalizing to unseen object categories
and in-the-wild captures.
\\ ( https://arxiv.org/abs/2402.14817 ,  8468kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2003.03546
replaced with revised version Thu, 22 Feb 2024 14:32:28 GMT   (441kb,D)

Title: Adversarial Machine Learning: Bayesian Perspectives
Authors: David Rios Insua, Roi Naveiro, Victor Gallego, Jason Poulos
Categories: cs.AI cs.LG stat.CO stat.ML
Journal-ref: Journal of the American Statistical Association. Volume 118, 2023
  - Issue 543
DOI: 10.1080/01621459.2023.2183129
\\ ( https://arxiv.org/abs/2003.03546 ,  441kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02536
replaced with revised version Wed, 21 Feb 2024 23:23:18 GMT   (2792kb,D)

Title: Finding Alignments Between Interpretable Causal Variables and
  Distributed Neural Representations
Authors: Atticus Geiger and Zhengxuan Wu and Christopher Potts and Thomas Icard
  and Noah D. Goodman
Categories: cs.AI
\\ ( https://arxiv.org/abs/2303.02536 ,  2792kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11625
replaced with revised version Thu, 22 Feb 2024 10:49:01 GMT   (306kb)

Title: Meaningful Causal Aggregation and Paradoxical Confounding
Authors: Yuchen Zhu and Kailash Budhathoki and Jonas Kuebler and Dominik
  Janzing
Categories: cs.AI stat.ML
Comments: CLeaR 2024
\\ ( https://arxiv.org/abs/2304.11625 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07515
replaced with revised version Thu, 22 Feb 2024 08:48:13 GMT   (271kb)

Title: Artificial intelligence is algorithmic mimicry: why artificial "agents"
  are not (and won't be) proper agents
Authors: Johannes Jaeger
Categories: cs.AI
\\ ( https://arxiv.org/abs/2307.07515 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07636
replaced with revised version Thu, 22 Feb 2024 17:47:23 GMT   (980kb,D)

Title: Dissenting Explanations: Leveraging Disagreement to Reduce Model
  Overreliance
Authors: Omer Reingold, Judy Hanwen Shen, Aditi Talati
Categories: cs.AI
Comments: V2: AAAI 2024 V1: AI & HCI Workshop at ICML 2023
MSC-class: 68
ACM-class: I.2
\\ ( https://arxiv.org/abs/2307.07636 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06032
replaced with revised version Thu, 22 Feb 2024 16:21:51 GMT   (415kb,D)

Title: Large Language Models in Cryptocurrency Securities Cases: Can a GPT
  Model Meaningfully Assist Lawyers?
Authors: Arianna Trozze, Toby Davies, and Bennett Kleinberg
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2308.06032 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15198
replaced with revised version Thu, 22 Feb 2024 04:31:26 GMT   (1839kb,D)

Title: Do LLM Agents Exhibit Social Behavior?
Authors: Yan Leng, Yuan Yuan
Categories: cs.AI cs.SI econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2312.15198 ,  1839kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16174
replaced with revised version Thu, 22 Feb 2024 02:28:57 GMT   (3190kb,D)

Title: Industrial Internet of Things Intelligence Empowering Smart
  Manufacturing: A Literature Review
Authors: Yujiao Hu, Qingmin Jia, Yuao Yao, Yong Lee, Mengjie Lee, Chenyi Wang,
  Xiaomao Zhou, Renchao Xie, F. Richard Yu
Categories: cs.AI cs.CY
Comments: Accepted by IoTJ
Journal-ref: IEEE Internet of Things Journal,2024
DOI: 10.1109/JIOT.2024.3367692
\\ ( https://arxiv.org/abs/2312.16174 ,  3190kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08025
replaced with revised version Wed, 21 Feb 2024 22:11:19 GMT   (4590kb,D)

Title: Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using
  Self-Imagination
Authors: Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, Eric Nyberg
Categories: cs.AI cs.CL cs.LG
Comments: 18 pages, 9 figures, 12 tables
\\ ( https://arxiv.org/abs/2401.08025 ,  4590kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02611
replaced with revised version Thu, 22 Feb 2024 14:42:45 GMT   (11800kb,D)

Title: PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial
  Reasoning Problems?
Authors: Chinmay Mittal, Krishna Kartik, Mausam, Parag Singla
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.02611 ,  11800kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05391
replaced with revised version Thu, 22 Feb 2024 10:04:46 GMT   (9617kb,D)

Title: Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
Authors: Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang
  Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, Jiaqi Li, Xiaoze Liu,
  Jeff Z. Pan, Ningyu Zhang, Huajun Chen
Categories: cs.AI cs.CV cs.IR cs.LG
Comments: Ongoing work; 41 pages (Main Text), 55 pages (Total), 11 Tables, 13
  Figures, 617 citations; Paper list is available at
  https://github.com/zjukg/KG-MM-Survey
\\ ( https://arxiv.org/abs/2402.05391 ,  9617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13399
replaced with revised version Thu, 22 Feb 2024 15:46:21 GMT   (4501kb,D)

Title: Learning and Sustaining Shared Normative Systems via Bayesian Rule
  Induction in Markov Games
Authors: Ninell Oldenburg and Tan Zhi-Xuan
Categories: cs.AI
Comments: Accepted to the 23rd International Conference on Autonomous Agents
  and Multi-Agent Systems, 8 pages (excl. references), 6 figures/tables,
  (Appendix: 7 pages, 6 figures/tables). Code available at:
  https://github.com/ninell-oldenburg/social-contracts
ACM-class: I.2.0; I.6.5; G.3
\\ ( https://arxiv.org/abs/2402.13399 ,  4501kb)
------------------------------------------------------------------------------
\\
arXiv:2209.08199
replaced with revised version Thu, 22 Feb 2024 08:07:33 GMT   (11029kb,D)

Title: ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
Authors: Yu-Chung Hsiao, Fedir Zubach, Maria Wang, Jindong Chen
Categories: cs.CL cs.CV cs.HC
\\ ( https://arxiv.org/abs/2209.08199 ,  11029kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00131
replaced with revised version Thu, 22 Feb 2024 18:52:15 GMT   (29091kb,D)

Title: Underspecification in Language Modeling Tasks: A Causality-Informed
  Study of Gendered Pronoun Resolution
Authors: Emily McMilin
Categories: cs.CL cs.AI
Comments: 24 pages, 41 figures
\\ ( https://arxiv.org/abs/2210.00131 ,  29091kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03533
replaced with revised version Thu, 22 Feb 2024 06:21:51 GMT   (105kb,D)

Title: Text Embeddings by Weakly-Supervised Contrastive Pre-training
Authors: Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin
  Jiang, Rangan Majumder, Furu Wei
Categories: cs.CL cs.IR
Comments: 17 pages, v2 fixes the SummEval numbers
\\ ( https://arxiv.org/abs/2212.03533 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2212.08897
replaced with revised version Thu, 22 Feb 2024 13:24:49 GMT   (825kb,D)

Title: PolQA: Polish Question Answering Dataset
Authors: Piotr Rybak, Piotr Przyby{\l}a, Maciej Ogrodniczuk
Categories: cs.CL
\\ ( https://arxiv.org/abs/2212.08897 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12665
replaced with revised version Thu, 22 Feb 2024 15:36:22 GMT   (119kb,D)

Title: Can We Identify Stance Without Target Arguments? A Study for Rumour
  Stance Classification
Authors: Yue Li and Carolina Scarton
Categories: cs.CL
Comments: This paper has been accepted by The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)
\\ ( https://arxiv.org/abs/2303.12665 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00450
replaced with revised version Thu, 22 Feb 2024 10:21:56 GMT   (8011kb,D)

Title: SMILE: Single-turn to Multi-turn Inclusive Language Expansion via
  ChatGPT for Mental Health Support
Authors: Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, Zhenzhong Lan
Categories: cs.CL cs.CY
Comments: 22 pages
\\ ( https://arxiv.org/abs/2305.00450 ,  8011kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01938
replaced with revised version Thu, 22 Feb 2024 12:12:54 GMT   (1961kb,D)

Title: Doc2SoarGraph: Discrete Reasoning over Visually-Rich Table-Text
  Documents via Semantic-Oriented Hierarchical Graphs
Authors: Fengbin Zhu, Chao Wang, Fuli Feng, Zifeng Ren, Moxin Li, Tat-Seng Chua
Categories: cs.CL cs.AI
Comments: Accepted by COLING 2024
\\ ( https://arxiv.org/abs/2305.01938 ,  1961kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13168
replaced with revised version Thu, 22 Feb 2024 10:15:25 GMT   (2633kb,D)

Title: LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities
  and Future Opportunities
Authors: Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao,
  Shumin Deng, Huajun Chen, Ningyu Zhang
Categories: cs.CL cs.AI cs.DB cs.IR cs.LG
Comments: Work in progress
\\ ( https://arxiv.org/abs/2305.13168 ,  2633kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09312
replaced with revised version Thu, 22 Feb 2024 06:17:39 GMT   (2675kb,D)

Title: Multi-Modal Discussion Transformer: Integrating Text, Images and Graph
  Transformers to Detect Hate Speech on Social Media
Authors: Liam Hebert, Gaurav Sahu, Yuxuan Guo, Nanda Kishore Sreenivas, Lukasz
  Golab, Robin Cohen
Categories: cs.CL cs.LG cs.MM cs.SI
Comments: Accepted to AAAI 2024 (AI for Social Impact Track)
\\ ( https://arxiv.org/abs/2307.09312 ,  2675kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08241
replaced with revised version Thu, 22 Feb 2024 02:03:42 GMT   (5607kb,D)

Title: TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for
  Time Series
Authors: Chenxi Sun and Hongyan Li and Yaliang Li and Shenda Hong
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2308.08241 ,  5607kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08493
replaced with revised version Wed, 21 Feb 2024 22:02:26 GMT   (80kb)

Title: Time Travel in LLMs: Tracing Data Contamination in Large Language Models
Authors: Shahriar Golchin, Mihai Surdeanu
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: Published at ICLR 2024 as a Spotlight paper (notable top 5%)
\\ ( https://arxiv.org/abs/2308.08493 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10410
replaced with revised version Thu, 22 Feb 2024 02:54:19 GMT   (1336kb,D)

Title: Large Language Models on Wikipedia-Style Survey Generation: an
  Evaluation in NLP Concepts
Authors: Fan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz
  Blum, Dairui Liu, Tianwei She, Yuang Jiang, Irene Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.10410 ,  1336kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15246
replaced with revised version Thu, 22 Feb 2024 09:27:09 GMT   (8131kb,D)

Title: A Classification-Guided Approach for Adversarial Attacks against Neural
  Machine Translation
Authors: Sahar Sadrizadeh, Ljiljana Dolamic, Pascal Frossard
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.15246 ,  8131kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01669
replaced with revised version Thu, 22 Feb 2024 09:16:47 GMT   (7269kb,D)

Title: Donkii: Can Annotation Error Detection Methods Find Errors in
  Instruction-Tuning Datasets?
Authors: Leon Weber-Genzel and Robert Litschko and Ekaterina Artemova and
  Barbara Plank
Categories: cs.CL
Comments: Camera ready version for LAW-XVIII
\\ ( https://arxiv.org/abs/2309.01669 ,  7269kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02233
replaced with revised version Thu, 22 Feb 2024 16:32:00 GMT   (809kb)

Title: Augmenting Black-box LLMs with Medical Textbooks for Clinical Question
  Answering
Authors: Yubo Wang, Xueguang Ma, Wenhu Chen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2309.02233 ,  809kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03882
replaced with revised version Thu, 22 Feb 2024 01:40:35 GMT   (3738kb,D)

Title: Large Language Models Are Not Robust Multiple Choice Selectors
Authors: Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang
Categories: cs.CL
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2309.03882 ,  3738kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08469
replaced with revised version Thu, 22 Feb 2024 13:26:31 GMT   (34kb,D)

Title: Silver Retriever: Advancing Neural Passage Retrieval for Polish Question
  Answering
Authors: Piotr Rybak, Maciej Ogrodniczuk
Categories: cs.CL cs.IR
\\ ( https://arxiv.org/abs/2309.08469 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13007
replaced with revised version Wed, 21 Feb 2024 23:07:11 GMT   (8057kb,D)

Title: ReConcile: Round-Table Conference Improves Reasoning via Consensus among
  Diverse LLMs
Authors: Justin Chih-Yao Chen, Swarnadeep Saha, Mohit Bansal
Categories: cs.CL cs.AI cs.LG
Comments: 20 pages, 9 figures, 14 tables
\\ ( https://arxiv.org/abs/2309.13007 ,  8057kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00313
replaced with revised version Wed, 21 Feb 2024 19:51:20 GMT   (10617kb,D)

Title: Decoding In-Context Learning: Neuroscience-inspired Analysis of
  Representations in Large Language Models
Authors: Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Rapha\"el
  Milli\`ere, Ida Momennejad
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.00313 ,  10617kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05130
replaced with revised version Thu, 22 Feb 2024 08:31:46 GMT   (586kb,D)

Title: Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text
  via Conditional Probability Curvature
Authors: Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.05130 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10690
replaced with revised version Thu, 22 Feb 2024 06:31:07 GMT   (897kb,D)

Title: Large Language Models for In-Context Student Modeling: Synthesizing
  Student's Behavior in Visual Programming
Authors: Manh Hung Nguyen, Sebastian Tschiatschek, Adish Singla
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.10690 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16183
replaced with revised version Thu, 22 Feb 2024 02:32:06 GMT   (53kb,D)

Title: BLP-2023 Task 2: Sentiment Analysis
Authors: Md. Arid Hasan, Firoj Alam, Anika Anjum, Shudipta Das, Afiyat Anjum
Categories: cs.CL cs.LG
Comments: Accepted in BLP Workshop at EMNLP-23
ACM-class: I.2.7
DOI: 10.18653/v1/2023.banglalp-1.48
\\ ( https://arxiv.org/abs/2310.16183 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19651
replaced with revised version Thu, 22 Feb 2024 13:21:27 GMT   (3751kb,D)

Title: Dynamics of Instruction Tuning: Each Ability of Large Language Models
  Has Its Own Growth Pace
Authors: Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan,
  Yue Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.19651 ,  3751kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04131
replaced with revised version Thu, 22 Feb 2024 17:07:39 GMT   (14307kb,D)

Title: Interpreting Shared Circuits for Ordered Sequence Prediction in a Large
  Language Model
Authors: Michael Lan, Fazl Barez
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.04131 ,  14307kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08369
replaced with revised version Wed, 21 Feb 2024 21:40:00 GMT   (9086kb,D)

Title: How You Prompt Matters! Even Task-Oriented Constraints in Instructions
  Affect LLM-Generated Text Detection
Authors: Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki
Categories: cs.CL
Comments: Under review
\\ ( https://arxiv.org/abs/2311.08369 ,  9086kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09606
replaced with revised version Thu, 22 Feb 2024 05:15:55 GMT   (2594kb,D)

Title: GistScore: Learning Better Representations for In-Context Example
  Selection with Gist Bottlenecks
Authors: Shivanshu Gupta, Clemens Rosenbaum, Ethan R. Elenberg
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09606 ,  2594kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09808
replaced with revised version Thu, 22 Feb 2024 16:59:55 GMT   (2442kb,D)

Title: PixT3: Pixel-based Table To Text generation
Authors: I\~nigo Alonso, Eneko Agirre, Mirella Lapata
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09808 ,  2442kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05930
replaced with revised version Thu, 22 Feb 2024 04:52:35 GMT   (8315kb,D)

Title: SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully
Authors: Jushi Kai, Hai Hu, Zhouhan Lin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.05930 ,  8315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06838
replaced with revised version Thu, 22 Feb 2024 12:58:23 GMT   (2929kb,D)

Title: MAPO: Advancing Multilingual Reasoning through Multilingual
  Alignment-as-Preference Optimization
Authors: Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang
  Geng, Jiajun Chen
Categories: cs.CL
Comments: The project is available at https://github.com/NJUNLP/MAPO
\\ ( https://arxiv.org/abs/2401.06838 ,  2929kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06855
replaced with revised version Wed, 21 Feb 2024 22:20:12 GMT   (9158kb,D)

Title: Fine-grained Hallucination Detection and Editing for Language Models
Authors: Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham
  Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi
Categories: cs.CL
Comments: Our code, data, and demo are available at
  https://fine-grained-hallucination.github.io. Expanded human annotations
  adding a new LM, as well as included more baselines for comparison
\\ ( https://arxiv.org/abs/2401.06855 ,  9158kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06951
replaced with revised version Thu, 22 Feb 2024 12:49:10 GMT   (4433kb,D)

Title: E^2-LLM: Efficient and Extreme Length Extension of Large Language Models
Authors: Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge
  Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie Fu,
  Wenhu Chen, Bo Zheng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.06951 ,  4433kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08190
replaced with revised version Wed, 21 Feb 2024 20:28:13 GMT   (937kb,D)

Title: MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible
  Pipeline
Authors: Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, Kai Fan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.08190 ,  937kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10407
replaced with revised version Thu, 22 Feb 2024 13:46:56 GMT   (445kb,D)

Title: Learning High-Quality and General-Purpose Phrase Representations
Authors: Lihu Chen and Ga\"el Varoquaux and Fabian M. Suchanek
Categories: cs.CL
Comments: Findings of EACL 2024
\\ ( https://arxiv.org/abs/2401.10407 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12713
replaced with revised version Wed, 21 Feb 2024 19:14:23 GMT   (318kb,D)

Title: Generating Zero-shot Abstractive Explanations for Rumour Verification
Authors: Iman Munire Bilal, Preslav Nakov, Rob Procter, Maria Liakata
Categories: cs.CL
Comments: Revised version of the original
\\ ( https://arxiv.org/abs/2401.12713 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12873
replaced with revised version Thu, 22 Feb 2024 03:30:23 GMT   (8352kb,D)

Title: Improving Machine Translation with Human Feedback: An Exploration of
  Quality Estimation as a Reward Model
Authors: Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang,
  Shuming Shi, Zhaopeng Tu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.12873 ,  8352kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12874
replaced with revised version Thu, 22 Feb 2024 04:28:03 GMT   (801kb,D)

Title: From Understanding to Utilization: A Survey on Explainability for Large
  Language Models
Authors: Haoyan Luo, Lucia Specia
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.12874 ,  801kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13223
replaced with revised version Thu, 22 Feb 2024 13:36:56 GMT   (392kb,D)

Title: TAT-LLM: A Specialized Language Model for Discrete Reasoning over
  Tabular and Textual Data
Authors: Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, Tat-Seng Chua
Categories: cs.CL cs.AI
Comments: ACL 2024 (Under Review)
\\ ( https://arxiv.org/abs/2401.13223 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01696
replaced with revised version Thu, 22 Feb 2024 03:16:46 GMT   (2276kb,D)

Title: HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text
  Classification
Authors: Vidit Jain, Mukund Rungta, Yuchen Zhuang, Yue Yu, Zeyu Wang, Mu Gao,
  Jeffrey Skolnick, Chao Zhang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.01696 ,  2276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01719
replaced with revised version Thu, 22 Feb 2024 17:25:45 GMT   (275kb,D)

Title: Measuring Moral Inconsistencies in Large Language Models
Authors: Vamshi Krishna Bonagiri, Sreeram Vennam, Manas Gaur, Ponnurangam
  Kumaraguru
Categories: cs.CL cs.LG
Comments: BlackBoxNLP 2023
\\ ( https://arxiv.org/abs/2402.01719 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03686
replaced with revised version Thu, 22 Feb 2024 04:13:36 GMT   (1333kb,D)

Title: Are Machines Better at Complex Reasoning? Unveiling Human-Machine
  Inference Gaps in Entailment Verification
Authors: Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, Xiang Ren
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.03686 ,  1333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03776
replaced with revised version Thu, 22 Feb 2024 00:43:19 GMT   (49kb)

Title: Large Language Models As MOOCs Graders
Authors: Shahriar Golchin, Nikhil Garuda, Christopher Impey, Matthew Wenger
Categories: cs.CL cs.AI
Comments: v1.2 preprint
\\ ( https://arxiv.org/abs/2402.03776 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03927
replaced with revised version Thu, 22 Feb 2024 12:32:24 GMT   (9055kb,D)

Title: Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in
  Closed-Source LLMs
Authors: Simone Balloccu, Patr\'icia Schmidtov\'a, Mateusz Lango, and
  Ond\v{r}ej Du\v{s}ek
Categories: cs.CL cs.AI
Comments: Accepted at EACL 2024 - main conference
\\ ( https://arxiv.org/abs/2402.03927 ,  9055kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07610
replaced with revised version Thu, 22 Feb 2024 04:53:46 GMT   (3387kb,D)

Title: Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
Authors: Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang,
  Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.07610 ,  3387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09808
replaced with revised version Thu, 22 Feb 2024 15:11:57 GMT   (2575kb,D)

Title: Knowledge of Pretrained Language Models on Surface Information of Tokens
Authors: Tatsuya Hiraoka, Naoaki Okazaki
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.09808 ,  2575kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11005
replaced with revised version Wed, 21 Feb 2024 22:02:18 GMT   (820kb,D)

Title: Exploring Value Biases: How LLMs Deviate Towards the Ideal
Authors: Sarath Sivaprasad, Pramod Kaushik, Sahar Abdelnabi, Mario Fritz
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.11005 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11525
replaced with revised version Thu, 22 Feb 2024 11:46:03 GMT   (320kb,D)

Title: Advancing Translation Preference Modeling with RLHF: A Step Towards
  Cost-Effective Solution
Authors: Nuo Xu, Jun Zhao, Can Zu, Tao Gui, Qi Zhang, Xuanjing Huang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.11525 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11753
replaced with revised version Thu, 22 Feb 2024 18:40:03 GMT   (968kb,D)

Title: ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
Authors: Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar
  Ramasubramanian, Bo Li, Radha Poovendran
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.11753 ,  968kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12011
replaced with revised version Thu, 22 Feb 2024 14:13:38 GMT   (1541kb,D)

Title: A Systematic Comparison of Contextualized Word Embeddings for Lexical
  Semantic Change
Authors: Francesco Periti, Nina Tahmasebi
Categories: cs.CL
Comments: Submitted to NAACL 2024
\\ ( https://arxiv.org/abs/2402.12011 ,  1541kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12052
replaced with revised version Thu, 22 Feb 2024 03:23:55 GMT   (1680kb,D)

Title: Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When
  and What to Retrieve for LLMs
Authors: Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong
  Wen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12052 ,  1680kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13372
replaced with revised version Thu, 22 Feb 2024 18:29:00 GMT   (2474kb,D)

Title: EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human
  Adversaries
Authors: Jing Han Sun and Ali Emami
Categories: cs.CL
Comments: Accepted for publication in main proceedings of LREC-COLING 2024, 16
  pages, 3 figures
\\ ( https://arxiv.org/abs/2402.13372 ,  2474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13463
replaced with revised version Thu, 22 Feb 2024 06:17:06 GMT   (945kb,D)

Title: RefuteBench: Evaluating Refuting Instruction-Following for Large
  Language Models
Authors: Jianhao Yan, Yun Luo, Yue Zhang
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2402.13463 ,  945kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13583
replaced with revised version Thu, 22 Feb 2024 03:06:55 GMT   (8493kb,D)

Title: LongWanjuan: Towards Systematic Measurement for Long Text Quality
Authors: Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu and
  Dahua Lin
Categories: cs.CL
Comments: Update Figures
\\ ( https://arxiv.org/abs/2402.13583 ,  8493kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13605
replaced with revised version Thu, 22 Feb 2024 10:08:57 GMT   (9560kb,D)

Title: KorNAT: LLM Alignment Benchmark for Korean Social Values and Common
  Knowledge
Authors: Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won,
  Hwaran Lee, Edward Choi
Categories: cs.CL
Comments: 35 pages, 7 figures, 16 tables
\\ ( https://arxiv.org/abs/2402.13605 ,  9560kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13718
replaced with revised version Thu, 22 Feb 2024 03:50:24 GMT   (249kb,D)

Title: $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
Authors: Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and
  Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and
  Zhiyuan Liu and Maosong Sun
Categories: cs.CL
Journal-ref: 2023.12.15ARR
\\ ( https://arxiv.org/abs/2402.13718 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13764
replaced with revised version Thu, 22 Feb 2024 02:39:02 GMT   (3183kb,D)

Title: CriticBench: Evaluating Large Language Models as Critic
Authors: Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen,
  Xian-ling Mao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.13764 ,  3183kb)
------------------------------------------------------------------------------
\\
arXiv:2110.08021
replaced with revised version Wed, 21 Feb 2024 21:48:55 GMT   (2842kb,D)

Title: StreaMulT: Streaming Multimodal Transformer for Heterogeneous and
  Arbitrary Long Sequential Data
Authors: Victor Pellegrain (1 and 2), Myriam Tami (2), Michel Batteux (1),
  C\'eline Hudelot (2) ((1) Institut de Recherche Technologique SystemX, (2)
  Universit\'e Paris-Saclay, CentraleSup\'elec, MICS)
Categories: cs.LG cs.CL cs.MM
Comments: 11 pages, 6 figures, 3 tables
\\ ( https://arxiv.org/abs/2110.08021 ,  2842kb)
------------------------------------------------------------------------------
\\
arXiv:2111.02164
replaced with revised version Thu, 22 Feb 2024 08:05:10 GMT   (1401kb,D)

Title: Data structure > labels? Unsupervised heuristics for SVM hyperparameter
  estimation
Authors: Micha{\l} Cholewa, Micha{\l} Romaszewski, Przemys{\l}aw G{\l}omb
Categories: cs.LG
MSC-class: 68T05
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2111.02164 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2202.07082
replaced with revised version Thu, 22 Feb 2024 04:38:25 GMT   (7098kb,D)

Title: Graph Neural Networks for Graphs with Heterophily: A Survey
Authors: Xin Zheng, Yi Wang, Yixin Liu, Ming Li, Miao Zhang, Di Jin, Philip S.
  Yu, Shirui Pan
Categories: cs.LG
Comments: 22 pages
\\ ( https://arxiv.org/abs/2202.07082 ,  7098kb)
------------------------------------------------------------------------------
\\
arXiv:2205.05396
replaced with revised version Wed, 21 Feb 2024 22:25:01 GMT   (5572kb,D)

Title: A Survey on Fairness for Machine Learning on Graphs
Authors: Charlotte Laclau and Christine Largeron and Manvi Choudhary
Categories: cs.LG
Comments: 25 pages
\\ ( https://arxiv.org/abs/2205.05396 ,  5572kb)
------------------------------------------------------------------------------
\\
arXiv:2211.12620
replaced with revised version Thu, 22 Feb 2024 02:47:53 GMT   (13643kb,D)

Title: Promises and Pitfalls of Threshold-based Auto-labeling
Authors: Harit Vishwakarma, Heguang Lin, Frederic Sala, Ramya Korlakai Vinayak
Categories: cs.LG cs.AI stat.ML
Comments: NeurIPS 2023 (Spotlight)
Journal-ref: Thirty Seventh Conference on Neural Information Processing Systems
  (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2211.12620 ,  13643kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09656
replaced with revised version Thu, 22 Feb 2024 09:55:03 GMT   (3413kb,D)

Title: Credal Bayesian Deep Learning
Authors: Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav
  Ivanov, Oleg Sokolsky, Insup Lee
Categories: cs.LG stat.ML
MSC-class: Primary: 68T37, Secondary: 68T05, 68W25
\\ ( https://arxiv.org/abs/2302.09656 ,  3413kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04040
replaced with revised version Thu, 22 Feb 2024 17:53:27 GMT   (8075kb,D)

Title: Uncertainty Quantification of Spatiotemporal Travel Demand with
  Probabilistic Graph Neural Networks
Authors: Qingyi Wang, Shenhao Wang, Dingyi Zhuang, Haris Koutsopoulos, Jinhua
  Zhao
Categories: cs.LG stat.AP stat.ML
\\ ( https://arxiv.org/abs/2303.04040 ,  8075kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04204
replaced with revised version Thu, 22 Feb 2024 17:46:54 GMT   (49359kb,D)

Title: Deep hybrid model with satellite imagery: how to combine demand modeling
  and computer vision for behavior analysis?
Authors: Qingyi Wang, Shenhao Wang, Yunhan Zheng, Hongzhou Lin, Xiaohu Zhang,
  Jinhua Zhao, Joan Walker
Categories: cs.LG cs.CV econ.GN q-fin.EC
Journal-ref: Transportation Research Part B: Methodological, Volume 179, 2024,
  102869
DOI: 10.1016/j.trb.2023.102869
\\ ( https://arxiv.org/abs/2303.04204 ,  49359kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11291
replaced with revised version Thu, 22 Feb 2024 16:48:50 GMT   (7002kb,D)

Title: Mobiprox: Supporting Dynamic Approximate Computing on Mobiles
Authors: Matev\v{z} Fabjan\v{c}i\v{c}, Octavian Machidon, Hashim Sharif, Yifan
  Zhao, Sa\v{s}a Misailovi\'c, Veljko Pejovi\'c
Categories: cs.LG cs.SY eess.SY
Comments: 14 pages, 8 figures. IEEE Internet of Things Journal (2024)
DOI: 10.1109/JIOT.2024.3365957
\\ ( https://arxiv.org/abs/2303.11291 ,  7002kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10031
replaced with revised version Wed, 21 Feb 2024 23:27:08 GMT   (9284kb,D)

Title: Architectures of Topological Deep Learning: A Survey of Message-Passing
  Topological Neural Networks
Authors: Mathilde Papillon, Sophia Sanborn, Mustafa Hajij, Nina Miolane
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.10031 ,  9284kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02299
replaced with revised version Wed, 21 Feb 2024 23:31:49 GMT   (2442kb,D)

Title: Dynamic Sparse Training with Structured Sparsity
Authors: Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, Yani Ioannou
Categories: cs.LG cs.CV
Comments: ICLR 2024, 29 pages, 22 figures
\\ ( https://arxiv.org/abs/2305.02299 ,  2442kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16202
replaced with revised version Thu, 22 Feb 2024 14:59:51 GMT   (9746kb,D)

Title: DP-SGD Without Clipping: The Lipschitz Neural Network Way
Authors: Louis Bethune, Thomas Massena, Thibaut Boissin, Yannick Prudent,
  Corentin Friedrich, Franck Mamalet, Aurelien Bellet, Mathieu Serrurier, David
  Vigouroux
Categories: cs.LG cs.CR
Comments: 46 pages, published at International Conferences on Learning
  Representations (ICLR), 2024
\\ ( https://arxiv.org/abs/2305.16202 ,  9746kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03552
replaced with revised version Thu, 22 Feb 2024 03:24:16 GMT   (2300kb,D)

Title: State Regularized Policy Optimization on Data with Dynamics Shift
Authors: Zhenghai Xue, Qingpeng Cai, Shuchang Liu, Dong Zheng, Peng Jiang, Kun
  Gai, Bo An
Categories: cs.LG cs.AI
Comments: Accepted at NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.03552 ,  2300kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05587
replaced with revised version Wed, 21 Feb 2024 22:52:08 GMT   (338kb,D)

Title: MC-NN: An End-to-End Multi-Channel Neural Network Approach for
  Predicting Influenza A Virus Hosts and Antigenic Types
Authors: Yanhua Xu and Dominik Wojtczak
Categories: cs.LG q-bio.QM
Comments: Accepted version submitted to the SN Computer Science; Published in
  the SN Computer Science 2023; V2: minor updates were made to the Results
  section; V3: minor updates regarding data description; V4: correct the time
  stamps mentioned in the legends of Figures 1 and 2
DOI: 10.1007/s42979-023-01839-5
\\ ( https://arxiv.org/abs/2306.05587 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05843
replaced with revised version Wed, 21 Feb 2024 22:07:52 GMT   (3036kb,D)

Title: Adaptive Batch Sizes for Active Learning A Probabilistic Numerics
  Approach
Authors: Masaki Adachi, Satoshi Hayakawa, Martin J{\o}rgensen, Xingchen Wan, Vu
  Nguyen, Harald Oberhauser, Michael A. Osborne
Categories: cs.LG cs.AI cs.NA math.NA stat.CO stat.ML
Comments: Accepted at AISTATS 2024. 33 pages, 6 figures
MSC-class: 62C10, 62F15
\\ ( https://arxiv.org/abs/2306.05843 ,  3036kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06268
replaced with revised version Thu, 22 Feb 2024 11:17:54 GMT   (1412kb)

Title: Attention-stacked Generative Adversarial Network (AS-GAN)-empowered
  Sensor Data Augmentation for Online Monitoring of Manufacturing System
Authors: Yuxuan Li, Chenang Liu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.06268 ,  1412kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09222
replaced with revised version Thu, 22 Feb 2024 09:03:12 GMT   (566kb,D)

Title: Stochastic Re-weighted Gradient Descent via Distributionally Robust
  Optimization
Authors: Ramnath Kumar and Kushal Majmundar and Dheeraj Nagaraj and Arun Sai
  Suggala
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.09222 ,  566kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04901
replaced with revised version Thu, 22 Feb 2024 15:30:42 GMT   (2023kb,D)

Title: Towards true discovery of the differential equations
Authors: Alexander Hvatov and Roman Titov
Categories: cs.LG
Comments: Knowledge and Logical Reasoning in the Era of Data-driven Learning
  workshop at ICML 2023
\\ ( https://arxiv.org/abs/2308.04901 ,  2023kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16898
replaced with revised version Thu, 22 Feb 2024 18:38:14 GMT   (1081kb,D)

Title: Transformers as Support Vector Machines
Authors: Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, Samet
  Oymak
Categories: cs.LG cs.AI cs.CL math.OC
Comments: The proof of global convergence for gradient descent in the equal
  score setting has been fixed, referring to Theorem 2 of [TLZO23], and the
  experimental results have been extended
\\ ( https://arxiv.org/abs/2308.16898 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16044
replaced with revised version Thu, 22 Feb 2024 05:09:56 GMT   (36kb)

Title: Improving Adaptive Online Learning Using Refined Discretization
Authors: Zhiyu Zhang, Heng Yang, Ashok Cutkosky, Ioannis Ch. Paschalidis
Categories: cs.LG stat.ML
Comments: ALT 2024
\\ ( https://arxiv.org/abs/2309.16044 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09358
replaced with revised version Thu, 22 Feb 2024 13:43:06 GMT   (8906kb,D)

Title: Bad Values but Good Behavior: Learning Highly Misspecified Bandits and
  MDPs
Authors: Debangshu Banerjee and Aditya Gopalan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.09358 ,  8906kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18285
replaced with revised version Thu, 22 Feb 2024 00:56:55 GMT   (11568kb,D)

Title: Unlocking the Potential of Prompt-Tuning in Bridging Generalized and
  Personalized Federated Learning
Authors: Wenlong Deng, Christos Thrampoulidis, Xiaoxiao Li
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2310.18285 ,  11568kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19253
replaced with revised version Thu, 22 Feb 2024 00:17:59 GMT   (2189kb,D)

Title: Flow-based Distributionally Robust Optimization
Authors: Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, Yao Xie
Categories: cs.LG stat.ME stat.ML
Comments: IEEE Journal on Selected Areas in Information Theory (JSAIT).
  Accepted. 2024
\\ ( https://arxiv.org/abs/2310.19253 ,  2189kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19608
replaced with revised version Thu, 22 Feb 2024 08:21:39 GMT   (2283kb,D)

Title: On Feynman--Kac training of partial Bayesian neural networks
Authors: Zheng Zhao and Sebastian Mair and Thomas B. Sch\"on and Jens Sj\"olund
Categories: cs.LG stat.ML
Comments: In AISTATS 2024
\\ ( https://arxiv.org/abs/2310.19608 ,  2283kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03233
replaced with revised version Wed, 21 Feb 2024 19:50:49 GMT   (12848kb,D)

Title: Navigating Scaling Laws: Compute Optimality in Adaptive Model Training
Authors: Sotiris Anagnostidis, Gregor Bachmann, Imanol Schlag, Thomas Hofmann
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2311.03233 ,  12848kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06973
replaced with revised version Thu, 22 Feb 2024 16:33:10 GMT   (672kb)

Title: Analytical Verification of Performance of Deep Neural Network Based
  Time-Synchronized Distribution System State Estimation
Authors: Behrouz Azimian, Shiva Moshtagh, Anamitra Pal, Shanshan Ma
Categories: cs.LG cs.SY eess.SY
Comments: 8 pages, in Journal of Modern Power Systems and Clean Energy, 2023
DOI: 10.35833/MPCE.2023.000432
\\ ( https://arxiv.org/abs/2311.06973 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10590
replaced with revised version Thu, 22 Feb 2024 13:05:50 GMT   (437kb,D)

Title: EduGym: An Environment and Notebook Suite for Reinforcement Learning
  Education
Authors: Thomas M. Moerland, Matthias M\"uller-Brockhausen, Zhao Yang, Andrius
  Bernatavicius, Koen Ponse, Tom Kouwenhoven, Andreas Sauter, Michiel van der
  Meer, Bram Renting, Aske Plaat
Categories: cs.LG cs.AI cs.CY stat.ML
\\ ( https://arxiv.org/abs/2311.10590 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04985
replaced with revised version Thu, 22 Feb 2024 16:07:47 GMT   (3810kb,D)

Title: SparQ Attention: Bandwidth-Efficient LLM Inference
Authors: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
  Luschi, Douglas Orr
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.04985 ,  3810kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08656
replaced with revised version Thu, 22 Feb 2024 05:35:52 GMT   (3403kb,D)

Title: MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural
  Networks Training
Authors: Hongwu Peng, Xi Xie, Kaustubh Shivdikar, MD Amit Hasan, Jiahui Zhao,
  Shaoyi Huang, Omer Khan, David Kaeli, Caiwen Ding
Categories: cs.LG cs.AI cs.DC
Comments: ASPLOS 2024 accepted publication
ACM-class: I.2; C.5
\\ ( https://arxiv.org/abs/2312.08656 ,  3403kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09187
replaced with revised version Thu, 22 Feb 2024 01:44:52 GMT   (940kb,D)

Title: Vision-Language Models as a Source of Rewards
Authors: Kate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Gheorghe
  Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian Holsheimer, Dan
  Horgan, Michael Laskin, Clare Lyle, Hussain Masoom, Kay McKinney, Volodymyr
  Mnih, Alexander Neitz, Fabio Pardo, Jack Parker-Holder, John Quan, Tim
  Rockt\"aschel, Himanshu Sahni, Tom Schaul, Yannick Schroecker, Stephen
  Spencer, Richie Steigerwald, Luyu Wang, Lei Zhang
Categories: cs.LG
Comments: 10 pages, 5 figures
\\ ( https://arxiv.org/abs/2312.09187 ,  940kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09236
replaced with revised version Thu, 22 Feb 2024 15:03:39 GMT   (1191kb,D)

Title: A framework for conditional diffusion modelling with applications in
  motif scaffolding for protein design
Authors: Kieran Didi, Francisco Vargas, Simon V Mathis, Vincent Dutordoir,
  Emile Mathieu, Urszula J Komorowska, Pietro Lio
Categories: cs.LG q-bio.BM
Comments: 9 pages
\\ ( https://arxiv.org/abs/2312.09236 ,  1191kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05240
replaced with revised version Wed, 21 Feb 2024 20:56:39 GMT   (218kb,D)

Title: Decoupling Decision-Making in Fraud Prevention through Classifier
  Calibration for Business Logic Action
Authors: Emanuele Luzio and Moacir Antonelli Ponti and Christian Ramirez
  Arevalo and Luis Argerich
Categories: cs.LG
Journal-ref: Long version of the paper of ACM-SAC 2024
\\ ( https://arxiv.org/abs/2401.05240 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02439
replaced with revised version Thu, 22 Feb 2024 00:05:12 GMT   (4549kb,D)

Title: DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based
  Trajectory Stitching
Authors: Guanghe Li, Yixiang Shan, Zhengbang Zhu, Ting Long, Weinan Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.02439 ,  4549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10991
replaced with revised version Thu, 22 Feb 2024 03:38:03 GMT   (379kb)

Title: Accelerating Semi-Asynchronous Federated Learning
Authors: Changxin Xu, Yuxin Qiao, Zhanxin Zhou, Fanghao Ni, and Jize Xiong
Categories: cs.LG cs.AI
Comments: 5 pages, 1 figures
\\ ( https://arxiv.org/abs/2402.10991 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11773
replaced with revised version Thu, 22 Feb 2024 01:17:29 GMT   (4678kb,D)

Title: Dynamic Multi-Network Mining of Tensor Time Series
Authors: Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai
Categories: cs.LG cs.AI cs.IT math.IT
Comments: Accepted by WWW 2024
\\ ( https://arxiv.org/abs/2402.11773 ,  4678kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12424
replaced with revised version Thu, 22 Feb 2024 15:34:50 GMT   (6911kb,D)

Title: Tables as Images? Exploring the Strengths and Limitations of LLMs on
  Multimodal Representations of Tabular Data
Authors: Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma,
  Yue Zhang, Rada Mihalcea
Categories: cs.LG cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2402.12424 ,  6911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13388
replaced with revised version Thu, 22 Feb 2024 05:32:51 GMT   (89kb,D)

Title: Transformer tricks: Precomputing the first layer
Authors: Nils Graef
Categories: cs.LG
Comments: 5 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.13388 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13711
replaced with revised version Thu, 22 Feb 2024 09:42:21 GMT   (1321kb,D)

Title: DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based
  Graph Continual Learning
Authors: Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim,
  Chanyoung Park
Categories: cs.LG cs.AI
Comments: Accepted at ACM TheWebConf 2024 (WWW 2024)
DOI: 10.1145/3589334.3645561
\\ ( https://arxiv.org/abs/2402.13711 ,  1321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13777
replaced with revised version Thu, 22 Feb 2024 03:18:46 GMT   (361kb,D)

Title: Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
  and Perspectives on Future Directions
Authors: Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet
  Aggarwal
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.13777 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2107.10969
replaced with revised version Thu, 22 Feb 2024 15:36:44 GMT   (13724kb,D)

Title: Learning Quadruped Locomotion Policies using Logical Rules
Authors: David DeFazio, Yohei Hayamizu, and Shiqi Zhang
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2107.10969 ,  13724kb)
------------------------------------------------------------------------------
\\
arXiv:2109.10241 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 13:35:10 GMT   (380kb,D)

Title: Life in a random universe: Sciama's argument reconsidered
Authors: Zhi-Wei Wang and Samuel L. Braunstein
Categories: physics.hist-ph astro-ph.CO cs.AI gr-qc physics.data-an
Comments: 9 pages, 4 figures, pulished on The Astrophysical Journal
Journal-ref: ApJ 962 55 (2024)
DOI: 10.3847/1538-4357/ad1994
\\ ( https://arxiv.org/abs/2109.10241 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2210.13664
replaced with revised version Thu, 22 Feb 2024 17:01:36 GMT   (1698kb,D)

Title: Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher
  Mixture Model
Authors: Jean-R\'emy Conti, Nathan Noiry, Vincent Despiegel, St\'ephane
  Gentric, St\'ephan Cl\'emen\c{c}on
Categories: cs.CV cs.AI
Comments: Accepted to ICML 2022
Journal-ref: Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:4344-4369, 2022
\\ ( https://arxiv.org/abs/2210.13664 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12906 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 15:55:13 GMT   (1768kb,D)

Title: Generative Invertible Quantum Neural Networks
Authors: Armand Rousselot and Michael Spannowsky
Categories: hep-ph cs.AI cs.LG quant-ph
Comments: 18 pages, 7 figures Changes in v2: Add references 49-51, provided
  gitlab link to code repository Changes in v3: Incorporate rebuttal from
  https://scipost.org/submissions/2302.12906v2/
Report-no: IPPP/23/10
\\ ( https://arxiv.org/abs/2302.12906 ,  1768kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16133
replaced with revised version Wed, 21 Feb 2024 22:53:33 GMT   (2294kb,D)

Title: Exposing and Addressing Cross-Task Inconsistency in Unified
  Vision-Language Models
Authors: Adyasha Maharana, Amita Kamath, Christopher Clark, Mohit Bansal,
  Aniruddha Kembhavi
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: TMLR 2024; Project Website: https://adymaharana.github.io/cococon/
\\ ( https://arxiv.org/abs/2303.16133 ,  2294kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07999
replaced with revised version Thu, 22 Feb 2024 14:27:15 GMT   (0kb,I)

Title: Everyone Can Be Picasso? A Computational Framework into the Myth of
  Human versus AI Painting
Authors: Yilin Ye, Rong Huang, Kang Zhang, Wei Zeng
Categories: cs.HC cs.AI cs.CV
Comments: The results in Figure 3 in Section 4 have error due to my mistakes in
  feature calculation. Particularly the error is in the classification accuracy
ACM-class: I.2.0; J.5; H.5.2
\\ ( https://arxiv.org/abs/2304.07999 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05962
replaced with revised version Wed, 21 Feb 2024 23:17:41 GMT   (215kb,D)

Title: Decentralised Governance-Driven Architecture for Designing Foundation
  Model based Systems: Exploring the Role of Blockchain in Responsible AI
Authors: Yue Liu, Qinghua Lu, Liming Zhu, Hye-Young Paik
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2308.05962 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04474
replaced with revised version Thu, 22 Feb 2024 09:53:02 GMT   (1389kb,D)

Title: Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning
Authors: Yinger Zhang, Hui Cai, Xeirui Song, Yicheng Chen, Rui Sun, Jing Zheng
Categories: cs.SE cs.AI cs.PL
\\ ( https://arxiv.org/abs/2310.04474 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05231
replaced with revised version Thu, 22 Feb 2024 05:54:18 GMT   (3146kb,D)

Title: MindfulDiary: Harnessing Large Language Model to Support Psychiatric
  Patients' Journaling
Authors: Taewan Kim, Seolyeong Bae, Hyun Ah Kim, Su-woo Lee, Hwajung Hong,
  Chanmo Yang, Young-Ho Kim
Categories: cs.HC cs.AI cs.CL
Comments: 20 pages, 6 figures, 4 tables. Accepted at ACM CHI 2024
ACM-class: H.5.2; I.2.7
Journal-ref: In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA
DOI: 10.1145/3613904.3642937
\\ ( https://arxiv.org/abs/2310.05231 ,  3146kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18297
replaced with revised version Thu, 22 Feb 2024 04:04:19 GMT   (21615kb,D)

Title: Image Clustering Conditioned on Text Criteria
Authors: Sehyun Kwon, Jaeseung Park, Minkyu Kim, Jaewoong Cho, Ernest K. Ryu,
  Kangwook Lee
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.18297 ,  21615kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18574
replaced with revised version Thu, 22 Feb 2024 04:14:37 GMT   (1999kb,D)

Title: Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable
  Machine Unlearning
Authors: Zheyuan Liu, Guangyao Dou, Yijun Tian, Chunhui Zhang, Eli Chien, Ziwei
  Zhu
Categories: cs.CR cs.AI cs.LG
Comments: Accepted by The Web Conference (WWW) 2024
DOI: 10.1145/3589334.3645669
\\ ( https://arxiv.org/abs/2310.18574 ,  1999kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05106
replaced with revised version Thu, 22 Feb 2024 09:13:35 GMT   (28329kb,D)

Title: A differentiable brain simulator bridging brain simulation and
  brain-inspired computing
Authors: Chaoming Wang, Tianqiu Zhang, Sichao He, Hongyaoxing Gu, Shangyang Li,
  Si Wu
Categories: cs.NE cs.AI q-bio.NC
Comments: 23 pages, 11 figures, ICLR 2024
\\ ( https://arxiv.org/abs/2311.05106 ,  28329kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06607
replaced with revised version Thu, 22 Feb 2024 06:23:26 GMT   (11735kb,D)

Title: Monkey: Image Resolution and Text Label Are Important Things for Large
  Multi-modal Models
Authors: Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang,
  Yabo Sun, Yuliang Liu, Xiang Bai
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.06607 ,  11735kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03631
replaced with revised version Wed, 21 Feb 2024 15:04:45 GMT   (9073kb,D)

Title: Mitigating Open-Vocabulary Caption Hallucinations
Authors: Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar
  Averbuch-Elor
Categories: cs.CV cs.AI
Comments: Website Link: https://assafbk.github.io/mocha/
\\ ( https://arxiv.org/abs/2312.03631 ,  9073kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05928
replaced with revised version Thu, 22 Feb 2024 18:57:44 GMT   (19461kb,D)

Title: AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer
Authors: Joonwoo Kwon, Sooyoung Kim, Yuewei Lin, Shinjae Yoo, Jiook Cha
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.05928 ,  19461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01330
replaced with revised version Thu, 22 Feb 2024 17:29:03 GMT   (245kb,D)

Title: TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview
Authors: Mohammad Aliannejadi and Zahra Abbasiantaeb and Shubham Chatterjee and
  Jeffery Dalton and Leif Azzopardi
Categories: cs.IR cs.AI cs.CL
Comments: TREC iKAT 2023 Overview Paper
\\ ( https://arxiv.org/abs/2401.01330 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03374
replaced with revised version Thu, 22 Feb 2024 00:29:37 GMT   (3493kb,D)

Title: LLM-Powered Code Vulnerability Repair with Reinforcement Learning and
  Semantic Reward
Authors: Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Mohammad Bahrami
  Karkevandi, Gonzalo De La Torre Parra, Elias Bou-Harb, Peyman Najafirad
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2401.03374 ,  3493kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12999 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 02:56:06 GMT   (264kb,D)

Title: Quantum-Inspired Machine Learning for Molecular Docking
Authors: Runqiu Shu, Bowen Liu, Zhaoping Xiong, Xiaopeng Cui, Yunting Li, Wei
  Cui, Man-Hong Yung and Nan Qiao
Categories: physics.chem-ph cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.12999 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17010
replaced with revised version Thu, 22 Feb 2024 10:09:39 GMT   (77kb)

Title: Finetuning Large Language Models for Vulnerability Detection
Authors: Alexey Shestov, Rodion Levichev, Ravil Mussabayev, Anton Cheshkov
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.17010 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09664
replaced with revised version Wed, 21 Feb 2024 20:23:08 GMT   (2354kb,D)

Title: CodeMind: A Framework to Challenge Large Language Models for Code
  Reasoning
Authors: Changshu Liu, Shizhuo Dylan Zhang, Reyhaneh Jabbarvand
Categories: cs.SE cs.AI cs.CL cs.PL
\\ ( https://arxiv.org/abs/2402.09664 ,  2354kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09721
replaced with revised version Thu, 22 Feb 2024 05:48:02 GMT   (70kb)

Title: Persuading a Learning Agent
Authors: Tao Lin, Yiling Chen
Categories: cs.GT cs.AI cs.LG econ.TH
\\ ( https://arxiv.org/abs/2402.09721 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10002
replaced with revised version Thu, 22 Feb 2024 07:42:24 GMT   (4186kb,D)

Title: MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D
  Point Cloud Understanding
Authors: Hai-Tao Yu, Mofei Song
Categories: cs.CV cs.AI cs.MM
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2402.10002 ,  4186kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10251 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 12:32:53 GMT   (5572kb,D)

Title: Brant-2: Foundation Model for Brain Signals
Authors: Zhizhang Yuan, Daoze Zhang, Junru Chen, Geifei Gu, Yang Yang
Categories: q-bio.NC cs.AI cs.LG eess.SP
Comments: 14 pages, 7 figures
\\ ( https://arxiv.org/abs/2402.10251 ,  5572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13349
replaced with revised version Thu, 22 Feb 2024 03:37:36 GMT   (45556kb,D)

Title: Aria Everyday Activities Dataset
Authors: Zhaoyang Lv, Nicholas Charron, Pierre Moulon, Alexander Gamino, Cheng
  Peng, Chris Sweeney, Edward Miller, Huixuan Tang, Jeff Meissner, Jing Dong,
  Kiran Somasundaram, Luis Pesqueira, Mark Schwesinger, Omkar Parkhi, Qiao Gu,
  Renzo De Nardi, Shangyi Cheng, Steve Saarinen, Vijay Baiyya, Yuyang Zou,
  Richard Newcombe, Jakob Julian Engel, Xiaqing Pan, Carl Ren
Categories: cs.CV cs.AI cs.HC
Comments: Dataset website: https://www.projectaria.com/datasets/aea/
\\ ( https://arxiv.org/abs/2402.13349 ,  45556kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13352 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 08:34:37 GMT   (1210kb,D)

Title: KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
Authors: Boran Apak, Medina Bandic, Aritra Sarkar and Sebastian Feld
Categories: quant-ph cs.AI cs.ET cs.LG
\\ ( https://arxiv.org/abs/2402.13352 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09546 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 15:10:06 GMT   (4037kb,D)

Title: Training dynamic models using early exits for automatic speech
  recognition on resource-constrained devices
Authors: George August Wright, Umberto Cappellazzo, Salah Zaiem, Desh Raj,
  Lucas Ondel Yang, Daniele Falavigna, Mohamed Nabih Ali, Alessio Brutti
Categories: eess.AS cs.CL cs.SD
Comments: Accepted at the ICASSP Workshop Self-supervision in Audio, Speech and
  Beyond 2024
\\ ( https://arxiv.org/abs/2309.09546 ,  4037kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16292
replaced with revised version Thu, 22 Feb 2024 03:24:26 GMT   (6751kb,D)

Title: DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large
  Language Models
Authors: Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min
  Dou, Botian Shi, Liang He, Yu Qiao
Categories: cs.RO cs.CL
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2309.16292 ,  6751kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02330
replaced with revised version Thu, 22 Feb 2024 07:12:44 GMT   (673kb,D)

Title: LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model
Authors: Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang
Categories: cs.CV cs.CL
Comments: The datasets were incomplete as they did not include all the
  necessary copyrights
\\ ( https://arxiv.org/abs/2401.02330 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12997
replaced with revised version Thu, 22 Feb 2024 11:00:16 GMT   (267kb,D)

Title: Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism
Authors: Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe,
  C\'eline Hudelot, Pierre Colombo
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2402.12997 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13528
replaced with revised version Thu, 22 Feb 2024 02:50:41 GMT   (3824kb,D)

Title: Infrastructure Ombudsman: Mining Future Failure Concerns from Structural
  Disaster Response
Authors: Md Towhidul Absar Chowdhury, Soumyajit Datta, Naveen Sharma, Ashiqur
  R. KhudaBukhsh
Categories: cs.CY cs.CL cs.LG cs.SI
DOI: 10.1145/3589334.3648153
\\ ( https://arxiv.org/abs/2402.13528 ,  3824kb)
------------------------------------------------------------------------------
\\
arXiv:2201.10936
replaced with revised version Thu, 22 Feb 2024 10:34:18 GMT   (1133kb,D)

Title: FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control
Authors: Dimitri von R\"utte, Luca Biggio, Yannic Kilcher, Thomas Hofmann
Categories: cs.SD cs.LG eess.AS stat.ML
Comments: Published in ICLR 2023
\\ ( https://arxiv.org/abs/2201.10936 ,  1133kb)
------------------------------------------------------------------------------
\\
arXiv:2202.05560 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 16:28:19 GMT   (719kb,D)

Title: Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound
Authors: Reuben Adams and John Shawe-Taylor and Benjamin Guedj
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 31 pages
\\ ( https://arxiv.org/abs/2202.05560 ,  719kb)
------------------------------------------------------------------------------
\\
arXiv:2203.15402 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 17:37:31 GMT   (4251kb,D)

Title: Physics-informed deep-learning applications to experimental fluid
  mechanics
Authors: Hamidreza Eivazi, Yuning Wang and Ricardo Vinuesa
Categories: physics.flu-dyn cs.LG
Comments: 33 pages, 15 figures
\\ ( https://arxiv.org/abs/2203.15402 ,  4251kb)
------------------------------------------------------------------------------
\\
arXiv:2205.05795 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 14:55:01 GMT   (3450kb,D)

Title: Algebraic Machine Learning with an Application to Chemistry
Authors: Ezzeddine El Sai, Parker Gara, Markus J. Pflaum
Categories: math.AG cs.CG cs.LG math-ph math.MP
Comments: minor revision with expanded section on preliminaries from real
  algebraic geometry and a revised section on related results, to appear in
  "Foundations of Data Science"
MSC-class: 14Q30, 62R01, 68T05, 62R07, 92E10
\\ ( https://arxiv.org/abs/2205.05795 ,  3450kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03719
replaced with revised version Thu, 22 Feb 2024 05:43:12 GMT   (1132kb,D)

Title: Persuading a Behavioral Agent: Approximately Best Responding and
  Learning
Authors: Yiling Chen, Tao Lin
Categories: cs.GT cs.LG econ.TH
Comments: The main results in this draft have been subsumed by our later paper
  "Persuading a Learning Agent"
\\ ( https://arxiv.org/abs/2302.03719 ,  1132kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06594
replaced with revised version Thu, 22 Feb 2024 05:58:36 GMT   (3631kb,D)

Title: V2Meow: Meowing to the Visual Beat via Video-to-Music Generation
Authors: Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Joonseok Lee,
  Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti, Timo I. Denk
Categories: cs.SD cs.CV cs.LG cs.MM eess.AS
Comments: accepted at AAAI 2024, music samples available at
  https://tinyurl.com/v2meow
\\ ( https://arxiv.org/abs/2305.06594 ,  3631kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14961 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 08:17:17 GMT   (2407kb,D)

Title: Deep Learning for Survival Analysis: A Review
Authors: Simon Wiegrebe, Philipp Kopper, Raphael Sonabend, Bernd Bischl, and
  Andreas Bender
Categories: stat.ML cs.LG
Comments: 29 pages, 7 figures, 2 tables, 1 interactive table
Journal-ref: Artif Intell Rev 57, 65 (2024)
DOI: 10.1007/s10462-023-10681-3
\\ ( https://arxiv.org/abs/2305.14961 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06210
replaced with revised version Thu, 22 Feb 2024 12:13:08 GMT   (3253kb,D)

Title: Single-Model Attribution of Generative Models Through Final-Layer
  Inversion
Authors: Mike Laszkiewicz, Jonas Ricker, Johannes Lederer, Asja Fischer
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.06210 ,  3253kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14642 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 23:03:37 GMT   (13054kb)

Title: Linear Convergence of Black-Box Variational Inference: Should We Stick
  the Landing?
Authors: Kyurae Kim, Yian Ma, and Jacob R. Gardner
Categories: stat.ML cs.LG stat.CO
Comments: Accepted to AISTATS'24
\\ ( https://arxiv.org/abs/2307.14642 ,  13054kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10547 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 02:51:06 GMT   (257kb,D)

Title: Decentralized Riemannian Conjugate Gradient Method on the Stiefel
  Manifold
Authors: Jun Chen, Haishan Ye, Mengmeng Wang, Tianxin Huang, Guang Dai, Ivor
  W.Tsang, Yong Liu
Categories: math.OC cs.LG cs.SY eess.SY
Journal-ref: International Conference on Learning Representations, 2024
\\ ( https://arxiv.org/abs/2308.10547 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05092 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 01:33:11 GMT   (765kb,D)

Title: Adaptive conformal classification with noisy labels
Authors: Matteo Sesia, Y. X. Rachel Wang, Xin Tong
Categories: stat.ME cs.LG math.ST stat.TH
Comments: 28 pages (127 pages including references and appendices)
\\ ( https://arxiv.org/abs/2309.05092 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03032
replaced with revised version Thu, 22 Feb 2024 04:15:36 GMT   (6553kb,D)

Title: Graph-enhanced Optimizers for Structure-aware Recommendation Embedding
  Evolution
Authors: Cong Xu, Jun Wang, Jianyong Wang, Wei Zhang
Categories: cs.IR cs.LG
Comments: 19 pages
\\ ( https://arxiv.org/abs/2310.03032 ,  6553kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14763 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 10:17:00 GMT   (6766kb,D)

Title: Externally Valid Policy Evaluation Combining Trial and Observational
  Data
Authors: Sofia Ek, Dave Zachariah
Categories: stat.ME cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.14763 ,  6766kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10879 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 10:46:16 GMT   (7443kb,D)

Title: Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour
  Segmentation
Authors: Richard Osuala, Smriti Joshi, Apostolia Tsirikoglou, Lidia Garrucho,
  Walter H. L. Pinaya, Oliver Diaz, and Karim Lekadir
Categories: eess.IV cs.CV cs.LG
Comments: Accepted as oral presentation at SPIE Medical Imaging 2024 (Image
  Processing)
\\ ( https://arxiv.org/abs/2311.10879 ,  7443kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00824
replaced with revised version Thu, 22 Feb 2024 06:32:31 GMT   (2030kb,D)

Title: Variational Self-Supervised Contrastive Learning Using Beta Divergence
  For Face Understanding
Authors: Mehmet Can Yavuz and Berrin Yanikoglu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.00824 ,  2030kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15591
replaced with revised version Thu, 22 Feb 2024 08:11:30 GMT   (429kb,D)

Title: Privacy-Preserving Neural Graph Databases
Authors: Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song
Categories: cs.DB cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.15591 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06362
replaced with revised version Thu, 22 Feb 2024 04:15:45 GMT   (9565kb,D)

Title: Attention, Distillation, and Tabularization: Towards Practical Neural
  Network-Based Prefetching
Authors: Pengmiao Zhang, Neelesh Gupta, Rajgopal Kannan, Viktor K. Prasanna
Categories: cs.NE cs.AR cs.LG cs.OS
\\ ( https://arxiv.org/abs/2401.06362 ,  9565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03367
replaced with revised version Wed, 21 Feb 2024 21:19:39 GMT   (250kb,D)

Title: RAG-Fusion: a New Take on Retrieval-Augmented Generation
Authors: Zackary Rackauckas
Categories: cs.IR cs.LG
Comments: 8 pages, 2 figures, 8 pages
\\ ( https://arxiv.org/abs/2402.03367 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10797
replaced with revised version Thu, 22 Feb 2024 10:58:50 GMT   (22kb,D)

Title: BlackJAX: Composable Bayesian inference in JAX
Authors: Alberto Cabezas, Adrien Corenflos, Junpeng Lao, R\'emi Louf, Antoine
  Carnec, Kaustubh Chaudhari, Reuben Cohn-Gordon, Jeremie Coullon, Wei Deng,
  Sam Duffield, Gerardo Dur\'an-Mart\'in, Marcin Elantkowski, Dan
  Foreman-Mackey, Michele Gregori, Carlos Iguaran, Ravin Kumar, Martin Lysy,
  Kevin Murphy, Juan Camilo Orduz, Karm Patel, Xi Wang, Rob Zinkov
Categories: cs.MS cs.LG stat.CO stat.ML
Comments: Companion paper for the library
  https://github.com/blackjax-devs/blackjax Update: minor changes and updated
  the list of authors to include technical contributors
\\ ( https://arxiv.org/abs/2402.10797 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12762 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 08:32:24 GMT   (12kb)

Title: Learning under Singularity: An Information Criterion improving WBIC and
  sBIC
Authors: Lirui Liu and Joe Suzuki
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.12762 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13651
replaced with revised version Thu, 22 Feb 2024 07:22:51 GMT   (1164kb,D)

Title: Robustness of Deep Neural Networks for Micro-Doppler Radar
  Classification
Authors: Mikolaj Czerkawski and Carmine Clemente and Craig Michie and Christos
  Tachtatzis
Categories: cs.CV cs.LG eess.SP
Journal-ref: International Radar Symposium 2022
DOI: 10.23919/IRS54158.2022.9905017
\\ ( https://arxiv.org/abs/2402.13651 ,  1164kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
