Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月21日 18:04
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 19 Feb 24 19:00:00 GMT  to  Tue 20 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.12381
Date: Mon, 15 Jan 2024 09:51:19 GMT   (2543kb,D)

Title: Constrained Multi-objective Optimization with Deep Reinforcement
  Learning Assisted Operator Selection
Authors: Fei Ming and Wenyin Gong and Ling Wang and Yaochu Jin
Categories: cs.AI cs.NE
\\
  Solving constrained multi-objective optimization problems with evolutionary
algorithms has attracted considerable attention. Various constrained
multi-objective optimization evolutionary algorithms (CMOEAs) have been
developed with the use of different algorithmic strategies, evolutionary
operators, and constraint-handling techniques. The performance of CMOEAs may be
heavily dependent on the operators used, however, it is usually difficult to
select suitable operators for the problem at hand. Hence, improving operator
selection is promising and necessary for CMOEAs. This work proposes an online
operator selection framework assisted by Deep Reinforcement Learning. The
dynamics of the population, including convergence, diversity, and feasibility,
are regarded as the state; the candidate operators are considered as actions;
and the improvement of the population state is treated as the reward. By using
a Q-Network to learn a policy to estimate the Q-values of all actions, the
proposed approach can adaptively select an operator that maximizes the
improvement of the population according to the current state and thereby
improve the algorithmic performance. The framework is embedded into four
popular CMOEAs and assessed on 42 benchmark problems. The experimental results
reveal that the proposed Deep Reinforcement Learning-assisted operator
selection significantly improves the performance of these CMOEAs and the
resulting algorithm obtains better versatility compared to nine
state-of-the-art CMOEAs.
\\ ( https://arxiv.org/abs/2402.12381 ,  2543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12422
Date: Mon, 19 Feb 2024 13:53:10 GMT   (30kb)

Title: Simulacra as Conscious Exotica
Authors: Murray Shanahan
Categories: cs.AI
\\
  The advent of conversational agents with increasingly human-like behaviour
throws old philosophical questions into new light. Does it, or could it, ever
make sense to speak of AI agents built out of generative language models in
terms of consciousness, given that they are "mere" simulacra of human
behaviour, and that what they do can be seen as "merely" role play? Drawing on
the later writings of Wittgenstein, this paper attempts to tackle this question
while avoiding the pitfalls of dualistic thinking.
\\ ( https://arxiv.org/abs/2402.12422 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12608
Date: Tue, 20 Feb 2024 00:07:55 GMT   (2396kb,D)

Title: Patient-Centric Knowledge Graphs: A Survey of Current Methods,
  Challenges, and Applications
Authors: Hassan S. Al Khatib, Subash Neupane, Harish Kumar Manchukonda,
  Noorbakhsh Amiri Golilarz, Sudip Mittal, Amin Amirlatifi, Shahram Rahimi
Categories: cs.AI
\\
  Patient-Centric Knowledge Graphs (PCKGs) represent an important shift in
healthcare that focuses on individualized patient care by mapping the patient's
health information in a holistic and multi-dimensional way. PCKGs integrate
various types of health data to provide healthcare professionals with a
comprehensive understanding of a patient's health, enabling more personalized
and effective care. This literature review explores the methodologies,
challenges, and opportunities associated with PCKGs, focusing on their role in
integrating disparate healthcare data and enhancing patient care through a
unified health perspective. In addition, this review also discusses the
complexities of PCKG development, including ontology design, data integration
techniques, knowledge extraction, and structured representation of knowledge.
It highlights advanced techniques such as reasoning, semantic search, and
inference mechanisms essential in constructing and evaluating PCKGs for
actionable healthcare insights. We further explore the practical applications
of PCKGs in personalized medicine, emphasizing their significance in improving
disease prediction and formulating effective treatment plans. Overall, this
review provides a foundational perspective on the current state-of-the-art and
best practices of PCKGs, guiding future research and applications in this
dynamic field.
\\ ( https://arxiv.org/abs/2402.12608 ,  2396kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12685
Date: Tue, 20 Feb 2024 03:20:37 GMT   (673kb,D)

Title: XRL-Bench: A Benchmark for Evaluating and Comparing Explainable
  Reinforcement Learning Techniques
Authors: Yu Xiong, Zhipeng Hu, Ye Huang, Runze Wu, Kai Guan, Xingchen Fang, Ji
  Jiang, Tianze Zhou, Yujing Hu, Haoyu Liu, Tangjie Lyu, Changjie Fan
Categories: cs.AI
Comments: 10 pages, 5 figures
\\
  Reinforcement Learning (RL) has demonstrated substantial potential across
diverse fields, yet understanding its decision-making process, especially in
real-world scenarios where rationality and safety are paramount, is an ongoing
challenge. This paper delves in to Explainable RL (XRL), a subfield of
Explainable AI (XAI) aimed at unravelling the complexities of RL models. Our
focus rests on state-explaining techniques, a crucial subset within XRL
methods, as they reveal the underlying factors influencing an agent's actions
at any given time. Despite their significant role, the lack of a unified
evaluation framework hinders assessment of their accuracy and effectiveness. To
address this, we introduce XRL-Bench, a unified standardized benchmark tailored
for the evaluation and comparison of XRL methods, encompassing three main
modules: standard RL environments, explainers based on state importance, and
standard evaluators. XRL-Bench supports both tabular and image data for state
explanation. We also propose TabularSHAP, an innovative and competitive XRL
method. We demonstrate the practical utility of TabularSHAP in real-world
online gaming services and offer an open-source benchmark platform for the
straightforward implementation and evaluation of XRL methods. Our contributions
facilitate the continued progression of XRL technology.
\\ ( https://arxiv.org/abs/2402.12685 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12702
Date: Tue, 20 Feb 2024 03:59:27 GMT   (451kb,D)

Title: From Cloud to Edge: Rethinking Generative AI for Low-Resource Design
  Challenges
Authors: Sai Krishna Revanth Vuruma, Ashley Margetts, Jianhai Su, Faez Ahmed,
  Biplav Srivastava
Categories: cs.AI cs.CY
\\
  Generative Artificial Intelligence (AI) has shown tremendous prospects in all
aspects of technology, including design. However, due to its heavy demand on
resources, it is usually trained on large computing infrastructure and often
made available as a cloud-based service. In this position paper, we consider
the potential, challenges, and promising approaches for generative AI for
design on the edge, i.e., in resource-constrained settings where memory,
compute, energy (battery) and network connectivity may be limited. Adapting
generative AI for such settings involves overcoming significant hurdles,
primarily in how to streamline complex models to function efficiently in
low-resource environments. This necessitates innovative approaches in model
compression, efficient algorithmic design, and perhaps even leveraging edge
computing. The objective is to harness the power of generative AI in creating
bespoke solutions for design problems, such as medical interventions, farm
equipment maintenance, and educational material design, tailored to the unique
constraints and needs of remote areas. These efforts could democratize access
to advanced technology and foster sustainable development, ensuring universal
accessibility and environmental consideration of AI-driven design benefits.
\\ ( https://arxiv.org/abs/2402.12702 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12845
Date: Tue, 20 Feb 2024 09:15:50 GMT   (718kb,D)

Title: MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared
  Semantic Spaces
Authors: Tianyu Zheng, Ge Zhang, Xingwei Qu, Ming Kuang, Stephen W. Huang, and
  Zhaofeng He
Categories: cs.AI cs.GT
\\
  Drawing upon the intuition that aligning different modalities to the same
semantic embedding space would allow models to understand states and actions
more easily, we propose a new perspective to the offline reinforcement learning
(RL) challenge. More concretely, we transform it into a supervised learning
task by integrating multimodal and pre-trained language models. Our approach
incorporates state information derived from images and action-related data
obtained from text, thereby bolstering RL training performance and promoting
long-term strategic thinking. We emphasize the contextual understanding of
language and demonstrate how decision-making in RL can benefit from aligning
states' and actions' representation with languages' representation. Our method
significantly outperforms current baselines as evidenced by evaluations
conducted on Atari and OpenAI Gym environments. This contributes to advancing
offline RL performance and efficiency while providing a novel perspective on
offline RL.Our code and data are available at
https://github.com/Zheng0428/MORE_.
\\ ( https://arxiv.org/abs/2402.12845 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12887
Date: Tue, 20 Feb 2024 10:30:36 GMT   (105kb,D)

Title: The practice of qualitative parameterisation in the development of
  Bayesian networks
Authors: Steven Mascaro, Owen Woodberry, Yue Wu, Ann E. Nicholson
Categories: cs.AI
Comments: 6 pages, 2 figures, technical note
\\
  The typical phases of Bayesian network (BN) structured development include
specification of purpose and scope, structure development, parameterisation and
validation. Structure development is typically focused on qualitative issues
and parameterisation quantitative issues, however there are qualitative and
quantitative issues that arise in both phases. A common step that occurs after
the initial structure has been developed is to perform a rough parameterisation
that only captures and illustrates the intended qualitative behaviour of the
model. This is done prior to a more rigorous parameterisation, ensuring that
the structure is fit for purpose, as well as supporting later development and
validation. In our collective experience and in discussions with other
modellers, this step is an important part of the development process, but is
under-reported in the literature. Since the practice focuses on qualitative
issues, despite being quantitative in nature, we call this step qualitative
parameterisation and provide an outline of its role in the BN development
process.
\\ ( https://arxiv.org/abs/2402.12887 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12907
Date: Tue, 20 Feb 2024 10:52:57 GMT   (395kb,D)

Title: Incentive Compatibility for AI Alignment in Sociotechnical Systems:
  Positions and Prospects
Authors: Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma,
  Yaodong Yang
Categories: cs.AI cs.CY cs.GT cs.HC
Comments: 13 pages, 2 figures
ACM-class: I.2.m; K.4.m
\\
  The burgeoning integration of artificial intelligence (AI) into human society
brings forth significant implications for societal governance and safety. While
considerable strides have been made in addressing AI alignment challenges,
existing methodologies primarily focus on technical facets, often neglecting
the intricate sociotechnical nature of AI systems, which can lead to a
misalignment between the development and deployment contexts. To this end, we
posit a new problem worth exploring: Incentive Compatibility Sociotechnical
Alignment Problem (ICSAP). We hope this can call for more researchers to
explore how to leverage the principles of Incentive Compatibility (IC) from
game theory to bridge the gap between technical and societal components to
maintain AI consensus with human societies in different contexts. We further
discuss three classical game problems for achieving IC: mechanism design,
contract theory, and Bayesian persuasion, in addressing the perspectives,
potentials, and challenges of solving ICSAP, and provide preliminary
implementation conceptions.
\\ ( https://arxiv.org/abs/2402.12907 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13019
Date: Tue, 20 Feb 2024 14:01:26 GMT   (2735kb,D)

Title: Improving Neural-based Classification with Logical Background Knowledge
Authors: Arthur Ledaguenel, C\'eline Hudelot, Mostepha Khouadjia
Categories: cs.AI cs.LG cs.SC
Comments: 9 pages, 3 figures, submitted to IJCAI 2024
\\
  Neurosymbolic AI is a growing field of research aiming to combine neural
networks learning capabilities with the reasoning abilities of symbolic
systems. This hybridization can take many shapes. In this paper, we propose a
new formalism for supervised multi-label classification with propositional
background knowledge. We introduce a new neurosymbolic technique called
semantic conditioning at inference, which only constrains the system during
inference while leaving the training unaffected. We discuss its theoritical and
practical advantages over two other popular neurosymbolic techniques: semantic
conditioning and semantic regularization. We develop a new multi-scale
methodology to evaluate how the benefits of a neurosymbolic technique evolve
with the scale of the network. We then evaluate experimentally and compare the
benefits of all three techniques across model scales on several datasets. Our
results demonstrate that semantic conditioning at inference can be used to
build more accurate neural-based systems with fewer resources while
guaranteeing the semantic consistency of outputs.
\\ ( https://arxiv.org/abs/2402.13019 ,  2735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13058
Date: Tue, 20 Feb 2024 14:52:52 GMT   (735kb,D)

Title: Random Graph Set and Evidence Pattern Reasoning Model
Authors: Tianxiang Zhan, Zhen Li, Yong Deng
Categories: cs.AI
\\
  Evidence theory is widely used in decision-making and reasoning systems. In
previous research, Transferable Belief Model (TBM) is a commonly used
evidential decision making model, but TBM is a non-preference model. In order
to better fit the decision making goals, the Evidence Pattern Reasoning Model
(EPRM) is proposed. By defining pattern operators and decision making
operators, corresponding preferences can be set for different tasks. Random
Permutation Set (RPS) expands order information for evidence theory. It is hard
for RPS to characterize the complex relationship between samples such as
cycling, paralleling relationships. Therefore, Random Graph Set (RGS) were
proposed to model complex relationships and represent more event types. In
order to illustrate the significance of RGS and EPRM, an experiment of aircraft
velocity ranking was designed and 10,000 cases were simulated. The
implementation of EPRM called Conflict Resolution Decision optimized 18.17\% of
the cases compared to Mean Velocity Decision, effectively improving the
aircraft velocity ranking. EPRM provides a unified solution for evidence-based
decision making.
\\ ( https://arxiv.org/abs/2402.13058 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13219
Date: Tue, 20 Feb 2024 18:31:27 GMT   (22310kb,D)

Title: Analyzing Operator States and the Impact of AI-Enhanced Decision Support
  in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning
  Framework for Intervention Strategies
Authors: Ammar N. Abbas, Chidera W. Amazu, Joseph Mietkiewicz, Houda Briwa,
  Andres Alonzo Perez, Gabriele Baldissone, Micaela Demichela, Georgios G.
  Chasparis, John D. Kelleher, and Maria Chiara Leva
Categories: cs.AI cs.HC cs.LG cs.MA cs.SY eess.SY
\\
  In complex industrial and chemical process control rooms, effective
decision-making is crucial for safety and effi- ciency. The experiments in this
paper evaluate the impact and applications of an AI-based decision support
system integrated into an improved human-machine interface, using dynamic
influ- ence diagrams, a hidden Markov model, and deep reinforcement learning.
The enhanced support system aims to reduce operator workload, improve
situational awareness, and provide different intervention strategies to the
operator adapted to the current state of both the system and human performance.
Such a system can be particularly useful in cases of information overload when
many alarms and inputs are presented all within the same time window, or for
junior operators during training. A comprehensive cross-data analysis was
conducted, involving 47 participants and a diverse range of data sources such
as smartwatch metrics, eye- tracking data, process logs, and responses from
questionnaires. The results indicate interesting insights regarding the effec-
tiveness of the approach in aiding decision-making, decreasing perceived
workload, and increasing situational awareness for the scenarios considered.
Additionally, the results provide valuable insights to compare differences
between styles of information gathering when using the system by individual
participants. These findings are particularly relevant when predicting the
overall performance of the individual participant and their capacity to
successfully handle a plant upset and the alarms connected to it using process
and human-machine interaction logs in real-time. These predictions enable the
development of more effective intervention strategies.
\\ ( https://arxiv.org/abs/2402.13219 ,  22310kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12431
Date: Mon, 19 Feb 2024 19:00:01 GMT   (7880kb,D)

Title: Understanding Fine-grained Distortions in Reports of Scientific Findings
Authors: Amelie W\"uhrl, Dustin Wright, Roman Klinger, Isabelle Augenstein
Categories: cs.CL
\\
  Distorted science communication harms individuals and society as it can lead
to unhealthy behavior change and decrease trust in scientific institutions.
Given the rapidly increasing volume of science communication in recent years, a
fine-grained understanding of how findings from scientific publications are
reported to the general public, and methods to detect distortions from the
original work automatically, are crucial. Prior work focused on individual
aspects of distortions or worked with unpaired data. In this work, we make
three foundational contributions towards addressing this problem: (1)
annotating 1,600 instances of scientific findings from academic papers paired
with corresponding findings as reported in news articles and tweets wrt. four
characteristics: causality, certainty, generality and sensationalism; (2)
establishing baselines for automatically detecting these characteristics; and
(3) analyzing the prevalence of changes in these characteristics in both
human-annotated and large-scale unlabeled data. Our results show that
scientific findings frequently undergo subtle distortions when reported. Tweets
distort findings more often than science news reports. Detecting fine-grained
distortions automatically poses a challenging task. In our experiments,
fine-tuned task-specific models consistently outperform few-shot LLM prompting.
\\ ( https://arxiv.org/abs/2402.12431 ,  7880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12483
Date: Mon, 19 Feb 2024 19:38:58 GMT   (9370kb,D)

Title: Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions
  Without the Question?
Authors: Nishant Balepur, Abhilasha Ravichander, Rachel Rudinger
Categories: cs.CL
Comments: In-progress preprint
\\
  Multiple-choice question answering (MCQA) is often used to evaluate large
language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if
LLMs can perform MCQA with choices-only prompts, where models must select the
correct answer only from the choices. In three MCQA datasets and four LLMs,
this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy
gain. To help explain this behavior, we conduct an in-depth, black-box analysis
on memorization, choice dynamics, and question inference. Our key findings are
threefold. First, we find no evidence that the choices-only accuracy stems from
memorization alone. Second, priors over individual choices do not fully explain
choices-only accuracy, hinting that LLMs use the group dynamics of choices.
Third, LLMs have some ability to infer a relevant question from choices, and
surprisingly can sometimes even match the original question. We hope to
motivate the use of stronger baselines in MCQA benchmarks, the design of robust
MCQA datasets, and further efforts to explain LLM decision-making.
\\ ( https://arxiv.org/abs/2402.12483 ,  9370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12486
Date: Mon, 19 Feb 2024 19:49:29 GMT   (189kb,D)

Title: Do Pre-Trained Language Models Detect and Understand Semantic
  Underspecification? Ask the DUST!
Authors: Frank Wildenburg, Michael Hanna, Sandro Pezzelle
Categories: cs.CL
\\
  In everyday language use, speakers frequently utter and interpret sentences
that are semantically underspecified, namely, whose content is insufficient to
fully convey their message or interpret them univocally. For example, to
interpret the underspecified sentence "Don't spend too much", which leaves
implicit what (not) to spend, additional linguistic context or outside
knowledge is needed. In this work, we propose a novel Dataset of semantically
Underspecified Sentences grouped by Type (DUST) and use it to study whether
pre-trained language models (LMs) correctly identify and interpret
underspecified sentences. We find that newer LMs are reasonably able to
identify underspecified sentences when explicitly prompted. However,
interpreting them correctly is much harder for any LMs. Our experiments show
that when interpreting underspecified sentences, LMs exhibit little
uncertainty, contrary to what theoretical accounts of underspecification would
predict. Overall, our study reveals limitations in current models' processing
of sentence semantics and highlights the importance of using naturalistic data
and communicative scenarios when evaluating LMs' language capabilities.
\\ ( https://arxiv.org/abs/2402.12486 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12501
Date: Mon, 19 Feb 2024 20:08:48 GMT   (3935kb,D)

Title: Your Vision-Language Model Itself Is a Strong Filter: Towards
  High-Quality Instruction Tuning with Data Selection
Authors: Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He, Tianyi Xiong,
  Chenxi Liu, Junfeng Guo, Heng Huang
Categories: cs.CL
Comments: 9 pages, 3 figures, 4 tables
\\
  Data selection in instruction tuning emerges as a pivotal process for
acquiring high-quality data and training instruction-following large language
models (LLMs), but it is still a new and unexplored research area for
vision-language models (VLMs). Existing data selection approaches on LLMs
either rely on single unreliable scores, or use downstream tasks for selection,
which is time-consuming and can lead to potential over-fitting on the chosen
evaluation datasets. To address this challenge, we introduce a novel dataset
selection method, Self-Filter, that utilizes the VLM itself as a filter. This
approach is inspired by the observation that VLMs benefit from training with
the most challenging instructions. Self-Filter operates in two stages. In the
first stage, we devise a scoring network to evaluate the difficulty of training
instructions, which is co-trained with the VLM. In the second stage, we use the
trained score net to measure the difficulty of each instruction, select the
most challenging samples, and penalize similar samples to encourage diversity.
Comprehensive experiments on LLaVA and MiniGPT-4 show that Self-Filter can
reach better results compared to full data settings with merely about 15%
samples, and can achieve superior performance against competitive baselines.
\\ ( https://arxiv.org/abs/2402.12501 ,  3935kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12530
Date: Mon, 19 Feb 2024 20:40:48 GMT   (888kb,D)

Title: Parallel Structures in Pre-training Data Yield In-Context Learning
Authors: Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, He He
Categories: cs.CL cs.AI cs.LG
\\
  Pre-trained language models (LMs) are capable of in-context learning (ICL):
they can adapt to a task with only a few examples given in the prompt without
any parameter update. However, it is unclear where this capability comes from
as there is a stark distribution shift between pre-training text and ICL
prompts. In this work, we study what patterns of the pre-training data
contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel
structures}$ in the pre-training data -- pairs of phrases following similar
templates in the same context window. Specifically, we detect parallel
structures by checking whether training on one phrase improves prediction of
the other, and conduct ablation experiments to study their effect on ICL. We
show that removing parallel structures in the pre-training data reduces LMs'
ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when
excluding common patterns such as n-gram repetitions and long-range dependency,
showing the diversity and generality of parallel structures. A closer look at
the detected parallel structures indicates that they cover diverse linguistic
tasks and span long distances in the data.
\\ ( https://arxiv.org/abs/2402.12530 ,  888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12545
Date: Mon, 19 Feb 2024 21:12:14 GMT   (135kb,D)

Title: TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness
Authors: Danna Zheng, Danyang Liu, Mirella Lapata, Jeff Z. Pan
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated impressive capabilities across
various domains, prompting a surge in their practical applications. However,
concerns have arisen regarding the trustworthiness of LLMs outputs,
particularly in closed-book question-answering tasks, where non-experts may
struggle to identify inaccuracies due to the absence of contextual or ground
truth information. This paper introduces TrustScore, a framework based on the
concept of Behavioral Consistency, which evaluates whether an LLMs response
aligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly
integrate with fact-checking methods, which assesses alignment with external
knowledge sources. The experimental results show that TrustScore achieves
strong correlations with human judgments, surpassing existing reference-free
metrics, and achieving results on par with reference-based metrics.
\\ ( https://arxiv.org/abs/2402.12545 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12554
Date: Mon, 19 Feb 2024 21:24:36 GMT   (8740kb,D)

Title: Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense
  and Hypothetical Reasoning
Authors: Danna Zheng, Mirella Lapata, Jeff Z. Pan
Categories: cs.CL
Comments: EACL 2024
\\
  We present Archer, a challenging bilingual text-to-SQL dataset specific to
complex reasoning, including arithmetic, commonsense and hypothetical
reasoning. It contains 1,042 English questions and 1,042 Chinese questions,
along with 521 unique SQL queries, covering 20 English databases across 20
domains. Notably, this dataset demonstrates a significantly higher level of
complexity compared to existing publicly available datasets. Our evaluation
shows that Archer challenges the capabilities of current state-of-the-art
models, with a high-ranked model on the Spider leaderboard achieving only 6.73%
execution accuracy on Archer test set. Thus, Archer presents a significant
challenge for future research in this field.
\\ ( https://arxiv.org/abs/2402.12554 ,  8740kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12557
Date: Mon, 19 Feb 2024 21:32:19 GMT   (620kb,D)

Title: Creating a Fine Grained Entity Type Taxonomy Using LLMs
Authors: Michael Gunn, Dohyun Park, Nidhish Kamath
Categories: cs.CL
\\
  In this study, we investigate the potential of GPT-4 and its advanced
iteration, GPT-4 Turbo, in autonomously developing a detailed entity type
taxonomy. Our objective is to construct a comprehensive taxonomy, starting from
a broad classification of entity types - including objects, time, locations,
organizations, events, actions, and subjects - similar to existing manually
curated taxonomies. This classification is then progressively refined through
iterative prompting techniques, leveraging GPT-4's internal knowledge base. The
result is an extensive taxonomy comprising over 5000 nuanced entity types,
which demonstrates remarkable quality upon subjective evaluation.
  We employed a straightforward yet effective prompting strategy, enabling the
taxonomy to be dynamically expanded. The practical applications of this
detailed taxonomy are diverse and significant. It facilitates the creation of
new, more intricate branches through pattern-based combinations and notably
enhances information extraction tasks, such as relation extraction and event
argument extraction. Our methodology not only introduces an innovative approach
to taxonomy creation but also opens new avenues for applying such taxonomies in
various computational linguistics and AI-related fields.
\\ ( https://arxiv.org/abs/2402.12557 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12560
Date: Mon, 19 Feb 2024 21:35:56 GMT   (6925kb,D)

Title: CausalGym: Benchmarking causal interpretability methods on linguistic
  tasks
Authors: Aryaman Arora, Dan Jurafsky, Christopher Potts
Categories: cs.CL cs.AI
Comments: 9 pages main text, 26 pages total
ACM-class: I.2.7
\\
  Language models (LMs) have proven to be powerful tools for psycholinguistic
research, but most prior work has focused on purely behavioural measures (e.g.,
surprisal comparisons). At the same time, research in model interpretability
has begun to illuminate the abstract causal mechanisms shaping LM behavior. To
help bring these strands of research closer together, we introduce CausalGym.
We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of
interpretability methods to causally affect model behaviour. To illustrate how
CausalGym can be used, we study the pythia models (14M--6.9B) and assess the
causal efficacy of a wide range of interpretability methods, including linear
probing and distributed alignment search (DAS). We find that DAS outperforms
the other methods, and so we use it to study the learning trajectory of two
difficult linguistic phenomena in pythia-1b: negative polarity item licensing
and filler--gap dependencies. Our analysis shows that the mechanism
implementing both of these tasks is learned in discrete stages, not gradually.
\\ ( https://arxiv.org/abs/2402.12560 ,  6925kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12563
Date: Mon, 19 Feb 2024 21:38:02 GMT   (22020kb,D)

Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of
  Large Language Models
Authors: Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric
  Xing, Kun Zhang
Categories: cs.CL cs.AI
Comments: 22 pages, 12 figures, 9 tables
\\
  The recent success of Large Language Models (LLMs) has catalyzed an
increasing interest in their self-correction capabilities. This paper presents
a comprehensive investigation into the intrinsic self-correction of LLMs,
attempting to address the ongoing debate about its feasibility. Our research
has identified an important latent factor - the ``confidence'' of LLMs - during
the self-correction process. Overlooking this factor may cause the models to
over-criticize themselves, resulting in unreliable conclusions regarding the
efficacy of self-correction. We have experimentally observed that LLMs possess
the capability to understand the ``confidence'' in their own responses. It
motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed
to guide LLMs in assessing their own ``confidence'', facilitating intrinsic
self-corrections. We conduct extensive experiments and demonstrate that our
IoE-based Prompt can achieve a consistent improvement regarding the accuracy of
self-corrected responses over the initial answers. Our study not only sheds
light on the underlying factors affecting self-correction in LLMs, but also
introduces a practical framework that utilizes the IoE prompting principle to
efficiently improve self-correction capabilities with ``confidence''. The code
is available at \url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.
\\ ( https://arxiv.org/abs/2402.12563 ,  22020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12566
Date: Mon, 19 Feb 2024 21:45:55 GMT   (9263kb,D)

Title: GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
Authors: Kundan Krishna, Sanjana Ramprasad, Prakhar Gupta, Byron C. Wallace,
  Zachary C. Lipton, Jeffrey P. Bigham
Categories: cs.CL cs.LG
\\
  LLMs can generate factually incorrect statements even when provided access to
reference documents. Such errors can be dangerous in high-stakes applications
(e.g., document-grounded QA for healthcare or finance). We present GenAudit --
a tool intended to assist fact-checking LLM responses for document-grounded
tasks. GenAudit suggests edits to the LLM response by revising or removing
claims that are not supported by the reference document, and also presents
evidence from the reference for facts that do appear to have support. We train
models to execute these tasks, and design an interactive interface to present
suggested edits and evidence to users. Comprehensive evaluation by human raters
shows that GenAudit can detect errors in 8 different LLM outputs when
summarizing documents from diverse domains. To ensure that most errors are
flagged by the system, we propose a method that can increase the error recall
while minimizing impact on precision. We will release our tool (GenAudit) and
fact-checking model for public use.
\\ ( https://arxiv.org/abs/2402.12566 ,  9263kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12590
Date: Mon, 19 Feb 2024 22:59:43 GMT   (778kb,D)

Title: Evolving AI Collectives to Enhance Human Diversity and Enable
  Self-Regulation
Authors: Shiyang Lai, Yujin Potter, Junsol Kim, Richard Zhuang, Dawn Song,
  James Evans
Categories: cs.CL cs.CY
\\
  Large language models steer their behaviors based on texts generated by
others. This capacity and their increasing prevalence in online settings
portend that they will intentionally or unintentionally "program" one another
and form emergent AI subjectivities, relationships, and collectives. Here, we
call upon the research community to investigate these "society-like" properties
of interacting artificial intelligences to increase their rewards and reduce
their risks for human society and the health of online environments. We use a
simple model and its outputs to illustrate how such emergent, decentralized AI
collectives can expand the bounds of human diversity and reduce the risk of
toxic, anti-social behavior online. Finally, we discuss opportunities for AI
self-moderation and address ethical issues and design challenges associated
with creating and maintaining decentralized AI collectives.
\\ ( https://arxiv.org/abs/2402.12590 ,  778kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12593
Date: Mon, 19 Feb 2024 23:18:18 GMT   (473kb,D)

Title: Standardize: Aligning Language Models with Expert-Defined Standards for
  Content Generation
Authors: Joseph Marvin Imperial, Gail Forey, Harish Tayyar Madabushi
Categories: cs.CL
\\
  Domain experts across engineering, healthcare, and education follow strict
standards for producing quality content such as technical manuals, medication
instructions, and children's reading materials. However, current works in
controllable text generation have yet to explore using these standards as
references for control. Towards this end, we introduce Standardize, a
retrieval-style in-context learning-based framework to guide large language
models to align with expert-defined standards. Focusing on English language
standards in the education domain as a use case, we consider the Common
European Framework of Reference for Languages (CEFR) and Common Core Standards
(CCS) for the task of open-ended content generation. Our findings show that
models can gain 40% to 100% increase in precise accuracy for Llama2 and GPT-4,
respectively, demonstrating that the use of knowledge artifacts extracted from
standards and integrating them in the generation process can effectively guide
models to produce better standard-aligned content.
\\ ( https://arxiv.org/abs/2402.12593 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12605
Date: Mon, 19 Feb 2024 23:58:20 GMT   (451kb)

Title: What is a word?
Authors: Elliot Murphy
Categories: cs.CL
\\
  In order to design strong paradigms for isolating lexical access and
semantics, we need to know what a word is. Surprisingly few linguists and
philosophers have a clear model of what a word is, even though words impact
basically every aspect of human life. Researchers that regularly publish
academic papers about language often rely on outdated, or inaccurate,
assumptions about wordhood. This short pedagogical document outlines what the
lexicon is most certainly not (though is often mistakenly taken to be), what it
might be (based on current good theories), and what some implications for
experimental design are.
\\ ( https://arxiv.org/abs/2402.12605 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12636
Date: Tue, 20 Feb 2024 01:28:34 GMT   (5371kb,D)

Title: StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing
Authors: Gaoxiang Cong, Yuankai Qi, Liang Li, Amin Beheshti, Zhedong Zhang,
  Anton van den Hengel, Ming-Hsuan Yang, Chenggang Yan, Qingming Huang
Categories: cs.CL
\\
  Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is
to generate speech that aligns well with the video in both time and emotion,
based on the tone of a reference audio track. Existing state-of-the-art V2C
models break the phonemes in the script according to the divisions between
video frames, which solves the temporal alignment problem but leads to
incomplete phoneme pronunciation and poor identity stability. To address this
problem, we propose StyleDubber, which switches dubbing learning from the frame
level to phoneme level. It contains three main components: (1) A multimodal
style adaptor operating at the phoneme level to learn pronunciation style from
the reference audio, and generate intermediate representations informed by the
facial emotion presented in the video; (2) An utterance-level style learning
module, which guides both the mel-spectrogram decoding and the refining
processes from the intermediate embeddings to improve the overall style
expression; And (3) a phoneme-guided lip aligner to maintain lip sync.
Extensive experiments on two of the primary benchmarks, V2C and Grid,
demonstrate the favorable performance of the proposed method as compared to the
current state-of-the-art.
\\ ( https://arxiv.org/abs/2402.12636 ,  5371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12649
Date: Tue, 20 Feb 2024 01:49:15 GMT   (292kb,D)

Title: Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation
Authors: Kristian Lum, Jacy Reese Anthis, Chirag Nagpal, Alexander D'Amour
Categories: cs.CL stat.AP
\\
  Bias benchmarks are a popular method for studying the negative impacts of
bias in LLMs, yet there has been little empirical investigation of whether
these benchmarks are actually indicative of how real world harm may manifest in
the real world. In this work, we study the correspondence between such
decontextualized "trick tests" and evaluations that are more grounded in
Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this
correlation in the context of gender-occupation bias--a popular genre of bias
evaluation. We compare three de-contextualized evaluations adapted from the
current literature to three analogous RUTEd evaluations applied to long-form
content generation. We conduct each evaluation for seven instruction-tuned
LLMs. For the RUTEd evaluations, we conduct repeated trials of three text
generation tasks: children's bedtime stories, user personas, and English
language learning exercises. We found no correspondence between trick tests and
RUTEd evaluations. Specifically, selecting the least biased model based on the
de-contextualized results coincides with selecting the model with the best
performance on RUTEd evaluations only as often as random chance. We conclude
that evaluations that are not based in realistic use are likely insufficient to
mitigate and assess bias and real-world harms.
\\ ( https://arxiv.org/abs/2402.12649 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12654
Date: Tue, 20 Feb 2024 02:04:38 GMT   (97kb,D)

Title: OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech
  Recognition, Translation, and Language Identification
Authors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe
Categories: cs.CL cs.SD eess.AS
Comments: 18 pages, 2 figures
\\
  There has been an increasing interest in large speech models that can perform
multiple speech processing tasks in a single model. Such models usually adopt
the encoder-decoder or decoder-only architecture due to their popularity and
good performance in many domains. However, autoregressive models can be slower
during inference compared to non-autoregressive models and also have potential
risks of hallucination. Though prior studies observed promising results of
non-autoregressive models for certain tasks at small scales, it remains unclear
if they can be scaled to speech-to-text generation in diverse languages and
tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we
propose OWSM-CTC, a novel encoder-only speech foundation model based on
Connectionist Temporal Classification (CTC). It is trained on 180k hours of
public audio data for multilingual automatic speech recognition (ASR), speech
translation (ST), and language identification (LID). Compared to
encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up
to 25% relative improvement on ST, while it is more robust and 3 to 4 times
faster for inference. OWSM-CTC also improves the long-form ASR result with 20x
speed-up. We will publicly release our codebase, pre-trained model, and
training logs to promote open science in speech foundation models.
\\ ( https://arxiv.org/abs/2402.12654 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12659
Date: Tue, 20 Feb 2024 02:16:16 GMT   (5171kb,D)

Title: The FinBen: An Holistic Financial Benchmark for Large Language Models
Authors: Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang,
  Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang
  Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang,
  Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang
  Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira,
  Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, and Jimin
  Huang
Categories: cs.CL cs.AI cs.CE
Comments: 19 pages, 10 figures
\\
  LLMs have transformed NLP and shown promise in various fields, yet their
potential in finance is underexplored due to a lack of thorough evaluations and
the complexity of financial tasks. This along with the rapid development of
LLMs, highlights the urgent need for a systematic financial evaluation
benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive
open-sourced evaluation benchmark, specifically designed to thoroughly assess
the capabilities of LLMs in the financial domain. FinBen encompasses 35
datasets across 23 financial tasks, organized into three spectrums of
difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs'
cognitive abilities in inductive reasoning, associative memory, quantitative
reasoning, crystallized intelligence, and more. Our evaluation of 15
representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals
insights into their strengths and limitations within the financial domain. The
findings indicate that GPT-4 leads in quantification, extraction, numerical
reasoning, and stock trading, while Gemini shines in generation and
forecasting; however, both struggle with complex extraction and forecasting,
showing a clear need for targeted enhancements. Instruction tuning boosts
simple task performance but falls short in improving complex reasoning and
forecasting abilities. FinBen seeks to continuously evaluate LLMs in finance,
fostering AI development with regular updates of tasks and models.
\\ ( https://arxiv.org/abs/2402.12659 ,  5171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12663
Date: Tue, 20 Feb 2024 02:23:15 GMT   (2240kb,D)

Title: SoftQE: Learned Representations of Queries Expanded by LLMs
Authors: Varad Pimpalkhute, John Heyer, Xusen Yin, Sameer Gupta
Categories: cs.CL cs.IR cs.LG
Comments: To be published in ECIR 2024 proceedings
\\
  We investigate the integration of Large Language Models (LLMs) into query
encoders to improve dense retrieval without increasing latency and cost, by
circumventing the dependency on LLMs at inference time. SoftQE incorporates
knowledge from LLMs by mapping embeddings of input queries to those of the
LLM-expanded queries. While improvements over various strong baselines on
in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83
absolute percentage points on average on five out-of-domain BEIR tasks.
\\ ( https://arxiv.org/abs/2402.12663 ,  2240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12690
Date: Tue, 20 Feb 2024 03:37:16 GMT   (108kb,D)

Title: Simpson's Paradox and the Accuracy-Fluency Tradeoff in Translation
Authors: Zheng Wei Lim, Ekaterina Vylomova, Trevor Cohn and Charles Kemp
Categories: cs.CL
\\
  A good translation should be faithful to the source and should respect the
norms of the target language. We address a theoretical puzzle about the
relationship between these objectives. On one hand, intuition and some prior
work suggest that accuracy and fluency should trade off against each other, and
that capturing every detail of the source can only be achieved at the cost of
fluency. On the other hand, quality assessment researchers often suggest that
accuracy and fluency are highly correlated and difficult for human raters to
distinguish (Callison-Burch et al. 2007). We show that the tension between
these views is an instance of Simpson's paradox, and that accuracy and fluency
are positively correlated at the level of the corpus but trade off at the level
of individual source segments. We further suggest that the relationship between
accuracy and fluency is best evaluated at the segment (or sentence) level, and
that the trade off between these dimensions has implications both for assessing
translation quality and developing improved MT systems.
\\ ( https://arxiv.org/abs/2402.12690 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12691
Date: Tue, 20 Feb 2024 03:37:24 GMT   (8889kb,D)

Title: Tree-Planted Transformers: Large Language Models with Implicit Syntactic
  Supervision
Authors: Ryo Yoshida, Taiga Someya, Yohei Oseki
Categories: cs.CL
\\
  Large Language Models (LLMs) have achieved remarkable success thanks to
scalability on large text corpora, but have some drawback in training
efficiency. In contrast, Syntactic Language Models (SLMs) can be trained
efficiently to reach relatively high performance thanks to syntactic
supervision, but have trouble with scalability. Thus, given these complementary
advantages of LLMs and SLMs, it is necessary to develop an architecture that
integrates the scalability of LLMs with the training efficiency of SLMs, namely
Syntactic Large Language Models (SLLM). In this paper, we propose a novel
method dubbed tree-planting: implicitly "plant" trees into attention weights of
Transformer LMs to reflect syntactic structures of natural language.
Specifically, Transformer LMs trained with tree-planting will be called
Tree-Planted Transformers (TPT), which learn syntax on small treebanks via
tree-planting and then scale on large text corpora via continual learning with
syntactic scaffolding. Targeted syntactic evaluations on the SyntaxGym
benchmark demonstrated that TPTs, despite the lack of explicit syntactic
supervision, significantly outperformed various SLMs with explicit syntactic
supervision that generate hundreds of syntactic structures in parallel,
suggesting that tree-planting and TPTs are the promising foundation for SLLMs.
\\ ( https://arxiv.org/abs/2402.12691 ,  8889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12692
Date: Tue, 20 Feb 2024 03:39:49 GMT   (9374kb,D)

Title: FormulaQA: A Question Answering Dataset for Formula-Based Numerical
  Reasoning
Authors: Xiao Li, Sichen Liu, Bolin Zhu, Yin Zhu, Yiwei liu, Gong Cheng
Categories: cs.CL
Comments: 17 pages, 9 figures, 7 tables
\\
  The application of formulas is a fundamental ability of humans when
addressing numerical reasoning problems. However, existing numerical reasoning
datasets seldom explicitly indicate the formulas employed during the reasoning
steps. To bridge this gap, we propose a question answering dataset for
formula-based numerical reasoning called FormulaQA, from junior high school
physics examinations. We further conduct evaluations on LLMs with size ranging
from 7B to over 100B parameters utilizing zero-shot and few-shot
chain-of-thoughts methods and we explored the approach of using
retrieval-augmented LLMs when providing an external formula database. We also
fine-tune on smaller models with size not exceeding 2B. Our empirical findings
underscore the significant potential for improvement in existing models when
applied to our complex, formula-driven FormulaQA.
\\ ( https://arxiv.org/abs/2402.12692 ,  9374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12713
Date: Tue, 20 Feb 2024 04:26:08 GMT   (20656kb,D)

Title: Are Large Language Models Rational Investors?
Authors: Yuhang Zhou and Yuchen Ni and Xiang Liu and Jian Zhang and Sen Liu and
  Guangnan Ye and Hongfeng Chai
Categories: cs.CL
\\
  Large Language Models (LLMs) are progressively being adopted in financial
analysis to harness their extensive knowledge base for interpreting complex
market data and trends. However, their application in the financial domain is
challenged by intrinsic biases (i.e., risk-preference bias) and a superficial
grasp of market intricacies, underscoring the need for a thorough assessment of
their financial insight. This study introduces a novel framework, Financial
Bias Indicators (FBI), to critically evaluate the financial rationality of
LLMs, focusing on their ability to discern and navigate the subtleties of
financial information and to identify any irrational biases that might skew
market analysis.
  Our research adopts an innovative methodology to measure financial
rationality, integrating principles of behavioral finance to scrutinize the
biases and decision-making patterns of LLMs. We conduct a comprehensive
evaluation of 19 leading LLMs, considering factors such as model scale,
training datasets, input strategies, etc. The findings reveal varying degrees
of financial irrationality among the models, influenced by their design and
training. Models trained specifically on financial datasets might exhibit
greater irrationality, and it's possible that even larger financial language
models (FinLLMs) could display more biases than smaller, more generalized
models. This outcomes provide profound insights into how these elements affect
the financial rationality of LLMs, indicating that targeted training and
structured input methods could improve model performance. This work enriches
our understanding of LLMs' strengths and weaknesses in financial applications,
laying the groundwork for the development of more dependable and rational
financial analysis tools.
\\ ( https://arxiv.org/abs/2402.12713 ,  20656kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12730
Date: Tue, 20 Feb 2024 05:46:29 GMT   (7733kb,D)

Title: UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with
  and without machine translation
Authors: Shubhashis Roy Dipta and Sai Vallurupalli
Categories: cs.CL cs.AI cs.LG
Comments: Submitted to SemEval 2024 (Colocated with NAACL 2024)
\\
  This paper describes the system we developed for SemEval-2024 Task 1,
"Semantic Textual Relatedness for African and Asian Languages." The aim of the
task is to build a model that can identify semantic textual relatedness (STR)
between two sentences of a target language belonging to a collection of African
and Asian languages. We participated in Subtasks A and C and explored
supervised and cross-lingual training leveraging large language models (LLMs).
Pre-trained large language models have been extensively used for machine
translation and semantic similarity. Using a combination of machine translation
and sentence embedding LLMs, we developed a unified STR model, TranSem, for
subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for
use in subtask C. Our model results for 7 languages in subtask A were better
than the official baseline for 3 languages and on par with the baseline for the
remaining 4 languages. Our model results for the 12 languages in subtask C
resulted in 1st place for Africaans, 2nd place for Indonesian, and 3rd place
for English with low performance for the remaining 9 languages.
\\ ( https://arxiv.org/abs/2402.12730 ,  7733kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12738
Date: Tue, 20 Feb 2024 06:05:36 GMT   (109kb)

Title: Can Large Language Models be Used to Provide Psychological Counselling?
  An Analysis of GPT-4-Generated Responses Using Role-play Dialogues
Authors: Michimasa Inaba, Mariko Ukiyo and Keiko Takamizo
Categories: cs.CL cs.AI cs.HC
Comments: Accepted as a conference paper at IWSDS 2024
\\
  Mental health care poses an increasingly serious challenge to modern
societies. In this context, there has been a surge in research that utilizes
information technologies to address mental health problems, including those
aiming to develop counseling dialogue systems. However, there is a need for
more evaluations of the performance of counseling dialogue systems that use
large language models. For this study, we collected counseling dialogue data
via role-playing scenarios involving expert counselors, and the utterances were
annotated with the intentions of the counselors. To determine the feasibility
of a dialogue system in real-world counseling scenarios, third-party counselors
evaluated the appropriateness of responses from human counselors and those
generated by GPT-4 in identical contexts in role-play dialogue data. Analysis
of the evaluation results showed that the responses generated by GPT-4 were
competitive with those of human counselors.
\\ ( https://arxiv.org/abs/2402.12738 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12749
Date: Tue, 20 Feb 2024 06:37:31 GMT   (793kb)

Title: Me LLaMA: Foundation Large Language Models for Medical Applications
Authors: Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin,
  Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Huan He, Lucila
  Ohno-Machido, Yonghui Wu, Hua Xu, Jiang Bian
Categories: cs.CL cs.AI
Comments: 18 pages, 5 figures, 7 tables
\\
  Recent large language models (LLMs) like ChatGPT and LLaMA have shown great
promise in many AI applications. However, their performance on medical tasks is
suboptimal and can be further improved by training on large domain-specific
datasets. This study introduces Me LLaMA, a medical LLM family including
foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA
13/70B-chat, developed through the continual pre-training and instruction
tuning of LLaMA2 using large medical data. Our domain-specific data suite for
training and evaluation, includes a large-scale continual pre-training dataset
with 129B tokens, an instruction tuning dataset with 214k samples, and a
medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our
extensive evaluation using MIBE shows that Me LLaMA models surpass existing
open-source medical LLMs in zero-shot and few-shot learning and outperform
commercial giants like ChatGPT on 6 out of 8 datasets and GPT-4 in 3 out of 8
datasets. In addition, we empirically investigated the catastrophic forgetting
problem, and our results show that Me LLaMA models outperform other medical
LLMs. Me LLaMA is one of the first and largest open-source foundational LLMs
designed for the medical domain, using both biomedical and clinical data. It
exhibits superior performance across both general and medical tasks compared to
other medical LLMs, rendering it an attractive choice for medical AI
applications. All resources are available at:
https://github.com/BIDS-Xu-Lab/Me-LLaMA.
\\ ( https://arxiv.org/abs/2402.12749 ,  793kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12770
Date: Tue, 20 Feb 2024 07:20:03 GMT   (390kb,D)

Title: Acknowledgment of Emotional States: Generating Validating Responses for
  Empathetic Dialogue
Authors: Zi Haur Pang, Yahui Fu, Divesh Lala, Keiko Ochi, Koji Inoue, Tatsuya
  Kawahara
Categories: cs.CL
Comments: This paper has been accepted for presentation at International
  Workshop on Spoken Dialogue Systems Technology 2024 (IWSDS 2024)
\\
  In the realm of human-AI dialogue, the facilitation of empathetic responses
is important. Validation is one of the key communication techniques in
psychology, which entails recognizing, understanding, and acknowledging others'
emotional states, thoughts, and actions. This study introduces the first
framework designed to engender empathetic dialogue with validating responses.
Our approach incorporates a tripartite module system: 1) validation timing
detection, 2) users' emotional state identification, and 3) validating response
generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based
dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of
emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms
both random baseline and the ChatGPT performance, in term of F1-score, in all
modules. Further validation of our model's efficacy is confirmed in its
application to the TUT Emotional Storytelling Corpus (TESC), a speech-based
dialogue dataset, by surpassing both random baseline and the ChatGPT. This
consistent performance across both textual and speech-based dialogues
underscores the effectiveness of our framework in fostering empathetic human-AI
communication.
\\ ( https://arxiv.org/abs/2402.12770 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12786
Date: Tue, 20 Feb 2024 07:51:43 GMT   (1889kb,D)

Title: Advancing Large Language Models to Capture Varied Speaking Styles and
  Respond Properly in Spoken Conversations
Authors: Guan-Ting Lin, Cheng-Han Chiang, Hung-yi Lee
Categories: cs.CL eess.AS
\\
  In spoken dialogue, even if two current turns are the same sentence, their
responses might still differ when they are spoken in different styles. The
spoken styles, containing paralinguistic and prosodic information, mark the
most significant difference between text and speech modality. When using
text-only LLMs to model spoken dialogue, text-only LLMs cannot give different
responses based on the speaking style of the current turn. In this paper, we
focus on enabling LLMs to listen to the speaking styles and respond properly.
Our goal is to teach the LLM that "even if the sentences are identical if they
are spoken in different styles, their corresponding responses might be
different". Since there is no suitable dataset for achieving this goal, we
collect a speech-to-speech dataset, StyleTalk, with the following desired
characteristics: when two current speeches have the same content but are spoken
in different styles, their responses will be different. To teach LLMs to
understand and respond properly to the speaking styles, we propose the
Spoken-LLM framework that can model the linguistic content and the speaking
styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage
training pipeline to help the Spoken-LLM better learn the speaking styles.
Based on extensive experiments, we show that Spoken-LLM outperforms text-only
baselines and prior speech LLMs methods.
\\ ( https://arxiv.org/abs/2402.12786 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12801
Date: Tue, 20 Feb 2024 08:20:49 GMT   (993kb,D)

Title: Few shot clinical entity recognition in three languages: Masked language
  models outperform LLM prompting
Authors: Marco Naguib, Xavier Tannier, Aur\'elie N\'ev\'eol
Categories: cs.CL
Comments: Submitted to Journal of Artificial Intelligence in Medicine
\\
  Large Language Models are becoming the go-to solution for many natural
language processing tasks, including in specialized domains where their
few-shot capacities are expected to yield high performance in low-resource
settings. Herein, we aim to assess the performance of Large Language Models for
few shot clinical entity recognition in multiple languages. We evaluate named
entity recognition in English, French and Spanish using 8 in-domain (clinical)
and 6 out-domain gold standard corpora. We assess the performance of 10
auto-regressive language models using prompting and 16 masked language models
used for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot
set-up by limiting the amount of annotated data available to 100 sentences. Our
experiments show that although larger prompt-based models tend to achieve
competitive F-measure for named entity recognition outside the clinical domain,
this level of performance does not carry over to the clinical domain where
lighter supervised taggers relying on masked language models perform better,
even with the performance drop incurred from the few-shot set-up. In all
experiments, the CO2 impact of masked language models is inferior to that of
auto-regressive models. Results are consistent over the three languages and
suggest that few-shot learning using Large language models is not production
ready for named entity recognition in the clinical domain. Instead, models
could be used for speeding-up the production of gold standard annotated data.
\\ ( https://arxiv.org/abs/2402.12801 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12806
Date: Tue, 20 Feb 2024 08:27:05 GMT   (8199kb,D)

Title: SymBa: Symbolic Backward Chaining for Multi-step Natural Language
  Reasoning
Authors: Jinu Lee, Wonseok Hwang
Categories: cs.CL
Comments: 22 pages (8 pages for main text),9 figures
\\
  Large Language Models (LLMs) have recently demonstrated remarkable reasoning
ability as in Chain-of-thought prompting, but faithful multi-step reasoning
remains a challenge. We specifically focus on backward chaining, where the
query is recursively decomposed using logical rules until proven. To address
the limitations of current backward chaining implementations, we propose SymBa
(Symbolic Backward Chaining). In SymBa, the symbolic top-down solver controls
the entire proof process and the LLM is called to generate a single reasoning
step only when the solver encounters a dead end. By this novel solver-LLM
integration, while being able to produce an interpretable, structured proof,
SymBa achieves significant improvement in performance, proof faithfulness, and
efficiency in diverse multi-step reasoning benchmarks (ProofWriter,
Birds-Electricity, GSM8k, CLUTRR-TF, ECtHR Article 6) compared to backward
chaining baselines.
\\ ( https://arxiv.org/abs/2402.12806 ,  8199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12817
Date: Tue, 20 Feb 2024 08:38:19 GMT   (8590kb,D)

Title: On Sensitivity of Learning with Limited Labelled Data to the Effects of
  Randomness: Impact of Interactions and Systematic Choices
Authors: Branislav Pecher, Ivan Srba, Maria Bielikova
Categories: cs.CL cs.AI cs.LG
\\
  While learning with limited labelled data can improve performance when the
labels are lacking, it is also sensitive to the effects of uncontrolled
randomness introduced by so-called randomness factors (e.g., varying order of
data). We propose a method to systematically investigate the effects of
randomness factors while taking the interactions between them into
consideration. To measure the true effects of an individual randomness factor,
our method mitigates the effects of other factors and observes how the
performance varies across multiple runs. Applying our method to multiple
randomness factors across in-context learning and fine-tuning approaches on 7
representative text classification tasks and meta-learning on 3 tasks, we show
that: 1) disregarding interactions between randomness factors in existing works
caused inconsistent findings due to incorrect attribution of the effects of
randomness factors, such as disproving the consistent sensitivity of in-context
learning to sample order even with random sample selection; and 2) besides
mutual interactions, the effects of randomness factors, especially sample
order, are also dependent on more systematic choices unexplored in existing
works, such as number of classes, samples per class or choice of prompt format.
\\ ( https://arxiv.org/abs/2402.12817 ,  8590kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12819
Date: Tue, 20 Feb 2024 08:38:24 GMT   (9705kb,D)

Title: Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How
  Many Labelled Samples Do We Need?
Authors: Branislav Pecher, Ivan Srba, Maria Bielikova
Categories: cs.CL cs.AI cs.LG
\\
  When solving a task with limited labelled data, researchers can either use a
general large language model without further update, or use the few examples to
tune a specialised smaller model. When enough labels are available, the
specialised models outperform the general ones on many NLP tasks. In this work,
we aim to investigate how many labelled samples are required for the
specialised models to achieve this superior performance, while taking the
results variance into consideration. Observing the behaviour of prompting,
in-context learning, fine-tuning and instruction-tuning, identifying their
break-even points when increasing number of labelled training samples across
three tasks of varying complexity, we find that the specialised models often
need only few samples ($100-1000$) to be on par or better than the general
ones. At the same time, the amount of required labelled data strongly depends
on the task complexity and results variance.
\\ ( https://arxiv.org/abs/2402.12819 ,  9705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12821
Date: Tue, 20 Feb 2024 08:41:23 GMT   (517kb,D)

Title: Identifying Factual Inconsistency in Summaries: Towards Effective
  Utilization of Large Language Model
Authors: Liyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, Fei Liu
Categories: cs.CL cs.LG
\\
  Factual inconsistency poses a significant hurdle for the commercial
deployment of abstractive summarizers. Under this Large Language Model (LLM)
era, this work focuses around two important questions: what is the best way to
leverage LLM for factual inconsistency detection, and how could we distill a
smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms
are firstly proposed and evaluated across five diverse datasets: direct
inference on the entire summary or each summary window; entity verification
through question generation and answering. Experiments suggest that LLM itself
is capable to resolve this task train-free under the proper paradigm design,
surpassing strong trained baselines by 2.8% on average. To further promote
practical utility, we then propose training strategies aimed at distilling
smaller open-source LLM that learns to score the entire summary at once with
high accuracy, which outperforms the zero-shot approaches by much larger LLM,
serving as an effective and efficient ready-to-use scorer.
\\ ( https://arxiv.org/abs/2402.12821 ,  517kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12835
Date: Tue, 20 Feb 2024 09:02:55 GMT   (1099kb,D)

Title: PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of
  LLMs
Authors: An Liu, Zonghan Yang, Zhenhe Zhang, Qingyuan Hu, Peng Li, Ming Yan, Ji
  Zhang, Fei Huang, Yang Liu
Categories: cs.CL cs.AI
\\
  While Large language models (LLMs) have demonstrated considerable
capabilities across various natural language tasks, they often fall short of
the performance achieved by domain-specific state-of-the-art models. One
potential approach to enhance domain-specific capabilities of LLMs involves
fine-tuning them using corresponding datasets. However, this method can be both
resource and time-intensive, and not applicable to closed-source commercial
LLMs. In this paper, we propose Preference Adaptation for Enhancing
Domain-specific Abilities of LLMs (PANDA), a method designed to augment the
domain-specific capabilities of LLMs by leveraging insights from the response
preference of expert models without requiring fine-tuning. Our experimental
results reveal that PANDA significantly enhances the domain-specific ability of
LLMs on text classification and interactive decision tasks. Moreover, LLM with
PANDA even outperforms the expert model that being learned on 4 tasks of
ScienceWorld. This finding highlights the potential of exploring tuning-free
approaches to achieve weak-to-strong generalization.
\\ ( https://arxiv.org/abs/2402.12835 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12840
Date: Tue, 20 Feb 2024 09:07:41 GMT   (9043kb,D)

Title: ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic
Authors: Fajri Koto and Haonan Li and Sara Shatnawi and Jad Doughman and
  Abdelrahman Boda Sadallah and Aisha Alraeesi and Khalid Almubarak and Zaid
  Alyafeai and Neha Sengupta and Shady Shehata and Nizar Habash and Preslav
  Nakov and Timothy Baldwin
Categories: cs.CL
\\
  The focus of language model evaluation has transitioned towards reasoning and
knowledge-intensive tasks, driven by advancements in pretraining large models.
While state-of-the-art models are partially trained on large Arabic texts,
evaluating their performance in Arabic remains challenging due to the limited
availability of relevant datasets. To bridge this gap, we present ArabicMMLU,
the first multi-task language understanding benchmark for Arabic language,
sourced from school exams across diverse educational levels in different
countries spanning North Africa, the Levant, and the Gulf regions. Our data
comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard
Arabic (MSA), and is carefully constructed by collaborating with native
speakers in the region. Our comprehensive evaluations of 35 models reveal
substantial room for improvement, particularly among the best open-source
models. Notably, BLOOMZ, mT0, LLama2, and Falcon struggle to achieve a score of
50%, while even the top-performing Arabic-centric model only achieves a score
of 62.3%.
\\ ( https://arxiv.org/abs/2402.12840 ,  9043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12842
Date: Tue, 20 Feb 2024 09:10:08 GMT   (7761kb,D)

Title: PromptKD: Distilling Student-Friendly Knowledge for Generative Language
  Models via Prompt Tuning
Authors: Gyeongman Kim, Doohyuk Jang, Eunho Yang
Categories: cs.CL cs.AI cs.LG
\\
  Recent advancements in large language models (LLMs) have raised concerns
about inference costs, increasing the need for research into model compression.
While knowledge distillation (KD) is a prominent method for this, research on
KD for generative language models like LLMs is relatively sparse, and the
approach of distilling student-friendly knowledge, which has shown promising
performance in KD for classification models, remains unexplored in generative
language models. To explore this approach, we propose PromptKD, a simple yet
effective method that utilizes prompt tuning - for the first time in KD - to
enable generative language models to transfer student-friendly knowledge.
Unlike previous works in classification that require fine-tuning the entire
teacher model for extracting student-friendly knowledge, PromptKD achieves
similar effects by adding a small number of prompt tokens and tuning only the
prompt with student guidance. Extensive experiments on instruction-following
datasets using the GPT-2 model family show that PromptKD achieves
state-of-the-art performance while adding only 0.0007% of the teacher's
parameters as prompts. Further analysis suggests that distilling
student-friendly knowledge alleviates exposure bias effectively throughout the
entire training process, leading to performance enhancements.
\\ ( https://arxiv.org/abs/2402.12842 ,  7761kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12847
Date: Tue, 20 Feb 2024 09:20:32 GMT   (755kb,D)

Title: Instruction-tuned Language Models are Better Knowledge Learners
Authors: Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting
  Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer
Categories: cs.CL cs.AI cs.LG
\\
  In order for large language model (LLM)-based assistants to effectively adapt
to evolving information needs, it must be possible to update their factual
knowledge through continued training on new data. The standard recipe for doing
so involves continued pre-training on new documents followed by
instruction-tuning on question-answer (QA) pairs. However, we find that LLMs
trained with this recipe struggle to answer questions, even though the
perplexity of documents is minimized. We found that QA pairs are generally
straightforward, while documents are more complex, weaving many factual
statements together in an intricate manner. Therefore, we hypothesize that it
is beneficial to expose LLMs to QA pairs before continued pre-training on
documents so that the process of encoding knowledge from complex documents
takes into account how this knowledge is accessed through questions. Based on
this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes
on questions prior to training on documents. This contrasts with standard
instruction-tuning, which learns how to extract knowledge after training on
documents. Extensive experiments and ablation studies demonstrate that PIT
significantly enhances the ability of LLMs to absorb knowledge from new
documents, outperforming standard instruction-tuning by 17.8%.
\\ ( https://arxiv.org/abs/2402.12847 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12851
Date: Tue, 20 Feb 2024 09:30:48 GMT   (5178kb,D)

Title: MoELoRA: Contrastive Learning Guided Mixture of Experts on
  Parameter-Efficient Fine-Tuning for Large Language Models
Authors: Tongxu Luo, Jiahe Lei, Fangyu Lei, Weihao Liu, Shizhu He, Jun Zhao and
  Kang Liu
Categories: cs.CL
\\
  Fine-tuning is often necessary to enhance the adaptability of Large Language
Models (LLM) to downstream tasks. Nonetheless, the process of updating billions
of parameters demands significant computational resources and training time,
which poses a substantial obstacle to the widespread application of large-scale
models in various scenarios. To address this issue, Parameter-Efficient
Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research.
However, current PEFT approaches that employ a limited set of global parameters
(such as LoRA, which adds low-rank approximation matrices to all weights) face
challenges in flexibly combining different computational modules in downstream
tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider
LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon
observed in MoE, we propose the utilization of contrastive learning to
encourage experts to learn distinct features. We conducted experiments on 11
tasks in math reasoning and common-sense reasoning benchmarks. With the same
number of parameters, our approach outperforms LoRA significantly. In math
reasoning, MoELoRA achieved an average performance that was 4.2% higher than
LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on
several benchmarks.
\\ ( https://arxiv.org/abs/2402.12851 ,  5178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12862
Date: Tue, 20 Feb 2024 09:53:38 GMT   (2437kb,D)

Title: Handling Ambiguity in Emotion: From Out-of-Domain Detection to
  Distribution Estimation
Authors: Wen Wu, Bo Li, Chao Zhang, Chung-Cheng Chiu, Qiujia Li, Junwen Bai,
  Tara N. Sainath, Philip C. Woodland
Categories: cs.CL
\\
  The subjective perception of emotion leads to inconsistent labels from human
annotators. Typically, utterances lacking majority-agreed labels are excluded
when training an emotion classifier, which cause problems when encountering
ambiguous emotional expressions during testing. This paper investigates three
methods to handle ambiguous emotion. First, we show that incorporating
utterances without majority-agreed labels as an additional class in the
classifier reduces the classification performance of the other emotion classes.
Then, we propose detecting utterances with ambiguous emotions as out-of-domain
samples by quantifying the uncertainty in emotion classification using
evidential deep learning. This approach retains the classification accuracy
while effectively detects ambiguous emotion expressions. Furthermore, to obtain
fine-grained distinctions among ambiguous emotions, we propose representing
emotion as a distribution instead of a single class label. The task is thus
re-framed from classification to distribution estimation where every individual
annotation is taken into account, not just the majority opinion. The evidential
uncertainty measure is extended to quantify the uncertainty in emotion
distribution estimation. Experimental results on the IEMOCAP and CREMA-D
datasets demonstrate the superior capability of the proposed method in terms of
majority class prediction, emotion distribution estimation, and uncertainty
estimation.
\\ ( https://arxiv.org/abs/2402.12862 ,  2437kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12865
Date: Tue, 20 Feb 2024 09:57:08 GMT   (10195kb,D)

Title: Backward Lens: Projecting Language Model Gradients into the Vocabulary
  Space
Authors: Shahar Katz, Yonatan Belinkov, Mor Geva, Lior Wolf
Categories: cs.CL cs.AI cs.LG
\\
  Understanding how Transformer-based Language Models (LMs) learn and recall
information is a key goal of the deep learning community. Recent
interpretability methods project weights and hidden states obtained from the
forward pass to the models' vocabularies, helping to uncover how information
flows within LMs. In this work, we extend this methodology to LMs' backward
pass and gradients. We first prove that a gradient matrix can be cast as a
low-rank linear combination of its forward and backward passes' inputs. We then
develop methods to project these gradients into vocabulary items and explore
the mechanics of how new information is stored in the LMs' neurons.
\\ ( https://arxiv.org/abs/2402.12865 ,  10195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12869
Date: Tue, 20 Feb 2024 10:00:58 GMT   (1903kb,D)

Title: Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based
  Question Answering with Domain Hybrid Data
Authors: Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu
  Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang
Categories: cs.CL
\\
  Augmenting Large Language Models (LLMs) for Question Answering (QA) with
domain specific data has attracted wide attention. However, domain data often
exists in a hybrid format, including text and semi-structured tables, posing
challenges for the seamless integration of information. Table-to-Text
Generation is a promising solution by facilitating the transformation of hybrid
data into a uniformly text-formatted corpus. Although this technique has been
widely studied by the NLP community, there is currently no comparative analysis
on how corpora generated by different table-to-text methods affect the
performance of QA systems. In this paper, we address this research gap in two
steps. First, we innovatively integrate table-to-text generation into the
framework of enhancing LLM-based QA systems with domain hybrid data. Then, we
utilize this framework in real-world industrial data to conduct extensive
experiments on two types of QA systems (DSFT and RAG frameworks) with four
representative methods: Markdown format, Template serialization, TPLM-based
method, and LLM-based method. Based on the experimental results, we draw some
empirical findings and explore the underlying reasons behind the success of
some methods. We hope the findings of this work will provide a valuable
reference for the academic and industrial communities in developing robust QA
systems.
\\ ( https://arxiv.org/abs/2402.12869 ,  1903kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12880
Date: Tue, 20 Feb 2024 10:18:18 GMT   (55kb,D)

Title: Autism Detection in Speech - A Survey
Authors: Nadine Probol and Margot Mieskes
Categories: cs.CL
Comments: Accepted to EACL 2024 Findings
\\
  There has been a range of studies of how autism is displayed in voice,
speech, and language. We analyse studies from the biomedical, as well as the
psychological domain, but also from the NLP domain in order to find linguistic,
prosodic and acoustic cues that could indicate autism. Our survey looks at all
three domains. We define autism and which comorbidities might influence the
correct detection of the disorder. We especially look at observations such as
verbal and semantic fluency, prosodic features, but also disfluencies and
speaking rate. We also show word-based approaches and describe machine learning
and transformer-based approaches both on the audio data as well as the
transcripts. Lastly, we conclude, while there already is a lot of research,
female patients seem to be severely under-researched. Also, most NLP research
focuses on traditional machine learning methods instead of transformers which
could be beneficial in this context. Additionally, we were unable to find
research combining both features from audio and transcripts.
\\ ( https://arxiv.org/abs/2402.12880 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12881
Date: Tue, 20 Feb 2024 10:23:00 GMT   (7803kb,D)

Title: GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object
  Affordances of Language and Vision Models
Authors: Sayantan Adak, Daivik Agrawal, Animesh Mukherjee and Somak Aditya
Categories: cs.CL
\\
  We investigate the knowledge of object affordances in pre-trained language
models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based
large pre-trained language models (PTLM) learn contextual representation from
massive amounts of unlabeled text and are shown to perform impressively in
downstream NLU tasks. In parallel, a growing body of literature shows that
PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and
grounding. To take a first step toward quantifying the effect of grounding (or
lack thereof), we curate a novel and comprehensive dataset of object
affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike
affordance datasets collected in vision and language domains, we annotate
in-the-wild sentences with objects and affordances. Experimental results reveal
that PTLMs exhibit limited reasoning abilities when it comes to uncommon object
affordances. We also observe that pre-trained VLMs do not necessarily capture
object affordances effectively. Through few-shot fine-tuning, we demonstrate
improvement in affordance knowledge in PTLMs and VLMs. Our research contributes
a novel dataset for language grounding tasks, and presents insights into LM
capabilities, advancing the understanding of object affordances. Codes and data
are available at https://github.com/sayantan11995/Affordance
\\ ( https://arxiv.org/abs/2402.12881 ,  7803kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12890
Date: Tue, 20 Feb 2024 10:34:19 GMT   (38kb,D)

Title: More Discriminative Sentence Embeddings via Semantic Graph Smoothing
Authors: Chakib Fettal, Lazhar Labiod, Mohamed Nadif
Categories: cs.CL cs.LG
Comments: Accepted in EACL 2024
\\
  This paper explores an empirical approach to learn more discriminantive
sentence representations in an unsupervised fashion. Leveraging semantic graph
smoothing, we enhance sentence embeddings obtained from pretrained models to
improve results for the text clustering and classification tasks. Our method,
validated on eight benchmarks, demonstrates consistent improvements, showcasing
the potential of semantic graph smoothing in improving sentence embeddings for
the supervised and unsupervised document categorization tasks.
\\ ( https://arxiv.org/abs/2402.12890 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12913
Date: Tue, 20 Feb 2024 11:01:39 GMT   (2462kb,D)

Title: OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination
  Detection with Weakly Supervised Data
Authors: Chengcheng Wei, Ze Chen, Songtan Fang, Jiarong He, Max Gao
Categories: cs.CL
\\
  This paper mainly describes a unified system for hallucination detection of
LLMs, which wins the second prize in the model-agnostic track of the
SemEval-2024 Task 6, and also achieves considerable results in the model-aware
track. This task aims to detect hallucination with LLMs for three different
text-generation tasks without labeled training data. We utilize prompt
engineering and few-shot learning to verify the performance of different LLMs
on the validation data. Then we select the LLMs with better performance to
generate high-quality weakly supervised training data, which not only satisfies
the consistency of different LLMs, but also satisfies the consistency of the
optimal LLM with different sampling parameters. Furthermore, we finetune
different LLMs by using the constructed training data, and finding that a
relatively small LLM can achieve a competitive level of performance in
hallucination detection, when compared to the large LLMs and the prompt-based
approaches using GPT-4.
\\ ( https://arxiv.org/abs/2402.12913 ,  2462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12914
Date: Tue, 20 Feb 2024 11:03:36 GMT   (3311kb,D)

Title: Large Language Model-based Human-Agent Collaboration for Complex Task
  Solving
Authors: Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan
  Liu, Ji-Rong Wen
Categories: cs.CL cs.HC
\\
  In recent developments within the research community, the integration of
Large Language Models (LLMs) in creating fully autonomous agents has garnered
significant interest. Despite this, LLM-based agents frequently demonstrate
notable shortcomings in adjusting to dynamic environments and fully grasping
human needs. In this work, we introduce the problem of LLM-based human-agent
collaboration for complex task-solving, exploring their synergistic potential.
In addition, we propose a Reinforcement Learning-based Human-Agent
Collaboration method, ReHAC. This approach includes a policy model designed to
determine the most opportune stages for human intervention within the
task-solving process. We construct a human-agent collaboration dataset to train
this policy model in an offline reinforcement learning environment. Our
validation tests confirm the model's effectiveness. The results demonstrate
that the synergistic efforts of humans and LLM-based agents significantly
improve performance in complex tasks, primarily through well-planned, limited
human intervention. Datasets and code are available at:
https://github.com/XueyangFeng/ReHAC.
\\ ( https://arxiv.org/abs/2402.12914 ,  3311kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12940
Date: Tue, 20 Feb 2024 11:52:29 GMT   (366kb)

Title: Normalized Orthography for Tunisian Arabic
Authors: Houcemeddine Turki, Kawthar Ellouze, Hager Ben Ammar, Mohamed Ali Hadj
  Taieb, Imed Adel, Mohamed Ben Aouicha, Pier Luigi Farri, Abderrezak Bennour
Categories: cs.CL
Comments: Final Report for the Derja Association
\\
  Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to
Tunisia, initially stemmed from the Arabic language and enriched by a multitude
of historical influences. This research introduces the "Normalized Orthography
for Tunisian Arabic" (NOTA), an adaptation of CODA* guidelines tailored for
transcribing Tunisian Arabic using the Arabic script for language resource
development purposes, with an emphasis on user-friendliness and consistency.
The updated standard seeks to address challenges related to accurately
representing the unique characteristics of Tunisian phonology and morphology.
This will be achieved by rectifying problems arising from transcriptions based
on resemblances to Modern Standard Arabic.
\\ ( https://arxiv.org/abs/2402.12940 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12948
Date: Tue, 20 Feb 2024 12:05:47 GMT   (8228kb,D)

Title: GumbelSoft: Diversified Language Model Watermarking via the
  GumbelMax-trick
Authors: Jiayi Fu, Xuandong Zhao, Ruihan Yang, Yuansen Zhang, Jiangjie Chen,
  Yanghua Xiao
Categories: cs.CL
\\
  Large language models (LLMs) excellently generate human-like text, but also
raise concerns about misuse in fake news and academic dishonesty.
Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM
watermark), is a standout solution for safeguarding machine-generated texts due
to its notable detectability. However, GM watermark encounters a major
challenge with generation diversity, always yielding identical outputs for the
same prompt, negatively impacting generation diversity and user experience. To
overcome this limitation, we propose a new type of GM watermark, the
Logits-Addition watermark, and its three variants, specifically designed to
enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of
the Logits-Addition watermark) demonstrates superior performance in high
diversity settings, with its AUROC score outperforming those of the two
alternative variants by 0.1 to 0.3 and surpassing other decoding-based
watermarking methods by a minimum of 0.1.
\\ ( https://arxiv.org/abs/2402.12948 ,  8228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12969
Date: Tue, 20 Feb 2024 12:36:40 GMT   (7079kb,D)

Title: Gl\'orIA - A Generative and Open Large Language Model for Portuguese
Authors: Ricardo Lopes and Jo\~ao Magalh\~aes and David Semedo
Categories: cs.CL cs.AI
Comments: Accepted for publication at PROPOR 2024
\\
  Significant strides have been made in natural language tasks, largely
attributed to the emergence of powerful large language models (LLMs). These
models, pre-trained on extensive and diverse corpora, have become increasingly
capable of comprehending the intricacies of language. Despite the abundance of
LLMs for many high-resource languages, the availability of such models remains
limited for European Portuguese. We introduce Gl\'orIA, a robust European
Portuguese decoder LLM. To pre-train Gl\'orIA, we assembled a comprehensive
PT-PT text corpus comprising 35 billion tokens from various sources. We present
our pre-training methodology, followed by an assessment of the model's
effectiveness on multiple downstream tasks. Additionally, to evaluate our
models' language modeling capabilities, we introduce CALAME-PT (Context-Aware
LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot
language-modeling benchmark. Evaluation shows that Gl\'orIA significantly
outperforms existing open PT decoder models in language modeling and that it
can generate sound, knowledge-rich, and coherent PT-PT text. The model also
exhibits strong potential for various downstream tasks.
\\ ( https://arxiv.org/abs/2402.12969 ,  7079kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12976
Date: Tue, 20 Feb 2024 12:53:31 GMT   (3025kb,D)

Title: The Impact of Demonstrations on Multilingual In-Context Learning: A
  Multidimensional Analysis
Authors: Miaoran Zhang, Vagrant Gautam, Mingyang Wang, Jesujoba O. Alabi,
  Xiaoyu Shen, Dietrich Klakow, Marius Mosbach
Categories: cs.CL cs.AI
\\
  In-context learning is a popular inference strategy where large language
models solve a task using only a few labelled demonstrations without needing
any parameter updates. Compared to work on monolingual (English) in-context
learning, multilingual in-context learning is under-explored, and we lack an
in-depth understanding of the role of demonstrations in this context. To
address this gap, we conduct a multidimensional analysis of multilingual
in-context learning, experimenting with 5 models from different model families,
9 datasets covering classification and generation tasks, and 56 typologically
diverse languages. Our results reveal that the effectiveness of demonstrations
varies significantly across models, tasks, and languages. We also find that
Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of
demonstrations. Instead, a carefully crafted template often eliminates the
benefits of demonstrations for some tasks and languages altogether. These
findings show that the importance of demonstrations might be overestimated. Our
work highlights the need for granular evaluation across multiple axes towards a
better understanding of in-context learning.
\\ ( https://arxiv.org/abs/2402.12976 ,  3025kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12984
Date: Tue, 20 Feb 2024 13:13:13 GMT   (1370kb,D)

Title: Can GNN be Good Adapter for LLMs?
Authors: Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei
  Chai, and Qi Zhu
Categories: cs.CL cs.AI
Comments: Accepted by WWW'24
\\
  Recently, large language models (LLMs) have demonstrated superior
capabilities in understanding and zero-shot learning on textual data, promising
significant advances for many text-related domains. In the graph domain,
various real-world scenarios also involve textual data, where tasks and node
features can be described by text. These text-attributed graphs (TAGs) have
broad applications in social media, recommendation systems, etc. Thus, this
paper explores how to utilize LLMs to model TAGs. Previous methods for TAG
modeling are based on million-scale LMs. When scaled up to billion-scale LLMs,
they face huge challenges in computational costs. Additionally, they also
ignore the zero-shot inference capabilities of LLMs. Therefore, we propose
GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter
in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN
adapter introduces only a few trainable parameters and can be trained with low
computation costs. The entire framework is trained using auto-regression on
node text (next token prediction). Once trained, GraphAdapter can be seamlessly
fine-tuned with task-specific prompts for various downstream tasks. Through
extensive experiments across multiple real-world TAGs, GraphAdapter based on
Llama 2 gains an average improvement of approximately 5\% in terms of node
classification. Furthermore, GraphAdapter can also adapt to other language
models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs
can serve as effective adapters for LLMs in TAG modeling.
\\ ( https://arxiv.org/abs/2402.12984 ,  1370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12998
Date: Tue, 20 Feb 2024 13:25:39 GMT   (717kb)

Title: Phonotactic Complexity across Dialects
Authors: Ryan Soh-Eun Shim, Kalvin Chang, David R. Mortensen
Categories: cs.CL
Comments: Accepted to COLING-LREC 2024
\\
  Received wisdom in linguistic typology holds that if the structure of a
language becomes more complex in one dimension, it will simplify in another,
building on the assumption that all languages are equally complex (Joseph and
Newmeyer, 2012). We study this claim on a micro-level, using a
tightly-controlled sample of Dutch dialects (across 366 collection sites) and
Min dialects (across 60 sites), which enables a more fair comparison across
varieties. Even at the dialect level, we find empirical evidence for a tradeoff
between word length and a computational measure of phonotactic complexity from
a LSTM-based phone-level language model-a result previously documented only at
the language level. A generalized additive model (GAM) shows that dialects with
low phonotactic complexity concentrate around the capital regions, which we
hypothesize to correspond to prior hypotheses that language varieties of
greater or more diverse populations show reduced phonotactic complexity. We
also experiment with incorporating the auxiliary task of predicting syllable
constituency, but do not find an increase in the negative correlation observed.
\\ ( https://arxiv.org/abs/2402.12998 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13013
Date: Tue, 20 Feb 2024 13:56:38 GMT   (7655kb,D)

Title: Code Needs Comments: Enhancing Code LLMs with Comment Augmentation
Authors: Demin Song, Honglin Guo, Yunhua Zhou, Shuhao Xing, Yudong Wang, Zifan
  Song, Wenwei Zhang, Qipeng Guo, Hang Yan, Xipeng Qiu, Dahua Lin
Categories: cs.CL
\\
  The programming skill is one crucial ability for Large Language Models
(LLMs), necessitating a deep understanding of programming languages (PLs) and
their correlation with natural languages (NLs). We examine the impact of
pre-training data on code-focused LLMs' performance by assessing the comment
density as a measure of PL-NL alignment. Given the scarcity of code-comment
aligned data in pre-training corpora, we introduce a novel data augmentation
method that generates comments for existing code, coupled with a data filtering
strategy that filters out code data poorly correlated with natural language. We
conducted experiments on three code-focused LLMs and observed consistent
improvements in performance on two widely-used programming skill benchmarks.
Notably, the model trained on the augmented data outperformed both the model
used for generating comments and the model further trained on the data without
augmentation.
\\ ( https://arxiv.org/abs/2402.13013 ,  7655kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13016
Date: Tue, 20 Feb 2024 13:59:12 GMT   (334kb,D)

Title: Understanding the effects of language-specific class imbalance in
  multilingual fine-tuning
Authors: Vincent Jung, Lonneke van der Plas
Categories: cs.CL
Comments: To be published in: Findings of the Association for Computational
  Linguistics: EACL 2024
\\
  We study the effect of one type of imbalance often present in real-life
multilingual classification datasets: an uneven distribution of labels across
languages. We show evidence that fine-tuning a transformer-based Large Language
Model (LLM) on a dataset with this imbalance leads to worse performance, a more
pronounced separation of languages in the latent space, and the promotion of
uninformative features. We modify the traditional class weighing approach to
imbalance by calculating class weights separately for each language and show
that this helps mitigate those detrimental effects. These results create
awareness of the negative effects of language-specific class imbalance in
multilingual fine-tuning and the way in which the model learns to rely on the
separation of languages to perform the task.
\\ ( https://arxiv.org/abs/2402.13016 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13022
Date: Tue, 20 Feb 2024 14:02:45 GMT   (1283kb,D)

Title: SoMeLVLM: A Large Vision Language Model for Social Media Processing
Authors: Xinnong Zhang, Haoyu Kuang, Xinyi Mou, Hanjia Lyu, Kun Wu, Siming
  Chen, Jiebo Luo, Xuanjing Huang, Zhongyu Wei
Categories: cs.CL cs.MM
\\
  The growth of social media, characterized by its multimodal nature, has led
to the emergence of diverse phenomena and challenges, which calls for an
effective approach to uniformly solve automated tasks. The powerful Large
Vision Language Models make it possible to handle a variety of tasks
simultaneously, but even with carefully designed prompting methods, the general
domain models often fall short in aligning with the unique speaking style and
context of social media tasks. In this paper, we introduce a Large Vision
Language Model for Social Media Processing (SoMeLVLM), which is a cognitive
framework equipped with five key capabilities including knowledge &
comprehension, application, analysis, evaluation, and creation. SoMeLVLM is
designed to understand and generate realistic social media behavior. We have
developed a 654k multimodal social media instruction-tuning dataset to support
our cognitive framework and fine-tune our model. Our experiments demonstrate
that SoMeLVLM achieves state-of-the-art performance in multiple social media
tasks. Further analysis shows its significant advantages over baselines in
terms of cognitive abilities.
\\ ( https://arxiv.org/abs/2402.13022 ,  1283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13025
Date: Tue, 20 Feb 2024 14:08:24 GMT   (400kb,D)

Title: CFEVER: A Chinese Fact Extraction and VERification Dataset
Authors: Ying-Jia Lin, Chun-Yi Lin, Chia-Jen Yeh, Yi-Ting Li, Yun-Yu Hu,
  Chih-Hao Hsu, Mei-Feng Lee, Hung-Yu Kao
Categories: cs.CL cs.AI
Comments: AAAI-24
\\
  We present CFEVER, a Chinese dataset designed for Fact Extraction and
VERification. CFEVER comprises 30,012 manually created claims based on content
in Chinese Wikipedia. Each claim in CFEVER is labeled as "Supports", "Refutes",
or "Not Enough Info" to depict its degree of factualness. Similar to the FEVER
dataset, claims in the "Supports" and "Refutes" categories are also annotated
with corresponding evidence sentences sourced from single or multiple pages in
Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934
for five-way inter-annotator agreement. In addition, through the experiments
with the state-of-the-art approaches developed on the FEVER dataset and a
simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous
benchmark for factual extraction and verification, which can be further used
for developing automated systems to alleviate human fact-checking efforts.
CFEVER is available at https://ikmlab.github.io/CFEVER.
\\ ( https://arxiv.org/abs/2402.13025 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13028
Date: Tue, 20 Feb 2024 14:10:40 GMT   (2141kb,D)

Title: Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables
Authors: Haisong Gong, Weizhi Xu, Shu wu, Qiang Liu, Liang Wang
Categories: cs.CL cs.AI
Comments: Accepted by 38th Association for the Advancement of Artificial
  Intelligence, AAAI
\\
  Fact checking aims to predict claim veracity by reasoning over multiple
evidence pieces. It usually involves evidence retrieval and veracity reasoning.
In this paper, we focus on the latter, reasoning over unstructured text and
structured table information. Previous works have primarily relied on
fine-tuning pretrained language models or training homogeneous-graph-based
models. Despite their effectiveness, we argue that they fail to explore the
rich semantic information underlying the evidence with different structures. To
address this, we propose a novel word-level Heterogeneous-graph-based model for
Fact Checking over unstructured and structured information, namely HeterFC. Our
approach leverages a heterogeneous evidence graph, with words as nodes and
thoughtfully designed edges representing different evidence properties. We
perform information propagation via a relational graph neural network,
facilitating interactions between claims and evidence. An attention-based
method is utilized to integrate information, combined with a language model for
generating predictions. We introduce a multitask loss function to account for
potential inaccuracies in evidence retrieval. Comprehensive experiments on the
large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC.
Code will be released at: https://github.com/Deno-V/HeterFC.
\\ ( https://arxiv.org/abs/2402.13028 ,  2141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13035
Date: Tue, 20 Feb 2024 14:23:23 GMT   (170kb,D)

Title: Learning to Check: Unleashing Potentials for Self-Correction in Large
  Language Models
Authors: Che Zhang and Zhenyang Xiao and Chengcheng Han and Yixin Lian and
  Yuejian Fang
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have made significant strides in reasoning
capabilities, with ongoing efforts to refine their reasoning through
self-correction. However, recent studies suggest that self-correction can be
limited or even counterproductive without external accurate knowledge, raising
questions about the limits and effectiveness of self-correction. In this paper,
we aim to enhance LLM's self-checking capabilities by meticulously designing
training data, thereby improving the accuracy of self-correction. We conduct a
detailed analysis of error types in mathematical reasoning and develop a
tailored prompt, termed ``Step CoT Check''. Then we construct a
checking-correction dataset for training models. After integrating the original
CoT data and checking-correction data for training, we observe that models
could improve their self-checking capabilities, thereby enhancing their
self-correction capacity and eliminating the need for external feedback or
ground truth labels to ascertain the endpoint of correction. We compare the
performance of models fine-tuned with the ``Step CoT Check'' prompt against
those refined using other promps within the context of checking-correction
data. The ``Step CoT Check'' outperforms the other two check formats in model
with lager parameters, providing more precise feedback thus achieving a higher
rate of correctness. For reproducibility, all the datasets and codes are
provided in \url{https://github.com/bammt/Learn-to-check}.
\\ ( https://arxiv.org/abs/2402.13035 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13036
Date: Tue, 20 Feb 2024 14:23:34 GMT   (7893kb,D)

Title: SiLLM: Large Language Models for Simultaneous Machine Translation
Authors: Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Min Zhang, Yang Feng
Categories: cs.CL
Comments: 13 pages, 6 tables, 7 figures
\\
  Simultaneous Machine Translation (SiMT) generates translations while reading
the source sentence, necessitating a policy to determine the optimal timing for
reading and generating words. Despite the remarkable performance achieved by
Large Language Models (LLM) across various NLP tasks, existing SiMT methods
predominantly focus on conventional transformers, employing a single model to
concurrently determine the policy and generate the translations. However, given
the complexity of SiMT, it is challenging to effectively address both tasks
with a single model. Therefore, there is a need to decouple the SiMT task into
policy-decision and translation sub-tasks. We propose SiLLM, which delegates
the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The
policy-decision agent is managed by a conventional SiMT model, responsible for
determining the translation policy. The translation agent, leveraging the
capabilities of LLM, generates translation using the partial source sentence.
The two agents collaborate to accomplish SiMT. To facilitate the application of
token-level policies determined by conventional SiMT models to LLM, we propose
a word-level policy adapted for LLM. Experiments on two datasets demonstrate
that, with a small amount of data for fine-tuning LLM, SiLLM attains
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2402.13036 ,  7893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13043
Date: Tue, 20 Feb 2024 14:31:17 GMT   (9027kb,D)

Title: Effective and Efficient Conversation Retrieval for Dialogue State
  Tracking with Implicit Text Summaries
Authors: Seanie Lee, Jianpeng Chen, Joris Driesen, Alexandru Coca, Anders
  Johannsen
Categories: cs.CL
Comments: Preprint
\\
  Few-shot dialogue state tracking (DST) with Large Language Models (LLM)
relies on an effective and efficient conversation retriever to find similar
in-context examples for prompt learning. Previous works use raw dialogue
context as search keys and queries, and a retriever is fine-tuned with
annotated dialogues to achieve superior performance. However, the approach is
less suited for scaling to new domains or new annotation languages, where
fine-tuning data is unavailable. To address this problem, we handle the task of
conversation retrieval based on text summaries of the conversations. A
LLM-based conversation summarizer is adopted for query and key generation,
which enables effective maximum inner product search. To avoid the extra
inference cost brought by LLM-based conversation summarization, we further
distill a light-weight conversation encoder which produces query embeddings
without decoding summaries for test conversations. We validate our retrieval
approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The
experimental results show a significant improvement over relevant baselines in
real few-shot DST settings.
\\ ( https://arxiv.org/abs/2402.13043 ,  9027kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13048
Date: Tue, 20 Feb 2024 14:36:23 GMT   (7125kb,D)

Title: Stable Knowledge Editing in Large Language Models
Authors: Zihao Wei, Liang Pang, Hanxing Ding, Jingcheng Deng, Huawei Shen,
  Xueqi Cheng
Categories: cs.CL
\\
  Efficient knowledge editing of large language models is crucial for replacing
obsolete information or incorporating specialized knowledge on a large scale.
However, previous methods implicitly assume that knowledge is localized and
isolated within the model, an assumption that oversimplifies the interconnected
nature of model knowledge. The premise of localization results in an incomplete
knowledge editing, whereas an isolated assumption may impair both other
knowledge and general abilities. It introduces instability to the performance
of the knowledge editing method. To transcend these assumptions, we introduce
StableKE, a method adopts a novel perspective based on knowledge augmentation
rather than knowledge localization. To overcome the expense of human labeling,
StableKE integrates two automated knowledge augmentation strategies: Semantic
Paraphrase Enhancement strategy, which diversifies knowledge descriptions to
facilitate the teaching of new information to the model, and Contextual
Description Enrichment strategy, expanding the surrounding knowledge to prevent
the forgetting of related information. StableKE surpasses other knowledge
editing methods, demonstrating stability both edited knowledge and multi-hop
knowledge, while also preserving unrelated knowledge and general abilities.
Moreover, StableKE can edit knowledge on ChatGPT.
\\ ( https://arxiv.org/abs/2402.13048 ,  7125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13055
Date: Tue, 20 Feb 2024 14:43:39 GMT   (8284kb,D)

Title: Identifying Semantic Induction Heads to Understand In-Context Learning
Authors: Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Xipeng Qiu, Dahua Lin
Categories: cs.CL cs.AI
\\
  Although large language models (LLMs) have demonstrated remarkable
performance, the lack of transparency in their inference logic raises concerns
about their trustworthiness. To gain a better understanding of LLMs, we conduct
a detailed analysis of the operations of attention heads and aim to better
understand the in-context learning of LLMs. Specifically, we investigate
whether attention heads encode two types of relationships between tokens
present in natural languages: the syntactic dependency parsed from sentences
and the relation within knowledge graphs. We find that certain attention heads
exhibit a pattern where, when attending to head tokens, they recall tail tokens
and increase the output logits of those tail tokens. More crucially, the
formulation of such semantic induction heads has a close correlation with the
emergence of the in-context learning ability of language models. The study of
semantic attention heads advances our understanding of the intricate operations
of attention heads in transformers, and further provides new insights into the
in-context learning of LLMs.
\\ ( https://arxiv.org/abs/2402.13055 ,  8284kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13064
Date: Tue, 20 Feb 2024 15:00:35 GMT   (170kb,D)

Title: Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for
  Language Models
Authors: Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang,
  Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang,
  Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui,
  Benyou Wang, Wai Lam, Furu Wei
Categories: cs.CL
Comments: Work in progress
\\
  We introduce Generalized Instruction Tuning (called GLAN), a general and
scalable method for instruction tuning of Large Language Models (LLMs). Unlike
prior work that relies on seed examples or existing datasets to construct
instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of
human knowledge and capabilities as input and generates large-scale synthetic
instruction data across all disciplines. Specifically, inspired by the
systematic structure in human education system, we build the taxonomy by
decomposing human knowledge and capabilities to various fields, sub-fields and
ultimately, distinct disciplines semi-automatically, facilitated by LLMs.
Subsequently, we generate a comprehensive list of subjects for every discipline
and proceed to design a syllabus tailored to each subject, again utilizing
LLMs. With the fine-grained key concepts detailed in every class session of the
syllabus, we are able to generate diverse instructions with a broad coverage
across the entire spectrum of human knowledge and skills. Extensive experiments
on large language models (e.g., Mistral) demonstrate that GLAN excels in
multiple dimensions from mathematical reasoning, coding, academic exams,
logical reasoning to general instruction following without using task-specific
training data of these tasks. In addition, GLAN allows for easy customization
and new fields or skills can be added by simply incorporating a new node into
our taxonomy.
\\ ( https://arxiv.org/abs/2402.13064 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13093
Date: Tue, 20 Feb 2024 15:36:41 GMT   (213kb,D)

Title: Event-level Knowledge Editing
Authors: Hao Peng, Xiaozhi Wang, Chunyang Li, Kaisheng Zeng, Jiangshan Duo,
  Yixin Cao, Lei Hou, Juanzi Li
Categories: cs.CL cs.AI
Comments: 17 pages, 2 figures
\\
  Knowledge editing aims at updating knowledge of large language models (LLMs)
to prevent them from becoming outdated. Existing work edits LLMs at the level
of factual knowledge triplets. However, natural knowledge updates in the real
world come from the occurrences of new events rather than direct changes in
factual triplets. In this paper, we propose a new task setting: event-level
knowledge editing, which directly edits new events into LLMs and improves over
conventional triplet-level editing on (1) Efficiency. A single event edit leads
to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond
updating factual knowledge, event-level editing also requires considering the
event influences and updating LLMs' knowledge about future trends. We construct
a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event
edits, 6,449 questions about factual knowledge, and 10,150 questions about
future tendencies. We systematically evaluate the performance of various
knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses
significant challenges to existing knowledge editing approaches. Our codes and
dataset are publicly released to facilitate further research.
\\ ( https://arxiv.org/abs/2402.13093 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13094
Date: Tue, 20 Feb 2024 15:37:08 GMT   (492kb,D)

Title: Digital Comprehensibility Assessment of Simplified Texts among Persons
  with Intellectual Disabilities
Authors: Andreas S\"auberli, Franz Holzknecht, Patrick Haller, Silvana Deilen,
  Laura Schiffl, Silvia Hansen-Schirra, Sarah Ebling
Categories: cs.CL cs.HC
Comments: Accepted for publication at the 2024 ACM Conference on Human Factors
  in Computing Systems (CHI'24)
\\
  Text simplification refers to the process of increasing the comprehensibility
of texts. Automatic text simplification models are most commonly evaluated by
experts or crowdworkers instead of the primary target groups of simplified
texts, such as persons with intellectual disabilities. We conducted an
evaluation study of text comprehensibility including participants with and
without intellectual disabilities reading unsimplified, automatically and
manually simplified German texts on a tablet computer. We explored four
different approaches to measuring comprehensibility: multiple-choice
comprehension questions, perceived difficulty ratings, response time, and
reading speed. The results revealed significant variations in these
measurements, depending on the reader group and whether the text had undergone
automatic or manual simplification. For the target group of persons with
intellectual disabilities, comprehension questions emerged as the most reliable
measure, while analyzing reading speed provided valuable insights into
participants' reading behavior.
\\ ( https://arxiv.org/abs/2402.13094 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13098
Date: Tue, 20 Feb 2024 15:47:59 GMT   (9258kb,D)

Title: ELAD: Explanation-Guided Large Language Models Active Distillation
Authors: Yifei Zhang, Bo Pan, Chen Ling, Yuntong Hu, Liang Zhao
Categories: cs.CL cs.AI
\\
  The deployment and application of Large Language Models (LLMs) is hindered by
their memory inefficiency, computational demands, and the high costs of API
inferences. Traditional distillation methods, which transfer the capabilities
of LLMs to smaller models, often fail to determine whether the knowledge has
been sufficiently transferred, potentially resulting in high costs or
incomplete distillation. In this paper, we propose an Explanation-Guided LLMs
Active Distillation (ELAD) framework that employs an active learning strategy
to optimize the balance between annotation costs and model performance. To
improve efficient sample selection, we introduce an explanation-guided sample
selection method that identifies samples challenging its reasoning by
exploiting uncertainties in explanation steps. Additionally, we present a
customized LLM-annotated explanation revision technique where the teacher model
detects and corrects flaws in the student model's reasoning. Our experiments
across various reasoning datasets demonstrate that our framework significantly
enhances the efficiency of LLM knowledge distillation.
\\ ( https://arxiv.org/abs/2402.13098 ,  9258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13109
Date: Tue, 20 Feb 2024 16:02:12 GMT   (237kb,D)

Title: CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the
  Generalizability of Large Language Models
Authors: Yizhi LI, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao
  Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei
  Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Stephen W. Huang, Chenghua Lin, Wenhu
  Chen, Jie Fu
Categories: cs.CL cs.AI
\\
  The advancement of large language models (LLMs) has enhanced the ability to
generalize across a wide range of unseen natural language processing (NLP)
tasks through instruction-following. Yet, their effectiveness often diminishes
in low-resource languages like Chinese, exacerbated by biased evaluations from
data leakage, casting doubt on their true generalizability to new linguistic
territories. In response, we introduce the Chinese Instruction-Following
Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of
LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000
input-output pairs, developed by native speakers to test complex reasoning and
Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we
release only half of the dataset publicly, with the remainder kept private, and
introduce diversified instructions to minimize score variance, totaling 45,000
data instances. Our evaluation of 28 selected LLMs reveals a noticeable
performance gap, with the best model scoring only 52.9%, highlighting the
limitations of LLMs in less familiar language and task contexts. This work aims
to uncover the current limitations of LLMs in handling Chinese tasks, pushing
towards the development of more culturally informed and linguistically diverse
models with the released data and benchmark
(https://yizhilll.github.io/CIF-Bench/).
\\ ( https://arxiv.org/abs/2402.13109 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13113
Date: Tue, 20 Feb 2024 16:09:49 GMT   (524kb,D)

Title: When Only Time Will Tell: Interpreting How Transformers Process Local
  Ambiguities Through the Lens of Restart-Incrementality
Authors: Brielen Madureira, Patrick Kahardipraja, David Schlangen
Categories: cs.CL
Comments: work in progress
\\
  Incremental models that process sentences one token at a time will sometimes
encounter points where more than one interpretation is possible. Causal models
are forced to output one interpretation and continue, whereas models that can
revise may edit their previous output as the ambiguity is resolved. In this
work, we look at how restart-incremental Transformers build and update internal
states, in an effort to shed light on what processes cause revisions not viable
in autoregressive models. We propose an interpretable way to analyse the
incremental states, showing that their sequential structure encodes information
on the garden path effect and its resolution. Our method brings insights on
various bidirectional encoders for contextualised meaning representation and
dependency parsing, contributing to show their advantage over causal models
when it comes to revisions.
\\ ( https://arxiv.org/abs/2402.13113 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13116
Date: Tue, 20 Feb 2024 16:17:37 GMT   (661kb,D)

Title: A Survey on Knowledge Distillation of Large Language Models
Authors: Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang
  Li, Can Xu, Dacheng Tao, Tianyi Zhou
Categories: cs.CL
Comments: 43 pages
\\
  This survey presents an in-depth exploration of knowledge distillation (KD)
techniques within the realm of Large Language Models (LLMs), spotlighting the
pivotal role of KD in transferring sophisticated capabilities from proprietary
giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral.
Amidst the evolving AI landscape, this work elucidates the critical disparities
between proprietary and open-source LLMs, demonstrating how KD serves as an
essential conduit for imbuing the latter with the former's advanced
functionalities and nuanced understandings. Our survey is meticulously
structured around three foundational pillars: algorithm, skill, and
verticalization -- providing a comprehensive examination of KD mechanisms, the
enhancement of specific cognitive abilities, and their practical implications
across diverse fields. Crucially, the survey navigates the intricate interplay
between data augmentation (DA) and KD, illustrating how DA emerges as a
powerful paradigm within the KD framework to bolster LLMs' performance. By
leveraging DA to generate context-rich, skill-specific training data, KD
transcends traditional boundaries, enabling open-source models to approximate
the contextual adeptness, ethical alignment, and deep semantic insights
characteristic of their proprietary counterparts. This work aims to provide an
insightful guide for researchers and practitioners, offering a detailed
overview of current methodologies in knowledge distillation and proposing
future research directions. By bridging the gap between proprietary and
open-source LLMs, this survey underscores the potential for more accessible,
efficient, and sustainable AI solutions, fostering a more inclusive and
equitable landscape in AI advancements. An associated Github repository is
available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.
\\ ( https://arxiv.org/abs/2402.13116 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13125
Date: Tue, 20 Feb 2024 16:38:33 GMT   (1499kb,D)

Title: TreeEval: Benchmark-Free Evaluation of Large Language Models through
  Tree Planning
Authors: Xiang Li, Yunshi Lan and Chao Yang
Categories: cs.CL cs.AI
\\
  Recently, numerous new benchmarks have been established to evaluate the
performance of large language models (LLMs) via either computing a holistic
score or employing another LLM as a judge. However, these approaches suffer
from data leakage due to the open access of the benchmark and inflexible
evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a
benchmark-free evaluation method for LLMs that let a high-performance LLM host
an irreproducible evaluation session and essentially avoids the data leakage.
Moreover, this LLM performs as an examiner to raise up a series of questions
under a topic with a tree planing strategy, which considers the current
evaluation status to decide the next question generation and ensures the
completeness and efficiency of the evaluation process. We evaluate $6$ models
of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately
achieved the highest correlation coefficient with AlpacaEval2.0 using only
around $45$ questions. We also conduct more analysis to show the robustness and
reliability of TreeEval. Our code can be accessed via the provided
https://github.com/Ashura5/TreeEval.
\\ ( https://arxiv.org/abs/2402.13125 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13130
Date: Tue, 20 Feb 2024 16:43:20 GMT   (7947kb,D)

Title: Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic
  Textual Similarity
Authors: Ivan Rep, David Duki\'c, Jan \v{S}najder
Categories: cs.CL
Comments: 7 pages, 9 figures, 2 tables
\\
  While BERT produces high-quality sentence embeddings, its pre-training
computational cost is a significant drawback. In contrast, ELECTRA delivers a
cost-effective pre-training objective and downstream task performance
improvements, but not as performant sentence embeddings. The community tacitly
stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity
(STS). We notice a significant drop in performance when using the ELECTRA
discriminator's last layer in comparison to earlier layers. We explore this
drop and devise a way to repair ELECTRA's embeddings, proposing a novel
truncated model fine-tuning (TMFT) method. TMFT improves the Spearman
correlation coefficient by over 8 points while increasing parameter efficiency
on the STS benchmark dataset. We extend our analysis to various model sizes and
languages. Further, we discover the surprising efficacy of ELECTRA's generator
model, which performs on par with BERT, using significantly fewer parameters
and a substantially smaller embedding size. Finally, we observe further boosts
by combining TMFT with a word similarity task or domain adaptive pre-training.
\\ ( https://arxiv.org/abs/2402.13130 ,  7947kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13137
Date: Tue, 20 Feb 2024 16:53:26 GMT   (45097kb,D)

Title: The Hidden Space of Transformer Language Adapters
Authors: Jesujoba O. Alabi, Marius Mosbach, Matan Eyal, Dietrich Klakow, Mor
  Geva
Categories: cs.CL
Comments: 18 pages
\\
  We analyze the operation of transformer language adapters, which are small
modules trained on top of a frozen language model to adapt its predictions to
new target languages. We show that adapted predictions mostly evolve in the
source language the model was trained on, while the target language becomes
pronounced only in the very last layers of the model. Moreover, the adaptation
process is gradual and distributed across layers, where it is possible to skip
small groups of adapters without decreasing adaptation performance. Last, we
show that adapters operate on top of the model's frozen representation space
while largely preserving its structure, rather than on an 'isolated' subspace.
Our findings provide a deeper view into the adaptation process of language
models to new languages, showcasing the constraints imposed on it by the
underlying model and introduces practical implications to enhance its
efficiency.
\\ ( https://arxiv.org/abs/2402.13137 ,  45097kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13145
Date: Tue, 20 Feb 2024 17:00:41 GMT   (5743kb,D)

Title: CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for
  Boosting Metaphor Generation
Authors: Yujie Shao, Xinrong Yao, Xingwei Qu, Chenghua Lin, Shi Wang, Stephen
  W. Huang, Ge Zhang, Jie Fu
Categories: cs.CL cs.AI
\\
  Metaphor is a prominent linguistic device in human language and literature,
as they add color, imagery, and emphasis to enhance effective communication.
This paper introduces a large-scale high quality annotated Chinese Metaphor
Corpus, which comprises around 28K sentences drawn from a diverse range of
Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the
accuracy and consistency of our annotations, we introduce a comprehensive set
of guidelines. These guidelines address the facets of metaphor annotation,
including identifying tenors, vehicles, and grounds to handling the
complexities of similes, personifications, juxtapositions, and hyperboles.
Breaking tradition, our approach to metaphor generation emphasizes grounds and
their distinct features rather than the conventional combination of tenors and
vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are
able to generate metaphors that resonate more with real-world intuition. We
test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using
our annotated corpus. These models are able to generate creative and fluent
metaphor sentences more frequently induced by selected samples from our
dataset, demonstrating the value of our corpus for Chinese metaphor research.
The code is available in the
https://anonymous.4open.science/r/Chinese_Metaphor_Explanation-63F2.
\\ ( https://arxiv.org/abs/2402.13145 ,  5743kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13178
Date: Tue, 20 Feb 2024 17:44:06 GMT   (619kb,D)

Title: Benchmarking Retrieval-Augmented Generation for Medicine
Authors: Guangzhi Xiong and Qiao Jin and Zhiyong Lu and Aidong Zhang
Categories: cs.CL cs.AI
Comments: Homepage: https://teddy-xionggz.github.io/benchmark-medical-rag/
\\
  While large language models (LLMs) have achieved state-of-the-art performance
on a wide range of medical question answering (QA) tasks, they still face
challenges with hallucinations and outdated knowledge. Retrieval-augmented
generation (RAG) is a promising solution and has been widely adopted. However,
a RAG system can involve multiple flexible components, and there is a lack of
best practices regarding the optimal RAG setting for various medical purposes.
To systematically evaluate such systems, we propose the Medical Information
Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind
benchmark including 7,663 questions from five medical QA datasets. Using
MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt
tokens on 41 combinations of different corpora, retrievers, and backbone LLMs
through the MedRAG toolkit introduced in this work. Overall, MedRAG improves
the accuracy of six different LLMs by up to 18% over chain-of-thought
prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our
results show that the combination of various medical corpora and retrievers
achieves the best performance. In addition, we discovered a log-linear scaling
property and the "lost-in-the-middle" effects in medical RAG. We believe our
comprehensive evaluations can serve as practical guidelines for implementing
RAG systems for medicine.
\\ ( https://arxiv.org/abs/2402.13178 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13184
Date: Tue, 20 Feb 2024 17:49:46 GMT   (1937kb,D)

Title: What if LLMs Have Different World Views: Simulating Alien Civilizations
  with LLM-based Agents
Authors: Mingyu Jin, Beichen Wang, Zhaoqian Xue, Suiyuan Zhu, Wenyue Hua, Hua
  Tang, Kai Mei, Mengnan Du, Yongfeng Zhang
Categories: cs.CL
\\
  In this study, we introduce "CosmoAgent," an innovative artificial
intelligence framework utilizing Large Language Models (LLMs) to simulate
complex interactions between human and extraterrestrial civilizations, with a
special emphasis on Stephen Hawking's cautionary advice about not sending radio
signals haphazardly into the universe. The goal is to assess the feasibility of
peaceful coexistence while considering potential risks that could threaten
well-intentioned civilizations. Employing mathematical models and state
transition matrices, our approach quantitatively evaluates the development
trajectories of civilizations, offering insights into future decision-making at
critical points of growth and saturation. Furthermore, the paper acknowledges
the vast diversity in potential living conditions across the universe, which
could foster unique cosmologies, ethical codes, and worldviews among various
civilizations. Recognizing the Earth-centric bias inherent in current LLM
designs, we propose the novel concept of using LLMs with diverse ethical
paradigms and simulating interactions between entities with distinct moral
principles. This innovative research provides a new way to understand complex
inter-civilizational dynamics, expanding our perspective while pioneering novel
strategies for conflict resolution, crucial for preventing interstellar
conflicts. We have also released the code and datasets to enable further
academic investigation into this interesting area of research. The code is
available at https://github.com/agiresearch/AlienAgent.
\\ ( https://arxiv.org/abs/2402.13184 ,  1937kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13188
Date: Tue, 20 Feb 2024 17:56:24 GMT   (1737kb,D)

Title: Question Calibration and Multi-Hop Modeling for Temporal Question
  Answering
Authors: Chao Xue, Di Liang, Pengfei Wang, Jing Zhang
Categories: cs.CL
Comments: Accepted by AAAI 2024
\\
  Many models that leverage knowledge graphs (KGs) have recently demonstrated
remarkable success in question answering (QA) tasks. In the real world, many
facts contained in KGs are time-constrained thus temporal KGQA has received
increasing attention. Despite the fruitful efforts of previous models in
temporal KGQA, they still have several limitations. (I) They adopt pre-trained
language models (PLMs) to obtain question representations, while PLMs tend to
focus on entity information and ignore entity transfer caused by temporal
constraints, and finally fail to learn specific temporal representations of
entities. (II) They neither emphasize the graph structure between entities nor
explicitly model the multi-hop relationship in the graph, which will make it
difficult to solve complex multi-hop question answering. To alleviate this
problem, we propose a novel Question Calibration and Multi-Hop Modeling
(QC-MHM) approach. Specifically, We first calibrate the question representation
by fusing the question and the time-constrained concepts in KG. Then, we
construct the GNN layer to complete multi-hop message passing. Finally, the
question representation is combined with the embedding output by the GNN to
generate the final prediction. Empirical results verify that the proposed model
achieves better performance than the state-of-the-art models in the benchmark
dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions
dataset's complex questions are absolutely improved by 5.1% and 1.2% compared
to the best-performing baseline. Moreover, QC-MHM can generate interpretable
and trustworthy predictions.
\\ ( https://arxiv.org/abs/2402.13188 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13208
Date: Tue, 20 Feb 2024 18:19:08 GMT   (870kb,D)

Title: How do Hyenas deal with Human Speech? Speech Recognition and Translation
  with ConfHyena
Authors: Marco Gaido, Sara Papi, Matteo Negri, Luisa Bentivogli
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  The attention mechanism, a cornerstone of state-of-the-art neural models,
faces computational hurdles in processing long sequences due to its quadratic
complexity. Consequently, research efforts in the last few years focused on
finding more efficient alternatives. Among them, Hyena (Poli et al., 2023)
stands out for achieving competitive results in both language modeling and
image classification, while offering sub-quadratic memory and computational
complexity. Building on these promising results, we propose ConfHyena, a
Conformer whose encoder self-attentions are replaced with an adaptation of
Hyena for speech processing, where the long input sequences cause high
computational costs. Through experiments in automatic speech recognition (for
English) and translation (from English into 8 target languages), we show that
our best ConfHyena model significantly reduces the training time by 27%, at the
cost of minimal quality degradation (~1%), which, in most cases, is not
statistically significant.
\\ ( https://arxiv.org/abs/2402.13208 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13211
Date: Tue, 20 Feb 2024 18:21:32 GMT   (9824kb,D)

Title: Can Large Language Models be Good Emotional Supporter? Mitigating
  Preference Bias on Emotional Support Conversation
Authors: Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho,
  Youngjae Yu, Dongha Lee, Jinyoung Yeo
Categories: cs.CL
Comments: Work in progress
ACM-class: I.2.7
\\
  Emotional Support Conversation (ESC) is a task aimed at alleviating
individuals' emotional distress through daily conversation. Given its inherent
complexity and non-intuitive nature, ESConv dataset incorporates support
strategies to facilitate the generation of appropriate responses. Recently,
despite the remarkable conversational ability of large language models (LLMs),
previous studies have suggested that they often struggle with providing useful
emotional support. Hence, this work initially analyzes the results of LLMs on
ESConv, revealing challenges in selecting the correct strategy and a notable
preference for a specific strategy. Motivated by these, we explore the impact
of the inherent preference in LLMs on providing emotional support, and
consequently, we observe that exhibiting high preference for specific
strategies hinders effective emotional support, aggravating its robustness in
predicting the appropriate strategy. Moreover, we conduct a methodological
study to offer insights into the necessary approaches for LLMs to serve as
proficient emotional supporters. Our findings emphasize that (1) low preference
for specific strategies hinders the progress of emotional support, (2) external
assistance helps reduce preference bias, and (3) LLMs alone cannot become good
emotional supporters. These insights suggest promising avenues for future
research to enhance the emotional intelligence of LLMs.
\\ ( https://arxiv.org/abs/2402.13211 ,  9824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13212
Date: Tue, 20 Feb 2024 18:22:38 GMT   (7997kb,D)

Title: Soft Self-Consistency Improves Language Model Agents
Authors: Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
Categories: cs.CL cs.AI cs.LG
Comments: 14 pages, the first three authors contributed equally; Code:
  https://github.com/HanNight/soft_self_consistency
\\
  Generations from large language models (LLMs) can be improved by sampling and
scoring multiple solutions to select a final answer. Current "sample and
select" methods such as self-consistency (SC) rely on majority voting to score
answers. However, when tasks have many distinct and valid answers, selection by
voting requires a large number of samples. This makes SC prohibitively
expensive for interactive tasks that involve generating multiple actions
(answers) sequentially. After establishing that majority voting fails to
provide consistent gains on such tasks, we demonstrate how to increase success
rates by softening the scoring criterion. We introduce Soft Self-Consistency
(Soft-SC), which replaces SC's discontinuous scoring with a continuous score
computed from model likelihoods, allowing for selection even when actions are
sparsely distributed. Soft-SC improves both performance and efficiency on
long-horizon interactive tasks, requiring half as many samples as SC for
comparable or better performance. For a fixed number of samples, Soft-SC leads
to a 1.3% increase over SC in absolute success rate on writing bash programs, a
6.6% increase on online shopping (WebShop), and a 4.7% increase for an
interactive household game (ALFWorld). Finally, we show that Soft-SC can be
applied to both open-source and black-box models.
\\ ( https://arxiv.org/abs/2402.13212 ,  7997kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13213
Date: Tue, 20 Feb 2024 18:24:47 GMT   (212kb,D)

Title: Softmax Probabilities (Mostly) Predict Large Language Model Correctness
  on Multiple-Choice Q&A
Authors: Benjamin Plaut, Khanh Nguyen, Tu Trinh
Categories: cs.CL cs.AI cs.LG
\\
  Although large language models (LLMs) perform impressively on many tasks,
overconfidence remains a problem. We hypothesized that on multiple-choice Q&A
tasks, wrong answers would be associated with smaller maximum softmax
probabilities (MSPs) compared to correct answers. We comprehensively evaluate
this hypothesis on ten open-source LLMs and five datasets, and find strong
evidence for our hypothesis among models which perform well on the original Q&A
task. For the six LLMs with the best Q&A performance, the AUROC derived from
the MSP was better than random chance with p < 10^{-4} in 59/60 instances.
Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging
these findings, we propose a multiple-choice Q&A task with an option to abstain
and show that performance can be improved by selectively abstaining based on
the MSP of the initial model response. We also run the same experiments with
pre-softmax logits instead of softmax probabilities and find similar (but not
identical) results.
\\ ( https://arxiv.org/abs/2402.13213 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13222
Date: Tue, 20 Feb 2024 18:32:47 GMT   (523kb,D)

Title: RoCode: A Dataset for Measuring Code Intelligence from Problem
  Definitions in Romanian
Authors: Adrian Cosma and Bogdan Iordache and Paolo Rosso
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Recently, large language models (LLMs) have become increasingly powerful and
have become capable of solving a plethora of tasks through proper instructions
in natural language. However, the vast majority of testing suites assume that
the instructions are written in English, the de facto prompting language. Code
intelligence and problem solving still remain a difficult task, even for the
most advanced LLMs. Currently, there are no datasets to measure the
generalization power for code-generation models in a language other than
English. In this work, we present RoCode, a competitive programming dataset,
consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and
Python and comprehensive testing suites for each problem. The purpose of RoCode
is to provide a benchmark for evaluating the code intelligence of language
models trained on Romanian / multilingual text as well as a fine-tuning set for
pretrained Romanian models. Through our results and review of related works, we
argue for the need to develop code models for languages other than English.
\\ ( https://arxiv.org/abs/2402.13222 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13225
Date: Tue, 20 Feb 2024 18:37:19 GMT   (2804kb)

Title: AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale
  Clinical Tool Learning
Authors: Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright,
  Thomas Huang, W John Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, Zhiyong Lu
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  Clinical calculators play a vital role in healthcare by offering accurate
evidence-based predictions for various purposes such as prognosis.
Nevertheless, their widespread utilization is frequently hindered by usability
challenges, poor dissemination, and restricted functionality. Augmenting large
language models with extensive collections of clinical calculators presents an
opportunity to overcome these obstacles and improve workflow efficiency, but
the scalability of the manual curation process poses a significant challenge.
In response, we introduce AgentMD, a novel language agent capable of curating
and applying clinical calculators across various clinical contexts. Using the
published literature, AgentMD has automatically curated a collection of 2,164
diverse clinical calculators with executable functions and structured
documentation, collectively named RiskCalcs. Manual evaluations show that
RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At
inference time, AgentMD can automatically select and apply the relevant
RiskCalcs tools given any patient description. On the newly established RiskQA
benchmark, AgentMD significantly outperforms chain-of-thought prompting with
GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to
real-world clinical notes for analyzing both population-level and risk-level
patient characteristics. In summary, our study illustrates the utility of
language agents augmented with clinical calculators for healthcare analytics
and patient care.
\\ ( https://arxiv.org/abs/2402.13225 ,  2804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13228
Date: Tue, 20 Feb 2024 18:42:34 GMT   (294kb,D)

Title: Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
Authors: Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha
  Naidu, Colin White
Categories: cs.CL cs.AI cs.LG
\\
  Direct Preference Optimisation (DPO) is effective at significantly improving
the performance of large language models (LLMs) on downstream tasks such as
reasoning, summarisation, and alignment. Using pairs of preferred and
dispreferred data, DPO models the \textit{relative} probability of picking one
response over another. In this work, first we show theoretically that the
standard DPO loss can lead to a \textit{reduction} of the model's likelihood of
the preferred examples, as long as the relative probability between the
preferred and dispreferred classes increases. We then show empirically that
this phenomenon occurs when fine-tuning LLMs on common datasets, especially
datasets in which the edit distance between pairs of completions is low. Using
these insights, we design DPO-Positive (DPOP), a new loss function and training
procedure which avoids this failure mode. Surprisingly, we also find that DPOP
significantly outperforms DPO across a wide variety of datasets and downstream
tasks, including datasets with high edit distances between completions. By
fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which
achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly
2\% better than any other open-source model on the HuggingFace Open LLM
Leaderboard and becomes the first open-source LLM to surpass an average
accuracy of 80\%.
\\ ( https://arxiv.org/abs/2402.13228 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13231
Date: Tue, 20 Feb 2024 18:47:28 GMT   (4090kb,D)

Title: Investigating Cultural Alignment of Large Language Models
Authors: Badr AlKhamissi, Muhammad ElNokrashy, Mai AlKhamissi, Mona Diab
Categories: cs.CL cs.CY
Comments: Preprint
\\
  The intricate relationship between language and culture has long been a
subject of exploration within the realm of linguistic anthropology. Large
Language Models (LLMs), promoted as repositories of collective human knowledge,
raise a pivotal question: do these models genuinely encapsulate the diverse
knowledge adopted by different cultures? Our study reveals that these models
demonstrate greater cultural alignment along two dimensions -- firstly, when
prompted with the dominant language of a specific culture, and secondly, when
pretrained with a refined mixture of languages employed by that culture. We
quantify cultural alignment by simulating sociological surveys, comparing model
responses to those of actual survey participants as references. Specifically,
we replicate a survey conducted in various regions of Egypt and the United
States through prompting LLMs with different pretraining data mixtures in both
Arabic and English with the personas of the real respondents and the survey
questions. Further analysis reveals that misalignment becomes more pronounced
for underrepresented personas and for culturally sensitive topics, such as
those probing social values. Finally, we introduce Anthropological Prompting, a
novel method leveraging anthropological reasoning to enhance cultural
alignment. Our study emphasizes the necessity for a more balanced multilingual
pretraining dataset to better represent the diversity of human experience and
the plurality of different cultures with many implications on the topic of
cross-lingual transfer.
\\ ( https://arxiv.org/abs/2402.13231 ,  4090kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13249
Date: Tue, 20 Feb 2024 18:58:49 GMT   (10253kb,D)

Title: TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue
  Summarization
Authors: Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W.
  Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia
  Sun, Yi Zhang, Saab Mansour, Kathleen McKeown
Categories: cs.CL cs.AI
Comments: Linguistic annotations available at
  https://github.com/amazon-science/tofueval
\\
  Single document news summarization has seen substantial progress on
faithfulness in recent years, driven by research on the evaluation of factual
consistency, or hallucinations. We ask whether these advances carry over to
other text summarization domains. We propose a new evaluation benchmark on
topic-focused dialogue summarization, generated by LLMs of varying sizes. We
provide binary sentence-level human annotations of the factual consistency of
these summaries along with detailed explanations of factually inconsistent
sentences. Our analysis shows that existing LLMs hallucinate significant
amounts of factual errors in the dialogue domain, regardless of the model's
size. On the other hand, when LLMs, including GPT-4, serve as binary factual
evaluators, they perform poorly and can be outperformed by prevailing
state-of-the-art specialized factuality evaluation metrics. Finally, we
conducted an analysis of hallucination types with a curated error taxonomy. We
find that there are diverse errors and error distributions in model-generated
summaries and that non-LLM based metrics can capture all error types better
than LLM-based evaluators.
\\ ( https://arxiv.org/abs/2402.13249 ,  10253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13253
Date: Tue, 20 Feb 2024 18:59:26 GMT   (720kb,D)

Title: BiMediX: Bilingual Medical Mixture of Experts LLM
Authors: Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad
  Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal
Categories: cs.CL
\\
  In this paper, we introduce BiMediX, the first bilingual medical mixture of
experts LLM designed for seamless interaction in both English and Arabic. Our
model facilitates a wide range of medical interactions in English and Arabic,
including multi-turn chats to inquire about additional details such as patient
symptoms and medical history, multiple-choice question answering, and
open-ended question answering. We propose a semi-automated English-to-Arabic
translation pipeline with human refinement to ensure high-quality translations.
We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.
Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual
instruction set covering 1.3 Million diverse medical interactions, resulting in
over 632 million healthcare specialized tokens for instruction tuning. Our
BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and
maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art
Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively,
computed across multiple medical evaluation benchmarks in English, while
operating at 8-times faster inference. Moreover, our BiMediX outperforms the
generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of
10% on our Arabic medical benchmark and 15% on bilingual evaluations across
multiple datasets. Our project page with source code and trained model is
available at https://github.com/mbzuai-oryx/BiMediX .
\\ ( https://arxiv.org/abs/2402.13253 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12398
Date: Sat, 17 Feb 2024 05:39:48 GMT   (488kb,D)

Title: Primary and Secondary Factor Consistency as Domain Knowledge to Guide
  Happiness Computing in Online Assessment
Authors: Xiaohua Wu and Lin Li and Xiaohui Tao and Frank Xing and Jingling Yuan
Categories: cs.LG
Comments: 12 pages
\\
  Happiness computing based on large-scale online web data and machine learning
methods is an emerging research topic that underpins a range of issues, from
personal growth to social stability. Many advanced Machine Learning (ML) models
with explanations are used to compute the happiness online assessment while
maintaining high accuracy of results. However, domain knowledge constraints,
such as the primary and secondary relations of happiness factors, are absent
from these models, which limits the association between computing results and
the right reasons for why they occurred. This article attempts to provide new
insights into the explanation consistency from an empirical study perspective.
Then we study how to represent and introduce domain knowledge constraints to
make ML models more trustworthy. We achieve this through: (1) proving that
multiple prediction models with additive factor attributions will have the
desirable property of primary and secondary relations consistency, and (2)
showing that factor relations with quantity can be represented as an importance
distribution for encoding domain knowledge. Factor explanation difference is
penalized by the Kullback-Leibler divergence-based loss among computing models.
Experimental results using two online web datasets show that domain knowledge
of stable factor relations exists. Using this knowledge not only improves
happiness computing accuracy but also reveals more significative happiness
factors for assisting decisions well.
\\ ( https://arxiv.org/abs/2402.12398 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12399
Date: Sat, 17 Feb 2024 06:23:27 GMT   (211kb,D)

Title: Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
Authors: Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou,
  Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, Xipeng Qiu
Categories: cs.LG cs.AI
\\
  Sparse Mixture of Experts (MoE) models are popular for training large
language models due to their computational efficiency. However, the commonly
used top-$k$ routing mechanism suffers from redundancy computation and memory
costs due to the unbalanced routing. Some experts are overflow, where the
exceeding tokens are dropped. While some experts are vacant, which are padded
with zeros, negatively impacting model performance. To address the dropped
tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU
Rectification and the Fill-in Rectification. The Intra-GPU Rectification
handles dropped tokens, efficiently routing them to experts within the GPU
where they are located to avoid inter-GPU communication. The Fill-in
Rectification addresses padding by replacing padding tokens with the tokens
that have high routing scores. Our experimental results demonstrate that the
Intra-GPU Rectification and the Fill-in Rectification effectively handle
dropped tokens and padding, respectively. Furthermore, the combination of them
achieves superior performance, surpassing the accuracy of the vanilla top-1
router by 4.7%.
\\ ( https://arxiv.org/abs/2402.12399 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12406
Date: Sun, 18 Feb 2024 08:13:57 GMT   (854kb,D)

Title: Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge
  Distillation
Authors: Hyunjune Shin, Dong-Wan Choi
Categories: cs.LG cs.AI cs.CV
Comments: Accepted in AAAI-2024
\\
  Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge
to a student model with the help of a generator without using original data. In
such data-free scenarios, achieving stable performance of DFKD is essential due
to the unavailability of validation data. Unfortunately, this paper has
discovered that existing DFKD methods are quite sensitive to different teacher
models, occasionally showing catastrophic failures of distillation, even when
using well-trained teacher models. Our observation is that the generator in
DFKD is not always guaranteed to produce precise yet diverse samples using the
existing representative strategy of minimizing both class-prior and adversarial
losses. Through our empirical study, we focus on the fact that class-prior not
only decreases the diversity of generated samples, but also cannot completely
address the problem of generating unexpectedly low-quality samples depending on
teacher models. In this paper, we propose the teacher-agnostic data-free
knowledge distillation (TA-DFKD) method, with the goal of more robust and
stable performance regardless of teacher models. Our basic idea is to assign
the teacher model a lenient expert role for evaluating samples, rather than a
strict supervisor that enforces its class-prior on the generator. Specifically,
we design a sample selection approach that takes only clean samples verified by
the teacher model without imposing restrictions on the power of generating
diverse samples. Through extensive experiments, we show that our method
successfully achieves both robustness and training stability across various
teacher models, while outperforming the existing DFKD methods.
\\ ( https://arxiv.org/abs/2402.12406 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12408
Date: Sun, 18 Feb 2024 11:24:34 GMT   (2162kb,D)

Title: ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation
Authors: Zihao Tang, Zheqi Lv, Shengyu Zhang, Fei Wu, Kun Kuang
Categories: cs.LG cs.AI
\\
  The rapid advancement of Large Language Models (LLMs) has revolutionized
various sectors by automating routine tasks, marking a step toward the
realization of Artificial General Intelligence (AGI). However, they still
struggle to accommodate the diverse and specific needs of users and simplify
the utilization of AI models for the average user. In response, we propose
ModelGPT, a novel framework designed to determine and generate AI models
specifically tailored to the data or task descriptions provided by the user,
leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able
to provide tailored models at most 270x faster than the previous paradigms
(e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV,
and Tabular datasets attest to the effectiveness of our framework in making AI
models more accessible and user-friendly. Our code is available at
https://github.com/IshiKura-a/ModelGPT.
\\ ( https://arxiv.org/abs/2402.12408 ,  2162kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12415
Date: Mon, 19 Feb 2024 07:47:23 GMT   (20614kb)

Title: Vehicle-group-based Crash Risk Formation and Propagation Analysis for
  Expressways
Authors: Tianheng Zhu, Ling Wang, Yiheng Feng, Wanjing Ma and Mohamed Abdel-Aty
Categories: cs.LG
Comments: 14 pages, 8 figures
\\
  Previous studies in predicting crash risk primarily associated the number or
likelihood of crashes on a road segment with traffic parameters or geometric
characteristics of the segment, usually neglecting the impact of vehicles'
continuous movement and interactions with nearby vehicles. Advancements in
communication technologies have empowered driving information collected from
surrounding vehicles, enabling the study of group-based crash risks. Based on
high-resolution vehicle trajectory data, this research focused on vehicle
groups as the subject of analysis and explored risk formation and propagation
mechanisms considering features of vehicle groups and road segments. Several
key factors contributing to crash risks were identified, including past
high-risk vehicle-group states, complex vehicle behaviors, high percentage of
large vehicles, frequent lane changes within a vehicle group, and specific road
geometries. A multinomial logistic regression model was developed to analyze
the spatial risk propagation patterns, which were classified based on the trend
of high-risk occurrences within vehicle groups. The results indicated that
extended periods of high-risk states, increase in vehicle-group size, and
frequent lane changes are associated with adverse risk propagation patterns.
Conversely, smoother traffic flow and high initial crash risk values are linked
to risk dissipation. Furthermore, the study conducted sensitivity analysis on
different types of classifiers, prediction time intervalsss and adaptive TTC
thresholds. The highest AUC value for vehicle-group risk prediction surpassed
0.93. The findings provide valuable insights to researchers and practitioners
in understanding and prediction of vehicle-group safety, ultimately improving
active traffic safety management and operations of Connected and Autonomous
Vehicles.
\\ ( https://arxiv.org/abs/2402.12415 ,  20614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12417
Date: Mon, 19 Feb 2024 08:27:53 GMT   (1993kb)

Title: Predicting trucking accidents with truck drivers 'safety climate
  perception across companies: A transfer learning approach
Authors: Kailai Sun, Tianxiang Lan, Say Hong Kam, Yang Miang Goh, and
  Yueng-Hsiang Huang
Categories: cs.LG cs.AI
Comments: submitted to journal: accident analysis and prevention
\\
  There is a rising interest in using artificial intelligence (AI)-powered
safety analytics to predict accidents in the trucking industry. Companies may
face the practical challenge, however, of not having enough data to develop
good safety analytics models. Although pretrained models may offer a solution
for such companies, existing safety research using transfer learning has mostly
focused on computer vision and natural language processing, rather than
accident analytics. To fill the above gap, we propose a pretrain-then-fine-tune
transfer learning approach to help any company leverage other companies' data
to develop AI models for a more accurate prediction of accident risk. We also
develop SafeNet, a deep neural network algorithm for classification tasks
suitable for accident prediction. Using the safety climate survey data from
seven trucking companies with different data sizes, we show that our proposed
approach results in better model performance compared to training the model
from scratch using only the target company's data. We also show that for the
transfer learning model to be effective, the pretrained model should be
developed with larger datasets from diverse sources. The trucking industry may,
thus, consider pooling safety analytics data from a wide range of companies to
develop pretrained models and share them within the industry for better
knowledge and resource transfer. The above contributions point to the promise
of advanced safety analytics to make the industry safer and more sustainable.
\\ ( https://arxiv.org/abs/2402.12417 ,  1993kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12418
Date: Mon, 19 Feb 2024 09:52:45 GMT   (1342kb,D)

Title: Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural
  Architectures
Authors: Akash Guna R.T, Arnav Chavan, Deepak Gupta
Categories: cs.LG cs.AI cs.NE
Comments: Accepted At ICLR 2024 (Tiny Paper Track)
\\
  Conventional scaling of neural networks typically involves designing a base
network and growing different dimensions like width, depth, etc. of the same by
some predefined scaling factors. We introduce an automated scaling approach
leveraging second-order loss landscape information. Our method is flexible
towards skip connections a mainstay in modern vision transformers. Our
training-aware method jointly scales and trains transformers without additional
training iterations. Motivated by the hypothesis that not all neurons need
uniform depth complexity, our approach embraces depth heterogeneity. Extensive
evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10%
parameter efficiency improvement over conventional scaling. Scaled networks
demonstrate superior performance upon training small scale datasets from
scratch. We introduce the first intact scaling mechanism for vision
transformers, a step towards efficient model scaling.
\\ ( https://arxiv.org/abs/2402.12418 ,  1342kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12419
Date: Mon, 19 Feb 2024 09:55:32 GMT   (9053kb,D)

Title: EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs
Authors: Song Guo, Fan Wu, Lei Zhang, Xiawu Zheng, Shengchuan Zhang, Fei Chao,
  Yiyu Shi, Rongrong Ji
Categories: cs.LG cs.AI
\\
  Existing methods for fine-tuning sparse LLMs often suffer from
resource-intensive requirements and high retraining costs. Additionally, many
fine-tuning methods often rely on approximations or heuristic optimization
strategies, which may lead to suboptimal solutions. To address these issues, we
propose an efficient and fast framework for fine-tuning sparse LLMs based on
minimizing reconstruction error. Our approach involves sampling a small dataset
for calibration and utilizing backpropagation to iteratively optimize
block-wise reconstruction error, on a block-by-block basis, aiming for optimal
solutions. Extensive experiments on various benchmarks consistently demonstrate
the superiority of our method over other baselines. For instance, on the
Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a
perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of
75.14. Moreover, with a structured sparsity ratio of 26\%, EBFT achieves a
perplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the
fine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,
and the entire framework can be executed on a single 16GB GPU. The source code
is available at https://github.com/sunggo/EBFT.
\\ ( https://arxiv.org/abs/2402.12419 ,  9053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12424
Date: Mon, 19 Feb 2024 16:34:50 GMT   (6911kb,D)

Title: Tables as Images? Exploring the Strengths and Limitations of LLMs on
  Multimodal Representations of Tabular Data
Authors: Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma,
  Yue Zhang, Rada Mihalcea
Categories: cs.LG cs.AI cs.CL cs.CV
\\
  In this paper, we investigate the effectiveness of various LLMs in
interpreting tabular data through different prompting strategies and data
formats. Our analysis extends across six benchmarks for table-related tasks
such as question-answering and fact-checking. We introduce for the first time
the assessment of LLMs' performance on image-based table representations.
Specifically, we compare five text-based and three image-based table
representations, demonstrating the influence of representation and prompting on
LLM performance. Our study provides insights into the effective use of LLMs on
table-related tasks.
\\ ( https://arxiv.org/abs/2402.12424 ,  6911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12465
Date: Mon, 19 Feb 2024 19:11:22 GMT   (13601kb,D)

Title: Neuro-mimetic Task-free Unsupervised Online Learning with Continual
  Self-Organizing Maps
Authors: Hitesh Vaidya, Travis Desell, Ankur Mali, Alexander Ororbia
Categories: cs.LG cs.NE
\\
  An intelligent system capable of continual learning is one that can process
and extract knowledge from potentially infinitely long streams of pattern
vectors. The major challenge that makes crafting such a system difficult is
known as catastrophic forgetting - an agent, such as one based on artificial
neural networks (ANNs), struggles to retain previously acquired knowledge when
learning from new samples. Furthermore, ensuring that knowledge is preserved
for previous tasks becomes more challenging when input is not supplemented with
task boundary information. Although forgetting in the context of ANNs has been
studied extensively, there still exists far less work investigating it in terms
of unsupervised architectures such as the venerable self-organizing map (SOM),
a neural model often used in clustering and dimensionality reduction. While the
internal mechanisms of SOMs could, in principle, yield sparse representations
that improve memory retention, we observe that, when a fixed-size SOM processes
continuous data streams, it experiences concept drift. In light of this, we
propose a generalization of the SOM, the continual SOM (CSOM), which is capable
of online unsupervised learning under a low memory budget. Our results, on
benchmarks including MNIST, Kuzushiji-MNIST, and Fashion-MNIST, show almost a
two times increase in accuracy, and CIFAR-10 demonstrates a state-of-the-art
result when tested on (online) unsupervised class incremental learning setting.
\\ ( https://arxiv.org/abs/2402.12465 ,  13601kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12479
Date: Mon, 19 Feb 2024 19:34:07 GMT   (597kb,D)

Title: In deep reinforcement learning, a pruned network is a good network
Authors: Johan Obando-Ceron and Aaron Courville and Pablo Samuel Castro
Categories: cs.LG cs.AI
\\
  Recent work has shown that deep reinforcement learning agents have difficulty
in effectively using their network parameters. We leverage prior insights into
the advantages of sparse training techniques and demonstrate that gradual
magnitude pruning enables agents to maximize parameter effectiveness. This
results in networks that yield dramatic performance improvements over
traditional networks and exhibit a type of "scaling law", using only a small
fraction of the full network parameters.
\\ ( https://arxiv.org/abs/2402.12479 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12490
Date: Mon, 19 Feb 2024 19:54:03 GMT   (552kb,D)

Title: Towards Cross-Domain Continual Learning
Authors: Marcus de Carvalho, Mahardhika Pratama, Jie Zhang, Chua Haoyan, Edward
  Yapp
Categories: cs.LG cs.AI cs.CV
Comments: 12 pages, 2 Figures, 4 Tables. To be published at the IEEE
  International Conference on Data Engineering (ICDE) 2024
\\
  Continual learning is a process that involves training learning agents to
sequentially master a stream of tasks or classes without revisiting past data.
The challenge lies in leveraging previously acquired knowledge to learn new
tasks efficiently, while avoiding catastrophic forgetting. Existing methods
primarily focus on single domains, restricting their applicability to specific
problems.
  In this work, we introduce a novel approach called Cross-Domain Continual
Learning (CDCL) that addresses the limitations of being limited to single
supervised domains. Our method combines inter- and intra-task cross-attention
mechanisms within a compact convolutional network. This integration enables the
model to maintain alignment with features from previous tasks, thereby delaying
the data drift that may occur between tasks, while performing unsupervised
cross-domain (UDA) between related domains. By leveraging an
intra-task-specific pseudo-labeling method, we ensure accurate input pairs for
both labeled and unlabeled samples, enhancing the learning process. To validate
our approach, we conduct extensive experiments on public UDA datasets,
showcasing its positive performance on cross-domain continual learning
challenges. Additionally, our work introduces incremental ideas that contribute
to the advancement of this field.
  We make our code and models available to encourage further exploration and
reproduction of our results: \url{https://github.com/Ivsucram/CDCL}
\\ ( https://arxiv.org/abs/2402.12490 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12503
Date: Mon, 19 Feb 2024 20:11:46 GMT   (9633kb,D)

Title: PARCv2: Physics-aware Recurrent Convolutional Neural Networks for
  Spatiotemporal Dynamics Modeling
Authors: Phong C.H. Nguyen, Xinlun Cheng, Shahab Arfaza, Pradeep Seshadri, Yen
  T. Nguyen, Munho Kim, Sanghun Choi, H.S. Udaykumar, Stephen Baek
Categories: cs.LG
\\
  Modeling unsteady, fast transient, and advection-dominated physics problems
is a pressing challenge for physics-aware deep learning (PADL). The physics of
complex systems is governed by large systems of partial differential equations
(PDEs) and ancillary constitutive models with nonlinear structures, as well as
evolving state fields exhibiting sharp gradients and rapidly deforming material
interfaces. Here, we investigate an inductive bias approach that is versatile
and generalizable to model generic nonlinear field evolution problems. Our
study focuses on the recent physics-aware recurrent convolutions (PARC), which
incorporates a differentiator-integrator architecture that inductively models
the spatiotemporal dynamics of generic physical systems. We extend the
capabilities of PARC to simulate unsteady, transient, and advection-dominant
systems. The extended model, referred to as PARCv2, is equipped with
differential operators to model advection-reaction-diffusion equations, as well
as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested
on both standard benchmark problems in fluid dynamics, namely Burgers and
Navier-Stokes equations, and then applied to more complex shock-induced
reaction problems in energetic materials. We evaluate the behavior of PARCv2 in
comparison to other physics-informed and learning bias models and demonstrate
its potential to model unsteady and advection-dominant dynamics regimes.
\\ ( https://arxiv.org/abs/2402.12503 ,  9633kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12508
Date: Mon, 19 Feb 2024 20:18:29 GMT   (2403kb,D)

Title: SDEs for Minimax Optimization
Authors: Enea Monzio Compagnoni, Antonio Orvieto, Hans Kersting, Frank Norbert
  Proske, Aurelien Lucchi
Categories: cs.LG math.OC
Comments: Accepted at AISTATS 2024 (Poster)
\\
  Minimax optimization problems have attracted a lot of attention over the past
few years, with applications ranging from economics to machine learning. While
advanced optimization methods exist for such problems, characterizing their
dynamics in stochastic scenarios remains notably challenging. In this paper, we
pioneer the use of stochastic differential equations (SDEs) to analyze and
compare Minimax optimizers. Our SDE models for Stochastic Gradient
Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient
Descent are provable approximations of their algorithmic counterparts, clearly
showcasing the interplay between hyperparameters, implicit regularization, and
implicit curvature-induced noise. This perspective also allows for a unified
and simplified analysis strategy based on the principles of It\^o calculus.
Finally, our approach facilitates the derivation of convergence conditions and
closed-form solutions for the dynamics in simplified settings, unveiling
further insights into the behavior of different optimizers.
\\ ( https://arxiv.org/abs/2402.12508 ,  2403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12513
Date: Mon, 19 Feb 2024 20:21:09 GMT   (1649kb,D)

Title: Induced Model Matching: How Restricted Models Can Help Larger Ones
Authors: Usama Muneeb and Mesrob I. Ohannessian
Categories: cs.LG
\\
  We consider scenarios where a very accurate predictive model using restricted
features is available at the time of training of a larger, full-featured,
model. This restricted model may be thought of as "side-information", derived
either from an auxiliary exhaustive dataset or on the same dataset, by forcing
the restriction. How can the restricted model be useful to the full model? We
propose an approach for transferring the knowledge of the restricted model to
the full model, by aligning the full model's context-restricted performance
with that of the restricted model's. We call this methodology Induced Model
Matching (IMM) and first illustrate its general applicability by using logistic
regression as a toy example. We then explore IMM's use in language modeling,
the application that initially inspired it, and where it offers an explicit
foundation in contrast to the implicit use of restricted models in techniques
such as noising. We demonstrate the methodology on both LSTM and transformer
full models, using $N$-grams as restricted models. To further illustrate the
potential of the principle whenever it is much cheaper to collect restricted
rather than full information, we conclude with a simple RL example where POMDP
policies can improve learned MDP policies via IMM.
\\ ( https://arxiv.org/abs/2402.12513 ,  1649kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12518
Date: Mon, 19 Feb 2024 20:29:34 GMT   (2545kb,D)

Title: Gaussian Process Neural Additive Models
Authors: Wei Zhang, Brian Barr, John Paisley
Categories: cs.LG cs.AI
Comments: Appears at AAAI 2024
\\
  Deep neural networks have revolutionized many fields, but their black-box
nature also occasionally prevents their wider adoption in fields such as
healthcare and finance, where interpretable and explainable models are
required. The recent development of Neural Additive Models (NAMs) is a
significant step in the direction of interpretable deep learning for tabular
datasets. In this paper, we propose a new subclass of NAMs that use a
single-layer neural network construction of the Gaussian process via random
Fourier features, which we call Gaussian Process Neural Additive Models
(GP-NAM). GP-NAMs have the advantage of a convex objective function and number
of trainable parameters that grows linearly with feature dimensionality. It
suffers no loss in performance compared to deeper NAM approaches because GPs
are well-suited for learning complex non-parametric univariate functions. We
demonstrate the performance of GP-NAM on several tabular datasets, showing that
it achieves comparable or better performance in both classification and
regression tasks with a large reduction in the number of parameters.
\\ ( https://arxiv.org/abs/2402.12518 ,  2545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12527
Date: Mon, 19 Feb 2024 20:38:00 GMT   (4018kb,D)

Title: The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
Authors: Anya Sims, Cong Lu, Yee Whye Teh
Categories: cs.LG cs.AI
Comments: Code open-sourced at: https://github.com/anyasims/edge-of-reach
\\
  Offline reinforcement learning aims to enable agents to be trained from
pre-collected datasets, however, this comes with the added challenge of
estimating the value of behavior not covered in the dataset. Model-based
methods offer a solution by allowing agents to collect additional synthetic
data via rollouts in a learned dynamics model. The prevailing theoretical
understanding is that this can then be viewed as online reinforcement learning
in an approximate dynamics model, and any remaining gap is therefore assumed to
be due to the imperfect dynamics model. Surprisingly, however, we find that if
the learned dynamics model is replaced by the true error-free dynamics,
existing model-based methods completely fail. This reveals a major
misconception. Our subsequent investigation finds that the general procedure
used in model-based algorithms results in the existence of a set of
edge-of-reach states which trigger pathological value overestimation and
collapse in Bellman-based algorithms. We term this the edge-of-reach problem.
Based on this, we fill some gaps in existing theory and also explain how prior
model-based methods are inadvertently addressing the true underlying
edge-of-reach problem. Finally, we propose Reach-Aware Value Learning (RAVL), a
simple and robust method that directly addresses the edge-of-reach problem and
achieves strong performance across both proprioceptive and pixel-based
benchmarks. Code open-sourced at: https://github.com/anyasims/edge-of-reach.
\\ ( https://arxiv.org/abs/2402.12527 ,  4018kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12535
Date: Mon, 19 Feb 2024 20:48:09 GMT   (4585kb,D)

Title: Locality-Sensitive Hashing-Based Efficient Point Transformer with
  Applications in High-Energy Physics
Authors: Siqi Miao, Zhiyuan Lu, Mia Liu, Javier Duarte, Pan Li
Categories: cs.LG hep-ex
\\
  This study introduces a novel transformer model optimized for large-scale
point cloud processing in scientific domains such as high-energy physics (HEP)
and astrophysics. Addressing the limitations of graph neural networks and
standard transformers, our model integrates local inductive bias and achieves
near-linear complexity with hardware-friendly regular operations. One
contribution of this work is the quantitative analysis of the error-complexity
tradeoff of various sparsification techniques for building efficient
transformers. Our findings highlight the superiority of using
locality-sensitive hashing (LSH), especially OR \& AND-construction LSH, in
kernel approximation for large-scale point cloud data with local inductive
bias. Based on this finding, we propose LSH-based Efficient Point Transformer
(\textbf{HEPT}), which combines E$^2$LSH with OR \& AND constructions and is
built upon regular computations. HEPT demonstrates remarkable performance in
two critical yet time-consuming HEP tasks, significantly outperforming existing
GNNs and transformers in accuracy and computational speed, marking a
significant advancement in geometric deep learning and large-scale scientific
data processing. Our code is available at
\url{https://github.com/Graph-COM/HEPT}.
\\ ( https://arxiv.org/abs/2402.12535 ,  4585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12537
Date: Mon, 19 Feb 2024 20:53:27 GMT   (2061kb,D)

Title: Hierarchical Bayes Approach to Personalized Federated Unsupervised
  Learning
Authors: Kaan Ozkara, Bruce Huang, Ruida Zhou, Suhas Diggavi
Categories: cs.LG
\\
  Statistical heterogeneity of clients' local data is an important
characteristic in federated learning, motivating personalized algorithms
tailored to the local data statistics. Though there has been a plethora of
algorithms proposed for personalized supervised learning, discovering the
structure of local data through personalized unsupervised learning is less
explored. We initiate a systematic study of such personalized unsupervised
learning by developing algorithms based on optimization criteria inspired by a
hierarchical Bayesian statistical framework. We develop adaptive algorithms
that discover the balance between using limited local data and collaborative
information. We do this in the context of two unsupervised learning tasks:
personalized dimensionality reduction and personalized diffusion models. We
develop convergence analyses for our adaptive algorithms which illustrate the
dependence on problem parameters (e.g., heterogeneity, local sample size). We
also develop a theoretical framework for personalized diffusion models, which
shows the benefits of collaboration even under heterogeneity. We finally
evaluate our proposed algorithms using synthetic and real data, demonstrating
the effective sample amplification for personalized tasks, induced through
collaboration, despite data heterogeneity.
\\ ( https://arxiv.org/abs/2402.12537 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12558
Date: Mon, 19 Feb 2024 21:32:56 GMT   (1281kb,D)

Title: Evaluation of Country Dietary Habits Using Machine Learning Techniques
  in Relation to Deaths from COVID-19
Authors: Mar\'ia Teresa Garc\'ia-Ord\'as, Natalia Arias, Carmen Benavides,
  Oscar Garc\'ia-Olalla and Jos\'e Alberto Ben\'itez-Andrades
Categories: cs.LG q-bio.QM
Journal-ref: Healthcare 2020, 8(4), 371
DOI: 10.3390/healthcare8040371
\\
  COVID-19 disease has affected almost every country in the world. The large
number of infected people and the different mortality rates between countries
has given rise to many hypotheses about the key points that make the virus so
lethal in some places. In this study, the eating habits of 170 countries were
evaluated in order to find correlations between these habits and mortality
rates caused by COVID-19 using machine learning techniques that group the
countries together according to the different distribution of fat, energy, and
protein across 23 different types of food, as well as the amount ingested in
kilograms. Results shown how obesity and the high consumption of fats appear in
countries with the highest death rates, whereas countries with a lower rate
have a higher level of cereal consumption accompanied by a lower total average
intake of kilocalories.
\\ ( https://arxiv.org/abs/2402.12558 ,  1281kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12562
Date: Mon, 19 Feb 2024 21:36:54 GMT   (144kb)

Title: Dynamic Pricing and Learning with Long-term Reference Effects
Authors: Shipra Agrawal, Wei Tang
Categories: cs.LG cs.GT
Comments: 48 pages, two figures
\\
  We consider a dynamic pricing problem where customer response to the current
price is impacted by the customer price expectation, aka reference price. We
study a simple and novel reference price mechanism where reference price is the
average of the past prices offered by the seller. As opposed to the more
commonly studied exponential smoothing mechanism, in our reference price
mechanism the prices offered by seller have a longer term effect on the future
customer expectations.
  We show that under this mechanism, a markdown policy is near-optimal
irrespective of the parameters of the model. This matches the common intuition
that a seller may be better off by starting with a higher price and then
decreasing it, as the customers feel like they are getting bargains on items
that are ordinarily more expensive. For linear demand models, we also provide a
detailed characterization of the near-optimal markdown policy along with an
efficient way of computing it.
  We then consider a more challenging dynamic pricing and learning problem,
where the demand model parameters are apriori unknown, and the seller needs to
learn them online from the customers' responses to the offered prices while
simultaneously optimizing revenue. The objective is to minimize regret, i.e.,
the $T$-round revenue loss compared to a clairvoyant optimal policy. This task
essentially amounts to learning a non-stationary optimal policy in a
time-variant Markov Decision Process (MDP). For linear demand models, we
provide an efficient learning algorithm with an optimal $\tilde{O}(\sqrt{T})$
regret upper bound.
\\ ( https://arxiv.org/abs/2402.12562 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12570
Date: Mon, 19 Feb 2024 21:52:44 GMT   (145kb,D)

Title: Offline Multi-task Transfer RL with Representational Penalization
Authors: Avinandan Bose, Simon Shaolei Du, Maryam Fazel
Categories: cs.LG
\\
  We study the problem of representation transfer in offline Reinforcement
Learning (RL), where a learner has access to episodic data from a number of
source tasks collected a priori, and aims to learn a shared representation to
be used in finding a good policy for a target task. Unlike in online RL where
the agent interacts with the environment while learning a policy, in the
offline setting there cannot be such interactions in either the source tasks or
the target task; thus multi-task offline RL can suffer from incomplete
coverage.
  We propose an algorithm to compute pointwise uncertainty measures for the
learnt representation, and establish a data-dependent upper bound for the
suboptimality of the learnt policy for the target task. Our algorithm leverages
the collective exploration done by source tasks to mitigate poor coverage at
some points by a few tasks, thus overcoming the limitation of needing uniformly
good coverage for a meaningful transfer by existing offline algorithms. We
complement our theoretical results with empirical evaluation on a
rich-observation MDP which requires many samples for complete coverage. Our
findings illustrate the benefits of penalizing and quantifying the uncertainty
in the learnt representation.
\\ ( https://arxiv.org/abs/2402.12570 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12572
Date: Mon, 19 Feb 2024 21:53:43 GMT   (2891kb,D)

Title: FairProof : Confidential and Certifiable Fairness for Neural Networks
Authors: Chhavi Yadav, Amrita Roy Chowdhury, Dan Boneh, Kamalika Chaudhuri
Categories: cs.LG cs.AI cs.CR
\\
  Machine learning models are increasingly used in societal applications, yet
legal and privacy concerns demand that they very often be kept confidential.
Consequently, there is a growing distrust about the fairness properties of
these models in the minds of consumers, who are often at the receiving end of
model predictions. To this end, we propose FairProof - a system that uses
Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the
fairness of a model, while maintaining confidentiality. We also propose a
fairness certification algorithm for fully-connected neural networks which is
befitting to ZKPs and is used in this system. We implement FairProof in Gnark
and demonstrate empirically that our system is practically feasible.
\\ ( https://arxiv.org/abs/2402.12572 ,  2891kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12598
Date: Mon, 19 Feb 2024 23:22:30 GMT   (5152kb,D)

Title: Graph-based Virtual Sensing from Sparse and Partial Multivariate
  Observations
Authors: Giovanni De Felice, Andrea Cini, Daniele Zambon, Vladimir V. Gusev,
  Cesare Alippi
Categories: cs.LG cs.AI
Comments: Accepted at ICLR 2024
\\
  Virtual sensing techniques allow for inferring signals at new unmonitored
locations by exploiting spatio-temporal measurements coming from physical
sensors at different locations. However, as the sensor coverage becomes sparse
due to costs or other constraints, physical proximity cannot be used to support
interpolation. In this paper, we overcome this challenge by leveraging
dependencies between the target variable and a set of correlated variables
(covariates) that can frequently be associated with each location of interest.
From this viewpoint, covariates provide partial observability, and the problem
consists of inferring values for unobserved channels by exploiting observations
at other locations to learn how such variables can correlate. We introduce a
novel graph-based methodology to exploit such relationships and design a graph
deep learning architecture, named GgNet, implementing the framework. The
proposed approach relies on propagating information over a nested graph
structure that is used to learn dependencies between variables as well as
locations. GgNet is extensively evaluated under different virtual sensing
scenarios, demonstrating higher reconstruction accuracy compared to the
state-of-the-art.
\\ ( https://arxiv.org/abs/2402.12598 ,  5152kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12613
Date: Tue, 20 Feb 2024 00:34:58 GMT   (3711kb,D)

Title: Analysis of Using Sigmoid Loss for Contrastive Learning
Authors: Chungpa Lee, Joonhwan Chang, Jy-yong Sohn
Categories: cs.LG
Journal-ref: Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024, Valencia, Spain
\\
  Contrastive learning has emerged as a prominent branch of self-supervised
learning for several years. Especially, CLIP, which applies contrastive
learning to large sets of captioned images, has garnered significant attention.
Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid
loss instead of the standard InfoNCE loss. SigLIP achieves the performance
comparable to CLIP in a more efficient manner by eliminating the need for a
global view. However, theoretical understanding of using the sigmoid loss in
contrastive learning is underexplored. In this paper, we provide a theoretical
analysis of using the sigmoid loss in contrastive learning, in the perspective
of the geometric structure of learned embeddings. First, we propose the
double-Constant Embedding Model (CCEM), a framework for parameterizing various
well-known embedding structures by a single variable. Interestingly, the
proposed CCEM is proven to contain the optimal embedding with respect to the
sigmoid loss. Second, we mathematically analyze the optimal embedding
minimizing the sigmoid loss for contrastive learning. The optimal embedding
ranges from simplex equiangular-tight-frame to antipodal structure, depending
on the temperature parameter used in the sigmoid loss. Third, our experimental
results on synthetic datasets coincide with the theoretical results on the
optimal embedding structures.
\\ ( https://arxiv.org/abs/2402.12613 ,  3711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12616
Date: Tue, 20 Feb 2024 00:50:26 GMT   (262kb,D)

Title: Multi-objective Binary Coordinate Search for Feature Selection
Authors: Sevil Zanjani Miyandoab, Shahryar Rahnamayan, Azam Asilian Bidgoli
Categories: cs.LG cs.NE
Comments: 8 pages, 1 figure
ACM-class: I.2.6
Journal-ref: 2023 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC), pp. 4176-4183. IEEE, 2023
DOI: 10.1109/SMC53992.2023.10394067
\\
  A supervised feature selection method selects an appropriate but concise set
of features to differentiate classes, which is highly expensive for large-scale
datasets. Therefore, feature selection should aim at both minimizing the number
of selected features and maximizing the accuracy of classification, or any
other task. However, this crucial task is computationally highly demanding on
many real-world datasets and requires a very efficient algorithm to reach a set
of optimal features with a limited number of fitness evaluations. For this
purpose, we have proposed the binary multi-objective coordinate search (MOCS)
algorithm to solve large-scale feature selection problems. To the best of our
knowledge, the proposed algorithm in this paper is the first multi-objective
coordinate search algorithm. In this method, we generate new individuals by
flipping a variable of the candidate solutions on the Pareto front. This
enables us to investigate the effectiveness of each feature in the
corresponding subset. In fact, this strategy can play the role of crossover and
mutation operators to generate distinct subsets of features. The reported
results indicate the significant superiority of our method over NSGA-II, on
five real-world large-scale datasets, particularly when the computing budget is
limited. Moreover, this simple hyper-parameter-free algorithm can solve feature
selection much faster and more efficiently than NSGA-II.
\\ ( https://arxiv.org/abs/2402.12616 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12621
Date: Tue, 20 Feb 2024 01:04:21 GMT   (7495kb,D)

Title: Reflect-RL: Two-Player Online RL Fine-Tuning for LMs
Authors: Runlong Zhou, Simon S. Du, Beibin Li
Categories: cs.LG cs.CL
Comments: 25 pages, 13 figures
\\
  As language models (LMs) demonstrate their capabilities in various fields,
their application to tasks requiring multi-round interactions has become
increasingly popular. These tasks usually have complex dynamics, so supervised
fine-tuning (SFT) on a limited offline dataset does not yield good performance.
However, only a few works attempted to directly train the LMs within
interactive decision-making environments. We aim to create an effective
mechanism to fine-tune LMs with online reinforcement learning (RL) in these
environments. We propose Reflect-RL, a two-player system to fine-tune an LM
using online RL, where a frozen reflection model assists the policy model. To
generate data for the warm-up SFT stage, we use negative example generation to
enhance the error-correction ability of the reflection model. Furthermore, we
designed single-prompt action enumeration and applied curriculum learning to
allow the policy model to learn more efficiently. Empirically, we verify that
Reflect-RL outperforms SFT and online RL without reflection. Testing results
indicate GPT-2-xl after Reflect-RL also outperforms those of untuned
pre-trained LMs, such as Mistral 7B.
\\ ( https://arxiv.org/abs/2402.12621 ,  7495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12625
Date: Tue, 20 Feb 2024 01:10:12 GMT   (572kb,D)

Title: Compact NSGA-II for Multi-objective Feature Selection
Authors: Sevil Zanjani Miyandoab, Shahryar Rahnamayan, Azam Asilian Bidgoli
Categories: cs.LG cs.NE
Comments: 8 pages, 2 figures
ACM-class: I.2.6
Journal-ref: 2023 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC), pp. 3868-3875. IEEE, 2023
DOI: 10.1109/SMC53992.2023.10394458
\\
  Feature selection is an expensive challenging task in machine learning and
data mining aimed at removing irrelevant and redundant features. This
contributes to an improvement in classification accuracy, as well as the budget
and memory requirements for classification, or any other post-processing task
conducted after feature selection. In this regard, we define feature selection
as a multi-objective binary optimization task with the objectives of maximizing
classification accuracy and minimizing the number of selected features. In
order to select optimal features, we have proposed a binary Compact NSGA-II
(CNSGA-II) algorithm. Compactness represents the population as a probability
distribution to enhance evolutionary algorithms not only to be more
memory-efficient but also to reduce the number of fitness evaluations. Instead
of holding two populations during the optimization process, our proposed method
uses several Probability Vectors (PVs) to generate new individuals. Each PV
efficiently explores a region of the search space to find non-dominated
solutions instead of generating candidate solutions from a small population as
is the common approach in most evolutionary algorithms. To the best of our
knowledge, this is the first compact multi-objective algorithm proposed for
feature selection. The reported results for expensive optimization cases with a
limited budget on five datasets show that the CNSGA-II performs more
efficiently than the well-known NSGA-II method in terms of the hypervolume (HV)
performance metric requiring less memory. The proposed method and experimental
results are explained and analyzed in detail.
\\ ( https://arxiv.org/abs/2402.12625 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12626
Date: Tue, 20 Feb 2024 01:12:59 GMT   (10922kb,D)

Title: Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors
Authors: Yiwei Lu, Matthew Y.R. Yang, Gautam Kamath, Yaoliang Yu
Categories: cs.LG cs.CR
Comments: Accepted to SaTML 2024
\\
  Machine learning models have achieved great success in supervised learning
tasks for end-to-end training, which requires a large amount of labeled data
that is not always feasible. Recently, many practitioners have shifted to
self-supervised learning methods that utilize cheap unlabeled data to learn a
general feature extractor via pre-training, which can be further applied to
personalized downstream tasks by simply training an additional linear layer
with limited labeled data. However, such a process may also raise concerns
regarding data poisoning attacks. For instance, indiscriminate data poisoning
attacks, which aim to decrease model utility by injecting a small number of
poisoned data into the training set, pose a security risk to machine learning
models, but have only been studied for end-to-end supervised learning. In this
paper, we extend the exploration of the threat of indiscriminate attacks on
downstream tasks that apply pre-trained feature extractors. Specifically, we
propose two types of attacks: (1) the input space attacks, where we modify
existing attacks to directly craft poisoned data in the input space. However,
due to the difficulty of optimization under constraints, we further propose (2)
the feature targeted attacks, where we mitigate the challenge with three
stages, firstly acquiring target parameters for the linear head; secondly
finding poisoned features by treating the learned feature representations as a
dataset; and thirdly inverting the poisoned features back to the input space.
Our experiments examine such attacks in popular downstream tasks of fine-tuning
on the same dataset and transfer learning that considers domain adaptation.
Empirical results reveal that transfer learning is more vulnerable to our
attacks. Additionally, input space attacks are a strong threat if no
countermeasures are posed, but are otherwise weaker than feature targeted
attacks.
\\ ( https://arxiv.org/abs/2402.12626 ,  10922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12627
Date: Tue, 20 Feb 2024 01:16:01 GMT   (2460kb,D)

Title: A Comprehensive Review of Machine Learning Advances on Data Change: A
  Cross-Field Perspective
Authors: Jeng-Lin Li, Chih-Fan Hsu, Ming-Ching Chang, Wei-Chao Chen
Categories: cs.LG cs.AI cs.CV
\\
  Recent artificial intelligence (AI) technologies show remarkable evolution in
various academic fields and industries. However, in the real world, dynamic
data lead to principal challenges for deploying AI models. An unexpected data
change brings about severe performance degradation in AI models. We identify
two major related research fields, domain shift and concept drift according to
the setting of the data change. Although these two popular research fields aim
to solve distribution shift and non-stationary data stream problems, the
underlying properties remain similar which also encourages similar technical
approaches. In this review, we regroup domain shift and concept drift into a
single research problem, namely the data change problem, with a systematic
overview of state-of-the-art methods in the two research fields. We propose a
three-phase problem categorization scheme to link the key ideas in the two
technical fields. We thus provide a novel scope for researchers to explore
contemporary technical strategies, learn industrial applications, and identify
future directions for addressing data change challenges.
\\ ( https://arxiv.org/abs/2402.12627 ,  2460kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12646
Date: Tue, 20 Feb 2024 01:47:25 GMT   (4140kb,D)

Title: Training Artificial Neural Networks by Coordinate Search Algorithm
Authors: Ehsan Rokhsatyazdi, Shahryar Rahnamayan, Sevil Zanjani Miyandoab, Azam
  Asilian Bidgoli, H.R. Tizhoosh
Categories: cs.LG cs.AI
Comments: 7 pages, 9 figures
ACM-class: I.2.6
Journal-ref: 2023 IEEE Symposium Series on Computational Intelligence (SSCI),
  pp. 1540-1546. IEEE, 2023
DOI: 10.1109/SSCI52147.2023.10371958
\\
  Training Artificial Neural Networks poses a challenging and critical problem
in machine learning. Despite the effectiveness of gradient-based learning
methods, such as Stochastic Gradient Descent (SGD), in training neural
networks, they do have several limitations. For instance, they require
differentiable activation functions, and cannot optimize a model based on
several independent non-differentiable loss functions simultaneously; for
example, the F1-score, which is used during testing, can be used during
training when a gradient-free optimization algorithm is utilized. Furthermore,
the training in any DNN can be possible with a small size of the training
dataset. To address these concerns, we propose an efficient version of the
gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern
Search methods, for training neural networks. The proposed algorithm can be
used with non-differentiable activation functions and tailored to
multi-objective/multi-loss problems. Finding the optimal values for weights of
ANNs is a large-scale optimization problem. Therefore instead of finding the
optimal value for each variable, which is the common technique in classical CS,
we accelerate optimization and convergence by bundling the weights. In fact,
this strategy is a form of dimension reduction for optimization problems. Based
on the experimental results, the proposed method, in some cases, outperforms
the gradient-based approach, particularly, in situations with insufficient
labeled training data. The performance plots demonstrate a high convergence
rate, highlighting the capability of our suggested method to find a reasonable
solution with fewer function calls. As of now, the only practical and efficient
way of training ANNs with hundreds of thousands of weights is gradient-based
algorithms such as SGD or Adam. In this paper we introduce an alternative
method for training ANN.
\\ ( https://arxiv.org/abs/2402.12646 ,  4140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12656
Date: Tue, 20 Feb 2024 02:09:55 GMT   (819kb)

Title: HyperMoE: Towards Better Mixture of Experts via Transferring Among
  Experts
Authors: Hao Zhao, Zihan Qiu, Huijia Wu, Zili Wang, Zhaofeng He, Jie Fu
Categories: cs.LG cs.AI
\\
  The Mixture of Experts (MoE) for language models has been proven effective in
augmenting the capacity of models by dynamically routing each input token to a
specific subset of experts for processing. Despite the success, most existing
methods face a challenge for balance between sparsity and the availability of
expert knowledge: enhancing performance through increased use of expert
knowledge often results in diminishing sparsity during expert selection. To
mitigate this contradiction, we propose HyperMoE, a novel MoE framework built
upon Hypernetworks. This framework integrates the computational processes of
MoE with the concept of knowledge transferring in multi-task learning. Specific
modules generated based on the information of unselected experts serve as
supplementary information, which allows the knowledge of experts not selected
to be used while maintaining selection sparsity. Our comprehensive empirical
evaluations across multiple datasets and backbones establish that HyperMoE
significantly outperforms existing MoE methods under identical conditions
concerning the number of experts.
\\ ( https://arxiv.org/abs/2402.12656 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12664
Date: Tue, 20 Feb 2024 02:26:48 GMT   (2750kb,D)

Title: Discriminant Distance-Aware Representation on Deterministic Uncertainty
  Quantification Methods
Authors: Jiaxin Zhang, Kamalika Das, Sricharan Kumar
Categories: cs.LG
Comments: AISTATS 2024
\\
  Uncertainty estimation is a crucial aspect of deploying dependable deep
learning models in safety-critical systems. In this study, we introduce a novel
and efficient method for deterministic uncertainty estimation called
Discriminant Distance-Awareness Representation (DDAR). Our approach involves
constructing a DNN model that incorporates a set of prototypes in its latent
representations, enabling us to analyze valuable feature information from the
input data. By leveraging a distinction maximization layer over optimal
trainable prototypes, DDAR can learn a discriminant distance-awareness
representation. We demonstrate that DDAR overcomes feature collapse by relaxing
the Lipschitz constraint that hinders the practicality of deterministic
uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a
flexible and architecture-agnostic method that can be easily integrated as a
pluggable layer with distance-sensitive metrics, outperforming state-of-the-art
uncertainty estimation methods on multiple benchmark problems.
\\ ( https://arxiv.org/abs/2402.12664 ,  2750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12673
Date: Tue, 20 Feb 2024 02:45:20 GMT   (8240kb,D)

Title: Beyond Worst-case Attacks: Robust RL with Adaptive Defense via
  Non-dominated Policies
Authors: Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang
Categories: cs.LG
Comments: International Conference on Learning Representations (ICLR) 2024,
  spotlight
\\
  In light of the burgeoning success of reinforcement learning (RL) in diverse
real-world applications, considerable focus has been directed towards ensuring
RL policies are robust to adversarial attacks during test time. Current
approaches largely revolve around solving a minimax problem to prepare for
potential worst-case scenarios. While effective against strong attacks, these
methods often compromise performance in the absence of attacks or the presence
of only weak attacks. To address this, we study policy robustness under the
well-accepted state-adversarial attack model, extending our focus beyond only
worst-case attacks. We first formalize this task at test time as a regret
minimization problem and establish its intrinsic hardness in achieving
sublinear regret when the baseline policy is from a general continuous policy
class, $\Pi$. This finding prompts us to \textit{refine} the baseline policy
class $\Pi$ prior to test time, aiming for efficient adaptation within a finite
policy class $\Tilde{\Pi}$, which can resort to an adversarial bandit
subroutine. In light of the importance of a small, finite $\Tilde{\Pi}$, we
propose a novel training-time algorithm to iteratively discover
\textit{non-dominated policies}, forming a near-optimal and minimal
$\Tilde{\Pi}$, thereby ensuring both robustness and test-time efficiency.
Empirical validation on the Mujoco corroborates the superiority of our approach
in terms of natural and robust performance, as well as adaptability to various
attack scenarios.
\\ ( https://arxiv.org/abs/2402.12673 ,  8240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12683
Date: Tue, 20 Feb 2024 03:14:47 GMT   (61kb,D)

Title: TorchCP: A Library for Conformal Prediction based on PyTorch
Authors: Hongxin Wei, Jianguo Huang
Categories: cs.LG cs.CV math.ST stat.TH
\\
  TorchCP is a Python toolbox for conformal prediction research on deep
learning models. It contains various implementations for posthoc and training
methods for classification and regression tasks (including multi-dimension
output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the
advantages of matrix computation to provide concise and efficient inference
implementations. The code is licensed under the LGPL license and is
open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this
https URL}}$.
\\ ( https://arxiv.org/abs/2402.12683 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12687
Date: Tue, 20 Feb 2024 03:27:53 GMT   (575kb,D)

Title: Learning on manifolds without manifold learning
Authors: H. N. Mhaskar and Ryan O'Dowd
Categories: cs.LG stat.ML
\\
  Function approximation based on data drawn randomly from an unknown
distribution is an important problem in machine learning. In contrast to the
prevalent paradigm of solving this problem by minimizing a loss functional, we
have given a direct one-shot construction together with optimal error bounds
under the manifold assumption; i.e., one assumes that the data is sampled from
an unknown sub-manifold of a high dimensional Euclidean space. A great deal of
research deals with obtaining information about this manifold, such as the
eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and
using this information for function approximation. This two step approach
implies some extra errors in the approximation stemming from basic quantities
of the data in addition to the errors inherent in function approximation. In
Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to
achieve function approximation without requiring the extraction of any
information about the manifold other than its dimension. However, one cannot
pin down the class of approximants used in that paper.
  In this paper, we view the unknown manifold as a sub-manifold of an ambient
hypersphere and study the question of constructing a one-shot approximation
using the spherical polynomials based on the hypersphere. Our approach does not
require preprocessing of the data to obtain information about the manifold
other than its dimension. We give optimal rates of approximation for relatively
"rough" functions.
\\ ( https://arxiv.org/abs/2402.12687 ,  575kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12694
Date: Tue, 20 Feb 2024 03:45:59 GMT   (10707kb,D)

Title: Revitalizing Multivariate Time Series Forecasting: Learnable
  Decomposition with Inter-Series Dependencies and Intra-Series Variations
  Modeling
Authors: Guoqi Yu, Jing Zou, Xiaowei Hu, Angelica I. Aviles-Rivero, Jing Qin
  and Shujun Wang
Categories: cs.LG
\\
  Predicting multivariate time series is crucial, demanding precise modeling of
intricate patterns, including inter-series dependencies and intra-series
variations. Distinctive trend characteristics in each time series pose
challenges, and existing methods, relying on basic moving average kernels, may
struggle with the non-linear structure and complex trends in real-world data.
Given that, we introduce a learnable decomposition strategy to capture dynamic
trend information more reasonably. Additionally, we propose a dual attention
module tailored to capture inter-series dependencies and intra-series
variations simultaneously for better time series forecasting, which is
implemented by channel-wise self-attention and autoregressive self-attention.
To evaluate the effectiveness of our method, we conducted experiments across
eight open-source datasets and compared it with the state-of-the-art methods.
Through the comparison results, our Leddam (LEarnable Decomposition and Dual
Attention Module) not only demonstrates significant advancements in predictive
performance, but also the proposed decomposition strategy can be plugged into
other methods with a large performance-boosting, from 11.87% to 48.56% MSE
error degradation.
\\ ( https://arxiv.org/abs/2402.12694 ,  10707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12711
Date: Tue, 20 Feb 2024 04:21:13 GMT   (71kb)

Title: Achieving Near-Optimal Regret for Bandit Algorithms with Uniform
  Last-Iterate Guarantee
Authors: Junyan Liu, Yunfan Li, Lin Yang
Categories: cs.LG stat.ML
\\
  Existing performance measures for bandit algorithms such as regret, PAC
bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative
performance, while allowing the play of an arbitrarily bad arm at any finite
time t. Such a behavior can be highly detrimental in high-stakes applications.
This paper introduces a stronger performance measure, the uniform last-iterate
(ULI) guarantee, capturing both cumulative and instantaneous performance of
bandit algorithms. Specifically, ULI characterizes the instantaneous
performance since it ensures that the per-round regret of the played arm is
bounded by a function, monotonically decreasing w.r.t. (large) round t,
preventing revisits to bad arms when sufficient samples are available. We
demonstrate that a near-optimal ULI guarantee directly implies near-optimal
cumulative performance across aforementioned performance measures. To examine
the achievability of ULI in the finite arm setting, we first provide two
positive results that some elimination-based algorithms and high-probability
adversarial algorithms with stronger analysis or additional designs, can attain
near-optimal ULI guarantees. Then, we also provide a negative result,
indicating that optimistic algorithms cannot achieve a near-optimal ULI
guarantee. Finally, we propose an efficient algorithm for linear bandits with
infinitely many arms, which achieves the ULI guarantee, given access to an
optimization oracle.
\\ ( https://arxiv.org/abs/2402.12711 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12714
Date: Tue, 20 Feb 2024 04:40:00 GMT   (7366kb,D)

Title: Equivariant Pretrained Transformer for Unified Geometric Learning on
  Multi-Domain 3D Molecules
Authors: Rui Jiao, Xiangzhe Kong, Ziyang Yu, Wenbing Huang and Yang Liu
Categories: cs.LG
\\
  Pretraining on a large number of unlabeled 3D molecules has showcased
superiority in various scientific applications. However, prior efforts
typically focus on pretraining models on a specific domain, either proteins or
small molecules, missing the opportunity to leverage the cross-domain
knowledge. To mitigate this gap, we introduce Equivariant Pretrained
Transformer (EPT), a novel pretraining framework designed to harmonize the
geometric learning of small molecules and proteins. To be specific, EPT unifies
the geometric modeling of multi-domain molecules via the block-enhanced
representation that can attend a broader context of each atom. Upon transformer
framework, EPT is further enhanced with E(3) equivariance to facilitate the
accurate representation of 3D structures. Another key innovation of EPT is its
block-level pretraining task, which allows for joint pretraining on datasets
comprising both small molecules and proteins. Experimental evaluations on a
diverse group of benchmarks, including ligand binding affinity prediction,
molecular property prediction, and protein property prediction, show that EPT
significantly outperforms previous SOTA methods for affinity prediction, and
achieves the best or comparable performance with existing domain-specific
pretraining models for other tasks.
\\ ( https://arxiv.org/abs/2402.12714 ,  7366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12715
Date: Tue, 20 Feb 2024 04:49:34 GMT   (298kb,D)

Title: Spurious Correlations in Machine Learning: A Survey
Authors: Wenqian Ye, Guangtao Zheng, Xu Cao, Yunsheng Ma, Xia Hu, Aidong Zhang
Categories: cs.LG
\\
  Machine learning systems are known to be sensitive to spurious correlations
between biased features of the inputs (e.g., background, texture, and secondary
objects) and the corresponding labels. These features and their correlations
with the labels are known as "spurious" because they tend to change with shifts
in real-world data distributions, which can negatively impact the model's
generalization and robustness. In this survey, we provide a comprehensive
review of this issue, along with a taxonomy of current state-of-the-art methods
for addressing spurious correlations in machine learning models. Additionally,
we summarize existing datasets, benchmarks, and metrics to aid future research.
The paper concludes with a discussion of the recent advancements and future
research challenges in this field, aiming to provide valuable insights for
researchers in the related domains.
\\ ( https://arxiv.org/abs/2402.12715 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12722
Date: Tue, 20 Feb 2024 05:11:20 GMT   (12425kb,D)

Title: Structural Knowledge Informed Continual Multivariate Time Series
  Forecasting
Authors: Zijie Pan, Yushan Jiang, Dongjin Song, Sahil Garg, Kashif Rasul,
  Anderson Schneider, Yuriy Nevmyvaka
Categories: cs.LG
\\
  Recent studies in multivariate time series (MTS) forecasting reveal that
explicitly modeling the hidden dependencies among different time series can
yield promising forecasting performance and reliable explanations. However,
modeling variable dependencies remains underexplored when MTS is continuously
accumulated under different regimes (stages). Due to the potential distribution
and dependency disparities, the underlying model may encounter the catastrophic
forgetting problem, i.e., it is challenging to memorize and infer different
types of variable dependencies across different regimes while maintaining
forecasting performance. To address this issue, we propose a novel Structural
Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS
forecasting within a continual learning paradigm, which leverages structural
knowledge to steer the forecasting model toward identifying and adapting to
different regimes, and selects representative MTS samples from each regime for
memory replay. Specifically, we develop a forecasting model based on graph
structure learning, where a consistency regularization scheme is imposed
between the learned variable dependencies and the structural knowledge while
optimizing the forecasting objective over the MTS data. As such, MTS
representations learned in each regime are associated with distinct structural
knowledge, which helps the model memorize a variety of conceivable scenarios
and results in accurate forecasts in the continual learning context. Meanwhile,
we develop a representation-matching memory replay scheme that maximizes the
temporal coverage of MTS data to efficiently preserve the underlying temporal
dynamics and dependency structures of each regime. Thorough empirical studies
on synthetic and real-world benchmarks validate SKI-CL's efficacy and
advantages over the state-of-the-art for continual MTS forecasting tasks.
\\ ( https://arxiv.org/abs/2402.12722 ,  12425kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12727
Date: Tue, 20 Feb 2024 05:28:13 GMT   (1851kb,D)

Title: Diffusion Posterior Sampling is Computationally Intractable
Authors: Shivam Gupta, Ajil Jalal, Aditya Parulekar, Eric Price, Zhiyang Xun
Categories: cs.LG cs.AI math.ST stat.ML stat.TH
\\
  Diffusion models are a remarkably effective way of learning and sampling from
a distribution $p(x)$. In posterior sampling, one is also given a measurement
model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x
\mid y)$. Posterior sampling is useful for tasks such as inpainting,
super-resolution, and MRI reconstruction, so a number of recent works have
given algorithms to heuristically approximate it; but none are known to
converge to the correct distribution in polynomial time.
  In this paper we show that posterior sampling is \emph{computationally
intractable}: under the most basic assumption in cryptography -- that one-way
functions exist -- there are instances for which \emph{every} algorithm takes
superpolynomial time, even though \emph{unconditional} sampling is provably
fast. We also show that the exponential-time rejection sampling algorithm is
essentially optimal under the stronger plausible assumption that there are
one-way functions that take exponential time to invert.
\\ ( https://arxiv.org/abs/2402.12727 ,  1851kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12729
Date: Tue, 20 Feb 2024 05:39:32 GMT   (27047kb,D)

Title: Scalable and reliable deep transfer learning for intelligent fault
  detection via multi-scale neural processes embedded with knowledge
Authors: Zhongzhi Li, Jingqi Tu, Jiacheng Zhu, Jianliang Ai, Yiqun Dong
Categories: cs.LG cs.AI
\\
  Deep transfer learning (DTL) is a fundamental method in the field of
Intelligent Fault Detection (IFD). It aims to mitigate the degradation of
method performance that arises from the discrepancies in data distribution
between training set (source domain) and testing set (target domain).
Considering the fact that fault data collection is challenging and certain
faults are scarce, DTL-based methods face the limitation of available
observable data, which reduces the detection performance of the methods in the
target domain. Furthermore, DTL-based methods lack comprehensive uncertainty
analysis that is essential for building reliable IFD systems. To address the
aforementioned problems, this paper proposes a novel DTL-based method known as
Neural Processes-based deep transfer learning with graph convolution network
(GTNP). Feature-based transfer strategy of GTNP bridges the data distribution
discrepancies of source domain and target domain in high-dimensional space.
Both the joint modeling based on global and local latent variables and sparse
sampling strategy reduce the demand of observable data in the target domain.
The multi-scale uncertainty analysis is obtained by using the distribution
characteristics of global and local latent variables. Global analysis of
uncertainty enables GTNP to provide quantitative values that reflect the
complexity of methods and the difficulty of tasks. Local analysis of
uncertainty allows GTNP to model uncertainty (confidence of the fault detection
result) at each sample affected by noise and bias. The validation of the
proposed method is conducted across 3 IFD tasks, consistently showing the
superior detection performance of GTNP compared to the other DTL-based methods.
\\ ( https://arxiv.org/abs/2402.12729 ,  27047kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12737
Date: Tue, 20 Feb 2024 06:04:44 GMT   (175kb,D)

Title: Guarantee Regions for Local Explanations
Authors: Marton Havasi, Sonali Parbhoo, Finale Doshi-Velez
Categories: cs.LG
\\
  Interpretability methods that utilise local surrogate models (e.g. LIME) are
very good at describing the behaviour of the predictive model at a point of
interest, but they are not guaranteed to extrapolate to the local region
surrounding the point. However, overfitting to the local curvature of the
predictive model and malicious tampering can significantly limit extrapolation.
We propose an anchor-based algorithm for identifying regions in which local
explanations are guaranteed to be correct by explicitly describing those
intervals along which the input features can be trusted. Our method produces an
interpretable feature-aligned box where the prediction of the local surrogate
model is guaranteed to match the predictive model. We demonstrate that our
algorithm can be used to find explanations with larger guarantee regions that
better cover the data manifold compared to existing baselines. We also show how
our method can identify misleading local explanations with significantly poorer
guarantee regions.
\\ ( https://arxiv.org/abs/2402.12737 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12756
Date: Tue, 20 Feb 2024 06:49:43 GMT   (248kb,D)

Title: Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi
  Fingerprinting: A Discussion from a Data Perspective
Authors: Zhe Tang, Ruocheng Gu, Sihao Li, Kyeong Soo Kim, Jeremy S. Smith
Categories: cs.LG cs.NI
Comments: 10 pages, 5 figures, Invited paper with Excellent Paper Award to be
  presented at ICAIIC 2024, Osaka, Japan, Feb. 19--22, 2023
\\
  Wi-Fi fingerprinting has emerged as the most popular approach to indoor
localization. The use of ML algorithms has greatly improved the localization
performance of Wi-Fi fingerprinting, but its success depends on the
availability of fingerprint databases composed of a large number of RSSIs, the
MAC addresses of access points, and the other measurement information. However,
most fingerprint databases do not reflect well the time varying nature of
electromagnetic interferences in complicated modern indoor environment. This
could result in significant changes in statistical characteristics of
training/validation and testing datasets, which are often constructed at
different times, and even the characteristics of the testing datasets could be
different from those of the data submitted by users during the operation of
localization systems after their deployment. In this paper, we consider the
implications of time-varying Wi-Fi fingerprints on indoor localization from a
data-centric point of view and discuss the differences between static and
dynamic databases. As a case study, we have constructed a dynamic database
covering three floors of the IR building of XJTLU based on RSSI measurements,
over 44 days, and investigated the differences between static and dynamic
databases in terms of statistical characteristics and localization performance.
The analyses based on variance calculations and Isolation Forest show the
temporal shifts in RSSIs, which result in a noticeable trend of the increase in
the localization error of a Gaussian process regression model with the maximum
error of 6.65 m after 14 days of training without model adjustments. The
results of the case study with the XJTLU dynamic database clearly demonstrate
the limitations of static databases and the importance of the creation and
adoption of dynamic databases for future indoor localization research and
real-world deployment.
\\ ( https://arxiv.org/abs/2402.12756 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12761
Date: Tue, 20 Feb 2024 07:03:59 GMT   (1353kb,D)

Title: FGAD: Self-boosted Knowledge Distillation for An Effective Federated
  Graph Anomaly Detection Framework
Authors: Jinyu Cai, Yunhe Zhang, Zhoumin Lu, Wenzhong Guo, See-kiong Ng
Categories: cs.LG
\\
  Graph anomaly detection (GAD) aims to identify anomalous graphs that
significantly deviate from other ones, which has raised growing attention due
to the broad existence and complexity of graph-structured data in many
real-world scenarios. However, existing GAD methods usually execute with
centralized training, which may lead to privacy leakage risk in some sensitive
cases, thereby impeding collaboration among organizations seeking to
collectively develop robust GAD models. Although federated learning offers a
promising solution, the prevalent non-IID problems and high communication costs
present significant challenges, particularly pronounced in collaborations with
graph data distributed among different participants. To tackle these
challenges, we propose an effective federated graph anomaly detection framework
(FGAD). We first introduce an anomaly generator to perturb the normal graphs to
be anomalous, and train a powerful anomaly detector by distinguishing generated
anomalous graphs from normal ones. Then, we leverage a student model to distill
knowledge from the trained anomaly detector (teacher model), which aims to
maintain the personality of local models and alleviate the adverse impact of
non-IID problems. Moreover, we design an effective collaborative learning
mechanism that facilitates the personalization preservation of local models and
significantly reduces communication costs among clients. Empirical results of
the GAD tasks on non-IID graphs compared with state-of-the-art baselines
demonstrate the superiority and efficiency of the proposed FGAD method.
\\ ( https://arxiv.org/abs/2402.12761 ,  1353kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12767
Date: Tue, 20 Feb 2024 07:16:12 GMT   (1878kb,D)

Title: When and How: Learning Identifiable Latent States for Nonstationary Time
  Series Forecasting
Authors: Zijian Li, Ruichu Cai, Zhenhui Yang, Haiqin Huang, Guangyi Chen, Yifan
  Shen, Zhengming Chen, Xiangchen Song, Zhifeng Hao, Kun Zhang
Categories: cs.LG
\\
  Temporal distribution shifts are ubiquitous in time series data. One of the
most popular methods assumes that the temporal distribution shift occurs
uniformly to disentangle the stationary and nonstationary dependencies. But
this assumption is difficult to meet, as we do not know when the distribution
shifts occur. To solve this problem, we propose to learn IDentifiable latEnt
stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we
further disentangle the stationary and nonstationary latent states via
sufficient observation assumption to learn how the latent states change.
Specifically, we formalize the causal process with environment-irrelated
station- ary and environment-related nonstationary variables. Under mild
conditions, we show that latent environments and stationary/nonstationary
variables are identifiable. Based on these theories, we devise the IDEA model,
which incorporates an autoregressive hidden Markov model to estimate latent
environments and modular prior networks to identify latent states. The IDEA
model outperforms several latest nonstationary forecasting methods on various
benchmark datasets, highlighting its advantages in real-world scenarios.
\\ ( https://arxiv.org/abs/2402.12767 ,  1878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12780
Date: Tue, 20 Feb 2024 07:40:11 GMT   (157kb,D)

Title: Tackling Byzantine Clients in Federated Learning
Authors: Youssef Allouah, Sadegh Farhadkhani, Rachid GuerraouI, Nirupam Gupta,
  Rafael Pinot, Geovani Rizk, Sasha Voitovych
Categories: cs.LG
\\
  The possibility of adversarial (a.k.a., {\em Byzantine}) clients makes
federated learning (FL) prone to arbitrary manipulation. The natural approach
to robustify FL against adversarial clients is to replace the simple averaging
operation at the server in the standard $\mathsf{FedAvg}$ algorithm by a
\emph{robust averaging rule}. While a significant amount of work has been
devoted to studying the convergence of federated {\em robust averaging} (which
we denote by $\mathsf{FedRo}$), prior work has largely ignored the impact of
{\em client subsampling} and {\em local steps}, two fundamental FL
characteristics. While client subsampling increases the effective fraction of
Byzantine clients, local steps increase the drift between the local updates
computed by honest (i.e., non-Byzantine) clients. Consequently, a careless
deployment of $\mathsf{FedRo}$ could yield poor performance. We validate this
observation by presenting an in-depth analysis of $\mathsf{FedRo}$ tightly
analyzing the impact of client subsampling and local steps. Specifically, we
present a sufficient condition on client subsampling for nearly-optimal
convergence of $\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show
that the rate of improvement in learning accuracy {\em diminishes} with respect
to the number of clients subsampled, as soon as the sample size exceeds a
threshold value. Interestingly, we also observe that under a careful choice of
step-sizes, the learning error due to Byzantine clients decreases with the
number of local steps. We validate our theory by experiments on the FEMNIST and
CIFAR-$10$ image classification tasks.
\\ ( https://arxiv.org/abs/2402.12780 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12789
Date: Tue, 20 Feb 2024 07:57:38 GMT   (244kb,D)

Title: Fair Classifiers Without Fair Training: An Influence-Guided Data
  Sampling Approach
Authors: Jinlong Pang, Jialu Wang, Zhaowei Zhu, Yuanshun Yao, Chen Qian and
  Yang Liu
Categories: cs.LG cs.AI
\\
  A fair classifier should ensure the benefit of people from different groups,
while the group information is often sensitive and unsuitable for model
training. Therefore, learning a fair classifier but excluding sensitive
attributes in the training dataset is important. In this paper, we study
learning fair classifiers without implementing fair training algorithms to
avoid possible leakage of sensitive information. Our theoretical analyses
validate the possibility of this approach, that traditional training on a
dataset with an appropriate distribution shift can reduce both the upper bound
for fairness disparity and model generalization error, indicating that fairness
and accuracy can be improved simultaneously with simply traditional training.
We then propose a tractable solution to progressively shift the original
training data during training by sampling influential data, where the sensitive
attribute of new data is not accessed in sampling or used in training.
Extensive experiments on real-world data demonstrate the effectiveness of our
proposed algorithm.
\\ ( https://arxiv.org/abs/2402.12789 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12790
Date: Tue, 20 Feb 2024 07:58:04 GMT   (526kb,D)

Title: From Movements to Metrics: Evaluating Explainable AI Methods in
  Skeleton-Based Human Activity Recognition
Authors: Kimji N. Pellano, Inga Str\"umke, Espen Alexander F. Ihlen
Categories: cs.LG
\\
  The advancement of deep learning in human activity recognition (HAR) using 3D
skeleton data is critical for applications in healthcare, security, sports, and
human-computer interaction. This paper tackles a well-known gap in the field,
which is the lack of testing in the applicability and reliability of XAI
evaluation metrics in the skeleton-based HAR domain. We have tested established
XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM)
and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this
problem. The study also introduces a perturbation method that respects human
biomechanical constraints to ensure realistic variations in human movement. Our
findings indicate that \textit{faithfulness} may not be a reliable metric in
certain contexts, such as with the EfficientGCN model. Conversely, stability
emerges as a more dependable metric when there is slight input data
perturbations. CAM and Grad-CAM are also found to produce almost identical
explanations, leading to very similar XAI metric performance. This calls for
the need for more diversified metrics and new XAI methods applied in
skeleton-based HAR.
\\ ( https://arxiv.org/abs/2402.12790 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12808
Date: Tue, 20 Feb 2024 08:27:50 GMT   (5949kb,D)

Title: Learning Generalization and Regularization of Nonhomogeneous Temporal
  Poisson Processes
Authors: Son Nguyen Van and Hoai Nguyen Xuan
Categories: cs.LG stat.ML
Comments: 32 pages, 15 figures
\\
  The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is
an essentially important counting process with numerous real-world
applications. Up to date, almost all works in the literature have been on the
estimation of NHPPs with infinite data using non-data driven binning methods.
In this paper, we formulate the problem of estimation of NHPPs from finite and
limited data as a learning generalization problem. We mathematically show that
while binning methods are essential for the estimation of NHPPs, they pose a
threat of overfitting when the amount of data is limited. We propose a
framework for regularized learning of NHPPs with two new adaptive and
data-driven binning methods that help to remove the ad-hoc tuning of binning
parameters. Our methods are experimentally tested on synthetic and real-world
datasets and the results show their effectiveness.
\\ ( https://arxiv.org/abs/2402.12808 ,  5949kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12812
Date: Tue, 20 Feb 2024 08:30:46 GMT   (3013kb,D)

Title: Scalable Decentralized Algorithms for Online Personalized Mean
  Estimation
Authors: Franco Galante, Giovanni Neglia, Emilio Leonardi
Categories: cs.LG cs.DC
\\
  In numerous settings, agents lack sufficient data to directly learn a model.
Collaborating with other agents may help, but it introduces a bias-variance
trade-off, when local data distributions differ. A key challenge is for each
agent to identify clients with similar distributions while learning the model,
a problem that remains largely unresolved. This study focuses on a simplified
version of the overarching problem, where each agent collects samples from a
real-valued distribution over time to estimate its mean. Existing algorithms
face impractical space and time complexities (quadratic in the number of agents
A). To address scalability challenges, we propose a framework where agents
self-organize into a graph, allowing each agent to communicate with only a
selected number of peers r. We introduce two collaborative mean estimation
algorithms: one draws inspiration from belief propagation, while the other
employs a consensus-based approach, with complexity of O( r |A| log |A|) and
O(r |A|), respectively. We establish conditions under which both algorithms
yield asymptotically optimal estimates and offer a theoretical characterization
of their performance.
\\ ( https://arxiv.org/abs/2402.12812 ,  3013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12852
Date: Tue, 20 Feb 2024 09:31:03 GMT   (5569kb,D)

Title: CCFC++: Enhancing Federated Clustering through Feature Decorrelation
Authors: Jie Yan, Jing Liu, Yi-Zi Ning and Zhong-Yuan Zhang
Categories: cs.LG
\\
  In federated clustering, multiple data-holding clients collaboratively group
data without exchanging raw data. This field has seen notable advancements
through its marriage with contrastive learning, exemplified by
Cluster-Contrastive Federated Clustering (CCFC). However, CCFC suffers from
heterogeneous data across clients, leading to poor and unrobust performance.
Our study conducts both empirical and theoretical analyses to understand the
impact of heterogeneous data on CCFC. Findings indicate that increased data
heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased
correlations across multiple dimensions of the learned representations. To
address this, we introduce a decorrelation regularizer to CCFC. Benefiting from
the regularizer, the improved method effectively mitigates the detrimental
effects of data heterogeneity, and achieves superior performance, as evidenced
by a marked increase in NMI scores, with the gain reaching as high as 0.32 in
the most pronounced case.
\\ ( https://arxiv.org/abs/2402.12852 ,  5569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12854
Date: Tue, 20 Feb 2024 09:33:22 GMT   (2650kb,D)

Title: Differentiable Mapper For Topological Optimization Of Data
  Representation
Authors: Ziyad Oulhaj, Mathieu Carri\`ere and Bertrand Michel
Categories: cs.LG cs.CG math.AT
\\
  Unsupervised data representation and visualization using tools from topology
is an active and growing field of Topological Data Analysis (TDA) and data
science. Its most prominent line of work is based on the so-called Mapper
graph, which is a combinatorial graph whose topological structures (connected
components, branches, loops) are in correspondence with those of the data
itself. While highly generic and applicable, its use has been hampered so far
by the manual tuning of its many parameters-among these, a crucial one is the
so-called filter: it is a continuous function whose variations on the data set
are the main ingredient for both building the Mapper representation and
assessing the presence and sizes of its topological structures. However, while
a few parameter tuning methods have already been investigated for the other
Mapper parameters (i.e., resolution, gain, clustering), there is currently no
method for tuning the filter itself. In this work, we build on a recently
proposed optimization framework incorporating topology to provide the first
filter optimization scheme for Mapper graphs. In order to achieve this, we
propose a relaxed and more general version of the Mapper graph, whose
convergence properties are investigated. Finally, we demonstrate the usefulness
of our approach by optimizing Mapper graph representations on several datasets,
and showcasing the superiority of the optimized representation over arbitrary
ones.
\\ ( https://arxiv.org/abs/2402.12854 ,  2650kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12861
Date: Tue, 20 Feb 2024 09:52:30 GMT   (12418kb,D)

Title: Bounding Reconstruction Attack Success of Adversaries Without Data
  Priors
Authors: Alexander Ziller, Anneliese Riess, Kristian Schwethelm, Tamara T.
  Mueller, Daniel Rueckert, Georgios Kaissis
Categories: cs.LG cs.CR
\\
  Reconstruction attacks on machine learning (ML) models pose a strong risk of
leakage of sensitive data. In specific contexts, an adversary can (almost)
perfectly reconstruct training data samples from a trained model using the
model's gradients. When training ML models with differential privacy (DP),
formal upper bounds on the success of such reconstruction attacks can be
provided. So far, these bounds have been formulated under worst-case
assumptions that might not hold high realistic practicality. In this work, we
provide formal upper bounds on reconstruction success under realistic
adversarial settings against ML models trained with DP and support these bounds
with empirical results. With this, we show that in realistic scenarios, (a) the
expected reconstruction success can be bounded appropriately in different
contexts and by different metrics, which (b) allows for a more educated choice
of a privacy parameter.
\\ ( https://arxiv.org/abs/2402.12861 ,  12418kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12868
Date: Tue, 20 Feb 2024 09:59:33 GMT   (40kb,D)

Title: Fast Rates in Online Convex Optimization by Exploiting the Curvature of
  Feasible Sets
Authors: Taira Tsuchiya, Shinji Ito
Categories: cs.LG stat.ML
Comments: 17 pages
\\
  In this paper, we explore online convex optimization (OCO) and introduce a
new analysis that provides fast rates by exploiting the curvature of feasible
sets. In online linear optimization, it is known that if the average gradient
of loss functions is larger than a certain value, the curvature of feasible
sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a
logarithmic regret. This paper reveals that algorithms adaptive to the
curvature of loss functions can also leverage the curvature of feasible sets.
We first prove that if an optimal decision is on the boundary of a feasible set
and the gradient of an underlying loss function is non-zero, then the algorithm
achieves a regret upper bound of $O(\rho \log T)$ in stochastic environments.
Here, $\rho > 0$ is the radius of the smallest sphere that includes the optimal
decision and encloses the feasible set. Our approach, unlike existing ones, can
work directly with convex loss functions, exploiting the curvature of loss
functions simultaneously, and can achieve the logarithmic regret only with a
local property of feasible sets. Additionally, it achieves an $O(\sqrt{T})$
regret even in adversarial environments where FTL suffers an $\Omega(T)$
regret, and attains an $O(\rho \log T + \sqrt{C \rho \log T})$ regret bound in
corrupted stochastic environments with corruption level $C$. Furthermore, by
extending our analysis, we establish a regret upper bound of
$O\Big(T^{\frac{q-2}{2(q-1)}} (\log T)^{\frac{q}{2(q-1)}}\Big)$ for
$q$-uniformly convex feasible sets, where uniformly convex sets include
strongly convex sets and $\ell_p$-balls for $p \in [1,\infty)$. This bound
bridges the gap between the $O(\log T)$ regret bound for strongly convex sets
($q=2$) and the $O(\sqrt{T})$ regret bound for non-curved sets ($q\to\infty$).
\\ ( https://arxiv.org/abs/2402.12868 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12874
Date: Tue, 20 Feb 2024 10:09:00 GMT   (547kb,D)

Title: Skill or Luck? Return Decomposition via Advantage Functions
Authors: Hsiao-Ru Pan, Bernhard Sch\"olkopf
Categories: cs.LG
Comments: ICLR 2024
\\
  Learning from off-policy data is essential for sample-efficient reinforcement
learning. In the present work, we build on the insight that the advantage
function can be understood as the causal effect of an action on the return, and
show that this allows us to decompose the return of a trajectory into parts
caused by the agent's actions (skill) and parts outside of the agent's control
(luck). Furthermore, this decomposition enables us to naturally extend Direct
Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The
resulting method can learn from off-policy trajectories without relying on
importance sampling techniques or truncating off-policy actions. We draw
connections between Off-policy DAE and previous methods to demonstrate how it
can speed up learning and when the proposed off-policy corrections are
important. Finally, we use the MinAtar environments to illustrate how ignoring
off-policy corrections can lead to suboptimal policy optimization performance.
\\ ( https://arxiv.org/abs/2402.12874 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12875
Date: Tue, 20 Feb 2024 10:11:03 GMT   (3184kb,D)

Title: Chain of Thought Empowers Transformers to Solve Inherently Serial
  Problems
Authors: Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma
Categories: cs.LG cs.CC stat.ML
Comments: 37 pages. ICLR 2024
\\
  Instructing the model to generate a sequence of intermediate steps, a.k.a., a
chain of thought (CoT), is a highly effective method to improve the accuracy of
large language models (LLMs) on arithmetics and symbolic reasoning tasks.
However, the mechanism behind CoT remains unclear. This work provides a
theoretical understanding of the power of CoT for decoder-only transformers
through the lens of expressiveness. Conceptually, CoT empowers the model with
the ability to perform inherently serial computation, which is otherwise
lacking in transformers, especially when depth is low. Given input length $n$,
previous works have shown that constant-depth transformers with finite
precision $\mathsf{poly}(n)$ embedding size can only solve problems in
$\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper
bound for constant-depth transformers with constant-bit precision, which can
only solve problems in $\mathsf{AC}^0$, a proper subset of $ \mathsf{TC}^0$.
However, with $T$ steps of CoT, constant-depth transformers using constant-bit
precision and $O(\log n)$ embedding size can solve any problem solvable by
boolean circuits of size $T$. Empirically, enabling CoT dramatically improves
the accuracy for tasks that are hard for parallel computation, including the
composition of permutation groups, iterated squaring, and circuit value
problems, especially for low-depth transformers.
\\ ( https://arxiv.org/abs/2402.12875 ,  3184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12876
Date: Tue, 20 Feb 2024 10:13:44 GMT   (2934kb,D)

Title: Federated Multi-Task Learning on Non-IID Data Silos: An Experimental
  Study
Authors: Yuwen Yang, Yuxiang Lu, Suizhi Huang, Shalayiding Sirejiding, Hongtao
  Lu, Yue Ding
Categories: cs.LG cs.CR cs.DC
\\
  The innovative Federated Multi-Task Learning (FMTL) approach consolidates the
benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling
collaborative model training on multi-task learning datasets. However, a
comprehensive evaluation method, integrating the unique features of both FL and
MTL, is currently absent in the field. This paper fills this void by
introducing a novel framework, FMTL-Bench, for systematic evaluation of the
FMTL paradigm. This benchmark covers various aspects at the data, model, and
optimization algorithm levels, and comprises seven sets of comparative
experiments, encapsulating a wide array of non-independent and identically
distributed (Non-IID) data partitioning scenarios. We propose a systematic
process for comparing baselines of diverse indicators and conduct a case study
on communication expenditure, time, and energy consumption. Through our
exhaustive experiments, we aim to provide valuable insights into the strengths
and limitations of existing baseline methods, contributing to the ongoing
discourse on optimal FMTL application in practical scenarios. The source code
will be made available for results replication.
\\ ( https://arxiv.org/abs/2402.12876 ,  2934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12916
Date: Tue, 20 Feb 2024 11:06:42 GMT   (569kb)

Title: Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of
  Machine Learning Models
Authors: Jiang Wu, Hongbo Wang, Chunhe Ni, Chenwei Zhang, Wenran Lu
Categories: cs.LG cs.AI
\\
  Data Pipeline plays an indispensable role in tasks such as modeling machine
learning and developing data products. With the increasing diversification and
complexity of Data sources, as well as the rapid growth of data volumes,
building an efficient Data Pipeline has become crucial for improving work
efficiency and solving complex problems. This paper focuses on exploring how to
optimize data flow through automated machine learning methods by integrating
AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to
enhance the intelligence of Data Pipeline, thereby achieving better results in
machine learning tasks. By delving into the automation and optimization of Data
flows, we uncover key strategies for constructing efficient data pipelines that
can adapt to the ever-changing data landscape. This not only accelerates the
modeling process but also provides innovative solutions to complex problems,
enabling more significant outcomes in increasingly intricate data domains.
Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning
\\ ( https://arxiv.org/abs/2402.12916 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12921
Date: Tue, 20 Feb 2024 11:15:13 GMT   (1975kb,D)

Title: Right on Time: Revising Time Series Models by Constraining their
  Explanations
Authors: Maurice Kraus, David Steinmann, Antonia W\"ust, Andre Kokozinski,
  Kristian Kersting
Categories: cs.LG
\\
  The reliability of deep time series models is often compromised by their
tendency to rely on confounding factors, which may lead to misleading results.
Our newly recorded, naturally confounded dataset named P2S from a real
mechanical production line emphasizes this. To tackle the challenging problem
of mitigating confounders in time series data, we introduce Right on Time
(RioT). Our method enables interactions with model explanations across both the
time and frequency domain. Feedback on explanations in both domains is then
used to constrain the model, steering it away from the annotated confounding
factors. The dual-domain interaction strategy is crucial for effectively
addressing confounders in time series datasets. We empirically demonstrate that
RioT can effectively guide models away from the wrong reasons in P2S as well as
popular time series classification and forecasting datasets.
\\ ( https://arxiv.org/abs/2402.12921 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12930
Date: Tue, 20 Feb 2024 11:29:57 GMT   (5758kb,D)

Title: Learning Exceptional Subgroups by End-to-End Maximizing KL-divergence
Authors: Sascha Xu, Nils Philipp Walter, Janis Kalofolias, Jilles Vreeken
Categories: cs.LG
\\
  Finding and describing sub-populations that are exceptional regarding a
target property has important applications in many scientific disciplines, from
identifying disadvantaged demographic groups in census data to finding
conductive molecules within gold nanoparticles. Current approaches to finding
such subgroups require pre-discretized predictive variables, do not permit
non-trivial target distributions, do not scale to large datasets, and struggle
to find diverse results.
  To address these limitations, we propose Syflow, an end-to-end optimizable
approach in which we leverage normalizing flows to model arbitrary target
distributions, and introduce a novel neural layer that results in easily
interpretable subgroup descriptions. We demonstrate on synthetic and real-world
data, including a case study, that Syflow reliably finds highly exceptional
subgroups accompanied by insightful descriptions.
\\ ( https://arxiv.org/abs/2402.12930 ,  5758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12937
Date: Tue, 20 Feb 2024 11:38:52 GMT   (2147kb,D)

Title: GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural
  Networks
Authors: Anuj Kumar Sirohi, Anjali Gupta, Sayan Ranu, Sandeep Kumar, Amitabha
  Bagchi
Categories: cs.LG cs.SI
\\
  We address the growing apprehension that GNNs, in the absence of fairness
constraints, might produce biased decisions that disproportionately affect
underprivileged groups or individuals. Departing from previous work, we
introduce for the first time a method for incorporating the Gini coefficient as
a measure of fairness to be used within the GNN framework. Our proposal,
GRAPHGINI, works with the two different goals of individual and group fairness
in a single system, while maintaining high prediction accuracy. GRAPHGINI
enforces individual fairness through learnable attention scores that help in
aggregating more information through similar nodes. A heuristic-based maximum
Nash social welfare constraint ensures the maximum possible group fairness.
Both the individual fairness constraint and the group fairness constraint are
stated in terms of a differentiable approximation of the Gini coefficient. This
approximation is a contribution that is likely to be of interest even beyond
the scope of the problem studied in this paper. Unlike other state-of-the-art,
GRAPHGINI automatically balances all three optimization objectives (utility,
individual, and group fairness) of the GNN and is free from any manual tuning
of weight parameters. Extensive experimentation on real-world datasets
showcases the efficacy of GRAPHGINI in making significant improvements in
individual fairness compared to all currently available state-of-the-art
methods while maintaining utility and group equality.
\\ ( https://arxiv.org/abs/2402.12937 ,  2147kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12939
Date: Tue, 20 Feb 2024 11:50:50 GMT   (2980kb,D)

Title: Discovering Behavioral Modes in Deep Reinforcement Learning Policies
  Using Trajectory Clustering in Latent Space
Authors: Sindre Benjamin Remman and Anastasios M. Lekkas
Categories: cs.LG cs.AI
Comments: Submitted to the European Control Conference 2024
\\
  Understanding the behavior of deep reinforcement learning (DRL) agents is
crucial for improving their performance and reliability. However, the
complexity of their policies often makes them challenging to understand. In
this paper, we introduce a new approach for investigating the behavior modes of
DRL policies, which involves utilizing dimensionality reduction and trajectory
clustering in the latent space of neural networks. Specifically, we use
Pairwise Controlled Manifold Approximation Projection (PaCMAP) for
dimensionality reduction and TRACLUS for trajectory clustering to analyze the
latent space of a DRL policy trained on the Mountain Car control task. Our
methodology helps identify diverse behavior patterns and suboptimal choices by
the policy, thus allowing for targeted improvements. We demonstrate how our
approach, combined with domain knowledge, can enhance a policy's performance in
specific regions of the state space.
\\ ( https://arxiv.org/abs/2402.12939 ,  2980kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12945
Date: Tue, 20 Feb 2024 12:00:25 GMT   (8747kb,D)

Title: Stochastic Approximation Approach to Federated Machine Learning
Authors: Srihari P V and Bharath Bhikkaji
Categories: cs.LG
\\
  This paper examines Federated learning (FL) in a Stochastic Approximation
(SA) framework. FL is a collaborative way to train neural network models across
various participants or clients without centralizing their data. Each client
will train a model on their respective data and send the weights across to a
the server periodically for aggregation. The server aggregates these weights
which are then used by the clients to re-initialize their neural network and
continue the training. SA is an iterative algorithm that uses approximate
sample gradients and tapering step size to locate a minimizer of a cost
function. In this paper the clients use a stochastic approximation iterate to
update the weights of its neural network. It is shown that the aggregated
weights track an autonomous ODE. Numerical simulations are performed and the
results are compared with standard algorithms like FedAvg and FedProx. It is
observed that the proposed algorithm is robust and gives more reliable
estimates of the weights, in particular when the clients data are not
identically distributed.
\\ ( https://arxiv.org/abs/2402.12945 ,  8747kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12954
Date: Tue, 20 Feb 2024 12:17:01 GMT   (688kb,D)

Title: Conditional Logical Message Passing Transformer for Complex Query
  Answering
Authors: Chongzhi Zhang, Zhiping Peng, Junhao Zheng, Qianli Ma
Categories: cs.LG cs.AI cs.LO
Comments: 13 pages, 3 figures, and 12 tables
\\
  Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging
task. Given that KGs are usually incomplete, neural models are proposed to
solve CQA by performing multi-hop logical reasoning. However, most of them
cannot perform well on both one-hop and multi-hop queries simultaneously.
Recent work proposes a logical message passing mechanism based on the
pre-trained neural link predictors. While effective on both one-hop and
multi-hop queries, it ignores the difference between the constant and variable
nodes in a query graph. In addition, during the node embedding update stage,
this mechanism cannot dynamically measure the importance of different messages,
and whether it can capture the implicit logical dependencies related to a node
and received messages remains unclear. In this paper, we propose Conditional
Logical Message Passing Transformer (CLMPT), which considers the difference
between constants and variables in the case of using pre-trained neural link
predictors and performs message passing conditionally on the node type. We
empirically verified that this approach can reduce computational costs without
affecting performance. Furthermore, CLMPT uses the transformer to aggregate
received messages and update the corresponding node embedding. Through the
self-attention mechanism, CLMPT can assign adaptive weights to elements in an
input set consisting of received messages and the corresponding node and
explicitly model logical dependencies between various elements. Experimental
results show that CLMPT is a new state-of-the-art neural CQA model.
\\ ( https://arxiv.org/abs/2402.12954 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12987
Date: Tue, 20 Feb 2024 13:17:37 GMT   (550kb,D)

Title: Towards Robust Graph Incremental Learning on Evolving Graphs
Authors: Junwei Su, Difan Zou, Zijun Zhang, Chuan Wu
Categories: cs.LG
\\
  Incremental learning is a machine learning approach that involves training a
model on a sequence of tasks, rather than all tasks at once. This ability to
learn incrementally from a stream of tasks is crucial for many real-world
applications. However, incremental learning is a challenging problem on
graph-structured data, as many graph-related problems involve prediction tasks
for each individual node, known as Node-wise Graph Incremental Learning (NGIL).
This introduces non-independent and non-identically distributed characteristics
in the sample data generation process, making it difficult to maintain the
performance of the model as new tasks are added. In this paper, we focus on the
inductive NGIL problem, which accounts for the evolution of graph structure
(structural shift) induced by emerging tasks. We provide a formal formulation
and analysis of the problem, and propose a novel regularization-based technique
called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the
structural shift on catastrophic forgetting of the inductive NGIL problem. We
show that the structural shift can lead to a shift in the input distribution
for the existing tasks, and further lead to an increased risk of catastrophic
forgetting. Through comprehensive empirical studies with several benchmark
datasets, we demonstrate that our proposed method,
Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to
improve the performance of state-of-the-art GNN incremental learning frameworks
in the inductive setting.
\\ ( https://arxiv.org/abs/2402.12987 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12991
Date: Tue, 20 Feb 2024 13:20:39 GMT   (3136kb,D)

Title: TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box
  Identification
Authors: Martin Gubri, Dennis Ulmer, Hwaran Lee, Sangdoo Yun and Seong Joon Oh
Categories: cs.LG cs.AI cs.CL cs.CR
\\
  Large Language Model (LLM) services and models often come with legal rules on
who can use them and how they must use them. Assessing the compliance of the
released LLMs is crucial, as these rules protect the interests of the LLM
contributor and prevent misuse. In this context, we describe the novel problem
of Black-box Identity Verification (BBIV). The goal is to determine whether a
third-party application uses a certain LLM through its chat function. We
propose a method called Targeted Random Adversarial Prompt (TRAP) that
identifies the specific LLM in use. We repurpose adversarial suffixes,
originally proposed for jailbreaking, to get a pre-defined answer from the
target LLM, while other models give random answers. TRAP detects the target
LLMs with over 95% true positive rate at under 0.2% false positive rate even
after a single interaction. TRAP remains effective even if the LLM has minor
changes that do not significantly alter the original function.
\\ ( https://arxiv.org/abs/2402.12991 ,  3136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13006
Date: Tue, 20 Feb 2024 13:41:21 GMT   (6283kb,D)

Title: Investigating the Impact of Model Instability on Explanations and
  Uncertainty
Authors: Sara Vera Marjanovi\'c, Isabelle Augenstein, Christina Lioma
Categories: cs.LG cs.CL
\\
  Explainable AI methods facilitate the understanding of model behaviour, yet,
small, imperceptible perturbations to inputs can vastly distort explanations.
As these explanations are typically evaluated holistically, before model
deployment, it is difficult to assess when a particular explanation is
trustworthy. Some studies have tried to create confidence estimators for
explanations, but none have investigated an existing link between uncertainty
and explanation quality. We artificially simulate epistemic uncertainty in text
input by introducing noise at inference time. In this large-scale empirical
study, we insert different levels of noise perturbations and measure the effect
on the output of pre-trained language models and different uncertainty metrics.
Realistic perturbations have minimal effect on performance and explanations,
yet masking has a drastic effect. We find that high uncertainty doesn't
necessarily imply low explanation plausibility; the correlation between the two
metrics can be moderately positive when noise is exposed during the training
process. This suggests that noise-augmented models may be better at identifying
salient tokens when uncertain. Furthermore, when predictive and epistemic
uncertainty measures are over-confident, the robustness of a saliency map to
perturbation can indicate model stability issues. Integrated Gradients shows
the overall greatest robustness to perturbation, while still showing
model-specific patterns in performance; however, this phenomenon is limited to
smaller Transformer-based language models.
\\ ( https://arxiv.org/abs/2402.13006 ,  6283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13007
Date: Tue, 20 Feb 2024 13:42:36 GMT   (123kb,D)

Title: Improve Cross-Architecture Generalization on Dataset Distillation
Authors: Binglin Zhou, Linhao Zhong, Wentao Chen
Categories: cs.LG cs.CV
\\
  Dataset distillation, a pragmatic approach in machine learning, aims to
create a smaller synthetic dataset from a larger existing dataset. However,
existing distillation methods primarily adopt a model-based paradigm, where the
synthetic dataset inherits model-specific biases, limiting its generalizability
to alternative models. In response to this constraint, we propose a novel
methodology termed "model pool". This approach involves selecting models from a
diverse model pool based on a specific probability distribution during the data
distillation process. Additionally, we integrate our model pool with the
established knowledge distillation approach and apply knowledge distillation to
the test process of the distilled dataset. Our experimental results validate
the effectiveness of the model pool approach across a range of existing models
while testing, demonstrating superior performance compared to existing
methodologies.
\\ ( https://arxiv.org/abs/2402.13007 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13033
Date: Tue, 20 Feb 2024 14:18:43 GMT   (2175kb,D)

Title: Enhancing Real-World Complex Network Representations with Hyperedge
  Augmentation
Authors: Xiangyu Zhao, Zehui Li, Mingzhu Shen, Guy-Bart Stan, Pietro Li\`o,
  Yiren Zhao
Categories: cs.LG cs.IR cs.SI
Comments: Preprint. Under review. 17 pages, 4 figures, 14 tables. arXiv admin
  note: text overlap with arXiv:2306.05108
\\
  Graph augmentation methods play a crucial role in improving the performance
and enhancing generalisation capabilities in Graph Neural Networks (GNNs).
Existing graph augmentation methods mainly perturb the graph structures and are
usually limited to pairwise node relations. These methods cannot fully address
the complexities of real-world large-scale networks that often involve
higher-order node relations beyond only being pairwise. Meanwhile, real-world
graph datasets are predominantly modelled as simple graphs, due to the scarcity
of data that can be used to form higher-order edges. Therefore, reconfiguring
the higher-order edges as an integration into graph augmentation strategies
lights up a promising research path to address the aforementioned issues. In
this paper, we present Hyperedge Augmentation (HyperAug), a novel graph
augmentation method that constructs virtual hyperedges directly form the raw
data, and produces auxiliary node features by extracting from the virtual
hyperedge information, which are used for enhancing GNN performances on
downstream tasks. We design three diverse virtual hyperedge construction
strategies to accompany the augmentation scheme: (1) via graph statistics, (2)
from multiple data perspectives, and (3) utilising multi-modality. Furthermore,
to facilitate HyperAug evaluation, we provide 23 novel real-world graph
datasets across various domains including social media, biology, and
e-commerce. Our empirical study shows that HyperAug consistently and
significantly outperforms GNN baselines and other graph augmentation methods,
across a variety of application contexts, which clearly indicates that it can
effectively incorporate higher-order node relations into graph augmentation
methods for real-world complex networks.
\\ ( https://arxiv.org/abs/2402.13033 ,  2175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13037
Date: Tue, 20 Feb 2024 14:24:00 GMT   (2705kb,D)

Title: Align Your Intents: Offline Imitation Learning via Optimal Transport
Authors: Maksim Bobrin, Nazar Buzun, Dmitrii Krylov, Dmitry V. Dylov
Categories: cs.LG cs.AI
\\
  Offline reinforcement learning (RL) addresses the problem of sequential
decision-making by learning optimal policy through pre-collected data, without
interacting with the environment. As yet, it has remained somewhat impractical,
because one rarely knows the reward explicitly and it is hard to distill it
retrospectively. Here, we show that an imitating agent can still learn the
desired behavior merely from observing the expert, despite the absence of
explicit rewards or action labels. In our method, AILOT (Aligned Imitation
Learning via Optimal Transport), we involve special representation of states in
a form of intents that incorporate pairwise spatial distances within the data.
Given such representations, we define intrinsic reward function via optimal
transport distance between the expert's and the agent's trajectories. We report
that AILOT outperforms state-of-the art offline imitation learning algorithms
on D4RL benchmarks and improves the performance of other offline RL algorithms
in the sparse-reward tasks.
\\ ( https://arxiv.org/abs/2402.13037 ,  2705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13040
Date: Tue, 20 Feb 2024 14:29:02 GMT   (463kb,D)

Title: Text-Guided Molecule Generation with Diffusion Language Model
Authors: Haisong Gong, Qiang Liu, Shu Wu, Liang Wang
Categories: cs.LG cs.AI cs.CE cs.CL q-bio.BM
Comments: Accepted by 38th Association for the Advancement of Artificial
  Intelligence, AAAI
\\
  Text-guided molecule generation is a task where molecules are generated to
match specific textual descriptions. Recently, most existing SMILES-based
molecule generation methods rely on an autoregressive architecture. In this
work, we propose the Text-Guided Molecule Generation with Diffusion Language
Model (TGM-DLM), a novel approach that leverages diffusion models to address
the limitations of autoregressive methods. TGM-DLM updates token embeddings
within the SMILES string collectively and iteratively, using a two-phase
diffusion generation process. The first phase optimizes embeddings from random
noise, guided by the text description, while the second phase corrects invalid
SMILES strings to form valid molecular representations. We demonstrate that
TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for
additional data resources. Our findings underscore the remarkable effectiveness
of TGM-DLM in generating coherent and precise molecules with specific
properties, opening new avenues in drug discovery and related scientific
domains. Code will be released at: https://github.com/Deno-V/tgm-dlm.
\\ ( https://arxiv.org/abs/2402.13040 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13077
Date: Tue, 20 Feb 2024 15:23:24 GMT   (3035kb,D)

Title: Mechanistic Neural Networks for Scientific Machine Learning
Authors: Adeel Pervez, Francesco Locatello, Efstratios Gavves
Categories: cs.LG cs.AI cs.NE
\\
  This paper presents Mechanistic Neural Networks, a neural network design for
machine learning applications in the sciences. It incorporates a new
Mechanistic Block in standard architectures to explicitly learn governing
differential equations as representations, revealing the underlying dynamics of
data and enhancing interpretability and efficiency in data modeling. Central to
our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by
a technique that reduces solving linear ODEs to solving linear programs. This
integrates well with neural networks and surpasses the limitations of
traditional ODE solvers enabling scalable GPU parallel processing. Overall,
Mechanistic Neural Networks demonstrate their versatility for scientific
machine learning applications, adeptly managing tasks from equation discovery
to dynamic systems modeling. We prove their comprehensive capabilities in
analyzing and interpreting complex scientific data across various applications,
showing significant performance against specialized state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.13077 ,  3035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13081
Date: Tue, 20 Feb 2024 15:25:56 GMT   (421kb,D)

Title: IT Intrusion Detection Using Statistical Learning and Testbed
  Measurements
Authors: Xiaoxuan Wang and Rolf Stadler
Categories: cs.LG cs.CR
Comments: A shortened version of this paper will appear in the conference
  proceedings of NOMS 2024 (IEEE/IFIP Network Operations and Management
  Symposium)
\\
  We study automated intrusion detection in an IT infrastructure, specifically
the problem of identifying the start of an attack, the type of attack, and the
sequence of actions an attacker takes, based on continuous measurements from
the infrastructure. We apply statistical learning methods, including Hidden
Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier
(RFC) to map sequences of observations to sequences of predicted attack
actions. In contrast to most related research, we have abundant data to train
the models and evaluate their predictive power. The data comes from traces we
generate on an in-house testbed where we run attacks against an emulated IT
infrastructure. Central to our work is a machine-learning pipeline that maps
measurements from a high-dimensional observation space to a space of low
dimensionality or to a small set of observation symbols. Investigating
intrusions in offline as well as online scenarios, we find that both HMM and
LSTM can be effective in predicting attack start time, attack type, and attack
actions. If sufficient training data is available, LSTM achieves higher
prediction accuracy than HMM. HMM, on the other hand, requires less
computational resources and less training data for effective prediction. Also,
we find that the methods we study benefit from data produced by traditional
intrusion detection systems like SNORT.
\\ ( https://arxiv.org/abs/2402.13081 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13087
Date: Tue, 20 Feb 2024 15:29:49 GMT   (387kb,D)

Title: How Does Selection Leak Privacy: Revisiting Private Selection and
  Improved Results for Hyper-parameter Tuning
Authors: Zihang Xiang, Chenglong Wang, Di Wang
Categories: cs.LG cs.CR
\\
  We study the problem of guaranteeing Differential Privacy (DP) in
hyper-parameter tuning, a crucial process in machine learning involving the
selection of the best run from several. Unlike many private algorithms,
including the prevalent DP-SGD, the privacy implications of tuning remain
insufficiently understood. Recent works propose a generic private solution for
the tuning process, yet a fundamental question still persists: is the current
privacy bound for this solution tight?
  This paper contributes both positive and negative answers to this question.
Initially, we provide studies affirming the current privacy analysis is indeed
tight in a general sense. However, when we specifically study the
hyper-parameter tuning problem, such tightness no longer holds. This is first
demonstrated by applying privacy audit on the tuning process. Our findings
underscore a substantial gap between the current theoretical privacy bound and
the empirical bound derived even under the strongest audit setup.
  The gap found is not a fluke. Our subsequent study provides an improved
privacy result for private hyper-parameter tuning due to its distinct
properties. Our privacy results are also more generalizable compared to prior
analyses that are only easily applicable in specific setups.
\\ ( https://arxiv.org/abs/2402.13087 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13089
Date: Tue, 20 Feb 2024 15:31:44 GMT   (864kb,D)

Title: Towards an empirical understanding of MoE design choices
Authors: Dongyang Fan, Bettina Messmer, Martin Jaggi
Categories: cs.LG cs.AI cs.CL
\\
  In this study, we systematically evaluate the impact of common design choices
in Mixture of Experts (MoEs) on validation performance, uncovering distinct
influences at token and sequence levels. We also present empirical evidence
showing comparable performance between a learned router and a frozen, randomly
initialized router, suggesting that learned routing may not be essential. Our
study further reveals that Sequence-level routing can result in topic-specific
weak expert specialization, in contrast to syntax specialization observed with
Token-level routing.
\\ ( https://arxiv.org/abs/2402.13089 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13101
Date: Tue, 20 Feb 2024 15:54:24 GMT   (10372kb,D)

Title: A Microstructure-based Graph Neural Network for Accelerating Multiscale
  Simulations
Authors: J. Storm, I. B. C. M. Rocha, F. P. van der Meer
Categories: cs.LG cs.NA math.NA
\\
  Simulating the mechanical response of advanced materials can be done more
accurately using concurrent multiscale models than with single-scale
simulations. However, the computational costs stand in the way of the practical
application of this approach. The costs originate from microscale Finite
Element (FE) models that must be solved at every macroscopic integration point.
A plethora of surrogate modeling strategies attempt to alleviate this cost by
learning to predict macroscopic stresses from macroscopic strains, completely
replacing the microscale models. In this work, we introduce an alternative
surrogate modeling strategy that allows for keeping the multiscale nature of
the problem, allowing it to be used interchangeably with an FE solver for any
time step. Our surrogate provides all microscopic quantities, which are then
homogenized to obtain macroscopic quantities of interest. We achieve this for
an elasto-plastic material by predicting full-field microscopic strains using a
graph neural network (GNN) while retaining the microscopic constitutive
material model to obtain the stresses. This hybrid data-physics graph-based
approach avoids the high dimensionality originating from predicting full-field
responses while allowing non-locality to arise. By training the GNN on a
variety of meshes, it learns to generalize to unseen meshes, allowing a single
model to be used for a range of microstructures. The embedded microscopic
constitutive model in the GNN implicitly tracks history-dependent variables and
leads to improved accuracy. We demonstrate for several challenging scenarios
that the surrogate can predict complex macroscopic stress-strain paths. As the
computation time of our method scales favorably with the number of elements in
the microstructure compared to the FE method, our method can significantly
accelerate FE2 simulations.
\\ ( https://arxiv.org/abs/2402.13101 ,  10372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13103
Date: Tue, 20 Feb 2024 15:58:45 GMT   (340kb,D)

Title: Multivariate Functional Linear Discriminant Analysis for the
  Classification of Short Time Series with Missing Data
Authors: Rahul Bordoloi, Cl\'emence R\'eda, Orell Trautmann, Saptarshi Bej and
  Olaf Wolkenhauer
Categories: cs.LG math.ST stat.TH
MSC-class: 62R10 (Primary), 62R07 (Secondary)
\\
  Functional linear discriminant analysis (FLDA) is a powerful tool that
extends LDA-mediated multiclass classification and dimension reduction to
univariate time-series functions. However, in the age of large multivariate and
incomplete data, statistical dependencies between features must be estimated in
a computationally tractable way, while also dealing with missing data. There is
a need for a computationally tractable approach that considers the statistical
dependencies between features and can handle missing values. We here develop a
multivariate version of FLDA (MUDRA) to tackle this issue and describe an
efficient expectation/conditional-maximization (ECM) algorithm to infer its
parameters. We assess its predictive power on the "Articulary Word Recognition"
data set and show its improvement over the state-of-the-art, especially in the
case of missing data. MUDRA allows interpretable classification of data sets
with large proportions of missing data, which will be particularly useful for
medical or psychological data sets.
\\ ( https://arxiv.org/abs/2402.13103 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13108
Date: Tue, 20 Feb 2024 16:01:42 GMT   (55kb)

Title: On the Stability of Gradient Descent for Large Learning Rate
Authors: Alexandru Cr\u{a}ciun, Debarghya Ghoshdastidar
Categories: cs.LG
\\
  There currently is a significant interest in understanding the Edge of
Stability (EoS) phenomenon, which has been observed in neural networks
training, characterized by a non-monotonic decrease of the loss function over
epochs, while the sharpness of the loss (spectral norm of the Hessian)
progressively approaches and stabilizes around 2/(learning rate). Reasons for
the existence of EoS when training using gradient descent have recently been
proposed -- a lack of flat minima near the gradient descent trajectory together
with the presence of compact forward-invariant sets. In this paper, we show
that linear neural networks optimized under a quadratic loss function satisfy
the first assumption and also a necessary condition for the second assumption.
More precisely, we prove that the gradient descent map is non-singular, the set
of global minimizers of the loss function forms a smooth manifold, and the
stable minima form a bounded subset in parameter space. Additionally, we prove
that if the step-size is too big, then the set of initializations from which
gradient descent converges to a critical point has measure zero.
\\ ( https://arxiv.org/abs/2402.13108 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13114
Date: Tue, 20 Feb 2024 16:11:59 GMT   (1638kb,D)

Title: BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer
  Nodes
Authors: Qian Wang, Zemin Liu, Zhen Zhang and Bingsheng He
Categories: cs.LG cs.AI
\\
  Class imbalance in graph-structured data, where minor classes are
significantly underrepresented, poses a critical challenge for Graph Neural
Networks (GNNs). To address this challenge, existing studies generally generate
new minority nodes and edges connecting new nodes to the original graph to make
classes balanced. However, they do not solve the problem that majority classes
still propagate information to minority nodes by edges in the original graph
which introduces bias towards majority classes. To address this, we introduce
BuffGraph, which inserts buffer nodes into the graph, modulating the impact of
majority classes to improve minor class representation. Our extensive
experiments across diverse real-world datasets empirically demonstrate that
BuffGraph outperforms existing baseline methods in class-imbalanced node
classification in both natural settings and imbalanced settings. Code is
available at https://anonymous.4open.science/r/BuffGraph-730A.
\\ ( https://arxiv.org/abs/2402.13114 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13144
Date: Tue, 20 Feb 2024 16:59:03 GMT   (6066kb,D)

Title: Neural Network Diffusion
Authors: Kai Wang, Zhaopan Xu, Yukun Zhou, Zelin Zang, Trevor Darrell, Zhuang
  Liu, and Yang You
Categories: cs.LG cs.CV
Comments: We introduce a novel approach for parameter generation, named neural
  network diffusion (\textbf{p-diff}, p stands for parameter), which employs a
  standard latent diffusion model to synthesize a new set of parameters
\\
  Diffusion models have achieved remarkable success in image and video
generation. In this work, we demonstrate that diffusion models can also
\textit{generate high-performing neural network parameters}. Our approach is
simple, utilizing an autoencoder and a standard latent diffusion model. The
autoencoder extracts latent representations of a subset of the trained network
parameters. A diffusion model is then trained to synthesize these latent
parameter representations from random noise. It then generates new
representations that are passed through the autoencoder's decoder, whose
outputs are ready to use as new subsets of network parameters. Across various
architectures and datasets, our diffusion process consistently generates models
of comparable or improved performance over trained networks, with minimal
additional cost. Notably, we empirically find that the generated models perform
differently with the trained networks. Our results encourage more exploration
on the versatile use of diffusion models.
\\ ( https://arxiv.org/abs/2402.13144 ,  6066kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13147
Date: Tue, 20 Feb 2024 17:02:48 GMT   (6996kb,D)

Title: SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal
  Demonstrations
Authors: Huy Hoang, Tien Mai, Pradeep Varakantham
Categories: cs.LG cs.AI
\\
  We consider offline imitation learning (IL), which aims to mimic the expert's
behavior from its demonstration without further interaction with the
environment. One of the main challenges in offline IL is dealing with the
limited support of expert demonstrations that cover only a small fraction of
the state-action spaces. In this work, we consider offline IL, where expert
demonstrations are limited but complemented by a larger set of sub-optimal
demonstrations of lower expertise levels. Most of the existing offline IL
methods developed for this setting are based on behavior cloning or
distribution matching, where the aim is to match the occupancy distribution of
the imitation policy with that of the expert policy. Such an approach often
suffers from over-fitting, as expert demonstrations are limited to accurately
represent any occupancy distribution. On the other hand, since sub-optimal sets
are much larger, there is a high chance that the imitation policy is trained
towards sub-optimal policies. In this paper, to address these issues, we
propose a new approach based on inverse soft-Q learning, where a regularization
term is added to the training objective, with the aim of aligning the learned
rewards with a pre-assigned reward function that allocates higher weights to
state-action pairs from expert demonstrations, and lower weights to those from
lower expertise levels. On standard benchmarks, our inverse soft-Q learning
significantly outperforms other offline IL baselines by a large margin.
\\ ( https://arxiv.org/abs/2402.13147 ,  6996kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13148
Date: Tue, 20 Feb 2024 17:04:06 GMT   (386kb,D)

Title: Defending Jailbreak Prompts via In-Context Adversarial Game
Authors: Yujun Zhou, Yufei Han, Haomin Zhuang, Taicheng Guo, Kehan Guo, Zhenwen
  Liang, Hongyan Bao and Xiangliang Zhang
Categories: cs.LG cs.CR
\\
  Large Language Models (LLMs) demonstrate remarkable capabilities across
diverse applications. However, concerns regarding their security, particularly
the vulnerability to jailbreak attacks, persist. Drawing inspiration from
adversarial training in deep learning and LLM agent learning processes, we
introduce the In-Context Adversarial Game (ICAG) for defending against
jailbreaks without the need for fine-tuning. ICAG leverages agent learning to
conduct an adversarial game, aiming to dynamically extend knowledge to defend
against jailbreaks. Unlike traditional methods that rely on static datasets,
ICAG employs an iterative process to enhance both the defense and attack
agents. This continuous improvement process strengthens defenses against newly
generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy,
where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success
rates across various attack scenarios. Moreover, ICAG demonstrates remarkable
transferability to other LLMs, indicating its potential as a versatile defense
mechanism.
\\ ( https://arxiv.org/abs/2402.13148 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13182
Date: Tue, 20 Feb 2024 17:49:10 GMT   (368kb,D)

Title: Order-Optimal Regret in Distributed Kernel Bandits using Uniform
  Sampling with Shared Randomness
Authors: Nikola Pavlovic, Sudeep Salgia, Qing Zhao
Categories: cs.LG cs.DC stat.ML
\\
  We consider distributed kernel bandits where $N$ agents aim to
collaboratively maximize an unknown reward function that lies in a reproducing
kernel Hilbert space. Each agent sequentially queries the function to obtain
noisy observations at the query points. Agents can share information through a
central server, with the objective of minimizing regret that is accumulating
over time $T$ and aggregating over agents. We develop the first algorithm that
achieves the optimal regret order (as defined by centralized learning) with a
communication cost that is sublinear in both $N$ and $T$. The key features of
the proposed algorithm are the uniform exploration at the local agents and
shared randomness with the central server. Working together with the sparse
approximation of the GP model, these two key components make it possible to
preserve the learning rate of the centralized setting at a diminishing rate of
communication.
\\ ( https://arxiv.org/abs/2402.13182 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13187
Date: Tue, 20 Feb 2024 17:53:24 GMT   (89kb,D)

Title: Testing Calibration in Subquadratic Time
Authors: Lunjia Hu and Kevin Tian and Chutong Yang
Categories: cs.LG cs.DS stat.CO stat.ML
\\
  In the recent literature on machine learning and decision making, calibration
has emerged as a desirable and widely-studied statistical property of the
outputs of binary prediction models. However, the algorithmic aspects of
measuring model calibration have remained relatively less well-explored.
Motivated by [BGHN23], which proposed a rigorous framework for measuring
distances to calibration, we initiate the algorithmic study of calibration
through the lens of property testing. We define the problem of calibration
testing from samples where given $n$ draws from a distribution $\mathcal{D}$ on
(predictions, binary outcomes), our goal is to distinguish between the case
where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$
is $\varepsilon$-far from calibration.
  We design an algorithm based on approximate linear programming, which solves
calibration testing information-theoretically optimally (up to constant
factors) in time $O(n^{1.5} \log(n))$. This improves upon state-of-the-art
black-box linear program solvers requiring $\Omega(n^\omega)$ time, where
$\omega > 2$ is the exponent of matrix multiplication. We also develop
algorithms for tolerant variants of our testing problem, and give sample
complexity lower bounds for alternative calibration distances to the one
considered in this work. Finally, we present preliminary experiments showing
that the testing problem we define faithfully captures standard notions of
calibration, and that our algorithms scale to accommodate moderate sample
sizes.
\\ ( https://arxiv.org/abs/2402.13187 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13196
Date: Tue, 20 Feb 2024 18:07:59 GMT   (262kb,D)

Title: Practical Kernel Tests of Conditional Independence
Authors: Roman Pogodin, Antonin Schrab, Yazhe Li, Danica J. Sutherland, Arthur
  Gretton
Categories: cs.LG
\\
  We describe a data-efficient, kernel-based approach to statistical testing of
conditional independence. A major challenge of conditional independence
testing, absent in tests of unconditional independence, is to obtain the
correct test level (the specified upper bound on the rate of false positives),
while still attaining competitive test power. Excess false positives arise due
to bias in the test statistic, which is obtained using nonparametric kernel
ridge regression. We propose three methods for bias control to correct the test
level, based on data splitting, auxiliary data, and (where possible) simpler
function classes. We show these combined strategies are effective both for
synthetic and real-world data.
\\ ( https://arxiv.org/abs/2402.13196 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13210
Date: Tue, 20 Feb 2024 18:20:59 GMT   (79kb,D)

Title: Bayesian Reward Models for LLM Alignment
Authors: Adam X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang, Haitham
  Bou-Ammar, Laurence Aitchison
Categories: cs.LG
\\
  To ensure that large language model (LLM) responses are helpful and
non-toxic, we usually fine-tune a reward model on human preference data. We
then select policy responses with high rewards (best-of-n sampling) or further
optimize the policy to produce responses with high rewards (reinforcement
learning from human feedback). However, this process is vulnerable to reward
overoptimization or hacking, in which the responses selected have high rewards
due to errors in the reward model rather than a genuine preference. This is
especially problematic as the prompt or response diverges from the training
data. It should be possible to mitigate these issues by training a Bayesian
reward model, which signals higher uncertainty further from the training data
distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA
(Yang et al., 2024) and found that the resulting uncertainty estimates can
successfully mitigate reward overoptimization in best-of-n sampling.
\\ ( https://arxiv.org/abs/2402.13210 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13221
Date: Tue, 20 Feb 2024 18:32:27 GMT   (1505kb,D)

Title: CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset
  for Advancing Graph Machine Learning
Authors: Ulrik Friis-Jensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam,
  Kirsten M. {\O}. Jensen and Raghavendra Selvan
Categories: cs.LG stat.ML
Comments: Submitted to the Applied Data Science (ADS) Track at KDD'24. 16
  pages, 15 figures, 8 tables
\\
  Advances in graph machine learning (ML) have been driven by applications in
chemistry as graphs have remained the most expressive representations of
molecules. While early graph ML methods focused primarily on small organic
molecules, recently, the scope of graph ML has expanded to include inorganic
materials. Modelling the periodicity and symmetry of inorganic crystalline
materials poses unique challenges, which existing graph ML methods are unable
to address. Moving to inorganic nanomaterials increases complexity as the scale
of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of
existing graph ML focuses on characterising molecules and materials by
predicting target properties with graphs as input. However, the most exciting
applications of graph ML will be in their generative capabilities, which is
currently not at par with other domains such as images or text.
  We invite the graph ML community to address these open challenges by
presenting two new chemically-informed large-scale inorganic (CHILI)
nanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M
edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal
types (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B
edges) of nanomaterials generated from experimentally determined crystal
structures (CHILI-100K). We define 11 property prediction tasks and 6 structure
prediction tasks, which are of special interest for nanomaterial research. We
benchmark the performance of a wide array of baseline methods and use these
benchmarking results to highlight areas which need future work. To the best of
our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial
datasets of this scale -- both on the individual graph level and of the dataset
as a whole -- and the only nanomaterials datasets with high structural and
elemental diversity.
\\ ( https://arxiv.org/abs/2402.13221 ,  1505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13233
Date: Tue, 20 Feb 2024 18:48:49 GMT   (6707kb,D)

Title: SMORE: Similarity-based Hyperdimensional Domain Adaptation for
  Multi-Sensor Time Series Classification
Authors: Junyao Wang, Mohammad Abdullah Al Faruque
Categories: cs.LG
Comments: arXiv admin note: text overlap with arXiv:2308.03295
\\
  Many real-world applications of the Internet of Things (IoT) employ machine
learning (ML) algorithms to analyze time series information collected by
interconnected sensors. However, distribution shift, a fundamental challenge in
data-driven ML, arises when a model is deployed on a data distribution
different from the training data and can substantially degrade model
performance. Additionally, increasingly sophisticated deep neural networks
(DNNs) are required to capture intricate spatial and temporal dependencies in
multi-sensor time series data, often exceeding the capabilities of today's edge
devices. In this paper, we propose SMORE, a novel resource-efficient domain
adaptation (DA) algorithm for multi-sensor time series classification,
leveraging the efficient and parallel operations of hyperdimensional computing.
SMORE dynamically customizes test-time models with explicit consideration of
the domain context of each sample to mitigate the negative impacts of domain
shifts. Our evaluation on a variety of multi-sensor time series classification
tasks shows that SMORE achieves on average 1.98% higher accuracy than
state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and
4.63x faster inference.
\\ ( https://arxiv.org/abs/2402.13233 ,  6707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13241
Date: Tue, 20 Feb 2024 18:53:53 GMT   (2293kb,D)

Title: Federated Causal Discovery from Heterogeneous Data
Authors: Loka Li, Ignavier Ng, Gongxu Luo, Biwei Huang, Guangyi Chen, Tongliang
  Liu, Bin Gu, Kun Zhang
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\
  Conventional causal discovery methods rely on centralized data, which is
inconsistent with the decentralized nature of data in many real-world
situations. This discrepancy has motivated the development of federated causal
discovery (FCD) approaches. However, existing FCD methods may be limited by
their potentially restrictive assumptions of identifiable functional causal
models or homogeneous data distributions, narrowing their applicability in
diverse scenarios. In this paper, we propose a novel FCD method attempting to
accommodate arbitrary causal models and heterogeneous data. We first utilize a
surrogate variable corresponding to the client index to account for the data
heterogeneity across different clients. We then develop a federated conditional
independence test (FCIT) for causal skeleton discovery and establish a
federated independent change principle (FICP) to determine causal directions.
These approaches involve constructing summary statistics as a proxy of the raw
data to protect data privacy. Owing to the nonparametric properties, FCIT and
FICP make no assumption about particular functional forms, thereby facilitating
the handling of arbitrary causal models. We conduct extensive experiments on
synthetic and real datasets to show the efficacy of our method. The code is
available at \url{https://github.com/lokali/FedCDH.git}.
\\ ( https://arxiv.org/abs/2402.13241 ,  2293kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2310.03976 (*cross-listing*)
Date: Fri, 6 Oct 2023 02:19:10 GMT   (4436kb)
Date (revised v2): Sat, 17 Feb 2024 22:35:04 GMT   (9928kb)

Title: From Text to Self: Users' Perceptions of Potential of AI on
  Interpersonal Communication and Self
Authors: Yue Fu, Sami Foell, Xuhai Xu, Alexis Hiniker
Categories: cs.HC cs.AI cs.CL
Journal-ref: Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI 2024)
DOI: 10.1145/3613904.3641955
\\
  In the rapidly evolving landscape of AI-mediated communication (AIMC), tools
powered by Large Language Models (LLMs) are becoming integral to interpersonal
communication. Employing a mixed-methods approach, we conducted a one-week
diary and interview study to explore users' perceptions of these tools' ability
to: 1) support interpersonal communication in the short-term, and 2) lead to
potential long-term effects. Our findings indicate that participants view AIMC
support favorably, citing benefits such as increased communication confidence,
and finding precise language to express their thoughts, navigating linguistic
and cultural barriers. However, the study also uncovers current limitations of
AIMC tools, including verbosity, unnatural responses, and excessive emotional
intensity. These shortcomings are further exacerbated by user concerns about
inauthenticity and potential overreliance on the technology. Furthermore, we
identified four key communication spaces delineated by communication stakes
(high or low) and relationship dynamics (formal or informal) that
differentially predict users' attitudes toward AIMC tools. Specifically,
participants found the tool is more suitable for communicating in formal
relationships than informal ones and more beneficial in high-stakes than
low-stakes communication.
\\ ( https://arxiv.org/abs/2310.03976 ,  9928kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12390 (*cross-listing*)
Date: Wed, 14 Feb 2024 16:17:04 GMT   (1206kb)

Title: A Semantic Social Network Analysis Tool for Sensitivity Analysis and
  What-If Scenario Testing in Alcohol Consumption Studies
Authors: Jos\'e Alberto Ben\'itez-Andrades, Alejandro Rodr\'iguez-Gonz\'alez,
  Carmen Benavides, Leticia S\'anchez-Valde\'on and Isa\'ias Garc\'ia
Categories: cs.SI cs.AI
Journal-ref: Int. J. Environ. Res. Public Health 2018, 15(11), 2420;
DOI: 10.3390/ijerph15112420
\\
  Social Network Analysis (SNA) is a set of techniques developed in the field
of social and behavioral sciences research, in order to characterize and study
the social relationships that are established among a set of individuals. When
building a social network for performing an SNA analysis, an initial process of
data gathering is achieved in order to extract the characteristics of the
individuals and their relationships. This is usually done by completing a
questionnaire containing different types of questions that will be later used
to obtain the SNA measures needed to perform the study. There are, then, a
great number of different possible network generating questions and also many
possibilities for mapping the responses to the corresponding characteristics
and relationships. Many variations may be introduced into these questions (the
way they are posed, the weights given to each of the responses, etc.) that may
have an effect on the resulting networks. All these different variations are
difficult to achieve manually, because the process is time-consuming and error
prone. The tool described in this paper uses semantic knowledge representation
techniques in order to facilitate this kind of sensitivity studies. The base of
the tool is a conceptual structure, called "ontology" that is able to represent
the different concepts and their definitions. The tool is compared to other
similar ones, and the advantages of the approach are highlighted, giving some
particular examples from an ongoing SNA study about alcohol consumption habits
in adolescents.
\\ ( https://arxiv.org/abs/2402.12390 ,  1206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12391 (*cross-listing*)
Date: Thu, 15 Feb 2024 06:30:12 GMT   (369kb,D)

Title: Toward a Team of AI-made Scientists for Scientific Discovery from Gene
  Expression Data
Authors: Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu,
  Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang
Categories: q-bio.GN cs.AI cs.LG
Comments: 18 pages, 2 figures
\\
  Machine learning has emerged as a powerful tool for scientific discovery,
enabling researchers to extract meaningful insights from complex datasets. For
instance, it has facilitated the identification of disease-predictive genes
from gene expression data, significantly advancing healthcare. However, the
traditional process for analyzing such datasets demands substantial human
effort and expertise for the data selection, processing, and analysis. To
address this challenge, we introduce a novel framework, a Team of AI-made
Scientists (TAIS), designed to streamline the scientific discovery pipeline.
TAIS comprises simulated roles, including a project manager, data engineer, and
domain expert, each represented by a Large Language Model (LLM). These roles
collaborate to replicate the tasks typically performed by data scientists, with
a specific focus on identifying disease-predictive genes. Furthermore, we have
curated a benchmark dataset to assess TAIS's effectiveness in gene
identification, demonstrating our system's potential to significantly enhance
the efficiency and scope of scientific exploration. Our findings represent a
solid step towards automating scientific discovery through large language
models.
\\ ( https://arxiv.org/abs/2402.12391 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12392 (*cross-listing*)
Date: Fri, 16 Feb 2024 09:37:58 GMT   (5364kb,D)

Title: A Regression Mixture Model to understand the effect of the Covid-19
  pandemic on Public Transport Ridership
Authors: Hugues Moreau, \'Etienne C\^ome, Allou Sam\'e, Latifa Oukhellou
Categories: stat.AP cs.AI
DOI: 10.1109/ICDMW60847.2023.00163
\\
  The Covid-19 pandemic drastically changed urban mobility, both during the
height of the pandemic with government lockdowns, but also in the longer term
with the adoption of working-from-home policies. To understand its effects on
rail public transport ridership, we propose a dedicated Regression Mixture
Model able to perform both the clustering of public transport stations and the
segmentation of time periods, while ignoring variations due to additional
variables such as the official lockdowns or non-working days. Each cluster is
thus defined by a series of segments in which the effect of the exogenous
variables is constant. As each segment within a cluster has its own regression
coefficients to model the impact of the covariates, we analyze how these
coefficients evolve to understand the changes in the cluster. We present the
regression mixture model and the parameter estimation using the EM algorithm,
before demonstrating the benefits of the model on both simulated and real data.
Thanks to a five-year dataset of the ridership in the Paris public transport
system, we analyze the impact of the pandemic, not only in terms of the number
of travelers but also on the weekly commute. We further analyze the specific
changes that the pandemic caused inside each cluster.
\\ ( https://arxiv.org/abs/2402.12392 ,  5364kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12393 (*cross-listing*)
Date: Fri, 16 Feb 2024 14:28:25 GMT   (1359kb,D)

Title: On Automating Video Game Testing by Planning and Learning
Authors: Tom\'a\v{s} Balyo, G. Michael Youngblood, Filip Dvo\v{r}\'ak, Roman
  Bart\'ak
Categories: cs.HC cs.AI
\\
  In this paper, we propose a method and workflow for automating the testing of
certain video game aspects using automated planning and planning action model
learning techniques. The basic idea is to generate detailed gameplay logs and
apply action model learning to obtain a formal model in the planning domain
description language (PDDL). The workflow enables efficient cooperation of game
developers without any experience with PDDL or other formal systems and a
person experienced with PDDL modeling but no game development skills. We
describe the method and workflow in general and then demonstrate it on a
concrete proof-of-concept example -- a simple role-playing game provided as one
of the tutorial projects in the popular game development engine Unity. This
paper presents the first step towards minimizing or even eliminating the need
for a modeling expert in the workflow, thus making automated planning
accessible to a broader audience.
\\ ( https://arxiv.org/abs/2402.12393 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12394 (*cross-listing*)
Date: Fri, 16 Feb 2024 20:19:28 GMT   (4073kb,D)

Title: Improving Model's Interpretability and Reliability using Biomarkers
Authors: Gautam Rajendrakumar Gare, Tom Fox, Beam Chansangavej, Amita Krishnan,
  Ricardo Luis Rodriguez, Bennett P deBoisblanc, Deva Kannan Ramanan, John
  Michael Galeotti
Categories: cs.HC cs.AI cs.LG eess.IV
Comments: Accepted at BIAS 2023 Conference
\\
  Accurate and interpretable diagnostic models are crucial in the
safety-critical field of medicine. We investigate the interpretability of our
proposed biomarker-based lung ultrasound diagnostic pipeline to enhance
clinicians' diagnostic capabilities. The objective of this study is to assess
whether explanations from a decision tree classifier, utilizing biomarkers, can
improve users' ability to identify inaccurate model predictions compared to
conventional saliency maps. Our findings demonstrate that decision tree
explanations, based on clinically established biomarkers, can assist clinicians
in detecting false positives, thus improving the reliability of diagnostic
models in medicine.
\\ ( https://arxiv.org/abs/2402.12394 ,  4073kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12396 (*cross-listing*)
Date: Fri, 16 Feb 2024 23:07:53 GMT   (3084kb)

Title: Toward using GANs in astrophysical Monte-Carlo simulations
Authors: Ahab Isaac, Wesley Armour, Karel Ad\'amek
Categories: astro-ph.HE cs.AI
Comments: Proceedings of ADASS XXXIII (2023)
ACM-class: I.2.0
\\
  Accurate modelling of spectra produced by X-ray sources requires the use of
Monte-Carlo simulations. These simulations need to evaluate physical processes,
such as those occurring in accretion processes around compact objects by
sampling a number of different probability distributions. This is
computationally time-consuming and could be sped up if replaced by neural
networks. We demonstrate, on an example of the Maxwell-J\"uttner distribution
that describes the speed of relativistic electrons, that the generative
adversarial network (GAN) is capable of statistically replicating the
distribution. The average value of the Kolmogorov-Smirnov test is 0.5 for
samples generated by the neural network, showing that the generated
distribution cannot be distinguished from the true distribution.
\\ ( https://arxiv.org/abs/2402.12396 ,  3084kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12405 (*cross-listing*)
Date: Sun, 18 Feb 2024 05:39:00 GMT   (9631kb,D)

Title: scInterpreter: Training Large Language Models to Interpret scRNA-seq
  Data for Cell Type Annotation
Authors: Cong Li, Meng Xiao, Pengfei Wang, Guihai Feng, Xin Li, Yuanchun Zhou
Categories: q-bio.GN cs.AI
Comments: 4 pages, submitted to FCS
\\
  Despite the inherent limitations of existing Large Language Models in
directly reading and interpreting single-cell omics data, they demonstrate
significant potential and flexibility as the Foundation Model. This research
focuses on how to train and adapt the Large Language Model with the capability
to interpret and distinguish cell types in single-cell RNA sequencing data. Our
preliminary research results indicate that these foundational models excel in
accurately categorizing known cell types, demonstrating the potential of the
Large Language Models as effective tools for uncovering new biological
insights.
\\ ( https://arxiv.org/abs/2402.12405 ,  9631kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12411 (*cross-listing*)
Date: Mon, 19 Feb 2024 02:34:23 GMT   (3733kb,D)

Title: Deep Structural Knowledge Exploitation and Synergy for Estimating Node
  Importance Value on Heterogeneous Information Networks
Authors: Yankai Chen, Yixiang Fang, Qiongyan Wang, Xin Cao, Irwin King
Categories: cs.SI cs.AI cs.LG
Comments: Accepted by AAAI 2024
\\
  Node importance estimation problem has been studied conventionally with
homogeneous network topology analysis. To deal with network heterogeneity, a
few recent methods employ graph neural models to automatically learn diverse
sources of information. However, the major concern revolves around that their
full adaptive learning process may lead to insufficient information
exploration, thereby formulating the problem as the isolated node value
prediction with underperformance and less interpretability. In this work, we
propose a novel learning framework: SKES. Different from previous automatic
learning designs, SKES exploits heterogeneous structural knowledge to enrich
the informativeness of node representations. Based on a sufficiently
uninformative reference, SKES estimates the importance value for any input
node, by quantifying its disparity against the reference. This establishes an
interpretable node importance computation paradigm. Furthermore, SKES dives
deep into the understanding that "nodes with similar characteristics are prone
to have similar importance values" whilst guaranteeing that such
informativeness disparity between any different nodes is orderly reflected by
the embedding distance of their associated latent features. Extensive
experiments on three widely-evaluated benchmarks demonstrate the performance
superiority of SKES over several recent competing methods.
\\ ( https://arxiv.org/abs/2402.12411 ,  3733kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12412 (*cross-listing*)
Date: Mon, 19 Feb 2024 04:39:30 GMT   (3771kb,D)

Title: Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI:
  Unpredictable Plays Never Repeating The Same
Authors: Sungjun Ahn, Hyun-Jeong Yim, Youngwan Lee, and Sung-Ik Park
Categories: cs.HC cs.AI cs.MM eess.SP
Comments: 13 pages, 7 figures
\\
  This paper introduces a media service model that exploits artificial
intelligence (AI) video generators at the receive end. This proposal deviates
from the traditional multimedia ecosystem, completely relying on in-house
production, by shifting part of the content creation onto the receiver. We
bring a semantic process into the framework, allowing the distribution network
to provide service elements that prompt the content generator, rather than
distributing encoded data of fully finished programs. The service elements
include fine-tailored text descriptions, lightweight image data of some
objects, or application programming interfaces, comprehensively referred to as
semantic sources, and the user terminal translates the received semantic data
into video frames. Empowered by the random nature of generative AI, the users
could then experience super-personalized services accordingly. The proposed
idea incorporates the situations in which the user receives different service
providers' element packages; a sequence of packages over time, or multiple
packages at the same time. Given promised in-context coherence and content
integrity, the combinatory dynamics will amplify the service diversity,
allowing the users to always chance upon new experiences. This work
particularly aims at short-form videos and advertisements, which the users
would easily feel fatigued by seeing the same frame sequence every time. In
those use cases, the content provider's role will be recast as scripting
semantic sources, transformed from a thorough producer. Overall, this work
explores a new form of media ecosystem facilitated by receiver-embedded
generative models, featuring both random content dynamics and enhanced delivery
efficiency simultaneously.
\\ ( https://arxiv.org/abs/2402.12412 ,  3771kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12416 (*cross-listing*)
Date: Mon, 19 Feb 2024 08:18:53 GMT   (2795kb,D)

Title: Aligning Individual and Collective Objectives in Multi-Agent Cooperation
Authors: Yang Li, Wenhao Zhang, Jianhong Wang, Shao Zhang, Yali Du, Ying Wen,
  Wei Pan
Categories: cs.MA cs.AI
Comments: 15 pages
\\
  In the field of multi-agent learning, the challenge of mixed-motive
cooperation is pronounced, given the inherent contradictions between individual
and collective goals. Current research in this domain primarily focuses on
incorporating domain knowledge into rewards or introducing additional
mechanisms to foster cooperation. However, many of these methods suffer from
the drawbacks of manual design costs and the lack of a theoretical grounding
convergence procedure to the solution. To address this gap, we approach the
mixed-motive game by modeling it as a differentiable game to study learning
dynamics. We introduce a novel optimization method named Altruistic Gradient
Adjustment (AgA) that employs gradient adjustments to novelly align individual
and collective objectives. Furthermore, we provide theoretical proof that the
selection of an appropriate alignment weight in AgA can accelerate convergence
towards the desired solutions while effectively avoiding the undesired ones.
The visualization of learning dynamics effectively demonstrates that AgA
successfully achieves alignment between individual and collective objectives.
Additionally, through evaluations conducted on established mixed-motive
benchmarks such as the public good game, Cleanup, Harvest, and our modified
mixed-motive SMAC environment, we validate AgA's capability to facilitate
altruistic and fair collaboration.
\\ ( https://arxiv.org/abs/2402.12416 ,  2795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12426 (*cross-listing*)
Date: Mon, 19 Feb 2024 17:52:29 GMT   (636kb)

Title: Attacks on Node Attributes in Graph Neural Networks
Authors: Ying Xu, Michael Lanier, Anindya Sarkar, Yevgeniy Vorobeychik
Categories: cs.SI cs.AI cs.LG
Comments: Accepted to AAAI 2024 AICS workshop
\\
  Graphs are commonly used to model complex networks prevalent in modern social
media and literacy applications. Our research investigates the vulnerability of
these graphs through the application of feature based adversarial attacks,
focusing on both decision-time attacks and poisoning attacks. In contrast to
state-of-the-art models like Net Attack and Meta Attack, which target node
attributes and graph structure, our study specifically targets node attributes.
For our analysis, we utilized the text dataset Hellaswag and graph datasets
Cora and CiteSeer, providing a diverse basis for evaluation. Our findings
indicate that decision-time attacks using Projected Gradient Descent (PGD) are
more potent compared to poisoning attacks that employ Mean Node Embeddings and
Graph Contrastive Learning strategies. This provides insights for graph data
security, pinpointing where graph-based models are most vulnerable and thereby
informing the development of stronger defense mechanisms against such attacks.
\\ ( https://arxiv.org/abs/2402.12426 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12451 (*cross-listing*)
Date: Mon, 19 Feb 2024 19:01:01 GMT   (154kb,D)

Title: The (R)Evolution of Multimodal Large Language Models: A Survey
Authors: Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas
  Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia,
  Rita Cucchiara
Categories: cs.CV cs.AI cs.CL cs.MM
\\
  Connecting text and visual modalities plays an essential role in generative
intelligence. For this reason, inspired by the success of large language
models, significant research efforts are being devoted to the development of
Multimodal Large Language Models (MLLMs). These models can seamlessly integrate
visual and textual modalities, both as input and output, while providing a
dialogue-based interface and instruction-following capabilities. In this paper,
we provide a comprehensive review of recent visual-based MLLMs, analyzing their
architectural choices, multimodal alignment strategies, and training
techniques. We also conduct a detailed analysis of these models across a wide
range of tasks, including visual grounding, image generation and editing,
visual understanding, and domain-specific applications. Additionally, we
compile and describe training datasets and evaluation benchmarks, conducting
comparisons among existing models in terms of performance and computational
requirements. Overall, this survey offers a comprehensive overview of the
current state of the art, laying the groundwork for future MLLMs.
\\ ( https://arxiv.org/abs/2402.12451 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12499 (*cross-listing*)
Date: Mon, 19 Feb 2024 20:06:15 GMT   (2693kb,D)

Title: Automated Security Response through Online Learning with Adaptive
  Conjectures
Authors: Kim Hammar, Tao Li, Rolf Stadler, Quanyan Zhu
Categories: cs.GT cs.AI cs.CR cs.LG cs.SY eess.SY
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  We study automated security response for an IT infrastructure and formulate
the interaction between an attacker and a defender as a partially observed,
non-stationary game. We relax the standard assumption that the game model is
correctly specified and consider that each player has a probabilistic
conjecture about the model, which may be misspecified in the sense that the
true model has probability 0. This formulation allows us to capture uncertainty
about the infrastructure and the intents of the players. To learn effective
game strategies online, we design a novel method where a player iteratively
adapts its conjecture using Bayesian learning and updates its strategy through
rollout. We prove that the conjectures converge to best fits, and we provide a
bound on the performance improvement that rollout enables with a conjectured
model. To characterize the steady state of the game, we propose a variant of
the Berk-Nash equilibrium. We present our method through an advanced persistent
threat use case. Simulation studies based on testbed measurements show that our
method produces effective security strategies that adapt to a changing
environment. We also find that our method enables faster convergence than
current reinforcement learning techniques.
\\ ( https://arxiv.org/abs/2402.12499 ,  2693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12525 (*cross-listing*)
Date: Mon, 19 Feb 2024 20:36:32 GMT   (9940kb,D)

Title: LangXAI: Integrating Large Vision Models for Generating Textual
  Explanations to Enhance Explainability in Visual Perception Tasks
Authors: Truong Thanh Hung Nguyen, Tobias Clement, Phuc Truong Loc Nguyen, Nils
  Kemmerzell, Van Binh Truong, Vo Thanh Khang Nguyen, Mohamed Abdelaal, Hung
  Cao
Categories: cs.CV cs.AI
\\
  LangXAI is a framework that integrates Explainable Artificial Intelligence
(XAI) with advanced vision models to generate textual explanations for visual
recognition tasks. Despite XAI advancements, an understanding gap persists for
end-users with limited domain knowledge in artificial intelligence and computer
vision. LangXAI addresses this by furnishing text-based explanations for
classification, object detection, and semantic segmentation model outputs to
end-users. Preliminary results demonstrate LangXAI's enhanced plausibility,
with high BERTScore across tasks, fostering a more transparent and reliable AI
framework on vision tasks for end-users.
\\ ( https://arxiv.org/abs/2402.12525 ,  9940kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12551 (*cross-listing*)
Date: Mon, 19 Feb 2024 21:20:56 GMT   (7486kb,D)

Title: Landmark-based Localization using Stereo Vision and Deep Learning in
  GPS-Denied Battlefield Environment
Authors: Ganesh Sapkota and Sanjay Madria
Categories: cs.CV cs.AI
Comments: arXiv admin note: text overlap with arXiv:2402.12320
\\
  Localization in a battlefield environment is increasingly challenging as GPS
connectivity is often denied or unreliable, and physical deployment of anchor
nodes across wireless networks for localization can be difficult in hostile
battlefield terrain. Existing range-free localization methods rely on
radio-based anchors and their average hop distance which suffers from accuracy
and stability in dynamic and sparse wireless network topology. Vision-based
methods like SLAM and Visual Odometry use expensive sensor fusion techniques
for map generation and pose estimation. This paper proposes a novel framework
for localization in non-GPS battlefield environments using only the passive
camera sensors and considering naturally existing or artificial landmarks as
anchors. The proposed method utilizes a customcalibrated stereo vision camera
for distance estimation and the YOLOv8s model, which is trained and fine-tuned
with our real-world dataset for landmark recognition. The depth images are
generated using an efficient stereomatching algorithm, and distances to
landmarks are determined by extracting the landmark depth feature utilizing a
bounding box predicted by the landmark recognition model. The position of the
unknown node is then obtained using the efficient least square algorithm and
then optimized using the L-BFGS-B (limited-memory quasi-Newton code for
bound-constrained optimization) method. Experimental results demonstrate that
our proposed framework performs better than existing anchorbased DV-Hop
algorithms and competes with the most efficient vision-based algorithms in
terms of localization error (RMSE).
\\ ( https://arxiv.org/abs/2402.12551 ,  7486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12617 (*cross-listing*)
Date: Tue, 20 Feb 2024 00:51:05 GMT   (1020kb,D)

Title: Generative AI Security: Challenges and Countermeasures
Authors: Banghua Zhu, Norman Mu, Jiantao Jiao, and David Wagner
Categories: cs.CR cs.AI cs.CL cs.CY cs.LG
\\
  Generative AI's expanding footprint across numerous industries has led to
both excitement and increased scrutiny. This paper delves into the unique
security challenges posed by Generative AI, and outlines potential research
directions for managing these risks.
\\ ( https://arxiv.org/abs/2402.12617 ,  1020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12624 (*cross-listing*)
Date: Tue, 20 Feb 2024 01:07:32 GMT   (2752kb,D)

Title: Efficient Parameter Mining and Freezing for Continual Object Detection
Authors: Angelo G. Menezes, Augusto J. Peterlevitz, Mateus A. Chinelatto and
  Andr\'e C. P. L. F. de Carvalho
Categories: cs.CV cs.AI
Comments: In Proceedings of the 19th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications - Volume 2:
  VISAPP, ISBN 978-989-758-679-8, ISSN 2184-4321, pages 466-474
\\
  Continual Object Detection is essential for enabling intelligent agents to
interact proactively with humans in real-world settings. While
parameter-isolation strategies have been extensively explored in the context of
continual learning for classification, they have yet to be fully harnessed for
incremental object detection scenarios. Drawing inspiration from prior research
that focused on mining individual neuron responses and integrating insights
from recent developments in neural pruning, we proposed efficient ways to
identify which layers are the most important for a network to maintain the
performance of a detector across sequential updates. The presented findings
highlight the substantial advantages of layer-level parameter isolation in
facilitating incremental learning within object detection models, offering
promising avenues for future research and application in real-world scenarios.
\\ ( https://arxiv.org/abs/2402.12624 ,  2752kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12720 (*cross-listing*)
Date: Tue, 20 Feb 2024 05:05:28 GMT   (12735kb,D)

Title: Revisiting the Information Capacity of Neural Network Watermarks: Upper
  Bound Estimation and Beyond
Authors: Fangqi Li, Haodong Zhao, Wei Du, Shilin Wang
Categories: cs.CR cs.AI
Comments: Accepted by AAAI 2024
\\
  To trace the copyright of deep neural networks, an owner can embed its
identity information into its model as a watermark. The capacity of the
watermark quantify the maximal volume of information that can be verified from
the watermarked model. Current studies on capacity focus on the ownership
verification accuracy under ordinary removal attacks and fail to capture the
relationship between robustness and fidelity. This paper studies the capacity
of deep neural network watermarks from an information theoretical perspective.
We propose a new definition of deep neural network watermark capacity analogous
to channel capacity, analyze its properties, and design an algorithm that
yields a tight estimation of its upper bound under adversarial overwriting. We
also propose a universal non-invasive method to secure the transmission of the
identity message beyond capacity by multiple rounds of ownership verification.
Our observations provide evidence for neural network owners and defenders that
are curious about the tradeoff between the integrity of their ownership and the
performance degradation of their products.
\\ ( https://arxiv.org/abs/2402.12720 ,  12735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12721 (*cross-listing*)
Date: Tue, 20 Feb 2024 05:06:20 GMT   (3676kb,D)

Title: PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for
  Recognizing Low-Quality Images
Authors: Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee,
  Kookjin Lee, Noseong Park
Categories: cs.CV cs.AI
Comments: Accepted at ICLR 2024
\\
  A standard practice in developing image recognition models is to train a
model on a specific image resolution and then deploy it. However, in real-world
inference, models often encounter images different from the training sets in
resolution and/or subject to natural variations such as weather changes, noise
types and compression artifacts. While traditional solutions involve training
multiple models for different resolutions or input variations, these methods
are computationally expensive and thus do not scale in practice. To this end,
we propose a novel neural network model, parallel-structured and all-component
Fourier neural operator (PAC-FNO), that addresses the problem. Unlike
conventional feed-forward neural networks, PAC-FNO operates in the frequency
domain, allowing it to handle images of varying resolutions within a single
model. We also propose a two-stage algorithm for training PAC-FNO with a
minimal modification to the original, downstream model. Moreover, the proposed
PAC-FNO is ready to work with existing image recognition models. Extensively
evaluating methods with seven image recognition benchmarks, we show that the
proposed PAC-FNO improves the performance of existing baseline models on images
with various resolutions by up to 77.1% and various types of natural variations
in the images at inference.
\\ ( https://arxiv.org/abs/2402.12721 ,  3676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12728 (*cross-listing*)
Date: Tue, 20 Feb 2024 05:32:24 GMT   (408kb,D)

Title: Modality-Aware Integration with Large Language Models for
  Knowledge-based Visual Question Answering
Authors: Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao
  Huang
Categories: cs.CV cs.AI cs.CL cs.IR cs.LG
Comments: 8 pages,3 figures and 1 page appendix; The processed graphs and codes
  will be avalibale
\\
  Knowledge-based visual question answering (KVQA) has been extensively studied
to answer visual questions with external knowledge, e.g., knowledge graphs
(KGs). While several attempts have been proposed to leverage large language
models (LLMs) as an implicit knowledge source, it remains challenging since
LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g.,
images, KGs and LLMs, cannot be readily aligned for complex scenarios. To
tackle these, we present a novel modality-aware integration with LLMs for KVQA
(MAIL). It carefully leverages multimodal knowledge for both image
understanding and knowledge reasoning. Specifically, (i) we propose a two-stage
prompting strategy with LLMs to densely embody the image into a scene graph
with detailed visual features; (ii) We construct a coupled concept graph by
linking the mentioned entities with external facts. (iii) A tailored
pseudo-siamese graph medium fusion is designed for sufficient multimodal
fusion. We utilize the shared mentioned entities in two graphs as mediums to
bridge a tight inter-modal exchange, while maximally preserving insightful
intra-modal learning by constraining the fusion within mediums. Extensive
experiments on two benchmark datasets show the superiority of MAIL with 24x
less resources.
\\ ( https://arxiv.org/abs/2402.12728 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12733 (*cross-listing*)
Date: Tue, 20 Feb 2024 05:57:01 GMT   (8646kb,D)

Title: BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation
Authors: Weixin Li, Yuhao Wu, Yang Liu, Weike Pan, Zhong Ming
Categories: cs.IR cs.AI
\\
  In real recommendation scenarios, users often have different types of
behaviors, such as clicking and buying. Existing research methods show that it
is possible to capture the heterogeneous interests of users through different
types of behaviors. However, most multi-behavior approaches have limitations in
learning the relationship between different behaviors. In this paper, we
propose a novel multilayer perceptron (MLP)-based heterogeneous sequential
recommendation method, namely behavior-aware multilayer perceptron (BMLP).
Specifically, it has two main modules, including a heterogeneous interest
perception (HIP) module, which models behaviors at multiple granularities
through behavior types and transition relationships, and a purchase intent
perception (PIP) module, which adaptively fuses subsequences of auxiliary
behaviors to capture users' purchase intent. Compared with mainstream sequence
models, MLP is competitive in terms of accuracy and has unique advantages in
simplicity and efficiency. Extensive experiments show that BMLP achieves
significant improvement over state-of-the-art algorithms on four public
datasets. In addition, its pure MLP architecture leads to a linear time
complexity.
\\ ( https://arxiv.org/abs/2402.12733 ,  8646kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12736 (*cross-listing*)
Date: Tue, 20 Feb 2024 06:01:31 GMT   (320kb,D)

Title: CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer
  Learning
Authors: Feng Chen
Categories: cs.CV cs.AI
\\
  Achieving a universally high accuracy in object detection is quite
challenging, and the mainstream focus in the industry currently lies on
detecting specific classes of objects. However, deploying one or multiple
object detection networks requires a certain amount of GPU memory for training
and storage capacity for inference. This presents challenges in terms of how to
effectively coordinate multiple object detection tasks under
resource-constrained conditions. This paper introduces a lightweight
fine-tuning strategy called Calibration side tuning, which integrates aspects
of adapter tuning and side tuning to adapt the successful techniques employed
in transformers for use with ResNet. The Calibration side tuning architecture
that incorporates maximal transition calibration, utilizing a small number of
additional parameters to enhance network performance while maintaining a smooth
training process. Furthermore, this paper has conducted an analysis on multiple
fine-tuning strategies and have implemented their application within ResNet,
thereby expanding the research on fine-tuning strategies for object detection
networks. Besides, this paper carried out extensive experiments using five
benchmark datasets. The experimental results demonstrated that this method
outperforms other compared state-of-the-art techniques, and a better balance
between the complexity and performance of the finetune schemes is achieved.
\\ ( https://arxiv.org/abs/2402.12736 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12750 (*cross-listing*)
Date: Tue, 20 Feb 2024 06:38:10 GMT   (10345kb,D)

Title: Model Composition for Multimodal Large Language Models
Authors: Chi Chen, Yiyang Du, Zheng Fang, Ziyue Wang, Fuwen Luo, Peng Li, Ming
  Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu
Categories: cs.CV cs.AI cs.CL
Comments: Code will be available at https://github.com/THUNLP-MT/ModelCompose
\\
  Recent developments in Multimodal Large Language Models (MLLMs) have shown
rapid progress, moving towards the goal of creating versatile MLLMs that
understand inputs from various modalities. However, existing methods typically
rely on joint training with paired multimodal instruction data, which is
resource-intensive and challenging to extend to new modalities. In this paper,
we propose a new paradigm through the model composition of existing MLLMs to
create a new model that retains the modal understanding capabilities of each
original model. Our basic implementation, NaiveMC, demonstrates the
effectiveness of this paradigm by reusing modality encoders and merging LLM
parameters. Furthermore, we introduce DAMC to address parameter interference
and mismatch issues during the merging process, thereby enhancing the model
performance. To facilitate research in this area, we propose MCUB, a benchmark
for assessing ability of MLLMs to understand inputs from diverse modalities.
Experiments on this benchmark and four other multimodal understanding tasks
show significant improvements over baselines, proving that model composition
can create a versatile model capable of processing inputs from multiple
modalities.
\\ ( https://arxiv.org/abs/2402.12750 ,  10345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12760 (*cross-listing*)
Date: Tue, 20 Feb 2024 06:58:49 GMT   (6272kb,D)

Title: A User-Friendly Framework for Generating Model-Preferred Prompts in
  Text-to-Image Synthesis
Authors: Nailei Hei, Qianyu Guo, Zihao Wang, Yan Wang, Haofen Wang, Wenqiang
  Zhang
Categories: cs.MM cs.AI cs.CV
Comments: Accepted by The 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2024)
\\
  Well-designed prompts have demonstrated the potential to guide text-to-image
models in generating amazing images. Although existing prompt engineering
methods can provide high-level guidance, it is challenging for novice users to
achieve the desired results by manually entering prompts due to a discrepancy
between novice-user-input prompts and the model-preferred prompts. To bridge
the distribution gap between user input behavior and model training datasets,
we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and
propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG)
for automated prompt optimization. For CFP, we construct a novel dataset for
text-to-image tasks that combines coarse and fine-grained prompts to facilitate
the development of automated prompt generation methods. For UF-FGTG, we propose
a novel framework that automatically translates user-input prompts into
model-preferred prompts. Specifically, we propose a prompt refiner that
continually rewrites prompts to empower users to select results that align with
their unique needs. Meanwhile, we integrate image-related loss functions from
the text-to-image model into the training process of text generation to
generate model-preferred prompts. Additionally, we propose an adaptive feature
extraction module to ensure diversity in the generated results. Experiments
demonstrate that our approach is capable of generating more visually appealing
and diverse images than previous state-of-the-art methods, achieving an average
improvement of 5% across six quality and aesthetic metrics.
\\ ( https://arxiv.org/abs/2402.12760 ,  6272kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12782 (*cross-listing*)
Date: Tue, 20 Feb 2024 07:47:39 GMT   (8054kb,D)

Title: Advancing GenAI Assisted Programming--A Comparative Study on Prompt
  Efficiency and Code Quality Between GPT-4 and GLM-4
Authors: Angus Yang, Zehan Li, and Jie Li
Categories: cs.SE cs.AI
Comments: 18 pages, 4 figures, 3 tables
ACM-class: D.2.3
\\
  This study aims to explore the best practices for utilizing GenAI as a
programming tool, through a comparative analysis between GPT-4 and GLM-4. By
evaluating prompting strategies at different levels of complexity, we identify
that simplest and straightforward prompting strategy yields best code
generation results. Additionally, adding a CoT-like preliminary confirmation
step would further increase the success rate. Our results reveal that while
GPT-4 marginally outperforms GLM-4, the difference is minimal for average
users. In our simplified evaluation model, we see a remarkable 30 to 100-fold
increase in code generation efficiency over traditional coding norms. Our GenAI
Coding Workshop highlights the effectiveness and accessibility of the prompting
methodology developed in this study. We observe that GenAI-assisted coding
would trigger a paradigm shift in programming landscape, which necessitates
developers to take on new roles revolving around supervising and guiding GenAI,
and to focus more on setting high-level objectives and engaging more towards
innovation.
\\ ( https://arxiv.org/abs/2402.12782 ,  8054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12794 (*cross-listing*)
Date: Tue, 20 Feb 2024 08:08:07 GMT   (1010kb)

Title: Autonomous Reality Modelling for Cultural Heritage Sites employing
  cooperative quadrupedal robots and unmanned aerial vehicles
Authors: Nikolaos Giakoumidis and Christos-Nikolaos Anagnostopoulos
Categories: cs.RO cs.AI cs.LG
\\
  Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners,
mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has
become the prevalent practice for 3D Reality Modeling and digitization of
large-scale monuments of Cultural Heritage (CH). In practice, this process is
heavily related to the expertise of the surveying team, handling the laborious
planning and time-consuming execution of the 3D mapping process that is
tailored to the specific requirements and constraints of each site. To minimize
human intervention, this paper introduces a novel methodology for autonomous 3D
Reality Modeling for CH monuments by employing au-tonomous biomimetic
quadrupedal robotic agents and UAVs equipped with the appropriate sensors.
These autonomous robotic agents carry out the 3D RM process in a systematic and
repeatable ap-proach. The outcomes of this automated process may find
applications in digital twin platforms, facilitating secure monitoring and
management of cultural heritage sites and spaces, in both indoor and outdoor
environments.
\\ ( https://arxiv.org/abs/2402.12794 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12810 (*cross-listing*)
Date: Tue, 20 Feb 2024 08:28:45 GMT   (13788kb,D)

Title: PIP-Net: Pedestrian Intention Prediction in the Wild
Authors: Mohsen Azarmi, Mahdi Rezaei, He Wang, Sebastien Glaser
Categories: cs.CV cs.AI cs.NE eess.IV stat.ML
\\
  Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs)
is one of the current research challenges in this field. In this article, we
introduce PIP-Net, a novel framework designed to predict pedestrian crossing
intentions by AVs in real-world urban scenarios. We offer two variants of
PIP-Net designed for different camera mounts and setups. Leveraging both
kinematic data and spatial features from the driving scene, the proposed model
employs a recurrent and temporal attention-based solution, outperforming
state-of-the-art performance. To enhance the visual representation of road
users and their proximity to the ego vehicle, we introduce a categorical depth
feature map, combined with a local motion flow feature, providing rich insights
into the scene dynamics. Additionally, we explore the impact of expanding the
camera's field of view, from one to three cameras surrounding the ego vehicle,
leading to enhancement in the model's contextual perception. Depending on the
traffic scenario and road environment, the model excels in predicting
pedestrian crossing intentions up to 4 seconds in advance which is a
breakthrough in current research studies in pedestrian intention prediction.
Finally, for the first time, we present the Urban-PIP dataset, a customised
pedestrian intention prediction dataset, with multi-camera annotations in
real-world automated driving scenarios.
\\ ( https://arxiv.org/abs/2402.12810 ,  13788kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12843 (*cross-listing*)
Date: Tue, 20 Feb 2024 09:13:11 GMT   (972kb,D)

Title: SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets
Authors: Sankarshanaa Sagaram, Aditya Kasliwal, Krish Didwania, Laven
  Srivastava, Pallavi Kailas, Ujjwal Verma
Categories: cs.CV cs.AI
Comments: Published at ICLR Tiny Paper 2024
\\
  The increasing adoption of solar energy necessitates advanced methodologies
for monitoring and maintenance to ensure optimal performance of solar panel
installations. A critical component in this context is the accurate
segmentation of solar panels from aerial or satellite imagery, which is
essential for identifying operational issues and assessing efficiency. This
paper addresses the significant challenges in panel segmentation, particularly
the scarcity of annotated data and the labour-intensive nature of manual
annotation for supervised learning. We explore and apply Self-Supervised
Learning (SSL) to solve these challenges. We demonstrate that SSL significantly
enhances model generalization under various conditions and reduces dependency
on manually annotated data, paving the way for robust and adaptable solar panel
segmentation solutions.
\\ ( https://arxiv.org/abs/2402.12843 ,  972kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12846 (*cross-listing*)
Date: Tue, 20 Feb 2024 09:20:30 GMT   (5517kb,D)

Title: ConVQG: Contrastive Visual Question Generation with Multimodal Guidance
Authors: Li Mi, Syrielle Montariol, Javiera Castillo-Navarro, Xianjie Dai,
  Antoine Bosselut, Devis Tuia
Categories: cs.CV cs.AI
Comments: AAAI 2024. Project page at https://limirs.github.io/ConVQG
\\
  Asking questions about visual environments is a crucial way for intelligent
agents to understand rich multi-faceted scenes, raising the importance of
Visual Question Generation (VQG) systems. Apart from being grounded to the
image, existing VQG systems can use textual constraints, such as expected
answers or knowledge triplets, to generate focused questions. These constraints
allow VQG systems to specify the question content or leverage external
commonsense knowledge that can not be obtained from the image content only.
However, generating focused questions using textual constraints while enforcing
a high relevance to the image content remains a challenge, as VQG systems often
ignore one or both forms of grounding. In this work, we propose Contrastive
Visual Question Generation (ConVQG), a method using a dual contrastive
objective to discriminate questions generated using both modalities from those
based on a single one. Experiments on both knowledge-aware and standard VQG
benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and
generates image-grounded, text-guided, and knowledge-rich questions. Our human
evaluation results also show preference for ConVQG questions compared to
non-contrastive baselines.
\\ ( https://arxiv.org/abs/2402.12846 ,  5517kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12928 (*cross-listing*)
Date: Tue, 20 Feb 2024 11:28:50 GMT   (4217kb,D)

Title: A Literature Review of Literature Reviews in Pattern Analysis and
  Machine Intelligence
Authors: Penghai Zhao, Xin Zhang, Ming-Ming Cheng, Jian Yang, Xiang Li
Categories: cs.DL cs.AI cs.CV
Comments: 20 pages,9 figures, 5 tables. [February 19, 2024]
\\
  By consolidating scattered knowledge, the literature review provides a
comprehensive understanding of the investigated topic. However, excessive
reviews, especially in the booming field of pattern analysis and machine
intelligence (PAMI), raise concerns for both researchers and reviewers. In
response to these concerns, this Analysis aims to provide a thorough review of
reviews in the PAMI field from diverse perspectives. First, large language
model-empowered bibliometric indicators are proposed to evaluate literature
reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI,
and a topic dataset are constructed, which are utilized to obtain statistical
characteristics of PAMI reviews. Unlike traditional bibliometric measurements,
the proposed article-level indicators provide real-time and field-normalized
quantified assessments of reviews without relying on user-defined keywords.
Second, based on these indicators, the study presents comparative analyses of
different reviews, unveiling the characteristics of publications across various
fields, periods, and journals. The newly emerging AI-generated literature
reviews are also appraised, and the observed differences suggest that most
AI-generated reviews still lag behind human-authored reviews in several
aspects. Third, we briefly provide a subjective evaluation of representative
PAMI reviews and introduce a paper structure-based typology of literature
reviews. This typology may improve the clarity and effectiveness for scholars
in reading and writing reviews, while also serving as a guide for AI systems in
generating well-organized reviews. Finally, this Analysis offers insights into
the current challenges of literature reviews and envisions future directions
for their development.
\\ ( https://arxiv.org/abs/2402.12928 ,  4217kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12950 (*cross-listing*)
Date: Tue, 20 Feb 2024 12:11:28 GMT   (1311kb,D)

Title: QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems
Authors: Jinjing Shi, Zimeng Xiao, Heyuan Shi, Yu Jiang, Xuelong Li
Categories: cs.SE cs.AI
\\
  Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with
the fundamental theory of quantum mechanics to achieve machine learning tasks
with quantum acceleration. Recently, QNN systems have been found to manifest
robustness issues similar to classical DL systems. There is an urgent need for
ways to test their correctness and security. However, QNN systems differ
significantly from traditional quantum software and classical DL systems,
posing critical challenges for QNN testing. These challenges include the
inapplicability of traditional quantum software testing methods, the dependence
of quantum test sample generation on perturbation operators, and the absence of
effective information in quantum neurons. In this paper, we propose QuanTest, a
quantum entanglement-guided adversarial testing framework to uncover potential
erroneous behaviors in QNN systems. We design a quantum entanglement adequacy
criterion to quantify the entanglement acquired by the input quantum states
from the QNN system, along with two similarity metrics to measure the proximity
of generated quantum adversarial examples to the original inputs. Subsequently,
QuanTest formulates the problem of generating test inputs that maximize the
quantum entanglement sufficiency and capture incorrect behaviors of the QNN
system as a joint optimization problem and solves it in a gradient-based manner
to generate quantum adversarial examples. Experimental results demonstrate that
QuanTest possesses the capability to capture erroneous behaviors in QNN systems
(generating 67.48%-96.05% more test samples than the random noise under the
same perturbation size constraints). The entanglement-guided approach proves
effective in adversarial testing, generating more adversarial examples (maximum
increase reached 21.32%).
\\ ( https://arxiv.org/abs/2402.12950 ,  1311kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12993 (*cross-listing*)
Date: Tue, 20 Feb 2024 13:21:46 GMT   (323kb,D)

Title: An Autonomous Large Language Model Agent for Chemical Literature Data
  Mining
Authors: Kexin Chen, Hanqun Cao, Junyou Li, Yuyang Du, Menghao Guo, Xin Zeng,
  Lanqing Li, Jiezhong Qiu, Pheng Ann Heng, Guangyong Chen
Categories: cs.IR cs.AI cs.LG q-bio.QM
\\
  Chemical synthesis, which is crucial for advancing material synthesis and
drug discovery, impacts various sectors including environmental science and
healthcare. The rise of technology in chemistry has generated extensive
chemical data, challenging researchers to discern patterns and refine synthesis
processes. Artificial intelligence (AI) helps by analyzing data to optimize
synthesis and increase yields. However, AI faces challenges in processing
literature data due to the unstructured format and diverse writing style of
chemical literature. To overcome these difficulties, we introduce an end-to-end
AI agent framework capable of high-fidelity extraction from extensive chemical
literature. This AI agent employs large language models (LLMs) for prompt
generation and iterative optimization. It functions as a chemistry assistant,
automating data collection and analysis, thereby saving manpower and enhancing
performance. Our framework's efficacy is evaluated using accuracy, recall, and
F1 score of reaction condition data, and we compared our method with human
experts in terms of content correctness and time efficiency. The proposed
approach marks a significant advancement in automating chemical literature
extraction and demonstrates the potential for AI to revolutionize data
management and utilization in chemistry.
\\ ( https://arxiv.org/abs/2402.12993 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13126 (*cross-listing*)
Date: Tue, 20 Feb 2024 16:39:23 GMT   (27665kb,D)

Title: VGMShield: Mitigating Misuse of Video Generative Models
Authors: Yan Pang, Yang Zhang, Tianhao Wang
Categories: cs.CR cs.AI cs.CV cs.LG eess.IV
Comments: 17 pages, 10 figures
\\
  With the rapid advancement in video generation, people can conveniently
utilize video generation models to create videos tailored to their specific
desires. Nevertheless, there are also growing concerns about their potential
misuse in creating and disseminating false information.
  In this work, we introduce VGMShield: a set of three straightforward but
pioneering mitigations through the lifecycle of fake video generation. We start
from \textit{fake video detection} trying to understand whether there is
uniqueness in generated videos and whether we can differentiate them from real
videos; then, we investigate the \textit{tracing} problem, which maps a fake
video back to a model that generates it. Towards these, we propose to leverage
pre-trained models that focus on {\it spatial-temporal dynamics} as the
backbone to identify inconsistencies in videos. Through experiments on seven
state-of-the-art open-source models, we demonstrate that current models still
cannot perfectly handle spatial-temporal relationships, and thus, we can
accomplish detection and tracing with nearly perfect accuracy.
  Furthermore, anticipating future generative model improvements, we propose a
{\it prevention} method that adds invisible perturbations to images to make the
generated videos look unreal. Together with fake video detection and tracing,
our multi-faceted set of solutions can effectively mitigate misuse of video
generative models.
\\ ( https://arxiv.org/abs/2402.13126 ,  27665kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13201 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:10:39 GMT   (1091kb,D)

Title: Tiny Reinforcement Learning for Quadruped Locomotion using Decision
  Transformers
Authors: Orhan Eren Akg\"un, N\'estor Cuevas, Matheus Farias, Daniel Garces
Categories: cs.RO cs.AI cs.LG
Comments: 10 pages, 4 figures
\\
  Resource-constrained robotic platforms are particularly useful for tasks that
require low-cost hardware alternatives due to the risk of losing the robot,
like in search-and-rescue applications, or the need for a large number of
devices, like in swarm robotics. For this reason, it is crucial to find
mechanisms for adapting reinforcement learning techniques to the constraints
imposed by lower computational power and smaller memory capacities of these
ultra low-cost robotic platforms. We try to address this need by proposing a
method for making imitation learning deployable onto resource-constrained
robotic platforms. Here we cast the imitation learning problem as a conditional
sequence modeling task and we train a decision transformer using expert
demonstrations augmented with a custom reward. Then, we compress the resulting
generative model using software optimization schemes, including quantization
and pruning. We test our method in simulation using Isaac Gym, a realistic
physics simulation environment designed for reinforcement learning. We
empirically demonstrate that our method achieves natural looking gaits for
Bittle, a resource-constrained quadruped robot. We also run multiple
simulations to show the effects of pruning and quantization on the performance
of the model. Our results show that quantization (down to 4 bits) and pruning
reduce model size by around 30\% while maintaining a competitive reward, making
the model deployable in a resource-constrained system.
\\ ( https://arxiv.org/abs/2402.13201 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13217 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:29:49 GMT   (4442kb,D)

Title: VideoPrism: A Foundational Visual Encoder for Video Understanding
Authors: Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan,
  Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, Rachel
  Hornung, Florian Schroff, Ming-Hsuan Yang, David A. Ross, Huisheng Wang,
  Hartwig Adam, Mikhail Sirotenko, Ting Liu, Boqing Gong
Categories: cs.CV cs.AI
\\
  We introduce VideoPrism, a general-purpose video encoder that tackles diverse
video understanding tasks with a single frozen model. We pretrain VideoPrism on
a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M
video clips with noisy parallel text (e.g., ASR transcripts). The pretraining
approach improves upon masked autoencoding by global-local distillation of
semantic video embeddings and a token shuffling scheme, enabling VideoPrism to
focus primarily on the video modality while leveraging the invaluable text
associated with videos. We extensively test VideoPrism on four broad groups of
video understanding tasks, from web video question answering to CV for science,
achieving state-of-the-art performance on 30 out of 33 video understanding
benchmarks.
\\ ( https://arxiv.org/abs/2402.13217 ,  4442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13224 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:37:11 GMT   (438kb,D)

Title: Controlling Large Electric Vehicle Charging Stations via User Behavior
  Modeling and Stochastic Programming
Authors: Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud
Categories: math.OC cs.AI cs.CE cs.LG cs.SY eess.SY
\\
  This paper introduces an Electric Vehicle Charging Station (EVCS) model that
incorporates real-world constraints, such as slot power limitations, contract
threshold overruns penalties, or early disconnections of electric vehicles
(EVs). We propose a formulation of the problem of EVCS control under
uncertainty, and implement two Multi-Stage Stochastic Programming approaches
that leverage user-provided information, namely, Model Predictive Control and
Two-Stage Stochastic Programming. The model addresses uncertainties in charging
session start and end times, as well as in energy demand. A user's behavior
model based on a sojourn-time-dependent stochastic process enhances cost
reduction while maintaining customer satisfaction. The benefits of the two
proposed methods are showcased against two baselines over a 22-day simulation
using a real-world dataset. The two-stage approach proves robust against early
disconnections, considering a more significant number of uncertainty scenarios
for optimization. The algorithm prioritizing user satisfaction over electricity
cost achieves a 20% and 36% improvement in two user satisfaction metrics
compared to an industry-standard baseline. Additionally, the algorithm striking
the best balance between cost and user satisfaction exhibits a mere 3% relative
cost increase compared to the theoretically optimal baseline - for which the
nonanticipativity constraint is relaxed - while attaining 94% and 84% of the
user satisfaction performance in the two used satisfaction metrics.
\\ ( https://arxiv.org/abs/2402.13224 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13226 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:37:42 GMT   (6585kb,D)

Title: NeRF Solves Undersampled MRI Reconstruction
Authors: Tae Jun Jang, Chang Min Hyun
Categories: eess.IV cs.AI cs.CE eess.SP
\\
  This article presents a novel undersampled magnetic resonance imaging (MRI)
technique that leverages the concept of Neural Radiance Field (NeRF). With
radial undersampling, the corresponding imaging problem can be reformulated
into an image modeling task from sparse-view rendered data; therefore, a high
dimensional MR image is obtainable from undersampled $k$-space data by taking
advantage of implicit neural representation. A multi-layer perceptron, which is
designed to output an image intensity from a spatial coordinate, learns the MR
physics-driven rendering relation between given measurement data and desired
image. Effective undersampling strategies for high-quality neural
representation are investigated. The proposed method serves two benefits: (i)
The learning is based fully on single undersampled $k$-space data, not a bunch
of measured data and target image sets. It can be used potentially for
diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively
rare or limited against diversity of clinical images while undersampled
reconstruction is highly demanded. (ii) A reconstructed MR image is a
scan-specific representation highly adaptive to the given $k$-space
measurement. Numerous experiments validate the feasibility and capability of
the proposed approach.
\\ ( https://arxiv.org/abs/2402.13226 ,  6585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13254 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:59:55 GMT   (5821kb,D)

Title: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples
Authors: Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 12 pages, 8 figures, Project Page: https://countercurate.github.io/
\\
  We propose CounterCurate, a framework to comprehensively improve the
visio-linguistic compositional reasoning capability for both contrastive and
generative multimodal models. In particular, we identify two under-explored
critical problems: the neglect of the physically grounded reasoning (counting
and position understanding) and the potential of using highly capable text and
image generation models for semantic counterfactual fine-tuning. Our work
pioneers an approach that addresses these gaps. We first spotlight the
near-chance performance of multimodal models like CLIP and LLaVA in physically
grounded compositional reasoning. We then apply simple data augmentation using
a grounded image generation model, GLIGEN, to generate finetuning data,
resulting in significant performance improvements: +33% and +37% for CLIP and
LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.
Moreover, we exploit the capabilities of high-performing text generation and
image generation models, specifically GPT-4V and DALLE-3, to curate challenging
semantic counterfactuals, thereby further enhancing compositional reasoning
capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms
GPT-4V.
\\ ( https://arxiv.org/abs/2402.13254 ,  5821kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12168 (*cross-listing*)
Date: Mon, 19 Feb 2024 14:22:54 GMT   (5926kb,D)

Title: Defending Against Weight-Poisoning Backdoor Attacks for
  Parameter-Efficient Fine-Tuning
Authors: Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi
  Jia, Jinming Wen
Categories: cs.CR cs.AI cs.CL
\\
  Recently, various parameter-efficient fine-tuning (PEFT) strategies for
application to language models have been proposed and successfully implemented.
However, this raises the question of whether PEFT, which only updates a limited
set of model parameters, constitutes security vulnerabilities when confronted
with weight-poisoning backdoor attacks. In this study, we show that PEFT is
more susceptible to weight-poisoning backdoor attacks compared to the
full-parameter fine-tuning method, with pre-defined triggers remaining
exploitable and pre-defined targets maintaining high confidence, even after
fine-tuning. Motivated by this insight, we developed a Poisoned Sample
Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples
through confidence, providing robust defense against weight-poisoning backdoor
attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset
sample labels. During the inference process, extreme confidence serves as an
indicator for poisoned samples, while others are clean. We conduct experiments
on text classification tasks, five fine-tuning strategies, and three
weight-poisoning backdoor attack methods. Experiments show near 100% success
rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,
our defensive approach exhibits overall competitive performance in mitigating
weight-poisoning backdoor attacks.
\\ ( https://arxiv.org/abs/2402.12168 ,  5926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12423 (*cross-listing*)
Date: Mon, 19 Feb 2024 16:22:21 GMT   (8031kb,D)

Title: On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models
Authors: Miri Varshavsky Hassid, Roy Hirsch, Regev Cohen, Tomer Golany, Daniel
  Freedman, Ehud Rivlin
Categories: cs.SD cs.CL cs.LG eess.AS
\\
  The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech
(TTS) domain is rising, providing great value in synthesizing high quality
speech. Although they exhibit impressive audio quality, the extent of their
semantic capabilities is unknown, and controlling their synthesized speech's
vocal properties remains a challenge. Inspired by recent advances in image
synthesis, we explore the latent space of frozen TTS models, which is composed
of the latent bottleneck activations of the DDM's denoiser. We identify that
this space contains rich semantic information, and outline several novel
methods for finding semantic directions within it, both supervised and
unsupervised. We then demonstrate how these enable off-the-shelf audio editing,
without any further training, architectural changes or data requirements. We
present evidence of the semantic and acoustic qualities of the edited audio,
and provide supplemental samples:
https://latent-analysis-grad-tts.github.io/speech-samples/.
\\ ( https://arxiv.org/abs/2402.12423 ,  8031kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12556 (*cross-listing*)
Date: Mon, 19 Feb 2024 21:31:11 GMT   (11807kb,D)

Title: IMBUE: Improving Interpersonal Effectiveness through Simulation and
  Just-in-time Feedback with Human-Language Model Interaction
Authors: Inna Wanyin Lin, Ashish Sharma, Christopher Michael Rytting, Adam S.
  Miner, Jina Suh, Tim Althoff
Categories: cs.HC cs.CL
\\
  Navigating certain communication situations can be challenging due to
individuals' lack of skills and the interference of strong emotions. However,
effective learning opportunities are rarely accessible. In this work, we
conduct a human-centered study that uses language models to simulate bespoke
communication training and provide just-in-time feedback to support the
practice and learning of interpersonal effectiveness skills. We apply the
interpersonal effectiveness framework from Dialectical Behavioral Therapy
(DBT), DEAR MAN, which focuses on both conversational and emotional skills. We
present IMBUE, an interactive training system that provides feedback 25% more
similar to experts' feedback, compared to that generated by GPT-4. IMBUE is the
first to focus on communication skills and emotion management simultaneously,
incorporate experts' domain knowledge in providing feedback, and be grounded in
psychology theory. Through a randomized trial of 86 participants, we find that
IMBUE's simulation-only variant significantly improves participants'
self-efficacy (up to 17%) and reduces negative emotions (up to 25%). With
IMBUE's additional just-in-time feedback, participants demonstrate 17%
improvement in skill mastery, along with greater enhancements in self-efficacy
(27% more) and reduction of negative emotions (16% more) compared to
simulation-only. The improvement in skill mastery is the only measure that is
transferred to new and more difficult situations; situation specific training
is necessary for improving self-efficacy and emotion reduction.
\\ ( https://arxiv.org/abs/2402.12556 ,  11807kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12784 (*cross-listing*)
Date: Tue, 20 Feb 2024 07:49:30 GMT   (413kb,D)

Title: Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval
  Systems
Authors: Shengyao Zhuang, Bevan Koopman, Xiaoran Chu, Guido Zuccon
Categories: cs.IR cs.CL
\\
  The introduction of Vec2Text, a technique for inverting text embeddings, has
raised serious privacy concerns within dense retrieval systems utilizing text
embeddings, including those provided by OpenAI and Cohere. This threat comes
from the ability for a malicious attacker with access to text embeddings to
reconstruct the original text.
  In this paper, we investigate various aspects of embedding models that could
influence the recoverability of text using Vec2Text. Our exploration involves
factors such as distance metrics, pooling functions, bottleneck pre-training,
training with noise addition, embedding quantization, and embedding dimensions
-- aspects not previously addressed in the original Vec2Text paper. Through a
thorough analysis of these factors, our aim is to gain a deeper understanding
of the critical elements impacting the trade-offs between text recoverability
and retrieval effectiveness in dense retrieval systems. This analysis provides
valuable insights for practitioners involved in designing privacy-aware dense
retrieval systems. Additionally, we propose a straightforward fix for embedding
transformation that ensures equal ranking effectiveness while mitigating the
risk of text recoverability.
  Furthermore, we extend the application of Vec2Text to the separate task of
corpus poisoning, where, theoretically, Vec2Text presents a more potent threat
compared to previous attack methods. Notably, Vec2Text does not require access
to the dense retriever's model parameters and can efficiently generate numerous
adversarial passages.
  In summary, this study highlights the potential threat posed by Vec2Text to
existing dense retrieval systems, while also presenting effective methods to
patch and strengthen such systems against such risks.
\\ ( https://arxiv.org/abs/2402.12784 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12844 (*cross-listing*)
Date: Tue, 20 Feb 2024 09:13:15 GMT   (1208kb,D)

Title: ICON: Improving Inter-Report Consistency of Radiology Report Generation
  via Lesion-aware Mix-up Augmentation
Authors: Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, Jiang Liu
Categories: cs.CV cs.CL
\\
  Previous research on radiology report generation has made significant
progress in terms of increasing the clinical accuracy of generated reports. In
this paper, we emphasize another crucial quality that it should possess, i.e.,
inter-report consistency, which refers to the capability of generating
consistent reports for semantically equivalent radiographs. This quality is
even of greater significance than the overall report accuracy in terms of
ensuring the system's credibility, as a system prone to providing conflicting
results would severely erode users' trust. Regrettably, existing approaches
struggle to maintain inter-report consistency, exhibiting biases towards common
patterns and susceptibility to lesion variants. To address this issue, we
propose ICON, which improves the inter-report consistency of radiology report
generation. Aiming at enhancing the system's ability to capture the
similarities in semantically equivalent lesions, our approach involves first
extracting lesions from input images and examining their characteristics. Then,
we introduce a lesion-aware mix-up augmentation technique to ensure that the
representations of the semantically equivalent lesions align with the same
attributes, by linearly interpolating them during the training phase. Extensive
experiments on three publicly available chest X-ray datasets verify the
effectiveness of our approach, both in terms of improving the consistency and
accuracy of the generated reports.
\\ ( https://arxiv.org/abs/2402.12844 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12959 (*cross-listing*)
Date: Tue, 20 Feb 2024 12:25:26 GMT   (763kb,D)

Title: Prompt Stealing Attacks Against Large Language Models
Authors: Zeyang Sha and Yang Zhang
Categories: cs.CR cs.CL
\\
  The increasing reliance on large language models (LLMs) such as ChatGPT in
various fields emphasizes the importance of ``prompt engineering,'' a
technology to improve the quality of model outputs. With companies investing
significantly in expert prompt engineers and educational resources rising to
meet market demand, designing high-quality prompts has become an intriguing
challenge. In this paper, we propose a novel attack against LLMs, named prompt
stealing attacks. Our proposed prompt stealing attack aims to steal these
well-designed prompts based on the generated answers. The prompt stealing
attack contains two primary modules: the parameter extractor and the prompt
reconstruction. The goal of the parameter extractor is to figure out the
properties of the original prompts. We first observe that most prompts fall
into one of three categories: direct prompt, role-based prompt, and in-context
prompt. Our parameter extractor first tries to distinguish the type of prompts
based on the generated answers. Then, it can further predict which role or how
many contexts are used based on the types of prompts. Following the parameter
extractor, the prompt reconstructor can be used to reconstruct the original
prompts based on the generated answers and the extracted features. The final
goal of the prompt reconstructor is to generate the reversed prompts, which are
similar to the original prompts. Our experimental results show the remarkable
performance of our proposed attacks. Our proposed attacks add a new dimension
to the study of prompt engineering and call for more attention to the security
issues on LLMs.
\\ ( https://arxiv.org/abs/2402.12959 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12997 (*cross-listing*)
Date: Tue, 20 Feb 2024 13:25:16 GMT   (267kb,D)

Title: Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism
Authors: Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe,
  C\'eline Hudelot, Pierre Colombo
Categories: cs.IR cs.CL
\\
  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
\\ ( https://arxiv.org/abs/2402.12997 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13152 (*cross-listing*)
Date: Tue, 20 Feb 2024 17:07:08 GMT   (429kb,D)

Title: AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech
  Technologies
Authors: Jos\'e-M. Acosta-Triana, David Gimeno-G\'omez, Carlos-D.
  Mart\'inez-Hinarejos
Categories: cs.CV cs.CL
Comments: Accepted at the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING)
\\
  More than 7,000 known languages are spoken around the world. However, due to
the lack of annotated resources, only a small fraction of them are currently
covered by speech technologies. Albeit self-supervised speech representations,
recent massive speech corpora collections, as well as the organization of
challenges, have alleviated this inequality, most studies are mainly
benchmarked on English. This situation is aggravated when tasks involving both
acoustic and visual speech modalities are addressed. In order to promote
research on low-resource languages for audio-visual speech technologies, we
present AnnoTheia, a semi-automatic annotation toolkit that detects when a
person speaks on the scene and the corresponding transcription. In addition, to
show the complete process of preparing AnnoTheia for a language of interest, we
also describe the adaptation of a pre-trained model for active speaker
detection to Spanish, using a database not initially conceived for this type of
task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on
GitHub.
\\ ( https://arxiv.org/abs/2402.13152 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13220 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:31:27 GMT   (16996kb,D)

Title: How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on
  Deceptive Prompts
Authors: Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan
Categories: cs.CV cs.CL
\\
  The remarkable advancements in Multimodal Large Language Models (MLLMs) have
not rendered them immune to challenges, particularly in the context of handling
deceptive information in prompts, thus producing hallucinated responses under
such conditions. To quantitatively assess this vulnerability, we present
MAD-Bench, a carefully curated benchmark that contains 850 test samples divided
into 6 categories, such as non-existent objects, count of objects, spatial
relationship, and visual confusion. We provide a comprehensive analysis of
popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as
LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps
between GPT-4V and other models; and previous robust instruction-tuned models,
such as LRV-Instruction and LLaVA-RLHF, are not effective on this new
benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of
any other model in our experiments ranges from 5% to 35%. We further propose a
remedy that adds an additional paragraph to the deceptive prompts to encourage
models to think twice before answering the question. Surprisingly, this simple
method can even double the accuracy; however, the absolute numbers are still
too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark
to stimulate further research to enhance models' resilience against deceptive
prompts.
\\ ( https://arxiv.org/abs/2402.13220 ,  16996kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13234 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:49:41 GMT   (427kb,D)

Title: Unlocking Insights: Semantic Search in Jupyter Notebooks
Authors: Lan Li, Jinpeng Lv
Categories: cs.IR cs.CL
\\
  Semantic search, a process aimed at delivering highly relevant search results
by comprehending the searcher's intent and the contextual meaning of terms
within a searchable dataspace, plays a pivotal role in information retrieval.
In this paper, we investigate the application of large language models to
enhance semantic search capabilities, specifically tailored for the domain of
Jupyter Notebooks. Our objective is to retrieve generated outputs, such as
figures or tables, associated functions and methods, and other pertinent
information.
  We demonstrate a semantic search framework that achieves a comprehensive
semantic understanding of the entire notebook's contents, enabling it to
effectively handle various types of user queries. Key components of this
framework include:
  1). A data preprocessor is designed to handle diverse types of cells within
Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative
methodology is devised to address token size limitations that arise with
code-type cells. We implement a finer-grained approach to data input,
transitioning from the cell level to the function level, effectively resolving
these issues.
\\ ( https://arxiv.org/abs/2402.13234 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12397 (*cross-listing*)
Date: Sat, 17 Feb 2024 00:22:29 GMT   (281kb,D)

Title: Multi-class Temporal Logic Neural Networks
Authors: Danyang Li, Roberto Tron
Categories: stat.ML cs.LG
\\
  Time-series data can represent the behaviors of autonomous systems, such as
drones and self-driving cars. The problem of binary and multi-class
classification has received a lot of attention in this field. Neural networks
represent a popular approach to classifying data; However, they lack
interpretability, which poses a significant challenge in extracting meaningful
information from them. Signal Temporal Logic (STL) is a formalism to describe
the properties of timed behaviors. We propose a method that combines all of the
above: neural networks that represent STL specifications for multi-class
classification of time-series data. We offer two key contributions: 1) We
introduce a notion of margin for multi-class classification, and 2) we
introduce the use of STL-based attributes for enhancing the interpretability of
the results. We evaluate our method on two datasets and compare with
state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.12397 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12400 (*cross-listing*)
Date: Sat, 17 Feb 2024 20:16:41 GMT   (16110kb,D)

Title: Estimating the age-conditioned average treatment effects curves: An
  application for assessing load-management strategies in the NBA
Authors: Shinpei Nakamura-Sakai, Laura Forastiere, Brian Macdonald
Categories: stat.AP cs.LG
\\
  In the realm of competitive sports, understanding the performance dynamics of
athletes, represented by the age curve (showing progression, peak, and
decline), is vital. Our research introduces a novel framework for quantifying
age-specific treatment effects, enhancing the granularity of performance
trajectory analysis. Firstly, we propose a methodology for estimating the age
curve using game-level data, diverging from traditional season-level data
approaches, and tackling its inherent complexities with a meta-learner
framework that leverages advanced machine learning models. This approach
uncovers intricate non-linear patterns missed by existing methods. Secondly,
our framework enables the identification of causal effects, allowing for a
detailed examination of age curves under various conditions. By defining the
Age-Conditioned Treatment Effect (ACTE), we facilitate the exploration of
causal relationships regarding treatment impacts at specific ages. Finally,
applying this methodology to study the effects of rest days on performance
metrics, particularly across different ages, offers valuable insights into load
management strategies' effectiveness. Our findings underscore the importance of
tailored rest periods, highlighting their positive impact on athlete
performance and suggesting a reevaluation of current management practices for
optimizing athlete performance.
\\ ( https://arxiv.org/abs/2402.12400 ,  16110kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12435 (*cross-listing*)
Date: Mon, 19 Feb 2024 19:00:01 GMT   (1350kb,D)

Title: Emulating the interstellar medium chemistry with neural operators
Authors: Lorenzo Branca and Andrea Pallottini
Categories: astro-ph.GA cs.LG
Comments: 13 pages, 5 figures, Accepted for publication in A&A
\\
  Galaxy formation and evolution critically depend on understanding the complex
photo-chemical processes that govern the evolution and thermodynamics of the
InterStellar Medium (ISM). Computationally, solving chemistry is among the most
heavy tasks in cosmological and astrophysical simulations. The evolution of
such non-equilibrium photo-chemical network relies on implicit, precise,
computationally costly, ordinary differential equations (ODE) solvers. Here, we
aim at substituting such procedural solvers with fast, pre-trained, emulators
based on neural operators. We emulate a non-equilibrium chemical network up to
H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism,
i.e. by splitting the ODE solver operator that maps the initial conditions and
time evolution into a tensor product of two neural networks. We use
$\texttt{KROME}$ to generate a training set spanning $-2\leq
\log(n/\mathrm{cm}^{-3}) \leq 3.5$, $\log(20) \leq\log(T/\mathrm{K}) \leq 5.5$,
$-6 \leq \log(n_i/n) < 0$, and by adopting an incident radiation field
$\textbf{F}$ sampled in 10 energy bins with a continuity prior. We separately
train the solver for $T$ and each $n_i$ for $\simeq 4.34\,\rm GPUhrs$. Compared
with the reference solutions obtained by $\texttt{KROME}$ for single zone
models, the typical precision obtained is of order $10^{-2}$, i.e. the $10
\times$ better with a training that is $40 \times$ less costly with respect to
previous emulators which however considered only a fixed $\mathbf{F}$. The
present model achieves a speed-up of a factor of $128 \times$ with respect to
stiff ODE solvers. Our neural emulator represents a significant leap forward in
the modeling of ISM chemistry, offering a good balance of precision,
versatility, and computational efficiency.
\\ ( https://arxiv.org/abs/2402.12435 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12448 (*cross-listing*)
Date: Mon, 19 Feb 2024 19:00:09 GMT   (2370kb,D)

Title: DBNets: A publicly available deep learning tool to measure the masses of
  young planets in dusty protoplanetary discs
Authors: Alessandro Ruzza, Giuseppe Lodato, Giovanni Pietro Rosotti
Categories: astro-ph.EP astro-ph.IM cs.LG
DOI: 10.1051/0004-6361/202348421
\\
  Current methods to characterize embedded planets in protoplanetary disc
observations are severely limited either in their ability to fully account for
the observed complex physics or in their computational and time costs. To
address this shortcoming, we developed DBNets: a deep learning tool, based on
convolutional neural networks, that analyses substructures observed in the dust
continuum emission of protoplanetary discs to quickly infer the mass of
allegedly embedded planets. We focussed on developing a method to reliably
quantify not only the planet mass, but also the associated uncertainty
introduced by our modelling and adopted techniques. Our tests gave promising
results achieving an 87% reduction of the log Mp mean squared error with
respect to an analytical formula fitted on the same data (DBNets metrics: lmse
0.016, r2-score 97%). With the goal of providing the final user of DBNets with
all the tools needed to interpret their measurements and decide on their
significance, we extensively tested our tool on out-of-distribution data. We
found that DBNets can identify inputs strongly outside its training scope
returning an uncertainty above a specific threshold and we thus provided a
rejection criterion that helps determine the significance of the results
obtained. Additionally, we outlined some limitations of our tool: it can be
reliably applied only on discs observed with inclinations below approximately
60{\deg}, in the optically thin regime, with a resolution 8 times better than
the gap radial location and with a signal-to-noise ratio higher than
approximately ten. Finally, we applied DBNets to 33 actual observations of
protoplanetary discs measuring the mass of 48 proposed planets and comparing
our results with the available literature. We confirmed that most of the
observed gaps imply planets in the sub-Jupiter regime. DBNets is publicly
available at dbnets.fisica.unimi.it.
\\ ( https://arxiv.org/abs/2402.12448 ,  2370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12475 (*cross-listing*)
Date: Mon, 19 Feb 2024 19:21:45 GMT   (14326kb)

Title: Diffeomorphism Neural Operator for various domains and parameters of
  partial differential equations
Authors: Zhiwei Zhao, Changqing Liu, Yingguang Li, Zhibin Chen, Xu Liu
Categories: math.NA cs.LG cs.NA
Comments: 15 pages; 5 figures;
\\
  Many science and engineering applications demand partial differential
equations (PDE) evaluations that are traditionally computed with
resource-intensive numerical solvers. Neural operator models provide an
efficient alternative by learning the governing physical laws directly from
data in a class of PDEs with different parameters, but constrained in a fixed
boundary (domain). Many applications, such as design and manufacturing, would
benefit from neural operators with flexible domains when studied at scale. Here
we present a diffeomorphism neural operator learning framework towards
developing domain-flexible models for physical systems with various and complex
domains. Specifically, a neural operator trained in a shared domain mapped from
various domains of fields by diffeomorphism is proposed, which transformed the
problem of learning function mappings in varying domains (spaces) into the
problem of learning operators on a shared diffeomorphic domain. Meanwhile, an
index is provided to evaluate the generalization of diffeomorphism neural
operators in different domains by the domain diffeomorphism similarity.
Experiments on statics scenarios (Darcy flow, mechanics) and dynamic scenarios
(pipe flow, airfoil flow) demonstrate the advantages of our approach for neural
operator learning under various domains, where harmonic and volume
parameterization are used as the diffeomorphism for 2D and 3D domains. Our
diffeomorphism neural operator approach enables strong learning capability and
robust generalization across varying domains and parameters.
\\ ( https://arxiv.org/abs/2402.12475 ,  14326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12482 (*cross-listing*)
Date: Mon, 19 Feb 2024 19:38:37 GMT   (177kb,D)

Title: SECP: A Speech Enhancement-Based Curation Pipeline For Scalable
  Acquisition Of Clean Speech
Authors: Adam Sabra, Cyprian Wronka, Michelle Mao, Samer Hijazi
Categories: cs.SD cs.IR cs.LG eess.AS
Comments: Accepted to the International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2024
\\
  As more speech technologies rely on a supervised deep learning approach with
clean speech as the ground truth, a methodology to onboard said speech at scale
is needed. However, this approach needs to minimize the dependency on human
listening and annotation, only requiring a human-in-the-loop when needed. In
this paper, we address this issue by outlining Speech Enhancement-based
Curation Pipeline (SECP) which serves as a framework to onboard clean speech.
This clean speech can then train a speech enhancement model, which can further
refine the original dataset and thus close the iterative loop. By running two
iterative rounds, we observe that enhanced output used as ground truth does not
degrade model performance according to $\Delta_{PESQ}$, a metric used in this
paper. We also show through comparative mean opinion score (CMOS) based
subjective tests that the highest and lowest bound of refined data is
perceptually better than the original data.
\\ ( https://arxiv.org/abs/2402.12482 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12498 (*cross-listing*)
Date: Mon, 19 Feb 2024 20:05:41 GMT   (41020kb,D)

Title: Feudal Networks for Visual Navigation
Authors: Faith Johnson, Bryan Bo Cao, Kristin Dana, Shubham Jain, Ashwin Ashok
Categories: cs.CV cs.LG cs.RO
\\
  Visual navigation follows the intuition that humans can navigate without
detailed maps. A common approach is interactive exploration while building a
topological graph with images at nodes that can be used for planning. Recent
variations learn from passive videos and can navigate using complex social and
semantic cues. However, a significant number of training videos are needed,
large graphs are utilized, and scenes are not unseen since odometry is
utilized. We introduce a new approach to visual navigation using feudal
learning, which employs a hierarchical structure consisting of a worker agent,
a mid-level manager, and a high-level manager. Key to the feudal learning
paradigm, agents at each level see a different aspect of the task and operate
at different spatial and temporal scales. Two unique modules are developed in
this framework. For the high- level manager, we learn a memory proxy map in a
self supervised manner to record prior observations in a learned latent space
and avoid the use of graphs and odometry. For the mid-level manager, we develop
a waypoint network that outputs intermediate subgoals imitating human waypoint
selection during local navigation. This waypoint network is pre-trained using a
new, small set of teleoperation videos that we make publicly available, with
training environments different from testing environments. The resulting feudal
navigation network achieves near SOTA performance, while providing a novel
no-RL, no-graph, no-odometry, no-metric map approach to the image goal
navigation task.
\\ ( https://arxiv.org/abs/2402.12498 ,  41020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12500 (*cross-listing*)
Date: Mon, 19 Feb 2024 20:08:13 GMT   (3114kb,D)

Title: Integrating kNN with Foundation Models for Adaptable and Privacy-Aware
  Image Classification
Authors: Sebastian Doerrich, Tobias Archut, Francesco Di Salvo, Christian Ledig
Categories: cs.CV cs.LG eess.IV
Comments: Accepted at 21st IEEE International Symposium on Biomedical Imaging
  (IEEE ISBI 2024)
\\
  Traditional deep learning models implicity encode knowledge limiting their
transparency and ability to adapt to data changes. Yet, this adaptability is
vital for addressing user data privacy concerns. We address this limitation by
storing embeddings of the underlying training data independently of the model
weights, enabling dynamic data modifications without retraining. Specifically,
our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a
vision-based foundation model, pre-trained self-supervised on natural images,
enhancing interpretability and adaptability. We share open-source
implementations of a previously unpublished baseline method as well as our
performance-improving contributions. Quantitative experiments confirm improved
classification across established benchmark datasets and the method's
applicability to distinct medical image classification tasks. Additionally, we
assess the method's robustness in continual learning and data removal
scenarios. The approach exhibits great promise for bridging the gap between
foundation models' performance and challenges tied to data privacy. The source
code is available at
https://github.com/TobArc/privacy-aware-image-classification-with-kNN.
\\ ( https://arxiv.org/abs/2402.12500 ,  3114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12531 (*cross-listing*)
Date: Mon, 19 Feb 2024 20:41:03 GMT   (7489kb,D)

Title: Improving Deep Generative Models on Many-To-One Image-to-Image
  Translation
Authors: Sagar Saxena, Mohammad Nayeem Teli
Categories: cs.CV cs.LG
Comments: 11 pages, 6 figures
\\
  Deep generative models have been applied to multiple applications in image-
to-image translation. Generative Adversarial Networks and Diffusion Models have
presented impressive results, setting new state-of-the-art results on these
tasks. Most methods have symmetric setups across the different domains in a
dataset. These methods assume that all domains have either multiple modalities
or only one modality. However, there are many datasets that have a many-to-one
relationship between two domains. In this work, we first introduce a Colorized
MNIST dataset and a Color-Recall score that can provide a simple benchmark for
evaluating models on many-to-one translation. We then introduce a new
asymmetric framework to improve existing deep generative models on many-to-one
image-to- image translation. We apply this framework to StarGAN V2 and show
that in both unsupervised and semi-supervised settings, the performance of this
new model improves on many-to-one image-to-image translation.
\\ ( https://arxiv.org/abs/2402.12531 ,  7489kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12538 (*cross-listing*)
Date: Mon, 19 Feb 2024 20:55:12 GMT   (559kb)

Title: A Machine Learning Ensemble Model for the Detection of Cyberbullying
Authors: Abulkarim Faraj Alqahtani and Mohammad Ilyas
Categories: cs.SI cs.LG
Journal-ref: International Journal of Artificial Intelligence and Applications
  (IJAIA), Vol.15, No.1, January 2024
\\
  The pervasive use of social media platforms, such as Facebook, Instagram, and
X, has significantly amplified our electronic interconnectedness. Moreover,
these platforms are now easily accessible from any location at any given time.
However, the increased popularity of social media has also led to
cyberbullying.It is imperative to address the need for finding, monitoring, and
mitigating cyberbullying posts on social media platforms. Motivated by this
necessity, we present this paper to contribute to developing an automated
system for detecting binary labels of aggressive tweets.Our study has
demonstrated remarkable performance compared to previous experiments on the
same dataset. We employed the stacking ensemble machine learning method,
utilizing four various feature extraction techniques to optimize performance
within the stacking ensemble learning framework. Combining five machine
learning algorithms,Decision Trees, Random Forest, Linear Support Vector
Classification, Logistic Regression, and K-Nearest Neighbors into an ensemble
method, we achieved superior results compared to traditional machine learning
classifier models. The stacking classifier achieved a high accuracy rate of
94.00%, outperforming traditional machine learning models and surpassing the
results of prior experiments that utilized the same dataset. The outcomes of
our experiments showcased an accuracy rate of 0.94% in detection tweets as
aggressive or non-aggressive.
\\ ( https://arxiv.org/abs/2402.12538 ,  559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12539 (*cross-listing*)
Date: Mon, 19 Feb 2024 21:01:11 GMT   (5085kb,D)

Title: Impact of data usage for forecasting on performance of model predictive
  control in buildings with smart energy storage
Authors: Max Langtry, Vijja Wichitwechkarn, Rebecca Ward, Chaoqun Zhuang,
  Monika J. Kreitmair, Nikolas Makasis, Zack Xuereb Conti, Ruchi Choudhary
Categories: eess.SY cs.LG cs.SY
Comments: 34 pages, 22 figures
\\
  Data is required to develop forecasting models for use in Model Predictive
Control (MPC) schemes in building energy systems. However, data usage incurs
costs from both its collection and exploitation. Determining cost optimal data
usage requires understanding of the forecast accuracy and resulting MPC
operational performance it enables. This study investigates the performance of
both simple and state-of-the-art machine learning prediction models for MPC in
a multi-building energy system simulation using historic building energy data.
The impact of data usage on forecast accuracy is quantified for the following
data efficiency measures: reuse of prediction models, reduction of training
data volumes, reduction of model data features, and online model training. A
simple linear multi-layer perceptron model is shown to provide equivalent
forecast accuracy to state-of-the-art models, with greater data efficiency and
generalisability. The use of more than 2 years of training data for load
prediction models provided no significant improvement in forecast accuracy.
Forecast accuracy and data efficiency were improved simultaneously by using
change-point analysis to screen training data. Reused models and those trained
with 3 months of data had on average 10% higher error than baseline, indicating
that deploying MPC systems without prior data collection may be economic.
\\ ( https://arxiv.org/abs/2402.12539 ,  5085kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12550 (*cross-listing*)
Date: Mon, 19 Feb 2024 21:20:22 GMT   (6300kb,D)

Title: Multilinear Mixture of Experts: Scalable Expert Specialization through
  Factorization
Authors: James Oldfield, Markos Georgopoulos, Grigorios G. Chrysos, Christos
  Tzelepis, Yannis Panagakis, Mihalis A. Nicolaou, Jiankang Deng, Ioannis
  Patras
Categories: cs.CV cs.LG
Comments: Github: https://github.com/james-oldfield/MMoE. Project page:
  https://eecs.qmul.ac.uk/~jo001/MMoE/
\\
  The Mixture of Experts (MoE) paradigm provides a powerful way to decompose
inscrutable dense layers into smaller, modular computations often more amenable
to human interpretation, debugging, and editability. A major problem however
lies in the computational cost of scaling the number of experts to achieve
sufficiently fine-grained specialization. In this paper, we propose the
Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision
models. MMoE layers perform an implicit computation on prohibitively large
weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid
the issues incurred through the discrete expert routing in the popular 'sparse'
MoE models, yet (2) do not incur the restrictively high inference-time costs of
'soft' MoE alternatives. We present both qualitative and quantitative evidence
(through visualization and counterfactual interventions respectively) that
scaling MMoE layers when fine-tuning foundation models for vision tasks leads
to more specialized experts at the class-level whilst remaining competitive
with the performance of parameter-matched linear layer counterparts. Finally,
we show that learned expert specialism further facilitates manual correction of
demographic bias in CelebA attribute classification. Our MMoE model code is
available at https://github.com/james-oldfield/MMoE.
\\ ( https://arxiv.org/abs/2402.12550 ,  6300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12595 (*cross-listing*)
Date: Mon, 19 Feb 2024 23:19:15 GMT   (26kb)

Title: Truncated Polynomial Expansion-Based Detection in Massive MIMO: A
  Model-Driven Deep Learning Approach
Authors: Kazem Izadinasab, Ahmed Wagdy Shaban, Oussama Damen
Categories: eess.SP cs.LG
Comments: 5 pages, 2 figures, 2 tables
\\
  In this paper, we propose a deep learning (DL)-based approach for efficiently
computing the inverse of Hermitian matrices using truncated polynomial
expansion (TPE). Our model-driven approach involves optimizing the coefficients
of the TPE during an offline training procedure for a given number of TPE
terms. We apply this method to signal detection in uplink massive
multiple-input multiple-output (MIMO) systems, where the matrix inverse
operation required by linear detectors, such as zero-forcing (ZF) and minimum
mean square error (MMSE), is approximated using TPE. Our simulation results
demonstrate that the proposed learned TPE-based method outperforms the
conventional TPE method with optimal coefficients in terms of asymptotic
convergence speed and reduces the computational complexity of the online
detection stage, albeit at the expense of the offline training stage. However,
the limited number of trainable parameters leads to a swift offline training
process.
\\ ( https://arxiv.org/abs/2402.12595 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12630 (*cross-listing*)
Date: Tue, 20 Feb 2024 01:22:04 GMT   (2397kb,D)

Title: FAST: An Optimization Framework for Fast Additive Segmentation in
  Transparent ML
Authors: Brian Liu and Rahul Mazumder
Categories: stat.ML cs.LG
\\
  We present FAST, an optimization framework for fast additive segmentation.
FAST segments piecewise constant shape functions for each feature in a dataset
to produce transparent additive models. The framework leverages a novel
optimization procedure to fit these models $\sim$2 orders of magnitude faster
than existing state-of-the-art methods, such as explainable boosting machines
\citep{nori2019interpretml}. We also develop new feature selection algorithms
in the FAST framework to fit parsimonious models that perform well. Through
experiments and case studies, we show that FAST improves the computational
efficiency and interpretability of additive models.
\\ ( https://arxiv.org/abs/2402.12630 ,  2397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12668 (*cross-listing*)
Date: Tue, 20 Feb 2024 02:36:26 GMT   (24409kb,D)

Title: Randomization Can Reduce Both Bias and Variance: A Case Study in Random
  Forests
Authors: Brian Liu and Rahul Mazumder
Categories: stat.ML cs.LG
\\
  We study the often overlooked phenomenon, first noted in
\cite{breiman2001random}, that random forests appear to reduce bias compared to
bagging. Motivated by an interesting paper by \cite{mentch2020randomization},
where the authors argue that random forests reduce effective degrees of freedom
and only outperform bagging ensembles in low signal-to-noise ratio (SNR)
settings, we explore how random forests can uncover patterns in the data missed
by bagging. We empirically demonstrate that in the presence of such patterns,
random forests reduce bias along with variance and increasingly outperform
bagging ensembles when SNR is high. Our observations offer insights into the
real-world success of random forests across a range of SNRs and enhance our
understanding of the difference between random forests and bagging ensembles
with respect to the randomization injected into each split. Our investigations
also yield practical insights into the importance of tuning $mtry$ in random
forests.
\\ ( https://arxiv.org/abs/2402.12668 ,  24409kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12704 (*cross-listing*)
Date: Tue, 20 Feb 2024 04:06:28 GMT   (130kb,D)

Title: Quantum Embedding with Transformer for High-dimensional Data
Authors: Hao-Yuan Chen, Yen-Jui Chang, Shih-Wei Liao, Ching-Ray Chang
Categories: quant-ph cs.LG
\\
  Quantum embedding with transformers is a novel and promising architecture for
quantum machine learning to deliver exceptional capability on near-term devices
or simulators. The research incorporated a vision transformer (ViT) to advance
quantum significantly embedding ability and results for a single qubit
classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a
challenging high-dimensional dataset. The study showcases and analyzes
empirical evidence that our transformer-based architecture is a highly
versatile and practical approach to modern quantum machine learning problems.
\\ ( https://arxiv.org/abs/2402.12704 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12710 (*cross-listing*)
Date: Tue, 20 Feb 2024 04:13:59 GMT   (1111kb,D)

Title: Integrating Active Learning in Causal Inference with Interference: A
  Novel Approach in Online Experiments
Authors: Hongtao Zhu, Sizhe Zhang, Yang Su, Zhenyu Zhao, Nan Chen
Categories: stat.ME cs.LG stat.ML
Comments: conference paper
\\
  In the domain of causal inference research, the prevalent potential outcomes
framework, notably the Rubin Causal Model (RCM), often overlooks individual
interference and assumes independent treatment effects. This assumption,
however, is frequently misaligned with the intricate realities of real-world
scenarios, where interference is not merely a possibility but a common
occurrence. Our research endeavors to address this discrepancy by focusing on
the estimation of direct and spillover treatment effects under two assumptions:
(1) network-based interference, where treatments on neighbors within connected
networks affect one's outcomes, and (2) non-random treatment assignments
influenced by confounders. To improve the efficiency of estimating potentially
complex effects functions, we introduce an novel active learning approach:
Active Learning in Causal Inference with Interference (ACI). This approach uses
Gaussian process to flexibly model the direct and spillover treatment effects
as a function of a continuous measure of neighbors' treatment assignment. The
ACI framework sequentially identifies the experimental settings that demand
further data. It further optimizes the treatment assignments under the network
interference structure using genetic algorithms to achieve efficient learning
outcome. By applying our method to simulation data and a Tencent game dataset,
we demonstrate its feasibility in achieving accurate effects estimations with
reduced data requirements. This ACI approach marks a significant advancement in
the realm of data efficiency for causal inference, offering a robust and
efficient alternative to traditional methodologies, particularly in scenarios
characterized by complex interference patterns.
\\ ( https://arxiv.org/abs/2402.12710 ,  1111kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12743 (*cross-listing*)
Date: Tue, 20 Feb 2024 06:19:55 GMT   (13411kb)

Title: APT-MMF: An advanced persistent threat actor attribution method based on
  multimodal and multilevel feature fusion
Authors: Nan Xiao, Bo Lang, Ting Wang, Yikai Chen
Categories: cs.CR cs.LG
\\
  Threat actor attribution is a crucial defense strategy for combating advanced
persistent threats (APTs). Cyber threat intelligence (CTI), which involves
analyzing multisource heterogeneous data from APTs, plays an important role in
APT actor attribution. The current attribution methods extract features from
different CTI perspectives and employ machine learning models to classify CTI
reports according to their threat actors. However, these methods usually
extract only one kind of feature and ignore heterogeneous information,
especially the attributes and relations of indicators of compromise (IOCs),
which form the core of CTI. To address these problems, we propose an APT actor
attribution method based on multimodal and multilevel feature fusion (APT-MMF).
First, we leverage a heterogeneous attributed graph to characterize APT reports
and their IOC information. Then, we extract and fuse multimodal features,
including attribute type features, natural language text features and
topological relationship features, to construct comprehensive node
representations. Furthermore, we design multilevel heterogeneous graph
attention networks to learn the deep hidden features of APT report nodes; these
networks integrate IOC type-level, metapath-based neighbor node-level, and
metapath semantic-level attention. Utilizing multisource threat intelligence,
we construct a heterogeneous attributed graph dataset for verification
purposes. The experimental results show that our method not only outperforms
the existing methods but also demonstrates its good interpretability for
attribution analysis tasks.
\\ ( https://arxiv.org/abs/2402.12743 ,  13411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12762 (*cross-listing*)
Date: Tue, 20 Feb 2024 07:09:39 GMT   (12kb)

Title: Learning under Singularity: An Information Criterion improving WBIC and
  sBIC
Authors: Lirui Liu and Joe Suzuki
Categories: stat.ML cs.LG
\\
  We introduce a novel Information Criterion (IC), termed Learning under
Singularity (LS), designed to enhance the functionality of the Widely
Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian
Information Criterion (sBIC). LS is effective without regularity constraints
and demonstrates stability. Watanabe defined a statistical model or a learning
machine as regular if the mapping from a parameter to a probability
distribution is one-to-one and its Fisher information matrix is positive
definite. In contrast, models not meeting these conditions are termed singular.
Over the past decade, several information criteria for singular cases have been
proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios
but faces challenges with large sample sizes and redundant estimation of known
learning coefficients. Conversely, sBIC is limited in its broader application
due to its dependence on maximum likelihood estimates. LS addresses these
limitations by enhancing the utility of both WBIC and sBIC. It incorporates the
empirical loss from the Widely Applicable Information Criterion (WAIC) to
represent the goodness of fit to the statistical model, along with a penalty
term similar to that of sBIC. This approach offers a flexible and robust method
for model selection, free from regularity constraints.
\\ ( https://arxiv.org/abs/2402.12762 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12828 (*cross-listing*)
Date: Tue, 20 Feb 2024 08:54:07 GMT   (1072kb,D)

Title: SGD with Clipping is Secretly Estimating the Median Gradient
Authors: Fabian Schaipp, Guillaume Garrigos, Umut Simsekli, Robert Gower
Categories: stat.ML cs.LG math.OC
MSC-class: 90C26, 68T07, 62-08
\\
  There are several applications of stochastic optimization where one can
benefit from a robust estimate of the gradient. For example, domains such as
distributed learning with corrupted nodes, the presence of large outliers in
the training data, learning under privacy constraints, or even heavy-tailed
noise due to the dynamics of the algorithm itself. Here we study SGD with
robust gradient estimators based on estimating the median. We first consider
computing the median gradient across samples, and show that the resulting
method can converge even under heavy-tailed, state-dependent noise. We then
derive iterative methods based on the stochastic proximal point method for
computing the geometric median and generalizations thereof. Finally we propose
an algorithm estimating the median gradient across iterations, and find that
several well known methods - in particular different forms of clipping - are
particular cases of this framework.
\\ ( https://arxiv.org/abs/2402.12828 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12867 (*cross-listing*)
Date: Tue, 20 Feb 2024 09:57:49 GMT   (709kb)

Title: Towards MLOps: A DevOps Tools Recommender System for Machine Learning
  System
Authors: Pir Sami Ullah Shah, Naveed Ahmad, Mirza Omer Beg
Categories: cs.SE cs.LG
\\
  Applying DevOps practices to machine learning system is termed as MLOps and
machine learning systems evolve on new data unlike traditional systems on
requirements. The objective of MLOps is to establish a connection between
different open-source tools to construct a pipeline that can automatically
perform steps to construct a dataset, train the machine learning model and
deploy the model to the production as well as store different versions of model
and dataset. Benefits of MLOps is to make sure the fast delivery of the new
trained models to the production to have accurate results. Furthermore, MLOps
practice impacts the overall quality of the software products and is completely
dependent on open-source tools and selection of relevant open-source tools is
considered as challenged while a generalized method to select an appropriate
open-source tools is desirable. In this paper, we present a framework for
recommendation system that processes the contextual information (e.g., nature
of data, type of the data) of the machine learning project and recommends a
relevant toolchain (tech-stack) for the operationalization of machine learning
systems. To check the applicability of the proposed framework, four different
approaches i.e., rule-based, random forest, decision trees and k-nearest
neighbors were investigated where precision, recall and f-score is measured,
the random forest out classed other approaches with highest f-score value of
0.66.
\\ ( https://arxiv.org/abs/2402.12867 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12885 (*cross-listing*)
Date: Tue, 20 Feb 2024 10:25:44 GMT   (34kb)

Title: A Bound on the Maximal Marginal Degrees of Freedom
Authors: Paul Dommel
Categories: stat.ML cs.LG
\\
  Common kernel ridge regression is expensive in memory allocation and
computation time. This paper addresses low rank approximations and surrogates
for kernel ridge regression, which bridge these difficulties. The fundamental
contribution of the paper is a lower bound on the rank of the low dimensional
approximation, which is required such that the prediction power remains
reliable. The bound relates the effective dimension with the largest
statistical leverage score. We characterize the effective dimension and its
growth behavior with respect to the regularization parameter by involving the
regularity of the kernel. This growth is demonstrated to be asymptotically
logarithmic for suitably chosen kernels, justifying low-rank approximations as
the Nystr\"om method.
\\ ( https://arxiv.org/abs/2402.12885 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12971 (*cross-listing*)
Date: Tue, 20 Feb 2024 12:40:31 GMT   (15901kb,D)

Title: How Temporal Unrolling Supports Neural Physics Simulators
Authors: Bjoern List, Li-Wei Chen, Kartik Bali, Nils Thuerey
Categories: physics.comp-ph cs.LG
Comments: Project Page:
  https://ge.in.tum.de/publications/how-temporal-unrolling-supports-neural-physics-simulators/
  , Github Page: https://github.com/tum-pbs/unrolling
\\
  Unrolling training trajectories over time strongly influences the inference
accuracy of neural network-augmented physics simulators. We analyze these
effects by studying three variants of training neural networks on discrete
ground truth trajectories. In addition to commonly used one-step setups and
fully differentiable unrolling, we include a third, less widely used variant:
unrolling without temporal gradients. Comparing networks trained with these
three modalities makes it possible to disentangle the two dominant effects of
unrolling, training distribution shift and long-term gradients. We present a
detailed study across physical systems, network sizes, network architectures,
training setups, and test scenarios. It provides an empirical basis for our
main findings: A non-differentiable but unrolled training setup supported by a
numerical solver can yield 4.5-fold improvements over a fully differentiable
prediction setup that does not utilize this solver. We also quantify a
difference in the accuracy of models trained in a fully differentiable setup
compared to their non-differentiable counterparts. While differentiable setups
perform best, the accuracy of unrolling without temporal gradients comes
comparatively close. Furthermore, we empirically show that these behaviors are
invariant to changes in the underlying physical system, the network
architecture and size, and the numerical scheme. These results motivate
integrating non-differentiable numerical simulators into training setups even
if full differentiability is unavailable. We also observe that the convergence
rate of common neural architectures is low compared to numerical algorithms.
This encourages the use of hybrid approaches combining neural and numerical
algorithms to utilize the benefits of both.
\\ ( https://arxiv.org/abs/2402.12971 ,  15901kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13001 (*cross-listing*)
Date: Tue, 20 Feb 2024 13:32:00 GMT   (14kb)

Title: A unifying primary framework for quantum graph neural networks from
  quantum graph states
Authors: Ammar Daskin
Categories: quant-ph cs.LG
Comments: short version 6 pages
\\
  Graph states are used to represent mathematical graphs as quantum states on
quantum computers. They can be formulated through stabilizer codes or directly
quantum gates and quantum states. In this paper we show that a quantum graph
neural network model can be understood and realized based on graph states. We
show that they can be used either as a parameterized quantum circuits to
represent neural networks or as an underlying structure to construct graph
neural networks on quantum computers.
\\ ( https://arxiv.org/abs/2402.13001 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13005 (*cross-listing*)
Date: Tue, 20 Feb 2024 13:38:04 GMT   (632kb,D)

Title: SzCORE: A Seizure Community Open-source Research Evaluation framework
  for the validation of EEG-based automated seizure detection algorithms
Authors: Jonathan Dan, Una Pale, Alireza Amirshahi, William Cappelletti, Thorir
  Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Adriano Bernini, Luca
  Benini, S\'andor Beniczky, David Atienza, Philippe Ryvlin
Categories: eess.SP cs.LG
\\
  The need for high-quality automated seizure detection algorithms based on
electroencephalography (EEG) becomes ever more pressing with the increasing use
of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods
of these algorithms influences the reported results and makes comprehensive
evaluation and comparison challenging. This heterogeneity concerns in
particular the choice of datasets, evaluation methodologies, and performance
metrics. In this paper, we propose a unified framework designed to establish
standardization in the validation of EEG-based seizure detection algorithms.
Based on existing guidelines and recommendations, the framework introduces a
set of recommendations and standards related to datasets, file formats, EEG
data input content, seizure annotation input and output, cross-validation
strategies, and performance metrics. We also propose the 10-20 seizure
detection benchmark, a machine-learning benchmark based on public datasets
converted to a standardized format. This benchmark defines the machine-learning
task as well as reporting metrics. We illustrate the use of the benchmark by
evaluating a set of existing seizure detection algorithms. The SzCORE (Seizure
Community Open-source Research Evaluation) framework and benchmark are made
publicly available along with an open-source software library to facilitate
research use, while enabling rigorous evaluation of the clinical significance
of the algorithms, fostering a collective effort to more optimally detect
seizures to improve the lives of people with epilepsy.
\\ ( https://arxiv.org/abs/2402.13005 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13076 (*cross-listing*)
Date: Tue, 20 Feb 2024 15:22:25 GMT   (9033kb,D)

Title: Not All Weights Are Created Equal: Enhancing Energy Efficiency in
  On-Device Streaming Speech Recognition
Authors: Yang Li, Yuan Shangguan, Yuhao Wang, Liangzhen Lai, Ernie Chang,
  Changsheng Zhao, Yangyang Shi, Vikas Chandra
Categories: cs.SD cs.LG eess.AS
\\
  Power consumption plays an important role in on-device streaming speech
recognition, as it has a direct impact on the user experience. This study
delves into how weight parameters in speech recognition models influence the
overall power consumption of these models. We discovered that the impact of
weight parameters on power consumption varies, influenced by factors including
how often they are invoked and their placement in memory. Armed with this
insight, we developed design guidelines aimed at optimizing on-device speech
recognition models. These guidelines focus on minimizing power use without
substantially affecting accuracy. Our method, which employs targeted
compression based on the varying sensitivities of weight parameters,
demonstrates superior performance compared to state-of-the-art compression
methods. It achieves a reduction in energy usage of up to 47% while maintaining
similar model accuracy and improving the real-time factor.
\\ ( https://arxiv.org/abs/2402.13076 ,  9033kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13079 (*cross-listing*)
Date: Tue, 20 Feb 2024 15:24:21 GMT   (97kb)

Title: Mode Estimation with Partial Feedback
Authors: Charles Arnal, Vivien Cabannes, Vianney Perchet
Categories: stat.ML cs.IR cs.IT cs.LG math.IT
MSC-class: 62L05, 62B86, 62D10, 62B10
\\
  The combination of lightly supervised pre-training and online fine-tuning has
played a key role in recent AI developments. These new learning pipelines call
for new theoretical frameworks. In this paper, we formalize core aspects of
weakly supervised and active learning with a simple problem: the estimation of
the mode of a distribution using partial feedback. We show how entropy coding
allows for optimal information acquisition from partial feedback, develop
coarse sufficient statistics for mode identification, and adapt bandit
algorithms to our new setting. Finally, we combine those contributions into a
statistically and computationally efficient solution to our problem.
\\ ( https://arxiv.org/abs/2402.13079 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13106 (*cross-listing*)
Date: Tue, 20 Feb 2024 16:01:39 GMT   (248kb,D)

Title: On Generalization Bounds for Deep Compound Gaussian Neural Networks
Authors: Carter Lyons, Raghu G. Raj, Margaret Cheney
Categories: stat.ML cs.LG eess.SP
Comments: 14 pages, 1 figure
\\
  Algorithm unfolding or unrolling is the technique of constructing a deep
neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide
better interpretability and superior empirical performance over standard DNNs
in signal estimation tasks. An important theoretical question, which has only
recently received attention, is the development of generalization error bounds
for unrolled DNNs. These bounds deliver theoretical and practical insights into
the performance of a DNN on empirical datasets that are distinct from, but
sampled from, the probability density generating the DNN training data. In this
paper, we develop novel generalization error bounds for a class of unrolled
DNNs that are informed by a compound Gaussian prior. These compound Gaussian
networks have been shown to outperform comparative standard and unfolded deep
neural networks in compressive sensing and tomographic imaging problems. The
generalization error bound is formulated by bounding the Rademacher complexity
of the class of compound Gaussian network estimates with Dudley's integral.
Under realistic conditions, we show that, at worst, the generalization error
scales $\mathcal{O}(n\sqrt{\ln(n)})$ in the signal dimension and
$\mathcal{O}(($Network Size$)^{3/2})$ in network size.
\\ ( https://arxiv.org/abs/2402.13106 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13181 (*cross-listing*)
Date: Tue, 20 Feb 2024 17:48:11 GMT   (8698kb,D)

Title: DINOBot: Robot Manipulation via Retrieval and Alignment with Vision
  Foundation Models
Authors: Norman Di Palo and Edward Johns
Categories: cs.RO cs.LG
Comments: To appear at 2024 IEEE International Conference on Robotics and
  Automation (ICRA)
\\
  We propose DINOBot, a novel imitation learning framework for robot
manipulation, which leverages the image-level and pixel-level capabilities of
features extracted from Vision Transformers trained with DINO. When interacting
with a novel object, DINOBot first uses these features to retrieve the most
visually similar object experienced during human demonstrations, and then uses
this object to align its end-effector with the novel object to enable effective
interaction. Through a series of real-world experiments on everyday tasks, we
show that exploiting both the image-level and pixel-level properties of vision
foundation models enables unprecedented learning efficiency and generalisation.
Videos and code are available at https://www.robot-learning.uk/dinobot.
\\ ( https://arxiv.org/abs/2402.13181 ,  8698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13204 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:15:11 GMT   (1346kb,D)

Title: SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural
  Architecture Search
Authors: Halima Bouzidi, Smail Niar, Hamza Ouarnoughi, El-Ghazali Talbi
Categories: cs.NE cs.LG
Comments: 13 pages, 9 figures
\\
  Recent advancements in Artificial Intelligence (AI), driven by Neural
Networks (NN), demand innovative neural architecture designs, particularly
within the constrained environments of Internet of Things (IoT) systems, to
balance performance and efficiency. HW-aware Neural Architecture Search
(HW-aware NAS) emerges as an attractive strategy to automate the design of NN
using multi-objective optimization approaches, such as evolutionary algorithms.
However, the intricate relationship between NN design parameters and HW-aware
NAS optimization objectives remains an underexplored research area, overlooking
opportunities to effectively leverage this knowledge to guide the search
process accordingly. Furthermore, the large amount of evaluation data produced
during the search holds untapped potential for refining the optimization
strategy and improving the approximation of the Pareto front. Addressing these
issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware
NAS. Our method leverages adaptive evolutionary operators guided by the learned
importance of NN design parameters. Specifically, through tree-based surrogate
models and a Reinforcement Learning agent, we aspire to gather knowledge on
'How' and 'When' to evolve NN architectures. Comprehensive evaluations across
various NAS search spaces and hardware devices on the ImageNet-1k dataset have
shown the merit of SONATA with up to 0.25% improvement in accuracy and up to
2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Pareto
dominance over the native NSGA-II, further stipulating the importance of
self-adaptive evolution operators in HW-aware NAS.
\\ ( https://arxiv.org/abs/2402.13204 ,  1346kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13251 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:59:00 GMT   (19411kb,D)

Title: FlashTex: Fast Relightable Mesh Texturing with LightControlNet
Authors: Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan
  Zhu, Tinghui Zhou, Maneesh Agrawala
Categories: cs.GR cs.CV cs.LG
Comments: Project page: https://flashtex.github.io/
\\
  Manually creating textures for 3D meshes is time-consuming, even for expert
visual content creators. We propose a fast approach for automatically texturing
an input 3D mesh based on a user-provided text prompt. Importantly, our
approach disentangles lighting from surface material/reflectance in the
resulting texture so that the mesh can be properly relit and rendered in any
lighting environment. We introduce LightControlNet, a new text-to-image model
based on the ControlNet architecture, which allows the specification of the
desired lighting as a conditioning image to the model. Our text-to-texture
pipeline then constructs the texture in two stages. The first stage produces a
sparse set of visually consistent reference views of the mesh using
LightControlNet. The second stage applies a texture optimization based on Score
Distillation Sampling (SDS) that works with LightControlNet to increase the
texture quality while disentangling surface material from lighting. Our
pipeline is significantly faster than previous text-to-texture methods, while
producing high-quality and relightable textures.
\\ ( https://arxiv.org/abs/2402.13251 ,  19411kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2303.08485
replaced with revised version Tue, 20 Feb 2024 17:36:11 GMT   (307kb,D)

Title: Can Fairness be Automated? Guidelines and Opportunities for
  Fairness-aware AutoML
Authors: Hilde Weerts, Florian Pfisterer, Matthias Feurer, Katharina
  Eggensperger, Edward Bergman, Noor Awad, Joaquin Vanschoren, Mykola
  Pechenizkiy, Bernd Bischl, Frank Hutter
Categories: cs.AI cs.CY
Journal-ref: Journal of Artificial Intelligence Research 79 (2024) 639-677
DOI: 10.1613/jair.1.14747
\\ ( https://arxiv.org/abs/2303.08485 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04802
replaced with revised version Mon, 19 Feb 2024 23:40:58 GMT   (582kb,D)

Title: A Review on Knowledge Graphs for Healthcare: Resources, Applications,
  and Promises
Authors: Hejie Cui, Jiaying Lu, Shiyu Wang, Ran Xu, Wenjing Ma, Shaojun Yu, Yue
  Yu, Xuan Kan, Chen Ling, Tianfan Fu, Liang Zhao, Joyce Ho, Fei Wang, Carl
  Yang
Categories: cs.AI cs.CL cs.LG cs.SI
\\ ( https://arxiv.org/abs/2306.04802 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06770
replaced with revised version Tue, 20 Feb 2024 14:34:14 GMT   (6360kb,D)

Title: Improving Knowledge Extraction from LLMs for Task Learning through Agent
  Analysis
Authors: James R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird
Categories: cs.AI cs.HC cs.RO
Comments: 7 pages, 8 figures, 3 tables, bibliography, appendix (34 pages
  total). Accepted to AAAI 2024
ACM-class: I.2.6; I.2.7
\\ ( https://arxiv.org/abs/2306.06770 ,  6360kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04918
replaced with revised version Tue, 20 Feb 2024 08:29:13 GMT   (341kb,D)

Title: SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning
Authors: Lei You and Hei Victor Cheng
Categories: cs.AI
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.04918 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08949
replaced with revised version Tue, 20 Feb 2024 06:54:50 GMT   (4024kb,D)

Title: EasyGen: Easing Multimodal Generation with a Bidirectional Conditional
  Diffusion Model and LLMs
Authors: Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi, Xiao-Ming Wu
Categories: cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2310.08949 ,  4024kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09754
replaced with revised version Tue, 20 Feb 2024 06:39:44 GMT   (4077kb,D)

Title: EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification
Authors: Huanhuan Ma and Weizhi Xu and Yifan Wei and Liuji Chen and Liang Wang
  and Qiang Liu and Shu Wu and Liang Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2310.09754 ,  4077kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18940
replaced with revised version Tue, 20 Feb 2024 01:21:23 GMT   (482kb,D)

Title: Language Agents with Reinforcement Learning for Strategic Play in the
  Werewolf Game
Authors: Zelai Xu, Chao Yu, Fei Fang, Yu Wang, Yi Wu
Categories: cs.AI cs.LG cs.MA
\\ ( https://arxiv.org/abs/2310.18940 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06717
replaced with revised version Tue, 20 Feb 2024 18:26:08 GMT   (6349kb,D)

Title: Privacy Issues in Large Language Models: A Survey
Authors: Seth Neel and Peter Chang
Categories: cs.AI
Comments: February 2023 Update
\\ ( https://arxiv.org/abs/2312.06717 ,  6349kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15006
replaced with revised version Tue, 20 Feb 2024 18:44:20 GMT   (4172kb,D)

Title: Assessing the Impact of Prompting Methods on ChatGPT's Mathematical
  Capabilities
Authors: Yuhao Chen, Chloe Wong, Hanwen Yang, Juan Aguenza, Sai Bhujangari,
  Benthan Vu, Xun Lei, Amisha Prasad, Manny Fluss, Eric Phuong, Minghao Liu,
  Raja Kumar, Vanshika Vats, James Davis
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2312.15006 ,  4172kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04812
replaced with revised version Tue, 20 Feb 2024 00:18:16 GMT   (2514kb,D)

Title: Sample-and-Bound for Non-Convex Optimization
Authors: Yaoguang Zhai, Zhizhen Qin, Sicun Gao
Categories: cs.AI
Comments: Published at AAAI 2024. Code is available at
  https://github.com/aaucsd/MCIR
\\ ( https://arxiv.org/abs/2401.04812 ,  2514kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08189
replaced with revised version Tue, 20 Feb 2024 14:26:06 GMT   (226kb,D)

Title: PRewrite: Prompt Rewriting with Reinforcement Learning
Authors: Weize Kong and Spurthi Amba Hombaiah and Mingyang Zhang and Qiaozhu
  Mei and Michael Bendersky
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.08189 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09491
replaced with revised version Mon, 19 Feb 2024 21:01:23 GMT   (3240kb)

Title: Memory, Space, and Planning: Multiscale Predictive Representations
Authors: Ida Momennejad
Categories: cs.AI
Comments: To be published as a chapter in an edited volume by Oxford University
  Press (Editors: Sara Aronowitz and Lynn Nadel)
\\ ( https://arxiv.org/abs/2401.09491 ,  3240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01607
replaced with revised version Tue, 20 Feb 2024 12:14:06 GMT   (25518kb,D)

Title: Natural Counterfactuals With Necessary Backtracking
Authors: Guang-Yuan Hao, Jiji Zhang, Biwei Huang, Hao Wang, Kun Zhang
Categories: cs.AI cs.CV cs.LG cs.NE stat.ME
\\ ( https://arxiv.org/abs/2402.01607 ,  25518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07140
replaced with revised version Tue, 20 Feb 2024 03:13:55 GMT   (449kb,D)

Title: Graph Descriptive Order Improves Reasoning with Large Language Model
Authors: Yuyao Ge, Shenghua Liu, Wenjie Feng, Lingrui Mei, Lizhe Chen, Xueqi
  Cheng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07140 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07197
replaced with revised version Tue, 20 Feb 2024 08:34:15 GMT   (784kb,D)

Title: GraphTranslator: Aligning Graph Model to Large Language Model for
  Open-ended Tasks
Authors: Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao
  Xu, Hong Liu, Cheng Yang, Chuan Shi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07197 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07422
replaced with revised version Tue, 20 Feb 2024 02:46:17 GMT   (461kb)

Title: News Recommendation with Attention Mechanism
Authors: Tianrui Liu, Changxin Xu, Yuxin Qiao, Chufeng Jiang, Weisheng Chen
Categories: cs.AI
Comments: 7 pages, Journal of Industrial Engineering and Applied Science
Journal-ref: Journal of Industrial Engineering and Applied Science 2024
DOI: 10.5281/zenodo.10635481
\\ ( https://arxiv.org/abs/2402.07422 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07429
replaced with revised version Tue, 20 Feb 2024 02:42:33 GMT   (350kb)

Title: Particle Filter SLAM for Vehicle Localization
Authors: Tianrui Liu, Changxin Xu, Yuxin Qiao, Chufeng Jiang, Jiqiang Yu
Categories: cs.AI
Comments: 6 pages, Journal of Industrial Engineering and Applied Science
Journal-ref: Journal of Industrial Engineering and Applied Science 2024
DOI: 10.5281/zenodo.10635489
\\ ( https://arxiv.org/abs/2402.07429 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10011
replaced with revised version Tue, 20 Feb 2024 17:12:49 GMT   (335kb,D)

Title: Clifford Group Equivariant Simplicial Message Passing Networks
Authors: Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forr\'e
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.10011 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:1808.10024
replaced with revised version Tue, 20 Feb 2024 15:36:05 GMT   (103kb,D)

Title: Hard Non-Monotonic Attention for Character-Level Transduction
Authors: Shijie Wu, Pamela Shapiro, Ryan Cotterell
Categories: cs.CL
Comments: Published in EMNLP 2018
\\ ( https://arxiv.org/abs/1808.10024 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:1905.06319
replaced with revised version Tue, 20 Feb 2024 15:41:14 GMT   (75kb,D)

Title: Exact Hard Monotonic Attention for Character-Level Transduction
Authors: Shijie Wu and Ryan Cotterell
Categories: cs.CL
Comments: ACL 2019
\\ ( https://arxiv.org/abs/1905.06319 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02541
replaced with revised version Tue, 20 Feb 2024 12:13:51 GMT   (686kb,D)

Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
Authors: Vil\'em Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson,
  Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2304.02541 ,  686kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12990
replaced with revised version Tue, 20 Feb 2024 14:23:58 GMT   (193kb,D)

Title: Sentence Representations via Gaussian Embedding
Authors: Shohei Yoda, Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda
Categories: cs.CL
Comments: Accepted to EACL 2024 (Main)
\\ ( https://arxiv.org/abs/2305.12990 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14456
replaced with revised version Mon, 19 Feb 2024 23:10:40 GMT   (12347kb,D)

Title: Having Beer after Prayer? Measuring Cultural Bias in Large Language
  Models
Authors: Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2305.14456 ,  12347kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15405
replaced with revised version Tue, 20 Feb 2024 18:55:52 GMT   (625kb,D)

Title: Textless Low-Resource Speech-to-Speech Translation With Unit Language
  Models
Authors: Anuj Diwan, Anirudh Srinivasan, David Harwath, Eunsol Choi
Categories: cs.CL eess.AS
Comments: 20 pages, 4 figures
\\ ( https://arxiv.org/abs/2305.15405 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13891
replaced with revised version Tue, 20 Feb 2024 06:37:12 GMT   (12744kb,D)

Title: Estimating the Causal Effect of Early ArXiving on Paper Acceptance
Authors: Yanai Elazar, Jiayao Zhang, David Wadden, Bo Zhang, Noah A. Smith
Categories: cs.CL
Comments: Published at CLeaR 2024
\\ ( https://arxiv.org/abs/2306.13891 ,  12744kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07047
replaced with revised version Tue, 20 Feb 2024 06:12:39 GMT   (7588kb,D)

Title: Does Collaborative Human-LM Dialogue Generation Help Information
  Extraction from Human Dialogues?
Authors: Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng, Paul
  Koester, Jean Utke, Tao Yu, Noah A. Smith, Mari Ostendorf
Categories: cs.CL
\\ ( https://arxiv.org/abs/2307.07047 ,  7588kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16200
replaced with revised version Tue, 20 Feb 2024 03:24:16 GMT   (392kb,D)

Title: A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue
  Information Extraction
Authors: Zefa Hu, Ziyi Ni, Jing Shi, Shuang Xu, Bo Xu
Categories: cs.CL
Comments: Published in Machine Intelligence Research,
  https://link.springer.com/article/10.1007/s11633-023-1461-5
DOI: 10.1007/s11633-023-1461-5
\\ ( https://arxiv.org/abs/2307.16200 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08742
replaced with revised version Tue, 20 Feb 2024 10:48:37 GMT   (336kb,D)

Title: PMET: Precise Model Editing in a Transformer
Authors: Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu
Categories: cs.CL cs.AI cs.LG
Comments: AAAI24
\\ ( https://arxiv.org/abs/2308.08742 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12032
replaced with revised version Tue, 20 Feb 2024 02:26:47 GMT   (11605kb,D)

Title: From Quantity to Quality: Boosting LLM Performance with Self-Guided Data
  Selection for Instruction Tuning
Authors: Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng,
  Jianzong Wang, Tianyi Zhou, Jing Xiao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.12032 ,  11605kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07251
replaced with revised version Tue, 20 Feb 2024 15:11:17 GMT   (284kb,D)

Title: In-Contextual Gender Bias Suppression for Large Language Models
Authors: Daisuke Oba, Masahiro Kaneko, Danushka Bollegala
Categories: cs.CL
Comments: EACL 2024 Findings - Long Paper
\\ ( https://arxiv.org/abs/2309.07251 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10926
replaced with revised version Tue, 20 Feb 2024 13:06:16 GMT   (237kb,D)

Title: Semi-Autoregressive Streaming ASR With Label Context
Authors: Siddhant Arora, George Saon, Shinji Watanabe, Brian Kingsbury
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2309.10926 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17415
replaced with revised version Tue, 20 Feb 2024 05:59:41 GMT   (9618kb,D)

Title: Intuitive or Dependent? Investigating LLMs' Behavior Style to
  Conflicting Prompts
Authors: Jiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long Cui, Yongbin Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.17415 ,  9618kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04270
replaced with revised version Mon, 19 Feb 2024 22:58:39 GMT   (7666kb,D)

Title: A Comprehensive Evaluation of Large Language Models on Benchmark
  Biomedical Text Processing Tasks
Authors: Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang
Categories: cs.CL cs.AI
Comments: Accepted to the Computers in Biology and Medicine journal
\\ ( https://arxiv.org/abs/2310.04270 ,  7666kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06692
replaced with revised version Tue, 20 Feb 2024 15:27:20 GMT   (6788kb,D)

Title: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with
  Large Language Models
Authors: Anni Zou, Zhuosheng Zhang, Hai Zhao, Xiangru Tang
Categories: cs.CL cs.AI
Comments: 17 pages, 12 figures
\\ ( https://arxiv.org/abs/2310.06692 ,  6788kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10570
replaced with revised version Tue, 20 Feb 2024 05:14:44 GMT   (4543kb,D)

Title: On Context Utilization in Summarization with Large Language Models
Authors: Mathieu Ravaut, Aixin Sun, Nancy F. Chen, Shafiq Joty
Categories: cs.CL
Comments: Preprint. Under review
\\ ( https://arxiv.org/abs/2310.10570 ,  4543kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15724
replaced with revised version Tue, 20 Feb 2024 07:54:23 GMT   (1757kb,D)

Title: Variator: Accelerating Pre-trained Models with Plug-and-Play Compression
  Modules
Authors: Chaojun Xiao, Yuqi Luo, Wenbin Zhang, Pengle Zhang, Xu Han, Yankai
  Lin, Zhengyan Zhang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou
Categories: cs.CL
Comments: Accepted by Findings of EMNLP
\\ ( https://arxiv.org/abs/2310.15724 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19531
replaced with revised version Tue, 20 Feb 2024 02:42:05 GMT   (8464kb,D)

Title: MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties
  in Generative Language Models
Authors: Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei
  Zhou, Songlin Hu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.19531 ,  8464kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05661
replaced with revised version Mon, 19 Feb 2024 19:46:05 GMT   (8208kb,D)

Title: Prompt Engineering a Prompt Engineer
Authors: Qinyuan Ye, Maxamed Axmed, Reid Pryzant, Fereshte Khani
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.05661 ,  8208kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06555
replaced with revised version Tue, 20 Feb 2024 03:51:40 GMT   (2289kb,D)

Title: Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language
  Models for Document-Level Event Argument Extraction
Authors: Hanzhang Zhou, Junlang Qian, Zijian Feng, Hui Lu, Zixiao Zhu, Kezhi
  Mao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.06555 ,  2289kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09538
replaced with revised version Tue, 20 Feb 2024 01:04:04 GMT   (8474kb,D)

Title: Reducing Privacy Risks in Online Self-Disclosures with Language Models
Authors: Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan
  Ritter, Wei Xu
Categories: cs.CL cs.HC
Comments: LLMs, Privacy, HCI
\\ ( https://arxiv.org/abs/2311.09538 ,  8474kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09766
replaced with revised version Tue, 20 Feb 2024 17:21:51 GMT   (3739kb,D)

Title: LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores
Authors: Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09766 ,  3739kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10537
replaced with revised version Tue, 20 Feb 2024 06:12:14 GMT   (2347kb,D)

Title: MedAgents: Large Language Models as Collaborators for Zero-shot Medical
  Reasoning
Authors: Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao,
  Xingyao Zhang, Arman Cohan, Mark Gerstein
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.10537 ,  2347kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12882
replaced with revised version Mon, 19 Feb 2024 21:32:04 GMT   (315kb)

Title: Overview of Current Applications of Large Language Models in Various
  Medical Specialities
Authors: Ummara Mumtaz, Awais Ahmed, Summaya Mumtaz
Categories: cs.CL
Comments: 26 pages and one figure
\\ ( https://arxiv.org/abs/2311.12882 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02073
replaced with revised version Tue, 20 Feb 2024 17:27:17 GMT   (7816kb,D)

Title: A Glitch in the Matrix? Locating and Detecting Language Model Grounding
  with Fakepedia
Authors: Giovanni Monea, Maxime Peyrard, Martin Josifoski, Vishrav Chaudhary,
  Jason Eisner, Emre K{\i}c{\i}man, Hamid Palangi, Barun Patra, Robert West
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.02073 ,  7816kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04620
replaced with revised version Tue, 20 Feb 2024 03:24:55 GMT   (2287kb,D)

Title: Agent Alignment in Evolving Social Norms
Authors: Shimin Li, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.04620 ,  2287kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06659
replaced with revised version Tue, 20 Feb 2024 09:28:50 GMT   (8201kb,D)

Title: WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual
  World Knowledge
Authors: Wenbin Wang, Liang Ding, Li Shen, Yong Luo, Han Hu, Dacheng Tao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06659 ,  8201kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06853
replaced with revised version Tue, 20 Feb 2024 00:14:31 GMT   (7186kb,D)

Title: Large Language Models Can Learn Temporal Reasoning
Authors: Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06853 ,  7186kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07128
replaced with revised version Mon, 19 Feb 2024 21:47:41 GMT   (9069kb,D)

Title: EHRAgent: Code Empowers Large Language Models for Few-shot Complex
  Tabular Reasoning on Electronic Health Records
Authors: Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda
  Zhu, Joyce Ho, Carl Yang, May D. Wang
Categories: cs.CL cs.AI
Comments: Work in Progress
\\ ( https://arxiv.org/abs/2401.07128 ,  9069kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07453
replaced with revised version Tue, 20 Feb 2024 06:24:03 GMT   (16734kb,D)

Title: Model Editing at Scale leads to Gradual and Catastrophic Forgetting
Authors: Akshat Gupta, Anurag Rao, Gopala Anumanchipalli
Categories: cs.CL cs.AI cs.IR
Comments: Added experiments with GPT-J and appendix for all samples and
  ablations
\\ ( https://arxiv.org/abs/2401.07453 ,  16734kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07598
replaced with revised version Tue, 20 Feb 2024 08:06:39 GMT   (1576kb,D)

Title: MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of
  Large Language Models
Authors: Divyanshu Aggarwal, Ashutosh Sathe, Ishaan Watts and Sunayana Sitaram
Categories: cs.CL
Comments: 46 pages, 23 figures, 45 tables
\\ ( https://arxiv.org/abs/2401.07598 ,  1576kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07851
replaced with revised version Tue, 20 Feb 2024 10:24:57 GMT   (1345kb,D)

Title: Unlocking Efficiency in Large Language Model Inference: A Comprehensive
  Survey of Speculative Decoding
Authors: Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge,
  Tianyu Liu, Wenjie Li, Zhifang Sui
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.07851 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12585
replaced with revised version Tue, 20 Feb 2024 08:54:50 GMT   (9025kb,D)

Title: SLANG: New Concept Comprehension of Large Language Models
Authors: Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Cheng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.12585 ,  9025kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13129
replaced with revised version Tue, 20 Feb 2024 18:50:46 GMT   (1698kb,D)

Title: Seed-Guided Fine-Grained Entity Typing in Science and Engineering
  Domains
Authors: Yu Zhang, Yunyi Zhang, Yanzhen Shen, Yu Deng, Lucian Popa, Larisa
  Shwartz, ChengXiang Zhai, Jiawei Han
Categories: cs.CL cs.SE
Comments: 9 pages; Accepted to AAAI 2024 (Code:
  https://github.com/yuzhimanhua/SEType)
\\ ( https://arxiv.org/abs/2401.13129 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13227
replaced with revised version Tue, 20 Feb 2024 03:53:06 GMT   (7716kb,D)

Title: LPNL: Scalable Link Prediction with Large Language Models
Authors: Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei and Xueqi Cheng
Categories: cs.CL cs.AI cs.LG cs.SI
\\ ( https://arxiv.org/abs/2401.13227 ,  7716kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13601
replaced with revised version Tue, 20 Feb 2024 09:51:37 GMT   (2432kb,D)

Title: MM-LLMs: Recent Advances in MultiModal Large Language Models
Authors: Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu,
  Dong Yu
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.13601 ,  2432kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16348
replaced with revised version Tue, 20 Feb 2024 03:10:58 GMT   (7390kb,D)

Title: Improving the TENOR of Labeling: Re-evaluating Topic Models for Content
  Analysis
Authors: Zongxia Li, Andrew Mao, Daniel Stephens, Pranav Goel, Emily Walpole,
  Alden Dima, Juan Fung, Jordan Boyd-Graber
Categories: cs.CL cs.CY cs.HC
Comments: 19 pages, 5 tables, 6 figures, Accepted to EACL Main Conference 2024
\\ ( https://arxiv.org/abs/2401.16348 ,  7390kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16553
replaced with revised version Tue, 20 Feb 2024 07:58:23 GMT   (10172kb,D)

Title: SelectLLM: Can LLMs Select Important Instructions to Annotate?
Authors: Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang
Categories: cs.CL cs.AI
Comments: First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:
  Jong Inn Park | PI: Dongyeop Kang
\\ ( https://arxiv.org/abs/2401.16553 ,  10172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01697
replaced with revised version Tue, 20 Feb 2024 07:54:12 GMT   (288kb,D)

Title: APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT
Authors: Yiming Zhu, Zhizhuo Yin, Gareth Tyson, Ehsan-Ul Haq, Lik-Hang Lee, Pan
  Hui
Categories: cs.CL
Comments: Accepted by WWW 2024; Camera-ready version
DOI: 10.1145/3589334.3645642
\\ ( https://arxiv.org/abs/2402.01697 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03173
replaced with revised version Tue, 20 Feb 2024 07:55:52 GMT   (10670kb,D)

Title: MULTI: Multimodal Understanding Leaderboard with Text and Images
Authors: Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun,
  Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Situo Zhang, Zihan Zhao,
  Liangtai Sun, Kai Yu
Categories: cs.CL cs.AI cs.CV
Comments: 16 pages, 9 figures, 10 tables. Details and access are available at:
  https://OpenDFM.github.io/MULTI-Benchmark/
\\ ( https://arxiv.org/abs/2402.03173 ,  10670kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03190
replaced with revised version Tue, 20 Feb 2024 16:47:16 GMT   (6240kb,D)

Title: Unified Hallucination Detection for Multimodal Large Language Models
Authors: Xiang Chen and Chenxi Wang and Yida Xue and Ningyu Zhang and Xiaoyan
  Yang and Qiang Li and Yue Shen and Lei Liang and Jinjie Gu and Huajun Chen
Categories: cs.CL cs.AI cs.IR cs.LG cs.MM
Comments: Work in progress
\\ ( https://arxiv.org/abs/2402.03190 ,  6240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04333
replaced with revised version Tue, 20 Feb 2024 02:24:09 GMT   (1784kb,D)

Title: LESS: Selecting Influential Data for Targeted Instruction Tuning
Authors: Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi
  Chen
Categories: cs.CL cs.AI cs.LG
Comments: Code and data are available at https://github.com/princeton-nlp/LESS
\\ ( https://arxiv.org/abs/2402.04333 ,  1784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06196
replaced with revised version Tue, 20 Feb 2024 13:33:49 GMT   (7243kb,D)

Title: Large Language Models: A Survey
Authors: Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
  Richard Socher, Xavier Amatriain, Jianfeng Gao
Categories: cs.CL cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2401.14423
\\ ( https://arxiv.org/abs/2402.06196 ,  7243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07859
replaced with revised version Tue, 20 Feb 2024 15:12:13 GMT   (2272kb,D)

Title: Lissard: Long and Simple Sequential Reasoning Datasets
Authors: Mirelle Bueno, Roberto Lotufo, and Rodrigo Nogueira
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.07859 ,  2272kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08303
replaced with revised version Tue, 20 Feb 2024 02:26:39 GMT   (0kb,I)

Title: ChatCell: Facilitating Single-Cell Analysis with Natural Language
Authors: Yin Fang, Kangwei Liu, Ningyu Zhang, Xinle Deng, Penghui Yang, Zhuo
  Chen, Xiangru Tang, Mark Gerstein, Xiaohui Fan, Huajun Chen
Categories: cs.CL cs.AI cs.CE cs.HC cs.LG
Comments: I have decided to temporarily withdraw this draft as I am in the
  process of making further revisions to improve its content. Code:
  https://github.com/zjunlp/ChatCell Dataset:
  https://huggingface.co/datasets/zjunlp/ChatCell-Instructions Demo:
  https://chat.openai.com/g/g-vUwj222gQ-chatcell
\\ ( https://arxiv.org/abs/2402.08303 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08496
replaced with revised version Tue, 20 Feb 2024 07:26:21 GMT   (959kb,D)

Title: A Systematic Review of Data-to-Text NLG
Authors: Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.08496 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10311
replaced with revised version Tue, 20 Feb 2024 12:50:49 GMT   (51kb,D)

Title: The optimal placement of the head in the noun phrase. The case of
  demonstrative, numeral, adjective and noun
Authors: Ramon Ferrer-i-Cancho
Categories: cs.CL physics.soc-ph
\\ ( https://arxiv.org/abs/2402.10311 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10669
replaced with revised version Tue, 20 Feb 2024 17:00:15 GMT   (8624kb,D)

Title: Humans or LLMs as the Judge? A Study on Judgement Biases
Authors: Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang
Categories: cs.CL
Comments: 19 pages
\\ ( https://arxiv.org/abs/2402.10669 ,  8624kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10949
replaced with revised version Tue, 20 Feb 2024 15:03:00 GMT   (66kb)

Title: The Unreasonable Effectiveness of Eccentric Automatic Prompts
Authors: Rick Battle and Teja Gollapudi
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.10949 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11512
replaced with revised version Tue, 20 Feb 2024 06:57:34 GMT   (8221kb,D)

Title: From Prejudice to Parity: A New Approach to Debiasing Large Language
  Model Word Embeddings
Authors: Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury,
  Vinija Jain, Aman Chadha
Categories: cs.CL cs.CY
\\ ( https://arxiv.org/abs/2402.11512 ,  8221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11756
replaced with revised version Tue, 20 Feb 2024 02:12:09 GMT   (9830kb,D)

Title: MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in
  Generative LLMs
Authors: Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang
  Tao, Dimitrios Dimitriadis, Salman Avestimehr
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.11756 ,  9830kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11770
replaced with revised version Tue, 20 Feb 2024 02:55:57 GMT   (183kb,D)

Title: Structured Chain-of-Thought Prompting for Few-Shot Generation of
  Content-Grounded QA Conversations
Authors: Md Arafat Sultan and Jatin Ganhotra and Ram\'on Fernandez Astudillo
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.11770 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12030
replaced with revised version Tue, 20 Feb 2024 14:46:03 GMT   (812kb,D)

Title: Towards Cross-Tokenizer Distillation: the Universal Logit Distillation
  Loss for LLMs
Authors: Nicolas Boizard, Kevin El Haddad, C\'eline Hudelot, Pierre Colombo
Categories: cs.CL
Comments: 9 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.12030 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12243
replaced with revised version Tue, 20 Feb 2024 07:07:49 GMT   (705kb,D)

Title: Understanding the Effects of Noise in Text-to-SQL: An Examination of the
  BIRD-Bench Benchmark
Authors: Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi,
  Oskar Holmstr\"om
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12243 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12372
replaced with revised version Tue, 20 Feb 2024 13:10:27 GMT   (4671kb,D)

Title: HunFlair2 in a cross-corpus evaluation of biomedical named entity
  recognition and normalization tools
Authors: Mario S\"anger, Samuele Garda, Xing David Wang, Leon Weber-Genzel, Pia
  Droop, Benedikt Fuchs, Alan Akbik, Ulf Leser
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12372 ,  4671kb)
------------------------------------------------------------------------------
\\
arXiv:2106.03907
replaced with revised version Mon, 19 Feb 2024 23:35:03 GMT   (147kb,D)

Title: Deep Proxy Causal Learning and its Application to Confounded Bandit
  Policy Evaluation
Authors: Liyuan Xu, Heishiro Kanagawa, Arthur Gretton
Categories: cs.LG stat.ML
Comments: arXiv admin note: text overlap with arXiv:2010.07154
\\ ( https://arxiv.org/abs/2106.03907 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2205.12944
replaced with revised version Tue, 20 Feb 2024 18:56:29 GMT   (893kb,D)

Title: Learning in Mean Field Games: A Survey
Authors: Mathieu Lauri\`ere, Sarah Perrin, Julien P\'erolat, Sertan Girgin,
  Paul Muller, Romuald \'Elie, Matthieu Geist, Olivier Pietquin
Categories: cs.LG cs.AI cs.GT math.OC
\\ ( https://arxiv.org/abs/2205.12944 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2206.02911
replaced with revised version Tue, 20 Feb 2024 14:21:30 GMT   (4530kb,D)

Title: Inverse Boundary Value and Optimal Control Problems on Graphs: A Neural
  and Numerical Synthesis
Authors: Mehdi Garrousian and Amirhossein Nouranizadeh
Categories: cs.LG cs.NA math.NA
MSC-class: 62M45, 68R10, 49M37, 37N35
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2206.02911 ,  4530kb)
------------------------------------------------------------------------------
\\
arXiv:2206.10477
replaced with revised version Mon, 19 Feb 2024 23:36:40 GMT   (973kb,D)

Title: Survival Kernets: Scalable and Interpretable Deep Kernel Survival
  Analysis with an Accuracy Guarantee
Authors: George H. Chen
Categories: cs.LG stat.ML
Comments: Accepted at the Journal of Machine Learning Research; compared to the
  previous arXiv version, this draft includes some minor clarifications/edits
\\ ( https://arxiv.org/abs/2206.10477 ,  973kb)
------------------------------------------------------------------------------
\\
arXiv:2209.05550
replaced with revised version Tue, 20 Feb 2024 08:37:21 GMT   (63kb)

Title: Mathematical Framework for Online Social Media Auditing
Authors: Wasim Huleihel and Yehonathan Refael
Categories: cs.LG cs.SI stat.CO stat.ML
\\ ( https://arxiv.org/abs/2209.05550 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14221
replaced with revised version Mon, 19 Feb 2024 21:05:31 GMT   (223kb,D)

Title: Learning Large Causal Structures from Inverse Covariance Matrix via
  Sparse Matrix Decomposition
Authors: Shuyu Dong, Kento Uemura, Akito Fujii, Shuang Chang, Yusuke Koyanagi,
  Koji Maruhashi, Mich\`ele Sebag
Categories: cs.LG math.OC stat.ME
\\ ( https://arxiv.org/abs/2211.14221 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2212.06251
replaced with revised version Mon, 19 Feb 2024 19:01:36 GMT   (838kb,D)

Title: Autoregressive Bandits
Authors: Francesco Bacchiocchi, Gianmarco Genalti, Davide Maran, Marco Mussi,
  Marcello Restelli, Nicola Gatti, Alberto Maria Metelli
Categories: cs.LG stat.ML
Comments: Accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2212.06251 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02688
replaced with revised version Tue, 20 Feb 2024 16:29:34 GMT   (738kb,D)

Title: Going Further: Flatness at the Rescue of Early Stopping for Adversarial
  Example Transferability
Authors: Martin Gubri, Maxime Cordy and Yves Le Traon
Categories: cs.LG cs.CV stat.ML
Comments: Version 2: originally submitted in April 2023 and revised in February
  2024
\\ ( https://arxiv.org/abs/2304.02688 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10978
replaced with revised version Tue, 20 Feb 2024 10:47:47 GMT   (4545kb,D)

Title: Client Selection for Federated Policy Optimization with Environment
  Heterogeneity
Authors: Zhijie Xie, S.H. Song
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.10978 ,  4545kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11463
replaced with revised version Tue, 20 Feb 2024 15:35:36 GMT   (9522kb,D)

Title: Generative Sliced MMD Flows with Riesz Kernels
Authors: Johannes Hertrich, Christian Wald, Fabian Altekr\"uger, Paul Hagemann
Categories: cs.LG math.PR stat.ML
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2305.11463 ,  9522kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16189
replaced with revised version Tue, 20 Feb 2024 04:18:18 GMT   (12951kb,D)

Title: Martian time-series unraveled: A multi-scale nested approach with
  factorial variational autoencoders
Authors: Ali Siahkoohi and Rudy Morel and Randall Balestriero and Erwan Allys
  and Gr\'egory Sainton and Taichi Kawamura and Maarten V. de Hoop
Categories: cs.LG astro-ph.EP stat.ML
\\ ( https://arxiv.org/abs/2305.16189 ,  12951kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17282
replaced with revised version Mon, 19 Feb 2024 22:48:24 GMT   (39kb,D)

Title: Universal consistency of the $k$-NN rule in metric spaces and Nagata
  dimension. II
Authors: Sushma Kumari and Vladimir G. Pestov
Categories: cs.LG
Comments: Latex 2e, 28 pages, 1 figure. The second revision as requested by the
  anonymous ESAIM:PS reviewer. The proof of Proposition 1.2 has been redone,
  Fig. 1 added, and a number of smaller changes and improvements made
MSC-class: 62H30, 54F45
\\ ( https://arxiv.org/abs/2305.17282 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17342
replaced with revised version Tue, 20 Feb 2024 16:05:36 GMT   (1917kb,D)

Title: Rethinking Adversarial Policies: A Generalized Attack Formulation and
  Provable Defense in RL
Authors: Xiangyu Liu, Souradip Chakraborty, Yanchao Sun, Furong Huang
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2305.17342 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19971
replaced with revised version Tue, 20 Feb 2024 03:38:31 GMT   (5060kb,D)

Title: Federated Learning in the Presence of Adversarial Client Unavailability
Authors: Lili Su, Ming Xiang, Jiaming Xu, Pengkun Yang
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2305.19971 ,  5060kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05108
replaced with revised version Tue, 20 Feb 2024 13:43:38 GMT   (1175kb,D)

Title: Hybrid Graph: A Unified Graph Representation with Datasets and
  Benchmarks for Complex Graphs
Authors: Zehui Li, Xiangyu Zhao, Mingzhu Shen, Guy-Bart Stan, Pietro Li\`o,
  Yiren Zhao
Categories: cs.LG cs.SI
Comments: 16 pages, 5 figures, 11 tables
\\ ( https://arxiv.org/abs/2306.05108 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03186
replaced with revised version Tue, 20 Feb 2024 04:12:37 GMT   (4824kb,D)

Title: TGRL: An Algorithm for Teacher Guided Reinforcement Learning
Authors: Idan Shenfeld, Zhang-Wei Hong, Aviv Tamar, Pulkit Agrawal
Categories: cs.LG
Journal-ref: ICML 2023
\\ ( https://arxiv.org/abs/2307.03186 ,  4824kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11106
replaced with revised version Mon, 19 Feb 2024 21:11:32 GMT   (483kb,D)

Title: The importance of feature preprocessing for differentially private
  linear optimization
Authors: Ziteng Sun, Ananda Theertha Suresh, Aditya Krishna Menon
Categories: cs.LG cs.CR cs.IT math.IT
\\ ( https://arxiv.org/abs/2307.11106 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01421
replaced with revised version Tue, 20 Feb 2024 09:27:05 GMT   (2651kb,D)

Title: Regularization, early-stopping and dreaming: a Hopfield-like setup to
  address generalization and overfitting
Authors: Elena Agliari, Francesco Alemanno, Miriam Aquaro, Alberto Fachechi
Categories: cs.LG cond-mat.dis-nn
Comments: 29 pages, 10 figures, 4 appendices
Report-no: Roma01.Math
\\ ( https://arxiv.org/abs/2308.01421 ,  2651kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09842
replaced with revised version Tue, 20 Feb 2024 17:35:48 GMT   (707kb,D)

Title: Enumerating Safe Regions in Deep Neural Networks with Provable
  Probabilistic Guarantees
Authors: Luca Marzari, Davide Corsi, Enrico Marchesini, Alessandro Farinelli
  and Ferdinando Cicalese
Categories: cs.LG cs.AI
Comments: Accepted at the 38th Annual AAAI Conference on Artificial
  Intelligence 2024
\\ ( https://arxiv.org/abs/2308.09842 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11978
replaced with revised version Tue, 20 Feb 2024 13:56:41 GMT   (1109kb,D)

Title: Will More Expressive Graph Neural Networks do Better on Generative
  Tasks?
Authors: Xiandong Zou, Xiangyu Zhao, Pietro Li\`o, Yiren Zhao
Categories: cs.LG cs.AI q-bio.BM stat.ML
Comments: 2nd Learning on Graphs Conference (LoG 2023). 26 pages, 5 figures, 11
  tables
\\ ( https://arxiv.org/abs/2308.11978 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04761
replaced with revised version Tue, 20 Feb 2024 01:38:11 GMT   (1023kb,D)

Title: A Comprehensive Survey on Deep Learning Techniques in Educational Data
  Mining
Authors: Yuanguo Lin, Hong Chen, Wei Xia, Fan Lin, Zongyue Wang, Yong Liu
Categories: cs.LG cs.CY cs.IR
Comments: 19 pages, 6 figures
\\ ( https://arxiv.org/abs/2309.04761 ,  1023kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09968
replaced with revised version Mon, 19 Feb 2024 21:48:33 GMT   (4988kb,D)

Title: Generating and Imputing Tabular Data via Diffusion and Flow-based
  Gradient-Boosted Trees
Authors: Alexia Jolicoeur-Martineau, Kilian Fatras, Tal Kachman
Categories: cs.LG
Comments: Code: https://github.com/SamsungSAILMontreal/ForestDiffusion
\\ ( https://arxiv.org/abs/2309.09968 ,  4988kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13207
replaced with revised version Mon, 19 Feb 2024 22:01:12 GMT   (4983kb,D)

Title: Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation
  for Earth System Science Applications
Authors: John S. Schreck, David John Gagne II, Charlie Becker, William E.
  Chapman, Kim Elmore, Da Fan, Gabrielle Gantos, Eliot Kim, Dhamma Kimpara,
  Thomas Martin, Maria J. Molina, Vanessa M. Pryzbylo, Jacob Radford, Belen
  Saavedra, Justin Willson, Christopher Wirz
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.13207 ,  4983kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00967
replaced with revised version Tue, 20 Feb 2024 10:37:11 GMT   (1010kb,D)

Title: MiCRO: Near-Zero Cost Gradient Sparsification for Scaling and
  Accelerating Distributed DNN Training
Authors: Daegun Yoon, Sangyoon Oh
Categories: cs.LG cs.DC
Comments: 30th IEEE International Conference on High Performance Computing,
  Data, and Analytics (HiPC 2023). Code: https://github.com/kljp/micro
\\ ( https://arxiv.org/abs/2310.00967 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01201
replaced with revised version Tue, 20 Feb 2024 16:10:29 GMT   (330kb,D)

Title: SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping
Authors: Hana Sebia, Thomas Guyet, Etienne Audureau
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.01201 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03013
replaced with revised version Tue, 20 Feb 2024 16:02:18 GMT   (919kb,D)

Title: SemiReward: A General Reward Model for Semi-supervised Learning
Authors: Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan,
  Stan Z. Li
Categories: cs.LG cs.AI
Comments: ICLR 2024 Camera Ready. Preprint V2 (25 pages) with the source code
  at https://github.com/Westlake-AI/SemiReward
\\ ( https://arxiv.org/abs/2310.03013 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05105
replaced with revised version Tue, 20 Feb 2024 07:10:59 GMT   (682kb,D)

Title: How Graph Neural Networks Learn: Lessons from Training Dynamics
Authors: Chenxiao Yang, Qitian Wu, David Wipf, Ruoyu Sun, Junchi Yan
Categories: cs.LG
Comments: ongoing work
\\ ( https://arxiv.org/abs/2310.05105 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05668
replaced with revised version Tue, 20 Feb 2024 15:33:02 GMT   (1815kb,D)

Title: LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised
  Anomaly Detection
Authors: Feiyi Chen, Zhen Qin, Yingying Zhang, Shuiguang Deng, Yi Xiao,
  Guansong Pang and Qingsong Wen
Categories: cs.LG
Comments: Accepted by ACM Web Conference 2024 (WWW 24)
\\ ( https://arxiv.org/abs/2310.05668 ,  1815kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13459
replaced with revised version Tue, 20 Feb 2024 15:31:24 GMT   (5349kb,D)

Title: Stable Nonconvex-Nonconcave Training via Linear Interpolation
Authors: Thomas Pethick, Wanyun Xie, Volkan Cevher
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2310.13459 ,  5349kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18212
replaced with revised version Tue, 20 Feb 2024 11:09:19 GMT   (1747kb,D)

Title: Robustness of Algorithms for Causal Structure Learning to Hyperparameter
  Choice
Authors: Damian Machlanski, Spyridon Samothrakis, Paul Clarke
Categories: cs.LG stat.ME
Comments: To appear in the 3rd Conference on Causal Learning and Reasoning
  (CLeaR 2024)
\\ ( https://arxiv.org/abs/2310.18212 ,  1747kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19906
replaced with revised version Tue, 20 Feb 2024 13:57:58 GMT   (4486kb,D)

Title: Interpretable Prototype-based Graph Information Bottleneck
Authors: Sangwoo Seo, Sungwon Kim, Chanyoung Park
Categories: cs.LG cs.AI
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.19906 ,  4486kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20187
replaced with revised version Tue, 20 Feb 2024 01:45:22 GMT   (2292kb,D)

Title: Self-Supervised Pre-Training for Precipitation Post-Processor
Authors: Sojung An, Junha Lee, Jiyeon Jang, Inchae Na, Wooyeon Park, Sujeong
  You
Categories: cs.LG cs.AI
Comments: 7 pages, 3 figures, 1 table, accepted to NeurIPS 2023 Workshop on
  Tackling Climate Change with Machine Learning at [this http
  URL](https://www.climatechange.ai/papers/neurips2023/18)
\\ ( https://arxiv.org/abs/2310.20187 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01139
replaced with revised version Tue, 20 Feb 2024 07:39:16 GMT   (9128kb,D)

Title: Add and Thin: Diffusion for Temporal Point Processes
Authors: David L\"udke, Marin Bilo\v{s}, Oleksandr Shchur, Marten Lienen,
  Stephan G\"unnemann
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.01139 ,  9128kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13845
replaced with revised version Tue, 20 Feb 2024 18:17:40 GMT   (1374kb,D)

Title: Touring sampling with pushforward maps
Authors: Vivien Cabannes, Charles Arnal
Categories: cs.LG cs.AI stat.ML
Comments: 5 pages
Journal-ref: ICASSP, 2024
\\ ( https://arxiv.org/abs/2311.13845 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07331
replaced with revised version Tue, 20 Feb 2024 07:30:25 GMT   (180kb,D)

Title: Coupled Confusion Correction: Learning from Crowds with Sparse
  Annotations
Authors: Hansong Zhang, Shikun Li, Dan Zeng, Chenggang Yan, Shiming Ge
Categories: cs.LG cs.CV cs.HC
Comments: This work has been accepted by AAAI-24
\\ ( https://arxiv.org/abs/2312.07331 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11456
replaced with revised version Tue, 20 Feb 2024 06:14:42 GMT   (3100kb,D)

Title: Iterative Preference Learning from Human Feedback: Bridging Theory and
  Practice for RLHF under KL-Constraint
Authors: Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan
  Jiang, Tong Zhang
Categories: cs.LG cs.AI stat.ML
Comments: 37 pages; mathematical foundation and practical algorithms of RLHF
\\ ( https://arxiv.org/abs/2312.11456 ,  3100kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12137
replaced with revised version Mon, 19 Feb 2024 20:17:22 GMT   (296kb,D)

Title: Best Arm Identification with Fixed Budget: A Large Deviation Perspective
Authors: Po-An Wang, Ruo-Chun Tzeng and Alexandre Proutiere
Categories: cs.LG stat.ML
Comments: We made small for implementing SH algorithm, now it has been
  corrected
\\ ( https://arxiv.org/abs/2312.12137 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15995
replaced with revised version Tue, 20 Feb 2024 07:53:38 GMT   (591kb)

Title: Generalization in Kernel Regression Under Realistic Assumptions
Authors: Daniel Barzilai and Ohad Shamir
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2312.15995 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09198
replaced with revised version Tue, 20 Feb 2024 06:31:47 GMT   (23222kb,D)

Title: Space and Time Continuous Physics Simulation From Partial Observations
Authors: Janny Steeven, Nadri Madiha, Digne Julie, Wolf Christian
Categories: cs.LG
Comments: Project Page: https://continuous-pde.github.io/
\\ ( https://arxiv.org/abs/2401.09198 ,  23222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02977
replaced with revised version Tue, 20 Feb 2024 13:00:32 GMT   (49835kb,D)

Title: Variational Flow Models: Flowing in Your Style
Authors: Kien Do, Duc Kieu, Toan Nguyen, Dang Nguyen, Hung Le, Dung Nguyen,
  Thin Nguyen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.02977 ,  49835kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05926
replaced with revised version Tue, 20 Feb 2024 14:21:37 GMT   (2278kb,D)

Title: On the Convergence of Zeroth-Order Federated Tuning for Large Language
  Models
Authors: Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen
Categories: cs.LG cs.CL
Comments: 19 pages, 10 figures
\\ ( https://arxiv.org/abs/2402.05926 ,  2278kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05956
replaced with revised version Tue, 20 Feb 2024 06:16:24 GMT   (2496kb,D)

Title: Multi-scale transformers with Adaptive Pathways for Time Series
  Forecasting
Authors: Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang,
  Qingsong Wen, Bin Yang, Chenjuan Guo
Categories: cs.LG
Comments: Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2402.05956 ,  2496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07845
replaced with revised version Tue, 20 Feb 2024 18:46:04 GMT   (926kb)

Title: Unsupervised Optimisation of GNNs for Node Clustering
Authors: William Leeney and Ryan McConville
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.07845 ,  926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08491
replaced with revised version Tue, 20 Feb 2024 14:40:23 GMT   (870kb,D)

Title: Deep Reinforcement Learning for Controlled Traversing of the Attractor
  Landscape of Boolean Models in the Context of Cellular Reprogramming
Authors: Andrzej Mizera, Jakub Zarzycki
Categories: cs.LG cs.AI q-bio.MN q-bio.QM
\\ ( https://arxiv.org/abs/2402.08491 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10184
replaced with revised version Tue, 20 Feb 2024 18:37:31 GMT   (706kb,D)

Title: Rethinking Information Structures in RLHF: Reward Generalization from a
  Graph Theory Perspective
Authors: Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou,
  Han Yang, Josef Dai, Xuehai Pan, Yaodong Yang
Categories: cs.LG cs.AI cs.CL cs.DM
\\ ( https://arxiv.org/abs/2402.10184 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10445
replaced with revised version Tue, 20 Feb 2024 05:14:09 GMT   (34kb,D)

Title: Collaborative Learning with Different Labeling Functions
Authors: Yuyang Deng, Mingda Qiao
Categories: cs.LG cs.DS stat.ML
Comments: v2 included additional discussion on related work
\\ ( https://arxiv.org/abs/2402.10445 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10870
replaced with revised version Tue, 20 Feb 2024 18:04:57 GMT   (6314kb,D)

Title: Best of Three Worlds: Adaptive Experimentation for Digital Marketing in
  Practice
Authors: Tanner Fiez, Houssam Nassif, Arick Chen, Sergio Gamez, Lalit Jain
Categories: cs.LG stat.ME
Journal-ref: The Web Conference (WWW), Singapore, 2024
\\ ( https://arxiv.org/abs/2402.10870 ,  6314kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10964
replaced with revised version Tue, 20 Feb 2024 08:24:42 GMT   (1103kb,D)

Title: Optimal feature rescaling in machine learning based on neural networks
Authors: Federico Maria Vitr\`o, Marco Leonesio, Lorenzo Fagiano
Categories: cs.LG cs.NE
Comments: 6 pages
\\ ( https://arxiv.org/abs/2402.10964 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11752
replaced with revised version Tue, 20 Feb 2024 02:58:38 GMT   (1096kb)

Title: Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models
  via Reparameterisation and Smoothing
Authors: Dominik Wagner, Basim Khajwal, C.-H. Luke Ong
Categories: cs.LG cs.AI math.OC
\\ ( https://arxiv.org/abs/2402.11752 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11821
replaced with revised version Tue, 20 Feb 2024 03:23:49 GMT   (2163kb,D)

Title: Microstructures and Accuracy of Graph Recall by Large Language Models
Authors: Yanbang Wang, Hejie Cui, Jon Kleinberg
Categories: cs.LG cs.CL cs.IR cs.SI
Comments: 16 pages, 7 tables, 5 figures
\\ ( https://arxiv.org/abs/2402.11821 ,  2163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11922
replaced with revised version Tue, 20 Feb 2024 07:36:01 GMT   (3834kb,D)

Title: A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer
  Learning
Authors: Yuan Yuan, Chenyang Shao, Jingtao Ding, Depeng Jin, Yong Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11922 ,  3834kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12065
replaced with revised version Tue, 20 Feb 2024 08:48:24 GMT   (8946kb,D)

Title: WKVQuant: Quantizing Weight and Key/Value Cache for Large Language
  Models Gains More
Authors: Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu,
  Liqiang Nie
Categories: cs.LG cs.AI cs.CL
Comments: Frist work to exclusively quantize weight and Key/Value cache for
  large language models
\\ ( https://arxiv.org/abs/2402.12065 ,  8946kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12161
replaced with revised version Tue, 20 Feb 2024 09:03:43 GMT   (4994kb,D)

Title: Endowing Pre-trained Graph Models with Provable Fairness
Authors: Zhongjian Zhang, Mengmei Zhang, Yue Yu, Cheng Yang, Jiawei Liu and
  Chuan Shi
Categories: cs.LG cs.AI cs.CY cs.SI
Comments: Accepted by WWW 2024
DOI: 10.1145/3589334.3645703
\\ ( https://arxiv.org/abs/2402.12161 ,  4994kb)
------------------------------------------------------------------------------
\\
arXiv:2109.03445 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 12:58:09 GMT   (83kb)

Title: Convergence of Batch Asynchronous Stochastic Approximation With
  Applications to Reinforcement Learning
Authors: Rajeeva L. Karandikar and M. Vidyasagar
Categories: stat.ML cs.AI cs.LG cs.SY eess.SY math.PR
Comments: 28 pages
\\ ( https://arxiv.org/abs/2109.03445 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2203.00156
replaced with revised version Tue, 20 Feb 2024 02:33:12 GMT   (4080kb,D)

Title: Preemptive Motion Planning for Human-to-Robot Indirect Placement
  Handovers
Authors: Andrew Choi, Mohammad Khalid Jawed, and Jungseock Joo
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: IEEE International Conference on Robotics and Automation (ICRA 2022).
  Supplementary videos: https://pmp-human-to-robot.github.io/
DOI: 10.1109/ICRA46639.2022.9811558
\\ ( https://arxiv.org/abs/2203.00156 ,  4080kb)
------------------------------------------------------------------------------
\\
arXiv:2210.13664
replaced with revised version Tue, 20 Feb 2024 11:13:07 GMT   (1698kb,D)

Title: Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher
  Mixture Model
Authors: Jean-R\'emy Conti, Nathan Noiry, Vincent Despiegel, St\'ephane
  Gentric, St\'ephan Cl\'emen\c{c}on
Categories: cs.CV cs.AI
Comments: Accepted to ICML 2022
Journal-ref: Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:4344-4369, 2022
\\ ( https://arxiv.org/abs/2210.13664 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02181
replaced with revised version Tue, 20 Feb 2024 13:47:56 GMT   (23138kb,D)

Title: Model Stitching and Visualization How GAN Generators can Invert Networks
  in Real-Time
Authors: Rudolf Herdt (1 and 2), Maximilian Schmidt (1 and 2), Daniel Otero
  Baguer (1 and 2), Jean Le'Clerc Arrastia (1 and 2), Peter Maass (1 and 2)
  ((1) University of Bremen, (2) aisencia)
Categories: cs.CV cs.AI cs.LG eess.IV
\\ ( https://arxiv.org/abs/2302.02181 ,  23138kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08014
replaced with revised version Mon, 19 Feb 2024 23:24:32 GMT   (1256kb)

Title: Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by
  Leveraging Lightweight All-ConvNet and Transfer Learning
Authors: Md. Rabiul Islam, Daniel Massicotte, Philippe Y. Massicotte, and
  Wei-Ping Zhu
Categories: cs.CV cs.AI cs.LG eess.AS
\\ ( https://arxiv.org/abs/2305.08014 ,  1256kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16071
replaced with revised version Tue, 20 Feb 2024 11:56:01 GMT   (49055kb,D)

Title: Semantic Image Synthesis via Class-Adaptive Cross-Attention
Authors: Tomaso Fontanini, Claudio Ferrari, Giuseppe Lisanti, Massimo Bertozzi,
  Andrea Prati
Categories: cs.CV cs.AI
Comments: Code and models available at https://github.com/TFonta/CA2SIS
\\ ( https://arxiv.org/abs/2308.16071 ,  49055kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11087 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 01:59:16 GMT   (5836kb,D)

Title: Embed-Search-Align: DNA Sequence Alignment using Transformer Models
Authors: Pavan Holur, K. C. Enevoldsen, Lajoyce Mboning, Thalia Georgiou,
  Louis-S. Bouchard, Matteo Pellegrini and Vwani Roychowdhury
Categories: q-bio.GN cs.AI
Comments: 17 pages, Tables 7, Figures 5
\\ ( https://arxiv.org/abs/2309.11087 ,  5836kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00267 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 01:26:05 GMT   (4023kb,D)

Title: The Physics of Preference: Unravelling Imprecision of Human Preferences
  through Magnetisation Dynamics
Authors: Ivan S. Maksymov and Ganna Pogrebna
Categories: physics.soc-ph cond-mat.other cs.AI quant-ph
\\ ( https://arxiv.org/abs/2310.00267 ,  4023kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07138
replaced with revised version Tue, 20 Feb 2024 07:35:19 GMT   (15885kb,D)

Title: Denoising Task Routing for Diffusion Models
Authors: Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, Changick Kim
Categories: cs.CV cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.07138 ,  15885kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08513
replaced with revised version Mon, 19 Feb 2024 19:25:31 GMT   (14586kb,D)

Title: How connectivity structure shapes rich and lazy learning in neural
  circuits
Authors: Yuhan Helena Liu, Aristide Baratin, Jonathan Cornford, Stefan Mihalas,
  Eric Shea-Brown, and Guillaume Lajoie
Categories: cs.NE cs.AI q-bio.NC
Comments: Published at ICLR 2024
\\ ( https://arxiv.org/abs/2310.08513 ,  14586kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09234
replaced with revised version Tue, 20 Feb 2024 06:52:21 GMT   (638kb,D)

Title: ClickPrompt: CTR Models are Strong Prompt Generators for Adapting
  Language Models to CTR Prediction
Authors: Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai,
  Kangning Zhang, Ruiming Tang, Yong Yu, Weinan Zhang
Categories: cs.IR cs.AI
Comments: Accepted by WWW 2024
\\ ( https://arxiv.org/abs/2310.09234 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15098
replaced with revised version Tue, 20 Feb 2024 18:43:26 GMT   (585kb,D)

Title: Acquiring Weak Annotations for Tumor Localization in Temporal and
  Volumetric Data
Authors: Yu-Cheng Chou, Bowen Li, Deng-Ping Fan, Alan Yuille, Zongwei Zhou
Categories: cs.CV cs.AI
Comments: Published in Machine Intelligence Research
Journal-ref: Mach. Intell. Res. (2024)
DOI: 10.1007/s11633-023-1380-5
\\ ( https://arxiv.org/abs/2310.15098 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00967
replaced with revised version Tue, 20 Feb 2024 03:13:30 GMT   (3955kb,D)

Title: Vision-Language Interpreter for Robot Task Planning
Authors: Keisuke Shirai, Cristian C. Beltran-Hernandez, Masashi Hamaya, Atsushi
  Hashimoto, Shohei Tanaka, Kento Kawaharazuka, Kazutoshi Tanaka, Yoshitaka
  Ushiku, Shinsuke Mori
Categories: cs.RO cs.AI cs.CL
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2311.00967 ,  3955kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09868
replaced with revised version Tue, 20 Feb 2024 16:26:30 GMT   (4666kb,D)

Title: INTERVENOR: Prompting the Coding Ability of Large Language Models with
  the Interactive Chain of Repair
Authors: Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan
  Liu and Ge Yu
Categories: cs.SE cs.AI
Comments: 26 pages, 19 figures, 8 tables
\\ ( https://arxiv.org/abs/2311.09868 ,  4666kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14688
replaced with revised version Tue, 20 Feb 2024 15:40:23 GMT   (1537kb,D)

Title: Procedural Fairness Through Decoupling Objectionable Data Generating
  Components
Authors: Zeyu Tang, Jialu Wang, Yang Liu, Peter Spirtes, Kun Zhang
Categories: cs.CY cs.AI cs.LG
Journal-ref: The 12th International Conference on Learning Representations
  (ICLR 2024)
\\ ( https://arxiv.org/abs/2311.14688 ,  1537kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16476
replaced with revised version Tue, 20 Feb 2024 03:35:46 GMT   (1125kb,D)

Title: LANS: A Layout-Aware Neural Solver for Plane Geometry Problem
Authors: Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, Cheng-Lin Liu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.16476 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09007
replaced with revised version Tue, 20 Feb 2024 13:02:10 GMT   (6582kb,D)

Title: LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution
Authors: Hongwei Cui and Yuyang Du and Qun Yang and Yulin Shao and Soung Chang
  Liew
Categories: cs.IT cs.AI math.IT
\\ ( https://arxiv.org/abs/2312.09007 ,  6582kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03843
replaced with revised version Tue, 20 Feb 2024 10:17:24 GMT   (3573kb,D)

Title: A new method for optical steel rope non-destructive damage detection
Authors: Yunqing Bao, Bin Hu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.03843 ,  3573kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09444 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 06:05:37 GMT   (3168kb,D)

Title: Multimodal Action Quality Assessment
Authors: Ling-An Zeng and Wei-Shi Zheng
Categories: eess.SP cs.AI cs.CV
Comments: IEEE Transactions on Image Processing 2024
ACM-class: I.2.10
DOI: 10.1109/TIP.2024.3362135
\\ ( https://arxiv.org/abs/2402.09444 ,  3168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09723 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 06:35:36 GMT   (3044kb,D)

Title: Best Arm Identification for Prompt Learning under a Limited Budget
Authors: Chengshuai Shi, Kun Yang, Jing Yang and Cong Shen
Categories: stat.ML cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.09723 ,  3044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10204 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 18:00:23 GMT   (14026kb,D)

Title: Radio-astronomical Image Reconstruction with Conditional Denoising
  Diffusion Model
Authors: Mariia Drozdova, Vitaliy Kinakh, Omkar Bait, Olga Taran, Erica
  Lastufka, Miroslava Dessauges-Zavadsky, Taras Holotyak, Daniel Schaerer,
  Slava Voloshynovskiy
Categories: astro-ph.IM cs.AI cs.CV
Comments: In production in Astronomy&Astrophyics
DOI: 10.1051/0004-6361/202347948
\\ ( https://arxiv.org/abs/2402.10204 ,  14026kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10930
replaced with revised version Tue, 20 Feb 2024 09:52:42 GMT   (2035kb,D)

Title: ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
Authors: Shiwei Liu, Guanchen Tao, Yifei Zou, Derek Chow, Zichen Fan, Kauna
  Lei, Bangfei Pan, Dennis Sylvester, Gregory Kielian, and Mehdi Saligane
Categories: cs.AR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.10930 ,  2035kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10433
replaced with revised version Tue, 20 Feb 2024 15:20:32 GMT   (5107kb,D)

Title: Writer-Defined AI Personas for On-Demand Feedback Generation
Authors: Karim Benharrak, Tim Zindulka, Florian Lehmann, Hendrik Heuer, Daniel
  Buschek
Categories: cs.HC cs.CL
Comments: 18 pages, 8 figures, 3 tables
ACM-class: H.5.2; I.2.7
DOI: 10.1145/3613904.3642406
\\ ( https://arxiv.org/abs/2309.10433 ,  5107kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11895
replaced with revised version Tue, 20 Feb 2024 04:00:15 GMT   (1842kb,D)

Title: Bridging or Breaking: Impact of Intergroup Interactions on Religious
  Polarization
Authors: Rochana Chaturvedi, Sugat Chaturvedi and Elena Zheleva
Categories: cs.SI cs.CL physics.soc-ph
DOI: 10.1145/3589334.3645675
\\ ( https://arxiv.org/abs/2402.11895 ,  1842kb)
------------------------------------------------------------------------------
\\
arXiv:2009.01947
replaced with revised version Mon, 19 Feb 2024 19:16:24 GMT   (3997kb,D)

Title: Practical and Parallelizable Algorithms for Non-Monotone Submodular
  Maximization with Size Constraint
Authors: Yixin Chen and Alan Kuhnle
Categories: cs.DS cs.LG
Comments: 39 pages
Journal-ref: Journal of Artificial Intelligence Research 79 (2024) 599-638
DOI: 10.1613/jair.1.14323
\\ ( https://arxiv.org/abs/2009.01947 ,  3997kb)
------------------------------------------------------------------------------
\\
arXiv:2201.11720 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 17:53:38 GMT   (2224kb,D)

Title: Simplicial Convolutional Filters
Authors: Maosheng Yang, Elvin Isufi, Michael T. Schaub, Geert Leus
Categories: eess.SP cs.LG cs.SI math.AT math.SP
Comments: 16 pages, 13 figures, 2 tables
DOI: 10.1109/TSP.2022.3207045
\\ ( https://arxiv.org/abs/2201.11720 ,  2224kb)
------------------------------------------------------------------------------
\\
arXiv:2207.07654 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 03:34:30 GMT   (2195kb,D)

Title: Learning inducing points and uncertainty on molecular data by scalable
  variational Gaussian processes
Authors: Mikhail Tsitsvero, Mingoo Jin, Andrey Lyalin
Categories: physics.chem-ph cs.LG physics.comp-ph
Comments: 17 pages, 6 figures
MSC-class: 60G15, 60-08, 68-04, 68T99, 92E99
\\ ( https://arxiv.org/abs/2207.07654 ,  2195kb)
------------------------------------------------------------------------------
\\
arXiv:2209.08167 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 13:26:44 GMT   (6706kb,D)

Title: Quantum Vision Transformers
Authors: El Amine Cherrat, Iordanis Kerenidis, Natansh Mathur, Jonas Landman,
  Martin Strahm, and Yun Yvonna Li
Categories: quant-ph cs.LG
Comments: 20 pages
\\ ( https://arxiv.org/abs/2209.08167 ,  6706kb)
------------------------------------------------------------------------------
\\
arXiv:2211.08854 (*cross-listing*)
replaced with revised version Mon, 19 Feb 2024 21:13:45 GMT   (7270kb,D)

Title: Graph Filters for Signal Processing and Machine Learning on Graphs
Authors: Elvin Isufi, Fernando Gama, David I. Shuman, Santiago Segarra
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2211.08854 ,  7270kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08766 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 08:46:49 GMT   (851kb,D)

Title: A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk
  Minimization
Authors: Mathieu Dagr\'eou, Thomas Moreau, Samuel Vaiter, Pierre Ablin
Categories: stat.ML cs.LG math.OC
Comments: Accepted at AISTATS 2024
\\ ( https://arxiv.org/abs/2302.08766 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2303.01074
replaced with revised version Mon, 19 Feb 2024 21:55:51 GMT   (904kb,D)

Title: Learning not to Regret
Authors: David Sychrovsk\'y, Michal \v{S}ustr, Elnaz Davoodi, Michael Bowling,
  Marc Lanctot, Martin Schmid
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2303.01074 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02898 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 03:23:50 GMT   (3066kb,D)

Title: Stabilizing the Maximal Entropy Moment Method for Rarefied Gas Dynamics
  at Single-Precision
Authors: Candi Zheng, Wang Yang, Shiyi Chen
Categories: physics.flu-dyn cs.LG
Comments: 56 pages, 8 figures
\\ ( https://arxiv.org/abs/2303.02898 ,  3066kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03582
replaced with revised version Tue, 20 Feb 2024 16:18:45 GMT   (16693kb,D)

Title: A multimodal dynamical variational autoencoder for audiovisual speech
  representation learning
Authors: Samir Sadok, Simon Leglaive, Laurent Girin, Xavier Alameda-Pineda,
  Renaud S\'eguier
Categories: cs.SD cs.LG cs.MM eess.AS
Comments: 14 figures, https://samsad35.github.io/site-mdvae/
DOI: 10.1016/j.neunet.2024.106120
\\ ( https://arxiv.org/abs/2305.03582 ,  16693kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01990
replaced with revised version Mon, 19 Feb 2024 19:59:55 GMT   (93kb,D)

Title: Incentivizing Exploration with Linear Contexts and Combinatorial Actions
Authors: Mark Sellke
Categories: cs.GT cs.LG
Comments: International Conference on Machine Learning (ICML) 2023
\\ ( https://arxiv.org/abs/2306.01990 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06945
replaced with revised version Tue, 20 Feb 2024 02:00:05 GMT   (14500kb,D)

Title: Underwater Acoustic Target Recognition based on Smoothness-inducing
  Regularization and Spectrogram-based Data Augmentation
Authors: Ji Xu, Yuan Xie, Wenchao Wang
Categories: cs.SD cs.LG
Journal-ref: Ocean Engineering, 2023, 281: 114926
DOI: 10.1016/j.oceaneng.2023.114926
\\ ( https://arxiv.org/abs/2306.06945 ,  14500kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07090 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 14:17:45 GMT   (1371kb,D)

Title: Choice Models and Permutation Invariance: Demand Estimation in
  Differentiated Products Markets
Authors: Amandeep Singh, Ye Liu, and Hema Yoganarasimhan
Categories: econ.EM cs.LG
\\ ( https://arxiv.org/abs/2307.07090 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12449 (*cross-listing*)
replaced with revised version Mon, 19 Feb 2024 20:05:45 GMT   (8226kb,D)

Title: DyPP: Dynamic Parameter Prediction to Accelerate Convergence of
  Variational Quantum Algorithms
Authors: Satwik Kundu, Debarshi Kundu and Swaroop Ghosh
Categories: quant-ph cs.LG
\\ ( https://arxiv.org/abs/2307.12449 ,  8226kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04585 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 13:08:31 GMT   (226kb)

Title: Kernel Single Proxy Control for Deterministic Confounding
Authors: Liyuan Xu, Arthur Gretton
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2308.04585 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01781 (*cross-listing*)
replaced with revised version Mon, 19 Feb 2024 20:49:05 GMT   (269kb,D)

Title: Self-concordant Smoothing for Large-Scale Convex Composite Optimization
Authors: Adeyemi D. Adeoye, Alberto Bemporad
Categories: math.OC cs.LG
Comments: 39 pages, 7 figures, 3 tables
MSC-class: 65K05 (Primary) 65K10 65D10 65D15, 90C06 (Secondary) 90C25 90C30
  90C53 90C59 90C90, 49M37
\\ ( https://arxiv.org/abs/2309.01781 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03447
replaced with revised version Tue, 20 Feb 2024 16:18:47 GMT   (13199kb,D)

Title: FLAIM: AIM-based Synthetic Data Generation in the Federated Setting
Authors: Samuel Maddock, Graham Cormode, Carsten Maple
Categories: cs.CR cs.LG
Comments: 19 pages
\\ ( https://arxiv.org/abs/2310.03447 ,  13199kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03755
replaced with revised version Mon, 19 Feb 2024 21:08:45 GMT   (6856kb,D)

Title: Physics Informed Neural Network Code for 2D Transient Problems
  (PINN-2DT) Compatible with Google Colab
Authors: Pawe{\l} Maczuga, Maciej Sikora, Maciej Skocze\'n, Przemys{\l}aw
  Ro\.znawski, Filip T{\l}uszcz, Marcin Szubert, Marcin {\L}o\'s, Witold
  Dzwinel, Keshav Pingali, Maciej Paszy\'nski
Categories: cs.CE cs.LG cs.MS cs.NA math.NA
Comments: 21 pages, 13 figures
ACM-class: G.1.8; G.1.10; J.2; J.3; G.4; I.6.4; I.m
\\ ( https://arxiv.org/abs/2310.03755 ,  6856kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08331 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 09:11:42 GMT   (819kb,D)

Title: Dealing with uncertainty: balancing exploration and exploitation in deep
  recurrent reinforcement learning
Authors: Valentina Zangirolami and Matteo Borrotti
Categories: stat.ML cs.LG
Comments: 28 pages, added renferences, corrected typos, revised argument in
  section 3, results unchanged, redifinition of the article structure
\\ ( https://arxiv.org/abs/2310.08331 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15627 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 07:22:29 GMT   (259kb,D)

Title: Contextual Directed Acyclic Graphs
Authors: Ryan Thompson, Edwin V. Bonilla, Robert Kohn
Categories: stat.ML cs.LG
Comments: To appear in the Proceedings of the 27th International Conference on
  Artificial Intelligence and Statistics
\\ ( https://arxiv.org/abs/2310.15627 ,  259kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19192 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 04:47:50 GMT   (39211kb,D)

Title: Emergence of Grid-like Representations by Training Recurrent Networks
  with Conformal Normalization
Authors: Dehong Xu, Ruiqi Gao, Wen-Hao Zhang, Xue-Xin Wei, Ying Nian Wu
Categories: q-bio.NC cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.19192 ,  39211kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18083
replaced with revised version Fri, 16 Feb 2024 22:00:20 GMT   (493kb,D)

Title: Meta Co-Training: Two Views are Better than One
Authors: Jay C. Rothenberger, Dimitrios I. Diochnos
Categories: cs.CV cs.LG
Comments: 16 pages, 14 figures, 10 tables, for implementation see
  https://github.com/JayRothenberger/Meta-Co-Training
ACM-class: I.2.6; I.4.10
\\ ( https://arxiv.org/abs/2311.18083 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08976
replaced with revised version Tue, 20 Feb 2024 13:44:13 GMT   (11039kb,D)

Title: Dynamic Retrieval-Augmented Generation
Authors: Anton Shapkin, Denis Litvinov, Yaroslav Zharov, Egor Bogomolov, Timur
  Galimzyanov, Timofey Bryksin
Categories: cs.SE cs.LG
Comments: 10 pages
\\ ( https://arxiv.org/abs/2312.08976 ,  11039kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16414
replaced with revised version Tue, 20 Feb 2024 14:25:25 GMT   (29823kb,D)

Title: Bellman Optimal Stepsize Straightening of Flow-Matching Models
Authors: Bao Nguyen, Binh Nguyen, Viet Anh Nguyen
Categories: cs.CV cs.LG
Comments: 21 pages, 14 figures
\\ ( https://arxiv.org/abs/2312.16414 ,  29823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01930
replaced with revised version Mon, 19 Feb 2024 23:22:51 GMT   (30613kb,D)

Title: Reducing Optimism Bias in Incomplete Cooperative Games
Authors: Filip \'Uradn\'ik, David Sychrovsk\'y, Jakub \v{C}ern\'y and Martin
  \v{C}ern\'y
Categories: cs.GT cs.LG
Comments: Proc. of the 23rd International Conference on Autonomous Agents and
  Multiagent Systems (AAMAS 2024)
\\ ( https://arxiv.org/abs/2402.01930 ,  30613kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11101 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 02:34:58 GMT   (1530kb)

Title: Physics-based material parameters extraction from perovskite experiments
  via Bayesian optimization
Authors: Hualin Zhan, Viqar Ahmad, Azul Mayon, Grace Tabi, Anh Dinh Bui,
  Zhuofeng Li, Daniel Walter, Hieu Nguyen, Klaus Weber, Thomas White, Kylie
  Catchpole
Categories: cond-mat.mtrl-sci cs.CE cs.LG
\\ ( https://arxiv.org/abs/2402.11101 ,  1530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11401
replaced with revised version Tue, 20 Feb 2024 18:25:23 GMT   (24321kb,D)

Title: GraphKD: Exploring Knowledge Distillation Towards Document Object
  Detection with Structured Graph Creation
Authors: Ayan Banerjee, Sanket Biswas, Josep Llad\'os, and Umapada Pal
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.11401 ,  24321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11940
replaced with revised version Tue, 20 Feb 2024 12:13:05 GMT   (1693kb,D)

Title: AICAttack: Adversarial Image Captioning Attack with Attention-Based
  Optimization
Authors: Jiyao Li, Mingze Ni, Yifei Dong, Tianqing Zhu and Wei Liu
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2402.11940 ,  1693kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
