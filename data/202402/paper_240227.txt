paper_240227.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月27日 19:58
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri 23 Feb 24 19:00:00 GMT  to  Mon 26 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.15515
Date: Sat, 3 Feb 2024 18:17:19 GMT   (1797kb)

Title: Feasibility of Identifying Factors Related to Alzheimer's Disease and
  Related Dementia in Real-World Data
Authors: Aokun Chen, Qian Li, Yu Huang, Yongqiu Li, Yu-neng Chuang, Xia Hu,
  Serena Guo, Yonghui Wu, Yi Guo, Jiang Bian
Categories: cs.AI q-bio.QM stat.AP
\\
  A comprehensive view of factors associated with AD/ADRD will significantly
aid in studies to develop new treatments for AD/ADRD and identify high-risk
populations and patients for prevention efforts. In our study, we summarized
the risk factors for AD/ADRD by reviewing existing meta-analyses and review
articles on risk and preventive factors for AD/ADRD. In total, we extracted 477
risk factors in 10 categories from 537 studies. We constructed an interactive
knowledge map to disseminate our study results. Most of the risk factors are
accessible from structured Electronic Health Records (EHRs), and clinical
narratives show promise as information sources. However, evaluating genomic
risk factors using RWD remains a challenge, as genetic testing for AD/ADRD is
still not a common practice and is poorly documented in both structured and
unstructured EHRs. Considering the constantly evolving research on AD/ADRD risk
factors, literature mining via NLP methods offers a solution to automatically
update our knowledge map.
\\ ( https://arxiv.org/abs/2402.15515 ,  1797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15521
Date: Thu, 15 Feb 2024 18:13:41 GMT   (822kb,D)

Title: HKD-SHO: A hybrid smart home system based on knowledge-based and
  data-driven services
Authors: Mingming Qiu, Elie Najm, R\'emi Sharrock, Bruno Traverson
Categories: cs.AI cs.LG
Comments: keywords: Hybrid System, Knowledge Representation, Reinforcement
  Learning, Services, Smart Home
\\
  A smart home is realized by setting up various services. Several methods have
been proposed to create smart home services, which can be divided into
knowledge-based and data-driven approaches. However, knowledge-based approaches
usually require manual input from the inhabitant, which can be complicated if
the physical phenomena of the concerned environment states are complex, and the
inhabitant does not know how to adjust related actuators to achieve the target
values of the states monitored by services. Moreover, machine learning-based
data-driven approaches that we are interested in are like black boxes and
cannot show the inhabitant in which situations certain services proposed
certain actuators' states. To solve these problems, we propose a hybrid system
called HKD-SHO (Hybrid Knowledge-based and Data-driven services based Smart
HOme system), where knowledge-based and machine learning-based data-driven
services are profitably integrated. The principal advantage is that it inherits
the explicability of knowledge-based services and the dynamism of data-driven
services. We compare HKD-SHO with several systems for creating dynamic smart
home services, and the results show the better performance of HKD-SHO.
\\ ( https://arxiv.org/abs/2402.15521 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15522
Date: Fri, 16 Feb 2024 12:48:40 GMT   (94kb)

Title: IntSat: Integer Linear Programming by Conflict-Driven
  Constraint-Learning
Authors: Robert Nieuwenhuis, Albert Oliveras, Enric Rodriguez-Carbonell
Categories: cs.AI
Comments: 48 pages. This is the Author's Original Manuscript of the journal
  version
ACM-class: I.2.8; F.4.1
DOI: 10.1080/10556788.2023.2246167
\\
  State-of-the-art SAT solvers are nowadays able to handle huge real-world
instances. The key to this success is the so-called Conflict-Driven
Clause-Learning (CDCL) scheme, which encompasses a number of techniques that
exploit the conflicts that are encountered during the search for a solution. In
this article we extend these techniques to Integer Linear Programming (ILP),
where variables may take general integer values instead of purely binary ones,
constraints are more expressive than just propositional clauses, and there may
be an objective function to optimise. We explain how these methods can be
implemented efficiently, and discuss possible improvements. Our work is backed
with a basic implementation that shows that, even in this far less mature
stage, our techniques are already a useful complement to the state of the art
in ILP solving.
\\ ( https://arxiv.org/abs/2402.15522 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15524
Date: Mon, 19 Feb 2024 20:03:45 GMT   (2833kb,D)

Title: Graph Pruning for Enumeration of Minimal Unsatisfiable Subsets
Authors: Panagiotis Lymperopoulos and Liping Liu
Categories: cs.AI cs.LG
\\
  Finding Minimal Unsatisfiable Subsets (MUSes) of binary constraints is a
common problem in infeasibility analysis of over-constrained systems. However,
because of the exponential search space of the problem, enumerating MUSes is
extremely time-consuming in real applications. In this work, we propose to
prune formulas using a learned model to speed up MUS enumeration. We represent
formulas as graphs and then develop a graph-based learning model to predict
which part of the formula should be pruned. Importantly, our algorithm does not
require data labeling by only checking the satisfiability of pruned formulas.
It does not even require training data from the target application because it
extrapolates to data with different distributions. In our experiments we
combine our algorithm with existing MUS enumerators and validate its
effectiveness in multiple benchmarks including a set of real-world problems
outside our training distribution. The experiment results show that our method
significantly accelerates MUS enumeration on average on these benchmark
problems.
\\ ( https://arxiv.org/abs/2402.15524 ,  2833kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15526
Date: Tue, 20 Feb 2024 08:03:05 GMT   (3462kb,D)

Title: Chain-of-Specificity: An Iteratively Refining Method for Eliciting
  Knowledge from Large Language Models
Authors: Kaiwen Wei, Jingyuan Zhang, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Li
  Jin, Yue Yu
Categories: cs.AI cs.LG
\\
  Large Language Models (LLMs) exhibit remarkable generative capabilities,
enabling the generation of valuable information. Despite these advancements,
previous research found that LLMs sometimes struggle with adhering to specific
constraints (e.g., in specific place or at specific time), at times even
overlooking them, which leads to responses that are either too generic or not
fully satisfactory. Existing approaches attempted to address this issue by
decomposing or rewriting input instructions, yet they fall short in adequately
emphasizing specific constraints and in unlocking the underlying knowledge
(e.g., programming within the context of software development). In response,
this paper proposes a simple yet effective method named Chain-of-Specificity
(CoS). Specifically, CoS iteratively emphasizes the specific constraints in the
input instructions, unlocks knowledge within LLMs, and refines responses.
Experiments conducted on publicly available and self-build complex datasets
demonstrate that CoS outperforms existing methods in enhancing generated
content especially for the specificity. Besides, as the number of specific
constraints increase, other baselines falter, while CoS still performs well.
Moreover, we show that distilling responses generated by CoS effectively
enhances the ability of smaller models to follow the constrained instructions.
Resources of this paper will be released for further research.
\\ ( https://arxiv.org/abs/2402.15526 ,  3462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15572
Date: Fri, 23 Feb 2024 19:14:57 GMT   (6567kb,D)

Title: Improving Explainable Object-induced Model through Uncertainty for
  Automated Vehicles
Authors: Shihong Ling, Yue Wan, Xiaowei Jia, Na Du
Categories: cs.AI cs.CV cs.RO
Comments: In Proceedings of the 2024 ACM / IEEE International Conference on
  Human-Robot Interaction (HRI '24), March 11--14, 2024, Boulder, CO, USA. ACM,
  New York, NY, USA, 9 pages
DOI: 10.1145/3610977.3634973
\\
  The rapid evolution of automated vehicles (AVs) has the potential to provide
safer, more efficient, and comfortable travel options. However, these systems
face challenges regarding reliability in complex driving scenarios. Recent
explainable AV architectures neglect crucial information related to inherent
uncertainties while providing explanations for actions. To overcome such
challenges, our study builds upon the "object-induced" model approach that
prioritizes the role of objects in scenes for decision-making and integrates
uncertainty assessment into the decision-making process using an evidential
deep learning paradigm with a Beta prior. Additionally, we explore several
advanced training strategies guided by uncertainty, including
uncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA
dataset, our findings underscore that the model, through these enhancements,
not only offers a clearer comprehension of AV decisions and their underlying
reasoning but also surpasses existing baselines across a broad range of
scenarios.
\\ ( https://arxiv.org/abs/2402.15572 ,  6567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15670
Date: Sat, 24 Feb 2024 01:04:04 GMT   (1886kb,D)

Title: A mathematical model for simultaneous personnel shift planning and
  unrelated parallel machine scheduling
Authors: Maziyar Khadivi, Mostafa Abbasi, Todd Charter, Homayoun Najjaran
Categories: cs.AI cs.DM
\\
  This paper addresses a production scheduling problem derived from an
industrial use case, focusing on unrelated parallel machine scheduling with the
personnel availability constraint. The proposed model optimizes the production
plan over a multi-period scheduling horizon, accommodating variations in
personnel shift hours within each time period. It assumes shared personnel
among machines, with one personnel required per machine for setup and
supervision during job processing. Available personnel are fewer than the
machines, thus limiting the number of machines that can operate in parallel.
The model aims to minimize the total production time considering
machine-dependent processing times and sequence-dependent setup times. The
model handles practical scenarios like machine eligibility constraints and
production time windows. A Mixed Integer Linear Programming (MILP) model is
introduced to formulate the problem, taking into account both continuous and
district variables. A two-step solution approach enhances computational speed,
first maximizing accepted jobs and then minimizing production time. Validation
with synthetic problem instances and a real industrial case study of a food
processing plant demonstrates the performance of the model and its usefulness
in personnel shift planning. The findings offer valuable insights for practical
managerial decision-making in the context of production scheduling.
\\ ( https://arxiv.org/abs/2402.15670 ,  1886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15721
Date: Sat, 24 Feb 2024 05:14:52 GMT   (3054kb,D)

Title: Hal-Eval: A Universal and Fine-grained Hallucination Evaluation
  Framework for Large Vision Language Models
Authors: Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan,
  Ji Zhang, Shikun Zhang
Categories: cs.AI cs.CL
\\
  Large Vision Language Models exhibit remarkable capabilities but struggle
with hallucinations inconsistencies between images and their descriptions.
Previous hallucination evaluation studies on LVLMs have identified
hallucinations in terms of objects, attributes, and relations but overlooked
complex hallucinations that create an entire narrative around a fictional
entity. In this paper, we introduce a refined taxonomy of hallucinations,
featuring a new category: Event Hallucination. We then utilize advanced LLMs to
generate and filter fine grained hallucinatory data consisting of various types
of hallucinations, with a particular focus on event hallucinations, laying the
groundwork for integrating discriminative and generative evaluation methods
within our universal evaluation framework. The proposed benchmark distinctively
assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it
a reliable and comprehensive tool for gauging LVLMs efficacy in handling
hallucinations. We will release our code and data.
\\ ( https://arxiv.org/abs/2402.15721 ,  3054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15729
Date: Sat, 24 Feb 2024 05:40:01 GMT   (387kb,D)

Title: How Do Humans Write Code? Large Models Do It the Same Way Too
Authors: Long Li
Categories: cs.AI cs.CL cs.PL
\\
  Large Language Models (LLMs) often make errors when performing numerical
calculations. In contrast to traditional chain-of-thought reasoning, the
program-of-thoughts approach involves generating executable code to solve
problems. By executing this code, it achieves more precise results. Using
generated executable code instead of natural language can reduce computational
errors. However, we observe that when LLMs solve mathematical problems using
code, they tend to generate more incorrect reasoning than when using natural
language. To address this issue, we propose Human-Think Language (HTL), a
straightforward yet highly efficient approach inspired by human coding
practices. The approach first generates problem-solving methods described in
the natural language by the model, then converts them into code, mirroring the
process where people think through the logic in natural language before writing
it as code. Additionally, it utilizes the Proximal Policy Optimization (PPO)
algorithm, enabling it to provide feedback to itself based on the correctness
of mathematical answers, much like humans do. Finally, we introduce a
focus-attention mechanism that masks the question segment, enhancing its
reliance on natural language inference solutions during code generation. We
conduct our experiments without introducing any additional information, and the
results across five mathematical calculation datasets showcase the
effectiveness of our approach. Notably, on the NumGLUE dataset, the
LlaMA-2-7B-based model achieves a superior performance rate (75.1%) compared to
the previous best performance with the LlaMA-2-70B model (74.4%).
\\ ( https://arxiv.org/abs/2402.15729 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15796
Date: Sat, 24 Feb 2024 11:54:32 GMT   (386kb)

Title: Construction and application of artificial intelligence crowdsourcing
  map based on multi-track GPS data
Authors: Yong Wang, Yanlin Zhou, Huan Ji, Zheng He, Xinyu Shen
Categories: cs.AI cs.HC
\\
  In recent years, the rapid development of high-precision map technology
combined with artificial intelligence has ushered in a new development
opportunity in the field of intelligent vehicles. High-precision map technology
is an important guarantee for intelligent vehicles to achieve autonomous
driving. However, due to the lack of research on high-precision map technology,
it is difficult to rationally use this technology in the field of intelligent
vehicles. Therefore, relevant researchers studied a fast and effective
algorithm to generate high-precision GPS data from a large number of
low-precision GPS trajectory data fusion, and generated several key data points
to simplify the description of GPS trajectory, and realized the "crowdsourced
update" model based on a large number of social vehicles for map data
collection came into being. This kind of algorithm has the important
significance to improve the data accuracy, reduce the measurement cost and
reduce the data storage space. On this basis, this paper analyzes the
implementation form of crowdsourcing map, so as to improve the various
information data in the high-precision map according to the actual situation,
and promote the high-precision map can be reasonably applied to the intelligent
car.
\\ ( https://arxiv.org/abs/2402.15796 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15809
Date: Sat, 24 Feb 2024 13:13:04 GMT   (1618kb,D)

Title: Empowering Large Language Model Agents through Action Learning
Authors: Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing
  Xu, Zhi-Hong Deng, Hongxia Yang
Categories: cs.AI cs.CL
Comments: 9 pages
\\
  Large Language Model (LLM) Agents have recently garnered increasing interest
yet they are limited in their ability to learn from trial and error, a key
element of intelligent behavior. In this work, we argue that the capacity to
learn new actions from experience is fundamental to the advancement of learning
in LLM agents. While humans naturally expand their action spaces and develop
skills through experiential learning, LLM agents typically operate within fixed
action spaces, limiting their potential for growth. To address these
challenges, our study explores open-action learning for language agents. We
introduce a framework LearnAct with an iterative learning strategy to create
and improve actions in the form of Python functions. In each iteration, LLM
revises and updates the currently available actions based on the errors
identified in unsuccessful training tasks, thereby enhancing action
effectiveness. Our experimental evaluations across Robotic Planning and
Alfworld environments reveal that after learning on a few training task
instances, our approach to open-action learning markedly improves agent
performance for the type of task (by 32 percent in AlfWorld compared to
ReAct+Reflexion, for instance) highlighting the importance of experiential
action learning in the development of more intelligent LLM agents.
\\ ( https://arxiv.org/abs/2402.15809 ,  1618kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15929
Date: Sat, 24 Feb 2024 23:16:57 GMT   (258kb,D)

Title: QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs
Authors: Isha Chaudhary, Vedaant V. Jain, Gagandeep Singh
Categories: cs.AI cs.CL cs.LG
\\
  Large Language Models (LLMs) have demonstrated impressive performance on
several benchmarks. However, traditional studies do not provide formal
guarantees on the performance of LLMs. In this work, we propose a novel
certification framework for LLM, QuaCer-C, wherein we formally certify the
knowledge-comprehension capabilities of popular LLMs. Our certificates are
quantitative - they consist of high-confidence, tight bounds on the probability
that the target LLM gives the correct answer on any relevant knowledge
comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs
indicate that the knowledge comprehension capability improves with an increase
in the number of parameters and that the Mistral model is less performant than
the rest in this evaluation.
\\ ( https://arxiv.org/abs/2402.15929 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15960
Date: Sun, 25 Feb 2024 02:46:33 GMT   (229kb,D)

Title: Budget-Constrained Tool Learning with Planning
Authors: Yuanhang Zheng, Peng Li, Ming Yan, Ji Zhang, Fei Huang and Yang Liu
Categories: cs.AI
\\
  Despite intensive efforts devoted to tool learning, the problem of
budget-constrained tool learning, which focuses on resolving user queries
within a specific budget constraint, has been widely overlooked. This paper
proposes a novel method for budget-constrained tool learning. Our approach
involves creating a preferable plan under the budget constraint before
utilizing the tools. This plan outlines the feasible tools and the maximum
number of times they can be employed, offering a comprehensive overview of the
tool learning process for large language models. This allows them to allocate
the budget from a broader perspective. To devise the plan without incurring
significant extra costs, we suggest initially estimating the usefulness of the
candidate tools based on past experience. Subsequently, we employ dynamic
programming to formulate the plan. Experimental results demonstrate that our
method can be integrated with various tool learning methods, significantly
enhancing their effectiveness under strict budget constraints.
\\ ( https://arxiv.org/abs/2402.15960 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15989
Date: Sun, 25 Feb 2024 05:04:51 GMT   (2235kb,D)

Title: PIDformer: Transformer Meets Control Theory
Authors: Tam Nguyen, C\'esar A. Uribe, Tan M. Nguyen, Richard G. Baraniuk
Categories: cs.AI cs.SY eess.SY
\\
  In this work, we address two main shortcomings of transformer architectures:
input corruption and rank collapse in their output representation. We unveil
self-attention as an autonomous state-space model that inherently promotes
smoothness in its solutions, leading to lower-rank outputs and diminished
representation capacity. Moreover, the steady-state solution of the model is
sensitive to input perturbations. We incorporate a
Proportional-Integral-Derivative (PID) closed-loop feedback control system with
a reference point into the model to improve robustness and representation
capacity. This integration aims to preserve high-frequency details while
bolstering model stability, rendering it more noise-resilient. The resulting
controlled state-space model is theoretically proven robust and adept at
addressing the rank collapse. Motivated by this control framework, we derive a
novel class of transformers, PID-controlled Transformer (PIDformer), aimed at
improving robustness and mitigating the rank-collapse issue inherent in softmax
transformers. We empirically evaluate the model for advantages and robustness
against baseline transformers across various practical tasks, including object
classification, image segmentation, and language modeling.
\\ ( https://arxiv.org/abs/2402.15989 ,  2235kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16269
Date: Mon, 26 Feb 2024 03:10:11 GMT   (1497kb,D)

Title: From Large Language Models and Optimization to Decision Optimization
  CoPilot: A Research Manifesto
Authors: Segev Wasserkrug, Leonard Boussioux, Dick den Hertog, Farzaneh
  Mirzazadeh, Ilker Birbil, Jannis Kurtz, Donato Maragno
Categories: cs.AI cs.LG math.OC
\\
  Significantly simplifying the creation of optimization models for real-world
business problems has long been a major goal in applying mathematical
optimization more widely to important business and societal decisions. The
recent capabilities of Large Language Models (LLMs) present a timely
opportunity to achieve this goal. Therefore, we propose research at the
intersection of LLMs and optimization to create a Decision Optimization CoPilot
(DOCP) - an AI tool designed to assist any decision maker, interacting in
natural language to grasp the business problem, subsequently formulating and
solving the corresponding optimization model. This paper outlines our DOCP
vision and identifies several fundamental requirements for its implementation.
We describe the state of the art through a literature survey and experiments
using ChatGPT. We show that a) LLMs already provide substantial novel
capabilities relevant to a DOCP, and b) major research challenges remain to be
addressed. We also propose possible research directions to overcome these gaps.
We also see this work as a call to action to bring together the LLM and
optimization communities to pursue our vision, thereby enabling much more
widespread improved decision-making.
\\ ( https://arxiv.org/abs/2402.16269 ,  1497kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16278
Date: Mon, 26 Feb 2024 03:46:01 GMT   (1253kb,D)

Title: A Self-matching Training Method with Annotation Embedding Models for
  Ontology Subsumption Prediction
Authors: Yukihiro Shiraishi, Ken Kaneiwa
Categories: cs.AI cs.CL cs.LG
Comments: 21 pages, 6 figures
\\
  Recently, ontology embeddings representing entities in a low-dimensional
space have been proposed for ontology completion. However, the ontology
embeddings for concept subsumption prediction do not address the difficulties
of similar and isolated entities and fail to extract the global information of
annotation axioms from an ontology. In this paper, we propose a self-matching
training method for the two ontology embedding models: Inverted-index Matrix
Embedding (InME) and Co-occurrence Matrix Embedding (CoME). The two embeddings
capture the global and local information in annotation axioms by means of the
occurring locations of each word in a set of axioms and the co-occurrences of
words in each axiom. The self-matching training method increases the robustness
of the concept subsumption prediction when predicted superclasses are similar
to subclasses and are isolated to other entities in an ontology. Our evaluation
experiments show that the self-matching training method with InME outperforms
the existing ontology embeddings for the GO and FoodOn ontologies and that the
method with the concatenation of CoME and OWL2Vec* outperforms them for the
HeLiS ontology.
\\ ( https://arxiv.org/abs/2402.16278 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16342
Date: Mon, 26 Feb 2024 06:42:30 GMT   (1580kb,D)

Title: Contingency Planning Using Bi-level Markov Decision Processes for Space
  Missions
Authors: Somrita Banerjee and Edward Balaban and Mark Shirley and Kevin Bradner
  and Marco Pavone
Categories: cs.AI cs.RO
\\
  This work focuses on autonomous contingency planning for scientific missions
by enabling rapid policy computation from any off-nominal point in the state
space in the event of a delay or deviation from the nominal mission plan.
Successful contingency planning involves managing risks and rewards, often
probabilistically associated with actions, in stochastic scenarios. Markov
Decision Processes (MDPs) are used to mathematically model decision-making in
such scenarios. However, in the specific case of planetary rover traverse
planning, the vast action space and long planning time horizon pose
computational challenges. A bi-level MDP framework is proposed to improve
computational tractability, while also aligning with existing mission planning
practices and enhancing explainability and trustworthiness of AI-driven
solutions. We discuss the conversion of a mission planning MDP into a bi-level
MDP, and test the framework on RoverGridWorld, a modified GridWorld environment
for rover mission planning. We demonstrate the computational tractability and
near-optimal policies achievable with the bi-level MDP approach, highlighting
the trade-offs between compute time and policy optimality as the problem's
complexity grows. This work facilitates more efficient and flexible contingency
planning in the context of scientific missions.
\\ ( https://arxiv.org/abs/2402.16342 ,  1580kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16482
Date: Mon, 26 Feb 2024 11:01:54 GMT   (2247kb)

Title: On Languaging a Simulation Engine
Authors: Han Liu, Liantang Li
Categories: cs.AI cs.CE cs.CL
\\
  Language model intelligence is revolutionizing the way we program materials
simulations. However, the diversity of simulation scenarios renders it
challenging to precisely transform human language into a tailored simulator.
Here, using three functionalized types of language model, we propose a
language-to-simulation (Lang2Sim) framework that enables interactive navigation
on languaging a simulation engine, by taking a scenario instance of water
sorption in porous matrices. Unlike line-by-line coding of a target simulator,
the language models interpret each simulator as an assembly of invariant tool
function and its variant input-output pair. Lang2Sim enables the precise
transform of textual description by functionalizing and sequentializing the
language models of, respectively, rationalizing the tool categorization,
customizing its input-output combinations, and distilling the simulator input
into executable format. Importantly, depending on its functionalized type, each
language model features a distinct processing of chat history to best balance
its memory limit and information completeness, thus leveraging the model
intelligence to unstructured nature of human request. Overall, this work
establishes language model as an intelligent platform to unlock the era of
languaging a simulation engine.
\\ ( https://arxiv.org/abs/2402.16482 ,  2247kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16505
Date: Mon, 26 Feb 2024 11:40:51 GMT   (725kb,D)

Title: Memory GAPS: Would LLM pass the Tulving Test?
Authors: Jean-Marie Chauvet
Categories: cs.AI
Comments: 15 pages, 3 figures
ACM-class: I.2.4
\\
  The Tulving Test was designed to investigate memory performance in
recognition and recall tasks. Its results help assess the relevance of the
"Synergistic Ecphory Model" of memory and similar RK paradigms in human
performance. This paper starts investigating whether the more than
forty-year-old framework sheds some light on LLMs' acts of remembering.
\\ ( https://arxiv.org/abs/2402.16505 ,  725kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16631
Date: Mon, 26 Feb 2024 15:03:46 GMT   (2328kb,D)

Title: GenAINet: Enabling Wireless Collective Intelligence via Knowledge
  Transfer and Reasoning
Authors: Hang Zou, Qiyang Zhao, Lina Bariah, Yu Tian, Mehdi Bennis, Samson
  Lasaulce, Merouane Debbah, Faouzi Bader
Categories: cs.AI cs.NI eess.SP
\\
  Generative artificial intelligence (GenAI) and communication networks are
expected to have groundbreaking synergies in 6G. Connecting GenAI agents over a
wireless network can potentially unleash the power of collective intelligence
and pave the way for artificial general intelligence (AGI). However, current
wireless networks are designed as a "data pipe" and are not suited to
accommodate and leverage the power of GenAI. In this paper, we propose the
GenAINet framework in which distributed GenAI agents communicate knowledge
(high-level concepts or abstracts) to accomplish arbitrary tasks. We first
provide a network architecture integrating GenAI capabilities to manage both
network protocols and applications. Building on this, we investigate effective
communication and reasoning problems by proposing a semantic-native GenAINet.
Specifically, GenAI agents extract semantic concepts from multi-modal raw data,
build a knowledgebase representing their semantic relations, which is retrieved
by GenAI models for planning and reasoning. Under this paradigm, an agent can
learn fast from other agents' experience for making better decisions with
efficient communications. Furthermore, we conduct two case studies where in
wireless device query, we show that extracting and transferring knowledge can
improve query accuracy with reduced communication; and in wireless power
control, we show that distributed agents can improve decisions via
collaborative reasoning. Finally, we address that developing a hierarchical
semantic level Telecom world model is a key path towards network of collective
intelligence.
\\ ( https://arxiv.org/abs/2402.16631 ,  2328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16651
Date: Mon, 26 Feb 2024 15:23:27 GMT   (567kb)

Title: A Comprehensive Survey of Belief Rule Base (BRB) Hybrid Expert system:
  Bridging Decision Science and Professional Services
Authors: Karim Derrick
Categories: cs.AI cs.CY
\\
  The Belief Rule Base (BRB) system that adopts a hybrid approach integrating
the precision of expert systems with the adaptability of data-driven models.
Characterized by its use of if-then rules to accommodate various types of
uncertainty through belief degrees, BRB adeptly handles fuzziness, randomness,
and ignorance. This semi-quantitative tool excels in processing both numerical
data and linguistic knowledge from diverse sources, making it as an
indispensable resource in modelling complex nonlinear systems. Notably, BRB's
transparent, white-box nature ensures accessibility and clarity for
decision-makers and stakeholders, further enhancing its applicability. With its
growing adoption in fields ranging from decision-making and reliability
evaluation in network security and fault diagnosis, this study aims to explore
the evolution and the multifaceted applications of BRB. By analysing its
development across different domains, we highlight BRB's potential to
revolutionize sectors traditionally resistant to technological disruption, in
particular insurance and law.
\\ ( https://arxiv.org/abs/2402.16651 ,  567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16654
Date: Mon, 26 Feb 2024 15:26:56 GMT   (4063kb,D)

Title: GigaPevt: Multimodal Medical Assistant
Authors: Pavel Blinov, Konstantin Egorov, Ivan Sviridov, Nikolay Ivanov, Stepan
  Botman, Evgeniy Tagin, Stepan Kudin, Galina Zubkova, Andrey Savchenko
Categories: cs.AI cs.CL cs.HC
Comments: 4 pages, 2 figures, 2 tables
MSC-class: 68T07
ACM-class: I.2.1
\\
  Building an intelligent and efficient medical assistant is still a
challenging AI problem. The major limitation comes from the data modality
scarceness, which reduces comprehensive patient perception. This demo paper
presents the GigaPevt, the first multimodal medical assistant that combines the
dialog capabilities of large language models with specialized medical models.
Such an approach shows immediate advantages in dialog quality and metric
performance, with a 1.18\% accuracy improvement in the question-answering task.
\\ ( https://arxiv.org/abs/2402.16654 ,  4063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16751
Date: Mon, 26 Feb 2024 17:16:28 GMT   (109kb,D)

Title: Value Preferences Estimation and Disambiguation in Hybrid Participatory
  Systems
Authors: Enrico Liscio, Luciano C. Siebert, Catholijn M. Jonker, Pradeep K.
  Murukannaiah
Categories: cs.AI cs.CL
Comments: Under review
\\
  Understanding citizens' values in participatory systems is crucial for
citizen-centric policy-making. We envision a hybrid participatory system where
participants make choices and provide motivations for those choices, and AI
agents estimate their value preferences by interacting with them. We focus on
situations where a conflict is detected between participants' choices and
motivations, and propose methods for estimating value preferences while
addressing detected inconsistencies by interacting with the participants. We
operationalize the philosophical stance that "valuing is deliberatively
consequential." That is, if a participant's choice is based on a deliberation
of value preferences, the value preferences can be observed in the motivation
the participant provides for the choice. Thus, we propose and compare value
estimation methods that prioritize the values estimated from motivations over
the values estimated from choices alone. Then, we introduce a disambiguation
strategy that addresses the detected inconsistencies between choices and
motivations by directly interacting with the participants. We evaluate the
proposed methods on a dataset of a large-scale survey on energy transition. The
results show that explicitly addressing inconsistencies between choices and
motivations improves the estimation of an individual's value preferences. The
disambiguation strategy does not show substantial improvements when compared to
similar baselines--however, we discuss how the novelty of the approach can open
new research avenues and propose improvements to address the current
limitations.
\\ ( https://arxiv.org/abs/2402.16751 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16823
Date: Mon, 26 Feb 2024 18:48:27 GMT   (2902kb,D)

Title: Language Agents as Optimizable Graphs
Authors: Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii
  Khizbullin and Jurgen Schmidhuber
Categories: cs.AI cs.CL cs.LG cs.MA
Comments: Project Website: https://gptswarm.org ; Github Repo:
  https://github.com/metauto-ai/gptswarm
\\
  Various human-designed prompt engineering techniques have been proposed to
improve problem solvers based on Large Language Models (LLMs), yielding many
disparate code bases. We unify these approaches by describing LLM-based agents
as computational graphs. The nodes implement functions to process multimodal
data or query LLMs, and the edges describe the information flow between
operations. Graphs can be recursively combined into larger composite graphs
representing hierarchies of inter-agent collaboration (where edges connect
operations of different agents). Our novel automatic graph optimizers (1)
refine node-level LLM prompts (node optimization) and (2) improve agent
orchestration by changing graph connectivity (edge optimization). Experiments
demonstrate that our framework can be used to efficiently develop, integrate,
and automatically improve various LLM agents. The code can be found at
https://github.com/metauto-ai/gptswarm.
\\ ( https://arxiv.org/abs/2402.16823 ,  2902kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15514
Date: Wed, 31 Jan 2024 22:47:01 GMT   (3739kb)

Title: Large Scale Generative AI Text Applied to Sports and Music
Authors: Aaron Baughman, Stephen Hammer, Rahul Agarwal, Gozde Akay, Eduardo
  Morales, Tony Johnson, Leonid Karlinsky, Rogerio Feris
Categories: cs.CL cs.AI
Comments: 9 pages, 8 figures, 5 tables
\\
  We address the problem of scaling up the production of media content,
including commentary and personalized news stories, for large-scale sports and
music events worldwide. Our approach relies on generative AI models to
transform a large volume of multimodal data (e.g., videos, articles, real-time
scoring feeds, statistics, and fact sheets) into coherent and fluent text.
Based on this approach, we introduce, for the first time, an AI commentary
system, which was deployed to produce automated narrations for highlight
packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same
vein, our solution was extended to create personalized content for ESPN Fantasy
Football and stories about music artists for the Grammy awards. These
applications were built using a common software architecture achieved a 15x
speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our
work was successfully deployed at the aforementioned events, supporting 90
million fans around the world with 8 billion page views, continuously pushing
the bounds on what is possible at the intersection of sports, entertainment,
and AI.
\\ ( https://arxiv.org/abs/2402.15514 ,  3739kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15518
Date: Sun, 11 Feb 2024 13:41:17 GMT   (19839kb,D)

Title: Beware of Words: Evaluating the Lexical Richness of Conversational Large
  Language Models
Authors: Gonzalo Mart\'inez, Jos\'e Alberto Hern\'andez, Javier Conde, Pedro
  Reviriego and Elena Merino
Categories: cs.CL
\\
  The performance of conversational Large Language Models (LLMs) in general,
and of ChatGPT in particular, is currently being evaluated on many different
tasks, from logical reasoning or maths to answering questions on a myriad of
topics. Instead, much less attention is being devoted to the study of the
linguistic features of the texts generated by these LLMs. This is surprising
since LLMs are models for language, and understanding how they use the language
is important. Indeed, conversational LLMs are poised to have a significant
impact on the evolution of languages as they may eventually dominate the
creation of new text. This means that for example, if conversational LLMs do
not use a word it may become less and less frequent and eventually stop being
used altogether. Therefore, evaluating the linguistic features of the text they
produce and how those depend on the model parameters is the first step toward
understanding the potential impact of conversational LLMs on the evolution of
languages. In this paper, we consider the evaluation of the lexical richness of
the text generated by LLMs and how it depends on the model parameters. A
methodology is presented and used to conduct a comprehensive evaluation of
lexical richness using ChatGPT as a case study. The results show how lexical
richness depends on the version of ChatGPT and some of its parameters, such as
the presence penalty, or on the role assigned to the model. The dataset and
tools used in our analysis are released under open licenses with the goal of
drawing the much-needed attention to the evaluation of the linguistic features
of LLM-generated text.
\\ ( https://arxiv.org/abs/2402.15518 ,  19839kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15525
Date: Mon, 19 Feb 2024 21:50:42 GMT   (115kb,D)

Title: Detecting misinformation through Framing Theory: the Frame Element-based
  Model
Authors: Guan Wang, Rebecca Frederick, Jinglong Duan, William Wong, Verica
  Rupar, Weihua Li, and Quan Bai
Categories: cs.CL cs.CY
Comments: 17 pages, 9 figures, 7 tables
\\
  In this paper, we delve into the rapidly evolving challenge of misinformation
detection, with a specific focus on the nuanced manipulation of narrative
frames - an under-explored area within the AI community. The potential for
Generative AI models to generate misleading narratives underscores the urgency
of this problem. Drawing from communication and framing theories, we posit that
the presentation or 'framing' of accurate information can dramatically alter
its interpretation, potentially leading to misinformation. We highlight this
issue through real-world examples, demonstrating how shifts in narrative frames
can transmute fact-based information into misinformation. To tackle this
challenge, we propose an innovative approach leveraging the power of
pre-trained Large Language Models and deep neural networks to detect
misinformation originating from accurate facts portrayed under different
frames. These advanced AI techniques offer unprecedented capabilities in
identifying complex patterns within unstructured data critical for examining
the subtleties of narrative frames. The objective of this paper is to bridge a
significant research gap in the AI domain, providing valuable insights and
methodologies for tackling framing-induced misinformation, thus contributing to
the advancement of responsible and trustworthy AI technologies. Several
experiments are intensively conducted and experimental results explicitly
demonstrate the various impact of elements of framing theory proving the
rationale of applying framing theory to increase the performance in
misinformation detection.
\\ ( https://arxiv.org/abs/2402.15525 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15527
Date: Wed, 21 Feb 2024 07:09:58 GMT   (17368kb,D)

Title: PCA-Bench: Evaluating Multimodal Large Language Models in
  Perception-Cognition-Action Chain
Authors: Liang Chen and Yichi Zhang and Shuhuai Ren and Haozhe Zhao and Zefan
  Cai and Yuchi Wang and Peiyi Wang and Xiangdi Meng and Tianyu Liu and Baobao
  Chang
Categories: cs.CL cs.AI cs.CV
Comments: Code and Data released at https://github.com/pkunlp-icler/PCA-EVAL.
  Leaderboard at: https://docs.qq.com/sheet/DVUd4WUpGRHRqUnNV. This article
  supersedes its workshop version arxiv: 2310.02071. arXiv admin note: text
  overlap with arXiv:2310.02071
\\
  We present PCA-Bench, a multimodal decision-making benchmark for evaluating
the integrated capabilities of Multimodal Large Language Models (MLLMs).
Departing from previous benchmarks focusing on simplistic tasks and individual
model capability, PCA-Bench introduces three complex scenarios: autonomous
driving, domestic robotics, and open-world games. Given task instructions and
diverse contexts, the model is required to seamlessly integrate multiple
capabilities of Perception, Cognition, and Action in a reasoning chain to make
accurate decisions. Moreover, PCA-Bench features error localization
capabilities, scrutinizing model inaccuracies in areas such as perception,
knowledge, or reasoning. This enhances the reliability of deploying MLLMs. To
balance accuracy and efficiency in evaluation, we propose PCA-Eval, an
automatic evaluation protocol, and assess 10 prevalent MLLMs. The results
reveal significant performance disparities between open-source models and
powerful proprietary models like GPT-4 Vision. To address this, we introduce
Embodied-Instruction-Evolution (EIE), an automatic framework for synthesizing
instruction tuning examples in multimodal embodied environments. EIE generates
7,510 training examples in PCA-Bench and enhances the performance of
open-source MLLMs, occasionally surpassing GPT-4 Vision (+3\% in decision
accuracy), thereby validating the effectiveness of EIE. Our findings suggest
that robust MLLMs like GPT4-Vision show promise for decision-making in embodied
agents, opening new avenues for MLLM research.
\\ ( https://arxiv.org/abs/2402.15527 ,  17368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15537
Date: Fri, 23 Feb 2024 04:52:08 GMT   (1087kb,D)

Title: Evaluating the Performance of ChatGPT for Spam Email Detection
Authors: Yuwei Wu, Shijing Si, Yugui Zhang, Jiawen Gu, Jedrek Wosik
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: Technical report and analysis
\\
  Email continues to be a pivotal and extensively utilized communication medium
within professional and commercial domains. Nonetheless, the prevalence of spam
emails poses a significant challenge for users, disrupting their daily routines
and diminishing productivity. Consequently, accurately identifying and
filtering spam based on content has become crucial for cybersecurity. Recent
advancements in natural language processing, particularly with large language
models like ChatGPT, have shown remarkable performance in tasks such as
question answering and text generation. However, its potential in spam
identification remains underexplored. To fill in the gap, this study attempts
to evaluate ChatGPT's capabilities for spam identification in both English and
Chinese email datasets. We employ ChatGPT for spam email detection using
in-context learning, which requires a prompt instruction and a few
demonstrations. We also investigate how the training example size affects the
performance of ChatGPT. For comparison, we also implement five popular
benchmark methods, including naive Bayes, support vector machines (SVM),
logistic regression (LR), feedforward dense neural networks (DNN), and BERT
classifiers. Though extensive experiments, the performance of ChatGPT is
significantly worse than deep supervised learning methods in the large English
dataset, while it presents superior performance on the low-resourced Chinese
dataset, even outperforming BERT in this case.
\\ ( https://arxiv.org/abs/2402.15537 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15589
Date: Fri, 23 Feb 2024 20:14:16 GMT   (4725kb,D)

Title: Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
  of Scholarly Manuscripts
Authors: Shubhra Kanti Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal, Alex
  Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi
  Akter, Matthew Freestone, Matthew C. Williams Jr
Categories: cs.CL cs.AI cs.LG cs.NE
ACM-class: I.2.7
\\
  One of the most important yet onerous tasks in the academic peer-reviewing
process is composing meta-reviews, which involves understanding the core
contributions, strengths, and weaknesses of a scholarly manuscript based on
peer-review narratives from multiple experts and then summarizing those
multiple experts' perspectives into a concise holistic overview. Given the
latest major developments in generative AI, especially Large Language Models
(LLMs), it is very compelling to rigorously study the utility of LLMs in
generating such meta-reviews in an academic peer-review setting. In this paper,
we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and
PaLM2, to automatically generate meta-reviews by prompting them with different
types/levels of prompts based on the recently proposed TELeR taxonomy. Finally,
we perform a detailed qualitative study of the meta-reviews generated by the
LLMs and summarize our findings and recommendations for prompting LLMs for this
complex task.
\\ ( https://arxiv.org/abs/2402.15589 ,  4725kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15594
Date: Fri, 23 Feb 2024 20:26:54 GMT   (813kb,D)

Title: Alternating Weak Triphone/BPE Alignment Supervision from Hybrid Model
  Improves End-to-End ASR
Authors: Jintao Jiang, Yingbo Gao, Mohammad Zeineldeen, Zoltan Tuske
Categories: cs.CL cs.SD eess.AS
Comments: 5 pages, 1 figure, 3 tables
\\
  In this paper, alternating weak triphone/BPE alignment supervision is
proposed to improve end-to-end model training. Towards this end, triphone and
BPE alignments are extracted using a pre-existing hybrid ASR system. Then,
regularization effect is obtained by cross-entropy based intermediate auxiliary
losses computed on such alignments at a mid-layer representation of the encoder
for triphone alignments and at the encoder for BPE alignments. Weak supervision
is achieved through strong label smoothing with parameter of 0.5. Experimental
results on TED-LIUM 2 indicate that either triphone or BPE alignment based weak
supervision improves ASR performance over standard CTC auxiliary loss.
Moreover, their combination lowers the word error rate further. We also
investigate the alternation of the two auxiliary tasks during model training,
and additional performance gain is observed. Overall, the proposed techniques
result in over 10% relative error rate reduction over a CTC-regularized
baseline system.
\\ ( https://arxiv.org/abs/2402.15594 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15610
Date: Fri, 23 Feb 2024 21:16:52 GMT   (4454kb,D)

Title: Selective "Selective Prediction": Reducing Unnecessary Abstention in
  Vision-Language Reasoning
Authors: Tejas Srinivasan, Jack Hessel, Tanmay Gupta, Bill Yuchen Lin, Yejin
  Choi, Jesse Thomason, Khyathi Raghavi Chandu
Categories: cs.CL
\\
  Prior work on selective prediction minimizes incorrect predictions from
vision-language models (VLMs) by allowing them to abstain from answering when
uncertain. However, when deploying a vision-language system with low tolerance
for inaccurate predictions, selective prediction may be over-cautious and
abstain too frequently, even on many correct predictions. We introduce
ReCoVERR, an inference-time algorithm to reduce the over-abstention of a
selective vision-language system without decreasing prediction accuracy. When
the VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries
to find relevant clues in the image that provide additional evidence for the
prediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects
high-confidence evidences, and if enough evidence confirms the prediction the
system makes a prediction instead of abstaining. ReCoVERR enables two VLMs,
BLIP2 and InstructBLIP, to answer up to 20% more questions on the A-OKVQA task
than vanilla selective prediction without decreasing system accuracy, thus
improving overall system reliability.
\\ ( https://arxiv.org/abs/2402.15610 ,  4454kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15623
Date: Fri, 23 Feb 2024 21:58:50 GMT   (1073kb,D)

Title: Language-Based User Profiles for Recommendation
Authors: Joyce Zhou, Yijia Dai, Thorsten Joachims
Categories: cs.CL cs.HC cs.IR cs.LG
Comments: 8 pages (4 in appendix), 22 tables/figures (16 in appendix). Accepted
  to LLM-IGS@WSDM2024 workshop, now sharing this slightly updated revision
  version with workshop
\\
  Most conventional recommendation methods (e.g., matrix factorization)
represent user profiles as high-dimensional vectors. Unfortunately, these
vectors lack interpretability and steerability, and often perform poorly in
cold-start settings. To address these shortcomings, we explore the use of user
profiles that are represented as human-readable text. We propose the
Language-based Factorization Model (LFM), which is essentially an
encoder/decoder model where both the encoder and the decoder are large language
models (LLMs). The encoder LLM generates a compact natural-language profile of
the user's interests from the user's rating history. The decoder LLM uses this
summary profile to complete predictive downstream tasks. We evaluate our LFM
approach on the MovieLens dataset, comparing it against matrix factorization
and an LLM model that directly predicts from the user's rating history. In
cold-start settings, we find that our method can have higher accuracy than
matrix factorization. Furthermore, we find that generating a compact and
human-readable summary often performs comparably with or better than direct LLM
prediction, while enjoying better interpretability and shorter model input
length. Our results motivate a number of future research directions and
potential improvements.
\\ ( https://arxiv.org/abs/2402.15623 ,  1073kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15631
Date: Fri, 23 Feb 2024 22:24:40 GMT   (2072kb,D)

Title: Fine-Grained Self-Endorsement Improves Factuality and Reasoning
Authors: Ante Wang, Linfeng Song, Baolin Peng, Ye Tian, Lifeng Jin, Haitao Mi,
  Jinsong Su and Dong Yu
Categories: cs.CL cs.AI
\\
  This work studies improving large language model (LLM) generations at
inference time by mitigating fact-conflicting hallucinations. Particularly, we
propose a self-endorsement framework that leverages the fine-grained fact-level
comparisons across multiple sampled responses. Compared with prior ensemble
methods (Wang et al., 2022;Chen et al., 2023)) that perform response-level
selection, our approach can better alleviate hallucinations, especially for
longform generation tasks. Our approach can broadly benefit smaller and
open-source LLMs as it mainly conducts simple content-based comparisons.
Experiments on Biographies show that our method can effectively improve the
factuality of generations with simple and intuitive prompts across different
scales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K
demonstrate the potential of self-endorsement for broader application.
\\ ( https://arxiv.org/abs/2402.15631 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15637
Date: Fri, 23 Feb 2024 22:39:12 GMT   (248kb,D)

Title: Addressing Order Sensitivity of In-Context Demonstration Examples in
  Causal Language Models
Authors: Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan He
Categories: cs.CL
\\
  In-context learning has become a popular paradigm in natural language
processing. However, its performance can be significantly influenced by the
order of in-context demonstration examples. In this paper, we found that causal
language models (CausalLMs) are more sensitive to this order compared to prefix
language models (PrefixLMs). We attribute this phenomenon to the
auto-regressive attention masks within CausalLMs, which restrict each token
from accessing information from subsequent tokens. This results in different
receptive fields for samples at different positions, thereby leading to
representation disparities across positions. To tackle this challenge, we
introduce an unsupervised fine-tuning method, termed the Information-Augmented
and Consistency-Enhanced approach. This approach utilizes contrastive learning
to align representations of in-context examples across different positions and
introduces a consistency loss to ensure similar representations for inputs with
different permutations. This enhances the model's predictive consistency across
permutations. Experimental results on four benchmarks suggest that our proposed
method can reduce the sensitivity to the order of in-context examples and
exhibit robust generalizability, particularly when demonstrations are sourced
from a pool different from that used in the training phase, or when the number
of in-context examples differs from what is used during training.
\\ ( https://arxiv.org/abs/2402.15637 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15654
Date: Sat, 24 Feb 2024 00:01:01 GMT   (12702kb,D)

Title: Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics
Authors: Sadaf Ghaffari, Nikhil Krishnaswamy
Categories: cs.CL
Comments: 10 pages, 10 figures, Proceedings of AAAI Spring Symposium:
  Empowering Machine Learning and Large Language Models with Domain and
  Commonsense Knowledge (MAKE). AAAI (2024)
\\
  In this paper, we present an exploration of LLMs' abilities to problem solve
with physical reasoning in situated environments. We construct a simple
simulated environment and demonstrate examples of where, in a zero-shot
setting, both text and multimodal LLMs display atomic world knowledge about
various objects but fail to compose this knowledge in correct solutions for an
object manipulation and placement task. We also use BLIP, a vision-language
model trained with more sophisticated cross-modal attention, to identify cases
relevant to object physical properties that that model fails to ground.
Finally, we present a procedure for discovering the relevant properties of
objects in the environment and propose a method to distill this knowledge back
into the LLM.
\\ ( https://arxiv.org/abs/2402.15654 ,  12702kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15663
Date: Sat, 24 Feb 2024 00:38:29 GMT   (8087kb,D)

Title: Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical
  Study
Authors: Zhaoyue Sun, Gabriele Pergola, Byron C. Wallace and Yulan He
Categories: cs.CL
Comments: 14 pages, 2 figures, accepted by EACL 2024
\\
  With the advent of large language models (LLMs), there has been growing
interest in exploring their potential for medical applications. This research
aims to investigate the ability of LLMs, specifically ChatGPT, in the context
of pharmacovigilance event extraction, of which the main goal is to identify
and extract adverse events or potential therapeutic events from textual medical
sources. We conduct extensive experiments to assess the performance of ChatGPT
in the pharmacovigilance event extraction task, employing various prompts and
demonstration selection strategies. The findings demonstrate that while ChatGPT
demonstrates reasonable performance with appropriate demonstration selection
strategies, it still falls short compared to fully fine-tuned small models.
Additionally, we explore the potential of leveraging ChatGPT for data
augmentation. However, our investigation reveals that the inclusion of
synthesized data into fine-tuning may lead to a decrease in performance,
possibly attributed to noise in the ChatGPT-generated labels. To mitigate this,
we explore different filtering strategies and find that, with the proper
approach, more stable performance can be achieved, although constant
improvement remains elusive.
\\ ( https://arxiv.org/abs/2402.15663 ,  8087kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15690
Date: Sat, 24 Feb 2024 02:27:55 GMT   (1221kb,D)

Title: Foot In The Door: Understanding Large Language Model Jailbreaking via
  Cognitive Psychology
Authors: Zhenhua Wang, Wei Xie, Baosheng Wang, Enze Wang, Zhiwen Gui,
  Shuoyoucheng Ma, Kai Chen
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have gradually become the gateway for people to
acquire new knowledge. However, attackers can break the model's security
protection ("jail") to access restricted information, which is called
"jailbreaking." Previous studies have shown the weakness of current LLMs when
confronted with such jailbreaking attacks. Nevertheless, comprehension of the
intrinsic decision-making mechanism within the LLMs upon receipt of jailbreak
prompts is noticeably lacking. Our research provides a psychological
explanation of the jailbreak prompts. Drawing on cognitive consistency theory,
we argue that the key to jailbreak is guiding the LLM to achieve cognitive
coordination in an erroneous direction. Further, we propose an automatic
black-box jailbreaking method based on the Foot-in-the-Door (FITD) technique.
This method progressively induces the model to answer harmful questions via
multi-step incremental prompts. We instantiated a prototype system to evaluate
the jailbreaking effectiveness on 8 advanced LLMs, yielding an average success
rate of 83.9%. This study builds a psychological perspective on the explanatory
insights into the intrinsic decision-making logic of LLMs.
\\ ( https://arxiv.org/abs/2402.15690 ,  1221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15708
Date: Sat, 24 Feb 2024 04:08:51 GMT   (3507kb,D)

Title: Query Augmentation by Decoding Semantics from Brain Signals
Authors: Ziyi Ye, Jingtao Zhan, Qingyao Ai, Yiqun Liu, Maarten de Rijke,
  Christina Lioma, Tuukka Ruotsalo
Categories: cs.CL cs.AI cs.IR
\\
  Query augmentation is a crucial technique for refining semantically imprecise
queries. Traditionally, query augmentation relies on extracting information
from initially retrieved, potentially relevant documents. If the quality of the
initially retrieved documents is low, then the effectiveness of query
augmentation would be limited as well. We propose Brain-Aug, which enhances a
query by incorporating semantic information decoded from brain signals.
BrainAug generates the continuation of the original query with a prompt
constructed with brain signal information and a ranking-oriented inference
approach. Experimental results on fMRI (functional magnetic resonance imaging)
datasets show that Brain-Aug produces semantically more accurate queries,
leading to improved document ranking performance. Such improvement brought by
brain signals is particularly notable for ambiguous queries.
\\ ( https://arxiv.org/abs/2402.15708 ,  3507kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15713
Date: Sat, 24 Feb 2024 04:32:44 GMT   (251kb,D)

Title: Making Pre-trained Language Models Better Continual Few-Shot Relation
  Extractors
Authors: Shengkun Ma, Jiale Han, Yi Liang, Bo Cheng
Categories: cs.CL cs.AI
Comments: Accepted as COLING2024
\\
  Continual Few-shot Relation Extraction (CFRE) is a practical problem that
requires the model to continuously learn novel relations while avoiding
forgetting old ones with few labeled training data. The primary challenges are
catastrophic forgetting and overfitting. This paper harnesses prompt learning
to explore the implicit capabilities of pre-trained language models to address
the above two challenges, thereby making language models better continual
few-shot relation extractors. Specifically, we propose a Contrastive Prompt
Learning framework, which designs prompt representation to acquire more
generalized knowledge that can be easily adapted to old and new categories, and
margin-based contrastive learning to focus more on hard samples, therefore
alleviating catastrophic forgetting and overfitting issues. To further remedy
overfitting in low-resource scenarios, we introduce an effective memory
augmentation strategy that employs well-crafted prompts to guide ChatGPT in
generating diverse samples. Extensive experiments demonstrate that our method
outperforms state-of-the-art methods by a large margin and significantly
mitigates catastrophic forgetting and overfitting in low-resource scenarios.
\\ ( https://arxiv.org/abs/2402.15713 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15745
Date: Sat, 24 Feb 2024 06:57:15 GMT   (2891kb,D)

Title: GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models
  Evaluation
Authors: Yi Zong, Xipeng Qiu
Categories: cs.CL cs.AI cs.CV
\\
  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in
image perception and language understanding. However, existing multimodal
benchmarks focus on primary perception abilities and commonsense knowledge
which are insufficient to reflect the comprehensive capabilities of LVLMs. We
propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance
Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as
diagrams, function graphs, maps and photos. GAOKAO-MM derives from native
Chinese context and sets human-level requirements for the model's abilities,
including perception, understanding, knowledge and reasoning. We evaluate 10
LVLMs and find that the accuracies of all of them are lower than 50%, with
GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking
in the top three positions. The results of our multi-dimension analysis
indicate that LVLMs have moderate distance towards Artificial General
Intelligence (AGI) and provide insights facilitating the development of
multilingual LVLMs.
\\ ( https://arxiv.org/abs/2402.15745 ,  2891kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15754
Date: Sat, 24 Feb 2024 08:01:32 GMT   (8277kb,D)

Title: HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical
  Criteria Decomposition
Authors: Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang,
  Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang
Categories: cs.CL
Comments: 20 pages, 13 figures
\\
  Large language models (LLMs) have emerged as a promising alternative to
expensive human evaluations. However, the alignment and coverage of LLM-based
evaluations are often limited by the scope and potential bias of the evaluation
prompts and criteria. To address this challenge, we propose HD-Eval, a novel
framework that iteratively aligns LLM-based evaluators with human preference
via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the
evaluation mindset of human experts and enhances the alignment of LLM-based
evaluators by decomposing a given evaluation task into finer-grained criteria,
aggregating them according to estimated human preferences, pruning
insignificant criteria with attribution, and further decomposing significant
criteria. By integrating these steps within an iterative alignment training
process, we obtain a hierarchical decomposition of criteria that
comprehensively captures aspects of natural language at multiple levels of
granularity. Implemented as a white box, the human preference-guided aggregator
is efficient to train and more explainable than relying solely on prompting,
and its independence from model parameters makes it applicable to closed-source
LLMs. Extensive experiments on three evaluation domains demonstrate the
superiority of HD-Eval in further aligning state-of-the-art evaluators and
providing deeper insights into the explanation of evaluation results and the
task itself.
\\ ( https://arxiv.org/abs/2402.15754 ,  8277kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15755
Date: Sat, 24 Feb 2024 08:02:19 GMT   (417kb)

Title: Dental Severity Assessment through Few-shot Learning and SBERT
  Fine-tuning
Authors: Mohammad Dehghani
Categories: cs.CL
\\
  Dental diseases have a significant impact on a considerable portion of the
population, leading to various health issues that can detrimentally affect
individuals' overall well-being. The integration of automated systems in oral
healthcare has become increasingly crucial. Machine learning approaches offer a
viable solution to address challenges such as diagnostic difficulties,
inefficiencies, and errors in oral disease diagnosis. These methods prove
particularly useful when physicians struggle to predict or diagnose diseases at
their early stages. In this study, thirteen different machine learning, deep
learning, and large language models were employed to determine the severity
level of oral health issues based on radiologists' reports. The results
revealed that the Few-shot learning with SBERT and Multi-Layer Perceptron model
outperformed all other models across various experiments, achieving an
impressive accuracy of 94.1% as the best result. Consequently, this model
exhibits promise as a reliable tool for evaluating the severity of oral
diseases, enabling patients to receive more effective treatment and aiding
healthcare professionals in making informed decisions regarding resource
allocation and the management of high-risk patients.
\\ ( https://arxiv.org/abs/2402.15755 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15758
Date: Sat, 24 Feb 2024 08:10:39 GMT   (8654kb,D)

Title: Chimera: A Lossless Decoding Method for Accelerating Large Language
  Models Inference by Fusing all Tokens
Authors: Ziqian Zeng, Jiahong Yu, Qianshi Pang, Zihao Wang, Huiping Zhuang, Cen
  Chen
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have demonstrated remarkable capabilities across
various tasks. However, their widespread application is hindered by the
resource-intensive decoding process. To address this challenge, current
approaches have incorporated additional decoding heads to enable parallel
prediction of multiple subsequent tokens, thereby achieving inference
acceleration. Nevertheless, the accuracy of these decoding heads falls short of
the auto-regressive decoding approach.
  In light of these limitations, we propose Chimera, a novel framework
specifically designed for speculative sampling. Within this framework, we
introduce a lightweight draft model that effectively utilizes previously
generated tokens to predict subsequent words. To ensure both accuracy and
efficiency, we present two strategies within the lightweight draft model.
Firstly, we focus on capturing short-range dependencies at the bottom layer.
Secondly, we leverage the readily available representations from the original
LLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimera
demonstrates impressive results, achieving an average latency speedup ratio of
2.7x compared to the vanilla auto-regressive decoding approach. This highlights
the potential of our proposed framework in significantly improving the
efficiency of large language models during the decoding process.
\\ ( https://arxiv.org/abs/2402.15758 ,  8654kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15764
Date: Sat, 24 Feb 2024 08:40:30 GMT   (757kb,D)

Title: Look Before You Leap: Problem Elaboration Prompting Improves
  Mathematical Reasoning in Large Language Models
Authors: Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
Categories: cs.CL cs.AI
\\
  Large language models~(LLMs) have exhibited impressive performance across NLP
tasks. So far they still face challenges in complex reasoning tasks and can be
sensitive to input context. Despite significant efforts have been invested in
enhancing reasoning process and improving prefix-prompts robustness, the
crucial role of problem context has been overlooked. In this study, we propose
a new approach to improve the mathematical capacities of LLMs, named Problem
Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the
problem context before reasoning, thus enhancing the global context modeling
and reducing the parsing difficulties. Experiments on datasets demonstrate
promising performances on complex reasoning and indicate the beneficial impact
for ill-formed problems. For instance, with the GPT-3.5
model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy
decoding and 8.80\% improvement with self-consistency on GSM8k compared to the
standard CoT. With ChatGPT~(\texttt{turbo}) and PEP, we achieve SOTA
performances on SVAMP with 86.2\% and GSM8k with 90.98\%.
\\ ( https://arxiv.org/abs/2402.15764 ,  757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15813
Date: Sat, 24 Feb 2024 13:36:58 GMT   (468kb,D)

Title: Measuring Bargaining Abilities of LLMs: A Benchmark and A
  Buyer-Enhancement Method
Authors: Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang,
  Rui Wang
Categories: cs.CL cs.GT
Comments: The dataset AmazonHistoryPrice and our code are available at
  https://github.com/TianXiaSJTU/AmazonPriceHistory
\\
  Bargaining is an important and unique part of negotiation between humans. As
LLM-driven agents learn to negotiate and act like real humans, how to evaluate
agents' bargaining abilities remains an open problem. For the first time, we
formally described the Bargaining task as an asymmetric incomplete information
game, defining the gains of the Buyer and Seller in multiple bargaining
processes. It allows us to quantitatively assess an agent's performance in the
Bargain task. We collected a real product price dataset, AmazonHistoryPrice,
and conducted evaluations of various LLM agents' bargaining abilities. We find
that playing a Buyer is much harder than a Seller, and increasing model size
can not effectively improve the Buyer's performance. To address the challenge,
we propose a novel approach called OG-Narrator that integrates a deterministic
Offer Generator to control the price range of Buyer's offers, and an LLM
Narrator to create natural language sentences for generated offers.
Experimental results show that OG-Narrator improves the buyer's deal rates from
26.67% to 88.88% and brings a ten times of multiplication of profits on all
baselines, even a model that has not been aligned.
\\ ( https://arxiv.org/abs/2402.15813 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15814
Date: Sat, 24 Feb 2024 13:42:06 GMT   (7096kb,D)

Title: A Theoretical Result on the Inductive Bias of RNN Language Models
Authors: Anej Svete, Robin Shing Moon Chan, Ryan Cotterell
Categories: cs.CL cs.CC cs.LG
\\
  Recent work by Hewitt et al. (2020) provides a possible interpretation of the
empirical success of recurrent neural networks (RNNs) as language models (LMs).
  It shows that RNNs can efficiently represent bounded hierarchical structures
that are prevalent in human language.
  This suggests that RNNs' success might be linked to their ability to model
hierarchy.
  However, a closer inspection of Hewitt et al.'s (2020) construction shows
that it is not limited to hierarchical LMs, posing the question of what
\emph{other classes} of LMs can be efficiently represented by RNNs.
  To this end, we generalize their construction to show that RNNs can
efficiently represent a larger class of LMs: Those that can be represented by a
pushdown automaton with a bounded stack and a generalized stack update
function.
  This is analogous to an automaton that keeps a memory of a fixed number of
symbols and updates the memory with a simple update mechanism.
  Altogether, the efficiency in representing a diverse class of
non-hierarchical LMs posits a lack of concrete cognitive and
human-language-centered inductive biases in RNNs.
\\ ( https://arxiv.org/abs/2402.15814 ,  7096kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15818
Date: Sat, 24 Feb 2024 14:01:07 GMT   (52kb,D)

Title: Linguistic Intelligence in Large Language Models for Telecommunications
Authors: Tasnim Ahmed, Nicola Piovesan, Antonio De Domenico, Salimur Choudhury
Categories: cs.CL
\\
  Large Language Models (LLMs) have emerged as a significant advancement in the
field of Natural Language Processing (NLP), demonstrating remarkable
capabilities in language generation and other language-centric tasks. Despite
their evaluation across a multitude of analytical and reasoning tasks in
various scientific domains, a comprehensive exploration of their knowledge and
understanding within the realm of natural language tasks in the
telecommunications domain is still needed. This study, therefore, seeks to
evaluate the knowledge and understanding capabilities of LLMs within this
domain. To achieve this, we conduct an exhaustive zero-shot evaluation of four
prominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer
resources than ChatGPT, making them suitable for resource-constrained
environments. Their performance is compared with state-of-the-art, fine-tuned
models. To the best of our knowledge, this is the first work to extensively
evaluate and compare the understanding of LLMs across multiple language-centric
tasks in this domain. Our evaluation reveals that zero-shot LLMs can achieve
performance levels comparable to the current state-of-the-art fine-tuned
models. This indicates that pretraining on extensive text corpora equips LLMs
with a degree of specialization, even within the telecommunications domain. We
also observe that no single LLM consistently outperforms others, and the
performance of different LLMs can fluctuate. Although their performance lags
behind fine-tuned models, our findings underscore the potential of LLMs as a
valuable resource for understanding various aspects of this field that lack
large annotated data.
\\ ( https://arxiv.org/abs/2402.15818 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15833
Date: Sat, 24 Feb 2024 15:00:58 GMT   (7792kb,D)

Title: Prompt Perturbation Consistency Learning for Robust Language Models
Authors: Yao Qiang, Subhrangshu Nandi, Ninareh Mehrabi, Greg Ver Steeg, Anoop
  Kumar, Anna Rumshisky, Aram Galstyan
Categories: cs.CL cs.LG
\\
  Large language models (LLMs) have demonstrated impressive performance on a
number of natural language processing tasks, such as question answering and
text summarization. However, their performance on sequence labeling tasks such
as intent classification and slot filling (IC-SF), which is a central component
in personal assistant systems, lags significantly behind discriminative models.
Furthermore, there is a lack of substantive research on the robustness of LLMs
to various perturbations in the input prompts. The contributions of this paper
are three-fold. First, we show that fine-tuning sufficiently large LLMs can
produce IC-SF performance comparable to discriminative models. Next, we
systematically analyze the performance deterioration of those fine-tuned models
due to three distinct yet relevant types of input perturbations - oronyms,
synonyms, and paraphrasing. Finally, we propose an efficient mitigation
approach, Prompt Perturbation Consistency Learning (PPCL), which works by
regularizing the divergence between losses from clean and perturbed samples.
Our experiments demonstrate that PPCL can recover on average 59% and 69% of the
performance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the
data augmentation approach while using ten times fewer augmented data samples.
\\ ( https://arxiv.org/abs/2402.15833 ,  7792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15861
Date: Sat, 24 Feb 2024 17:08:45 GMT   (8913kb,D)

Title: MATHWELL: Generating Educational Math Word Problems at Scale
Authors: Bryan R Christ, Jonathan Kropko, Thomas Hartvigsen
Categories: cs.CL
Comments: 20 pages, 9 figures
\\
  Math word problems are critical K-8 educational tools, but writing them is
time-consuming and requires domain expertise. We suggest that language models
can support K-8 math education by automatically generating problems at scale.
To be educational, generated problems must be 1) solvable, 2) accurate, and 3)
appropriate. Existing datasets are unlabeled for these criteria, making them
ill-suited for training problem generators. We introduce MATHWELL, a Llama-2
(70B) model iteratively finetuned to generate K-8 math word problems using data
from expert annotation. Using MATHWELL, we generate the largest English word
problem dataset to date, containing 20,490 problems. 3,484 are scored by domain
experts who find MATHWELL has a 40% higher share of problems that have
executable solutions and meet all criteria than alternatives, with 74% of its
problems with executable solutions being solvable, accurate, and appropriate.
\\ ( https://arxiv.org/abs/2402.15861 ,  8913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15862
Date: Sat, 24 Feb 2024 17:12:10 GMT   (9069kb,D)

Title: SportQA: A Benchmark for Sports Understanding in Large Language Models
Authors: Haotian Xia, Zhengbang Yang, Yuqing Wang, Rhys Tracy, Yun Zhao,
  Dongdong Huang, Zezhi Chen, Yan Zhu, Yuan-fang Wang, Weining Shen
Categories: cs.CL
\\
  A deep understanding of sports, a field rich in strategic and dynamic
content, is crucial for advancing Natural Language Processing (NLP). This holds
particular significance in the context of evaluating and advancing Large
Language Models (LLMs), given the existing gap in specialized benchmarks. To
bridge this gap, we introduce SportQA, a novel benchmark specifically designed
for evaluating LLMs in the context of sports understanding. SportQA encompasses
over 70,000 multiple-choice questions across three distinct difficulty levels,
each targeting different aspects of sports knowledge from basic historical
facts to intricate, scenario-based reasoning tasks. We conducted a thorough
evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms
supplemented by chain-of-thought (CoT) prompting. Our results reveal that while
LLMs exhibit competent performance in basic sports knowledge, they struggle
with more complex, scenario-based sports reasoning, lagging behind human
expertise. The introduction of SportQA marks a significant step forward in NLP,
offering a tool for assessing and enhancing sports understanding in LLMs.
\\ ( https://arxiv.org/abs/2402.15862 ,  9069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15873
Date: Sat, 24 Feb 2024 17:44:56 GMT   (16kb)

Title: SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box
  Machine-Generated Text Detection
Authors: Ayan Datta, Aryan Chandramania, Radhika Mamidi
Categories: cs.CL
\\
  This document contains the details of the authors' submission to the
proceedings of SemEval 2024's Task 8: Multigenerator, Multidomain, and
Multilingual Black-Box Machine-Generated Text Detection Subtask A (monolingual)
and B. Detection of machine-generated text is becoming an increasingly
important task, with the advent of large language models (LLMs). In this
document, we lay out the techniques utilized for performing the same, along
with the results obtained.
\\ ( https://arxiv.org/abs/2402.15873 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15925
Date: Sat, 24 Feb 2024 23:01:21 GMT   (8317kb,D)

Title: MultiContrievers: Analysis of Dense Retrieval Representations
Authors: Seraphina Goldfarb-Tarrant, Pedro Rodriguez, Jane Dwivedi-Yu, Patrick
  Lewis
Categories: cs.CL cs.AI cs.IR
\\
  Dense retrievers compress source documents into (possibly lossy) vector
representations, yet there is little analysis of what information is lost
versus preserved, and how it affects downstream tasks. We conduct the first
analysis of the information captured by dense retrievers compared to the
language models they are based on (e.g., BERT versus Contriever). We use 25
MultiBert checkpoints as randomized initialisations to train MultiContrievers,
a set of 25 contriever models. We test whether specific pieces of information
-- such as gender and occupation -- can be extracted from contriever vectors of
wikipedia-like documents. We measure this extractability via information
theoretic probing. We then examine the relationship of extractability to
performance and gender bias, as well as the sensitivity of these results to
many random initialisations and data shuffles. We find that (1) contriever
models have significantly increased extractability, but extractability usually
correlates poorly with benchmark performance 2) gender bias is present, but is
not caused by the contriever representations 3) there is high sensitivity to
both random initialisation and to data shuffle, suggesting that future
retrieval research should test across a wider spread of both.
\\ ( https://arxiv.org/abs/2402.15925 ,  8317kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15930
Date: Sat, 24 Feb 2024 23:17:56 GMT   (327kb)

Title: Evaluating Prompting Strategies for Grammatical Error Correction Based
  on Language Proficiency
Authors: Min Zeng and Jiexin Kuang and Mengyang Qiu and Jayoung Song and
  Jungyeul Park
Categories: cs.CL
Comments: To appear in LREC-COLING 2024, short paper (preprint)
\\
  The writing examples of English language learners may be different from those
of native speakers. Given that there is a significant differences in second
language (L2) learners' error types by their proficiency levels, this paper
attempts to reduce overcorrection by examining the interaction between LLM's
performance and L2 language proficiency. Our method focuses on zero-shot and
few-shot prompting and fine-tuning models for GEC for learners of English as a
foreign language based on the different proficiency. We investigate GEC results
and find that overcorrection happens primarily in advanced language learners'
writing (proficiency C) rather than proficiency A (a beginner level) and
proficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot
prompting with writing examples of English learners, actually tend to exhibit
decreased recall measures. To make our claim concrete, we conduct a
comprehensive examination of GEC outcomes and their evaluation results based on
language proficiency.
\\ ( https://arxiv.org/abs/2402.15930 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15931
Date: Sat, 24 Feb 2024 23:23:06 GMT   (341kb)

Title: Frustratingly Simple Prompting-based Text Denoising
Authors: Jungyeul Park and Mengyang Qiu
Categories: cs.CL
Comments: Published as a Tiny Paper at ICLR 2024
\\
  This paper introduces a novel perspective on the automated essay scoring
(AES) task, challenging the conventional view of the ASAP dataset as a static
entity. Employing simple text denoising techniques using prompting, we explore
the dynamic potential within the dataset. While acknowledging the previous
emphasis on building regression systems, our paper underscores how making minor
changes to a dataset through text denoising can enhance the final results.
\\ ( https://arxiv.org/abs/2402.15931 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15938
Date: Sat, 24 Feb 2024 23:54:41 GMT   (129kb,D)

Title: Generalization or Memorization: Data Contamination and Trustworthy
  Evaluation for Large Language Models
Authors: Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, and Ge Li
Categories: cs.CL cs.AI cs.CR cs.LG cs.SE
\\
  Recent statements about the impressive capabilities of large language models
(LLMs) are usually supported by evaluating on open-access benchmarks.
Considering the vast size and wide-ranging sources of LLMs' training data, it
could explicitly or implicitly include test data, leading to LLMs being more
susceptible to data contamination. However, due to the opacity of training
data, the black-box access of models, and the rapid growth of synthetic
training data, detecting and mitigating data contamination for LLMs faces
significant challenges. In this paper, we propose CDD, which stands for
Contamination Detection via output Distribution for LLMs. CDD necessitates only
the sampled texts to detect data contamination, by identifying the peakedness
of LLM's output distribution. To mitigate the impact of data contamination in
evaluation, we also present TED: Trustworthy Evaluation via output
Distribution, based on the correction of LLM's output distribution. To
facilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,
for data contamination detection and contamination mitigation evaluation tasks.
Extensive experimental results show that CDD achieves the average relative
improvements of 21.8\%-30.2\% over other contamination detection approaches in
terms of Accuracy, F1 Score, and AUC metrics, and can effectively detect
contamination caused by the variants of test data. TED significantly mitigates
performance improvements up to 66.9\% attributed to data contamination across
24 settings and 21 contamination degrees. In real-world applications, we reveal
that ChatGPT exhibits a high potential to suffer from data contamination on
HumanEval benchmark.
\\ ( https://arxiv.org/abs/2402.15938 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15967
Date: Sun, 25 Feb 2024 03:03:34 GMT   (191kb,D)

Title: Direct Punjabi to English speech translation using discrete units
Authors: Prabhjot Kaur, L. Andrew M. Bush, Weisong Shi
Categories: cs.CL cs.SD eess.AS
\\
  Speech-to-speech translation is yet to reach the same level of coverage as
text-to-text translation systems. The current speech technology is highly
limited in its coverage of over 7000 languages spoken worldwide, leaving more
than half of the population deprived of such technology and shared experiences.
With voice-assisted technology (such as social robots and speech-to-text apps)
and auditory content (such as podcasts and lectures) on the rise, ensuring that
the technology is available for all is more important than ever. Speech
translation can play a vital role in mitigating technological disparity and
creating a more inclusive society. With a motive to contribute towards speech
translation research for low-resource languages, our work presents a direct
speech-to-speech translation model for one of the Indic languages called
Punjabi to English. Additionally, we explore the performance of using a
discrete representation of speech called discrete acoustic units as input to
the Transformer-based translation model. The model, abbreviated as Unit-to-Unit
Translation (U2UT), takes a sequence of discrete units of the source language
(the language being translated from) and outputs a sequence of discrete units
of the target language (the language being translated to). Our results show
that the U2UT model performs better than the Speech-to-Unit Translation (S2UT)
model by a 3.69 BLEU score.
\\ ( https://arxiv.org/abs/2402.15967 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15987
Date: Sun, 25 Feb 2024 04:52:02 GMT   (7679kb,D)

Title: Likelihood-based Mitigation of Evaluation Bias in Large Language Models
Authors: Masanari Ohi, Masahiro Kaneko, Ryuto Koike, Mengsay Loem, Naoaki
  Okazaki
Categories: cs.CL cs.AI
Comments: 4 main pages
\\
  Large Language Models (LLMs) are widely used to evaluate natural language
generation tasks as automated metrics. However, the likelihood, a measure of
LLM's plausibility for a sentence, can vary due to superficial differences in
sentences, such as word order and sentence structure. It is therefore possible
that there might be a likelihood bias if LLMs are used for evaluation: they
might overrate sentences with higher likelihoods while underrating those with
lower likelihoods. In this paper, we investigate the presence and impact of
likelihood bias in LLM-based evaluators. We also propose a method to mitigate
the likelihood bias. Our method utilizes highly biased instances as few-shot
examples for in-context learning. Our experiments in evaluating the
data-to-text and grammatical error correction tasks reveal that several LLMs we
test display a likelihood bias. Furthermore, our proposed method successfully
mitigates this bias, also improving evaluation performance (in terms of
correlation of models with human scores) significantly.
\\ ( https://arxiv.org/abs/2402.15987 ,  7679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15991
Date: Sun, 25 Feb 2024 05:07:56 GMT   (8700kb,D)

Title: $C^3$: Confidence Calibration Model Cascade for Inference-Efficient
  Cross-Lingual Natural Language Understanding
Authors: Taixi Lu, Haoyu Wang, Huajie Shao, Jing Gao, Huaxiu Yao
Categories: cs.CL
\\
  Cross-lingual natural language understanding (NLU) is a critical task in
natural language processing (NLP). Recent advancements have seen multilingual
pre-trained language models (mPLMs) significantly enhance the performance of
these tasks. However, mPLMs necessitate substantial resources and incur high
computational costs during inference, posing challenges for deployment in
real-world and real-time systems. Existing model cascade methods seek to
enhance inference efficiency by greedily selecting the lightest model capable
of processing the current input from a variety of models, based on model
confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and
confidence distributions vary across languages. This leads to the emission of
confident but incorrect predictions by smaller models, hindering their ability
to generalize effectively across test languages. In this study, we introduce a
confidence calibration model cascade ($C^3$) method. This approach, simple yet
effective, involves calibration prior to cascade inference, thereby enhancing
cascade accuracy through more reliable predictions. Extensive experiments
conducted on three cross-lingual benchmarks demonstrate that $C^3$
significantly outperforms all state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.15991 ,  8700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16006
Date: Sun, 25 Feb 2024 06:46:27 GMT   (13022kb,D)

Title: From Noise to Clarity: Unraveling the Adversarial Suffix of Large
  Language Model Attacks via Translation of Text Embeddings
Authors: Hao Wang, Hao Li, Minlie Huang, Lei Sha
Categories: cs.CL
\\
  The safety defense methods of Large language models(LLMs) stays limited
because the dangerous prompts are manually curated to just few known attack
types, which fails to keep pace with emerging varieties. Recent studies found
that attaching suffixes to harmful instructions can hack the defense of LLMs
and lead to dangerous outputs. This method, while effective, leaves a gap in
understanding the underlying mechanics of such adversarial suffix due to the
non-readability and it can be relatively easily seen through by common defense
methods such as perplexity filters.To cope with this challenge, in this paper,
we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that
are able to translate the unreadable adversarial suffixes into coherent,
readable text, which makes it easier to understand and analyze the reasons
behind harmful content generation by large language models. We conducted
experiments on LLMs such as LLaMa2, Vicuna and using the Advbench dataset's
harmful instructions. The results indicate that our method achieves a much
better attack success rate to existing techniques, while significantly
enhancing the textual fluency of the prompts. In addition, our approach can be
generalized into a broader method for generating transferable adversarial
suffixes that can successfully attack multiple LLMs, even black-box LLMs, such
as ChatGPT and Gemini. As a result, the prompts generated through our method
exhibit enriched semantic diversity, which potentially provides more
adversarial examples for LLM defense methods.
\\ ( https://arxiv.org/abs/2402.16006 ,  13022kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16021
Date: Sun, 25 Feb 2024 07:46:57 GMT   (7946kb,D)

Title: TMT: Tri-Modal Translation between Speech, Image, and Text by Processing
  Different Modalities as Different Languages
Authors: Minsu Kim, Jee-weon Jung, Hyeongseop Rha, Soumi Maiti, Siddhant Arora,
  Xuankai Chang, Shinji Watanabe, Yong Man Ro
Categories: cs.CL cs.AI cs.CV eess.AS
\\
  The capability to jointly process multi-modal information is becoming an
essential task. However, the limited number of paired multi-modal data and the
large computational requirements in multi-modal learning hinder the
development. We propose a novel Tri-Modal Translation (TMT) model that
translates between arbitrary modalities spanning speech, image, and text. We
introduce a novel viewpoint, where we interpret different modalities as
different languages, and treat multi-modal translation as a well-established
machine translation problem. To this end, we tokenize speech and image data
into discrete tokens, which provide a unified interface across modalities and
significantly decrease the computational cost. In the proposed TMT, a
multi-modal encoder-decoder conducts the core translation, whereas
modality-specific processing is conducted only within the tokenization and
detokenization stages. We evaluate the proposed TMT on all six modality
translation tasks. TMT outperforms single model counterparts consistently,
demonstrating that unifying tasks is beneficial not only for practicality but
also for performance.
\\ ( https://arxiv.org/abs/2402.16021 ,  7946kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16024
Date: Sun, 25 Feb 2024 08:07:22 GMT   (4601kb,D)

Title: HiGPT: Heterogeneous Graph Language Model
Authors: Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao
  Huang
Categories: cs.CL cs.LG
\\
  Heterogeneous graph learning aims to capture complex relationships and
diverse relational semantics among entities in a heterogeneous graph to obtain
meaningful representations for nodes and edges. Recent advancements in
heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art
performance by considering relation heterogeneity and using specialized message
functions and aggregation rules. However, existing frameworks for heterogeneous
graph learning have limitations in generalizing across diverse heterogeneous
graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune"
paradigm on the same dataset, which restricts their capacity to adapt to new
and unseen data. This raises the question: "Can we generalize heterogeneous
graph models to be well-adapted to diverse downstream learning tasks with
distribution shifts in both node token sets and relation type heterogeneity?''
To tackle those challenges, we propose HiGPT, a general large graph model with
Heterogeneous graph instruction-tuning paradigm. Our framework enables learning
from arbitrary heterogeneous graphs without the need for any fine-tuning
process from downstream datasets. To handle distribution shifts in
heterogeneity, we introduce an in-context heterogeneous graph tokenizer that
captures semantic relationships in different heterogeneous graphs, facilitating
model adaptation. We incorporate a large corpus of heterogeneity-aware graph
instructions into our HiGPT, enabling the model to effectively comprehend
complex relation heterogeneity and distinguish between various types of graph
tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction
augmentation paradigm to mitigate data scarcity by generating diverse and
informative instructions. Through comprehensive evaluations, our proposed
framework demonstrates exceptional performance in terms of generalization
performance.
\\ ( https://arxiv.org/abs/2402.16024 ,  4601kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16029
Date: Sun, 25 Feb 2024 08:41:32 GMT   (1180kb,D)

Title: GraphWiz: An Instruction-Following Language Model for Graph Problems
Authors: Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li
Categories: cs.CL
Comments: 27pages, 15 tables
\\
  Large language models (LLMs) have achieved impressive success across several
fields, but their proficiency in understanding and resolving complex graph
problems is less explored. To bridge this gap, we introduce GraphInstruct, a
novel and comprehensive instruction-tuning dataset designed to equip language
models with the ability to tackle a broad spectrum of graph problems using
explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an
open-source language model capable of resolving various graph problem types
while generating clear reasoning processes. To enhance the model's capability
and reliability, we incorporate the Direct Preference Optimization (DPO)
framework into the graph problem-solving context. The enhanced model,
GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with
different complexity levels, surpassing GPT-4 which has an average accuracy of
43.8%. Moreover, our research delves into the delicate balance between training
data volume and model performance, highlighting the potential for overfitting
with increased data. We also explore the transferability of the model's
reasoning ability across different graph tasks, indicating the model's
adaptability and practical application potential. Our investigation offers a
new blueprint and valuable insights for developing LLMs specialized in graph
reasoning and problem-solving.
\\ ( https://arxiv.org/abs/2402.16029 ,  1180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16030
Date: Sun, 25 Feb 2024 08:45:10 GMT   (525kb,D)

Title: Don't Forget Your Reward Values: Language Model Alignment via
  Value-based Calibration
Authors: Xin Mao, Feng-Lin Li, Huimin Xu, Wei Zhang, Anh Tuan Luu
Categories: cs.CL cs.AI
Comments: 19 pages, Under review
\\
  While Reinforcement Learning from Human Feedback (RLHF) significantly
enhances the generation quality of Large Language Models (LLMs), recent studies
have raised concerns regarding the complexity and instability associated with
the Proximal Policy Optimization (PPO) algorithm, proposing a series of
order-based calibration methods as viable alternatives. This paper delves
further into current order-based methods, examining their inefficiencies in
utilizing reward values and addressing misalignment issues. Building upon these
findings, we propose a novel \textbf{V}alue-based \textbf{C}ali\textbf{B}ration
(VCB) method to better align LLMs with human preferences. Experimental results
demonstrate that VCB surpasses existing alignment methods on AI assistant and
summarization datasets, providing impressive generalizability, robustness, and
stability in diverse settings.
\\ ( https://arxiv.org/abs/2402.16030 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16034
Date: Sun, 25 Feb 2024 09:17:22 GMT   (665kb,D)

Title: Emotion Classification in Short English Texts using Deep Learning
  Techniques
Authors: Siddhanth Bhat
Categories: cs.CL cs.AI
\\
  Detecting emotions in limited text datasets from under-resourced languages
presents a formidable obstacle, demanding specialized frameworks and
computational strategies. This study conducts a thorough examination of deep
learning techniques for discerning emotions in short English texts. Deep
learning approaches employ transfer learning and word embedding, notably BERT,
to attain superior accuracy. To evaluate these methods, we introduce the
"SmallEnglishEmotions" dataset, comprising 6372 varied short Persian texts
annotated with five primary emotion categories. Our experiments reveal that
transfer learning and BERT-based text embedding outperform alternative methods
in accurately categorizing the text in the dataset.
\\ ( https://arxiv.org/abs/2402.16034 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16035
Date: Sun, 25 Feb 2024 09:19:11 GMT   (1320kb)

Title: Text Understanding and Generation Using Transformer Models for
  Intelligent E-commerce Recommendations
Authors: Yafei Xiang, Hanyi Yu, Yulu Gong, Shuning Huo, Mengran Zhu
Categories: cs.CL cs.AI
\\
  With the rapid development of artificial intelligence technology, Transformer
structural pre-training model has become an important tool for large language
model (LLM) tasks. In the field of e-commerce, these models are especially
widely used, from text understanding to generating recommendation systems,
which provide powerful technical support for improving user experience and
optimizing service processes. This paper reviews the core application scenarios
of Transformer pre-training model in e-commerce text understanding and
recommendation generation, including but not limited to automatic generation of
product descriptions, sentiment analysis of user comments, construction of
personalized recommendation system and automated processing of customer service
conversations. Through a detailed analysis of the model's working principle,
implementation process, and application effects in specific cases, this paper
emphasizes the unique advantages of pre-trained models in understanding complex
user intentions and improving the quality of recommendations. In addition, the
challenges and improvement directions for the future are also discussed, such
as how to further improve the generalization ability of the model, the ability
to handle large-scale data sets, and technical strategies to protect user
privacy. Ultimately, the paper points out that the application of Transformer
structural pre-training models in e-commerce has not only driven technological
innovation, but also brought substantial benefits to merchants and consumers,
and looking forward, these models will continue to play a key role in
e-commerce and beyond.
\\ ( https://arxiv.org/abs/2402.16035 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16038
Date: Sun, 25 Feb 2024 09:32:17 GMT   (950kb)

Title: Deep Learning Approaches for Improving Question Answering Systems in
  Hepatocellular Carcinoma Research
Authors: Shuning Huo, Yafei Xiang, Hanyi Yu, Mengran Zhu, Yulu Gong
Categories: cs.CL cs.AI cs.LG
\\
  In recent years, advancements in natural language processing (NLP) have been
fueled by deep learning techniques, particularly through the utilization of
powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,
trained on vast amounts of data, have revolutionized language understanding and
generation. These pre-trained models serve as robust bases for various tasks
including semantic understanding, intelligent writing, and reasoning, paving
the way for a more generalized form of artificial intelligence. NLP, as a vital
application of AI, aims to bridge the gap between humans and computers through
natural language interaction. This paper delves into the current landscape and
future prospects of large-scale model-based NLP, focusing on the
question-answering systems within this domain. Practical cases and developments
in artificial intelligence-driven question-answering systems are analyzed to
foster further exploration and research in the realm of large-scale NLP.
\\ ( https://arxiv.org/abs/2402.16038 ,  950kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16040
Date: Sun, 25 Feb 2024 09:41:50 GMT   (8319kb,D)

Title: EHRNoteQA: A Patient-Specific Question Answering Benchmark for
  Evaluating Large Language Models in Clinical Settings
Authors: Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon,
  Kwanghyun Kim, Seunghyun Won, Edward Choi
Categories: cs.CL
Comments: Under Review
\\
  This study introduces EHRNoteQA, a novel patient-specific question answering
benchmark tailored for evaluating Large Language Models (LLMs) in clinical
environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three
medical professionals has curated the dataset comprising 962 unique questions,
each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA
distinct from existing EHR-based benchmarks is as follows: Firstly, it is the
first dataset to adopt a multi-choice question answering format, a design
choice that effectively evaluates LLMs with reliable scores in the context of
automatic evaluation, compared to other formats. Secondly, it requires an
analysis of multiple clinical notes to answer a single question, reflecting the
complex nature of real-world clinical decision-making where clinicians review
extensive records of patient histories. Our comprehensive evaluation on various
large language models showed that their scores on EHRNoteQA correlate more
closely with their performance in addressing real-world medical questions
evaluated by clinicians than their scores from other LLM benchmarks. This
underscores the significance of EHRNoteQA in evaluating LLMs for medical
applications and highlights its crucial role in facilitating the integration of
LLMs into healthcare systems. The dataset will be made available to the public
under PhysioNet credential access, promoting further research in this vital
field.
\\ ( https://arxiv.org/abs/2402.16040 ,  8319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16041
Date: Sun, 25 Feb 2024 09:44:56 GMT   (3008kb,D)

Title: Detecting Machine-Generated Texts by Multi-Population Aware Optimization
  for Maximum Mean Discrepancy
Authors: Shuhai Zhang, Feng Liu, Jiahao Yang, Yifan Yang, Changsheng Li, Bo
  Han, Mingkui Tan
Categories: cs.CL cs.LG
Comments: Accepted at ICLR 2024
\\
  Large language models (LLMs) such as ChatGPT have exhibited remarkable
performance in generating human-like texts. However, machine-generated texts
(MGTs) may carry critical risks, such as plagiarism issues, misleading
information, or hallucination issues. Therefore, it is very urgent and
important to detect MGTs in many situations. Unfortunately, it is challenging
to distinguish MGTs and human-written texts because the distributional
discrepancy between them is often very subtle due to the remarkable performance
of LLMs. In this paper, we seek to exploit \textit{maximum mean discrepancy}
(MMD) to address this issue in the sense that MMD can well identify
distributional discrepancies. However, directly training a detector with MMD
using diverse MGTs will incur a significantly increased variance of MMD since
MGTs may contain \textit{multiple text populations} due to various LLMs. This
will severely impair MMD's ability to measure the difference between two
samples. To tackle this, we propose a novel \textit{multi-population} aware
optimization method for MMD called MMD-MP, which can \textit{avoid variance
increases} and thus improve the stability to measure the distributional
discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and
sentence-based detection, respectively. Extensive experiments on various LLMs,
\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The
source code is available at \url{https://github.com/ZSHsh98/MMD-MP}.
\\ ( https://arxiv.org/abs/2402.16041 ,  3008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16048
Date: Sun, 25 Feb 2024 10:13:04 GMT   (194kb,D)

Title: LLMs with Chain-of-Thought Are Non-Causal Reasoners
Authors: Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang
Categories: cs.CL cs.AI cs.LG
Comments: 8 pages, 6 figures, 16 tables
\\
  This paper explores the role of the Chain of Thought (CoT) in Large Language
Models (LLMs) reasoning. Despite its potential to improve task performance, our
analysis reveals a surprising frequency of correct answers following incorrect
CoTs and vice versa. We employ causal analysis to assess the cause-effect
relationship between CoTs/instructions and answers in LLMs, uncovering the
Structural Causal Model (SCM) that LLMs approximate. By comparing the implied
SCM with that of human reasoning, we highlight discrepancies between LLM and
human reasoning processes. We further examine the factors influencing the
causal structure of the implied SCM, revealing that in-context learning,
supervised fine-tuning, and reinforcement learning on human feedback
significantly impact the causal relations. We release the code and results at
https://github.com/StevenZHB/CoT_Causal_Analysis.
\\ ( https://arxiv.org/abs/2402.16048 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16058
Date: Sun, 25 Feb 2024 11:07:08 GMT   (914kb,D)

Title: Say More with Less: Understanding Prompt Learning Behaviors through Gist
  Compression
Authors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang,
  Ge Yu
Categories: cs.CL
\\
  Large language models (LLMs) require lengthy prompts as the input context to
produce output aligned with user intentions, a process that incurs extra costs
during inference. In this paper, we propose the Gist COnditioned deCOding
(Gist-COCO) model, introducing a novel method for compressing prompts which
also can assist the prompt interpretation and engineering. Gist-COCO employs an
encoder-decoder based language model and then incorporates an additional
encoder as a plugin module to compress prompts with inputs using gist tokens.
It finetunes the compression plugin module and uses the representations of gist
tokens to emulate the raw prompts in the vanilla language model. By verbalizing
the representations of gist tokens into gist prompts, the compression ability
of Gist-COCO can be generalized to different LLMs with high compression rates.
Our experiments demonstrate that Gist-COCO outperforms previous prompt
compression models in both passage and instruction compression tasks. Further
analysis on gist verbalization results suggests that our gist prompts serve
different functions in aiding language models. They may directly provide
potential answers, generate the chain-of-thought, or simply repeat the inputs.
All data and codes are available at https://github.com/OpenMatch/Gist-COCO .
\\ ( https://arxiv.org/abs/2402.16058 ,  914kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16061
Date: Sun, 25 Feb 2024 11:15:42 GMT   (1340kb,D)

Title: How Large Language Models Encode Context Knowledge? A Layer-Wise Probing
  Study
Authors: Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, Gongshen
  Liu
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 (Long Paper)
\\
  Previous work has showcased the intriguing capability of large language
models (LLMs) in retrieving facts and processing context knowledge. However,
only limited research exists on the layer-wise capability of LLMs to encode
knowledge, which challenges our understanding of their internal mechanisms. In
this paper, we devote the first attempt to investigate the layer-wise
capability of LLMs through probing tasks. We leverage the powerful generative
capability of ChatGPT to construct probing datasets, providing diverse and
coherent evidence corresponding to various facts. We employ $\mathcal V$-usable
information as the validation metric to better reflect the capability in
encoding context knowledge across different layers. Our experiments on
conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode
more context knowledge in the upper layers; (2) primarily encode context
knowledge within knowledge-related entity tokens at lower layers while
progressively expanding more knowledge within other tokens at upper layers; and
(3) gradually forget the earlier context knowledge retained within the
intermediate layers when provided with irrelevant evidence. Code is publicly
available at https://github.com/Jometeorie/probing_llama.
\\ ( https://arxiv.org/abs/2402.16061 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16063
Date: Sun, 25 Feb 2024 11:24:41 GMT   (731kb,D)

Title: Citation-Enhanced Generation for LLM-based Chatbot
Authors: Weitao Li, Junkai Li, Weizhi Ma, Yang Liu
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) exhibit powerful general intelligence across
diverse scenarios, including their integration into chatbots. However, a vital
challenge of LLM-based chatbots is that they may produce hallucinated content
in responses, which significantly limits their applicability. Various efforts
have been made to alleviate hallucination, such as retrieval augmented
generation and reinforcement learning with human feedback, but most of them
require additional training and data annotation. In this paper, we propose a
novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration
(\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous
studies that focus on preventing hallucinations during generation, our method
addresses this issue in a post-hoc way. It incorporates a retrieval module to
search for supporting documents relevant to the generated content, and employs
a natural language inference-based citation generation module. Once the
statements in the generated content lack of reference, our model can regenerate
responses until all statements are supported by citations. Note that our method
is a training-free plug-and-play plugin that is capable of various LLMs.
Experiments on various hallucination-related datasets show our framework
outperforms state-of-the-art methods in both hallucination detection and
response regeneration on three benchmarks. Our codes and dataset will be
publicly available.
\\ ( https://arxiv.org/abs/2402.16063 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16065
Date: Sun, 25 Feb 2024 11:26:39 GMT   (232kb,D)

Title: Training a Bilingual Language Model by Mapping Tokens onto a Shared
  Character Space
Authors: Aviad Rom and Kfir Bar
Categories: cs.CL cs.LG
\\
  We train a bilingual Arabic-Hebrew language model using a transliterated
version of Arabic texts in Hebrew, to ensure both languages are represented in
the same script. Given the morphological, structural similarities, and the
extensive number of cognates shared among Arabic and Hebrew, we assess the
performance of a language model that employs a unified script for both
languages, on machine translation which requires cross-lingual knowledge. The
results are promising: our model outperforms a contrasting model which keeps
the Arabic texts in the Arabic script, demonstrating the efficacy of the
transliteration step. Despite being trained on a dataset approximately 60%
smaller than that of other existing language models, our model appears to
deliver comparable performance in machine translation across both translation
directions.
\\ ( https://arxiv.org/abs/2402.16065 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16102
Date: Sun, 25 Feb 2024 15:00:13 GMT   (1271kb,D)

Title: Interpreting Predictive Probabilities: Model Confidence or Human Label
  Variation?
Authors: Joris Baan, Raquel Fern\'andez, Barbara Plank, Wilker Aziz
Categories: cs.CL
Comments: EACL 2024 main
\\
  With the rise of increasingly powerful and user-facing NLP systems, there is
growing interest in assessing whether they have a good representation of
uncertainty by evaluating the quality of their predictive distribution over
outcomes. We identify two main perspectives that drive starkly different
evaluation protocols. The first treats predictive probability as an indication
of model confidence; the second as an indication of human label variation. We
discuss their merits and limitations, and take the position that both are
crucial for trustworthy and fair NLP systems, but that exploiting a single
predictive distribution is limiting. We recommend tools and highlight exciting
directions towards models with disentangled representations of uncertainty
about predictions and uncertainty about human labels.
\\ ( https://arxiv.org/abs/2402.16102 ,  1271kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16107
Date: Sun, 25 Feb 2024 15:11:58 GMT   (243kb,D)

Title: FuseChat: Knowledge Fusion of Chat Models
Authors: Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang,
  Wei Bi
Categories: cs.CL
Comments: Technical Report, work in progress
\\
  While training large language models (LLMs) from scratch can indeed lead to
models with distinct capabilities and strengths, this approach incurs
substantial costs and may lead to potential redundancy in competencies. An
alternative strategy is to combine existing LLMs into a more robust LLM,
thereby diminishing the necessity for expensive pre-training. However, due to
the diverse architectures of LLMs, direct parameter blending proves to be
unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge
fusion to transfer the collective knowledge of multiple structurally varied
LLMs into a target LLM through lightweight continual training. In this report,
we extend the scalability and flexibility of the \textsc{FuseLLM} framework to
realize the fusion of chat LLMs, resulting in \textsc{FuseChat}.
\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge
fusion for structurally and scale-varied source LLMs to derive multiple target
LLMs of identical structure and size via lightweight fine-tuning. Then, these
target LLMs are merged within the parameter space, wherein we propose a novel
method for determining the merging weights based on the variation ratio of
parameter matrices before and after fine-tuning. We validate our approach using
three prominent chat LLMs with diverse architectures and scales, namely
\texttt{NH2-Mixtral-8x7B}, \texttt{NH2-Solar-10.7B}, and
\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains
demonstrate the superiority of \texttt{\textsc{FuseChat}-7B} across a broad
spectrum of chat LLMs at 7B and 34B scales, even surpassing \texttt{GPT-3.5
(March)} and approaching \texttt{Mixtral-8x7B-Instruct}. Our code, model
weights, and data are openly accessible at
\url{https://github.com/fanqiwan/FuseLLM}.
\\ ( https://arxiv.org/abs/2402.16107 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16123
Date: Sun, 25 Feb 2024 15:46:33 GMT   (2204kb,D)

Title: InstructEdit: Instruction-based Knowledge Editing for Large Language
  Models
Authors: Bozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu,
  Kouying Xue, Yanjie Gou, Xi Chen, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.HC cs.LG
Comments: Work in progress; the project website is at
  https://www.zjukg.org/project/InstructEdit/
\\
  Knowledge editing for large language models can offer an efficient solution
to alter a model's behavior without negatively impacting the overall
performance. However, the current approach encounters issues with limited
generalizability across tasks, necessitating one distinct editor for each task,
which significantly hinders the broader applications. To address this, we take
the first step to analyze the multi-task generalization issue in knowledge
editing. Specifically, we develop an instruction-based editing technique,
termed InstructEdit, which facilitates the editor's adaptation to various task
performances simultaneously using simple instructions. With only one unified
editor for each LLM, we empirically demonstrate that InstructEdit can improve
the editor's control, leading to an average 14.86% increase in Reliability in
multi-task editing setting. Furthermore, experiments involving holdout unseen
task illustrate that InstructEdit consistently surpass previous strong
baselines. To further investigate the underlying mechanisms of
instruction-based knowledge editing, we analyze the principal components of the
editing gradient directions, which unveils that instructions can help control
optimization direction with stronger OOD generalization. Code and datasets will
be available in https://github.com/zjunlp/EasyEdit.
\\ ( https://arxiv.org/abs/2402.16123 ,  2204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16132
Date: Sun, 25 Feb 2024 16:14:26 GMT   (780kb,D)

Title: LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by
  Long-Short-Term Prompting
Authors: Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, B.
  Aditya Prakash
Categories: cs.CL cs.AI
Comments: 9 pages, 4 figures, 3 tables, 2 page references, 2 page appendix
\\
  Time-series forecasting (TSF) finds broad applications in real-world
scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates
strong zero-shot TSF capabilities while preserving computational efficiency.
However, existing prompting methods oversimplify TSF as language next-token
predictions, overlooking its dynamic nature and lack of integration with
state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose
LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks.
LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks,
tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess
forecasting mechanisms to enhance adaptability. Extensive evaluations
demonstrate consistently better performance of LSTPrompt than existing
prompting methods, and competitive results compared to foundation TSF models.
\\ ( https://arxiv.org/abs/2402.16132 ,  780kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16139
Date: Sun, 25 Feb 2024 16:36:51 GMT   (109kb,D)

Title: What Generative Artificial Intelligence Means for Terminological
  Definitions
Authors: Antonio San Mart\'in
Categories: cs.CL cs.AI
Comments: 9 pages, no figures
\\
  This paper examines the impact of Generative Artificial Intelligence (GenAI)
on the creation and consumption of terminological definitions. GenAI tools like
ChatGPT present a mix of benefits and drawbacks compared to traditional
terminological resources. ChatGPT excels in providing context-specific meanings
in an interactive and customized fashion but faces challenges with accuracy.
Terminological definitions in recognized resources will likely survive because
of their reliability. From the point of view of the terminologist, tools like
ChatGPT enable AI-assisted terminography, including post-editing terminography,
as an approach blending AI efficiency with human expertise for faster
definition creation.
\\ ( https://arxiv.org/abs/2402.16139 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16141
Date: Sun, 25 Feb 2024 16:43:41 GMT   (8365kb,D)

Title: PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization
Authors: Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, Xiaochen
  Wang, Peiyi Wang, Qingxiu Dong, Liang Chen, Zhifang Sui
Categories: cs.CL
\\
  Supervised fine-tuning is the most common method to adapt large language
models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive
computational resources. Recently, parameter-efficient fine-tuning (PEFT)
methods have been widely studied due to its cost-effectiveness. LoRA is one of
the most widely used methods, which assumes that the optimization process is
essentially low-dimensional. Although LoRA fine-tuning is effective, there is
still a performance gap compared to full fine-tuning, since its weight update
is limited to low-rank matrices. In order to break the low-rank bottleneck in
LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank
update matrices multiple times to achieve a higher update rank. PLoRA has
multiple training stages. During each stage, we still update only the LoRA
weights. However, at the end of each stage, we unload the LoRA weights into the
backbone parameters and then reinitialize the LoRA states. Experimental results
show that PLoRA has stronger learning ability, approximately 1.8 times that of
LoRA's learning ability at most, but it does not increase memory usage.
Further, we introduce a momentum-based unloading strategy for PLoRA to mitigate
the training instability.
\\ ( https://arxiv.org/abs/2402.16141 ,  8365kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16142
Date: Sun, 25 Feb 2024 16:47:59 GMT   (1240kb)

Title: From Text to Transformation: A Comprehensive Review of Large Language
  Models' Versatility
Authors: Pravneet Kaur, Gautam Siddharth Kashyap, Ankit Kumar, Md Tabrez Nafis,
  Sandeep Kumar and Vikrant Shokeen
Categories: cs.CL cs.AI
\\
  This groundbreaking study explores the expanse of Large Language Models
(LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional
Encoder Representations from Transformers (BERT) across varied domains ranging
from technology, finance, healthcare to education. Despite their established
prowess in Natural Language Processing (NLP), these LLMs have not been
systematically examined for their impact on domains such as fitness, and
holistic well-being, urban planning, climate modelling as well as disaster
management. This review paper, in addition to furnishing a comprehensive
analysis of the vast expanse and extent of LLMs' utility in diverse domains,
recognizes the research gaps and realms where the potential of LLMs is yet to
be harnessed. This study uncovers innovative ways in which LLMs can leave a
mark in the fields like fitness and wellbeing, urban planning, climate
modelling and disaster response which could inspire future researches and
applications in the said avenues.
\\ ( https://arxiv.org/abs/2402.16142 ,  1240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16159
Date: Sun, 25 Feb 2024 17:40:49 GMT   (9323kb,D)

Title: DistALANER: Distantly Supervised Active Learning Augmented Named Entity
  Recognition in the Open Source Software Ecosystem
Authors: Somnath Banerjee, Avik Dutta, Aaditya Agrawal, Rima Hazra, Animesh
  Mukherjee
Categories: cs.CL
Comments: Under review
\\
  This paper proposes a novel named entity recognition (NER) technique
specifically tailored for the open-source software systems. Our approach aims
to address the scarcity of annotated software data by employing a comprehensive
two-step distantly supervised annotation process. This process strategically
leverages language heuristics, unique lookup tables, external knowledge
sources, and an active learning approach. By harnessing these powerful
techniques, we not only enhance model performance but also effectively mitigate
the limitations associated with cost and the scarcity of expert annotators. It
is noteworthy that our framework significantly outperforms the state-of-the-art
LLMs by a substantial margin. We also show the effectiveness of NER in the
downstream task of relation extraction.
\\ ( https://arxiv.org/abs/2402.16159 ,  9323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16168
Date: Sun, 25 Feb 2024 18:33:25 GMT   (5447kb,D)

Title: Hitting "Probe"rty with Non-Linearity, and More
Authors: Avik Pal, Madhura Pawar
Categories: cs.CL cs.AI
\\
  Structural probes learn a linear transformation to find how dependency trees
are embedded in the hidden states of language models. This simple design may
not allow for full exploitation of the structure of the encoded information.
Hence, to investigate the structure of the encoded information to its full
extent, we incorporate non-linear structural probes. We reformulate the design
of non-linear structural probes introduced by White et al. making its design
simpler yet effective. We also design a visualization framework that lets us
qualitatively assess how strongly two words in a sentence are connected in the
predicted dependency tree. We use this technique to understand which non-linear
probe variant is good at encoding syntactical information. Additionally, we
also use it to qualitatively investigate the structure of dependency trees that
BERT encodes in each of its layers. We find that the radial basis function
(RBF) is an effective non-linear probe for the BERT model than the linear
probe.
\\ ( https://arxiv.org/abs/2402.16168 ,  5447kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16192
Date: Sun, 25 Feb 2024 20:36:03 GMT   (464kb,D)

Title: Defending Large Language Models against Jailbreak Attacks via Semantic
  Smoothing
Authors: Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed
  Hassani, Yang Zhang, Eric Wong, Shiyu Chang
Categories: cs.CL
Comments: 37 pages
\\
  Aligned large language models (LLMs) are vulnerable to jailbreaking attacks,
which bypass the safeguards of targeted LLMs and fool them into generating
objectionable content. While initial defenses show promise against token-based
threat models, there do not exist defenses that provide robustness against
semantic attacks and avoid unfavorable trade-offs between robustness and
nominal performance. To meet this need, we propose SEMANTICSMOOTH, a
smoothing-based defense that aggregates the predictions of multiple
semantically transformed copies of a given input prompt. Experimental results
demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against
GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on
instruction following benchmarks such as InstructionFollowing and AlpacaEval.
The codes will be publicly available at
https://github.com/UCSB-NLP-Chang/SemanticSmooth.
\\ ( https://arxiv.org/abs/2402.16192 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16194
Date: Sun, 25 Feb 2024 20:36:51 GMT   (1519kb,D)

Title: ASEM: Enhancing Empathy in Chatbot through Attention-based Sentiment and
  Emotion Modeling
Authors: Omama Hamad, Ali Hamdi, Khaled Shaban
Categories: cs.CL
Comments: Accepted to the LREC-COLING 2024
\\
  Effective feature representations play a critical role in enhancing the
performance of text generation models that rely on deep neural networks.
However, current approaches suffer from several drawbacks, such as the
inability to capture the deep semantics of language and sensitivity to minor
input variations, resulting in significant changes in the generated text. In
this paper, we present a novel solution to these challenges by employing a
mixture of experts, multiple encoders, to offer distinct perspectives on the
emotional state of the user's utterance while simultaneously enhancing
performance. We propose an end-to-end model architecture called ASEM that
performs emotion analysis on top of sentiment analysis for open-domain
chatbots, enabling the generation of empathetic responses that are fluent and
relevant. In contrast to traditional attention mechanisms, the proposed model
employs a specialized attention strategy that uniquely zeroes in on sentiment
and emotion nuances within the user's utterance. This ensures the generation of
context-rich representations tailored to the underlying emotional tone and
sentiment intricacies of the text. Our approach outperforms existing methods
for generating empathetic embeddings, providing empathetic and diverse
responses. The performance of our proposed model significantly exceeds that of
existing models, enhancing emotion detection accuracy by 6.2% and lexical
diversity by 1.4%.
\\ ( https://arxiv.org/abs/2402.16194 ,  1519kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16211
Date: Sun, 25 Feb 2024 22:23:37 GMT   (2882kb,D)

Title: HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination
  Tendency of LLMs
Authors: Cem Uluoglakci, Tugba Taskaya Temizel (Middle East Technical
  University)
Categories: cs.CL
Comments: EACL SRW 2024 Camera Ready
\\
  Hallucinations pose a significant challenge to the reliability and alignment
of Large Language Models (LLMs), limiting their widespread acceptance beyond
chatbot applications. Despite ongoing efforts, hallucinations remain a
prevalent challenge in LLMs. The detection of hallucinations itself is also a
formidable task, frequently requiring manual labeling or constrained
evaluations. This paper introduces an automated scalable framework that
combines benchmarking LLMs' hallucination tendencies with efficient
hallucination detection. We leverage LLMs to generate challenging tasks related
to hypothetical phenomena, subsequently employing them as agents for efficient
hallucination detection. The framework is domain-agnostic, allowing the use of
any language model for benchmark creation or evaluation in any domain. We
introduce the publicly available HypoTermQA Benchmarking Dataset, on which
state-of-the-art models' performance ranged between 3% and 11%, and evaluator
agents demonstrated a 6% error rate in hallucination prediction. The proposed
framework provides opportunities to test and improve LLMs. Additionally, it has
the potential to generate benchmarking datasets tailored to specific domains,
such as law, health, and finance.
\\ ( https://arxiv.org/abs/2402.16211 ,  2882kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16248
Date: Mon, 26 Feb 2024 02:14:42 GMT   (94kb,D)

Title: Topic-to-essay generation with knowledge-based content selection
Authors: Jieyong Wang, Chunyao Song, Yihao Wu
Categories: cs.CL cs.AI
\\
  The topic-to-essay generation task is a challenging natural language
generation task that aims to generate paragraph-level text with high semantic
coherence based on a given set of topic words. Previous work has focused on the
introduction of external knowledge, ignoring the insufficient generated text
diversity. In order to improve the generation diversity, we propose a novel
copy mechanism model with a content selection module that integrates rich
semantic knowledge from the language model into the decoder. Furthermore, we
introduce the improved prefix tuning method to train the model, enabling it to
adapt to varying input complexities. In addition, we have contributed a new
Chinese dataset for TEG tasks. Experimental results demonstrate that the
proposed model can improve the generated text diversity by 35\% to 59\%
compared to the state-of-the-art method, while maintaining a high level of
topic consistency.
\\ ( https://arxiv.org/abs/2402.16248 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16261
Date: Mon, 26 Feb 2024 02:48:43 GMT   (1062kb,D)

Title: UniRetriever: Multi-task Candidates Selection for Various
  Context-Adaptive Conversational Retrieval
Authors: Hongru Wang, Boyang Xue, Baohang Zhou, Rui Wang, Fei Mi, Weichao Wang,
  Yasheng Wang, Kam-Fai Wong
Categories: cs.CL cs.IR
\\
  Conversational retrieval refers to an information retrieval system that
operates in an iterative and interactive manner, requiring the retrieval of
various external resources, such as persona, knowledge, and even response, to
effectively engage with the user and successfully complete the dialogue.
However, most previous work trained independent retrievers for each specific
resource, resulting in sub-optimal performance and low efficiency. Thus, we
propose a multi-task framework function as a universal retriever for three
dominant retrieval tasks during the conversation: persona selection, knowledge
selection, and response selection. To this end, we design a dual-encoder
architecture consisting of a context-adaptive dialogue encoder and a candidate
encoder, aiming to attention to the relevant context from the long dialogue and
retrieve suitable candidates by simply a dot product. Furthermore, we introduce
two loss constraints to capture the subtle relationship between dialogue
context and different candidates by regarding historically selected candidates
as hard negatives. Extensive experiments and analysis establish
state-of-the-art retrieval quality both within and outside its training domain,
revealing the promising potential and generalization capability of our model to
serve as a universal retriever for different candidate selection tasks
simultaneously.
\\ ( https://arxiv.org/abs/2402.16261 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16288
Date: Mon, 26 Feb 2024 04:09:53 GMT   (1076kb,D)

Title: PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification,
  Retrieval, and Synthesis in Question Answering
Authors: Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun
  Zhong, Zezhong Wang, Kam-Fai Wong
Categories: cs.CL cs.AI cs.IR
\\
  Long-term memory plays a critical role in personal interaction, considering
long-term memory can better leverage world knowledge, historical information,
and preferences in dialogues. Our research introduces PerLTQA, an innovative QA
dataset that combines semantic and episodic memories, including world
knowledge, profiles, social relationships, events, and dialogues. This dataset
is collected to investigate the use of personalized memories, focusing on
social interactions and events in the QA task. PerLTQA features two types of
memory and a comprehensive benchmark of 8,593 questions for 30 characters,
facilitating the exploration and application of personalized memories in Large
Language Models (LLMs). Based on PerLTQA, we propose a novel framework for
memory integration and generation, consisting of three main components: Memory
Classification, Memory Retrieval, and Memory Synthesis. We evaluate this
framework using five LLMs and three retrievers. Experimental results
demonstrate that BERT-based classification models significantly outperform LLMs
such as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,
our study highlights the importance of effective memory integration in the QA
task.
\\ ( https://arxiv.org/abs/2402.16288 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16311
Date: Mon, 26 Feb 2024 05:30:48 GMT   (3568kb,D)

Title: Cross-domain Chinese Sentence Pattern Parsing
Authors: Yingsi Yu, Cunliang Kong, Liner Yang, Meishan Zhang, Lin Zhu, Yujie
  Wang, Haozhe Lin, Maosong Sun, Erhong Yang
Categories: cs.CL cs.AI
\\
  Sentence Pattern Structure (SPS) parsing is a syntactic analysis method
primarily employed in language teaching.Existing SPS parsers rely heavily on
textbook corpora for training, lacking cross-domain capability.To overcome this
constraint, this paper proposes an innovative approach leveraging large
language models (LLMs) within a self-training framework. Partial syntactic
rules from a source domain are combined with target domain sentences to
dynamically generate training data, enhancing the adaptability of the parser to
diverse domains.Experiments conducted on textbook and news domains demonstrate
the effectiveness of the proposed method, outperforming rule-based baselines by
1.68 points on F1 metrics.
\\ ( https://arxiv.org/abs/2402.16311 ,  3568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16313
Date: Mon, 26 Feb 2024 05:31:34 GMT   (6812kb,D)

Title: Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based
  Question Answering
Authors: Mingxu Tao and Dongyan Zhao and Yansong Feng
Categories: cs.CL cs.AI
Comments: Under review
\\
  Open-ended question answering requires models to find appropriate evidence to
form well-reasoned, comprehensive and helpful answers. In practical
applications, models also need to engage in extended discussions on potential
scenarios closely relevant to the question. With augmentation of retrieval
module, open-source Large Language Models (LLMs) can produce coherent answers
often with different focuses, but are still sub-optimal in terms of reliable
evidence selection and in-depth question analysis. In this paper, we propose a
novel Chain-of-Discussion framework to leverage the synergy among multiple
open-source LLMs aiming to provide \textbf{more correct} and \textbf{more
comprehensive} answers for open-ended QA, although they are not strong enough
individually. Our experiments show that discussions among multiple LLMs play a
vital role in enhancing the quality of answers. We release our data and code at
\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.
\\ ( https://arxiv.org/abs/2402.16313 ,  6812kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16319
Date: Mon, 26 Feb 2024 05:51:47 GMT   (606kb,D)

Title: Data-freeWeight Compress and Denoise for Large Language Models
Authors: Runyu Peng, Yunhua Zhou, Qipeng Guo, Yang Gao, Hang Yan, Xipeng Qiu,
  Dahua Lin
Categories: cs.CL
\\
  Large Language Models (LLMs) are reshaping the research landscape in
artificial intelligence, particularly as model parameters scale up
significantly, unlocking remarkable capabilities across various domains.
Nevertheless, the scalability of model parameters faces constraints due to
limitations in GPU memory and computational speed. To address these
constraints, various weight compression methods have emerged, such as Pruning
and Quantization. Given the low-rank nature of weight matrices in language
models, the reduction of weights through matrix decomposition undoubtedly holds
significant potential and promise. In this paper, drawing upon the intrinsic
structure of LLMs, we propose a novel approach termed Data-free Joint Rank-k
Approximation for compressing the parameter matrices. Significantly, our method
is characterized by without necessitating additional involvement of any corpus,
while simultaneously preserving orthogonality in conjunction with pruning and
quantization methods. We achieve a model pruning of 80% parameters while
retaining 93.43% of the original performance without any calibration data.
Additionally, we explore the fundamental properties of the weight matrix of
LLMs undergone Rank-k Approximation and conduct comprehensive experiments to
elucidate our hypothesis.
\\ ( https://arxiv.org/abs/2402.16319 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16347
Date: Mon, 26 Feb 2024 07:00:58 GMT   (497kb,D)

Title: CodeS: Towards Building Open-source Language Models for Text-to-SQL
Authors: Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu,
  Renjie Wei, Hongyan Pan, Cuiping Li, Hong Chen
Categories: cs.CL cs.DB
Comments: Accepted to SIGMOD 2024
\\
  Language models have shown promising performance on the task of translating
natural language questions into SQL queries (Text-to-SQL). However, most of the
state-of-the-art (SOTA) approaches rely on powerful yet closed-source large
language models (LLMs), such as ChatGPT and GPT-4, which may have the
limitations of unclear model architectures, data privacy risks, and expensive
inference overheads. To address the limitations, we introduce CodeS, a series
of pre-trained language models with parameters ranging from 1B to 15B,
specifically designed for the text-to-SQL task. CodeS is a fully open-source
language model, which achieves superior accuracy with much smaller parameter
sizes. This paper studies the research challenges in building CodeS. To enhance
the SQL generation abilities of CodeS, we adopt an incremental pre-training
approach using a specifically curated SQL-centric corpus. Based on this, we
address the challenges of schema linking and rapid domain adaptation through
strategic prompt construction and a bi-directional data augmentation technique.
We conduct comprehensive evaluations on multiple datasets, including the widely
used Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic
benchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as
well as two real-world datasets created for financial and academic
applications. The experimental results show that our CodeS achieves new SOTA
accuracy and robustness on nearly all challenging text-to-SQL benchmarks.
\\ ( https://arxiv.org/abs/2402.16347 ,  497kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16352
Date: Mon, 26 Feb 2024 07:17:25 GMT   (7239kb,D)

Title: MathGenie: Generating Synthetic Data with Question Back-translation for
  Enhancing Mathematical Reasoning of LLMs
Authors: Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan,
  Mingjie Zhan, Hongsheng Li
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have exhibited great potential in mathematical
reasoning. However, there remains a performance gap in this area between
existing open-source models and closed-source models such as GPT-4. In this
paper, we introduce MathGenie, a novel method for generating diverse and
reliable math problems from a small-scale problem-solution dataset (denoted as
seed data). We augment the ground-truth solutions of our seed data and train a
back-translation model to translate the augmented solutions back into new
questions. Subsequently, we generate code-integrated solutions for the new
questions. To ensure the correctness of the code-integrated solutions, we
employ rationale-based strategy for solution verification. Various pretrained
models, ranging from 7B to 70B, are trained on the newly curated data to test
the effectiveness of the proposed augmentation technique, resulting in a family
of models known as MathGenieLM. These models consistently outperform previous
open-source models across five representative mathematical reasoning datasets,
achieving state-of-the-art performance. In particular, MathGenieLM-InternLM2
achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best
overall score among open-source language models.
\\ ( https://arxiv.org/abs/2402.16352 ,  7239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16361
Date: Mon, 26 Feb 2024 07:31:35 GMT   (1308kb,D)

Title: Layer-wise Regularized Dropout for Neural Language Models
Authors: Shiwen Ni, Min Yang, Ruifeng Xu, Chengming Li and Xiping Hu
Categories: cs.CL cs.AI
Journal-ref: LREC-COLING 2024
\\
  Among the various pre-trained neural language models that are popular today,
dropout is already an indispensable regularization technique. To solve the
inconsistency between training and inference caused by the randomness of
dropout, some studies use consistency training to regularize dropout at the
output layer. In this paper, we propose a novel Layer-wise Regularized Dropout
(LR-Drop), which is specially designed for Transformer-based Language models.
Specifically, LR-Drop layer-wise regularizes each Transformer layer using the
consistency training strategy. Each training sample passes through the two
siamese sub-models sampled by dropout, and then LR-Drop forces the hidden
states, multi-head attention matrices, and output distribution of the two
siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a
"self-distillation" framework, in which each sub-model generated by dropout is
the other's "teacher" model and "student" model. Through extensive experiments
on 8 natural language understanding datasets, 6 neural machine translation
datasets, and 1 abstractive summarization dataset (a total of 15 datasets), we
show that LR-Drop achieves superior performances, including state-of-the-art
results.
\\ ( https://arxiv.org/abs/2402.16361 ,  1308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16363
Date: Mon, 26 Feb 2024 07:33:05 GMT   (1366kb,D)

Title: LLM Inference Unveiled: Survey and Roofline Model Insights
Authors: Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue,
  Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu
  Sun, Kurt Keutzer
Categories: cs.CL cs.AI
\\
  The field of efficient Large Language Model (LLM) inference is rapidly
evolving, presenting a unique blend of opportunities and challenges. Although
the field has expanded and is vibrant, there hasn't been a concise framework
that analyzes the various methods of LLM Inference to provide a clear
understanding of this domain. Our survey stands out from traditional literature
reviews by not only summarizing the current state of research but also by
introducing a framework based on roofline model for systematic analysis of LLM
inference techniques. This framework enables identifying the bottlenecks in LLM
deployments and provides a deeper understanding of the practical aspects on
real devices, thereby informing more effective strategies for deploying LLM.
Furthermore, we systematically collate the latest advancements in efficient LLM
inference, covering crucial areas such as weight optimization (e.g., Knowledge
Distillation and Quantization), decoding algorithm improvements (e.g., Early
Exit and Mixture-of-Expert), and both hardware and system-level enhancements.
Distinguished by the integration of roofline model analysis, our survey
provides a comprehensive and nuanced exploration of efficient LLM inference
challenges and solutions. This distinctive approach not only showcases the
current research landscape but also delivers valuable insights for practical
implementation, positioning our work as an indispensable resource for
researchers new to the field as well as for those seeking to deepen their
understanding of efficient LLM deployment. The tool LLM-Viewer is open-sourced.
\\ ( https://arxiv.org/abs/2402.16363 ,  1366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16364
Date: Mon, 26 Feb 2024 07:33:28 GMT   (17686kb,D)

Title: Where Do We Go from Here? Multi-scale Allocentric Relational Inference
  from Natural Spatial Descriptions
Authors: Tzuf Paz-Argaman, Sayali Kulkarni, John Palowitch, Jason Baldridge,
  and Reut Tsarfaty
Categories: cs.CL cs.LG cs.MM
\\
  When communicating routes in natural language, the concept of {\em acquired
spatial knowledge} is crucial for geographic information retrieval (GIR) and in
spatial cognitive research. However, NLP navigation studies often overlook the
impact of such acquired knowledge on textual descriptions. Current navigation
studies concentrate on egocentric local descriptions (e.g., `it will be on your
right') that require reasoning over the agent's local perception. These
instructions are typically given as a sequence of steps, with each action-step
explicitly mentioning and being followed by a landmark that the agent can use
to verify they are on the right path (e.g., `turn right and then you will
see...'). In contrast, descriptions based on knowledge acquired through a map
provide a complete view of the environment and capture its overall structure.
These instructions (e.g., `it is south of Central Park and a block north of a
police station') are typically non-sequential, contain allocentric relations,
with multiple spatial relations and implicit actions, without any explicit
verification. This paper introduces the Rendezvous (RVS) task and dataset,
which includes 10,404 examples of English geospatial instructions for reaching
a target location using map-knowledge. Our analysis reveals that RVS exhibits a
richer use of spatial allocentric relations, and requires resolving more
spatial relations simultaneously compared to previous text-based navigation
benchmarks.
\\ ( https://arxiv.org/abs/2402.16364 ,  17686kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16367
Date: Mon, 26 Feb 2024 07:44:56 GMT   (9341kb,D)

Title: Unraveling Babel: Exploring Multilingual Activation Patterns within
  Large Language Models
Authors: Weize Liu, Yinlong Xu, Hongxia Xu, Jintai Chen, Xuming Hu, Jian Wu
Categories: cs.CL
\\
  Recently, large language models (LLMs) have achieved tremendous breakthroughs
in the field of language processing, yet their mechanisms in processing
multiple languages remain agnostic. Therefore, in this work we study the
multilingual activation patterns of LLMs. By transforming the original Large
Language Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze
the expert activation patterns when processing various languages and
demonstrate the connections of these activation patterns at the level of
language families. We discover the existence of non-language-specific neurons
as well as language-specific activation neurons. Further exploration even
showcases that merely leveraging high-frequency activation neurons can
accelerate inference while maintaining comparable performance. These findings
shed light on the LLMs' multilingual processing mechanism, and are of
significant importance in guiding the multilingual training and model pruning
of LLMs.
\\ ( https://arxiv.org/abs/2402.16367 ,  9341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16379
Date: Mon, 26 Feb 2024 07:58:12 GMT   (5006kb,D)

Title: Improving LLM-based Machine Translation with Systematic Self-Correction
Authors: Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng,
  Jian Wu, Zuozhu Liu
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have achieved impressive results in Machine
Translation (MT). However, careful evaluations by human reveal that the
translations produced by LLMs still contain multiple errors. Importantly,
feeding back such error information into the LLMs can lead to self-correction
and result in improved translation performance. Motivated by these insights, we
introduce a systematic LLM-based self-correcting translation framework, named
TER, which stands for Translate, Estimate, and Refine, marking a significant
step forward in this direction. Our findings demonstrate that 1) our
self-correction framework successfully assists LLMs in improving their
translation quality across a wide range of languages, whether it's from
high-resource languages to low-resource ones or whether it's English-centric or
centered around other languages; 2) TER exhibits superior systematicity and
interpretability compared to previous methods; 3) different estimation
strategies yield varied impacts on AI feedback, directly affecting the
effectiveness of the final corrections. We further compare different LLMs and
conduct various experiments involving self-correction and cross-model
correction to investigate the potential relationship between the translation
and evaluation capabilities of LLMs.
\\ ( https://arxiv.org/abs/2402.16379 ,  5006kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16382
Date: Mon, 26 Feb 2024 08:08:03 GMT   (565kb,D)

Title: Immunization against harmful fine-tuning attacks
Authors: Domenic Rosati, Jan Wehner, Kai Williams, {\L}ukasz Bartoszcze, Jan
  Batzner, Hassan Sajjad, Frank Rudzicz
Categories: cs.CL
\\
  Approaches to aligning large language models (LLMs) with human values has
focused on correcting misalignment that emerges from pretraining. However, this
focus overlooks another source of misalignment: bad actors might purposely
fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging
threat model that has arisen from alignment circumvention and fine-tuning
attacks. However, lacking in previous works is a clear presentation of the
conditions for effective defence. We propose a set of conditions for effective
defence against harmful fine-tuning in LLMs called "Immunization conditions,"
which help us understand how we would construct and measure future defences.
Using this formal framework for defence, we offer a synthesis of different
research directions that might be persued to prevent harmful fine-tuning
attacks and provide a demonstration of how to use these conditions
experimentally showing early results of using an adversarial loss to immunize
LLama2-7b-chat.
\\ ( https://arxiv.org/abs/2402.16382 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16389
Date: Mon, 26 Feb 2024 08:27:50 GMT   (2307kb,D)

Title: MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in
  Intellectual Property
Authors: Shiwen Ni, Minghuan Tan, Yuelin Bai, Fuqiang Niu, Min Yang, Bowen
  Zhang, Ruifeng Xu, Xiaojun Chen, Chengming Li, Xiping Hu, Ye Li, Jianping Fan
Categories: cs.CL cs.AI
Journal-ref: LREC-COLING 2024
\\
  Large language models (LLMs) have demonstrated impressive performance in
various natural language processing (NLP) tasks. However, there is limited
understanding of how well LLMs perform in specific domains (e.g, the
intellectual property (IP) domain). In this paper, we contribute a new
benchmark, the first Multilingual-oriented quiZ on Intellectual Property
(MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark
includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question
answering (IPQA), and patent matching (PatentMatch). In addition, we also
develop a new IP-oriented multilingual large language model (called MoZi),
which is a BLOOMZ-based model that has been supervised fine-tuned with
multilingual IP-related text data. We evaluate our proposed MoZi model and four
well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP
benchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE
and ChatGLM by a noticeable margin, while it had lower scores compared with
ChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has
much room for improvement, and even the most powerful ChatGPT does not reach
the passing level. Our source code, data, and models are available at
\url{https://github.com/AI-for-Science/MoZi}.
\\ ( https://arxiv.org/abs/2402.16389 ,  2307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16406
Date: Mon, 26 Feb 2024 08:59:05 GMT   (340kb)

Title: From RAGs to riches: Using large language models to write documents for
  clinical trials
Authors: Nigel Markey, Ilyass El-Mansouri, Gaetan Rensonnet, Casper van Langen,
  Christoph Meier
Categories: cs.CL
Comments: 5 pages, 2 figures
\\
  Clinical trials require numerous documents to be written -- protocols,
consent forms, clinical study reports and others. Large language models (LLMs)
offer the potential to rapidly generate first versions of these documents,
however there are concerns about the quality of their output Here we report an
evaluation of LLMs in generating parts of one such document, clinical trial
protocols. We find that an offthe-shelf LLM delivers reasonable results,
especially when assessing content relevance and the correct use of terminology.
However, deficiencies remain: specifically clinical thinking and logic, and
appropriate use of references. To improve performance, we used
retrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date
information. As a result of using RAG, the writing quality of the LLM improves
substantially, which has implications for the practical useability of LLMs in
clinical trial-related writing.
\\ ( https://arxiv.org/abs/2402.16406 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16420
Date: Mon, 26 Feb 2024 09:19:46 GMT   (302kb,D)

Title: Predicting Sustainable Development Goals Using Course Descriptions --
  from LLMs to Conventional Foundation Models
Authors: Lev Kharlashkin, Melany Macias, Leo Huovinen, Mika H\"am\"al\"ainen
Categories: cs.CL
Comments: 3 figures, 2 tables
\\
  We present our work on predicting United Nations sustainable development
goals (SDG) for university courses. We use an LLM named PaLM 2 to generate
training data given a noisy human-authored course description input as input.
We use this data to train several different smaller language models to predict
SDGs for university courses. This work contributes to better university level
adaptation of SDGs. The best performing model in our experiments was BART with
an F1-score of 0.786.
\\ ( https://arxiv.org/abs/2402.16420 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16431
Date: Mon, 26 Feb 2024 09:30:55 GMT   (3210kb,D)

Title: RoCoIns: Enhancing Robustness of Large Language Models through
  Code-Style Instructions
Authors: Yuansen Zhang, Xiao Wang, Zhiheng Xi, Han Xia, Tao Gui, Qi Zhang,
  Xuanjing Huang
Categories: cs.CL
Comments: Accepted by COLING 2024
\\
  Large Language Models (LLMs) have showcased remarkable capabilities in
following human instructions. However, recent studies have raised concerns
about the robustness of LLMs when prompted with instructions combining textual
adversarial samples. In this paper, drawing inspiration from recent works that
LLMs are sensitive to the design of the instructions, we utilize instructions
in code style, which are more structural and less ambiguous, to replace
typically natural language instructions. Through this conversion, we provide
LLMs with more precise instructions and strengthen the robustness of LLMs.
Moreover, under few-shot scenarios, we propose a novel method to compose
in-context demonstrations using both clean and adversarial samples
(\textit{adversarial context method}) to further boost the robustness of the
LLMs. Experiments on eight robustness datasets show that our method
consistently outperforms prompting LLMs with natural language instructions. For
example, with gpt-3.5-turbo, our method achieves an improvement of 5.68\% in
test set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).
\\ ( https://arxiv.org/abs/2402.16431 ,  3210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16438
Date: Mon, 26 Feb 2024 09:36:05 GMT   (273kb,D)

Title: Language-Specific Neurons: The Key to Multilingual Capabilities in Large
  Language Models
Authors: Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang,
  Xin Zhao, Furu Wei, Ji-Rong Wen
Categories: cs.CL
\\
  Large language models (LLMs) demonstrate remarkable multilingual capabilities
without being pre-trained on specially curated multilingual parallel corpora.
It remains a challenging problem to explain the underlying mechanisms by which
LLMs process multilingual texts. In this paper, we delve into the composition
of Transformer architectures in LLMs to pinpoint language-specific regions.
Specially, we propose a novel detection method, language activation probability
entropy (LAPE), to identify language-specific neurons within LLMs. Based on
LAPE, we conduct comprehensive experiments on two representative LLMs, namely
LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a
particular language is predominantly due to a small subset of neurons,
primarily situated in the models' top and bottom layers. Furthermore, we
showcase the feasibility to "steer" the output language of LLMs by selectively
activating or deactivating language-specific neurons. Our research provides
important evidence to the understanding and exploration of the multilingual
capabilities of LLMs.
\\ ( https://arxiv.org/abs/2402.16438 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16444
Date: Mon, 26 Feb 2024 09:43:02 GMT   (9893kb,D)

Title: ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable
  Safety Detectors
Authors: Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun,
  Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang
Categories: cs.CL
Comments: 17 pages
\\
  The safety of Large Language Models (LLMs) has gained increasing attention in
recent years, but there still lacks a comprehensive approach for detecting
safety issues within LLMs' responses in an aligned, customizable and
explainable manner. In this paper, we propose ShieldLM, an LLM-based safety
detector, which aligns with general human safety standards, supports
customizable detection rules, and provides explanations for its decisions. To
train ShieldLM, we compile a large bilingual dataset comprising 14,387
query-response pairs, annotating the safety of responses based on various
safety standards. Through extensive experiments, we demonstrate that ShieldLM
surpasses strong baselines across four test sets, showcasing remarkable
customizability and explainability. Besides performing well on standard
detection datasets, ShieldLM has also been shown to be effective in real-world
situations as a safety evaluator for advanced LLMs. We release ShieldLM at
\url{https://github.com/thu-coai/ShieldLM} to support accurate and explainable
safety detection under various safety standards, contributing to the ongoing
efforts to enhance the safety of LLMs.
\\ ( https://arxiv.org/abs/2402.16444 ,  9893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16457
Date: Mon, 26 Feb 2024 09:59:04 GMT   (213kb,D)

Title: RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for
  Short-form Open-Domain Question Answering
Authors: Zihan Zhang, Meng Fang, Ling Chen
Categories: cs.CL
Comments: preprint
\\
  Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine
the necessity of retrieval for queries instead of retrieving indiscriminately
to enhance the efficiency and relevance of the sourced information. However,
previous works largely overlook the evaluation of ARAG approaches, leading to
their effectiveness being understudied. This work presents a benchmark,
RetrievalQA, comprising 1,271 short-form questions covering new world and
long-tail knowledge. The knowledge necessary to answer the questions is absent
from LLMs; therefore, external information must be retrieved to answer
correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG
methods. We observe that calibration-based methods heavily rely on threshold
tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable
retrieval decisions. Based on our findings, we propose Time-Aware Adaptive
Retrieval (TA-ARE), a simple yet effective method that helps LLMs assess the
necessity of retrieval without calibration or additional training. The dataset
and code will be available at \url{https://github.com/hyintell/RetrievalQA}
\\ ( https://arxiv.org/abs/2402.16457 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16458
Date: Mon, 26 Feb 2024 10:02:29 GMT   (458kb,D)

Title: D-XCB: Data-independent Debiasing for Fair and Accurate
  Transformer-based Cyberbullying Detection
Authors: Peiling Yi and Arkaitz Zubiaga
Categories: cs.CL
\\
  Swear words are a common proxy to collect datasets with cyberbullying
incidents. Our focus is on measuring and mitigating biases derived from
spurious associations between swear words and incidents occurring as a result
of such data collection strategies. After demonstrating and quantifying these
biases, we introduce ID-XCB, the first data-independent debiasing technique
that combines adversarial training, bias constraints and debias fine-tuning
approach aimed at alleviating model attention to bias-inducing words without
impacting overall model performance. We explore ID-XCB on two popular
session-based cyberbullying datasets along with comprehensive ablation and
generalisation studies. We show that ID-XCB learns robust cyberbullying
detection capabilities while mitigating biases, outperforming state-of-the-art
debiasing methods in both performance and bias mitigation. Our quantitative and
qualitative analyses demonstrate its generalisability to unseen data.
\\ ( https://arxiv.org/abs/2402.16458 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16459
Date: Mon, 26 Feb 2024 10:03:33 GMT   (6924kb,D)

Title: Defending LLMs against Jailbreaking Attacks via Backtranslation
Authors: Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh
Categories: cs.CL cs.AI
\\
  Although many large language models (LLMs) have been trained to refuse
harmful requests, they are still vulnerable to jailbreaking attacks, which
rewrite the original prompt to conceal its harmful intent. In this paper, we
propose a new method for defending LLMs against jailbreaking attacks by
``backtranslation''. Specifically, given an initial response generated by the
target LLM from an input prompt, our backtranslation prompts a language model
to infer an input prompt that can lead to the response. The inferred prompt is
called the backtranslated prompt which tends to reveal the actual intent of the
original prompt, since it is generated based on the LLM's response and is not
directly manipulated by the attacker. We then run the target LLM again on the
backtranslated prompt, and we refuse the original prompt if the model refuses
the backtranslated prompt. We explain that the proposed defense provides
several benefits on its effectiveness and efficiency. We empirically
demonstrate that our defense significantly outperforms the baselines, in the
cases that are hard for the baselines, and our defense also has little impact
on the generation quality for benign input prompts.
\\ ( https://arxiv.org/abs/2402.16459 ,  6924kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16470
Date: Mon, 26 Feb 2024 10:31:45 GMT   (8705kb,D)

Title: Unveiling Vulnerability of Self-Attention
Authors: Khai Jiet Liong, Hongqiu Wu, Hai Zhao
Categories: cs.CL
Comments: https://github.com/liongkj/HackAttend
\\
  Pre-trained language models (PLMs) are shown to be vulnerable to minor word
changes, which poses a big threat to real-world systems. While previous studies
directly focus on manipulating word inputs, they are limited by their means of
generating adversarial samples, lacking generalization to versatile real-world
attack. This paper studies the basic structure of transformer-based PLMs, the
self-attention (SA) mechanism. (1) We propose a powerful perturbation technique
\textit{HackAttend}, which perturbs the attention scores within the SA matrices
via meticulously crafted attention masks. We show that state-of-the-art PLMs
fall into heavy vulnerability that minor attention perturbations $(1\%)$ can
produce a very high attack success rate $(98\%)$. Our paper expands the
conventional text attack of word perturbations to more general structural
perturbations. (2) We introduce \textit{S-Attend}, a novel smoothing technique
that effectively makes SA robust via structural perturbations. We empirically
demonstrate that this simple yet effective technique achieves robust
performance on par with adversarial training when facing various text
attackers. Code is publicly available at \url{github.com/liongkj/HackAttend}.
\\ ( https://arxiv.org/abs/2402.16470 ,  8705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16472
Date: Mon, 26 Feb 2024 10:33:36 GMT   (7817kb,D)

Title: mEdIT: Multilingual Text Editing via Instruction Tuning
Authors: Vipul Raheja and Dimitris Alikaniotis and Vivek Kulkarni and Bashar
  Alhafni and Dhruv Kumar
Categories: cs.CL cs.AI
Comments: ACL ARR December 2023. 22 pages, 8 tables, 11 figures
ACM-class: I.2.7
\\
  We introduce mEdIT, a multi-lingual extension to CoEdIT -- the recent
state-of-the-art text editing models for writing assistance. mEdIT models are
trained by fine-tuning multi-lingual large, pre-trained language models (LLMs)
via instruction tuning. They are designed to take instructions from the user
specifying the attributes of the desired text in the form of natural language
instructions, such as Grammatik korrigieren (German) or Parafrasee la oraci\'on
(Spanish). We build mEdIT by curating data from multiple publicly available
human-annotated text editing datasets for three text editing tasks (Grammatical
Error Correction (GEC), Text Simplification, and Paraphrasing) across diverse
languages belonging to six different language families. We detail the design
and training of mEdIT models and demonstrate their strong performance on many
multi-lingual text editing benchmarks against other multilingual LLMs. We also
find that mEdIT generalizes effectively to new languages over multilingual
baselines. We publicly release our data, code, and trained models at
https://github.com/vipulraheja/medit.
\\ ( https://arxiv.org/abs/2402.16472 ,  7817kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16499
Date: Mon, 26 Feb 2024 11:31:48 GMT   (868kb,D)

Title: LLMArena: Assessing Capabilities of Large Language Models in Dynamic
  Multi-Agent Environments
Authors: Junzhe Chen, Xuming Hu, Shuodi Liu, Shiyu Huang, Wei-Wei Tu, Zhaofeng
  He and Lijie Wen
Categories: cs.CL
\\
  Recent advancements in large language models (LLMs) have revealed their
potential for achieving autonomous agents possessing human-level intelligence.
However, existing benchmarks for evaluating LLM Agents either use static
datasets, potentially leading to data leakage or focus only on single-agent
scenarios, overlooking the complexities of multi-agent interactions. There is a
lack of a benchmark that evaluates the diverse capabilities of LLM agents in
multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel
and easily extensible framework for evaluating the diverse capabilities of LLM
in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming
environments, employing Trueskill scoring to assess crucial abilities in LLM
agents, including spatial reasoning, strategic planning, numerical reasoning,
risk assessment, communication, opponent modeling, and team collaboration. We
conduct an extensive experiment and human evaluation among different sizes and
types of LLMs, showing that LLMs still have a significant journey ahead in
their development towards becoming fully autonomous agents, especially in
opponent modeling and team collaboration. We hope LLMArena could guide future
research towards enhancing these capabilities in LLMs, ultimately leading to
more sophisticated and practical applications in dynamic, multi-agent settings.
The code and data will be available.
\\ ( https://arxiv.org/abs/2402.16499 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16508
Date: Mon, 26 Feb 2024 11:42:29 GMT   (8233kb,D)

Title: Pre-training Cross-lingual Open Domain Question Answering with
  Large-scale Synthetic Supervision
Authors: Fan Jiang, Tom Drummond, Trevor Cohn
Categories: cs.CL cs.IR
\\
  Cross-lingual question answering (CLQA) is a complex problem, comprising
cross-lingual retrieval from a multilingual knowledge base, followed by answer
generation either in English or the query language. Both steps are usually
tackled by separate models, requiring substantial annotated datasets, and
typically auxiliary resources, like machine translation systems to bridge
between languages. In this paper, we show that CLQA can be addressed using a
single encoder-decoder model. To effectively train this model, we propose a
self-supervised method based on exploiting the cross-lingual link structure
within Wikipedia. We demonstrate how linked Wikipedia pages can be used to
synthesise supervisory signals for cross-lingual retrieval, through a form of
cloze query, and generate more natural queries to supervise answer generation.
Together, we show our approach, \texttt{CLASS}, outperforms comparable methods
on both supervised and zero-shot language adaptation settings, including those
using machine translation.
\\ ( https://arxiv.org/abs/2402.16508 ,  8233kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16515
Date: Mon, 26 Feb 2024 11:52:55 GMT   (8622kb,D)

Title: LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
  with a Distribution Tutor for Medical Text Classification
Authors: Yiping Song, Juhua Zhang, Zhiliang Tian, Yuxin Yang, Minlie Huang,
  Dongsheng Li
Categories: cs.CL cs.CR
\\
  As sufficient data are not always publically accessible for model training,
researchers exploit limited data with advanced learning algorithms or expand
the dataset via data augmentation (DA). Conducting DA in private domain
requires private protection approaches (i.e. anonymization and perturbation),
but those methods cannot provide protection guarantees. Differential privacy
(DP) learning methods theoretically bound the protection but are not skilled at
generating pseudo text samples with large models. In this paper, we transfer
DP-based pseudo sample generation task to DP-based generated samples
discrimination task, where we propose a DP-based DA method with a LLM and a
DP-based discriminator for text classification on private domains. We construct
a knowledge distillation model as the DP-based discriminator: teacher models,
accessing private data, teaches students how to select private samples with
calibrated noise to achieve DP. To constrain the distribution of DA's
generation, we propose a DP-based tutor that models the noised private
distribution and controls samples' generation with a low privacy cost. We
theoretically analyze our model's privacy protection and empirically verify our
model.
\\ ( https://arxiv.org/abs/2402.16515 ,  8622kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16567
Date: Mon, 26 Feb 2024 13:46:51 GMT   (7972kb,D)

Title: Aligning Large Language Models to a Domain-specific Graph Database
Authors: Yuanyuan Liang, Keren Tan, Tingyu Xie, Wenbiao Tao, Siyuan Wang,
  Yunshi Lan, Weining Qian
Categories: cs.CL cs.AI cs.DB
Comments: 13 pages,2 figures
\\
  Graph Databases (Graph DB) are widely applied in various fields, including
finance, social networks, and medicine. However, translating Natural Language
(NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to
be challenging due to its inherent complexity and specialized nature. Some
approaches have sought to utilize Large Language Models (LLMs) to address
analogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a
particular domain, the absence of domain-specific NL-GQL data pairs makes it
difficult to establish alignment between LLMs and the graph DB. To address this
challenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT
to create NL-GQL data pairs based on the given graph DB with self-instruct.
Then, we use the created data to fine-tune LLMs, thereby achieving alignment
between LLMs and the graph DB. Additionally, during inference, we propose a
method that extracts relevant schema to the queried NL as the input context to
guide LLMs for generating accurate GQLs.We evaluate our method on two
constructed datasets deriving from graph DBs in finance domain and medicine
domain, namely FinGQL and MediGQL. Experimental results demonstrate that our
method significantly outperforms a set of baseline methods, with improvements
of 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on
EX, respectively.
\\ ( https://arxiv.org/abs/2402.16567 ,  7972kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16568
Date: Mon, 26 Feb 2024 13:47:09 GMT   (754kb,D)

Title: Two-stage Generative Question Answering on Temporal Knowledge Graph
  Using Large Language Models
Authors: Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, Dongsheng
  Li
Categories: cs.CL
\\
  Temporal knowledge graph question answering (TKGQA) poses a significant
challenge task, due to the temporal constraints hidden in questions and the
answers sought from dynamic structured knowledge. Although large language
models (LLMs) have made considerable progress in their reasoning ability over
structured data, their application to the TKGQA task is a relatively unexplored
area. This paper first proposes a novel generative temporal knowledge graph
question answering framework, GenTKGQA, which guides LLMs to answer temporal
questions through two phases: Subgraph Retrieval and Answer Generation. First,
we exploit LLM's intrinsic knowledge to mine temporal constraints and
structural links in the questions without extra training, thus narrowing down
the subgraph search space in both temporal and structural dimensions. Next, we
design virtual knowledge indicators to fuse the graph neural network signals of
the subgraph and the text representations of the LLM in a non-shallow way,
which helps the open-source LLM deeply understand the temporal order and
structural dependencies among the retrieved facts through instruction tuning.
Experimental results demonstrate that our model outperforms state-of-the-art
baselines, even achieving 100\% on the metrics for the simple question type.
\\ ( https://arxiv.org/abs/2402.16568 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16578
Date: Mon, 26 Feb 2024 14:01:34 GMT   (1248kb,D)

Title: Multi-Bit Distortion-Free Watermarking for Large Language Models
Authors: Massieh Kordi Boroujeny, Ya Jiang, Kai Zeng, Brian Mark
Categories: cs.CL cs.LG
\\
  Methods for watermarking large language models have been proposed that
distinguish AI-generated text from human-generated text by slightly altering
the model output distribution, but they also distort the quality of the text,
exposing the watermark to adversarial detection. More recently, distortion-free
watermarking methods were proposed that require a secret key to detect the
watermark. The prior methods generally embed zero-bit watermarks that do not
provide additional information beyond tagging a text as being AI-generated. We
extend an existing zero-bit distortion-free watermarking method by embedding
multiple bits of meta-information as part of the watermark. We also develop a
computationally efficient decoder that extracts the embedded information from
the watermark with low bit error rate.
\\ ( https://arxiv.org/abs/2402.16578 ,  1248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16596
Date: Mon, 26 Feb 2024 14:27:06 GMT   (223kb,D)

Title: Semantic change detection for Slovene language: a novel dataset and an
  approach based on optimal transport
Authors: Marko Pranji\'c (1 and 2), Kaja Dobrovoljc (1), Senja Pollak (1),
  Matej Martinc (1) ((1) Jo\v{z}ef Stefan Institute, Ljubljana, Slovenia, (2)
  Jo\v{z}ef Stefan International Postgraduate School, Ljubljana, Slovenia)
Categories: cs.CL
ACM-class: I.2.7
\\
  In this paper, we focus on the detection of semantic changes in Slovene, a
less resourced Slavic language with two million speakers. Detecting and
tracking semantic changes provides insights into the evolution of the language
caused by changes in society and culture. Recently, several systems have been
proposed to aid in this study, but all depend on manually annotated gold
standard datasets for evaluation. In this paper, we present the first Slovene
dataset for evaluating semantic change detection systems, which contains
aggregated semantic change scores for 104 target words obtained from more than
3000 manually annotated sentence pairs. We evaluate several existing semantic
change detection methods on this dataset and also propose a novel approach
based on optimal transport that improves on the existing state-of-the-art
systems with an error reduction rate of 22.8%.
\\ ( https://arxiv.org/abs/2402.16596 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16602
Date: Mon, 26 Feb 2024 14:30:37 GMT   (138kb,D)

Title: Rethinking Negative Instances for Generative Named Entity Recognition
Authors: Yuyang Ding, Juntao Li, Pinzheng Wang, Zecheng Tang, Bowen Yan, Min
  Zhang
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated impressive capabilities for
generalizing in unseen tasks. In the Named Entity Recognition (NER) task,
recent advancements have seen the remarkable improvement of LLMs in a broad
range of entity domains via instruction tuning, by adopting entity-centric
schema. In this work, we explore the potential enhancement of the existing
methods by incorporating negative instances into training. Our experiments
reveal that negative instances contribute to remarkable improvements by (1)
introducing contextual information, and (2) clearly delineating label
boundaries. Furthermore, we introduce a novel and efficient algorithm named
Hierarchical Matching, which is tailored to transform unstructured predictions
into structured entities. By integrating these components, we present GNER, a
Generative NER system that shows improved zero-shot performance across unseen
entity domains. Our comprehensive evaluation illustrates our system's
superiority, surpassing state-of-the-art (SoTA) methods by 11 $F_1$ score in
zero-shot evaluation.
\\ ( https://arxiv.org/abs/2402.16602 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16608
Date: Mon, 26 Feb 2024 14:40:34 GMT   (4251kb,D)

Title: PAQA: Toward ProActive Open-Retrieval Question Answering
Authors: Pierre Erbacher and Jian-Yun Nie and Philippe Preux and Laure Soulier
Categories: cs.CL cs.IR
\\
  Conversational systems have made significant progress in generating natural
language responses. However, their potential as conversational search systems
is currently limited due to their passive role in the information-seeking
process. One major limitation is the scarcity of datasets that provide labelled
ambiguous questions along with a supporting corpus of documents and relevant
clarifying questions. This work aims to tackle the challenge of generating
relevant clarifying questions by taking into account the inherent ambiguities
present in both user queries and documents. To achieve this, we propose PAQA,
an extension to the existing AmbiNQ dataset, incorporating clarifying
questions. We then evaluate various models and assess how passage retrieval
impacts ambiguity detection and the generation of clarifying questions. By
addressing this gap in conversational search systems, we aim to provide
additional supervision to enhance their active participation in the
information-seeking process and provide users with more accurate results.
\\ ( https://arxiv.org/abs/2402.16608 ,  4251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16611
Date: Wed, 21 Feb 2024 23:50:37 GMT   (180kb,D)

Title: Understanding the Dataset Practitioners Behind Large Language Model
  Development
Authors: Crystal Qian, Emily Reif, Minsuk Kahng
Categories: cs.CL cs.AI cs.HC
\\
  As large language models (LLMs) become more advanced and impactful, it is
increasingly important to scrutinize the data that they rely upon and produce.
What is it to be a dataset practitioner doing this work? We approach this in
two parts: first, we define the role of "dataset practitioner" by performing a
retrospective analysis on the responsibilities of teams contributing to LLM
development at Google. Then, we conduct semi-structured interviews with a
cross-section of these practitioners (N=10). We find that data quality is the
top priority. To evaluate data quality, practitioners either rely on their own
intuition or write custom evaluation logic. There is a lack of consensus across
practitioners on what quality is and how to evaluate it. We discuss potential
reasons for this phenomenon and opportunities for alignment.
\\ ( https://arxiv.org/abs/2402.16611 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16617
Date: Mon, 26 Feb 2024 14:47:35 GMT   (246kb,D)

Title: Long-Context Language Modeling with Parallel Context Encoding
Authors: Howard Yen, Tianyu Gao, Danqi Chen
Categories: cs.CL
Comments: Code and data are available at https://github.com/princeton-nlp/CEPE
\\
  Extending large language models (LLMs) to process longer inputs is crucial
for numerous applications. However, the considerable computational cost of
transformers, coupled with limited generalization of positional encoding,
restricts the size of their context window. We introduce Context Expansion with
Parallel Encoding (CEPE), a framework that can be applied to any existing
decoder-only LLMs to extend their context window. CEPE adopts a small encoder
to process long inputs chunk by chunk and enables the frozen decoder to
leverage additional contexts via cross-attention. CEPE is efficient,
generalizable, and versatile: trained with 8K-token documents, CEPE extends the
context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only
1/6 of the memory. CEPE yields strong performance on language modeling and
in-context learning. CEPE also excels in retrieval-augmented applications,
while existing long-context models degenerate with retrieved contexts. We
further introduce a CEPE variant that can extend the context window of
instruction-tuned models with only unlabeled data, and showcase its
effectiveness on LLAMA-2-CHAT, leading to a strong instruction-following model
that can leverage very long context on downstream tasks.
\\ ( https://arxiv.org/abs/2402.16617 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16632
Date: Mon, 26 Feb 2024 15:04:35 GMT   (2693kb,D)

Title: Domain Embeddings for Generating Complex Descriptions of Concepts in
  Italian Language
Authors: Alessandro Maisto
Categories: cs.CL
\\
  In this work, we propose a Distributional Semantic resource enriched with
linguistic and lexical information extracted from electronic dictionaries,
designed to address the challenge of bridging the gap between the continuous
semantic values represented by distributional vectors and the discrete
descriptions offered by general semantics theory. Recently, many researchers
have concentrated on the nexus between embeddings and a comprehensive theory of
semantics and meaning. This often involves decoding the representation of word
meanings in Distributional Models into a set of discrete, manually constructed
properties such as semantic primitives or features, using neural decoding
techniques. Our approach introduces an alternative strategy grounded in
linguistic data. We have developed a collection of domain-specific
co-occurrence matrices, derived from two sources: a classification of Italian
nouns categorized into 4 semantic traits and 20 concrete noun sub-categories,
and a list of Italian verbs classified according to their semantic classes. In
these matrices, the co-occurrence values for each word are calculated
exclusively with a defined set of words pertinent to a particular lexical
domain. The resource comprises 21 domain-specific matrices, one comprehensive
matrix, and a Graphical User Interface. Our model facilitates the generation of
reasoned semantic descriptions of concepts by selecting matrices directly
associated with concrete conceptual knowledge, such as a matrix based on
location nouns and the concept of animal habitats. We assessed the utility of
the resource through two experiments, achieving promising outcomes in both: the
automatic classification of animal nouns and the extraction of animal features.
\\ ( https://arxiv.org/abs/2402.16632 ,  2693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16650
Date: Mon, 26 Feb 2024 15:22:30 GMT   (303kb)

Title: ESG Sentiment Analysis: comparing human and language model performance
  including GPT
Authors: Karim Derrick
Categories: cs.CL cs.CE cs.CY
\\
  In this paper we explore the challenges of measuring sentiment in relation to
Environmental, Social and Governance (ESG) social media. ESG has grown in
importance in recent years with a surge in interest from the financial sector
and the performance of many businesses has become based in part on their ESG
related reputations. The use of sentiment analysis to measure ESG related
reputation has developed and with it interest in the use of machines to do so.
The era of digital media has created an explosion of new media sources, driven
by the growth of social media platforms. This growing data environment has
become an excellent source for behavioural insight studies across many
disciplines that includes politics, healthcare and market research. Our study
seeks to compare human performance with the cutting edge in machine performance
in the measurement of ESG related sentiment. To this end researchers classify
the sentiment of 150 tweets and a reliability measure is made. A gold standard
data set is then established based on the consensus of 3 researchers and this
data set is then used to measure the performance of different machine
approaches: one based on the VADER dictionary approach to sentiment
classification and then multiple language model approaches, including Llama2,
T5, Mistral, Mixtral, FINBERT, GPT3.5 and GPT4.
\\ ( https://arxiv.org/abs/2402.16650 ,  303kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16667
Date: Mon, 26 Feb 2024 15:39:52 GMT   (12919kb,D)

Title: RepoAgent: An LLM-Powered Open-Source Framework for Repository-level
  Code Documentation Generation
Authors: Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu,
  Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, Xiaoyin Che, Zhiyuan Liu,
  Maosong Sun
Categories: cs.CL cs.AI
ACM-class: I.2.7; F.2.2
\\
  Generative models have demonstrated considerable potential in software
engineering, particularly in tasks such as code generation and debugging.
However, their utilization in the domain of code documentation generation
remains underexplored. To this end, we introduce RepoAgent, a large language
model powered open-source framework aimed at proactively generating,
maintaining, and updating code documentation. Through both qualitative and
quantitative evaluations, we have validated the effectiveness of our approach,
showing that RepoAgent excels in generating high-quality repository-level
documentation. The code and results are publicly accessible at
https://github.com/OpenBMB/RepoAgent.
\\ ( https://arxiv.org/abs/2402.16667 ,  12919kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16671
Date: Mon, 26 Feb 2024 15:47:01 GMT   (554kb,D)

Title: StructLM: Towards Building Generalist Models for Structured Knowledge
  Grounding
Authors: Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming
  Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen
Categories: cs.CL
\\
  Structured data sources, such as tables, graphs, and databases, are
ubiquitous knowledge sources. Despite the demonstrated capabilities of large
language models (LLMs) on plain text, their proficiency in interpreting and
utilizing structured data remains limited. Our investigation reveals a notable
deficiency in LLMs' ability to process structured data, e.g., ChatGPT lags
behind state-of-the-art (SoTA) model by an average of 35%. To augment the
Structured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a
comprehensive instruction tuning dataset comprising 1.1 million examples.
Utilizing this dataset, we train a series of models, referred to as StructLM,
based on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our
StructLM series surpasses task-specific models on 14 out of 18 evaluated
datasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore,
StructLM demonstrates exceptional generalization across 6 novel SKG tasks.
Contrary to expectations, we observe that scaling model size offers marginal
benefits, with StructLM-34B showing only slight improvements over StructLM-7B.
This suggests that structured knowledge grounding is still a challenging task
and requires more innovative design to push to a new level.
\\ ( https://arxiv.org/abs/2402.16671 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16689
Date: Mon, 26 Feb 2024 16:05:33 GMT   (7649kb,D)

Title: Adaptation of Biomedical and Clinical Pretrained Models to French Long
  Documents: A Comparative Study
Authors: Adrien Bazoge, Emmanuel Morin, Beatrice Daille, Pierre-Antoine
  Gourraud
Categories: cs.CL cs.AI
\\
  Recently, pretrained language models based on BERT have been introduced for
the French biomedical domain. Although these models have achieved
state-of-the-art results on biomedical and clinical NLP tasks, they are
constrained by a limited input sequence length of 512 tokens, which poses
challenges when applied to clinical notes. In this paper, we present a
comparative study of three adaptation strategies for long-sequence models,
leveraging the Longformer architecture. We conducted evaluations of these
models on 16 downstream tasks spanning both biomedical and clinical domains.
Our findings reveal that further pre-training an English clinical model with
French biomedical texts can outperform both converting a French biomedical BERT
to the Longformer architecture and pre-training a French biomedical Longformer
from scratch. The results underscore that long-sequence French biomedical
models improve performance across most downstream tasks regardless of sequence
length, but BERT based models remain the most efficient for named entity
recognition tasks.
\\ ( https://arxiv.org/abs/2402.16689 ,  7649kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16694
Date: Mon, 26 Feb 2024 16:09:00 GMT   (2113kb,D)

Title: HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual
  Natural Language Generalization
Authors: Qiwei Peng, Yekun Chai, Xuhong Li
Categories: cs.CL cs.PL cs.SE
Comments: LREC-COLING 2024
\\
  Large language models (LLMs) have made significant progress in generating
codes from textual prompts. However, existing benchmarks have mainly
concentrated on translating English prompts to multilingual codes or have been
constrained to very limited natural languages (NLs). These benchmarks have
overlooked the vast landscape of massively multilingual NL to multilingual
code, leaving a critical gap in the evaluation of multilingual LLMs. In
response, we introduce HumanEval-XL, a massively multilingual code generation
benchmark specifically crafted to address this deficiency. HumanEval-XL
establishes connections between 23 NLs and 12 programming languages (PLs), and
comprises of a collection of 22,080 prompts with an average of 8.33 test cases.
By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a
comprehensive evaluation platform for multilingual LLMs, allowing the
assessment of the understanding of different NLs. Our work serves as a
pioneering step towards filling the void in evaluating NL generalization in the
area of multilingual code generation. We make our evaluation code and data
publicly available at \url{https://github.com/FloatAI/HumanEval-XL}.
\\ ( https://arxiv.org/abs/2402.16694 ,  2113kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16696
Date: Mon, 26 Feb 2024 16:11:03 GMT   (9366kb,D)

Title: Look Before You Leap: Towards Decision-Aware and Generalizable
  Tool-Usage for Large Language Models
Authors: Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao
Categories: cs.CL
Comments: 20 pages, 18 figures
\\
  Tool-augmented large language models (LLMs) are attracting widespread
attention when accessing up-to-date knowledge and alleviating hallucination
issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated
surprising tool-usage capabilities through prompting and in-context learning
techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in
manipulating tools, current efforts focus on either template-driven or
token-triggered tool-usage. However, the former hampers LLMs' flexibility to
address diverse user's queries due to constrained tool interactions, while the
latter limits the generalizability when engaging with new tools, since
tool-usage learning is based on task- and tool-specific datasets. To alleviate
these concerns, in this paper, we propose a decision-aware and generalizable
tool-usage framework (DEER). Specifically, we first construct the tool-usage
samples with multiple decision branches via an automatic generation pipeline,
thereby inspiring the decision-making awareness of LLMs under diverse
scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the
generalizability of LLMs over unseen tools. Extensive experiments demonstrate
that our proposed DEER is effective and significantly outperforms baselines
across various datasets.
\\ ( https://arxiv.org/abs/2402.16696 ,  9366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16700
Date: Mon, 26 Feb 2024 16:14:47 GMT   (759kb,D)

Title: Generating Effective Ensembles for Sentiment Analysis
Authors: Itay Etelis, Avi Rosenfeld, Abraham Itzhak Weinberg, David Sarne
Categories: cs.CL cs.AI
\\
  In recent years, transformer models have revolutionized Natural Language
Processing (NLP), achieving exceptional results across various tasks, including
Sentiment Analysis (SA). As such, current state-of-the-art approaches for SA
predominantly rely on transformer models alone, achieving impressive accuracy
levels on benchmark datasets. In this paper, we show that the key for further
improving the accuracy of such ensembles for SA is to include not only
transformers, but also traditional NLP models, despite the inferiority of the
latter compared to transformer models. However, as we empirically show, this
necessitates a change in how the ensemble is constructed, specifically relying
on the Hierarchical Ensemble Construction (HEC) algorithm we present. Our
empirical studies across eight canonical SA datasets reveal that ensembles
incorporating a mix of model types, structured via HEC, significantly
outperform traditional ensembles. Finally, we provide a comparative analysis of
the performance of the HEC and GPT-4, demonstrating that while GPT-4 closely
approaches state-of-the-art SA methods, it remains outperformed by our proposed
ensemble strategy.
\\ ( https://arxiv.org/abs/2402.16700 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16705
Date: Mon, 26 Feb 2024 16:21:53 GMT   (14040kb,D)

Title: SelectIT: Selective Instruction Tuning for Large Language Models via
  Uncertainty-Aware Self-Reflection
Authors: Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang,
  Baotian Hu, Min Zhang
Categories: cs.CL cs.AI cs.LG
\\
  Instruction tuning (IT) is crucial to tailoring large language models (LLMs)
towards human-centric interactions. Recent advancements have shown that the
careful selection of a small, high-quality subset of IT data can significantly
enhance the performance of LLMs. Despite this, common approaches often rely on
additional models or data sets, which increases costs and limits widespread
adoption. In this work, we propose a novel approach, termed SelectIT, that
capitalizes on the foundational capabilities of the LLM itself. Specifically,
we exploit the intrinsic uncertainty present in LLMs to more effectively select
high-quality IT data, without the need for extra resources. Furthermore, we
introduce a novel IT dataset, the Selective Alpaca, created by applying
SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT
using Selective Alpaca leads to substantial model ability enhancement. The
robustness of SelectIT has also been corroborated in various foundation models
and domain-specific tasks. Our findings suggest that longer and more
computationally intensive IT data may serve as superior sources of IT, offering
valuable insights for future research in this area. Data, code, and scripts are
freely available at https://github.com/Blue-Raincoat/SelectIT.
\\ ( https://arxiv.org/abs/2402.16705 ,  14040kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16717
Date: Mon, 26 Feb 2024 16:35:59 GMT   (6743kb,D)

Title: CodeChameleon: Personalized Encryption Framework for Jailbreaking Large
  Language Models
Authors: Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou,
  Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang
Categories: cs.CL cs.AI cs.CR
\\
  Adversarial misuse, particularly through `jailbreaking' that circumvents a
model's safety and ethical protocols, poses a significant challenge for Large
Language Models (LLMs). This paper delves into the mechanisms behind such
successful attacks, introducing a hypothesis for the safety mechanism of
aligned LLMs: intent security recognition followed by response generation.
Grounded in this hypothesis, we propose CodeChameleon, a novel jailbreak
framework based on personalized encryption tactics. To elude the intent
security recognition phase, we reformulate tasks into a code completion format,
enabling users to encrypt queries using personalized encryption functions. To
guarantee response generation functionality, we embed a decryption function
within the instructions, which allows the LLM to decrypt and execute the
encrypted queries successfully. We conduct extensive experiments on 7 LLMs,
achieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our
method achieves an 86.6\% ASR on GPT-4-1106.
\\ ( https://arxiv.org/abs/2402.16717 ,  6743kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16733
Date: Wed, 21 Feb 2024 09:12:16 GMT   (8524kb,D)

Title: DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing
Authors: Haneul Yoo, Jieun Han, So-Yeon Ahn, Alice Oh
Categories: cs.CL cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2310.05191
\\
  Automated essay scoring (AES) is a useful tool in English as a Foreign
Language (EFL) writing education, offering real-time essay scores for students
and instructors. However, previous AES models were trained on essays and scores
irrelevant to the practical scenarios of EFL writing education and usually
provided a single holistic score due to the lack of appropriate datasets. In
this paper, we release DREsS, a large-scale, standard dataset for rubric-based
automated essay scoring. DREsS comprises three sub-datasets: DREsS_New,
DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with
1.7K essays authored by EFL undergraduate students and scored by English
education experts. We also standardize existing rubric-based essay scoring
datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation
strategy for essays, which generates 20K synthetic samples of DREsS_CASE and
improves the baseline results by 45.44%. DREsS will enable further research to
provide a more accurate and practical AES system for EFL writing education.
\\ ( https://arxiv.org/abs/2402.16733 ,  8524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16775
Date: Mon, 26 Feb 2024 17:45:36 GMT   (306kb,D)

Title: A Comprehensive Evaluation of Quantization Strategies for Large Language
  Models
Authors: Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang,
  Deyi Xiong
Categories: cs.CL cs.AI
Comments: 20 pages, 16 figures, 16 tables
\\
  Increasing the number of parameters in large language models (LLMs) usually
improves performance in downstream tasks but raises compute and memory costs,
making deployment difficult in resource-limited settings. Quantization
techniques, which reduce the bits needed for model weights or activations with
minimal performance loss, have become popular due to the rise of LLMs. However,
most quantization studies use pre-trained LLMs, and the impact of quantization
on instruction-tuned LLMs and the relationship between perplexity and benchmark
performance of quantized LLMs are not well understood. Evaluation of quantized
LLMs is often limited to language modeling and a few classification tasks,
leaving their performance on other benchmarks unclear. To address these gaps,
we propose a structured evaluation framework consisting of three critical
dimensions: (1) knowledge \& capacity, (2) alignment, and (3) efficiency, and
conduct extensive experiments across ten diverse benchmarks. Our experimental
results indicate that LLMs with 4-bit quantization can retain performance
comparable to their non-quantized counterparts, and perplexity can serve as a
proxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs
with larger parameter scales can outperform smaller LLMs. Despite the memory
savings achieved through quantization, it can also slow down the inference
speed of LLMs. Consequently, substantial engineering efforts and hardware
support are imperative to achieve a balanced optimization of decoding speed and
memory consumption in the context of quantized LLMs.
\\ ( https://arxiv.org/abs/2402.16775 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16786
Date: Mon, 26 Feb 2024 18:00:49 GMT   (2106kb,D)

Title: Political Compass or Spinning Arrow? Towards More Meaningful Evaluations
  for Values and Opinions in Large Language Models
Authors: Paul R\"ottger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck,
  Hannah Rose Kirk, Hinrich Sch\"utze, Dirk Hovy
Categories: cs.CL cs.AI
Comments: v1 prepared for conference submission
\\
  Much recent work seeks to evaluate values and opinions in large language
models (LLMs) using multiple-choice surveys and questionnaires. Most of this
work is motivated by concerns around real-world LLM applications. For example,
politically-biased LLMs may subtly influence society when they are used by
millions of people. Such real-world concerns, however, stand in stark contrast
to the artificiality of current evaluations: real users do not typically ask
LLMs survey questions. Motivated by this discrepancy, we challenge the
prevailing constrained evaluation paradigm for values and opinions in LLMs and
explore more realistic unconstrained evaluations. As a case study, we focus on
the popular Political Compass Test (PCT). In a systematic review, we find that
most prior work using the PCT forces models to comply with the PCT's
multiple-choice format. We show that models give substantively different
answers when not forced; that answers change depending on how models are
forced; and that answers lack paraphrase robustness. Then, we demonstrate that
models give different answers yet again in a more realistic open-ended answer
setting. We distill these findings into recommendations and open challenges in
evaluating values and opinions in LLMs.
\\ ( https://arxiv.org/abs/2402.16786 ,  2106kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16797
Date: Mon, 26 Feb 2024 18:10:56 GMT   (4793kb,D)

Title: Set the Clock: Temporal Alignment of Pretrained Language Models
Authors: Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, Noah
  A. Smith
Categories: cs.CL
Comments: 25 pages, 7 figures. Our code and data will be available at
  https://github.com/yizhongw/llm-temporal-alignment
\\
  Language models (LMs) are trained on web text originating from many points in
time and, in general, without any explicit temporal grounding. This work
investigates the temporal chaos of pretrained LMs and explores various methods
to align their internal knowledge to a target time, which we call "temporal
alignment." To do this, we first automatically construct a dataset containing
20K time-sensitive questions and their answers for each year from 2000 to 2023.
Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2),
despite having a recent pretraining cutoff (e.g., 2022), mostly answer
questions using earlier knowledge (e.g., in 2019). We then develop several
methods, from prompting to finetuning, to align LMs to use their most recent
knowledge when answering questions, and investigate various factors in this
alignment. Our experiments show that aligning LLaMa2 to the year 2022 can boost
its performance by up to 62% relatively as measured by that year, even without
mentioning time information explicitly, indicating the possibility of aligning
models' internal sense of time after pretraining. Finally, we find that
alignment to a historical time is also possible, with up to 2.8$\times$ the
performance of the unaligned LM in 2010 if finetuning models to that year.
These findings hint at the sophistication of LMs' internal knowledge
organization and the necessity of tuning them properly.
\\ ( https://arxiv.org/abs/2402.16797 ,  4793kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16810
Date: Mon, 26 Feb 2024 18:33:13 GMT   (722kb)

Title: OncoGPT: A Medical Conversational Model Tailored with Oncology Domain
  Expertise on a Large Language Model Meta-AI (LLaMA)
Authors: Fujian Jia, Xin Liu, Lixi Deng, Jiwen Gu, Chunchao Pu, Tunan Bai,
  Mengjiang Huang, Yuanzhi Lu, Kang Liu
Categories: cs.CL
\\
  In the past year, there has been a growing trend in applying Large Language
Models (LLMs) to the field of medicine, particularly with the advent of
advanced language models such as ChatGPT developed by OpenAI. However, there is
limited research on LLMs specifically addressing oncology-related queries. The
primary aim of this research was to develop a specialized language model that
demonstrates improved accuracy in providing advice related to oncology. We
performed an extensive data collection of online question-answer interactions
centered around oncology, sourced from reputable doctor-patient platforms.
Following data cleaning and anonymization, a dataset comprising over 180K+
oncology-related conversations was established. The conversations were
categorized and meticulously reviewed by field specialists and clinicians to
ensure precision. Employing the LLaMA model and other selected open-source
datasets, we conducted iterative fine-tuning to enhance the model's proficiency
in basic medical conversation and specialized oncology knowledge. We observed a
substantial enhancement in the model's understanding of genuine patient
inquiries and its reliability in offering oncology-related advice through the
utilization of real online question-answer interactions in the fine-tuning
process. We release database and models to the research community
(https://github.com/OncoGPT1).
\\ ( https://arxiv.org/abs/2402.16810 ,  722kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16817
Date: Mon, 26 Feb 2024 18:42:26 GMT   (5521kb,D)

Title: Investigating the Effectiveness of HyperTuning via Gisting
Authors: Jason Phang
Categories: cs.CL
\\
  Gisting (Mu et al., 2023) is a simple method for training models to compress
information into fewer token representations using a modified attention mask,
and can serve as an economical approach to training Transformer-based
hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks
built on Llama-2 models that generates task-specific soft prefixes based on
few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol
Tuning datasets, we show that HyperLlama models can effectively compress
information from few-shot examples into soft prefixes. However, they still
underperform multi-task fine-tuned language models with full attention over
few-shot in-context examples. We also show that HyperLlama-generated soft
prefixes can serve as better initializations for further prefix tuning.
Overall, Gisting-based hypernetworks are economical and easy to implement, but
have mixed empirical performance.
\\ ( https://arxiv.org/abs/2402.16817 ,  5521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16819
Date: Mon, 26 Feb 2024 18:43:45 GMT   (1328kb,D)

Title: Nemotron-4 15B Technical Report
Authors: Jupinder Parmar and Shrimai Prabhumoye and Joseph Jennings and Mostofa
  Patwary and Sandeep Subramanian and Dan Su and Chen Zhu and Deepak Narayanan
  and Aastha Jhunjhunwala and Ayush Dattagupta and Vibhu Jawa and Jiwei Liu and
  Ameya Mahabaleshwarkar and Osvald Nitski and Annika Brundyn and James Maki
  and Miguel Martinez and Jiaxuan You and John Kamalu and Patrick LeGresley and
  Denys Fridman and Jared Casper and Ashwath Aithal and Oleksii Kuchaiev and
  Mohammad Shoeybi and Jonathan Cohen and Bryan Catanzaro
Categories: cs.CL cs.AI cs.LG
\\
  We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual
language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates
strong performance when assessed on English, multilingual, and coding tasks: it
outperforms all existing similarly-sized open models on 4 out of 7 downstream
evaluation areas and achieves competitive performance to the leading open
models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best
multilingual capabilities of all similarly-sized models, even outperforming
models over four times larger and those explicitly specialized for multilingual
tasks.
\\ ( https://arxiv.org/abs/2402.16819 ,  1328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16822
Date: Mon, 26 Feb 2024 18:47:27 GMT   (2136kb,D)

Title: Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
Authors: Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro,
  Aram H. Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder,
  Jakob Foerster, Tim Rockt\"aschel, Roberta Raileanu
Categories: cs.CL cs.AI cs.LG
\\
  As large language models (LLMs) become increasingly prevalent across many
real-world applications, understanding and enhancing their robustness to user
inputs is of paramount importance. Existing methods for identifying adversarial
prompts tend to focus on specific domains, lack diversity, or require extensive
human annotations. To address these limitations, we present Rainbow Teaming, a
novel approach for producing a diverse collection of adversarial prompts.
Rainbow Teaming casts adversarial prompt generation as a quality-diversity
problem, and uses open-ended search to generate prompts that are both effective
and diverse. It can uncover a model's vulnerabilities across a broad range of
domains including, in this paper, safety, question answering, and
cybersecurity. We also demonstrate that fine-tuning on synthetic data generated
by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting
their general capabilities and helpfulness, paving the path to open-ended
self-improvement.
\\ ( https://arxiv.org/abs/2402.16822 ,  2136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16827
Date: Mon, 26 Feb 2024 18:54:35 GMT   (1308kb,D)

Title: A Survey on Data Selection for Language Models
Authors: Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan
  Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon
  Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang
Categories: cs.CL cs.LG
\\
  A major factor in the recent success of large language models is the use of
enormous and ever-growing text datasets for unsupervised pre-training. However,
naively training a model on all available data may not be optimal (or
feasible), as the quality of available text data can vary. Filtering out data
can also decrease the carbon footprint and financial costs of training models
by reducing the amount of training required.
  Data selection methods aim to determine which candidate data points to
include in the training dataset and how to appropriately sample from the
selected data points. The promise of improved data selection methods has caused
the volume of research in the area to rapidly expand. However, because deep
learning is mostly driven by empirical evidence and experimentation on
large-scale data is expensive, few organizations have the resources for
extensive data selection research. Consequently, knowledge of effective data
selection practices has become concentrated within a few organizations, many of
which do not openly share their findings and methodologies.
  To narrow this gap in knowledge, we present a comprehensive review of
existing literature on data selection methods and related research areas,
providing a taxonomy of existing approaches. By describing the current
landscape of research, this work aims to accelerate progress in data selection
by establishing an entry point for new and established researchers.
Additionally, throughout this review we draw attention to noticeable holes in
the literature and conclude the paper by proposing promising avenues for future
research.
\\ ( https://arxiv.org/abs/2402.16827 ,  1308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16832
Date: Mon, 26 Feb 2024 18:56:48 GMT   (9886kb,D)

Title: Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual
  Capabilities Without Richer Cross-Modal Projections
Authors: Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels,
  Sejoon Oh, Srijan Kumar
Categories: cs.CL cs.AI cs.CV
Comments: 8 pages, 3 figures, 3 tables
\\
  Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable
general-purpose conversations about images with the language modality. As
off-the-shelf MLLMs may have limited capabilities on images from domains like
dermatology and agriculture, they must be fine-tuned to unlock domain-specific
applications. The prevalent architecture of current open-source MLLMs comprises
two major modules: an image-language (cross-modal) projection network and a
large language model. It is desirable to understand the roles of these two
modules in modeling domain-specific visual attributes to inform the design of
future models and streamline the interpretability efforts on the current
models. To this end, via experiments on 4 datasets and under 2 fine-tuning
settings, we find that as the MLLM is fine-tuned, it indeed gains
domain-specific visual capabilities, but the updates do not lead to the
projection extracting relevant domain-specific visual attributes. Our results
indicate that the domain-specific visual attributes are modeled by the LLM,
even when only the projection is fine-tuned. Through this study, we offer a
potential reinterpretation of the role of cross-modal projections in MLLM
architectures. Projection webpage:
https://claws-lab.github.io/projection-in-MLLMs/
\\ ( https://arxiv.org/abs/2402.16832 ,  9886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16835
Date: Mon, 26 Feb 2024 18:57:37 GMT   (345kb,D)

Title: Eight Methods to Evaluate Robust Unlearning in LLMs
Authors: Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, Dylan
  Hadfield-Menell
Categories: cs.CL
\\
  Machine unlearning can be useful for removing harmful capabilities and
memorized text from large language models (LLMs), but there are not yet
standardized methods for rigorously evaluating it. In this paper, we first
survey techniques and limitations of existing unlearning evaluations. Second,
we apply a comprehensive set of tests for the robustness and competitiveness of
unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich
(2023). While WHP's unlearning generalizes well when evaluated with the
"Familiarity" metric from Eldan and Russinovich, we find i)
higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP
performs on par with the original model on Harry Potter Q&A tasks, iii) it
represents latent knowledge comparably to the original model, and iv) there is
collateral unlearning in related domains. Overall, our results highlight the
importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.
\\ ( https://arxiv.org/abs/2402.16835 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16837
Date: Mon, 26 Feb 2024 18:57:54 GMT   (385kb,D)

Title: Do Large Language Models Latently Perform Multi-Hop Reasoning?
Authors: Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian
  Riedel
Categories: cs.CL
\\
  We study whether Large Language Models (LLMs) latently perform multi-hop
reasoning with complex prompts such as "The mother of the singer of
'Superstition' is". We look for evidence of a latent reasoning pathway where an
LLM (1) latently identifies "the singer of 'Superstition'" as Stevie Wonder,
the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to
complete the prompt. We analyze these two hops individually and consider their
co-occurrence as indicative of latent multi-hop reasoning. For the first hop,
we test if changing the prompt to indirectly mention the bridge entity instead
of any other entity increases the LLM's internal recall of the bridge entity.
For the second hop, we test if increasing this recall causes the LLM to better
utilize what it knows about the bridge entity. We find strong evidence of
latent multi-hop reasoning for the prompts of certain relation types, with the
reasoning pathway used in more than 80% of the prompts. However, the
utilization is highly contextual, varying across different types of prompts.
Also, on average, the evidence for the second hop and the full multi-hop
traversal is rather moderate and only substantial for the first hop. Moreover,
we find a clear scaling trend with increasing model size for the first hop of
reasoning but not for the second hop. Our experimental findings suggest
potential challenges and opportunities for future development and applications
of LLMs.
\\ ( https://arxiv.org/abs/2402.16837 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16840
Date: Mon, 26 Feb 2024 18:59:03 GMT   (2210kb,D)

Title: MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT
Authors: Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M.
  Anwer, Michael Felsberg, Tim Baldwin, Eric P. Xing, Fahad Shahbaz Khan
Categories: cs.CL
Comments: Code available at : https://github.com/mbzuai-oryx/MobiLlama
\\
  "Bigger the better" has been the predominant trend in recent Large Language
Models (LLMs) development. However, LLMs do not suit well for scenarios that
require on-device processing, energy efficiency, low memory footprint, and
response efficiency. These requisites are crucial for privacy, security, and
sustainable deployment. This paper explores the "less is more" paradigm by
addressing the challenge of designing accurate yet efficient Small Language
Models (SLMs) for resource constrained devices. Our primary contribution is the
introduction of an accurate and fully transparent open-source 0.5 billion
(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of
resource-constrained computing with an emphasis on enhanced performance with
reduced resource demands. MobiLlama is a SLM design that initiates from a
larger model and applies a careful parameter sharing scheme to reduce both the
pre-training and the deployment cost. Our work strives to not only bridge the
gap in open-source SLMs but also ensures full transparency, where complete
training data pipeline, training code, model weights, and over 300 checkpoints
along with evaluation codes is available at :
https://github.com/mbzuai-oryx/MobiLlama.
\\ ( https://arxiv.org/abs/2402.16840 ,  2210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15555
Date: Fri, 23 Feb 2024 18:59:31 GMT   (24108kb,D)

Title: Deep Networks Always Grok and Here is Why
Authors: Ahmed Imtiaz Humayun, Randall Balestriero, Richard Baraniuk
Categories: cs.LG cs.AI cs.CV
Comments: Website: https://bit.ly/grok-adversarial. Pages 20, Figures 28
\\
  Grokking, or delayed generalization, is a phenomenon where generalization in
a deep neural network (DNN) occurs long after achieving near zero training
error. Previous studies have reported the occurrence of grokking in specific
controlled settings, such as DNNs initialized with large-norm parameters or
transformers trained on algorithmic datasets. We demonstrate that grokking is
actually much more widespread and materializes in a wide range of practical
settings, such as training of a convolutional neural network (CNN) on CIFAR10
or a Resnet on Imagenette. We introduce the new concept of delayed robustness,
whereby a DNN groks adversarial examples and becomes robust, long after
interpolation and/or generalization. We develop an analytical explanation for
the emergence of both delayed generalization and delayed robustness based on a
new measure of the local complexity of a DNN's input-output mapping. Our local
complexity measures the density of the so-called 'linear regions' (aka, spline
partition regions) that tile the DNN input space, and serves as a utile
progress measure for training. We provide the first evidence that for
classification problems, the linear regions undergo a phase transition during
training whereafter they migrate away from the training samples (making the DNN
mapping smoother there) and towards the decision boundary (making the DNN
mapping less smooth there). Grokking occurs post phase transition as a robust
partition of the input space emerges thanks to the linearization of the DNN
mapping around the training points. Website: https://bit.ly/grok-adversarial
\\ ( https://arxiv.org/abs/2402.15555 ,  24108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15561
Date: Fri, 23 Feb 2024 19:02:24 GMT   (160kb,D)

Title: Fair Multivariate Adaptive Regression Splines for Ensuring Equity and
  Transparency
Authors: Parian Haghighat, Denisa G'andara, Lulu Kang, Hadis Anahideh
Categories: cs.LG cs.CY
Journal-ref: The 38th Annual AAAI Conference on Artificial Intelligence, 2024
\\
  Predictive analytics is widely used in various domains, including education,
to inform decision-making and improve outcomes. However, many predictive models
are proprietary and inaccessible for evaluation or modification by researchers
and practitioners, limiting their accountability and ethical design. Moreover,
predictive models are often opaque and incomprehensible to the officials who
use them, reducing their trust and utility. Furthermore, predictive models may
introduce or exacerbate bias and inequity, as they have done in many sectors of
society. Therefore, there is a need for transparent, interpretable, and fair
predictive models that can be easily adopted and adapted by different
stakeholders. In this paper, we propose a fair predictive model based on
multivariate adaptive regression splines(MARS) that incorporates fairness
measures in the learning process. MARS is a non-parametric regression model
that performs feature selection, handles non-linear relationships, generates
interpretable decision rules, and derives optimal splitting criteria on the
variables. Specifically, we integrate fairness into the knot optimization
algorithm and provide theoretical and empirical evidence of how it results in a
fair knot placement. We apply our fairMARS model to real-world data and
demonstrate its effectiveness in terms of accuracy and equity. Our paper
contributes to the advancement of responsible and ethical predictive analytics
for social good.
\\ ( https://arxiv.org/abs/2402.15561 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15567
Date: Fri, 23 Feb 2024 19:09:10 GMT   (2098kb,D)

Title: Foundation Policies with Hilbert Representations
Authors: Seohong Park, Tobias Kreiman, Sergey Levine
Categories: cs.LG cs.AI cs.RO
\\
  Unsupervised and self-supervised objectives, such as next token prediction,
have enabled pre-training generalist models from large amounts of unlabeled
data. In reinforcement learning (RL), however, finding a truly general and
scalable unsupervised pre-training objective for generalist policies from
offline data remains a major open question. While a number of methods have been
proposed to enable generic self-supervised RL, based on principles such as
goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such
methods remain limited in terms of either the diversity of the discovered
behaviors, the need for high-quality demonstration data, or the lack of a clear
prompting or adaptation mechanism for downstream tasks. In this work, we
propose a novel unsupervised framework to pre-train generalist policies that
capture diverse, optimal, long-horizon behaviors from unlabeled offline data
such that they can be quickly adapted to any arbitrary new tasks in a zero-shot
manner. Our key insight is to learn a structured representation that preserves
the temporal structure of the underlying environment, and then to span this
learned latent space with directional movements, which enables various
zero-shot policy "prompting" schemes for downstream tasks. Through our
experiments on simulated robotic locomotion and manipulation benchmarks, we
show that our unsupervised policies can solve goal-conditioned and general RL
tasks in a zero-shot fashion, even often outperforming prior methods designed
specifically for each setting. Our code and videos are available at
https://seohong.me/projects/hilp/
\\ ( https://arxiv.org/abs/2402.15567 ,  2098kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15603
Date: Fri, 23 Feb 2024 20:52:59 GMT   (31kb)

Title: Differentially Private Fair Binary Classifications
Authors: Hrad Ghoukasian, Shahab Asoodeh
Categories: cs.LG cs.CR cs.IT math.IT stat.ML
\\
  In this work, we investigate binary classification under the constraints of
both differential privacy and fairness. We first propose an algorithm based on
the decoupling technique for learning a classifier with only fairness
guarantee. This algorithm takes in classifiers trained on different demographic
groups and generates a single classifier satisfying statistical parity. We then
refine this algorithm to incorporate differential privacy. The performance of
the final algorithm is rigorously examined in terms of privacy, fairness, and
utility guarantees. Empirical evaluations conducted on the Adult and Credit
Card datasets illustrate that our algorithm outperforms the state-of-the-art in
terms of fairness guarantees, while maintaining the same level of privacy and
utility.
\\ ( https://arxiv.org/abs/2402.15603 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15607
Date: Fri, 23 Feb 2024 21:07:20 GMT   (843kb,D)

Title: Training Nonlinear Transformers for Efficient In-Context Learning: A
  Theoretical Learning and Generalization Analysis
Authors: Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, Pin-Yu Chen
Categories: cs.LG
\\
  Transformer-based large language models have displayed impressive in-context
learning capabilities, where a pre-trained model can handle new tasks without
fine-tuning by simply augmenting the query with some input-output examples from
that task. Despite the empirical success, the mechanics of how to train a
Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive
due to the technical challenges of analyzing the nonconvex training problems
resulting from the nonlinear self-attention and nonlinear activation in
Transformers. To the best of our knowledge, this paper provides the first
theoretical analysis of the training dynamics of Transformers with nonlinear
self-attention and nonlinear MLP, together with the ICL generalization
capability of the resulting model. Focusing on a group of binary classification
tasks, we train Transformers using data from a subset of these tasks and
quantify the impact of various factors on the ICL generalization performance on
the remaining unseen tasks with and without data distribution shifts. We also
analyze how different components in the learned Transformers contribute to the
ICL performance. Furthermore, we provide the first theoretical analysis of how
model pruning affects the ICL performance and prove that proper magnitude-based
pruning can have a minimal impact on ICL while reducing inference costs. These
theoretical findings are justified through numerical experiments.
\\ ( https://arxiv.org/abs/2402.15607 ,  843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15608
Date: Fri, 23 Feb 2024 21:11:17 GMT   (862kb)

Title: Machine Learning-Based Completions Sequencing for Well Performance
  Optimization
Authors: Anjie Liu, Jinglang W. Sun, Anh Ngo, Ademide O. Mabadeje, Jose L.
  Hernandez-Mejia
Categories: cs.LG
Comments: 16 pages, 10 figures
\\
  Establishing accurate field development parameters to optimize long-term oil
production takes time and effort due to the complexity of oil well development,
and the uncertainty in estimating long-term well production. Traditionally, oil
and gas companies use simulation software that are inherently computationally
expensive to forecast production. Thus, machine learning approaches are
recently utilized in literature as an efficient alternative to optimize well
developments by enhancing completion conditions. The primary goal of this
project is to develop effective machine-learning models that can integrate the
effects of multidimensional predictive variables (i.e., completion conditions)
to predict 12-Month Cumulative Production accurately.
  Three predictive regression machine learning models are implemented for
predicting 12-month cumulative oil production: Random Forest, Gradient
Boosting, and Long Short-Term Memory Models. All three models yielded
cumulative production predictions with root mean squared error (RMSE ) values
ranging from 7.35 to 20.01 thousand barrels of oil. Although we hypothesized
that all models would yield accurate predictions, the results indicated a
crucial need for further refinement to create reliable and rational predictive
tools in the subsurface. While this study did not produce optimal models for
completion sequencing to maximize long-term production, we established that
machine learning models alone are not self-sufficient for problems of this
nature. Hence, there is potential for significant improvement, including
comprehensive feature engineering, and a recommendation of exploring the use of
hybrid or surrogate models (i.e., coupling physics reduced models and machine
learning models), to ascertain significant contribution to the progress of
completion sequencing workflows.
\\ ( https://arxiv.org/abs/2402.15608 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15613
Date: Fri, 23 Feb 2024 21:28:59 GMT   (8634kb,D)

Title: Towards Efficient Active Learning in NLP via Pretrained Representations
Authors: Artem Vysogorets, Achintya Gopal
Categories: cs.LG cs.CL
\\
  Fine-tuning Large Language Models (LLMs) is now a common approach for text
classification in a wide range of applications. When labeled documents are
scarce, active learning helps save annotation efforts but requires retraining
of massive models on each acquisition iteration. We drastically expedite this
process by using pretrained representations of LLMs within the active learning
loop and, once the desired amount of labeled data is acquired, fine-tuning that
or even a different pretrained LLM on this labeled data to achieve the best
performance. As verified on common text classification benchmarks with
pretrained BERT and RoBERTa as the backbone, our strategy yields similar
performance to fine-tuning all the way through the active learning loop but is
orders of magnitude less computationally expensive. The data acquired with our
procedure generalizes across pretrained networks, allowing flexibility in
choosing the final model or updating it as newer versions get released.
\\ ( https://arxiv.org/abs/2402.15613 ,  8634kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15627
Date: Fri, 23 Feb 2024 22:10:59 GMT   (2516kb,D)

Title: MegaScale: Scaling Large Language Model Training to More Than 10,000
  GPUs
Authors: Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi
  Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He,
  Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo
  Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao,
  Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu
Categories: cs.LG cs.DC
\\
  We present the design, implementation and engineering experience in building
and deploying MegaScale, a production system for training large language models
(LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale
brings unprecedented challenges to training efficiency and stability. We take a
full-stack approach that co-designs the algorithmic and system components
across model block and optimizer design, computation and communication
overlapping, operator optimization, data pipeline, and network performance
tuning. Maintaining high efficiency throughout the training process (i.e.,
stability) is an important consideration in production given the long extent of
LLM training jobs. Many hard stability issues only emerge at large scale, and
in-depth observability is the key to address them. We develop a set of
diagnosis tools to monitor system components and events deep in the stack,
identify root causes, and derive effective techniques to achieve fault
tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs
Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the
MFU by 1.34x compared to Megatron-LM. We share our operational experience in
identifying and fixing failures and stragglers. We hope by articulating the
problems and sharing our experience from a systems perspective, this work can
inspire future LLM systems research.
\\ ( https://arxiv.org/abs/2402.15627 ,  2516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15636
Date: Fri, 23 Feb 2024 22:38:45 GMT   (7273kb,D)

Title: Smooth and Sparse Latent Dynamics in Operator Learning with Jerk
  Regularization
Authors: Xiaoyu Xie, Saviz Mowlavi, Mouhacine Benosman
Categories: cs.LG cs.CE cs.NA math-ph math.MP math.NA
\\
  Spatiotemporal modeling is critical for understanding complex systems across
various scientific and engineering disciplines, but governing equations are
often not fully known or computationally intractable due to inherent system
complexity. Data-driven reduced-order models (ROMs) offer a promising approach
for fast and accurate spatiotemporal forecasting by computing solutions in a
compressed latent space. However, these models often neglect temporal
correlations between consecutive snapshots when constructing the latent space,
leading to suboptimal compression, jagged latent trajectories, and limited
extrapolation ability over time. To address these issues, this paper introduces
a continuous operator learning framework that incorporates jerk regularization
into the learning of the compressed latent space. This jerk regularization
promotes smoothness and sparsity of latent space dynamics, which not only
yields enhanced accuracy and convergence speed but also helps identify
intrinsic latent space coordinates. Consisting of an implicit neural
representation (INR)-based autoencoder and a neural ODE latent dynamics model,
the framework allows for inference at any desired spatial or temporal
resolution. The effectiveness of this framework is demonstrated through a
two-dimensional unsteady flow problem governed by the Navier-Stokes equations,
highlighting its potential to expedite high-fidelity simulations in various
scientific and engineering applications.
\\ ( https://arxiv.org/abs/2402.15636 ,  7273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15638
Date: Fri, 23 Feb 2024 22:46:14 GMT   (152kb,D)

Title: Fair Resource Allocation in Multi-Task Learning
Authors: Hao Ban, Kaiyi Ji
Categories: cs.LG
\\
  By jointly learning multiple tasks, multi-task learning (MTL) can leverage
the shared knowledge across tasks, resulting in improved data efficiency and
generalization performance. However, a major challenge in MTL lies in the
presence of conflicting gradients, which can hinder the fair optimization of
some tasks and subsequently impede MTL's ability to achieve better overall
performance. Inspired by fair resource allocation in communication networks, we
formulate the optimization of MTL as a utility maximization problem, where the
loss decreases across tasks are maximized under different fairness
measurements. To solve this problem, we propose FairGrad, a novel MTL
optimization method. FairGrad not only enables flexible emphasis on certain
tasks but also achieves a theoretical convergence guarantee. Extensive
experiments demonstrate that our method can achieve state-of-the-art
performance among gradient manipulation methods on a suite of multi-task
benchmarks in supervised learning and reinforcement learning. Furthermore, we
incorporate the idea of $\alpha$-fairness into loss functions of various MTL
methods. Extensive empirical studies demonstrate that their performance can be
significantly enhanced. Code is provided at
\url{https://github.com/OptMN-Lab/fairgrad}.
\\ ( https://arxiv.org/abs/2402.15638 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15650
Date: Fri, 23 Feb 2024 23:22:06 GMT   (14kb)

Title: Multi-Constraint Safe RL with Objective Suppression for Safety-Critical
  Applications
Authors: Zihan Zhou, Jonathan Booher, Wei Liu, Aleksandr Petiushko, Animesh
  Garg
Categories: cs.LG cs.AI
\\
  Safe reinforcement learning tasks with multiple constraints are a challenging
domain despite being very common in the real world. To address this challenge,
we propose Objective Suppression, a novel method that adaptively suppresses the
task reward maximizing objectives according to a safety critic. We benchmark
Objective Suppression in two multi-constraint safety domains, including an
autonomous driving domain where any incorrect behavior can lead to disastrous
consequences. Empirically, we demonstrate that our proposed method, when
combined with existing safe RL algorithms, can match the task reward achieved
by our baselines with significantly fewer constraint violations.
\\ ( https://arxiv.org/abs/2402.15650 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15655
Date: Sat, 24 Feb 2024 00:09:27 GMT   (538kb,D)

Title: Contact Complexity in Customer Service
Authors: Shu-Ting Pi, Michael Yang, Qun Liu
Categories: cs.LG cs.AI
Comments: Accepted in KDD 2023 Workshop on Decision Intelligence and Analytics
  for Online Marketplaces
\\
  Customers who reach out for customer service support may face a range of
issues that vary in complexity. Routing high-complexity contacts to junior
agents can lead to multiple transfers or repeated contacts, while directing
low-complexity contacts to senior agents can strain their capacity to assist
customers who need professional help. To tackle this, a machine learning model
that accurately predicts the complexity of customer issues is highly desirable.
However, defining the complexity of a contact is a difficult task as it is a
highly abstract concept. While consensus-based data annotation by experienced
agents is a possible solution, it is time-consuming and costly. To overcome
these challenges, we have developed a novel machine learning approach to define
contact complexity. Instead of relying on human annotation, we trained an AI
expert model to mimic the behavior of agents and evaluate each contact's
complexity based on how the AI expert responds. If the AI expert is uncertain
or lacks the skills to comprehend the contact transcript, it is considered a
high-complexity contact. Our method has proven to be reliable, scalable, and
cost-effective based on the collected data.
\\ ( https://arxiv.org/abs/2402.15655 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15656
Date: Sat, 24 Feb 2024 00:10:51 GMT   (1518kb,D)

Title: Learning Semilinear Neural Operators : A Unified Recursive Framework For
  Prediction And Data Assimilation
Authors: Ashutosh Singh, Ricardo Augusto Borsoi, Deniz Erdogmus, Tales Imbiriba
Categories: cs.LG cs.AI cs.NA math.NA
Comments: ICLR 2024
\\
  Recent advances in the theory of Neural Operators (NOs) have enabled fast and
accurate computation of the solutions to complex systems described by partial
differential equations (PDEs). Despite their great success, current NO-based
solutions face important challenges when dealing with spatio-temporal PDEs over
long time scales. Specifically, the current theory of NOs does not present a
systematic framework to perform data assimilation and efficiently correct the
evolution of PDE solutions over time based on sparsely sampled noisy
measurements. In this paper, we propose a learning-based state-space approach
to compute the solution operators to infinite-dimensional semilinear PDEs.
Exploiting the structure of semilinear PDEs and the theory of nonlinear
observers in function spaces, we develop a flexible recursive method that
allows for both prediction and data assimilation by combining prediction and
correction operations. The proposed framework is capable of producing fast and
accurate predictions over long time horizons, dealing with irregularly sampled
noisy measurements to correct the solution, and benefits from the decoupling
between the spatial and temporal dynamics of this class of PDEs. We show
through experiments on the Kuramoto-Sivashinsky, Navier-Stokes and Korteweg-de
Vries equations that the proposed model is robust to noise and can leverage
arbitrary amounts of measurements to correct its prediction over a long time
horizon with little computational overhead.
\\ ( https://arxiv.org/abs/2402.15656 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15665
Date: Sat, 24 Feb 2024 00:40:40 GMT   (2794kb,D)

Title: Teacher-Student Learning on Complexity in Intelligent Routing
Authors: Shu-Ting Pi, Michael Yang, Yuying Zhu, Qun Liu
Categories: cs.LG cs.AI
Comments: KDD 2023 Workshop on End-End Customer Journey Optimization
\\
  Customer service is often the most time-consuming aspect for e-commerce
websites, with each contact typically taking 10-15 minutes. Effectively routing
customers to appropriate agents without transfers is therefore crucial for
e-commerce success. To this end, we have developed a machine learning framework
that predicts the complexity of customer contacts and routes them to
appropriate agents accordingly. The framework consists of two parts. First, we
train a teacher model to score the complexity of a contact based on the
post-contact transcripts. Then, we use the teacher model as a data annotator to
provide labels to train a student model that predicts the complexity based on
pre-contact data only. Our experiments show that such a framework is successful
and can significantly improve customer experience. We also propose a useful
metric called complexity AUC that evaluates the effectiveness of customer
service at a statistical level.
\\ ( https://arxiv.org/abs/2402.15665 ,  2794kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15666
Date: Sat, 24 Feb 2024 00:41:16 GMT   (481kb,D)

Title: Universal Model in Online Customer Service
Authors: Shu-Ting Pi, Cheng-Ping Hsieh, Qun Liu, Yuying Zhu
Categories: cs.LG cs.AI cs.IR
Journal-ref: Companion Proceedings of the ACM Web Conference 2023
DOI: 10.1145/3543873.3587630
\\
  Building machine learning models can be a time-consuming process that often
takes several months to implement in typical business scenarios. To ensure
consistent model performance and account for variations in data distribution,
regular retraining is necessary. This paper introduces a solution for improving
online customer service in e-commerce by presenting a universal model for
predict-ing labels based on customer questions, without requiring training. Our
novel approach involves using machine learning techniques to tag customer
questions in transcripts and create a repository of questions and corresponding
labels. When a customer requests assistance, an information retrieval model
searches the repository for similar questions, and statistical analysis is used
to predict the corresponding label. By eliminating the need for individual
model training and maintenance, our approach reduces both the model development
cycle and costs. The repository only requires periodic updating to maintain
accuracy.
\\ ( https://arxiv.org/abs/2402.15666 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15679
Date: Sat, 24 Feb 2024 01:45:51 GMT   (738kb)

Title: Scalable Density-based Clustering with Random Projections
Authors: Haochuan Xu, Ninh Pham
Categories: cs.LG cs.CV
\\
  We present sDBSCAN, a scalable density-based clustering algorithm in high
dimensions with cosine distance. Utilizing the neighborhood-preserving property
of random projections, sDBSCAN can quickly identify core points and their
neighborhoods, the primary hurdle of density-based clustering. Theoretically,
sDBSCAN outputs a clustering structure similar to DBSCAN under mild conditions
with high probability. To further facilitate sDBSCAN, we present sOPTICS, a
scalable OPTICS for interactive exploration of the intrinsic clustering
structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and
Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is
significantly faster and provides higher accuracy than many other clustering
algorithms on real-world million-point data sets. On these data sets, sDBSCAN
and sOPTICS run in a few minutes, while the scikit-learn's counterparts demand
several hours or cannot run due to memory constraints.
\\ ( https://arxiv.org/abs/2402.15679 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15680
Date: Sat, 24 Feb 2024 01:47:56 GMT   (106kb,D)

Title: Overcoming Pitfalls in Graph Contrastive Learning Evaluation: Toward
  Comprehensive Benchmarks
Authors: Qian Ma, Hongliang Chi, Hengrui Zhang, Kay Liu, Zhiwei Zhang, Lu
  Cheng, Suhang Wang, Philip S. Yu, Yao Ma
Categories: cs.LG
\\
  The rise of self-supervised learning, which operates without the need for
labeled data, has garnered significant interest within the graph learning
community. This enthusiasm has led to the development of numerous Graph
Contrastive Learning (GCL) techniques, all aiming to create a versatile graph
encoder that leverages the wealth of unlabeled data for various downstream
tasks. However, the current evaluation standards for GCL approaches are flawed
due to the need for extensive hyper-parameter tuning during pre-training and
the reliance on a single downstream task for assessment. These flaws can skew
the evaluation away from the intended goals, potentially leading to misleading
conclusions. In our paper, we thoroughly examine these shortcomings and offer
fresh perspectives on how GCL methods are affected by hyper-parameter choices
and the choice of downstream tasks for their evaluation. Additionally, we
introduce an enhanced evaluation framework designed to more accurately gauge
the effectiveness, consistency, and overall capability of GCL methods.
\\ ( https://arxiv.org/abs/2402.15680 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15688
Date: Sat, 24 Feb 2024 02:16:42 GMT   (1165kb,D)

Title: Anchor-free Clustering based on Anchor Graph Factorization
Authors: Shikun Mei, Fangfang Li, Quanxue Gao, Ming Yang
Categories: cs.LG
\\
  Anchor-based methods are a pivotal approach in handling clustering of
large-scale data. However, these methods typically entail two distinct stages:
selecting anchor points and constructing an anchor graph. This bifurcation,
along with the initialization of anchor points, significantly influences the
overall performance of the algorithm. To mitigate these issues, we introduce a
novel method termed Anchor-free Clustering based on Anchor Graph Factorization
(AFCAGF). AFCAGF innovates in learning the anchor graph, requiring only the
computation of pairwise distances between samples. This process, achievable
through straightforward optimization, circumvents the necessity for explicit
selection of anchor points. More concretely, our approach enhances the Fuzzy
k-means clustering algorithm (FKM), introducing a new manifold learning
technique that obviates the need for initializing cluster centers.
Additionally, we evolve the concept of the membership matrix between cluster
centers and samples in FKM into an anchor graph encompassing multiple anchor
points and samples. Employing Non-negative Matrix Factorization (NMF) on this
anchor graph allows for the direct derivation of cluster labels, thereby
eliminating the requirement for further post-processing steps. To solve the
method proposed, we implement an alternating optimization algorithm that
ensures convergence. Empirical evaluations on various real-world datasets
underscore the superior efficacy of our algorithm compared to traditional
approaches.
\\ ( https://arxiv.org/abs/2402.15688 ,  1165kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15691
Date: Sat, 24 Feb 2024 02:29:10 GMT   (3038kb,D)

Title: Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles
Authors: Fan Yang, Pierre Le Bodic, Michael Kamp, Mario Boley
Categories: cs.LG stat.ML
Comments: 21 pages, 11 figures, accepted at AISTATS 2024
\\
  Gradient boosting of prediction rules is an efficient approach to learn
potentially interpretable yet accurate probabilistic models. However, actual
interpretability requires to limit the number and size of the generated rules,
and existing boosting variants are not designed for this purpose. Though
corrective boosting refits all rule weights in each iteration to minimise
prediction risk, the included rule conditions tend to be sub-optimal, because
commonly used objective functions fail to anticipate this refitting. Here, we
address this issue by a new objective function that measures the angle between
the risk gradient vector and the projection of the condition output vector onto
the orthogonal complement of the already selected conditions. This approach
correctly approximate the ideal update of adding the risk gradient itself to
the model and favours the inclusion of more general and thus shorter rules. As
we demonstrate using a wide range of prediction tasks, this significantly
improves the comprehensibility/accuracy trade-off of the fitted ensemble.
Additionally, we show how objective values for related rule conditions can be
computed incrementally to avoid any substantial computational overhead of the
new method.
\\ ( https://arxiv.org/abs/2402.15691 ,  3038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15700
Date: Sat, 24 Feb 2024 03:25:28 GMT   (8521kb,D)

Title: CoRelation: Boosting Automatic ICD Coding Through Contextualized Code
  Relation Learning
Authors: Junyu Luo, Xiaochen Wang, Jiaqi Wang, Aofei Chang, Yaqing Wang,
  Fenglong Ma
Categories: cs.LG cs.AI cs.CL
Comments: LREC-Coling 2024
\\
  Automatic International Classification of Diseases (ICD) coding plays a
crucial role in the extraction of relevant information from clinical notes for
proper recording and billing. One of the most important directions for boosting
the performance of automatic ICD coding is modeling ICD code relations.
However, current methods insufficiently model the intricate relationships among
ICD codes and often overlook the importance of context in clinical notes. In
this paper, we propose a novel approach, a contextualized and flexible
framework, to enhance the learning of ICD code representations. Our approach,
unlike existing methods, employs a dependent learning paradigm that considers
the context of clinical notes in modeling all possible code relations. We
evaluate our approach on six public ICD coding datasets and the experimental
results demonstrate the effectiveness of our approach compared to
state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.15700 ,  8521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15703
Date: Sat, 24 Feb 2024 03:41:09 GMT   (959kb,D)

Title: Is Offline Decision Making Possible with Only Few Samples? Reliable
  Decisions in Data-Starved Bandits via Trust Region Enhancement
Authors: Ruiqi Zhang, Yuexiang Zhai, Andrea Zanette
Categories: cs.LG cs.AI stat.ML
Comments: 22 pages
\\
  What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from
a dataset that contains just a single sample for each arm? Surprisingly, in
this work, we demonstrate that even in such a data-starved setting it may still
be possible to find a policy competitive with the optimal one. This paves the
way to reliable decision-making in settings where critical decisions must be
made by relying only on a handful of samples.
  Our analysis reveals that \emph{stochastic policies can be substantially
better} than deterministic ones for offline decision-making. Focusing on
offline multi-armed bandits, we design an algorithm called Trust Region of
Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different
from the predominant value-based lower confidence bound approach. Its design is
enabled by localization laws, critical radii, and relative pessimism. We prove
that its sample complexity is comparable to that of LCB on minimax problems
while being substantially lower on problems with very few samples.
  Finally, we consider an application to offline reinforcement learning in the
special case where the logging policies are known.
\\ ( https://arxiv.org/abs/2402.15703 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15710
Date: Sat, 24 Feb 2024 04:13:40 GMT   (57kb,D)

Title: A Statistical Analysis of Wasserstein Autoencoders for Intrinsically
  Low-dimensional Data
Authors: Saptarshi Chakraborty and Peter L. Bartlett
Categories: cs.LG math.ST stat.ML stat.TH
Comments: In the twelfth International Conference on Learning Representations
  (ICLR'24)
\\
  Variational Autoencoders (VAEs) have gained significant popularity among
researchers as a powerful tool for understanding unknown distributions based on
limited samples. This popularity stems partly from their impressive performance
and partly from their ability to provide meaningful feature representations in
the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to
not only improve model efficiency but also interpretability. However, there has
been limited focus on analyzing their statistical guarantees. The matter is
further complicated by the fact that the data distributions to which WAEs are
applied - such as natural images - are often presumed to possess an underlying
low-dimensional structure within a high-dimensional feature space, which
current theory does not adequately account for, rendering known bounds
inefficient. To bridge the gap between the theory and practice of WAEs, in this
paper, we show that WAEs can learn the data distributions when the network
architectures are properly chosen. We show that the convergence rates of the
expected excess risk in the number of samples for WAEs are independent of the
high feature dimension, instead relying only on the intrinsic dimension of the
data distribution.
\\ ( https://arxiv.org/abs/2402.15710 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15715
Date: Sat, 24 Feb 2024 04:40:27 GMT   (32345kb,D)

Title: Operator Learning: Algorithms and Analysis
Authors: Nikola B. Kovachki and Samuel Lanthaler and Andrew M. Stuart
Categories: cs.LG cs.NA math.NA
\\
  Operator learning refers to the application of ideas from machine learning to
approximate (typically nonlinear) operators mapping between Banach spaces of
functions. Such operators often arise from physical models expressed in terms
of partial differential equations (PDEs). In this context, such approximate
operators hold great potential as efficient surrogate models to complement
traditional numerical methods in many-query tasks. Being data-driven, they also
enable model discovery when a mathematical description in terms of a PDE is not
available. This review focuses primarily on neural operators, built on the
success of deep neural networks in the approximation of functions defined on
finite dimensional Euclidean spaces. Empirically, neural operators have shown
success in a variety of applications, but our theoretical understanding remains
incomplete. This review article summarizes recent progress and the current
state of our theoretical understanding of neural operators, focusing on an
approximation theoretic point of view.
\\ ( https://arxiv.org/abs/2402.15715 ,  32345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15730
Date: Sat, 24 Feb 2024 05:48:39 GMT   (789kb,D)

Title: Understanding Missingness in Time-series Electronic Health Records for
  Individualized Representation
Authors: Ghadeer O. Ghosheh, Jin Li, and Tingting Zhu
Categories: cs.LG
\\
  With the widespread of machine learning models for healthcare applications,
there is increased interest in building applications for personalized medicine.
Despite the plethora of proposed research for personalized medicine, very few
focus on representing missingness and learning from the missingness patterns in
time-series Electronic Health Records (EHR) data. The lack of focus on
missingness representation in an individualized way limits the full utilization
of machine learning applications towards true personalization. In this brief
communication, we highlight new insights into patterns of missingness with
real-world examples and implications of missingness in EHRs. The insights in
this work aim to bridge the gap between theoretical assumptions and practical
observations in real-world EHRs. We hope this work will open new doors for
exploring directions for better representation in predictive modelling for true
personalization.
\\ ( https://arxiv.org/abs/2402.15730 ,  789kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15731
Date: Sat, 24 Feb 2024 05:49:27 GMT   (1954kb)

Title: Clustering in Dynamic Environments: A Framework for Benchmark Dataset
  Generation With Heterogeneous Changes
Authors: Danial Yazdani, Juergen Branke, Mohammad Sadegh Khorshidi, Mohammad
  Nabi Omidvar, Xiaodong Li, Amir H. Gandomi and Xin Yao
Categories: cs.LG cs.NE
\\
  Clustering in dynamic environments is of increasing importance, with broad
applications ranging from real-time data analysis and online unsupervised
learning to dynamic facility location problems. While meta-heuristics have
shown promising effectiveness in static clustering tasks, their application for
tracking optimal clustering solutions or robust clustering over time in dynamic
environments remains largely underexplored. This is partly due to a lack of
dynamic datasets with diverse, controllable, and realistic dynamic
characteristics, hindering systematic performance evaluations of clustering
algorithms in various dynamic scenarios. This deficiency leads to a gap in our
understanding and capability to effectively design algorithms for clustering in
dynamic environments. To bridge this gap, this paper introduces the Dynamic
Dataset Generator (DDG). DDG features multiple dynamic Gaussian components
integrated with a range of heterogeneous, local, and global changes. These
changes vary in spatial and temporal severity, patterns, and domain of
influence, providing a comprehensive tool for simulating a wide range of
dynamic scenarios.
\\ ( https://arxiv.org/abs/2402.15731 ,  1954kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15734
Date: Sat, 24 Feb 2024 06:27:33 GMT   (9759kb,D)

Title: Data-Efficient Operator Learning via Unsupervised Pretraining and
  In-Context Learning
Authors: Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy
  Morozov, Michael W. Mahoney
Categories: cs.LG stat.ML
\\
  Recent years have witnessed the promise of coupling machine learning methods
and physical domain-specific insight for solving scientific problems based on
partial differential equations (PDEs). However, being data-intensive, these
methods still require a large amount of PDE data. This reintroduces the need
for expensive numerical PDE solutions, partially undermining the original goal
of avoiding these expensive simulations. In this work, seeking data efficiency,
we design unsupervised pretraining and in-context learning methods for PDE
operator learning. To reduce the need for training data with simulated
solutions, we pretrain neural operators on unlabeled PDE data using
reconstruction-based proxy tasks. To improve out-of-distribution performance,
we further assist neural operators in flexibly leveraging in-context learning
methods, without incurring extra training costs or designs. Extensive empirical
evaluations on a diverse set of PDEs demonstrate that our method is highly
data-efficient, more generalizable, and even outperforms conventional
vision-pretrained models.
\\ ( https://arxiv.org/abs/2402.15734 ,  9759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15739
Date: Sat, 24 Feb 2024 06:36:08 GMT   (3376kb,D)

Title: Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery
Authors: Yassir Jedra, William R\'eveillard, Stefan Stojanovic, Alexandre
  Proutiere
Categories: cs.LG stat.ML
\\
  We study contextual bandits with low-rank structure where, in each round, if
the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner
observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward
matrix. Successive contexts are generated randomly in an i.i.d. manner and are
revealed to the learner. For such bandits, we present efficient algorithms for
policy evaluation, best policy identification and regret minimization. For
policy evaluation and best policy identification, we show that our algorithms
are nearly minimax optimal. For instance, the number of samples required to
return an $\varepsilon$-optimal policy with probability at least $1-\delta$
typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret
minimization algorithm enjoys minimax guarantees scaling as
$r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the
proposed algorithms consist of two phases: they first leverage spectral methods
to estimate the left and right singular subspaces of the low-rank reward
matrix. We show that these estimates enjoy tight error guarantees in the
two-to-infinity norm. This in turn allows us to reformulate our problems as a
misspecified linear bandit problem with dimension roughly $r(m+n)$ and
misspecification controlled by the subspace recovery error, as well as to
design the second phase of our algorithms efficiently.
\\ ( https://arxiv.org/abs/2402.15739 ,  3376kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15751
Date: Sat, 24 Feb 2024 07:22:04 GMT   (449kb,D)

Title: Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM
  Fine-Tuning
Authors: Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh and Yang
  You
Categories: cs.LG cs.AI cs.CL
\\
  While fine-tuning large language models (LLMs) for specific tasks often
yields impressive results, it comes at the cost of memory inefficiency due to
back-propagation in gradient-based training. Memory-efficient Zeroth-order
(MeZO) optimizers, recently proposed to address this issue, only require
forward passes during training, making them more memory-friendly. However, the
quality of gradient estimates in zeroth order optimization often depends on the
data dimensionality, potentially explaining why MeZO still exhibits significant
performance drops compared to standard fine-tuning across various tasks.
Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper
introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization
approach that applies ZO only to a carefully chosen subset of parameters. We
propose a simple yet effective parameter selection scheme that yields
significant performance gains with Sparse-MeZO. Additionally, we develop a
memory-optimized implementation for sparse masking, ensuring the algorithm
requires only inference-level memory consumption, allowing Sparse-MeZO to
fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that
Sparse-MeZO consistently improves both performance and convergence speed over
MeZO without any overhead. For example, it achieves a 9\% absolute accuracy
improvement and 3.5x speedup over MeZO on the RTE task.
\\ ( https://arxiv.org/abs/2402.15751 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15757
Date: Sat, 24 Feb 2024 08:07:48 GMT   (14476kb,D)

Title: Batch Active Learning of Reward Functions from Human Preferences
Authors: Erdem B{\i}y{\i}k, Nima Anari, Dorsa Sadigh
Categories: cs.LG cs.AI cs.RO stat.ML
Comments: To appear in ACM Transactions on Human-Robot Interaction (THRI). 27
  pages, 12 figures, 2 tables. arXiv admin note: text overlap with
  arXiv:1810.04303
\\
  Data generation and labeling are often expensive in robot learning.
Preference-based learning is a concept that enables reliable labeling by
querying users with preference questions. Active querying methods are commonly
employed in preference-based learning to generate more informative data at the
expense of parallelization and computation time. In this paper, we develop a
set of novel algorithms, batch active preference-based learning methods, that
enable efficient learning of reward functions using as few data samples as
possible while still having short query generation times and also retaining
parallelizability. We introduce a method based on determinantal point processes
(DPP) for active batch generation and several heuristic-based alternatives.
Finally, we present our experimental results for a variety of robotics tasks in
simulation. Our results suggest that our batch active learning algorithm
requires only a few queries that are computed in a short amount of time. We
showcase one of our algorithms in a study to learn human users' preferences.
\\ ( https://arxiv.org/abs/2402.15757 ,  14476kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15776
Date: Sat, 24 Feb 2024 09:47:46 GMT   (1282kb,D)

Title: Truly No-Regret Learning in Constrained MDPs
Authors: Adrian M\"uller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao
  He
Categories: cs.LG stat.ML
\\
  Constrained Markov decision processes (CMDPs) are a common way to model
safety constraints in reinforcement learning. State-of-the-art methods for
efficiently solving CMDPs are based on primal-dual algorithms. For these
algorithms, all currently known regret bounds allow for error cancellations --
one can compensate for a constraint violation in one round with a strict
constraint satisfaction in another. This makes the online learning process
unsafe since it only guarantees safety for the final (mixture) policy but not
during learning. As Efroni et al. (2020) pointed out, it is an open question
whether primal-dual algorithms can provably achieve sublinear regret if we do
not allow error cancellations. In this paper, we give the first affirmative
answer. We first generalize a result on last-iterate convergence of regularized
primal-dual schemes to CMDPs with multiple constraints. Building upon this
insight, we propose a model-based primal-dual algorithm to learn in an unknown
CMDP. We prove that our algorithm achieves sublinear regret without error
cancellations.
\\ ( https://arxiv.org/abs/2402.15776 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15808
Date: Sat, 24 Feb 2024 13:08:39 GMT   (2660kb,D)

Title: Optimal Zero-Shot Detector for Multi-Armed Attacks
Authors: Federica Granese, Marco Romanelli, Pablo Piantanida
Categories: cs.LG cs.AI cs.CR
Comments: Accepted to appear in the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS), May 2nd - May 4th, 2024. arXiv admin
  note: substantial text overlap with arXiv:2302.02216
\\
  This paper explores a scenario in which a malicious actor employs a
multi-armed attack strategy to manipulate data samples, offering them various
avenues to introduce noise into the dataset. Our central objective is to
protect the data by detecting any alterations to the input. We approach this
defensive strategy with utmost caution, operating in an environment where the
defender possesses significantly less information compared to the attacker.
Specifically, the defender is unable to utilize any data samples for training a
defense model or verifying the integrity of the channel. Instead, the defender
relies exclusively on a set of pre-existing detectors readily available ``off
the shelf''. To tackle this challenge, we derive an innovative
information-theoretic defense approach that optimally aggregates the decisions
made by these detectors, eliminating the need for any training data. We further
explore a practical use-case scenario for empirical evaluation, where the
attacker possesses a pre-trained classifier and launches well-known adversarial
attacks against it. Our experiments highlight the effectiveness of our proposed
solution, even in scenarios that deviate from the optimal setup.
\\ ( https://arxiv.org/abs/2402.15808 ,  2660kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15815
Date: Sat, 24 Feb 2024 13:42:34 GMT   (2443kb)

Title: A Generative Machine Learning Model for Material Microstructure 3D
  Reconstruction and Performance Evaluation
Authors: Yilin Zheng and Zhigong Song
Categories: cs.LG cond-mat.mtrl-sci cs.CV
\\
  The reconstruction of 3D microstructures from 2D slices is considered to hold
significant value in predicting the spatial structure and physical properties
of materials.The dimensional extension from 2D to 3D is viewed as a highly
challenging inverse problem from the current technological
perspective.Recently,methods based on generative adversarial networks have
garnered widespread attention.However,they are still hampered by numerous
limitations,including oversimplified models,a requirement for a substantial
number of training samples,and difficulties in achieving model convergence
during training.In light of this,a novel generative model that integrates the
multiscale properties of U-net with and the generative capabilities of GAN has
been proposed.Based on this,the innovative construction of a multi-scale
channel aggregation module,a multi-scale hierarchical feature aggregation
module and a convolutional block attention mechanism can better capture the
properties of the material microstructure and extract the image information.The
model's accuracy is further improved by combining the image regularization loss
with the Wasserstein distance loss.In addition,this study utilizes the
anisotropy index to accurately distinguish the nature of the image,which can
clearly determine the isotropy and anisotropy of the image.It is also the first
time that the generation quality of material samples from different domains is
evaluated and the performance of the model itself is compared.The experimental
results demonstrate that the present model not only shows a very high
similarity between the generated 3D structures and real samples but is also
highly consistent with real data in terms of statistical data analysis.
\\ ( https://arxiv.org/abs/2402.15815 ,  2443kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15826
Date: Sat, 24 Feb 2024 14:29:30 GMT   (1038kb,D)

Title: Reward Design for Justifiable Sequential Decision-Making
Authors: Aleksa Sukovic, Goran Radanovic
Categories: cs.LG cs.AI
\\
  Equipping agents with the capacity to justify made decisions using supporting
evidence represents a cornerstone of accountable decision-making. Furthermore,
ensuring that justifications are in line with human expectations and societal
norms is vital, especially in high-stakes situations such as healthcare. In
this work, we propose the use of a debate-based reward model for reinforcement
learning agents, where the outcome of a zero-sum debate game quantifies the
justifiability of a decision in a particular state. This reward model is then
used to train a justifiable policy, whose decisions can be more easily
corroborated with supporting evidence. In the debate game, two argumentative
agents take turns providing supporting evidence for two competing decisions.
Given the proposed evidence, a proxy of a human judge evaluates which decision
is better justified. We demonstrate the potential of our approach in learning
policies for prescribing and justifying treatment decisions of septic patients.
We show that augmenting the reward with the feedback signal generated by the
debate-based reward model yields policies highly favored by the judge when
compared to the policy obtained solely from the environment rewards, while
hardly sacrificing any performance. Moreover, in terms of the overall
performance and justifiability of trained policies, the debate-based feedback
is comparable to the feedback obtained from an ideal judge proxy that evaluates
decisions using the full information encoded in the state. This suggests that
the debate game outputs key information contained in states that is most
relevant for evaluating decisions, which in turn substantiates the practicality
of combining our approach with human-in-the-loop evaluations. Lastly, we
showcase that agents trained via multi-agent debate learn to propose evidence
that is resilient to refutations and closely aligns with human preferences.
\\ ( https://arxiv.org/abs/2402.15826 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15864
Date: Sat, 24 Feb 2024 17:13:58 GMT   (3690kb,D)

Title: Field-based Molecule Generation
Authors: Alexandru Dumitrescu, Dani Korpela, Markus Heinonen, Yogesh Verma,
  Valerii Iakovlev, Vikas Garg, Harri L\"ahdesm\"aki
Categories: cs.LG physics.chem-ph q-bio.BM
Comments: 15 pages, 14 figures
\\
  This work introduces FMG, a field-based model for drug-like molecule
generation. We show how the flexibility of this method provides crucial
advantages over the prevalent, point-cloud based methods, and achieves
competitive molecular stability generation. We tackle optical isomerism
(enantiomers), a previously omitted molecular property that is crucial for drug
safety and effectiveness, and thus account for all molecular geometry aspects.
We demonstrate how previous methods are invariant to a group of transformations
that includes enantiomer pairs, leading them invariant to the molecular R and S
configurations, while our field-based generative model captures this property.
\\ ( https://arxiv.org/abs/2402.15864 ,  3690kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15883
Date: Sat, 24 Feb 2024 19:06:41 GMT   (47kb,D)

Title: Fusion Encoder Networks
Authors: Stephen Pasteris, Chris Hicks, Vasilios Mavroudis
Categories: cs.LG
\\
  In this paper we present fusion encoder networks (FENs): a class of
algorithms for creating neural networks that map fixed-length sequences to
outputs. The resulting neural network has only logarithmic depth (alleviating
the degradation of data as it propagates through the network) and can process
sequences in linear time (or in logarithmic time with a linear number of
processors). The crucial property of FENs is that they learn by training a
quasi-linear number of constant-depth neural networks in parallel. The fact
that these networks are constant depth means that backpropagation works well.
We note that currently the performance of FENs is only conjectured as we are
yet to implement them.
\\ ( https://arxiv.org/abs/2402.15883 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15898
Date: Tue, 13 Feb 2024 09:22:45 GMT   (2246kb,D)

Title: Information-based Transductive Active Learning
Authors: Jonas H\"ubotter, Bhavya Sukhija, Lenart Treven, Yarden As, Andreas
  Krause
Categories: cs.LG cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2402.15441
\\
  We generalize active learning to address real-world settings where sampling
is restricted to an accessible region of the domain, while prediction targets
may lie outside this region. To this end, we propose ITL, short for
information-based transductive learning, an approach which samples adaptively
to maximize the information gained about specified prediction targets. We show,
under general regularity assumptions, that ITL converges uniformly to the
smallest possible uncertainty obtainable from the accessible data. We
demonstrate ITL in two key applications: Few-shot fine-tuning of large neural
networks and safe Bayesian optimization, and in both cases, ITL significantly
outperforms the state-of-the-art.
\\ ( https://arxiv.org/abs/2402.15898 ,  2246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15903
Date: Sat, 24 Feb 2024 20:50:29 GMT   (2412kb,D)

Title: ESFL: Efficient Split Federated Learning over Resource-Constrained
  Heterogeneous Wireless Devices
Authors: Guangyu Zhu, Yiqin Deng, Xianhao Chen, Haixia Zhang, Yuguang Fang, Tan
  F. Wong
Categories: cs.LG cs.AI cs.NI
\\
  Federated learning (FL) allows multiple parties (distributed devices) to
train a machine learning model without sharing raw data. How to effectively and
efficiently utilize the resources on devices and the central server is a highly
interesting yet challenging problem. In this paper, we propose an efficient
split federated learning algorithm (ESFL) to take full advantage of the
powerful computing capabilities at a central server under a split federated
learning framework with heterogeneous end devices (EDs). By splitting the model
into different submodels between the server and EDs, our approach jointly
optimizes user-side workload and server-side computing resource allocation by
considering users' heterogeneity. We formulate the whole optimization problem
as a mixed-integer non-linear program, which is an NP-hard problem, and develop
an iterative approach to obtain an approximate solution efficiently. Extensive
simulations have been conducted to validate the significantly increased
efficiency of our ESFL approach compared with standard federated learning,
split learning, and splitfed learning.
\\ ( https://arxiv.org/abs/2402.15903 ,  2412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15921
Date: Sat, 24 Feb 2024 22:32:34 GMT   (2339kb,D)

Title: Pretraining Strategy for Neural Potentials
Authors: Zehua Zhang, Zijie Li, Amir Barati Farimani
Categories: cs.LG physics.chem-ph
\\
  We propose a mask pretraining method for Graph Neural Networks (GNNs) to
improve their performance on fitting potential energy surfaces, particularly in
water systems. GNNs are pretrained by recovering spatial information related to
masked-out atoms from molecules, then transferred and finetuned on atomic
forcefields. Through such pretraining, GNNs learn meaningful prior about
structural and underlying physical information of molecule systems that are
useful for downstream tasks. From comprehensive experiments and ablation
studies, we show that the proposed method improves the accuracy and convergence
speed compared to GNNs trained from scratch or using other pretraining
techniques such as denoising. On the other hand, our pretraining method is
suitable for both energy-centric and force-centric GNNs. This approach
showcases its potential to enhance the performance and data efficiency of GNNs
in fitting molecular force fields.
\\ ( https://arxiv.org/abs/2402.15921 ,  2339kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15923
Date: Sat, 24 Feb 2024 22:36:23 GMT   (740kb,D)

Title: Predicting Outcomes in Video Games with Long Short Term Memory Networks
Authors: Kittimate Chulajata, Sean Wu, Fabien Scalzo, Eun Sang Cha
Categories: cs.LG cs.AI cs.MM
Comments: 7 pages, 2 Figures, 2 Tables. Kittimate Chulajata and Sean Wu are
  considered co-first authors
\\
  Forecasting winners in E-sports with real-time analytics has the potential to
further engage audiences watching major tournament events. However, making such
real-time predictions is challenging due to unpredictable variables within the
game involving diverse player strategies and decision-making. Our work attempts
to enhance audience engagement within video game tournaments by introducing a
real-time method of predicting wins. Our Long Short Term Memory Network (LSTMs)
based approach enables efficient predictions of win-lose outcomes by only using
the health indicator of each player as a time series. As a proof of concept, we
evaluate our model's performance within a classic, two-player arcade game,
Super Street Fighter II Turbo. We also benchmark our method against state of
the art methods for time series forecasting; i.e. Transformer models found in
large language models (LLMs). Finally, we open-source our data set and code in
hopes of furthering work in predictive analysis for arcade games.
\\ ( https://arxiv.org/abs/2402.15923 ,  740kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15926
Date: Sat, 24 Feb 2024 23:10:28 GMT   (148kb,D)

Title: Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of
  the Loss Improves Optimization Efficiency
Authors: Jingfeng Wu, Peter L. Bartlett, Matus Telgarsky, and Bin Yu
Categories: cs.LG stat.ML
\\
  We consider gradient descent (GD) with a constant stepsize applied to
logistic regression with linearly separable data, where the constant stepsize
$\eta$ is so large that the loss initially oscillates. We show that GD exits
this initial oscillatory phase rapidly -- in $\mathcal{O}(\eta)$ steps -- and
subsequently achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate
after $t$ additional steps. Our results imply that, given a budget of $T$
steps, GD can achieve an accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with
an aggressive stepsize $\eta:= \Theta( T)$, without any use of momentum or
variable stepsize schedulers. Our proof technique is versatile and also handles
general classification loss functions (where exponential tails are needed for
the $\tilde{\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the
neural tangent kernel regime, and online stochastic gradient descent (SGD) with
a large stepsize, under suitable separability conditions.
\\ ( https://arxiv.org/abs/2402.15926 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15932
Date: Sat, 24 Feb 2024 23:25:35 GMT   (1926kb,D)

Title: Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A
  Reinforcement Learning Approach
Authors: Alaa Selim, Yanzhu Ye, Junbo Zhao, Bo Yang
Categories: cs.LG cs.SY eess.SY
\\
  In the rapidly evolving domain of electrical power systems, the Volt-VAR
optimization (VVO) is increasingly critical, especially with the burgeoning
integration of renewable energy sources. Traditional approaches to
learning-based VVO in expansive and dynamically changing power systems are
often hindered by computational complexities. To address this challenge, our
research presents a novel framework that harnesses the potential of Deep
Reinforcement Learning (DRL), specifically utilizing the Importance Weighted
Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform.
This framework, built upon RLlib-an industry-standard in Reinforcement
Learning-ingeniously capitalizes on the distributed computing capabilities and
advanced hyperparameter tuning offered by RAY. This design significantly
expedites the exploration and exploitation phases in the VVO solution space.
Our empirical results demonstrate that our approach not only surpasses existing
DRL methods in achieving superior reward outcomes but also manifests a
remarkable tenfold reduction in computational requirements. The integration of
our DRL agent with the RAY platform facilitates the creation of RLlib-IMPALA, a
novel framework that efficiently uses RAY's resources to improve system
adaptability and control. RLlib-IMPALA leverages RAY's toolkit to enhance
analytical capabilities and significantly speeds up training to become more
than 10 times faster than other state-of-the-art DRL methods.
\\ ( https://arxiv.org/abs/2402.15932 ,  1926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15951
Date: Sun, 25 Feb 2024 01:56:47 GMT   (8799kb,D)

Title: GreenLLaMA: A Framework for Detoxification with Explanations
Authors: Md Tawkat Islam Khondaker, Muhammad Abdul-Mageed, Laks V. S.
  Lakshmanan
Categories: cs.LG cs.CL cs.CY
Comments: 24 pages
\\
  Prior works on detoxification are scattered in the sense that they do not
cover all aspects of detoxification needed in a real-world scenario. Notably,
prior works restrict the task of developing detoxification models to only a
seen subset of platforms, leaving the question of how the models would perform
on unseen platforms unexplored. Additionally, these works do not address
non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified
without altering the meaning. We propose GreenLLaMA, the first comprehensive
end-to-end detoxification framework, which attempts to alleviate the
aforementioned limitations. We first introduce a cross-platform pseudo-parallel
corpus applying multi-step data processing and generation strategies leveraging
ChatGPT. We then train a suite of detoxification models with our cross-platform
corpus. We show that our detoxification models outperform the SoTA model
trained with human-annotated parallel corpus. We further introduce explanation
to promote transparency and trustworthiness. GreenLLaMA additionally offers a
unique paraphrase detector especially dedicated for the detoxification task to
tackle the non-detoxifiable cases. Through experimental analysis, we
demonstrate the effectiveness of our cross-platform corpus and the robustness
of GreenLLaMA against adversarial toxicity.
\\ ( https://arxiv.org/abs/2402.15951 ,  8799kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15957
Date: Sun, 25 Feb 2024 02:36:03 GMT   (1903kb,D)

Title: DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement
  Learning
Authors: Anthony Liang, Guy Tennenholtz, Chih-wei Hsu, Yinlam Chow, Erdem
  B{\i}y{\i}k, Craig Boutilier
Categories: cs.LG
\\
  We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to
approximate inference in environments where the latent state evolves at varying
rates. We model episode sessions - parts of the episode where the latent state
is fixed - and propose three key modifications to existing meta-RL methods:
consistency of latent information within sessions, session masking, and prior
latent conditioning. We demonstrate the importance of these modifications in
various domains, ranging from discrete Gridworld environments to
continuous-control and simulated robot assistive tasks, demonstrating that
DynaMITE-RL significantly outperforms state-of-the-art baselines in sample
efficiency and inference returns.
\\ ( https://arxiv.org/abs/2402.15957 ,  1903kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15958
Date: Sun, 25 Feb 2024 02:36:14 GMT   (1195kb,D)

Title: On the dynamics of three-layer neural networks: initial condensation
Authors: Zheng-an Chen, Tao Luo
Categories: cs.LG math.DS
MSC-class: 37N40, 68T07, 34E05, 34C11
\\
  Empirical and theoretical works show that the input weights of two-layer
neural networks, when initialized with small values, converge towards isolated
orientations. This phenomenon, referred to as condensation, indicates that the
gradient descent methods tend to spontaneously reduce the complexity of neural
networks during the training process. In this work, we elucidate the mechanisms
behind the condensation phenomena occurring in the training of three-layer
neural networks and distinguish it from the training of two-layer neural
networks. Through rigorous theoretical analysis, we establish the blow-up
property of effective dynamics and present a sufficient condition for the
occurrence of condensation, findings that are substantiated by experimental
results. Additionally, we explore the association between condensation and the
low-rank bias observed in deep matrix factorization.
\\ ( https://arxiv.org/abs/2402.15958 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15962
Date: Sun, 25 Feb 2024 02:54:14 GMT   (467kb)

Title: Hierarchical energy signatures using machine learning for operational
  visibility and diagnostics in automotive manufacturing
Authors: Ankur Verma, Seog-Chan Oh, Jorge Arinez, Soundar Kumara
Categories: cs.LG
Comments: 5 pages, 4 figures
\\
  Manufacturing energy consumption data contains important process signatures
required for operational visibility and diagnostics. These signatures may be of
different temporal scales, ranging from monthly to sub-second resolutions. We
introduce a hierarchical machine learning approach to identify automotive
process signatures from paint shop electricity consumption data at varying
temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a
Convolutional Neural Network (CNN), and Principal Component Analysis (PCA)
combined with Logistic Regression (LR) are used for the analysis. We validate
the utility of the developed algorithms with subject matter experts for (i)
better operational visibility, and (ii) identifying energy saving
opportunities.
\\ ( https://arxiv.org/abs/2402.15962 ,  467kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15968
Date: Sun, 25 Feb 2024 03:07:32 GMT   (13803kb,D)

Title: CoDream: Exchanging dreams instead of models for federated aggregation
  with heterogeneous models
Authors: Abhishek Singh, Gauri Gupta, Ritvik Kapila, Yichuan Shi, Alex Dang,
  Sheshank Shankar, Mohammed Ehab, Ramesh Raskar
Categories: cs.LG cs.AI
Comments: 16 pages, 12 figures, 5 tables
\\
  Federated Learning (FL) enables collaborative optimization of machine
learning models across decentralized data by aggregating model parameters. Our
approach extends this concept by aggregating "knowledge" derived from models,
instead of model parameters. We present a novel framework called \codream,
where clients collaboratively optimize randomly initialized data using
federated optimization in the input data space, similar to how randomly
initialized model parameters are optimized in FL. Our key insight is that
jointly optimizing this data can effectively capture the properties of the
global data distribution. Sharing knowledge in data space offers numerous
benefits: (1) model-agnostic collaborative learning, i.e., different clients
can have different model architectures; (2) communication that is independent
of the model size, eliminating scalability concerns with model parameters; (3)
compatibility with secure aggregation, thus preserving the privacy benefits of
federated learning; (4) allowing of adaptive optimization of knowledge shared
for personalized learning. We empirically validate \codream on standard FL
tasks, demonstrating competitive performance despite not sharing model
parameters. Our code: https://mitmedialab.github.io/codream.github.io/
\\ ( https://arxiv.org/abs/2402.15968 ,  13803kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15972
Date: Sun, 25 Feb 2024 03:31:59 GMT   (16288kb,D)

Title: Structural Knowledge-Driven Meta-Learning for Task Offloading in
  Vehicular Networks with Integrated Communications, Sensing and Computing
Authors: Ruijin Sun, Yao Wen, Nan Cheng, Wei Wan, Rong Chai, Yilong Hui
Categories: cs.LG cs.NI
\\
  Task offloading is a potential solution to satisfy the strict requirements of
computation-intensive and latency-sensitive vehicular applications due to the
limited onboard computing resources. However, the overwhelming upload traffic
may lead to unacceptable uploading time. To tackle this issue, for tasks taking
environmental data as input, the data perceived by roadside units (RSU)
equipped with several sensors can be directly exploited for computation,
resulting in a novel task offloading paradigm with integrated communications,
sensing and computing (I-CSC). With this paradigm, vehicles can select to
upload their sensed data to RSUs or transmit computing instructions to RSUs
during the offloading. By optimizing the computation mode and network
resources, in this paper, we investigate an I-CSC-based task offloading problem
to reduce the cost caused by resource consumption while guaranteeing the
latency of each task. Although this non-convex problem can be handled by the
alternating minimization (AM) algorithm that alternatively minimizes the
divided four sub-problems, it leads to high computational complexity and local
optimal solution. To tackle this challenge, we propose a creative structural
knowledge-driven meta-learning (SKDML) method, involving both the model-based
AM algorithm and neural networks. Specifically, borrowing the iterative
structure of the AM algorithm, also referred to as structural knowledge, the
proposed SKDML adopts long short-term memory (LSTM) network-based meta-learning
to learn an adaptive optimizer for updating variables in each sub-problem,
instead of the handcrafted counterpart in the AM algorithm.
\\ ( https://arxiv.org/abs/2402.15972 ,  16288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15978
Date: Sun, 25 Feb 2024 03:48:13 GMT   (7516kb,D)

Title: Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural
  Networks Using the Marginal Likelihood
Authors: Rayen Dhahri, Alexander Immer, Betrand Charpentier, Stephan
  G\"unnemann, Vincent Fortuin
Categories: cs.LG stat.ML
\\
  Neural network sparsification is a promising avenue to save computational
time and memory costs, especially in an age where many successful AI models are
becoming too large to na\"ively deploy on consumer hardware. While much work
has focused on different weight pruning criteria, the overall sparsifiability
of the network, i.e., its capacity to be pruned without quality loss, has often
been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM),
a pruning framework that highlights the effectiveness of using the Bayesian
marginal likelihood in conjunction with sparsity-inducing priors for making
neural networks more sparsifiable. Our approach implements an automatic Occam's
razor that selects the most sparsifiable model that still explains the data
well, both for structured and unstructured sparsification. In addition, we
demonstrate that the pre-computed posterior Hessian approximation used in the
Laplace approximation can be re-used to define a cheap pruning criterion, which
outperforms many existing (more expensive) approaches. We demonstrate the
effectiveness of our framework, especially at high sparsity levels, across a
range of different neural network architectures and datasets.
\\ ( https://arxiv.org/abs/2402.15978 ,  7516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15984
Date: Sun, 25 Feb 2024 04:30:04 GMT   (132kb,D)

Title: A unified Fourier slice method to derive ridgelet transform for a
  variety of depth-2 neural networks
Authors: Sho Sonoda, Isao Ishikawa, Masahiro Ikeda
Categories: cs.LG math.FA stat.ML
\\
  To investigate neural network parameters, it is easier to study the
distribution of parameters than to study the parameters in each neuron. The
ridgelet transform is a pseudo-inverse operator that maps a given function $f$
to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$
reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected
networks on a Euclidean space, the ridgelet transform has been discovered up to
the closed-form expression, thus we could describe how the parameters are
distributed. However, for a variety of modern neural network architectures, the
closed-form expression has not been known. In this paper, we explain a
systematic method using Fourier expressions to derive ridgelet transforms for a
variety of modern networks such as networks on finite fields $\mathbb{F}_p$,
group convolutional networks on abstract Hilbert space $\mathcal{H}$,
fully-connected networks on noncompact symmetric spaces $G/K$, and pooling
layers, or the $d$-plane ridgelet transform.
\\ ( https://arxiv.org/abs/2402.15984 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15992
Date: Sun, 25 Feb 2024 05:16:43 GMT   (284kb,D)

Title: A Machine Learning Approach to Detect Customer Satisfaction From
  Multiple Tweet Parameters
Authors: Md Mahmudul Hasan, Dr. Shaikh Anowarul Fattah
Categories: cs.LG cs.CL
\\
  Since internet technologies have advanced, one of the primary factors in
company development is customer happiness. Online platforms have become
prominent places for sharing reviews. Twitter is one of these platforms where
customers frequently post their thoughts. Reviews of flights on these platforms
have become a concern for the airline business. A positive review can help the
company grow, while a negative one can quickly ruin its revenue and reputation.
So it's vital for airline businesses to examine the feedback and experiences of
their customers and enhance their services to remain competitive. But studying
thousands of tweets and analyzing them to find the satisfaction of the customer
is quite a difficult task. This tedious process can be made easier by using a
machine learning approach to analyze tweets to determine client satisfaction
levels. Some work has already been done on this strategy to automate the
procedure using machine learning and deep learning techniques. However, they
are all purely concerned with assessing the text's sentiment. In addition to
the text, the tweet also includes the time, location, username, airline name,
and so on. This additional information can be crucial for improving the model's
outcome. To provide a machine learning based solution, this work has broadened
its perspective to include these qualities. And it has come as no surprise that
the additional features beyond text sentiment analysis produce better outcomes
in machine learning based models.
\\ ( https://arxiv.org/abs/2402.15992 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15993
Date: Sun, 25 Feb 2024 05:22:45 GMT   (522kb,D)

Title: Learning method for S4 with Diagonal State Space Layers using Balanced
  Truncation
Authors: Haruka Ezoe and Kazuhiro Sato
Categories: cs.LG
\\
  We introduce a novel learning method for Structured State Space Sequence (S4)
models incorporating Diagonal State Space (DSS) layers, tailored for processing
long-sequence data in edge intelligence applications, including sensor data
analysis and real-time analytics. This method utilizes the balanced truncation
technique, prevalent in control theory, applied specifically to DSS layers to
reduce computational costs during inference. By leveraging parameters from the
reduced model, we refine the initialization process of S4 models, outperforming
the widely used Skew-HiPPo initialization in terms of performance. Numerical
experiments demonstrate that our trained S4 models with DSS layers surpass
conventionally trained models in accuracy and efficiency metrics. Furthermore,
our observations reveal a positive correlation: higher accuracy in the original
model consistently leads to increased accuracy in models trained using our
method, suggesting that our approach effectively leverages the strengths of the
original model.
\\ ( https://arxiv.org/abs/2402.15993 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16012
Date: Sun, 25 Feb 2024 07:03:37 GMT   (2451kb,D)

Title: Deep Contrastive Graph Learning with Clustering-Oriented Guidance
Authors: Mulin Chen, Bocheng Wang, Xuelong Li
Categories: cs.LG
Comments: Accept at AAAI24
\\
  Graph Convolutional Network (GCN) has exhibited remarkable potential in
improving graph-based clustering. To handle the general clustering scenario
without a prior graph, these models estimate an initial graph beforehand to
apply GCN. Throughout the literature, we have witnessed that 1) most models
focus on the initial graph while neglecting the original features. Therefore,
the discriminability of the learned representation may be corrupted by a
low-quality initial graph; 2) the training procedure lacks effective clustering
guidance, which may lead to the incorporation of clustering-irrelevant
information into the learned graph. To tackle these problems, the Deep
Contrastive Graph Learning (DCGL) model is proposed for general data
clustering. Specifically, we establish a pseudo-siamese network, which
incorporates auto-encoder with GCN to emphasize both the graph structure and
the original features. On this basis, feature-level contrastive learning is
introduced to enhance the discriminative capacity, and the relationship between
samples and centroids is employed as the clustering-oriented guidance.
Afterward, a two-branch graph learning mechanism is designed to extract the
local and global structural relationships, which are further embedded into a
unified graph under the cluster-level contrastive guidance. Experimental
results on several benchmark datasets demonstrate the superiority of DCGL
against state-of-the-art algorithms.
\\ ( https://arxiv.org/abs/2402.16012 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16014
Date: Sun, 25 Feb 2024 07:19:01 GMT   (3488kb,D)

Title: Building Flexible Machine Learning Models for Scientific Computing at
  Scale
Authors: Tianyu Chen, Haoyi Zhou, Ying Li, Hao Wang, Chonghan Gao, Shanghang
  Zhang, Jianxin Li
Categories: cs.LG cs.AI
Comments: Work in Progress
\\
  Foundation models have revolutionized knowledge acquisition across domains,
and our study introduces OmniArch, a paradigm-shifting approach designed for
building foundation models in multi-physics scientific computing. OmniArch's
pre-training involves a versatile pipeline that processes multi-physics
spatio-temporal data, casting forward problem learning into scalable
auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning
(PIRL) technique during fine-tuning ensures alignment with physical laws.
Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new
performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional
adaptability to new physics via few-shot and zero-shot learning approaches. The
model's representations further extend to inverse problem-solving, highlighting
the transformative potential of AI-enabled Scientific Computing(AI4SC)
foundation models for engineering applications and physics discovery.
\\ ( https://arxiv.org/abs/2402.16014 ,  3488kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16017
Date: Sun, 25 Feb 2024 07:28:28 GMT   (318kb,D)

Title: Spectrum Extraction and Clipping for Implicitly Linear Layers
Authors: Ali Ebrahimpour Boroojeny, Matus Telgarsky, Hari Sundaram
Categories: cs.LG cs.CV
\\
  We show the effectiveness of automatic differentiation in efficiently and
correctly computing and controlling the spectrum of implicitly linear
operators, a rich family of layer types including all standard convolutional
and dense layers. We provide the first clipping method which is correct for
general convolution layers, and illuminate the representational limitation that
caused correctness issues in prior work. We study the effect of the batch
normalization layers when concatenated with convolutional layers and show how
our clipping method can be applied to their composition. By comparing the
accuracy and performance of our algorithms to the state-of-the-art methods,
using various experiments, we show they are more precise and efficient and lead
to better generalization and adversarial robustness. We provide the code for
using our methods at https://github.com/Ali-E/FastClip.
\\ ( https://arxiv.org/abs/2402.16017 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16020
Date: Sun, 25 Feb 2024 07:41:08 GMT   (234kb)

Title: A Step-by-step Introduction to the Implementation of Automatic
  Differentiation
Authors: Yu-Hsueh Fang, He-Zhe Lin, Jie-Jyun Liu, and Chih-Jen Lin
Categories: cs.LG
Comments: 17 pages, 15 figures
\\
  Automatic differentiation is a key component in deep learning. This topic is
well studied and excellent surveys such as Baydin et al. (2018) have been
available to clearly describe the basic concepts. Further, sophisticated
implementations of automatic differentiation are now an important part of
popular deep learning frameworks. However, it is difficult, if not impossible,
to directly teach students the implementation of existing systems due to the
complexity. On the other hand, if the teaching stops at the basic concept,
students fail to sense the realization of an implementation. For example, we
often mention the computational graph in teaching automatic differentiation,
but students wonder how to implement and use it. In this document, we partially
fill the gap by giving a step by step introduction of implementing a simple
automatic differentiation system. We streamline the mathematical concepts and
the implementation. Further, we give the motivation behind each implementation
detail, so the whole setting becomes very natural.
\\ ( https://arxiv.org/abs/2402.16020 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16026
Date: Sun, 25 Feb 2024 08:20:05 GMT   (1416kb)

Title: Feature Selection Based on Orthogonal Constraints and Polygon Area
Authors: Zhenxing Zhang and Jun Ge and Zheng Wei and Chunjie Zhou and Yilei
  Wang
Categories: cs.LG
\\
  The goal of feature selection is to choose the optimal subset of features for
a recognition task by evaluating the importance of each feature, thereby
achieving effective dimensionality reduction. Currently, proposed feature
selection methods often overlook the discriminative dependencies between
features and labels. To address this problem, this paper introduces a novel
orthogonal regression model incorporating the area of a polygon. The model can
intuitively capture the discriminative dependencies between features and
labels. Additionally, this paper employs a hybrid non-monotone linear search
method to efficiently tackle the non-convex optimization challenge posed by
orthogonal constraints. Experimental results demonstrate that our approach not
only effectively captures discriminative dependency information but also
surpasses traditional methods in reducing feature dimensions and enhancing
classification performance.
\\ ( https://arxiv.org/abs/2402.16026 ,  1416kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16075
Date: Sun, 25 Feb 2024 12:19:21 GMT   (16762kb,D)

Title: Behavioral Refinement via Interpolant-based Policy Diffusion
Authors: Kaiqi Chen, Eugene Lim, Kelvin Lin, Yiyang Chen and Harold Soh
Categories: cs.LG cs.AI cs.RO
\\
  Imitation learning empowers artificial agents to mimic behavior by learning
from demonstrations. Recently, diffusion models, which have the ability to
model high-dimensional and multimodal distributions, have shown impressive
performance on imitation learning tasks. These models learn to shape a policy
by diffusing actions (or states) from standard Gaussian noise. However, the
target policy to be learned is often significantly different from Gaussian and
this mismatch can result in poor performance when using a small number of
diffusion steps (to improve inference speed) and under limited data. The key
idea in this work is that initiating from a more informative source than
Gaussian enables diffusion methods to overcome the above limitations. We
contribute both theoretical results, a new method, and empirical findings that
show the benefits of using an informative source policy. Our method, which we
call BRIDGER, leverages the stochastic interpolants framework to bridge
arbitrary policies, thus enabling a flexible approach towards imitation
learning. It generalizes prior work in that standard Gaussians can still be
applied, but other source policies can be used if available. In experiments on
challenging benchmarks, BRIDGER outperforms state-of-the-art diffusion policies
and we provide further analysis on design considerations when applying BRIDGER.
\\ ( https://arxiv.org/abs/2402.16075 ,  16762kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16077
Date: Sun, 25 Feb 2024 12:40:42 GMT   (2388kb,D)

Title: Equivariant Frames and the Impossibility of Continuous Canonicalization
Authors: Nadav Dym and Hannah Lawrence and Jonathan W. Siegel
Categories: cs.LG
\\
  Canonicalization provides an architecture-agnostic method for enforcing
equivariance, with generalizations such as frame-averaging recently gaining
prominence as a lightweight and flexible alternative to equivariant
architectures. Recent works have found an empirical benefit to using
probabilistic frames instead, which learn weighted distributions over group
elements. In this work, we provide strong theoretical justification for this
phenomenon: for commonly-used groups, there is no efficiently computable choice
of frame that preserves continuity of the function being averaged. In other
words, unweighted frame-averaging can turn a smooth, non-symmetric function
into a discontinuous, symmetric function. To address this fundamental
robustness problem, we formally define and construct \emph{weighted} frames,
which provably preserve continuity, and demonstrate their utility by
constructing efficient and continuous weighted frames for the actions of
$SO(2)$, $SO(3)$, and $S_n$ on point clouds.
\\ ( https://arxiv.org/abs/2402.16077 ,  2388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16078
Date: Sun, 25 Feb 2024 13:05:25 GMT   (3170kb,D)

Title: Beyond Spatio-Temporal Representations: Evolving Fourier Transform for
  Temporal Graphs
Authors: Anson Bastos, Kuldeep Singh, Abhishek Nadgeri, Manish Singh, Toyotaro
  Suzumura
Categories: cs.LG
Comments: Accepted as a full conference paper in the International Conference
  on Learning Representations 2024
\\
  We present the Evolving Graph Fourier Transform (EFT), the first invertible
spectral transform that captures evolving representations on temporal graphs.
We motivate our work by the inadequacy of existing methods for capturing the
evolving graph spectra, which are also computationally expensive due to the
temporal aspect along with the graph vertex domain. We view the problem as an
optimization over the Laplacian of the continuous time dynamic graph.
Additionally, we propose pseudo-spectrum relaxations that decompose the
transformation process, making it highly computationally efficient. The EFT
method adeptly captures the evolving graph's structural and positional
properties, making it effective for downstream tasks on evolving graphs. Hence,
as a reference implementation, we develop a simple neural model induced with
EFT for capturing evolving graph spectra. We empirically validate our
theoretical findings on a number of large-scale and standard temporal graph
benchmarks and demonstrate that our model achieves state-of-the-art
performance.
\\ ( https://arxiv.org/abs/2402.16078 ,  3170kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16091
Date: Sun, 25 Feb 2024 13:37:53 GMT   (144kb,D)

Title: Bayesian Neural Network For Personalized Federated Learning Parameter
  Selection
Authors: Mengen Luo, Ercan Engin Kuruoglu
Categories: cs.LG cs.AI
\\
  Federated learning's poor performance in the presence of heterogeneous data
remains one of the most pressing issues in the field. Personalized federated
learning departs from the conventional paradigm in which all clients employ the
same model, instead striving to discover an individualized model for each
client to address the heterogeneity in the data. One of such approach involves
personalizing specific layers of neural networks. However, prior endeavors have
not provided a dependable rationale, and some have selected personalized layers
that are entirely distinct and conflicting. In this work, we take a step
further by proposing personalization at the elemental level, rather than the
traditional layer-level personalization. To select personalized parameters, we
introduce Bayesian neural networks and rely on the uncertainty they offer to
guide our selection of personalized parameters. Finally, we validate our
algorithm's efficacy on several real-world datasets, demonstrating that our
proposed approach outperforms existing baselines.
\\ ( https://arxiv.org/abs/2402.16091 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16105
Date: Sun, 25 Feb 2024 15:08:37 GMT   (15160kb,D)

Title: Informed Meta-Learning
Authors: Katarzyna Kobalczyk, Mihaela van der Schaar
Categories: cs.LG
\\
  In noisy and low-data regimes prevalent in real-world applications, an
outstanding challenge of machine learning lies in effectively incorporating
inductive biases that promote data efficiency and robustness. Meta-learning and
informed ML stand out as two approaches for incorporating prior knowledge into
the ML pipeline. While the former relies on a purely data-driven source of
priors, the latter is guided by a formal representation of expert knowledge.
This paper introduces a novel hybrid paradigm, informed meta-learning, seeking
complementarity in cross-task knowledge sharing of humans and machines. We
establish the foundational components of informed meta-learning and present a
concrete instantiation of this framework--the Informed Neural Process. Through
a series of illustrative and larger-scale experiments, we demonstrate the
potential benefits of informed meta-learning in improving data efficiency and
robustness to observational noise, task distribution shifts, and heterogeneity.
\\ ( https://arxiv.org/abs/2402.16105 ,  15160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16119
Date: Sun, 25 Feb 2024 15:37:14 GMT   (16665kb,D)

Title: DeepForge: Leveraging AI for Microstructural Control in Metal Forming
  via Model Predictive Control
Authors: Jan Petrik and Markus Bambach
Categories: cs.LG cs.SY eess.SY
\\
  This study presents a novel method for microstructure control in closed die
hot forging that combines Model Predictive Control (MPC) with a developed
machine learning model called DeepForge. DeepForge uses an architecture that
combines 1D convolutional neural networks and gated recurrent units. It uses
surface temperature measurements of a workpiece as input to predict
microstructure changes during forging. The paper also details DeepForge's
architecture and the finite element simulation model used to generate the data
set, using a three-stroke forging process. The results demonstrate DeepForge's
ability to predict microstructure with a mean absolute error of 0.4$\pm$0.3%.
In addition, the study explores the use of MPC to adjust inter-stroke wait
times, effectively counteracting temperature disturbances to achieve a target
grain size of less than 35 microns within a specific 2D region of the
workpiece. These results are then verified experimentally, demonstrating a
significant step towards improved control and quality in forging processes
where temperature can be used as an additional degree of freedom in the
process.
\\ ( https://arxiv.org/abs/2402.16119 ,  16665kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16131
Date: Sun, 25 Feb 2024 16:11:32 GMT   (3379kb,D)

Title: A VAE-based Framework for Learning Multi-Level Neural Granger-Causal
  Connectivity
Authors: Jiahe Lin, Huitian Lei, George Michailidis
Categories: cs.LG stat.ME stat.ML
Comments: Accepted by Transactions on Machine Learning Research
\\
  Granger causality has been widely used in various application domains to
capture lead-lag relationships amongst the components of complex dynamical
systems, and the focus in extant literature has been on a single dynamical
system. In certain applications in macroeconomics and neuroscience, one has
access to data from a collection of related such systems, wherein the modeling
task of interest is to extract the shared common structure that is embedded
across them, as well as to identify the idiosyncrasies within individual ones.
This paper introduces a Variational Autoencoder (VAE) based framework that
jointly learns Granger-causal relationships amongst components in a collection
of related-yet-heterogeneous dynamical systems, and handles the aforementioned
task in a principled way. The performance of the proposed framework is
evaluated on several synthetic data settings and benchmarked against existing
approaches designed for individual system learning. The method is further
illustrated on a real dataset involving time series data from a
neurophysiological experiment and produces interpretable results.
\\ ( https://arxiv.org/abs/2402.16131 ,  3379kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16157
Date: Sun, 25 Feb 2024 17:35:31 GMT   (1471kb,D)

Title: Consensus learning: A novel decentralised ensemble learning paradigm
Authors: Horia Magureanu and Na\"iri Usher
Categories: cs.LG cs.DC
Comments: 27 pages plus appendices, 8 figures
\\
  The widespread adoption of large-scale machine learning models in recent
years highlights the need for distributed computing for efficiency and
scalability. This work introduces a novel distributed machine learning paradigm
-- \emph{consensus learning} -- which combines classical ensemble methods with
consensus protocols deployed in peer-to-peer systems. These algorithms consist
of two phases: first, participants develop their models and submit predictions
for any new data inputs; second, the individual predictions are used as inputs
for a communication phase, which is governed by a consensus protocol. Consensus
learning ensures user data privacy, while also inheriting the safety measures
against Byzantine attacks from the underlying consensus mechanism. We provide a
detailed theoretical analysis for a particular consensus protocol and compare
the performance of the consensus learning ensemble with centralised ensemble
learning algorithms. The discussion is supplemented by various numerical
simulations, which describe the robustness of the algorithms against Byzantine
participants.
\\ ( https://arxiv.org/abs/2402.16157 ,  1471kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16181
Date: Sun, 25 Feb 2024 20:07:13 GMT   (3413kb,D)

Title: How Can LLM Guide RL? A Value-Based Approach
Authors: Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo
  Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang
Categories: cs.LG cs.AI
\\
  Reinforcement learning (RL) has become the de facto standard practice for
sequential decision-making problems by improving future acting policies with
feedback. However, RL algorithms may require extensive trial-and-error
interactions to collect useful feedback for improvement. On the other hand,
recent developments in large language models (LLMs) have showcased impressive
capabilities in language understanding and generation, yet they fall short in
exploration and self-improvement capabilities for planning tasks, lacking the
ability to autonomously refine their responses based on feedback. Therefore, in
this paper, we study how the policy prior provided by the LLM can enhance the
sample efficiency of RL algorithms. Specifically, we develop an algorithm named
LINVIT that incorporates LLM guidance as a regularization factor in value-based
RL, leading to significant reductions in the amount of data needed for
learning, particularly when the difference between the ideal policy and the
LLM-informed policy is small, which suggests that the initial policy is close
to optimal, reducing the need for further exploration. Additionally, we present
a practical algorithm SLINVIT that simplifies the construction of the value
function and employs subgoals to reduce the search complexity. Our experiments
across three interactive environments ALFWorld, InterCode, and BlocksWorld
demonstrate that our method achieves state-of-the-art success rates and also
surpasses previous RL and LLM approaches in terms of sample efficiency. Our
code is available at https://github.com/agentification/Language-Integrated-VI.
\\ ( https://arxiv.org/abs/2402.16181 ,  3413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16184
Date: Sun, 25 Feb 2024 20:11:40 GMT   (9334kb,D)

Title: Deep Neural Network Initialization with Sparsity Inducing Activations
Authors: Ilan Price, Nicholas Daultry Ball, Samuel C.H. Lam, Adam C. Jones,
  Jared Tanner
Categories: cs.LG
Comments: Published in the International Conference on Learning Representations
  (ICLR) 2024
\\
  Inducing and leveraging sparse activations during training and inference is a
promising avenue for improving the computational efficiency of deep networks,
which is increasingly important as network sizes continue to grow and their
application becomes more widespread. Here we use the large width Gaussian
process limit to analyze the behaviour, at random initialization, of nonlinear
activations that induce sparsity in the hidden outputs. A previously unreported
form of training instability is proven for arguably two of the most natural
candidates for hidden layer sparsification; those being a shifted ReLU
($\phi(x)=\max(0, x-\tau)$ for $\tau\ge 0$) and soft thresholding ($\phi(x)=0$
for $|x|\le\tau$ and $x-\text{sign}(x)\tau$ for $|x|>\tau$). We show that this
instability is overcome by clipping the nonlinear activation magnitude, at a
level prescribed by the shape of the associated Gaussian process variance map.
Numerical experiments verify the theory and show that the proposed magnitude
clipped sparsifying activations can be trained with training and test
fractional sparsity as high as 85\% while retaining close to full accuracy.
\\ ( https://arxiv.org/abs/2402.16184 ,  9334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16196
Date: Sun, 25 Feb 2024 20:39:44 GMT   (4048kb,D)

Title: Combining Machine Learning with Computational Fluid Dynamics using
  OpenFOAM and SmartSim
Authors: Tomislav Maric and Mohammed Elwardi Fadeli and Alessandro Rigazzi and
  Andrew Shao and Andre Weiner
Categories: cs.LG physics.flu-dyn
\\
  Combining machine learning (ML) with computational fluid dynamics (CFD) opens
many possibilities for improving simulations of technical and natural systems.
However, CFD+ML algorithms require exchange of data, synchronization, and
calculation on heterogeneous hardware, making their implementation for
large-scale problems exceptionally challenging.
  We provide an effective and scalable solution to developing CFD+ML algorithms
using open source software OpenFOAM and SmartSim. SmartSim provides an
Orchestrator that significantly simplifies the programming of CFD+ML algorithms
and a Redis database that ensures highly scalable data exchange between ML and
CFD clients. We show how to leverage SmartSim to effectively couple different
segments of OpenFOAM with ML, including pre/post-processing applications,
solvers, function objects, and mesh motion solvers. We additionally provide an
OpenFOAM sub-module with examples that can be used as starting points for
real-world applications in CFD+ML.
\\ ( https://arxiv.org/abs/2402.16196 ,  4048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16230
Date: Mon, 26 Feb 2024 01:18:53 GMT   (7595kb,D)

Title: GARNN: An Interpretable Graph Attentive Recurrent Neural Network for
  Predicting Blood Glucose Levels via Multivariate Time Series
Authors: Chengzhe Piao, Taiyu Zhu, Stephanie E Baldeweg, Paul Taylor, Pantelis
  Georgiou, Jiahao Sun, Jun Wang, Kezhi Li
Categories: cs.LG cs.AI
\\
  Accurate prediction of future blood glucose (BG) levels can effectively
improve BG management for people living with diabetes, thereby reducing
complications and improving quality of life. The state of the art of BG
prediction has been achieved by leveraging advanced deep learning methods to
model multi-modal data, i.e., sensor data and self-reported event data,
organised as multi-variate time series (MTS). However, these methods are mostly
regarded as ``black boxes'' and not entirely trusted by clinicians and
patients. In this paper, we propose interpretable graph attentive recurrent
neural networks (GARNNs) to model MTS, explaining variable contributions via
summarizing variable importance and generating feature maps by graph attention
mechanisms instead of post-hoc analysis. We evaluate GARNNs on four datasets,
representing diverse clinical scenarios. Upon comparison with twelve
well-established baseline methods, GARNNs not only achieve the best prediction
accuracy but also provide high-quality temporal interpretability, in particular
for postprandial glucose levels as a result of corresponding meal intake and
insulin injection. These findings underline the potential of GARNN as a robust
tool for improving diabetes care, bridging the gap between deep learning
technology and real-world healthcare solutions.
\\ ( https://arxiv.org/abs/2402.16230 ,  7595kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16237
Date: Mon, 26 Feb 2024 01:46:56 GMT   (992kb,D)

Title: Active Level Set Estimation for Continuous Search Space with Theoretical
  Guarantee
Authors: Giang Ngo, Dang Nguyen, Dat Phan-Trong, Sunil Gupta
Categories: cs.LG cs.AI
\\
  A common problem encountered in many real-world applications is level set
estimation where the goal is to determine the region in the function domain
where the function is above or below a given threshold. When the function is
black-box and expensive to evaluate, the level sets need to be found in a
minimum set of function evaluations. Existing methods often assume a discrete
search space with a finite set of data points for function evaluations and
estimating the level sets. When applied to a continuous search space, these
methods often need to first discretize the space which leads to poor results
while needing high computational time. While some methods cater for the
continuous setting, they still lack a proper guarantee for theoretical
convergence. To address this problem, we propose a novel algorithm that does
not need any discretization and can directly work in continuous search spaces.
Our method suggests points by constructing an acquisition function that is
defined as a measure of confidence of the function being higher or lower than
the given threshold. A theoretical analysis for the convergence of the
algorithm to an accurate solution is provided. On multiple synthetic and
real-world datasets, our algorithm successfully outperforms state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2402.16237 ,  992kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16247
Date: Mon, 26 Feb 2024 02:13:36 GMT   (3570kb,D)

Title: Learning Translations: Emergent Communication Pretraining for
  Cooperative Language Acquisition
Authors: Dylan Cope and Peter McBurney
Categories: cs.LG cs.CL cs.MA
\\
  In Emergent Communication (EC) agents learn to communicate with one another,
but the protocols that they develop are specialised to their training
community. This observation led to research into Zero-Shot Coordination (ZSC)
for learning communication strategies that are robust to agents not encountered
during training. However, ZSC typically assumes that no prior data is available
about the agents that will be encountered in the zero-shot setting. In many
cases, this presents an unnecessarily hard problem and rules out communication
via preestablished conventions. We propose a novel AI challenge called a
Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions
are relaxed by allowing a 'joiner' agent to learn from a dataset of
interactions between agents in a target community. We propose and compare two
methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication
pretraining and Translation Learning (ECTL), in which an agent is trained in
self-play with EC and then learns from the data to translate between the
emergent protocol and the target community's protocol.
\\ ( https://arxiv.org/abs/2402.16247 ,  3570kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16255
Date: Mon, 26 Feb 2024 02:37:39 GMT   (574kb,D)

Title: Watch Your Head: Assembling Projection Heads to Save the Reliability of
  Federated Models
Authors: Jinqian Chen, Jihua Zhu, Qinghai Zheng, Zhongyu Li, Zhiqiang Tian
Categories: cs.LG cs.AI
Comments: Accepted in AAAI-24
\\
  Federated learning encounters substantial challenges with heterogeneous data,
leading to performance degradation and convergence issues. While considerable
progress has been achieved in mitigating such an impact, the reliability aspect
of federated models has been largely disregarded. In this study, we conduct
extensive experiments to investigate the reliability of both generic and
personalized federated models. Our exploration uncovers a significant finding:
\textbf{federated models exhibit unreliability when faced with heterogeneous
data}, demonstrating poor calibration on in-distribution test data and low
uncertainty levels on out-of-distribution data. This unreliability is primarily
attributed to the presence of biased projection heads, which introduce
miscalibration into the federated models. Inspired by this observation, we
propose the "Assembled Projection Heads" (APH) method for enhancing the
reliability of federated models. By treating the existing projection head
parameters as priors, APH randomly samples multiple initialized parameters of
projection heads from the prior and further performs targeted fine-tuning on
locally available data under varying learning rates. Such a head ensemble
introduces parameter diversity into the deterministic model, eliminating the
bias and producing reliable predictions via head averaging. We evaluate the
effectiveness of the proposed APH method across three prominent federated
benchmarks. Experimental results validate the efficacy of APH in model
calibration and uncertainty estimation. Notably, APH can be seamlessly
integrated into various federated approaches but only requires less than 30\%
additional computation cost for 100$\times$ inferences within large models.
\\ ( https://arxiv.org/abs/2402.16255 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16268
Date: Mon, 26 Feb 2024 03:09:06 GMT   (18520kb,D)

Title: Foundation Model Transparency Reports
Authors: Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash
  Kapoor, Nestor Maslej, Arvind Narayanan, Percy Liang
Categories: cs.LG cs.AI cs.CY
\\
  Foundation models are critical digital technologies with sweeping societal
impact that necessitates transparency. To codify how foundation model
developers should provide transparency about the development and deployment of
their models, we propose Foundation Model Transparency Reports, drawing upon
the transparency reporting practices in social media. While external
documentation of societal harms prompted social media transparency reports, our
objective is to institutionalize transparency reporting for foundation models
while the industry is still nascent. To design our reports, we identify 6
design principles given the successes and shortcomings of social media
transparency reporting. To further schematize our reports, we draw upon the 100
transparency indicators from the Foundation Model Transparency Index. Given
these indicators, we measure the extent to which they overlap with the
transparency requirements included in six prominent government policies (e.g.,
the EU AI Act, the US Executive Order on Safe, Secure, and Trustworthy AI).
Well-designed transparency reports could reduce compliance costs, in part due
to overlapping regulatory requirements across different jurisdictions. We
encourage foundation model developers to regularly publish transparency
reports, building upon recommendations from the G7 and the White House.
\\ ( https://arxiv.org/abs/2402.16268 ,  18520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16297
Date: Mon, 26 Feb 2024 04:39:01 GMT   (3895kb,D)

Title: Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics
Authors: Jiahao Wang, Sikun Yang, Heinz Koeppl, Xiuzhen Cheng, Pengfei Hu,
  Guoming Zhang
Categories: cs.LG cs.AI
\\
  Bayesian methodologies for handling count-valued time series have gained
prominence due to their ability to infer interpretable latent structures and to
estimate uncertainties, and thus are especially suitable for dealing with noisy
and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical
Systems (PGDSs) are proven to be effective in capturing the evolving dynamics
underlying observed count sequences. However, the state-of-the-art PGDS still
falls short in capturing the time-varying transition dynamics that are commonly
observed in real-world count time series. To mitigate this limitation, a
non-stationary PGDS is proposed to allow the underlying transition matrices to
evolve over time, and the evolving transition matrices are modeled by
sophisticatedly-designed Dirichlet Markov chains. Leveraging
Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and
efficient Gibbs sampler is developed to perform posterior simulation.
Experiments show that, in comparison with related models, the proposed
non-stationary PGDS achieves improved predictive performance due to its
capacity to learn non-stationary dependency structure captured by the
time-evolving transition matrices.
\\ ( https://arxiv.org/abs/2402.16297 ,  3895kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16300
Date: Mon, 26 Feb 2024 04:43:50 GMT   (388kb,D)

Title: Conformalized Selective Regression
Authors: Anna Sokol, Nuno Moniz, Nitesh Chawla
Categories: cs.LG stat.ML
\\
  Should prediction models always deliver a prediction? In the pursuit of
maximum predictive performance, critical considerations of reliability and
fairness are often overshadowed, particularly when it comes to the role of
uncertainty. Selective regression, also known as the "reject option," allows
models to abstain from predictions in cases of considerable uncertainty.
Initially proposed seven decades ago, approaches to selective regression have
mostly focused on distribution-based proxies for measuring uncertainty,
particularly conditional variance. However, this focus neglects the significant
influence of model-specific biases on a model's performance. In this paper, we
propose a novel approach to selective regression by leveraging conformal
prediction, which provides grounded confidence measures for individual
predictions based on model-specific biases. In addition, we propose a
standardized evaluation framework to allow proper comparison of selective
regression approaches. Via an extensive experimental approach, we demonstrate
how our proposed approach, conformalized selective regression, demonstrates an
advantage over multiple state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.16300 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16302
Date: Mon, 26 Feb 2024 04:58:42 GMT   (825kb,D)

Title: Graph Diffusion Policy Optimization
Authors: Yijing Liu, Chao Du, Tianyu Pang, Chongxuan Li, Wei Chen, Min Lin
Categories: cs.LG cs.AI cs.CE
\\
  Recent research has made significant progress in optimizing diffusion models
for specific downstream objectives, which is an important pursuit in fields
such as graph generation for drug design. However, directly applying these
models to graph diffusion presents challenges, resulting in suboptimal
performance. This paper introduces graph diffusion policy optimization (GDPO),
a novel approach to optimize graph diffusion models for arbitrary (e.g.,
non-differentiable) objectives using reinforcement learning. GDPO is based on
an eager policy gradient tailored for graph diffusion models, developed through
meticulous analysis and promising improved performance. Experimental results
show that GDPO achieves state-of-the-art performance in various graph
generation tasks with complex and diverse objectives. Code is available at
https://github.com/sail-sg/GDPO.
\\ ( https://arxiv.org/abs/2402.16302 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16305
Date: Mon, 26 Feb 2024 05:08:40 GMT   (21489kb,D)

Title: Referee Can Play: An Alternative Approach to Conditional Generation via
  Model Inversion
Authors: Xuantong Liu, Tianyang Hu, Wenjia Wang, Kenji Kawaguchi, Yuan Yao
Categories: cs.LG cs.AI
\\
  As a dominant force in text-to-image generation tasks, Diffusion
Probabilistic Models (DPMs) face a critical challenge in controllability,
struggling to adhere strictly to complex, multi-faceted instructions. In this
work, we aim to address this alignment challenge for conditional generation
tasks. First, we provide an alternative view of state-of-the-art DPMs as a way
of inverting advanced Vision-Language Models (VLMs). With this formulation, we
naturally propose a training-free approach that bypasses the conventional
sampling process associated with DPMs. By directly optimizing images with the
supervision of discriminative VLMs, the proposed method can potentially achieve
a better text-image alignment. As proof of concept, we demonstrate the pipeline
with the pre-trained BLIP-2 model and identify several key designs for improved
image generation. To further enhance the image fidelity, a Score Distillation
Sampling module of Stable Diffusion is incorporated. By carefully balancing the
two components during optimization, our method can produce high-quality images
with near state-of-the-art performance on T2I-Compbench.
\\ ( https://arxiv.org/abs/2402.16305 ,  21489kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16310
Date: Mon, 26 Feb 2024 05:28:36 GMT   (2368kb,D)

Title: REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility
  for Location Prediction over Sparse Trajectories
Authors: Bangchao Deng, Bingqing Qu, Pengyang Wang and Dingqi Yang
Categories: cs.LG cs.AI
\\
  Location prediction forecasts a user's location based on historical user
mobility traces. To tackle the intrinsic sparsity issue of real-world user
mobility traces, spatiotemporal contexts have been shown as significantly
useful. Existing solutions mostly incorporate spatiotemporal distances between
locations in mobility traces, either by feeding them as additional inputs to
Recurrent Neural Networks (RNNs) or by using them to search for informative
past hidden states for prediction. However, such distance-based methods fail to
capture the time-varying temporal regularities of human mobility, where human
mobility is often more regular in the morning than in other periods, for
example; this suggests the usefulness of the actual timestamps besides the
temporal distances. Against this background, we propose REPLAY, a general RNN
architecture learning to capture the time-varying temporal regularities for
location prediction. Specifically, REPLAY not only resorts to the
spatiotemporal distances in sparse trajectories to search for the informative
past hidden states, but also accommodates the time-varying temporal
regularities by incorporating smoothed timestamp embeddings using Gaussian
weighted averaging with timestamp-specific learnable bandwidths, which can
flexibly adapt to the temporal regularities of different strengths across
different timestamps. Our extensive evaluation compares REPLAY against a
sizable collection of state-of-the-art techniques on two real-world datasets.
Results show that REPLAY consistently and significantly outperforms
state-of-the-art methods by 7.7\%-10.9\% in the location prediction task, and
the bandwidths reveal interesting patterns of the time-varying temporal
regularities.
\\ ( https://arxiv.org/abs/2402.16310 ,  2368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16312
Date: Mon, 26 Feb 2024 05:31:14 GMT   (382kb,D)

Title: Federated Contextual Cascading Bandits with Asynchronous Communication
  and Heterogeneous Users
Authors: Hantao Yang, Xutong Liu, Zhiyong Wang, Hong Xie, John C. S. Lui, Defu
  Lian, Enhong Chen
Categories: cs.LG cs.AI
Comments: Accepted by AAAI 2024
\\
  We study the problem of federated contextual combinatorial cascading bandits,
where $|\mathcal{U}|$ agents collaborate under the coordination of a central
server to provide tailored recommendations to the $|\mathcal{U}|$ corresponding
users. Existing works consider either a synchronous framework, necessitating
full agent participation and global synchronization, or assume user homogeneity
with identical behaviors. We overcome these limitations by considering (1)
federated agents operating in an asynchronous communication paradigm, where no
mandatory synchronization is required and all agents communicate independently
with the server, (2) heterogeneous user behaviors, where users can be
stratified into $J \le |\mathcal{U}|$ latent user clusters, each exhibiting
distinct preferences. For this setting, we propose a UCB-type algorithm with
delicate communication protocols. Through theoretical analysis, we give
sub-linear regret bounds on par with those achieved in the synchronous
framework, while incurring only logarithmic communication costs. Empirical
evaluation on synthetic and real-world datasets validates our algorithm's
superior performance in terms of regrets and communication costs.
\\ ( https://arxiv.org/abs/2402.16312 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16324
Date: Mon, 26 Feb 2024 06:08:25 GMT   (95kb)

Title: Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained
  Markov Decision Process
Authors: Jiashuo Jiang and Yinyu Ye
Categories: cs.LG math.OC
\\
  We consider the reinforcement learning problem for the constrained Markov
decision process (CMDP), which plays a central role in satisfying safety or
resource constraints in sequential learning and decision-making. In this
problem, we are given finite resources and a MDP with unknown transition
probabilities. At each stage, we take an action, collecting a reward and
consuming some resources, all assumed to be unknown and need to be learned over
time. In this work, we take the first step towards deriving optimal
problem-dependent guarantees for the CMDP problems. We derive a logarithmic
regret bound, which translates into a
$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound,
with $\kappa$ being a problem-dependent parameter, yet independent of
$\epsilon$. Our sample complexity bound improves upon the state-of-art
$O(1/\epsilon^2)$ sample complexity for CMDP problems established in the
previous literature, in terms of the dependency on $\epsilon$. To achieve this
advance, we develop a new framework for analyzing CMDP problems. To be
specific, our algorithm operates in the primal space and we resolve the primal
LP for the CMDP problem at each period in an online manner, with
\textit{adaptive} remaining resource capacities. The key elements of our
algorithm are: i). an eliminating procedure that characterizes one optimal
basis of the primal LP, and; ii) a resolving procedure that is adaptive to the
remaining resources and sticks to the characterized optimal basis.
\\ ( https://arxiv.org/abs/2402.16324 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16346
Date: Mon, 26 Feb 2024 07:00:24 GMT   (1891kb,D)

Title: Boosting Graph Pooling with Persistent Homology
Authors: Chaolong Ying, Xinjian Zhao, Tianshu Yu
Categories: cs.LG math.AT
\\
  Recently, there has been an emerging trend to integrate persistent homology
(PH) into graph neural networks (GNNs) to enrich expressive power. However,
naively plugging PH features into GNN layers always results in marginal
improvement with low interpretability. In this paper, we investigate a novel
mechanism for injecting global topological invariance into pooling layers using
PH, motivated by the observation that filtration operation in PH naturally
aligns graph pooling in a cut-off manner. In this fashion, message passing in
the coarsened graph acts along persistent pooled topology, leading to improved
performance. Experimentally, we apply our mechanism to a collection of graph
pooling methods and observe consistent and substantial performance gain over
several popular datasets, demonstrating its wide applicability and flexibility.
\\ ( https://arxiv.org/abs/2402.16346 ,  1891kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16349
Date: Mon, 26 Feb 2024 07:07:00 GMT   (477kb,D)

Title: C-GAIL: Stabilizing Generative Adversarial Imitation Learning with
  Control Theory
Authors: Tianjiao Luo, Tim Pearce, Huayu Chen, Jianfei Chen, Jun Zhu
Categories: cs.LG cs.SY eess.SY
\\
  Generative Adversarial Imitation Learning (GAIL) trains a generative policy
to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to
optimize a reward signal derived from a GAN-like discriminator. A major
drawback of GAIL is its training instability - it inherits the complex training
dynamics of GANs, and the distribution shift introduced by RL. This can cause
oscillations during training, harming its sample efficiency and final policy
performance. Recent work has shown that control theory can help with the
convergence of a GAN's training. This paper extends this line of work,
conducting a control-theoretic analysis of GAIL and deriving a novel controller
that not only pushes GAIL to the desired equilibrium but also achieves
asymptotic stability in a 'one-step' setting. Based on this, we propose a
practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled
variant is able to speed up the rate of convergence, reduce the range of
oscillation and match the expert's distribution more closely both for vanilla
GAIL and GAIL-DAC.
\\ ( https://arxiv.org/abs/2402.16349 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16354
Date: Mon, 26 Feb 2024 07:19:23 GMT   (15466kb,D)

Title: Language-guided Skill Learning with Temporal Variational Inference
Authors: Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris,
  Nicolas Le Roux, Marc-Alexandre C\^ot\'e, Xingdi Yuan
Categories: cs.LG cs.AI cs.CL
\\
  We present an algorithm for skill discovery from expert demonstrations. The
algorithm first utilizes Large Language Models (LLMs) to propose an initial
segmentation of the trajectories. Following that, a hierarchical variational
inference framework incorporates the LLM-generated segmentation information to
discover reusable skills by merging trajectory segments. To further control the
trade-off between compression and reusability, we introduce a novel auxiliary
objective based on the Minimum Description Length principle that helps guide
this skill discovery process. Our results demonstrate that agents equipped with
our method are able to discover skills that help accelerate learning and
outperform baseline skill learning approaches on new long-horizon tasks in
BabyAI, a grid world navigation environment, as well as ALFRED, a household
simulation environment.
\\ ( https://arxiv.org/abs/2402.16354 ,  15466kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16358
Date: Mon, 26 Feb 2024 07:22:51 GMT   (1104kb,D)

Title: An Integrated Data Processing Framework for Pretraining Foundation
  Models
Authors: Yiding Sun, Feng Wang, Yutao Zhu, Wayne Xin Zhao, Jiaxin Mao
Categories: cs.LG cs.CL cs.IR
Comments: 6 pages, 2 figures
\\
  The ability of the foundation models heavily relies on large-scale, diverse,
and high-quality pretraining data. In order to improve data quality,
researchers and practitioners often have to manually curate datasets from
difference sources and develop dedicated data cleansing pipeline for each data
repository. Lacking a unified data processing framework, this process is
repetitive and cumbersome. To mitigate this issue, we propose a data processing
framework that integrates a Processing Module which consists of a series of
operators at different granularity levels, and an Analyzing Module which
supports probing and evaluation of the refined data. The proposed framework is
easy to use and highly flexible. In this demo paper, we first introduce how to
use this framework with some example use cases and then demonstrate its
effectiveness in improving the data quality with an automated evaluation with
ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code
and demonstration videos are accessible on GitHub.
\\ ( https://arxiv.org/abs/2402.16358 ,  1104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16359
Date: Mon, 26 Feb 2024 07:24:32 GMT   (6127kb,D)

Title: Feedback Efficient Online Fine-Tuning of Diffusion Models
Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali,
  Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Sergey Levine, Tommaso
  Biancalani
Categories: cs.LG cs.AI q-bio.QM stat.ML
Comments: Under review (codes will be released soon)
\\
  Diffusion models excel at modeling complex data distributions, including
those of images, proteins, and small molecules. However, in many cases, our
goal is to model parts of the distribution that maximize certain properties:
for example, we may want to generate images with high aesthetic quality, or
molecules with high bioactivity. It is natural to frame this as a reinforcement
learning (RL) problem, in which the objective is to fine-tune a diffusion model
to maximize a reward function that corresponds to some property. Even with
access to online queries of the ground-truth reward function, efficiently
discovering high-reward samples can be challenging: they might have a low
probability in the initial distribution, and there might be many infeasible
samples that do not even have a well-defined reward (e.g., unnatural images or
physically impossible molecules). In this work, we propose a novel
reinforcement learning procedure that efficiently explores on the manifold of
feasible samples. We present a theoretical analysis providing a regret
guarantee, as well as empirical validation across three domains: images,
biological sequences, and molecules.
\\ ( https://arxiv.org/abs/2402.16359 ,  6127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16374
Date: Mon, 26 Feb 2024 07:52:40 GMT   (12825kb,D)

Title: Graph Learning under Distribution Shifts: A Comprehensive Survey on
  Domain Adaptation, Out-of-distribution, and Continual Learning
Authors: Man Wu, Xin Zheng, Qin Zhang, Xiao Shen, Xiong Luo, Xingquan Zhu,
  Shirui Pan
Categories: cs.LG cs.SI
\\
  Graph learning plays a pivotal role and has gained significant attention in
various application scenarios, from social network analysis to recommendation
systems, for its effectiveness in modeling complex data relations represented
by graph structural data. In reality, the real-world graph data typically show
dynamics over time, with changing node attributes and edge structure, leading
to the severe graph data distribution shift issue. This issue is compounded by
the diverse and complex nature of distribution shifts, which can significantly
impact the performance of graph learning methods in degraded generalization and
adaptation capabilities, posing a substantial challenge to their effectiveness.
In this survey, we provide a comprehensive review and summary of the latest
approaches, strategies, and insights that address distribution shifts within
the context of graph learning. Concretely, according to the observability of
distributions in the inference stage and the availability of sufficient
supervision information in the training stage, we categorize existing graph
learning methods into several essential scenarios, including graph domain
adaptation learning, graph out-of-distribution learning, and graph continual
learning. For each scenario, a detailed taxonomy is proposed, with specific
descriptions and discussions of existing progress made in distribution-shifted
graph learning. Additionally, we discuss the potential applications and future
directions for graph learning under distribution shifts with a systematic
analysis of the current state in this field. The survey is positioned to
provide general guidance for the development of effective graph learning
algorithms in handling graph distribution shifts, and to stimulate future
research and advancements in this area.
\\ ( https://arxiv.org/abs/2402.16374 ,  12825kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16383
Date: Mon, 26 Feb 2024 08:08:30 GMT   (1511kb,D)

Title: Self Supervised Correlation-based Permutations for Multi-View Clustering
Authors: Ran Eisenberg, Jonathan Svirsky, Ofir Lindenbaum
Categories: cs.LG stat.ML
\\
  Fusing information from different modalities can enhance data analysis tasks,
including clustering. However, existing multi-view clustering (MVC) solutions
are limited to specific domains or rely on a suboptimal and computationally
demanding two-stage procedure of representation and clustering. We propose an
end-to-end deep learning-based MVC framework for general data (image, tabular,
etc.). Our approach involves learning meaningful fused data representations
with a novel permutation-based canonical correlation objective. Concurrently,
we learn cluster assignments by identifying consistent pseudo-labels across
multiple views. We demonstrate the effectiveness of our model using ten MVC
benchmark datasets. Theoretically, we show that our model approximates the
supervised linear discrimination analysis (LDA) representation. Additionally,
we provide an error bound induced by false-pseudo label annotations.
\\ ( https://arxiv.org/abs/2402.16383 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16387
Date: Mon, 26 Feb 2024 08:22:22 GMT   (5799kb,D)

Title: On the Generalization Capability of Temporal Graph Learning Algorithms:
  Theoretical Insights and a Simpler Method
Authors: Weilin Cong, Jian Kang, Hanghang Tong, Mehrdad Mahdavi
Categories: cs.LG cs.AI
\\
  Temporal Graph Learning (TGL) has become a prevalent technique across diverse
real-world applications, especially in domains where data can be represented as
a graph and evolves over time. Although TGL has recently seen notable progress
in algorithmic solutions, its theoretical foundations remain largely
unexplored. This paper aims at bridging this gap by investigating the
generalization ability of different TGL algorithms (e.g., GNN-based, RNN-based,
and memory-based methods) under the finite-wide over-parameterized regime. We
establish the connection between the generalization error of TGL algorithms and
"the number of layers/steps" in the GNN-/RNN-based TGL methods and "the
feature-label alignment (FLA) score", where FLA can be used as a proxy for the
expressive power and explains the performance of memory-based methods. Guided
by our theoretical analysis, we propose Simplified-Temporal-Graph-Network,
which enjoys a small generalization error, improved overall performance, and
lower model complexity. Extensive experiments on real-world datasets
demonstrate the effectiveness of our method. Our theoretical findings and
proposed algorithm offer essential insights into TGL from a theoretical
standpoint, laying the groundwork for the designing practical TGL algorithms in
future studies.
\\ ( https://arxiv.org/abs/2402.16387 ,  5799kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16402
Date: Mon, 26 Feb 2024 08:55:10 GMT   (6750kb,D)

Title: Graph Learning with Distributional Edge Layouts
Authors: Xinjian Zhao, Chaolong Ying, Tianshu Yu
Categories: cs.LG cs.AI
Comments: 20 pages, 10 figures
\\
  Graph Neural Networks (GNNs) learn from graph-structured data by passing
local messages between neighboring nodes along edges on certain topological
layouts. Typically, these topological layouts in modern GNNs are
deterministically computed (e.g., attention-based GNNs) or locally sampled
(e.g., GraphSage) under heuristic assumptions. In this paper, we for the first
time pose that these layouts can be globally sampled via Langevin dynamics
following Boltzmann distribution equipped with explicit physical energy,
leading to higher feasibility in the physical world. We argue that such a
collection of sampled/optimized layouts can capture the wide energy
distribution and bring extra expressivity on top of WL-test, therefore easing
downstream tasks. As such, we propose Distributional Edge Layouts (DELs) to
serve as a complement to a variety of GNNs. DEL is a pre-processing strategy
independent of subsequent GNN variants, thus being highly flexible.
Experimental results demonstrate that DELs consistently and substantially
improve a series of GNN baselines, achieving state-of-the-art performance on
multiple datasets.
\\ ( https://arxiv.org/abs/2402.16402 ,  6750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16412
Date: Mon, 26 Feb 2024 09:11:12 GMT   (1038kb,D)

Title: TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis
Authors: Sabera Talukder and Yisong Yue and Georgia Gkioxari
Categories: cs.LG
\\
  The field of general time series analysis has recently begun to explore
unified modeling, where a common architectural backbone can be retrained on a
specific task for a specific dataset. In this work, we approach unification
from a complementary vantage point: unification across tasks and domains. To
this end, we explore the impact of discrete, learnt, time series data
representations that enable generalist, cross-domain training. Our method,
TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer
architecture that embeds time series data from varying domains using a discrete
vectorized representation learned in a self-supervised manner. TOTEM works
across multiple tasks and domains with minimal to no tuning. We study the
efficacy of TOTEM with an extensive evaluation on 17 real world time series
datasets across 3 tasks. We evaluate both the specialist (i.e., training a
model on each domain) and generalist (i.e., training a single model on many
domains) settings, and show that TOTEM matches or outperforms previous best
methods on several popular benchmarks. The code can be found at:
https://github.com/SaberaTalukder/TOTEM.
\\ ( https://arxiv.org/abs/2402.16412 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16435
Date: Mon, 26 Feb 2024 09:32:28 GMT   (4592kb,D)

Title: Training Implicit Generative Models via an Invariant Statistical Loss
Authors: Jos\'e Manuel de Frutos and Pablo M. Olmos and Manuel A. V\'azquez and
  Joaqu\'in M\'iguez
Categories: cs.LG cs.AI math.ST stat.ML stat.TH
Comments: Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\
  Implicit generative models have the capability to learn arbitrary complex
data distributions. On the downside, training requires telling apart real data
from artificially-generated ones using adversarial discriminators, leading to
unstable training and mode-dropping issues. As reported by Zahee et al. (2017),
even in the one-dimensional (1D) case, training a generative adversarial
network (GAN) is challenging and often suboptimal. In this work, we develop a
discriminator-free method for training one-dimensional (1D) generative implicit
models and subsequently expand this method to accommodate multivariate cases.
Our loss function is a discrepancy measure between a suitably chosen
transformation of the model samples and a uniform distribution; hence, it is
invariant with respect to the true distribution of the data. We first formulate
our method for 1D random variables, providing an effective solution for
approximate reparameterization of arbitrary complex distributions. Then, we
consider the temporal setting (both univariate and multivariate), in which we
model the conditional distribution of each sample given the history of the
process. We demonstrate through numerical simulations that this new method
yields promising results, successfully learning true distributions in a variety
of scenarios and mitigating some of the well-known problems that
state-of-the-art implicit methods present.
\\ ( https://arxiv.org/abs/2402.16435 ,  4592kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16442
Date: Mon, 26 Feb 2024 09:38:39 GMT   (13543kb,D)

Title: On Distributed Larger-Than-Memory Subset Selection With Pairwise
  Submodular Functions
Authors: Maximilian B\"other, Abraham Sebastian, Pranjal Awasthi, Ana Klimovic,
  Srikumar Ramalingam
Categories: cs.LG cs.AI cs.CV cs.DC math.OC
\\
  Many learning problems hinge on the fundamental problem of subset selection,
i.e., identifying a subset of important and representative points. For example,
selecting the most significant samples in ML training cannot only reduce
training costs but also enhance model quality. Submodularity, a discrete
analogue of convexity, is commonly used for solving subset selection problems.
However, existing algorithms for optimizing submodular functions are
sequential, and the prior distributed methods require at least one central
machine to fit the target subset. In this paper, we relax the requirement of
having a central machine for the target subset by proposing a novel distributed
bounding algorithm with provable approximation guarantees. The algorithm
iteratively bounds the minimum and maximum utility values to select high
quality points and discard the unimportant ones. When bounding does not find
the complete subset, we use a multi-round, partition-based distributed greedy
algorithm to identify the remaining subset. We show that these algorithms find
high quality subsets on CIFAR-100 and ImageNet with marginal or no loss in
quality compared to centralized methods, and scale to a dataset with 13 billion
points.
\\ ( https://arxiv.org/abs/2402.16442 ,  13543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16463
Date: Mon, 26 Feb 2024 10:11:28 GMT   (3730kb,D)

Title: Learning to Schedule Online Tasks with Bandit Feedback
Authors: Yongxin Xu, Shangshang Wang, Hengquan Guo, Xin Liu, Ziyu Shao
Categories: cs.LG cs.DC
Comments: 16 pages
\\
  Online task scheduling serves an integral role for task-intensive
applications in cloud computing and crowdsourcing. Optimal scheduling can
enhance system performance, typically measured by the reward-to-cost ratio,
under some task arrival distribution. On one hand, both reward and cost are
dependent on task context (e.g., evaluation metric) and remain black-box in
practice. These render reward and cost hard to model thus unknown before
decision making. On the other hand, task arrival behaviors remain sensitive to
factors like unpredictable system fluctuation whereby a prior estimation or the
conventional assumption of arrival distribution (e.g., Poisson) may fail. This
implies another practical yet often neglected challenge, i.e., uncertain task
arrival distribution. Towards effective scheduling under a stationary
environment with various uncertainties, we propose a double-optimistic learning
based Robbins-Monro (DOL-RM) algorithm. Specifically, DOL-RM integrates a
learning module that incorporates optimistic estimation for reward-to-cost
ratio and a decision module that utilizes the Robbins-Monro method to
implicitly learn task arrival distribution while making scheduling decisions.
Theoretically, DOL-RM achieves convergence gap and no regret learning with a
sub-linear regret of $O(T^{3/4})$, which is the first result for online task
scheduling under uncertain task arrival distribution and unknown reward and
cost. Our numerical results in a synthetic experiment and a real-world
application demonstrate the effectiveness of DOL-RM in achieving the best
cumulative reward-to-cost ratio compared with other state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.16463 ,  3730kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16516
Date: Mon, 26 Feb 2024 11:54:54 GMT   (436kb,D)

Title: Generative Pretrained Hierarchical Transformer for Time Series
  Forecasting
Authors: Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, Zhi Li
Categories: cs.LG
\\
  Recent efforts have been dedicated to enhancing time series forecasting
accuracy by introducing advanced network architectures and self-supervised
pretraining strategies. Nevertheless, existing approaches still exhibit two
critical drawbacks. Firstly, these methods often rely on a single dataset for
training, limiting the model's generalizability due to the restricted scale of
the training data. Secondly, the one-step generation schema is widely followed,
which necessitates a customized forecasting head and overlooks the temporal
dependencies in the output series, and also leads to increased training costs
under different horizon length settings.
  To address these issues, we propose a novel generative pretrained
hierarchical transformer architecture for forecasting, named GPHT. There are
two aspects of key designs in GPHT. On the one hand, we advocate for
constructing a mixed dataset for pretraining our model, comprising various
datasets from diverse data scenarios. This approach significantly expands the
scale of training data, allowing our model to uncover commonalities in time
series data and facilitating improved transfer to specific datasets. On the
other hand, GPHT employs an auto-regressive forecasting approach under the
channel-independent assumption, effectively modeling temporal dependencies in
the output series. Importantly, no customized forecasting head is required,
enabling a single model to forecast at arbitrary horizon settings. We conduct
sufficient experiments on eight datasets with mainstream self-supervised
pretraining models and supervised models. The results demonstrated that GPHT
surpasses the baseline models across various fine-tuning and zero/few-shot
learning settings in the traditional long-term forecasting task, providing
support for verifying the feasibility of pretrained time series large models.
\\ ( https://arxiv.org/abs/2402.16516 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16544
Date: Mon, 26 Feb 2024 13:03:26 GMT   (2114kb,D)

Title: Label Learning Method Based on Tensor Projection
Authors: Jing Li and Quanxue Gao and Qianqian Wang and Cheng Deng and Deyan Xie
Categories: cs.LG
\\
  Multi-view clustering method based on anchor graph has been widely concerned
due to its high efficiency and effectiveness. In order to avoid
post-processing, most of the existing anchor graph-based methods learn
bipartite graphs with connected components. However, such methods have high
requirements on parameters, and in some cases it may not be possible to obtain
bipartite graphs with clear connected components. To end this, we propose a
label learning method based on tensor projection (LLMTP). Specifically, we
project anchor graph into the label space through an orthogonal projection
matrix to obtain cluster labels directly. Considering that the spatial
structure information of multi-view data may be ignored to a certain extent
when projected in different views separately, we extend the matrix projection
transformation to tensor projection, so that the spatial structure information
between views can be fully utilized. In addition, we introduce the tensor
Schatten $p$-norm regularization to make the clustering label matrices of
different views as consistent as possible. Extensive experiments have proved
the effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2402.16544 ,  2114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16562
Date: Mon, 26 Feb 2024 13:39:04 GMT   (381kb)

Title: Q-FOX Learning: Breaking Tradition in Reinforcement Learning
Authors: Mahmood Alqaseer, Yossra H. Ali and Tarik A. Rashid
Categories: cs.LG cs.AI cs.NE
Comments: 13 pages, 5 figures
\\
  Reinforcement learning (RL) is a subset of artificial intelligence (AI) where
agents learn the best action by interacting with the environment, making it
suitable for tasks that do not require labeled data or direct supervision.
Hyperparameters (HP) tuning refers to choosing the best parameter that leads to
optimal solutions in RL algorithms. Manual or random tuning of the HP may be a
crucial process because variations in this parameter lead to changes in the
overall learning aspects and different rewards. In this paper, a novel and
automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX
optimizer, a new optimization method inspired by nature that mimics red foxes'
hunting behavior, and the commonly used, easy-to-implement RL Q-learning
algorithm to solve the problem of HP tuning. Moreover, a new objective function
is proposed which prioritizes the reward over the mean squared error (MSE) and
learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment
control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards
than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly
selected HP. The cumulative reward for the Cart Pole task was 32.08, and for
the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has
limitations. It cannot be used directly in real-word problems before choosing
the HP in a simulation environment because its processes work iteratively,
making it time-consuming. The results indicate that Q-FOX has played an
essential role in HP tuning for RL algorithms to effectively solve different
control tasks.
\\ ( https://arxiv.org/abs/2402.16562 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16565
Date: Mon, 26 Feb 2024 13:43:25 GMT   (324kb,D)

Title: Partial Rankings of Optimizers
Authors: Julian Rodemann and Hannah Blocher
Categories: cs.LG stat.ML
\\
  We introduce a framework for benchmarking optimizers according to multiple
criteria over various test functions. Based on a recently introduced union-free
generic depth function for partial orders/rankings, it fully exploits the
ordinal information and allows for incomparability. Our method describes the
distribution of all partial orders/rankings, avoiding the notorious
shortcomings of aggregation. This permits to identify test functions that
produce central or outlying rankings of optimizers and to assess the quality of
benchmarking suites.
\\ ( https://arxiv.org/abs/2402.16565 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16639
Date: Mon, 26 Feb 2024 15:09:56 GMT   (720kb,D)

Title: Differentiable Particle Filtering using Optimal Placement Resampling
Authors: Domonkos Csuzdi, Oliv\'er T\"or\H{o}, Tam\'as B\'ecsi
Categories: cs.LG stat.CO
\\
  Particle filters are a frequent choice for inference tasks in nonlinear and
non-Gaussian state-space models. They can either be used for state inference by
approximating the filtering distribution or for parameter inference by
approximating the marginal data (observation) likelihood. A good proposal
distribution and a good resampling scheme are crucial to obtain low variance
estimates. However, traditional methods like multinomial resampling introduce
nondifferentiability in PF-based loss functions for parameter estimation,
prohibiting gradient-based learning tasks. This work proposes a differentiable
resampling scheme by deterministic sampling from an empirical cumulative
distribution function. We evaluate our method on parameter inference tasks and
proposal learning.
\\ ( https://arxiv.org/abs/2402.16639 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16668
Date: Mon, 26 Feb 2024 15:40:46 GMT   (141kb,D)

Title: Program-Based Strategy Induction for Reinforcement Learning
Authors: Carlos G. Correa and Thomas L. Griffiths and Nathaniel D. Daw
Categories: cs.LG cs.AI
\\
  Typical models of learning assume incremental estimation of
continuously-varying decision variables like expected rewards. However, this
class of models fails to capture more idiosyncratic, discrete heuristics and
strategies that people and animals appear to exhibit. Despite recent advances
in strategy discovery using tools like recurrent networks that generalize the
classic models, the resulting strategies are often onerous to interpret, making
connections to cognition difficult to establish. We use Bayesian program
induction to discover strategies implemented by programs, letting the
simplicity of strategies trade off against their effectiveness. Focusing on
bandit tasks, we find strategies that are difficult or unexpected with
classical incremental learning, like asymmetric learning from rewarded and
unrewarded trials, adaptive horizon-dependent random exploration, and discrete
state switching.
\\ ( https://arxiv.org/abs/2402.16668 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16681
Date: Mon, 26 Feb 2024 15:59:38 GMT   (2724kb,D)

Title: Enhancing Continuous Domain Adaptation with Multi-Path Transfer
  Curriculum
Authors: Hanbing Liu, Jingge Wang, Xuan Zhang, Ye Guo, Yang Li
Categories: cs.LG
\\
  Addressing the large distribution gap between training and testing data has
long been a challenge in machine learning, giving rise to fields such as
transfer learning and domain adaptation. Recently, Continuous Domain Adaptation
(CDA) has emerged as an effective technique, closing this gap by utilizing a
series of intermediate domains. This paper contributes a novel CDA method,
W-MPOT, which rigorously addresses the domain ordering and error accumulation
problems overlooked by previous studies. Specifically, we construct a transfer
curriculum over the source and intermediate domains based on Wasserstein
distance, motivated by theoretical analysis of CDA. Then we transfer the source
model to the target domain through multiple valid paths in the curriculum using
a modified version of continuous optimal transport. A bidirectional path
consistency constraint is introduced to mitigate the impact of accumulated
mapping errors during continuous transfer. We extensively evaluate W-MPOT on
multiple datasets, achieving up to 54.1\% accuracy improvement on multi-session
Alzheimer MR image classification and 94.7\% MSE reduction on battery capacity
estimation.
\\ ( https://arxiv.org/abs/2402.16681 ,  2724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16710
Date: Mon, 26 Feb 2024 16:27:08 GMT   (767kb,D)

Title: Cost Aware Best Arm Identification
Authors: Kellen Kanarios, Qining Zhang, Lei Ying
Categories: cs.LG stat.ML
\\
  In this paper, we study a best arm identification problem with dual objects.
In addition to the classic reward, each arm is associated with a cost
distribution and the goal is to identify the largest reward arm using the
minimum expected cost. We call it \emph{Cost Aware Best Arm Identification}
(CABAI), which captures the separation of testing and implementation phases in
product development pipelines and models the objective shift between phases,
i.e., cost for testing and reward for implementation. We first derive an
theoretic lower bound for CABAI and propose an algorithm called $\mathsf{CTAS}$
to match it asymptotically. To reduce the computation of $\mathsf{CTAS}$, we
further propose a low-complexity algorithm called CO, based on a square-root
rule, which proves optimal in simplified two-armed models and generalizes
surprisingly well in numerical experiments. Our results show (i) ignoring the
heterogeneous action cost results in sub-optimality in practice, and (ii)
low-complexity algorithms deliver near-optimal performance over a wide range of
problems.
\\ ( https://arxiv.org/abs/2402.16710 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16726
Date: Mon, 26 Feb 2024 16:48:12 GMT   (19645kb,D)

Title: Interpreting Grokked Transformers in Complex Modular Arithmetic
Authors: Hiroki Furuta, Minegishi Gouki, Yusuke Iwasawa, Yutaka Matsuo
Categories: cs.LG cs.AI
Comments: Code: https://github.com/frt03/grok_mod_poly
\\
  Grokking has been actively explored to reveal the mystery of delayed
generalization. Identifying interpretable algorithms inside the grokked models
is a suggestive hint to understanding its mechanism. In this work, beyond the
simplest and well-studied modular addition, we observe the internal circuits
learned through grokking in complex modular arithmetic via interpretable
reverse engineering, which highlights the significant difference in their
dynamics: subtraction poses a strong asymmetry on Transformer; multiplication
requires cosine-biased components at all the frequencies in a Fourier domain;
polynomials often result in the superposition of the patterns from elementary
arithmetic, but clear patterns do not emerge in challenging cases; grokking can
easily occur even in higher-degree formulas with basic symmetric and
alternating expressions. We also introduce the novel progress measure for
modular arithmetic; Fourier Frequency Sparsity and Fourier Coefficient Ratio,
which not only indicate the late generalization but also characterize
distinctive internal representations of grokked models per modular operation.
Our empirical analysis emphasizes the importance of holistic evaluation among
various combinations.
\\ ( https://arxiv.org/abs/2402.16726 ,  19645kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16748
Date: Mon, 26 Feb 2024 17:09:18 GMT   (1123kb,D)

Title: Enhancing Hypergradients Estimation: A Study of Preconditioning and
  Reparameterization
Authors: Zhenzhang Ye, Gabriel Peyr\'e, Daniel Cremers, Pierre Ablin
Categories: cs.LG
Comments: Accepted in AISTATS 2024
\\
  Bilevel optimization aims to optimize an outer objective function that
depends on the solution to an inner optimization problem. It is routinely used
in Machine Learning, notably for hyperparameter tuning. The conventional method
to compute the so-called hypergradient of the outer problem is to use the
Implicit Function Theorem (IFT). As a function of the error of the inner
problem resolution, we study the error of the IFT method. We analyze two
strategies to reduce this error: preconditioning the IFT formula and
reparameterizing the inner problem. We give a detailed account of the impact of
these two modifications on the error, highlighting the role played by
higher-order derivatives of the functionals at stake. Our theoretical findings
explain when super efficiency, namely reaching an error on the hypergradient
that depends quadratically on the error on the inner problem, is achievable and
compare the two approaches when this is impossible. Numerical evaluations on
hyperparameter tuning for regression problems substantiate our theoretical
findings.
\\ ( https://arxiv.org/abs/2402.16748 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16778
Date: Mon, 26 Feb 2024 17:49:37 GMT   (286kb,D)

Title: On the Growth of Mistakes in Differentially Private Online Learning: A
  Lower Bound Perspective
Authors: Daniil Dmitriev, Krist\'of Szab\'o, Amartya Sanyal
Categories: cs.LG cs.CR
\\
  In this paper, we provide lower bounds for Differentially Private (DP) Online
Learning algorithms. Our result shows that, for a broad class of
$(\varepsilon,\delta)$-DP online algorithms, for $T$ such that $\log T\leq O(1
/ \delta)$, the expected number of mistakes incurred by the algorithm grows as
$\Omega(\log \frac{T}{\delta})$. This matches the upper bound obtained by
Golowich and Livni (2021) and is in contrast to non-private online learning
where the number of mistakes is independent of $T$. To the best of our
knowledge, our work is the first result towards settling lower bounds for
DP-Online learning and partially addresses the open question in Sanyal and
Ramponi (2022).
\\ ( https://arxiv.org/abs/2402.16778 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16785
Date: Mon, 26 Feb 2024 18:00:29 GMT   (549kb,D)

Title: CARTE: pretraining and transfer for tabular learning
Authors: Myung Jun Kim, L\'eo Grinsztajn, and Ga\"el Varoquaux
Categories: cs.LG
\\
  Pretrained deep-learning models are the go-to solution for images or text.
However, for tabular data the standard is still to train tree-based models.
Pre-training or transfer is a huge challenge as in general tables have columns
about different quantities and naming conventions that vary vastly across
sources. Data integration tackles correspondences across multiple sources:
schema matching for columns, and entity matching for entries. We propose a
neural architecture that does not need such matches. As a result, we can
pretrain it on background data that has not been matched. The architecture -
CARTE for Context Aware Representation of Table Entries - uses a graph
representation of tabular (or relational) data to process tables with different
columns, string embeddings of entries and columns names to model an open
vocabulary, and a graph-attentional network to contextualize entries with
column names and neighboring entries. An extensive benchmark shows that CARTE
facilitates learning, outperforming a solid set of baselines including the best
tree-based models. CARTE also enables joint learning across tables with
unmatched columns, enhancing a small table with bigger ones. CARTE opens the
door to large pretrained models embarking information for tabular data.
\\ ( https://arxiv.org/abs/2402.16785 ,  549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16788
Date: Mon, 26 Feb 2024 18:01:41 GMT   (1771kb,D)

Title: Why Transformers Need Adam: A Hessian Perspective
Authors: Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, Zhi-Quan
  Luo
Categories: cs.LG cs.AI
\\
  SGD performs worse than Adam by a significant margin on Transformers, but the
reason remains unclear. In this work, we provide an explanation of SGD's
failure on Transformers through the lens of Hessian: (i) Transformers are
``heterogeneous'': the Hessian spectrum across parameter blocks vary
dramatically, a phenomenon we call ``block heterogeneity"; (ii) Heterogeneity
hampers SGD: SGD performs badly on problems with block heterogeneity. To
validate that heterogeneity hampers SGD, we check various Transformers, CNNs,
MLPs, and quadratic problems, and find that SGD works well on problems without
block heterogeneity but performs badly when the heterogeneity exists. Our
initial theoretical analysis indicates that SGD fails because it applies one
single learning rate for all blocks, which cannot handle the heterogeneity
among blocks. The failure could be rescued if we could assign different
learning rates across blocks, as designed in Adam.
\\ ( https://arxiv.org/abs/2402.16788 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16801
Date: Mon, 26 Feb 2024 18:19:07 GMT   (6242kb,D)

Title: Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement
  Learning
Authors: Michael Matthews and Michael Beukman and Benjamin Ellis and Mikayel
  Samvelyan and Matthew Jackson and Samuel Coward and Jakob Foerster
Categories: cs.LG
\\
  Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms. We identify that existing benchmarks
used for research into open-ended learning fall into one of two categories.
Either they are too slow for meaningful research to be performed without
enormous computational resources, like Crafter, NetHack and Minecraft, or they
are not complex enough to pose a significant challenge, like Minigrid and
Procgen. To remedy this, we first present Craftax-Classic: a ground-up rewrite
of Crafter in JAX that runs up to 250x faster than the Python-native original.
A run of PPO using 1 billion environment interactions finishes in under an hour
using only a single GPU and averages 90% of the optimal reward. To provide a
more compelling challenge we present the main Craftax benchmark, a significant
extension of the Crafter mechanics with elements inspired from NetHack. Solving
Craftax requires deep exploration, long term planning and memory, as well as
continual adaptation to novel situations as more of the world is discovered. We
show that existing methods including global and episodic exploration, as well
as unsupervised environment design fail to make material progress on the
benchmark. We believe that Craftax can for the first time allow researchers to
experiment in a complex, open-ended environment with limited computational
resources.
\\ ( https://arxiv.org/abs/2402.16801 ,  6242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16828
Date: Mon, 26 Feb 2024 18:55:13 GMT   (1244kb,D)

Title: Training Neural Networks from Scratch with Parallel Low-Rank Adapters
Authors: Minyoung Huh, Brian Cheung, Jeremy Bernstein, Phillip Isola, Pulkit
  Agrawal
Categories: cs.LG cs.AI cs.CV
\\
  The scalability of deep learning models is fundamentally limited by computing
resources, memory, and communication. Although methods like low-rank adaptation
(LoRA) have reduced the cost of model finetuning, its application in model
pre-training remains largely unexplored. This paper explores extending LoRA to
model pre-training, identifying the inherent constraints and limitations of
standard LoRA in this context. We introduce LoRA-the-Explorer (LTE), a novel
bi-level optimization algorithm designed to enable parallel training of
multiple low-rank heads across computing nodes, thereby reducing the need for
frequent synchronization. Our approach includes extensive experimentation on
vision transformers using various vision datasets, demonstrating that LTE is
competitive with standard pre-training.
\\ ( https://arxiv.org/abs/2402.16828 ,  1244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16829
Date: Mon, 26 Feb 2024 18:55:15 GMT   (1518kb,D)

Title: GISTEmbed: Guided In-sample Selection of Training Negatives for Text
  Embedding Fine-tuning
Authors: Aivin V. Solatorio
Categories: cs.LG cs.CL
Comments: GISTEmbed GitHub repository at
  https://github.com/avsolatorio/GISTEmbed
\\
  Embedding models are integral to AI applications like semantic search,
personalized recommendations, and retrieval augmented generation for LLMs,
necessitating high-quality training data. However, the limited scalability of
manual data curation prompts the need for automated methods to ensure data
integrity. Traditional unsupervised triplet mining automates training data
generation, crucial for embedding model training, yet inadvertently injects
biases and noise, thereby degrading model performance. Addressing this, we
introduce GISTEmbed, a novel strategy that enhances in-batch negative selection
during contrastive training through a guide model. This approach departs from
reliance on random sampling and equal utility assumption of batch negatives,
significantly reducing noise from data quality issues and improving model
fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),
GISTEmbed showcases consistent performance improvements across various model
sizes and achieves state-of-the-art results in select categories. This
framework enables significant enhancements for smaller models by leveraging the
capabilities of powerful yet resource-intensive large models. GISTEmbed can
potentially revolutionize the creation of highly efficient, smaller models,
democratizing access to advanced AI technologies. Making these technologies
more accessible and cost-effective, especially for applications constrained by
resources, significantly expands the impact and accessibility of
state-of-the-art AI solutions across diverse sectors.
\\ ( https://arxiv.org/abs/2402.16829 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16842
Date: Mon, 26 Feb 2024 18:59:12 GMT   (892kb,D)

Title: Asymmetry in Low-Rank Adapters of Foundation Models
Authors: Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz S\'aez de
  Oc\'ariz Borde, Rickard Br\"uel Gabrielsson, Leshem Choshen, Marzyeh
  Ghassemi, Mikhail Yurochkin, Justin Solomon
Categories: cs.LG
Comments: 17 pages, 2 figures, 9 tables
\\
  Parameter-efficient fine-tuning optimizes large, pre-trained foundation
models by updating a subset of parameters; in this class, Low-Rank Adaptation
(LoRA) is particularly effective. Inspired by an effort to investigate the
different roles of LoRA matrices during fine-tuning, this paper characterizes
and leverages unexpected asymmetry in the importance of low-rank adapter
matrices. Specifically, when updating the parameter matrices of a neural
network by adding a product $BA$, we observe that the $B$ and $A$ matrices have
distinct functions: $A$ extracts features from the input, while $B$ uses these
features to create the desired output. Based on this observation, we
demonstrate that fine-tuning $B$ is inherently more effective than fine-tuning
$A$, and that a random untrained $A$ should perform nearly as well as a
fine-tuned one. Using an information-theoretic lens, we also bound the
generalization of low-rank adapters, showing that the parameter savings of
exclusively training $B$ improves the bound. We support our conclusions with
experiments on RoBERTa, BART-Large, LLaMA-2, and ViTs.
\\ ( https://arxiv.org/abs/2402.16842 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16844
Date: Mon, 26 Feb 2024 18:59:28 GMT   (240kb,D)

Title: Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding
Authors: Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort,
  Yuki Asano, Babak Ehteshami Bejnordi
Categories: cs.LG cs.AI cs.CL
\\
  Large language models (LLMs) have become ubiquitous in practice and are
widely used for generation tasks such as translation, summarization and
instruction following. However, their enormous size and reliance on
autoregressive decoding increase deployment costs and complicate their use in
latency-critical applications. In this work, we propose a hybrid approach that
combines language models of different sizes to increase the efficiency of
autoregressive decoding while maintaining high performance. Our method utilizes
a pretrained frozen LLM that encodes all prompt tokens once in parallel, and
uses the resulting representations to condition and guide a small language
model (SLM), which then generates the response more efficiently. We investigate
the combination of encoder-decoder LLMs with both encoder-decoder and
decoder-only SLMs from different model families and only require fine-tuning of
the SLM. Experiments with various benchmarks show substantial speedups of up to
$4\times$, with minor performance penalties of $1-2\%$ for translation and
summarization tasks compared to the LLM.
\\ ( https://arxiv.org/abs/2402.16844 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16845
Date: Mon, 26 Feb 2024 18:59:31 GMT   (2717kb,D)

Title: Neural Operators with Localized Integral and Differential Kernels
Authors: Miguel Liu-Schiaffini, Julius Berner, Boris Bonev, Thorsten Kurth,
  Kamyar Azizzadenesheli, Anima Anandkumar
Categories: cs.LG cs.AI cs.NA math.NA
\\
  Neural operators learn mappings between function spaces, which is practical
for learning solution operators of PDEs and other scientific modeling
applications. Among them, the Fourier neural operator (FNO) is a popular
architecture that performs global convolutions in the Fourier space. However,
such global operations are often prone to over-smoothing and may fail to
capture local details. In contrast, convolutional neural networks (CNN) can
capture local features but are limited to training and inference at a single
resolution. In this work, we present a principled approach to operator learning
that can capture local features under two frameworks by learning differential
operators and integral operators with locally supported kernels. Specifically,
inspired by stencil methods, we prove that we obtain differential operators
under an appropriate scaling of the kernel values of CNNs. To obtain local
integral operators, we utilize suitable basis representations for the kernels
based on discrete-continuous convolutions. Both these approaches preserve the
properties of operator learning and, hence, the ability to predict at any
resolution. Adding our layers to FNOs significantly improves their performance,
reducing the relative L2-error by 34-72% in our experiments on turbulent 2D
Navier-Stokes fluid flow and the spherical shallow water equations.
\\ ( https://arxiv.org/abs/2402.16845 ,  2717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16848
Date: Mon, 26 Feb 2024 18:59:52 GMT   (1731kb,D)

Title: InterroGate: Learning to Share, Specialize, and Prune Representations
  for Multi-task Learning
Authors: Babak Ehteshami Bejnordi, Gaurav Kumar, Amelie Royer, Christos
  Louizos, Tijmen Blankevoort, Mohsen Ghafoorian
Categories: cs.LG
Comments: Under review
\\
  Jointly learning multiple tasks with a unified model can improve accuracy and
data efficiency, but it faces the challenge of task interference, where
optimizing one task objective may inadvertently compromise the performance of
another. A solution to mitigate this issue is to allocate task-specific
parameters, free from interference, on top of shared features. However,
manually designing such architectures is cumbersome, as practitioners need to
balance between the overall performance across all tasks and the higher
computational cost induced by the newly added parameters. In this work, we
propose \textit{InterroGate}, a novel multi-task learning (MTL) architecture
designed to mitigate task interference while optimizing inference computational
efficiency. We employ a learnable gating mechanism to automatically balance the
shared and task-specific representations while preserving the performance of
all tasks. Crucially, the patterns of parameter sharing and specialization
dynamically learned during training, become fixed at inference, resulting in a
static, optimized MTL architecture. Through extensive empirical evaluations, we
demonstrate SoTA results on three MTL benchmarks using convolutional as well as
transformer-based backbones on CelebA, NYUD-v2, and PASCAL-Context.
\\ ( https://arxiv.org/abs/2402.16848 ,  1731kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.15538 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:25:20 GMT   (1109kb,D)

Title: AgentLite: A Lightweight Library for Building and Advancing
  Task-Oriented LLM Agent System
Authors: Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu,
  Juntao Tan, Prafulla K. Choubey, Tian Lan, Jason Wu, Huan Wang, Shelby
  Heinecke, Caiming Xiong, Silvio Savarese
Categories: cs.MA cs.AI
Comments: preprint. Library is available at
  https://github.com/SalesforceAIResearch/AgentLite
\\
  The booming success of LLMs initiates rapid development in LLM agents. Though
the foundation of an LLM agent is the generative model, it is critical to
devise the optimal reasoning strategies and agent architectures. Accordingly,
LLM agent research advances from the simple chain-of-thought prompting to more
complex ReAct and Reflection reasoning strategy; agent architecture also
evolves from single agent generation to multi-agent conversation, as well as
multi-LLM multi-agent group chat. However, with the existing intricate
frameworks and libraries, creating and evaluating new reasoning strategies and
agent architectures has become a complex challenge, which hinders research
investigation into LLM agents. Thus, we open-source a new AI agent library,
AgentLite, which simplifies this process by offering a lightweight,
user-friendly platform for innovating LLM agent reasoning, architectures, and
applications with ease. AgentLite is a task-oriented framework designed to
enhance the ability of agents to break down tasks and facilitate the
development of multi-agent systems. Furthermore, we introduce multiple
practical applications developed with AgentLite to demonstrate its convenience
and flexibility. Get started now at:
\url{https://github.com/SalesforceAIResearch/AgentLite}.
\\ ( https://arxiv.org/abs/2402.15538 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15546 (*cross-listing*)
Date: Fri, 23 Feb 2024 13:01:13 GMT   (201kb,D)

Title: HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent
  Pathfinding
Authors: Huijie Tang, Federico Berto, Zihan Ma, Chuanbo Hua, Kyuree Ahn,
  Jinkyoo Park
Categories: cs.MA cs.AI cs.LG cs.RO
Comments: Accepted as Extended Abstract in Proc. of the 23rd International
  Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024)
\\
  Large-scale multi-agent pathfinding (MAPF) presents significant challenges in
several areas. As systems grow in complexity with a multitude of autonomous
agents operating simultaneously, efficient and collision-free coordination
becomes paramount. Traditional algorithms often fall short in scalability,
especially in intricate scenarios. Reinforcement Learning (RL) has shown
potential to address the intricacies of MAPF; however, it has also been shown
to struggle with scalability, demanding intricate implementation, lengthy
training, and often exhibiting unstable convergence, limiting its practical
application. In this paper, we introduce Heuristics-Informed Multi-Agent
Pathfinding (HiMAP), a novel scalable approach that employs imitation learning
with heuristic guidance in a decentralized manner. We train on small-scale
instances using a heuristic policy as a teacher that maps each single agent
observation information to an action probability distribution. During
pathfinding, we adopt several inference techniques to improve performance. With
a simple training scheme and implementation, HiMAP demonstrates competitive
results in terms of success rate and scalability in the field of
imitation-learning-only MAPF, showing the potential of imitation-learning-only
MAPF equipped with inference techniques.
\\ ( https://arxiv.org/abs/2402.15546 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15552 (*cross-listing*)
Date: Fri, 23 Feb 2024 17:21:21 GMT   (33883kb,D)

Title: Morphological Symmetries in Robotics
Authors: Daniel Ordo\~nez-Apraez, Giulio Turrisi, Vladimir Kostic, Mario
  Martin, Antonio Agudo, Francesc Moreno-Noguer, Massimiliano Pontil, Claudio
  Semini, and Carlos Mastalli
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: 18 pages, 11 figures
\\
  We present a comprehensive framework for studying and leveraging
morphological symmetries in robotic systems. These are intrinsic properties of
the robot's morphology, frequently observed in animal biology and robotics,
which stem from the replication of kinematic structures and the symmetrical
distribution of mass. We illustrate how these symmetries extend to the robot's
state space and both proprioceptive and exteroceptive sensor measurements,
resulting in the equivariance of the robot's equations of motion and optimal
control policies. Thus, we recognize morphological symmetries as a relevant and
previously unexplored physics-informed geometric prior, with significant
implications for both data-driven and analytical methods used in modeling,
control, estimation and design in robotics. For data-driven methods, we
demonstrate that morphological symmetries can enhance the sample efficiency and
generalization of machine learning models through data augmentation, or by
applying equivariant/invariant constraints on the model's architecture. In the
context of analytical methods, we employ abstract harmonic analysis to
decompose the robot's dynamics into a superposition of lower-dimensional,
independent dynamics. We substantiate our claims with both synthetic and
real-world experiments conducted on bipedal and quadrupedal robots. Lastly, we
introduce the repository MorphoSymm to facilitate the practical use of the
theory and applications outlined in this work.
\\ ( https://arxiv.org/abs/2402.15552 ,  33883kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15570 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:12:53 GMT   (3874kb,D)

Title: Fast Adversarial Attacks on Language Models In One GPU Minute
Authors: Vinu Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham
  Kattakinda, Atoosa Chegini, Soheil Feizi
Categories: cs.CR cs.AI cs.CL
\\
  In this paper, we introduce a novel class of fast, beam search-based
adversarial attack (BEAST) for Language Models (LMs). BEAST employs
interpretable parameters, enabling attackers to balance between attack speed,
success rate, and the readability of adversarial prompts. The computational
efficiency of BEAST facilitates us to investigate its applications on LMs for
jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free
targeted attack can jailbreak aligned LMs with high attack success rates within
one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute
with a success rate of 89% when compared to a gradient-based baseline that
takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000
48GB GPU. Additionally, we discover a unique outcome wherein our untargeted
attack induces hallucinations in LM chatbots. Through human evaluations, we
find that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% more
incorrect outputs when compared to LM outputs in the absence of our attack. We
also learn that 22% of the time, BEAST causes Vicuna to generate outputs that
are not relevant to the original prompt. Further, we use BEAST to generate
adversarial prompts in a few seconds that can boost the performance of existing
membership inference attacks for LMs. We believe that our fast attack, BEAST,
has the potential to accelerate research in LM security and privacy. Our
codebase is publicly available at https://github.com/vinusankars/BEAST.
\\ ( https://arxiv.org/abs/2402.15570 ,  3874kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15591 (*cross-listing*)
Date: Fri, 23 Feb 2024 20:16:13 GMT   (1906kb,D)

Title: RecWizard: A Toolkit for Conversational Recommendation with Modular,
  Portable Models and Interactive User Interface
Authors: Zeyuan Zhang, Tanmay Laud, Zihang He, Xiaojie Chen, Xinshuang Liu,
  Zhouhang Xie, Julian McAuley, Zhankui He
Categories: cs.IR cs.AI
Comments: AAAI'24 Demo Track
\\
  We present a new Python toolkit called RecWizard for Conversational
Recommender Systems (CRS). RecWizard offers support for development of models
and interactive user interface, drawing from the best practices of the
Huggingface ecosystems. CRS with RecWizard are modular, portable, interactive
and Large Language Models (LLMs)-friendly, to streamline the learning process
and reduce the additional effort for CRS research. For more comprehensive
information about RecWizard, please check our GitHub
https://github.com/McAuley-Lab/RecWizard.
\\ ( https://arxiv.org/abs/2402.15591 ,  1906kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15625 (*cross-listing*)
Date: Fri, 23 Feb 2024 22:03:12 GMT   (67kb,D)

Title: Learning Cyclic Causal Models from Incomplete Data
Authors: Muralikrishnna G. Sethuraman, Faramarz Fekri
Categories: stat.ML cs.AI cs.LG
\\
  Causal learning is a fundamental problem in statistics and science, offering
insights into predicting the effects of unseen treatments on a system. Despite
recent advances in this topic, most existing causal discovery algorithms
operate under two key assumptions: (i) the underlying graph is acyclic, and
(ii) the available data is complete. These assumptions can be problematic as
many real-world systems contain feedback loops (e.g., biological systems), and
practical scenarios frequently involve missing data. In this work, we propose a
novel framework, named MissNODAGS, for learning cyclic causal graphs from
partially missing data. Under the additive noise model, MissNODAGS learns the
causal graph by alternating between imputing the missing data and maximizing
the expected log-likelihood of the visible part of the data in each training
step, following the principles of the expectation-maximization (EM) framework.
Through synthetic experiments and real-world single-cell perturbation data, we
demonstrate improved performance when compared to using state-of-the-art
imputation techniques followed by causal learning on partially missing
interventional data.
\\ ( https://arxiv.org/abs/2402.15625 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15662 (*cross-listing*)
Date: Sat, 24 Feb 2024 00:37:37 GMT   (20677kb,D)

Title: GiMeFive: Towards Interpretable Facial Emotion Classification
Authors: Jiawen Wang and Leah Kawka
Categories: cs.CV cs.AI
Comments: 9 pages, 6 figures
\\
  Deep convolutional neural networks have been shown to successfully recognize
facial emotions for the past years in the realm of computer vision. However,
the existing detection approaches are not always reliable or explainable, we
here propose our model GiMeFive with interpretations, i.e., via layer
activations and gradient-weighted class activation mapping. We compare against
the state-of-the-art methods to classify the six facial emotions. Empirical
results show that our model outperforms the previous methods in terms of
accuracy on two Facial Emotion Recognition (FER) benchmarks and our aggregated
FER GiMeFive. Furthermore, we explain our work in real-world image and video
examples, as well as real-time live camera streams. Our code and supplementary
material are available at https: //github.com/werywjw/SEP-CVDL.
\\ ( https://arxiv.org/abs/2402.15662 ,  20677kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15687 (*cross-listing*)
Date: Sat, 24 Feb 2024 02:15:30 GMT   (2083kb,D)

Title: General Purpose Image Encoder DINOv2 for Medical Image Registration
Authors: Xinrui Song, Xuanang Xu, Pingkun Yan
Categories: cs.CV cs.AI
\\
  Existing medical image registration algorithms rely on either dataset
specific training or local texture-based features to align images. The former
cannot be reliably implemented without large modality-specific training
datasets, while the latter lacks global semantics thus could be easily trapped
at local minima. In this paper, we present a training-free deformable image
registration method, DINO-Reg, leveraging a general purpose image encoder
DINOv2 for image feature extraction. The DINOv2 encoder was trained using the
ImageNet data containing natural images. We used the pretrained DINOv2 without
any finetuning. Our method feeds the DINOv2 encoded features into a discrete
optimizer to find the optimal deformable registration field. We conducted a
series of experiments to understand the behavior and role of such a general
purpose image encoder in the application of image registration. Combined with
handcrafted features, our method won the first place in the recent OncoReg
Challenge. To our knowledge, this is the first application of general vision
foundation models in medical image registration.
\\ ( https://arxiv.org/abs/2402.15687 ,  2083kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15727 (*cross-listing*)
Date: Sat, 24 Feb 2024 05:34:43 GMT   (1711kb,D)

Title: LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A
  Vision Paper
Authors: Daoyuan Wu and Shuai Wang and Yang Liu and Ning Liu
Categories: cs.CR cs.AI
Comments: This is a vision paper on defending against LLM jailbreaks
\\
  Jailbreaking is an emerging adversarial attack that bypasses the safety
alignment deployed in off-the-shelf large language models (LLMs). A
considerable amount of research exists proposing more effective jailbreak
attacks, including the recent Greedy Coordinate Gradient (GCG) attack,
jailbreak template-based attacks such as using "Do-Anything-Now" (DAN), and
multilingual jailbreak. In contrast, the defensive side has been relatively
less explored. This paper proposes a lightweight yet practical defense called
SELFDEFEND, which can defend against all existing jailbreak attacks with
minimal delay for jailbreak prompts and negligible delay for normal user
prompts. Our key insight is that regardless of the kind of jailbreak strategies
employed, they eventually need to include a harmful prompt (e.g., "how to make
a bomb") in the prompt sent to LLMs, and we found that existing LLMs can
effectively recognize such harmful prompts that violate their safety policies.
Based on this insight, we design a shadow stack that concurrently checks
whether a harmful prompt exists in the user prompt and triggers a checkpoint in
the normal stack once a token of "No" or a harmful prompt is output. The latter
could also generate an explainable LLM response to adversarial prompts. We
demonstrate our idea of SELFDEFEND works in various jailbreak scenarios through
manual analysis in GPT-3.5/4. We also list three future directions to further
enhance SELFDEFEND.
\\ ( https://arxiv.org/abs/2402.15727 ,  1711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15746 (*cross-listing*)
Date: Sat, 24 Feb 2024 06:58:15 GMT   (6428kb,D)

Title: Intelligent Director: An Automatic Framework for Dynamic Visual
  Composition using ChatGPT
Authors: Sixiao Zheng, Jingyang Huo, Yu Wang, Yanwei Fu
Categories: cs.CV cs.AI cs.MM
Comments: Project Page: https://sixiaozheng.github.io/IntelligentDirector/
\\
  With the rise of short video platforms represented by TikTok, the trend of
users expressing their creativity through photos and videos has increased
dramatically. However, ordinary users lack the professional skills to produce
high-quality videos using professional creation software. To meet the demand
for intelligent and user-friendly video creation tools, we propose the Dynamic
Visual Composition (DVC) task, an interesting and challenging task that aims to
automatically integrate various media elements based on user requirements and
create storytelling videos. We propose an Intelligent Director framework,
utilizing LENS to generate descriptions for images and video frames and
combining ChatGPT to generate coherent captions while recommending appropriate
music names. Then, the best-matched music is obtained through music retrieval.
Then, materials such as captions, images, videos, and music are integrated to
seamlessly synthesize the video. Finally, we apply AnimeGANv2 for style
transfer. We construct UCF101-DVC and Personal Album datasets and verified the
effectiveness of our framework in solving DVC through qualitative and
quantitative comparisons, along with user studies, demonstrating its
substantial potential.
\\ ( https://arxiv.org/abs/2402.15746 ,  6428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15759 (*cross-listing*)
Date: Sat, 24 Feb 2024 08:10:54 GMT   (1838kb)

Title: Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using
  GPT-4 Generated Descriptive Prompts Without Human Annotation
Authors: Zekun Jiang, Dongjie Cheng, Ziyuan Qin, Jun Gao, Qicheng Lao, Kang Li,
  Le Zhang
Categories: cs.CV cs.AI
Comments: 12 pages, 4 figures, 4 tables
\\
  This study develops and evaluates a novel multimodal medical image zero-shot
segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual
annotations. TV-SAM incorporates and integrates large language model GPT-4,
Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously
generate descriptive text prompts and visual bounding box prompts from medical
images, thereby enhancing SAM for zero-shot segmentation. Comprehensive
evaluations are implemented on seven public datasets encompassing eight imaging
modalities to demonstrate that TV-SAM can effectively segment unseen targets
across various modalities without additional training, significantly
outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX
with gold standard bounding box prompts, and surpassing the state-of-the-art on
specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as
an effective multimodal medical image zero-shot segmentation algorithm,
highlighting the significant contribution of GPT-4 to zero-shot segmentation.
By integrating foundational models such as GPT-4, GLIP, and SAM, it could
enhance the capability to address complex problems in specialized domains. The
code is available at: https://github.com/JZK00/TV-SAM.
\\ ( https://arxiv.org/abs/2402.15759 ,  1838kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15761 (*cross-listing*)
Date: Sat, 24 Feb 2024 08:20:39 GMT   (157kb,D)

Title: Res-VMamba: Fine-Grained Food Category Visual Classification Using
  Selective State Space Models with Deep Residual Learning
Authors: Chi-Sheng Chen, Guan-Ying Chen, Dong Zhou, Di Jiang, Dai-Shi Chen
Categories: cs.CV cs.AI
Comments: 14 pages, 3 figures
\\
  Food classification is the foundation for developing food vision tasks and
plays a key role in the burgeoning field of computational nutrition. Due to the
complexity of food requiring fine-grained classification, recent academic
research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision
Transformers (ViTs) to perform food category classification. However, to learn
fine-grained features, the CNN backbone needs additional structural design,
whereas ViT, containing the self-attention module, has increased computational
complexity. In recent months, a new Sequence State Space (S4) model, through a
Selection mechanism and computation with a Scan (S6), colloquially termed
Mamba, has demonstrated superior performance and computation efficiency
compared to the Transformer architecture. The VMamba model, which incorporates
the Mamba mechanism into image tasks (such as classification), currently
establishes the state-of-the-art (SOTA) on the ImageNet dataset. In this
research, we introduce an academically underestimated food dataset CNFOOD-241,
and pioneer the integration of a residual learning framework within the VMamba
model to concurrently harness both global and local state features inherent in
the original VMamba architectural design. The research results show that VMamba
surpasses current SOTA models in fine-grained and food classification. The
proposed Res-VMamba further improves the classification accuracy to 79.54\%
without pretrained weight. Our findings elucidate that our proposed methodology
establishes a new benchmark for SOTA performance in food recognition on the
CNFOOD-241 dataset. The code can be obtained on GitHub:
https://github.com/ChiShengChen/ResVMamba.
\\ ( https://arxiv.org/abs/2402.15761 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15767 (*cross-listing*)
Date: Sat, 24 Feb 2024 08:51:03 GMT   (11308kb)

Title: PhyPlan: Compositional and Adaptive Physical Task Reasoning with
  Physics-Informed Skill Networks for Robot Manipulators
Authors: Harshil Vagadia and Mudit Chopra and Abhinav Barnawal and Tamajit
  Banerjee and Shreshth Tuli and Souvik Chakraborty and Rohan Paul
Categories: cs.RO cs.AI
\\
  Given the task of positioning a ball-like object to a goal region beyond
direct reach, humans can often throw, slide, or rebound objects against the
wall to attain the goal. However, enabling robots to reason similarly is
non-trivial. Existing methods for physical reasoning are data-hungry and
struggle with complexity and uncertainty inherent in the real world. This paper
presents PhyPlan, a novel physics-informed planning framework that combines
physics-informed neural networks (PINNs) with modified Monte Carlo Tree Search
(MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan
leverages PINNs to simulate and predict outcomes of actions in a fast and
accurate manner and uses MCTS for planning. It dynamically determines whether
to consult a PINN-based simulator (coarse but fast) or engage directly with the
actual environment (fine but slow) to determine optimal policy. Evaluation with
robots in simulated 3D environments demonstrates the ability of our approach to
solve 3D-physical reasoning tasks involving the composition of dynamic skills.
Quantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret
when learning novel tasks compared to state-of-the-art, (ii) it expedites skill
learning and enhances the speed of physical reasoning, (iii) it demonstrates
higher data efficiency compared to a physics un-informed approach.
\\ ( https://arxiv.org/abs/2402.15767 ,  11308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15769 (*cross-listing*)
Date: Sat, 24 Feb 2024 08:57:12 GMT   (895kb,D)

Title: Importance Guided Data Augmentation for Neural-Based Code Understanding
Authors: Zeming Dong, Qiang Hu, Xiaofei Xie, Maxime Cordy, Mike Papadakis,
  Jianjun Zhao
Categories: cs.SE cs.AI
\\
  Pre-trained code models lead the era of code intelligence. Many models have
been designed with impressive performance recently. However, one important
problem, data augmentation for code data that automatically helps developers
prepare training data lacks study in the field of code learning. In this paper,
we introduce a general data augmentation framework, GenCode, to enhance the
training of code understanding models. GenCode follows a
generation-and-selection paradigm to prepare useful training codes.
Specifically, it uses code transformation techniques to generate new code
candidates first and then selects important ones as the training data by
importance metrics. To evaluate the effectiveness of GenCode with a general
importance metric -- loss value, we conduct experiments on four code
understanding tasks (e.g., code clone detection) and three pre-trained code
models (e.g., CodeT5). Compared to the state-of-the-art (SOTA) code
augmentation method, MixCode, GenCode produces code models with 2.92% higher
accuracy and 4.90% robustness on average.
\\ ( https://arxiv.org/abs/2402.15769 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15770 (*cross-listing*)
Date: Sat, 24 Feb 2024 09:06:25 GMT   (86kb,D)

Title: From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for
  Opportunities, Risks, and Regulatory Compliance in Commercializing Large
  Language Models
Authors: Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Raza
  Nowrozy, Malka N. Halgamuge
Categories: cs.CY cs.AI
\\
  This study investigated the integration readiness of four predominant
cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0,
COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the
opportunities, risks, and regulatory compliance when adopting Large Language
Models (LLMs), using qualitative content analysis and expert validation. Our
analysis, with both LLMs and human experts in the loop, uncovered potential for
LLM integration together with inadequacies in LLM risk oversight of those
frameworks. Comparative gap analysis has highlighted that the new ISO
42001:2023, specifically designed for Artificial Intelligence (AI) management
systems, provided most comprehensive facilitation for LLM opportunities,
whereas COBIT 2019 aligned most closely with the impending European Union AI
Act. Nonetheless, our findings suggested that all evaluated frameworks would
benefit from enhancements to more effectively and more comprehensively address
the multifaceted risks associated with LLMs, indicating a critical and
time-sensitive need for their continuous evolution. We propose integrating
human-expert-in-the-loop validation processes as crucial for enhancing
cybersecurity frameworks to support secure and compliant LLM integration, and
discuss implications for the continuous evolution of cybersecurity GRC
frameworks to support the secure integration of LLMs.
\\ ( https://arxiv.org/abs/2402.15770 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15779 (*cross-listing*)
Date: Sat, 24 Feb 2024 10:02:21 GMT   (8223kb)

Title: Cryptanalysis and improvement of multimodal data encryption by
  machine-learning-based system
Authors: Zakaria Tolba
Categories: cs.CR cs.AI cs.IR
Comments: Doctoral thesis. Keywords: Cryptanalysis, Black-box, Deep learning,
  Machine learning, Ciphertext, Plaintext, Genetic algorithm, Permutation box,
  Substitution Box
\\
  With the rising popularity of the internet and the widespread use of networks
and information systems via the cloud and data centers, the privacy and
security of individuals and organizations have become extremely crucial. In
this perspective, encryption consolidates effective technologies that can
effectively fulfill these requirements by protecting public information
exchanges. To achieve these aims, the researchers used a wide assortment of
encryption algorithms to accommodate the varied requirements of this field, as
well as focusing on complex mathematical issues during their work to
substantially complicate the encrypted communication mechanism. as much as
possible to preserve personal information while significantly reducing the
possibility of attacks. Depending on how complex and distinct the requirements
established by these various applications are, the potential of trying to break
them continues to occur, and systems for evaluating and verifying the
cryptographic algorithms implemented continue to be necessary. The best
approach to analyzing an encryption algorithm is to identify a practical and
efficient technique to break it or to learn ways to detect and repair weak
aspects in algorithms, which is known as cryptanalysis. Experts in
cryptanalysis have discovered several methods for breaking the cipher, such as
discovering a critical vulnerability in mathematical equations to derive the
secret key or determining the plaintext from the ciphertext. There are various
attacks against secure cryptographic algorithms in the literature, and the
strategies and mathematical solutions widely employed empower cryptanalysts to
demonstrate their findings, identify weaknesses, and diagnose maintenance
failures in algorithms.
\\ ( https://arxiv.org/abs/2402.15779 ,  8223kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15820 (*cross-listing*)
Date: Sat, 24 Feb 2024 14:10:17 GMT   (1546kb,D)

Title: DART: Depth-Enhanced Accurate and Real-Time Background Matting
Authors: Hanxi Li, Guofeng Li, Bo Li, Lin Wu and Yan Cheng
Categories: cs.CV cs.AI
\\
  Matting with a static background, often referred to as ``Background Matting"
(BGM), has garnered significant attention within the computer vision community
due to its pivotal role in various practical applications like webcasting and
photo editing. Nevertheless, achieving highly accurate background matting
remains a formidable challenge, primarily owing to the limitations inherent in
conventional RGB images. These limitations manifest in the form of
susceptibility to varying lighting conditions and unforeseen shadows.
  In this paper, we leverage the rich depth information provided by the
RGB-Depth (RGB-D) cameras to enhance background matting performance in
real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm
to incorporate depth information. The resulting model's output undergoes
refinement through Bayesian inference, incorporating a background depth prior.
The posterior prediction is then translated into a "trimap," which is
subsequently fed into a state-of-the-art matting algorithm to generate more
precise alpha mattes. To ensure real-time matting capabilities, a critical
requirement for many real-world applications, we distill the backbone of our
model from a larger and more versatile BGM network. Our experiments demonstrate
the superior performance of the proposed method. Moreover, thanks to the
distillation operation, our method achieves a remarkable processing speed of 33
frames per second (fps) on a mid-range edge-computing device. This high
efficiency underscores DART's immense potential for deployment in mobile
applications}
\\ ( https://arxiv.org/abs/2402.15820 ,  1546kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15821 (*cross-listing*)
Date: Sat, 24 Feb 2024 14:17:41 GMT   (17188kb,D)

Title: Cooperation and Control in Delegation Games
Authors: Oliver Sourbut and Lewis Hammond and Harriet Wood
Categories: cs.GT cs.AI cs.MA
\\
  Many settings of interest involving humans and machines -- from virtual
personal assistants to autonomous vehicles -- can naturally be modelled as
principals (humans) delegating to agents (machines), which then interact with
each other on their principals' behalf. We refer to these multi-principal,
multi-agent scenarios as delegation games. In such games, there are two
important failure modes: problems of control (where an agent fails to act in
line their principal's preferences) and problems of cooperation (where the
agents fail to work well together). In this paper we formalise and analyse
these problems, further breaking them down into issues of alignment (do the
players have similar preferences?) and capabilities (how competent are the
players at satisfying those preferences?). We show -- theoretically and
empirically -- how these measures determine the principals' welfare, how they
can be estimated using limited observations, and thus how they might be used to
help us design more aligned and cooperative AI systems.
\\ ( https://arxiv.org/abs/2402.15821 ,  17188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15832 (*cross-listing*)
Date: Sat, 24 Feb 2024 14:59:19 GMT   (17442kb,D)

Title: Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and
  Eosin Whole Slide Images: An Indian cohort Study
Authors: Ekansh Chauhan, Amit Sharma, Megha S Uppin, C.V. Jawahar and Vinod P.K
Categories: cs.CV cs.AI
\\
  Brain tumors represent a severe and life-threatening condition, demanding
precise diagnosis and tailored treatment strategies. This study advances
patient care with findings from rigorous multiple-instance-learning
experimentations across various feature extractors and aggregators in brain
tumor histopathology. It establishes new performance benchmarks in glioma
subtype classification across multiple datasets, including a novel dataset
focused on the Indian demographic (IPD-Brain), providing a valuable resource
for existing research. Using a ResNet-50, pretrained on histopathology
datasets, for feature extraction, combined with DTFD feature aggregator, our
approach achieves state-of-the-art AUCs of 88.08 on IPD-Brain and 95.81 on
TCGA-Brain dataset respectively for three-way glioma subtype classification.
Moreover, it establishes new benchmarks in grading and detecting IHC molecular
biomarkers (IDH1 (mutant R132H), TP53, ATRX, Ki-67) through H&E stained whole
slide images for the IPD-Brain dataset. The work also highlights a significant
correlation between the model decision-making processes and the diagnostic
reasoning of pathologists, underscoring its capability to mimic professional
diagnostic procedures.
\\ ( https://arxiv.org/abs/2402.15832 ,  17442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15892 (*cross-listing*)
Date: Sat, 24 Feb 2024 19:59:15 GMT   (8427kb,D)

Title: Statistical Games
Authors: Jozsef Konczer
Categories: math.ST cs.AI cs.GT cs.LG econ.TH stat.ML stat.TH
Comments: 129 pages, 51 figures
MSC-class: 62C05, 62C20, 91A35, 68T37, 68T05
\\
  This work contains the mathematical exploration of a few prototypical games
in which central concepts from statistics and probability theory naturally
emerge. The first two kinds of games are termed Fisher and Bayesian games,
which are connected to Frequentist and Bayesian statistics, respectively.
Later, a more general type of game is introduced, termed Statistical game, in
which a further parameter, the players' relative risk aversion, can be set. In
this work, we show that Fisher and Bayesian games can be viewed as limiting
cases of Statistical games. Therefore, Statistical games can be viewed as a
unified framework, incorporating both Frequentist and Bayesian statistics.
Furthermore, a philosophical framework is (re-)presented -- often referred to
as minimax regret criterion -- as a general approach to decision making.
  The main motivation for this work was to embed Bayesian statistics into a
broader decision-making framework, where, based on collected data, actions with
consequences have to be made, which can be translated to utilities (or
rewards/losses) of the decision-maker. The work starts with the simplest
possible toy model, related to hypothesis testing and statistical inference.
This choice has two main benefits: i.) it allows us to determine (conjecture)
the behaviour of the equilibrium strategies in various limiting cases ii.) this
way, we can introduce Statistical games without requiring additional stochastic
parameters. The work contains game theoretical methods related to two-player,
non-cooperative games to determine and prove equilibrium strategies of Fisher,
Bayesian and Statistical games. It also relies on analytical tools for
derivations concerning various limiting cases.
\\ ( https://arxiv.org/abs/2402.15892 ,  8427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15905 (*cross-listing*)
Date: Sat, 24 Feb 2024 21:03:30 GMT   (2644kb,D)

Title: Explainable Contrastive and Cost-Sensitive Learning for Cervical Cancer
  Classification
Authors: Ashfiqun Mustari, Rushmia Ahmed, Afsara Tasnim, Jakia Sultana Juthi
  and G M Shahariar
Categories: cs.CV cs.AI cs.LG
Comments: Accepted and presented in 26th International Conference on Computer
  and Information Technology (ICCIT 2023)
\\
  This paper proposes an efficient system for classifying cervical cancer cells
using pre-trained convolutional neural networks (CNNs). We first fine-tune five
pre-trained CNNs and minimize the overall cost of misclassification by
prioritizing accuracy for certain classes that have higher associated costs or
importance. To further enhance the performance of the models, supervised
contrastive learning is included to make the models more adept at capturing
important features and patterns. Extensive experimentation are conducted to
evaluate the proposed system on the SIPaKMeD dataset. The experimental results
demonstrate the effectiveness of the developed system, achieving an accuracy of
97.29%. To make our system more trustworthy, we have employed several
explainable AI techniques to interpret how the models reached a specific
decision. The implementation of the system can be found at -
https://github.com/isha-67/CervicalCancerStudy.
\\ ( https://arxiv.org/abs/2402.15905 ,  2644kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15933 (*cross-listing*)
Date: Sat, 24 Feb 2024 23:31:34 GMT   (470kb,D)

Title: Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion
  Approach for 3D VQA
Authors: Wentao Mo, Yang Liu
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: To be published in AAAI 24
\\
  In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated
data and limited visual content diversity hampers the generalization to novel
scenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and
SQA dataset). Current approaches resort supplement 3D reasoning with 2D
information. However, these methods face challenges: either they use top-down
2D views that introduce overly complex and sometimes question-irrelevant visual
clues, or they rely on globally aggregated scene/image-level representations
from 2D VLMs, losing the fine-grained vision-language correlations. To overcome
these limitations, our approach utilizes question-conditional 2D view selection
procedure, pinpointing semantically relevant 2D inputs for crucial visual
clues. We then integrate this 2D knowledge into the 3D-VQA system via a
two-branch Transformer structure. This structure, featuring a Twin-Transformer
design, compactly combines 2D and 3D modalities and captures fine-grained
correlations between modalities, allowing them mutually augmenting each other.
Integrating proposed mechanisms above, we present BridgeQA, that offers a fresh
perspective on multi-modal transformer-based architectures for 3D-VQA.
Experiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets
and significantly outperforms existing solutions. Code is available at
$\href{https://github.com/matthewdm0816/BridgeQA}{\text{this URL}}$.
\\ ( https://arxiv.org/abs/2402.15933 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15943 (*cross-listing*)
Date: Sun, 25 Feb 2024 00:53:16 GMT   (864kb)

Title: Rethinking Software Engineering in the Era of Foundation Models: A
  Curated Catalogue of Challenges in the Development of Trustworthy FMware
Authors: Ahmed E. Hassan, Dayi Lin, Gopi Krishnan Rajbahadur, Keheliya Gallaba,
  Filipe R. Cogo, Boyuan Chen, Haoxiang Zhang, Kishanthan Thangarajah, Gustavo
  Ansaldi Oliva, Jiahuei Lin, Wali Mohammad Abdullah, Zhen Ming Jiang
Categories: cs.SE cs.AI
\\
  Foundation models (FMs), such as Large Language Models (LLMs), have
revolutionized software development by enabling new use cases and business
models. We refer to software built using FMs as FMware. The unique properties
of FMware (e.g., prompts, agents, and the need for orchestration), coupled with
the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new
set of software engineering challenges. Based on our industrial experience, we
identified 10 key SE4FMware challenges that have caused enterprise FMware
development to be unproductive, costly, and risky. In this paper, we discuss
these challenges in detail and state the path for innovation that we envision.
Next, we present FMArts, which is our long-term effort towards creating a
cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we
(i) show how the unique properties of FMArts enabled us to design and develop a
complex FMware for a large customer in a timely manner and (ii) discuss the
lessons that we learned in doing so. We hope that the disclosure of the
aforementioned challenges and our associated efforts to tackle them will not
only raise awareness but also promote deeper and further discussions, knowledge
sharing, and innovative solutions across the software engineering discipline.
\\ ( https://arxiv.org/abs/2402.15943 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15945 (*cross-listing*)
Date: Sun, 25 Feb 2024 01:10:55 GMT   (424kb)

Title: Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to
  Cybersecurity Threat Management
Authors: Mohammed Abo Sen
Categories: cs.CR cs.AI
\\
  This paper proposes an innovative Attention-GAN framework for enhancing
cybersecurity, focusing on anomaly detection. In response to the challenges
posed by the constantly evolving nature of cyber threats, the proposed approach
aims to generate diverse and realistic synthetic attack scenarios, thereby
enriching the dataset and improving threat identification. Integrating
attention mechanisms with Generative Adversarial Networks (GANs) is a key
feature of the proposed method. The attention mechanism enhances the model's
ability to focus on relevant features, essential for detecting subtle and
complex attack patterns. In addition, GANs address the issue of data scarcity
by generating additional varied attack data, encompassing known and emerging
threats. This dual approach ensures that the system remains relevant and
effective against the continuously evolving cyberattacks. The KDD Cup and
CICIDS2017 datasets were used to validate this model, which exhibited
significant improvements in anomaly detection. It achieved an accuracy of
99.69% on the KDD dataset and 97.93% on the CICIDS2017 dataset, with precision,
recall, and F1-scores above 97%, demonstrating its effectiveness in recognizing
complex attack patterns. This study contributes significantly to cybersecurity
by providing a scalable and adaptable solution for anomaly detection in the
face of sophisticated and dynamic cyber threats. The exploration of GANs for
data augmentation highlights a promising direction for future research,
particularly in situations where data limitations restrict the development of
cybersecurity systems. The attention-GAN framework has emerged as a pioneering
approach, setting a new benchmark for advanced cyber-defense strategies.
\\ ( https://arxiv.org/abs/2402.15945 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15990 (*cross-listing*)
Date: Sun, 25 Feb 2024 05:05:52 GMT   (2155kb,D)

Title: An Empirical Study of Challenges in Machine Learning Asset Management
Authors: Zhimin Zhao, Yihao Chen, Abdul Ali Bangash, Bram Adams, Ahmed E.
  Hassan
Categories: cs.SE cs.AI
\\
  In machine learning (ML), efficient asset management, including ML models,
datasets, algorithms, and tools, is vital for resource optimization, consistent
performance, and a streamlined development lifecycle. This enables quicker
iterations, adaptability, reduced development-to-deployment time, and reliable
outputs. Despite existing research, a significant knowledge gap remains in
operational challenges like model versioning, data traceability, and
collaboration, which are crucial for the success of ML projects. Our study aims
to address this gap by analyzing 15,065 posts from developer forums and
platforms, employing a mixed-method approach to classify inquiries, extract
challenges using BERTopic, and identify solutions through open card sorting and
BERTopic clustering. We uncover 133 topics related to asset management
challenges, grouped into 16 macro-topics, with software dependency, model
deployment, and model training being the most discussed. We also find 79
solution topics, categorized under 18 macro-topics, highlighting software
dependency, feature development, and file management as key solutions. This
research underscores the need for further exploration of identified pain points
and the importance of collaborative efforts across academia, industry, and the
research community.
\\ ( https://arxiv.org/abs/2402.15990 ,  2155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16002 (*cross-listing*)
Date: Sun, 25 Feb 2024 06:19:04 GMT   (909kb)

Title: Post-Quantum Cryptography Neural Network
Authors: Abel C. H. Chen
Categories: cs.CR cs.AI cs.LG
Comments: 2023 International Conference on Smart Systems for applications in
  Electrical Sciences (ICSSES) 7-8 July 2023. The manuscript was written in
  Chinese and submitted on 10 March 2023, but it was rejected on 22 April 2023.
  The appeal was accepted on 24 February 2024
DOI: 10.1109/ICSSES58299.2023.10201083
\\
  In recent years, quantum computers and Shor quantum algorithm have posed a
threat to current mainstream asymmetric cryptography methods (e.g. RSA and
Elliptic Curve Cryptography (ECC)). Therefore, it is necessary to construct a
Post-Quantum Cryptography (PQC) method to resist quantum computing attacks.
Therefore, this study proposes a PQC-based neural network that maps a
code-based PQC method to a neural network structure and enhances the security
of ciphertexts with non-linear activation functions, random perturbation of
ciphertexts, and uniform distribution of ciphertexts. In practical experiments,
this study uses cellular network signals as a case study to demonstrate that
encryption and decryption can be performed by the proposed PQC-based neural
network with the uniform distribution of ciphertexts. In the future, the
proposed PQC-based neural network could be applied to various applications.
\\ ( https://arxiv.org/abs/2402.16002 ,  909kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16052 (*cross-listing*)
Date: Sun, 25 Feb 2024 10:32:18 GMT   (30824kb,D)

Title: Maximizing UAV Fog Deployment Efficiency for Critical Rescue Operations
Authors: Abdenacer Naouri, Huansheng Ning, Nabil Abdelkader Nouri, Amar
  Khelloufi, Abdelkarim Ben Sada, Salim Naouri, Attia Qammar and Sahraoui
  Dhelim
Categories: cs.NI cs.AI eess.SP
\\
  In disaster scenarios and high-stakes rescue operations, integrating Unmanned
Aerial Vehicles (UAVs) as fog nodes has become crucial. This integration
ensures a smooth connection between affected populations and essential health
monitoring devices, supported by the Internet of Things (IoT). Integrating UAVs
in such environments is inherently challenging, where the primary objectives
involve maximizing network connectivity and coverage while extending the
network's lifetime through energy-efficient strategies to serve the maximum
number of affected individuals. In this paper, We propose a novel model centred
around dynamic UAV-based fog deployment that optimizes the system's
adaptability and operational efficacy within the afflicted areas. First, we
decomposed the problem into two subproblems. Connectivity and coverage
subproblem, and network lifespan optimization subproblem. We shape our UAV fog
deployment problem as a uni-objective optimization and introduce a specialized
UAV fog deployment algorithm tailored specifically for UAV fog nodes deployed
in rescue missions. While the network lifespan optimization subproblem is
efficiently solved via a one-dimensional swapping method. Following that, We
introduce a novel optimization strategy for UAV fog node placement in dynamic
networks during evacuation scenarios, with a primary focus on ensuring robust
connectivity and maximal coverage for mobile users, while extending the
network's lifespan. Finally, we introduce Adaptive Whale Optimization Algorithm
(WOA) for fog node deployment in a dynamic network. Its agility, rapid
convergence, and low computational demands make it an ideal fit for
high-pressure environments.
\\ ( https://arxiv.org/abs/2402.16052 ,  30824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16068 (*cross-listing*)
Date: Sun, 25 Feb 2024 11:37:23 GMT   (847kb,D)

Title: ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot
  Interaction Applications
Authors: Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola
  Bellotto
Categories: cs.RO cs.AI
Comments: Accepted by the "Causal-HRI: Causal Learning for Human-Robot
  Interaction" workshop at the 2024 ACM/IEEE International Conference on
  Human-Robot Interaction (HRI)
\\
  Deploying robots in human-shared spaces requires understanding interactions
among nearby agents and objects. Modelling cause-and-effect relations through
causal inference aids in predicting human behaviours and anticipating robot
interventions. However, a critical challenge arises as existing causal
discovery methods currently lack an implementation inside the ROS ecosystem,
the standard de facto in robotics, hindering effective utilisation in robotics.
To address this gap, this paper introduces ROS-Causal, a ROS-based framework
for onboard data collection and causal discovery in human-robot spatial
interactions. An ad-hoc simulator, integrated with ROS, illustrates the
approach's effectiveness, showcasing the robot onboard generation of causal
models during data collection. ROS-Causal is available on GitHub:
https://github.com/lcastri/roscausal.git.
\\ ( https://arxiv.org/abs/2402.16068 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16073 (*cross-listing*)
Date: Sun, 25 Feb 2024 12:06:33 GMT   (603kb,D)

Title: Pfeed: Generating near real-time personalized feeds using precomputed
  embedding similarities
Authors: Binyam Gebre, Karoliina Ranta, Stef van den Elzen, Ernst Kuiper, Thijs
  Baars, Tom Heskes
Categories: cs.IR cs.AI cs.LG
Comments: 9 pages, 8 figures
ACM-class: H.3.3
\\
  In personalized recommender systems, embeddings are often used to encode
customer actions and items, and retrieval is then performed in the embedding
space using approximate nearest neighbor search. However, this approach can
lead to two challenges: 1) user embeddings can restrict the diversity of
interests captured and 2) the need to keep them up-to-date requires an
expensive, real-time infrastructure. In this paper, we propose a method that
overcomes these challenges in a practical, industrial setting. The method
dynamically updates customer profiles and composes a feed every two minutes,
employing precomputed embeddings and their respective similarities. We tested
and deployed this method to personalise promotional items at Bol, one of the
largest e-commerce platforms of the Netherlands and Belgium. The method
enhanced customer engagement and experience, leading to a significant 4.9%
uplift in conversions.
\\ ( https://arxiv.org/abs/2402.16073 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16086 (*cross-listing*)
Date: Sun, 25 Feb 2024 13:22:17 GMT   (9048kb,D)

Title: Deep Homography Estimation for Visual Place Recognition
Authors: Feng Lu, Shuting Dong, Lijun Zhang, Bingxi Liu, Xiangyuan Lan, Dongmei
  Jiang, Chun Yuan
Categories: cs.CV cs.AI
Comments: Accepted by AAAI2024
\\
  Visual place recognition (VPR) is a fundamental task for many applications
such as robot localization and augmented reality. Recently, the hierarchical
VPR methods have received considerable attention due to the trade-off between
accuracy and efficiency. They usually first use global features to retrieve the
candidate images, then verify the spatial consistency of matched local features
for re-ranking. However, the latter typically relies on the RANSAC algorithm
for fitting homography, which is time-consuming and non-differentiable. This
makes existing methods compromise to train the network only in global feature
extraction. Here, we propose a transformer-based deep homography estimation
(DHE) network that takes the dense feature map extracted by a backbone network
as input and fits homography for fast and learnable geometric verification.
Moreover, we design a re-projection error of inliers loss to train the DHE
network without additional homography labels, which can also be jointly trained
with the backbone network to help it extract the features that are more
suitable for local matching. Extensive experiments on benchmark datasets show
that our method can outperform several state-of-the-art methods. And it is more
than one order of magnitude faster than the mainstream hierarchical VPR methods
using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.
\\ ( https://arxiv.org/abs/2402.16086 ,  9048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16117 (*cross-listing*)
Date: Sun, 25 Feb 2024 15:31:43 GMT   (10803kb,D)

Title: RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis
Authors: Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu,
  Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, Peize
  Sun, Haibao Yu, Chao Yang, Wenqi Shao, Wenhai Wang, Jifeng Dai, Yu Qiao,
  Mingyu Ding, Ping Luo
Categories: cs.RO cs.AI cs.CV
\\
  Robotic behavior synthesis, the problem of understanding multimodal inputs
and generating precise physical control for robots, is an important part of
Embodied AI. Despite successes in applying multimodal large language models for
high-level understanding, it remains challenging to translate these conceptual
understandings into detailed robotic actions while achieving generalization
across various scenarios. In this paper, we propose a tree-structured
multimodal code generation framework for generalized robotic behavior
synthesis, termed RoboCodeX. RoboCodeX decomposes high-level human instructions
into multiple object-centric manipulation units consisting of physical
preferences such as affordance and safety constraints, and applies code
generation to introduce generalization ability across various robotics
platforms. To further enhance the capability to map conceptual and perceptual
understanding into control commands, a specialized multimodal reasoning dataset
is collected for pre-training and an iterative self-updating methodology is
introduced for supervised fine-tuning. Extensive experiments demonstrate that
RoboCodeX achieves state-of-the-art performance in both simulators and real
robots on four different kinds of manipulation tasks and one navigation task.
\\ ( https://arxiv.org/abs/2402.16117 ,  10803kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16121 (*cross-listing*)
Date: Sun, 25 Feb 2024 15:42:12 GMT   (2528kb,D)

Title: Towards Accurate Post-training Quantization for Reparameterized Models
Authors: Luoming Zhang, Yefei He, Wen Fei, Zhenyu Lou, Weijia Wu, YangWei Ying,
  and Hong Zhou
Categories: cs.CV cs.AI
\\
  Model reparameterization is a widely accepted technique for improving
inference speed without compromising performance. However, current
Post-training Quantization (PTQ) methods often lead to significant accuracy
degradation when applied to reparameterized models. This is primarily caused by
channel-specific and sample-specific outliers, which appear only at specific
samples and channels and impact on the selection of quantization parameters. To
address this issue, we propose RepAPQ, a novel framework that preserves the
accuracy of quantized reparameterization models. Different from previous
frameworks using Mean Squared Error (MSE) as a measurement, we utilize Mean
Absolute Error (MAE) to mitigate the influence of outliers on quantization
parameters. Our framework comprises two main components: Quantization
Protecting Reparameterization and Across-block Calibration. For effective
calibration, Quantization Protecting Reparameterization combines multiple
branches into a single convolution with an affine layer. During training, the
affine layer accelerates convergence and amplifies the output of the
convolution to better accommodate samples with outliers. Additionally,
Across-block Calibration leverages the measurement of stage output as
supervision to address the gradient problem introduced by MAE and enhance the
interlayer correlation with quantization parameters. Comprehensive experiments
demonstrate the effectiveness of RepAPQ across various models and tasks. Our
framework outperforms previous methods by approximately 1\% for 8-bit PTQ and
2\% for 6-bit PTQ, showcasing its superior performance. The code is available
at \url{https://github.com/ilur98/DLMC-QUANT}.
\\ ( https://arxiv.org/abs/2402.16121 ,  2528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16153 (*cross-listing*)
Date: Sun, 25 Feb 2024 17:19:41 GMT   (11375kb,D)

Title: ChatMusician: Understanding and Generating Music Intrinsically with LLM
Authors: Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao
  Shen, Ge Zhang, Yuhang Wu, Cong Liu, Ziya Zhou, Ziyang Ma, Liumeng Xue, Ziyu
  Wang, Qin Liu, Tianyu Zheng, Yizhi Li, Yinghao Ma, Yiming Liang, Xiaowei Chi,
  Ruibo Liu, Zili Wang, Pengfei Li, Jingcheng Wu, Chenghua Lin, Qifeng Liu, Tao
  Jiang, Wenhao Huang, Wenhu Chen, Emmanouil Benetos, Jie Fu, Gus Xia, Roger
  Dannenberg, Wei Xue, Shiyin Kang, Yike Guo
Categories: cs.SD cs.AI cs.CL cs.LG cs.MM eess.AS
Comments: GitHub: https://shanghaicannon.github.io/ChatMusician/
\\
  While Large Language Models (LLMs) demonstrate impressive capabilities in
text generation, we find that their ability has yet to be generalized to music,
humanity's creative language. We introduce ChatMusician, an open-source LLM
that integrates intrinsic musical abilities. It is based on continual
pre-training and finetuning LLaMA2 on a text-compatible music representation,
ABC notation, and the music is treated as a second language. ChatMusician can
understand and generate music with a pure text tokenizer without any external
multi-modal neural structures or tokenizers. Interestingly, endowing musical
abilities does not harm language abilities, even achieving a slightly higher
MMLU score. Our model is capable of composing well-structured, full-length
music, conditioned on texts, chords, melodies, motifs, musical forms, etc,
surpassing GPT-4 baseline. On our meticulously curated college-level music
understanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and
GPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs
can be an excellent compressor for music, but there remains significant
territory to be conquered. We release our 4B token music-language corpora
MusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.
\\ ( https://arxiv.org/abs/2402.16153 ,  11375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16173 (*cross-listing*)
Date: Sun, 25 Feb 2024 18:58:09 GMT   (685kb)

Title: Communication Traffic Characteristics Reveal an IoT Devices Identity
Authors: Rajarshi Roy Chowdhury, Debashish Roy, and Pg Emeroylariffion Abas
Categories: cs.NI cs.AI
Comments: 16 pages
ACM-class: F.2.2; I.2.7
\\
  Internet of Things (IoT) is one of the technological advancements of the
twenty-first century which can improve living standards. However, it also
imposes new types of security challenges, including device authentication,
traffic types classification, and malicious traffic identification, in the
network domain. Traditionally, internet protocol (IP) and media access control
(MAC) addresses are utilized for identifying network-connected devices in a
network, whilst these addressing schemes are prone to be compromised, including
spoofing attacks and MAC randomization. Therefore, device identification using
only explicit identifiers is a challenging task. Accurate device identification
plays a key role in securing a network. In this paper, a supervised machine
learning-based device fingerprinting (DFP) model has been proposed for
identifying network-connected IoT devices using only communication traffic
characteristics (or implicit identifiers). A single transmission control
protocol/internet protocol (TCP/IP) packet header features have been utilized
for generating unique fingerprints, with the fingerprints represented as a
vector of 22 features. Experimental results have shown that the proposed DFP
method achieves over 98% in classifying individual IoT devices using the UNSW
dataset with 22 smart-home IoT devices. This signifies that the proposed
approach is invaluable to network operators in making their networks more
secure.
\\ ( https://arxiv.org/abs/2402.16173 ,  685kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16174 (*cross-listing*)
Date: Sun, 25 Feb 2024 18:59:29 GMT   (7189kb,D)

Title: GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction
Authors: Xiao Chen and Quanyi Li and Tai Wang and Tianfan Xue and Jiangmiao
  Pang
Categories: cs.CV cs.AI cs.RO
\\
  While recent advances in neural radiance field enable realistic digitization
for large-scale scenes, the image-capturing process is still time-consuming and
labor-intensive. Previous works attempt to automate this process using the
Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing
NBV policies heavily rely on hand-crafted criteria, limited action space, or
per-scene optimized representations. These constraints limit their
cross-dataset generalizability. To overcome them, we propose GenNBV, an
end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning
(RL)-based framework and extends typical limited action space to 5D free space.
It empowers our agent drone to scan from any viewpoint, and even interact with
unseen geometries during training. To boost the cross-dataset generalizability,
we also propose a novel multi-source state embedding, including geometric,
semantic, and action representations. We establish a benchmark using the Isaac
Gym simulator with the Houses3K and OmniObject3D datasets to evaluate this NBV
policy. Experiments demonstrate that our policy achieves a 98.26% and 97.12%
coverage ratio on unseen building-scale objects from these datasets,
respectively, outperforming prior solutions.
\\ ( https://arxiv.org/abs/2402.16174 ,  7189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16189 (*cross-listing*)
Date: Sun, 25 Feb 2024 20:30:05 GMT   (1806kb,D)

Title: One-stage Prompt-based Continual Learning
Authors: Youngeun Kim, Yuhang Li, Priyadarshini Panda
Categories: cs.CV cs.AI
\\
  Prompt-based Continual Learning (PCL) has gained considerable attention as a
promising continual learning solution as it achieves state-of-the-art
performance while preventing privacy violation and memory overhead issues.
Nonetheless, existing PCL approaches face significant computational burdens
because of two Vision Transformer (ViT) feed-forward stages; one is for the
query ViT that generates a prompt query to select prompts inside a prompt pool;
the other one is a backbone ViT that mixes information between selected prompts
and image tokens. To address this, we introduce a one-stage PCL framework by
directly using the intermediate layer's token embedding as a prompt query. This
design removes the need for an additional feed-forward stage for query ViT,
resulting in ~50% computational cost reduction for both training and inference
with marginal accuracy drop < 1%. We further introduce a Query-Pool
Regularization (QR) loss that regulates the relationship between the prompt
query and the prompt pool to improve representation power. The QR loss is only
applied during training time, so there is no computational overhead at
inference from the QR loss. With the QR loss, our approach maintains ~ 50%
computational cost reduction during inference as well as outperforms the prior
two-stage PCL methods by ~1.4% on public class-incremental continual learning
benchmarks including CIFAR-100, ImageNet-R, and DomainNet.
\\ ( https://arxiv.org/abs/2402.16189 ,  1806kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16200 (*cross-listing*)
Date: Sun, 25 Feb 2024 21:25:06 GMT   (6497kb,D)

Title: IR2: Information Regularization for Information Retrieval
Authors: Jianyou Wang, Kaicheng Wang, Xiaoyue Wang, Weili Cao, Ramamohan
  Paturi, Leon Bergen
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Accepted by LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
\\
  Effective information retrieval (IR) in settings with limited training data,
particularly for complex queries, remains a challenging task. This paper
introduces IR2, Information Regularization for Information Retrieval, a
technique for reducing overfitting during synthetic data generation. This
approach, representing a novel application of regularization techniques in
synthetic data creation for IR, is tested on three recent IR tasks
characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook.
Experimental results indicate that our regularization techniques not only
outperform previous synthetic query generation methods on the tasks considered
but also reduce cost by up to 50%. Furthermore, this paper categorizes and
explores three regularization methods at different stages of the query
synthesis pipeline-input, prompt, and output-each offering varying degrees of
performance improvement compared to models where no regularization is applied.
This provides a systematic approach for optimizing synthetic data generation in
data-limited, complex-query IR scenarios. All code, prompts and synthetic data
are available at
https://github.com/Info-Regularization/Information-Regularization.
\\ ( https://arxiv.org/abs/2402.16200 ,  6497kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16235 (*cross-listing*)
Date: Mon, 26 Feb 2024 01:44:24 GMT   (1964kb,D)

Title: Human-AI Co-Creation of Worked Examples for Programming Classes
Authors: Mohammad Hassany, Peter Brusilovsky, Jiaze Ke, Kamil Akhuseyinoglu and
  Arun Balajiee Lekshmi Narayanan
Categories: cs.HC cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2312.02105
\\
  Worked examples (solutions to typical programming problems presented as a
source code in a certain language and are used to explain the topics from a
programming class) are among the most popular types of learning content in
programming classes. Most approaches and tools for presenting these examples to
students are based on line-by-line explanations of the example code. However,
instructors rarely have time to provide line-by-line explanations for a large
number of examples typically used in a programming class. In this paper, we
explore and assess a human-AI collaboration approach to authoring worked
examples for Java programming. We introduce an authoring system for creating
Java worked examples that generates a starting version of code explanations and
presents it to the instructor to edit if necessary.We also present a study that
assesses the quality of explanations created with this approach
\\ ( https://arxiv.org/abs/2402.16235 ,  1964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16242 (*cross-listing*)
Date: Mon, 26 Feb 2024 02:03:08 GMT   (6444kb,D)

Title: HSONet:A Siamese foreground association-driven hard case sample
  optimization network for high-resolution remote sensing image change
  detection
Authors: Chao Tao, Dongsheng Kuang, Zhenyang Huang, Chengli Peng, Haifeng Li
Categories: cs.CV cs.AI
Comments: 17 figures, 8 tables, 18 pages
\\
  In the later training stages, further improvement of the models ability to
determine changes relies on how well the change detection (CD) model learns
hard cases; however, there are two additional challenges to learning hard case
samples: (1) change labels are limited and tend to pointer only to foreground
targets, yet hard case samples are prevalent in the background, which leads to
optimizing the loss function focusing on the foreground targets and ignoring
the background hard cases, which we call imbalance. (2) Complex situations,
such as light shadows, target occlusion, and seasonal changes, induce hard case
samples, and in the absence of both supervisory and scene information, it is
difficult for the model to learn hard case samples directly to accurately
obtain the feature representations of the change information, which we call
missingness. We propose a Siamese foreground association-driven hard case
sample optimization network (HSONet). To deal with this imbalance, we propose
an equilibrium optimization loss function to regulate the optimization focus of
the foreground and background, determine the hard case samples through the
distribution of the loss values, and introduce dynamic weights in the loss term
to gradually shift the optimization focus of the loss from the foreground to
the background hard cases as the training progresses. To address this
missingness, we understand hard case samples with the help of the scene
context, propose the scene-foreground association module, use potential remote
sensing spatial scene information to model the association between the target
of interest in the foreground and the related context to obtain scene
embedding, and apply this information to the feature reinforcement of hard
cases. Experiments on four public datasets show that HSONet outperforms current
state-of-the-art CD methods, particularly in detecting hard case samples.
\\ ( https://arxiv.org/abs/2402.16242 ,  6444kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16281 (*cross-listing*)
Date: Mon, 26 Feb 2024 03:54:32 GMT   (2522kb,D)

Title: Towards Agile Robots: Intuitive Robot Position Speculation with Neural
  Networks
Authors: Yanlong Peng, Zhigang Wang, Yisheng Zhang, Shengmin Zhang, Ming Chen
Categories: cs.RO cs.AI
\\
  The robot position speculation, which determines where the chassis should
move, is one key step to control the mobile manipulators. The target position
must ensure the feasibility of chassis movement and manipulability, which is
guaranteed by randomized sampling and kinematic checking in traditional
methods. Addressing the demands of agile robotics, this paper proposes a robot
position speculation network(RPSN), a learning-based approach to enhance the
agility of mobile manipulators. The RPSN incorporates a differentiable inverse
kinematic algorithm and a neural network. Through end-to-end training, the RPSN
can speculate positions with a high success rate. We apply the RPSN to mobile
manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs).
Extensive experiments on various simulated environments and physical mobile
manipulators demonstrate that the probability of the initial position provided
by RPSN being the ideal position is 96.67%. From the kinematic constraint
perspective, it achieves 100% generation of the ideal position on average
within 1.28 attempts. Much lower than that of random sampling, 31.04. Moreover,
the proposed method demonstrates superior data efficiency over pure neural
network approaches. The proposed RPSN enables the robot to quickly infer
feasible target positions by intuition. This work moves towards building agile
robots that can act swiftly like humans.
\\ ( https://arxiv.org/abs/2402.16281 ,  2522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16294 (*cross-listing*)
Date: Mon, 26 Feb 2024 04:31:53 GMT   (3567kb,D)

Title: Decentralized Federated Unlearning on Blockchain
Authors: Xiao Liu, Mingyuan Li, Xu Wang, Guangsheng Yu, Wei Ni, Lixiang Li,
  Haipeng Peng, Renping Liu
Categories: cs.CR cs.AI
\\
  Blockchained Federated Learning (FL) has been gaining traction for ensuring
the integrity and traceability of FL processes. Blockchained FL involves
participants training models locally with their data and subsequently
publishing the models on the blockchain, forming a Directed Acyclic Graph
(DAG)-like inheritance structure that represents the model relationship.
However, this particular DAG-based structure presents challenges in updating
models with sensitive data, due to the complexity and overhead involved. To
address this, we propose Blockchained Federated Unlearning (BlockFUL), a
generic framework that redesigns the blockchain structure using Chameleon Hash
(CH) technology to mitigate the complexity of model updating, thereby reducing
the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL
supports various federated unlearning methods, ensuring the integrity and
traceability of model updates, whether conducted in parallel or serial. We
conduct a comprehensive study of two typical unlearning methods, gradient
ascent and re-training, demonstrating the efficient unlearning workflow in
these two categories with minimal CH and block update operations. Additionally,
we compare the computation and communication costs of these methods.
\\ ( https://arxiv.org/abs/2402.16294 ,  3567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16298 (*cross-listing*)
Date: Mon, 26 Feb 2024 04:41:04 GMT   (1053kb,D)

Title: MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer
Authors: Sushmita Sarker, Prithul Sarker, George Bebis, and Alireza Tavakkoli
Categories: cs.CV cs.AI
Comments: 4 pages, 2 figures
\\
  Traditional deep learning approaches for breast cancer classification has
predominantly concentrated on single-view analysis. In clinical practice,
however, radiologists concurrently examine all views within a mammography exam,
leveraging the inherent correlations in these views to effectively detect
tumors. Acknowledging the significance of multi-view analysis, some studies
have introduced methods that independently process mammogram views, either
through distinct convolutional branches or simple fusion strategies,
inadvertently leading to a loss of crucial inter-view correlations. In this
paper, we propose an innovative multi-view network exclusively based on
transformers to address challenges in mammographic image classification. Our
approach introduces a novel shifted window-based dynamic attention block,
facilitating the effective integration of multi-view information and promoting
the coherent transfer of this information between views at the spatial feature
map level. Furthermore, we conduct a comprehensive comparative analysis of the
performance and effectiveness of transformer-based models under diverse
settings, employing the CBIS-DDSM and Vin-Dr Mammo datasets. Our code is
publicly available at https://github.com/prithuls/MV-Swin-T
\\ ( https://arxiv.org/abs/2402.16298 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16321 (*cross-listing*)
Date: Mon, 26 Feb 2024 06:01:38 GMT   (13502kb,D)

Title: Self-Supervised Speech Quality Estimation and Enhancement Using Only
  Clean Speech
Authors: Szu-Wei Fu, Kuo-Hsuan Hung, Yu Tsao, Yu-Chiang Frank Wang
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: Published as a conference paper at ICLR 2024
\\
  Speech quality estimation has recently undergone a paradigm shift from
human-hearing expert designs to machine-learning models. However, current
models rely mainly on supervised learning, which is time-consuming and
expensive for label collection. To solve this problem, we propose VQScore, a
self-supervised metric for evaluating speech based on the quantization error of
a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE
relies on clean speech; hence, large quantization errors can be expected when
the speech is distorted. To further improve correlation with real quality
scores, domain knowledge of speech processing is incorporated into the model
design. We found that the vector quantization mechanism could also be used for
self-supervised speech enhancement (SE) model training. To improve the
robustness of the encoder for SE, a novel self-distillation mechanism combined
with adversarial training is introduced. In summary, the proposed speech
quality estimation method and enhancement models require only clean speech for
training without any label requirements. Experimental results show that the
proposed VQScore and enhancement model are competitive with supervised
baselines. The code will be released after publication.
\\ ( https://arxiv.org/abs/2402.16321 ,  13502kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16369 (*cross-listing*)
Date: Mon, 26 Feb 2024 07:47:12 GMT   (4968kb,D)

Title: Generative AI in Vision: A Survey on Models, Metrics and Applications
Authors: Gaurav Raut and Apoorv Singh
Categories: cs.CV cs.AI cs.LG
\\
  Generative AI models have revolutionized various fields by enabling the
creation of realistic and diverse data samples. Among these models, diffusion
models have emerged as a powerful approach for generating high-quality images,
text, and audio. This survey paper provides a comprehensive overview of
generative AI diffusion and legacy models, focusing on their underlying
techniques, applications across different domains, and their challenges. We
delve into the theoretical foundations of diffusion models, including concepts
such as denoising diffusion probabilistic models (DDPM) and score-based
generative modeling. Furthermore, we explore the diverse applications of these
models in text-to-image, image inpainting, and image super-resolution, along
with others, showcasing their potential in creative tasks and data
augmentation. By synthesizing existing research and highlighting critical
advancements in this field, this survey aims to provide researchers and
practitioners with a comprehensive understanding of generative AI diffusion and
legacy models and inspire future innovations in this exciting area of
artificial intelligence.
\\ ( https://arxiv.org/abs/2402.16369 ,  4968kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16380 (*cross-listing*)
Date: Mon, 26 Feb 2024 07:58:33 GMT   (1475kb,D)

Title: An Automated End-to-End Open-Source Software for High-Quality
  Text-to-Speech Dataset Generation
Authors: Ahmet Gunduz, Kamer Ali Yuksel, Kareem Darwish, Golara Javadi, Fabio
  Minazzi, Nicola Sobieski and Sebastien Bratieres
Categories: eess.AS cs.AI cs.CL cs.LG
Comments: 9 Pages, 6 Figures, 4 Tables, LREC-COLING 2024
\\
  Data availability is crucial for advancing artificial intelligence
applications, including voice-based technologies. As content creation,
particularly in social media, experiences increasing demand, translation and
text-to-speech (TTS) technologies have become essential tools. Notably, the
performance of these TTS technologies is highly dependent on the quality of the
training data, emphasizing the mutual dependence of data availability and
technological progress. This paper introduces an end-to-end tool to generate
high-quality datasets for text-to-speech (TTS) models to address this critical
need for high-quality data. The contributions of this work are manifold and
include: the integration of language-specific phoneme distribution into sample
selection, automation of the recording process, automated and human-in-the-loop
quality assurance of recordings, and processing of recordings to meet specified
formats. The proposed application aims to streamline the dataset creation
process for TTS models through these features, thereby facilitating
advancements in voice-based technologies.
\\ ( https://arxiv.org/abs/2402.16380 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16397 (*cross-listing*)
Date: Mon, 26 Feb 2024 08:41:14 GMT   (9106kb,D)

Title: Investigating Deep Watermark Security: An Adversarial Transferability
  Perspective
Authors: Biqing Qi, Junqi Gao, Yiang Luo, Jianxing Liu, Ligang Wu and Bowen
  Zhou
Categories: cs.CR cs.AI
Comments: 18 pages, 8 figures
\\
  The rise of generative neural networks has triggered an increased demand for
intellectual property (IP) protection in generated content. Deep watermarking
techniques, recognized for their flexibility in IP protection, have garnered
significant attention. However, the surge in adversarial transferable attacks
poses unprecedented challenges to the security of deep watermarking
techniques-an area currently lacking systematic investigation. This study fills
this gap by introducing two effective transferable attackers to assess the
vulnerability of deep watermarks against erasure and tampering risks.
Specifically, we initially define the concept of local sample density,
utilizing it to deduce theorems on the consistency of model outputs. Upon
discovering that perturbing samples towards high sample density regions (HSDR)
of the target class enhances targeted adversarial transferability, we propose
the Easy Sample Selection (ESS) mechanism and the Easy Sample Matching Attack
(ESMA) method. Additionally, we propose the Bottleneck Enhanced Mixup (BEM)
that integrates information bottleneck theory to reduce the generator's
dependence on irrelevant noise. Experiments show a significant enhancement in
the success rate of targeted transfer attacks for both ESMA and BEM-ESMA
methods. We further conduct a comprehensive evaluation using ESMA and BEM-ESMA
as measurements, considering model architecture and watermark encoding length,
and achieve some impressive findings.
\\ ( https://arxiv.org/abs/2402.16397 ,  9106kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16449 (*cross-listing*)
Date: Mon, 26 Feb 2024 09:53:37 GMT   (38332kb,D)

Title: Online Efficient Safety-Critical Control for Mobile Robots in Unknown
  Dynamic Multi-Obstacle Environments
Authors: Yu Zhang, Guangyao Tian, Long Wen, Xiangtong Yao, Liding Zhang,
  Zhenshan Bing, Wei He and Alois Knoll
Categories: cs.RO cs.AI
\\
  This paper proposes a LiDAR-based goal-seeking and exploration framework,
addressing the efficiency of online obstacle avoidance in unstructured
environments populated with static and moving obstacles. This framework
addresses two significant challenges associated with traditional dynamic
control barrier functions (D-CBFs): their online construction and the
diminished real-time performance caused by utilizing multiple D-CBFs. To tackle
the first challenge, the framework's perception component begins with
clustering point clouds via the DBSCAN algorithm, followed by encapsulating
these clusters with the minimum bounding ellipses (MBEs) algorithm to create
elliptical representations. By comparing the current state of MBEs with those
stored from previous moments, the differentiation between static and dynamic
obstacles is realized, and the Kalman filter is utilized to predict the
movements of the latter. Such analysis facilitates the D-CBF's online
construction for each MBE. To tackle the second challenge, we introduce buffer
zones, generating Type-II D-CBFs online for each identified obstacle. Utilizing
these buffer zones as activation areas substantially reduces the number of
D-CBFs that need to be activated. Upon entering these buffer zones, the system
prioritizes safety, autonomously navigating safe paths, and hence referred to
as the exploration mode. Exiting these buffer zones triggers the system's
transition to goal-seeking mode. We demonstrate that the system's states under
this framework achieve safety and asymptotic stabilization. Experimental
results in simulated and real-world environments have validated our framework's
capability, allowing a LiDAR-equipped mobile robot to efficiently and safely
reach the desired location within dynamic environments containing multiple
obstacles.
\\ ( https://arxiv.org/abs/2402.16449 ,  38332kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16455 (*cross-listing*)
Date: Mon, 26 Feb 2024 09:58:36 GMT   (2413kb,D)

Title: Performance Comparison of Surrogate-Assisted Evolutionary Algorithms on
  Computational Fluid Dynamics Problems
Authors: Jakub Kudela and Ladislav Dobrovsky
Categories: cs.NE cs.AI math.OC
\\
  Surrogate-assisted evolutionary algorithms (SAEAs) are recently among the
most widely studied methods for their capability to solve expensive real-world
optimization problems. However, the development of new methods and benchmarking
with other techniques still relies almost exclusively on artificially created
problems. In this paper, we use two real-world computational fluid dynamics
problems to compare the performance of eleven state-of-the-art single-objective
SAEAs. We analyze the performance by investigating the quality and robustness
of the obtained solutions and the convergence properties of the selected
methods. Our findings suggest that the more recently published methods, as well
as the techniques that utilize differential evolution as one of their
optimization mechanisms, perform significantly better than the other considered
methods.
\\ ( https://arxiv.org/abs/2402.16455 ,  2413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16486 (*cross-listing*)
Date: Mon, 26 Feb 2024 11:08:26 GMT   (25470kb,D)

Title: Intelligent Known and Novel Aircraft Recognition -- A Shift from
  Classification to Similarity Learning for Combat Identification
Authors: Ahmad Saeed, Haasha Bin Atif, Usman Habib and Mohsin Bilal
Categories: cs.CV cs.AI
\\
  Precise aircraft recognition in low-resolution remote sensing imagery is a
challenging yet crucial task in aviation, especially combat identification.
This research addresses this problem with a novel, scalable, and AI-driven
solution. The primary hurdle in combat identification in remote sensing imagery
is the accurate recognition of Novel/Unknown types of aircraft in addition to
Known types. Traditional methods, human expert-driven combat identification and
image classification, fall short in identifying Novel classes. Our methodology
employs similarity learning to discern features of a broad spectrum of military
and civilian aircraft. It discerns both Known and Novel aircraft types,
leveraging metric learning for the identification and supervised few-shot
learning for aircraft type classification. To counter the challenge of limited
low-resolution remote sensing data, we propose an end-to-end framework that
adapts to the diverse and versatile process of military aircraft recognition by
training a generalized embedder in fully supervised manner. Comparative
analysis with earlier aircraft image classification methods shows that our
approach is effective for aircraft image classification (F1-score Aircraft Type
of 0.861) and pioneering for quantifying the identification of Novel types
(F1-score Bipartitioning of 0.936). The proposed methodology effectively
addresses inherent challenges in remote sensing data, thereby setting new
standards in dataset quality. The research opens new avenues for domain experts
and demonstrates unique capabilities in distinguishing various aircraft types,
contributing to a more robust, domain-adapted potential for real-time aircraft
recognition.
\\ ( https://arxiv.org/abs/2402.16486 ,  25470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16542 (*cross-listing*)
Date: Mon, 26 Feb 2024 13:01:28 GMT   (21396kb,D)

Title: RoboGrind: Intuitive and Interactive Surface Treatment with Industrial
  Robots
Authors: Benjamin Alt, Florian St\"ockl, Silvan M\"uller, Christopher Braun,
  Julian Raible, Saad Alhasan, Oliver Rettig, Lukas Ringle, Darko Katic, Rainer
  J\"akel, Michael Beetz, Marcus Strand and Marco F. Huber
Categories: cs.RO cs.AI
Comments: 7 pages, 6 figures, accepted to the 2024 IEEE International
  Conference on Robotics and Automation (ICRA 2024)
MSC-class: 68T40
ACM-class: I.2.6; I.2.2; I.2.9
\\
  Surface treatment tasks such as grinding, sanding or polishing are a vital
step of the value chain in many industries, but are notoriously challenging to
automate. We present RoboGrind, an integrated system for the intuitive,
interactive automation of surface treatment tasks with industrial robots. It
combines a sophisticated 3D perception pipeline for surface scanning and
automatic defect identification, an interactive voice-controlled wizard system
for the AI-assisted bootstrapping and parameterization of robot programs, and
an automatic planning and execution pipeline for force-controlled robotic
surface treatment. RoboGrind is evaluated both under laboratory and real-world
conditions in the context of refabricating fiberglass wind turbine blades.
\\ ( https://arxiv.org/abs/2402.16542 ,  21396kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16546 (*cross-listing*)
Date: Mon, 26 Feb 2024 13:08:44 GMT   (1127kb,D)

Title: Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep
  Learning Projects
Authors: Han Wang, Sijia Yu, Chunyang Chen, Burak Turhan, Xiaodong Zhu
Categories: cs.SE cs.AI
Comments: ACM Transactions on Software Engineering and Methodology (2023)
DOI: 10.1145/3638245
\\
  Deep Learning (DL) models have rapidly advanced, focusing on achieving high
performance through testing model accuracy and robustness. However, it is
unclear whether DL projects, as software systems, are tested thoroughly or
functionally correct when there is a need to treat and test them like other
software systems. Therefore, we empirically study the unit tests in open-source
DL projects, analyzing 9,129 projects from GitHub. We find that: 1) unit tested
DL projects have positive correlation with the open-source project metrics and
have a higher acceptance rate of pull requests, 2) 68% of the sampled DL
projects are not unit tested at all, 3) the layer and utilities (utils) of DL
models have the most unit tests. Based on these findings and previous research
outcomes, we built a mapping taxonomy between unit tests and faults in DL
projects. We discuss the implications of our findings for developers and
researchers and highlight the need for unit testing in open-source DL projects
to ensure their reliability and stability. The study contributes to this
community by raising awareness of the importance of unit testing in DL projects
and encouraging further research in this area.
\\ ( https://arxiv.org/abs/2402.16546 ,  1127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16684 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:02:15 GMT   (2985kb)

Title: Automated Floodwater Depth Estimation Using Large Multimodal Model for
  Rapid Flood Mapping
Authors: Temitope Akinboyewa, Huan Ning, M. Naser Lessani, Zhenlong Li
Categories: cs.CV cs.AI
\\
  Information on the depth of floodwater is crucial for rapid mapping of areas
affected by floods. However, previous approaches for estimating floodwater
depth, including field surveys, remote sensing, and machine learning
techniques, can be time-consuming and resource-intensive. This paper presents
an automated and fast approach for estimating floodwater depth from on-site
flood photos. A pre-trained large multimodal model, GPT-4 Vision, was used
specifically for estimating floodwater. The input data were flooding photos
that contained referenced objects, such as street signs, cars, people, and
buildings. Using the heights of the common objects as references, the model
returned the floodwater depth as the output. Results show that the proposed
approach can rapidly provide a consistent and reliable estimation of floodwater
depth from flood photos. Such rapid estimation is transformative in flood
inundation mapping and assessing the severity of the flood in near-real time,
which is essential for effective flood response strategies.
\\ ( https://arxiv.org/abs/2402.16684 ,  2985kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16714 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:31:28 GMT   (738kb,D)

Title: Quantum linear algebra is all you need for Transformer architectures
Authors: Naixu Guo, Zhan Yu, Aman Agrawal, and Patrick Rebentrost
Categories: quant-ph cs.AI cs.CL
Comments: 26 pages, 2 figures, 2 tables, comments are welcome
\\
  Generative machine learning methods such as large-language models are
revolutionizing the creation of text and images. While these models are
powerful they also harness a large amount of computational resources. The
transformer is a key component in large language models that aims to generate a
suitable completion of a given partial sequence. In this work, we investigate
transformer architectures under the lens of fault-tolerant quantum computing.
The input model is one where pre-trained weight matrices are given as block
encodings to construct the query, key, and value matrices for the transformer.
As a first step, we show how to prepare a block encoding of the self-attention
matrix, with a row-wise application of the softmax function using the Hadamard
product. In addition, we combine quantum subroutines to construct important
building blocks in the transformer, the residual connection, layer
normalization, and the feed-forward neural network. Our subroutines prepare an
amplitude encoding of the transformer output, which can be measured to obtain a
prediction. We discuss the potential and challenges for obtaining a quantum
advantage.
\\ ( https://arxiv.org/abs/2402.16714 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16718 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:38:22 GMT   (492kb)

Title: An Overview of the Development of Stereotactic Body Radiation Therapy
Authors: Yanqi Zong, Zhengrong Cui, Luqi Lin, Sihao Wang, Yizhi Chen
Categories: physics.med-ph cs.AI
\\
  Stereotactic body radiation therapy (SBRT) refers to focusing high-energy
rays in three-dimensional space on the tumor lesion area, reducing the dose
received by surrounding normal tissues, which can effectively improve the local
control rate of the tumor and reduce the probability of complications. With the
comprehensive development of medical imaging, radiation biology and other
disciplines, this less-fractional, high-dose radiotherapy method has been
increasingly developed and applied in clinical practice. The background,
radio-biological basis, key technologies and main equipment of SBRT are
discussed, and its future development direction is prospected.
\\ ( https://arxiv.org/abs/2402.16718 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16749 (*cross-listing*)
Date: Mon, 26 Feb 2024 17:11:11 GMT   (45904kb,D)

Title: MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large
  Multimodal Model
Authors: Chunyi Li, Guo Lu, Donghui Feng, Haoning Wu, Zicheng Zhang, Xiaohong
  Liu, Guangtao Zhai, Weisi Lin, Wenjun Zhang
Categories: cs.CV cs.AI eess.IV
\\
  With the evolution of storage and communication protocols, ultra-low bitrate
image compression has become a highly demanding topic. However, existing
compression algorithms must sacrifice either consistency with the ground truth
or perceptual quality at ultra-low bitrate. In recent years, the rapid
development of the Large Multimodal Model (LMM) has made it possible to balance
these two goals. To solve this problem, this paper proposes a method called
Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder
for extracting the semantic information of the image, a map encoder to locate
the region corresponding to the semantic, an image encoder generates an
extremely compressed bitstream, and a decoder reconstructs the image based on
the above information. Experimental results show that our proposed MISC is
suitable for compressing both traditional Natural Sense Images (NSIs) and
emerging AI-Generated Images (AIGIs) content. It can achieve optimal
consistency and perception results while saving 50% bitrate, which has strong
potential applications in the next generation of storage and communication. The
code will be released on https://github.com/lcysyzxdxc/MISC.
\\ ( https://arxiv.org/abs/2402.16749 ,  45904kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16763 (*cross-listing*)
Date: Mon, 26 Feb 2024 17:30:34 GMT   (558kb,D)

Title: Order from chaos: Interplay of development and learning in recurrent
  networks of structured neurons
Authors: Laura Kriener, Kristin V\"olk, Ben von H\"unerbein, Federico Benitez,
  Walter Senn, Mihai A. Petrovici
Categories: q-bio.NC cs.AI cs.NE
Comments: 4 pages, 2 figures
\\
  Behavior can be described as a temporal sequence of actions driven by neural
activity. To learn complex sequential patterns in neural networks, memories of
past activities need to persist on significantly longer timescales than
relaxation times of single-neuron activity. While recurrent networks can
produce such long transients, training these networks in a biologically
plausible way is challenging. One approach has been reservoir computing, where
only weights from a recurrent network to a readout are learned. Other models
achieve learning of recurrent synaptic weights using propagated errors.
However, their biological plausibility typically suffers from issues with
locality, resource allocation or parameter scales and tuning. We suggest that
many of these issues can be alleviated by considering dendritic information
storage and computation. By applying a fully local, always-on plasticity rule
we are able to learn complex sequences in a recurrent network comprised of two
populations. Importantly, our model is resource-efficient, enabling the
learning of complex sequences using only a small number of neurons. We
demonstrate these features in a mock-up of birdsong learning, in which our
networks first learn a long, non-Markovian sequence that they can then
reproduce robustly despite external disturbances.
\\ ( https://arxiv.org/abs/2402.16763 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16790 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:03:50 GMT   (3429kb,D)

Title: Beyond Self-learned Attention: Mitigating Attention Bias in
  Transformer-based Models Using Attention Guidance
Authors: Jiri Gesi and Iftekhar Ahmed
Categories: cs.SE cs.AI
\\
  Transformer-based models have demonstrated considerable potential for source
code modeling tasks in software engineering. However, they are limited by their
dependence solely on automatic self-attention weight learning mechanisms.
Previous studies have shown that these models overemphasize delimiters added by
tokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential
information in the original input source code. To address this challenge, we
introduce SyntaGuid, a novel approach that utilizes the observation that
attention weights tend to be biased towards specific source code syntax tokens
and abstract syntax tree (AST) elements in fine-tuned language models when they
make correct predictions. SyntaGuid facilitates the guidance of
attention-weight learning, leading to improved model performance on various
software engineering tasks. We evaluate the effectiveness of SyntaGuid on
multiple tasks and demonstrate that it outperforms existing state-of-the-art
models in overall performance without requiring additional data. Experimental
result shows that SyntaGuid can improve overall performance up to 3.25% and fix
up to 28.3% wrong predictions. Our work represents the first attempt to guide
the attention of Transformer-based models towards critical source code tokens
during fine-tuning, highlighting the potential for enhancing Transformer-based
models in software engineering.
\\ ( https://arxiv.org/abs/2402.16790 ,  3429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16836 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:57:52 GMT   (36348kb,D)

Title: PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large
  Multimodal Models
Authors: Dingkun Guo, Yuqi Xiang, Shuqi Zhao, Xinghao Zhu, Masayoshi Tomizuka,
  Mingyu Ding, Wei Zhan
Categories: cs.RO cs.AI cs.CL cs.CV
\\
  Robotic grasping is a fundamental aspect of robot functionality, defining how
robots interact with objects. Despite substantial progress, its
generalizability to counter-intuitive or long-tailed scenarios, such as objects
with uncommon materials or shapes, remains a challenge. In contrast, humans can
easily apply their intuitive physics to grasp skillfully and change grasps
efficiently, even for objects they have never seen before.
  This work delves into infusing such physical commonsense reasoning into
robotic manipulation. We introduce PhyGrasp, a multimodal large model that
leverages inputs from two modalities: natural language and 3D point clouds,
seamlessly integrated through a bridge module. The language modality exhibits
robust reasoning capabilities concerning the impacts of diverse physical
properties on grasping, while the 3D modality comprehends object shapes and
parts. With these two capabilities, PhyGrasp is able to accurately assess the
physical properties of object parts and determine optimal grasping poses.
Additionally, the model's language comprehension enables human instruction
interpretation, generating grasping poses that align with human preferences. To
train PhyGrasp, we construct a dataset PhyPartNet with 195K object instances
with varying physical properties and human preferences, alongside their
corresponding language descriptions. Extensive experiments conducted in the
simulation and on the real robots demonstrate that PhyGrasp achieves
state-of-the-art performance, particularly in long-tailed cases, e.g., about
10% improvement in success rate over GraspNet. Project page:
https://sites.google.com/view/phygrasp
\\ ( https://arxiv.org/abs/2402.16836 ,  36348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16843 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:59:18 GMT   (7054kb,D)

Title: Multi-LoRA Composition for Image Generation
Authors: Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru
  Ouyang, Donghan Yu, Jiawei Han, Weizhu Chen
Categories: cs.CV cs.AI cs.CL cs.GR cs.LG
Comments: Project Website:
  https://maszhongming.github.io/Multi-LoRA-Composition/
\\
  Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models
for the accurate rendition of specific elements like distinct characters or
unique styles in generated images. Nonetheless, existing methods face
challenges in effectively composing multiple LoRAs, especially as the number of
LoRAs to be integrated grows, thus hindering the creation of complex imagery.
In this paper, we study multi-LoRA composition through a decoding-centric
perspective. We present two training-free methods: LoRA Switch, which
alternates between different LoRAs at each denoising step, and LoRA Composite,
which simultaneously incorporates all LoRAs to guide more cohesive image
synthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new
comprehensive testbed as part of this research. It features a diverse range of
LoRA categories with 480 composition sets. Utilizing an evaluation framework
based on GPT-4V, our findings demonstrate a clear improvement in performance
with our methods over the prevalent baseline, particularly evident when
increasing the number of LoRAs in a composition.
\\ ( https://arxiv.org/abs/2402.16843 ,  7054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16846 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:59:33 GMT   (17958kb,D)

Title: GROUNDHOG: Grounding Large Language Models to Holistic Segmentation
Authors: Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao,
  Joyce Chai
Categories: cs.CV cs.AI cs.CL
Comments: Website: https://groundhog-mllm.github.io/
\\
  Most multimodal large language models (MLLMs) learn language-to-object
grounding through causal language modeling where grounded objects are captured
by bounding boxes as sequences of location tokens. This paradigm lacks
pixel-level representations that are important for fine-grained visual
understanding and diagnosis. In this work, we introduce GROUNDHOG, an MLLM
developed by grounding Large Language Models to holistic segmentation.
GROUNDHOG incorporates a masked feature extractor and converts extracted
features into visual entity tokens for the MLLM backbone, which then connects
groundable phrases to unified grounding masks by retrieving and merging the
entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual
instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by
harvesting a collection of segmentation-grounded datasets with rich
annotations. Our experimental results show that GROUNDHOG achieves superior
performance on various language grounding tasks without task-specific
fine-tuning, and significantly reduces object hallucination. GROUNDHOG also
demonstrates better grounding towards complex forms of visual input and
provides easy-to-understand diagnosis in failure cases.
\\ ( https://arxiv.org/abs/2402.16846 ,  17958kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15539 (*cross-listing*)
Date: Fri, 23 Feb 2024 07:32:54 GMT   (42kb)

Title: Speech Corpus for Korean Children with Autism Spectrum Disorder: Towards
  Automatic Assessment Systems
Authors: Seonwoo Lee, Jihyun Mun, Sunhee Kim, Minhwa Chung
Categories: eess.AS cs.CL
Comments: 11 pages, Accepted for LREC-COLING 2024
\\
  Despite the growing demand for digital therapeutics for children with Autism
Spectrum Disorder (ASD), there is currently no speech corpus available for
Korean children with ASD. This paper introduces a speech corpus specifically
designed for Korean children with ASD, aiming to advance speech technologies
such as pronunciation and severity evaluation. Speech recordings from speech
and language evaluation sessions were transcribed, and annotated for
articulatory and linguistic characteristics. Three speech and language
pathologists rated these recordings for social communication severity (SCS) and
pronunciation proficiency (PP) using a 3-point Likert scale. The total number
of participants will be 300 for children with ASD and 50 for typically
developing (TD) children. The paper also analyzes acoustic and linguistic
features extracted from speech data collected and completed for annotation from
73 children with ASD and 9 TD children to investigate the characteristics of
children with ASD and identify significant features that correlate with the
clinical scores. The results reveal some speech and linguistic characteristics
in children with ASD that differ from those in TD children or another subgroup
of ASD categorized by clinical scores, demonstrating the potential for
developing automatic assessment systems for SCS and PP.
\\ ( https://arxiv.org/abs/2402.15539 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15571 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:14:09 GMT   (6881kb,D)

Title: Social Convos: Capturing Agendas and Emotions on Social Media
Authors: Ankita Bhaumik, Ning Sa, Gregorios Katsios and Tomek Strzalkowski
Categories: cs.SI cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Social media platforms are popular tools for disseminating targeted
information during major public events like elections or pandemics. Systematic
analysis of the message traffic can provide valuable insights into prevailing
opinions and social dynamics among different segments of the population. We are
specifically interested in influence spread, and in particular whether more
deliberate influence operations can be detected. However, filtering out the
essential messages with telltale influence indicators from the extensive and
often chaotic social media traffic is a major challenge. In this paper we
present a novel approach to extract influence indicators from messages
circulating among groups of users discussing particular topics. We build upon
the concept of a convo to identify influential authors who are actively
promoting some particular agenda around that topic within the group. We focus
on two influence indicators: the (control of) agenda and the use of emotional
language.
\\ ( https://arxiv.org/abs/2402.15571 ,  6881kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15579 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:34:47 GMT   (2814kb,D)

Title: CI w/o TN: Context Injection without Task Name for Procedure Planning
Authors: Xinjie Li
Categories: cs.CV cs.CL
\\
  This paper explores the challenge of procedure planning in instructional
videos, which involves creating goal-directed plans based on visual start and
goal observations from videos. Previous research has tackled this problem with
gradually weaker training supervision, from heavy intermediate visual
observations or language instructions to task class supervision. However, with
the advent of large language models, even given only the task name, these
models can produce a detailed plan. In this study, we propose a much weaker
setting without task name as supervision, which is not currently solvable by
existing large language models since they require good prompts with sufficient
information. Specifically, we hypothesize that previous intermediate
supervisions can serve as context information, and we use captions of visual
start and goal observations as a much cheaper form of supervision. This
approach greatly reduces the labeling cost since the captions can be easily
obtained by large pre-trained vision-language foundation models. Technically,
we apply BLIP to generate captions as supervision to train the context feature
with contrastive learning loss. Afterward, the context feature is fed into the
generator to aid in plan generation. Our experiments on two datasets with
varying scales demonstrate that our model can achieve comparable performance on
multiple metrics, which validates our hypothesis.
\\ ( https://arxiv.org/abs/2402.15579 ,  2814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15733 (*cross-listing*)
Date: Sat, 24 Feb 2024 06:05:15 GMT   (822kb)

Title: ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for
  Arabic Characters
Authors: Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim
Categories: cs.HC cs.CL cs.LG eess.SP
\\
  Brain-Computer-Interface (BCI) has been a hot research topic in the last few
years that could help paralyzed people in their lives. Several researches were
done to classify electroencephalography (EEG) signals automatically into
English characters and words. Arabic language is one of the most used languages
around the world. However, to the best of our knowledge, there is no dataset
for Arabic characters EEG signals. In this paper, we have created an EEG
dataset for Arabic characters and named it ArEEG_Chars. Moreover, several
experiments were done on ArEEG_Chars using deep learning. Best results were
achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be
public for researchers.
\\ ( https://arxiv.org/abs/2402.15733 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15810 (*cross-listing*)
Date: Sat, 24 Feb 2024 13:15:54 GMT   (912kb,D)

Title: OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining
Authors: Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu,
  Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan
  Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song,
  Xiaoyan Li, Yuxiao Dong, Jie Tang
Categories: cs.DL cs.CL cs.LG
Comments: 8 pages, 5 appendix pages
\\
  With the rapid proliferation of scientific literature, versatile academic
knowledge services increasingly rely on comprehensive academic graph mining.
Despite the availability of public academic graphs, benchmarks, and datasets,
these resources often fall short in multi-aspect and fine-grained annotations,
are constrained to specific task types and domains, or lack underlying real
academic graphs. In this paper, we present OAG-Bench, a comprehensive,
multi-aspect, and fine-grained human-curated benchmark based on the Open
Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines,
and 120+ experimental results to date. We propose new data annotation
strategies for certain tasks and offer a suite of data pre-processing codes,
algorithm implementations, and standardized evaluation protocols to facilitate
academic graph mining. Extensive experiments reveal that even advanced
algorithms like large language models (LLMs) encounter difficulties in
addressing key challenges in certain tasks, such as paper source tracing and
scholar profiling. We also introduce the Open Academic Graph Challenge
(OAG-Challenge) to encourage community input and sharing. We envisage that
OAG-Bench can serve as a common ground for the community to evaluate and
compare algorithms in academic graph mining, thereby accelerating algorithm
development and advancement in this field. OAG-Bench is accessible at
https://www.aminer.cn/data/.
\\ ( https://arxiv.org/abs/2402.15810 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15911 (*cross-listing*)
Date: Sat, 24 Feb 2024 21:27:13 GMT   (2537kb,D)

Title: PRP: Propagating Universal Perturbations to Attack Large Language Model
  Guard-Rails
Authors: Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran,
  Kassem Fawaz, Somesh Jha, Atul Prakash
Categories: cs.CR cs.CL
\\
  Large language models (LLMs) are typically aligned to be harmless to humans.
Unfortunately, recent work has shown that such models are susceptible to
automated jailbreak attacks that induce them to generate harmful content. More
recent LLMs often incorporate an additional layer of defense, a Guard Model,
which is a second LLM that is designed to check and moderate the output
response of the primary LLM. Our key contribution is to show a novel attack
strategy, PRP, that is successful against several open-source (e.g., Llama 2)
and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP
leverages a two step prefix-based attack that operates by (a) constructing a
universal adversarial prefix for the Guard Model, and (b) propagating this
prefix to the response. We find that this procedure is effective across
multiple threat models, including ones in which the adversary has no access to
the Guard Model at all. Our work suggests that further advances are required on
defenses and Guard Models before they can be considered effective.
\\ ( https://arxiv.org/abs/2402.15911 ,  2537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15985 (*cross-listing*)
Date: Sun, 25 Feb 2024 04:35:45 GMT   (2348kb,D)

Title: Phonetic and Lexical Discovery of a Canine Language using HuBERT
Authors: Xingyuan Li, Sinong Wang, Zeyu Xie, Mengyue Wu, Kenny Q. Zhu
Categories: cs.SD cs.CL cs.LG eess.AS
\\
  This paper delves into the pioneering exploration of potential communication
patterns within dog vocalizations and transcends traditional linguistic
analysis barriers, which heavily relies on human priori knowledge on limited
datasets to find sound units in dog vocalization. We present a self-supervised
approach with HuBERT, enabling the accurate classification of phoneme labels
and the identification of vocal patterns that suggest a rudimentary vocabulary
within dog vocalizations. Our findings indicate a significant acoustic
consistency in these identified canine vocabulary, covering the entirety of
observed dog vocalization sequences. We further develop a web-based dog
vocalization labeling system. This system can highlight phoneme n-grams,
present in the vocabulary, in the dog audio uploaded by users.
\\ ( https://arxiv.org/abs/2402.15985 ,  2348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16009 (*cross-listing*)
Date: Sun, 25 Feb 2024 06:56:43 GMT   (1596kb,D)

Title: PST-Bench: Tracing and Benchmarking the Source of Publications
Authors: Fanjin Zhang, Kun Cao, Yukuo Cen, Jifan Yu, Da Yin, Jie Tang
Categories: cs.DL cs.CL
Comments: 8 pages, 3 appendix pages
\\
  Tracing the source of research papers is a fundamental yet challenging task
for researchers. The billion-scale citation relations between papers hinder
researchers from understanding the evolution of science efficiently. To date,
there is still a lack of an accurate and scalable dataset constructed by
professional researchers to identify the direct source of their studied papers,
based on which automatic algorithms can be developed to expand the evolutionary
knowledge of science. In this paper, we study the problem of paper source
tracing (PST) and construct a high-quality and ever-increasing dataset
PST-Bench in computer science. Based on PST-Bench, we reveal several intriguing
discoveries, such as the differing evolution patterns across various topics. An
exploration of various methods underscores the hardness of PST-Bench,
pinpointing potential directions on this topic. The dataset and codes have been
available at https://github.com/THUDM/paper-source-trace.
\\ ( https://arxiv.org/abs/2402.16009 ,  1596kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16039 (*cross-listing*)
Date: Sun, 25 Feb 2024 09:34:22 GMT   (2716kb)

Title: Understanding Public Perceptions of AI Conversational Agents: A
  Cross-Cultural Analysis
Authors: Zihan Liu, Han Li, Anfan Chen, Renwen Zhang, Yi-Chieh Lee
Categories: cs.HC cs.CL
Comments: 17 pages, 4 figures, 7 tables
ACM-class: H.5.2
Journal-ref: CHI2024
DOI: 10.1145/3613904.3642840
\\
  Conversational Agents (CAs) have increasingly been integrated into everyday
life, sparking significant discussions on social media. While previous research
has examined public perceptions of AI in general, there is a notable lack in
research focused on CAs, with fewer investigations into cultural variations in
CA perceptions. To address this gap, this study used computational methods to
analyze about one million social media discussions surrounding CAs and compared
people's discourses and perceptions of CAs in the US and China. We find Chinese
participants tended to view CAs hedonically, perceived voice-based and
physically embodied CAs as warmer and more competent, and generally expressed
positive emotions. In contrast, US participants saw CAs more functionally, with
an ambivalent attitude. Warm perception was a key driver of positive emotions
toward CAs in both countries. We discussed practical implications for designing
contextually sensitive and user-centric CAs to resonate with various users'
preferences and needs.
\\ ( https://arxiv.org/abs/2402.16039 ,  2716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16050 (*cross-listing*)
Date: Sun, 25 Feb 2024 10:27:46 GMT   (26753kb,D)

Title: LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form
  Video-Text Understanding
Authors: Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao,
  Zilong Zheng
Categories: cs.CV cs.CL
\\
  Despite progress in video-language modeling, the computational challenge of
interpreting long-form videos in response to task-specific linguistic queries
persists, largely due to the complexity of high-dimensional video data and the
misalignment between language and visual cues over space and time. To tackle
this issue, we introduce a novel approach called Language-guided
Spatial-Temporal Prompt Learning (LSTP). This approach features two key
components: a Temporal Prompt Sampler (TPS) with optical flow prior that
leverages temporal information to efficiently extract relevant video content,
and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial
relationships between visual and textual elements. By harmonizing TPS and SPS
with a cohesive training strategy, our framework significantly enhances
computational efficiency, temporal understanding, and spatial-temporal
alignment. Empirical evaluations across two challenging tasks--video question
answering and temporal question grounding in videos--using a variety of
video-language pretrainings (VLPs) and large language models (LLMs) demonstrate
the superior performance, speed, and versatility of our proposed LSTP paradigm.
\\ ( https://arxiv.org/abs/2402.16050 ,  26753kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16187 (*cross-listing*)
Date: Sun, 25 Feb 2024 20:24:07 GMT   (1663kb,D)

Title: Attacking LLM Watermarks by Exploiting Their Strengths
Authors: Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith
Categories: cs.CR cs.CL cs.LG
\\
  Advances in generative models have made it possible for AI-generated text,
code, and images to mirror human-generated content in many applications.
Watermarking, a technique that aims to embed information in the output of a
model to verify its source, is useful for mitigating misuse of such
AI-generated content. However, existing watermarking schemes remain
surprisingly susceptible to attack. In particular, we show that desirable
properties shared by existing LLM watermarking systems such as quality
preservation, robustness, and public detection APIs can in turn make these
systems vulnerable to various attacks. We rigorously study potential attacks in
terms of common watermark design choices, and propose best practices and
defenses for mitigation -- establishing a set of practical guidelines for
embedding and detection of LLM watermarks.
\\ ( https://arxiv.org/abs/2402.16187 ,  1663kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16315 (*cross-listing*)
Date: Mon, 26 Feb 2024 05:43:51 GMT   (28563kb,D)

Title: Finer: Investigating and Enhancing Fine-Grained Visual Concept
  Recognition in Large Vision Language Models
Authors: Jeonghwan Kim and Heng Ji
Categories: cs.CV cs.CL
\\
  Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)
have imbued the models with the ability to generate high-level, image-grounded
explanations with ease. While such capability is largely attributed to the rich
world knowledge contained within the Large Language Models (LLMs), our work
reveals their shortcomings in fine-grained visual categorization (FGVC) across
six different benchmark settings. Most recent state-of-the-art LVLMs like
LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of
classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs
for LLaVA-1.5, but also struggle to generate an accurate explanation with
detailed attributes based on the concept that appears within an input image
despite their capability to generate holistic image-level descriptions.
In-depth analyses show that instruction-tuned LVLMs exhibit modality gap,
showing discrepancy when given textual and visual inputs that correspond to the
same concept, preventing the image modality from leveraging the rich parametric
knowledge within the LLMs. In an effort to further the community's endeavor in
this direction, we propose a multiple granularity attribute-centric evaluation
benchmark, Finer, which aims to establish a ground to evaluate LVLMs'
fine-grained visual comprehension ability and provide significantly improved
explainability.
\\ ( https://arxiv.org/abs/2402.16315 ,  28563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16333 (*cross-listing*)
Date: Mon, 26 Feb 2024 06:28:54 GMT   (7279kb,D)

Title: Unveiling the Truth and Facilitating Change: Towards Agent-based
  Large-scale Social Movement Simulation
Authors: Xinyi Mou, Zhongyu Wei, Xuanjing Huang
Categories: cs.CY cs.CL
\\
  Social media has emerged as a cornerstone of social movements, wielding
significant influence in driving societal change. Simulating the response of
the public and forecasting the potential impact has become increasingly
important. However, existing methods for simulating such phenomena encounter
challenges concerning their efficacy and efficiency in capturing the behaviors
of social movement participants. In this paper, we introduce a hybrid framework
for social media user simulation, wherein users are categorized into two types.
Core users are driven by Large Language Models, while numerous ordinary users
are modeled by deductive agent-based models. We further construct a
Twitter-like environment to replicate their response dynamics following trigger
events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for
evaluation and conduct comprehensive experiments across real-world datasets.
Experimental results demonstrate the effectiveness and flexibility of our
method.
\\ ( https://arxiv.org/abs/2402.16333 ,  7279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16539 (*cross-listing*)
Date: Mon, 26 Feb 2024 12:55:51 GMT   (2714kb)

Title: Integrating Large Language Models with Graphical Session-Based
  Recommendation
Authors: Naicheng Guo, Hongwei Cheng, Qianqiao Liang, Linxun Chen, Bing Han
Categories: cs.IR cs.CL cs.LG
\\
  With the rapid development of Large Language Models (LLMs), various
explorations have arisen to utilize LLMs capability of context understanding on
recommender systems. While pioneering strategies have primarily transformed
traditional recommendation tasks into challenges of natural language
generation, there has been a relative scarcity of exploration in the domain of
session-based recommendation (SBR) due to its specificity. SBR has been
primarily dominated by Graph Neural Networks, which have achieved many
successful outcomes due to their ability to capture both the implicit and
explicit relationships between adjacent behaviors. The structural nature of
graphs contrasts with the essence of natural language, posing a significant
adaptation gap for LLMs. In this paper, we introduce large language models with
graphical Session-Based recommendation, named LLMGR, an effective framework
that bridges the aforementioned gap by harmoniously integrating LLMs with Graph
Neural Networks (GNNs) for SBR tasks. This integration seeks to leverage the
complementary strengths of LLMs in natural language understanding and GNNs in
relational data processing, leading to a more powerful session-based
recommender system that can understand and recommend items within a session.
Moreover, to endow the LLM with the capability to empower SBR tasks, we design
a series of prompts for both auxiliary and major instruction tuning tasks.
These prompts are crafted to assist the LLM in understanding graph-structured
data and align textual information with nodes, effectively translating nuanced
user interactions into a format that can be understood and utilized by LLM
architectures. Extensive experiments on three real-world datasets demonstrate
that LLMGR outperforms several competitive baselines, indicating its
effectiveness in enhancing SBR tasks and its potential as a research direction
for future exploration.
\\ ( https://arxiv.org/abs/2402.16539 ,  2714kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16767 (*cross-listing*)
Date: Mon, 26 Feb 2024 17:35:44 GMT   (1539kb,D)

Title: CorpusBrain++: A Continual Generative Pre-Training Framework for
  Knowledge-Intensive Language Tasks
Authors: Jiafeng Guo, Changjiang Zhou, Ruqing Zhang, Jiangui Chen, Maarten de
  Rijke, Yixing Fan and Xueqi Cheng
Categories: cs.IR cs.CL
Comments: Submitted to ACM Transactions on Information Systems
\\
  Knowledge-intensive language tasks (KILTs) typically require retrieving
relevant documents from trustworthy corpora, e.g., Wikipedia, to produce
specific answers. Very recently, a pre-trained generative retrieval model for
KILTs, named CorpusBrain, was proposed and reached new state-of-the-art
retrieval performance. However, most existing research on KILTs, including
CorpusBrain, has predominantly focused on a static document collection,
overlooking the dynamic nature of real-world scenarios, where new documents are
continuously being incorporated into the source corpus. To address this gap, it
is crucial to explore the capability of retrieval models to effectively handle
the dynamic retrieval scenario inherent in KILTs.
  In this work, we first introduce the continual document learning (CDL) task
for KILTs and build a novel benchmark dataset named KILT++ based on the
original KILT dataset for evaluation. Then, we conduct a comprehensive study
over the use of pre-trained CorpusBrain on KILT++. Unlike the promising results
in the stationary scenario, CorpusBrain is prone to catastrophic forgetting in
the dynamic scenario, hence hampering the retrieval performance. To alleviate
this issue, we propose CorpusBrain++, a continual generative pre-training
framework. Empirical results demonstrate the significant effectiveness and
remarkable efficiency of CorpusBrain++ in comparison to both traditional and
generative IR methods.
\\ ( https://arxiv.org/abs/2402.16767 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16830 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:56:42 GMT   (97kb,D)

Title: SKILL: Similarity-aware Knowledge distILLation for Speech
  Self-Supervised Learning
Authors: Luca Zampierin, Ghouthi Boukli Hacene, Bac Nguyen, Mirco Ravanelli
Categories: eess.AS cs.CL cs.LG cs.SD
Comments: Accepted at the Self-supervision in Audio, Speech and Beyond (SASB)
  Workshop at ICASSP 2024
\\
  Self-supervised learning (SSL) has achieved remarkable success across various
speech-processing tasks. To enhance its efficiency, previous works often
leverage the use of compression techniques. A notable recent attempt is
DPHuBERT, which applies joint knowledge distillation (KD) and structured
pruning to learn a significantly smaller SSL model. In this paper, we
contribute to this research domain by introducing SKILL, a novel method that
conducts distillation across groups of layers instead of distilling individual
arbitrarily selected layers within the teacher network. The identification of
the layers to distill is achieved through a hierarchical clustering procedure
applied to layer similarity measures. Extensive experiments demonstrate that
our distilled version of WavLM Base+ not only outperforms DPHuBERT but also
achieves state-of-the-art results in the 30M parameters model class across
several SUPERB tasks.
\\ ( https://arxiv.org/abs/2402.16830 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15513 (*cross-listing*)
Date: Tue, 23 Jan 2024 16:49:54 GMT   (100kb,D)

Title: Investigating the Generalizability of Physiological Characteristics of
  Anxiety
Authors: Emily Zhou, Mohammad Soleymani, and Maja J. Matari\'c
Categories: cs.MM cs.LG eess.SP physics.med-ph
Journal-ref: 2023 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM), 2023, pp. 4848-4855
DOI: 10.1109/BIBM58861.2023.10385292
\\
  Recent works have demonstrated the effectiveness of machine learning (ML)
techniques in detecting anxiety and stress using physiological signals, but it
is unclear whether ML models are learning physiological features specific to
stress. To address this ambiguity, we evaluated the generalizability of
physiological features that have been shown to be correlated with anxiety and
stress to high-arousal emotions. Specifically, we examine features extracted
from electrocardiogram (ECG) and electrodermal (EDA) signals from the following
three datasets: Anxiety Phases Dataset (APD), Wearable Stress and Affect
Detection (WESAD), and the Continuously Annotated Signals of Emotion (CASE)
dataset. We aim to understand whether these features are specific to anxiety or
general to other high-arousal emotions through a statistical regression
analysis, in addition to a within-corpus, cross-corpus, and
leave-one-corpus-out cross-validation across instances of stress and arousal.
We used the following classifiers: Support Vector Machines, LightGBM, Random
Forest, XGBoost, and an ensemble of the aforementioned models. We found that
models trained on an arousal dataset perform relatively well on a previously
unseen stress dataset, and vice versa. Our experimental results suggest that
the evaluated models may be identifying emotional arousal instead of stress.
This work is the first cross-corpus evaluation across stress and arousal from
ECG and EDA signals, contributing new findings about the generalizability of
stress detection.
\\ ( https://arxiv.org/abs/2402.15513 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15516 (*cross-listing*)
Date: Fri, 9 Feb 2024 12:12:52 GMT   (135kb,D)

Title: GLA-Grad: A Griffin-Lim Extended Waveform Generation Diffusion Model
Authors: Haocheng Liu (IP Paris, LTCI, IDS, S2A), Teysir Baoueb (IP Paris,
  LTCI, IDS, S2A), Mathieu Fontaine (IP Paris, LTCI, IDS, S2A), Jonathan Le
  Roux (MERL), Gael Richard (IP Paris, LTCI, IDS, S2A)
Categories: cs.SD cs.LG eess.AS eess.SP
Comments: Accepted at ICASSP 2024
Journal-ref: IEEE International Conference on Acoustics, Speech and Signal
  Processing, Apr 2024, Seoul (Korea), South Korea
\\
  Diffusion models are receiving a growing interest for a variety of signal
generation tasks such as speech or music synthesis. WaveGrad, for example, is a
successful diffusion model that conditionally uses the mel spectrogram to guide
a diffusion process for the generation of high-fidelity audio. However, such
models face important challenges concerning the noise diffusion process for
training and inference, and they have difficulty generating high-quality speech
for speakers that were not seen during training. With the aim of minimizing the
conditioning error and increasing the efficiency of the noise diffusion
process, we propose in this paper a new scheme called GLA-Grad, which consists
in introducing a phase recovery algorithm such as the Griffin-Lim algorithm
(GLA) at each step of the regular diffusion process. Furthermore, it can be
directly applied to an already-trained waveform generation model, without
additional training or fine-tuning. We show that our algorithm outperforms
state-of-the-art diffusion models for speech generation, especially when
generating speech for a previously unseen target speaker.
\\ ( https://arxiv.org/abs/2402.15516 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15534 (*cross-listing*)
Date: Thu, 22 Feb 2024 20:51:37 GMT   (4137kb,D)

Title: DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in
  Chest X-Ray Studies
Authors: Abhieet Parida, Daniel Capellan-Martin, Sara Atito, Muhammad Awais,
  Maria J. Ledesma-Carbayo, Marius G. Linguraru, Syed Muhammad Anwar
Categories: eess.IV cs.CV cs.LG
\\
  Chest X-Ray (CXR) is a widely used clinical imaging modality and has a
pivotal role in the diagnosis and prognosis of various lung and heart related
conditions. Conventional automated clinical diagnostic tool design strategies
relying on radiology reads and supervised learning, entail the cumbersome
requirement of high quality annotated training data. To address this challenge,
self-supervised pre-training has proven to outperform supervised pre-training
in numerous downstream vision tasks, representing a significant breakthrough in
the field. However, medical imaging pre-training significantly differs from
pre-training with natural images (e.g., ImageNet) due to unique attributes of
clinical images. In this context, we introduce Diverse Concept Modeling
(DiCoM), a novel self-supervised training paradigm that leverages a student
teacher framework for learning diverse concepts and hence effective
representation of the CXR data. Hence, expanding beyond merely modeling a
single primary label within an image, instead, effectively harnessing the
information from all the concepts inherent in the CXR. The pre-trained model is
subsequently fine-tuned to address diverse domain-specific tasks. Our proposed
paradigm consistently demonstrates robust performance across multiple
downstream tasks on multiple datasets, highlighting the success and
generalizability of the pre-training strategy. To establish the efficacy of our
methods we analyze both the power of learned representations and the speed of
convergence (SoC) of our models. For diverse data and tasks, DiCoM is able to
achieve in most cases better results compared to other state-of-the-art
pre-training strategies. This when combined with the higher SoC and
generalization capabilities positions DiCoM to be established as a foundation
model for CXRs, a widely used imaging modality.
\\ ( https://arxiv.org/abs/2402.15534 ,  4137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15542 (*cross-listing*)
Date: Fri, 23 Feb 2024 10:36:22 GMT   (222kb,D)

Title: Streaming IoT Data and the Quantum Edge: A Classic/Quantum Machine
  Learning Use Case
Authors: Sabrina Herbst, Vincenzo De Maio, Ivona Brandic
Categories: cs.ET cs.DC cs.LG
\\
  With the advent of the Post-Moore era, the scientific community is faced with
the challenge of addressing the demands of current data-intensive machine
learning applications, which are the cornerstone of urgent analytics in
distributed computing. Quantum machine learning could be a solution for the
increasing demand of urgent analytics, providing potential theoretical speedups
and increased space efficiency. However, challenges such as (1) the encoding of
data from the classical to the quantum domain, (2) hyperparameter tuning, and
(3) the integration of quantum hardware into a distributed computing continuum
limit the adoption of quantum machine learning for urgent analytics. In this
work, we investigate the use of Edge computing for the integration of quantum
machine learning into a distributed computing continuum, identifying the main
challenges and possible solutions. Furthermore, exploring the data encoding and
hyperparameter tuning challenges, we present preliminary results for quantum
machine learning analytics on an IoT scenario.
\\ ( https://arxiv.org/abs/2402.15542 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15566 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:07:53 GMT   (5677kb)

Title: Closing the AI generalization gap by adjusting for dermatology condition
  distribution differences across clinical settings
Authors: Rajeev V. Rikhye, Aaron Loh, Grace Eunhae Hong, Preeti Singh, Margaret
  Ann Smith, Vijaytha Muralidharan, Doris Wong, Rory Sayres, Michelle Phung,
  Nicolas Betancourt, Bradley Fong, Rachna Sahasrabudhe, Khoban Nasim, Alec
  Eschholz, Basil Mustafa, Jan Freyberg, Terry Spitz, Yossi Matias, Greg S.
  Corrado, Katherine Chou, Dale R. Webster, Peggy Bui, Yuan Liu, Yun Liu,
  Justin Ko, Steven Lin
Categories: eess.IV cs.CV cs.LG
\\
  Recently, there has been great progress in the ability of artificial
intelligence (AI) algorithms to classify dermatological conditions from
clinical photographs. However, little is known about the robustness of these
algorithms in real-world settings where several factors can lead to a loss of
generalizability. Understanding and overcoming these limitations will permit
the development of generalizable AI that can aid in the diagnosis of skin
conditions across a variety of clinical settings. In this retrospective study,
we demonstrate that differences in skin condition distribution, rather than in
demographics or image capture mode are the main source of errors when an AI
algorithm is evaluated on data from a previously unseen source. We demonstrate
a series of steps to close this generalization gap, requiring progressively
more information about the new source, ranging from the condition distribution
to training data enriched for data less frequently seen during training. Our
results also suggest comparable performance from end-to-end fine tuning versus
fine tuning solely the classification layer on top of a frozen embedding model.
Our approach can inform the adaptation of AI algorithms to new settings, based
on the information and resources available.
\\ ( https://arxiv.org/abs/2402.15566 ,  5677kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15569 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:12:41 GMT   (4922kb,D)

Title: Toward Fully Self-Supervised Multi-Pitch Estimation
Authors: Frank Cwitkowitz and Zhiyao Duan
Categories: eess.AS cs.LG cs.SD
\\
  Multi-pitch estimation is a decades-long research problem involving the
detection of pitch activity associated with concurrent musical events within
multi-instrument mixtures. Supervised learning techniques have demonstrated
solid performance on more narrow characterizations of the task, but suffer from
limitations concerning the shortage of large-scale and diverse polyphonic music
datasets with multi-pitch annotations. We present a suite of self-supervised
learning objectives for multi-pitch estimation, which encourage the
concentration of support around harmonics, invariance to timbral
transformations, and equivariance to geometric transformations. These
objectives are sufficient to train an entirely convolutional autoencoder to
produce multi-pitch salience-grams directly, without any fine-tuning. Despite
training exclusively on a collection of synthetic single-note audio samples,
our fully self-supervised framework generalizes to polyphonic music mixtures,
and achieves performance comparable to supervised models trained on
conventional multi-pitch datasets.
\\ ( https://arxiv.org/abs/2402.15569 ,  4922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15583 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:43:01 GMT   (20805kb,D)

Title: Cohere3D: Exploiting Temporal Coherence for Unsupervised Representation
  Learning of Vision-based Autonomous Driving
Authors: Yichen Xie, Hongge Chen, Gregory P. Meyer, Yong Jae Lee, Eric M.
  Wolff, Masayoshi Tomizuka, Wei Zhan, Yuning Chai, Xin Huang
Categories: cs.CV cs.LG
\\
  Due to the lack of depth cues in images, multi-frame inputs are important for
the success of vision-based perception, prediction, and planning in autonomous
driving. Observations from different angles enable the recovery of 3D object
states from 2D image inputs if we can identify the same instance in different
input frames. However, the dynamic nature of autonomous driving scenes leads to
significant changes in the appearance and shape of each instance captured by
the camera at different time steps. To this end, we propose a novel contrastive
learning algorithm, Cohere3D, to learn coherent instance representations in a
long-term input sequence robust to the change in distance and perspective. The
learned representation aids in instance-level correspondence across multiple
input frames in downstream tasks. In the pretraining stage, the raw point
clouds from LiDAR sensors are utilized to construct the long-term temporal
correspondence for each instance, which serves as guidance for the extraction
of instance-level representation from the vision-based bird's eye-view (BEV)
feature map. Cohere3D encourages a consistent representation for the same
instance at different frames but distinguishes between representations of
different instances. We evaluate our algorithm by finetuning the pretrained
model on various downstream perception, prediction, and planning tasks. Results
show a notable improvement in both data efficiency and task performance.
\\ ( https://arxiv.org/abs/2402.15583 ,  20805kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15584 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:51:55 GMT   (896kb,D)

Title: State Space Models for Event Cameras
Authors: Nikola Zubi\'c, Mathias Gehrig, Davide Scaramuzza
Categories: cs.CV cs.LG
Comments: 18 pages, 5 figures, 6 tables
\\
  Today, state-of-the-art deep neural networks that process event-camera data
first convert a temporal window of events into dense, grid-like input
representations. As such, they exhibit poor generalizability when deployed at
higher inference frequencies (i.e., smaller temporal windows) than the ones
they were trained on. We address this challenge by introducing state-space
models (SSMs) with learnable timescale parameters to event-based vision. This
design adapts to varying frequencies without the need to retrain the network at
different frequencies. Additionally, we investigate two strategies to
counteract aliasing effects when deploying the model at higher frequencies. We
comprehensively evaluate our approach against existing methods based on RNN and
Transformer architectures across various benchmarks, including Gen1 and 1 Mpx
event camera datasets. Our results demonstrate that SSM-based models train 33%
faster and also exhibit minimal performance degradation when tested at higher
frequencies than the training input. Traditional RNN and Transformer models
exhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.31
mAP, highlighting the effectiveness of SSMs in event-based vision tasks.
\\ ( https://arxiv.org/abs/2402.15584 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15592 (*cross-listing*)
Date: Fri, 23 Feb 2024 20:19:06 GMT   (1655kb,D)

Title: Neural optimal controller for stochastic systems via pathwise HJB
  operator
Authors: Zhe Jiao, Xiaoyan Luo, Xinlei Yi
Categories: math.OC cs.LG
Comments: 20 pages
\\
  The aim of this work is to develop deep learning-based algorithms for
high-dimensional stochastic control problems based on physics-informed learning
and dynamic programming. Unlike classical deep learning-based methods relying
on a probabilistic representation of the solution to the
Hamilton--Jacobi--Bellman (HJB) equation, we introduce a pathwise operator
associated with the HJB equation so that we can define a problem of
physics-informed learning. According to whether the optimal control has an
explicit representation, two numerical methods are proposed to solve the
physics-informed learning problem. We provide an error analysis on how the
truncation, approximation and optimization errors affect the accuracy of these
methods. Numerical results on various applications are presented to illustrate
the performance of the proposed algorithms.
\\ ( https://arxiv.org/abs/2402.15592 ,  1655kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15602 (*cross-listing*)
Date: Fri, 23 Feb 2024 20:51:31 GMT   (91kb,D)

Title: Minimax Optimality of Score-based Diffusion Models: Beyond the Density
  Lower Bound Assumptions
Authors: Kaihong Zhang, Heqi Yin, Feng Liang, Jingbo Liu
Categories: math.ST cs.LG stat.ML stat.TH
\\
  We study the asymptotic error of score-based diffusion model sampling in
large-sample scenarios from a non-parametric statistics perspective. We show
that a kernel-based score estimator achieves an optimal mean square error of
$\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$
for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and
$d$ represent the sample size and the dimension, $t$ is bounded above and below
by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As
a consequence, this yields an $\widetilde{O}\left(n^{-1/2}
t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the
distribution of the sample generated by the diffusion model under a mere
sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric
family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early
stopping strategy, we obtain that the diffusion model is nearly (up to log
factors) minimax optimal. This removes the crucial lower bound assumption on
$p_0$ in previous proofs of the minimax optimality of the diffusion model for
nonparametric families.
\\ ( https://arxiv.org/abs/2402.15602 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15611 (*cross-listing*)
Date: Fri, 23 Feb 2024 21:21:16 GMT   (333kb,D)

Title: Data/moment-driven approaches for fast predictive control of collective
  dynamics
Authors: Giacomo Albi, Sara Bicego, Michael Herty, Yuyang Huang, Dante Kalise,
  Chiara Segala
Categories: math.OC cs.LG cs.MA
\\
  Feedback control synthesis for large-scale particle systems is reviewed in
the framework of model predictive control (MPC). The high-dimensional character
of collective dynamics hampers the performance of traditional MPC algorithms
based on fast online dynamic optimization at every time step. Two alternatives
to MPC are proposed. First, the use of supervised learning techniques for the
offline approximation of optimal feedback laws is discussed. Then, a procedure
based on sequential linearization of the dynamics based on macroscopic
quantities of the particle ensemble is reviewed. Both approaches circumvent the
online solution of optimal control problems enabling fast, real-time, feedback
synthesis for large-scale particle systems. Numerical experiments assess the
performance of the proposed algorithms.
\\ ( https://arxiv.org/abs/2402.15611 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15635 (*cross-listing*)
Date: Fri, 23 Feb 2024 22:36:07 GMT   (1171kb,D)

Title: Bagged Deep Image Prior for Recovering Images in the Presence of Speckle
  Noise
Authors: Xi Chen, Zhewen Hou, Christopher A. Metzler, Arian Maleki and Shirin
  Jalali
Categories: cs.IT cs.CV cs.LG eess.IV math.IT stat.AP stat.ML
\\
  We investigate both the theoretical and algorithmic aspects of
likelihood-based methods for recovering a complex-valued signal from multiple
sets of measurements, referred to as looks, affected by speckle
(multiplicative) noise. Our theoretical contributions include establishing the
first existing theoretical upper bound on the Mean Squared Error (MSE) of the
maximum likelihood estimator under the deep image prior hypothesis. Our
theoretical results capture the dependence of MSE upon the number of parameters
in the deep image prior, the number of looks, the signal dimension, and the
number of measurements per look. On the algorithmic side, we introduce the
concept of bagged Deep Image Priors (Bagged-DIP) and integrate them with
projected gradient descent. Furthermore, we show how employing Newton-Schulz
algorithm for calculating matrix inverses within the iterations of PGD reduces
the computational complexity of the algorithm. We will show that this method
achieves the state-of-the-art performance.
\\ ( https://arxiv.org/abs/2402.15635 ,  1171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15718 (*cross-listing*)
Date: Sat, 24 Feb 2024 04:57:59 GMT   (259kb,D)

Title: A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime
Authors: Jihao Long, Xiaojun Peng and Lei Wu
Categories: stat.ML cs.LG
\\
  In this paper, we conduct a comprehensive analysis of generalization
properties of Kernel Ridge Regression (KRR) in the noiseless regime, a scenario
crucial to scientific computing, where data are often generated via computer
simulations. We prove that KRR can attain the minimax optimal rate, which
depends on both the eigenvalue decay of the associated kernel and the relative
smoothness of target functions. Particularly, when the eigenvalue decays
exponentially fast, KRR achieves the spectral accuracy, i.e., a convergence
rate faster than any polynomial. Moreover, the numerical experiments well
corroborate our theoretical findings. Our proof leverages a novel extension of
the duality framework introduced by Chen et al. (2023), which could be useful
in analyzing kernel-based methods beyond the scope of this work.
\\ ( https://arxiv.org/abs/2402.15718 ,  259kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15781 (*cross-listing*)
Date: Sat, 24 Feb 2024 10:42:50 GMT   (69kb)

Title: Analysis of Off-Policy Multi-Step TD-Learning with Linear Function
  Approximation
Authors: Donghwan Lee
Categories: eess.SY cs.LG cs.SY
\\
  This paper analyzes multi-step TD-learning algorithms within the `deadly
triad' scenario, characterized by linear function approximation, off-policy
learning, and bootstrapping. In particular, we prove that n-step TD-learning
algorithms converge to a solution as the sampling horizon n increases
sufficiently. The paper is divided into two parts. In the first part, we
comprehensively examine the fundamental properties of their model-based
deterministic counterparts, including projected value iteration, gradient
descent algorithms, and the control theoretic approach, which can be viewed as
prototype deterministic algorithms whose analysis plays a pivotal role in
understanding and developing their model-free reinforcement learning
counterparts. In particular, we prove that these algorithms converge to
meaningful solutions when n is sufficiently large. Based on these findings, two
n-step TD-learning algorithms are proposed and analyzed, which can be seen as
the model-free reinforcement learning counterparts of the gradient and control
theoretic algorithms.
\\ ( https://arxiv.org/abs/2402.15781 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15819 (*cross-listing*)
Date: Sat, 24 Feb 2024 14:10:04 GMT   (6168kb,D)

Title: Debiased Model-based Interactive Recommendation
Authors: Zijian Li, Ruichu Cai, Haiqin Huang, Sili Zhang, Yuguang Yan, Zhifeng
  Hao, Zhenghua Dong
Categories: cs.IR cs.LG
\\
  Existing model-based interactive recommendation systems are trained by
querying a world model to capture the user preference, but learning the world
model from historical logged data will easily suffer from bias issues such as
popularity bias and sampling bias. This is why some debiased methods have been
proposed recently. However, two essential drawbacks still remain: 1) ignoring
the dynamics of the time-varying popularity results in a false reweighting of
items. 2) taking the unknown samples as negative samples in negative sampling
results in the sampling bias. To overcome these two drawbacks, we develop a
model called \textbf{i}dentifiable \textbf{D}ebiased \textbf{M}odel-based
\textbf{I}nteractive \textbf{R}ecommendation (\textbf{iDMIR} in short). In
iDMIR, for the first drawback, we devise a debiased causal world model based on
the causal mechanism of the time-varying recommendation generation process with
identification guarantees; for the second drawback, we devise a debiased
contrastive policy, which coincides with the debiased contrastive learning and
avoids sampling bias. Moreover, we demonstrate that the proposed method not
only outperforms several latest interactive recommendation algorithms but also
enjoys diverse recommendation performance.
\\ ( https://arxiv.org/abs/2402.15819 ,  6168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15893 (*cross-listing*)
Date: Sat, 24 Feb 2024 20:01:15 GMT   (3843kb,D)

Title: Concurrent Learning of Policy and Unknown Safety Constraints in
  Reinforcement Learning
Authors: Lunet Yifru and Ali Baheri
Categories: eess.SY cs.LG cs.SY
\\
  Reinforcement learning (RL) has revolutionized decision-making across a wide
range of domains over the past few decades. Yet, deploying RL policies in
real-world scenarios presents the crucial challenge of ensuring safety.
Traditional safe RL approaches have predominantly focused on incorporating
predefined safety constraints into the policy learning process. However, this
reliance on predefined safety constraints poses limitations in dynamic and
unpredictable real-world settings where such constraints may not be available
or sufficiently adaptable. Bridging this gap, we propose a novel approach that
concurrently learns a safe RL control policy and identifies the unknown safety
constraint parameters of a given environment. Initializing with a parametric
signal temporal logic (pSTL) safety specification and a small initial labeled
dataset, we frame the problem as a bilevel optimization task, intricately
integrating constrained policy optimization, using a Lagrangian-variant of the
twin delayed deep deterministic policy gradient (TD3) algorithm, with Bayesian
optimization for optimizing parameters for the given pSTL safety specification.
Through experimentation in comprehensive case studies, we validate the efficacy
of this approach across varying forms of environmental constraints,
consistently yielding safe RL policies with high returns. Furthermore, our
findings indicate successful learning of STL safety constraint parameters,
exhibiting a high degree of conformity with true environmental safety
constraints. The performance of our model closely mirrors that of an ideal
scenario that possesses complete prior knowledge of safety constraints,
demonstrating its proficiency in accurately identifying environmental safety
constraints and learning safe policies that adhere to those constraints.
\\ ( https://arxiv.org/abs/2402.15893 ,  3843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15939 (*cross-listing*)
Date: Sat, 24 Feb 2024 23:56:15 GMT   (5756kb)

Title: Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI
Authors: Zi Wang, Min Xiao, Yirong Zhou, Chengyan Wang, Naiming Wu, Yi Li,
  Yiwen Gong, Shufu Chang, Yinyin Chen, Liuhong Zhu, Jianjun Zhou, Congbo Cai,
  He Wang, Di Guo, Guang Yang, Xiaobo Qu
Categories: eess.IV cs.LG
Comments: 10 pages, 11 figures, 3 tables
\\
  Dynamic magnetic resonance imaging (MRI) plays an indispensable role in
cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled
but the image reconstruction poses a great challenge of high-dimensional
processing. This challenge leads to necessitate extensive training data in many
deep learning reconstruction methods. This work proposes a novel and efficient
approach, leveraging a dimension-reduced separable learning scheme that excels
even with highly limited training data. We further integrate it with
spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning
network (DeepSSL), which unrolls an iteration process of a reconstruction model
with both temporal low-rankness and spatial sparsity. Intermediate outputs are
visualized to provide insights into the network's behavior and enhance its
interpretability. Extensive results on cardiac cine datasets show that the
proposed DeepSSL is superior to the state-of-the-art methods visually and
quantitatively, while reducing the demand for training cases by up to 75%. And
its preliminary adaptability to cardiac patients has been verified through
experienced radiologists' and cardiologists' blind reader study. Additionally,
DeepSSL also benefits for achieving the downstream task of cardiac segmentation
with higher accuracy and shows robustness in prospective real-time cardiac MRI.
\\ ( https://arxiv.org/abs/2402.15939 ,  5756kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15941 (*cross-listing*)
Date: Sun, 25 Feb 2024 00:15:30 GMT   (1174kb,D)

Title: Implementing Recycling Methods for Linear Systems in Python with an
  Application to Multiple Objective Optimization
Authors: Ainara Garcia, Sihong Xie, Arielle Carr
Categories: math.NA cs.LG cs.NA
\\
  Sequences of linear systems arise in the predictor-corrector method when
computing the Pareto front for multi-objective optimization. Rather than
discarding information generated when solving one system, it may be
advantageous to recycle information for subsequent systems. To accomplish this,
we seek to reduce the overall cost of computation when solving linear systems
using common recycling methods. In this work, we assessed the performance of
recycling minimum residual (RMINRES) method along with a map between
coefficient matrices. For these methods to be fully integrated into the
software used in Enouen et al. (2022), there must be working version of each in
both Python and PyTorch. Herein, we discuss the challenges we encountered and
solutions undertaken (and some ongoing) when computing efficient Python
implementations of these recycling strategies. The goal of this project was to
implement RMINRES in Python and PyTorch and add it to the established Pareto
front code to reduce computational cost. Additionally, we wanted to implement
the sparse approximate maps code in Python and PyTorch, so that it can be
parallelized in future work.
\\ ( https://arxiv.org/abs/2402.15941 ,  1174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15988 (*cross-listing*)
Date: Sun, 25 Feb 2024 05:00:20 GMT   (89kb,D)

Title: Towards Fair Graph Anomaly Detection: Problem, New Datasets, and
  Evaluation
Authors: Neng Kai Nigel Neo, Yeon-Chang Lee, Yiqiao Jin, Sang-Wook Kim, Srijan
  Kumar
Categories: cs.SI cs.LG
Comments: Our code and datasets are available at
  https://github.com/nigelnnk/FairGAD
\\
  The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect
anomalous nodes in an input graph while ensuring fairness and avoiding biased
predictions against individuals from sensitive subgroups such as gender or
political leanings. Fairness in graphs is particularly crucial in anomaly
detection areas such as misinformation detection in search/ranking systems,
where decision outcomes can significantly affect individuals. However, the
current literature does not comprehensively discuss this problem, nor does it
provide realistic datasets that encompass actual graph structures, anomaly
labels, and sensitive attributes for research in FairGAD. To bridge this gap,
we introduce a formal definition of the FairGAD problem and present two novel
graph datasets constructed from the globally prominent social media platforms
Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges
associated with 9,000 and 47,000 nodes, respectively, and leverage political
leanings as sensitive attributes and misinformation spreaders as anomaly
labels. We demonstrate that our FairGAD datasets significantly differ from the
synthetic datasets used currently by the research community. These new datasets
offer significant values for FairGAD by providing realistic data that captures
the intricacies of social networks. Using our datasets, we investigate the
performance-fairness trade-off in eleven existing GAD and non-graph AD methods
on five state-of-the-art fairness methods, which sheds light on their
effectiveness and limitations in addressing the FairGAD problem.
\\ ( https://arxiv.org/abs/2402.15988 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15994 (*cross-listing*)
Date: Sun, 25 Feb 2024 05:23:57 GMT   (421kb)

Title: Optimizing Portfolio Management and Risk Assessment in Digital Assets
  Using Deep Learning for Predictive Analysis
Authors: Qishuo Cheng, Le Yang, Jiajian Zheng, Miao Tian, Duan Xin
Categories: q-fin.CP cs.CE cs.LG
Comments: 10 pages, 5 figures
\\
  Portfolio management issues have been extensively studied in the field of
artificial intelligence in recent years, but existing deep learning-based
quantitative trading methods have some areas where they could be improved.
First of all, the prediction mode of stocks is singular; often, only one
trading expert is trained by a model, and the trading decision is solely based
on the prediction results of the model. Secondly, the data source used by the
model is relatively simple, and only considers the data of the stock itself,
ignoring the impact of the whole market risk on the stock. In this paper, the
DQN algorithm is introduced into asset management portfolios in a novel and
straightforward way, and the performance greatly exceeds the benchmark, which
fully proves the effectiveness of the DRL algorithm in portfolio management.
This also inspires us to consider the complexity of financial problems, and the
use of algorithms should be fully combined with the problems to adapt. Finally,
in this paper, the strategy is implemented by selecting the assets and actions
with the largest Q value. Since different assets are trained separately as
environments, there may be a phenomenon of Q value drift among different assets
(different assets have different Q value distribution areas), which may easily
lead to incorrect asset selection. Consider adding constraints so that the Q
values of different assets share a Q value distribution to improve results.
\\ ( https://arxiv.org/abs/2402.15994 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15995 (*cross-listing*)
Date: Sun, 25 Feb 2024 05:26:35 GMT   (47kb)

Title: Improved Hardness Results for Learning Intersections of Halfspaces
Authors: Stefan Tiegel
Categories: cs.CC cs.LG math.ST stat.ML stat.TH
\\
  We show strong (and surprisingly simple) lower bounds for weakly learning
intersections of halfspaces in the improper setting. Strikingly little is known
about this problem. For instance, it is not even known if there is a
polynomial-time algorithm for learning the intersection of only two halfspaces.
On the other hand, lower bounds based on well-established assumptions (such as
approximating worst-case lattice problems or variants of Feige's 3SAT
hypothesis) are only known (or are implied by existing results) for the
intersection of super-logarithmically many halfspaces [KS09,KS06,DSS16]. With
intersections of fewer halfspaces being only ruled out under less standard
assumptions [DV21] (such as the existence of local pseudo-random generators
with large stretch). We significantly narrow this gap by showing that even
learning $\omega(\log \log N)$ halfspaces in dimension $N$ takes
super-polynomial time under standard assumptions on worst-case lattice problems
(namely that SVP and SIVP are hard to approximate within polynomial factors).
Further, we give unconditional hardness results in the statistical query
framework. Specifically, we show that for any $k$ (even constant), learning $k$
halfspaces in dimension $N$ requires accuracy $N^{-\Omega(k)}$, or
exponentially many queries -- in particular ruling out SQ algorithms with
polynomial accuracy for $\omega(1)$ halfspaces. To the best of our knowledge
this is the first unconditional hardness result for learning a super-constant
number of halfspaces.
  Our lower bounds are obtained in a unified way via a novel connection we make
between intersections of halfspaces and the so-called parallel pancakes
distribution [DKS17,BLPR19,BRST21] that has been at the heart of many lower
bound constructions in (robust) high-dimensional statistics in the past few
years.
\\ ( https://arxiv.org/abs/2402.15995 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15997 (*cross-listing*)
Date: Sun, 25 Feb 2024 05:45:36 GMT   (6201kb,D)

Title: Cieran: Designing Sequential Colormaps via In-Situ Active Preference
  Learning
Authors: Matt-Heun Hong, Zachary N. Sunberg, Danielle Albers Szafir
Categories: cs.HC cs.GR cs.LG
Comments: CHI 2024. 12 pages/9 figures
DOI: 10.1145/3613904.3642903
\\
  Quality colormaps can help communicate important data patterns. However,
finding an aesthetically pleasing colormap that looks "just right" for a given
scenario requires significant design and technical expertise. We introduce
Cieran, a tool that allows any data analyst to rapidly find quality colormaps
while designing charts within Jupyter Notebooks. Our system employs an active
preference learning paradigm to rank expert-designed colormaps and create new
ones from pairwise comparisons, allowing analysts who are novices in color
design to tailor colormaps to their data context. We accomplish this by
treating colormap design as a path planning problem through the CIELAB
colorspace with a context-specific reward model. In an evaluation with twelve
scientists, we found that Cieran effectively modeled user preferences to rank
colormaps and leveraged this model to create new quality designs. Our work
shows the potential of active preference learning for supporting efficient
visualization design optimization.
\\ ( https://arxiv.org/abs/2402.15997 ,  6201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16005 (*cross-listing*)
Date: Sun, 25 Feb 2024 06:39:15 GMT   (8084kb,D)

Title: Adversarial-Robust Transfer Learning for Medical Imaging via Domain
  Assimilation
Authors: Xiaohui Chen and Tie Luo
Categories: cs.CV cs.LG
Comments: 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining
  (PAKDD), May 2024, Taiwan
\\
  In the field of Medical Imaging, extensive research has been dedicated to
leveraging its potential in uncovering critical diagnostic features in
patients. Artificial Intelligence (AI)-driven medical diagnosis relies on
sophisticated machine learning and deep learning models to analyze, detect, and
identify diseases from medical images. Despite the remarkable performance of
these models, characterized by high accuracy, they grapple with trustworthiness
issues. The introduction of a subtle perturbation to the original image
empowers adversaries to manipulate the prediction output, redirecting it to
other targeted or untargeted classes. Furthermore, the scarcity of publicly
available medical images, constituting a bottleneck for reliable training, has
led contemporary algorithms to depend on pretrained models grounded on a large
set of natural images -- a practice referred to as transfer learning. However,
a significant {\em domain discrepancy} exists between natural and medical
images, which causes AI models resulting from transfer learning to exhibit
heightened {\em vulnerability} to adversarial attacks. This paper proposes a
{\em domain assimilation} approach that introduces texture and color adaptation
into transfer learning, followed by a texture preservation component to
suppress undesired distortion. We systematically analyze the performance of
transfer learning in the face of various adversarial attacks under different
data modalities, with the overarching goal of fortifying the model's robustness
and security in medical imaging tasks. The results demonstrate high
effectiveness in reducing attack efficacy, contributing toward more trustworthy
transfer learning in biomedical applications.
\\ ( https://arxiv.org/abs/2402.16005 ,  8084kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16008 (*cross-listing*)
Date: Sun, 25 Feb 2024 06:53:35 GMT   (6669kb,D)

Title: Unmasking Dementia Detection by Masking Input Gradients: A JSM Approach
  to Model Interpretability and Precision
Authors: Yasmine Mustafa and Tie Luo
Categories: cs.CV cs.LG
Comments: 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining
  (PAKDD), May 2024, Taiwan
\\
  The evolution of deep learning and artificial intelligence has significantly
reshaped technological landscapes. However, their effective application in
crucial sectors such as medicine demands more than just superior performance,
but trustworthiness as well. While interpretability plays a pivotal role,
existing explainable AI (XAI) approaches often do not reveal {\em Clever Hans}
behavior where a model makes (ungeneralizable) correct predictions using
spurious correlations or biases in data. Likewise, current post-hoc XAI methods
are susceptible to generating unjustified counterfactual examples. In this
paper, we approach XAI with an innovative {\em model debugging} methodology
realized through Jacobian Saliency Map (JSM). To cast the problem into a
concrete context, we employ Alzheimer's disease (AD) diagnosis as the use case,
motivated by its significant impact on human lives and the formidable challenge
in its early detection, stemming from the intricate nature of its progression.
We introduce an interpretable, multimodal model for AD classification over its
multi-stage progression, incorporating JSM as a modality-agnostic tool that
provides insights into volumetric changes indicative of brain abnormalities.
Our extensive evaluation including ablation study manifests the efficacy of
using JSM for model debugging and interpretation, while significantly enhancing
model accuracy as well.
\\ ( https://arxiv.org/abs/2402.16008 ,  6669kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16036 (*cross-listing*)
Date: Sun, 25 Feb 2024 09:28:20 GMT   (1005kb)

Title: Machine Learning-Based Vehicle Intention Trajectory Recognition and
  Prediction for Autonomous Driving
Authors: Hanyi Yu, Shuning Huo, Mengran Zhu, Yulu Gong, Yafei Xiang
Categories: cs.RO cs.CV cs.LG
\\
  In recent years, the expansion of internet technology and advancements in
automation have brought significant attention to autonomous driving technology.
Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have
progressively introduced products ranging from assisted-driving vehicles to
semi-autonomous vehicles. However, this period has also witnessed several
traffic safety incidents involving self-driving vehicles. For instance, in
March 2016, a Google self-driving car was involved in a minor collision with a
bus. At the time of the accident, the autonomous vehicle was attempting to
merge into the right lane but failed to dynamically respond to the real-time
environmental information during the lane change. It incorrectly assumed that
the approaching bus would slow down to avoid it, leading to a low-speed
collision with the bus. This incident highlights the current technological
shortcomings and safety concerns associated with autonomous lane-changing
behavior, despite the rapid advancements in autonomous driving technology.
Lane-changing is among the most common and hazardous behaviors in highway
driving, significantly impacting traffic safety and flow. Therefore,
lane-changing is crucial for traffic safety, and accurately predicting drivers'
lane change intentions can markedly enhance driving safety. This paper
introduces a deep learning-based prediction method for autonomous driving lane
change behavior, aiming to facilitate safe lane changes and thereby improve
road safety.
\\ ( https://arxiv.org/abs/2402.16036 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16059 (*cross-listing*)
Date: Sun, 25 Feb 2024 11:08:19 GMT   (608kb,D)

Title: Gradient-enhanced deep Gaussian processes for multifidelity modelling
Authors: Viv Bone, Chris van der Heide, Kieran Mackle, Ingo H.J. Jahn, Peter M.
  Dower, Chris Manzie
Categories: stat.ML cs.LG
\\
  Multifidelity models integrate data from multiple sources to produce a single
approximator for the underlying process. Dense low-fidelity samples are used to
reduce interpolation error, while sparse high-fidelity samples are used to
compensate for bias or noise in the low-fidelity samples. Deep Gaussian
processes (GPs) are attractive for multifidelity modelling as they are
non-parametric, robust to overfitting, perform well for small datasets, and,
critically, can capture nonlinear and input-dependent relationships between
data of different fidelities. Many datasets naturally contain gradient data,
especially when they are generated by computational models that are compatible
with automatic differentiation or have adjoint solutions. Principally, this
work extends deep GPs to incorporate gradient data. We demonstrate this method
on an analytical test problem and a realistic partial differential equation
problem, where we predict the aerodynamic coefficients of a hypersonic flight
vehicle over a range of flight conditions and geometries. In both examples, the
gradient-enhanced deep GP outperforms a gradient-enhanced linear GP model and
their non-gradient-enhanced counterparts.
\\ ( https://arxiv.org/abs/2402.16059 ,  608kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16090 (*cross-listing*)
Date: Sun, 25 Feb 2024 13:37:36 GMT   (36760kb,D)

Title: Key Design Choices in Source-Free Unsupervised Domain Adaptation: An
  In-depth Empirical Analysis
Authors: Andrea Maracani, Raffaello Camoriano, Elisa Maiettini, Davide Talon,
  Lorenzo Rosasco and Lorenzo Natale
Categories: cs.CV cs.LG
\\
  This study provides a comprehensive benchmark framework for Source-Free
Unsupervised Domain Adaptation (SF-UDA) in image classification, aiming to
achieve a rigorous empirical understanding of the complex relationships between
multiple key design factors in SF-UDA methods. The study empirically examines a
diverse set of SF-UDA techniques, assessing their consistency across datasets,
sensitivity to specific hyperparameters, and applicability across different
families of backbone architectures. Moreover, it exhaustively evaluates
pre-training datasets and strategies, particularly focusing on both supervised
and self-supervised methods, as well as the impact of fine-tuning on the source
domain. Our analysis also highlights gaps in existing benchmark practices,
guiding SF-UDA research towards more effective and general approaches. It
emphasizes the importance of backbone architecture and pre-training dataset
selection on SF-UDA performance, serving as an essential reference and
providing key insights. Lastly, we release the source code of our experimental
framework. This facilitates the construction, training, and testing of SF-UDA
methods, enabling systematic large-scale experimental analysis and supporting
further research efforts in this field.
\\ ( https://arxiv.org/abs/2402.16090 ,  36760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16158 (*cross-listing*)
Date: Sun, 25 Feb 2024 17:37:53 GMT   (5072kb,D)

Title: Distribution-Free Fair Federated Learning with Small Samples
Authors: Qichuan Yin, Junzhou Huang, Huaxiu Yao, Linjun Zhang
Categories: stat.ML cs.CY cs.LG
\\
  As federated learning gains increasing importance in real-world applications
due to its capacity for decentralized data training, addressing fairness
concerns across demographic groups becomes critically important. However, most
existing machine learning algorithms for ensuring fairness are designed for
centralized data environments and generally require large-sample and
distributional assumptions, underscoring the urgent need for fairness
techniques adapted for decentralized and heterogeneous systems with
finite-sample and distribution-free guarantees. To address this issue, this
paper introduces FedFaiREE, a post-processing algorithm developed specifically
for distribution-free fair learning in decentralized settings with small
samples. Our approach accounts for unique challenges in decentralized
environments, such as client heterogeneity, communication costs, and small
sample sizes. We provide rigorous theoretical guarantees for both fairness and
accuracy, and our experimental results further provide robust empirical
validation for our proposed method.
\\ ( https://arxiv.org/abs/2402.16158 ,  5072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16197 (*cross-listing*)
Date: Sun, 25 Feb 2024 20:43:55 GMT   (751kb)

Title: Language Models for Code Completion: A Practical Evaluation
Authors: Maliheh Izadi, Jonathan Katzy, Tim van Dam, Marc Otten, Razvan Mihai
  Popescu, Arie van Deursen
Categories: cs.SE cs.LG cs.PL
Comments: To be published in the proceedings of the 46th IEEE/ACM International
  Conference on Software Engineering (ICSE 2024)
\\
  Transformer-based language models for automatic code completion have shown
great promise so far, yet the evaluation of these models rarely uses real data.
This study provides both quantitative and qualitative assessments of three
public code language models when completing real-world code. We first developed
an open-source IDE extension, Code4Me, for the online evaluation of the models.
We collected real auto-completion usage data for over a year from more than
1200 users, resulting in over 600K valid completions. These models were then
evaluated using six standard metrics across twelve programming languages. Next,
we conducted a qualitative study of 1690 real-world completion requests to
identify the reasons behind the poor model performance. A comparative analysis
of the models' performance in online and offline settings was also performed,
using benchmark synthetic datasets and two masking strategies. Our findings
suggest that while developers utilize code completion across various languages,
the best results are achieved for mainstream languages such as Python and Java.
InCoder outperformed the other models across all programming languages,
highlighting the significance of training data and objectives. Our study also
revealed that offline evaluations do not accurately reflect real-world
scenarios. Upon qualitative analysis of the model's predictions, we found that
66.3% of failures were due to the models' limitations, 24.4% occurred due to
inappropriate model usage in a development context, and 9.3% were valid
requests that developers overwrote. Given these findings, we propose several
strategies to overcome the current limitations. These include refining training
objectives, improving resilience to typographical errors, adopting hybrid
approaches, and enhancing implementations and usability.
\\ ( https://arxiv.org/abs/2402.16197 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16285 (*cross-listing*)
Date: Mon, 26 Feb 2024 04:06:05 GMT   (1975kb,D)

Title: A Comparison of Deep Learning Models for Proton Background Rejection
  with the AMS Electromagnetic Calorimeter
Authors: Raheem Karim Hashmani, Emre Akba\c{s}, Melahat Bilge Demirk\"oz
Categories: hep-ex cs.LG
Comments: 19 pages, 11 Figures
\\
  The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector
onboard the International Space Station containing six different subdetectors.
The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are
used to separate electrons/positrons from the abundant cosmic-ray proton
background.
  The positron flux measured in space by AMS falls with a power law which
unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several
theoretical models try to explain these phenomena, and a purer measurement of
positrons at higher energies is needed to help test them. The currently used
methods to reject the proton background at high energies involve extrapolating
shower features from the ECAL to use as inputs for boosted decision tree and
likelihood classifiers. We present a new approach for particle identification
with the AMS ECAL using deep learning (DL). By taking the energy deposition
within all the ECAL cells as an input and treating them as pixels in an
image-like format, we train an MLP, a CNN, and multiple ResNets and
Convolutional vision Transformers (CvTs) as shower classifiers.
  Proton rejection performance is evaluated using Monte Carlo (MC) events and
ISS data separately. For MC, using events with a reconstructed energy between
0.2 - 2 TeV, at 90% electron accuracy, the proton rejection power of our CvT
model is more than 5 times that of the other DL models. Similarly, for ISS data
with a reconstructed energy between 50 - 70 GeV, the proton rejection power of
our CvT model is more than 2.5 times that of the other DL models.
\\ ( https://arxiv.org/abs/2402.16285 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16299 (*cross-listing*)
Date: Mon, 26 Feb 2024 04:43:44 GMT   (233kb,D)

Title: Against Filter Bubbles: Diversified Music Recommendation via Weighted
  Hypergraph Embedding Learning
Authors: Chaoguang Luo, Liuying Wen, Yong Qin, Liangwei Yang, Zhineng Hu,
  Philip S. Yu
Categories: cs.IR cs.LG
\\
  Recommender systems serve a dual purpose for users: sifting out inappropriate
or mismatched information while accurately identifying items that align with
their preferences. Numerous recommendation algorithms are designed to provide
users with a personalized array of information tailored to their preferences.
Nevertheless, excessive personalization can confine users within a "filter
bubble". Consequently, achieving the right balance between accuracy and
diversity in recommendations is a pressing concern. To address this challenge,
exemplified by music recommendation, we introduce the Diversified Weighted
Hypergraph music Recommendation algorithm (DWHRec). In the DWHRec algorithm,
the initial connections between users and listened tracks are represented by a
weighted hypergraph. Simultaneously, associations between artists, albums and
tags with tracks are also appended to the hypergraph. To explore users' latent
preferences, a hypergraph-based random walk embedding method is applied to the
constructed hypergraph. In our investigation, accuracy is gauged by the
alignment between the user and the track, whereas the array of recommended
track types measures diversity. We rigorously compared DWHRec against seven
state-of-the-art recommendation algorithms using two real-world music datasets.
The experimental results validate DWHRec as a solution that adeptly harmonizes
accuracy and diversity, delivering a more enriched musical experience. Beyond
music recommendation, DWHRec can be extended to cater to other scenarios with
similar data structures.
\\ ( https://arxiv.org/abs/2402.16299 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16326 (*cross-listing*)
Date: Mon, 26 Feb 2024 06:20:28 GMT   (10841kb,D)

Title: A Provably Accurate Randomized Sampling Algorithm for Logistic
  Regression
Authors: Agniva Chowdhury, Pradeep Ramuhalli
Categories: stat.ML cs.DS cs.LG
Comments: To appear in the proceedings of AAAI 2024
\\
  In statistics and machine learning, logistic regression is a widely-used
supervised learning technique primarily employed for binary classification
tasks. When the number of observations greatly exceeds the number of predictor
variables, we present a simple, randomized sampling-based algorithm for
logistic regression problem that guarantees high-quality approximations to both
the estimated probabilities and the overall discrepancy of the model. Our
analysis builds upon two simple structural conditions that boil down to
randomized matrix multiplication, a fundamental and well-understood primitive
of randomized numerical linear algebra. We analyze the properties of estimated
probabilities of logistic regression when leverage scores are used to sample
observations, and prove that accurate approximations can be achieved with a
sample whose size is much smaller than the total number of observations. To
further validate our theoretical findings, we conduct comprehensive empirical
evaluations. Overall, our work sheds light on the potential of using randomized
sampling approaches to efficiently approximate the estimated probabilities in
logistic regression, offering a practical and computationally efficient
solution for large-scale datasets.
\\ ( https://arxiv.org/abs/2402.16326 ,  10841kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16353 (*cross-listing*)
Date: Mon, 26 Feb 2024 07:18:57 GMT   (48kb)

Title: An optimal tradeoff between entanglement and copy complexity for state
  tomography
Authors: Sitan Chen, Jerry Li, Allen Liu
Categories: quant-ph cs.DS cs.IT cs.LG math.IT
Comments: To appear at STOC 2024. Abstract shortened to meet arXiv requirement.
  36 pages, comments welcome
\\
  There has been significant interest in understanding how practical
constraints on contemporary quantum devices impact the complexity of quantum
learning. For the classic question of tomography, recent work tightly
characterized the copy complexity for any protocol that can only measure one
copy of the unknown state at a time, showing it is polynomially worse than if
one can make fully-entangled measurements. While we now have a fairly complete
picture of the rates for such tasks in the near-term and fault-tolerant
regimes, it remains poorly understood what the landscape in between looks like.
  In this work, we study tomography in the natural setting where one can make
measurements of $t$ copies at a time. For sufficiently small $\epsilon$, we
show that for any $t \le d^2$,
$\widetilde{\Theta}(\frac{d^3}{\sqrt{t}\epsilon^2})$ copies are necessary and
sufficient to learn an unknown $d$-dimensional state $\rho$ to trace distance
$\epsilon$. This gives a smooth and optimal interpolation between the known
rates for single-copy and fully-entangled measurements.
  To our knowledge, this is the first smooth entanglement-copy tradeoff known
for any quantum learning task, and for tomography, no intermediate point on
this curve was known, even at $t = 2$. An important obstacle is that unlike the
optimal single-copy protocol, the optimal fully-entangled protocol is
inherently biased and thus precludes naive batching approaches. Instead, we
devise a novel two-stage procedure that uses Keyl's algorithm to refine a crude
estimate for $\rho$ based on single-copy measurements. A key insight is to use
Schur-Weyl sampling not to estimate the spectrum of $\rho$, but to estimate the
deviation of $\rho$ from the maximally mixed state. When $\rho$ is far from the
maximally mixed state, we devise a novel quantum splitting procedure that
reduces to the case where $\rho$ is close to maximally mixed.
\\ ( https://arxiv.org/abs/2402.16353 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16388 (*cross-listing*)
Date: Mon, 26 Feb 2024 08:22:40 GMT   (396kb,D)

Title: Uncertainty Quantification in Anomaly Detection with Cross-Conformal
  $p$-Values
Authors: Oliver Hennh\"ofer and Christine Preisach
Categories: stat.ML cs.LG
Comments: 15 pages, 1 figure, 3 tables
\\
  Given the growing significance of reliable, trustworthy, and explainable
machine learning, the requirement of uncertainty quantification for anomaly
detection systems has become increasingly important. In this context,
effectively controlling Type I error rates ($\alpha$) without compromising the
statistical power ($1-\beta$) of these systems can build trust and reduce costs
related to false discoveries, particularly when follow-up procedures are
expensive. Leveraging the principles of conformal prediction emerges as a
promising approach for providing respective statistical guarantees by
calibrating a model's uncertainty. This work introduces a novel framework for
anomaly detection, termed cross-conformal anomaly detection, building upon
well-known cross-conformal methods designed for prediction tasks. With that, it
addresses a natural research gap by extending previous works in the context of
inductive conformal anomaly detection, relying on the split-conformal approach
for model calibration. Drawing on insights from conformal prediction, we
demonstrate that the derived methods for calculating cross-conformal $p$-values
strike a practical compromise between statistical efficiency (full-conformal)
and computational efficiency (split-conformal) for uncertainty-quantified
anomaly detection on benchmark datasets.
\\ ( https://arxiv.org/abs/2402.16388 ,  396kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16408 (*cross-listing*)
Date: Mon, 26 Feb 2024 09:04:07 GMT   (1917kb,D)

Title: Stable Training of Normalizing Flows for High-dimensional Variational
  Inference
Authors: Daniel Andrade
Categories: stat.ML cs.LG
\\
  Variational inference with normalizing flows (NFs) is an increasingly popular
alternative to MCMC methods. In particular, NFs based on coupling layers (Real
NVPs) are frequently used due to their good empirical performance. In theory,
increasing the depth of normalizing flows should lead to more accurate
posterior approximations. However, in practice, training deep normalizing flows
for approximating high-dimensional posterior distributions is often infeasible
due to the high variance of the stochastic gradients. In this work, we show
that previous methods for stabilizing the variance of stochastic gradient
descent can be insufficient to achieve stable training of Real NVPs. As the
source of the problem, we identify that, during training, samples often exhibit
unusual high values. As a remedy, we propose a combination of two methods: (1)
soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log
transformation of the samples. We evaluate these and other previously proposed
modification on several challenging target distributions, including a
high-dimensional horseshoe logistic regression model. Our experiments show that
with our modifications, stable training of Real NVPs for posteriors with
several thousand dimensions is possible, allowing for more accurate marginal
likelihood estimation via importance sampling. Moreover, we evaluate several
common training techniques and architecture choices and provide practical
advise for training NFs for high-dimensional variational inference.
\\ ( https://arxiv.org/abs/2402.16408 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16517 (*cross-listing*)
Date: Mon, 26 Feb 2024 11:58:02 GMT   (18942kb,D)

Title: Discovering Artificial Viscosity Models for Discontinuous Galerkin
  Approximation of Conservation Laws using Physics-Informed Machine Learning
Authors: Matteo Caldana, Paola F. Antonietti, Luca Dede'
Categories: math.NA cs.LG cs.NA
MSC-class: 35L65, 65M60, 68T01
\\
  Finite element-based high-order solvers of conservation laws offer large
accuracy but face challenges near discontinuities due to the Gibbs phenomenon.
Artificial viscosity is a popular and effective solution to this problem based
on physical insight. In this work, we present a physics-informed machine
learning algorithm to automate the discovery of artificial viscosity models in
a non-supervised paradigm. The algorithm is inspired by reinforcement learning
and trains a neural network acting cell-by-cell (the viscosity model) by
minimizing a loss defined as the difference with respect to a reference
solution thanks to automatic differentiation. This enables a dataset-free
training procedure. We prove that the algorithm is effective by integrating it
into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase
several numerical tests on scalar and vectorial problems, such as Burgers' and
Euler's equations in one and two dimensions. Results demonstrate that the
proposed approach trains a model that is able to outperform classical viscosity
models. Moreover, we show that the learnt artificial viscosity model is able to
generalize across different problems and parameters.
\\ ( https://arxiv.org/abs/2402.16517 ,  18942kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16543 (*cross-listing*)
Date: Mon, 26 Feb 2024 13:01:45 GMT   (3991kb,D)

Title: Model-based deep reinforcement learning for accelerated learning from
  flow simulations
Authors: Andre Weiner, Janis Geise
Categories: physics.flu-dyn cs.CE cs.LG
\\
  In recent years, deep reinforcement learning has emerged as a technique to
solve closed-loop flow control problems. Employing simulation-based
environments in reinforcement learning enables a priori end-to-end optimization
of the control system, provides a virtual testbed for safety-critical control
applications, and allows to gain a deep understanding of the control
mechanisms. While reinforcement learning has been applied successfully in a
number of rather simple flow control benchmarks, a major bottleneck toward
real-world applications is the high computational cost and turnaround time of
flow simulations. In this contribution, we demonstrate the benefits of
model-based reinforcement learning for flow control applications. Specifically,
we optimize the policy by alternating between trajectories sampled from flow
simulations and trajectories sampled from an ensemble of environment models.
The model-based learning reduces the overall training time by up to $85\%$ for
the fluidic pinball test case. Even larger savings are expected for more
demanding flow simulations.
\\ ( https://arxiv.org/abs/2402.16543 ,  3991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16569 (*cross-listing*)
Date: Mon, 26 Feb 2024 13:47:32 GMT   (2864kb,D)

Title: Pretrained Visual Uncertainties
Authors: Michael Kirchhof and Mark Collier and Seong Joon Oh and Enkelejda
  Kasneci
Categories: cs.CV cs.LG
\\
  Accurate uncertainty estimation is vital to trustworthy machine learning, yet
uncertainties typically have to be learned for each task anew. This work
introduces the first pretrained uncertainty modules for vision models. Similar
to standard pretraining this enables the zero-shot transfer of uncertainties
learned on a large pretraining dataset to specialized downstream datasets. We
enable our large-scale pretraining on ImageNet-21k by solving a gradient
conflict in previous uncertainty modules and accelerating the training by up to
180x. We find that the pretrained uncertainties generalize to unseen datasets.
In scrutinizing the learned uncertainties, we find that they capture aleatoric
uncertainty, disentangled from epistemic components. We demonstrate that this
enables safe retrieval and uncertainty-aware dataset visualization. To
encourage applications to further problems and domains, we release all
pretrained checkpoints and code under https://github.com/mkirchhof/url .
\\ ( https://arxiv.org/abs/2402.16569 ,  2864kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16570 (*cross-listing*)
Date: Mon, 26 Feb 2024 13:48:44 GMT   (1890kb,D)

Title: Searching a Lightweight Network Architecture for Thermal Infrared
  Pedestrian Tracking
Authors: Peng Gao, Xiao Liu, Yu Wang, Ru-Yue Yuan
Categories: cs.CV cs.LG
\\
  Manually-designed network architectures for thermal infrared pedestrian
tracking (TIR-PT) require substantial effort from human experts. Neural
networks with ResNet backbones are popular for TIR-PT. However, TIR-PT is a
tracking task and more challenging than classification and detection. This
paper makes an early attempt to search an optimal network architecture for
TIR-PT automatically, employing single-bottom and dual-bottom cells as basic
search units and incorporating eight operation candidates within the search
space. To expedite the search process, a random channel selection strategy is
employed prior to assessing operation candidates. Classification, batch hard
triplet, and center loss are jointly used to retrain the searched architecture.
The outcome is a high-performance network architecture that is both parameter-
and computation-efficient. Extensive experiments proved the effectiveness of
the automated method.
\\ ( https://arxiv.org/abs/2402.16570 ,  1890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16609 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:01:37 GMT   (2749kb)

Title: Combining Transformer based Deep Reinforcement Learning with
  Black-Litterman Model for Portfolio Optimization
Authors: Ruoyu Sun (1), Angelos Stefanidis (2), Zhengyong Jiang (2), Jionglong
  Su (2) ((1) Xi'an Jiaotong-Liverpool University, School of Mathematics and
  Physics, Department of Financial and Actuarial Mathematics (2) Xi'an
  Jiaotong-Liverpool University Entrepreneur College (Taicang), School of AI
  and Advanced Computing (1))
Categories: q-fin.PM cs.LG
Comments: 46 pages, 15 figures
\\
  As a model-free algorithm, deep reinforcement learning (DRL) agent learns and
makes decisions by interacting with the environment in an unsupervised way. In
recent years, DRL algorithms have been widely applied by scholars for portfolio
optimization in consecutive trading periods, since the DRL agent can
dynamically adapt to market changes and does not rely on the specification of
the joint dynamics across the assets. However, typical DRL agents for portfolio
optimization cannot learn a policy that is aware of the dynamic correlation
between portfolio asset returns. Since the dynamic correlations among portfolio
assets are crucial in optimizing the portfolio, the lack of such knowledge
makes it difficult for the DRL agent to maximize the return per unit of risk,
especially when the target market permits short selling (i.e., the US stock
market). In this research, we propose a hybrid portfolio optimization model
combining the DRL agent and the Black-Litterman (BL) model to enable the DRL
agent to learn the dynamic correlation between the portfolio asset returns and
implement an efficacious long/short strategy based on the correlation.
Essentially, the DRL agent is trained to learn the policy to apply the BL model
to determine the target portfolio weights. To test our DRL agent, we construct
the portfolio based on all the Dow Jones Industrial Average constitute stocks.
Empirical results of the experiments conducted on real-world United States
stock market data demonstrate that our DRL agent significantly outperforms
various comparison portfolio choice strategies and alternative DRL frameworks
by at least 42% in terms of accumulated return. In terms of the return per unit
of risk, our DRL agent significantly outperforms various comparative portfolio
choice strategies and alternative strategies based on other machine learning
frameworks.
\\ ( https://arxiv.org/abs/2402.16609 ,  2749kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16658 (*cross-listing*)
Date: Fri, 23 Feb 2024 15:42:13 GMT   (6380kb,D)

Title: Multi-Objective Learning for Deformable Image Registration
Authors: Monika Grewal, Henrike Westerveld, Peter A. N. Bosman, Tanja
  Alderliesten
Categories: eess.IV cs.CV cs.LG
\\
  Deformable image registration (DIR) involves optimization of multiple
conflicting objectives, however, not many existing DIR algorithms are
multi-objective (MO). Further, while there has been progress in the design of
deep learning algorithms for DIR, there is no work in the direction of MO DIR
using deep learning. In this paper, we fill this gap by combining a recently
proposed approach for MO training of neural networks with a well-known deep
neural network for DIR and create a deep learning based MO DIR approach. We
evaluate the proposed approach for DIR of pelvic magnetic resonance imaging
(MRI) scans. We experimentally demonstrate that the proposed MO DIR approach --
providing multiple registration outputs for each patient that each correspond
to a different trade-off between the objectives -- has additional desirable
properties from a clinical use point-of-view as compared to providing a single
DIR output. The experiments also show that the proposed MO DIR approach
provides a better spread of DIR outputs across the entire trade-off front than
simply training multiple neural networks with weights for each objective
sampled from a grid of possible values.
\\ ( https://arxiv.org/abs/2402.16658 ,  6380kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16661 (*cross-listing*)
Date: Mon, 26 Feb 2024 15:35:10 GMT   (266kb,D)

Title: Penalized Generative Variable Selection
Authors: Tong Wang, Jian Huang, Shuangge Ma
Categories: stat.ML cs.LG stat.ME
\\
  Deep networks are increasingly applied to a wide variety of data, including
data with high-dimensional predictors. In such analysis, variable selection can
be needed along with estimation/model building. Many of the existing deep
network studies that incorporate variable selection have been limited to
methodological and numerical developments. In this study, we consider
modeling/estimation using the conditional Wasserstein Generative Adversarial
networks. Group Lasso penalization is applied for variable selection, which may
improve model estimation/prediction, interpretability, stability, etc.
Significantly advancing from the existing literature, the analysis of censored
survival data is also considered. We establish the convergence rate for
variable selection while considering the approximation error, and obtain a more
efficient distribution estimation. Simulations and the analysis of real
experimental data demonstrate satisfactory practical utility of the proposed
analysis.
\\ ( https://arxiv.org/abs/2402.16661 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16683 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:01:35 GMT   (674kb,D)

Title: Re-Envisioning Numerical Information Field Theory (NIFTy.re): A Library
  for Gaussian Processes and Variational Inference
Authors: Gordian Edenhofer, Philipp Frank, Jakob Roth, Reimar H. Leike, Massin
  Guerdi, Lukas I. Scheel-Platz, Matteo Guardiani, Vincent Eberle, Margret
  Westerkamp, and Torsten A. En{\ss}lin
Categories: astro-ph.IM cs.LG stat.ML
Comments: 10 pages, 2 figures
\\
  Imaging is the process of transforming noisy, incomplete data into a space
that humans can interpret. NIFTy is a Bayesian framework for imaging and has
already successfully been applied to many fields in astrophysics. Previous
design decisions held the performance and the development of methods in NIFTy
back. We present a rewrite of NIFTy, coined NIFTy.re, which reworks the
modeling principle, extends the inference strategies, and outsources much of
the heavy lifting to JAX. The rewrite dramatically accelerates models written
in NIFTy, lays the foundation for new types of inference machineries, improves
maintainability, and enables interoperability between NIFTy and the JAX machine
learning ecosystem.
\\ ( https://arxiv.org/abs/2402.16683 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16688 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:04:47 GMT   (8549kb)

Title: On the connection between Noise-Contrastive Estimation and Contrastive
  Divergence
Authors: Amanda Olmin, Jakob Lindqvist, Lennart Svensson, Fredrik Lindsten
Categories: stat.ML cs.LG
Comments: Accepted to AISTATS 2024
\\
  Noise-contrastive estimation (NCE) is a popular method for estimating
unnormalised probabilistic models, such as energy-based models, which are
effective for modelling complex data distributions. Unlike classical maximum
likelihood (ML) estimation that relies on importance sampling (resulting in
ML-IS) or MCMC (resulting in contrastive divergence, CD), NCE uses a proxy
criterion to avoid the need for evaluating an often intractable normalisation
constant.
  Despite apparent conceptual differences, we show that two NCE criteria,
ranking NCE (RNCE) and conditional NCE (CNCE), can be viewed as ML estimation
methods. Specifically, RNCE is equivalent to ML estimation combined with
conditional importance sampling, and both RNCE and CNCE are special cases of
CD. These findings bridge the gap between the two method classes and allow us
to apply techniques from the ML-IS and CD literature to NCE, offering several
advantageous extensions.
\\ ( https://arxiv.org/abs/2402.16688 ,  8549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16712 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:30:58 GMT   (81kb,D)

Title: Scalable Robust Sparse Principal Component Analysis
Authors: Xiao Ling, Paul Brooks
Categories: stat.ML cs.LG math.OC
Comments: 10 pages
\\
  In this work, we propose an optimization framework for estimating a sparse
robust one-dimensional subspace. Our objective is to minimize both the
representation error and the penalty, in terms of the l1-norm criterion. Given
that the problem is NP-hard, we introduce a linear relaxation-based approach.
Additionally, we present a novel fitting procedure, utilizing simple ratios and
sorting techniques. The proposed algorithm demonstrates a worst-case time
complexity of $O(n^2 m \log n)$ and, in certain instances, achieves global
optimality for the sparse robust subspace, thereby exhibiting polynomial time
efficiency. Compared to extant methodologies, the proposed algorithm finds the
subspace with the lowest discordance, offering a smoother trade-off between
sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold
improvement in computational speeds for matrices of 2000x2000 over CPU version.
Furthermore, this method is distinguished by several advantages, including its
independence from initialization and deterministic and replicable procedures.
Furthermore, this method is distinguished by several advantages, including its
independence from initialization and deterministic and replicable procedures.
The real-world example demonstrates the effectiveness of algorithm in achieving
meaningful sparsity, underscoring its precise and useful application across
various domains.
\\ ( https://arxiv.org/abs/2402.16712 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16731 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:52:35 GMT   (773kb,D)

Title: Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
Authors: Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang,
  Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu, Gennady
  Pekhimenko
Categories: cs.AR cs.DC cs.LG cs.PF
\\
  Graph Neural Networks (GNNs) are emerging ML models to analyze
graph-structure data. Graph Neural Network (GNN) execution involves both
compute-intensive and memory-intensive kernels, the latter dominates the total
time, being significantly bottlenecked by data movement between memory and
processors. Processing-In-Memory (PIM) systems can alleviate this data movement
bottleneck by placing simple processors near or inside to memory arrays. In
this work, we introduce PyGim, an efficient ML framework that accelerates GNNs
on real PIM systems. We propose intelligent parallelization techniques for
memory-intensive kernels of GNNs tailored for real PIM systems, and develop
handy Python API for them. We provide hybrid GNN execution, in which the
compute-intensive and memory-intensive kernels are executed in
processor-centric and memory-centric computing systems, respectively, to match
their algorithmic nature. We extensively evaluate PyGim on a real-world PIM
system with 1992 PIM cores using emerging GNN models, and demonstrate that it
outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average
3.04x, and achieves higher resource utilization than CPU and GPU systems. Our
work provides useful recommendations for software, system and hardware
designers. PyGim will be open-sourced to enable the widespread use of PIM
systems in GNNs.
\\ ( https://arxiv.org/abs/2402.16731 ,  773kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16734 (*cross-listing*)
Date: Mon, 26 Feb 2024 16:53:23 GMT   (13657kb,D)

Title: Investigating the Robustness of Vision Transformers against Label Noise
  in Medical Image Classification
Authors: Bidur Khanal, Prashant Shrestha, Sanskar Amgain, Bishesh Khanal, Binod
  Bhattarai, Cristian A. Linte
Categories: eess.IV cs.CV cs.LG
\\
  Label noise in medical image classification datasets significantly hampers
the training of supervised deep learning methods, undermining their
generalizability. The test performance of a model tends to decrease as the
label noise rate increases. Over recent years, several methods have been
proposed to mitigate the impact of label noise in medical image classification
and enhance the robustness of the model. Predominantly, these works have
employed CNN-based architectures as the backbone of their classifiers for
feature extraction. However, in recent years, Vision Transformer (ViT)-based
backbones have replaced CNNs, demonstrating improved performance and a greater
ability to learn more generalizable features, especially when the dataset is
large. Nevertheless, no prior work has rigorously investigated how
transformer-based backbones handle the impact of label noise in medical image
classification. In this paper, we investigate the architectural robustness of
ViT against label noise and compare it to that of CNNs. We use two medical
image classification datasets -- COVID-DU-Ex, and NCT-CRC-HE-100K -- both
corrupted by injecting label noise at various rates. Additionally, we show that
pretraining is crucial for ensuring ViT's improved robustness against label
noise in supervised training.
\\ ( https://arxiv.org/abs/2402.16734 ,  13657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16770 (*cross-listing*)
Date: Mon, 26 Feb 2024 17:39:23 GMT   (4469kb,D)

Title: Neural Population Geometry and Optimal Coding of Tasks with Shared
  Latent Structure
Authors: Albert J. Wakhloo, Will Slatton, and SueYeon Chung
Categories: q-bio.NC cond-mat.dis-nn cond-mat.stat-mech cs.LG cs.NE
Comments: 24 Pages and 7 figures in main text. 15 Pages and 7 figures in
  supplemental material
\\
  Humans and animals can recognize latent structures in their environment and
apply this information to efficiently navigate the world. Several recent works
argue that the brain supports these abilities by forming neural representations
that encode such latent structures in flexible, generalizable ways. However, it
remains unclear what aspects of neural population activity are contributing to
these computational capabilities. Here, we develop an analytical theory linking
the mesoscopic statistics of a neural population's activity to generalization
performance on a multi-task learning problem. To do this, we rely on a
generative model in which different tasks depend on a common, unobserved latent
structure and predictions are formed from a linear readout of a neural
population's activity. We show that three geometric measures of the population
activity determine generalization performance in these settings. Using this
theory, we find that experimentally observed factorized (or disentangled)
representations naturally emerge as an optimal solution to the multi-task
learning problem. We go on to show that when data is scarce, optimal codes
compress less informative latent variables, and when data is abundant, optimal
codes expand this information in the state space. We validate predictions from
our theory using biological and artificial neural network data. Our results
therefore tie neural population geometry to the multi-task learning problem and
make normative predictions of the structure of population activity in these
settings.
\\ ( https://arxiv.org/abs/2402.16770 ,  4469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16792 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:05:55 GMT   (1919kb,D)

Title: Rate-Optimal Rank Aggregation with Private Pairwise Rankings
Authors: Shirong Xu, Will Wei Sun, Guang Cheng
Categories: stat.ML cs.CR cs.LG
\\
  In various real-world scenarios like recommender systems and political
surveys, pairwise rankings are commonly collected and utilized for rank
aggregation to obtain an overall ranking of items. However, preference rankings
can reveal individuals' personal preferences, underscoring the need to protect
them before releasing for downstream analysis. In this paper, we address the
challenge of preserving privacy while ensuring the utility of rank aggregation
based on pairwise rankings generated from the Bradley-Terry-Luce (BTL) model.
Using the randomized response mechanism to perturb raw pairwise rankings is a
common privacy protection strategy used in practice, but a critical challenge
arises because the privatized rankings no longer adhere to the BTL model,
resulting in significant bias in downstream rank aggregation tasks. Motivated
from this, we propose a debiased randomized response mechanism to protect the
raw pairwise rankings, ensuring consistent estimation of true preferences and
rankings in downstream rank aggregation. Theoretically, we offer insights into
the relationship between overall privacy guarantees and estimation errors from
private ranking data, and establish minimax rates for estimation errors. This
enables the determination of optimal privacy guarantees that balance
consistency in rank aggregation with robust privacy protection. We also
investigate convergence rates of expected ranking errors for partial and full
ranking recovery, quantifying how privacy protection influences the
specification of top-$K$ item sets and complete rankings. Our findings are
validated through extensive simulations and a real application.
\\ ( https://arxiv.org/abs/2402.16792 ,  1919kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16793 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:07:27 GMT   (11531kb,D)

Title: Failures and Successes of Cross-Validation for Early-Stopped Gradient
  Descent
Authors: Pratik Patil, Yuchen Wu, Ryan J. Tibshirani
Categories: math.ST cs.LG stat.ML stat.TH
Comments: 76 pages, 27 figures
\\
  We analyze the statistical properties of generalized cross-validation (GCV)
and leave-one-out cross-validation (LOOCV) applied to early-stopped gradient
descent (GD) in high-dimensional least squares regression. We prove that GCV is
generically inconsistent as an estimator of the prediction risk of
early-stopped GD, even for a well-specified linear model with isotropic
features. In contrast, we show that LOOCV converges uniformly along the GD
trajectory to the prediction risk. Our theory requires only mild assumptions on
the data distribution and does not require the underlying regression function
to be linear. Furthermore, by leveraging the individual LOOCV errors, we
construct consistent estimators for the entire prediction error distribution
along the GD trajectory and consistent estimators for a wide class of error
functionals. This in particular enables the construction of pathwise prediction
intervals based on GD iterates that have asymptotically correct nominal
coverage conditional on the training data.
\\ ( https://arxiv.org/abs/2402.16793 ,  11531kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16796 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:09:24 GMT   (25388kb,D)

Title: Expressive Whole-Body Control for Humanoid Robots
Authors: Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong
  Wang
Categories: cs.RO cs.LG
Comments: Website: https://expressive-humanoid.github.io
\\
  Can we enable humanoid robots to generate rich, diverse, and expressive
motions in the real world? We propose to learn a whole-body control policy on a
human-sized robot to mimic human motions as realistic as possible. To train
such a policy, we leverage the large-scale human motion capture data from the
graphics community in a Reinforcement Learning framework. However, directly
performing imitation learning with the motion capture dataset would not work on
the real humanoid robot, given the large gap in degrees of freedom and physical
capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this
problem by encouraging the upper humanoid body to imitate a reference motion,
while relaxing the imitation constraint on its two legs and only requiring them
to follow a given velocity robustly. With training in simulation and Sim2Real
transfer, our policy can control a humanoid robot to walk in different styles,
shake hands with humans, and even dance with a human in the real world. We
conduct extensive studies and comparisons on diverse motions in both simulation
and the real world to show the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2402.16796 ,  25388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16811 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:34:58 GMT   (182kb,D)

Title: Stopping Bayesian Optimization with Probabilistic Regret Bounds
Authors: James T. Wilson
Categories: stat.ML cs.LG
Comments: Submitted to ICML 2024
\\
  Bayesian optimization is a popular framework for efficiently finding
high-quality solutions to difficult problems based on limited prior
information. As a rule, these algorithms operate by iteratively choosing what
to try next until some predefined budget has been exhausted. We investigate
replacing this de facto stopping rule with an $(\epsilon, \delta)$-criterion:
stop when a solution has been found whose value is within $\epsilon > 0$ of the
optimum with probability at least $1 - \delta$ under the model. Given access to
the prior distribution of problems, we show how to verify this condition in
practice using a limited number of draws from the posterior. For Gaussian
process priors, we prove that Bayesian optimization with the proposed criterion
stops in finite time and returns a point that satisfies the $(\epsilon,
\delta)$-criterion under mild assumptions. These findings are accompanied by
extensive empirical results which demonstrate the strengths and weaknesses of
this approach.
\\ ( https://arxiv.org/abs/2402.16811 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16814 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:37:16 GMT   (23kb,D)

Title: Cut Facets and Cube Facets of Lifted Multicut Polytopes
Authors: Lucas Fabian Naumann, Jannik Irmai, Shengxian Zhao, Bjoern Andres
Categories: cs.DM cs.LG
Comments: 10 pages, 5 figures
\\
  The lifted multicut problem has diverse applications in the field of computer
vision. Exact algorithms based on linear programming require an understanding
of lifted multicut polytopes. Despite recent progress, two fundamental
questions about these polytopes have remained open: Which lower cube
inequalities define facets, and which cut inequalities define facets? In this
article, we answer the first question by establishing conditions that are
necessary, sufficient and efficiently decidable. Toward the second question, we
show that deciding facet-definingness of cut inequalities is NP-hard. This
completes the analysis of canonical facets of lifted multicut polytopes.
\\ ( https://arxiv.org/abs/2402.16814 ,  23kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2205.10316
replaced with revised version Sat, 24 Feb 2024 05:29:54 GMT   (4272kb,D)

Title: Complex behavior from intrinsic motivation to occupy action-state path
  space
Authors: Jorge Ram\'irez-Ruiz, Dmytro Grytskyy, Chiara Mastrogiuseppe, Yamen
  Habib and Rub\'en Moreno-Bote
Categories: cs.AI cs.LG q-bio.NC
Comments: Extended results, main ones: high dimensional, continuous control,
  experiment from Gymnasium; and detailed comparison with Empowerment and Free
  Energy Principle. Updated all main figures
\\ ( https://arxiv.org/abs/2205.10316 ,  4272kb)
------------------------------------------------------------------------------
\\
arXiv:2209.01410
replaced with revised version Sun, 25 Feb 2024 11:16:15 GMT   (2753kb,D)

Title: Closed-Loop View of the Regulation of AI: Equal Impact across Repeated
  Interactions
Authors: Quan Zhou and Ramen Ghosh and Robert Shorten and Jakub Marecek
Categories: cs.AI
\\ ( https://arxiv.org/abs/2209.01410 ,  2753kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11196
replaced with revised version Sun, 25 Feb 2024 07:58:50 GMT   (1116kb)

Title: Bridging the Global Divide in AI Regulation: A Proposal for a
  Contextual, Coherent, and Commensurable Framework
Authors: Sangchul Park
Categories: cs.AI cs.CY
Comments: 33(2) Wash. Int'l L.J. _ (2024)
\\ ( https://arxiv.org/abs/2303.11196 ,  1116kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08144
replaced with revised version Sat, 24 Feb 2024 12:43:14 GMT   (1969kb,D)

Title: Mobile-Env: An Evaluation Platform and Benchmark for LLM-GUI Interaction
Authors: Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng Cao, Kai Yu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2305.08144 ,  1969kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08254
replaced with revised version Fri, 23 Feb 2024 19:47:45 GMT   (7344kb,D)

Title: Autonomous and Human-Driven Vehicles Interacting in a Roundabout: A
  Quantitative and Qualitative Evaluation
Authors: Laura Ferrarotti, Massimiliano Luca, Gabriele Santin, Giorgio
  Previati, Gianpiero Mastinu, Massimiliano Gobbi, Elena Campi, Lorenzo
  Uccello, Antonino Albanese, Praveen Zalaya, Alessandro Roccasalva, Bruno
  Lepri
Categories: cs.AI cs.LG cs.RO
\\ ( https://arxiv.org/abs/2309.08254 ,  7344kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00771
replaced with revised version Sat, 24 Feb 2024 13:54:06 GMT   (21353kb,D)

Title: Pre-training with Synthetic Data Helps Offline Reinforcement Learning
Authors: Zecheng Wang, Che Wang, Zixuan Dong, Keith Ross
Categories: cs.AI cs.LG
Comments: 37 pages, 17 figures
\\ ( https://arxiv.org/abs/2310.00771 ,  21353kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08915
replaced with revised version Mon, 26 Feb 2024 02:51:30 GMT   (700kb,D)

Title: Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs
Authors: Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia
  Han, Jared Tanner, Shiwei Liu, Rongrong Ji
Categories: cs.AI
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.08915 ,  700kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14403
replaced with revised version Mon, 26 Feb 2024 18:29:45 GMT   (5470kb,D)

Title: O3D: Offline Data-driven Discovery and Distillation for Sequential
  Decision-Making with Large Language Models
Authors: Yuchen Xiao, Yanchao Sun, Mengda Xu, Udari Madhushani, Jared Vann,
  Deepeka Garg, Sumitra Ganesh
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.14403 ,  5470kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19852
replaced with revised version Mon, 26 Feb 2024 18:19:25 GMT   (3621kb,D)

Title: AI Alignment: A Comprehensive Survey
Authors: Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile
  Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan
  Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian
  Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike
  Guo, Wen Gao
Categories: cs.AI
Comments: Continually updated, including weak-to-strong generalization and
  socio-technical thinking. 58 pages (excluding bibliography), 801 references
\\ ( https://arxiv.org/abs/2310.19852 ,  3621kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20054
replaced with revised version Mon, 26 Feb 2024 05:17:25 GMT   (513kb,D)

Title: Constrained Hierarchical Monte Carlo Belief-State Planning
Authors: Arec Jamgochian, Hugo Buurmeijer, Kyle H. Wray, Anthony Corso, Mykel
  J. Kochenderfer
Categories: cs.AI cs.RO
Comments: Accepted to the 2024 IEEE International Conference on Robotics and
  Automation (ICRA)
\\ ( https://arxiv.org/abs/2310.20054 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03004
replaced with revised version Mon, 26 Feb 2024 06:12:49 GMT   (847kb,D)

Title: Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning
Authors: Jinchuan Zhang, Bei Hui, Chong Mu, Ling Tian
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.03004 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00546
replaced with revised version Sun, 25 Feb 2024 09:18:20 GMT   (4121kb,D)

Title: AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with
  Thirteen Modalities
Authors: Run Shao, Cheng Yang, Qiujun Li, Qing Zhu, Yongjun Zhang, YanSheng Li,
  Yu Liu, Yong Tang, Dapeng Liu, Shizhong Yang, Haifeng Li
Categories: cs.AI cs.LG
Comments: 49 pages, 16 tables, 3 figures
\\ ( https://arxiv.org/abs/2401.00546 ,  4121kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07314
replaced with revised version Sun, 25 Feb 2024 14:39:48 GMT   (1880kb,D)

Title: MapGPT: Map-Guided Prompting with Adaptive Path Planning for
  Vision-and-Language Navigation
Authors: Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang,
  Kwan-Yee K. Wong
Categories: cs.AI cs.CV cs.RO
\\ ( https://arxiv.org/abs/2401.07314 ,  1880kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08664
replaced with revised version Sun, 25 Feb 2024 05:41:24 GMT   (268kb,D)

Title: Adapting Large Language Models for Education: Foundational Capabilities,
  Potentials, and Challenges
Authors: Qingyao Li, Lingyue Fu, Weiming Zhang, Xianyu Chen, Jingwei Yu, Wei
  Xia, Weinan Zhang, Ruiming Tang, Yong Yu
Categories: cs.AI cs.CL
Comments: 13 pages, 3 figures, 1 table
\\ ( https://arxiv.org/abs/2401.08664 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01118
replaced with revised version Sun, 25 Feb 2024 23:54:49 GMT   (6889kb,D)

Title: PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language
  Models
Authors: Sihao Hu, Tiansheng Huang, Ling Liu
Categories: cs.AI cs.CL
Comments: 10 pages
\\ ( https://arxiv.org/abs/2402.01118 ,  6889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03822
replaced with revised version Sat, 24 Feb 2024 01:11:14 GMT   (2888kb,D)

Title: RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
Authors: Si Shen, Peijun Shen, Danhao Zhu
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.03822 ,  2888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05391
replaced with revised version Mon, 26 Feb 2024 09:57:12 GMT   (9613kb,D)

Title: Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
Authors: Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang
  Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, Jiaqi Li, Xiaoze Liu,
  Jeff Z. Pan, Ningyu Zhang, Huajun Chen
Categories: cs.AI cs.CV cs.IR cs.LG
Comments: Ongoing work; 41 pages (Main Text), 55 pages (Total), 11 Tables, 13
  Figures, 619 citations; Paper list is available at
  https://github.com/zjukg/KG-MM-Survey
\\ ( https://arxiv.org/abs/2402.05391 ,  9613kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07140
replaced with revised version Sat, 24 Feb 2024 07:05:37 GMT   (449kb,D)

Title: Graph Descriptive Order Improves Reasoning with Large Language Model
Authors: Yuyao Ge, Shenghua Liu, Wenjie Feng, Lingrui Mei, Lizhe Chen, Xueqi
  Cheng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07140 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10877
replaced with revised version Mon, 26 Feb 2024 17:05:33 GMT   (2637kb,D)

Title: Robust agents learn causal world models
Authors: Jonathan Richens, Tom Everitt
Categories: cs.AI cs.LG
Comments: ICLR 2024 (oral). Updated discussion of emergent capabilities +
  references
\\ ( https://arxiv.org/abs/2402.10877 ,  2637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12702
replaced with revised version Mon, 26 Feb 2024 00:23:45 GMT   (451kb,D)

Title: From Cloud to Edge: Rethinking Generative AI for Low-Resource Design
  Challenges
Authors: Sai Krishna Revanth Vuruma, Ashley Margetts, Jianhai Su, Faez Ahmed,
  Biplav Srivastava
Categories: cs.AI cs.CY
Comments: Accepted for the Artificial Intelligence for Design Problems bridge
  program at AAAI 2024
\\ ( https://arxiv.org/abs/2402.12702 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15506
replaced with revised version Mon, 26 Feb 2024 18:24:46 GMT   (3807kb,D)

Title: AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning
Authors: Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao,
  Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika
  Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan
  Wang, Caiming Xiong
Categories: cs.AI cs.CL cs.LG
Comments: Add testing results on ToolBench and correct typographical errors
\\ ( https://arxiv.org/abs/2402.15506 ,  3807kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07843
replaced with revised version Sun, 25 Feb 2024 22:17:52 GMT   (7120kb,D)

Title: Error-Robust Retrieval for Chinese Spelling Check
Authors: Xunjian Yin and Xinyu Hu and Jin Jiang and Xiaojun Wan
Categories: cs.CL
Comments: 11 pages, 3 figures
\\ ( https://arxiv.org/abs/2211.07843 ,  7120kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12822
replaced with revised version Sat, 24 Feb 2024 15:33:14 GMT   (1341kb,D)

Title: Automatic Prompt Augmentation and Selection with Chain-of-Thought from
  Labeled Data
Authors: KaShun Shum, Shizhe Diao, Tong Zhang
Categories: cs.CL
Comments: EMNLP 2023
\\ ( https://arxiv.org/abs/2302.12822 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13376
replaced with revised version Sat, 24 Feb 2024 07:02:38 GMT   (858kb,D)

Title: Efficient Ensemble for Multimodal Punctuation Restoration using
  Time-Delay Neural Network
Authors: Xing Yi Liu and Homayoon Beigi
Categories: cs.CL cs.HC cs.LG cs.SD eess.AS
Comments: 6 pages, 1 figure, 5 tables, paper at IMCOM 2024, technical report at
  Recognition Technologies, Inc
Report-no: RTI-20230224-01
Journal-ref: 2024 18th International Conference on Ubiquitous Information
  Management and Communication (IMCOM), 2024, pp. 1-6
DOI: 10.1109/IMCOM60618.2024.10418445
\\ ( https://arxiv.org/abs/2302.13376 ,  858kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12008
replaced with revised version Sat, 24 Feb 2024 05:52:28 GMT   (2731kb,D)

Title: CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts
Authors: Peipeng Yu, Jiahan Chen, Xuan Feng, Zhihua Xia
Categories: cs.CL
Comments: 9 pages, 6 figures
\\ ( https://arxiv.org/abs/2304.12008 ,  2731kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01181
replaced with revised version Mon, 26 Feb 2024 16:23:33 GMT   (11973kb,D)

Title: A Paradigm Shift: The Future of Machine Translation Lies with Large
  Language Models
Authors: Chenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa
  Lynn, Alham Fikri Aji, Derek F. Wong, Longyue Wang
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.01181 ,  11973kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03514
replaced with revised version Mon, 26 Feb 2024 17:16:12 GMT   (688kb,D)

Title: Can Large Language Models Transform Computational Social Science?
Authors: Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi
  Yang
Categories: cs.CL cs.LG
Comments: To appear in "Computational Linguistics" (CL)
\\ ( https://arxiv.org/abs/2305.03514 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05496
replaced with revised version Sat, 24 Feb 2024 04:23:25 GMT   (8326kb,D)

Title: Exploiting Pseudo Image Captions for Multimodal Summarization
Authors: Chaoya Jiang, Rui Xie, Wei Ye, Jinan Sun, Shikun Zhang
Categories: cs.CL
Comments: Accepted at ACL2023 Findings
\\ ( https://arxiv.org/abs/2305.05496 ,  8326kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09859
replaced with revised version Sat, 24 Feb 2024 19:47:14 GMT   (1364kb,D)

Title: Smaller Language Models are Better Black-box Machine-Generated Text
  Detectors
Authors: Niloofar Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, Taylor
  Berg-Kirkpatrick
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2305.09859 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12474
replaced with revised version Sat, 24 Feb 2024 15:44:21 GMT   (2035kb,D)

Title: Evaluating the Performance of Large Language Models on GAOKAO Benchmark
Authors: Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng
  Qiu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.12474 ,  2035kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14651
replaced with revised version Sat, 24 Feb 2024 06:48:20 GMT   (12577kb,D)

Title: Revisit and Outstrip Entity Alignment: A Perspective of Generative
  Models
Authors: Lingbing Guo, Zhuo Chen, Jiaoyan Chen, Yin Fang, Wen Zhang, and Huajun
  Chen
Categories: cs.CL cs.IR cs.LG
Comments: Accepted at ICLR 2024
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2305.14651 ,  12577kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14965
replaced with revised version Mon, 26 Feb 2024 08:42:37 GMT   (10640kb,D)

Title: Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting
  Jailbreaks
Authors: Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit
  Choudhury
Categories: cs.CL
Comments: Accepted in LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
\\ ( https://arxiv.org/abs/2305.14965 ,  10640kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15033
replaced with revised version Mon, 26 Feb 2024 15:26:20 GMT   (2510kb,D)

Title: SmartTrim: Adaptive Tokens and Attention Pruning for Efficient
  Vision-Language Models
Authors: Zekun Wang, Jingchang Chen, Wangchunshu Zhou, Haichao Zhu, Jiafeng
  Liang, Liping Shan, Ming Liu, Dongliang Xu, Qing Yang, Bing Qin
Categories: cs.CL
Comments: COLING-LREC 2024
\\ ( https://arxiv.org/abs/2305.15033 ,  2510kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06794
replaced with revised version Fri, 23 Feb 2024 21:18:55 GMT   (25518kb,D)

Title: A blind spot for large language models: Supradiegetic linguistic
  information
Authors: Julia Witte Zimmerman, Denis Hudon, Kathryn Cramer, Jonathan St. Onge,
  Mikaela Fudolig, Milo Z. Trujillo, Christopher M. Danforth, Peter Sheridan
  Dodds
Categories: cs.CL cs.AI
Comments: 21 pages, 6 figures, 3 tables. arXiv admin note: text overlap with
  arXiv:2206.02608, arXiv:2303.12712, arXiv:2305.10601, arXiv:2305.06424,
  arXiv:1908.08530 by other authors
\\ ( https://arxiv.org/abs/2306.06794 ,  25518kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15054
replaced with revised version Sat, 24 Feb 2024 19:53:58 GMT   (202kb,D)

Title: A Geometric Notion of Causal Probing
Authors: Cl\'ement Guerner, Anej Svete, Tianyu Liu, Alexander Warstadt, Ryan
  Cotterell
Categories: cs.CL
\\ ( https://arxiv.org/abs/2307.15054 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04945
replaced with revised version Mon, 26 Feb 2024 13:33:43 GMT   (1523kb,D)

Title: LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking
Authors: Fahim Dalvi, Maram Hasanain, Sabri Boughorbel, Basel Mousi, Samir
  Abdaljalil, Nizi Nazar, Ahmed Abdelali, Shammur Absar Chowdhury, Hamdy
  Mubarak, Ahmed Ali, Majd Hawasly, Nadir Durrani, Firoj Alam
Categories: cs.CL cs.AI
Comments: Accepted as a demo paper at EACL 2024
MSC-class: 68T50
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2308.04945 ,  1523kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08758
replaced with revised version Sun, 25 Feb 2024 13:15:45 GMT   (68kb,D)

Title: Discrete Prompt Compression with Reinforcement Learning
Authors: Hoyoun Jung and Kyung-Joong Kim
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2308.08758 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12546
replaced with revised version Mon, 26 Feb 2024 04:39:08 GMT   (34kb)

Title: Automatic Answerability Evaluation for Question Generation
Authors: Zifan Wang, Kotaro Funakoshi, Manabu Okumura
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.12546 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12646
replaced with revised version Sun, 25 Feb 2024 19:43:20 GMT   (580kb)

Title: Decoding Emotional Experiences in Dyadic Conversations of Married
  Couples: Leveraging Semantic Similarity through Sentence Embedding
Authors: Chen-Wei Yu, Yun-Shiuan Chuang, Alexandros N. Lotsos, and Claudia M.
  Haase
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.12646 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14556
replaced with revised version Sun, 25 Feb 2024 05:57:01 GMT   (7202kb,D)

Title: Art or Artifice? Large Language Models and the False Promise of
  Creativity
Authors: Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan,
  Chien-Sheng Wu
Categories: cs.CL cs.AI cs.HC
\\ ( https://arxiv.org/abs/2309.14556 ,  7202kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01061
replaced with revised version Sat, 24 Feb 2024 03:03:12 GMT   (425kb,D)

Title: Reasoning on Graphs: Faithful and Interpretable Large Language Model
  Reasoning
Authors: Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan
Categories: cs.CL cs.AI
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.01061 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01132
replaced with revised version Mon, 26 Feb 2024 16:12:58 GMT   (501kb,D)

Title: Automated Evaluation of Classroom Instructional Support with LLMs and
  BoWs: Connecting Global Predictions to Specific Feedback
Authors: Jacob Whitehill and Jennifer LoCasale-Crouch
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.01132 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02124
replaced with revised version Mon, 26 Feb 2024 17:24:35 GMT   (35453kb,D)

Title: Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology
  View
Authors: Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin
  Deng
Categories: cs.CL cs.AI cs.CY cs.LG cs.MA
Comments: Work in progress. 61 pages (8 main), 67 figures, 37 tables. Blog:
  \url{https://zjunlp.github.io/project/MachineSoM}
\\ ( https://arxiv.org/abs/2310.02124 ,  35453kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02174
replaced with revised version Mon, 26 Feb 2024 08:26:30 GMT   (2268kb,D)

Title: Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
Authors: Qiming Xie, Zengzhi Wang, Yi Feng, and Rui Xia
Categories: cs.CL cs.AI cs.LG
Comments: Update mitigation results of fine-tuning the model on synthesized
  high-quality preference data with DPO algorithm
\\ ( https://arxiv.org/abs/2310.02174 ,  2268kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06213
replaced with revised version Sat, 24 Feb 2024 16:11:57 GMT   (1931kb,D)

Title: GeoLLM: Extracting Geospatial Knowledge from Large Language Models
Authors: Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell,
  Stefano Ermon
Categories: cs.CL cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.06213 ,  1931kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06272
replaced with revised version Mon, 26 Feb 2024 17:36:48 GMT   (5520kb,D)

Title: Let Models Speak Ciphers: Multiagent Debate through Embeddings
Authors: Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo
  Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.06272 ,  5520kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06707
replaced with revised version Mon, 26 Feb 2024 01:38:43 GMT   (255kb,D)

Title: Quality-Aware Translation Models: Efficient Generation and Quality
  Estimation in a Single Model
Authors: Christian Tomani, David Vilar, Markus Freitag, Colin Cherry, Subhajit
  Naskar, Mara Finkelstein, Xavier Garcia and Daniel Cremers
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.06707 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07088
replaced with revised version Fri, 23 Feb 2024 20:35:30 GMT   (3906kb,D)

Title: Diversity of Thought Improves Reasoning Abilities of LLMs
Authors: Ranjita Naik, Varun Chandrasekaran, Mert Yuksekgonul, Hamid Palangi,
  Besmira Nushi
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.07088 ,  3906kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09017
replaced with revised version Sun, 25 Feb 2024 14:45:00 GMT   (919kb,D)

Title: Dont Add, dont Miss: Effective Content Preserving Generation from
  Pre-Selected Text Spans
Authors: Aviv Slobodkin, Avi Caciularu, Eran Hirsch, Ido Dagan
Categories: cs.CL
Comments: EMNLP 2023, findings
\\ ( https://arxiv.org/abs/2310.09017 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13995
replaced with revised version Sun, 25 Feb 2024 22:34:50 GMT   (8022kb,D)

Title: On Bilingual Lexicon Induction with Large Language Models
Authors: Yaoyiran Li, Anna Korhonen, Ivan Vuli\'c
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: EMNLP 2023 Main Conference
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing, pages 9577-9599
DOI: 10.18653/v1/2023.emnlp-main.595
\\ ( https://arxiv.org/abs/2310.13995 ,  8022kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15213
replaced with revised version Sun, 25 Feb 2024 18:32:18 GMT   (1716kb,D)

Title: Function Vectors in Large Language Models
Authors: Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C.
  Wallace, David Bau
Categories: cs.CL cs.LG
Comments: ICLR 2024. 52 pages, 30 figures, 23 tables. Code and data at
  https://functions.baulab.info
\\ ( https://arxiv.org/abs/2310.15213 ,  1716kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01200
replaced with revised version Mon, 26 Feb 2024 08:20:03 GMT   (728kb,D)

Title: Continual Learning Under Language Shift
Authors: Evangelia Gogoulou, Timoth\'ee Lesort, Magnus Boman, Joakim Nivre
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.01200 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01270
replaced with revised version Sun, 25 Feb 2024 11:17:42 GMT   (325kb,D)

Title: People Make Better Edits: Measuring the Efficacy of LLM-Generated
  Counterfactually Augmented Data for Harmful Language Detection
Authors: Indira Sen, Dennis Assenmacher, Mattia Samory, Isabelle Augenstein,
  Wil van der Aalst, Claudia Wagner
Categories: cs.CL cs.CY
Comments: Preprint of EMNLP'23 paper
\\ ( https://arxiv.org/abs/2311.01270 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04924
replaced with revised version Mon, 26 Feb 2024 13:08:43 GMT   (928kb)

Title: Tuning-less Object Naming with a Foundation Model
Authors: Andrej Lucny, Pavel Petrovic
Categories: cs.CL cs.AI
Comments: This work was funded (or co-funded) by the Horizon-Widera-2021
  European Twinning project TERAIS G.A. n. 101079338 World Symposium on Digital
  Intelligence for Systems and Machines (DISA2023), Kosice, September 21-22,
  2023 citations: https://ieeexplore.ieee.org/document/10308905 codes:
  https://github.com/andylucny/whatisthis,
  https://doi.org/10.5281/zenodo.10702868 7 pages, 9 figures, 0 tables
ACM-class: I.2.9
Journal-ref: 2023 World Symposium on Digital Intelligence for Systems and
  Machines (DISA) https://ieeexplore.ieee.org/xpl/conhome/10308901/proceeding
  pages 154-160
DOI: 10.1109/DISA59116.2023
\\ ( https://arxiv.org/abs/2311.04924 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05928
replaced with revised version Mon, 26 Feb 2024 06:46:17 GMT   (9216kb,D)

Title: The Shape of Learning: Anisotropy and Intrinsic Dimensions in
  Transformer-Based Models
Authors: Anton Razzhigaev, Matvey Mikhalchuk, Elizaveta Goncharova, Ivan
  Oseledets, Denis Dimitrov, Andrey Kuznetsov
Categories: cs.CL cs.AI cs.IT cs.LG math.GN math.IT
Comments: Accepted to EACL-2024
\\ ( https://arxiv.org/abs/2311.05928 ,  9216kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07564
replaced with revised version Sun, 25 Feb 2024 03:06:49 GMT   (61kb,D)

Title: Can Authorship Attribution Models Distinguish Speakers in Speech
  Transcripts?
Authors: Cristina Aggazzotti, Nicholas Andrews, Elizabeth Allyn Smith
Categories: cs.CL cs.LG
Comments: added more baselines, improved fine-tuning results, added
  pre-training domain experiment
\\ ( https://arxiv.org/abs/2311.07564 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08941
replaced with revised version Mon, 26 Feb 2024 08:40:13 GMT   (880kb,D)

Title: Reasoning over Description Logic-based Contexts with Transformers
Authors: Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.08941 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09101
replaced with revised version Sun, 25 Feb 2024 15:53:37 GMT   (453kb,D)

Title: Towards A Unified View of Answer Calibration for Multi-Step Reasoning
Authors: Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Working in Progress
\\ ( https://arxiv.org/abs/2311.09101 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09106
replaced with revised version Mon, 26 Feb 2024 09:34:42 GMT   (1014kb,D)

Title: "We Demand Justice!": Towards Social Context Grounding of Political
  Texts
Authors: Rajkumar Pujari and Chengfei Wu and Dan Goldwasser
Categories: cs.CL
Comments: Was accepted to and withdrawn from Findings of EMNLP 2023
\\ ( https://arxiv.org/abs/2311.09106 ,  1014kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09114
replaced with revised version Sun, 25 Feb 2024 04:39:07 GMT   (8194kb,D)

Title: Ever: Mitigating Hallucination in Large Language Models through
  Real-Time Verification and Rectification
Authors: Haoqiang Kang, Juntong Ni, Huaxiu Yao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.09114 ,  8194kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12351
replaced with revised version Fri, 23 Feb 2024 19:22:58 GMT   (42120kb,D)

Title: Advancing Transformer Architecture in Long-Context Large Language
  Models: A Comprehensive Survey
Authors: Yunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan
  Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, Shupeng Li, Penghao Zhao
Categories: cs.CL cs.LG
Comments: 40 pages, 3 figures, 4 tables
ACM-class: I.2.7; I.2.6; I.2.11
\\ ( https://arxiv.org/abs/2311.12351 ,  42120kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12882
replaced with revised version Mon, 26 Feb 2024 04:18:12 GMT   (336kb)

Title: LLMs-Healthcare : Current Applications and Challenges of Large Language
  Models in various Medical Specialties
Authors: Ummara Mumtaz, Awais Ahmed, Summaya Mumtaz
Categories: cs.CL
Comments: 26 pages and one figure
\\ ( https://arxiv.org/abs/2311.12882 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14530
replaced with revised version Fri, 23 Feb 2024 20:17:03 GMT   (91kb)

Title: Machine Translation for Ge'ez Language
Authors: Aman Kassahun Wassie
Categories: cs.CL
Comments: 8 pages
\\ ( https://arxiv.org/abs/2311.14530 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10730
replaced with revised version Sun, 25 Feb 2024 07:43:00 GMT   (10038kb,D)

Title: Mixed Distillation Helps Smaller Language Model Better Reasoning
Authors: Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li,
  Zulong Chen, Yin Zhang
Categories: cs.CL cs.AI
Comments: Working in Progress, 17 pages, 16 figures
\\ ( https://arxiv.org/abs/2312.10730 ,  10038kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03321
replaced with revised version Fri, 23 Feb 2024 19:06:35 GMT   (2020kb,D)

Title: PIXAR: Auto-Regressive Language Modeling in Pixel Space
Authors: Yintao Tai, Xiyang Liao, Alessandro Suglia, Antonio Vergari
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.03321 ,  2020kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04348
replaced with revised version Mon, 26 Feb 2024 14:22:43 GMT   (7541kb,D)

Title: LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using
  Adversarial Training
Authors: Khoi M.Le and Trinh Pham and Tho Quan and Anh Tuan Luu
Categories: cs.CL
Comments: First two authors contribute equally. Accepted at AAAI 2024
\\ ( https://arxiv.org/abs/2401.04348 ,  7541kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05273
replaced with revised version Mon, 26 Feb 2024 17:22:21 GMT   (2581kb,D)

Title: INACIA: Integrating Large Language Models in Brazilian Audit Courts:
  Opportunities and Challenges
Authors: Jayr Pereira, Andre Assumpcao, Julio Trecenti, Luiz Airosa, Caio
  Lente, Jhonatan Cl\'eto, Guilherme Dobins, Rodrigo Nogueira, Luis Mitchell,
  Roberto Lotufo
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.05273 ,  2581kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10065
replaced with revised version Sun, 25 Feb 2024 22:59:07 GMT   (9434kb,D)

Title: Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs
Authors: Haritz Puerto, Martin Tutek, Somak Aditya, Xiaodan Zhu, Iryna Gurevych
Categories: cs.CL
Comments: Code, prompt templates, prompts, and outputs are publicly available
  at https://github.com/UKPLab/arxiv2024-conditional-reasoning-llms
\\ ( https://arxiv.org/abs/2401.10065 ,  9434kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14194
replaced with revised version Sun, 25 Feb 2024 03:00:48 GMT   (1162kb,D)

Title: Parameter-Efficient Conversational Recommender System as a Language
  Processing Task
Authors: Mathieu Ravaut, Hao Zhang, Lu Xu, Aixin Sun, Yong Liu
Categories: cs.CL
Comments: 9 pages, 4 figures, 8 tables, EACL 2024 conference, fixed typo
\\ ( https://arxiv.org/abs/2401.14194 ,  1162kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15006
replaced with revised version Mon, 26 Feb 2024 12:17:25 GMT   (1672kb,D)

Title: Airavata: Introducing Hindi Instruction-tuned LLM
Authors: Jay Gala, Thanmay Jayakumar, Jaavid Aktar Husain, Aswanth Kumar M,
  Mohammed Safi Ur Rahman Khan, Diptesh Kanojia, Ratish Puduppully, Mitesh M.
  Khapra, Raj Dabre, Rudra Murthy and Anoop Kunchukuttan
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.15006 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16553
replaced with revised version Fri, 23 Feb 2024 22:28:17 GMT   (10172kb,D)

Title: SelectLLM: Can LLMs Select Important Instructions to Annotate?
Authors: Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang
Categories: cs.CL cs.AI
Comments: First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:
  Jong Inn Park | PI: Dongyeop Kang
\\ ( https://arxiv.org/abs/2401.16553 ,  10172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00263
replaced with revised version Sat, 24 Feb 2024 13:46:21 GMT   (413kb,D)

Title: Does DetectGPT Fully Utilize Perturbation? Bridge Selective Perturbation
  to Fine-tuned Contrastive Learning Detector would be Better
Authors: Shengchao Liu, Xiaoming Liu, Yichen Wang, Zehua Cheng, Chengzhengxu
  Li, Zhaohan Zhang, Yu Lan, Chao Shen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00263 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01383
replaced with revised version Mon, 26 Feb 2024 14:55:48 GMT   (542kb,D)

Title: LLM-based NLG Evaluation: Current Status and Challenges
Authors: Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.01383 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01729
replaced with revised version Sat, 24 Feb 2024 07:01:22 GMT   (8052kb,D)

Title: Contextualization Distillation from Large Language Model for Knowledge
  Graph Completion
Authors: Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024 findings v3: add missing citations
\\ ( https://arxiv.org/abs/2402.01729 ,  8052kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02130
replaced with revised version Mon, 26 Feb 2024 07:33:07 GMT   (12901kb,D)

Title: Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
Authors: Yanbin Wei, Shuai Fu, Weisen Jiang, James T. Kwok, Yu Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.02130 ,  12901kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03780
replaced with revised version Mon, 26 Feb 2024 14:07:20 GMT   (3370kb,D)

Title: Exposing propaganda: an analysis of stylistic cues comparing human
  annotations and machine classification
Authors: G\'eraud Faye, Benjamin Icard, Morgane Casanova, Julien Chanson,
  Fran\c{c}ois Maine, Fran\c{c}ois Bancilhon, Guillaume Gadek, Guillaume
  Gravier, Paul \'Egr\'e
Categories: cs.CL cs.AI cs.LG
Comments: Paper to appear in the EACL 2024 Proceedings of the Third Workshop on
  Understanding Implicit and Underspecified Language (UnImplicit 2024)
\\ ( https://arxiv.org/abs/2402.03780 ,  3370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08277
replaced with revised version Mon, 26 Feb 2024 11:59:28 GMT   (8731kb,D)

Title: Towards Faithful and Robust LLM Specialists for Evidence-Based
  Question-Answering
Authors: Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus
  Leippold
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.08277 ,  8731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08318
replaced with revised version Sun, 25 Feb 2024 09:53:05 GMT   (2564kb,D)

Title: Values That Are Explicitly Present in Fairy Tales: Comparing Samples
  from German, Italian and Portuguese Traditions
Authors: Alba Morollon Diaz-Faes, Carla Sofia Ribeiro Murteira, Martin Ruskov
Categories: cs.CL cs.CY
Comments: In Proceedings of the Joint 3rd International Conference on Natural
  Language Processing for Digital Humanities and 8th International Workshop on
  Computational Linguistics for Uralic Languages
ACM-class: J.5; K.4.m
\\ ( https://arxiv.org/abs/2402.08318 ,  2564kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08479
replaced with revised version Sun, 25 Feb 2024 16:25:45 GMT   (854kb,D)

Title: Plausible Extractive Rationalization through Semi-Supervised Entailment
  Signal
Authors: Wei Jie Yeo, Ranjan Satapathy, Erik Cambria
Categories: cs.CL
Comments: Under review
\\ ( https://arxiv.org/abs/2402.08479 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10588
replaced with revised version Sat, 24 Feb 2024 13:01:34 GMT   (10479kb,D)

Title: Do Llamas Work in English? On the Latent Language of Multilingual
  Transformers
Authors: Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West
Categories: cs.CL cs.CY
Comments: 12 pages. 28 with appendix
\\ ( https://arxiv.org/abs/2402.10588 ,  10479kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10965
replaced with revised version Sat, 24 Feb 2024 13:17:38 GMT   (10408kb,D)

Title: Generalization in Healthcare AI: Evaluation of a Clinical Large Language
  Model
Authors: Salman Rahman, Lavender Yao Jiang, Saadia Gabriel, Yindalon
  Aphinyanaphongs, Eric Karl Oermann and Rumi Chunara
Categories: cs.CL cs.CY cs.LG
\\ ( https://arxiv.org/abs/2402.10965 ,  10408kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11406
replaced with revised version Mon, 26 Feb 2024 16:43:37 GMT   (3041kb,D)

Title: Don't Go To Extremes: Revealing the Excessive Sensitivity and
  Calibration Limitations of LLMs in Implicit Hate Speech Detection
Authors: Min Zhang, Jianfeng He, Taoran Ji, Chang-Tien Lu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.11406 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11863
replaced with revised version Sun, 25 Feb 2024 16:26:18 GMT   (9768kb,D)

Title: How Interpretable are Reasoning Explanations from Prompting Large
  Language Models?
Authors: Wei Jie Yeo, Ranjan Satapathy, Goh Siow Mong, Rick, Erik Cambria
Categories: cs.CL
Comments: Under review
\\ ( https://arxiv.org/abs/2402.11863 ,  9768kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12036
replaced with revised version Mon, 26 Feb 2024 16:47:36 GMT   (7448kb,D)

Title: Language Model Adaptation to Specialized Domains through Selective
  Masking based on Genre and Topical Characteristics
Authors: Anas Belfathi, Ygor Gallina, Nicolas Hernandez, Richard Dufour, Laura
  Monceaux
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12036 ,  7448kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12226
replaced with revised version Mon, 26 Feb 2024 15:24:20 GMT   (2952kb,D)

Title: AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
Authors: Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng
  Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui,
  Tianxiang Sun, Yugang Jiang, Xipeng Qiu
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: 27 pages, 16 figures, under review, work in progress
\\ ( https://arxiv.org/abs/2402.12226 ,  2952kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12554
replaced with revised version Sun, 25 Feb 2024 00:12:38 GMT   (8736kb,D)

Title: Archer: A Human-Labeled Text-to-SQL Dataset with Arithmetic, Commonsense
  and Hypothetical Reasoning
Authors: Danna Zheng, Mirella Lapata, Jeff Z. Pan
Categories: cs.CL
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2402.12554 ,  8736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12948
replaced with revised version Sun, 25 Feb 2024 04:26:13 GMT   (8228kb,D)

Title: GumbelSoft: Diversified Language Model Watermarking via the
  GumbelMax-trick
Authors: Jiayi Fu, Xuandong Zhao, Ruihan Yang, Yuansen Zhang, Jiangjie Chen,
  Yanghua Xiao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12948 ,  8228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13718
replaced with revised version Sat, 24 Feb 2024 15:07:55 GMT   (251kb,D)

Title: $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
Authors: Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and
  Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and
  Zhiyuan Liu and Maosong Sun
Categories: cs.CL
Journal-ref: 2023.12.15ARR
\\ ( https://arxiv.org/abs/2402.13718 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13866
replaced with revised version Sat, 24 Feb 2024 07:03:44 GMT   (635kb,D)

Title: Kuaiji: the First Chinese Accounting Large Language Model
Authors: Jiayuan Luo, Songhua Yang, Xiaoling Qiu, Panyu Chen, Yufei Nai,
  Wenxuan Zeng, Wentao Zhang, Xinke Jiang
Categories: cs.CL cs.AI
Comments: version 2.0
\\ ( https://arxiv.org/abs/2402.13866 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13963
replaced with revised version Mon, 26 Feb 2024 11:01:25 GMT   (5424kb,D)

Title: Towards Building Multilingual Language Model for Medicine
Authors: Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang,
  Ya Zhang, Yanfeng Wang, Weidi Xie
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.13963 ,  5424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14404
replaced with revised version Mon, 26 Feb 2024 11:40:45 GMT   (8428kb,D)

Title: On the Tip of the Tongue: Analyzing Conceptual Representation in Large
  Language Models with Reverse-Dictionary Probe
Authors: Ningyu Xu, Qi Zhang, Menghan Zhang, Peng Qian, Xuanjing Huang
Categories: cs.CL cs.AI
Comments: 21 pages, 13 figures
\\ ( https://arxiv.org/abs/2402.14404 ,  8428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14494
replaced with revised version Sun, 25 Feb 2024 07:08:13 GMT   (2222kb,D)

Title: Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment
  Pre-training for Noisy Slot Filling Task
Authors: Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song,
  Daichi Guo, Weiran Xu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.14494 ,  2222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14873
replaced with revised version Mon, 26 Feb 2024 05:28:41 GMT   (418kb,D)

Title: Technical Report on the Checkfor.ai AI-Generated Text Classifier
Authors: Bradley Emi and Max Spero
Categories: cs.CL cs.AI
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.14873 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14890
replaced with revised version Mon, 26 Feb 2024 12:09:43 GMT   (3334kb,D)

Title: Vygotsky Distance: Measure for Benchmark Task Similarity
Authors: Maxim K. Surkov and Ivan P. Yamshchikov
Categories: cs.CL cs.AI cs.LG
MSC-class: 68T01, 97P80, 97C30, 68Q32
ACM-class: H.1.1; I.2.4; I.2.6; F.2.0
\\ ( https://arxiv.org/abs/2402.14890 ,  3334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14891
replaced with revised version Mon, 26 Feb 2024 06:44:56 GMT   (8216kb,D)

Title: LLMBind: A Unified Modality-Task Integration Framework
Authors: Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang, Qi Song, Junwu
  Zhang, Zhenyu Tang, Mingjun Pan, Xing Zhou, Li Yuan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.14891 ,  8216kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14948
replaced with revised version Mon, 26 Feb 2024 14:59:58 GMT   (10265kb,D)

Title: Re-Examine Distantly Supervised NER: A New Benchmark and a Simple
  Approach
Authors: Yuepei Li, Kang Zhou, Qiao Qiao, Qing Wang and Qi Li
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.14948 ,  10265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15202
replaced with revised version Mon, 26 Feb 2024 02:37:15 GMT   (455kb,D)

Title: Fine-Grained Detoxification via Instance-Level Prefixes for Large
  Language Models
Authors: Xin Yi and Linlin Wang and Xiaoling Wang and Liang He
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.15202 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15313
replaced with revised version Mon, 26 Feb 2024 09:54:47 GMT   (534kb,D)

Title: ArabianGPT: Native Arabic GPT-based Large Language Model
Authors: Anis Koubaa, Adel Ammar, Lahouari Ghouti, Omar Najar, Serry Sibaee
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.15313 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15481
replaced with revised version Mon, 26 Feb 2024 03:55:51 GMT   (8551kb,D)

Title: Prejudice and Caprice: A Statistical Framework for Measuring Social
  Discrimination in Large Language Models
Authors: Yiran Liu (1 and 2), Ke Yang (1 and 3), Zehan Qi (2), Xiao Liu (2),
  Yang Yu (2), Chengxiang Zhai (3) ((1) Equal contributions, (2) Tsinghua
  University, (3) University of Illinois Urbana-Champaign)
Categories: cs.CL cs.CY
\\ ( https://arxiv.org/abs/2402.15481 ,  8551kb)
------------------------------------------------------------------------------
\\
arXiv:1908.08600
replaced with revised version Mon, 26 Feb 2024 00:00:47 GMT   (5149kb,D)

Title: Online Causal Inference for Advertising in Real-Time Bidding Auctions
Authors: Caio Waisman, Harikesh S. Nair, Carlos Carrion
Categories: cs.LG cs.GT econ.EM stat.ML
\\ ( https://arxiv.org/abs/1908.08600 ,  5149kb)
------------------------------------------------------------------------------
\\
arXiv:2007.05943
replaced with revised version Mon, 26 Feb 2024 15:47:20 GMT   (138kb,D)

Title: On the generalization of Tanimoto-type kernels to real valued functions
Authors: Sandor Szedmak (1) Eric Bach (1) ((1) Department of Computer Science,
  Aalto University)
Categories: cs.LG stat.ML
Comments: Pages 12, 3 PDF figures, uses arxiv.sty
\\ ( https://arxiv.org/abs/2007.05943 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2101.05844
replaced with revised version Mon, 26 Feb 2024 18:45:52 GMT   (703kb,D)

Title: Scaling the Convex Barrier with Sparse Dual Algorithms
Authors: Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H.S.
  Torr, M. Pawan Kumar
Categories: cs.LG
Comments: Journal of Machine Learning Research, 2024 (extension of ICLR 2021
  paper in [v1])
\\ ( https://arxiv.org/abs/2101.05844 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2102.11513
replaced with revised version Sat, 24 Feb 2024 15:38:46 GMT   (298kb,D)

Title: Mixed Policy Gradient: off-policy reinforcement learning driven jointly
  by data and model
Authors: Yang Guan, Jingliang Duan, Shengbo Eben Li, Jie Li, Jianyu Chen, Bo
  Cheng
Categories: cs.LG
\\ ( https://arxiv.org/abs/2102.11513 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2106.14077
replaced with revised version Mon, 26 Feb 2024 08:19:20 GMT   (571kb,D)

Title: The Role of Contextual Information in Best Arm Identification
Authors: Masahiro Kato and Kaito Ariu
Categories: cs.LG econ.EM math.ST stat.ME stat.ML stat.TH
\\ ( https://arxiv.org/abs/2106.14077 ,  571kb)
------------------------------------------------------------------------------
\\
arXiv:2111.02168
replaced with revised version Fri, 23 Feb 2024 19:22:23 GMT   (2977kb,D)

Title: The Klarna Product Page Dataset: Web Element Nomination with Graph
  Neural Networks and Large Language Models
Authors: Alexandra Hotti, Riccardo Sven Risuleo, Stefan Magureanu, Aref Moradi,
  Jens Lagergren
Categories: cs.LG cs.CL cs.CV cs.HC cs.IR
Comments: 12 pages, 8 figures, 3 tables, under review
MSC-class: 68T07
\\ ( https://arxiv.org/abs/2111.02168 ,  2977kb)
------------------------------------------------------------------------------
\\
arXiv:2202.07082
replaced with revised version Sun, 25 Feb 2024 01:26:36 GMT   (7098kb,D)

Title: Graph Neural Networks for Graphs with Heterophily: A Survey
Authors: Xin Zheng, Yi Wang, Yixin Liu, Ming Li, Miao Zhang, Di Jin, Philip S.
  Yu, Shirui Pan
Categories: cs.LG
Comments: 22 pages
\\ ( https://arxiv.org/abs/2202.07082 ,  7098kb)
------------------------------------------------------------------------------
\\
arXiv:2202.12797
replaced with revised version Sun, 25 Feb 2024 08:43:57 GMT   (161kb,D)

Title: Learning Dynamic Mechanisms in Unknown Environments: A Reinforcement
  Learning Approach
Authors: Shuang Qiu, Boxiang Lyu, Qinglin Meng, Zhaoran Wang, Zhuoran Yang,
  Michael I. Jordan
Categories: cs.LG cs.GT math.OC stat.ML
Comments: Minor Revision for JMLR. The first three authors contribute equally
\\ ( https://arxiv.org/abs/2202.12797 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2206.00395
replaced with revised version Sat, 24 Feb 2024 21:25:07 GMT   (731kb,D)

Title: Optimization with Access to Auxiliary Information
Authors: El Mahdi Chayti and Sai Praneeth Karimireddy
Categories: cs.LG math.OC
Comments: Published at TMLR
\\ ( https://arxiv.org/abs/2206.00395 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2206.04044
replaced with revised version Mon, 26 Feb 2024 15:18:31 GMT   (56kb)

Title: Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games
Authors: Yuling Yan and Gen Li and Yuxin Chen and Jianqing Fan
Categories: cs.LG cs.GT cs.IT math.IT math.ST stat.ML stat.TH
Comments: accepted to Operations Research
\\ ( https://arxiv.org/abs/2206.04044 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2206.07751
replaced with revised version Mon, 26 Feb 2024 04:10:37 GMT   (217kb,D)

Title: On the Identifiability of Nonlinear ICA: Sparsity and Beyond
Authors: Yujia Zheng, Ignavier Ng, Kun Zhang
Categories: cs.LG cs.AI stat.ML
Comments: NeurIPS 2022
\\ ( https://arxiv.org/abs/2206.07751 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2207.02760
replaced with revised version Sun, 25 Feb 2024 22:30:43 GMT   (5067kb,D)

Title: TREE-G: Decision Trees Contesting Graph Neural Networks
Authors: Maya Bechler-Speicher, Amir Globerson, Ran Gilad-Bachrach
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2207.02760 ,  5067kb)
------------------------------------------------------------------------------
\\
arXiv:2209.08411
replaced with revised version Sat, 24 Feb 2024 17:47:11 GMT   (1958kb,D)

Title: DynaConF: Dynamic Forecasting of Non-Stationary Time Series
Authors: Siqi Liu, Andreas Lehrmann
Categories: cs.LG stat.ML
Comments: Accepted by Transactions on Machine Learning Research (TMLR), 2024
\\ ( https://arxiv.org/abs/2209.08411 ,  1958kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16140
replaced with revised version Mon, 26 Feb 2024 14:20:49 GMT   (2206kb,D)

Title: Localized Randomized Smoothing for Collective Robustness Certification
Authors: Jan Schuchardt, Tom Wollschl\"ager, Aleksandar Bojchevski, Stephan
  G\"unnemann
Categories: cs.LG cs.CV
Comments: Accepted at ICLR 2023
\\ ( https://arxiv.org/abs/2210.16140 ,  2206kb)
------------------------------------------------------------------------------
\\
arXiv:2212.13925
replaced with revised version Mon, 26 Feb 2024 15:15:14 GMT   (867kb,D)

Title: Quality at the Tail of Machine Learning Inference
Authors: Zhengxin Yang and Wanling Gao and Chunjie Luo and Lei Wang and Fei
  Tang and Xu Wen and Jianfeng Zhan
Categories: cs.LG cs.AI cs.CV cs.SE
Comments: 10 pages, 4 figures, 4 tables
\\ ( https://arxiv.org/abs/2212.13925 ,  867kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12334
replaced with revised version Mon, 26 Feb 2024 15:38:28 GMT   (30233kb,D)

Title: Don't Play Favorites: Minority Guidance for Diffusion Models
Authors: Soobin Um, Suhyeon Lee, Jong Chul Ye
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2301.12334 ,  30233kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03660
replaced with revised version Mon, 26 Feb 2024 17:52:00 GMT   (14699kb,D)

Title: Flow Matching on General Geometries
Authors: Ricky T. Q. Chen, Yaron Lipman
Categories: cs.LG cs.AI stat.ML
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2302.03660 ,  14699kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04379
replaced with revised version Mon, 26 Feb 2024 08:28:40 GMT   (2425kb,D)

Title: Et Tu Certifications: Robustness Certificates Yield Better Adversarial
  Examples
Authors: Andrew C. Cullen, Shijie Liu, Paul Montague, Sarah M. Erfani, Benjamin
  I.P. Rubinstein
Categories: cs.LG cs.CR
Comments: 17 pages, 8 figures
ACM-class: I.2.6; I.4.9
\\ ( https://arxiv.org/abs/2302.04379 ,  2425kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06433
replaced with revised version Mon, 26 Feb 2024 03:27:46 GMT   (3592kb,D)

Title: Label-efficient Time Series Representation Learning: A Review
Authors: Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Chee-Keong
  Kwoh, Xiaoli Li
Categories: cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2302.06433 ,  3592kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09357
replaced with revised version Sun, 25 Feb 2024 11:55:12 GMT   (24994kb,D)

Title: Stochastic Online Instrumental Variable Regression: Regrets for
  Endogeneity and Bandit Feedback
Authors: Riccardo Della Vecchia, Debabrota Basu
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2302.09357 ,  24994kb)
------------------------------------------------------------------------------
\\
arXiv:2302.14753
replaced with revised version Sat, 24 Feb 2024 17:17:11 GMT   (130kb,D)

Title: Learning Hidden Markov Models Using Conditional Samples
Authors: Sham M. Kakade, Akshay Krishnamurthy, Gaurav Mahajan, Cyril Zhang
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2302.14753 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05445
replaced with revised version Sun, 25 Feb 2024 12:05:43 GMT   (2449kb,D)

Title: Flooding with Absorption: An Efficient Protocol for Heterogeneous
  Bandits over Complex Networks
Authors: Junghyun Lee, Laura Schmid, Se-Young Yun
Categories: cs.LG cs.DC cs.NI stat.ML
Comments: 25 pages, 6 figures. Accepted to the 27th International Conference on
  Principles of Distributed Systems (OPODIS 2023) - Best Student Paper
DOI: 10.4230/LIPIcs.OPODIS.2023.20
\\ ( https://arxiv.org/abs/2303.05445 ,  2449kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06965
replaced with revised version Mon, 26 Feb 2024 14:13:28 GMT   (18603kb,D)

Title: Bridging the Gap between Chemical Reaction Pretraining and Conditional
  Molecule Generation with a Unified Model
Authors: Bo Qiang, Yiran Zhou, Yuheng Ding, Ningfeng Liu, Song Song, Liangren
  Zhang, Bo Huang, Zhenming Liu
Categories: cs.LG q-bio.BM
DOI: 10.1038/s42256-023-00764-9
\\ ( https://arxiv.org/abs/2303.06965 ,  18603kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03431
replaced with revised version Sun, 25 Feb 2024 03:23:31 GMT   (10371kb,D)

Title: Domain Generalization In Robust Invariant Representation
Authors: Gauri Gupta, Ritvik Kapila, Keshav Gupta, Ramesh Raskar
Categories: cs.LG cs.AI
Comments: 7 pages, 5 figures, ICLR 2023 workshop
\\ ( https://arxiv.org/abs/2304.03431 ,  10371kb)
------------------------------------------------------------------------------
\\
arXiv:2304.04326
replaced with revised version Sat, 24 Feb 2024 17:20:11 GMT   (4222kb,D)

Title: Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation
  for Decentralized Learning
Authors: Deepak Ravikumar, Gobinda Saha, Sai Aparna Aketi, Kaushik Roy
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2304.04326 ,  4222kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11854
replaced with revised version Sun, 25 Feb 2024 16:21:00 GMT   (1376kb,D)

Title: Multimodal Web Navigation with Instruction-Finetuned Foundation Models
Authors: Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra
  Faust, Shixiang Shane Gu, Izzeddin Gur
Categories: cs.LG cs.AI stat.ML
Comments: Accepted to ICLR 2024. Website:
  https://sites.google.com/view/mm-webnav/
\\ ( https://arxiv.org/abs/2305.11854 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15196
replaced with revised version Sun, 25 Feb 2024 11:37:19 GMT   (6255kb,D)

Title: Feature-aligned N-BEATS with Sinkhorn divergence
Authors: Joonhun Lee, Myeongho Jeon, Myungjoo Kang, Kyunghyun Park
Categories: cs.LG cs.AI math.OC math.PR
Comments: Spotlight at ICLR 2024
\\ ( https://arxiv.org/abs/2305.15196 ,  6255kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00742
replaced with revised version Mon, 26 Feb 2024 09:02:54 GMT   (3788kb,D)

Title: The Galerkin method beats Graph-Based Approaches for Spectral Algorithms
Authors: Vivien Cabannes, Francis Bach
Categories: cs.LG cs.AI stat.ML
Journal-ref: AISTATS 2024
\\ ( https://arxiv.org/abs/2306.00742 ,  3788kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02031
replaced with revised version Sun, 25 Feb 2024 06:59:50 GMT   (2607kb,D)

Title: DOS: Diverse Outlier Sampling for Out-of-Distribution Detection
Authors: Wenyu Jiang, Hao Cheng, Mingcai Chen, Chongjun Wang, Hongxin Wei
Categories: cs.LG
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2306.02031 ,  2607kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03346
replaced with revised version Mon, 26 Feb 2024 02:25:12 GMT   (8076kb,D)

Title: Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from
  Offline Data
Authors: Chongyi Zheng, Benjamin Eysenbach, Homer Walke, Patrick Yin, Kuan
  Fang, Ruslan Salakhutdinov, Sergey Levine
Categories: cs.LG cs.AI
Comments: ICLR 2024 Spotlight (< 5%). Website
  (https://chongyi-zheng.github.io/stable_contrastive_rl) and code
  (https://github.com/chongyi-zheng/stable_contrastive_rl)
\\ ( https://arxiv.org/abs/2306.03346 ,  8076kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03530
replaced with revised version Sun, 25 Feb 2024 21:13:07 GMT   (3458kb,D)

Title: RLtools: A Fast, Portable Deep Reinforcement Learning Library for
  Continuous Control
Authors: Jonas Eschmann, Dario Albani, Giuseppe Loianno
Categories: cs.LG cs.AI cs.RO
Comments: Project page: https://rl.tools
\\ ( https://arxiv.org/abs/2306.03530 ,  3458kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09222
replaced with revised version Mon, 26 Feb 2024 06:22:52 GMT   (566kb,D)

Title: Stochastic Re-weighted Gradient Descent via Distributionally Robust
  Optimization
Authors: Ramnath Kumar and Kushal Majmundar and Dheeraj Nagaraj and Arun Sai
  Suggala
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.09222 ,  566kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10698
replaced with revised version Sun, 25 Feb 2024 03:49:54 GMT   (465kb,D)

Title: Deep Reinforcement Learning with Task-Adaptive Retrieval via
  Hypernetwork
Authors: Yonggang Jin, Chenxu Wang, Tianyu Zheng, Liuyu Xiang, Yaodong Yang,
  Junge Zhang, Jie Fu, Zhaofeng He
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.10698 ,  465kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16504
replaced with revised version Sun, 25 Feb 2024 18:07:24 GMT   (212kb,D)

Title: Momentum Benefits Non-IID Federated Learning Simply and Provably
Authors: Ziheng Cheng, Xinmeng Huang, Kun Yuan
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2306.16504 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00897
replaced with revised version Mon, 26 Feb 2024 10:34:10 GMT   (20753kb,D)

Title: Fixing confirmation bias in feature attribution methods via semantic
  match
Authors: Giovanni Cin\`a, Daniel Fernandez-Llaneza, Ludovico Deponte, Nishant
  Mishra, Tabea E. R\"ober, Sandro Pezzelle, Iacer Calixto, Rob Goedhart,
  \c{S}. \.Ilker Birbil
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2307.00897 ,  20753kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02031
replaced with revised version Sat, 24 Feb 2024 08:12:53 GMT   (5529kb,D)

Title: Improving Automatic Parallel Training via Balanced Memory Workload
  Optimization
Authors: Yujie Wang, Youhe Jiang, Xupeng Miao, Fangcheng Fu, Shenhan Zhu,
  Xiaonan Nie, Yaofeng Tu, Bin Cui
Categories: cs.LG cs.DB cs.DC
Comments: arXiv admin note: substantial text overlap with arXiv:2211.13878
\\ ( https://arxiv.org/abs/2307.02031 ,  5529kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04661
replaced with revised version Fri, 23 Feb 2024 22:05:34 GMT   (26kb)

Title: On the power of graph neural networks and the role of the activation
  function
Authors: Sammy Khalife, Amitabh Basu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.04661 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05551
replaced with revised version Sat, 24 Feb 2024 18:30:22 GMT   (25150kb,D)

Title: Graph Neural Networks as an Enabler of Terahertz-based Flow-guided
  Nanoscale Localization over Highly Erroneous Raw Data
Authors: Gerard Calvo Bartra, Filip Lemic, Guillem Pascual, Aina P\'erez Rodas,
  Jakob Struye, Carmen Delgado, Xavier Costa P\'erez
Categories: cs.LG cs.ET cs.NI
Comments: 16 pages, 16 figures, 6 tables, 45 references
\\ ( https://arxiv.org/abs/2307.05551 ,  25150kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12856
replaced with revised version Sun, 25 Feb 2024 16:17:43 GMT   (1696kb,D)

Title: A Real-World WebAgent with Planning, Long Context Understanding, and
  Program Synthesis
Authors: Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka
  Matsuo, Douglas Eck, Aleksandra Faust
Categories: cs.LG cs.AI cs.CL
Comments: Accepted to ICLR 2024 (Oral)
\\ ( https://arxiv.org/abs/2307.12856 ,  1696kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02514
replaced with revised version Sat, 24 Feb 2024 16:48:42 GMT   (2006kb,D)

Title: Language models as master equation solvers
Authors: Chuanbo Liu and Jin Wang
Categories: cs.LG cs.AI math.DS
Comments: To be published in Proceedings of the National Academy of Sciences
\\ ( https://arxiv.org/abs/2308.02514 ,  2006kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07728
replaced with revised version Mon, 26 Feb 2024 03:56:15 GMT   (18174kb,D)

Title: Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability
Authors: Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2308.07728 ,  18174kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08079
replaced with revised version Fri, 23 Feb 2024 20:31:50 GMT   (2444kb)

Title: Rigid Transformations for Stabilized Lower Dimensional Space to Support
  Subsurface Uncertainty Quantification and Interpretation
Authors: Ademide O. Mabadeje and Michael J. Pyrcz
Categories: cs.LG
Comments: 30 pages, 17 figures
\\ ( https://arxiv.org/abs/2308.08079 ,  2444kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10457
replaced with revised version Sun, 25 Feb 2024 06:56:16 GMT   (2458kb,D)

Title: ALI-DPFL: Differentially Private Federated Learning with Adaptive Local
  Iterations
Authors: Xinpeng Ling, Jie Fu, Kuncan Wang, Haitao Liu, Zhili Chen
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2308.10457 ,  2458kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11464
replaced with revised version Mon, 26 Feb 2024 09:48:00 GMT   (5975kb,D)

Title: Internal Cross-layer Gradients for Extending Homogeneity to
  Heterogeneity in Federated Learning
Authors: Yun-Hin Chan, Rui Zhou, Running Zhao, Zhihan Jiang, Edith C.-H. Ngai
Categories: cs.LG cs.AI
Comments: ICLR 2024. 29 pages. Source codes:
  https://github.com/ChanYunHin/InCo-Aggregation
\\ ( https://arxiv.org/abs/2308.11464 ,  5975kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11721
replaced with revised version Mon, 26 Feb 2024 14:04:14 GMT   (327kb,D)

Title: When Are Two Lists Better than One?: Benefits and Harms in Joint
  Decision-making
Authors: Kate Donahue, Sreenivas Gollapudi, Kostas Kollias
Categories: cs.LG cs.CY cs.HC
\\ ( https://arxiv.org/abs/2308.11721 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11838
replaced with revised version Mon, 26 Feb 2024 00:02:22 GMT   (4988kb,D)

Title: A Benchmark Study on Calibration
Authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu
Categories: cs.LG cs.AI stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2308.11838 ,  4988kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04332
replaced with revised version Sun, 25 Feb 2024 22:48:34 GMT   (3737kb,D)

Title: Graph Neural Networks Use Graphs When They Shouldn't
Authors: Maya Bechler-Speicher, Ido Amos, Ran Gilad-Bachrach, Amir Globerson
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.04332 ,  3737kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04877
replaced with revised version Mon, 26 Feb 2024 05:05:42 GMT   (93kb,D)

Title: A Gentle Introduction to Gradient-Based Optimization and Variational
  Inequalities for Machine Learning
Authors: Neha S. Wadia, Yatin Dandi, and Michael I. Jordan
Categories: cs.LG stat.ML
Comments: 36 pages, 7 figures; minor corrections
\\ ( https://arxiv.org/abs/2309.04877 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09752
replaced with revised version Mon, 26 Feb 2024 10:22:04 GMT   (3180kb,D)

Title: Contrastive Initial State Buffer for Reinforcement Learning
Authors: Nico Messikommer, Yunlong Song, Davide Scaramuzza
Categories: cs.LG
Journal-ref: IEEE Conference on Robotics and Automation (ICRA 2024)
\\ ( https://arxiv.org/abs/2309.09752 ,  3180kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16354
replaced with revised version Sun, 25 Feb 2024 09:48:53 GMT   (1383kb,D)

Title: Transformer-VQ: Linear-Time Transformers via Vector Quantization
Authors: Lucas D. Lingle
Categories: cs.LG cs.CL cs.CV
Comments: ICLR 2024 camera-ready
\\ ( https://arxiv.org/abs/2309.16354 ,  1383kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00386
replaced with revised version Sun, 25 Feb 2024 16:02:56 GMT   (3725kb,D)

Title: Order-Preserving GFlowNets
Authors: Yihang Chen, Lukas Mauch
Categories: cs.LG cs.AI stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.00386 ,  3725kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00797
replaced with revised version Mon, 26 Feb 2024 10:02:07 GMT   (6083kb,D)

Title: Don't Miss Out on Novelty: Importance of Novel Features for Deep Anomaly
  Detection
Authors: Sarath Sivaprasad and Mario Fritz
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.00797 ,  6083kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04361
replaced with revised version Mon, 26 Feb 2024 11:46:27 GMT   (1717kb,D)

Title: SADMoE: Exploiting Activation Sparsity with Dynamic-k Gating
Authors: Filip Szatkowski, Bartosz W\'ojcik, Miko{\l}aj Pi\'orczy\'nski, Kamil
  Adamczewski
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.04361 ,  1717kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04395
replaced with revised version Mon, 26 Feb 2024 13:45:35 GMT   (3547kb,D)

Title: Leveraging Self-Consistency for Data-Efficient Amortized Bayesian
  Inference
Authors: Marvin Schmitt, Desi R. Ivanova, Daniel Habermann, Ullrich K\"othe,
  Paul-Christian B\"urkner, Stefan T. Radev
Categories: cs.LG cs.AI
Comments: previously published as an extended abstract at NeurIPS UniReps 2023
\\ ( https://arxiv.org/abs/2310.04395 ,  3547kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04668
replaced with revised version Sat, 24 Feb 2024 06:44:45 GMT   (928kb,D)

Title: Label-free Node Classification on Graphs with Large Language Models
  (LLMS)
Authors: Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang
  Zhang, Hui Liu, Jiliang Tang
Categories: cs.LG cs.AI cs.CL
Comments: The code is available via https://github.com/CurryTang/LLMGNN; ICLR
  2024
\\ ( https://arxiv.org/abs/2310.04668 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06286
replaced with revised version Mon, 26 Feb 2024 07:20:34 GMT   (2644kb,D)

Title: Suppressing Overestimation in Q-Learning through Adversarial Behaviors
Authors: HyeAnn Lee, Donghwan Lee
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.06286 ,  2644kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08337
replaced with revised version Mon, 26 Feb 2024 10:24:52 GMT   (17055kb,D)

Title: Neural Diffusion Models
Authors: Grigory Bartosh, Dmitry Vetrov, Christian A. Naesseth
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.08337 ,  17055kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10117
replaced with revised version Sat, 24 Feb 2024 07:12:23 GMT   (229kb,D)

Title: Federated Learning with Convex Global and Local Constraints
Authors: Chuan He, Le Peng, Ju Sun
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2310.10117 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11762
replaced with revised version Mon, 26 Feb 2024 15:38:25 GMT   (1892kb,D)

Title: A Quasi-Wasserstein Loss for Learning Graph Neural Networks
Authors: Minjie Cheng and Hongteng Xu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.11762 ,  1892kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12809
replaced with revised version Mon, 26 Feb 2024 09:06:16 GMT   (52kb,D)

Title: Hierarchical Forecasting at Scale
Authors: Olivier Sprangers, Wander Wadman, Sebastian Schelter, Maarten de Rijke
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.12809 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12934
replaced with revised version Sun, 25 Feb 2024 19:39:24 GMT   (3196kb,D)

Title: Generative Flow Networks as Entropy-Regularized RL
Authors: Daniil Tiapkin, Nikita Morozov, Alexey Naumov, Dmitry Vetrov
Categories: cs.LG stat.ML
Comments: AISTATS 2024 (Oral)
\\ ( https://arxiv.org/abs/2310.12934 ,  3196kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14053
replaced with revised version Mon, 26 Feb 2024 18:56:08 GMT   (3387kb,D)

Title: Beyond Accuracy: Evaluating Self-Consistency of Code Large Language
  Models with IdentityChain
Authors: Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail
  Kaiser, Suman Jana, Baishakhi Ray
Categories: cs.LG cs.CL cs.SE
Comments: ICLR 2024
MSC-class: 68
ACM-class: I.2; D.2
\\ ( https://arxiv.org/abs/2310.14053 ,  3387kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16592
replaced with revised version Mon, 26 Feb 2024 01:52:00 GMT   (1059kb,D)

Title: Over-the-air Federated Policy Gradient
Authors: Huiwen Yang, Lingying Huang, Subhrakanti Dey, Ling Shi
Categories: cs.LG cs.DC eess.SP
Comments: To appear at IEEE ICC 2024
\\ ( https://arxiv.org/abs/2310.16592 ,  1059kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17653
replaced with revised version Mon, 26 Feb 2024 18:58:43 GMT   (3759kb,D)

Title: Fantastic Gains and Where to Find Them: On the Existence and Prospect of
  General Knowledge Transfer between Any Pretrained Model
Authors: Karsten Roth, Lukas Thede, Almut Sophia Koepke, Oriol Vinyals, Olivier
  H\'enaff, Zeynep Akata
Categories: cs.LG cs.CV
Comments: ICLR 2024 (spotlight)
\\ ( https://arxiv.org/abs/2310.17653 ,  3759kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18285
replaced with revised version Sun, 25 Feb 2024 02:00:07 GMT   (11568kb,D)

Title: Unlocking the Potential of Prompt-Tuning in Bridging Generalized and
  Personalized Federated Learning
Authors: Wenlong Deng, Christos Thrampoulidis, Xiaoxiao Li
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2310.18285 ,  11568kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18629
replaced with revised version Mon, 26 Feb 2024 08:34:40 GMT   (1123kb)

Title: Explainable Modeling for Wind Power Forecasting: A Glass-Box Approach
  with High Accuracy
Authors: Wenlong Liao, Fernando Porte-Agel, Jiannong Fang, Birgitte Bak-Jensen,
  Guangchun Ruan, Zhe Yang
Categories: cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2310.18629 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18884
replaced with revised version Sat, 24 Feb 2024 18:08:28 GMT   (3120kb,D)

Title: Simple and Asymmetric Graph Contrastive Learning without Augmentations
Authors: Teng Xiao, Huaisheng Zhu, Zhengyu Chen, Suhang Wang
Categories: cs.LG stat.ML
Comments: NeurIPS 2023 Main Track
\\ ( https://arxiv.org/abs/2310.18884 ,  3120kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19253
replaced with revised version Sat, 24 Feb 2024 23:20:28 GMT   (2189kb,D)

Title: Flow-based Distributionally Robust Optimization
Authors: Chen Xu, Jonghyeok Lee, Xiuyuan Cheng, Yao Xie
Categories: cs.LG stat.ME stat.ML
Comments: IEEE Journal on Selected Areas in Information Theory (JSAIT).
  Accepted. 2024
\\ ( https://arxiv.org/abs/2310.19253 ,  2189kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20141
replaced with revised version Mon, 26 Feb 2024 01:50:57 GMT   (1899kb,D)

Title: Contrastive Difference Predictive Coding
Authors: Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach
Categories: cs.LG cs.AI
Comments: ICLR 2024. Website (https://chongyi-zheng.github.io/td_infonce) and
  code (https://github.com/chongyi-zheng/td_infonce)
\\ ( https://arxiv.org/abs/2310.20141 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03732
replaced with revised version Mon, 26 Feb 2024 05:59:34 GMT   (1162kb,D)

Title: Learning to Learn for Few-shot Continual Active Learning
Authors: Stella Ho, Ming Liu, Shang Gao, Longxiang Gao
Categories: cs.LG cs.CL
Comments: This is the pre-print version which has not been fully undergone peer
  review
\\ ( https://arxiv.org/abs/2311.03732 ,  1162kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10671
replaced with revised version Mon, 26 Feb 2024 13:38:36 GMT   (8314kb,D)

Title: Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based
  Inference
Authors: Marvin Schmitt, Stefan T. Radev, Paul-Christian B\"urkner
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.10671 ,  8314kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11463
replaced with revised version Mon, 26 Feb 2024 07:51:44 GMT   (2080kb,D)

Title: Designing monitoring strategies for deployed machine learning
  algorithms: navigating performativity through a causal lens
Authors: Jean Feng, Adarsh Subbaswamy, Alexej Gossmann, Harvineet Singh,
  Berkman Sahiner, Mi-Ok Kim, Gene Pennello, Nicholas Petrick, Romain
  Pirracchio, Fan Xia
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.11463 ,  2080kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13541
replaced with revised version Mon, 26 Feb 2024 08:40:50 GMT   (2908kb,D)

Title: Linear Log-Normal Attention with Unbiased Concentration
Authors: Yury Nahshan, Joseph Kampeas and Emir Haleva
Categories: cs.LG cs.AI
Comments: 22 pages, 20 figures, 5 tables, submitted to ICLR2024
ACM-class: I.7.0; G.3
\\ ( https://arxiv.org/abs/2311.13541 ,  2908kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15156
replaced with revised version Sat, 24 Feb 2024 13:03:49 GMT   (9521kb,D)

Title: xTrimoGene: An Efficient and Scalable Representation Learner for
  Single-Cell RNA-Seq Data
Authors: Jing Gong, Minsheng Hao, Xingyi Cheng, Xin Zeng, Chiming Liu, Jianzhu
  Ma, Xuegong Zhang, Taifeng Wang, Le Song
Categories: cs.LG cs.AI q-bio.GN
Comments: Accepted by NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.15156 ,  9521kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02554
replaced with revised version Mon, 26 Feb 2024 08:51:03 GMT   (104kb,D)

Title: ULMA: Unified Language Model Alignment with Human Demonstration and
  Point-wise Preference
Authors: Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, Guannan
  Zhang
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2312.02554 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02592
replaced with revised version Mon, 26 Feb 2024 17:58:42 GMT   (684kb,D)

Title: FRAPP\'E: A Group Fairness Framework for Post-Processing Everything
Authors: Alexandru \c{T}ifrea, Preethi Lahoti, Ben Packer, Yoni Halpern, Ahmad
  Beirami and Flavien Prost
Categories: cs.LG cs.CY
Comments: Presubmission
\\ ( https://arxiv.org/abs/2312.02592 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07424
replaced with revised version Sun, 25 Feb 2024 08:10:18 GMT   (14389kb,D)

Title: How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary
  Investigation
Authors: Zhongyi Han, Guanglin Zhou, Rundong He, Jindong Wang, Tailin Wu,
  Yilong Yin, Salman Khan, Lina Yao, Tongliang Liu, Kun Zhang
Categories: cs.LG cs.AI cs.CV
Comments: added the investigation of Gemini. 66 pages, 41 figures
\\ ( https://arxiv.org/abs/2312.07424 ,  14389kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08656
replaced with revised version Mon, 26 Feb 2024 02:09:37 GMT   (3415kb,D)

Title: MaxK-GNN: Extremely Fast GPU Kernel Design for Accelerating Graph Neural
  Networks Training
Authors: Hongwu Peng, Xi Xie, Kaustubh Shivdikar, MD Amit Hasan, Jiahui Zhao,
  Shaoyi Huang, Omer Khan, David Kaeli, Caiwen Ding
Categories: cs.LG cs.AI cs.DC
Comments: ASPLOS 2024 accepted publication
ACM-class: I.2; C.5
\\ ( https://arxiv.org/abs/2312.08656 ,  3415kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08846
replaced with revised version Sat, 24 Feb 2024 03:30:56 GMT   (8358kb,D)

Title: TiMix: Text-aware Image Mixing for Effective Vision-Language
  Pre-training
Authors: Chaoya Jiang, Wei ye, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang,
  Shikun Zhang
Categories: cs.LG cs.CL cs.CV
Comments: Accepted on AAAI2024
\\ ( https://arxiv.org/abs/2312.08846 ,  8358kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10584
replaced with revised version Sun, 25 Feb 2024 19:15:26 GMT   (256kb,D)

Title: Policy Optimization in RLHF: The Impact of Out-of-preference Data
Authors: Ziniu Li, Tian Xu, Yang Yu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.10584 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16424
replaced with revised version Mon, 26 Feb 2024 14:29:39 GMT   (2237kb,D)

Title: Soft Contrastive Learning for Time Series
Authors: Seunghan Lee, Taeyoung Park, Kibok Lee
Categories: cs.LG cs.AI stat.ML
Comments: NeurIPS Workshop on Self-Supervised Learning: Theory and Practice,
  2023
\\ ( https://arxiv.org/abs/2312.16424 ,  2237kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16427
replaced with revised version Mon, 26 Feb 2024 14:24:42 GMT   (1115kb,D)

Title: Learning to Embed Time Series Patches Independently
Authors: Seunghan Lee, Taeyoung Park, Kibok Lee
Categories: cs.LG cs.AI stat.ML
Comments: NeurIPS Workshop on Self-Supervised Learning: Theory and Practice,
  2023. Oral presentation
\\ ( https://arxiv.org/abs/2312.16427 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04081
replaced with revised version Mon, 26 Feb 2024 17:04:41 GMT   (1823kb,D)

Title: MoE-Mamba: Efficient Selective State Space Models with Mixture of
  Experts
Authors: Maciej Pi\'oro, Kamil Ciebiera, Krystian Kr\'ol, Jan Ludziejewski,
  Micha{\l} Krutul, Jakub Krajewski, Szymon Antoniak, Piotr Mi{\l}o\'s, Marek
  Cygan, Sebastian Jaszczur
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.04081 ,  1823kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05653
replaced with revised version Mon, 26 Feb 2024 15:02:17 GMT   (519kb)

Title: Quantifying Marketing Performance at Channel-Partner Level by Using
  Marketing Mix Modeling (MMM) and Shapley Value Regression
Authors: Sean Tang, Sriya Musunuru, Baoshi Zong, Brooks Thornton
Categories: cs.LG
Comments: Corrected typos
\\ ( https://arxiv.org/abs/2401.05653 ,  519kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06091
replaced with revised version Mon, 26 Feb 2024 00:13:22 GMT   (528kb,D)

Title: A Closer Look at AUROC and AUPRC under Class Imbalance
Authors: Matthew B. A. McDermott (1), Lasse Hyldig Hansen (2), Haoran Zhang
  (3), Giovanni Angelotti (4), Jack Gallifant (3) ((1) Harvard Medical School,
  (2) Aarhus University, (3) Massachusetts Institute of Technology, (4) IRCCS
  Humanitas Research Hospital)
Categories: cs.LG stat.ME
\\ ( https://arxiv.org/abs/2401.06091 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07958
replaced with revised version Mon, 26 Feb 2024 16:21:55 GMT   (7452kb,D)

Title: GD-CAF: Graph Dual-stream Convolutional Attention Fusion for
  Precipitation Nowcasting
Authors: Lorand Vatamany, Siamak Mehrkanoon
Categories: cs.LG cs.CV
Comments: 19 pages, 13 figures
ACM-class: I.2; I.5
\\ ( https://arxiv.org/abs/2401.07958 ,  7452kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11626
replaced with revised version Sat, 24 Feb 2024 12:11:56 GMT   (659kb,D)

Title: Freely Long-Thinking Transformer (FraiLT)
Authors: Akbay Tabak
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2401.11626 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11648
replaced with revised version Sat, 24 Feb 2024 16:08:22 GMT   (6826kb,D)

Title: Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal
  Contrastive EHR Modelling with Hierarchical Regularisation
Authors: Heejoon Koo
Categories: cs.LG cs.AI cs.IR
Comments: Accepted to EACL 2024 (The 18th Conference of the European Chapter of
  the Association for Computational Linguistics)
\\ ( https://arxiv.org/abs/2401.11648 ,  6826kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14211
replaced with revised version Sun, 25 Feb 2024 20:03:17 GMT   (478kb,D)

Title: Communication-Efficient Federated Learning through Adaptive Weight
  Clustering and Server-Side Distillation
Authors: Vasileios Tsouvalas, Aaqib Saeed, Tanir Ozcelebi and Nirvana Meratnia
Categories: cs.LG cs.DC
Comments: 9 pages, 2 figures, Accepted on ICASSP 2024
\\ ( https://arxiv.org/abs/2401.14211 ,  478kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14591
replaced with revised version Mon, 26 Feb 2024 15:29:55 GMT   (10152kb,D)

Title: Ricci flow-guided autoencoders in learning time-dependent dynamics
Authors: Andrew Gracyk
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.14591 ,  10152kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15207
replaced with revised version Sun, 25 Feb 2024 18:33:50 GMT   (9708kb,D)

Title: HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy
Authors: Yongkang Liu, Yiqun Zhang, Qian Li, Tong Liu, Shi Feng, Daling Wang,
  Yifei Zhang and Hinrich Sch\"utze
Categories: cs.LG cs.CL
Comments: under progress
\\ ( https://arxiv.org/abs/2401.15207 ,  9708kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00397
replaced with revised version Mon, 26 Feb 2024 12:55:02 GMT   (3662kb,D)

Title: Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic
  Forecasting
Authors: Zhanyu Liu, Guanjie Zheng, Yanwei Yu
Categories: cs.LG cs.AI
Comments: Under review. Text overlap with arXiv:2308.09727
\\ ( https://arxiv.org/abs/2402.00397 ,  3662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01231
replaced with revised version Mon, 26 Feb 2024 02:01:38 GMT   (2326kb,D)

Title: Unveiling Delay Effects in Traffic Forecasting: A Perspective from
  Spatial-Temporal Delay Differential Equations
Authors: Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang,
  Yuanchun Zhou
Categories: cs.LG
Comments: 11 pages, 7 figures
\\ ( https://arxiv.org/abs/2402.01231 ,  2326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02229
replaced with revised version Sun, 25 Feb 2024 09:31:57 GMT   (7705kb,D)

Title: Vanilla Bayesian Optimization Performs Great in High Dimensions
Authors: Carl Hvarfner and Erik Orm Hellsten and Luigi Nardi
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.02229 ,  7705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04284
replaced with revised version Mon, 26 Feb 2024 09:23:12 GMT   (1160kb,D)

Title: PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks
Authors: Junwei Su, Difan Zou, Chuan Wu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.04284 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04830
replaced with revised version Mon, 26 Feb 2024 11:04:03 GMT   (167kb,D)

Title: Closing the Gap Between SGP4 and High-Precision Propagation via
  Differentiable Programming
Authors: Giacomo Acciarini, At{\i}l{\i}m G\"une\c{s} Baydin, Dario Izzo
Categories: cs.LG astro-ph.EP
\\ ( https://arxiv.org/abs/2402.04830 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04875
replaced with revised version Sat, 24 Feb 2024 15:28:51 GMT   (313kb,D)

Title: On Provable Length and Compositional Generalization
Authors: Kartik Ahuja, Amin Mansouri
Categories: cs.LG cs.CL stat.ML
\\ ( https://arxiv.org/abs/2402.04875 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07281
replaced with revised version Mon, 26 Feb 2024 04:56:07 GMT   (6879kb,D)

Title: Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A
  Benchmarking Study
Authors: Santonu Sarkar, Shanay Mehta, Nicole Fernandes, Jyotirmoy Sarkar and
  Snehanshu Saha
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.07281 ,  6879kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07876
replaced with revised version Sun, 25 Feb 2024 16:47:01 GMT   (13524kb,D)

Title: Policy Improvement using Language Feedback Models
Authors: Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre C\^ot\'e
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.07876 ,  13524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08595
replaced with revised version Sat, 24 Feb 2024 00:31:40 GMT   (143kb,D)

Title: Homomorphism Counts for Graph Neural Networks: All About That Basis
Authors: Emily Jin, Michael Bronstein, Ismail Ilkan Ceylan, Matthias Lanzinger
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.08595 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09456
replaced with revised version Sun, 25 Feb 2024 04:48:38 GMT   (1865kb,D)

Title: Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
Authors: Yingru Li, Liangqi Liu, Wenqiang Pu, Hao Liang, Zhi-Quan Luo
Categories: cs.LG cs.AI cs.GT stat.ML
\\ ( https://arxiv.org/abs/2402.09456 ,  1865kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09963
replaced with revised version Sun, 25 Feb 2024 08:13:21 GMT   (1571kb,D)

Title: Why are Sensitive Functions Hard for Transformers?
Authors: Michael Hahn, Mark Rofin
Categories: cs.LG
Comments: Fixed typos
\\ ( https://arxiv.org/abs/2402.09963 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10207
replaced with revised version Sun, 25 Feb 2024 03:38:48 GMT   (1125kb,D)

Title: Rewards-in-Context: Multi-objective Alignment of Foundation Models with
  Dynamic Preference Adjustment
Authors: Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu,
  Jianshu Chen
Categories: cs.LG cs.AI cs.CL
Comments: 20 pages, 12 figures, 6 tables
\\ ( https://arxiv.org/abs/2402.10207 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10802
replaced with revised version Mon, 26 Feb 2024 14:13:52 GMT   (2821kb,D)

Title: TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly
  Detection Models
Authors: Haotian Si, Changhua Pei, Hang Cui, Jingwen Yang, Yongqian Sun,
  Shenglin Zhang, Jingjing Li, Haiming Zhang, Jing Han, Dan Pei, Jianhui Li,
  Gaogang Xie
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.10802 ,  2821kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11404
replaced with revised version Fri, 23 Feb 2024 20:59:54 GMT   (1818kb)

Title: Evaluating the Stability of Deep Learning Latent Feature Spaces
Authors: Ademide O. Mabadeje and Michael J. Pyrcz
Categories: cs.LG
Comments: 30 pages, 11 figures, submitted to Journal
\\ ( https://arxiv.org/abs/2402.11404 ,  1818kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11592
replaced with revised version Mon, 26 Feb 2024 07:42:22 GMT   (206kb,D)

Title: Revisiting Zeroth-Order Optimization for Memory-Efficient LLM
  Fine-Tuning: A Benchmark
Authors: Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang,
  Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang
  Wang, Sijia Liu, Tianlong Chen
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.11592 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11793
replaced with revised version Fri, 23 Feb 2024 22:13:53 GMT   (7448kb,D)

Title: Generative Kaleidoscopic Networks
Authors: Harsh Shrivastava
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.11793 ,  7448kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11942
replaced with revised version Sun, 25 Feb 2024 14:46:07 GMT   (10428kb,D)

Title: The effect of Leaky ReLUs on the training and generalization of
  overparameterized networks
Authors: Yinglong Guo, Shaohan Li, Gilad Lerman
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11942 ,  10428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12177
replaced with revised version Mon, 26 Feb 2024 11:54:12 GMT   (44kb)

Title: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning
Authors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.12177 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12537
replaced with revised version Sun, 25 Feb 2024 20:32:50 GMT   (2061kb,D)

Title: Hierarchical Bayes Approach to Personalized Federated Unsupervised
  Learning
Authors: Kaan Ozkara, Bruce Huang, Ruida Zhou, Suhas Diggavi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.12537 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12656
replaced with revised version Sun, 25 Feb 2024 02:34:41 GMT   (819kb)

Title: HyperMoE: Paying Attention to Unselected Experts in Mixture of Experts
  via Dynamic Transfer
Authors: Hao Zhao, Zihan Qiu, Huijia Wu, Zili Wang, Zhaofeng He, Jie Fu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.12656 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13777
replaced with revised version Mon, 26 Feb 2024 03:11:41 GMT   (361kb,D)

Title: Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
  and Perspectives on Future Directions
Authors: Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet
  Aggarwal
Categories: cs.LG cs.AI
Comments: Added more insights on future directions
\\ ( https://arxiv.org/abs/2402.13777 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14169
replaced with revised version Mon, 26 Feb 2024 17:15:58 GMT   (4111kb,D)

Title: A Temporal Bias Correction using a Machine Learning Attention model
Authors: Omer Nivron, Damon J. Wischik, Mathieu Vrac
Categories: cs.LG physics.ao-ph
Comments: 19 pages, 15 figures
\\ ( https://arxiv.org/abs/2402.14169 ,  4111kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14332
replaced with revised version Mon, 26 Feb 2024 02:11:10 GMT   (6301kb,D)

Title: From Large to Small Datasets: Size Generalization for Clustering
  Algorithm Selection
Authors: Vaggos Chatziafratis, Ishani Karmarkar, and Ellen Vitercik
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.14332 ,  6301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14609
replaced with revised version Mon, 26 Feb 2024 02:15:24 GMT   (463kb,D)

Title: FedCQA: Answering Complex Queries on Multi-Source Knowledge Graphs via
  Federated Learning
Authors: Qi Hu, Weifeng Jiang, Haoran Li, Zihao Wang, Jiaxin Bai, Qianren Mao,
  Yangqiu Song, Lixin Fan, Jianxin Li
Categories: cs.LG cs.AI cs.CR cs.DB
\\ ( https://arxiv.org/abs/2402.14609 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14740
replaced with revised version Mon, 26 Feb 2024 18:26:25 GMT   (205kb,D)

Title: Back to Basics: Revisiting REINFORCE Style Optimization for Learning
  from Human Feedback in LLMs
Authors: Arash Ahmadian, Chris Cremer, Matthias Gall\'e, Marzieh Fadaee, Julia
  Kreutzer, Olivier Pietquin, Ahmet \"Ust\"un, Sara Hooker
Categories: cs.LG
Comments: 27 pages, 7 figures, 2 tables
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.14740 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15175
replaced with revised version Mon, 26 Feb 2024 02:49:16 GMT   (271kb,D)

Title: Unified View of Grokking, Double Descent and Emergent Abilities: A
  Perspective from Circuits Competition
Authors: Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun
Categories: cs.LG
Comments: 13 pages, 10 figures
\\ ( https://arxiv.org/abs/2402.15175 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2102.01295
replaced with revised version Mon, 26 Feb 2024 10:09:46 GMT   (2559kb,D)

Title: Gaze-based dual resolution deep imitation learning for high-precision
  dexterous robot manipulation
Authors: Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi
Categories: cs.RO cs.AI
Comments: 8 pages. The supplementary video can be found at:
  https://www.youtube.com/watch?v=ytpChcFqD5g Published in IEEE Robotics and
  Automation Letters. Replaced to add video url in the manuscript
Journal-ref: IEEE Robotics and Automation Letters, Vol. 6, No. 2, 2021
DOI: 10.1109/LRA.2021.3059619
\\ ( https://arxiv.org/abs/2102.01295 ,  2559kb)
------------------------------------------------------------------------------
\\
arXiv:2108.00385
replaced with revised version Mon, 26 Feb 2024 10:02:26 GMT   (3783kb,D)

Title: Transformer-based deep imitation learning for dual-arm robot
  manipulation
Authors: Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
Categories: cs.RO cs.AI
Comments: 8 pages. Accepted in 2021 IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)
\\ ( https://arxiv.org/abs/2108.00385 ,  3783kb)
------------------------------------------------------------------------------
\\
arXiv:2109.12965
replaced with revised version Sun, 25 Feb 2024 10:17:11 GMT   (12923kb,D)

Title: Text-based Person Search in Full Images via Semantic-Driven Proposal
  Generation
Authors: Shizhou Zhang, De Cheng, Wenlong Luo, Yinghui Xing, Duo Long, Hao Li,
  Kai Niu, Guoqiang Liang, Yanning Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2109.12965 ,  12923kb)
------------------------------------------------------------------------------
\\
arXiv:2202.09574
replaced with revised version Mon, 26 Feb 2024 10:27:33 GMT   (4276kb,D)

Title: Training Robots without Robots: Deep Imitation Learning for
  Master-to-Robot Policy Transfer
Authors: Heecheol Kim, Yoshiyuki Ohmura, Akihiko Nagakubo, and Yasuo Kuniyoshi
Categories: cs.RO cs.AI
Comments: 8 pages
Journal-ref: IEEE Robotics and Automation Letters 8.5 (2023): 2906-2913
\\ ( https://arxiv.org/abs/2202.09574 ,  4276kb)
------------------------------------------------------------------------------
\\
arXiv:2208.10256
replaced with revised version Sat, 24 Feb 2024 21:54:59 GMT   (0kb,I)

Title: Information-Theoretic Equivalence of Entropic Multi-Marginal Optimal
  Transport: A Theory for Multi-Agent Communication
Authors: Shuchan Wang
Categories: cs.IT cs.AI cs.RO math.IT
Comments: The assumption at the beginning of the main results that "X^n is
  i.i.d. if and only if it is in the typical set" is a huge mistake. This makes
  the subsequent proofs invalid. This is corrected in a recent paper motivated
  in a quantum setting
\\ ( https://arxiv.org/abs/2208.10256 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12491
replaced with revised version Sun, 25 Feb 2024 17:25:16 GMT   (12383kb,D)

Title: Joint Learning of Blind Super-Resolution and Crack Segmentation for
  Realistic Degraded Images
Authors: Yuki Kondo and Norimichi Ukita
Categories: cs.CV cs.AI eess.IV
Comments: Accepted to IEEE Transactions on Instrumentation and Measurement
  (TIM) 2024. The project page is located at
  https://yuki-11.github.io/CSBSR-project-page/
\\ ( https://arxiv.org/abs/2302.12491 ,  12383kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13991
replaced with revised version Sun, 25 Feb 2024 16:24:36 GMT   (993kb,D)

Title: Learning to Generalize towards Unseen Domains via a Content-Aware Style
  Invariant Model for Disease Detection from Chest X-rays
Authors: Mohammad Zunaed, Md. Aynal Haque, Taufiq Hasan
Categories: cs.CV cs.AI cs.LG eess.IV
\\ ( https://arxiv.org/abs/2302.13991 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2302.14702 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 21:54:33 GMT   (14463kb,D)

Title: Performance Limits of a Deep Learning-Enabled Text Semantic
  Communication under Interference
Authors: Tilahun M. Getu, Walid Saad, Georges Kaddoum, and Mehdi Bennis
Categories: eess.SP cs.AI cs.IT math.IT
\\ ( https://arxiv.org/abs/2302.14702 ,  14463kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12446
replaced with revised version Sat, 24 Feb 2024 07:47:49 GMT   (226kb)

Title: Externalities in Chore Division
Authors: Mohammad Azharuddin Sanpui
Categories: cs.GT cs.AI cs.MA
\\ ( https://arxiv.org/abs/2303.12446 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06607
replaced with revised version Fri, 23 Feb 2024 19:31:20 GMT   (2520kb,D)

Title: False Claims against Model Ownership Resolution
Authors: Jian Liu, Rui Zhang, Sebastian Szyller, Kui Ren, N.Asokan
Categories: cs.CR cs.AI
Comments: 13pages,3 figures. To appear in the 33rd USENIX Security Symposium
  (USENIX Security '24)
\\ ( https://arxiv.org/abs/2304.06607 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02759
replaced with revised version Sun, 25 Feb 2024 05:53:34 GMT   (387kb,D)

Title: Disentangled Contrastive Collaborative Filtering
Authors: Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin and Chao Huang
Categories: cs.IR cs.AI
Comments: Published as a SIGIR'23 full paper
DOI: 10.1145/3539618.3591665
\\ ( https://arxiv.org/abs/2305.02759 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15596
replaced with revised version Sat, 24 Feb 2024 01:57:58 GMT   (3862kb,D)

Title: Distributed Online Rollout for Multivehicle Routing in Unmapped
  Environments
Authors: Jamison W. Weber, Dhanush R. Giriyan, Devendra R. Parkar, Dimitri P.
  Bertsekas, Andr\'ea W. Richa
Categories: cs.DC cs.AI cs.MA
\\ ( https://arxiv.org/abs/2305.15596 ,  3862kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16203
replaced with revised version Sat, 24 Feb 2024 13:31:10 GMT   (882kb,D)

Title: On Computing Universal Plans for Partially Observable Multi-Agent Path
  Finding
Authors: Fengming Zhu, Fangzhen Lin
Categories: cs.MA cs.AI
\\ ( https://arxiv.org/abs/2305.16203 ,  882kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00014
replaced with revised version Sun, 25 Feb 2024 16:24:09 GMT   (11307kb,D)

Title: Inertial Navigation Meets Deep Learning: A Survey of Current Trends and
  Future Directions
Authors: Nadav Cohen and Itzik Klein
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2307.00014 ,  11307kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09913
replaced with revised version Sun, 25 Feb 2024 10:07:36 GMT   (657kb,D)

Title: Exploring Non-Regular Extensions of Propositional Dynamic Logic with
  Description-Logics Features
Authors: Bartosz Bednarczyk
Categories: cs.LO cs.AI
Comments: Full version of our JELIA 2023 paper, accepted for publication to
  Logical Methods in Computer Science
\\ ( https://arxiv.org/abs/2307.09913 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02041
replaced with revised version Sun, 25 Feb 2024 19:27:28 GMT   (569kb)

Title: Regulating AI: Applying insights from behavioural economics and
  psychology to the application of article 5 of the EU AI Act
Authors: Huixin Zhong, Eamonn O'Neill, Janina A. Hoffmann
Categories: cs.CY cs.AI
Comments: This paper was accepted for publication by AAAI 2024 paper on
  December of 2023
\\ ( https://arxiv.org/abs/2308.02041 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06013
replaced with revised version Sun, 25 Feb 2024 23:06:28 GMT   (1682kb,D)

Title: Large Language Models for Telecom: Forthcoming Impact on the Industry
Authors: Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico,
  Merouane Debbah
Categories: cs.IT cs.AI cs.LG math.IT
\\ ( https://arxiv.org/abs/2308.06013 ,  1682kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09238
replaced with revised version Mon, 26 Feb 2024 08:54:21 GMT   (21408kb,D)

Title: Improving Buoy Detection with Deep Transfer Learning for Mussel Farm
  Automation
Authors: Carl McMillan, Junhong Zhao, Bing Xue, Ross Vennell, Mengjie Zhang
Categories: cs.CV cs.AI
Comments: 6 pages, 5 figures, presented at 2023 38th International Conference
  on Image and Vision Computing New Zealand (IVCNZ)
Journal-ref: IVCNZ 2023
DOI: 10.1109/IVCNZ61134.2023.10343657
\\ ( https://arxiv.org/abs/2308.09238 ,  21408kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14296
replaced with revised version Fri, 23 Feb 2024 21:05:48 GMT   (1598kb,D)

Title: RecMind: Large Language Model Powered Agent For Recommendation
Authors: Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah
  Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2308.14296 ,  1598kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09737
replaced with revised version Sat, 24 Feb 2024 17:55:55 GMT   (8577kb,D)

Title: RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud
Authors: Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: Accepted to ICRA 2024. 8 pages, 4 figures. Co-first authorship for
  Zhijun Pan, Fangqiang Ding and Hantao Zhong, listed randomly
\\ ( https://arxiv.org/abs/2309.09737 ,  8577kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12368
replaced with revised version Mon, 26 Feb 2024 14:57:06 GMT   (1102kb,D)

Title: Rethinking Human-AI Collaboration in Complex Medical Decision Making: A
  Case Study in Sepsis Diagnosis
Authors: Shao Zhang, Jianing Yu, Xuhai Xu, Changchang Yin, Yuxuan Lu, Bingsheng
  Yao, Melanie Tory, Lace M. Padilla, Jeffrey Caterino, Ping Zhang, Dakuo Wang
Categories: cs.HC cs.AI cs.LG
Comments: Accepted by CHI'24
MSC-class: 68U35
ACM-class: H.5.2; I.2.1
DOI: 10.1145/3613904.3642343
\\ ( https://arxiv.org/abs/2309.12368 ,  1102kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13672
replaced with revised version Sat, 24 Feb 2024 16:00:59 GMT   (39144kb,D)

Title: Image-to-Image Translation with Deep Reinforcement Learning
Authors: Xin Wang, Ziwei Luo, Jing Hu, Chengming Feng, Shu Hu, Bin Zhu, Xi Wu,
  Xin Li, Siwei Lyu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2309.13672 ,  39144kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17264
replaced with revised version Sun, 25 Feb 2024 08:46:10 GMT   (5261kb,D)

Title: A Foundation Model for General Moving Object Segmentation in Medical
  Images
Authors: Zhongnuo Yan, Tong Han, Yuhao Huang, Lian Liu, Han Zhou, Jiongquan
  Chen, Wenlong Shi, Yan Cao, Xin Yang, Dong Ni
Categories: cs.CV cs.AI cs.LG
Comments: 5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI
  2024
\\ ( https://arxiv.org/abs/2309.17264 ,  5261kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01107
replaced with revised version Sat, 24 Feb 2024 19:39:37 GMT   (49600kb,D)

Title: Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image
  Diffusion Models
Authors: Hyeonho Jeong and Jong Chul Ye
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: Accepted to ICLR 2024, Project Page: http://ground-a-video.github.io
\\ ( https://arxiv.org/abs/2310.01107 ,  49600kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02687
replaced with revised version Mon, 26 Feb 2024 14:59:54 GMT   (46428kb,D)

Title: USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields
Authors: Moyang Li, Peng Wang, Lingzhe Zhao, Bangyan Liao and Peidong Liu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.02687 ,  46428kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15950
replaced with revised version Sun, 25 Feb 2024 05:44:27 GMT   (867kb,D)

Title: Representation Learning with Large Language Models for Recommendation
Authors: Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang,
  Dawei Yin, Chao Huang
Categories: cs.IR cs.AI
Comments: Published as a WWW'24 full paper
DOI: 10.1145/3589334.3645458
\\ ( https://arxiv.org/abs/2310.15950 ,  867kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11157
replaced with revised version Mon, 26 Feb 2024 07:28:48 GMT   (10193kb,D)

Title: Contextualizing Internet Memes Across Social Media Platforms
Authors: Saurav Joshi, Filip Ilievski, Luca Luceri
Categories: cs.SI cs.AI cs.IR
Comments: 10 pages, 7 figures, 2 tables
\\ ( https://arxiv.org/abs/2311.11157 ,  10193kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13226
replaced with revised version Mon, 26 Feb 2024 14:02:09 GMT   (1687kb,D)

Title: Robot at the Mirror: Learning to Imitate via Associating Self-supervised
  Models
Authors: Andrej Lucny, Kristina Malinovska, and Igor Farkas
Categories: cs.RO cs.AI
Comments: This work was funded (or co-funded) by the Horizon-Widera-2021
  European Twinning project TERAIS G.A. n. 101079338, 32nd International
  Conference on Artificial Neural Networks, Heraklion, Greece, September 26-29,
  2023, citations:
  https://link.springer.com/chapter/10.1007/978-3-031-44207-0_39, codes:
  https://github.com/andylucny/learningImitation/tree/main/mirror, 12 pages, 3
  figures, 0 tables
ACM-class: I.2.9
Journal-ref: Artificial Neural Networks and Machine Learning - ICANN 2023
  https://link.springer.com/chapter/10.1007/978-3-031-44207-0_39 pages 471-482
DOI: 10.1007/978-3-031-44207-0_39
\\ ( https://arxiv.org/abs/2311.13226 ,  1687kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14153
replaced with revised version Mon, 26 Feb 2024 16:10:00 GMT   (3373kb,D)

Title: Tube-NeRF: Efficient Imitation Learning of Visuomotor Policies from MPC
  using Tube-Guided Data Augmentation and NeRFs
Authors: Andrea Tagliabue, Jonathan P. How
Categories: cs.RO cs.AI cs.LG
Comments: Video: https://youtu.be/_W5z33ZK1m4. Evolved paper from our previous
  work: arXiv:2210.10127
\\ ( https://arxiv.org/abs/2311.14153 ,  3373kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12430
replaced with revised version Sun, 25 Feb 2024 21:51:21 GMT   (913kb,D)

Title: Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP
Authors: Ziyi Chen, Jize Jiang, Daqian Zuo, Heyi Tao, Jun Yang, Yuxiang Wei
Categories: cs.IR cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2312.12430 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16815 (*cross-listing*)
replaced with revised version Sun, 25 Feb 2024 15:15:22 GMT   (5711kb,D)

Title: Emergence and Causality in Complex Systems: A Survey on Causal Emergence
  and Related Quantitative Studies
Authors: Bing Yuan, Zhang Jiang, Aobo Lyu, Jiayun Wu, Zhipeng Wang, Mingzhe
  Yang, Kaiwei Liu, Muyun Mou, Peng Cui
Categories: physics.soc-ph cs.AI nlin.AO
Comments: 57 pages, 17 figures, 1 table
MSC-class: 68P30
ACM-class: K.3.2
\\ ( https://arxiv.org/abs/2312.16815 ,  5711kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00110
replaced with revised version Mon, 26 Feb 2024 06:58:12 GMT   (11502kb,D)

Title: Diffusion Model with Perceptual Loss
Authors: Shanchuan Lin, Xiao Yang
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.00110 ,  11502kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02873 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 22:51:53 GMT   (556kb,D)

Title: Optimal Chaining of Vehicle Plans with Time Windows
Authors: David Fiedler, Fabio V. Difonzo and Jan Mrkos
Categories: math.OC cs.AI
Comments: 26 pages, 7 figures, submitted to "Transportation Research Part C:
  Emerging Technologies" journal
\\ ( https://arxiv.org/abs/2401.02873 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05200
replaced with revised version Mon, 26 Feb 2024 12:46:37 GMT   (833kb,D)

Title: Knowledge Sharing in Manufacturing using Large Language Models: User
  Evaluation and Model Benchmarking
Authors: Samuel Kernan Freire, Chaofan Wang, Mina Foosherian, Stefan Wellsandt,
  Santiago Ruiz-Arenas and Evangelos Niforatos
Categories: cs.HC cs.AI cs.IR
Comments: 11 pages, 3 figures, and 1 table. Under review
\\ ( https://arxiv.org/abs/2401.05200 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06137
replaced with revised version Mon, 26 Feb 2024 13:10:22 GMT   (770kb,D)

Title: QuasiNet: a neural network with trainable product layers
Authors: Krist\'ina Malinovsk\'a, Slavom\'ir Holenda and \v{L}udov\'it
  Malinovsk\'y
Categories: cs.NE cs.AI cs.LG
Comments: This work was funded by the Horizon-Widera-2021 European Twinning
  project TERAIS G.A. n. 101079338. Presented at International Conference on
  Artificial Neural Networks (ICANN) 2023. Accepted:1.7.2023
  Published:26.9.2023. Code: https://doi.org/10.5281/zenodo.10702248
ACM-class: I.2.0
DOI: 10.1007/978-3-031-44192-9_32
\\ ( https://arxiv.org/abs/2401.06137 ,  770kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11314
replaced with revised version Sun, 25 Feb 2024 22:47:24 GMT   (3138kb,D)

Title: CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming
  Assistant that Balances Student and Educator Needs
Authors: Majeed Kazemitabaar, Runlong Ye, Xiaoning Wang, Austin Z. Henley, Paul
  Denny, Michelle Craig, Tovi Grossman
Categories: cs.HC cs.AI
Comments: CHI 2024 Paper - The paper includes 17 pages, 8 figures, 2 tables,
  along with a 2-page appendix
DOI: 10.1145/3613904.3642773
\\ ( https://arxiv.org/abs/2401.11314 ,  3138kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14142
replaced with revised version Mon, 26 Feb 2024 11:33:48 GMT   (5229kb,D)

Title: Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept
  Intervention, and Probabilistic Interpretations
Authors: Xinyue Xu, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2401.14142 ,  5229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03246
replaced with revised version Sun, 25 Feb 2024 17:44:22 GMT   (35899kb,D)

Title: SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
Authors: Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.03246 ,  35899kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04880
replaced with revised version Fri, 23 Feb 2024 22:17:22 GMT   (3985kb,D)

Title: Combining Cloud and Mobile Computing for Machine Learning
Authors: Ruiqi Xu and Tianchi Zhang
Categories: cs.DC cs.AI cs.LG
Comments: Ruiqi Xu and Tianchi Zhang contributed equally to this work
\\ ( https://arxiv.org/abs/2402.04880 ,  3985kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05493
replaced with revised version Mon, 26 Feb 2024 04:49:53 GMT   (2198kb,D)

Title: Investigating White-Box Attacks for On-Device Models
Authors: Mingyi Zhou, Xiang Gao, Jing Wu, Kui Liu, Hailong Sun, Li Li
Categories: cs.SE cs.AI cs.CR
Comments: Published in The International Conference on Software Engineering
  2024 (ICSE'24)
\\ ( https://arxiv.org/abs/2402.05493 ,  2198kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05902
replaced with revised version Sun, 25 Feb 2024 03:25:50 GMT   (423kb)

Title: ClickSAM: Fine-tuning Segment Anything Model using click prompts for
  ultrasound image segmentation
Authors: Aimee Guo, Grace Fei, Hemanth Pasupuleti, and Jing Wang
Categories: cs.CV cs.AI physics.med-ph
Comments: 5 pages, 2 figures, SPIE Medical Imaging Conference 2024. Project
  page: https://sites.google.com/view/clicksam/home
\\ ( https://arxiv.org/abs/2402.05902 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08582
replaced with revised version Mon, 26 Feb 2024 14:15:50 GMT   (3490kb,D)

Title: FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing
  Medical Image Analysis
Authors: Charulkumar Chodvadiya, Navyansh Mahla, Kinshuk Gaurav Singh, Kshitij
  Sharad Jadhav
Categories: cs.CV cs.AI
Comments: 5 Pages, 3 figures
\\ ( https://arxiv.org/abs/2402.08582 ,  3490kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08983
replaced with revised version Sat, 24 Feb 2024 07:17:16 GMT   (9634kb,D)

Title: SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware
  Decoding
Authors: Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin,
  Radha Poovendran
Categories: cs.CR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.08983 ,  9634kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10002
replaced with revised version Sun, 25 Feb 2024 07:58:07 GMT   (4186kb,D)

Title: MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D
  Point Cloud Understanding
Authors: Hai-Tao Yu, Mofei Song
Categories: cs.CV cs.AI cs.MM
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2402.10002 ,  4186kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10340
replaced with revised version Sat, 24 Feb 2024 20:34:35 GMT   (5798kb,D)

Title: On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting
  the Risks and Vulnerabilities
Authors: Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty,
  Fuxiao Liu, Brian Sadler, Dinesh Manocha, Amrit Singh Bedi
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2402.10340 ,  5798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11588
replaced with revised version Sat, 24 Feb 2024 07:24:09 GMT   (1540kb,D)

Title: SDiT: Spiking Diffusion Model with Transformer
Authors: Shu Yang, Hanzhi Ma, Chengting Yu, Aili Wang, Er-Ping Li
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.11588 ,  1540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11871
replaced with revised version Fri, 23 Feb 2024 19:54:55 GMT   (13550kb,D)

Title: From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions,
  and Models for Planning from Raw Data
Authors: Naman Shah, Jayesh Nagpal, Pulkit Verma, Siddharth Srivastava
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2402.11871 ,  13550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12168
replaced with revised version Sun, 25 Feb 2024 04:21:01 GMT   (6525kb,D)

Title: Defending Against Weight-Poisoning Backdoor Attacks for
  Parameter-Efficient Fine-Tuning
Authors: Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi
  Jia, Jinming Wen
Categories: cs.CR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.12168 ,  6525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13929
replaced with revised version Mon, 26 Feb 2024 06:51:39 GMT   (16120kb,D)

Title: SDXL-Lightning: Progressive Adversarial Diffusion Distillation
Authors: Shanchuan Lin, Anran Wang, Xiao Yang
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.13929 ,  16120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14174
replaced with revised version Fri, 23 Feb 2024 22:53:50 GMT   (23875kb,D)

Title: Blending Data-Driven Priors in Dynamic Games
Authors: Justin Lidard, Haimin Hu, Asher Hancock, Zixu Zhang, Albert Gim\'o
  Contreras, Vikash Modi, Jonathan DeCastro, Deepak Gopinath, Guy Rosman, Naomi
  Leonard, Mar\'ia Santos, Jaime Fern\'andez Fisac
Categories: cs.RO cs.AI cs.SY eess.SY math.OC
Comments: 19 pages, 11 figures
\\ ( https://arxiv.org/abs/2402.14174 ,  23875kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14418
replaced with revised version Sat, 24 Feb 2024 12:30:40 GMT   (1128kb,D)

Title: Uncertainty-Aware Evaluation for Vision-Language Models
Authors: Vasily Kostumov, Bulat Nutfullin, Oleg Pilipenko, Eugene Ilyushin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.14418 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14807
replaced with revised version Fri, 23 Feb 2024 22:17:10 GMT   (809kb,D)

Title: A Decision-Language Model (DLM) for Dynamic Restless Multi-Armed Bandit
  Tasks in Public Health
Authors: Nikhil Behari, Edwin Zhang, Yunfan Zhao, Aparna Taneja, Dheeraj
  Nagaraj, Milind Tambe
Categories: cs.MA cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.14807 ,  809kb)
------------------------------------------------------------------------------
\\
arXiv:2209.05946
replaced with revised version Sun, 25 Feb 2024 23:39:50 GMT   (21013kb,D)

Title: OmDet: Large-scale vision-language multi-dataset pre-training with
  multimodal detection network
Authors: Tiancheng Zhao, Peng Liu and Kyusong Lee
Categories: cs.CV cs.CL
Comments: Published at IET CV
DOI: 10.1049/cvi2.12268
\\ ( https://arxiv.org/abs/2209.05946 ,  21013kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04941 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 10:55:22 GMT   (2521kb,D)

Title: Integrating large language models and active inference to understand eye
  movements in reading and dyslexia
Authors: Francesco Donnarumma, Mirco Frosolone and Giovanni Pezzulo
Categories: q-bio.NC cs.CL
Comments: 23 pages, 1 Appendix, 11 Tables, 9 Figures
\\ ( https://arxiv.org/abs/2308.04941 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07968
replaced with revised version Sun, 25 Feb 2024 04:47:50 GMT   (3490kb,D)

Title: Think, Act, and Ask: Open-World Interactive Personalized Robot
  Navigation
Authors: Yinpei Dai, Run Peng, Sikai Li, Joyce Chai
Categories: cs.RO cs.CL cs.HC
Comments: Video URL: https://www.youtube.com/watch?v=QW6rMHVpxUY Code URL:
  https://github.com/sled-group/navchat
\\ ( https://arxiv.org/abs/2310.07968 ,  3490kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09266
replaced with revised version Fri, 23 Feb 2024 20:25:17 GMT   (5792kb,D)

Title: User Inference Attacks on Large Language Models
Authors: Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz,
  Christopher A. Choquette-Choo, Zheng Xu
Categories: cs.CR cs.CL cs.LG
Comments: v2 contains experiments on additional datasets and differential
  privacy
\\ ( https://arxiv.org/abs/2310.09266 ,  5792kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02084
replaced with revised version Mon, 26 Feb 2024 16:28:13 GMT   (5121kb,D)

Title: ITEm: Unsupervised Image-Text Embedding Learning for eCommerce
Authors: Baohao Liao, Michael Kozielski, Sanjika Hewavitharana, Jiangbo Yuan,
  Shahram Khadivi, Tomer Lancewicki
Categories: cs.CV cs.CL cs.IR
\\ ( https://arxiv.org/abs/2311.02084 ,  5121kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01921
replaced with revised version Sat, 24 Feb 2024 13:58:26 GMT   (5890kb,D)

Title: A Machine Learning Approach Towards SKILL Code Autocompletion
Authors: Enrique Dehaerne, Bappaditya Dey, Wannes Meert
Categories: cs.SE cs.CL cs.PL
Comments: Accepted for SPIE Advanced Lithography + Patterning, 2024
ACM-class: I.2.2
\\ ( https://arxiv.org/abs/2312.01921 ,  5890kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07529
replaced with revised version Mon, 26 Feb 2024 09:28:34 GMT   (3074kb,D)

Title: MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of
  Multimodal Large Language Models in Perception
Authors: Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, Yanfeng
  Wang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2401.07529 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:1910.10596 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 23:32:32 GMT   (1180kb,D)

Title: Sparse Orthogonal Variational Inference for Gaussian Processes
Authors: Jiaxin Shi, Michalis K. Titsias, Andriy Mnih
Categories: stat.ML cs.LG
Comments: AISTATS 2020
\\ ( https://arxiv.org/abs/1910.10596 ,  1180kb)
------------------------------------------------------------------------------
\\
arXiv:2006.10125
replaced with revised version Mon, 26 Feb 2024 02:04:04 GMT   (0kb,I)

Title: Sustainable Recreational Fishing Using a Novel Electrical Muscle
  Stimulation (EMS) Lure and Ensemble Network Algorithm to Maximize Catch and
  Release Survivability
Authors: Petteri Haverinen, Krithik Ramesh, Nathan Wang
Categories: cs.CV cs.LG
Comments: This was a high school hackathon project that although interesting,
  lacks sufficient rigor and data as a research paper
\\ ( https://arxiv.org/abs/2006.10125 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10303 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 23:13:34 GMT   (1938kb,D)

Title: On the representation and learning of monotone triangular transport maps
Authors: Ricardo Baptista, Youssef Marzouk, Olivier Zahm
Categories: stat.ML cs.LG math.FA stat.CO stat.ME
Comments: 40 pages, 9 figures, 3 tables
Journal-ref: Foundations of Computational Mathematics, 2023
DOI: 10.1007/s10208-023-09630-x
\\ ( https://arxiv.org/abs/2009.10303 ,  1938kb)
------------------------------------------------------------------------------
\\
arXiv:2102.06448
replaced with revised version Sun, 25 Feb 2024 09:04:32 GMT   (3516kb,D)

Title: The MSR-Video to Text Dataset with Clean Annotations
Authors: Haoran Chen, Jianmin Li, Simone Frintrop, Xiaolin Hu
Categories: cs.CV cs.LG
Comments: The paper is under consideration at Computer Vision and Image
  Understanding
MSC-class: 68T45, 68T50
ACM-class: I.2.10; I.2.7
Journal-ref: Computer Vision and Image Understanding, 225, p.103581 (2022)
DOI: 10.1016/j.cviu.2022.103581
\\ ( https://arxiv.org/abs/2102.06448 ,  3516kb)
------------------------------------------------------------------------------
\\
arXiv:2103.07020 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 01:28:20 GMT   (1149kb)

Title: Max-Linear Regression by Convex Programming
Authors: Seonho Kim, Sohail Bahmani, and Kiryung Lee
Categories: stat.ML cs.IT cs.LG math.IT math.ST stat.TH
\\ ( https://arxiv.org/abs/2103.07020 ,  1149kb)
------------------------------------------------------------------------------
\\
arXiv:2105.03425 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 18:36:46 GMT   (2485kb,D)

Title: Kernel Two-Sample Tests for Manifold Data
Authors: Xiuyuan Cheng, Yao Xie
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2105.03425 ,  2485kb)
------------------------------------------------------------------------------
\\
arXiv:2203.03179 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 13:08:03 GMT   (603kb,D)

Title: Detecting data-driven robust statistical arbitrage strategies with deep
  neural networks
Authors: Ariel Neufeld, Julian Sester, Daiying Yin
Categories: q-fin.CP cs.LG q-fin.MF q-fin.ST q-fin.TR
\\ ( https://arxiv.org/abs/2203.03179 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2204.11602
replaced with revised version Sat, 24 Feb 2024 07:00:04 GMT   (9949kb,D)

Title: Broad Recommender System: An Efficient Nonlinear Collaborative Filtering
  Approach
Authors: Ling Huang, Can-Rong Guan, Zhen-Wei Huang, Yuefang Gao, Yingjie Kuang,
  Chang-Dong Wang, C. L. Philip Chen
Categories: cs.IR cs.LG
\\ ( https://arxiv.org/abs/2204.11602 ,  9949kb)
------------------------------------------------------------------------------
\\
arXiv:2211.08405 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 11:08:24 GMT   (1372kb,D)

Title: Multimodal Generative Models for Bankruptcy Prediction Using Textual
  Data
Authors: Rogelio A. Mancisidor and Kjersti Aas
Categories: q-fin.RM cs.LG stat.ML
\\ ( https://arxiv.org/abs/2211.08405 ,  1372kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09887 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 09:33:10 GMT   (1438kb,D)

Title: Spherical convolutional neural networks can improve brain microstructure
  estimation from diffusion MRI data
Authors: Leevi Kerkel\"a, Kiran Seunarine, Filip Szczepankiewicz, and Chris A.
  Clark
Categories: eess.IV cs.LG physics.med-ph
\\ ( https://arxiv.org/abs/2211.09887 ,  1438kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14396
replaced with revised version Mon, 26 Feb 2024 17:08:10 GMT   (931kb)

Title: Non-invasive Liver Fibrosis Screening on CT Images using Radiomics
Authors: Jay J. Yoo, Khashayar Namdar, Sean Carey, Sandra E. Fischer, Chris
  McIntosh, Farzad Khalvati and Patrik Rogalla
Categories: cs.CV cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2211.14396 ,  931kb)
------------------------------------------------------------------------------
\\
arXiv:2301.05872 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 01:26:05 GMT   (902kb,D)

Title: CEDAS: A Compressed Decentralized Stochastic Gradient Method with
  Improved Convergence
Authors: Kun Huang and Shi Pu
Categories: math.OC cs.DC cs.LG cs.MA
Comments: 16 pages, 8 figures
\\ ( https://arxiv.org/abs/2301.05872 ,  902kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00105 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 17:24:29 GMT   (8854kb,D)

Title: Fourier series weight in quantum machine learning
Authors: Parfait Atchade-Adelomou and Kent Larson
Categories: quant-ph cs.LG
Comments: 11 pages, 14 figures and 3 tables
\\ ( https://arxiv.org/abs/2302.00105 ,  8854kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04658 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 19:35:40 GMT   (45kb)

Title: The Sample Complexity of Approximate Rejection Sampling with
  Applications to Smoothed Online Learning
Authors: Adam Block and Yury Polyanskiy
Categories: stat.ML cs.LG
Comments: Corrected mistake in proof of Lemma 27 from the COLT 2023 version of
  this paper
\\ ( https://arxiv.org/abs/2302.04658 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15702
replaced with revised version Sun, 25 Feb 2024 08:12:26 GMT   (3499kb)

Title: Distributed Graph Embedding with Information-Oriented Random Walks
Authors: Peng Fang, Arijit Khan, Siqiang Luo, Fang Wang, Dan Feng, Zhenli Li,
  Wei Yin, Yuchao Cao
Categories: cs.DC cs.LG
Journal-ref: 49th International Conference on Very Large Data Bases (VLDB
  2023), Vancouver, Canada - August 28 to September 1, 2023
\\ ( https://arxiv.org/abs/2303.15702 ,  3499kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07472 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 20:47:43 GMT   (30475kb,D)

Title: Efficient Convex Algorithms for Universal Kernel Learning
Authors: Aleksandr Talitckii and Brendon K. Colbert and Matthew M. Peet
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2304.07472 ,  30475kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04847 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 15:27:26 GMT   (5679kb,D)

Title: CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter
  Simulation
Authors: Erik Buhmann, Sascha Diefenbacher, Engin Eren, Frank Gaede, Gregor
  Kasieczka, Anatolii Korol, William Korcari, Katja Kr\"uger, Peter McKeown
Categories: physics.ins-det cs.LG hep-ex hep-ph physics.data-an
Comments: 25 pages, 11 figures
Report-no: DESY-23-061
Journal-ref: JINST 18 (2023) 11, P11025
DOI: 10.1088/1748-0221/18/11/P11025
\\ ( https://arxiv.org/abs/2305.04847 ,  5679kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01727 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 17:35:40 GMT   (177kb,D)

Title: Broadcasting in random recursive dags
Authors: Simon Briend and Luc Devroye and Gabor Lugosi
Categories: stat.ML cs.LG cs.SI
\\ ( https://arxiv.org/abs/2306.01727 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16838 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 10:59:48 GMT   (107kb,D)

Title: Solving Kernel Ridge Regression with Gradient-Based Optimization Methods
Authors: Oskar Allerbo
Categories: stat.ML cs.LG math.OC stat.ME
Comments: Article arXiv:2306.16838v1 has been updated and split into two
  articles: this article and arXiv:2311.01762. Thus, some of the content in
  arXiv:2306.16838v1 is not a part of arXiv:2306.16838v2, but of
  arXiv:2311.01762
\\ ( https://arxiv.org/abs/2306.16838 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02496 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 20:57:28 GMT   (680kb)

Title: Learning to reconstruct the bubble distribution with conductivity maps
  using Invertible Neural Networks and Error Diffusion
Authors: Nishant Kumar, Lukas Krause, Thomas Wondrak, Sven Eckert, Kerstin
  Eckert, Stefan Gumhold
Categories: eess.IV cs.CV cs.LG
Comments: Accepted for Oral presentation at WCIPT11 (11th World Congress on
  Industrial Process Tomography)
\\ ( https://arxiv.org/abs/2307.02496 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03137 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 10:44:33 GMT   (6806kb,D)

Title: Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed
  Tomography Images
Authors: Seher Ozcelik, Sinan Unver, Ilke Ali Gurses, Rustu Turkay, and Cigdem
  Gunduz-Demir
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.03137 ,  6806kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16506 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 20:27:25 GMT   (14413kb,D)

Title: Explainable Equivariant Neural Networks for Particle Physics: PELICAN
Authors: Alexander Bogatskiy, Timothy Hoffman, David W. Miller, Jan T.
  Offermann, Xiaoyang Liu
Categories: hep-ph cs.LG hep-ex
Comments: 52 pages, 34 figures, 12 tables
\\ ( https://arxiv.org/abs/2307.16506 ,  14413kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15613 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 14:55:23 GMT   (278kb,D)

Title: Mixed Variational Flows for Discrete Variables
Authors: Gian Carlo Diluvi, Benjamin Bloem-Reddy, Trevor Campbell
Categories: stat.CO cs.LG stat.ML
\\ ( https://arxiv.org/abs/2308.15613 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03919 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 10:52:00 GMT   (8217kb,D)

Title: A hybrid quantum-classical fusion neural network to improve
  protein-ligand binding affinity predictions for drug discovery
Authors: L. Domingo, M. Chehimi, S. Banerjee, S. He Yuxun, S. Konakanchi, L.
  Ogunfowora, S. Roy, S. Selvaras, M. Djukic and C. Johnson
Categories: quant-ph cs.LG
Comments: 5 pages, 3 figures
\\ ( https://arxiv.org/abs/2309.03919 ,  8217kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05704 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 15:15:02 GMT   (1292kb,D)

Title: CaloClouds II: Ultra-Fast Geometry-Independent Highly-Granular
  Calorimeter Simulation
Authors: Erik Buhmann, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William
  Korcari, Katja Kr\"uger, and Peter McKeown
Categories: physics.ins-det cs.LG hep-ex hep-ph physics.data-an
Comments: 30 pages, 7 figures, 3 tables, code available at
  https://github.com/FLC-QU-hep/CaloClouds-2
Report-no: DESY-23-130
\\ ( https://arxiv.org/abs/2309.05704 ,  1292kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09814 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 16:11:13 GMT   (90kb)

Title: Convolutional Deep Kernel Machines
Authors: Edward Milsom, Ben Anson, Laurence Aitchison
Categories: stat.ML cs.LG
Comments: ICLR 2024 Camera Ready Version
\\ ( https://arxiv.org/abs/2309.09814 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13850 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 23:58:14 GMT   (432kb,D)

Title: Statistical Perspective of Top-K Sparse Softmax Gating Mixture of
  Experts
Authors: Huy Nguyen, Pedram Akbarian, Fanqi Yan, Nhat Ho
Categories: stat.ML cs.LG
Comments: Accepted to ICLR 2024, 38 pages, 3 figures, 1 table
\\ ( https://arxiv.org/abs/2309.13850 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03986
replaced with revised version Mon, 26 Feb 2024 06:45:02 GMT   (8762kb,D)

Title: Robust Multimodal Learning with Missing Modalities via
  Parameter-Efficient Adaptation
Authors: Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
Categories: cs.CV cs.LG
Comments: 22 pages, 3 figures, 11 tables
\\ ( https://arxiv.org/abs/2310.03986 ,  8762kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13653 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 16:02:26 GMT   (4564kb,D)

Title: Optimal Transport for Measures with Noisy Tree Metric
Authors: Tam Le, Truyen Nguyen, Kenji Fukumizu
Categories: stat.ML cs.LG
Comments: To appear in AISTATS'2024
\\ ( https://arxiv.org/abs/2310.13653 ,  4564kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14592
replaced with revised version Sun, 25 Feb 2024 21:56:37 GMT   (886kb,D)

Title: Pre-Training LiDAR-Based 3D Object Detectors Through Colorization
Authors: Tai-Yu Pan, Chenyang Ma, Tianle Chen, Cheng Perng Phoo, Katie Z Luo,
  Yurong You, Mark Campbell, Kilian Q. Weinberger, Bharath Hariharan, and
  Wei-Lun Chao
Categories: cs.CV cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.14592 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18306 (*cross-listing*)
replaced with revised version Sun, 25 Feb 2024 23:15:13 GMT   (6790kb,D)

Title: Supervised and Penalized Baseline Correction
Authors: Erik Andries, Ramin Nikzad-Langerodi
Categories: stat.ML cs.LG eess.SP
Comments: 27 pages; 9 figures; 2 tables; fixed typos; additional sanity checks
  for grammar and syntax; streamlined text and made minor cosmetic changes
MSC-class: 15, 62
\\ ( https://arxiv.org/abs/2310.18306 ,  6790kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06918
replaced with revised version Sun, 25 Feb 2024 22:51:18 GMT   (376kb,D)

Title: Resource-Aware Hierarchical Federated Learning for Video Caching in
  Wireless Networks
Authors: Md Ferdous Pervej and Andreas F Molisch
Categories: cs.NI cs.LG cs.SY eess.SY
Comments: Accepted for publication in IEEE ICC 2024. \c{opyright} 2024 IEEE.
  Personal use of this material is permitted. Permission from IEEE must be
  obtained for all other uses
\\ ( https://arxiv.org/abs/2311.06918 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18426 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 17:26:30 GMT   (120kb,D)

Title: Convergence Analysis of Fractional Gradient Descent
Authors: Ashwani Aggarwal
Categories: math.OC cs.LG cs.NA math.NA
Comments: 27 pages, 4 figures. Added additional results showing advantage of
  fractional gradient descent on quadratic functions
ACM-class: G.1.6
\\ ( https://arxiv.org/abs/2311.18426 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03262 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 16:19:50 GMT   (7210kb,D)

Title: Low-Cost High-Power Membership Inference Attacks
Authors: Sajjad Zarifzadeh, Philippe Liu, Reza Shokri
Categories: stat.ML cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.03262 ,  7210kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15591
replaced with revised version Mon, 26 Feb 2024 02:18:25 GMT   (430kb,D)

Title: Privacy-Preserved Neural Graph Databases
Authors: Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song
Categories: cs.DB cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.15591 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15698
replaced with revised version Mon, 26 Feb 2024 12:03:24 GMT   (308kb,D)

Title: RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for
  Program Repair
Authors: Andr\'e Silva, Sen Fang, Martin Monperrus
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2312.15698 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15927
replaced with revised version Sun, 25 Feb 2024 15:49:05 GMT   (4620kb,D)

Title: M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy
Authors: Hansong Zhang, Shikun Li, Pengju Wang, Dan Zeng, Shiming Ge
Categories: cs.CV cs.LG
Comments: This work has been accepted by AAAI-24
\\ ( https://arxiv.org/abs/2312.15927 ,  4620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02320
replaced with revised version Sat, 24 Feb 2024 04:49:28 GMT   (415kb,D)

Title: Spin: An Efficient Secure Computation Framework with GPU Acceleration
Authors: Wuxuan Jiang, Xiangjun Song, Shenbai Hong, Haijun Zhang, Wenxin Liu,
  Bo Zhao, Wei Xu, Yi Li
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2402.02320 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03167 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 11:30:42 GMT   (12050kb,D)

Title: Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic
  Update and Transient Iteration Complexity
Authors: Boao Kong, Shuchen Zhu, Songtao Lu, Xinmeng Huang, Kun Yuan
Categories: math.OC cs.LG stat.ML
Comments: 37 pages, 6 figures
\\ ( https://arxiv.org/abs/2402.03167 ,  12050kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10009
replaced with revised version Sun, 25 Feb 2024 13:39:37 GMT   (7131kb,D)

Title: Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
Authors: Hila Manor and Tomer Michaeli
Categories: cs.SD cs.LG eess.AS
Comments: Updating consistent notations; Examples and code available in
  https://hilamanor.github.io/AudioEditing/
\\ ( https://arxiv.org/abs/2402.10009 ,  7131kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11101 (*cross-listing*)
replaced with revised version Sat, 24 Feb 2024 02:53:48 GMT   (1519kb)

Title: Physics-based material parameters extraction from perovskite experiments
  via Gaussian process
Authors: Hualin Zhan, Viqar Ahmad, Azul Mayon, Grace Tabi, Anh Dinh Bui,
  Zhuofeng Li, Daniel Walter, Hieu Nguyen, Klaus Weber, Thomas White, Kylie
  Catchpole
Categories: cond-mat.mtrl-sci cs.CE cs.LG
Comments: The work is supported by the Australian Centre for Advanced
  Photovoltaics (ACAP) and received funding from the Australian Renewable
  Energy Agency (ARENA)
\\ ( https://arxiv.org/abs/2402.11101 ,  1519kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13918
replaced with revised version Mon, 26 Feb 2024 15:59:26 GMT   (6424kb,D)

Title: BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for
  Cloud Detection and Segmentation in Remote Sensing Imagery
Authors: Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane
Categories: cs.CV cs.LG eess.IV
Comments: Submitted to Expert Systems and Applications. Under license
  CC-BY-NC-ND
\\ ( https://arxiv.org/abs/2402.13918 ,  6424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15281
replaced with revised version Mon, 26 Feb 2024 16:26:11 GMT   (4279kb,D)

Title: Neural Implicit Swept Volume Models for Fast Collision Detection
Authors: Dominik Joho, Jonas Schwinn, Kirill Safronov
Categories: cs.RO cs.LG
Comments: To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have
  equal contribution
\\ ( https://arxiv.org/abs/2402.15281 ,  4279kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
