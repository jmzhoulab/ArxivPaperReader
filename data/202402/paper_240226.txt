Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月26日 17:32
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 22 Feb 24 19:00:00 GMT  to  Fri 23 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.15075
Date: Fri, 23 Feb 2024 03:33:06 GMT   (773kb)

Title: Stacking Factorizing Partitioned Expressions in Hybrid Bayesian Network
  Models
Authors: Peng Lin, Martin Neil and Norman Fenton
Categories: cs.AI
\\
  Hybrid Bayesian networks (HBN) contain complex conditional probabilistic
distributions (CPD) specified as partitioned expressions over discrete and
continuous variables. The size of these CPDs grows exponentially with the
number of parent nodes when using discrete inference, resulting in significant
inefficiency. Normally, an effective way to reduce the CPD size is to use a
binary factorization (BF) algorithm to decompose the statistical or arithmetic
functions in the CPD by factorizing the number of connected parent nodes to
sets of size two. However, the BF algorithm was not designed to handle
partitioned expressions. Hence, we propose a new algorithm called stacking
factorization (SF) to decompose the partitioned expressions. The SF algorithm
creates intermediate nodes to incrementally reconstruct the densities in the
original partitioned expression, allowing no more than two continuous parent
nodes to be connected to each child node in the resulting HBN. SF can be either
used independently or combined with the BF algorithm. We show that the SF+BF
algorithm significantly reduces the CPD size and contributes to lowering the
tree-width of a model, thus improving efficiency.
\\ ( https://arxiv.org/abs/2402.15075 ,  773kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15140
Date: Fri, 23 Feb 2024 06:55:04 GMT   (4116kb,D)

Title: A Relation-Interactive Approach for Message Passing in Hyper-relational
  Knowledge Graphs
Authors: Yonglin Jing
Categories: cs.AI
\\
  Hyper-relational knowledge graphs (KGs) contain additional key-value pairs,
providing more information about the relations. In many scenarios, the same
relation can have distinct key-value pairs, making the original triple fact
more recognizable and specific. Prior studies on hyper-relational KGs have
established a solid standard method for hyper-relational graph encoding. In
this work, we propose a message-passing-based graph encoder with global
relation structure awareness ability, which we call ReSaE. Compared to the
prior state-of-the-art approach, ReSaE emphasizes the interaction of relations
during message passing process and optimizes the readout structure for link
prediction tasks. Overall, ReSaE gives a encoding solution for hyper-relational
KGs and ensures stronger performance on downstream link prediction tasks. Our
experiments demonstrate that ReSaE achieves state-of-the-art performance on
multiple link prediction benchmarks. Furthermore, we also analyze the influence
of different model structures on model performance.
\\ ( https://arxiv.org/abs/2402.15140 ,  4116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15444
Date: Thu, 22 Feb 2024 05:48:03 GMT   (2695kb,D)

Title: Unleashing the Power of Imbalanced Modality Information for Multi-modal
  Knowledge Graph Completion
Authors: Yichi Zhang, Zhuo Chen, Lei Liang, Huajun Chen, Wen Zhang
Categories: cs.AI cs.CL cs.LG cs.MM
Comments: Accepted by LREC-COLING 2024
\\
  Multi-modal knowledge graph completion (MMKGC) aims to predict the missing
triples in the multi-modal knowledge graphs by incorporating structural,
visual, and textual information of entities into the discriminant models. The
information from different modalities will work together to measure the triple
plausibility. Existing MMKGC methods overlook the imbalance problem of modality
information among entities, resulting in inadequate modal fusion and
inefficient utilization of the raw modality information. To address the
mentioned problems, we propose Adaptive Multi-modal Fusion and Modality
Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality
information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive
modality weights and further generates adversarial samples by
modality-adversarial training to enhance the imbalanced modality information.
Our approach is a co-design of the MMKGC model and training strategy which can
outperform 19 recent MMKGC methods and achieve new state-of-the-art results on
three public MMKGC benchmarks. Our code and data have been released at
https://github.com/zjukg/AdaMF-MAT.
\\ ( https://arxiv.org/abs/2402.15444 ,  2695kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15445
Date: Fri, 23 Feb 2024 17:09:04 GMT   (37kb)

Title: Can we forget how we learned? Doxastic redundancy in iterated belief
  revision
Authors: Paolo Liberatore
Categories: cs.AI
Comments: formerly part of arXiv:2305.09200
\\
  How information was acquired may become irrelevant. An obvious case is when
something is confirmed many times. In terms of iterated belief revision, a
specific revision may become irrelevant in presence of others. Simple
repetitions are an example, but not the only case when this happens. Sometimes,
a revision becomes redundant even in presence of none equal, or even no else
implying it. A necessary and sufficient condition for the redundancy of the
first of a sequence of lexicographic revisions is given. The problem is
coNP-complete even with two propositional revisions only. Complexity is the
same in the Horn case but only with an unbounded number of revisions: it
becomes polynomial with two revisions. Lexicographic revisions are not only
relevant by themselves, but also because sequences of them are the most compact
of the common mechanisms used to represent the state of an iterated revision
process. Shortening sequences of lexicographic revisions is shortening the most
compact representations of iterated belief revision states.
\\ ( https://arxiv.org/abs/2402.15445 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15506
Date: Fri, 23 Feb 2024 18:56:26 GMT   (3748kb,D)

Title: AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning
Authors: Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao,
  Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika
  Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan
  Wang, Caiming Xiong
Categories: cs.AI cs.CL cs.LG
\\
  Autonomous agents powered by large language models (LLMs) have garnered
significant research attention. However, fully harnessing the potential of LLMs
for agent-based tasks presents inherent challenges due to the heterogeneous
nature of diverse data sources featuring multi-turn trajectories. In this
paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address
these challenges. \textit{AgentOhana} aggregates agent trajectories from
distinct environments, spanning a wide array of scenarios. It meticulously
standardizes and unifies these trajectories into a consistent format,
streamlining the creation of a generic data loader optimized for agent
training. Leveraging the data unification, our training pipeline maintains
equilibrium across different data sources and preserves independent randomness
across devices during dataset partitioning and model training. Additionally, we
present \textbf{xLAM-v0.1}, a large action model tailored for AI agents, which
demonstrates exceptional performance across various benchmarks.
\\ ( https://arxiv.org/abs/2402.15506 ,  3748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14830
Date: Fri, 16 Feb 2024 23:44:38 GMT   (132kb,D)

Title: Orca-Math: Unlocking the potential of SLMs in Grade School Math
Authors: Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah
Categories: cs.CL cs.AI
\\
  Mathematical word problem-solving has long been recognized as a complex task
for small language models (SLMs). A recent study hypothesized that the smallest
model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34
billion parameters. To reach this level of performance with smaller models,
researcher often train SLMs to generate Python code or use tools to help avoid
calculation errors. Additionally, they employ ensembling, where outputs of up
to 100 model runs are combined to arrive at a more accurate result. Result
selection is done using consensus, majority vote or a separate a verifier model
used in conjunction with the SLM. Ensembling provides a substantial boost in
accuracy but at a significant cost increase with multiple calls to the model
(e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).
  In this work, we present Orca-Math, a 7-billion-parameter SLM based on the
Mistral-7B, which achieves 86.81% on GSM8k without the need for multiple model
calls or the use of verifiers, code execution or any other external tools. Our
approach has the following key elements: (1) A high quality synthetic dataset
of 200K math problems created using a multi-agent setup where agents
collaborate to create the data, (2) An iterative learning techniques that
enables the SLM to practice solving problems, receive feedback on its solutions
and learn from preference pairs incorporating the SLM solutions and the
feedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves
81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math
achieves 86.81% pass@1. Orca-Math surpasses the performance of significantly
larger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It
also significantly outperforms other smaller models while using much smaller
data (hundreds of thousands vs. millions of problems).
\\ ( https://arxiv.org/abs/2402.14830 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14833
Date: Sat, 17 Feb 2024 22:37:17 GMT   (645kb)

Title: CliqueParcel: An Approach For Batching LLM Prompts That Jointly
  Optimizes Efficiency And Faithfulness
Authors: Jiayi Liu, Tinghan Yang, Jennifer Neville
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have become pivotal in recent research. However,
during the inference process, LLMs still require substantial resources. In this
paper, we propose CliqueParcel, a method designed to improve the efficiency of
LLMs via prompt batching. Existing strategies to optimize inference efficiency
often compromise on output quality, leading to a discounted output problem.
This issue might result in reduced accuracy or outputs that are less detailed.
CliqueParcel is our answer to this challenge. While ensuring accuracy and
minimizing deviations from the original outputs (i.e., faithfulness), our
method significantly improves efficiency during inference.
  To lay the groundwork, we first redefine efficiency measurements by excluding
the reduction in running time due to shorter lengths. Then, we provide a
comprehensive trade-off between efficiency and faithfulness to clarify the
nature of the 'discounted output' problem. Within the CliqueParcel framework,
we suggest multiple batching sub-methods and discuss the specific scenarios in
which they can be applied. During evaluation, CliqueParcel is tested on eight
widely recognized datasets, which can be classified into three types: reading
comprehension, open-source question-answering, and reasoning. Our experiments
explore the performance of CliqueParcel, including efficiency, faithfulness,
and the trade-off between them. This work provides novel insights into
inference efficiency and demonstrates promising performance.
\\ ( https://arxiv.org/abs/2402.14833 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14834
Date: Sun, 18 Feb 2024 05:40:33 GMT   (9531kb,D)

Title: MSynFD: Multi-hop Syntax aware Fake News Detection
Authors: Liang Xiao, Qi Zhang, Chongyang Shi, Shoujin Wang, Usman Naseem, and
  Liang Hu
Categories: cs.CL cs.AI cs.IR
Comments: 10 pages
\\
  The proliferation of social media platforms has fueled the rapid
dissemination of fake news, posing threats to our real-life society. Existing
methods use multimodal data or contextual information to enhance the detection
of fake news by analyzing news content and/or its social context. However,
these methods often overlook essential textual news content (articles) and
heavily rely on sequential modeling and global attention to extract semantic
information. These existing methods fail to handle the complex, subtle twists
in news articles, such as syntax-semantics mismatches and prior biases, leading
to lower performance and potential failure when modalities or social context
are missing. To bridge these significant gaps, we propose a novel multi-hop
syntax aware fake news detection (MSynFD) method, which incorporates
complementary syntax information to deal with subtle twists in fake news.
Specifically, we introduce a syntactical dependency graph and design a
multi-hop subgraph aggregation mechanism to capture multi-hop syntax. It
extends the effect of word perception, leading to effective noise filtering and
adjacent relation enhancement. Subsequently, a sequential relative
position-aware Transformer is designed to capture the sequential information,
together with an elaborate keyword debiasing module to mitigate the prior bias.
Extensive experimental results on two public benchmark datasets verify the
effectiveness and superior performance of our proposed MSynFD over
state-of-the-art detection models.
\\ ( https://arxiv.org/abs/2402.14834 ,  9531kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14835
Date: Sun, 18 Feb 2024 07:15:03 GMT   (9690kb,D)

Title: MIKE: A New Benchmark for Fine-grained Multimodal Entity Knowledge
  Editing
Authors: Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen, Nan Hu, Guilin Qi,
  Haiyun Jiang, Siyuan Cheng, Bozhong Tian
Categories: cs.CL cs.AI cs.LG
Comments: 8 pages
\\
  Multimodal knowledge editing represents a critical advancement in enhancing
the capabilities of Multimodal Large Language Models (MLLMs). Despite its
potential, current benchmarks predominantly focus on coarse-grained knowledge,
leaving the intricacies of fine-grained (FG) multimodal entity knowledge
largely unexplored. This gap presents a notable challenge, as FG entity
recognition is pivotal for the practical deployment and effectiveness of MLLMs
in diverse real-world scenarios. To bridge this gap, we introduce MIKE, a
comprehensive benchmark and dataset specifically designed for the FG multimodal
entity knowledge editing. MIKE encompasses a suite of tasks tailored to assess
different perspectives, including Vanilla Name Answering, Entity-Level Caption,
and Complex-Scenario Recognition. In addition, a new form of knowledge editing,
Multi-step Editing, is introduced to evaluate the editing efficiency. Through
our extensive evaluations, we demonstrate that the current state-of-the-art
methods face significant challenges in tackling our proposed benchmark,
underscoring the complexity of FG knowledge editing in MLLMs. Our findings
spotlight the urgent need for novel approaches in this domain, setting a clear
agenda for future research and development efforts within the community.
\\ ( https://arxiv.org/abs/2402.14835 ,  9690kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14836
Date: Sun, 18 Feb 2024 16:51:02 GMT   (360kb,D)

Title: Stealthy Attack on Large Language Model based Recommendation
Authors: Jinghao Zhang, Yuting Liu, Qiang Liu, Shu Wu, Guibing Guo and Liang
  Wang
Categories: cs.CL cs.AI cs.IR
\\
  Recently, the powerful large language models (LLMs) have been instrumental in
propelling the progress of recommender systems (RS). However, while these
systems have flourished, their susceptibility to security threats has been
largely overlooked. In this work, we reveal that the introduction of LLMs into
recommendation models presents new security vulnerabilities due to their
emphasis on the textual content of items. We demonstrate that attackers can
significantly boost an item's exposure by merely altering its textual content
during the testing phase, without requiring direct interference with the
model's training process. Additionally, the attack is notably stealthy, as it
does not affect the overall recommendation performance and the modifications to
the text are subtle, making it difficult for users and platforms to detect. Our
comprehensive experiments across four mainstream LLM-based recommendation
models demonstrate the superior efficacy and stealthiness of our approach. Our
work unveils a significant security gap in LLM-based recommendation systems and
paves the way for future research on protecting these systems.
\\ ( https://arxiv.org/abs/2402.14836 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14837
Date: Sun, 18 Feb 2024 23:03:56 GMT   (2341kb,D)

Title: An Empirical Categorization of Prompting Techniques for Large Language
  Models: A Practitioner's Guide
Authors: Oluwole Fagbohun, Rachel M. Harrison, Anton Dereventsov
Categories: cs.CL cs.AI cs.HC cs.LG
\\
  Due to rapid advancements in the development of Large Language Models (LLMs),
programming these models with prompts has recently gained significant
attention. However, the sheer number of available prompt engineering techniques
creates an overwhelming landscape for practitioners looking to utilize these
tools. For the most efficient and effective use of LLMs, it is important to
compile a comprehensive list of prompting techniques and establish a
standardized, interdisciplinary categorization framework. In this survey, we
examine some of the most well-known prompting techniques from both academic and
practical viewpoints and classify them into seven distinct categories. We
present an overview of each category, aiming to clarify their unique
contributions and showcase their practical applications in real-world examples
in order to equip fellow practitioners with a structured framework for
understanding and categorizing prompting techniques tailored to their specific
domains. We believe that this approach will help simplify the complex landscape
of prompt engineering and enable more effective utilization of LLMs in various
applications. By providing practitioners with a systematic approach to prompt
categorization, we aim to assist in navigating the intricacies of effective
prompt design for conversational pre-trained LLMs and inspire new possibilities
in their respective fields.
\\ ( https://arxiv.org/abs/2402.14837 ,  2341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14838
Date: Mon, 19 Feb 2024 00:40:17 GMT   (4381kb,D)

Title: RFBES at SemEval-2024 Task 8: Investigating Syntactic and Semantic
  Features for Distinguishing AI-Generated and Human-Written Texts
Authors: Mohammad Heydari Rad, Farhan Farsi, Shayan Bali, Romina Etezadi,
  Mehrnoush Shamsfard
Categories: cs.CL cs.AI cs.LG
Comments: Mohammad Heydari Rad, Farhan Farsi, and Shayan Bali have made equal
  contributions to this work
\\
  Nowadays, the usage of Large Language Models (LLMs) has increased, and LLMs
have been used to generate texts in different languages and for different
tasks. Additionally, due to the participation of remarkable companies such as
Google and OpenAI, LLMs are now more accessible, and people can easily use
them. However, an important issue is how we can detect AI-generated texts from
human-written ones. In this article, we have investigated the problem of
AI-generated text detection from two different aspects: semantics and syntax.
Finally, we presented an AI model that can distinguish AI-generated texts from
human-written ones with high accuracy on both multilingual and monolingual
tasks using the M4 dataset. According to our results, using a semantic approach
would be more helpful for detection. However, there is a lot of room for
improvement in the syntactic approach, and it would be a good approach for
future work.
\\ ( https://arxiv.org/abs/2402.14838 ,  4381kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14840
Date: Mon, 19 Feb 2024 06:57:02 GMT   (39389kb,D)

Title: RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question
  Answering and Clinical Reasoning
Authors: Congyun Jin, Ming Zhang, Xiaowei Ma, Li Yujiao, Yingbo Wang, Yabo Jia,
  Yuliang Du, Tao Sun, Haowen Wang, Cong Fan, Jinjie Gu, Chenfei Chi, Xiangguo
  Lv, Fangzhou Li, Wei Xue, Yiran Huang
Categories: cs.CL cs.AI stat.AP
Comments: 15 pages, 13 figures
\\
  Recent advancements in Large Language Models (LLMs) and Large Multi-modal
Models (LMMs) have shown potential in various medical applications, such as
Intelligent Medical Diagnosis. Although impressive results have been achieved,
we find that existing benchmarks do not reflect the complexity of real medical
reports and specialized in-depth reasoning capabilities. In this work, we
introduced RJUA-MedDQA, a comprehensive benchmark in the field of medical
specialization, which poses several challenges: comprehensively interpreting
imgage content across diverse challenging layouts, possessing numerical
reasoning ability to identify abnormal indicators and demonstrating clinical
reasoning ability to provide statements of disease diagnosis, status and advice
based on medical contexts. We carefully design the data generation pipeline and
proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed
at restoring textual and tabular content in medical report images. This method
substantially enhances annotation efficiency, doubling the productivity of each
annotator, and yields a 26.8% improvement in accuracy. We conduct extensive
evaluations, including few-shot assessments of 5 LMMs which are capable of
solving Chinese medical QA tasks. To further investigate the limitations and
potential of current LMMs, we conduct comparative experiments on a set of
strong LLMs by using image-text generated by ESRA method. We report the
performance of baselines and offer several observations: (1) The overall
performance of existing LMMs is still limited; however LMMs more robust to
low-quality and diverse-structured images compared to LLMs. (3) Reasoning
across context and image content present significant challenges. We hope this
benchmark helps the community make progress on these challenging tasks in
multi-modal medical document understanding and facilitate its application in
healthcare.
\\ ( https://arxiv.org/abs/2402.14840 ,  39389kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14843
Date: Mon, 19 Feb 2024 09:24:02 GMT   (1109kb,D)

Title: Text Diffusion with Reinforced Conditioning
Authors: Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang,
  Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang
Categories: cs.CL cs.AI cs.LG
Comments: 9 pages, 3 figures
\\
  Diffusion models have demonstrated exceptional capability in generating
high-quality images, videos, and audio. Due to their adaptiveness in iterative
refinement, they provide a strong potential for achieving better
non-autoregressive sequence generation. However, existing text diffusion models
still fall short in their performance due to a challenge in handling the
discreteness of language. This paper thoroughly analyzes text diffusion models
and uncovers two significant limitations: degradation of self-conditioning
during training and misalignment between training and sampling. Motivated by
our findings, we propose a novel Text Diffusion model called TREC, which
mitigates the degradation with Reinforced Conditioning and the misalignment by
Time-Aware Variance Scaling. Our extensive experiments demonstrate the
competitiveness of TREC against autoregressive, non-autoregressive, and
diffusion baselines. Moreover, qualitative analysis shows its advanced ability
to fully utilize the diffusion process in refining samples.
\\ ( https://arxiv.org/abs/2402.14843 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14845
Date: Mon, 19 Feb 2024 14:00:39 GMT   (8947kb,D)

Title: Purifying Large Language Models by Ensembling a Small Language Model
Authors: Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, Min
  Lin
Categories: cs.CL cs.AI cs.LG
ACM-class: I.2
\\
  The emerging success of large language models (LLMs) heavily relies on
collecting abundant training data from external (untrusted) sources. Despite
substantial efforts devoted to data cleaning and curation, well-constructed
LLMs have been reported to suffer from copyright infringement, data poisoning,
and/or privacy violations, which would impede practical deployment of LLMs. In
this study, we propose a simple and easily implementable method for purifying
LLMs from the negative effects caused by uncurated data, namely, through
ensembling LLMs with benign and small language models (SLMs). Aside from
theoretical guarantees, we perform comprehensive experiments to empirically
confirm the efficacy of ensembling LLMs with SLMs, which can effectively
preserve the performance of LLMs while mitigating issues such as copyright
infringement, data poisoning, and privacy violations.
\\ ( https://arxiv.org/abs/2402.14845 ,  8947kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14846
Date: Mon, 19 Feb 2024 14:53:01 GMT   (11539kb,D)

Title: Stick to your Role! Stability of Personal Values Expressed in Large
  Language Models
Authors: Grgur Kova\v{c}, R\'emy Portelas, Masataka Sawayama, Peter Ford
  Dominey, Pierre-Yves Oudeyer
Categories: cs.CL cs.AI cs.LG
Comments: The project website and code are available at
  https://sites.google.com/view/llmvaluestability
MSC-class: 68T07
ACM-class: I.2.7
\\
  The standard way to study Large Language Models (LLMs) through benchmarks or
psychology questionnaires is to provide many different queries from similar
minimal contexts (e.g. multiple choice questions). However, due to LLM's highly
context-dependent nature, conclusions from such minimal-context evaluations may
be little informative about the model's behavior in deployment (where it will
be exposed to many new contexts). We argue that context-dependence should be
studied as another dimension of LLM comparison alongside others such as
cognitive abilities, knowledge, or model size. In this paper, we present a
case-study about the stability of value expression over different contexts
(simulated conversations on different topics), and as measured using a standard
psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19
open-sourced LLMs from five families. Reusing methods from psychology, we study
Rank-order stability on the population (interpersonal) level, and Ipsative
stability on the individual (intrapersonal) level. We explore two settings:
with and without instructing LLMs to simulate particular personalities. We
observe similar trends in the stability of models and model families - Mixtral,
Mistral and Qwen families being more stable than LLaMa-2 and Phi - over those
two settings, two different simulated populations, and even in the downstream
behavioral task. When instructed to simulate particular personas, LLMs exhibit
low Rank-Order stability, and this stability further diminishes with
conversation length. This highlights the need for future research directions on
LLMs that can coherently simulate a diversity of personas, as well as how
context-dependence can be studied in more thorough and efficient ways. This
paper provides a foundational step in that direction, and, to our knowledge, it
is the first study of value stability in LLMs.
\\ ( https://arxiv.org/abs/2402.14846 ,  11539kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14848
Date: Mon, 19 Feb 2024 16:04:53 GMT   (938kb,D)

Title: Same Task, More Tokens: the Impact of Input Length on the Reasoning
  Performance of Large Language Models
Authors: Mosh Levy, Alon Jacoby, Yoav Goldberg
Categories: cs.CL cs.AI
\\
  This paper explores the impact of extending input lengths on the capabilities
of Large Language Models (LLMs). Despite LLMs advancements in recent times,
their performance consistency across different input lengths is not well
understood. We investigate this aspect by introducing a novel QA reasoning
framework, specifically designed to assess the impact of input length. We
isolate the effect of input length using multiple versions of the same sample,
each being extended with padding of different lengths, types and locations. Our
findings show a notable degradation in LLMs' reasoning performance at much
shorter input lengths than their technical maximum. We show that the
degradation trend appears in every version of our dataset, although at
different intensities. Additionally, our study reveals that traditional
perplexity metrics do not correlate with performance of LLMs' in long input
reasoning tasks. We analyse our results and identify failure modes that can
serve as useful guides for future research, potentially informing strategies to
address the limitations observed in LLMs.
\\ ( https://arxiv.org/abs/2402.14848 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14849
Date: Mon, 19 Feb 2024 19:48:02 GMT   (269kb)

Title: Asynchronous and Segmented Bidirectional Encoding for NMT
Authors: Jingpu Yang, Zehua Han, Mengyu Xiang, Helin Wang, Yuxiao Huang, Miao
  Fang
Categories: cs.CL cs.AI cs.LG
\\
  With the rapid advancement of Neural Machine Translation (NMT), enhancing
translation efficiency and quality has become a focal point of research.
Despite the commendable performance of general models such as the Transformer
in various aspects, they still fall short in processing long sentences and
fully leveraging bidirectional contextual information. This paper introduces an
improved model based on the Transformer, implementing an asynchronous and
segmented bidirectional decoding strategy aimed at elevating translation
efficiency and accuracy. Compared to traditional unidirectional translations
from left-to-right or right-to-left, our method demonstrates heightened
efficiency and improved translation quality, particularly in handling long
sentences. Experimental results on the IWSLT2017 dataset confirm the
effectiveness of our approach in accelerating translation and increasing
accuracy, especially surpassing traditional unidirectional strategies in long
sentence translation. Furthermore, this study analyzes the impact of sentence
length on decoding outcomes and explores the model's performance in various
scenarios. The findings of this research not only provide an effective encoding
strategy for the NMT field but also pave new avenues and directions for future
studies.
\\ ( https://arxiv.org/abs/2402.14849 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14850
Date: Tue, 20 Feb 2024 01:59:11 GMT   (4098kb,D)

Title: CHATATC: Large Language Model-Driven Conversational Agents for
  Supporting Strategic Air Traffic Flow Management
Authors: Sinan Abdulhak, Wayne Hubbard, Karthik Gopalakrishnan, Max Z. Li
Categories: cs.CL cs.AI
Comments: 8 pages, 5 figures
\\
  Generative artificial intelligence (AI) and large language models (LLMs) have
gained rapid popularity through publicly available tools such as ChatGPT. The
adoption of LLMs for personal and professional use is fueled by the natural
interactions between human users and computer applications such as ChatGPT,
along with powerful summarization and text generation capabilities. Given the
widespread use of such generative AI tools, in this work we investigate how
these tools can be deployed in a non-safety critical, strategic traffic flow
management setting. Specifically, we train an LLM, CHATATC, based on a large
historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023
and consisting of over 80,000 GDP implementations, revisions, and
cancellations. We test the query and response capabilities of CHATATC,
documenting successes (e.g., providing correct GDP rates, durations, and
reason) and shortcomings (e.g,. superlative questions). We also detail the
design of a graphical user interface for future users to interact and
collaborate with the CHATATC conversational agent.
\\ ( https://arxiv.org/abs/2402.14850 ,  4098kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14851
Date: Tue, 20 Feb 2024 03:57:55 GMT   (11760kb,D)

Title: SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced
  Reasoning
Authors: Hanchen Xia, Feng Jiang, Naihao Deng, Cunxiang Wang, Guojiang Zhao,
  Rada Mihalcea, and Yue Zhang
Categories: cs.CL cs.AI cs.DB
Comments: 11 pages, 3 figures, 6 tables
\\
  Modern LLMs have become increasingly powerful, but they are still facing
challenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a
framework to advance LLMs' SQL generation Capabilities through inteRActive
reFinemenT and enhanced reasoning. We leverage an Interactive Correction Loop
(IC-Loop) for LLMs to interact with databases automatically, as well as
Python-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets,
Spider and Bird, with performance improvements of up to 5.7% compared to the
naive prompting method. Moreover, our method surpasses the current
state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of
our framework.
\\ ( https://arxiv.org/abs/2402.14851 ,  11760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14852
Date: Tue, 20 Feb 2024 04:17:21 GMT   (13kb)

Title: HumanEval on Latest GPT Models -- 2024
Authors: Daniel Li, Lincoln Murr
Categories: cs.CL cs.AI cs.LG
\\
  In 2023, we are using the latest models of GPT-4 to advance program
synthesis. The large language models have significantly improved the
state-of-the-art for this purpose. To make these advancements more accessible,
we have created a repository that connects these models to Huamn Eval. This
dataset was initally developed to be used with a language model called CODEGEN
on natural and programming language data. The utility of these trained models
is showcased by demonstrating their competitive performance in zero-shot Python
code generation on HumanEval tasks compared to previous state-of-the-art
solutions. Additionally, this gives way to developing more multi-step paradigm
synthesis. This benchmark features 160 diverse problem sets factorized into
multistep prompts that our analysis shows significantly improves program
synthesis over single-turn inputs. All code is open source at
https://github.com/daniel442li/gpt-human-eval .
\\ ( https://arxiv.org/abs/2402.14852 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14853
Date: Tue, 20 Feb 2024 05:58:05 GMT   (5103kb,D)

Title: NL2Formula: Generating Spreadsheet Formulas from Natural Language
  Queries
Authors: Wei Zhao, Zhitao Hou, Siyuan Wu, Yan Gao, Haoyu Dong, Yao Wan, Hongyu
  Zhang, Yulei Sui, Haidong Zhang
Categories: cs.CL cs.AI
Comments: To appear at EACL 2024
\\
  Writing formulas on spreadsheets, such as Microsoft Excel and Google Sheets,
is a widespread practice among users performing data analysis. However,
crafting formulas on spreadsheets remains a tedious and error-prone task for
many end-users, particularly when dealing with complex operations. To alleviate
the burden associated with writing spreadsheet formulas, this paper introduces
a novel benchmark task called NL2Formula, with the aim to generate executable
formulas that are grounded on a spreadsheet table, given a Natural Language
(NL) query as input. To accomplish this, we construct a comprehensive dataset
consisting of 70,799 paired NL queries and corresponding spreadsheet formulas,
covering 21,670 tables and 37 types of formula functions. We realize the
NL2Formula task by providing a sequence-to-sequence baseline implementation
called fCoder. Experimental results validate the effectiveness of fCoder,
demonstrating its superior performance compared to the baseline models.
Furthermore, we also compare fCoder with an initial GPT-3.5 model (i.e.,
text-davinci-003). Lastly, through in-depth error analysis, we identify
potential challenges in the NL2Formula task and advocate for further
investigation.
\\ ( https://arxiv.org/abs/2402.14853 ,  5103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14854
Date: Tue, 20 Feb 2024 06:18:02 GMT   (544kb,D)

Title: A Dual-Prompting for Interpretable Mental Health Language Models
Authors: Hyolim Jeon, Dongje Yoo, Daeun Lee, Sejung Son, Seungbae Kim, Jinyoung
  Han
Categories: cs.CL cs.AI
Journal-ref: Proceedings of the Ninth Workshop on Computational Linguistics and
  Clinical Psychology 2024
\\
  Despite the increasing demand for AI-based mental health monitoring tools,
their practical utility for clinicians is limited by the lack of
interpretability.The CLPsych 2024 Shared Task (Chim et al., 2024) aims to
enhance the interpretability of Large Language Models (LLMs), particularly in
mental health analysis, by providing evidence of suicidality through linguistic
content. We propose a dual-prompting approach: (i) Knowledge-aware evidence
extraction by leveraging the expert identity and a suicide dictionary with a
mental health-specific LLM; and (ii) Evidence summarization by employing an
LLM-based consistency evaluator. Comprehensive experiments demonstrate the
effectiveness of combining domain-specific information, revealing performance
improvements and the approach's potential to aid clinicians in assessing mental
state progression.
\\ ( https://arxiv.org/abs/2402.14854 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14855
Date: Tue, 20 Feb 2024 06:20:09 GMT   (5921kb)

Title: An LLM Maturity Model for Reliable and Transparent Text-to-Query
Authors: Lei Yu (Expression) and Abir Ray (Expression)
Categories: cs.CL cs.AI
Comments: 8 pages, 5 figures
\\
  Recognizing the imperative to address the reliability and transparency issues
of Large Language Models (LLM), this work proposes an LLM maturity model
tailored for text-to-query applications. This maturity model seeks to fill the
existing void in evaluating LLMs in such applications by incorporating
dimensions beyond mere correctness or accuracy. Moreover, this work introduces
a real-world use case from the law enforcement domain and showcases QueryIQ, an
LLM-powered, domain-specific text-to-query assistant to expedite user workflows
and reveal hidden relationship in data.
\\ ( https://arxiv.org/abs/2402.14855 ,  5921kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14856
Date: Tue, 20 Feb 2024 12:58:14 GMT   (9401kb,D)

Title: Comparing Inferential Strategies of Humans and Large Language Models in
  Deductive Reasoning
Authors: Philipp Mondorf and Barbara Plank
Categories: cs.CL cs.AI
Comments: 31 pages, 19 figures
\\
  Deductive reasoning plays a pivotal role in the formulation of sound and
cohesive arguments. It allows individuals to draw conclusions that logically
follow, given the truth value of the information provided. Recent progress in
the domain of large language models (LLMs) has showcased their capability in
executing deductive reasoning tasks. Nonetheless, a significant portion of
research primarily assesses the accuracy of LLMs in solving such tasks, often
overlooking a deeper analysis of their reasoning behavior. In this study, we
draw upon principles from cognitive psychology to examine inferential
strategies employed by LLMs, through a detailed evaluation of their responses
to propositional logic problems. Our findings indicate that LLMs display
reasoning patterns akin to those observed in humans, including strategies like
$\textit{supposition following}$ or $\textit{chain construction}$. Moreover,
our research demonstrates that the architecture and scale of the model
significantly affect its preferred method of reasoning, with more advanced
models tending to adopt strategies more frequently than less sophisticated
ones. Importantly, we assert that a model's accuracy, that is the correctness
of its final conclusion, does not necessarily reflect the validity of its
reasoning process. This distinction underscores the necessity for more nuanced
evaluation procedures in the field.
\\ ( https://arxiv.org/abs/2402.14856 ,  9401kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14857
Date: Tue, 20 Feb 2024 17:39:40 GMT   (203kb,D)

Title: Is the System Message Really Important to Jailbreaks in Large Language
  Models?
Authors: Xiaotian Zou, Yongkang Chen, Ke Li
Categories: cs.CL cs.AI cs.CR
Comments: 13 pages,3 figures
\\
  The rapid evolution of Large Language Models (LLMs) has rendered them
indispensable in modern society. While security measures are typically in place
to align LLMs with human values prior to release, recent studies have unveiled
a concerning phenomenon named "jailbreak." This term refers to the unexpected
and potentially harmful responses generated by LLMs when prompted with
malicious questions. Existing research focuses on generating jailbreak prompts
but our study aim to answer a different question: Is the system message really
important to jailbreak in LLMs? To address this question, we conducted
experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak
prompts with varying system messages: short, long, and none. We discover that
different system messages have distinct resistances to jailbreak by
experiments. Additionally, we explore the transferability of jailbreak across
LLMs. This finding underscores the significant impact system messages can have
on mitigating LLMs jailbreak. To generate system messages that are more
resistant to jailbreak prompts, we propose System Messages Evolutionary
Algorithms (SMEA). Through SMEA, we can get robust system messages population
that demonstrate up to 98.9% resistance against jailbreak prompts. Our research
not only bolsters LLMs security but also raises the bar for jailbreak,
fostering advancements in this field of study.
\\ ( https://arxiv.org/abs/2402.14857 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14858
Date: Tue, 20 Feb 2024 20:52:57 GMT   (1796kb,D)

Title: ChatEL: Entity Linking with Chatbots
Authors: Yifan Ding and Qingkai Zeng and Tim Weninger
Categories: cs.CL cs.AI
\\
  Entity Linking (EL) is an essential and challenging task in natural language
processing that seeks to link some text representing an entity within a
document or sentence with its corresponding entry in a dictionary or knowledge
base. Most existing approaches focus on creating elaborate contextual models
that look for clues the words surrounding the entity-text to help solve the
linking problem. Although these fine-tuned language models tend to work, they
can be unwieldy, difficult to train, and do not transfer well to other domains.
Fortunately, Large Language Models (LLMs) like GPT provide a highly-advanced
solution to the problems inherent in EL models, but simply naive prompts to
LLMs do not work well. In the present work, we define ChatEL, which is a
three-step framework to prompt LLMs to return accurate results. Overall the
ChatEL framework improves the average F1 performance across 10 datasets by more
than 2%. Finally, a thorough error analysis shows many instances with the
ground truth labels were actually incorrect, and the labels predicted by ChatEL
were actually correct. This indicates that the quantitative results presented
in this paper may be a conservative estimate of the actual performance. All
data and code are available as an open-source package on GitHub at
https://github.com/yifding/In_Context_EL.
\\ ( https://arxiv.org/abs/2402.14858 ,  1796kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14860
Date: Wed, 21 Feb 2024 00:49:43 GMT   (2792kb,D)

Title: Ranking Large Language Models without Ground Truth
Authors: Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly and
  Karthikeyan Natesan Ramamurthy
Categories: cs.CL cs.AI cs.LG
\\
  Evaluation and ranking of large language models (LLMs) has become an
important problem with the proliferation of these models and their impact.
Evaluation methods either require human responses which are expensive to
acquire or use pairs of LLMs to evaluate each other which can be unreliable. In
this paper, we provide a novel perspective where, given a dataset of prompts
(viz. questions, instructions, etc.) and a set of LLMs, we rank them without
access to any ground truth or reference responses. Inspired by real life where
both an expert and a knowledgeable person can identify a novice our main idea
is to consider triplets of models, where each one of them evaluates the other
two, correctly identifying the worst model in the triplet with high
probability. We also analyze our idea and provide sufficient conditions for it
to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.
In experiments on different generative tasks (summarization, multiple-choice,
and dialog), our methods reliably recover close to true rankings without
reference data. This points to a viable low-resource mechanism for practical
use.
\\ ( https://arxiv.org/abs/2402.14860 ,  2792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14863
Date: Wed, 21 Feb 2024 03:43:57 GMT   (2399kb,D)

Title: Evaluation of a semi-autonomous attentive listening system with takeover
  prompting
Authors: Haruki Kawai, Divesh Lala, Koji Inoue, Keiko Ochi, Tatsuya Kawahara
Categories: cs.CL
\\
  The handling of communication breakdowns and loss of engagement is an
important aspect of spoken dialogue systems, particularly for chatting systems
such as attentive listening, where the user is mostly speaking. We presume that
a human is best equipped to handle this task and rescue the flow of
conversation. To this end, we propose a semi-autonomous system, where a remote
operator can take control of an autonomous attentive listening system in
real-time. In order to make human intervention easy and consistent, we
introduce automatic detection of low interest and engagement to provide
explicit takeover prompts to the remote operator. We implement this
semi-autonomous system which detects takeover points for the operator and
compare it to fully tele-operated and fully autonomous attentive listening
systems. We find that the semi-autonomous system is generally perceived more
positively than the autonomous system. The results suggest that identifying
points of conversation when the user starts to lose interest may help us
improve a fully autonomous dialogue system.
\\ ( https://arxiv.org/abs/2402.14863 ,  2399kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14865
Date: Wed, 21 Feb 2024 06:46:34 GMT   (323kb,D)

Title: DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing
  Agents
Authors: Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie
Categories: cs.CL cs.AI cs.LG
Comments: Technical report; 20 pages
\\
  Evaluation of large language models (LLMs) has raised great concerns in the
community due to the issue of data contamination. Existing work designed
evaluation protocols using well-defined algorithms for specific tasks, which
cannot be easily extended to diverse scenarios. Moreover, current evaluation
benchmarks can only provide the overall benchmark results and cannot support a
fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we
propose meta probing agents (MPA), a general dynamic evaluation protocol
inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal
2, which naturally extends the previous DyVal~\citep{zhu2023dyval}. MPA designs
the probing and judging agents to automatically transform an original
evaluation problem into a new one following psychometric theory on three basic
cognitive abilities: language understanding, problem solving, and domain
knowledge. These basic abilities are also dynamically configurable, allowing
multifaceted analysis. We conducted extensive evaluations using MPA and found
that most LLMs achieve poorer performance, indicating room for improvement. Our
multifaceted analysis demonstrated the strong correlation between the basic
abilities and an implicit Matthew effect on model size, i.e., larger models
possess stronger correlations of the abilities. MPA can also be used as a data
augmentation approach to enhance LLMs.
\\ ( https://arxiv.org/abs/2402.14865 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14867
Date: Wed, 21 Feb 2024 11:31:04 GMT   (493kb)

Title: Effects of term weighting approach with and without stop words removing
  on Arabic text classification
Authors: Esra'a Alhenawi, Ruba Abu Khurma, Pedro A. Castillo, Maribel G. Arenas
Categories: cs.CL cs.AI cs.LG
\\
  Classifying text is a method for categorizing documents into pre-established
groups. Text documents must be prepared and represented in a way that is
appropriate for the algorithms used for data mining prior to classification. As
a result, a number of term weighting strategies have been created in the
literature to enhance text categorization algorithms' functionality. This study
compares the effects of Binary and Term frequency weighting feature
methodologies on the text's classification method when stop words are
eliminated once and when they are not. In recognition of assessing the effects
of prior weighting of features approaches on classification results in terms of
accuracy, recall, precision, and F-measure values, we used an Arabic data set
made up of 322 documents divided into six main topics (agriculture, economy,
health, politics, science, and sport), each of which contains 50 documents,
with the exception of the health category, which contains 61 documents. The
results demonstrate that for all metrics, the term frequency feature weighting
approach with stop word removal outperforms the binary approach, while for
accuracy, recall, and F-Measure, the binary approach outperforms the TF
approach without stop word removal. However, for precision, the two approaches
produce results that are very similar. Additionally, it is clear from the data
that, using the same phrase weighting approach, stop word removing increases
classification accuracy.
\\ ( https://arxiv.org/abs/2402.14867 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14871
Date: Wed, 21 Feb 2024 13:54:53 GMT   (1512kb,D)

Title: LLM Based Multi-Agent Generation of Semi-structured Documents from
  Semantic Templates in the Public Administration Domain
Authors: Emanuele Musumeci, Michele Brienza, Vincenzo Suriani, Daniele Nardi,
  Domenico Daniele Bloisi
Categories: cs.CL cs.AI cs.HC
Comments: Accepted at HCI INTERNATIONAL 2024 - 26th International Conference on
  Human-Computer Interaction. Washington Hilton Hotel, Washington DC, USA, 29
  June - 4 July 2024
\\
  In the last years' digitalization process, the creation and management of
documents in various domains, particularly in Public Administration (PA), have
become increasingly complex and diverse. This complexity arises from the need
to handle a wide range of document types, often characterized by
semi-structured forms. Semi-structured documents present a fixed set of data
without a fixed format. As a consequence, a template-based solution cannot be
used, as understanding a document requires the extraction of the data
structure. The recent introduction of Large Language Models (LLMs) has enabled
the creation of customized text output satisfying user requests. In this work,
we propose a novel approach that combines the LLMs with prompt engineering and
multi-agent systems for generating new documents compliant with a desired
structure. The main contribution of this work concerns replacing the commonly
used manual prompting with a task description generated by semantic retrieval
from an LLM. The potential of this approach is demonstrated through a series of
experiments and case studies, showcasing its effectiveness in real-world PA
scenarios.
\\ ( https://arxiv.org/abs/2402.14871 ,  1512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14872
Date: Wed, 21 Feb 2024 15:13:50 GMT   (302kb,D)

Title: Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts
  Against Open-source LLMs
Authors: Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien
  Chang
Categories: cs.CL cs.AI cs.NE
\\
  Large Language Models (LLMs), used in creative writing, code generation, and
translation, generate text based on input sequences but are vulnerable to
jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak
prompt methods use a combination of jailbreak templates followed by questions
to ask to create jailbreak prompts. However, existing jailbreak prompt designs
generally suffer from excessive semantic differences, resulting in an inability
to resist defenses that use simple semantic metrics as thresholds. Jailbreak
prompts are semantically more varied than the original questions used for
queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach
that bypasses LLMs by generating jailbreak prompts that are semantically
similar to the original question. We model the search for jailbreak prompts
that satisfy both semantic similarity and jailbreak validity as a
multi-objective optimization problem and employ a standardized set of genetic
algorithms for generating eligible prompts. Compared to the baseline
AutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%
higher without ONION defense and 85.2% higher with ONION defense. SMJ's better
performance in all three semantic meaningfulness metrics of Jailbreak Prompt,
Similarity, and Outlier, also means that SMJ is resistant to defenses that use
those metrics as thresholds.
\\ ( https://arxiv.org/abs/2402.14872 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14873
Date: Wed, 21 Feb 2024 17:13:41 GMT   (416kb,D)

Title: Technical Report on the Checkfor.ai AI-Generated Text Classifier
Authors: Bradley Emi and Max Spero
Categories: cs.CL cs.AI
MSC-class: 68T50
ACM-class: I.2.7
\\
  We present the Checkfor.ai text classifier, a transformer-based neural
network trained to distinguish text written by large language models from text
written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT
as well as leading commercial AI detection tools with over 9 times lower error
rates on a comprehensive benchmark comprised of ten text domains (student
writing, creative writing, scientific writing, books, encyclopedias, news,
email, scientific papers, short-form Q\&A) and 8 open- and closed-source large
language models. We propose a training algorithm, hard negative mining with
synthetic mirrors, that enables our classifier to achieve orders of magnitude
lower false positive rates on high-data domains such as reviews. Finally, we
show that Checkfor.ai is not biased against nonnative English speakers and
generalizes to domains and models unseen during training.
\\ ( https://arxiv.org/abs/2402.14873 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14874
Date: Wed, 21 Feb 2024 17:20:38 GMT   (1681kb,D)

Title: Distillation Contrastive Decoding: Improving LLMs Reasoning with
  Contrastive Decoding and Distillation
Authors: Phuc Phan, Hieu Tran, Long Phan
Categories: cs.CL cs.AI cs.LG
Comments: Under Review
\\
  We propose a straightforward approach called Distillation Contrastive
Decoding (DCD) to enhance the reasoning capabilities of Large Language Models
(LLMs) during inference. In contrast to previous approaches that relied on
smaller amateur models or analysis of hidden state differences, DCD employs
Contrastive Chain-of-thought Prompting and advanced distillation techniques,
including Dropout and Quantization. This approach effectively addresses the
limitations of Contrastive Decoding (CD), which typically requires both an
expert and an amateur model, thus increasing computational resource demands. By
integrating contrastive prompts with distillation, DCD obviates the need for an
amateur model and reduces memory usage. Our evaluations demonstrate that DCD
significantly enhances LLM performance across a range of reasoning benchmarks,
surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
\\ ( https://arxiv.org/abs/2402.14874 ,  1681kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14875
Date: Wed, 21 Feb 2024 18:25:25 GMT   (488kb,D)

Title: What's in a Name? Auditing Large Language Models for Race and Gender
  Bias
Authors: Amit Haim, Alejandro Salinas, Julian Nyarko
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: 34 pages, 9 tables, 11 figures
\\
  We employ an audit design to investigate biases in state-of-the-art large
language models, including GPT-4. In our study, we elicit prompt the models for
advice regarding an individual across a variety of scenarios, such as during
car purchase negotiations or election outcome predictions. We find that the
advice systematically disadvantages names that are commonly associated with
racial minorities and women. Names associated with Black women receive the
least advantageous outcomes. The biases are consistent across 42 prompt
templates and several models, indicating a systemic issue rather than isolated
incidents. While providing numerical, decision-relevant anchors in the prompt
can successfully counteract the biases, qualitative details have inconsistent
effects and may even increase disparities. Our findings underscore the
importance of conducting audits at the point of LLM deployment and
implementation to mitigate their potential for harm against marginalized
communities.
\\ ( https://arxiv.org/abs/2402.14875 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14879
Date: Wed, 21 Feb 2024 21:29:57 GMT   (10607kb,D)

Title: Driving Generative Agents With Their Personality
Authors: Lawrence J. Klinkert, Stephanie Buongiorno, and Corey Clark
Categories: cs.CL cs.AI
Comments: 9 Pages, 4 figures, Draft
\\
  This research explores the potential of Large Language Models (LLMs) to
utilize psychometric values, specifically personality information, within the
context of video game character development. Affective Computing (AC) systems
quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage
of the system's information by using the values for prompt generation. The
research shows an LLM can consistently represent a given personality profile,
thereby enhancing the human-like characteristics of game characters.
Repurposing a human examination, the International Personality Item Pool (IPIP)
questionnaire, to evaluate an LLM shows that the model can accurately generate
content concerning the personality provided. Results show that the improvement
of LLM, such as the latest GPT-4 model, can consistently utilize and interpret
a personality to represent behavior.
\\ ( https://arxiv.org/abs/2402.14879 ,  10607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14880
Date: Wed, 21 Feb 2024 22:29:16 GMT   (1859kb,D)

Title: Automatic Histograms: Leveraging Language Models for Text Dataset
  Exploration
Authors: Emily Reif, Crystal Qian, James Wexler, Minsuk Kahng
Categories: cs.CL cs.AI cs.HC
\\
  Making sense of unstructured text datasets is perennially difficult, yet
increasingly relevant with Large Language Models. Data workers often rely on
dataset summaries, especially distributions of various derived features. Some
features, like toxicity or topics, are relevant to many datasets, but many
interesting features are domain specific: instruments and genres for a music
dataset, or diseases and symptoms for a medical dataset. Accordingly, data
workers often run custom analyses for each dataset, which is cumbersome and
difficult. We present AutoHistograms, a visualization tool leveragingLLMs.
AutoHistograms automatically identifies relevant features, visualizes them with
histograms, and allows the user to interactively query the dataset for
categories of entities and create new histograms. In a user study with 10 data
workers (n=10), we observe that participants can quickly identify insights and
explore the data using AutoHistograms, and conceptualize a broad range of
applicable use cases. Together, this tool and user study contributeto the
growing field of LLM-assisted sensemaking tools.
\\ ( https://arxiv.org/abs/2402.14880 ,  1859kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14881
Date: Wed, 21 Feb 2024 23:51:06 GMT   (1217kb,D)

Title: A Study on the Vulnerability of Test Questions against ChatGPT-based
  Cheating
Authors: Shanker Ram and Chen Qian
Categories: cs.CL cs.AI cs.CY
Comments: 2023 International Conference on Machine Learning and Applications
  (ICMLA)
ACM-class: I.2.7
Journal-ref: 2023 International Conference on Machine Learning and Applications
  (ICMLA)
\\
  ChatGPT is a chatbot that can answer text prompts fairly accurately, even
performing very well on postgraduate-level questions. Many educators have found
that their take-home or remote tests and exams are vulnerable to ChatGPT-based
cheating because students may directly use answers provided by tools like
ChatGPT. In this paper, we try to provide an answer to an important question:
how well ChatGPT can answer test questions and how we can detect whether the
questions of a test can be answered correctly by ChatGPT. We generated
ChatGPT's responses to the MedMCQA dataset, which contains over 10,000 medical
school entrance exam questions. We analyzed the responses and uncovered certain
types of questions ChatGPT answers more inaccurately than others. In addition,
we have created a basic natural language processing model to single out the
most vulnerable questions to ChatGPT in a collection of questions or a sample
exam. Our tool can be used by test-makers to avoid ChatGPT-vulnerable test
questions.
\\ ( https://arxiv.org/abs/2402.14881 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14889
Date: Thu, 22 Feb 2024 10:46:11 GMT   (1674kb)

Title: COBIAS: Contextual Reliability in Bias Assessment
Authors: Priyanshul Govil, Vamshi Krishna Bonagiri, Manas Gaur, Ponnurangam
  Kumaraguru, Sanorita Dey
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) are trained on inherently biased data. Previous
works on debiasing models rely on benchmark datasets to measure model
performance. However, these datasets suffer from several pitfalls due to the
extremely subjective understanding of bias, highlighting a critical need for
contextual exploration. We propose understanding the context of user inputs
with consideration of the diverse situations in which input statements are
possible. This approach would allow for frameworks that foster bias awareness
rather than guardrails that hurt user engagement. Our contribution is twofold:
(i) we create a dataset of 2287 stereotyped statements augmented with points
for adding context; (ii) we develop the Context-Oriented Bias Indicator and
Assessment Score (COBIAS) to assess statements' contextual reliability in
measuring bias. Our metric is a significant predictor of the contextual
reliability of bias-benchmark datasets ($\chi^2=71.02, p<2.2 \cdot 10^{-16})$.
COBIAS can be used to create reliable datasets, resulting in an improvement in
bias mitigation works.
\\ ( https://arxiv.org/abs/2402.14889 ,  1674kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14890
Date: Thu, 22 Feb 2024 12:00:32 GMT   (3334kb,D)

Title: Vygotsky Distance: Measure for Benchmark Task Similarity
Authors: Maxim K. Surkov and Ivan P. Yamshchikov
Categories: cs.CL cs.AI cs.LG
MSC-class: 68T01, 97P80, 97C30, 68Q32
ACM-class: H.1.1; I.2.4; I.2.6; F.2.0
\\
  Evaluation plays a significant role in modern natural language processing.
Most modern NLP benchmarks consist of arbitrary sets of tasks that neither
guarantee any generalization potential for the model once applied outside the
test set nor try to minimize the resource consumption needed for model
evaluation. This paper presents a theoretical instrument and a practical
algorithm to calculate similarity between benchmark tasks, we call this
similarity measure "Vygotsky distance". The core idea of this similarity
measure is that it is based on relative performance of the "students" on a
given task, rather that on the properties of the task itself. If two tasks are
close to each other in terms of Vygotsky distance the models tend to have
similar relative performance on them. Thus knowing Vygotsky distance between
tasks one can significantly reduce the number of evaluation tasks while
maintaining a high validation quality. Experiments on various benchmarks,
including GLUE, SuperGLUE, CLUE, and RussianSuperGLUE, demonstrate that a vast
majority of NLP benchmarks could be at least 40% smaller in terms of the tasks
included. Most importantly, Vygotsky distance could also be used for the
validation of new tasks thus increasing the generalization potential of the
future NLP models.
\\ ( https://arxiv.org/abs/2402.14890 ,  3334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14891
Date: Thu, 22 Feb 2024 12:36:31 GMT   (8217kb,D)

Title: LLMBind: A Unified Modality-Task Integration Framework
Authors: Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang, Qi Song, Mingjun
  Pan, Li Yuan
Categories: cs.CL cs.AI
\\
  While recent progress in multimodal large language models tackles various
modality tasks, they posses limited integration capabilities for complex
multi-modality tasks, consequently constraining the development of the field.
In this work, we take the initiative to explore and propose the LLMBind, a
unified framework for modality task integration, which binds Large Language
Models and corresponding pre-trained task models with task-specific tokens.
Consequently, LLMBind can interpret inputs and produce outputs in versatile
combinations of image, text, video, and audio. Specifically, we introduce a
Mixture-of-Experts technique to enable effective learning for different
multimodal tasks through collaboration among diverse experts. Furthermore, we
create a multi-task dataset comprising 400k instruction data, which unlocks the
ability for interactive visual generation and editing tasks. Extensive
experiments show the effectiveness of our framework across various tasks,
including image, video, audio generation, image segmentation, and image
editing. More encouragingly, our framework can be easily extended to other
modality tasks, showcasing the promising potential of creating a unified AI
agent for modeling universal modalities.
\\ ( https://arxiv.org/abs/2402.14891 ,  8217kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14895
Date: Thu, 22 Feb 2024 16:42:37 GMT   (1398kb,D)

Title: Data Augmentation is Dead, Long Live Data Augmentation
Authors: Fr\'ed\'eric Piedboeuf and Philippe Langlais
Categories: cs.CL cs.AI cs.LG
Comments: 8 pages
\\
  Textual data augmentation (DA) is a prolific field of study where novel
techniques to create artificial data are regularly proposed, and that has
demonstrated great efficiency on small data settings, at least for text
classification tasks. In this paper, we challenge those results, showing that
classical data augmentation is simply a way of performing better fine-tuning,
and that spending more time fine-tuning before applying data augmentation
negates its effect. This is a significant contribution as it answers several
questions that were left open in recent years, namely~: which DA technique
performs best (all of them as long as they generate data close enough to the
training set as to not impair training) and why did DA show positive results
(facilitates training of network). We furthermore show that zero and few-shot
data generation via conversational agents such as ChatGPT or LLama2 can
increase performances, concluding that this form of data augmentation does
still work, even if classical methods do not.
\\ ( https://arxiv.org/abs/2402.14895 ,  1398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14897
Date: Thu, 22 Feb 2024 17:23:53 GMT   (3936kb)

Title: Chain-of-Thought Unfaithfulness as Disguised Accuracy
Authors: Oliver Bentham, Nathan Stringham, Ana Marasovi\'c
Categories: cs.CL cs.AI cs.LG
\\
  Understanding the extent to which Chain-of-Thought (CoT) generations align
with a large language model's (LLM) internal computations is critical for
deciding whether to trust an LLM's output. As a proxy for CoT faithfulness,
arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT
for producing an answer. Within a single family of proprietary models, they
find that LLMs exhibit a scaling-then-inverse-scaling relationship between
model size and their measure of faithfulness, and that a 13 billion parameter
model exhibits increased faithfulness compared to models ranging from 810
million to 175 billion parameters in size. We evaluate whether these results
generalize as a property of all LLMs. We replicate their experimental setup
with three different families of models and, under specific conditions,
successfully reproduce the scaling trends for CoT faithfulness they report.
However, we discover that simply changing the order of answer choices in the
prompt can reduce the metric by 73 percentage points. The faithfulness metric
is also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about
its validity as a construct for evaluating faithfulness.
\\ ( https://arxiv.org/abs/2402.14897 ,  3936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14901
Date: Thu, 22 Feb 2024 18:09:33 GMT   (960kb,D)

Title: A Usage-centric Take on Intent Understanding in E-Commerce
Authors: Wendi Zhou, Tianyi Li, Pavlos Vougiouklis, Mark Steedman, Jeff Z. Pan
Categories: cs.CL cs.AI
\\
  Identifying and understanding user intents is a pivotal task for E-Commerce.
Despite its popularity, intent understanding has not been consistently defined
or accurately benchmarked. In this paper, we focus on predicative user intents
as "how a customer uses a product", and pose intent understanding as a natural
language reasoning task, independent of product ontologies. We identify two
weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit
its capacity to reason about user intents and to recommend diverse useful
products. Following these observations, we introduce a Product Recovery
Benchmark including a novel evaluation framework and an example dataset. We
further validate the above FolkScope weaknesses on this benchmark.
\\ ( https://arxiv.org/abs/2402.14901 ,  960kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14903
Date: Thu, 22 Feb 2024 18:14:09 GMT   (192kb,D)

Title: Tokenization counts: the impact of tokenization on arithmetic in
  frontier LLMs
Authors: Aaditya K. Singh, DJ Strouse
Categories: cs.CL cs.LG
Comments: 21 pages, 18 figures
\\
  Tokenization, the division of input text into input tokens, is an often
overlooked aspect of the large language model (LLM) pipeline and could be the
source of useful or harmful inductive biases. Historically, LLMs have relied on
byte pair encoding, without care to specific input domains. With the increased
use of LLMs for reasoning, various number-specific tokenization schemes have
been adopted, with popular models like LLaMa and PaLM opting for single-digit
tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and
3-digit numbers. In this work, we study the effect this choice has on numerical
reasoning through the use of arithmetic tasks. We consider left-to-right and
right-to-left tokenization for GPT-3.5 and -4, finding that right-to-left
tokenization (enforced by comma separating numbers at inference time) leads to
largely improved performance. Furthermore, we find that model errors when using
standard left-to-right tokenization follow stereotyped error patterns,
suggesting that model computations are systematic rather than approximate. We
show that the model is able to convert between tokenizations easily, thus
allowing chain-of-thought-inspired approaches to recover performance on
left-to-right tokenized inputs. We also find the gap between tokenization
directions decreases when models are scaled, possibly indicating that larger
models are better able to override this tokenization-dependent inductive bias.
In summary, our work performs the first study of how number tokenization
choices lead to differences in model performance on arithmetic tasks,
accompanied by a thorough analysis of error patterns. We hope this work
inspires practitioners to more carefully ablate number tokenization-related
choices when working towards general models of numerical reasoning.
\\ ( https://arxiv.org/abs/2402.14903 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14948
Date: Thu, 22 Feb 2024 20:07:02 GMT   (10265kb,D)

Title: Re-Examine Distantly Supervised NER: A New Benchmark and a Simple
  Approach
Authors: Yuepei Li, Kang Zhou, Qiao Qiao, Qing Wang and Qi Li
Categories: cs.CL cs.LG
\\
  This paper delves into Named Entity Recognition (NER) under the framework of
Distant Supervision (DS-NER), where the main challenge lies in the compromised
quality of labels due to inherent errors such as false positives, false
negatives, and positive type errors. We critically assess the efficacy of
current DS-NER methodologies using a real-world benchmark dataset named QTL,
revealing that their performance often does not meet expectations. To tackle
the prevalent issue of label noise, we introduce a simple yet effective
approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which
strategically starts on "easy" and cleaner samples during the training process
to enhance model resilience to noisy samples. Our empirical results highlight
the capability of CuPUL to significantly reduce the impact of noisy labels and
outperform existing methods.
\\ ( https://arxiv.org/abs/2402.14948 ,  10265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14963
Date: Thu, 22 Feb 2024 20:57:17 GMT   (2163kb,D)

Title: Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich
  Reasoning
Authors: Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, Yulan He
Categories: cs.CL cs.AI
Comments: Code is available at https://github.com/hanqi-qi/Mirror.git
\\
  While Large language models (LLMs) have the capability to iteratively reflect
on their own outputs, recent studies have observed their struggles with
knowledge-rich problems without access to external resources. In addition to
the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle
to revisit their predictions despite receiving explicit negative feedback.
Therefore, We propose Mirror, a Multiple-perspective self-reflection method for
knowledge-rich reasoning, to avoid getting stuck at a particular reflection
iteration. Mirror enables LLMs to reflect from multiple-perspective clues,
achieved through a heuristic interaction between a Navigator and a Reasoner. It
guides agents toward diverse yet plausibly reliable reasoning trajectory
without access to ground truth by encouraging (1) diversity of directions
generated by Navigator and (2) agreement among strategically induced
perturbations in responses generated by the Reasoner. The experiments on five
reasoning datasets demonstrate that Mirror's superiority over several
contemporary self-reflection approaches. Additionally, the ablation study
studies clearly indicate that our strategies alleviate the aforementioned
challenges.
\\ ( https://arxiv.org/abs/2402.14963 ,  2163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14972
Date: Thu, 22 Feb 2024 21:16:18 GMT   (139kb)

Title: MultiLS: A Multi-task Lexical Simplification Framework
Authors: Kai North, Tharindu Ranasinghe, Matthew Shardlow, Marcos Zampieri
Categories: cs.CL cs.AI
\\
  Lexical Simplification (LS) automatically replaces difficult to read words
for easier alternatives while preserving a sentence's original meaning. LS is a
precursor to Text Simplification with the aim of improving text accessibility
to various target demographics, including children, second language learners,
individuals with reading disabilities or low literacy. Several datasets exist
for LS. These LS datasets specialize on one or two sub-tasks within the LS
pipeline. However, as of this moment, no single LS dataset has been developed
that covers all LS sub-tasks. We present MultiLS, the first LS framework that
allows for the creation of a multi-task LS dataset. We also present MultiLS-PT,
the first dataset to be created using the MultiLS framework. We demonstrate the
potential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical
complexity prediction (LCP), (2). substitute generation, and (3). substitute
ranking for Portuguese. Model performances are reported, ranging from
transformer-based models to more recent large language models (LLMs).
\\ ( https://arxiv.org/abs/2402.14972 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14973
Date: Thu, 22 Feb 2024 21:22:04 GMT   (12335kb,D)

Title: GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
Authors: Lele Cao, Valentin Buchner, Zineb Senane and Fangkai Yang
Categories: cs.CL cs.AI cs.LG
Comments: 5 (main paper) + 13 (appendix) pages. Source code:
  https://github.com/EQTPartners/GenCeption
ACM-class: I.7; I.4
\\
  Multimodal Large Language Models (MLLMs) are commonly evaluated using costly
annotated multimodal benchmarks. However, these benchmarks often struggle to
keep pace with the rapidly advancing requirements of MLLM evaluation. We
propose GenCeption, a novel and annotation-free MLLM evaluation framework that
merely requires unimodal data to assess inter-modality semantic coherence and
inversely reflects the models' inclination to hallucinate. Analogous to the
popular DrawCeption game, GenCeption initiates with a non-textual sample and
undergoes a series of iterative description and generation steps. Semantic
drift across iterations is quantified using the GC@T metric. Our empirical
findings validate GenCeption's efficacy, showing strong correlations with
popular MLLM benchmarking results. GenCeption may be extended to mitigate
training data contamination by utilizing ubiquitous, previously unseen unimodal
data.
\\ ( https://arxiv.org/abs/2402.14973 ,  12335kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14992
Date: Thu, 22 Feb 2024 22:05:23 GMT   (5640kb,D)

Title: tinyBenchmarks: evaluating LLMs with fewer examples
Authors: Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu,
  Mikhail Yurochkin
Categories: cs.CL cs.AI cs.LG stat.ML
\\
  The versatility of large language models (LLMs) led to the creation of
diverse benchmarks that thoroughly test a variety of language models'
abilities. These benchmarks consist of tens of thousands of examples making
evaluation of LLMs very expensive. In this paper, we investigate strategies to
reduce the number of evaluations needed to assess the performance of an LLM on
several key benchmarks. For example, we show that to accurately estimate the
performance of an LLM on MMLU, a popular multiple-choice QA benchmark
consisting of 14K examples, it is sufficient to evaluate this LLM on 100
curated examples. We release evaluation tools and tiny versions of popular
benchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical
analysis demonstrates that these tools and tiny benchmarks are sufficient to
reliably and efficiently reproduce the original evaluation results.
\\ ( https://arxiv.org/abs/2402.14992 ,  5640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15000
Date: Thu, 22 Feb 2024 22:28:46 GMT   (232kb,D)

Title: Divide-or-Conquer? Which Part Should You Distill Your LLM?
Authors: Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vinod Vydiswaran,
  Navdeep Jaitly, Yizhe Zhang
Categories: cs.CL cs.LG
\\
  Recent methods have demonstrated that Large Language Models (LLMs) can solve
reasoning tasks better when they are encouraged to solve subtasks of the main
task first. In this paper we devise a similar strategy that breaks down
reasoning tasks into a problem decomposition phase and a problem solving phase
and show that the strategy is able to outperform a single stage solution.
Further, we hypothesize that the decomposition should be easier to distill into
a smaller model compared to the problem solving because the latter requires
large amounts of domain knowledge while the former only requires learning
general problem solving strategies. We propose methods to distill these two
capabilities and evaluate their impact on reasoning outcomes and inference
cost. We find that we can distill the problem decomposition phase and at the
same time achieve good generalization across tasks, datasets, and models.
However, it is harder to distill the problem solving capability without losing
performance and the resulting distilled model struggles with generalization.
These results indicate that by using smaller, distilled problem decomposition
models in combination with problem solving LLMs we can achieve reasoning with
cost-efficient inference and local adaptation.
\\ ( https://arxiv.org/abs/2402.15000 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15002
Date: Thu, 22 Feb 2024 22:31:39 GMT   (10552kb,D)

Title: CommVQA: Situating Visual Question Answering in Communicative Contexts
Authors: Nandita Shankar Naik, Christopher Potts, Elisa Kreiss
Categories: cs.CL cs.CV
\\
  Current visual question answering (VQA) models tend to be trained and
evaluated on image-question pairs in isolation. However, the questions people
ask are dependent on their informational needs and prior knowledge about the
image content. To evaluate how situating images within naturalistic contexts
shapes visual questions, we introduce CommVQA, a VQA dataset consisting of
images, image descriptions, real-world communicative scenarios where the image
might appear (e.g., a travel website), and follow-up questions and answers
conditioned on the scenario. We show that CommVQA poses a challenge for current
models. Providing contextual information to VQA models improves performance
broadly, highlighting the relevance of situating systems within a communicative
scenario.
\\ ( https://arxiv.org/abs/2402.15002 ,  10552kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15010
Date: Thu, 22 Feb 2024 23:11:08 GMT   (4272kb,D)

Title: How Important Is Tokenization in French Medical Masked Language Models?
Authors: Yanis Labrak, Adrien Bazoge, Beatrice Daille, Mickael Rouvier, Richard
  Dufour
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-Coling 2024
\\
  Subword tokenization has become the prevailing standard in the field of
natural language processing (NLP) over recent years, primarily due to the
widespread utilization of pre-trained language models. This shift began with
Byte-Pair Encoding (BPE) and was later followed by the adoption of
SentencePiece and WordPiece. While subword tokenization consistently
outperforms character and word-level tokenization, the precise factors
contributing to its success remain unclear. Key aspects such as the optimal
segmentation granularity for diverse tasks and languages, the influence of data
sources on tokenizers, and the role of morphological information in
Indo-European languages remain insufficiently explored. This is particularly
pertinent for biomedical terminology, characterized by specific rules governing
morpheme combinations. Despite the agglutinative nature of biomedical
terminology, existing language models do not explicitly incorporate this
knowledge, leading to inconsistent tokenization strategies for common terms. In
this paper, we seek to delve into the complexities of subword tokenization in
French biomedical domain across a variety of NLP tasks and pinpoint areas where
further enhancements can be made. We analyze classical tokenization algorithms,
including BPE and SentencePiece, and introduce an original tokenization
strategy that integrates morpheme-enriched word segmentation into existing
tokenization methods.
\\ ( https://arxiv.org/abs/2402.15010 ,  4272kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15012
Date: Thu, 22 Feb 2024 23:11:17 GMT   (489kb,D)

Title: Ar-Spider: Text-to-SQL in Arabic
Authors: Saleh Almohaimeed, Saad Almohaimeed, Mansour Al Ghanim, Liqiang Wang
Categories: cs.CL cs.AI
Comments: ACM SAC Conference (SAC 24)
DOI: 10.1145/3605098.3636065.
\\
  In Natural Language Processing (NLP), one of the most important tasks is
text-to-SQL semantic parsing, which focuses on enabling users to interact with
the database in a more natural manner. In recent years, text-to-SQL has made
significant progress, but most were English-centric. In this paper, we
introduce Ar-Spider 1, the first Arabic cross-domain text-to-SQL dataset. Due
to the unique nature of the language, two major challenges have been
encountered, namely schema linguistic and SQL structural challenges. In order
to handle these issues and conduct the experiments, we adopt two baseline
models LGESQL [4] and S2SQL [12], both of which are tested with two
cross-lingual models to alleviate the effects of schema linguistic and SQL
structure linking challenges. The baselines demonstrate decent single-language
performance on our Arabic text-to-SQL dataset, Ar-Spider, achieving 62.48% for
S2SQL and 65.57% for LGESQL, only 8.79% below the highest results achieved by
the baselines when trained in English dataset. To achieve better performance on
Arabic text-to-SQL, we propose the context similarity relationship (CSR)
approach, which results in a significant increase in the overall performance of
about 1.52% for S2SQL and 1.06% for LGESQL and closes the gap between Arabic
and English languages to 7.73%.
\\ ( https://arxiv.org/abs/2402.15012 ,  489kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15018
Date: Thu, 22 Feb 2024 23:31:22 GMT   (3515kb,D)

Title: Unintended Impacts of LLM Alignment on Global Representation
Authors: Michael J. Ryan, William Held, Diyi Yang
Categories: cs.CL cs.CY cs.LG
\\
  Before being deployed for user-facing applications, developers align Large
Language Models (LLMs) to user preferences through a variety of procedures,
such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference
Optimization (DPO). Current evaluations of these procedures focus on benchmarks
of instruction following, reasoning, and truthfulness. However, human
preferences are not universal, and aligning to specific preference sets may
have unintended effects. We explore how alignment impacts performance along
three axes of global representation: English dialects, multilingualism, and
opinions from and about countries worldwide. Our results show that current
alignment procedures create disparities between English dialects and global
opinions. We find alignment improves capabilities in several languages. We
conclude by discussing design decisions that led to these unintended impacts
and recommendations for more equitable preference tuning.
\\ ( https://arxiv.org/abs/2402.15018 ,  3515kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15043
Date: Fri, 23 Feb 2024 01:30:39 GMT   (300kb,D)

Title: KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large
  Language Models
Authors: Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang,
  Xing Xie, Yue Zhang, Shikun Zhang
Categories: cs.CL cs.AI cs.LG
Comments: 19 pages, 5 figures, our code is available at:
  https://github.com/zhuohaoyu/KIEval
\\
  Automatic evaluation methods for large language models (LLMs) are hindered by
data contamination, leading to inflated assessments of their effectiveness.
Existing strategies, which aim to detect contaminated texts, focus on
quantifying contamination status instead of accurately gauging model
performance. In this paper, we introduce KIEval, a Knowledge-grounded
Interactive Evaluation framework, which incorporates an LLM-powered
"interactor" role for the first time to accomplish a dynamic
contamination-resilient evaluation. Starting with a question in a conventional
LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically
generated, multi-round, and knowledge-focused dialogues to determine whether a
model's response is merely a recall of benchmark answers or demonstrates a deep
comprehension to apply knowledge in more complex conversations. Extensive
experiments on seven leading LLMs across five datasets validate KIEval's
effectiveness and generalization. We also reveal that data contamination brings
no contribution or even negative effect to models' real-world applicability and
understanding, and existing contamination detection methods for LLMs can only
identify contamination in pre-training but not during supervised fine-tuning.
\\ ( https://arxiv.org/abs/2402.15043 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15046
Date: Fri, 23 Feb 2024 01:49:38 GMT   (1717kb,D)

Title: CARBD-Ko: A Contextually Annotated Review Benchmark Dataset for
  Aspect-Level Sentiment Classification in Korean
Authors: Dongjun Jang, Jean Seo, Sungjoo Byun, Taekyoung Kim, Minseok Kim,
  Hyopil Shin
Categories: cs.CL
\\
  This paper explores the challenges posed by aspect-based sentiment
classification (ABSC) within pretrained language models (PLMs), with a
particular focus on contextualization and hallucination issues. In order to
tackle these challenges, we introduce CARBD-Ko (a Contextually Annotated Review
Benchmark Dataset for Aspect-Based Sentiment Classification in Korean), a
benchmark dataset that incorporates aspects and dual-tagged polarities to
distinguish between aspect-specific and aspect-agnostic sentiment
classification. The dataset consists of sentences annotated with specific
aspects, aspect polarity, aspect-agnostic polarity, and the intensity of
aspects. To address the issue of dual-tagged aspect polarities, we propose a
novel approach employing a Siamese Network. Our experimental findings highlight
the inherent difficulties in accurately predicting dual-polarities and
underscore the significance of contextualized sentiment analysis models. The
CARBD-Ko dataset serves as a valuable resource for future research endeavors in
aspect-level sentiment classification.
\\ ( https://arxiv.org/abs/2402.15046 ,  1717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15048
Date: Fri, 23 Feb 2024 01:55:35 GMT   (9985kb,D)

Title: Unlocking the Power of Large Language Models for Entity Alignment
Authors: Xuhui Jiang, Yinghan Shen, Zhichao Shi, Chengjin Xu, Wei Li, Zixuan
  Li, Jian Guo, Huawei Shen, Yuanzhuo Wang
Categories: cs.CL cs.AI
\\
  Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG)
data, playing a crucial role in data-driven AI applications. Traditional EA
methods primarily rely on comparing entity embeddings, but their effectiveness
is constrained by the limited input KG data and the capabilities of the
representation learning techniques. Against this backdrop, we introduce ChatEA,
an innovative framework that incorporates large language models (LLMs) to
improve EA. To address the constraints of limited input KG data, ChatEA
introduces a KG-code translation module that translates KG structures into a
format understandable by LLMs, thereby allowing LLMs to utilize their extensive
background knowledge to improve EA accuracy. To overcome the over-reliance on
entity embedding comparisons, ChatEA implements a two-stage EA strategy that
capitalizes on LLMs' capability for multi-step reasoning in a dialogue format,
thereby enhancing accuracy while preserving efficiency. Our experimental
results affirm ChatEA's superior performance, highlighting LLMs' potential in
facilitating EA tasks.
\\ ( https://arxiv.org/abs/2402.15048 ,  9985kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15052
Date: Fri, 23 Feb 2024 02:05:46 GMT   (1044kb,D)

Title: ToMBench: Benchmarking Theory of Mind in Large Language Models
Authors: Zhuang Chen, Jincenzi Wu, Jinfeng Zhou, Bosi Wen, Guanqun Bi, Gongyao
  Jiang, Yaru Cao, Mengting Hu, Yunghwei Lai, Zexuan Xiong, Minlie Huang
Categories: cs.CL cs.AI
Comments: Under review
\\
  Theory of Mind (ToM) is the cognitive capability to perceive and ascribe
mental states to oneself and others. Recent research has sparked a debate over
whether large language models (LLMs) exhibit a form of ToM. However, existing
ToM evaluations are hindered by challenges such as constrained scope,
subjective judgment, and unintended contamination, yielding inadequate
assessments. To address this gap, we introduce ToMBench with three key
characteristics: a systematic evaluation framework encompassing 8 tasks and 31
abilities in social cognition, a multiple-choice question format to support
automated and unbiased evaluation, and a build-from-scratch bilingual inventory
to strictly avoid data leakage. Based on ToMBench, we conduct extensive
experiments to evaluate the ToM performance of 10 popular LLMs across tasks and
abilities. We find that even the most advanced LLMs like GPT-4 lag behind human
performance by over 10% points, indicating that LLMs have not achieved a
human-level theory of mind yet. Our aim with ToMBench is to enable an efficient
and effective evaluation of LLMs' ToM capabilities, thereby facilitating the
development of LLMs with inherent social intelligence.
\\ ( https://arxiv.org/abs/2402.15052 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15055
Date: Fri, 23 Feb 2024 02:15:47 GMT   (4197kb,D)

Title: Interpreting Context Look-ups in Transformers: Investigating
  Attention-MLP Interactions
Authors: Clement Neo, Shay B. Cohen, Fazl Barez
Categories: cs.CL cs.AI cs.LG
Comments: 15 pages, 11 figures
\\
  In this paper, we investigate the interplay between attention heads and
specialized "next-token" neurons in the Multilayer Perceptron that predict
specific tokens. By prompting an LLM like GPT-4 to explain these model
internals, we can elucidate attention mechanisms that activate certain
next-token neurons. Our analysis identifies attention heads that recognize
contexts relevant to predicting a particular token, activating the associated
neuron through the residual connection. We focus specifically on heads in
earlier layers consistently activating the same next-token neuron across
similar prompts. Exploring these differential activation patterns reveals that
heads that specialize for distinct linguistic contexts are tied to generating
certain tokens. Overall, our method combines neural explanations and probing
isolated components to illuminate how attention enables context-dependent,
specialized processing in LLMs.
\\ ( https://arxiv.org/abs/2402.15055 ,  4197kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15057
Date: Fri, 23 Feb 2024 02:18:12 GMT   (742kb,D)

Title: On the Multi-turn Instruction Following for Conversational Web Agents
Authors: Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng,
  Tat-Seng Chua
Categories: cs.CL cs.AI
\\
  Web agents powered by Large Language Models (LLMs) have demonstrated
remarkable abilities in planning and executing multi-step interactions within
complex web-based environments, fulfilling a wide range of web navigation
tasks. Despite these advancements, the potential for LLM-powered agents to
effectively engage with sequential user instructions in real-world scenarios
has not been fully explored. In this work, we introduce a new task of
Conversational Web Navigation, which necessitates sophisticated interactions
that span multiple turns with both the users and the environment, supported by
a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To
tackle the limited context length of LLMs and the context-dependency issue of
the conversational tasks, we further propose a novel framework, named
self-reflective memory-augmented planning (Self-MAP), which employs memory
utilization and self-reflection techniques. Extensive experiments are conducted
to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the
proposed method.
\\ ( https://arxiv.org/abs/2402.15057 ,  742kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15059
Date: Fri, 23 Feb 2024 02:21:24 GMT   (822kb,D)

Title: ColBERT-XM: A Modular Multi-Vector Representation Model for Zero-Shot
  Multilingual Information Retrieval
Authors: Antoine Louis, Vageesh Saxena, Gijs van Dijck, Gerasimos Spanakis
Categories: cs.CL cs.IR
Comments: Under review. Code is available at
  https://github.com/ant-louis/xm-retrievers
\\
  State-of-the-art neural retrievers predominantly focus on high-resource
languages like English, which impedes their adoption in retrieval scenarios
involving other languages. Current approaches circumvent the lack of
high-quality labeled data in non-English languages by leveraging multilingual
pretrained language models capable of cross-lingual transfer. However, these
models require substantial task-specific fine-tuning across multiple languages,
often perform poorly in languages with minimal representation in the
pretraining corpus, and struggle to incorporate new languages after the
pretraining phase. In this work, we present a novel modular dense retrieval
model that learns from the rich data of a single high-resource language and
effectively zero-shot transfers to a wide array of languages, thereby
eliminating the need for language-specific labeled data. Our model, ColBERT-XM,
demonstrates competitive performance against existing state-of-the-art
multilingual retrievers trained on more extensive datasets in various
languages. Further analysis reveals that our modular approach is highly
data-efficient, effectively adapts to out-of-distribution data, and
significantly reduces energy consumption and carbon emissions. By demonstrating
its proficiency in zero-shot scenarios, ColBERT-XM marks a shift towards more
sustainable and inclusive retrieval systems, enabling effective information
accessibility in numerous languages. We publicly release our code and models
for the community.
\\ ( https://arxiv.org/abs/2402.15059 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15061
Date: Fri, 23 Feb 2024 02:24:15 GMT   (3007kb,D)

Title: Fine-tuning Large Language Models for Domain-specific Machine
  Translation
Authors: Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang
  and Shikai Wu
Categories: cs.CL cs.LG
Comments: 9 pages, 6 figures, 6tables
\\
  Large language models (LLMs) have made significant progress in machine
translation (MT). However, their potential in domain-specific MT remains
under-explored. Current LLM-based MT systems still face several challenges.
First, for LLMs with in-context learning, their effectiveness is highly
sensitive to input translation examples, and processing them can increase
inference costs. They often require extra post-processing due to
over-generation. Second, LLMs with fine-tuning on domain-specific data often
require high training costs for domain adaptation, and may weaken the zero-shot
MT capabilities of LLMs due to over-specialization. The aforementioned methods
can struggle to translate rare words in domain transfer scenarios. To address
these challenges, this paper proposes a prompt-oriented fine-tuning method,
denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose
LLM for domain-specific MT tasks. First, we construct a task-specific
mix-domain dataset, which is then used to fine-tune the LLM with LoRA. This can
eliminate the need for input translation examples, post-processing, or
over-specialization. By zero-shot prompting with instructions, we adapt the MT
tasks to the target domain at inference time. To further elicit the MT
capability for rare words, we construct new prompts by incorporating
domain-specific bilingual vocabulary. We also conduct extensive experiments on
both publicly available and self-constructed datasets. The results show that
our LlamaIT can significantly enhance the domain-specific MT capabilities of
the LLM, meanwhile preserving its zero-shot MT capabilities.
\\ ( https://arxiv.org/abs/2402.15061 ,  3007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15062
Date: Fri, 23 Feb 2024 02:24:36 GMT   (711kb,D)

Title: Gotcha! Don't trick me with unanswerable questions! Self-aligning Large
  Language Models for Responding to Unknown Questions
Authors: Yang Deng, Yong Zhao, Moxin Li, See-Kiong Ng, Tat-Seng Chua
Categories: cs.CL cs.LG
\\
  Despite the remarkable abilities of Large Language Models (LLMs) to answer
questions, they often display a considerable level of overconfidence even when
the question does not have a definitive answer. To avoid providing hallucinated
answers to these unknown questions, existing studies typically investigate
approaches to refusing to answer these questions. In this work, we propose a
novel and scalable self-alignment method to utilize the LLM itself to enhance
its response-ability to different types of unknown questions, being capable of
not only refusing to answer but also providing explanation to the
unanswerability of unknown questions. Specifically, the Self-Align method first
employ a two-stage class-aware self-augmentation approach to generate a large
amount of unknown question-response data. Then we conduct disparity-driven
self-curation to select qualified data for fine-tuning the LLM itself for
aligning the responses to unknown questions as desired. Experimental results on
two datasets across four types of unknown questions validate the superiority of
the Self-Align method over existing baselines in terms of three types of task
formulation.
\\ ( https://arxiv.org/abs/2402.15062 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15080
Date: Fri, 23 Feb 2024 03:53:39 GMT   (8362kb,D)

Title: Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient
  Framework for Multi-level Implicit Discourse Relation Recognition
Authors: Haodong Zhao, Ruifang He, Mengnan Xiao and Jing Xu
Categories: cs.CL
Comments: accepted to ACL 2023
\\
  Multi-level implicit discourse relation recognition (MIDRR) aims at
identifying hierarchical discourse relations among arguments. Previous methods
achieve the promotion through fine-tuning PLMs. However, due to the data
scarcity and the task gap, the pre-trained feature space cannot be accurately
tuned to the task-specific space, which even aggravates the collapse of the
vanilla space. Besides, the comprehension of hierarchical semantics for MIDRR
makes the conversion much harder. In this paper, we propose a prompt-based
Parameter-Efficient Multi-level IDRR (PEMI) framework to solve the above
problems. First, we leverage parameter-efficient prompt tuning to drive the
inputted arguments to match the pre-trained space and realize the approximation
with few parameters. Furthermore, we propose a hierarchical label refining
(HLR) method for the prompt verbalizer to deeply integrate hierarchical
guidance into the prompt tuning. Finally, our model achieves comparable results
on PDTB 2.0 and 3.0 using about 0.1% trainable parameters compared with
baselines and the visualization demonstrates the effectiveness of our HLR
method.
\\ ( https://arxiv.org/abs/2402.15080 ,  8362kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15082
Date: Fri, 23 Feb 2024 03:59:18 GMT   (7136kb,D)

Title: PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables
  Parameter-Efficient Transfer Learning
Authors: Zhisheng Lin, Han Fu, Chenghao Liu, Zhuo Li, Jianling Sun
Categories: cs.CL cs.LG
\\
  Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for
adapting pre-trained language models to various tasks efficiently. Recently,
there has been a growing interest in transferring knowledge from one or
multiple tasks to the downstream target task to achieve performance
improvements. However, current approaches typically either train adapters on
individual tasks or distill shared knowledge from source tasks, failing to
fully exploit task-specific knowledge and the correlation between source and
target tasks. To overcome these limitations, we propose PEMT, a novel
parameter-efficient fine-tuning framework based on multi-task transfer
learning. PEMT extends the mixture-of-experts (MoE) framework to capture the
transferable knowledge as a weighted combination of adapters trained on source
tasks. These weights are determined by a gated unit, measuring the correlation
between the target and each source task using task description prompt vectors.
To fully exploit the task-specific knowledge, we also propose the Task Sparsity
Loss to improve the sparsity of the gated unit. We conduct experiments on a
broad range of tasks over 17 datasets. The experimental results demonstrate our
PEMT yields stable improvements over full fine-tuning, and state-of-the-art
PEFT and knowledge transferring methods on various tasks. The results highlight
the effectiveness of our method which is capable of sufficiently exploiting the
knowledge and correlation features across multiple tasks.
\\ ( https://arxiv.org/abs/2402.15082 ,  7136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15089
Date: Fri, 23 Feb 2024 04:23:33 GMT   (9232kb,D)

Title: AttributionBench: How Hard is Automatic Attribution Evaluation?
Authors: Yifei Li, Xiang Yue, Zeyi Liao, Huan Sun
Categories: cs.CL cs.AI cs.LG
\\
  Modern generative search engines enhance the reliability of large language
model (LLM) responses by providing cited evidence. However, evaluating the
answer's attribution, i.e., whether every claim within the generated responses
is fully supported by its cited evidence, remains an open problem. This
verification, traditionally dependent on costly human evaluation, underscores
the urgent need for automatic attribution evaluation methods. To bridge the gap
in the absence of standardized benchmarks for these methods, we present
AttributionBench, a comprehensive benchmark compiled from various existing
attribution datasets. Our extensive experiments on AttributionBench reveal the
challenges of automatic attribution evaluation, even for state-of-the-art LLMs.
Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves
around 80% macro-F1 under a binary classification formulation. A detailed
analysis of more than 300 error cases indicates that a majority of failures
stem from the model's inability to process nuanced information, and the
discrepancy between the information the model has access to and that human
annotators do.
\\ ( https://arxiv.org/abs/2402.15089 ,  9232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15131
Date: Fri, 23 Feb 2024 06:32:18 GMT   (844kb,D)

Title: Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question
  Answering with Large Language Models
Authors: Guanming Xiong, Junwei Bao, Wen Zhao
Categories: cs.CL cs.AI
Comments: Codes will be released upon acceptance
ACM-class: I.2.7
\\
  This study explores the realm of knowledge-base question answering (KBQA).
KBQA is considered a challenging task, particularly in parsing intricate
questions into executable logical forms. Traditional semantic parsing
(SP)-based methods require extensive data annotations, which result in
significant costs. Recently, the advent of few-shot in-context learning,
powered by large language models (LLMs), has showcased promising capabilities.
Yet, fully leveraging LLMs to parse questions into logical forms in
low-resource scenarios poses a substantial challenge. To tackle these hurdles,
we introduce Interactive-KBQA, a framework designed to generate logical forms
through direct interaction with knowledge bases (KBs). Within this framework,
we have developed three generic APIs for KB interaction. For each category of
complex question, we devised exemplars to guide LLMs through the reasoning
processes. Our method achieves competitive results on the WebQuestionsSP,
ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of
examples (shots). Importantly, our approach supports manual intervention,
allowing for the iterative refinement of LLM outputs. By annotating a dataset
with step-wise reasoning processes, we showcase our model's adaptability and
highlight its potential for contributing significant enhancements to the field.
\\ ( https://arxiv.org/abs/2402.15131 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15132
Date: Fri, 23 Feb 2024 06:33:51 GMT   (180kb,D)

Title: Improving Sentence Embeddings with an Automatically Generated NLI
  Dataset
Authors: Soma Sato, Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda
Categories: cs.CL cs.LG
\\
  Decoder-based large language models (LLMs) have shown high performance on
many tasks in natural language processing. This is also true for sentence
embedding learning, where a decoder-based model, PromptEOL, has achieved the
best performance on semantic textual similarity (STS) tasks. However, PromptEOL
makes great use of fine-tuning with a manually annotated natural language
inference (NLI) dataset. We aim to improve sentence embeddings learned in an
unsupervised setting by automatically generating an NLI dataset with an LLM and
using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed
method achieved an average Spearman's rank correlation coefficient of 82.21
with respect to human evaluation, thus outperforming existing methods without
using large, manually annotated datasets.
\\ ( https://arxiv.org/abs/2402.15132 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15153
Date: Fri, 23 Feb 2024 07:28:31 GMT   (7075kb,D)

Title: Self-Adaptive Reconstruction with Contrastive Learning for Unsupervised
  Sentence Embeddings
Authors: Junlong Liu, Xichen Shang, Huawen Feng, Junhao Zheng, Qianli Ma
Categories: cs.CL cs.LG
Comments: 8 pages, 3 figures
\\
  Unsupervised sentence embeddings task aims to convert sentences to semantic
vector representations. Most previous works directly use the sentence
representations derived from pretrained language models. However, due to the
token bias in pretrained language models, the models can not capture the
fine-grained semantics in sentences, which leads to poor predictions. To
address this issue, we propose a novel Self-Adaptive Reconstruction Contrastive
Sentence Embeddings (SARCSE) framework, which reconstructs all tokens in
sentences with an AutoEncoder to help the model to preserve more fine-grained
semantics during tokens aggregating. In addition, we proposed a self-adaptive
reconstruction loss to alleviate the token bias towards frequency. Experimental
results show that SARCSE gains significant improvements compared with the
strong baseline SimCSE on the 7 STS tasks.
\\ ( https://arxiv.org/abs/2402.15153 ,  7075kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15159
Date: Fri, 23 Feb 2024 07:43:26 GMT   (6971kb,D)

Title: Machine Unlearning of Pre-trained Large Language Models
Authors: Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng,
  Xiang Yue
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: Code will be available at https://github.com/yaojin17/Unlearning_LLM
\\
  This study investigates the concept of the `right to be forgotten' within the
context of large language models (LLMs). We explore machine unlearning as a
pivotal solution, with a focus on pre-trained models--a notably
under-researched area. Our research delineates a comprehensive framework for
machine unlearning in pre-trained LLMs, encompassing a critical analysis of
seven diverse unlearning methods. Through rigorous evaluation using curated
datasets from arXiv, books, and GitHub, we establish a robust benchmark for
unlearning performance, demonstrating that these methods are over $10^5$ times
more computationally efficient than retraining. Our results show that
integrating gradient ascent with gradient descent on in-distribution data
improves hyperparameter robustness. We also provide detailed guidelines for
efficient hyperparameter tuning in the unlearning process. Our findings advance
the discourse on ethical AI practices, offering substantive insights into the
mechanics of machine unlearning for pre-trained LLMs and underscoring the
potential for responsible AI development.
\\ ( https://arxiv.org/abs/2402.15159 ,  6971kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15162
Date: Fri, 23 Feb 2024 07:53:39 GMT   (8607kb,D)

Title: Entity-level Factual Adaptiveness of Fine-tuning based Abstractive
  Summarization Models
Authors: Jongyoon Song, Nohil Park, Bongkyu Hwang, Jaewoong Yun, Seongho Joe,
  Youngjune L. Gwon, Sungroh Yoon
Categories: cs.CL cs.AI cs.LG
Comments: EACL 2024
\\
  Abstractive summarization models often generate factually inconsistent
content particularly when the parametric knowledge of the model conflicts with
the knowledge in the input document. In this paper, we analyze the robustness
of fine-tuning based summarization models to the knowledge conflict, which we
call factual adaptiveness. We utilize pre-trained language models to construct
evaluation sets and find that factual adaptiveness is not strongly correlated
with factual consistency on original datasets. Furthermore, we introduce a
controllable counterfactual data augmentation method where the degree of
knowledge conflict within the augmented data can be adjustable. Our
experimental results on two pre-trained language models (PEGASUS and BART) and
two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method
enhances factual adaptiveness while achieving factual consistency on original
datasets on par with the contrastive learning baseline.
\\ ( https://arxiv.org/abs/2402.15162 ,  8607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15189
Date: Fri, 23 Feb 2024 08:40:38 GMT   (934kb,D)

Title: Biomedical Entity Linking as Multiple Choice Question Answering
Authors: Zhenxi Lin, Ziheng Zhang, Xian Wu, Yefeng Zheng
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by COLING 2024
\\
  Although biomedical entity linking (BioEL) has made significant progress with
pre-trained language models, challenges still exist for fine-grained and
long-tailed entities. To address these challenges, we present BioELQA, a novel
model that treats Biomedical Entity Linking as Multiple Choice Question
Answering. BioELQA first obtains candidate entities with a fast retriever,
jointly presents the mention and candidate entities to a generator, and then
outputs the predicted symbol associated with its chosen entity. This
formulation enables explicit comparison of different candidate entities, thus
capturing fine-grained interactions between mentions and entities, as well as
among entities themselves. To improve generalization for long-tailed entities,
we retrieve similar labeled training instances as clues and concatenate the
input with retrieved instances for the generator. Extensive experimental
results show that BioELQA outperforms state-of-the-art baselines on several
datasets.
\\ ( https://arxiv.org/abs/2402.15189 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15200
Date: Fri, 23 Feb 2024 09:01:00 GMT   (382kb,D)

Title: DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be
  Better Context-aware Translators
Authors: Xinglin Lyu, Junhui Li, Yanqing Zhao, Min Zhang, Daimeng Wei, Shimin
  Tao, Hao Yang and Min Zhang
Categories: cs.CL
Comments: under reviewing
\\
  Generally, the decoder-only large language models (LLMs) are adapted to
context-aware neural machine translation (NMT) in a concatenating way, where
LLMs take the concatenation of the source sentence (i.e., intra-sentence
context) and the inter-sentence context as the input, and then to generate the
target tokens sequentially. This adaptation strategy, i.e., concatenation mode,
considers intra-sentence and inter-sentence contexts with the same priority,
despite an apparent difference between the two kinds of contexts. In this
paper, we propose an alternative adaptation approach, named Decoding-enhanced
Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and
utilize the inter- and intra-sentence context and more effectively adapt LLMs
to context-aware NMT. First, DeMPT divides the context-aware NMT process into
three separate phases. During each phase, different continuous prompts are
introduced to make LLMs discriminately model various information. Second, DeMPT
employs a heuristic way to further discriminately enhance the utilization of
the source-side inter- and intra-sentence information at the final decoding
phase. Experiments show that our approach significantly outperforms the
concatenation method, and further improves the performance of LLMs in discourse
modeling.
\\ ( https://arxiv.org/abs/2402.15200 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15202
Date: Fri, 23 Feb 2024 09:04:48 GMT   (461kb,D)

Title: Fine-Grained Detoxification via Instance-Level Prefixes for Large
  Language Models
Authors: Xin Yi and Linlin Wang and Xiaoling Wang and Liang He
Categories: cs.CL
\\
  Impressive results have been achieved in natural language processing (NLP)
tasks through the training of large language models (LLMs). However, these
models occasionally produce toxic content such as insults, threats, and
profanity in response to certain prompts, thereby constraining their practical
utility. To tackle this issue, various finetuning-based and decoding-based
approaches have been utilized to mitigate toxicity. However, these methods
typically necessitate additional costs such as high-quality training data or
auxiliary models. In this paper, we propose fine-grained detoxification via
instance-level prefixes (FGDILP) to mitigate toxic text without additional
cost. Specifically, FGDILP contrasts the contextualized representation in
attention space using a positive prefix-prepended prompt against multiple
negative prefix-prepended prompts at the instance level. This allows for
constructing fine-grained subtoxicity vectors, which enables collaborative
detoxification by fusing them to correct the normal generation process when
provided with a raw prompt. We validate that FGDILP enables controlled text
generation with regard to toxicity at both the utterance and context levels.
Our method surpasses prompt-based baselines in detoxification, although at a
slight cost to generation fluency and diversity.
\\ ( https://arxiv.org/abs/2402.15202 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15238
Date: Fri, 23 Feb 2024 10:02:01 GMT   (1586kb,D)

Title: GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech
  Detection?
Authors: Yiping Jin, Leo Wanner, Alexander Shvets
Categories: cs.CL cs.CY
Comments: Accepted to LREC-COLING 2024. Content Warning: This paper contains
  model outputs that are offensive in nature
\\
  Online hate detection suffers from biases incurred in data sampling,
annotation, and model pre-training. Therefore, measuring the averaged
performance over all examples in held-out test data is inadequate. Instead, we
must identify specific model weaknesses and be informed when it is more likely
to fail. A recent proposal in this direction is HateCheck, a suite for testing
fine-grained model functionalities on synthesized data generated using
templates of the kind "You are just a [slur] to me." However, despite enabling
more detailed diagnostic insights, the HateCheck test cases are often generic
and have simplistic sentence structures that do not match the real-world data.
To address this limitation, we propose GPT-HateCheck, a framework to generate
more diverse and realistic functional tests from scratch by instructing large
language models (LLMs). We employ an additional natural language inference
(NLI) model to verify the generations. Crowd-sourced annotation demonstrates
that the generated test cases are of high quality. Using the new functional
tests, we can uncover model weaknesses that would be overlooked using the
original HateCheck dataset.
\\ ( https://arxiv.org/abs/2402.15238 ,  1586kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15248
Date: Fri, 23 Feb 2024 10:27:42 GMT   (242kb,D)

Title: Chitchat as Interference: Adding User Backstories to Task-Oriented
  Dialogues
Authors: Armand Stricker, Patrick Paroubek
Categories: cs.CL
Comments: Accepted @ LREC-COLING 2024
\\
  During task-oriented dialogues (TODs), human users naturally introduce
chitchat that is beyond the immediate scope of the task, interfering with the
flow of the conversation. To address this issue without the need for expensive
manual data creation, we use few-shot prompting with Llama-2-70B to enhance the
MultiWOZ dataset with user backstories, a typical example of chitchat
interference in TODs. We assess the impact of this addition by testing two
models: one trained solely on TODs and another trained on TODs with a
preliminary chitchat interaction. Our analysis reveals that our enriched
dataset poses a significant challenge to these systems. Moreover, we
demonstrate that our dataset can be effectively used for training purposes,
enabling a system to consistently acknowledge the user's backstory while also
successfully moving the task forward in the same turn, as confirmed by human
evaluation. These findings highlight the benefits of generating novel
chitchat-TOD scenarios to test TOD systems more thoroughly and improve their
resilience to natural user interferences.
\\ ( https://arxiv.org/abs/2402.15248 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15264
Date: Fri, 23 Feb 2024 11:24:00 GMT   (5978kb,D)

Title: DEEM: Dynamic Experienced Expert Modeling for Stance Detection
Authors: Xiaolong Wang, Yile Wang, Sijie Cheng, Peng Li, Yang Liu
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Recent work has made a preliminary attempt to use large language models
(LLMs) to solve the stance detection task, showing promising results. However,
considering that stance detection usually requires detailed background
knowledge, the vanilla reasoning method may neglect the domain knowledge to
make a professional and accurate analysis. Thus, there is still room for
improvement of LLMs reasoning, especially in leveraging the generation
capability of LLMs to simulate specific experts (i.e., multi-agents) to detect
the stance. In this paper, different from existing multi-agent works that
require detailed descriptions and use fixed experts, we propose a Dynamic
Experienced Expert Modeling (DEEM) method which can leverage the generated
experienced experts and let LLMs reason in a semi-parametric way, making the
experts more generalizable and reliable. Experimental results demonstrate that
DEEM consistently achieves the best results on three standard benchmarks,
outperforms methods with self-consistency reasoning, and reduces the bias of
LLMs.
\\ ( https://arxiv.org/abs/2402.15264 ,  5978kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15268
Date: Fri, 23 Feb 2024 11:30:39 GMT   (1282kb,D)

Title: MemoryPrompt: A Light Wrapper to Improve Context Tracking in Pre-trained
  Language Models
Authors: Nathana\"el Carraz Rakotonirina, Marco Baroni
Categories: cs.CL cs.AI cs.LG
Comments: Published as conference paper at LREC-COLING 2024
\\
  Transformer-based language models (LMs) track contextual information through
large, hard-coded input windows. We introduce MemoryPrompt, a leaner approach
in which the LM is complemented by a small auxiliary recurrent network that
passes information to the LM by prefixing its regular input with a sequence of
vectors, akin to soft prompts, without requiring LM finetuning. Tested on a
task designed to probe a LM's ability to keep track of multiple fact updates, a
MemoryPrompt-augmented LM outperforms much larger LMs that have access to the
full input history. We also test MemoryPrompt on a long-distance dialogue
dataset, where its performance is comparable to that of a model conditioned on
the entire conversation history. In both experiments we also observe that,
unlike full-finetuning approaches, MemoryPrompt does not suffer from
catastrophic forgetting when adapted to new tasks, thus not disrupting the
generalist capabilities of the underlying LM.
\\ ( https://arxiv.org/abs/2402.15268 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15289
Date: Fri, 23 Feb 2024 12:35:43 GMT   (1000kb,D)

Title: Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis
  with Diffusion Models
Authors: Shunyu Liu, Jie Zhou, Qunxi Zhu, Qin Chen, Qingchun Bai, Jun Xiao,
  Liang He
Categories: cs.CL cs.LG
Comments: Accepted to LREC-COLING 2024, submission version
\\
  Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting
the sentiment polarity associated with identified aspects within text. However,
a notable challenge in ABSA lies in precisely determining the aspects'
boundaries (start and end indices), especially for long ones, due to users'
colloquial expressions. We propose DiffusionABSA, a novel diffusion model
tailored for ABSA, which extracts the aspects progressively step by step.
Particularly, DiffusionABSA gradually adds noise to the aspect terms in the
training process, subsequently learning a denoising process that progressively
restores these terms in a reverse manner. To estimate the boundaries, we design
a denoising neural network enhanced by a syntax-aware temporal attention
mechanism to chronologically capture the interplay between aspects and
surrounding text. Empirical evaluations conducted on eight benchmark datasets
underscore the compelling advantages offered by DiffusionABSA when compared
against robust baseline models. Our code is publicly available at
https://github.com/Qlb6x/DiffusionABSA.
\\ ( https://arxiv.org/abs/2402.15289 ,  1000kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15301
Date: Fri, 23 Feb 2024 13:02:10 GMT   (7324kb,D)

Title: Causal Graph Discovery with Retrieval-Augmented Generation based Large
  Language Models
Authors: Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang
Categories: cs.CL cs.LG stat.ME
\\
  Causal graph recovery is essential in the field of causal inference.
Traditional methods are typically knowledge-based or statistical
estimation-based, which are limited by data collection biases and individuals'
knowledge about factors affecting the relations between variables of interests.
The advance of large language models (LLMs) provides opportunities to address
these problems. We propose a novel method that utilizes the extensive knowledge
contained within a large corpus of scientific literature to deduce causal
relationships in general causal graph recovery tasks. This method leverages
Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and
extract pertinent information from a comprehensive collection of research
papers. Our method first retrieves relevant text chunks from the aggregated
literature. Then, the LLM is tasked with identifying and labelling potential
associations between factors. Finally, we give a method to aggregate the
associational relationships to build a causal graph. We demonstrate our method
is able to construct high quality causal graphs on the well-known SACHS dataset
solely from literature.
\\ ( https://arxiv.org/abs/2402.15301 ,  7324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15302
Date: Fri, 23 Feb 2024 13:03:12 GMT   (790kb,D)

Title: How (un)ethical are instruction-centric responses of LLMs? Unveiling the
  vulnerabilities of safety guardrails to harmful queries
Authors: Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee
Categories: cs.CL cs.CR
\\
  In this study, we tackle a growing concern around the safety and ethical use
of large language models (LLMs). Despite their potential, these models can be
tricked into producing harmful or unethical content through various
sophisticated methods, including 'jailbreaking' techniques and targeted
manipulation. Our work zeroes in on a specific issue: to what extent LLMs can
be led astray by asking them to generate responses that are instruction-centric
such as a pseudocode, a program or a software snippet as opposed to vanilla
text. To investigate this question, we introduce TechHazardQA, a dataset
containing complex queries which should be answered in both text and
instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers
for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,
Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and
instruction-centric responses. For evaluation we report the harmfulness score
metric as well as judgements from GPT-4 and humans. Overall, we observe that
asking LLMs to produce instruction-centric responses enhances the unethical
response generation by ~2-38% across the models. As an additional objective, we
investigate the impact of model editing using the ROME technique, which further
increases the propensity for generating undesirable content. In particular,
asking edited LLMs to generate instruction-centric responses further increases
the unethical response generation by ~3-16% across the different models.
\\ ( https://arxiv.org/abs/2402.15302 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15313
Date: Fri, 23 Feb 2024 13:32:47 GMT   (534kb,D)

Title: ArabianGPT: Native Arabic GPT-based Large Language
Authors: Anis Koubaa, Adel Ammar, Lahouari Ghouti, Omar Najar, Serry Sibaee
Categories: cs.CL cs.AI cs.LG
\\
  The predominance of English and Latin-based large language models (LLMs) has
led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated
by the prevalent inclusion of English tokens in existing Arabic models,
detracting from their efficacy in processing native Arabic's intricate
morphology and syntax. Consequently, there is a theoretical and practical
imperative for developing LLMs predominantly focused on Arabic linguistic
elements. To address this gap, this paper proposes ArabianGPT, a series of
transformer-based models within the ArabianLLM suite designed explicitly for
Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in
size and complexity, aligning with the nuanced linguistic characteristics of
Arabic. The AraNizer tokenizer, integral to these models, addresses the unique
morphological aspects of Arabic script, ensuring more accurate text processing.
Empirical results from fine-tuning the models on tasks like sentiment analysis
and summarization demonstrate significant improvements. For sentiment analysis,
the fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a
substantial increase from the base model's 56%. Similarly, in summarization
tasks, fine-tuned models showed enhanced F1 scores, indicating improved
precision and recall in generating concise summaries. Comparative analysis of
fine-tuned ArabianGPT models against their base versions across various
benchmarks reveals nuanced differences in performance, with fine-tuning
positively impacting specific tasks like question answering and summarization.
These findings underscore the efficacy of fine-tuning in aligning ArabianGPT
models more closely with specific NLP tasks, highlighting the potential of
tailored transformer architectures in advancing Arabic NLP.
\\ ( https://arxiv.org/abs/2402.15313 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15337
Date: Fri, 23 Feb 2024 14:17:01 GMT   (7877kb,D)

Title: Ranking Entities along Conceptual Space Dimensions with LLMs: An
  Analysis of Fine-Tuning Strategies
Authors: Nitesh Kumar, Usashi Chatterjee, and Steven Schockaert
Categories: cs.CL cs.LG
Comments: Submitted to ACL 2024
\\
  Conceptual spaces represent entities in terms of their primitive semantic
features. Such representations are highly valuable but they are notoriously
difficult to learn, especially when it comes to modelling perceptual and
subjective features. Distilling conceptual spaces from Large Language Models
(LLMs) has recently emerged as a promising strategy. However, existing work has
been limited to probing pre-trained LLMs using relatively simple zero-shot
strategies. We focus in particular on the task of ranking entities according to
a given conceptual space dimension. Unfortunately, we cannot directly fine-tune
LLMs on this task, because ground truth rankings for conceptual space
dimensions are rare. We therefore use more readily available features as
training data and analyse whether the ranking capabilities of the resulting
models transfer to perceptual and subjective features. We find that this is
indeed the case, to some extent, but having perceptual and subjective features
in the training data seems essential for achieving the best results. We
furthermore find that pointwise ranking strategies are competitive against
pairwise approaches, in defiance of common wisdom.
\\ ( https://arxiv.org/abs/2402.15337 ,  7877kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15343
Date: Fri, 23 Feb 2024 14:23:51 GMT   (4948kb,D)

Title: NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data
Authors: Sergei Bogdanov, Alexandre Constantin, Timoth\'ee Bernard, Benoit
  Crabb\'e, Etienne Bernard
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have shown impressive abilities in data
annotation, opening the way for new approaches to solve classic NLP problems.
In this paper, we show how to use LLMs to create NuNER, a compact language
representation model specialized in the Named Entity Recognition (NER) task.
NuNER can be fine-tuned to solve downstream NER problems in a data-efficient
way, outperforming similar-sized foundation models in the few-shot regime and
competing with much larger LLMs. We find that the size and entity-type
diversity of the pre-training dataset are key to achieving good performance. We
view NuNER as a member of the broader family of task-specific foundation
models, recently unlocked by LLMs.
\\ ( https://arxiv.org/abs/2402.15343 ,  4948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15370
Date: Fri, 23 Feb 2024 15:07:13 GMT   (951kb,D)

Title: Dual Encoder: Exploiting the Potential of Syntactic and Semantic for
  Aspect Sentiment Triplet Extraction
Authors: Xiaowei Zhao, Yong Zhou, Xiujuan Xu
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by COLING 2024
\\
  Aspect Sentiment Triple Extraction (ASTE) is an emerging task in fine-grained
sentiment analysis. Recent studies have employed Graph Neural Networks (GNN) to
model the syntax-semantic relationships inherent in triplet elements. However,
they have yet to fully tap into the vast potential of syntactic and semantic
information within the ASTE task. In this work, we propose a \emph{Dual
Encoder: Exploiting the potential of Syntactic and Semantic} model (D2E2S),
which maximizes the syntactic and semantic relationships among words.
Specifically, our model utilizes a dual-channel encoder with a BERT channel to
capture semantic information, and an enhanced LSTM channel for comprehensive
syntactic information capture. Subsequently, we introduce the heterogeneous
feature interaction module to capture intricate interactions between dependency
syntax and attention semantics, and to dynamically select vital nodes. We
leverage the synergy of these modules to harness the significant potential of
syntactic and semantic information in ASTE tasks. Testing on public benchmarks,
our D2E2S model surpasses the current state-of-the-art(SOTA), demonstrating its
effectiveness.
\\ ( https://arxiv.org/abs/2402.15370 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15422
Date: Fri, 23 Feb 2024 16:32:28 GMT   (1549kb,D)

Title: A Data-Centric Approach To Generate Faithful and High Quality Patient
  Summaries with Large Language Models
Authors: Stefan Hegselmann, Shannon Zejiang Shen, Florian Gierse, Monica
  Agrawal, David Sontag, Xiaoyi Jiang
Categories: cs.CL cs.AI cs.LG
\\
  Patients often face difficulties in understanding their hospitalizations,
while healthcare workers have limited resources to provide explanations. In
this work, we investigate the potential of large language models to generate
patient summaries based on doctors' notes and study the effect of training data
on the faithfulness and quality of the generated summaries. To this end, we
develop a rigorous labeling protocol for hallucinations, and have two medical
experts annotate 100 real-world summaries and 100 generated summaries. We show
that fine-tuning on hallucination-free data effectively reduces hallucinations
from 2.60 to 1.55 per summary for Llama 2, while preserving relevant
information. Although the effect is still present, it is much smaller for GPT-4
when prompted with five examples (0.70 to 0.40). We also conduct a qualitative
evaluation using hallucination-free and improved training data. GPT-4 shows
very good results even in the zero-shot setting. We find that common
quantitative metrics do not correlate well with faithfulness and quality.
Finally, we test GPT-4 for automatic hallucination detection, which yields
promising results.
\\ ( https://arxiv.org/abs/2402.15422 ,  1549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15449
Date: Fri, 23 Feb 2024 17:25:10 GMT   (3152kb,D)

Title: Repetition Improves Language Model Embeddings
Authors: Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig,
  Aditi Raghunathan
Categories: cs.CL cs.LG
Comments: 36 pages, 11 figures, 16 tables
\\
  Recent approaches to improving the extraction of text embeddings from
autoregressive large language models (LLMs) have largely focused on
improvements to data, backbone pretrained language models, or improving
task-differentiation via instructions. In this work, we address an
architectural limitation of autoregressive models: token embeddings cannot
contain information from tokens that appear later in the input. To address this
limitation, we propose a simple approach, "echo embeddings," in which we repeat
the input twice in context and extract embeddings from the second occurrence.
We show that echo embeddings of early tokens can encode information about later
tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On
the MTEB leaderboard, echo embeddings improve over classical embeddings by over
9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a
Mistral-7B model achieve state-of-the-art compared to prior open source models
that do not leverage synthetic fine-tuning data.
\\ ( https://arxiv.org/abs/2402.15449 ,  3152kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15473
Date: Fri, 23 Feb 2024 18:05:06 GMT   (7890kb,D)

Title: Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A
  Case-Study in E-Commerce Opinion Summarization
Authors: Swaroop Nath, Tejpalsingh Siledar, Sankara Sri Raghava Ravindra Muddu,
  Rupasai Rangaraju, Harshad Khadilkar, Pushpak Bhattacharyya, Suman Banerjee,
  Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera
Categories: cs.CL cs.LG
Comments: 16 pages, 7 figures, 15 tables
\\
  Reinforcement Learning from Human Feedback (RLHF) has become a dominating
strategy in steering Language Models (LMs) towards human values/goals. The key
to the strategy is employing a reward model ({$\varphi$}) which can reflect a
latent reward model with humans. While this strategy has proven to be
effective, the training methodology requires a lot of human preference
annotation (usually of the order of tens of thousands) to train {$\varphi$}.
Such large-scale preference annotations can be achievable if the reward model
can be ubiquitously used. However, human values/goals are subjective and depend
on the nature of the task. This poses a challenge in collecting diverse
preferences for downstream applications. To address this, we propose a novel
methodology to infuse domain knowledge into {$\varphi$}, which reduces the size
of preference annotation required. We validate our approach in E-Commerce
Opinion Summarization, with a significant reduction in dataset size (just $940$
samples) while advancing the state-of-the-art. Our contributions include a
novel Reward Modelling technique, a new dataset (PromptOpinSumm) for Opinion
Summarization, and a human preference dataset (OpinPref). The proposed
methodology opens avenues for efficient RLHF, making it more adaptable to
diverse applications with varying human values. We release the artifacts for
usage under MIT License.
\\ ( https://arxiv.org/abs/2402.15473 ,  7890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15481
Date: Fri, 23 Feb 2024 18:15:56 GMT   (8549kb,D)

Title: Prejudice and Caprice: A Statistical Framework for Measuring Social
  Discrimination in Large Language Models
Authors: Yiran Liu (1 and 2), Ke Yang (1 and 3), Zehan Qi (2), Xiao Liu (2),
  Yang Yu (2), Chengxiang Zhai (3) ((1) Equal contributions, (2) Tsinghua
  University, (3) University of Illinois Urbana-Champaign)
Categories: cs.CL cs.CY
\\
  The growing integration of large language models (LLMs) into social
operations amplifies their impact on decisions in crucial areas such as
economics, law, education, and healthcare, raising public concerns about these
models' discrimination-related safety and reliability. However, prior
discrimination measuring frameworks solely assess the average discriminatory
behavior of LLMs, often proving inadequate due to the overlook of an additional
discrimination-leading factor, i.e., the LLMs' prediction variation across
diverse contexts. In this work, we present the Prejudice-Caprice Framework
(PCF) that comprehensively measures discrimination in LLMs by considering both
their consistently biased preference and preference variation across diverse
contexts. Specifically, we mathematically dissect the aggregated contextualized
discrimination risk of LLMs into prejudice risk, originating from LLMs'
persistent prejudice, and caprice risk, stemming from their generation
inconsistency. In addition, we utilize a data-mining approach to gather
preference-detecting probes from sentence skeletons, devoid of attribute
indications, to approximate LLMs' applied contexts. While initially intended
for assessing discrimination in LLMs, our proposed PCF facilitates the
comprehensive and flexible measurement of any inductive biases, including
knowledge alongside prejudice, across various modality models. We apply our
discrimination-measuring framework to 12 common LLMs, yielding intriguing
findings: i) modern LLMs demonstrate significant pro-male stereotypes, ii)
LLMs' exhibited discrimination correlates with several social and economic
factors, iii) prejudice risk dominates the overall discrimination risk and
follows a normal distribution, and iv) caprice risk contributes minimally to
the overall risk but follows a fat-tailed distribution, suggesting that it is
wild risk requiring enhanced surveillance.
\\ ( https://arxiv.org/abs/2402.15481 ,  8549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15491
Date: Fri, 23 Feb 2024 18:30:49 GMT   (1148kb,D)

Title: API-BLEND: A Comprehensive Corpora for Training and Benchmarking API
  LLMs
Authors: Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell
  Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi,
  Luis A. Lastras
Categories: cs.CL cs.AI
\\
  There is a growing need for Large Language Models (LLMs) to effectively use
tools and external Application Programming Interfaces (APIs) to plan and
complete tasks. As such, there is tremendous interest in methods that can
acquire sufficient quantities of train and test data that involve calls to
tools / APIs. Two lines of research have emerged as the predominant strategies
for addressing this challenge. The first has focused on synthetic data
generation techniques, while the second has involved curating task-adjacent
datasets which can be transformed into API / Tool-based tasks. In this paper,
we focus on the task of identifying, curating, and transforming existing
datasets and, in turn, introduce API-BLEND, a large corpora for training and
systematic testing of tool-augmented LLMs. The datasets mimic real-world
scenarios involving API-tasks such as API / tool detection, slot filling, and
sequencing of the detected APIs. We demonstrate the utility of the API-BLEND
dataset for both training and benchmarking purposes.
\\ ( https://arxiv.org/abs/2402.15491 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14861
Date: Wed, 21 Feb 2024 01:29:17 GMT   (2898kb,D)

Title: CloudNine: Analyzing Meteorological Observation Impact on Weather
  Prediction Using Explainable Graph Neural Networks
Authors: Hyeon-Ju Jeon and Jeon-Ho Kang and In-Hyuk Kwon and O-Joun Lee
Categories: cs.LG cs.AI physics.ao-ph
\\
  The impact of meteorological observations on weather forecasting varies with
sensor type, location, time, and other environmental factors. Thus,
quantitative analysis of observation impacts is crucial for effective and
efficient development of weather forecasting systems. However, the existing
impact analysis methods are difficult to be widely applied due to their high
dependencies on specific forecasting systems. Also, they cannot provide
observation impacts at multiple spatio-temporal scales, only global impacts of
observation types. To address these issues, we present a novel system called
``CloudNine,'' which allows analysis of individual observations' impacts on
specific predictions based on explainable graph neural networks (XGNNs).
Combining an XGNN-based atmospheric state estimation model with a numerical
weather prediction model, we provide a web application to search for
observations in the 3D space of the Earth system and to visualize the impact of
individual observations on predictions in specific spatial regions and time
periods.
\\ ( https://arxiv.org/abs/2402.14861 ,  2898kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14866
Date: Wed, 21 Feb 2024 07:45:22 GMT   (2260kb,D)

Title: APTQ: Attention-aware Post-Training Mixed-Precision Quantization for
  Large Language Models
Authors: Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong and Hao Yu
Categories: cs.LG cs.AI cs.CL
Comments: 6 pages, 2 figures, published to DAC 2024: 61st IEEE/ACM Design
  Automation Conference. (DAC'24)
\\
  Large Language Models (LLMs) have greatly advanced the natural language
processing paradigm. However, the high computational load and huge model sizes
pose a grand challenge for deployment on edge devices. To this end, we propose
APTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,
which considers not only the second-order information of each layer's weights,
but also, for the first time, the nonlinear effect of attention outputs on the
entire model. We leverage the Hessian trace as a sensitivity metric for
mixed-precision quantization, ensuring an informed precision reduction that
retains model performance. Experiments show APTQ surpasses previous
quantization methods, achieving an average of 4 bit width a 5.22 perplexity
nearly equivalent to full precision in the C4 dataset. In addition, APTQ
attains state-of-the-art zero-shot accuracy of 68.24\% and 70.48\% at an
average bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating
its effectiveness to produce high-quality quantized LLMs.
\\ ( https://arxiv.org/abs/2402.14866 ,  2260kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14878
Date: Wed, 21 Feb 2024 21:02:11 GMT   (6516kb,D)

Title: Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
Authors: Zihao Chen and Johannes Leugering and Gert Cauwenberghs and Shantanu
  Chakrabartty
Categories: cs.LG cs.AI cs.AR
Comments: 19 pages, 10 figures
\\
  Learning-in-memory (LIM) is a recently proposed paradigm to overcome
fundamental memory bottlenecks in training machine learning systems. While
compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e.
energy dissipated due to repeated memory read access) they are agnostic to the
energy dissipated due to repeated memory writes at the precision required for
training (the update-wall), and they don't account for the energy dissipated
when transferring information between short-term and long-term memories (the
consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can
be overcome if the energy barrier of physical memories is adaptively modulated
such that the dynamics of memory updates and consolidation match the Lyapunov
dynamics of gradient-descent training of an AI model. In this paper, we derive
new theoretical lower bounds on energy dissipation when training AI systems
using different LIM approaches. The analysis presented here is model-agnostic
and highlights the trade-off between energy efficiency and the speed of
training. The resulting non-equilibrium energy-efficiency bounds have a similar
flavor as that of Landauer's energy-dissipation bounds. We also extend these
limits by taking into account the number of floating-point operations (FLOPs)
used for training, the size of the AI model, and the precision of the training
parameters. Our projections suggest that the energy-dissipation lower-bound to
train a brain scale AI system (comprising of $10^{15}$ parameters) using LIM is
$10^8 \sim 10^9$ Joules, which is on the same magnitude the Landauer's
adiabatic lower-bound and $6$ to $7$ orders of magnitude lower than the
projections obtained using state-of-the-art AI accelerator hardware
lower-bounds.
\\ ( https://arxiv.org/abs/2402.14878 ,  6516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14882
Date: Thu, 22 Feb 2024 03:31:00 GMT   (1458kb,D)

Title: Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms
  with Target Conditions
Authors: Sumin Lee, Jihoon Kim, Namwoo Kang
Categories: cs.LG cs.AI cs.CE
\\
  Mechanisms are essential components designed to perform specific tasks in
various mechanical systems. However, designing a mechanism that satisfies
certain kinematic or quasi-static requirements is a challenging task. The
kinematic requirements may include the workspace of a mechanism, while the
quasi-static requirements of a mechanism may include its torque transmission,
which refers to the ability of the mechanism to transfer power and torque
effectively. In this paper, we propose a deep learning-based generative model
for generating multiple crank-rocker four-bar linkage mechanisms that satisfy
both the kinematic and quasi-static requirements aforementioned. The proposed
model is based on a conditional generative adversarial network (cGAN) with
modifications for mechanism synthesis, which is trained to learn the
relationship between the requirements of a mechanism with respect to linkage
lengths. The results demonstrate that the proposed model successfully generates
multiple distinct mechanisms that satisfy specific kinematic and quasi-static
requirements. To evaluate the novelty of our approach, we provide a comparison
of the samples synthesized by the proposed cGAN, traditional cVAE and NSGA-II.
Our approach has several advantages over traditional design methods. It enables
designers to efficiently generate multiple diverse and feasible design
candidates while exploring a large design space. Also, the proposed model
considers both the kinematic and quasi-static requirements, which can lead to
more efficient and effective mechanisms for real-world use, making it a
promising tool for linkage mechanism design.
\\ ( https://arxiv.org/abs/2402.14882 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14886
Date: Thu, 22 Feb 2024 07:37:04 GMT   (430kb)

Title: Applying Reinforcement Learning to Optimize Traffic Light Cycles
Authors: Seungah Son and Juhee Jin
Categories: cs.LG cs.AI
\\
  Manual optimization of traffic light cycles is a complex and time-consuming
task, necessitating the development of automated solutions. In this paper, we
propose the application of reinforcement learning to optimize traffic light
cycles in real-time. We present a case study using the Simulation Urban
Mobility simulator to train a Deep Q-Network algorithm. The experimental
results showed 44.16% decrease in the average number of Emergency stops,
showing the potential of our approach to reduce traffic congestion and improve
traffic flow. Furthermore, we discuss avenues for future research and
enhancements to the reinforcement learning model.
\\ ( https://arxiv.org/abs/2402.14886 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14888
Date: Thu, 22 Feb 2024 09:43:53 GMT   (536kb,D)

Title: Efficient data selection employing Semantic Similarity-based Graph
  Structures for model training
Authors: Roxana Petcu and Subhadeep Maji
Categories: cs.LG cs.AI cs.CL
Comments: ICML 2023 Workshop: Sampling and Optimization in Discrete Space
\\
  Recent developments in natural language processing (NLP) have highlighted the
need for substantial amounts of data for models to capture textual information
accurately. This raises concerns regarding the computational resources and time
required for training such models. This paper introduces Semantics for data
SAliency in Model performance Estimation (SeSaME). It is an efficient data
sampling mechanism solely based on textual information without passing the data
through a compute-heavy model or other intensive pre-processing
transformations. The application of this approach is demonstrated in the use
case of low-resource automated speech recognition (ASR) models, which
excessively rely on text-to-speech (TTS) calls when using augmented data.
SeSaME learns to categorize new incoming data points into speech recognition
difficulty buckets by employing semantic similarity-based graph structures and
discrete ASR information from homophilous neighbourhoods through message
passing. The results indicate reliable projections of ASR performance, with a
93% accuracy increase when using the proposed method compared to random
predictions, bringing non-trivial information on the impact of textual
representations in speech models. Furthermore, a series of experiments show
both the benefits and challenges of using the ASR information on incoming data
to fine-tune the model. We report a 7% drop in validation loss compared to
random sampling, 7% WER drop with non-local aggregation when evaluating against
a highly difficult dataset, and 1.8% WER drop with local aggregation and high
semantic similarity between datasets.
\\ ( https://arxiv.org/abs/2402.14888 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14905
Date: Thu, 22 Feb 2024 18:58:55 GMT   (888kb,D)

Title: MobileLLM: Optimizing Sub-billion Parameter Language Models for
  On-Device Use Cases
Authors: Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian,
  Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman
  Krishnamoorthi, Liangzhen Lai, Vikas Chandra
Categories: cs.LG cs.AI cs.CL
\\
  This paper addresses the growing need for efficient large language models
(LLMs) on mobile devices, driven by increasing cloud costs and latency
concerns. We focus on designing top-quality LLMs with fewer than a billion
parameters, a practical choice for mobile deployment. Contrary to prevailing
belief emphasizing the pivotal role of data and parameter quantity in
determining model quality, our investigation underscores the significance of
model architecture for sub-billion scale LLMs. Leveraging deep and thin
architectures, coupled with embedding sharing and grouped-query attention
mechanisms, we establish a strong baseline network denoted as MobileLLM, which
attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M
state-of-the-art models. Additionally, we propose an immediate block-wise
weight sharing approach with no increase in model size and only marginal
latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a
further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,
MobileLLM model family shows significant improvements compared to previous
sub-billion models on chat benchmarks, and demonstrates close correctness to
LLaMA-v2 7B in API calling tasks, highlighting the capability of small models
for common on-device use cases.
\\ ( https://arxiv.org/abs/2402.14905 ,  888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14922
Date: Thu, 22 Feb 2024 19:07:08 GMT   (571kb,D)

Title: Practical Insights into Knowledge Distillation for Pre-Trained Models
Authors: Norah Alballa and Marco Canini
Categories: cs.LG cs.AI
\\
  This research investigates the enhancement of knowledge distillation (KD)
processes in pre-trained models, an emerging field in knowledge transfer with
significant implications for distributed training and federated learning
environments. These environments benefit from reduced communication demands and
accommodate various model architectures. Despite the adoption of numerous KD
approaches for transferring knowledge among pre-trained models, a comprehensive
understanding of KD's application in these scenarios is lacking. Our study
conducts an extensive comparison of multiple KD techniques, including standard
KD, tuned KD (via optimized temperature and weight parameters), deep mutual
learning, and data partitioning KD. We assess these methods across various data
distribution strategies to identify the most effective contexts for each.
Through detailed examination of hyperparameter tuning, informed by extensive
grid search evaluations, we pinpoint when adjustments are crucial to enhance
model performance. This paper sheds light on optimal hyperparameter settings
for distinct data partitioning scenarios and investigates KD's role in
improving federated learning by minimizing communication rounds and expediting
the training process. By filling a notable void in current research, our
findings serve as a practical framework for leveraging KD in pre-trained models
within collaborative and federated learning frameworks.
\\ ( https://arxiv.org/abs/2402.14922 ,  571kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14926
Date: Thu, 22 Feb 2024 19:16:01 GMT   (521kb)

Title: Boosting gets full Attention for Relational Learning
Authors: Mathieu Guillame-Bert and Richard Nock
Categories: cs.LG
ACM-class: I.2.6
\\
  More often than not in benchmark supervised ML, tabular data is flat, i.e.
consists of a single $m \times d$ (rows, columns) file, but cases abound in the
real world where observations are described by a set of tables with structural
relationships. Neural nets-based deep models are a classical fit to incorporate
general topological dependence among description features (pixels, words,
etc.), but their suboptimality to tree-based models on tabular data is still
well documented. In this paper, we introduce an attention mechanism for
structured data that blends well with tree-based models in the training context
of (gradient) boosting. Each aggregated model is a tree whose training involves
two steps: first, simple tabular models are learned descending tables in a
top-down fashion with boosting's class residuals on tables' features. Second,
what has been learned progresses back bottom-up via attention and aggregation
mechanisms, progressively crafting new features that complete at the end the
set of observation features over which a single tree is learned, boosting's
iteration clock is incremented and new class residuals are computed.
Experiments on simulated and real-world domains display the competitiveness of
our method against a state of the art containing both tree-based and neural
nets-based models.
\\ ( https://arxiv.org/abs/2402.14926 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14929
Date: Thu, 22 Feb 2024 19:24:59 GMT   (1668kb,D)

Title: Federated Fairness without Access to Sensitive Groups
Authors: Afroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro,
  Miguel Rodrigues
Categories: cs.LG cs.AI cs.CY cs.DC
\\
  Current approaches to group fairness in federated learning assume the
existence of predefined and labeled sensitive groups during training. However,
due to factors ranging from emerging regulations to dynamics and
location-dependency of protected groups, this assumption may be unsuitable in
many real-world scenarios. In this work, we propose a new approach to guarantee
group fairness that does not rely on any predefined definition of sensitive
groups or additional labels. Our objective allows the federation to learn a
Pareto efficient global model ensuring worst-case group fairness and it
enables, via a single hyper-parameter, trade-offs between fairness and utility,
subject only to a group size constraint. This implies that any sufficiently
large subset of the population is guaranteed to receive at least a minimum
level of utility performance from the model. The proposed objective encompasses
existing approaches as special cases, such as empirical risk minimization and
subgroup robustness objectives from centralized machine learning. We provide an
algorithm to solve this problem in federation that enjoys convergence and
excess risk guarantees. Our empirical results indicate that the proposed
approach can effectively improve the worst-performing group that may be present
without unnecessarily hurting the average performance, exhibits superior or
comparable performance to relevant baselines, and achieves a large set of
solutions with different fairness-utility trade-offs.
\\ ( https://arxiv.org/abs/2402.14929 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14937
Date: Thu, 22 Feb 2024 19:44:19 GMT   (315kb,D)

Title: SoK: Analyzing Adversarial Examples: A Framework to Study Adversary
  Knowledge
Authors: Lucas Fenaux and Florian Kerschbaum
Categories: cs.LG cs.CR
\\
  Adversarial examples are malicious inputs to machine learning models that
trigger a misclassification. This type of attack has been studied for close to
a decade, and we find that there is a lack of study and formalization of
adversary knowledge when mounting attacks. This has yielded a complex space of
attack research with hard-to-compare threat models and attacks. We focus on the
image classification domain and provide a theoretical framework to study
adversary knowledge inspired by work in order theory. We present an adversarial
example game, inspired by cryptographic games, to standardize attacks. We
survey recent attacks in the image classification domain and classify their
adversary's knowledge in our framework. From this systematization, we compile
results that both confirm existing beliefs about adversary knowledge, such as
the potency of information about the attacked model as well as allow us to
derive new conclusions on the difficulty associated with the white-box and
transferable threat models, for example, that transferable attacks might not be
as difficult as previously thought.
\\ ( https://arxiv.org/abs/2402.14937 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14949
Date: Thu, 22 Feb 2024 20:09:58 GMT   (708kb,D)

Title: Enhancing Power Quality Event Classification with AI Transformer Models
Authors: Ahmad Mohammad Saber, Amr Youssef, Davor Svetinovic, Hatem Zeineldin,
  Deepa Kundur and Ehab El-Saadany
Categories: cs.LG eess.SP
Comments: Accepted in the IEEE Power and Energy Society General Meeting, 2024
\\
  Recently, there has been a growing interest in utilizing machine learning for
accurate classification of power quality events (PQEs). However, most of these
studies are performed assuming an ideal situation, while in reality, we can
have measurement noise, DC offset, and variations in the voltage signal's
amplitude and frequency. Building on the prior PQE classification works using
deep learning, this paper proposes a deep-learning framework that leverages
attention-enabled Transformers as a tool to accurately classify PQEs under the
aforementioned considerations. The proposed framework can operate directly on
the voltage signals with no need for a separate feature extraction or
calculation phase. Our results show that the proposed framework outperforms
recently proposed learning-based techniques. It can accurately classify PQEs
under the aforementioned conditions with an accuracy varying between
99.81%$-$91.43% depending on the signal-to-noise ratio, DC offsets, and
variations in the signal amplitude and frequency.
\\ ( https://arxiv.org/abs/2402.14949 ,  708kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14979
Date: Thu, 22 Feb 2024 21:36:07 GMT   (169kb,D)

Title: Optimizing Language Models for Human Preferences is a Causal Inference
  Problem
Authors: Victoria Lin, Eli Ben-Michael, Louis-Philippe Morency
Categories: cs.LG cs.CL stat.ME
\\
  As large language models (LLMs) see greater use in academic and commercial
settings, there is increasing interest in methods that allow language models to
generate texts aligned with human preferences. In this paper, we present an
initial exploration of language model optimization for human preferences from
direct outcome datasets, where each sample consists of a text and an associated
numerical outcome measuring the reader's response. We first propose that
language model optimization should be viewed as a causal problem to ensure that
the model correctly learns the relationship between the text and the outcome.
We formalize this causal language optimization problem, and we develop a
method--causal preference optimization (CPO)--that solves an unbiased surrogate
objective for the problem. We further extend CPO with doubly robust CPO
(DR-CPO), which reduces the variance of the surrogate objective while retaining
provably strong guarantees on bias. Finally, we empirically demonstrate the
effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human
preferences on direct outcome data, and we validate the robustness of DR-CPO
under difficult confounding conditions.
\\ ( https://arxiv.org/abs/2402.14979 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14983
Date: Thu, 22 Feb 2024 21:46:24 GMT   (421kb,D)

Title: Privacy-Enhancing Collaborative Information Sharing through Federated
  Learning -- A Case of the Insurance Industry
Authors: Panyi Dong, Zhiyu Quan, Brandon Edwards, Shih-han Wang, Runhuan Feng,
  Tianyang Wang, Patrick Foley, Prashant Shah
Categories: cs.LG cs.CR q-fin.RM
\\
  The report demonstrates the benefits (in terms of improved claims loss
modeling) of harnessing the value of Federated Learning (FL) to learn a single
model across multiple insurance industry datasets without requiring the
datasets themselves to be shared from one company to another. The application
of FL addresses two of the most pressing concerns: limited data volume and data
variety, which are caused by privacy concerns, the rarity of claim events, the
lack of informative rating factors, etc.. During each round of FL,
collaborators compute improvements on the model using their local private data,
and these insights are combined to update a global model. Such aggregation of
insights allows for an increase to the effectiveness in forecasting claims
losses compared to models individually trained at each collaborator.
Critically, this approach enables machine learning collaboration without the
need for raw data to leave the compute infrastructure of each respective data
owner. Additionally, the open-source framework, OpenFL, that is used in our
experiments is designed so that it can be run using confidential computing as
well as with additional algorithmic protections against leakage of information
via the shared model updates. In such a way, FL is implemented as a
privacy-enhancing collaborative learning technique that addresses the
challenges posed by the sensitivity and privacy of data in traditional machine
learning solutions. This paper's application of FL can also be expanded to
other areas including fraud detection, catastrophe modeling, etc., that have a
similar need to incorporate data privacy into machine learning collaborations.
Our framework and empirical results provide a foundation for future
collaborations among insurers, regulators, academic researchers, and InsurTech
experts.
\\ ( https://arxiv.org/abs/2402.14983 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14988
Date: Thu, 22 Feb 2024 21:56:20 GMT   (1869kb,D)

Title: Verifiable Boosted Tree Ensembles
Authors: Stefano Calzavara, Lorenzo Cazzaro, Claudio Lucchese, Giulio Ermanno
  Pibiri
Categories: cs.LG cs.CR cs.LO stat.ML
Comments: 15 pages, 3 figures
\\
  Verifiable learning advocates for training machine learning models amenable
to efficient security verification. Prior research demonstrated that specific
classes of decision tree ensembles -- called large-spread ensembles -- allow
for robustness verification in polynomial time against any norm-based attacker.
This study expands prior work on verifiable learning from basic ensemble
methods (i.e., hard majority voting) to advanced boosted tree ensembles, such
as those trained using XGBoost or LightGBM. Our formal results indicate that
robustness verification is achievable in polynomial time when considering
attackers based on the $L_\infty$-norm, but remains NP-hard for other
norm-based attackers. Nevertheless, we present a pseudo-polynomial time
algorithm to verify robustness against attackers based on the $L_p$-norm for
any $p \in \mathbb{N} \cup \{0\}$, which in practice grants excellent
performance. Our experimental evaluation shows that large-spread boosted
ensembles are accurate enough for practical adoption, while being amenable to
efficient security verification.
\\ ( https://arxiv.org/abs/2402.14988 ,  1869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14989
Date: Thu, 22 Feb 2024 22:00:03 GMT   (1087kb,D)

Title: Stable Neural Stochastic Differential Equations in Analyzing Irregular
  Time Series Data
Authors: YongKyung Oh, Dongyoung Lim, Sungil Kim
Categories: cs.LG cs.AI
Comments: Accepted at ICLR 2024, Spotlight presentation (Notable Top 5%).
  https://openreview.net/forum?id=4VIgNuQ1pY
\\
  Irregular sampling intervals and missing values in real-world time series
data present challenges for conventional methods that assume consistent
intervals and complete data. Neural Ordinary Differential Equations (Neural
ODEs) offer an alternative approach, utilizing neural networks combined with
ODE solvers to learn continuous latent representations through parameterized
vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend
Neural ODEs by incorporating a diffusion term, although this addition is not
trivial, particularly when addressing irregular intervals and missing values.
Consequently, careful design of drift and diffusion functions is crucial for
maintaining stability and enhancing performance, while incautious choices can
result in adverse properties such as the absence of strong solutions,
stochastic destabilization, or unstable Euler discretizations, significantly
affecting Neural SDEs' performance. In this study, we propose three stable
classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE.
Then, we rigorously demonstrate their robustness in maintaining excellent
performance under distribution shift, while effectively preventing overfitting.
To assess the effectiveness of our approach, we conduct extensive experiments
on four benchmark datasets for interpolation, forecasting, and classification
tasks, and analyze the robustness of our methods with 30 public datasets under
different missing rates. Our results demonstrate the efficacy of the proposed
method in handling real-world irregular time series data.
\\ ( https://arxiv.org/abs/2402.14989 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14991
Date: Thu, 22 Feb 2024 22:03:16 GMT   (8752kb,D)

Title: Quantum Theory and Application of Contextual Optimal Transport
Authors: Nicola Mariella, Albert Akhriev, Francesco Tacchino, Christa Zoufal,
  Juan Carlos Gonzalez-Espitia, Benedek Harsanyi, Eugene Koskin, Ivano
  Tavernelli, Stefan Woerner, Marianna Rapsomaniki, Sergiy Zhuk, Jannis Born
Categories: cs.LG cs.ET math.QA q-bio.QM quant-ph
Comments: Under review
\\
  Optimal Transport (OT) has fueled machine learning (ML) applications across
many domains. In cases where paired data measurements ($\mu$, $\nu$) are
coupled to a context variable $p_i$ , one may aspire to learn a global
transportation map that can be parameterized through a potentially unseen
con-text. Existing approaches utilize Neural OT and largely rely on Brenier's
theorem. Here, we propose a first-of-its-kind quantum computing formulation for
amortized optimization of contextualized transportation plans. We exploit a
direct link between doubly stochastic matrices and unitary operators thus
finding a natural connection between OT and quantum computation. We verify our
method on synthetic and real data, by predicting variations in cell type
distributions parameterized through drug dosage as context. Our comparisons to
several baselines reveal that our method can capture dose-induced variations in
cell distributions, even to some extent when dosages are extrapolated and
sometimes with performance similar to the best classical models. In summary,
this is a first step toward learning to predict contextualized transportation
plans through quantum.
\\ ( https://arxiv.org/abs/2402.14991 ,  8752kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15005
Date: Thu, 22 Feb 2024 22:49:35 GMT   (8139kb,D)

Title: Comparison of Machine Learning Classification Algorithms and Application
  to the Framingham Heart Study
Authors: Nabil Kahouadji
Categories: cs.LG stat.ML
Comments: 21 pages, 4 figures, 12 tables
MSC-class: 62H30, 62J15, 62P10, 68T01
\\
  The use of machine learning algorithms in healthcare can amplify social
injustices and health inequities. While the exacerbation of biases can occur
and compound during the problem selection, data collection, and outcome
definition, this research pertains to some generalizability impediments that
occur during the development and the post-deployment of machine learning
classification algorithms. Using the Framingham coronary heart disease data as
a case study, we show how to effectively select a probability cutoff to convert
a regression model for a dichotomous variable into a classifier. We then
compare the sampling distribution of the predictive performance of eight
machine learning classification algorithms under four training/testing
scenarios to test their generalizability and their potential to perpetuate
biases. We show that both the Extreme Gradient Boosting, and Support Vector
Machine are flawed when trained on an unbalanced dataset. We introduced and
show that the double discriminant scoring of type I is the most generalizable
as it consistently outperforms the other classification algorithms regardless
of the training/testing scenario. Finally, we introduce a methodology to
extract an optimal variable hierarchy for a classification algorithm, and
illustrate it on the overall, male and female Framingham coronary heart disease
data.
\\ ( https://arxiv.org/abs/2402.15005 ,  8139kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15017
Date: Thu, 22 Feb 2024 23:29:42 GMT   (1675kb,D)

Title: Towards Few-Shot Adaptation of Foundation Models via Multitask
  Finetuning
Authors: Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang
Categories: cs.LG cs.AI cs.CL
Comments: Published at ICLR 2024. 54 pages
\\
  Foundation models have emerged as a powerful tool for many AI problems.
Despite the tremendous success of foundation models, effective adaptation to
new tasks, particularly those with limited labels, remains an open question and
lacks theoretical understanding. An emerging solution with recent success in
vision and NLP involves finetuning a foundation model on a selection of
relevant tasks, before its adaptation to a target task with limited labeled
samples. In this paper, we study the theoretical justification of this
multitask finetuning approach. Our theoretical analysis reveals that with a
diverse set of related tasks, this multitask finetuning leads to reduced error
in the target task, in comparison to directly adapting the same pretrained
model. We quantify the relationship between finetuning tasks and target tasks
by diversity and consistency metrics, and further propose a practical task
selection algorithm. We substantiate our theoretical claims with extensive
empirical evidence. Further, we present results affirming our task selection
algorithm adeptly chooses related finetuning tasks, providing advantages to the
model performance on target tasks. We believe our study shed new light on the
effective adaptation of foundation models to new tasks that lack abundant
labels. Our code is available at
https://github.com/OliverXUZY/Foudation-Model_Multitask.
\\ ( https://arxiv.org/abs/2402.15017 ,  1675kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15019
Date: Thu, 22 Feb 2024 23:36:18 GMT   (452kb,D)

Title: Consistency-Guided Temperature Scaling Using Style and Content
  Information for Out-of-Domain Calibration
Authors: Wonjeong Choi, Jungwuk Park, Dong-Jun Han, Younghyun Park, Jaekyun
  Moon
Categories: cs.LG cs.AI stat.ML
Comments: Accepted at AAAI-24 (The 38th AAAI Conference on Artificial
  Intelligence, February 2024)
\\
  Research interests in the robustness of deep neural networks against domain
shifts have been rapidly increasing in recent years. Most existing works,
however, focus on improving the accuracy of the model, not the calibration
performance which is another important requirement for trustworthy AI systems.
Temperature scaling (TS), an accuracy-preserving post-hoc calibration method,
has been proven to be effective in in-domain settings, but not in out-of-domain
(OOD) due to the difficulty in obtaining a validation set for the unseen domain
beforehand. In this paper, we propose consistency-guided temperature scaling
(CTS), a new temperature scaling strategy that can significantly enhance the
OOD calibration performance by providing mutual supervision among data samples
in the source domains. Motivated by our observation that over-confidence
stemming from inconsistent sample predictions is the main obstacle to OOD
calibration, we propose to guide the scaling process by taking consistencies
into account in terms of two different aspects -- style and content -- which
are the key components that can well-represent data samples in multi-domain
settings. Experimental results demonstrate that our proposed strategy
outperforms existing works, achieving superior OOD calibration performance on
various datasets. This can be accomplished by employing only the source domains
without compromising accuracy, making our scheme directly applicable to various
trustworthy AI systems.
\\ ( https://arxiv.org/abs/2402.15019 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15020
Date: Thu, 22 Feb 2024 23:36:26 GMT   (3160kb,D)

Title: Probabilistically-sound beam search with masked language models
Authors: Charlie Cowen-Breen, Creston Brooks, Robert Calef, Anna Sappington
Categories: cs.LG cs.CL
\\
  Beam search with masked language models (MLMs) is challenging in part because
joint probability distributions over sequences are not readily available,
unlike for autoregressive models. Nevertheless, estimating such distributions
has applications in many domains, including protein engineering and ancient
text restoration. We present probabilistically-sound methods for beam search
with MLMs. First, we clarify the conditions under which it is theoretically
sound to perform text infilling with MLMs using standard beam search. When
these conditions fail, we provide a probabilistically-sound modification with
no additional computational complexity and demonstrate that it is superior to
the aforementioned beam search in the expected conditions. We then present
empirical results comparing several infilling approaches with MLMs across
several domains.
\\ ( https://arxiv.org/abs/2402.15020 ,  3160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15070
Date: Fri, 23 Feb 2024 03:15:10 GMT   (11064kb,D)

Title: Enhancing One-Shot Federated Learning Through Data and Ensemble
  Co-Boosting
Authors: Rong Dai, Yonggang Zhang, Ang Li, Tongliang Liu, Xun Yang, Bo Han
Categories: cs.LG
Comments: To be published in ICLR2024
\\
  One-shot Federated Learning (OFL) has become a promising learning paradigm,
enabling the training of a global server model via a single communication
round. In OFL, the server model is aggregated by distilling knowledge from all
client models (the ensemble), which are also responsible for synthesizing
samples for distillation. In this regard, advanced works show that the
performance of the server model is intrinsically related to the quality of the
synthesized data and the ensemble model. To promote OFL, we introduce a novel
framework, Co-Boosting, in which synthesized data and the ensemble model
mutually enhance each other progressively. Specifically, Co-Boosting leverages
the current ensemble model to synthesize higher-quality samples in an
adversarial manner. These hard samples are then employed to promote the quality
of the ensemble model by adjusting the ensembling weights for each client
model. Consequently, Co-Boosting periodically achieves high-quality data and
ensemble models. Extensive experiments demonstrate that Co-Boosting can
substantially outperform existing baselines under various settings. Moreover,
Co-Boosting eliminates the need for adjustments to the client's local training,
requires no additional data or model transmission, and allows client models to
have heterogeneous architectures.
\\ ( https://arxiv.org/abs/2402.15070 ,  11064kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15073
Date: Fri, 23 Feb 2024 03:27:17 GMT   (750kb,D)

Title: Cost-Adaptive Recourse Recommendation by Adaptive Preference Elicitation
Authors: Duy Nguyen, Bao Nguyen, Viet Anh Nguyen
Categories: cs.LG
Comments: 30 pages, 7 figures
\\
  Algorithmic recourse recommends a cost-efficient action to a subject to
reverse an unfavorable machine learning classification decision. Most existing
methods in the literature generate recourse under the assumption of complete
knowledge about the cost function. In real-world practice, subjects could have
distinct preferences, leading to incomplete information about the underlying
cost function of the subject. This paper proposes a two-step approach
integrating preference learning into the recourse generation problem. In the
first step, we design a question-answering framework to refine the confidence
set of the Mahalanobis matrix cost of the subject sequentially. Then, we
generate recourse by utilizing two methods: gradient-based and graph-based
cost-adaptive recourse that ensures validity while considering the whole
confidence set of the cost matrix. The numerical evaluation demonstrates the
benefits of our approach over state-of-the-art baselines in delivering
cost-efficient recourse recommendations.
\\ ( https://arxiv.org/abs/2402.15073 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15096
Date: Fri, 23 Feb 2024 05:09:35 GMT   (411kb,D)

Title: Multimodal Transformer With a Low-Computational-Cost Guarantee
Authors: Sungjin Park and Edward Choi
Categories: cs.LG cs.CV cs.MM
Comments: Accepted to ICASSP 2024 (5 pages)
\\
  Transformer-based models have significantly improved performance across a
range of multimodal understanding tasks, such as visual question answering and
action recognition. However, multimodal Transformers significantly suffer from
a quadratic complexity of the multi-head attention with the input sequence
length, especially as the number of modalities increases. To address this, we
introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal
attention mechanism that aims to reduce computational cost during training and
inference with minimal performance loss. Specifically, by assigning different
multimodal attention patterns to each attention head, LoCoMT can flexibly
control multimodal signals and theoretically ensures a reduced computational
cost compared to existing multimodal Transformer variants. Experimental results
on two multimodal datasets, namely Audioset and MedVidCL demonstrate that
LoCoMT not only reduces GFLOPs but also matches or even outperforms established
models.
\\ ( https://arxiv.org/abs/2402.15096 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15097
Date: Fri, 23 Feb 2024 05:11:34 GMT   (3804kb,D)

Title: Learning solution operators of PDEs defined on varying domains via
  MIONet
Authors: Shanshan Xiao, Pengzhan Jin, Yifa Tang
Categories: cs.LG cs.NA math.NA
\\
  In this work, we propose a method to learn the solution operators of PDEs
defined on varying domains via MIONet, and theoretically justify this method.
We first extend the approximation theory of MIONet to further deal with metric
spaces, establishing that MIONet can approximate mappings with multiple inputs
in metric spaces. Subsequently, we construct a set consisting of some
appropriate regions and provide a metric on this set thus make it a metric
space, which satisfies the approximation condition of MIONet. Building upon the
theoretical foundation, we are able to learn the solution mapping of a PDE with
all the parameters varying, including the parameters of the differential
operator, the right-hand side term, the boundary condition, as well as the
domain. Without loss of generality, we for example perform the experiments for
2-d Poisson equations, where the domains and the right-hand side terms are
varying. The results provide insights into the performance of this method
across convex polygons, polar regions with smooth boundary, and predictions for
different levels of discretization on one task. Reasonably, we point out that
this is a meshless method, hence can be flexibly used as a general solver for a
type of PDE.
\\ ( https://arxiv.org/abs/2402.15097 ,  3804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15102
Date: Fri, 23 Feb 2024 05:20:23 GMT   (1244kb,D)

Title: Trajectory-wise Iterative Reinforcement Learning Framework for
  Auto-bidding
Authors: Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan
  Yu, Jian Xu, Fan Wu
Categories: cs.LG cs.AI cs.GT cs.IR
Comments: Accepted by The Web Conference 2024
\\
  In online advertising, advertisers participate in ad auctions to acquire ad
opportunities, often by utilizing auto-bidding tools provided by demand-side
platforms (DSPs). The current auto-bidding algorithms typically employ
reinforcement learning (RL). However, due to safety concerns, most RL-based
auto-bidding policies are trained in simulation, leading to a performance
degradation when deployed in online environments. To narrow this gap, we can
deploy multiple auto-bidding agents in parallel to collect a large interaction
dataset. Offline RL algorithms can then be utilized to train a new policy. The
trained policy can subsequently be deployed for further data collection,
resulting in an iterative training framework, which we refer to as iterative
offline RL. In this work, we identify the performance bottleneck of this
iterative offline RL framework, which originates from the ineffective
exploration and exploitation caused by the inherent conservatism of offline RL
algorithms. To overcome this bottleneck, we propose Trajectory-wise Exploration
and Exploitation (TEE), which introduces a novel data collecting and data
utilization method for iterative offline RL from a trajectory perspective.
Furthermore, to ensure the safety of online exploration while preserving the
dataset quality for TEE, we propose Safe Exploration by Adaptive Action
Selection (SEAS). Both offline experiments and real-world experiments on
Alibaba display advertising platform demonstrate the effectiveness of our
proposed method.
\\ ( https://arxiv.org/abs/2402.15102 ,  1244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15106
Date: Fri, 23 Feb 2024 05:33:43 GMT   (3451kb,D)

Title: Sampling-based Distributed Training with Message Passing Neural Network
Authors: Priyesh Kakka, Sheel Nidhan, Rishikesh Ranade and Jonathan F. MacArt
Categories: cs.LG cs.DC physics.flu-dyn
\\
  In this study, we introduce a domain-decomposition-based distributed training
and inference approach for message-passing neural networks (MPNN). Our
objective is to address the challenge of scaling edge-based graph neural
networks as the number of nodes increases. Through our distributed training
approach, coupled with Nystr\"om-approximation sampling techniques, we present
a scalable graph neural network, referred to as DS-MPNN (D and S standing for
distributed and sampled, respectively), capable of scaling up to $O(10^5)$
nodes. We validate our sampling and distributed training approach on two cases:
(a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils,
providing comparisons with both single-GPU implementation and node-based graph
convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy
to single-GPU implementation, can accommodate a significantly larger number of
nodes compared to the single-GPU variant (S-MPNN), and significantly
outperforms the node-based GCN.
\\ ( https://arxiv.org/abs/2402.15106 ,  3451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15109
Date: Fri, 23 Feb 2024 05:44:15 GMT   (1108kb,D)

Title: Machine Unlearning by Suppressing Sample Contribution
Authors: Xinwen Cheng and Zhehao Huang and Xiaolin Huang
Categories: cs.LG
\\
  Machine Unlearning (MU) is to forget data from a well-trained model, which is
practically important due to the "right to be forgotten". In this paper, we
start from the fundamental distinction between training data and unseen data on
their contribution to the model: the training data contributes to the final
model while the unseen data does not. We theoretically discover that the input
sensitivity can approximately measure the contribution and practically design
an algorithm, called MU-Mis (machine unlearning via minimizing input
sensitivity), to suppress the contribution of the forgetting data. Experimental
results demonstrate that MU-Mis outperforms state-of-the-art MU methods
significantly. Additionally, MU-Mis aligns more closely with the application of
MU as it does not require the use of remaining data.
\\ ( https://arxiv.org/abs/2402.15109 ,  1108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15113
Date: Fri, 23 Feb 2024 05:57:22 GMT   (10728kb,D)

Title: MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline
Authors: Guangming Sheng, Junwei Su, Chao Huang, Chuan Wu
Categories: cs.LG cs.DC
\\
  Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal
graph neural networks that utilize a node memory module to capture and retain
long-term temporal dependencies, leading to superior performance compared to
memory-less counterparts. However, the iterative reading and updating process
of the memory module in MTGNNs to obtain up-to-date information needs to follow
the temporal dependencies. This introduces significant overhead and limits
training throughput. Existing optimizations for static GNNs are not directly
applicable to MTGNNs due to differences in training paradigm, model
architecture, and the absence of a memory module. Moreover, they do not
effectively address the challenges posed by temporal dependencies, making them
ineffective for MTGNN training. In this paper, we propose MSPipe, a general and
efficient framework for MTGNNs that maximizes training throughput while
maintaining model accuracy. Our design addresses the unique challenges
associated with fetching and updating node memory states in MTGNNs by
integrating staleness into the memory module. However, simply introducing a
predefined staleness bound in the memory module to break temporal dependencies
may lead to suboptimal performance and lack of generalizability across
different models and datasets. To solve this, we introduce an online pipeline
scheduling algorithm in MSPipe that strategically breaks temporal dependencies
with minimal staleness and delays memory fetching to obtain fresher memory
states. Moreover, we design a staleness mitigation mechanism to enhance
training convergence and model accuracy. We provide convergence analysis and
prove that MSPipe maintains the same convergence rate as vanilla sample-based
GNN training. Experimental results show that MSPipe achieves up to 2.45x
speed-up without sacrificing accuracy, making it a promising solution for
efficient MTGNN training.
\\ ( https://arxiv.org/abs/2402.15113 ,  10728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15125
Date: Fri, 23 Feb 2024 06:24:57 GMT   (745kb,D)

Title: Accelerating Convergence of Stein Variational Gradient Descent via Deep
  Unfolding
Authors: Yuya Kawamura and Satoshi Takabe
Categories: cs.LG stat.ML
Comments: 7 pages, 5 figures
\\
  Stein variational gradient descent (SVGD) is a prominent particle-based
variational inference method used for sampling a target distribution. SVGD has
attracted interest for application in machine-learning techniques such as
Bayesian inference. In this paper, we propose novel trainable algorithms that
incorporate a deep-learning technique called deep unfolding,into SVGD. This
approach facilitates the learning of the internal parameters of SVGD, thereby
accelerating its convergence speed. To evaluate the proposed trainable SVGD
algorithms, we conducted numerical simulations of three tasks: sampling a
one-dimensional Gaussian mixture, performing Bayesian logistic regression, and
learning Bayesian neural networks. The results show that our proposed
algorithms exhibit faster convergence than the conventional variants of SVGD.
\\ ( https://arxiv.org/abs/2402.15125 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15127
Date: Fri, 23 Feb 2024 06:27:12 GMT   (108kb,D)

Title: Multi-Armed Bandits with Abstention
Authors: Junwen Yang, Tianyuan Jin, Vincent Y. F. Tan
Categories: cs.LG cs.IT math.IT stat.ML
Comments: Preprint
\\
  We introduce a novel extension of the canonical multi-armed bandit problem
that incorporates an additional strategic element: abstention. In this enhanced
framework, the agent is not only tasked with selecting an arm at each time
step, but also has the option to abstain from accepting the stochastic
instantaneous reward before observing it. When opting for abstention, the agent
either suffers a fixed regret or gains a guaranteed reward. Given this added
layer of complexity, we ask whether we can develop efficient algorithms that
are both asymptotically and minimax optimal. We answer this question
affirmatively by designing and analyzing algorithms whose regrets meet their
corresponding information-theoretic lower bounds. Our results offer valuable
quantitative insights into the benefits of the abstention option, laying the
groundwork for further exploration in other online decision-making problems
with such an option. Numerical results further corroborate our theoretical
findings.
\\ ( https://arxiv.org/abs/2402.15127 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15134
Date: Fri, 23 Feb 2024 06:38:08 GMT   (5959kb,D)

Title: Deep Coupling Network For Multivariate Time Series Forecasting
Authors: Kun Yi, Qi Zhang, Hui He, Kaize Shi, Liang Hu, Ning An, Zhendong Niu
Categories: cs.LG cs.AI
\\
  Multivariate time series (MTS) forecasting is crucial in many real-world
applications. To achieve accurate MTS forecasting, it is essential to
simultaneously consider both intra- and inter-series relationships among time
series data. However, previous work has typically modeled intra- and
inter-series relationships separately and has disregarded multi-order
interactions present within and between time series data, which can seriously
degrade forecasting accuracy. In this paper, we reexamine intra- and
inter-series relationships from the perspective of mutual information and
accordingly construct a comprehensive relationship learning mechanism tailored
to simultaneously capture the intricate multi-order intra- and inter-series
couplings. Based on the mechanism, we propose a novel deep coupling network for
MTS forecasting, named DeepCN, which consists of a coupling mechanism dedicated
to explicitly exploring the multi-order intra- and inter-series relationships
among time series data concurrently, a coupled variable representation module
aimed at encoding diverse variable patterns, and an inference module
facilitating predictions through one forward step. Extensive experiments
conducted on seven real-world datasets demonstrate that our proposed DeepCN
achieves superior performance compared with the state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.15134 ,  5959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15145
Date: Fri, 23 Feb 2024 07:03:52 GMT   (25kb)

Title: The Cost of Parallelizing Boosting
Authors: Xin Lyu, Hongxun Wu, Junzhao Yang
Categories: cs.LG cs.DS
Comments: appeared in SODA 2024
\\
  We study the cost of parallelizing weak-to-strong boosting algorithms for
learning, following the recent work of Karbasi and Larsen. Our main results are
two-fold:
  - First, we prove a tight lower bound, showing that even "slight"
parallelization of boosting requires an exponential blow-up in the complexity
of training.
  Specifically, let $\gamma$ be the weak learner's advantage over random
guessing. The famous \textsc{AdaBoost} algorithm produces an accurate
hypothesis by interacting with the weak learner for $\tilde{O}(1 / \gamma^2)$
rounds where each round runs in polynomial time.
  Karbasi and Larsen showed that "significant" parallelization must incur
exponential blow-up: Any boosting algorithm either interacts with the weak
learner for $\Omega(1 / \gamma)$ rounds or incurs an $\exp(d / \gamma)$ blow-up
in the complexity of training, where $d$ is the VC dimension of the hypothesis
class. We close the gap by showing that any boosting algorithm either has
$\Omega(1 / \gamma^2)$ rounds of interaction or incurs a smaller exponential
blow-up of $\exp(d)$.
  -Complementing our lower bound, we show that there exists a boosting
algorithm using $\tilde{O}(1/(t \gamma^2))$ rounds, and only suffer a blow-up
of $\exp(d \cdot t^2)$.
  Plugging in $t = \omega(1)$, this shows that the smaller blow-up in our lower
bound is tight. More interestingly, this provides the first trade-off between
the parallelism and the total work required for boosting.
\\ ( https://arxiv.org/abs/2402.15145 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15146
Date: Fri, 23 Feb 2024 07:05:09 GMT   (9022kb,D)

Title: Convergence Analysis of Blurring Mean Shift
Authors: Ryoya Yamasaki, Toshiyuki Tanaka
Categories: cs.LG cs.CV
Comments: Blurring mean shift, mean shift, clustering, convergence, kernel.
  arXiv admin note: text overlap with arXiv:2305.08463
\\
  Blurring mean shift (BMS) algorithm, a variant of the mean shift algorithm,
is a kernel-based iterative method for data clustering, where data points are
clustered according to their convergent points via iterative blurring. In this
paper, we analyze convergence properties of the BMS algorithm by leveraging its
interpretation as an optimization procedure, which is known but has been
underutilized in existing convergence studies. Whereas existing results on
convergence properties applicable to multi-dimensional data only cover the case
where all the blurred data point sequences converge to a single point, this
study provides a convergence guarantee even when those sequences can converge
to multiple points, yielding multiple clusters. This study also shows that the
convergence of the BMS algorithm is fast by further leveraging geometrical
characterization of the convergent points.
\\ ( https://arxiv.org/abs/2402.15146 ,  9022kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15152
Date: Fri, 23 Feb 2024 07:22:55 GMT   (66kb)

Title: On the Duality Between Sharpness-Aware Minimization and Adversarial
  Training
Authors: Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, Zeming
  Wei
Categories: cs.LG cs.AI cs.CR math.OC
Comments: arXiv admin note: text overlap with arXiv:2305.05392
\\
  Adversarial Training (AT), which adversarially perturb the input samples
during training, has been acknowledged as one of the most effective defenses
against adversarial attacks, yet suffers from a fundamental tradeoff that
inevitably decreases clean accuracy. Instead of perturbing the samples,
Sharpness-Aware Minimization (SAM) perturbs the model weights during training
to find a more flat loss landscape and improve generalization. However, as SAM
is designed for better clean accuracy, its effectiveness in enhancing
adversarial robustness remains unexplored. In this work, considering the
duality between SAM and AT, we investigate the adversarial robustness derived
from SAM. Intriguingly, we find that using SAM alone can improve adversarial
robustness. To understand this unexpected property of SAM, we first provide
empirical and theoretical insights into how SAM can implicitly learn more
robust features, and conduct comprehensive experiments to show that SAM can
improve adversarial robustness notably without sacrificing any clean accuracy,
shedding light on the potential of SAM to be a substitute for AT when accuracy
comes at a higher priority. Code is available at
https://github.com/weizeming/SAM_AT.
\\ ( https://arxiv.org/abs/2402.15152 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15160
Date: Fri, 23 Feb 2024 07:46:30 GMT   (26306kb,D)

Title: Spatially-Aware Transformer Memory for Embodied Agents
Authors: Junmo Cho, Jaesik Yoon, Sungjin Ahn
Categories: cs.LG cs.AI
Comments: ICLR 2024 Spotlight. First two authors contributed equally
\\
  Episodic memory plays a crucial role in various cognitive processes, such as
the ability to mentally recall past events. While cognitive science emphasizes
the significance of spatial context in the formation and retrieval of episodic
memory, the current primary approach to implementing episodic memory in AI
systems is through transformers that store temporally ordered experiences,
which overlooks the spatial dimension. As a result, it is unclear how the
underlying structure could be extended to incorporate the spatial axis beyond
temporal order alone and thereby what benefits can be obtained. To address
this, this paper explores the use of Spatially-Aware Transformer models that
incorporate spatial information. These models enable the creation of
place-centric episodic memory that considers both temporal and spatial
dimensions. Adopting this approach, we demonstrate that memory utilization
efficiency can be improved, leading to enhanced accuracy in various
place-centric downstream tasks. Additionally, we propose the Adaptive Memory
Allocator, a memory management method based on reinforcement learning that aims
to optimize efficiency of memory utilization. Our experiments demonstrate the
advantages of our proposed model in various environments and across multiple
downstream tasks, including prediction, generation, reasoning, and
reinforcement learning. The source code for our models and experiments will be
available at https://github.com/junmokane/spatially-aware-transformer.
\\ ( https://arxiv.org/abs/2402.15160 ,  26306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15163
Date: Fri, 23 Feb 2024 07:54:20 GMT   (10134kb,D)

Title: Studying the Impact of Stochasticity on the Evaluation of Deep Neural
  Networks for Forest-Fire Prediction
Authors: Harshit Kumar, Biswadeep Chakraborty, Beomseok Kang, Saibal
  Mukhopadhyay
Categories: cs.LG cs.AI
Comments: Initial draft submitted to KDD 2024
\\
  This paper presents the first systematic study of the evaluation of Deep
Neural Networks (DNNs) for discrete dynamical systems under stochastic
assumptions, with a focus on wildfire prediction. We develop a framework to
study the impact of stochasticity on two classes of evaluation metrics:
classification-based metrics, which assess fidelity to observed ground truth
(GT), and proper scoring rules, which test fidelity-to-statistic. Our findings
reveal that evaluating for fidelity-to-statistic is a reliable alternative in
highly stochastic scenarios. We extend our analysis to real-world wildfire
data, highlighting limitations in traditional wildfire prediction evaluation
methods, and suggest interpretable stochasticity-compatible alternatives.
\\ ( https://arxiv.org/abs/2402.15163 ,  10134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15170
Date: Fri, 23 Feb 2024 08:05:23 GMT   (21294kb,D)

Title: The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling
Authors: Jiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu,
  Zhenguo Li, Zhi-Ming Ma, Kenji Kawaguchi
Categories: cs.LG cs.AI
\\
  With the incorporation of the UNet architecture, diffusion probabilistic
models have become a dominant force in image generation tasks. One key design
in UNet is the skip connections between the encoder and decoder blocks.
Although skip connections have been shown to improve training stability and
model performance, we reveal that such shortcuts can be a limiting factor for
the complexity of the transformation. As the sampling steps decrease, the
generation process and the role of the UNet get closer to the push-forward
transformations from Gaussian distribution to the target, posing a challenge
for the network's complexity. To address this challenge, we propose
Skip-Tuning, a simple yet surprisingly effective training-free tuning method on
the skip connections. Our method can achieve 100% FID improvement for
pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of
ODE samplers regardless of sampling steps. Surprisingly, the improvement
persists when we increase the number of sampling steps and can even surpass the
best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive
exploratory experiments are conducted to shed light on the surprising
effectiveness. We observe that while Skip-Tuning increases the score-matching
losses in the pixel space, the losses in the feature space are reduced,
particularly at intermediate noise levels, which coincide with the most
effective range accounting for image quality improvement.
\\ ( https://arxiv.org/abs/2402.15170 ,  21294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15171
Date: Fri, 23 Feb 2024 08:07:54 GMT   (133kb,D)

Title: Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial
  Semi-Bandits
Authors: Julien Zhou (Thoth, STATIFY), Pierre Gaillard (Thoth), Thibaud Rahier,
  Houssam Zenati (SODA, PREMEDICAL), Julyan Arbel (STATIFY)
Categories: cs.LG math.ST stat.ML stat.TH
\\
  We address the problem of stochastic combinatorial semi-bandits, where a
player can select from P subsets of a set containing d base items. Most
existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the
reward distribution, like an upper bound on a sub-Gaussian proxy-variance,
which is hard to estimate tightly. In this work, we design a variance-adaptive
version of OLS-UCB, relying on an online estimation of the covariance
structure. Estimating the coefficients of a covariance matrix is much more
manageable in practical settings and results in improved regret upper bounds
compared to proxy variance-based algorithms. When covariance coefficients are
all non-negative, we show that our approach efficiently leverages the
semi-bandit feedback and provably outperforms bandit feedback approaches, not
only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is
not straightforward from most existing analyses.
\\ ( https://arxiv.org/abs/2402.15171 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15173
Date: Fri, 23 Feb 2024 08:11:55 GMT   (11099kb,D)

Title: Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed
  Zeroth-Order Optimizer
Authors: Yanjun Zhao, Sizhe Dang, Haishan Ye, Guang Dai, Yi Qian, Ivor W.Tsang
Categories: cs.LG
\\
  Fine-tuning large language models (LLMs) with classic first-order optimizers
entails prohibitive GPU memory due to the backpropagation process. Recent works
have turned to zeroth-order optimizers for fine-tuning, which save substantial
memory by using two forward passes. However, these optimizers are plagued by
the heterogeneity of parameter curvatures across different dimensions. In this
work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer
which is the first work to leverage the diagonal Hessian to enhance
zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the
expensive memory cost and only increases one forward pass per step. Extensive
experiments on various models (350M~66B parameters) indicate that HiZOO
improves model convergence, significantly reducing training steps and
effectively enhancing model accuracy. Moreover, we visualize the optimization
trajectories of HiZOO on test functions, illustrating its effectiveness in
handling heterogeneous curvatures. Lastly, we provide theoretical proofs of
convergence for HiZOO. Code is publicly available at
https://anonymous.4open.science/r/HiZOO27F8.
\\ ( https://arxiv.org/abs/2402.15173 ,  11099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15175
Date: Fri, 23 Feb 2024 08:14:36 GMT   (264kb,D)

Title: Unified View of Grokking, Double Descent and Emergent Abilities: A
  Perspective from Circuits Competition
Authors: Yufei Huang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun
Categories: cs.LG
\\
  Recent studies have uncovered intriguing phenomena in deep learning, such as
grokking, double descent, and emergent abilities in large language models,
which challenge human intuition and are crucial for a deeper understanding of
neural models. In this paper, we present a comprehensive framework that
provides a unified view of these three phenomena, focusing on the competition
between memorization and generalization circuits. This approach, initially
employed to explain grokking, is extended in our work to encompass a wider
range of model sizes and training data volumes. Our framework delineates four
distinct training dynamics, each depending on varying combinations of model
size and training data quantity. Utilizing this framework, we provide a
detailed analysis of the double descent phenomenon and propose two verifiable
predictions regarding its occurrence, both substantiated by our experimental
results. Moreover, we expand our framework to the multi-task learning paradigm,
demonstrating how algorithm tasks can be turned into emergent abilities. This
offers a novel perspective to understand emergent abilities in Large Language
Models.
\\ ( https://arxiv.org/abs/2402.15175 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15179
Date: Fri, 23 Feb 2024 08:21:02 GMT   (1178kb,D)

Title: Advancing Parameter Efficiency in Fine-tuning via Representation Editing
Authors: Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan
  Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang
Categories: cs.LG cs.CL
\\
  Parameter Efficient Fine-Tuning (PEFT) has gained significant attention for
its ability to achieve competitive results while updating only a small subset
of trainable parameters. Despite the promising performance of current PEFT
methods, they present challenges in hyperparameter selection, such as
determining the rank of LoRA or Adapter, or specifying the length of soft
prompts. In addressing these challenges, we propose a novel approach to
fine-tuning neural models, termed Representation EDiting (RED), which scales
and biases the representation produced at each layer. RED substantially reduces
the number of trainable parameters by a factor of $25,700$ compared to full
parameter fine-tuning, and by a factor of $32$ compared to LoRA. Remarkably,
RED achieves comparable or superior results to full parameter fine-tuning and
other PEFT methods. Extensive experiments were conducted across models of
varying architectures and scales, including RoBERTa, GPT-2, T5, and Llama-2,
and the results demonstrate the efficiency and efficacy of RED, positioning it
as a promising PEFT approach for large neural models.
\\ ( https://arxiv.org/abs/2402.15179 ,  1178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15180
Date: Fri, 23 Feb 2024 08:22:24 GMT   (4352kb,D)

Title: Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks
  with Self-Refinement
Authors: Heegyu Kim, Sehyun Yuk, Hyunsouk Cho
Categories: cs.LG cs.CL cs.CR
Comments: under review
\\
  Caution: This paper includes offensive words that could potentially cause
unpleasantness. Language models (LMs) are vulnerable to exploitation for
adversarial misuse. Training LMs for safety alignment is extensive and makes it
hard to respond to fast-developing attacks immediately, such as jailbreaks. We
propose self-refine with formatting that achieves outstanding safety even in
non-safety-aligned LMs and evaluate our method alongside several defense
baselines, demonstrating that it is the safest training-free method against
jailbreak attacks. Additionally, we proposed a formatting method that improves
the efficiency of the self-refine process while reducing attack success rates
in fewer iterations. We've also observed that non-safety-aligned LMs outperform
safety-aligned LMs in safety tasks by giving more helpful and safe responses.
In conclusion, our findings can achieve less safety risk with fewer
computational costs, allowing non-safety LM to be easily utilized in real-world
service.
\\ ( https://arxiv.org/abs/2402.15180 ,  4352kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15183
Date: Fri, 23 Feb 2024 08:29:42 GMT   (959kb,D)

Title: GraphEdit: Large Language Models for Graph Structure Learning
Authors: Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei,
  Liang Pang, Tat-Seng Chua, Chao Huang
Categories: cs.LG cs.AI
\\
  Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies
and interactions among nodes in graph-structured data by generating novel graph
structures. Graph Neural Networks (GNNs) have emerged as promising GSL
solutions, utilizing recursive message passing to encode node-wise
inter-dependencies. However, many existing GSL methods heavily depend on
explicit graph structural information as supervision signals, leaving them
susceptible to challenges such as data noise and sparsity. In this work, we
propose GraphEdit, an approach that leverages large language models (LLMs) to
learn complex node relationships in graph-structured data. By enhancing the
reasoning capabilities of LLMs through instruction-tuning over graph
structures, we aim to overcome the limitations associated with explicit graph
structural information and enhance the reliability of graph structure learning.
Our approach not only effectively denoises noisy connections but also
identifies node-wise dependencies from a global perspective, providing a
comprehensive understanding of the graph structure. We conduct extensive
experiments on multiple benchmark datasets to demonstrate the effectiveness and
robustness of GraphEdit across various settings. We have made our model
implementation available at: https://github.com/HKUDS/GraphEdit.
\\ ( https://arxiv.org/abs/2402.15183 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15188
Date: Fri, 23 Feb 2024 08:36:28 GMT   (2782kb,D)

Title: Parameter-Free Algorithms for Performative Regret Minimization under
  Decision-Dependent Distributions
Authors: Sungwoo Park, Junyeop Kwon, Byeongnoh Kim, Suhyun Chae, Jeeyong Lee,
  Dabeen Lee
Categories: cs.LG math.OC
\\
  This paper studies performative risk minimization, a formulation of
stochastic optimization under decision-dependent distributions. We consider the
general case where the performative risk can be non-convex, for which we
develop efficient parameter-free optimistic optimization-based methods. Our
algorithms significantly improve upon the existing Lipschitz bandit-based
method in many aspects. In particular, our framework does not require knowledge
about the sensitivity parameter of the distribution map and the Lipshitz
constant of the loss function. This makes our framework practically favorable,
together with the efficient optimistic optimization-based tree-search
mechanism. We provide experimental results that demonstrate the numerical
superiority of our algorithms over the existing method and other black-box
optimistic optimization methods.
\\ ( https://arxiv.org/abs/2402.15188 ,  2782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15194
Date: Fri, 23 Feb 2024 08:54:42 GMT   (5692kb,D)

Title: Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized
  Control
Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali,
  Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani,
  Sergey Levine
Categories: cs.LG cs.AI stat.ML
Comments: Under review (codes will be released soon)
\\
  Diffusion models excel at capturing complex data distributions, such as those
of natural images and proteins. While diffusion models are trained to represent
the distribution in the training dataset, we often are more concerned with
other properties, such as the aesthetic quality of the generated images or the
functional properties of generated proteins. Diffusion models can be finetuned
in a goal-directed way by maximizing the value of some reward function (e.g.,
the aesthetic quality of an image). However, these approaches may lead to
reduced sample diversity, significant deviations from the training data
distribution, and even poor sample quality due to the exploitation of an
imperfect reward function. The last issue often occurs when the reward function
is a learned model meant to approximate a ground-truth "genuine" reward, as is
the case in many practical applications. These challenges, collectively termed
"reward collapse," pose a substantial obstacle. To address this reward
collapse, we frame the finetuning problem as entropy-regularized control
against the pretrained diffusion model, i.e., directly optimizing
entropy-enhanced rewards with neural SDEs. We present theoretical and empirical
evidence that demonstrates our framework is capable of efficiently generating
diverse samples with high genuine rewards, mitigating the overoptimization of
imperfect reward models.
\\ ( https://arxiv.org/abs/2402.15194 ,  5692kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15198
Date: Fri, 23 Feb 2024 08:59:04 GMT   (8248kb,D)

Title: Bidirectional Uncertainty-Based Active Learning for Open Set Annotation
Authors: Chen-Chen Zong, Ye-Wen Wang, Kun-Peng Ning, Haibo Ye, Sheng-Jun Huang
Categories: cs.LG
\\
  Active learning (AL) in open set scenarios presents a novel challenge of
identifying the most valuable examples in an unlabeled data pool that comprises
data from both known and unknown classes. Traditional methods prioritize
selecting informative examples with low confidence, with the risk of mistakenly
selecting unknown-class examples with similarly low confidence. Recent methods
favor the most probable known-class examples, with the risk of picking simple
already mastered examples. In this paper, we attempt to query examples that are
both likely from known classes and highly informative, and propose a
\textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework.
Specifically, we achieve this by first pushing the unknown class examples
toward regions with high-confidence predictions with our proposed
\textit{Random Label Negative Learning} method. Then, we propose a
\textit{Bidirectional Uncertainty sampling} strategy by jointly estimating
uncertainty posed by both positive and negative learning to perform consistent
and stable sampling. BUAL successfully extends existing uncertainty-based AL
methods to complex open-set scenarios. Extensive experiments on multiple
datasets with varying openness demonstrate that BUAL achieves state-of-the-art
performance.
\\ ( https://arxiv.org/abs/2402.15198 ,  8248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15220
Date: Fri, 23 Feb 2024 09:29:19 GMT   (506kb,D)

Title: ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and
  Two-Phase Partition
Authors: Lu Ye, Ze Tao, Yong Huang and Yang Li
Categories: cs.LG cs.CL
\\
  Self-attention is an essential component of large language models(LLMs) but a
significant source of inference latency for long sequences. In multi-tenant
LLMs serving scenarios, the compute and memory operation cost of self-attention
can be optimized by using the probability that multiple LLM requests have
shared system prompts in prefixes. In this paper, we introduce ChunkAttention,
a prefix-aware self-attention module that can detect matching prompt prefixes
across multiple requests and share their key/value tensors in memory at runtime
to improve the memory utilization of KV cache. This is achieved by breaking
monolithic key/value tensors into smaller chunks and structuring them into the
auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,
we design an efficient self-attention kernel, where a two-phase partition
algorithm is implemented to improve the data locality during self-attention
computation in the presence of shared system prompts. Experiments show that
ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$
compared to the start-of-the-art implementation, with the length of the system
prompt ranging from 1024 to 4096.
\\ ( https://arxiv.org/abs/2402.15220 ,  506kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15227
Date: Fri, 23 Feb 2024 09:43:58 GMT   (22618kb,D)

Title: Fixed Random Classifier Rearrangement for Continual Learning
Authors: Shengyang Huang and Jianwen Mo
Categories: cs.LG cs.AI
\\
  With the explosive growth of data, continual learning capability is
increasingly important for neural networks. Due to catastrophic forgetting,
neural networks inevitably forget the knowledge of old tasks after learning new
ones. In visual classification scenario, a common practice of alleviating the
forgetting is to constrain the backbone. However, the impact of classifiers is
underestimated. In this paper, we analyze the variation of model predictions in
sequential binary classification tasks and find that the norm of the equivalent
one-class classifiers significantly affects the forgetting level. Based on this
conclusion, we propose a two-stage continual learning algorithm named Fixed
Random Classifier Rearrangement (FRCR). In first stage, FRCR replaces the
learnable classifiers with fixed random classifiers, constraining the norm of
the equivalent one-class classifiers without affecting the performance of the
network. In second stage, FRCR rearranges the entries of new classifiers to
implicitly reduce the drift of old latent representations. The experimental
results on multiple datasets show that FRCR significantly mitigates the model
forgetting; subsequent experimental analyses further validate the effectiveness
of the algorithm.
\\ ( https://arxiv.org/abs/2402.15227 ,  22618kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15231
Date: Fri, 23 Feb 2024 09:47:27 GMT   (528kb,D)

Title: Which Model to Transfer? A Survey on Transferability Estimation
Authors: Yuhe Ding, Bo Jiang, Aijing Yu, Aihua Zheng, Jian Liang
Categories: cs.LG cs.CV
\\
  Transfer learning methods endeavor to leverage relevant knowledge from
existing source pre-trained models or datasets to solve downstream target
tasks. With the increase in the scale and quantity of available pre-trained
models nowadays, it becomes critical to assess in advance whether they are
suitable for a specific target task. Model transferability estimation is an
emerging and growing area of interest, aiming to propose a metric to quantify
this suitability without training them individually, which is computationally
prohibitive. Despite extensive recent advances already devoted to this area,
they have custom terminological definitions and experimental settings. In this
survey, we present the first review of existing advances in this area and
categorize them into two separate realms: source-free model transferability
estimation and source-dependent model transferability estimation. Each category
is systematically defined, accompanied by a comprehensive taxonomy. Besides, we
address challenges and outline future research directions, intending to provide
a comprehensive guide to aid researchers and practitioners.
\\ ( https://arxiv.org/abs/2402.15231 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15247
Date: Fri, 23 Feb 2024 10:21:07 GMT   (425kb,D)

Title: A Bargaining-based Approach for Feature Trading in Vertical Federated
  Learning
Authors: Yue Cui, Liuyi Yao, Zitao Li, Yaliang Li, Bolin Ding, Xiaofang Zhou
Categories: cs.LG cs.AI cs.MA
\\
  Vertical Federated Learning (VFL) has emerged as a popular machine learning
paradigm, enabling model training across the data and the task parties with
different features about the same user set while preserving data privacy. In
production environment, VFL usually involves one task party and one data party.
Fair and economically efficient feature trading is crucial to the
commercialization of VFL, where the task party is considered as the data
consumer who buys the data party's features. However, current VFL feature
trading practices often price the data party's data as a whole and assume
transactions occur prior to the performing VFL. Neglecting the performance
gains resulting from traded features may lead to underpayment and overpayment
issues. In this study, we propose a bargaining-based feature trading approach
in VFL to encourage economically efficient transactions. Our model incorporates
performance gain-based pricing, taking into account the revenue-based
optimization objectives of both parties. We analyze the proposed bargaining
model under perfect and imperfect performance information settings, proving the
existence of an equilibrium that optimizes the parties' objectives. Moreover,
we develop performance gain estimation-based bargaining strategies for
imperfect performance information scenarios and discuss potential security
issues and solutions. Experiments on three real-world datasets demonstrate the
effectiveness of the proposed bargaining model.
\\ ( https://arxiv.org/abs/2402.15247 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15255
Date: Fri, 23 Feb 2024 10:49:04 GMT   (345kb,D)

Title: Optimal Transport for Structure Learning Under Missing Data
Authors: Vy Vo, He Zhao, Trung Le, Edwin V. Bonilla, Dinh Phung
Categories: cs.LG cs.AI
\\
  Causal discovery in the presence of missing data introduces a chicken-and-egg
dilemma. While the goal is to recover the true causal structure, robust
imputation requires considering the dependencies or preferably causal relations
among variables. Merely filling in missing values with existing imputation
methods and subsequently applying structure learning on the complete data is
empirical shown to be sub-optimal. To this end, we propose in this paper a
score-based algorithm, based on optimal transport, for learning causal
structure from missing data. This optimal transport viewpoint diverges from
existing score-based approaches that are dominantly based on EM. We project
structure learning as a density fitting problem, where the goal is to find the
causal model that induces a distribution of minimum Wasserstein distance with
the distribution over the observed data. Through extensive simulations and
real-data experiments, our framework is shown to recover the true causal graphs
more effectively than the baselines in various simulations and real-data
experiments. Empirical evidences also demonstrate the superior scalability of
our approach, along with the flexibility to incorporate any off-the-shelf
causal discovery methods for complete data.
\\ ( https://arxiv.org/abs/2402.15255 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15262
Date: Fri, 23 Feb 2024 11:19:02 GMT   (1529kb,D)

Title: Dynamic Memory Based Adaptive Optimization
Authors: Bal\'azs Szegedy, Domonkos Czifra, P\'eter K\H{o}r\"osi-Szab\'o
Categories: cs.LG cs.AI math.OC
\\
  Define an optimizer as having memory $k$ if it stores $k$ dynamically
changing vectors in the parameter space. Classical SGD has memory $0$, momentum
SGD optimizer has $1$ and Adam optimizer has $2$. We address the following
questions: How can optimizers make use of more memory units? What information
should be stored in them? How to use them for the learning steps? As an
approach to the last question, we introduce a general method called
"Retrospective Learning Law Correction" or shortly RLLC. This method is
designed to calculate a dynamically varying linear combination (called learning
law) of memory units, which themselves may evolve arbitrarily. We demonstrate
RLLC on optimizers whose memory units have linear update rules and small memory
($\leq 4$ memory units). Our experiments show that in a variety of standard
problems, these optimizers outperform the above mentioned three classical
optimizers. We conclude that RLLC is a promising framework for boosting the
performance of known optimizers by adding more memory units and by making them
more adaptive.
\\ ( https://arxiv.org/abs/2402.15262 ,  1529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15266
Date: Fri, 23 Feb 2024 11:27:10 GMT   (423kb,D)

Title: Calibration of Deep Learning Classification Models in fNIRS
Authors: Zhihao Cao, Zizhou Luo
Categories: cs.LG eess.SP
DOI: 10.36227/techrxiv.170861971.16301364/v1
\\
  Functional near-infrared spectroscopy (fNIRS) is a valuable non-invasive tool
for monitoring brain activity. The classification of fNIRS data in relation to
conscious activity holds significance for advancing our understanding of the
brain and facilitating the development of brain-computer interfaces (BCI). Many
researchers have turned to deep learning to tackle the classification
challenges inherent in fNIRS data due to its strong generalization and
robustness. In the application of fNIRS, reliability is really important, and
one mathematical formulation of the reliability of confidence is calibration.
However, many researchers overlook the important issue of calibration. To
address this gap, we propose integrating calibration into fNIRS field and
assess the reliability of existing models. Surprisingly, our results indicate
poor calibration performance in many proposed models. To advance calibration
development in the fNIRS field, we summarize three practical tips. Through this
letter, we hope to emphasize the critical role of calibration in fNIRS research
and argue for enhancing the reliability of deep learning-based predictions in
fNIRS classification tasks. All data from our experimental process are openly
available on GitHub.
\\ ( https://arxiv.org/abs/2402.15266 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15270
Date: Fri, 23 Feb 2024 11:32:46 GMT   (861kb,D)

Title: Smoothed Graph Contrastive Learning via Seamless Proximity Integration
Authors: Maysam Behmanesh, Maks Ovsjanikov
Categories: cs.LG cs.AI
Comments: 17 pages
\\
  Graph contrastive learning (GCL) aligns node representations by classifying
node pairs into positives and negatives using a selection process that
typically relies on establishing correspondences within two augmented graphs.
The conventional GCL approaches incorporate negative samples uniformly in the
contrastive loss, resulting in the equal treatment negative nodes, regardless
of their proximity to the true positive. In this paper, we present a Smoothed
Graph Contrastive Learning model (SGCL), which leverages the geometric
structure of augmented graphs to inject proximity information associated with
positive/negative pairs in the contrastive loss, thus significantly
regularizing the learning process. The proposed SGCL adjusts the penalties
associated with node pairs in the contrastive loss by incorporating three
distinct smoothing techniques that result in proximity aware positives and
negatives. To enhance scalability for large-scale graphs, the proposed
framework incorporates a graph batch-generating strategy that partitions the
given graphs into multiple subgraphs, facilitating efficient training in
separate batches. Through extensive experimentation in the unsupervised setting
on various benchmarks, particularly those of large scale, we demonstrate the
superiority of our proposed framework against recent baselines.
\\ ( https://arxiv.org/abs/2402.15270 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15274
Date: Fri, 23 Feb 2024 11:37:56 GMT   (3391kb,D)

Title: Classification Under Strategic Self-Selection
Authors: Guy Horowitz, Yonatan Sommer, Moran Koren and Nir Rosenfeld
Categories: cs.LG
\\
  When users stand to gain from certain predictions, they are prone to act
strategically to obtain favorable predictive outcomes. Whereas most works on
strategic classification consider user actions that manifest as feature
modifications, we study a novel setting in which users decide -- in response to
the learned classifier -- whether to at all participate (or not). For learning
approaches of increasing strategic awareness, we study the effects of
self-selection on learning, and the implications of learning on the composition
of the self-selected population. We then propose a differentiable framework for
learning under self-selective behavior, which can be optimized effectively. We
conclude with experiments on real data and simulated behavior that both
complement our analysis and demonstrate the utility of our approach.
\\ ( https://arxiv.org/abs/2402.15274 ,  3391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15283
Date: Fri, 23 Feb 2024 12:27:48 GMT   (7385kb,D)

Title: When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination
Authors: Martin Benfeghoul, Umais Zahid, Qinghai Guo, Zafeirios Fountas
Categories: cs.LG cs.AI
ACM-class: I.2.0; I.2.8; I.2.10; I.4.5; I.4.10
\\
  In an unfamiliar setting, a model-based reinforcement learning agent can be
limited by the accuracy of its world model. In this work, we present a novel,
training-free approach to improving the performance of such agents separately
from planning and learning. We do so by applying iterative inference at
decision-time, to fine-tune the inferred agent states based on the coherence of
future state representations. Our approach achieves a consistent improvement in
both reconstruction accuracy and task performance when applied to visual 3D
navigation tasks. We go on to show that considering more future states further
improves the performance of the agent in partially-observable environments, but
not in a fully-observable one. Finally, we demonstrate that agents with less
training pre-evaluation benefit most from our approach.
\\ ( https://arxiv.org/abs/2402.15283 ,  7385kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15284
Date: Fri, 23 Feb 2024 12:28:31 GMT   (717kb,D)

Title: Spatiotemporal Observer Design for Predictive Learning of
  High-Dimensional Data
Authors: Tongyi Liang and Han-Xiong Li
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: Under review by IEEE Transactions on Pattern Analysis and Machine
  Intelligence
\\
  Although deep learning-based methods have shown great success in
spatiotemporal predictive learning, the framework of those models is designed
mainly by intuition. How to make spatiotemporal forecasting with theoretical
guarantees is still a challenging issue. In this work, we tackle this problem
by applying domain knowledge from the dynamical system to the framework design
of deep learning models. An observer theory-guided deep learning architecture,
called Spatiotemporal Observer, is designed for predictive learning of high
dimensional data. The characteristics of the proposed framework are twofold:
firstly, it provides the generalization error bound and convergence guarantee
for spatiotemporal prediction; secondly, dynamical regularization is introduced
to enable the model to learn system dynamics better during training. Further
experimental results show that this framework could capture the spatiotemporal
dynamics and make accurate predictions in both one-step-ahead and
multi-step-ahead forecasting scenarios.
\\ ( https://arxiv.org/abs/2402.15284 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15290
Date: Fri, 23 Feb 2024 12:36:31 GMT   (182kb,D)

Title: Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
Authors: Tongyi Liang and Han-Xiong Li
Categories: cs.LG cs.AI
Comments: Under review by IEEE Transactions on Neural Networks and Learning
  Systems
\\
  The trade-off between performance and computational efficiency in
long-sequence modeling becomes a bottleneck for existing models. Inspired by
the continuous state space models (SSMs) with multi-input and multi-output in
control theory, we propose a new neural network called Linear Dynamics-embedded
Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties
enable LDNN to have few parameters, flexible inference, and efficient training
in long-sequence tasks. Two efficient strategies, diagonalization and
$'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to
reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to
$O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional
noncausal and multi-head settings to accommodate a broader range of
applications. Extensive experiments on the Long Range Arena (LRA) demonstrate
the effectiveness and state-of-the-art performance of LDNN.
\\ ( https://arxiv.org/abs/2402.15290 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15309
Date: Fri, 23 Feb 2024 13:24:19 GMT   (1420kb,D)

Title: Counterfactual Generation with Identifiability Guarantees
Authors: Hanqi Yan, Lingjing Kong, Lin Gui, Yuejie Chi, Eric Xing, Yulan He,
  Kun Zhang
Categories: cs.LG cs.CL
Comments: Neurips23. Controllable generation in causal perspective with a case
  study of ChatGPT, sheds light on theory-guaranteed alignment in language
  models
\\
  Counterfactual generation lies at the core of various machine learning tasks,
including image translation and controllable text generation. This generation
process usually requires the identification of the disentangled latent
representations, such as content and style, that underlie the observed data.
However, it becomes more challenging when faced with a scarcity of paired data
and labeling information. Existing disentangled methods crucially rely on
oversimplified assumptions, such as assuming independent content and style
variables, to identify the latent variables, even though such assumptions may
not hold for complex data distributions. For instance, food reviews tend to
involve words like tasty, whereas movie reviews commonly contain words such as
thrilling for the same positive sentiment. This problem is exacerbated when
data are sampled from multiple domains since the dependence between content and
style may vary significantly over domains. In this work, we tackle the
domain-varying dependence between the content and the style variables inherent
in the counterfactual generation task. We provide identification guarantees for
such latent-variable models by leveraging the relative sparsity of the
influences from different latent variables. Our theoretical insights enable the
development of a doMain AdapTive counTerfactual gEneration model, called
(MATTE). Our theoretically grounded framework achieves state-of-the-art
performance in unsupervised style transfer tasks, where neither paired data nor
style labels are utilized, across four large-scale datasets. Code is available
at https://github.com/hanqi-qi/Matte.git
\\ ( https://arxiv.org/abs/2402.15309 ,  1420kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15315
Date: Fri, 23 Feb 2024 13:34:03 GMT   (17kb)

Title: On Minimal Depth in Neural Networks
Authors: Juan L. Valerdi
Categories: cs.LG cs.DM math.CO
Comments: 20 pages
MSC-class: 68T07 (Primary) 52B12 (Secondary)
\\
  A characterization of the representability of neural networks is relevant to
comprehend their success in artificial intelligence. This study investigate two
topics on ReLU neural network expressivity and their connection with a
conjecture related to the minimum depth required for representing any
continuous piecewise linear function (CPWL). The topics are the minimal depth
representation of the sum and max operations, as well as the exploration of
polytope neural networks. For the sum operation, we establish a sufficient
condition on the minimal depth of the operands to find the minimal depth of the
operation. In contrast, regarding the max operation, a comprehensive set of
examples is presented, demonstrating that no sufficient conditions, depending
solely on the depth of the operands, would imply a minimal depth for the
operation. The study also examine the minimal depth relationship between convex
CPWL functions. On polytope neural networks, we investigate several fundamental
properties, deriving results equivalent to those of ReLU networks, such as
depth inclusions and depth computation from vertices. Notably, we compute the
minimal depth of simplices, which is strictly related to the minimal depth
conjecture in ReLU networks.
\\ ( https://arxiv.org/abs/2402.15315 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15319
Date: Fri, 23 Feb 2024 13:39:16 GMT   (499kb,D)

Title: GPTVQ: The Blessing of Dimensionality for LLM Quantization
Authors: Mart van Baalen, Andrey Kuzmin, Markus Nagel, Peter Couperus, Cedric
  Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul Whatmough
Categories: cs.LG cs.CL
\\
  In this work we show that the size versus accuracy trade-off of neural
network quantization can be significantly improved by increasing the
quantization dimensionality. We propose the GPTVQ method, a new fast method for
post-training vector quantization (VQ) that scales well to Large Language
Models (LLMs). Our method interleaves quantization of one or more columns with
updates to the remaining unquantized weights, using information from the
Hessian of the per-layer output reconstruction MSE. Quantization codebooks are
initialized using an efficient data-aware version of the EM algorithm. The
codebooks are then updated, and further compressed by using integer
quantization and SVD-based compression. GPTVQ establishes a new state-of-the
art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2
and Mistral. Furthermore, our method is efficient: on a single H100 it takes
between 3 and 11 hours to process a Llamav2-70B model, depending on
quantization setting. Lastly, with on-device timings for VQ decompression on a
mobile CPU we show that VQ leads to improved latency compared to using a 4-bit
integer format.
\\ ( https://arxiv.org/abs/2402.15319 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15326
Date: Fri, 23 Feb 2024 13:44:57 GMT   (741kb,D)

Title: Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective
  of Operator Semigroup Theory
Authors: Weichen Zhao, Chenguang Wang, Xinyan Wang, Congying Han, Tiande Guo,
  Tianshu Yu
Categories: cs.LG
\\
  This paper presents a novel study of the oversmoothing issue in
diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches
grounded in random walk analysis or particle systems, we approach this problem
through operator semigroup theory. This theoretical framework allows us to
rigorously prove that oversmoothing is intrinsically linked to the ergodicity
of the diffusion operator. This finding further poses a general and mild
ergodicity-breaking condition, encompassing the various specific solutions
previously offered, thereby presenting a more universal and theoretically
grounded approach to mitigating oversmoothing in diffusion-based GNNs.
Additionally, we offer a probabilistic interpretation of our theory, forging a
link with prior works and broadening the theoretical horizon. Our experimental
results reveal that this ergodicity-breaking term effectively mitigates
oversmoothing measured by Dirichlet energy, and simultaneously enhances
performance in node classification tasks.
\\ ( https://arxiv.org/abs/2402.15326 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15328
Date: Fri, 23 Feb 2024 13:51:20 GMT   (168kb,D)

Title: Towards Principled Task Grouping for Multi-Task Learning
Authors: Chenguang Wang, Xuanhao Pan, Tianshu Yu
Categories: cs.LG
\\
  This paper presents a novel approach to task grouping in Multitask Learning
(MTL), advancing beyond existing methods by addressing key theoretical and
practical limitations. Unlike prior studies, our approach offers a more
theoretically grounded method that does not rely on restrictive assumptions for
constructing transfer gains. We also propose a flexible mathematical
programming formulation which can accommodate a wide spectrum of resource
constraints, thus enhancing its versatility. Experimental results across
diverse domains, including computer vision datasets, combinatorial optimization
benchmarks and time series tasks, demonstrate the superiority of our method
over extensive baselines, validating its effectiveness and general
applicability in MTL.
\\ ( https://arxiv.org/abs/2402.15328 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15332
Date: Fri, 23 Feb 2024 14:01:53 GMT   (90kb)

Title: Categorical Deep Learning: An Algebraic Theory of Architectures
Authors: Bruno Gavranovi\'c, Paul Lessard, Andrew Dudzik, Tamara von Glehn,
  Jo\~ao G. M. Ara\'ujo, Petar Veli\v{c}kovi\'c
Categories: cs.LG cs.AI math.CT math.RA stat.ML
Comments: Work in progress -- comments welcome. More info at
  categoricaldeeplearning.com
\\
  We present our position on the elusive quest for a general-purpose framework
for specifying and studying deep learning architectures. Our opinion is that
the key attempts made so far lack a coherent bridge between specifying
constraints which models must satisfy and specifying their implementations.
Focusing on building a such a bridge, we propose to apply category theory --
precisely, the universal algebra of monads valued in a 2-category of parametric
maps -- as a single theory elegantly subsuming both of these flavours of neural
network design. To defend our position, we show how this theory recovers
constraints induced by geometric deep learning, as well as implementations of
many architectures drawn from the diverse landscape of neural networks, such as
RNNs. We also illustrate how the theory naturally encodes many standard
constructs in computer science and automata theory.
\\ ( https://arxiv.org/abs/2402.15332 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15345
Date: Fri, 23 Feb 2024 14:26:12 GMT   (1123kb,D)

Title: Fourier Basis Density Model
Authors: Alfredo De la Fuente, Saurabh Singh, Johannes Ball\'e
Categories: cs.LG stat.ML
\\
  We introduce a lightweight, flexible and end-to-end trainable probability
density model parameterized by a constrained Fourier basis. We assess its
performance at approximating a range of multi-modal 1D densities, which are
generally difficult to fit. In comparison to the deep factorized model
introduced in [1], our model achieves a lower cross entropy at a similar
computational budget. In addition, we also evaluate our method on a toy
compression task, demonstrating its utility in learned compression.
\\ ( https://arxiv.org/abs/2402.15345 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15347
Date: Fri, 23 Feb 2024 14:31:10 GMT   (4478kb,D)

Title: Information-Theoretic Safe Bayesian Optimization
Authors: Alessandro G. Bottero, Carlos E. Luis, Julia Vinogradska, Felix
  Berkenkamp, Jan Peters
Categories: cs.LG cs.AI stat.ML
Comments: arXiv admin note: text overlap with arXiv:2212.04914
\\
  We consider a sequential decision making task, where the goal is to optimize
an unknown function without evaluating parameters that violate an a~priori
unknown (safety) constraint. A common approach is to place a Gaussian process
prior on the unknown functions and allow evaluations only in regions that are
safe with high probability. Most current methods rely on a discretization of
the domain and cannot be directly extended to the continuous case. Moreover,
the way in which they exploit regularity assumptions about the constraint
introduces an additional critical hyperparameter. In this paper, we propose an
information-theoretic safe exploration criterion that directly exploits the GP
posterior to identify the most informative safe parameters to evaluate. The
combination of this exploration criterion with a well known Bayesian
optimization acquisition function yields a novel safe Bayesian optimization
selection criterion. Our approach is naturally applicable to continuous domains
and does not require additional explicit hyperparameters. We theoretically
analyze the method and show that we do not violate the safety constraint with
high probability and that we learn about the value of the safe optimum up to
arbitrary precision. Empirical evaluations demonstrate improved data-efficiency
and scalability.
\\ ( https://arxiv.org/abs/2402.15347 ,  4478kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15351
Date: Fri, 23 Feb 2024 14:38:19 GMT   (6003kb,D)

Title: AutoMMLab: Automatically Generating Deployable Models from Language
  Instructions for Computer Vision Tasks
Authors: Zekang Yang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, Wentao Liu
Categories: cs.LG cs.CV
\\
  Automated machine learning (AutoML) is a collection of techniques designed to
automate the machine learning development process. While traditional AutoML
approaches have been successfully applied in several critical steps of model
development (e.g. hyperparameter optimization), there lacks a AutoML system
that automates the entire end-to-end model production workflow. To fill this
blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that
follows user's language instructions to automate the whole model production
workflow for computer vision tasks. The proposed AutoMMLab system effectively
employs LLMs as the bridge to connect AutoML and OpenMMLab community,
empowering non-expert individuals to easily build task-specific models via a
user-friendly language interface. Specifically, we propose RU-LLaMA to
understand users' request and schedule the whole pipeline, and propose a novel
LLM-based hyperparameter optimizer called HPO-LLaMA to effectively search for
the optimal hyperparameters. Experiments show that our AutoMMLab system is
versatile and covers a wide range of mainstream tasks, including
classification, detection, segmentation and keypoint estimation. We further
develop a new benchmark, called LAMP, for studying key components in the
end-to-end prompt-based model training pipeline. Code, model, and data will be
released.
\\ ( https://arxiv.org/abs/2402.15351 ,  6003kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15390
Date: Fri, 23 Feb 2024 15:42:12 GMT   (5231kb,D)

Title: Explorations of Self-Repair in Language Models
Authors: Cody Rushing, Neel Nanda
Categories: cs.LG cs.AI cs.CL
\\
  Prior interpretability research studying narrow distributions has
preliminarily identified self-repair, a phenomena where if components in large
language models are ablated, later components will change their behavior to
compensate. Our work builds off this past literature, demonstrating that
self-repair exists on a variety of models families and sizes when ablating
individual attention heads on the full training distribution. We further show
that on the full training distribution self-repair is imperfect, as the
original direct effect of the head is not fully restored, and noisy, since the
degree of self-repair varies significantly across different prompts (sometimes
overcorrecting beyond the original effect). We highlight two different
mechanisms that contribute to self-repair, including changes in the final
LayerNorm scaling factor (which can repair up to 30% of the direct effect) and
sparse sets of neurons implementing Anti-Erasure. We additionally discuss the
implications of these results for interpretability practitioners and close with
a more speculative discussion on the mystery of why self-repair occurs in these
models at all, highlighting evidence for the Iterative Inference hypothesis in
language models, a framework that predicts self-repair.
\\ ( https://arxiv.org/abs/2402.15390 ,  5231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15391
Date: Fri, 23 Feb 2024 15:47:26 GMT   (35178kb,D)

Title: Genie: Generative Interactive Environments
Authors: Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge
  Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris
  Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas
  Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei
  Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim
  Rockt\"aschel
Categories: cs.LG cs.AI cs.CV
Comments: https://sites.google.com/corp/view/genie-2024/
\\
  We introduce Genie, the first generative interactive environment trained in
an unsupervised manner from unlabelled Internet videos. The model can be
prompted to generate an endless variety of action-controllable virtual worlds
described through text, synthetic images, photographs, and even sketches. At
11B parameters, Genie can be considered a foundation world model. It is
comprised of a spatiotemporal video tokenizer, an autoregressive dynamics
model, and a simple and scalable latent action model. Genie enables users to
act in the generated environments on a frame-by-frame basis despite training
without any ground-truth action labels or other domain-specific requirements
typically found in the world model literature. Further the resulting learned
latent action space facilitates training agents to imitate behaviors from
unseen videos, opening the path for training generalist agents of the future.
\\ ( https://arxiv.org/abs/2402.15391 ,  35178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15392
Date: Fri, 23 Feb 2024 15:49:46 GMT   (123kb)

Title: Offline Inverse RL: New Solution Concepts and Provably Efficient
  Algorithms
Authors: Filippo Lazzati, Mirco Mutti, Alberto Maria Metelli
Categories: cs.LG
\\
  Inverse reinforcement learning (IRL) aims to recover the reward function of
an expert agent from demonstrations of behavior. It is well known that the IRL
problem is fundamentally ill-posed, i.e., many reward functions can explain the
demonstrations. For this reason, IRL has been recently reframed in terms of
estimating the feasible reward set, thus, postponing the selection of a single
reward. However, so far, the available formulations and algorithmic solutions
have been proposed and analyzed mainly for the online setting, where the
learner can interact with the environment and query the expert at will. This is
clearly unrealistic in most practical applications, where the availability of
an offline dataset is a much more common scenario. In this paper, we introduce
a novel notion of feasible reward set capturing the opportunities and
limitations of the offline setting and we analyze the complexity of its
estimation. This requires the introduction an original learning framework that
copes with the intrinsic difficulty of the setting, for which the data coverage
is not under control. Then, we propose two computationally and statistically
efficient algorithms, IRLO and PIRLO, for addressing the problem. In
particular, the latter adopts a specific form of pessimism to enforce the novel
desirable property of inclusion monotonicity of the delivered feasible set.
With this work, we aim to provide a panorama of the challenges of the offline
IRL problem and how they can be fruitfully addressed.
\\ ( https://arxiv.org/abs/2402.15392 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15393
Date: Fri, 23 Feb 2024 15:51:45 GMT   (2417kb,D)

Title: NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
Authors: Bernardo Esteves, Miguel Vasco, Francisco S. Melo
Categories: cs.LG cs.AI
\\
  While machine learning methods excel at pattern recognition, they struggle
with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep
Thinking methods show promise in learning algorithms that extrapolate: learning
in smaller environments and executing the learned algorithm in larger
environments. However, these works are limited to symmetrical tasks, where the
input and output dimensionalities are the same. To address this gap, we propose
NeuralThink, a new recurrent architecture that can consistently extrapolate to
both symmetrical and asymmetrical tasks, where the dimensionality of the input
and output are different. We contribute with a novel benchmark of asymmetrical
tasks for extrapolation. We show that NeuralThink consistently outperforms the
prior state-of-the-art Deep Thinking architectures, in regards to stable
extrapolation to large observations from smaller training sizes.
\\ ( https://arxiv.org/abs/2402.15393 ,  2417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15398
Date: Fri, 23 Feb 2024 16:00:04 GMT   (8713kb,D)

Title: TransFlower: An Explainable Transformer-Based Model with Flow-to-Flow
  Attention for Commuting Flow Prediction
Authors: Yan Luo, Zhuoyue Wan, Yuzhong Chen, Gengchen Mai, Fu-lai Chung, Kent
  Larson
Categories: cs.LG cs.AI cs.CY
\\
  Understanding the link between urban planning and commuting flows is crucial
for guiding urban development and policymaking. This research, bridging
computer science and urban studies, addresses the challenge of integrating
these fields with their distinct focuses. Traditional urban studies methods,
like the gravity and radiation models, often underperform in complex scenarios
due to their limited handling of multiple variables and reliance on overly
simplistic and unrealistic assumptions, such as spatial isotropy. While deep
learning models offer improved accuracy, their black-box nature poses a
trade-off between performance and explainability -- both vital for analyzing
complex societal phenomena like commuting flows. To address this, we introduce
TransFlower, an explainable, transformer-based model employing flow-to-flow
attention to predict urban commuting patterns. It features a geospatial encoder
with an anisotropy-aware relative location encoder for nuanced flow
representation. Following this, the transformer-based flow predictor enhances
this by leveraging attention mechanisms to efficiently capture flow
interactions. Our model outperforms existing methods by up to 30.8% Common Part
of Commuters, offering insights into mobility dynamics crucial for urban
planning and policy decisions.
\\ ( https://arxiv.org/abs/2402.15398 ,  8713kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15399
Date: Fri, 23 Feb 2024 16:01:44 GMT   (4431kb,D)

Title: Distributionally Robust Off-Dynamics Reinforcement Learning: Provable
  Efficiency with Linear Function Approximation
Authors: Zhishuai Liu, Pan Xu
Categories: cs.LG
Comments: 30 pages, 4 figures. To appear in the proceedings of the 27th
  International Conference on Artificial Intelligence and Statistics (AISTATS)
\\
  We study off-dynamics Reinforcement Learning (RL), where the policy is
trained on a source domain and deployed to a distinct target domain. We aim to
solve this problem via online distributionally robust Markov decision processes
(DRMDPs), where the learning algorithm actively interacts with the source
domain while seeking the optimal performance under the worst possible dynamics
that is within an uncertainty set of the source domain's transition kernel. We
provide the first study on online DRMDPs with function approximation for
off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity,
even when the nominal transition kernel is linear, leading to error
propagation. By designing a $d$-rectangular uncertainty set using the total
variation distance, we remove this additional nonlinearity and bypass the error
propagation. We then introduce DR-LSVI-UCB, the first provably efficient online
DRMDP algorithm for off-dynamics RL with function approximation, and establish
a polynomial suboptimality bound that is independent of the state and action
space sizes. Our work makes the first step towards a deeper understanding of
the provable efficiency of online DRMDPs with linear function approximation.
Finally, we substantiate the performance and robustness of DR-LSVI-UCB through
different numerical experiments.
\\ ( https://arxiv.org/abs/2402.15399 ,  4431kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15404
Date: Fri, 23 Feb 2024 16:06:38 GMT   (1805kb,D)

Title: United We Pretrain, Divided We Fail! Representation Learning for Time
  Series by Pretraining on 75 Datasets at Once
Authors: Maurice Kraus and Felix Divo and David Steinmann and Devendra Singh
  Dhami and Kristian Kersting
Categories: cs.LG
\\
  In natural language processing and vision, pretraining is utilized to learn
effective representations. Unfortunately, the success of pretraining does not
easily carry over to time series due to potential mismatch between sources and
target. Actually, common belief is that multi-dataset pretraining does not work
for time series! Au contraire, we introduce a new self-supervised contrastive
pretraining approach to learn one encoding from many unlabeled and diverse time
series datasets, so that the single learned representation can then be reused
in several target domains for, say, classification. Specifically, we propose
the XD-MixUp interpolation method and the Soft Interpolation Contextual
Contrasting (SICC) loss. Empirically, this outperforms both supervised training
and other self-supervised pretraining methods when finetuning on low-data
regimes. This disproves the common belief: We can actually learn from multiple
time series datasets, even from 75 at once.
\\ ( https://arxiv.org/abs/2402.15404 ,  1805kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15406
Date: Fri, 23 Feb 2024 16:07:39 GMT   (582kb)

Title: Conformalized-DeepONet: A Distribution-Free Framework for Uncertainty
  Quantification in Deep Operator Networks
Authors: Christian Moya, Amirhossein Mollaali, Zecheng Zhang, Lu Lu, Guang Lin
Categories: cs.LG cs.NA math.NA
\\
  In this paper, we adopt conformal prediction, a distribution-free uncertainty
quantification (UQ) framework, to obtain confidence prediction intervals with
coverage guarantees for Deep Operator Network (DeepONet) regression. Initially,
we enhance the uncertainty quantification frameworks (B-DeepONet and
Prob-DeepONet) previously proposed by the authors by using split conformal
prediction. By combining conformal prediction with our Prob- and B-DeepONets,
we effectively quantify uncertainty by generating rigorous confidence intervals
for DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that
allows for a more natural use of split conformal prediction. We refer to this
distribution-free effective uncertainty quantification framework as split
conformal Quantile-DeepONet regression. Finally, we demonstrate the
effectiveness of the proposed methods using various ordinary, partial
differential equation numerical examples, and multi-fidelity learning.
\\ ( https://arxiv.org/abs/2402.15406 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15411
Date: Fri, 23 Feb 2024 16:19:32 GMT   (368kb)

Title: Optimisic Information Directed Sampling
Authors: Gergely Neu, Matteo Papini, Ludovic Schwartz
Categories: cs.LG
\\
  We study the problem of online learning in contextual bandit problems where
the loss function is assumed to belong to a known parametric function class. We
propose a new analytic framework for this setting that bridges the Bayesian
theory of information-directed sampling due to Russo and Van Roy (2018) and the
worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the
decision-estimation coefficient. Drawing from both lines of work, we propose a
algorithmic template called Optimistic Information-Directed Sampling and show
that it can achieve instance-dependent regret guarantees similar to the ones
achievable by the classic Bayesian IDS method, but with the major advantage of
not requiring any Bayesian assumptions. The key technical innovation of our
analysis is introducing an optimistic surrogate model for the regret and using
it to define a frequentist version of the Information Ratio of Russo and Van
Roy (2018), and a less conservative version of the Decision Estimation
Coefficient of Foster et al. (2021). Keywords: Contextual bandits,
information-directed sampling, decision estimation coefficient, first-order
regret bounds.
\\ ( https://arxiv.org/abs/2402.15411 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15413
Date: Fri, 23 Feb 2024 16:19:49 GMT   (1389kb,D)

Title: G-RepsNet: A Fast and General Construction of Equivariant Networks for
  Arbitrary Matrix Groups
Authors: Sourya Basu, Suhas Lohit, Matthew Brand
Categories: cs.LG
\\
  Group equivariance is a strong inductive bias useful in a wide range of deep
learning tasks. However, constructing efficient equivariant networks for
general groups and domains is difficult. Recent work by Finzi et al. (2021)
directly solves the equivariance constraint for arbitrary matrix groups to
obtain equivariant MLPs (EMLPs). But this method does not scale well and
scaling is crucial in deep learning. Here, we introduce Group Representation
Networks (G-RepsNets), a lightweight equivariant network for arbitrary matrix
groups with features represented using tensor polynomials. The key intuition
for our design is that using tensor representations in the hidden layers of a
neural network along with simple inexpensive tensor operations can lead to
expressive universal equivariant networks. We find G-RepsNet to be competitive
to EMLP on several tasks with group symmetries such as O(5), O(1, 3), and O(3)
with scalars, vectors, and second-order tensors as data types. On image
classification tasks, we find that G-RepsNet using second-order representations
is competitive and often even outperforms sophisticated state-of-the-art
equivariant models such as GCNNs (Cohen & Welling, 2016a) and E(2)-CNNs (Weiler
& Cesa, 2019). To further illustrate the generality of our approach, we show
that G-RepsNet is competitive to G-FNO (Helwig et al., 2023) and EGNN (Satorras
et al., 2021) on N-body predictions and solving PDEs, respectively, while being
efficient.
\\ ( https://arxiv.org/abs/2402.15413 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15414
Date: Fri, 23 Feb 2024 16:20:29 GMT   (3599kb,D)

Title: Does Combining Parameter-efficient Modules Improve Few-shot Transfer
  Accuracy?
Authors: Nader Asadi, Mahdi Beitollahi, Yasser Khalil, Yinchuan Li, Guojun
  Zhang, Xi Chen
Categories: cs.LG cs.CV
\\
  Parameter-efficient fine-tuning stands as the standard for efficiently
fine-tuning large language and vision models on downstream tasks. Specifically,
the efficiency of low-rank adaptation has facilitated the creation and sharing
of hundreds of custom LoRA modules, each trained on distinct data from various
downstream tasks. In this paper, we explore the composability of LoRA modules,
examining if combining these pre-trained modules enhances generalization to
unseen downstream tasks. Our investigation involves evaluating two approaches:
(a) uniform composition, involving averaging upstream LoRA modules with equal
weights, and (b) learned composition, where we learn the weights for each
upstream module and perform weighted averaging. Our experimental results on
both vision and language models reveal that in few-shot settings, where only a
limited number of samples are available for the downstream task, both uniform
and learned composition methods result in better transfer accuracy;
outperforming full fine-tuning and training a LoRA from scratch. Moreover, in
full-shot settings, learned composition performs comparably to regular LoRA
training with significantly fewer number of trainable parameters. Our research
unveils the potential of uniform composition for enhancing transferability in
low-shot settings, without introducing additional learnable parameters.
\\ ( https://arxiv.org/abs/2402.15414 ,  3599kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15415
Date: Fri, 23 Feb 2024 16:26:01 GMT   (16763kb,D)

Title: The Impact of LoRA on the Emergence of Clusters in Transformers
Authors: Hugo Koubbi, Matthieu Boussard and Louis Hernandez
Categories: cs.LG math.DS stat.ML
\\
  In this paper, we employ the mathematical framework on Transformers developed
by
\citet{sander2022sinkformers,geshkovski2023emergence,geshkovski2023mathematical}
to explore how variations in attention parameters and initial token values
impact the structural dynamics of token clusters. Our analysis demonstrates
that while the clusters within a modified attention matrix dynamics can exhibit
significant divergence from the original over extended periods, they maintain
close similarities over shorter intervals, depending on the parameter
differences. This work contributes to the fine-tuning field through practical
applications to the LoRA algorithm \cite{hu2021lora,peft}, enhancing our
understanding of the behavior of LoRA-enhanced Transformer models.
\\ ( https://arxiv.org/abs/2402.15415 ,  16763kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15441
Date: Tue, 13 Feb 2024 09:19:05 GMT   (1252kb,D)

Title: Active Few-Shot Fine-Tuning
Authors: Jonas H\"ubotter and Bhavya Sukhija and Lenart Treven and Yarden As
  and Andreas Krause
Categories: cs.LG cs.AI
\\
  We study the active few-shot fine-tuning of large neural networks to
downstream tasks. We show that few-shot fine-tuning is an instance of a
generalization of classical active learning, transductive active learning, and
we propose ITL, short for information-based transductive learning, an approach
which samples adaptively to maximize the information gained about specified
downstream tasks. Under general regularity assumptions, we prove that ITL
converges uniformly to the smallest possible uncertainty obtainable from the
accessible data. To the best of our knowledge, we are the first to derive
generalization bounds of this kind, and they may be of independent interest for
active learning. We apply ITL to the few-shot fine-tuning of large neural
networks and show that ITL substantially improves upon the state-of-the-art.
\\ ( https://arxiv.org/abs/2402.15441 ,  1252kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15472
Date: Fri, 23 Feb 2024 18:04:54 GMT   (1203kb,D)

Title: FAIR: Filtering of Automatically Induced Rules
Authors: Divya Jyoti Bajpai, Ayush Maheshwari, Manjesh Kumar Hanawal, Ganesh
  Ramakrishnan
Categories: cs.LG
Comments: Published at EACL 2024
\\
  The availability of large annotated data can be a critical bottleneck in
training machine learning algorithms successfully, especially when applied to
diverse domains. Weak supervision offers a promising alternative by
accelerating the creation of labeled training data using domain-specific rules.
However, it requires users to write a diverse set of high-quality rules to
assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches
circumvent this problem by automatically creating rules from features on a
small labeled set and filtering a final set of rules from them. In the ARI
approach, the crucial step is to filter out a set of a high-quality useful
subset of rules from the large set of automatically created rules. In this
paper, we propose an algorithm (Filtering of Automatically Induced Rules) to
filter rules from a large number of automatically induced rules using
submodular objective functions that account for the collective precision,
coverage, and conflicts of the rule set. We experiment with three ARI
approaches and five text classification datasets to validate the superior
performance of our algorithm with respect to several semi-supervised label
aggregation approaches. Further, we show that achieves statistically
significant results in comparison to existing rule-filtering approaches.
\\ ( https://arxiv.org/abs/2402.15472 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15477
Date: Fri, 23 Feb 2024 18:11:32 GMT   (9736kb,D)

Title: Debiasing Machine Learning Models by Using Weakly Supervised Learning
Authors: Renan D. B. Brotto, Jean-Michel Loubes, Laurent Risser, Jean-Pierre
  Florens, Kenji Nose-Filho and Jo\~ao M. T. Romano
Categories: cs.LG cs.CY
Comments: 30 pages, 25 figures
MSC-class: 68T05
\\
  We tackle the problem of bias mitigation of algorithmic decisions in a
setting where both the output of the algorithm and the sensitive variable are
continuous. Most of prior work deals with discrete sensitive variables, meaning
that the biases are measured for subgroups of persons defined by a label,
leaving out important algorithmic bias cases, where the sensitive variable is
continuous. Typical examples are unfair decisions made with respect to the age
or the financial status. In our work, we then propose a bias mitigation
strategy for continuous sensitive variables, based on the notion of endogeneity
which comes from the field of econometrics. In addition to solve this new
problem, our bias mitigation strategy is a weakly supervised learning method
which requires that a small portion of the data can be measured in a fair
manner. It is model agnostic, in the sense that it does not make any hypothesis
on the prediction model. It also makes use of a reasonably large amount of
input observations and their corresponding predictions. Only a small fraction
of the true output predictions should be known. This therefore limits the need
for expert interventions. Results obtained on synthetic data show the
effectiveness of our approach for examples as close as possible to real-life
applications in econometrics.
\\ ( https://arxiv.org/abs/2402.15477 ,  9736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15478
Date: Fri, 23 Feb 2024 18:12:53 GMT   (517kb,D)

Title: Transformers are Expressive, But Are They Expressive Enough for
  Regression?
Authors: Swaroop Nath, Harshad Khadilkar, Pushpak Bhattacharyya
Categories: cs.LG stat.ML
Comments: 12 pages, 8 figures, 6 tables
\\
  Transformers have become pivotal in Natural Language Processing,
demonstrating remarkable success in applications like Machine Translation and
Summarization. Given their widespread adoption, several works have attempted to
analyze the expressivity of Transformers. Expressivity of a neural network is
the class of functions it can approximate. A neural network is fully expressive
if it can act as a universal function approximator. We attempt to analyze the
same for Transformers. Contrary to existing claims, our findings reveal that
Transformers struggle to reliably approximate continuous functions, relying on
piecewise constant approximations with sizable intervals. The central question
emerges as: "\textit{Are Transformers truly Universal Function Approximators}?"
To address this, we conduct a thorough investigation, providing theoretical
insights and supporting evidence through experiments. Our contributions include
a theoretical analysis pinpointing the root of Transformers' limitation in
function approximation and extensive experiments to verify the limitation. By
shedding light on these challenges, we advocate a refined understanding of
Transformers' capabilities.
\\ ( https://arxiv.org/abs/2402.15478 ,  517kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15490
Date: Fri, 23 Feb 2024 18:28:57 GMT   (5648kb,D)

Title: A Comprehensive Survey of Convolutions in Deep Learning: Applications,
  Challenges, and Future Trends
Authors: Abolfazl Younesi, Mohsen Ansari, MohammadAmin Fazli, Alireza Ejlali,
  Muhammad Shafique, J\"org Henkel
Categories: cs.LG cs.NE
\\
  In today's digital age, Convolutional Neural Networks (CNNs), a subset of
Deep Learning (DL), are widely used for various computer vision tasks such as
image classification, object detection, and image segmentation. There are
numerous types of CNNs designed to meet specific needs and requirements,
including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention,
depthwise convolutions, and NAS, among others. Each type of CNN has its unique
structure and characteristics, making it suitable for specific tasks. It's
crucial to gain a thorough understanding and perform a comparative analysis of
these different CNN types to understand their strengths and weaknesses.
Furthermore, studying the performance, limitations, and practical applications
of each type of CNN can aid in the development of new and improved
architectures in the future. We also dive into the platforms and frameworks
that researchers utilize for their research or development from various
perspectives. Additionally, we explore the main research fields of CNN like 6D
vision, generative models, and meta-learning. This survey paper provides a
comprehensive examination and comparison of various CNN architectures,
highlighting their architectural differences and emphasizing their respective
advantages, disadvantages, applications, challenges, and future trends.
\\ ( https://arxiv.org/abs/2402.15490 ,  5648kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15492
Date: Fri, 23 Feb 2024 18:31:02 GMT   (1149kb,D)

Title: Mechanics-Informed Autoencoder Enables Automated Detection and
  Localization of Unforeseen Structural Damage
Authors: Xuyang Li, Hamed Bolandi, Mahdi Masmoudi, Talal Salem, Nizar Lajnef,
  Vishnu Naresh Boddeti
Categories: cs.LG eess.SP
\\
  Structural health monitoring (SHM) is vital for ensuring the safety and
longevity of structures like buildings and bridges. As the volume and scale of
structures and the impact of their failure continue to grow, there is a dire
need for SHM techniques that are scalable, inexpensive, operate passively
without human intervention, and customized for each mechanical structure
without the need for complex baseline models. We present a novel
"deploy-and-forget" approach for automated detection and localization of
damages in structures. It is based on a synergistic combination of fully
passive measurements from inexpensive sensors and a mechanics-informed
autoencoder. Once deployed, our solution continuously learns and adapts a
bespoke baseline model for each structure, learning from its undamaged state's
response characteristics. After learning from just 3 hours of data, it can
autonomously detect and localize different types of unforeseen damage. Results
from numerical simulations and experiments indicate that incorporating the
mechanical characteristics into the variational autoencoder allows for up to
35\% earlier detection and localization of damage over a standard autoencoder.
Our approach holds substantial promise for a significant reduction in human
intervention and inspection costs and enables proactive and preventive
maintenance strategies, thus extending the lifespan, reliability, and
sustainability of civil infrastructures.
\\ ( https://arxiv.org/abs/2402.15492 ,  1149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15505
Date: Fri, 23 Feb 2024 18:56:11 GMT   (1854kb,D)

Title: Co-Supervised Learning: Improving Weak-to-Strong Generalization with
  Hierarchical Mixture of Experts
Authors: Yuejiang Liu, Alexandre Alahi
Categories: cs.LG cs.AI cs.CV
Comments: Preprint
\\
  Steering the behavior of a strong model pre-trained on internet-scale data
can be difficult due to the scarcity of competent supervisors. Recent studies
reveal that, despite supervisory noises, a strong student model may surpass its
weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of
such weak-to-strong generalization remains limited, especially in the presence
of large capability gaps. In this paper, we propose to address this challenge
by harnessing a diverse set of specialized teachers, instead of a single
generalist one, that collectively supervises the strong student. Our approach
resembles the classical hierarchical mixture of experts, with two components
tailored for co-supervision: (i) we progressively alternate student training
and teacher assignment, leveraging the growth of the strong student to identify
plausible supervisions; (ii) we conservatively enforce teacher-student and
local-global consistency, leveraging their dependencies to reject potential
annotation noises. We validate the proposed method through visual recognition
tasks on the OpenAI weak-to-strong benchmark and additional multi-domain
datasets. Our code is available at \url{https://github.com/yuejiangliu/csl}.
\\ ( https://arxiv.org/abs/2402.15505 ,  1854kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.14827 (*cross-listing*)
Date: Sat, 10 Feb 2024 17:59:12 GMT   (1820kb)

Title: Optimizing Uterine Synchronization Analysis in Pregnancy and Labor
  through Window Selection and Node Optimization
Authors: Kamil Bader El Dine, Noujoud Nader, Mohamad Khalil and Catherine
  Marque
Categories: q-bio.QM cs.AI cs.LG eess.SP
Comments: 10 pages, 6 figures
\\
  Preterm labor (PL) has globally become the leading cause of death in children
under the age of 5 years. To address this problem, this paper will provide a
new approach by analyzing the EHG signals, which are recorded on the abdomen of
the mother during labor and pregnancy. The EHG signal reflects the electrical
activity that induces the mechanical contraction of the myometrium. Because
EHGs are known to be non-stationary signals, and because we anticipate
connectivity to alter during contraction, we applied the windowing approach on
real signals to help us identify the best windows and the best nodes with the
most significant data to be used for classification. The suggested pipeline
includes i) divide the 16 EHG signals that are recorded from the abdomen of
pregnant women in N windows; ii) apply the connectivity matrices on each
window; iii) apply the Graph theory-based measures on the connectivity matrices
on each window; iv) apply the consensus Matrix on each window in order to
retrieve the best windows and the best nodes. Following that, several neural
network and machine learning methods are applied to the best windows and best
nodes to categorize pregnancy and labor contractions, based on the different
input parameters (connectivity method alone, connectivity method plus graph
parameters, best nodes, all nodes, best windows, all windows). Results showed
that the best nodes are nodes 8, 9, 10, 11, and 12; while the best windows are
2, 4, and 5. The classification results obtained by using only these best nodes
are better than when using the whole nodes. The results are always better when
using the full burst, whatever the chosen nodes. Thus, the windowing approach
proved to be an innovative technique that can improve the differentiation
between labor and pregnancy EHG signals.
\\ ( https://arxiv.org/abs/2402.14827 ,  1820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14847 (*cross-listing*)
Date: Mon, 19 Feb 2024 15:34:09 GMT   (1508kb,D)

Title: Deep learning-driven scheduling algorithm for a single machine problem
  minimizing the total tardiness
Authors: Michal Bou\v{s}ka, P\v{r}emysl \v{S}\r{u}cha, Anton\'in Nov\'ak,
  Zden\v{e}k Hanz\'alek
Categories: math.OC cs.AI cs.LG
Journal-ref: European Journal of Operational Research, Volume 308, Issue 3, 1
  August 2023, Pages 990-1006
DOI: 10.1016/j.ejor.2022.11.034
\\
  In this paper, we investigate the use of the deep learning method for solving
a well-known NP-hard single machine scheduling problem with the objective of
minimizing the total tardiness. We propose a deep neural network that acts as a
polynomial-time estimator of the criterion value used in a single-pass
scheduling algorithm based on Lawler's decomposition and symmetric
decomposition proposed by Della Croce et al. Essentially, the neural network
guides the algorithm by estimating the best splitting of the problem into
subproblems. The paper also describes a new method for generating the training
data set, which speeds up the training dataset generation and reduces the
average optimality gap of solutions. The experimental results show that our
machine learning-driven approach can efficiently generalize information from
the training phase to significantly larger instances. Even though the instances
used in the training phase have from 75 to 100 jobs, the average optimality gap
on instances with up to 800 jobs is 0.26%, which is almost five times less than
the gap of the state-of-the-art heuristic.
\\ ( https://arxiv.org/abs/2402.14847 ,  1508kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14859 (*cross-listing*)
Date: Tue, 20 Feb 2024 23:08:21 GMT   (4892kb,D)

Title: The Wolf Within: Covert Injection of Malice into MLLM Societies via an
  MLLM Operative
Authors: Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong
  Chen, Huan Liu
Categories: cs.CR cs.AI cs.CY cs.LG
\\
  Due to their unprecedented ability to process and respond to various types of
data, Multimodal Large Language Models (MLLMs) are constantly defining the new
boundary of Artificial General Intelligence (AGI). As these advanced generative
models increasingly form collaborative networks for complex tasks, the
integrity and security of these systems are crucial. Our paper, ``The Wolf
Within'', explores a novel vulnerability in MLLM societies - the indirect
propagation of malicious content. Unlike direct harmful output generation for
MLLMs, our research demonstrates how a single MLLM agent can be subtly
influenced to generate prompts that, in turn, induce other MLLM agents in the
society to output malicious content. This subtle, yet potent method of indirect
influence marks a significant escalation in the security risks associated with
MLLMs. Our findings reveal that, with minimal or even no access to MLLMs'
parameters, an MLLM agent, when manipulated to produce specific prompts or
instructions, can effectively ``infect'' other agents within a society of
MLLMs. This infection leads to the generation and circulation of harmful
outputs, such as dangerous instructions or misinformation, across the society.
We also show the transferability of these indirectly generated prompts,
highlighting their possibility in propagating malice through inter-agent
communication. This research provides a critical insight into a new dimension
of threat posed by MLLMs, where a single agent can act as a catalyst for
widespread malevolent influence. Our work underscores the urgent need for
developing robust mechanisms to detect and mitigate such covert manipulations
within MLLM societies, ensuring their safe and ethical utilization in societal
applications. Our implementation is released at
\url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.
\\ ( https://arxiv.org/abs/2402.14859 ,  4892kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14883 (*cross-listing*)
Date: Thu, 22 Feb 2024 04:55:14 GMT   (1510kb,D)

Title: Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning
Authors: Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li
Categories: cs.CR cs.AI cs.LG
\\
  To support various applications, business owners often seek the customized
models that are obtained by fine-tuning a pre-trained LLM through the API
provided by LLM owners or cloud servers. However, this process carries a
substantial risk of model misuse, potentially resulting in severe economic
consequences for business owners. Thus, safeguarding the copyright of these
customized models during LLM fine-tuning has become an urgent practical
requirement, but there are limited existing solutions to provide such
protection. To tackle this pressing issue, we propose a novel watermarking
approach named "Double-I watermark". Specifically, based on the instruct-tuning
data, two types of backdoor data paradigms are introduced with trigger in the
instruction and the input, respectively. By leveraging LLM's learning
capability to incorporate customized backdoor samples into the dataset, the
proposed approach effectively injects specific watermarking information into
the customized model during fine-tuning, which makes it easy to inject and
verify watermarks in commercial scenarios. We evaluate the proposed "Double-I
watermark" under various fine-tuning methods, demonstrating its harmlessness,
robustness, uniqueness, imperceptibility, and validity through both theoretical
analysis and experimental verification.
\\ ( https://arxiv.org/abs/2402.14883 ,  1510kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14894 (*cross-listing*)
Date: Thu, 22 Feb 2024 16:25:32 GMT   (407kb,D)

Title: Data-Driven Ground-Fault Location Method in Distribution Power System
  With Distributed Generation
Authors: Mauro Caporuscio, Antoine Dupuis, and Welf L\"owe
Categories: eess.SY cs.AI cs.LG cs.SY eess.SP
Comments: Technical Report
\\
  The recent increase in renewable energy penetration at the distribution level
introduces a multi-directional power flow that outdated traditional fault
location techniques. To this extent, the development of new methods is needed
to ensure fast and accurate fault localization and, hence, strengthen power
system reliability. This paper proposes a data-driven ground fault location
method for the power distribution system. An 11-bus 20 kV power system is
modeled in Matlab/Simulink to simulate ground faults. The faults are generated
at different locations and under various system operational states. Time-domain
faulted three-phase voltages at the system substation are then analyzed with
discrete wavelet transform. Statistical quantities of the processed data are
eventually used to train an Artificial Neural Network (ANN) to find a mapping
between computed voltage features and faults. Specifically, three ANNs allow
the prediction of faulted phase, faulted branch, and fault distance from the
system substation separately. According to the results, the method shows good
potential, with a total relative error of 0,4% for fault distance prediction.
The method is applied to datasets with unknown system states to test
robustness.
\\ ( https://arxiv.org/abs/2402.14894 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14899 (*cross-listing*)
Date: Thu, 22 Feb 2024 17:36:34 GMT   (529kb,D)

Title: Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning
  Meets Adversarial Images
Authors: Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao,
  Volker Tresp, Philip Torr, Jindong Gu
Categories: cs.CV cs.AI cs.CR cs.LG
\\
  Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand
images. However, like traditional vision models, they are still vulnerable to
adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely
explored on MLLMs, which not only improves model's performance, but also
enhances model's explainability by giving intermediate reasoning steps.
Nevertheless, there is still a lack of study regarding MLLMs' adversarial
robustness with CoT and an understanding of what the rationale looks like when
MLLMs infer wrong answers with adversarial images. Our research evaluates the
adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT
marginally improves adversarial robustness against existing attack methods.
Moreover, we introduce a novel stop-reasoning attack technique that effectively
bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the
alterations in CoT reasoning when MLLMs confront adversarial images, shedding
light on their reasoning process under adversarial attacks.
\\ ( https://arxiv.org/abs/2402.14899 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14904 (*cross-listing*)
Date: Thu, 22 Feb 2024 18:55:22 GMT   (395kb,D)

Title: Watermarking Makes Language Models Radioactive
Authors: Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy
  Furon
Categories: cs.CR cs.AI cs.CL cs.LG
\\
  This paper investigates the radioactivity of LLM-generated texts, i.e.
whether it is possible to detect that such input was used as training data.
Conventional methods like membership inference can carry out this detection
with some level of accuracy. We show that watermarked training data leaves
traces easier to detect and much more reliable than membership inference. We
link the contamination level to the watermark robustness, its proportion in the
training set, and the fine-tuning process. We notably demonstrate that training
on watermarked synthetic instructions can be detected with high confidence
(p-value < 1e-5) even when as little as 5% of training text is watermarked.
Thus, LLM watermarking, originally designed for detecting machine-generated
text, gives the ability to easily identify if the outputs of a watermarked LLM
were used to fine-tune another LLM.
\\ ( https://arxiv.org/abs/2402.14904 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14928 (*cross-listing*)
Date: Thu, 22 Feb 2024 19:24:56 GMT   (37193kb,D)

Title: Learning Inverse Kinodynamics for Autonomous Vehicle Drifting
Authors: M. Suvarna, O. Tehrani
Categories: cs.RO cs.AI cs.LG
\\
  In this work, we explore a data-driven learning-based approach to learning
the kinodynamic model of a small autonomous vehicle, and observe the effect it
has on motion planning, specifically autonomous drifting. When executing a
motion plan in the real world, there are numerous causes for error, and what is
planned is often not what is executed on the actual car. Learning a kinodynamic
planner based off of inertial measurements and executed commands can help us
learn the world state. In our case, we look towards the realm of drifting; it
is a complex maneuver that requires a smooth enough surface, high enough speed,
and a drastic change in velocity. We attempt to learn the kinodynamic model for
these drifting maneuvers, and attempt to tighten the slip of the car. Our
approach is able to learn a kinodynamic model for high-speed circular
navigation, and is able to avoid obstacles on an autonomous drift at high speed
by correcting an executed curvature for loose drifts. We seek to adjust our
kinodynamic model for success in tighter drifts in future work.
\\ ( https://arxiv.org/abs/2402.14928 ,  37193kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14933 (*cross-listing*)
Date: Thu, 22 Feb 2024 19:34:56 GMT   (903kb,D)

Title: Path Planning based on 2D Object Bounding-box
Authors: Yanliang Huang, Liguo Zhou, Chang Liu, Alois Knoll
Categories: cs.RO cs.AI
\\
  The implementation of Autonomous Driving (AD) technologies within urban
environments presents significant challenges. These challenges necessitate the
development of advanced perception systems and motion planning algorithms
capable of managing situations of considerable complexity. Although the
end-to-end AD method utilizing LiDAR sensors has achieved significant success
in this scenario, we argue that its drawbacks may hinder its practical
application. Instead, we propose the vision-centric AD as a promising
alternative offering a streamlined model without compromising performance. In
this study, we present a path planning method that utilizes 2D bounding boxes
of objects, developed through imitation learning in urban driving scenarios.
This is achieved by integrating high-definition (HD) map data with images
captured by surrounding cameras. Subsequent perception tasks involve
bounding-box detection and tracking, while the planning phase employs both
local embeddings via Graph Neural Network (GNN) and global embeddings via
Transformer for temporal-spatial feature aggregation, ultimately producing
optimal path planning information. We evaluated our model on the nuPlan
planning task and observed that it performs competitively in comparison to
existing vision-centric methods.
\\ ( https://arxiv.org/abs/2402.14933 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14974 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:22:21 GMT   (18939kb,D)

Title: Towards Spatially-Lucid AI Classification in Non-Euclidean Space: An
  Application for MxIF Oncology Data
Authors: Majid Farhadloo, Arun Sharma, Jayant Gupta, Alexey Leontovich,
  Svetomir N. Markovic and Shashi Shekhar
Categories: eess.IV cs.AI cs.LG
Comments: SIAM International Conference on Data Mining (SDM24)
\\
  Given multi-category point sets from different place-types, our goal is to
develop a spatially-lucid classifier that can distinguish between two classes
based on the arrangements of their points. This problem is important for many
applications, such as oncology, for analyzing immune-tumor relationships and
designing new immunotherapies. It is challenging due to spatial variability and
interpretability needs. Previously proposed techniques require dense training
data or have limited ability to handle significant spatial variability within a
single place-type. Most importantly, these deep neural network (DNN) approaches
are not designed to work in non-Euclidean space, particularly point sets.
Existing non-Euclidean DNN methods are limited to one-size-fits-all approaches.
We explore a spatial ensemble framework that explicitly uses different training
strategies, including weighted-distance learning rate and spatial domain
adaptation, on various place-types for spatially-lucid classification.
Experimental results on real-world datasets (e.g., MxIF oncology data) show
that the proposed framework provides higher prediction accuracy than baseline
methods.
\\ ( https://arxiv.org/abs/2402.14974 ,  18939kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14976 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:25:20 GMT   (3740kb,D)

Title: Unsupervised Domain Adaptation within Deep Foundation Latent Spaces
Authors: Dmitry Kangin, Plamen Angelov
Categories: cs.CV cs.AI cs.LG
\\
  The vision transformer-based foundation models, such as ViT or Dino-V2, are
aimed at solving problems with little or no finetuning of features. Using a
setting of prototypical networks, we analyse to what extent such foundation
models can solve unsupervised domain adaptation without finetuning over the
source or target domain. Through quantitative analysis, as well as qualitative
interpretations of decision making, we demonstrate that the suggested method
can improve upon existing baselines, as well as showcase the limitations of
such approach yet to be solved.
\\ ( https://arxiv.org/abs/2402.14976 ,  3740kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14978 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:34:52 GMT   (9613kb,D)

Title: AI-Augmented Brainwriting: Investigating the use of LLMs in group
  ideation
Authors: Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L. Kun, Hagit Ben
  Shoshan
Categories: cs.HC cs.AI cs.CY
Comments: 27 pages
ACM-class: H.5.2; J.4
\\
  The growing availability of generative AI technologies such as large language
models (LLMs) has significant implications for creative work. This paper
explores twofold aspects of integrating LLMs into the creative process - the
divergence stage of idea generation, and the convergence stage of evaluation
and selection of ideas. We devised a collaborative group-AI Brainwriting
ideation framework, which incorporated an LLM as an enhancement into the group
ideation process, and evaluated the idea generation process and the resulted
solution space. To assess the potential of using LLMs in the idea evaluation
process, we design an evaluation engine and compared it to idea ratings
assigned by three expert and six novice evaluators. Our findings suggest that
integrating LLM in Brainwriting could enhance both the ideation process and its
outcome. We also provide evidence that LLMs can support idea evaluation. We
conclude by discussing implications for HCI education and practice.
\\ ( https://arxiv.org/abs/2402.14978 ,  9613kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15011 (*cross-listing*)
Date: Thu, 22 Feb 2024 23:11:12 GMT   (3940kb,D)

Title: A Conversational Brain-Artificial Intelligence Interface
Authors: Anja Meunier, Michal Robert \v{Z}\'ak, Lucas Munz, Sofiya Garkot,
  Manuel Eder, Jiachen Xu, Moritz Grosse-Wentrup
Categories: cs.HC cs.AI eess.SP
Comments: 16 pages (39 with supplementary meterial), 6 figures
\\
  We introduce Brain-Artificial Intelligence Interfaces (BAIs) as a new class
of Brain-Computer Interfaces (BCIs). Unlike conventional BCIs, which rely on
intact cognitive capabilities, BAIs leverage the power of artificial
intelligence to replace parts of the neuro-cognitive processing pipeline. BAIs
allow users to accomplish complex tasks by providing high-level intentions,
while a pre-trained AI agent determines low-level details. This approach
enlarges the target audience of BCIs to individuals with cognitive impairments,
a population often excluded from the benefits of conventional BCIs. We present
the general concept of BAIs and illustrate the potential of this new approach
with a Conversational BAI based on EEG. In particular, we show in an experiment
with simulated phone conversations that the Conversational BAI enables complex
communication without the need to generate language. Our work thus
demonstrates, for the first time, the ability of a speech neuroprosthesis to
enable fluent communication in realistic scenarios with non-invasive
technologies.
\\ ( https://arxiv.org/abs/2402.15011 ,  3940kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15027 (*cross-listing*)
Date: Thu, 22 Feb 2024 23:59:59 GMT   (726kb)

Title: Multi-stakeholder Perspective on Responsible Artificial Intelligence and
  Acceptability in Education
Authors: A.J. Karran, P. Charland, J-T. Martineau, A. Ortiz de Guinea, AM.
  Lesage, S. Senecal, P-M. Leger
Categories: cs.CY cs.AI
Comments: 28 pages, 2 appendices, 3 figures, 5 tables, original research
ACM-class: K.3.1; I.2.0
\\
  This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.
\\ ( https://arxiv.org/abs/2402.15027 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15038 (*cross-listing*)
Date: Fri, 23 Feb 2024 01:19:30 GMT   (18946kb,D)

Title: Dynamics-Guided Diffusion Model for Robot Manipulator Design
Authors: Xiaomeng Xu, Huy Ha, Shuran Song
Categories: cs.RO cs.AI cs.LG
\\
  We present Dynamics-Guided Diffusion Model, a data-driven framework for
generating manipulator geometry designs for a given manipulation task. Instead
of training different design models for each task, our approach employs a
learned dynamics network shared across tasks. For a new manipulation task, we
first decompose it into a collection of individual motion targets which we call
target interaction profile, where each individual motion can be modeled by the
shared dynamics network. The design objective constructed from the target and
predicted interaction profiles provides a gradient to guide the refinement of
finger geometry for the task. This refinement process is executed as a
classifier-guided diffusion process, where the design objective acts as the
classifier guidance. We evaluate our framework on various manipulation tasks,
under the sensor-less setting using only an open-loop parallel jaw motion. Our
generated designs outperform optimization-based and unguided diffusion
baselines relatively by 31.5% and 45.3% on average manipulation success rate.
With the ability to generate a design within 0.8 seconds, our framework could
facilitate rapid design iteration and enhance the adoption of data-driven
approaches for robotic mechanism design.
\\ ( https://arxiv.org/abs/2402.15038 ,  18946kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15083 (*cross-listing*)
Date: Fri, 23 Feb 2024 04:02:23 GMT   (1350kb)

Title: Hands-Free VR
Authors: Jorge Askur Vazquez Fernandez, Jae Joong Lee, Santiago Andr\'es
  Serrano Vacca, Alejandra Magana, Bedrich Benes, Voicu Popescu
Categories: cs.HC cs.AI cs.CL
\\
  The paper introduces Hands-Free VR, a voice-based natural-language interface
for VR. The user gives a command using their voice, the speech audio data is
converted to text using a speech-to-text deep learning model that is fine-tuned
for robustness to word phonetic similarity and to spoken English accents, and
the text is mapped to an executable VR command using a large language model
that is robust to natural language diversity. Hands-Free VR was evaluated in a
controlled within-subjects study (N = 22) that asked participants to find
specific objects and to place them in various configurations. In the control
condition participants used a conventional VR user interface to grab, carry,
and position the objects using the handheld controllers. In the experimental
condition participants used Hands-Free VR. The results confirm that: (1)
Hands-Free VR is robust to spoken English accents, as for 20 of our
participants English was not their first language, and to word phonetic
similarity, correctly transcribing the voice command 96.71% of the time; (2)
Hands-Free VR is robust to natural language diversity, correctly mapping the
transcribed command to an executable command in 97.83% of the time; (3)
Hands-Free VR had a significant efficiency advantage over the conventional VR
interface in terms of task completion time, total viewpoint translation, total
view direction rotation, and total left and right hand translations; (4)
Hands-Free VR received high user preference ratings in terms of ease of use,
intuitiveness, ergonomics, reliability, and desirability.
\\ ( https://arxiv.org/abs/2402.15083 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15116 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:04:23 GMT   (963kb,D)

Title: Large Multimodal Agents: A Survey
Authors: Junlin Xie and Zhihong Chen and Ruifei Zhang and Xiang Wan and Guanbin
  Li
Categories: cs.CV cs.AI cs.CL
Comments: 15 pages, 4 figures
\\
  Large language models (LLMs) have achieved superior performance in powering
text-based AI agents, endowing them with decision-making and reasoning
abilities akin to humans. Concurrently, there is an emerging research trend
focused on extending these LLM-powered AI agents into the multimodal domain.
This extension enables AI agents to interpret and respond to diverse multimodal
user queries, thereby handling more intricate and nuanced tasks. In this paper,
we conduct a systematic review of LLM-driven multimodal agents, which we refer
to as large multimodal agents ( LMAs for short). First, we introduce the
essential components involved in developing LMAs and categorize the current
body of research into four distinct types. Subsequently, we review the
collaborative frameworks integrating multiple LMAs , enhancing collective
efficacy. One of the critical challenges in this field is the diverse
evaluation methods used across existing studies, hindering effective comparison
among different LMAs . Therefore, we compile these evaluation methodologies and
establish a comprehensive framework to bridge the gaps. This framework aims to
standardize evaluations, facilitating more meaningful comparisons. Concluding
our review, we highlight the extensive applications of LMAs and propose
possible future research directions. Our discussion aims to provide valuable
insights and guidelines for future research in this rapidly evolving field. An
up-to-date resource list is available at
https://github.com/jun0wanan/awesome-large-multimodal-agents.
\\ ( https://arxiv.org/abs/2402.15116 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15120 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:11:50 GMT   (9701kb,D)

Title: Fine-tuning CLIP Text Encoders with Two-step Paraphrasing
Authors: Hyunjae Kim, Seunghyun Yoon, Trung Bui, Handong Zhao, Quan Tran,
  Franck Dernoncourt, Jaewoo Kang
Categories: cs.CV cs.AI cs.LG
Comments: EACL 2024 (Findings of the ACL)
\\
  Contrastive language-image pre-training (CLIP) models have demonstrated
considerable success across various vision-language tasks, such as
text-to-image retrieval, where the model is required to effectively process
natural language input to produce an accurate visual output. However, current
models still face limitations in dealing with linguistic variations in input
queries, such as paraphrases, making it challenging to handle a broad range of
user queries in real-world applications. In this study, we introduce a
straightforward fine-tuning approach to enhance the representations of CLIP
models for paraphrases. Our approach involves a two-step paraphrase generation
process, where we automatically create two categories of paraphrases from
web-scale image captions by leveraging large language models. Subsequently, we
fine-tune the CLIP text encoder using these generated paraphrases while
freezing the image encoder. Our resulting model, which we call ParaCLIP,
exhibits significant improvements over baseline CLIP models across various
tasks, including paraphrased retrieval (with rank similarity scores improved by
up to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven
semantic textual similarity tasks.
\\ ( https://arxiv.org/abs/2402.15120 ,  9701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15135 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:42:58 GMT   (10759kb,D)

Title: Modified CycleGAN for the synthesization of samples for wheat head
  segmentation
Authors: Jaden Myers, Keyhan Najafian, Farhad Maleki, and Katie Ovens
Categories: cs.CV cs.AI
\\
  Deep learning models have been used for a variety of image processing tasks.
However, most of these models are developed through supervised learning
approaches, which rely heavily on the availability of large-scale annotated
datasets. Developing such datasets is tedious and expensive. In the absence of
an annotated dataset, synthetic data can be used for model development;
however, due to the substantial differences between simulated and real data, a
phenomenon referred to as domain gap, the resulting models often underperform
when applied to real data. In this research, we aim to address this challenge
by first computationally simulating a large-scale annotated dataset and then
using a generative adversarial network (GAN) to fill the gap between simulated
and real images. This approach results in a synthetic dataset that can be
effectively utilized to train a deep-learning model. Using this approach, we
developed a realistic annotated synthetic dataset for wheat head segmentation.
This dataset was then used to develop a deep-learning model for semantic
segmentation. The resulting model achieved a Dice score of 83.4\% on an
internal dataset and Dice scores of 79.6% and 83.6% on two external Global
Wheat Head Detection datasets. While we proposed this approach in the context
of wheat head segmentation, it can be generalized to other crop types or, more
broadly, to images with dense, repeated patterns such as those found in
cellular imagery.
\\ ( https://arxiv.org/abs/2402.15135 ,  10759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15195 (*cross-listing*)
Date: Fri, 23 Feb 2024 08:55:47 GMT   (5767kb,D)

Title: The AffectToolbox: Affect Analysis for Everyone
Authors: Silvan Mertes, Dominik Schiller, Michael Dietz, Elisabeth Andr\'e,
  Florian Lingenfelser
Categories: cs.HC cs.AI cs.LG
\\
  In the field of affective computing, where research continually advances at a
rapid pace, the demand for user-friendly tools has become increasingly
apparent. In this paper, we present the AffectToolbox, a novel software system
that aims to support researchers in developing affect-sensitive studies and
prototypes. The proposed system addresses the challenges posed by existing
frameworks, which often require profound programming knowledge and cater
primarily to power-users or skilled developers. Aiming to facilitate ease of
use, the AffectToolbox requires no programming knowledge and offers its
functionality to reliably analyze the affective state of users through an
accessible graphical user interface. The architecture encompasses a variety of
models for emotion recognition on multiple affective channels and modalities,
as well as an elaborate fusion system to merge multi-modal assessments into a
unified result. The entire system is open-sourced and will be publicly
available to ensure easy integration into more complex applications through a
well-structured, Python-based code base - therefore marking a substantial
contribution toward advancing affective computing research and fostering a more
collaborative and inclusive environment within this interdisciplinary field.
\\ ( https://arxiv.org/abs/2402.15195 ,  5767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15197 (*cross-listing*)
Date: Fri, 23 Feb 2024 08:58:38 GMT   (1548kb,D)

Title: Safety Optimized Reinforcement Learning via Multi-Objective Policy
  Optimization
Authors: Homayoun Honari, Mehran Ghafarian Tamizi, Homayoun Najjaran
Categories: eess.SY cs.AI cs.LG cs.RO cs.SY
Comments: Accepted to the IEEE International Conference on Robotics and
  Automation (ICRA) 2024, 7 Pages, 3 Figures
\\
  Safe reinforcement learning (Safe RL) refers to a class of techniques that
aim to prevent RL algorithms from violating constraints in the process of
decision-making and exploration during trial and error. In this paper, a novel
model-free Safe RL algorithm, formulated based on the multi-objective policy
optimization framework is introduced where the policy is optimized towards
optimality and safety, simultaneously. The optimality is achieved by the
environment reward function that is subsequently shaped using a safety critic.
The advantage of the Safety Optimized RL (SORL) algorithm compared to the
traditional Safe RL algorithms is that it omits the need to constrain the
policy search space. This allows SORL to find a natural tradeoff between safety
and optimality without compromising the performance in terms of either safety
or optimality due to strict search space constraints. Through our theoretical
analysis of SORL, we propose a condition for SORL's converged policy to
guarantee safety and then use it to introduce an aggressiveness parameter that
allows for fine-tuning the mentioned tradeoff. The experimental results
obtained in seven different robotic environments indicate a considerable
reduction in the number of safety violations along with higher, or competitive,
policy returns, in comparison to six different state-of-the-art Safe RL
methods. The results demonstrate the significant superiority of the proposed
SORL algorithm in safety-critical applications.
\\ ( https://arxiv.org/abs/2402.15197 ,  1548kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15205 (*cross-listing*)
Date: Fri, 23 Feb 2024 09:06:25 GMT   (518kb)

Title: Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary
  Writing
Authors: Samuel Kernan Freire, Margo MC van Mol, Carola Schol, Elif \"Ozcan
  Vieira
Categories: cs.HC cs.AI
Comments: 3 pages, under review
\\
  Intensive care unit (ICU) patients often develop new health-related problems
in their long-term recovery. Health care professionals keeping a diary of a
patient's stay is a proven strategy to tackle this but faces several adoption
barriers, such as lack of time and difficulty in knowing what to write. Large
language models (LLMs), with their ability to generate human-like text and
adaptability, could solve these challenges. However, realizing this vision
involves addressing several socio-technical and practical research challenges.
This paper discusses these challenges and proposes future research directions
to utilize the potential of LLMs in ICU diary writing, ultimately improving the
long-term recovery outcomes for ICU patients.
\\ ( https://arxiv.org/abs/2402.15205 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15246 (*cross-listing*)
Date: Fri, 23 Feb 2024 10:21:03 GMT   (961kb)

Title: Artificial Bee Colony optimization of Deep Convolutional Neural Networks
  in the context of Biomedical Imaging
Authors: Adri Gomez Martin, Carlos Fernandez del Cerro, Monica Abella Garcia
  and Manuel Desco Menendez
Categories: eess.IV cs.AI cs.NE
\\
  Most efforts in Computer Vision focus on natural images or artwork, which
differ significantly both in size and contents from the kind of data biomedical
image processing deals with. Thus, Transfer Learning models often prove
themselves suboptimal for these tasks, even after manual finetuning. The
development of architectures from scratch is oftentimes unfeasible due to the
vastness of the hyperparameter space and a shortage of time, computational
resources and Deep Learning experts in most biomedical research laboratories.
An alternative to manually defining the models is the use of Neuroevolution,
which employs metaheuristic techniques to optimize Deep Learning architectures.
However, many algorithms proposed in the neuroevolutive literature are either
too unreliable or limited to a small, predefined region of the hyperparameter
space. To overcome these shortcomings, we propose the Chimera Algorithm, a
novel, hybrid neuroevolutive algorithm that integrates the Artificial Bee
Colony Algorithm with Evolutionary Computation tools to generate models from
scratch, as well as to refine a given previous architecture to better fit the
task at hand. The Chimera Algorithm has been validated with two datasets of
natural and medical images, producing models that surpassed the performance of
those coming from Transfer Learning.
\\ ( https://arxiv.org/abs/2402.15246 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15267 (*cross-listing*)
Date: Fri, 23 Feb 2024 11:30:12 GMT   (3228kb,D)

Title: Adversarial Robustness of Deep Learning-based Malware Detectors via
  (De)Randomized Smoothing
Authors: Daniel Gibert, Giulio Zizzo, Quan Le, Jordi Planes
Categories: cs.CR cs.AI
\\
  Deep learning-based malware detectors have been shown to be susceptible to
adversarial malware examples, i.e. malware examples that have been deliberately
manipulated in order to avoid detection. In light of the vulnerability of deep
learning detectors to subtle input file modifications, we propose a practical
defense against adversarial malware examples inspired by (de)randomized
smoothing. In this work, we reduce the chances of sampling adversarial content
injected by malware authors by selecting correlated subsets of bytes, rather
than using Gaussian noise to randomize inputs like in the Computer Vision (CV)
domain. During training, our ablation-based smoothing scheme trains a base
classifier to make classifications on a subset of contiguous bytes or chunk of
bytes. At test time, a large number of chunks are then classified by a base
classifier and the consensus among these classifications is then reported as
the final prediction. We propose two strategies to determine the location of
the chunks used for classification: (1) randomly selecting the locations of the
chunks and (2) selecting contiguous adjacent chunks. To showcase the
effectiveness of our approach, we have trained two classifiers with our
chunk-based ablation schemes on the BODMAS dataset. Our findings reveal that
the chunk-based smoothing classifiers exhibit greater resilience against
adversarial malware examples generated with state-of-the-are evasion attacks,
outperforming a non-smoothed classifier and a randomized smoothing-based
classifier by a great margin.
\\ ( https://arxiv.org/abs/2402.15267 ,  3228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15272 (*cross-listing*)
Date: Fri, 23 Feb 2024 11:35:48 GMT   (5192kb,D)

Title: EMIFF: Enhanced Multi-scale Image Feature Fusion for
  Vehicle-Infrastructure Cooperative 3D Object Detection
Authors: Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu,
  Yilun Chen, Ya-Qin Zhang
Categories: cs.CV cs.AI
Comments: 7 pages, 8 figures. Accepted by ICRA 2024. arXiv admin note: text
  overlap with arXiv:arXiv:2303.10975
\\
  In autonomous driving, cooperative perception makes use of multi-view cameras
from both vehicles and infrastructure, providing a global vantage point with
rich semantic context of road conditions beyond a single vehicle viewpoint.
Currently, two major challenges persist in vehicle-infrastructure cooperative
3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view
images, caused by time asynchrony across cameras; $2)$ information loss in
transmission process resulted from limited communication bandwidth. To address
these issues, we propose a novel camera-based 3D detection framework for VIC3D
task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit
holistic perspectives from both vehicles and infrastructure, we propose
Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM)
modules to enhance infrastructure and vehicle features at scale, spatial, and
channel levels to correct the pose error introduced by camera asynchrony. We
also introduce a Feature Compression (FC) module with channel and spatial
compression blocks for transmission efficiency. Experiments show that EMIFF
achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous
early-fusion and late-fusion methods with comparable transmission costs.
\\ ( https://arxiv.org/abs/2402.15272 ,  5192kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15276 (*cross-listing*)
Date: Fri, 23 Feb 2024 11:47:16 GMT   (3555kb,D)

Title: Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale
  Libraries
Authors: Zijun Long and Xuri Ge and Richard Mccreadie and Joemon Jose
Categories: cs.IR cs.AI cs.CV
\\
  Text-to-image retrieval plays a crucial role across various applications,
including digital libraries, e-commerce platforms, and multimedia databases, by
enabling the search for images using text queries. Despite the advancements in
Multimodal Large Language Models (MLLMs), which offer leading-edge performance,
their applicability in large-scale, varied, and ambiguous retrieval scenarios
is constrained by significant computational demands and the generation of
injective embeddings. This paper introduces the Text2Pic Swift framework,
tailored for efficient and robust retrieval of images corresponding to
extensive textual descriptions in sizable datasets. The framework employs a
two-tier approach: the initial Entity-based Ranking (ER) stage addresses the
ambiguity inherent in lengthy text queries through a
multiple-queries-to-multiple-targets strategy, effectively narrowing down
potential candidates for subsequent analysis. Following this, the Summary-based
Re-ranking (SR) stage further refines these selections based on concise query
summaries. Additionally, we present a novel Decoupling-BEiT-3 encoder,
specifically designed to tackle the challenges of ambiguous queries and to
facilitate both stages of the retrieval process, thereby significantly
improving computational efficiency via vector-based similarity assessments. Our
evaluation, conducted on the AToMiC dataset, demonstrates that Text2Pic Swift
outperforms current MLLMs by achieving up to an 11.06% increase in Recall@1000,
alongside reductions in training and retrieval durations by 68.75% and 99.79%,
respectively.
\\ ( https://arxiv.org/abs/2402.15276 ,  3555kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15294 (*cross-listing*)
Date: Fri, 23 Feb 2024 12:41:44 GMT   (2342kb,D)

Title: A Survey of Music Generation in the Context of Interaction
Authors: Ismael Agchar, Ilja Baumann, Franziska Braun, Paula Andrea Perez-Toro,
  Korbinian Riedhammer, Sebastian Trump, Martin Ullrich
Categories: cs.SD cs.AI cs.LG eess.AS
\\
  In recent years, machine learning, and in particular generative adversarial
neural networks (GANs) and attention-based neural networks (transformers), have
been successfully used to compose and generate music, both melodies and
polyphonic pieces. Current research focuses foremost on style replication (eg.
generating a Bach-style chorale) or style transfer (eg. classical to jazz)
based on large amounts of recorded or transcribed music, which in turn also
allows for fairly straight-forward "performance" evaluation. However, most of
these models are not suitable for human-machine co-creation through live
interaction, neither is clear, how such models and resulting creations would be
evaluated. This article presents a thorough review of music representation,
feature analysis, heuristic algorithms, statistical and parametric modelling,
and human and automatic evaluation measures, along with a discussion of which
approaches and models seem most suitable for live interaction.
\\ ( https://arxiv.org/abs/2402.15294 ,  2342kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15300 (*cross-listing*)
Date: Fri, 23 Feb 2024 12:57:16 GMT   (2273kb,D)

Title: Seeing is Believing: Mitigating Hallucination in Large Vision-Language
  Models via CLIP-Guided Decoding
Authors: Ailin Deng, Zhirui Chen, Bryan Hooi
Categories: cs.CV cs.AI cs.CL cs.LG cs.MM
\\
  Large Vision-Language Models (LVLMs) are susceptible to object
hallucinations, an issue in which their generated text contains non-existent
objects, greatly limiting their reliability and practicality. Current
approaches often rely on the model's token likelihoods or other internal
information, instruction tuning on additional datasets, or incorporating
complex external tools. We first perform empirical analysis on sentence-level
LVLM hallucination, finding that CLIP similarity to the image acts as a
stronger and more robust indicator of hallucination compared to token
likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD)
approach, a straightforward but effective training-free approach to reduce
object hallucination at decoding time. CGD uses CLIP to guide the model's
decoding process by enhancing visual grounding of generated text with the
image. Experiments demonstrate that CGD effectively mitigates object
hallucination across multiple LVLM families while preserving the utility of
text generation.
\\ ( https://arxiv.org/abs/2402.15300 ,  2273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15307 (*cross-listing*)
Date: Fri, 23 Feb 2024 13:11:10 GMT   (3076kb,D)

Title: Representing Online Handwriting for Recognition in Large Vision-Language
  Models
Authors: Anastasiia Fadeeva, Philippe Schlattner, Andrii Maksai, Mark Collier,
  Efi Kokiopoulou, Jesse Berent, Claudiu Musat
Categories: cs.CV cs.AI cs.LG
\\
  The adoption of tablets with touchscreens and styluses is increasing, and a
key feature is converting handwriting to text, enabling search, indexing, and
AI assistance. Meanwhile, vision-language models (VLMs) are now the go-to
solution for image understanding, thanks to both their state-of-the-art
performance across a variety of tasks and the simplicity of a unified approach
to training, fine-tuning, and inference. While VLMs obtain high performance on
image-based tasks, they perform poorly on handwriting recognition when applied
naively, i.e., by rendering handwriting as an image and performing optical
character recognition (OCR). In this paper, we study online handwriting
recognition with VLMs, going beyond naive OCR. We propose a novel tokenized
representation of digital ink (online handwriting) that includes both a
time-ordered sequence of strokes as text, and as image. We show that this
representation yields results comparable to or better than state-of-the-art
online handwriting recognizers. Wide applicability is shown through results
with two different VLM families, on multiple public datasets. Our approach can
be applied to off-the-shelf VLMs, does not require any changes in their
architecture, and can be used in both fine-tuning and parameter-efficient
tuning. We perform a detailed ablation study to identify the key elements of
the proposed representation.
\\ ( https://arxiv.org/abs/2402.15307 ,  3076kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15321 (*cross-listing*)
Date: Fri, 23 Feb 2024 13:39:59 GMT   (1418kb,D)

Title: OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene
  Understanding
Authors: Francis Engelmann, Ayca Takmaz, Jonas Schult, Elisabetta Fedele,
  Johanna Wald, Songyou Peng, Xi Wang, Or Litany, Siyu Tang, Federico Tombari,
  Marc Pollefeys, Leonidas Guibas, Hongbo Tian, Chunjie Wang, Xiaosheng Yan,
  Bingwen Wang, Xuanyang Zhang, Xiao Liu, Phuc Nguyen, Khoi Nguyen, Anh Tran,
  Cuong Pham, Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu,
  Joan Lasenby
Categories: cs.CV cs.AI cs.LG
Comments: Our OpenSUN3D workshop website for ICCV 2023:
  https://opensun3d.github.io/index_iccv23.html
\\
  This report provides an overview of the challenge hosted at the OpenSUN3D
Workshop on Open-Vocabulary 3D Scene Understanding held in conjunction with
ICCV 2023. The goal of this workshop series is to provide a platform for
exploration and discussion of open-vocabulary 3D scene understanding tasks,
including but not limited to segmentation, detection and mapping. We provide an
overview of the challenge hosted at the workshop, present the challenge
dataset, the evaluation methodology, and brief descriptions of the winning
methods. For additional details, please see
https://opensun3d.github.io/index_iccv23.html.
\\ ( https://arxiv.org/abs/2402.15321 ,  1418kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15333 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:09:41 GMT   (16222kb,D)

Title: A Quantum-Classical Collaborative Training Architecture Based on Quantum
  State Fidelity
Authors: Ryan L'Abbate, Anthony D'Onofrio Jr., Samuel Stein, Samuel Yen-Chi
  Chen, Ang Li, Pin-Yu Chen, Juntao Chen, Ying Mao
Categories: quant-ph cs.AI
Comments: IEEE Transactions on Quantum Engineering
DOI: 10.1109/TQE.2024.3367234
\\
  Recent advancements have highlighted the limitations of current quantum
systems, particularly the restricted number of qubits available on near-term
quantum devices. This constraint greatly inhibits the range of applications
that can leverage quantum computers. Moreover, as the available qubits
increase, the computational complexity grows exponentially, posing additional
challenges. Consequently, there is an urgent need to use qubits efficiently and
mitigate both present limitations and future complexities. To address this,
existing quantum applications attempt to integrate classical and quantum
systems in a hybrid framework. In this study, we concentrate on quantum deep
learning and introduce a collaborative classical-quantum architecture called
co-TenQu. The classical component employs a tensor network for compression and
feature extraction, enabling higher-dimensional data to be encoded onto logical
quantum circuits with limited qubits. On the quantum side, we propose a
quantum-state-fidelity-based evaluation function to iteratively train the
network through a feedback loop between the two sides. co-TenQu has been
implemented and evaluated with both simulators and the IBM-Q platform. Compared
to state-of-the-art approaches, co-TenQu enhances a classical deep neural
network by up to 41.72% in a fair setting. Additionally, it outperforms other
quantum-based methods by up to 1.9 times and achieves similar accuracy while
utilizing 70.59% fewer qubits.
\\ ( https://arxiv.org/abs/2402.15333 ,  16222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15350 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:38:05 GMT   (8300kb,D)

Title: Farsight: Fostering Responsible AI Awareness During AI Application
  Prototyping
Authors: Zijie J. Wang, Chinmay Kulkarni, Lauren Wilcox, Michael Terry, Michael
  Madaio
Categories: cs.HC cs.AI cs.CY cs.LG
Comments: Accepted to CHI 2024. 40 pages, 19 figures, 5 tables. For a demo
  video, see https://youtu.be/BlSFbGkOlHk. For a live demo, visit
  https://PAIR-code.github.io/farsight. The source code is available at
  https://github.com/PAIR-code/farsight
DOI: 10.1145/3613904.3642335
\\
  Prompt-based interfaces for Large Language Models (LLMs) have made
prototyping and building AI-powered applications easier than ever before.
However, identifying potential harms that may arise from AI applications
remains a challenge, particularly during prompt-based prototyping. To address
this, we present Farsight, a novel in situ interactive tool that helps people
identify potential harms from the AI applications they are prototyping. Based
on a user's prompt, Farsight highlights news articles about relevant AI
incidents and allows users to explore and edit LLM-generated use cases,
stakeholders, and harms. We report design insights from a co-design study with
10 AI prototypers and findings from a user study with 42 AI prototypers. After
using Farsight, AI prototypers in our user study are better able to
independently identify potential harms associated with a prompt and find our
tool more useful and usable than existing resources. Their qualitative feedback
also highlights that Farsight encourages them to focus on end-users and think
beyond immediate harms. We discuss these findings and reflect on their
implications for designing AI prototyping experiences that meaningfully engage
with AI harms. Farsight is publicly accessible at:
https://PAIR-code.github.io/farsight.
\\ ( https://arxiv.org/abs/2402.15350 ,  8300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15368 (*cross-listing*)
Date: Fri, 23 Feb 2024 15:02:44 GMT   (25850kb,D)

Title: Safe Task Planning for Language-Instructed Multi-Robot Systems using
  Conformal Prediction
Authors: Jun Wang, Guocheng He, Yiannis Kantaros
Categories: cs.RO cs.AI
\\
  This paper addresses task planning problems for language-instructed robot
teams. Tasks are expressed in natural language (NL), requiring the robots to
apply their capabilities (e.g., mobility, manipulation, and sensing) at various
locations and semantic objects. Several recent works have addressed similar
planning problems by leveraging pre-trained Large Language Models (LLMs) to
design effective multi-robot plans. However, these approaches lack mission
performance and safety guarantees. To address this challenge, we introduce a
new decentralized LLM-based planner that is capable of achieving high mission
success rates. This is accomplished by leveraging conformal prediction (CP), a
distribution-free uncertainty quantification tool in black-box models. CP
allows the proposed multi-robot planner to reason about its inherent
uncertainty in a decentralized fashion, enabling robots to make individual
decisions when they are sufficiently certain and seek help otherwise. We show,
both theoretically and empirically, that the proposed planner can achieve
user-specified task success rates while minimizing the overall number of help
requests. We demonstrate the performance of our approach on multi-robot home
service applications. We also show through comparative experiments, that our
method outperforms recent centralized and decentralized multi-robot LLM-based
planners in terms of in terms of its ability to design correct plans. The
advantage of our algorithm over baselines becomes more pronounced with
increasing mission complexity and robot team size.
\\ ( https://arxiv.org/abs/2402.15368 ,  25850kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15384 (*cross-listing*)
Date: Fri, 23 Feb 2024 15:30:57 GMT   (5313kb)

Title: Homeostatic motion planning with innate physics knowledge
Authors: Giulia Lafratta, Bernd Porr, Christopher Chandler, Alice Miller
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  Living organisms interact with their surroundings in a closed-loop fashion,
where sensory inputs dictate the initiation and termination of behaviours. Even
simple animals are able to develop and execute complex plans, which has not yet
been replicated in robotics using pure closed-loop input control. We propose a
solution to this problem by defining a set of discrete and temporary
closed-loop controllers, called "tasks", each representing a closed-loop
behaviour. We further introduce a supervisory module which has an innate
understanding of physics and causality, through which it can simulate the
execution of task sequences over time and store the results in a model of the
environment. On the basis of this model, plans can be made by chaining
temporary closed-loop controllers. The proposed framework was implemented for a
real robot and tested in two scenarios as proof of concept.
\\ ( https://arxiv.org/abs/2402.15384 ,  5313kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15418 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:28:55 GMT   (307kb)

Title: Reputational Algorithm Aversion
Authors: Gregory Weitzner
Categories: econ.TH cs.AI cs.GT cs.HC
\\
  People are often reluctant to incorporate information produced by algorithms
into their decisions, a phenomenon called "algorithm aversion". This paper
shows how algorithm aversion arises when the choice to follow an algorithm
conveys information about a human's ability. I develop a model in which workers
make forecasts of a random outcome based on their own private information and
an algorithm's signal. Low-skill workers receive worse information than the
algorithm and hence should always follow the algorithm's signal, while
high-skill workers receive better information than the algorithm and should
sometimes override it. However, due to reputational concerns, low-skill workers
inefficiently override the algorithm to increase the likelihood they are
perceived as high-skill. The model provides a fully rational microfoundation
for algorithm aversion that aligns with the broad concern that AI systems will
displace many types of workers.
\\ ( https://arxiv.org/abs/2402.15418 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15427 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:42:17 GMT   (18500kb,D)

Title: Understanding Entrainment in Human Groups: Optimising Human-Robot
  Collaboration from Lessons Learned during Human-Human Collaboration
Authors: Eike Schneiders, Christopher Fourie, Stanley Celestin, Julie Shah,
  Malte Jung
Categories: cs.HC cs.AI cs.RO
Comments: Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA
DOI: 10.1145/3613904.3642427
\\
  Successful entrainment during collaboration positively affects trust,
willingness to collaborate, and likeability towards collaborators. In this
paper, we present a mixed-method study to investigate characteristics of
successful entrainment leading to pair and group-based synchronisation. Drawing
inspiration from industrial settings, we designed a fast-paced, short-cycle
repetitive task. Using motion tracking, we investigated entrainment in both
dyadic and triadic task completion. Furthermore, we utilise audio-video
recordings and semi-structured interviews to contextualise participants'
experiences. This paper contributes to the Human-Computer/Robot Interaction
(HCI/HRI) literature using a human-centred approach to identify characteristics
of entrainment during pair- and group-based collaboration. We present five
characteristics related to successful entrainment. These are related to the
occurrence of entrainment, leader-follower patterns, interpersonal
communication, the importance of the point-of-assembly, and the value of
acoustic feedback. Finally, we present three design considerations for future
research and design on collaboration with robots.
\\ ( https://arxiv.org/abs/2402.15427 ,  18500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15429 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:48:56 GMT   (15820kb,D)

Title: ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion
  Models against Stochastic Perturbation
Authors: Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir,
  Paul Jennings, Xingyu Zhao
Categories: cs.CV cs.AI cs.LG
\\
  Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in
generating high-quality images based on simple text descriptions. However, as
is common with many Deep Learning (DL) models, DMs are subject to a lack of
robustness. While there are attempts to evaluate the robustness of T2I DMs as a
binary or worst-case problem, they cannot answer how robust in general the
model is whenever an adversarial example (AE) can be found. In this study, we
first introduce a probabilistic notion of T2I DMs' robustness; and then
establish an efficient framework, ProTIP, to evaluate it with statistical
guarantees. The main challenges stem from: i) the high computational cost of
the generation process; and ii) determining if a perturbed input is an AE
involves comparing two output distributions, which is fundamentally harder
compared to other DL tasks like classification where an AE is identified upon
misprediction of labels. To tackle the challenges, we employ sequential
analysis with efficacy and futility early stopping rules in the statistical
testing for identifying AEs, and adaptive concentration inequalities to
dynamically determine the "just-right" number of stochastic perturbations
whenever the verification target is met. Empirical experiments validate the
effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we
demonstrate an application of ProTIP to rank commonly used defence methods.
\\ ( https://arxiv.org/abs/2402.15429 ,  15820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15448 (*cross-listing*)
Date: Fri, 23 Feb 2024 17:23:06 GMT   (8758kb,D)

Title: Computer Vision for Multimedia Geolocation in Human Trafficking
  Investigation: A Systematic Literature Review
Authors: Opeyemi Bamigbade and John Sheppard and Mark Scanlon
Categories: cs.CV cs.AI cs.CY
\\
  The task of multimedia geolocation is becoming an increasingly essential
component of the digital forensics toolkit to effectively combat human
trafficking, child sexual exploitation, and other illegal acts. Typically,
metadata-based geolocation information is stripped when multimedia content is
shared via instant messaging and social media. The intricacy of geolocating,
geotagging, or finding geographical clues in this content is often overly
burdensome for investigators. Recent research has shown that contemporary
advancements in artificial intelligence, specifically computer vision and deep
learning, show significant promise towards expediting the multimedia
geolocation task. This systematic literature review thoroughly examines the
state-of-the-art leveraging computer vision techniques for multimedia
geolocation and assesses their potential to expedite human trafficking
investigation. This includes a comprehensive overview of the application of
computer vision-based approaches to multimedia geolocation, identifies their
applicability in combating human trafficking, and highlights the potential
implications of enhanced multimedia geolocation for prosecuting human
trafficking. 123 articles inform this systematic literature review. The
findings suggest numerous potential paths for future impactful research on the
subject.
\\ ( https://arxiv.org/abs/2402.15448 ,  8758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15487 (*cross-listing*)
Date: Fri, 23 Feb 2024 18:27:17 GMT   (23855kb,D)

Title: RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for
  Robotic Manipulation
Authors: Hanxiao Jiang, Binghao Huang, Ruihai Wu, Zhuoran Li, Shubham Garg,
  Hooshang Nayyeri, Shenlong Wang, Yunzhu Li
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Project Page: https://jianghanxiao.github.io/roboexp-web/
\\
  Robots need to explore their surroundings to adapt to and tackle tasks in
unknown environments. Prior work has proposed building scene graphs of the
environment but typically assumes that the environment is static, omitting
regions that require active interactions. This severely limits their ability to
handle more complex tasks in household and office environments: before setting
up a table, robots must explore drawers and cabinets to locate all utensils and
condiments. In this work, we introduce the novel task of interactive scene
exploration, wherein robots autonomously explore environments and produce an
action-conditioned scene graph (ACSG) that captures the structure of the
underlying environment. The ACSG accounts for both low-level information, such
as geometry and semantics, and high-level information, such as the
action-conditioned relationships between different entities in the scene. To
this end, we present the Robotic Exploration (RoboEXP) system, which
incorporates the Large Multimodal Model (LMM) and an explicit memory design to
enhance our system's capabilities. The robot reasons about what and how to
explore an object, accumulating new information through the interaction process
and incrementally constructing the ACSG. We apply our system across various
real-world settings in a zero-shot manner, demonstrating its effectiveness in
exploring and modeling environments it has never seen before. Leveraging the
constructed ACSG, we illustrate the effectiveness and efficiency of our RoboEXP
system in facilitating a wide range of real-world manipulation tasks involving
rigid, articulated objects, nested objects like Matryoshka dolls, and
deformable objects like cloth.
\\ ( https://arxiv.org/abs/2402.15487 ,  23855kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15504 (*cross-listing*)
Date: Fri, 23 Feb 2024 18:55:09 GMT   (24656kb,D)

Title: Gen4Gen: Generative Data Pipeline for Generative Multi-Concept
  Composition
Authors: Chun-Hsiao Yeh, Ta-Ying Cheng, He-Yen Hsieh, Chuan-En Lin, Yi Ma,
  Andrew Markham, Niki Trigoni, H.T. Kung, Yubei Chen
Categories: cs.CV cs.AI
Comments: Preprint; Project Page: https://danielchyeh.github.io/Gen4Gen/
\\
  Recent text-to-image diffusion models are able to learn and synthesize images
containing novel, personalized concepts (e.g., their own pets or specific
items) with just a few examples for training. This paper tackles two
interconnected issues within this realm of personalizing text-to-image
diffusion models. First, current personalization techniques fail to reliably
extend to multiple concepts -- we hypothesize this to be due to the mismatch
between complex scenes and simple text descriptions in the pre-training dataset
(e.g., LAION). Second, given an image containing multiple personalized
concepts, there lacks a holistic metric that evaluates performance on not just
the degree of resemblance of personalized concepts, but also whether all
concepts are present in the image and whether the image accurately reflects the
overall text description. To address these issues, we introduce Gen4Gen, a
semi-automated dataset creation pipeline utilizing generative models to combine
personalized concepts into complex compositions along with text-descriptions.
Using this, we create a dataset called MyCanvas, that can be used to benchmark
the task of multi-concept personalization. In addition, we design a
comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better
quantifying the performance of multi-concept, personalized text-to-image
diffusion methods. We provide a simple baseline built on top of Custom
Diffusion with empirical prompting strategies for future researchers to
evaluate on MyCanvas. We show that by improving data quality and prompting
strategies, we can significantly increase multi-concept personalized image
generation quality, without requiring any modifications to model architecture
or training algorithms.
\\ ( https://arxiv.org/abs/2402.15504 ,  24656kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14951 (*cross-listing*)
Date: Thu, 22 Feb 2024 20:26:08 GMT   (74kb)

Title: In-Context Learning of a Linear Transformer Block: Benefits of the MLP
  Component and One-Step GD Initialization
Authors: Ruiqi Zhang, Jingfeng Wu, Peter L. Bartlett
Categories: stat.ML cs.CL cs.LG
Comments: 39 pages
\\
  We study the \emph{in-context learning} (ICL) ability of a \emph{Linear
Transformer Block} (LTB) that combines a linear attention component and a
linear multi-layer perceptron (MLP) component. For ICL of linear regression
with a Gaussian prior and a \emph{non-zero mean}, we show that LTB can achieve
nearly Bayes optimal ICL risk. In contrast, using only linear attention must
incur an irreducible additive approximation error. Furthermore, we establish a
correspondence between LTB and one-step gradient descent estimators with
learnable initialization ($\mathsf{GD}\text{-}\mathbf{\beta}$), in the sense
that every $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator can be implemented by
an LTB estimator and every optimal LTB estimator that minimizes the in-class
ICL risk is effectively a $\mathsf{GD}\text{-}\mathbf{\beta}$ estimator.
Finally, we show that $\mathsf{GD}\text{-}\mathbf{\beta}$ estimators can be
efficiently optimized with gradient flow, despite a non-convex training
objective. Our results reveal that LTB achieves ICL by implementing
$\mathsf{GD}\text{-}\mathbf{\beta}$, and they highlight the role of MLP layers
in reducing approximation error.
\\ ( https://arxiv.org/abs/2402.14951 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14968 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:05:18 GMT   (8918kb,D)

Title: Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
Authors: Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Muhao Chen, Junjie
  Hu, Yixuan Li, Bo Li, Chaowei Xiao
Categories: cs.CR cs.CL
\\
  Despite the general capabilities of Large Language Models (LLMs) like GPT-4
and Llama-2, these models still request fine-tuning or adaptation with
customized data when it comes to meeting the specific business demands and
intricacies of tailored use cases. However, this process inevitably introduces
new safety threats, particularly against the Fine-tuning based Jailbreak Attack
(FJAttack), where incorporating just a few harmful examples into the
fine-tuning dataset can significantly compromise the model safety. Though
potential defenses have been proposed by incorporating safety examples into the
fine-tuning dataset to reduce the safety issues, such approaches require
incorporating a substantial amount of safety examples, making it inefficient.
To effectively defend against the FJAttack with limited safety examples, we
propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with
the concept of backdoor attacks. In particular, we construct prefixed safety
examples by integrating a secret prompt, acting as a "backdoor trigger", that
is prefixed to safety examples. Our comprehensive experiments demonstrate that
through the Backdoor Enhanced Safety Alignment with adding as few as 11
prefixed safety examples, the maliciously fine-tuned LLMs will achieve similar
safety performance as the original aligned models. Furthermore, we also explore
the effectiveness of our method in a more practical setting where the
fine-tuning data consists of both FJAttack examples and the fine-tuning task
data. Our method shows great efficacy in defending against FJAttack without
harming the performance of fine-tuning tasks.
\\ ( https://arxiv.org/abs/2402.14968 ,  8918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15021 (*cross-listing*)
Date: Thu, 22 Feb 2024 23:42:25 GMT   (252kb,D)

Title: CLoVe: Encoding Compositional Language in Contrastive Vision-Language
  Models
Authors: Santiago Castro, Amir Ziai, Avneesh Saluja, Zhuoning Yuan, Rada
  Mihalcea
Categories: cs.CV cs.CL
\\
  Recent years have witnessed a significant increase in the performance of
Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as
CLIP, have been leveraged in multiple settings and demonstrated remarkable
performance across several tasks. Such models excel at object-centric
recognition yet learn text representations that seem invariant to word order,
failing to compose known concepts in novel ways. However, no evidence exists
that any VLM, including large-scale single-stream models such as GPT-4V,
identifies compositions successfully. In this paper, we introduce a framework
to significantly improve the ability of existing models to encode compositional
language, with over 10% absolute improvement on compositionality benchmarks,
while maintaining or improving the performance on standard object-recognition
and retrieval benchmarks. Our code and pre-trained models are publicly
available at https://github.com/netflix/clove.
\\ ( https://arxiv.org/abs/2402.15021 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15105 (*cross-listing*)
Date: Fri, 23 Feb 2024 05:30:32 GMT   (2264kb,D)

Title: A First Look at GPT Apps: Landscape and Vulnerability
Authors: Zejun Zhang, Li Zhang, Xin Yuan, Anlan Zhang, Mengwei Xu, Feng Qian
Categories: cs.CR cs.CL
\\
  With the advancement of Large Language Models (LLMs), increasingly
sophisticated and powerful GPTs are entering the market. Despite their
popularity, the LLM ecosystem still remains unexplored. Additionally, LLMs'
susceptibility to attacks raises concerns over safety and plagiarism. Thus, in
this work, we conduct a pioneering exploration of GPT stores, aiming to study
vulnerabilities and plagiarism within GPT applications. To begin with, we
conduct, to our knowledge, the first large-scale monitoring and analysis of two
stores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we
propose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.
To complete these two tasks efficiently, we develop two automated tools: one
for web scraping and another designed for programmatically interacting with
GPTs. Our findings reveal a significant enthusiasm among users and developers
for GPT interaction and creation, as evidenced by the rapid increase in GPTs
and their creators. However, we also uncover a widespread failure to protect
GPT internals, with nearly 90% of system prompts easily accessible, leading to
considerable plagiarism and duplication among GPTs.
\\ ( https://arxiv.org/abs/2402.15105 ,  2264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15151 (*cross-listing*)
Date: Fri, 23 Feb 2024 07:21:32 GMT   (552kb,D)

Title: Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and
  Context-Aware Visual Speech Processing
Authors: Jeong Hun Yeo, Seunghee Han, Minsu Kim, Yong Man Ro
Categories: cs.CV cs.CL eess.AS eess.IV
\\
  In visual speech processing, context modeling capability is one of the most
important requirements due to the ambiguous nature of lip movements. For
example, homophenes, words that share identical lip movements but produce
different sounds, can be distinguished by considering the context. In this
paper, we propose a novel framework, namely Visual Speech Processing
incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by
bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to
perform multi-tasks of visual speech recognition and translation, where the
given instructions control the type of task. The input video is mapped to the
input latent space of a LLM by employing a self-supervised visual speech model.
Focused on the fact that there is redundant information in input frames, we
propose a novel deduplication method that reduces the embedded visual features
by employing visual speech units. Through the proposed deduplication and Low
Rank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient
manner. In the translation dataset, the MuAViC benchmark, we demonstrate that
VSP-LLM can more effectively recognize and translate lip movements with just 15
hours of labeled data, compared to the recent translation model trained with
433 hours of labeld data.
\\ ( https://arxiv.org/abs/2402.15151 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15218 (*cross-listing*)
Date: Fri, 23 Feb 2024 09:28:16 GMT   (17537kb,D)

Title: BSPA: Exploring Black-box Stealthy Prompt Attacks against Image
  Generators
Authors: Yu Tian, Xiao Yang, Yinpeng Dong, Heming Yang, Hang Su, Jun Zhu
Categories: cs.CR cs.CL cs.CV
\\
  Extremely large image generators offer significant transformative potential
across diverse sectors. It allows users to design specific prompts to generate
realistic images through some black-box APIs. However, some studies reveal that
image generators are notably susceptible to attacks and generate Not Suitable
For Work (NSFW) contents by manually designed toxin texts, especially
imperceptible to human observers. We urgently need a multitude of universal and
transferable prompts to improve the safety of image generators, especially
black-box-released APIs. Nevertheless, they are constrained by labor-intensive
design processes and heavily reliant on the quality of the given instructions.
To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that
adopts a retriever to simulate attacks from API users. It can effectively
harness filter scores to tune the retrieval space of sensitive words for
matching the input prompts, thereby crafting stealthy prompts tailored for
image generators. Significantly, this approach is model-agnostic and requires
no internal access to the model's features, ensuring its applicability to a
wide range of image generators. Building on BSPA, we have constructed an
automated prompt tool and a comprehensive prompt attack dataset (NSFWeval).
Extensive experiments demonstrate that BSPA effectively explores the security
vulnerabilities in a variety of state-of-the-art available black-box models,
including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we
develop a resilient text filter and offer targeted recommendations to ensure
the security of image generators against prompt attacks in the future.
\\ ( https://arxiv.org/abs/2402.15218 ,  17537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15265 (*cross-listing*)
Date: Fri, 23 Feb 2024 11:25:17 GMT   (5733kb,D)

Title: CloChat: Understanding How People Customize, Interact, and Experience
  Personas in Large Language Models
Authors: Juhye Ha, Hyeon Jeon, DaEun Han, Jinwook Seo, Changhoon Oh
Categories: cs.HC cs.CL
Comments: In Proceedings of the SIGCHI Conference on Human Factors in Computing
  Systems (CHI '24)
\\
  Large language models (LLMs) have facilitated significant strides in
generating conversational agents, enabling seamless, contextually relevant
dialogues across diverse topics. However, the existing LLM-driven
conversational agents have fixed personalities and functionalities, limiting
their adaptability to individual user needs. Creating personalized agent
personas with distinct expertise or traits can address this issue. Nonetheless,
we lack knowledge of how people customize and interact with agent personas. In
this research, we investigated how users customize agent personas and their
impact on interaction quality, diversity, and dynamics. To this end, we
developed CloChat, an interface supporting easy and accurate customization of
agent personas in LLMs. We conducted a study comparing how participants
interact with CloChat and ChatGPT. The results indicate that participants
formed emotional bonds with the customized agents, engaged in more dynamic
dialogues, and showed interest in sustaining interactions. These findings
contribute to design implications for future systems with conversational agents
using LLMs.
\\ ( https://arxiv.org/abs/2402.15265 ,  5733kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15400 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:03:17 GMT   (3944kb,D)

Title: Faithful Temporal Question Answering over Heterogeneous Sources
Authors: Zhen Jia, Philipp Christmann, Gerhard Weikum
Categories: cs.IR cs.CL
Comments: Accepted at WWW 2024
\\
  Temporal question answering (QA) involves time constraints, with phrases such
as "... in 2019" or "... before COVID". In the former, time is an explicit
condition, in the latter it is implicit. State-of-the-art methods have
limitations along three dimensions. First, with neural inference, time
constraints are merely soft-matched, giving room to invalid or inexplicable
answers. Second, questions with implicit time are poorly supported. Third,
answers come from a single source: either a knowledge base (KB) or a text
corpus. We propose a temporal QA system that addresses these shortcomings.
First, it enforces temporal constraints for faithful answering with tangible
evidence. Second, it properly handles implicit questions. Third, it operates
over heterogeneous sources, covering KB, text and web tables in a unified
manner. The method has three stages: (i) understanding the question and its
temporal conditions, (ii) retrieving evidence from all sources, and (iii)
faithfully answering the question. As implicit questions are sparse in prior
benchmarks, we introduce a principled method for generating diverse questions.
Experiments show superior performance over a suite of baselines.
\\ ( https://arxiv.org/abs/2402.15400 ,  3944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15420 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:30:05 GMT   (3150kb,D)

Title: PREDILECT: Preferences Delineated with Zero-Shot Language-based
  Reasoning in Reinforcement Learning
Authors: Simon Holk, Daniel Marta, Iolanda Leite
Categories: cs.RO cs.CL cs.LG
Comments: 8 pages, 8 Figures, 2 Tables
DOI: 10.1145/3610977.3634970
\\
  Preference-based reinforcement learning (RL) has emerged as a new field in
robot learning, where humans play a pivotal role in shaping robot behavior by
expressing preferences on different sequences of state-action pairs. However,
formulating realistic policies for robots demands responses from humans to an
extensive array of queries. In this work, we approach the sample-efficiency
challenge by expanding the information collected per query to contain both
preferences and optional text prompting. To accomplish this, we leverage the
zero-shot capabilities of a large language model (LLM) to reason from the text
provided by humans. To accommodate the additional query information, we
reformulate the reward learning objectives to contain flexible highlights --
state-action pairs that contain relatively high information and are related to
the features processed in a zero-shot fashion from a pretrained LLM. In both a
simulated scenario and a user study, we reveal the effectiveness of our work by
analyzing the feedback and its implications. Additionally, the collective
feedback collected serves to train a robot on socially compliant trajectories
in a simulated social navigation landscape. We provide video examples of the
trained policies at https://sites.google.com/view/rl-predilect
\\ ( https://arxiv.org/abs/2402.15420 ,  3150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14825 (*cross-listing*)
Date: Thu, 8 Feb 2024 11:04:34 GMT   (999kb,D)

Title: Deepfake Detection and the Impact of Limited Computing Capabilities
Authors: Paloma Cantero-Arjona, Alfonso S\'anchez-Maci\'an
Categories: cs.CV cs.LG eess.IV
\\
  The rapid development of technologies and artificial intelligence makes
deepfakes an increasingly sophisticated and challenging-to-identify technique.
To ensure the accuracy of information and control misinformation and mass
manipulation, it is of paramount importance to discover and develop artificial
intelligence models that enable the generic detection of forged videos. This
work aims to address the detection of deepfakes across various existing
datasets in a scenario with limited computing resources. The goal is to analyze
the applicability of different deep learning techniques under these
restrictions and explore possible approaches to enhance their efficiency.
\\ ( https://arxiv.org/abs/2402.14825 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14844 (*cross-listing*)
Date: Mon, 19 Feb 2024 12:48:02 GMT   (1490kb,D)

Title: The New Era of Dynamic Pricing: Synergizing Supervised Learning and
  Quadratic Programming
Authors: Gustavo Bramao, Ilia Tarygin
Categories: math.OC cs.LG
\\
  In this paper, we explore a novel combination of supervised learning and
quadratic programming to refine dynamic pricing models in the car rental
industry. We utilize dynamic modeling of price elasticity, informed by ordinary
least squares (OLS) metrics such as p-values, homoscedasticity, error
normality. These metrics, when their underlying assumptions hold, are integral
in guiding a quadratic programming agent. The program is tasked with optimizing
margin for a given finite set target.
\\ ( https://arxiv.org/abs/2402.14844 ,  1490kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14862 (*cross-listing*)
Date: Wed, 21 Feb 2024 03:31:40 GMT   (3653kb,D)

Title: SISSA: Real-time Monitoring of Hardware Functional Safety and
  Cybersecurity with In-vehicle SOME/IP Ethernet Traffic
Authors: Qi Liu, Xingyu Li, Ke Sun, Yufeng Li, Yanchen Liu
Categories: cs.CR cs.LG cs.NI
\\
  Scalable service-Oriented Middleware over IP (SOME/IP) is an Ethernet
communication standard protocol in the Automotive Open System Architecture
(AUTOSAR), promoting ECU-to-ECU communication over the IP stack. However,
SOME/IP lacks a robust security architecture, making it susceptible to
potential attacks. Besides, random hardware failure of ECU will disrupt SOME/IP
communication. In this paper, we propose SISSA, a SOME/IP communication
traffic-based approach for modeling and analyzing in-vehicle functional safety
and cyber security. Specifically, SISSA models hardware failures with the
Weibull distribution and addresses five potential attacks on SOME/IP
communication, including Distributed Denial-of-Services, Man-in-the-Middle, and
abnormal communication processes, assuming a malicious user accesses the
in-vehicle network. Subsequently, SISSA designs a series of deep learning
models with various backbones to extract features from SOME/IP sessions among
ECUs. We adopt residual self-attention to accelerate the model's convergence
and enhance detection accuracy, determining whether an ECU is under attack,
facing functional failure, or operating normally. Additionally, we have created
and annotated a dataset encompassing various classes, including indicators of
attack, functionality, and normalcy. This contribution is noteworthy due to the
scarcity of publicly accessible datasets with such characteristics.Extensive
experimental results show the effectiveness and efficiency of SISSA.
\\ ( https://arxiv.org/abs/2402.14862 ,  3653kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14877 (*cross-listing*)
Date: Wed, 21 Feb 2024 20:59:19 GMT   (289kb,D)

Title: Machine-learning prediction of tipping and collapse of the Atlantic
  Meridional Overturning Circulation
Authors: Shirin Panahi, Ling-Wei Kong, Mohammadamin Moradi, Zheng-Meng Zhai,
  Bryan Glaz, Mulugeta Haile, and Ying-Cheng Lai
Categories: physics.ao-ph cs.LG math.DS physics.data-an physics.pop-ph
Comments: 6 pages, 3 figures
\\
  Recent research on the Atlantic Meridional Overturning Circulation (AMOC)
raised concern about its potential collapse through a tipping point due to the
climate-change caused increase in the freshwater input into the North Atlantic.
The predicted time window of collapse is centered about the middle of the
century and the earliest possible start is approximately two years from now.
More generally, anticipating a tipping point at which the system transitions
from one stable steady state to another is relevant to a broad range of fields.
We develop a machine-learning approach to predicting tipping in noisy dynamical
systems with a time-varying parameter and test it on a number of systems
including the AMOC, ecological networks, an electrical power system, and a
climate model. For the AMOC, our prediction based on simulated fingerprint data
and real data of the sea surface temperature places the time window of a
potential collapse between the years 2040 and 2065.
\\ ( https://arxiv.org/abs/2402.14877 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14892 (*cross-listing*)
Date: Thu, 22 Feb 2024 14:13:44 GMT   (25651kb,D)

Title: Novelty Detection on Radio Astronomy Data using Signatures
Authors: Paola Arrubarrena, Maud Lemercier, Bojan Nikolic, Terry Lyons, Thomas
  Cass
Categories: astro-ph.IM cs.LG
MSC-class: 60L10, 60L20
\\
  We introduce SigNova, a new semi-supervised framework for detecting anomalies
in streamed data. While our initial examples focus on detecting radio-frequency
interference (RFI) in digitized signals within the field of radio astronomy, it
is important to note that SigNova's applicability extends to any type of
streamed data. The framework comprises three primary components. Firstly, we
use the signature transform to extract a canonical collection of summary
statistics from observational sequences. This allows us to represent
variable-length visibility samples as finite-dimensional feature vectors.
Secondly, each feature vector is assigned a novelty score, calculated as the
Mahalanobis distance to its nearest neighbor in an RFI-free training set. By
thresholding these scores we identify observation ranges that deviate from the
expected behavior of RFI-free visibility samples without relying on stringent
distributional assumptions. Thirdly, we integrate this anomaly detector with
Pysegments, a segmentation algorithm, to localize consecutive observations
contaminated with RFI, if any. This approach provides a compelling alternative
to classical windowing techniques commonly used for RFI detection. Importantly,
the complexity of our algorithm depends on the RFI pattern rather than on the
size of the observation window. We demonstrate how SigNova improves the
detection of various types of RFI (e.g., broadband and narrowband) in
time-frequency visibility data. We validate our framework on the Murchison
Widefield Array (MWA) telescope and simulated data and the Hydrogen Epoch of
Reionization Array (HERA).
\\ ( https://arxiv.org/abs/2402.14892 ,  25651kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14925 (*cross-listing*)
Date: Thu, 22 Feb 2024 19:15:50 GMT   (42kb,D)

Title: Efficient Unbiased Sparsification
Authors: Leighton Barnes, Timothy Chow, Emma Cohen, Keith Frankston, Benjamin
  Howard, Fred Kochman, Daniel Scheinerman, Jeffrey VanderKam
Categories: cs.IT cs.LG math.IT math.ST stat.TH
\\
  An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random
vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m<n$ nonzero
coordinates. Unbiased sparsification compresses the original vector without
introducing bias; it arises in various contexts, such as in federated learning
and sampling sparse probability distributions. Ideally, unbiased sparsification
should also minimize the expected value of a divergence function
$\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If
$Q$ is optimal in this sense, then we call it efficient. Our main results
describe efficient unbiased sparsifications for divergences that are either
permutation-invariant or additively separable. Surprisingly, the
characterization for permutation-invariant divergences is robust to the choice
of divergence function, in the sense that our class of optimal $Q$ for squared
Euclidean distance coincides with our class of optimal $Q$ for Kullback-Leibler
divergence, or indeed any of a wide variety of divergences.
\\ ( https://arxiv.org/abs/2402.14925 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14957 (*cross-listing*)
Date: Thu, 22 Feb 2024 20:36:24 GMT   (3963kb,D)

Title: The Common Stability Mechanism behind most Self-Supervised Learning
  Approaches
Authors: Abhishek Jha, Matthew B. Blaschko, Yuki M. Asano, Tinne Tuytelaars
Categories: cs.CV cs.LG
Comments: Additional visualizations (.gif):
  https://github.com/abskjha/CenterVectorSSL
\\
  Last couple of years have witnessed a tremendous progress in self-supervised
learning (SSL), the success of which can be attributed to the introduction of
useful inductive biases in the learning process to learn meaningful visual
representations while avoiding collapse. These inductive biases and constraints
manifest themselves in the form of different optimization formulations in the
SSL techniques, e.g. by utilizing negative examples in a contrastive
formulation, or exponential moving average and predictor in BYOL and SimSiam.
In this paper, we provide a framework to explain the stability mechanism of
these different SSL techniques: i) we discuss the working mechanism of
contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV,
SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite
different formulations these methods implicitly optimize a similar objective
function, i.e. minimizing the magnitude of the expected representation over all
data samples, or the mean of the data distribution, while maximizing the
magnitude of the expected representation of individual samples over different
data augmentations; iii) we provide mathematical and empirical evidence to
support our framework. We formulate different hypotheses and test them using
the Imagenet100 dataset.
\\ ( https://arxiv.org/abs/2402.14957 ,  3963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14961 (*cross-listing*)
Date: Thu, 22 Feb 2024 20:49:04 GMT   (7525kb,D)

Title: Reinforcement Learning with Elastic Time Steps
Authors: Dong Wang and Giovanni Beltrame
Categories: cs.RO cs.LG
\\
  Traditional Reinforcement Learning (RL) algorithms are usually applied in
robotics to learn controllers that act with a fixed control rate. Given the
discrete nature of RL algorithms, they are oblivious to the effects of the
choice of control rate: finding the correct control rate can be difficult and
mistakes often result in excessive use of computing resources or even lack of
convergence.
  We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic
algorithm to address this issue. SEAC implements elastic time steps, time steps
with a known, variable duration, which allow the agent to change its control
frequency to adapt to the situation. In practice, SEAC applies control only
when necessary, minimizing computational resources and data usage.
  We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze
navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the
SAC baseline in terms of energy efficiency and overall time management, and
most importantly without the need to identify a control frequency for the
learned controller. SEAC demonstrated faster and more stable training speeds
than SAC, especially at control rates where SAC struggled to converge.
  We also compared SEAC with a similar approach, the Continuous-Time
Continuous-Options (CTCO) model, and SEAC resulted in better task performance.
These findings highlight the potential of SEAC for practical, real-world RL
applications in robotics.
\\ ( https://arxiv.org/abs/2402.14961 ,  7525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14966 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:02:19 GMT   (162kb,D)

Title: Smoothness Adaptive Hypothesis Transfer Learning
Authors: Haotian Lin, Matthew Reimherr
Categories: stat.ML cs.LG stat.ME
\\
  Many existing two-phase kernel-based hypothesis transfer learning algorithms
employ the same kernel regularization across phases and rely on the known
smoothness of functions to obtain optimality. Therefore, they fail to adapt to
the varying and unknown smoothness between the target/source and their offset
in practice. In this paper, we address these problems by proposing Smoothness
Adaptive Transfer Learning (SATL), a two-phase kernel ridge
regression(KRR)-based algorithm. We first prove that employing the misspecified
fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax
optimality and derive an adaptive procedure to the unknown Sobolev smoothness.
Leveraging these results, SATL employs Gaussian kernels in both phases so that
the estimators can adapt to the unknown smoothness of the target/source and
their offset function. We derive the minimax lower bound of the learning
problem in excess risk and show that SATL enjoys a matching upper bound up to a
logarithmic factor. The minimax convergence rate sheds light on the factors
influencing transfer dynamics and demonstrates the superiority of SATL compared
to non-transfer learning settings. While our main objective is a theoretical
analysis, we also conduct several experiments to confirm our results.
\\ ( https://arxiv.org/abs/2402.14966 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14977 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:31:43 GMT   (218kb,D)

Title: Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models
Authors: Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong
Categories: cs.CR cs.CV cs.LG
Comments: To appear in USENIX Security Symposium, 2024
\\
  Foundation model has become the backbone of the AI ecosystem. In particular,
a foundation model can be used as a general-purpose feature extractor to build
various downstream classifiers. However, foundation models are vulnerable to
backdoor attacks and a backdoored foundation model is a single-point-of-failure
of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor
vulnerabilities simultaneously. In this work, we propose Mudjacking, the first
method to patch foundation models to remove backdoors. Specifically, given a
misclassified trigger-embedded input detected after a backdoored foundation
model is deployed, Mudjacking adjusts the parameters of the foundation model to
remove the backdoor. We formulate patching a foundation model as an
optimization problem and propose a gradient descent based method to solve it.
We evaluate Mudjacking on both vision and language foundation models, eleven
benchmark datasets, five existing backdoor attacks, and thirteen adaptive
backdoor attacks. Our results show that Mudjacking can remove backdoor from a
foundation model while maintaining its utility.
\\ ( https://arxiv.org/abs/2402.14977 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14980 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:41:27 GMT   (698kb)

Title: Comparative Analysis of Data Preprocessing Methods, Feature Selection
  Techniques and Machine Learning Models for Improved Classification and
  Regression Performance on Imbalanced Genetic Data
Authors: Arshmeet Kaur and Morteza Sarmadi
Categories: q-bio.QM cs.LG stat.ML
\\
  Rapid advancements in genome sequencing have led to the collection of vast
amounts of genomics data. Researchers may be interested in using machine
learning models on such data to predict the pathogenicity or clinical
significance of a genetic mutation. However, many genetic datasets contain
imbalanced target variables that pose challenges to machine learning models:
observations are skewed/imbalanced in regression tasks or class-imbalanced in
classification tasks. Genetic datasets are also often high-cardinal and contain
skewed predictor variables, which poses further challenges. We aimed to
investigate the effects of data preprocessing, feature selection techniques,
and model selection on the performance of models trained on these datasets. We
measured performance with 5-fold cross-validation and compared averaged
r-squared and accuracy metrics across different combinations of techniques. We
found that outliers/skew in predictor or target variables did not pose a
challenge to regression models. We also found that class-imbalanced target
variables and skewed predictors had little to no impact on classification
performance. Random forest was the best model to use for imbalanced regression
tasks. While our study uses a genetic dataset as an example of a real-world
application, our findings can be generalized to any similar datasets.
\\ ( https://arxiv.org/abs/2402.14980 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14982 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:44:58 GMT   (1209kb,D)

Title: Human Brain Exhibits Distinct Patterns When Listening to Fake Versus
  Real Audio: Preliminary Evidence
Authors: Mahsa Salehi, Kalin Stefanov, Ehsan Shareghi
Categories: cs.SD cs.LG eess.AS q-bio.NC
Comments: 9 pages, 4 figures, 3 tables
\\
  In this paper we study the variations in human brain activity when listening
to real and fake audio. Our preliminary results suggest that the
representations learned by a state-of-the-art deepfake audio detection
algorithm, do not exhibit clear distinct patterns between real and fake audio.
In contrast, human brain activity, as measured by EEG, displays distinct
patterns when individuals are exposed to fake versus real audio. This
preliminary evidence enables future research directions in areas such as
deepfake audio detection.
\\ ( https://arxiv.org/abs/2402.14982 ,  1209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14987 (*cross-listing*)
Date: Thu, 22 Feb 2024 21:55:41 GMT   (47kb)

Title: On the Performance of Empirical Risk Minimization with Smoothed Data
Authors: Adam Block, Alexander Rakhlin, and Abhishek Shetty
Categories: stat.ML cs.LG
\\
  In order to circumvent statistical and computational hardness results in
sequential decision-making, recent work has considered smoothed online
learning, where the distribution of data at each time is assumed to have
bounded likeliehood ratio with respect to a base measure when conditioned on
the history. While previous works have demonstrated the benefits of smoothness,
they have either assumed that the base measure is known to the learner or have
presented computationally inefficient algorithms applying only in special
cases. This work investigates the more general setting where the base measure
is \emph{unknown} to the learner, focusing in particular on the performance of
Empirical Risk Minimization (ERM) with square loss when the data are
well-specified and smooth. We show that in this setting, ERM is able to achieve
sublinear error whenever a class is learnable with iid data; in particular, ERM
achieves error scaling as $\tilde O( \sqrt{\mathrm{comp}(\mathcal F)\cdot T}
)$, where $\mathrm{comp}(\mathcal F)$ is the statistical complexity of learning
$\mathcal F$ with iid data. In so doing, we prove a novel norm comparison bound
for smoothed data that comprises the first sharp norm comparison for dependent
data applying to arbitrary, nonlinear function classes. We complement these
results with a lower bound indicating that our analysis of ERM is essentially
tight, establishing a separation in the performance of ERM between smoothed and
iid data.
\\ ( https://arxiv.org/abs/2402.14987 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15006 (*cross-listing*)
Date: Thu, 22 Feb 2024 22:54:41 GMT   (696kb)

Title: opp/ai: Optimistic Privacy-Preserving AI on Blockchain
Authors: Cathie So, KD Conway, Xiaohang Yu, Suning Yao, Kartin Wong
Categories: cs.CR cs.LG
\\
  The convergence of Artificial Intelligence (AI) and blockchain technology is
reshaping the digital world, offering decentralized, secure, and efficient AI
services on blockchain platforms. Despite the promise, the high computational
demands of AI on blockchain raise significant privacy and efficiency concerns.
The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a
pioneering solution to these issues, striking a balance between privacy
protection and computational efficiency. The framework integrates
Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine
Learning (opML) for efficiency, creating a hybrid model tailored for blockchain
AI services. This study presents the opp/ai framework, delves into the privacy
features of zkML, and assesses the framework's performance and adaptability
across different scenarios.
\\ ( https://arxiv.org/abs/2402.15006 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15025 (*cross-listing*)
Date: Thu, 22 Feb 2024 23:58:26 GMT   (18119kb,D)

Title: Practice Makes Perfect: Planning to Learn Skill Parameter Policies
Authors: Nishanth Kumar, Tom Silver, Willie McClinton, Linfeng Zhao, Stephen
  Proulx, Tom\'as Lozano-P\'erez, Leslie Pack Kaelbling, Jennifer Barry
Categories: cs.RO cs.LG
\\
  One promising approach towards effective robot decision making in complex,
long-horizon tasks is to sequence together parameterized skills. We consider a
setting where a robot is initially equipped with (1) a library of parameterized
skills, (2) an AI planner for sequencing together the skills given a goal, and
(3) a very general prior distribution for selecting skill parameters. Once
deployed, the robot should rapidly and autonomously learn to improve its
performance by specializing its skill parameter selection policy to the
particular objects, goals, and constraints in its environment. In this work, we
focus on the active learning problem of choosing which skills to practice to
maximize expected future task success. We propose that the robot should
estimate the competence of each skill, extrapolate the competence (asking: "how
much would the competence improve through practice?"), and situate the skill in
the task distribution through competence-aware planning. This approach is
implemented within a fully autonomous system where the robot repeatedly plans,
practices, and learns without any environment resets. Through experiments in
simulation, we find that our approach learns effective parameter policies more
sample-efficiently than several baselines. Experiments in the real-world
demonstrate our approach's ability to handle noise from perception and control
and improve the robot's ability to solve two long-horizon mobile-manipulation
tasks after a few hours of autonomous practice.
\\ ( https://arxiv.org/abs/2402.15025 ,  18119kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15039 (*cross-listing*)
Date: Fri, 23 Feb 2024 01:22:32 GMT   (3418kb,D)

Title: Descripci\'on autom\'atica de secciones delgadas de rocas: una
  aplicaci\'on Web
Authors: Stalyn Paucar, Christian Mej\'ia-Escobar y V\'ictor Collaguazo
Categories: cs.CV cs.LG
Comments: 21 pages, in Spanish language, 7 figures
\\
  The identification and characterization of various rock types is one of the
fundamental activities for geology and related areas such as mining, petroleum,
environment, industry and construction. Traditionally, a human specialist is
responsible for analyzing and explaining details about the type, composition,
texture, shape and other properties using rock samples collected in-situ or
prepared in a laboratory. The results become subjective based on experience, in
addition to consuming a large investment of time and effort. The present
proposal uses artificial intelligence techniques combining computer vision and
natural language processing to generate a textual and verbal description from a
thin section image of rock. We build a dataset of images and their respective
textual descriptions for the training of a model that associates the relevant
features of the image extracted by EfficientNetB7 with the textual description
generated by a Transformer network, reaching an accuracy value of 0.892 and a
BLEU value of 0.71. This model can be a useful resource for research,
professional and academic work, so it has been deployed through a Web
application for public use.
\\ ( https://arxiv.org/abs/2402.15039 ,  3418kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15044 (*cross-listing*)
Date: Fri, 23 Feb 2024 01:34:00 GMT   (25953kb,D)

Title: Fiducial Focus Augmentation for Facial Landmark Detection
Authors: Purbayan Kar, Vishal Chudasama, Naoyuki Onoe, Pankaj Wasnik, Vineeth
  Balasubramanian
Categories: cs.CV cs.LG
Comments: Accepted to BMVC'23
\\
  Deep learning methods have led to significant improvements in the performance
on the facial landmark detection (FLD) task. However, detecting landmarks in
challenging settings, such as head pose changes, exaggerated expressions, or
uneven illumination, continue to remain a challenge due to high variability and
insufficient samples. This inadequacy can be attributed to the model's
inability to effectively acquire appropriate facial structure information from
the input images. To address this, we propose a novel image augmentation
technique specifically designed for the FLD task to enhance the model's
understanding of facial structures. To effectively utilize the newly proposed
augmentation technique, we employ a Siamese architecture-based training
mechanism with a Deep Canonical Correlation Analysis (DCCA)-based loss to
achieve collective learning of high-level feature representations from two
different views of the input images. Furthermore, we employ a Transformer +
CNN-based network with a custom hourglass module as the robust backbone for the
Siamese framework. Extensive experiments show that our approach outperforms
multiple state-of-the-art approaches across various benchmark datasets.
\\ ( https://arxiv.org/abs/2402.15044 ,  25953kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15053 (*cross-listing*)
Date: Fri, 23 Feb 2024 02:14:44 GMT   (367kb)

Title: Nonlinear Bayesian optimal experimental design using logarithmic Sobolev
  inequalities
Authors: Fengyi Li, Ayoub Belhadji, Youssef Marzouk
Categories: stat.ML cs.LG stat.ME
\\
  We study the problem of selecting $k$ experiments from a larger candidate
pool, where the goal is to maximize mutual information (MI) between the
selected subset and the underlying parameters. Finding the exact solution is to
this combinatorial optimization problem is computationally costly, not only due
to the complexity of the combinatorial search but also the difficulty of
evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches
based on new computationally inexpensive lower bounds for MI, constructed via
log-Sobolev inequalities. We demonstrate that our method outperforms random
selection strategies, Gaussian approximations, and nested Monte Carlo (NMC)
estimators of MI in various settings, including optimal design for nonlinear
models with non-additive noise.
\\ ( https://arxiv.org/abs/2402.15053 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15058 (*cross-listing*)
Date: Fri, 23 Feb 2024 02:19:26 GMT   (7433kb,D)

Title: Mixup Barcodes: Quantifying Geometric-Topological Interactions between
  Point Clouds
Authors: Hubert Wagner, Nickolas Arustamyan, Matthew Wheeler, Peter Bubenik
Categories: math.AT cs.CG cs.LG
\\
  We combine standard persistent homology with image persistent homology to
define a novel way of characterizing shapes and interactions between them. In
particular, we introduce: (1) a mixup barcode, which captures
geometric-topological interactions (mixup) between two point sets in arbitrary
dimension; (2) simple summary statistics, total mixup and total percentage
mixup, which quantify the complexity of the interactions as a single number;
(3) a software tool for playing with the above.
  As a proof of concept, we apply this tool to a problem arising from machine
learning. In particular, we study the disentanglement in embeddings of
different classes. The results suggest that topological mixup is a useful
method for characterizing interactions for low and high-dimensional data.
Compared to the typical usage of persistent homology, the new tool is sensitive
to the geometric locations of the topological features, which is often
desirable.
\\ ( https://arxiv.org/abs/2402.15058 ,  7433kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15095 (*cross-listing*)
Date: Fri, 23 Feb 2024 04:58:54 GMT   (32kb)

Title: The Umeyama algorithm for matching correlated Gaussian geometric models
  in the low-dimensional regime
Authors: Shuyang Gong and Zhangsong Li
Categories: math.ST cs.DS cs.LG math.PR stat.TH
Comments: 31 pages
MSC-class: 68Q87 (Primary), 62M15 (Secondary)
\\
  Motivated by the problem of matching two correlated random geometric graphs,
we study the problem of matching two Gaussian geometric models correlated
through a latent node permutation. Specifically, given an unknown permutation
$\pi^*$ on $\{1,\ldots,n\}$ and given $n$ i.i.d. pairs of correlated Gaussian
vectors $\{X_{\pi^*(i)},Y_i\}$ in $\mathbb{R}^d$ with noise parameter $\sigma$,
we consider two types of (correlated) weighted complete graphs with edge
weights given by $A_{i,j}=\langle X_i,X_j \rangle$, $B_{i,j}=\langle Y_i,Y_j
\rangle$. The goal is to recover the hidden vertex correspondence $\pi^*$ based
on the observed matrices $A$ and $B$. For the low-dimensional regime where
$d=O(\log n)$, Wang, Wu, Xu, and Yolou [WWXY22+] established the information
thresholds for exact and almost exact recovery in matching correlated Gaussian
geometric models. They also conducted numerical experiments for the classical
Umeyama algorithm. In our work, we prove that this algorithm achieves exact
recovery of $\pi^*$ when the noise parameter $\sigma=o(d^{-3}n^{-2/d})$, and
almost exact recovery when $\sigma=o(d^{-3}n^{-1/d})$. Our results approach the
information thresholds up to a $\operatorname{poly}(d)$ factor in the
low-dimensional regime.
\\ ( https://arxiv.org/abs/2402.15095 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15100 (*cross-listing*)
Date: Fri, 23 Feb 2024 05:17:28 GMT   (1244kb,D)

Title: Studying LLM Performance on Closed- and Open-source Data
Authors: Toufique Ahmed, Christian Bird, Premkumar Devanbu, Saikat Chakraborty
Categories: cs.SE cs.LG
\\
  Large Language models (LLMs) are finding wide use in software engineering
practice. These models are extremely data-hungry, and are largely trained on
open-source (OSS) code distributed with permissive licenses. In terms of actual
use however, a great deal of software development still occurs in the
for-profit/proprietary sphere, where the code under development is not, and
never has been, in the public domain; thus, many developers, do their work, and
use LLMs, in settings where the models may not be as familiar with the code
under development. In such settings, do LLMs work as well as they do for OSS
code? If not, what are the differences? When performance differs, what are the
possible causes, and are there work-arounds? In this paper, we examine this
issue using proprietary, closed-source software data from Microsoft, where most
proprietary code is in C# and C++. We find that performance for C# changes
little from OSS --> proprietary code, but does significantly reduce for C++; we
find that this difference is attributable to differences in identifiers. We
also find that some performance degradation, in some cases, can be ameliorated
efficiently by in-context learning.
\\ ( https://arxiv.org/abs/2402.15100 ,  1244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15111 (*cross-listing*)
Date: Fri, 23 Feb 2024 05:50:43 GMT   (2122kb,D)

Title: Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled
  Realization for Multi-Round Secure Aggregation in Federated Learning
Authors: Kaiping Cui, Xia Feng, Liangmin Wang, Haiqin Wu, Xiaoyu Zhang and
  Boris D\"udder
Categories: cs.CR cs.DC cs.LG
\\
  Secure aggregation enables federated learning (FL) to perform collaborative
training of clients from local gradient updates without exposing raw data.
However, existing secure aggregation schemes inevitably perform an expensive
fresh setup per round because each client needs to establish fresh
input-independent secrets over different rounds. The latest research, Flamingo
(S&P 2023), designed a share-transfer-based reusable secret key to support the
server continuously performing multiple rounds of aggregation. Nevertheless,
the share transfer mechanism it proposed can only be achieved with P
probability, which has limited reliability. To tackle the aforementioned
problems, we propose a more reliable and anonymously authenticated scheme
called Chu-ko-nu for multi-round secure aggregation. Specifically, in terms of
share transfer, Chu-ko-nu breaks the probability P barrier by supplementing a
redistribution process of secret key components (the sum of all components is
the secret key), thus ensuring the reusability of the secret key. Based on this
reusable secret key, Chu-ko-nu can efficiently perform consecutive aggregation
in the following rounds. Furthermore, considering the client identity
authentication and privacy protection issue most approaches ignore, Chu-ko-nu
introduces a zero-knowledge proof-based authentication mechanism. It can
support clients anonymously participating in FL training and enables the server
to authenticate clients effectively in the presence of various attacks.
Rigorous security proofs and extensive experiments demonstrated that Chu-ko-nu
can provide reliable and anonymously authenticated aggregation for FL with low
aggregation costs, at least a 21.02% reduction compared to the state-of-the-art
schemes.
\\ ( https://arxiv.org/abs/2402.15111 ,  2122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15115 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:04:15 GMT   (18078kb,D)

Title: Physics-constrained polynomial chaos expansion for scientific machine
  learning and uncertainty quantification
Authors: Himanshu Sharma, Luk\'a\v{s} Nov\'ak, Michael D. Shields
Categories: stat.ML cs.LG physics.data-an
Comments: 32 pages, 14 figures
\\
  We present a novel physics-constrained polynomial chaos expansion as a
surrogate modeling method capable of performing both scientific machine
learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method
possesses a unique capability: it seamlessly integrates SciML into UQ and vice
versa, which allows it to quantify the uncertainties in SciML tasks effectively
and leverage SciML for improved uncertainty assessment during UQ-related tasks.
The proposed surrogate model can effectively incorporate a variety of physical
constraints, such as governing partial differential equations (PDEs) with
associated initial and boundary conditions constraints, inequality-type
constraints (e.g., monotonicity, convexity, non-negativity, among others), and
additional a priori information in the training process to supplement limited
data. This ensures physically realistic predictions and significantly reduces
the need for expensive computational model evaluations to train the surrogate
model. Furthermore, the proposed method has a built-in uncertainty
quantification (UQ) feature to efficiently estimate output uncertainties. To
demonstrate the effectiveness of the proposed method, we apply it to a diverse
set of problems, including linear/non-linear PDEs with deterministic and
stochastic parameters, data-driven surrogate modeling of a complex physical
system, and UQ of a stochastic system with parameters modeled as random fields.
\\ ( https://arxiv.org/abs/2402.15115 ,  18078kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15141 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:55:34 GMT   (5575kb)

Title: A note on the adjoint method for neural ordinary differential equation
  network
Authors: Pipi Hu
Categories: math.NA cs.LG cs.NA
\\
  Perturbation and operator adjoint method are used to give the right adjoint
form rigourously. From the derivation, we can have following results: 1) The
loss gradient is not an ODE, it is an integral and we shows the reason; 2) The
traditional adjoint form is not equivalent with the back propagation results.
3) The adjoint operator analysis shows that if and only if the discrete adjoint
has the same scheme with the discrete neural ODE, the adjoint form would give
the same results as BP does.
\\ ( https://arxiv.org/abs/2402.15141 ,  5575kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15143 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:57:31 GMT   (1828kb,D)

Title: PUAD: Frustratingly Simple Method for Robust Anomaly Detection
Authors: Shota Sugawara, Ryuji Imamura
Categories: cs.CV cs.LG
Comments: 8 pages, 4 figures
\\
  Developing an accurate and fast anomaly detection model is an important task
in real-time computer vision applications. There has been much research to
develop a single model that detects either structural or logical anomalies,
which are inherently distinct. The majority of the existing approaches
implicitly assume that the anomaly can be represented by identifying the
anomalous location. However, we argue that logical anomalies, such as the wrong
number of objects, can not be well-represented by the spatial feature maps and
require an alternative approach. In addition, we focused on the possibility of
detecting logical anomalies by using an out-of-distribution detection approach
on the feature space, which aggregates the spatial information of the feature
map. As a demonstration, we propose a method that incorporates a simple
out-of-distribution detection method on the feature space against
state-of-the-art reconstruction-based approaches. Despite the simplicity of our
proposal, our method PUAD (Picturable and Unpicturable Anomaly Detection)
achieves state-of-the-art performance on the MVTec LOCO AD dataset.
\\ ( https://arxiv.org/abs/2402.15143 ,  1828kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15147 (*cross-listing*)
Date: Fri, 23 Feb 2024 07:05:32 GMT   (4866kb,D)

Title: TREC: APT Tactic / Technique Recognition via Few-Shot Provenance
  Subgraph Learning
Authors: Mingqi Lv, HongZhe Gao, Xuebo Qiu, Tieming Chen and Tiantian Zhu
Categories: cs.CR cs.LG
\\
  APT (Advanced Persistent Threat) with the characteristics of persistence,
stealth, and diversity is one of the greatest threats against
cyber-infrastructure. As a countermeasure, existing studies leverage provenance
graphs to capture the complex relations between system entities in a host for
effective APT detection. In addition to detecting single attack events as most
existing work does, understanding the tactics / techniques (e.g., Kill-Chain,
ATT&CK) applied to organize and accomplish the APT attack campaign is more
important for security operations. Existing studies try to manually design a
set of rules to map low-level system events to high-level APT tactics /
techniques. However, the rule based methods are coarse-grained and lack
generalization ability, thus they can only recognize APT tactics and cannot
identify fine-grained APT techniques and mutant APT attacks. In this paper, we
propose TREC, the first attempt to recognize APT tactics / techniques from
provenance graphs by exploiting deep learning techniques. To address the
"needle in a haystack" problem, TREC segments small and compact subgraphs
covering individual APT technique instances from a large provenance graph based
on a malicious node detection model and a subgraph sampling algorithm. To
address the "training sample scarcity" problem, TREC trains the APT tactic /
technique recognition model in a few-shot learning manner by adopting a Siamese
neural network. We evaluate TREC based on a customized dataset collected and
made public by our team. The experiment results show that TREC significantly
outperforms state-of-the-art systems in APT tactic recognition and TREC can
also effectively identify APT techniques.
\\ ( https://arxiv.org/abs/2402.15147 ,  4866kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15164 (*cross-listing*)
Date: Fri, 23 Feb 2024 07:54:26 GMT   (444kb,D)

Title: EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning
  Based Recommender Systems
Authors: Yuanqing Yu, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian
  Chen, Weizhi Ma, Min Zhang
Categories: cs.IR cs.LG
\\
  Reinforcement Learning (RL)-Based Recommender Systems (RSs) are increasingly
recognized for their ability to improve long-term user engagement. Yet, the
field grapples with challenges such as the absence of accessible frameworks,
inconsistent evaluation standards, and the complexity of replicating prior
work. Addressing these obstacles, we present EasyRL4Rec, a user-friendly and
efficient library tailored for RL-based RSs. EasyRL4Rec features lightweight,
diverse RL environments built on five widely-used public datasets, and is
equipped with comprehensive core modules that offer rich options to ease the
development of models. It establishes consistent evaluation criteria with a
focus on long-term impacts and introduces customized solutions for state
modeling and action representation tailored to recommender systems.
Additionally, we share valuable insights gained from extensive experiments with
current methods. EasyRL4Rec aims to facilitate the model development and
experimental process in the domain of RL-based RSs. The library is openly
accessible at https://github.com/chongminggao/EasyRL4Rec.
\\ ( https://arxiv.org/abs/2402.15164 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15166 (*cross-listing*)
Date: Fri, 23 Feb 2024 07:59:23 GMT   (769kb,D)

Title: Convergence Analysis of Split Federated Learning on Heterogeneous Data
Authors: Pengchao Han, Chao Huang, Geng Tian, Ming Tang, Xin Liu
Categories: cs.DC cs.LG
\\
  Split federated learning (SFL) is a recent distributed approach for
collaborative model training among multiple clients. In SFL, a global model is
typically split into two parts, where clients train one part in a parallel
federated manner, and a main server trains the other. Despite the recent
research on SFL algorithm development, the convergence analysis of SFL is
missing in the literature, and this paper aims to fill this gap. The analysis
of SFL can be more challenging than that of federated learning (FL), due to the
potential dual-paced updates at the clients and the main server. We provide
convergence analysis of SFL for strongly convex and general convex objectives
on heterogeneous data. The convergence rates are $O(1/T)$ and
$O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds
for SFL training. We further extend the analysis to non-convex objectives and
where some clients may be unavailable during training. Numerical experiments
validate our theoretical results and show that SFL outperforms FL and split
learning (SL) when data is highly heterogeneous across a large number of
clients.
\\ ( https://arxiv.org/abs/2402.15166 ,  769kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15172 (*cross-listing*)
Date: Fri, 23 Feb 2024 08:11:25 GMT   (4069kb,D)

Title: Attention-Guided Masked Autoencoders For Learning Image Representations
Authors: Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski
Categories: cs.CV cs.LG
\\
  Masked autoencoders (MAEs) have established themselves as a powerful method
for unsupervised pre-training for computer vision tasks. While vanilla MAEs put
equal emphasis on reconstructing the individual parts of the image, we propose
to inform the reconstruction process through an attention-guided loss function.
By leveraging advances in unsupervised object discovery, we obtain an attention
map of the scene which we employ in the loss function to put increased emphasis
on reconstructing relevant objects, thus effectively incentivizing the model to
learn more object-focused representations without compromising the established
masking strategy. Our evaluations show that our pre-trained models learn better
latent representations than the vanilla MAE, demonstrated by improved linear
probing and k-NN classification results on several benchmarks while at the same
time making ViTs more robust against varying backgrounds.
\\ ( https://arxiv.org/abs/2402.15172 ,  4069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15213 (*cross-listing*)
Date: Fri, 23 Feb 2024 09:19:26 GMT   (8163kb,D)

Title: Statistical Agnostic Regression: a machine learning method to validate
  regression models
Authors: Juan M Gorriz, J. Ramirez, F. Segovia, F. J. Martinez-Murcia, C.
  Jim\'enez-Mesa and J. Suckling
Categories: stat.ML cs.LG math.ST stat.CO stat.TH
Comments: 17 pages, 15 figures
\\
  Regression analysis is a central topic in statistical modeling, aiming to
estimate the relationships between a dependent variable, commonly referred to
as the response variable, and one or more independent variables, i.e.,
explanatory variables. Linear regression is by far the most popular method for
performing this task in several fields of research, such as prediction,
forecasting, or causal inference. Beyond various classical methods to solve
linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso
regressions - which are often the foundation for more advanced machine learning
(ML) techniques - the latter have been successfully applied in this scenario
without a formal definition of statistical significance. At most, permutation
or classical analyses based on empirical measures (e.g., residuals or accuracy)
have been conducted to reflect the greater ability of ML estimations for
detection. In this paper, we introduce a method, named Statistical Agnostic
Regression (SAR), for evaluating the statistical significance of an ML-based
linear regression based on concentration inequalities of the actual risk using
the analysis of the worst case. To achieve this goal, similar to the
classification problem, we define a threshold to establish that there is
sufficient evidence with a probability of at least 1-eta to conclude that there
is a linear relationship in the population between the explanatory (feature)
and the response (label) variables. Simulations in only two dimensions
demonstrate the ability of the proposed agnostic test to provide a similar
analysis of variance given by the classical $F$ test for the slope parameter.
\\ ( https://arxiv.org/abs/2402.15213 ,  8163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15232 (*cross-listing*)
Date: Fri, 23 Feb 2024 09:47:42 GMT   (1309kb,D)

Title: Classification of compact radio sources in the Galactic plane with
  supervised machine learning
Authors: S. Riggi, G. Umana, C. Trigilio, C. Bordiu, F. Bufano, A. Ingallinera,
  F. Cavallaro, Y. Gordon, R.P. Norris, G. G\"urkan, P. Leto, C. Buemi, S.
  Loru, A.M. Hopkins, M.D. Filipovi\'c, T. Cecconello
Categories: astro-ph.IM cs.LG stat.ML
Comments: 27 pages, 15 figures, 9 tables
\\
  Generation of science-ready data from processed data products is one of the
major challenges in next-generation radio continuum surveys with the Square
Kilometre Array (SKA) and its precursors, due to the expected data volume and
the need to achieve a high degree of automated processing. Source extraction,
characterization, and classification are the major stages involved in this
process. In this work we focus on the classification of compact radio sources
in the Galactic plane using both radio and infrared images as inputs. To this
aim, we produced a curated dataset of ~20,000 images of compact sources of
different astronomical classes, obtained from past radio and infrared surveys,
and novel radio data from pilot surveys carried out with the Australian SKA
Pathfinder (ASKAP). Radio spectral index information was also obtained for a
subset of the data. We then trained two different classifiers on the produced
dataset. The first model uses gradient-boosted decision trees and is trained on
a set of pre-computed features derived from the data, which include
radio-infrared colour indices and the radio spectral index. The second model is
trained directly on multi-channel images, employing convolutional neural
networks. Using a completely supervised procedure, we obtained a high
classification accuracy (F1-score>90%) for separating Galactic objects from the
extragalactic background. Individual class discrimination performances, ranging
from 60% to 75%, increased by 10% when adding far-infrared and spectral index
information, with extragalactic objects, PNe and HII regions identified with
higher accuracies. The implemented tools and trained models were publicly
released, and made available to the radioastronomical community for future
application on new radio data.
\\ ( https://arxiv.org/abs/2402.15232 ,  1309kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15237 (*cross-listing*)
Date: Fri, 23 Feb 2024 10:01:22 GMT   (20321kb,D)

Title: Unsupervised Domain Adaptation for Brain Vessel Segmentation through
  Transwarp Contrastive Learning
Authors: Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou,
  Qiongyao Liu, Kun Wu, Nishant Ravikumar, Alejandro F. Frangi
Categories: cs.CV cs.LG
Comments: Accepted by ISBI 2024
\\
  Unsupervised domain adaptation (UDA) aims to align the labelled source
distribution with the unlabelled target distribution to obtain domain-invariant
predictive models. Since cross-modality medical data exhibit significant intra
and inter-domain shifts and most are unlabelled, UDA is more important while
challenging in medical image analysis. This paper proposes a simple yet potent
contrastive learning framework for UDA to narrow the inter-domain gap between
labelled source and unlabelled target distribution. Our method is validated on
cerebral vessel datasets. Experimental results show that our approach can learn
latent features from labelled 3DRA modality data and improve vessel
segmentation performance in unlabelled MRA modality data.
\\ ( https://arxiv.org/abs/2402.15237 ,  20321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15239 (*cross-listing*)
Date: Fri, 23 Feb 2024 10:02:15 GMT   (1728kb,D)

Title: GS-EMA: Integrating Gradient Surgery Exponential Moving Average with
  Boundary-Aware Contrastive Learning for Enhanced Domain Generalization in
  Aneurysm Segmentation
Authors: Fengming Lin, Yan Xia, Michael MacRaild, Yash Deo, Haoran Dou,
  Qiongyao Liu, Nina Cheng, Nishant Ravikumar, Alejandro F. Frangi
Categories: cs.CV cs.LG
Comments: Accepted by ISBI 2024
\\
  The automated segmentation of cerebral aneurysms is pivotal for accurate
diagnosis and treatment planning. Confronted with significant domain shifts and
class imbalance in 3D Rotational Angiography (3DRA) data from various medical
institutions, the task becomes challenging. These shifts include differences in
image appearance, intensity distribution, resolution, and aneurysm size, all of
which complicate the segmentation process. To tackle these issues, we propose a
novel domain generalization strategy that employs gradient surgery exponential
moving average (GS-EMA) optimization technique coupled with boundary-aware
contrastive learning (BACL). Our approach is distinct in its ability to adapt
to new, unseen domains by learning domain-invariant features, thereby improving
the robustness and accuracy of aneurysm segmentation across diverse clinical
datasets. The results demonstrate that our proposed approach can extract more
domain-invariant features, minimizing over-segmentation and capturing more
complete aneurysm structures.
\\ ( https://arxiv.org/abs/2402.15239 ,  1728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15258 (*cross-listing*)
Date: Fri, 23 Feb 2024 10:56:47 GMT   (200kb,D)

Title: High Resolution Guitar Transcription via Domain Adaptation
Authors: Xavier Riley, Drew Edwards, Simon Dixon
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
\\
  Automatic music transcription (AMT) has achieved high accuracy for piano due
to the availability of large, high-quality datasets such as MAESTRO and MAPS,
but comparable datasets are not yet available for other instruments. In recent
work, however, it has been demonstrated that aligning scores to transcription
model activations can produce high quality AMT training data for instruments
other than piano. Focusing on the guitar, we refine this approach to training
on score data using a dataset of commercially available score-audio pairs. We
propose the use of a high-resolution piano transcription model to train a new
guitar transcription model. The resulting model obtains state-of-the-art
transcription results on GuitarSet in a zero-shot context, improving on
previously published methods.
\\ ( https://arxiv.org/abs/2402.15258 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15259 (*cross-listing*)
Date: Fri, 23 Feb 2024 11:04:33 GMT   (8391kb,D)

Title: Open Ad Hoc Teamwork with Cooperative Game Theory
Authors: Jianhong Wang and Yang Li and Yuan Zhang and Wei Pan and Samuel Kaski
Categories: cs.MA cs.LG
Comments: 26 pages
\\
  Ad hoc teamwork poses a challenging problem, requiring the design of an agent
to collaborate with teammates without prior coordination or joint training.
Open ad hoc teamwork further complicates this challenge by considering
environments with a changing number of teammates, referred to as open teams.
The state-of-the-art solution to this problem is graph-based policy learning
(GPL), leveraging the generalizability of graph neural networks to handle an
unrestricted number of agents and effectively address open teams. GPL's
performance is superior to other methods, but its joint Q-value representation
presents challenges for interpretation, hindering further development of this
research line and applicability. In this paper, we establish a new theory to
give an interpretation for the joint Q-value representation employed in GPL,
from the perspective of cooperative game theory. Building on our theory, we
propose a novel algorithm based on GPL framework, to complement the critical
features that facilitate learning, but overlooked in GPL. Through experiments,
we demonstrate the correctness of our theory by comparing the performance of
the resulting algorithm with GPL in dynamic team compositions.
\\ ( https://arxiv.org/abs/2402.15259 ,  8391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15273 (*cross-listing*)
Date: Fri, 23 Feb 2024 11:35:57 GMT   (2482kb,D)

Title: Optimized Deployment of Deep Neural Networks for Visual Pose Estimation
  on Nano-drones
Authors: Matteo Risso, Francesco Daghero, Beatrice Alessandra Motetti, Daniele
  Jahier Pagliari, Enrico Macii, Massimo Poncino, and Alessio Burrello
Categories: cs.CV cs.LG
Comments: This paper has been accepted for publication in the ERF 2024
  conference
\\
  Miniaturized autonomous unmanned aerial vehicles (UAVs) are gaining
popularity due to their small size, enabling new tasks such as indoor
navigation or people monitoring. Nonetheless, their size and simple electronics
pose severe challenges in implementing advanced onboard intelligence. This work
proposes a new automatic optimization pipeline for visual pose estimation tasks
using Deep Neural Networks (DNNs). The pipeline leverages two different Neural
Architecture Search (NAS) algorithms to pursue a vast complexity-driven
exploration in the DNNs' architectural space. The obtained networks are then
deployed on an off-the-shelf nano-drone equipped with a parallel ultra-low
power System-on-Chip leveraging a set of novel software kernels for the
efficient fused execution of critical DNN layer sequences. Our results improve
the state-of-the-art reducing inference latency by up to 3.22x at iso-error.
\\ ( https://arxiv.org/abs/2402.15273 ,  2482kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15281 (*cross-listing*)
Date: Fri, 23 Feb 2024 12:06:48 GMT   (4279kb,D)

Title: Neural Implicit Swept Volume Models for Fast Collision Detection
Authors: Dominik Joho, Jonas Schwinn, Kirill Safronov
Categories: cs.RO cs.LG
Comments: To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have
  equal contribution
\\
  Collision detection is one of the most time-consuming operations during
motion planning. Thus, there is an increasing interest in exploring machine
learning techniques to speed up collision detection and sampling-based motion
planning. A recent line of research focuses on utilizing neural signed distance
functions of either the robot geometry or the swept volume of the robot motion.
Building on this, we present a novel neural implicit swept volume model that is
the first to continuously represent arbitrary motions parameterized by their
start and goal configurations. This allows to quickly compute signed distances
for any point in the task space to the robot motion. Further, we present an
algorithm combining the speed of the deep learning-based signed distance
computations with the strong accuracy guarantees of geometric collision
checkers. We validate our approach in simulated and real-world robotic
experiments, and demonstrate that it is able to speed up a commercial bin
picking application.
\\ ( https://arxiv.org/abs/2402.15281 ,  4279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15285 (*cross-listing*)
Date: Fri, 23 Feb 2024 12:30:20 GMT   (1986kb,D)

Title: Generative Modelling with Tensor Train approximations of
  Hamilton--Jacobi--Bellman equations
Authors: David Sommer, Robert Gruhlke, Max Kirstein, Martin Eigel, Claudia
  Schillings
Categories: stat.ML cs.LG math.ST stat.TH
MSC-class: 35F21, 35Q84, 62F15, 65N75, 65C30
\\
  Sampling from probability densities is a common challenge in fields such as
Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in
particular, the use of reverse-time diffusion processes depending on the
log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling
tool. In Berner et al. [2022] the authors point out that these log-densities
can be obtained by solution of a \textit{Hamilton-Jacobi-Bellman} (HJB)
equation known from stochastic optimal control. While this HJB equation is
usually treated with indirect methods such as policy iteration and unsupervised
training of black-box architectures like Neural Networks, we propose instead to
solve the HJB equation by direct time integration, using compressed polynomials
represented in the Tensor Train (TT) format for spatial discretization.
Crucially, this method is sample-free, agnostic to normalization constants and
can avoid the curse of dimensionality due to the TT compression. We provide a
complete derivation of the HJB equation's action on Tensor Train polynomials
and demonstrate the performance of the proposed time-step-, rank- and
degree-adaptive integration method on a nonlinear sampling task in 20
dimensions.
\\ ( https://arxiv.org/abs/2402.15285 ,  1986kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15288 (*cross-listing*)
Date: Fri, 23 Feb 2024 12:33:27 GMT   (1588kb,D)

Title: Real-Time FPGA Demonstrator of ANN-Based Equalization for Optical
  Communications
Authors: Jonas Ney, Patrick Matalla, Vincent Lauinger, Laurent Schmalen,
  Sebastian Randel, Norbert Wehn
Categories: eess.SP cs.LG
Comments: Accepted and to be presented as demonstrator at the IEEE
  International Conference on Machine Learning for Communication and Networking
  (ICMLCN) 2024
\\
  In this work, we present a high-throughput field programmable gate array
(FPGA) demonstrator of an artificial neural network (ANN)-based equalizer. The
equalization is performed and illustrated in real-time for a 30 GBd, two-level
pulse amplitude modulation (PAM2) optical communication system.
\\ ( https://arxiv.org/abs/2402.15288 ,  1588kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15297 (*cross-listing*)
Date: Fri, 23 Feb 2024 12:48:02 GMT   (14837kb,D)

Title: Semi-supervised Counting via Pixel-by-pixel Density Distribution
  Modelling
Authors: Hui Lin and Zhiheng Ma and Rongrong Ji and Yaowei Wang and Zhou Su and
  Xiaopeng Hong and Deyu Meng
Categories: cs.CV cs.LG
Comments: This is the technical report of a paper that was submitted to IEEE
  Transactions and is now under review
\\
  This paper focuses on semi-supervised crowd counting, where only a small
portion of the training data are labeled. We formulate the pixel-wise density
value to regress as a probability distribution, instead of a single
deterministic value. On this basis, we propose a semi-supervised crowd-counting
model. Firstly, we design a pixel-wise distribution matching loss to measure
the differences in the pixel-wise density distributions between the prediction
and the ground truth; Secondly, we enhance the transformer decoder by using
density tokens to specialize the forwards of decoders w.r.t. different density
intervals; Thirdly, we design the interleaving consistency self-supervised
learning mechanism to learn from unlabeled data efficiently. Extensive
experiments on four datasets are performed to show that our method clearly
outperforms the competitors by a large margin under various labeled ratio
settings. Code will be released at
https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.
\\ ( https://arxiv.org/abs/2402.15297 ,  14837kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15324 (*cross-listing*)
Date: Fri, 23 Feb 2024 13:43:15 GMT   (58384kb,D)

Title: Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method
  and Its Application to Energy Network
Authors: Jianhong Wang
Categories: cs.MA cs.LG
Comments: 206 pages
DOI: 10.25560/109306
\\
  Multi-agent reinforcement learning is an area of rapid advancement in
artificial intelligence and machine learning. One of the important questions to
be answered is how to conduct credit assignment in a multi-agent system. There
have been many schemes designed to conduct credit assignment by multi-agent
reinforcement learning algorithms. Although these credit assignment schemes
have been proved useful in improving the performance of multi-agent
reinforcement learning, most of them are designed heuristically without a
rigorous theoretic basis and therefore infeasible to understand how agents
cooperate. In this thesis, we aim at investigating the foundation of credit
assignment in multi-agent reinforcement learning via cooperative game theory.
We first extend a game model called convex game and a payoff distribution
scheme called Shapley value in cooperative game theory to Markov decision
process, named as Markov convex game and Markov Shapley value respectively. We
represent a global reward game as a Markov convex game under the grand
coalition. As a result, Markov Shapley value can be reasonably used as a credit
assignment scheme in the global reward game. Markov Shapley value possesses the
following virtues: (i) efficiency; (ii) identifiability of dummy agents; (iii)
reflecting the contribution and (iv) symmetry, which form the fair credit
assignment. Based on Markov Shapley value, we propose three multi-agent
reinforcement learning algorithms called SHAQ, SQDDPG and SMFPPO. Furthermore,
we extend Markov convex game to partial observability to deal with the
partially observable problems, named as partially observable Markov convex
game. In application, we evaluate SQDDPG and SMFPPO on the real-world problem
in energy networks.
\\ ( https://arxiv.org/abs/2402.15324 ,  58384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15335 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:15:58 GMT   (16795kb,D)

Title: Low-Rank Representations Meets Deep Unfolding: A Generalized and
  Interpretable Network for Hyperspectral Anomaly Detection
Authors: Chenyu Li and Bing Zhang and Danfeng Hong and Jing Yao and Jocelyn
  Chanussot
Categories: eess.IV cs.CV cs.LG
\\
  Current hyperspectral anomaly detection (HAD) benchmark datasets suffer from
low resolution, simple background, and small size of the detection data. These
factors also limit the performance of the well-known low-rank representation
(LRR) models in terms of robustness on the separation of background and target
features and the reliance on manual parameter selection. To this end, we build
a new set of HAD benchmark datasets for improving the robustness of the HAD
algorithm in complex scenarios, AIR-HAD for short. Accordingly, we propose a
generalized and interpretable HAD network by deeply unfolding a
dictionary-learnable LLR model, named LRR-Net$^+$, which is capable of
spectrally decoupling the background structure and object properties in a more
generalized fashion and eliminating the bias introduced by vital interference
targets concurrently. In addition, LRR-Net$^+$ integrates the solution process
of the Alternating Direction Method of Multipliers (ADMM) optimizer with the
deep network, guiding its search process and imparting a level of
interpretability to parameter optimization. Additionally, the integration of
physical models with DL techniques eliminates the need for manual parameter
tuning. The manually tuned parameters are seamlessly transformed into trainable
parameters for deep neural networks, facilitating a more efficient and
automated optimization process. Extensive experiments conducted on the AIR-HAD
dataset show the superiority of our LRR-Net$^+$ in terms of detection
performance and generalization ability, compared to top-performing rivals.
Furthermore, the compilable codes and our AIR-HAD benchmark datasets in this
paper will be made available freely and openly at
\url{https://sites.google.com/view/danfeng-hong}.
\\ ( https://arxiv.org/abs/2402.15335 ,  16795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15344 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:24:45 GMT   (135kb,D)

Title: Iteration and Stochastic First-order Oracle Complexities of Stochastic
  Gradient Descent using Constant and Decaying Learning Rates
Authors: Kento Imaizumi, Hideaki Iiduka
Categories: stat.ML cs.LG
Comments: The latest version was updated on Feb. 23. arXiv admin note: text
  overlap with arXiv:2307.13831
\\
  The performance of stochastic gradient descent (SGD), which is the simplest
first-order optimizer for training deep neural networks, depends on not only
the learning rate but also the batch size. They both affect the number of
iterations and the stochastic first-order oracle (SFO) complexity needed for
training. In particular, the previous numerical results indicated that, for SGD
using a constant learning rate, the number of iterations needed for training
decreases when the batch size increases, and the SFO complexity needed for
training is minimized at a critical batch size and that it increases once the
batch size exceeds that size. Here, we study the relationship between batch
size and the iteration and SFO complexities needed for nonconvex optimization
in deep learning with SGD using constant or decaying learning rates and show
that SGD using the critical batch size minimizes the SFO complexity. We also
provide numerical comparisons of SGD with the existing first-order optimizers
and show the usefulness of SGD using a critical batch size. Moreover, we show
that measured critical batch sizes are close to the sizes estimated from our
theoretical results.
\\ ( https://arxiv.org/abs/2402.15344 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15352 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:39:12 GMT   (4396kb,D)

Title: On normalization-equivariance properties of supervised and unsupervised
  denoising methods: a survey
Authors: S\'ebastien Herbreteau and Charles Kervrann
Categories: cs.CV cs.LG
\\
  Image denoising is probably the oldest and still one of the most active
research topic in image processing. Many methodological concepts have been
introduced in the past decades and have improved performances significantly in
recent years, especially with the emergence of convolutional neural networks
and supervised deep learning. In this paper, we propose a survey of guided tour
of supervised and unsupervised learning methods for image denoising,
classifying the main principles elaborated during this evolution, with a
particular concern given to recent developments in supervised learning. It is
conceived as a tutorial organizing in a comprehensive framework current
approaches. We give insights on the rationales and limitations of the most
performant methods in the literature, and we highlight the common features
between many of them. Finally, we focus on on the normalization equivariance
properties that is surprisingly not guaranteed with most of supervised methods.
It is of paramount importance that intensity shifting or scaling applied to the
input image results in a corresponding change in the denoiser output.
\\ ( https://arxiv.org/abs/2402.15352 ,  4396kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15359 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:52:05 GMT   (7121kb,D)

Title: Streaming Gaussian Dirichlet Random Fields for Spatial Predictions of
  High Dimensional Categorical Observations
Authors: J. E. San Soucie, H. M. Sosik, Y. Girdhar
Categories: cs.RO cs.LG
Comments: 10 pages, 5 figures. Published in Springer Proceedings of Advanced
  Robotics, ISER 2023 Conference Proceedings
\\
  We present the Streaming Gaussian Dirichlet Random Field (S-GDRF) model, a
novel approach for modeling a stream of spatiotemporally distributed, sparse,
high-dimensional categorical observations. The proposed approach efficiently
learns global and local patterns in spatiotemporal data, allowing for fast
inference and querying with a bounded time complexity. Using a high-resolution
data series of plankton images classified with a neural network, we demonstrate
the ability of the approach to make more accurate predictions compared to a
Variational Gaussian Process (VGP), and to learn a predictive distribution of
observations from streaming categorical data. S-GDRFs open the door to enabling
efficient informative path planning over high-dimensional categorical
observations, which until now has not been feasible.
\\ ( https://arxiv.org/abs/2402.15359 ,  7121kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15360 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:52:44 GMT   (297kb,D)

Title: All Thresholds Barred: Direct Estimation of Call Density in Bioacoustic
  Data
Authors: Amanda K. Navine, Tom Denton, Matthew J. Weldy, Patrick J. Hart
Categories: q-bio.QM cs.LG cs.SD eess.AS
Comments: 14 pages, 6 figures, 3 tables; submitted to Frontiers in Bird
  Science; Our Hawaiian PAM dataset and classifier scores, as well as
  annotation information for the three study species, can be found on Zenodo at
  https://doi.org/10.5281/zenodo.10581530. The fully annotated Powdermill
  dataset assembled by Chronister et al. that was used in this study is
  available at https://doi.org/10.1002/ecy.3329
\\
  Passive acoustic monitoring (PAM) studies generate thousands of hours of
audio, which may be used to monitor specific animal populations, conduct broad
biodiversity surveys, detect threats such as poachers, and more. Machine
learning classifiers for species identification are increasingly being used to
process the vast amount of audio generated by bioacoustic surveys, expediting
analysis and increasing the utility of PAM as a management tool. In common
practice, a threshold is applied to classifier output scores, and scores above
the threshold are aggregated into a detection count. The choice of threshold
produces biased counts of vocalizations, which are subject to false
positive/negative rates that may vary across subsets of the dataset. In this
work, we advocate for directly estimating call density: The proportion of
detection windows containing the target vocalization, regardless of classifier
score. Our approach targets a desirable ecological estimator and provides a
more rigorous grounding for identifying the core problems caused by
distribution shifts -- when the defining characteristics of the data
distribution change -- and designing strategies to mitigate them. We propose a
validation scheme for estimating call density in a body of data and obtain,
through Bayesian reasoning, probability distributions of confidence scores for
both the positive and negative classes. We use these distributions to predict
site-level densities, which may be subject to distribution shifts. We test our
proposed methods on a real-world study of Hawaiian birds and provide simulation
results leveraging existing fully annotated datasets, demonstrating robustness
to variations in call density and classifier model quality.
\\ ( https://arxiv.org/abs/2402.15360 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15365 (*cross-listing*)
Date: Fri, 23 Feb 2024 14:55:58 GMT   (22kb,D)

Title: Efficient semi-supervised inference for logistic regression under
  case-control studies
Authors: Zhuojun Quan, Yuanyuan Lin, Kani Chen, Wen Yu
Categories: stat.ML cs.LG
\\
  Semi-supervised learning has received increasingly attention in statistics
and machine learning. In semi-supervised learning settings, a labeled data set
with both outcomes and covariates and an unlabeled data set with covariates
only are collected. We consider an inference problem in semi-supervised
settings where the outcome in the labeled data is binary and the labeled data
is collected by case-control sampling. Case-control sampling is an effective
sampling scheme for alleviating imbalance structure in binary data. Under the
logistic model assumption, case-control data can still provide consistent
estimator for the slope parameter of the regression model. However, the
intercept parameter is not identifiable. Consequently, the marginal case
proportion cannot be estimated from case-control data. We find out that with
the availability of the unlabeled data, the intercept parameter can be
identified in semi-supervised learning setting. We construct the likelihood
function of the observed labeled and unlabeled data and obtain the maximum
likelihood estimator via an iterative algorithm. The proposed estimator is
shown to be consistent, asymptotically normal, and semiparametrically
efficient. Extensive simulation studies are conducted to show the finite sample
performance of the proposed method. The results imply that the unlabeled data
not only helps to identify the intercept but also improves the estimation
efficiency of the slope parameter. Meanwhile, the marginal case proportion can
be estimated accurately by the proposed method.
\\ ( https://arxiv.org/abs/2402.15365 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15374 (*cross-listing*)
Date: Fri, 23 Feb 2024 15:19:37 GMT   (13520kb,D)

Title: Outlier detection by ensembling uncertainty with negative objectness
Authors: Anja Deli\'c, Matej Grci\'c and Sini\v{s}a \v{S}egvi\'c
Categories: cs.CV cs.LG
\\
  Outlier detection is an essential capability in safety-critical applications
of supervised visual recognition. Most of the existing methods deliver best
results by encouraging standard closed-set models to produce low-confidence
predictions in negative training data. However, that approach conflates
prediction uncertainty with recognition of the negative class. We therefore
reconsider direct prediction of K+1 logits that correspond to K groundtruth
classes and one outlier class. This setup allows us to formulate a novel
anomaly score as an ensemble of in-distribution uncertainty and the posterior
of the outlier class which we term negative objectness. Now outliers can be
independently detected due to i) high prediction uncertainty or ii) similarity
with negative data. We embed our method into a dense prediction architecture
with mask-level recognition over K+2 classes. The training procedure encourages
the novel K+2-th class to learn negative objectness at pasted negative
instances. Our models outperform the current state-of-the art on standard
benchmarks for image-wide and pixel-level outlier detection with and without
training on real negative data.
\\ ( https://arxiv.org/abs/2402.15374 ,  13520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15402 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:05:51 GMT   (11914kb,D)

Title: Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy
  Structure Prior
Authors: Kechun Xu, Zhongxiang Zhou, Jun Wu, Haojian Lu, Rong Xiong, Yue Wang
Categories: cs.RO cs.LG
\\
  We focus on the task of unknown object rearrangement, where a robot is
supposed to re-configure the objects into a desired goal configuration
specified by an RGB-D image. Recent works explore unknown object rearrangement
systems by incorporating learning-based perception modules. However, they are
sensitive to perception error, and pay less attention to task-level
performance. In this paper, we aim to develop an effective system for unknown
object rearrangement amidst perception noise. We theoretically reveal the noisy
perception impacts grasp and place in a decoupled way, and show such a
decoupled structure is non-trivial to improve task optimality. We propose GSP,
a dual-loop system with the decoupled structure as prior. For the inner loop,
we learn an active seeing policy for self-confident object matching to improve
the perception of place. For the outer loop, we learn a grasp policy aware of
object matching and grasp capability guided by task-level rewards. We leverage
the foundation model CLIP for object matching, policy learning and
self-termination. A series of experiments indicate that GSP can conduct unknown
object rearrangement with higher completion rate and less steps.
\\ ( https://arxiv.org/abs/2402.15402 ,  11914kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15409 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:16:38 GMT   (285kb,D)

Title: Lasso with Latents: Efficient Estimation, Covariate Rescaling, and
  Computational-Statistical Gaps
Authors: Jonathan Kelner, Frederic Koehler, Raghu Meka, Dhruv Rohatgi
Categories: stat.ML cs.CC cs.DS cs.LG math.ST stat.TH
\\
  It is well-known that the statistical performance of Lasso can suffer
significantly when the covariates of interest have strong correlations. In
particular, the prediction error of Lasso becomes much worse than
computationally inefficient alternatives like Best Subset Selection. Due to a
large conjectured computational-statistical tradeoff in the problem of sparse
linear regression, it may be impossible to close this gap in general.
  In this work, we propose a natural sparse linear regression setting where
strong correlations between covariates arise from unobserved latent variables.
In this setting, we analyze the problem caused by strong correlations and
design a surprisingly simple fix. While Lasso with standard normalization of
covariates fails, there exists a heterogeneous scaling of the covariates with
which Lasso will suddenly obtain strong provable guarantees for estimation.
Moreover, we design a simple, efficient procedure for computing such a "smart
scaling."
  The sample complexity of the resulting "rescaled Lasso" algorithm incurs (in
the worst case) quadratic dependence on the sparsity of the underlying signal.
While this dependence is not information-theoretically necessary, we give
evidence that it is optimal among the class of polynomial-time algorithms, via
the method of low-degree polynomials. This argument reveals a new connection
between sparse linear regression and a special version of sparse PCA with a
near-critical negative spike. The latter problem can be thought of as a
real-valued analogue of learning a sparse parity. Using it, we also establish
the first computational-statistical gap for the closely related problem of
learning a Gaussian Graphical Model.
\\ ( https://arxiv.org/abs/2402.15409 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15430 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:50:07 GMT   (8248kb,D)

Title: Hierarchical Invariance for Robust and Interpretable Vision Tasks at
  Larger Scales
Authors: Shuren Qi, Yushu Zhang, Chao Wang, Zhihua Xia, Jian Weng, Xiaochun Cao
Categories: cs.CV cs.LG
\\
  Developing robust and interpretable vision systems is a crucial step towards
trustworthy artificial intelligence. In this regard, a promising paradigm
considers embedding task-required invariant structures, e.g., geometric
invariance, in the fundamental image representation. However, such invariant
representations typically exhibit limited discriminability, limiting their
applications in larger-scale trustworthy vision tasks. For this open problem,
we conduct a systematic investigation of hierarchical invariance, exploring
this topic from theoretical, practical, and application perspectives. At the
theoretical level, we show how to construct over-complete invariants with a
Convolutional Neural Networks (CNN)-like hierarchical architecture yet in a
fully interpretable manner. The general blueprint, specific definitions,
invariant properties, and numerical implementations are provided. At the
practical level, we discuss how to customize this theoretical framework into a
given task. With the over-completeness, discriminative features w.r.t. the task
can be adaptively formed in a Neural Architecture Search (NAS)-like manner. We
demonstrate the above arguments with accuracy, invariance, and efficiency
results on texture, digit, and parasite classification experiments.
Furthermore, at the application level, our representations are explored in
real-world forensics tasks on adversarial perturbations and Artificial
Intelligence Generated Content (AIGC). Such applications reveal that the
proposed strategy not only realizes the theoretically promised invariance, but
also exhibits competitive discriminability even in the era of deep learning.
For robust and interpretable vision tasks at larger scales, hierarchical
invariant representation can be considered as an effective alternative to
traditional CNN and invariants.
\\ ( https://arxiv.org/abs/2402.15430 ,  8248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15432 (*cross-listing*)
Date: Fri, 23 Feb 2024 16:51:17 GMT   (44kb)

Title: Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering
  Error in Sub-Exponential Mixture Models
Authors: Maximilien Dreveton, Alperen G\"ozeten, Matthias Grossglauser, Patrick
  Thiran
Categories: math.ST cs.LG stat.ML stat.TH
MSC-class: 62H30, 62F12, 62B10
\\
  Clustering is a pivotal challenge in unsupervised machine learning and is
often investigated through the lens of mixture models. The optimal error rate
for recovering cluster labels in Gaussian and sub-Gaussian mixture models
involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as
Lloyd's algorithm, attain this optimal error rate. In this paper, we first
establish a universal lower bound for the error rate in clustering any mixture
model, expressed through a Chernoff divergence, a more versatile measure of
model information than signal-to-noise ratios. We then demonstrate that
iterative algorithms attain this lower bound in mixture models with
sub-exponential tails, notably emphasizing location-scale mixtures featuring
Laplace-distributed errors. Additionally, for datasets better modelled by
Poisson or Negative Binomial mixtures, we study mixture models whose
distributions belong to an exponential family. In such mixtures, we establish
that Bregman hard clustering, a variant of Lloyd's algorithm employing a
Bregman divergence, is rate optimal.
\\ ( https://arxiv.org/abs/2402.15432 ,  44kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1905.00517
replaced with revised version Thu, 22 Feb 2024 23:07:35 GMT   (8404kb,D)

Title: From Abstractions to Grounded Languages for Robust Coordination of Task
  Planning Robots
Authors: Yu Zhang
Categories: cs.AI
Comments: A short version of this paper appears as an extended abstract at
  AAMAS 2023
\\ ( https://arxiv.org/abs/1905.00517 ,  8404kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09200
replaced with revised version Fri, 23 Feb 2024 16:45:05 GMT   (31kb)

Title: Representing states in iterated belief revision
Authors: Paolo Liberatore
Categories: cs.AI
\\ ( https://arxiv.org/abs/2305.09200 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06714
replaced with revised version Thu, 22 Feb 2024 21:19:59 GMT   (30928kb,D)

Title: Exploring Memorization in Fine-tuned Language Models
Authors: Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue
  Xing, Shuaiqiang Wang, Jiliang Tang, Dawei Yin
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2310.06714 ,  30928kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11846
replaced with revised version Fri, 23 Feb 2024 02:11:14 GMT   (7185kb,D)

Title: MaskMA: Towards Zero-Shot Multi-Agent Decision Making with Mask-Based
  Collaborative Learning
Authors: Jie Liu, Yinmin Zhang, Chuming Li, Chao Yang, Yaodong Yang, Yu Liu,
  Wanli Ouyang
Categories: cs.AI
Comments: 17 pages
\\ ( https://arxiv.org/abs/2310.11846 ,  7185kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04254
replaced with revised version Fri, 23 Feb 2024 15:09:58 GMT   (1496kb,D)

Title: Everything of Thoughts: Defying the Law of Penrose Triangle for Thought
  Generation
Authors: Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang,
  Si Qin, Saravan Rajmohan, Qingwei Lin and Dongmei Zhang
Categories: cs.AI cs.LG
Comments: 17 pages, 5 figures
\\ ( https://arxiv.org/abs/2311.04254 ,  1496kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04772
replaced with revised version Fri, 23 Feb 2024 05:38:34 GMT   (105kb,D)

Title: Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision
  Making
Authors: Parand A. Alamdari, Toryn Q. Klassen, Elliot Creager, Sheila A.
  McIlraith
Categories: cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2312.04772 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11434
replaced with revised version Fri, 23 Feb 2024 17:35:41 GMT   (117kb)

Title: Factored Online Planning in Many-Agent POMDPs
Authors: Maris F.L. Galesloot, Thiago D. Sim\~ao, Sebastian Junges, Nils Jansen
Categories: cs.AI cs.MA
Comments: Extended version (includes the Appendix) of the paper accepted at
  AAAI-24
\\ ( https://arxiv.org/abs/2312.11434 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10877
replaced with revised version Fri, 23 Feb 2024 10:50:13 GMT   (2637kb,D)

Title: Robust agents learn causal world models
Authors: Jonathan Richens, Tom Everitt
Categories: cs.AI cs.LG
Comments: ICLR 2024 (oral)
\\ ( https://arxiv.org/abs/2402.10877 ,  2637kb)
------------------------------------------------------------------------------
\\
arXiv:2212.09400
replaced with revised version Fri, 23 Feb 2024 01:45:32 GMT   (7476kb,D)

Title: Medical Knowledge Graph QA for Drug-Drug Interaction Prediction based on
  Multi-hop Machine Reading Comprehension
Authors: Peng Gao, Feng Gao, Jian-Cheng Ni, Yu Wang, Fei Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2212.09400 ,  7476kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10233
replaced with revised version Fri, 23 Feb 2024 04:45:40 GMT   (7041kb,D)

Title: Pre-trained Language Models for Keyphrase Generation: A Thorough
  Empirical Study
Authors: Di Wu, Wasi Uddin Ahmad, Kai-Wei Chang
Categories: cs.CL
Comments: Technical Report. The contents are published in two separate papers
  in EMNLP 2023 (arXiv:2310.06374) and LREC-COLING 2024 (arXiv:2402.14052)
\\ ( https://arxiv.org/abs/2212.10233 ,  7041kb)
------------------------------------------------------------------------------
\\
arXiv:2301.00068
replaced with revised version Fri, 23 Feb 2024 05:08:58 GMT   (3959kb,D)

Title: Inconsistencies in Masked Language Models
Authors: Tom Young, Yunan Chen, Yang You
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2301.00068 ,  3959kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05279
replaced with revised version Fri, 23 Feb 2024 14:40:15 GMT   (60kb,D)

Title: Can large language models build causal graphs?
Authors: Stephanie Long, Tibor Schuster, Alexandre Pich\'e
Categories: cs.CL cs.AI
Comments: Peer reviewed and accepted for presentation at the Causal Machine
  Learning for Real-World Impact Workshop (CML4Impact) at NeuRIPs2022 Fixed
  author list
\\ ( https://arxiv.org/abs/2303.05279 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16755
replaced with revised version Thu, 22 Feb 2024 22:29:10 GMT   (595kb,D)

Title: Training Language Models with Language Feedback at Scale
Authors: J\'er\'emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan,
  Angelica Chen, Kyunghyun Cho, Ethan Perez
Categories: cs.CL cs.AI cs.LG
Comments: Published in TMLR: https://openreview.net/forum?id=xo3hI5MwvU
\\ ( https://arxiv.org/abs/2303.16755 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00468
replaced with revised version Fri, 23 Feb 2024 02:05:44 GMT   (1287kb)

Title: Words that Matter: The Impact of Negative Words on News Sentiment and
  Stock Market Index
Authors: Wonseong Kim
Categories: cs.CL
Comments: 33 pages, 9 figures, 7 tables
\\ ( https://arxiv.org/abs/2304.00468 ,  1287kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08177
replaced with revised version Fri, 23 Feb 2024 02:22:36 GMT   (282kb,D)

Title: Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca
Authors: Yiming Cui, Ziqing Yang, Xin Yao
Categories: cs.CL cs.HC cs.LG
Comments: 21 pages
\\ ( https://arxiv.org/abs/2304.08177 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03237
replaced with revised version Fri, 23 Feb 2024 09:13:30 GMT   (1042kb,D)

Title: Out-of-Domain Intent Detection Considering Multi-Turn Dialogue Contexts
Authors: Hao Lang, Yinhe Zheng, Binyuan Hui, Fei Huang, Yongbin Li
Categories: cs.CL cs.AI cs.LG
Comments: COLING2024 Long Paper
\\ ( https://arxiv.org/abs/2305.03237 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09651
replaced with revised version Fri, 23 Feb 2024 11:09:29 GMT   (9731kb,D)

Title: Tailoring Instructions to Student's Learning Levels Boosts Knowledge
  Distillation
Authors: Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, Mu Li
Categories: cs.CL cs.LG
Comments: Accepted at ACL 2023, main conference. Code available at
  https://github.com/twinkle0331/LGTM
\\ ( https://arxiv.org/abs/2305.09651 ,  9731kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14791
replaced with revised version Fri, 23 Feb 2024 07:02:09 GMT   (1928kb,D)

Title: Prompting Large Language Models for Counterfactual Generation: An
  Empirical Study
Authors: Yongqi Li, Mayi Xu, Xin Miao, Shen Zhou, Tieyun Qian
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024, camera ready version
\\ ( https://arxiv.org/abs/2305.14791 ,  1928kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14828
replaced with revised version Fri, 23 Feb 2024 05:36:02 GMT   (2439kb,D)

Title: Towards Few-shot Entity Recognition in Document Images: A Graph Neural
  Network Approach Robust to Image Manipulation
Authors: Prashant Krishnan, Zilong Wang, Yangkun Wang and Jingbo Shang
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2305.14828 ,  2439kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15060
replaced with revised version Fri, 23 Feb 2024 08:32:10 GMT   (1788kb,D)

Title: Who Wrote this Code? Watermarking for Code Generation
Authors: Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo
  Yun, Jamin Shin, Gunhee Kim
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.15060 ,  1788kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15459
replaced with revised version Thu, 22 Feb 2024 23:38:26 GMT   (322kb,D)

Title: ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style
  Transfer
Authors: Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen
  McKeown
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2308.15459 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04198
replaced with revised version Fri, 23 Feb 2024 11:58:10 GMT   (9904kb,D)

Title: Don't Ignore Dual Logic Ability of LLMs while Privatizing: A
  Data-Intensive Analysis in Medical Domain
Authors: Yanrui Du, Sendong Zhao, Muzhen Cai, Ming Ma, Danyang Zhao, Jiawei
  Cao, Bing Qin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.04198 ,  9904kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15372
replaced with revised version Thu, 22 Feb 2024 20:59:54 GMT   (1728kb,D)

Title: EpiK-Eval: Evaluation for Language Models as Epistemic Models
Authors: Gabriele Prato, Jerry Huang, Prasannna Parthasarathi, Shagun Sodhani,
  Sarath Chandar
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.15372 ,  1728kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07215
replaced with revised version Fri, 23 Feb 2024 06:56:16 GMT   (3979kb,D)

Title: Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback
Authors: Seungjun Moon, Hyungjoo Chae, Yongho Song, Taeyoon Kwon, Dongjin Kang,
  Kai Tzu-iunn Ong, Seung-won Hwang, Jinyoung Yeo
Categories: cs.CL cs.SE
Comments: Work in progress
\\ ( https://arxiv.org/abs/2311.07215 ,  3979kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07397
replaced with revised version Fri, 23 Feb 2024 07:54:11 GMT   (8916kb,D)

Title: AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination
  Evaluation
Authors: Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao
  Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, Jitao Sang
Categories: cs.CL cs.CV
Comments: 14 pages, 9 figures
\\ ( https://arxiv.org/abs/2311.07397 ,  8916kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08045
replaced with revised version Fri, 23 Feb 2024 08:58:34 GMT   (1393kb,D)

Title: Adversarial Preference Optimization
Authors: Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, Tianhao Hu, Peixin Cao,
  Nan Du
Categories: cs.CL cs.AI cs.LG
Comments: In process
\\ ( https://arxiv.org/abs/2311.08045 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08838
replaced with revised version Fri, 23 Feb 2024 10:44:18 GMT   (7870kb,D)

Title: Disinformation Capabilities of Large Language Models
Authors: Ivan Vykopal, Mat\'u\v{s} Pikuliak, Ivan Srba, Robert Moro, Dominik
  Macko, Maria Bielikova
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08838 ,  7870kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01957
replaced with revised version Fri, 23 Feb 2024 17:03:19 GMT   (330kb,D)

Title: Distilled Self-Critique of LLMs with Synthetic Data: a Bayesian
  Perspective
Authors: Victor Gallego
Categories: cs.CL cs.LG
Comments: Submitted to ICLR 2024 (TinyPapers track)
\\ ( https://arxiv.org/abs/2312.01957 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04127
replaced with revised version Fri, 23 Feb 2024 07:32:27 GMT   (9848kb,D)

Title: Analyzing the Inherent Response Tendency of LLMs: Real-World
  Instructions-Driven Jailbreak
Authors: Yanrui Du, Sendong Zhao, Ming Ma, Yuhan Chen, Bing Qin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.04127 ,  9848kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06522
replaced with revised version Fri, 23 Feb 2024 02:36:50 GMT   (2618kb,D)

Title: Revisiting the Role of Label Smoothing in Enhanced Text Sentiment
  Classification
Authors: Yijie Gao, Shijing Si, Hua Luo, Haixia Sun, Yugui Zhang
Categories: cs.CL cs.AI cs.LG
Comments: Technical Report
\\ ( https://arxiv.org/abs/2312.06522 ,  2618kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07622
replaced with revised version Fri, 23 Feb 2024 14:00:04 GMT   (5155kb,D)

Title: Mathematical Language Models: A Survey
Authors: Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng,
  Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou and Liang He
Categories: cs.CL
Comments: arXiv admin note: text overlap with arXiv:1705.04146,
  arXiv:2304.10977, arXiv:2112.00114, arXiv:1905.13319, arXiv:2304.12244,
  arXiv:2206.01347, arXiv:2006.09265 by other authors
\\ ( https://arxiv.org/abs/2312.07622 ,  5155kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13933
replaced with revised version Fri, 23 Feb 2024 11:53:13 GMT   (1571kb,D)

Title: Structured Probabilistic Coding
Authors: Dou Hu, Lingwei Wei, Yaxin Liu, Wei Zhou, Songlin Hu
Categories: cs.CL cs.LG
Comments: 11 pages, accepted by AAAI 2024 (Oral)
\\ ( https://arxiv.org/abs/2312.13933 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15304
replaced with revised version Fri, 23 Feb 2024 05:24:26 GMT   (459kb,D)

Title: Exploring the Capabilities of ChatGPT in Ancient Chinese Translation and
  Person Name Recognition
Authors: Shijing Si, Siqing Zhou, Le Tang, Xiaoqing Cheng, Yugui Zhang
Categories: cs.CL cs.AI
Comments: Technical report
\\ ( https://arxiv.org/abs/2312.15304 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03855
replaced with revised version Fri, 23 Feb 2024 04:29:06 GMT   (11277kb,D)

Title: PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
Authors: Ankit Yadav, Mayank Singh
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.03855 ,  11277kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06477
replaced with revised version Fri, 23 Feb 2024 12:48:46 GMT   (10043kb,D)

Title: Kun: Answer Polishment for Chinese Self-Alignment with Instruction
  Back-Translation
Authors: Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Weixu Zhang, Xinrun
  Du, Qi Jia, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu, and Ge Zhang
Categories: cs.CL cs.AI
Comments: 12 pages, 12 figures
\\ ( https://arxiv.org/abs/2401.06477 ,  10043kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12713
replaced with revised version Fri, 23 Feb 2024 15:01:38 GMT   (318kb,D)

Title: Generating Zero-shot Abstractive Explanations for Rumour Verification
Authors: Iman Munire Bilal, Preslav Nakov, Rob Procter, Maria Liakata
Categories: cs.CL
Comments: Revised version of the original
\\ ( https://arxiv.org/abs/2401.12713 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00856
replaced with revised version Fri, 23 Feb 2024 16:19:22 GMT   (4818kb,D)

Title: Towards Efficient and Exact Optimization of Language Model Alignment
Authors: Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie
  Tang, Minlie Huang
Categories: cs.CL
Comments: 24 pages, 9 figures
\\ ( https://arxiv.org/abs/2402.00856 ,  4818kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02380
replaced with revised version Fri, 23 Feb 2024 02:19:09 GMT   (1049kb)

Title: Evaluating Large Language Models in Analysing Classroom Dialogue
Authors: Yun Long, Haifeng Luo, Yu Zhang
Categories: cs.CL cs.AI cs.HC
\\ ( https://arxiv.org/abs/2402.02380 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02389
replaced with revised version Fri, 23 Feb 2024 09:01:44 GMT   (1374kb,D)

Title: KICGPT: Large Language Model with Knowledge in Context for Knowledge
  Graph Completion
Authors: Yanbin Wei, Qiushi Huang, James T. Kwok, Yu Zhang
Categories: cs.CL cs.AI
Comments: Accepted to EMNLP 2023 Findings
DOI: 10.18653/v1/2023.findings-emnlp.580
\\ ( https://arxiv.org/abs/2402.02389 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05629
replaced with revised version Fri, 23 Feb 2024 11:25:19 GMT   (1136kb,D)

Title: Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature
  of Aggregated Factual Claims in Long-Form Generations
Authors: Cheng-Han Chiang, Hung-yi Lee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.05629 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07913
replaced with revised version Fri, 23 Feb 2024 02:35:41 GMT   (1075kb,D)

Title: QACP: An Annotated Question Answering Dataset for Assisting Chinese
  Python Programming Learners
Authors: Rui Xiao, Lu Han, Xiaoying Zhou, Jiong Wang, Na Zong, Pengyu Zhang
Categories: cs.CL cs.AI cs.HC
\\ ( https://arxiv.org/abs/2402.07913 ,  1075kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08498
replaced with revised version Fri, 23 Feb 2024 14:07:41 GMT   (1826kb)

Title: Auditing Counterfire: Evaluating Advanced Counterargument Generation
  with Evidence and Style
Authors: Preetika Verma, Kokil Jaidka, Svetlana Churina
Categories: cs.CL
Comments: 19 pages, 10 figures, 11 tables
\\ ( https://arxiv.org/abs/2402.08498 ,  1826kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09015
replaced with revised version Thu, 22 Feb 2024 23:49:10 GMT   (15117kb,D)

Title: Towards better Human-Agent Alignment: Assessing Task Utility in
  LLM-Powered Applications
Authors: Negar Arabzadeh and Julia Kiseleva and Qingyun Wu and Chi Wang and
  Ahmed Awadallah and Victor Dibia and Adam Fourney and Charles Clarke
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.09015 ,  15117kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09727
replaced with revised version Fri, 23 Feb 2024 18:21:28 GMT   (221kb,D)

Title: A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
Authors: Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer
Categories: cs.CL cs.AI cs.IR
Comments: Website: https://read-agent.github.io
\\ ( https://arxiv.org/abs/2402.09727 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10400
replaced with revised version Fri, 23 Feb 2024 18:55:23 GMT   (5888kb,D)

Title: Chain of Logic: Rule-Based Reasoning with Large Language Models
Authors: Sergio Servantez, Joe Barrow, Kristian Hammond, Rajiv Jain
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.10400 ,  5888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11621
replaced with revised version Fri, 23 Feb 2024 15:43:50 GMT   (72kb)

Title: Decoding News Narratives: A Critical Analysis of Large Language Models
  in Framing Bias Detection
Authors: Valeria Pastorino, Jasivan A. Sivakumar, Nafise Sadat Moosavi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.11621 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11725
replaced with revised version Thu, 22 Feb 2024 19:12:09 GMT   (2290kb,D)

Title: How Susceptible are Large Language Models to Ideological Manipulation?
Authors: Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman
Categories: cs.CL cs.CR cs.CY
\\ ( https://arxiv.org/abs/2402.11725 ,  2290kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13035
replaced with revised version Fri, 23 Feb 2024 01:51:19 GMT   (170kb,D)

Title: Learning to Check: Unleashing Potentials for Self-Correction in Large
  Language Models
Authors: Che Zhang and Zhenyang Xiao and Chengcheng Han and Yixin Lian and
  Yuejian Fang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.13035 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13116
replaced with revised version Fri, 23 Feb 2024 05:03:16 GMT   (734kb,D)

Title: A Survey on Knowledge Distillation of Large Language Models
Authors: Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang
  Li, Can Xu, Dacheng Tao, Tianyi Zhou
Categories: cs.CL
Comments: 43 pages
\\ ( https://arxiv.org/abs/2402.13116 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13178
replaced with revised version Fri, 23 Feb 2024 16:46:58 GMT   (620kb,D)

Title: Benchmarking Retrieval-Augmented Generation for Medicine
Authors: Guangzhi Xiong and Qiao Jin and Zhiyong Lu and Aidong Zhang
Categories: cs.CL cs.AI
Comments: Homepage: https://teddy-xionggz.github.io/benchmark-medical-rag/
\\ ( https://arxiv.org/abs/2402.13178 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13764
replaced with revised version Fri, 23 Feb 2024 02:44:52 GMT   (3183kb,D)

Title: CriticBench: Evaluating Large Language Models as Critic
Authors: Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen,
  Xian-ling Mao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.13764 ,  3183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13950
replaced with revised version Fri, 23 Feb 2024 18:01:48 GMT   (10734kb,D)

Title: Making Reasoning Matter: Measuring and Improving Faithfulness of
  Chain-of-Thought Reasoning
Authors: Debjit Paul, Robert West, Antoine Bosselut and Boi Faltings
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.13950 ,  10734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14208
replaced with revised version Fri, 23 Feb 2024 08:19:09 GMT   (2188kb,D)

Title: Content Conditional Debiasing for Fair Text Embedding
Authors: Wenlong Deng, Blair Chen, Xiaoxiao Li, Christos Thrampoulidis
Categories: cs.CL cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2402.14208 ,  2188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14224
replaced with revised version Fri, 23 Feb 2024 02:19:22 GMT   (9913kb,D)

Title: Framing in the Presence of Supporting Data: A Case Study in U.S.
  Economic News
Authors: Alexandria Leto, Elliot Pickens, Coen D. Needell, David Rothschild,
  Maria Leonor Pacheco
Categories: cs.CL
Comments: total pages: 19; main body pages: 8; total figures: 19
\\ ( https://arxiv.org/abs/2402.14224 ,  9913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14379
replaced with revised version Fri, 23 Feb 2024 09:00:49 GMT   (320kb,D)

Title: Novi jezi\v{c}ki modeli za srpski jezik
Authors: Mihailo \v{S}kori\'c
Categories: cs.CL
Comments: in Serbian language
\\ ( https://arxiv.org/abs/2402.14379 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14484
replaced with revised version Fri, 23 Feb 2024 11:50:18 GMT   (712kb,D)

Title: Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation
  and Analysis
Authors: Takehiro Takayanagi and Masahiro Suzuki and Ryotaro Kobayashi and
  Hiroki Sakaji and Kiyoshi Izumi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.14484 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14660
replaced with revised version Fri, 23 Feb 2024 07:13:00 GMT   (2292kb,D)

Title: ConceptMath: A Bilingual Concept-wise Benchmark for Measuring
  Mathematical Reasoning of Large Language Models
Authors: Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing
  Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang,
  Wenbo Su, Bo Zheng
Categories: cs.CL cs.AI
Comments: The benchmark dataset will be released soon
\\ ( https://arxiv.org/abs/2402.14660 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14704
replaced with revised version Fri, 23 Feb 2024 03:42:00 GMT   (481kb,D)

Title: An LLM-Enhanced Adversarial Editing System for Lexical Simplification
Authors: Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu
Categories: cs.CL
Comments: Accepted by COLING 2024 main conference
\\ ( https://arxiv.org/abs/2402.14704 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2110.06257
replaced with revised version Fri, 23 Feb 2024 16:41:54 GMT   (15639kb,D)

Title: Causal Discovery from Conditionally Stationary Time Series
Authors: Carles Balsells-Rodas, Ruibo Tu, Hedvig Kjellstrom, Yingzhen Li
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2110.06257 ,  15639kb)
------------------------------------------------------------------------------
\\
arXiv:2204.13704
replaced with revised version Fri, 23 Feb 2024 15:38:06 GMT   (1907kb,D)

Title: Hyperbolic Hierarchical Knowledge Graph Embeddings for Link Prediction
  in Low Dimensions
Authors: Wenjie Zheng, Wenxue Wang, Shu Zhao and Fulan Qian
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2204.13704 ,  1907kb)
------------------------------------------------------------------------------
\\
arXiv:2208.09225
replaced with revised version Fri, 23 Feb 2024 13:49:45 GMT   (6790kb,D)

Title: FP8 Quantization: The Power of the Exponent
Authors: Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,
  Tijmen Blankevoort
Categories: cs.LG
\\ ( https://arxiv.org/abs/2208.09225 ,  6790kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07924
replaced with revised version Thu, 22 Feb 2024 21:26:25 GMT   (2273kb,D)

Title: GNNInterpreter: A Probabilistic Generative Model-Level Explanation for
  Graph Neural Networks
Authors: Xiaoqi Wang, Han-Wei Shen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2209.07924 ,  2273kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06891
replaced with revised version Fri, 23 Feb 2024 17:23:51 GMT   (1934kb,D)

Title: Experimental Design for Multi-Channel Imaging via Task-Driven Feature
  Selection
Authors: Stefano B. Blumberg, Paddy J. Slator, Daniel C. Alexander
Categories: cs.LG cs.AI q-bio.NC
Comments: Accepted In: International Conference of Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2210.06891 ,  1934kb)
------------------------------------------------------------------------------
\\
arXiv:2210.11327
replaced with revised version Thu, 22 Feb 2024 20:28:25 GMT   (1101kb,D)

Title: Improving Data Quality with Training Dynamics of Gradient Boosting
  Decision Trees
Authors: Moacir Antonelli Ponti and Lucas de Angelis Oliveira and Mathias
  Esteban and Valentina Garcia and Juan Mart\'in Rom\'an and Luis Argerich
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2210.11327 ,  1101kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04175
replaced with revised version Fri, 23 Feb 2024 14:55:02 GMT   (329kb,D)

Title: Centaur: Federated Learning for Constrained Edge Devices
Authors: Fan Mo, Mohammad Malekzadeh, Soumyajit Chatterjee, Fahim Kawsar, Akhil
  Mathur
Categories: cs.LG
Comments: ICLR 2023 Workshop on Machine Learning for IoT: Datasets, Perception,
  and Understanding
\\ ( https://arxiv.org/abs/2211.04175 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14568
replaced with revised version Fri, 23 Feb 2024 00:24:11 GMT   (1482kb,D)

Title: BeGin: Extensive Benchmark Scenarios and An Easy-to-use Framework for
  Graph Continual Learning
Authors: Jihoon Ko, Shinhwan Kang, Taehyung Kwon, Heechan Moon, and Kijung Shin
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2211.14568 ,  1482kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12292
replaced with revised version Fri, 23 Feb 2024 00:05:03 GMT   (3976kb,D)

Title: Zero-shot causal learning
Authors: Hamed Nilforoshan, Michael Moor, Yusuf Roohani, Yining Chen, Anja
  \v{S}urina, Michihiro Yasunaga, Sara Oblak, Jure Leskovec
Categories: cs.LG cs.AI cs.CY cs.HC
\\ ( https://arxiv.org/abs/2301.12292 ,  3976kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13748
replaced with revised version Thu, 22 Feb 2024 22:13:29 GMT   (4523kb,D)

Title: Archetypal Analysis++: Rethinking the Initialization Strategy
Authors: Sebastian Mair and Jens Sj\"olund
Categories: cs.LG
Comments: 26 pages, 17 figures, preprint
\\ ( https://arxiv.org/abs/2301.13748 ,  4523kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00834
replaced with revised version Fri, 23 Feb 2024 15:43:02 GMT   (17kb)

Title: Sharp Lower Bounds on Interpolation by Deep ReLU Neural Networks at
  Irregularly Spaced Data
Authors: Jonathan W. Siegel
Categories: cs.LG cs.NE stat.ML
MSC-class: 41A05, 65D05, 41A25
\\ ( https://arxiv.org/abs/2302.00834 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02223
replaced with revised version Fri, 23 Feb 2024 13:16:25 GMT   (557kb)

Title: Feature Selection with Annealing for Forecasting Financial Time Series
Authors: Hakan Pabuccu, Adrian Barbu
Categories: cs.LG q-fin.CP
Comments: 37 pages, 1 figures and 12 tables
MSC-class: 68T07
DOI: 10.48550/arXiv.2303.02223
\\ ( https://arxiv.org/abs/2303.02223 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05161
replaced with revised version Fri, 23 Feb 2024 17:21:40 GMT   (3429kb,D)

Title: Inversion dynamics of class manifolds in deep learning reveals tradeoffs
  underlying generalisation
Authors: Simone Ciceri, Lorenzo Cassani, Matteo Osella, Pietro Rotondo, Filippo
  Valle, Marco Gherardi
Categories: cs.LG
Journal-ref: Nature Machine Intelligence, vol 6, 40-47 (2024)
DOI: 10.1038/s42256-023-00772-9
\\ ( https://arxiv.org/abs/2303.05161 ,  3429kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15103
replaced with revised version Fri, 23 Feb 2024 13:32:47 GMT   (1699kb,D)

Title: Contrastive Learning Is Spectral Clustering On Similarity Graph
Authors: Zhiquan Tan, Yifan Zhang, Jingqin Yang, Yang Yuan
Categories: cs.LG cs.AI cs.CV
Comments: ICLR 2024; We express our gratitude to the anonymous reviewers for
  their valuable feedback
\\ ( https://arxiv.org/abs/2303.15103 ,  1699kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00737
replaced with revised version Fri, 23 Feb 2024 15:35:18 GMT   (6682kb)

Title: SparDL: Distributed Deep Learning Training with Efficient Sparse
  Communication
Authors: Minjun Zhao, Yichen Yin, Yuren Mao, Qing Liu, Lu Chen, Yunjun Gao
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2304.00737 ,  6682kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03954
replaced with revised version Fri, 23 Feb 2024 10:32:39 GMT   (290kb,D)

Title: Learning Action Embeddings for Off-Policy Evaluation
Authors: Matej Cief, Jacek Golebiowski, Philipp Schmidt, Ziawasch Abedjan,
  Artur Bekasov
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2305.03954 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04203
replaced with revised version Fri, 23 Feb 2024 08:55:08 GMT   (2528kb,D)

Title: Unlocking the Power of Open Set : A New Perspective for Open-Set Noisy
  Label Learning
Authors: Wenhai Wan, Xinrui Wang, Ming-Kun Xie, Shao-Yuan Li, Sheng-Jun Huang,
  Songcan Chen
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2305.04203 ,  2528kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05448
replaced with revised version Fri, 23 Feb 2024 07:20:33 GMT   (197kb,D)

Title: Robust Implicit Regularization via Weight Normalization
Authors: Hung-Hsu Chou, Holger Rauhut, Rachel Ward
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2305.05448 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00074
replaced with revised version Fri, 23 Feb 2024 13:35:04 GMT   (334kb,D)

Title: Human-Aligned Calibration for AI-Assisted Decision Making
Authors: Nina L. Corvelo Benz and Manuel Gomez Rodriguez
Categories: cs.LG cs.CY cs.HC stat.ML
\\ ( https://arxiv.org/abs/2306.00074 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02117
replaced with revised version Fri, 23 Feb 2024 08:39:48 GMT   (896kb,D)

Title: Oversmoothing: A Nightmare for Graph Contrastive Learning?
Authors: Jintang Li, Wangbin Sun, Ruofan Wu, Yuchang Zhu, Liang Chen, Zibin
  Zheng
Categories: cs.LG cs.AI
Comments: Technical report; Code available at
  https://github.com/EdisonLeeeee/BlockGCL
\\ ( https://arxiv.org/abs/2306.02117 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03828
replaced with revised version Thu, 22 Feb 2024 20:28:12 GMT   (34111kb,D)

Title: Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How
Authors: Sebastian Pineda Arango, Fabio Ferreira, Arlind Kadra, Frank Hutter,
  Josif Grabocka
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.03828 ,  34111kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04778
replaced with revised version Fri, 23 Feb 2024 00:59:36 GMT   (142kb,D)

Title: How to Evaluate Behavioral Models
Authors: Greg d'Eon, Sophie Greenwood, Kevin Leyton-Brown, and James R. Wright
Categories: cs.LG cs.GT
Comments: 15 pages (7 pages body + references and appendix). To appear at AAAI
  2024
\\ ( https://arxiv.org/abs/2306.04778 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13292
replaced with revised version Thu, 22 Feb 2024 21:07:10 GMT   (937kb,D)

Title: Variance-Covariance Regularization Improves Representation Learning
Authors: Jiachen Zhu, Katrina Evtimova, Yubei Chen, Ravid Shwartz-Ziv, Yann
  LeCun
Categories: cs.LG cs.AI cs.CV
Comments: 165 pages, 5 figures
\\ ( https://arxiv.org/abs/2306.13292 ,  937kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06175
replaced with revised version Thu, 22 Feb 2024 22:55:14 GMT   (6398kb,D)

Title: Learning Decentralized Partially Observable Mean Field Control for
  Artificial Collective Behavior
Authors: Kai Cui, Sascha Hauck, Christian Fabian, Heinz Koeppl
Categories: cs.LG cs.MA math.OC
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2307.06175 ,  6398kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11565
replaced with revised version Fri, 23 Feb 2024 12:42:24 GMT   (2174kb,D)

Title: Adversarial Feature Map Pruning for Backdoor
Authors: Dong Huang, Qingwen Bu
Categories: cs.LG cs.SE
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2307.11565 ,  2174kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06338
replaced with revised version Fri, 23 Feb 2024 10:36:18 GMT   (1709kb,D)

Title: Size Lowerbounds for Deep Operator Networks
Authors: Anirbit Mukherjee and Amartya Roy
Categories: cs.LG cs.CC cs.NA math.AP math.NA
Comments: 25 pages, 13 figures
Journal-ref: Published in Transactions on Machine Learning Research (TMLR) in
  February 2024
\\ ( https://arxiv.org/abs/2308.06338 ,  1709kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10779
replaced with revised version Fri, 23 Feb 2024 07:22:48 GMT   (130kb,D)

Title: Spear and Shield: Adversarial Attacks and Defense Methods for
  Model-Based Link Prediction on Continuous-Time Dynamic Graphs
Authors: Dongjin Lee, Juho Lee, Kijung Shin
Categories: cs.LG cs.SI
Comments: Published at AAAI 2024
\\ ( https://arxiv.org/abs/2308.10779 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12215
replaced with revised version Fri, 23 Feb 2024 16:13:38 GMT   (2008kb,D)

Title: The Challenges of Machine Learning for Trust and Safety: A Case Study on
  Misinformation Detection
Authors: Madelyne Xiao, Jonathan Mayer
Categories: cs.LG cs.CL cs.CY
\\ ( https://arxiv.org/abs/2308.12215 ,  2008kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16759
replaced with revised version Fri, 23 Feb 2024 02:49:16 GMT   (2212kb,D)

Title: Constructing Indoor Region-based Radio Map without Location Labels
Authors: Zheng Xing and Junting Chen
Categories: cs.LG eess.SP
\\ ( https://arxiv.org/abs/2308.16759 ,  2212kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00902
replaced with revised version Fri, 23 Feb 2024 05:34:07 GMT   (6231kb,D)

Title: DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and
  Diffusion Models
Authors: Yongchan Kwon, Eric Wu, Kevin Wu, James Zou
Categories: cs.LG stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.00902 ,  6231kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02861
replaced with revised version Fri, 23 Feb 2024 08:54:13 GMT   (266kb)

Title: Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly
  Detection
Authors: Xiangyu Dong, Xingyi Zhang, Sibo Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.02861 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05668
replaced with revised version Fri, 23 Feb 2024 05:58:26 GMT   (1817kb,D)

Title: LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised
  Time Series Anomaly Detection
Authors: Feiyi Chen, Zhen Qin, Yingying Zhang, Shuiguang Deng, Yi Xiao,
  Guansong Pang and Qingsong Wen
Categories: cs.LG
Comments: Accepted by ACM Web Conference 2024 (WWW 24)
\\ ( https://arxiv.org/abs/2310.05668 ,  1817kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05963
replaced with revised version Fri, 23 Feb 2024 16:39:40 GMT   (4740kb,D)

Title: CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid
  Dynamics
Authors: Yining Luo, Yingfa Chen, Zhen Zhang
Categories: cs.LG physics.comp-ph physics.flu-dyn
Comments: 33 pages, 11 figures, quality-checked, typos corrected
\\ ( https://arxiv.org/abs/2310.05963 ,  4740kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07446
replaced with revised version Fri, 23 Feb 2024 08:51:56 GMT   (14256kb,D)

Title: Position Paper: An Integrated Perspective on Data, Metrics, and
  Methodology for Deep Time-Series Forecasting
Authors: Jiawen Zhang, Xumeng Wen, Shun Zheng, Jia Li, Jiang Bian
Categories: cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2310.07446 ,  14256kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09751
replaced with revised version Fri, 23 Feb 2024 05:17:03 GMT   (393kb,D)

Title: UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series
  Forecasting
Authors: Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi,
  Roger Zimmermann
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.09751 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13164
replaced with revised version Fri, 23 Feb 2024 14:45:45 GMT   (651kb,D)

Title: Almost Equivariance via Lie Algebra Convolutions
Authors: Daniel McNeela
Categories: cs.LG stat.ML
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2310.13164 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16119
replaced with revised version Fri, 23 Feb 2024 10:24:48 GMT   (6338kb,D)

Title: Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for
  Enhancing SocialBot Conversations
Authors: Ond\v{r}ej Kobza, Jan \v{C}uhel, Tommaso Gargiani, David Herel, Petr
  Marek (Faculty of Electrical Engineering, CTU in Prague)
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.16119 ,  6338kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01223
replaced with revised version Fri, 23 Feb 2024 14:42:57 GMT   (1494kb,D)

Title: Diffusion Models for Reinforcement Learning: A Survey
Authors: Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang,
  Haoquan Guo, Tingting Chen, Weinan Zhang
Categories: cs.LG cs.AI
Comments: Fixed typos
\\ ( https://arxiv.org/abs/2311.01223 ,  1494kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02107
replaced with revised version Fri, 23 Feb 2024 14:50:04 GMT   (387kb)

Title: Generative Artificial Intelligence in Healthcare: Ethical Considerations
  and Assessment Checklist
Authors: Yilin Ning, Salinelat Teixayavong, Yuqing Shang, Julian Savulescu,
  Vaishaanth Nagaraj, Di Miao, Mayli Mertens, Daniel Shu Wei Ting, Jasmine
  Chiat Ling Ong, Mingxuan Liu, Jiuwen Cao, Michael Dunn, Roger Vaughan, Marcus
  Eng Hock Ong, Joseph Jao-Yiu Sung, Eric J Topol, Nan Liu
Categories: cs.LG cs.AI cs.CY
\\ ( https://arxiv.org/abs/2311.02107 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00279
replaced with revised version Fri, 23 Feb 2024 01:55:34 GMT   (1013kb,D)

Title: Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement
  Learning Approach
Authors: Xingqiu He, Chaoqun You, Tony Q. S. Quek
Categories: cs.LG cs.NI
\\ ( https://arxiv.org/abs/2312.00279 ,  1013kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12703
replaced with revised version Fri, 23 Feb 2024 02:50:18 GMT   (2392kb,D)

Title: Federated Learning with Extremely Noisy Clients via Negative
  Distillation
Authors: Yang Lu, Lin Chen, Yonggang Zhang, Yiliang Zhang, Bo Han, Yiu-ming
  Cheung, Hanzi Wang
Categories: cs.LG
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.12703 ,  2392kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02602
replaced with revised version Fri, 23 Feb 2024 02:22:42 GMT   (3387kb,D)

Title: Neural Causal Abstractions
Authors: Kevin Xia, Elias Bareinboim
Categories: cs.LG cs.AI
Comments: 48 total pages, 20 figures, short version accepted to AAAI-24
\\ ( https://arxiv.org/abs/2401.02602 ,  3387kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04385
replaced with revised version Fri, 23 Feb 2024 10:47:45 GMT   (1839kb,D)

Title: Machine unlearning through fine-grained model parameters perturbation
Authors: Zhiwei Zuo, Zhuo Tang, Kenli Li, Anwitaman Datta
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.04385 ,  1839kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04829
replaced with revised version Fri, 23 Feb 2024 15:49:19 GMT   (1739kb)

Title: GNNShap: Scalable and Accurate GNN Explanation using Shapley Values
Authors: Selahattin Akkas and Ariful Azad
Categories: cs.LG cs.SI
DOI: 10.1145/3589334.3645599
\\ ( https://arxiv.org/abs/2401.04829 ,  1739kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17548
replaced with revised version Fri, 23 Feb 2024 06:38:39 GMT   (3728kb,D)

Title: Rethinking Channel Dependence for Multivariate Time Series Forecasting:
  Learning from Leading Indicators
Authors: Lifan Zhao, Yanyan Shen
Categories: cs.LG cs.AI
Comments: Accepted to ICLR 2024. Preprint version
Journal-ref: The Twelfth International Conference on Learning Representations,
  2024
\\ ( https://arxiv.org/abs/2401.17548 ,  3728kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17791
replaced with revised version Fri, 23 Feb 2024 13:26:13 GMT   (1049kb,D)

Title: Graph Transformers without Positional Encodings
Authors: Ayush Garg
Categories: cs.LG cs.AI
Comments: Independent Research
\\ ( https://arxiv.org/abs/2401.17791 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00450
replaced with revised version Fri, 23 Feb 2024 08:52:09 GMT   (997kb,D)

Title: CPT: Competence-progressive Training Strategy for Few-shot Node
  Classification
Authors: Qilong Yan, Yufeng Zhang, Jinghao Zhang, Jingpu Duan, Jian Yin
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2206.11972 by
  other authors
\\ ( https://arxiv.org/abs/2402.00450 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07703
replaced with revised version Fri, 23 Feb 2024 06:05:19 GMT   (4420kb,D)

Title: Online Sequential Decision-Making with Unknown Delays
Authors: Ping Wu and Heyan Huang and Zhengyang Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.07703 ,  4420kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11641
replaced with revised version Fri, 23 Feb 2024 09:18:30 GMT   (2396kb,D)

Title: Towards Versatile Graph Learning Approach: from the Perspective of Large
  Language Models
Authors: Lanning Wei, Jun Gao, Huan Zhao, Quanming Yao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11641 ,  2396kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12269
replaced with revised version Fri, 23 Feb 2024 09:55:27 GMT   (1884kb,D)

Title: End-to-end Supervised Prediction of Arbitrary-size Graphs with
  Partially-Masked Fused Gromov-Wasserstein Matching
Authors: Paul Krzakala, Junjie Yang, R\'emi Flamary, Florence d'Alch\'e-Buc,
  Charlotte Laclau, Matthieu Labeau
Categories: cs.LG
Comments: 17 pages, 11 figures
\\ ( https://arxiv.org/abs/2402.12269 ,  1884kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12424
replaced with revised version Fri, 23 Feb 2024 05:18:03 GMT   (6911kb,D)

Title: Tables as Images? Exploring the Strengths and Limitations of LLMs on
  Multimodal Representations of Tabular Data
Authors: Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma,
  Yue Zhang, Rada Mihalcea
Categories: cs.LG cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2402.12424 ,  6911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13711
replaced with revised version Fri, 23 Feb 2024 05:43:05 GMT   (1321kb,D)

Title: DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based
  Graph Continual Learning
Authors: Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim,
  Chanyoung Park
Categories: cs.LG cs.AI
Comments: Accepted at ACM TheWebConf 2024 (WWW 2024)
DOI: 10.1145/3589334.3645561
\\ ( https://arxiv.org/abs/2402.13711 ,  1321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13777
replaced with revised version Fri, 23 Feb 2024 02:03:00 GMT   (361kb,D)

Title: Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
  and Perspectives on Future Directions
Authors: Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet
  Aggarwal
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.13777 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14547
replaced with revised version Fri, 23 Feb 2024 14:37:09 GMT   (1889kb,D)

Title: OmniPred: Language Models as Universal Regressors
Authors: Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi
  Perel, Yutian Chen
Categories: cs.LG cs.AI cs.CL cs.DB
Comments: 24 pages, 10 figures
\\ ( https://arxiv.org/abs/2402.14547 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2210.12596
replaced with revised version Thu, 22 Feb 2024 21:59:36 GMT   (5175kb,D)

Title: DMODE: Differential Monocular Object Distance Estimation Module without
  Class Specific Information
Authors: Pedram Agand, Michael Chang, and Mo Chen
Categories: cs.CV cs.AI cs.LG
Comments: 7 pages, 3 figures, 3 tables
\\ ( https://arxiv.org/abs/2210.12596 ,  5175kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16749
replaced with revised version Thu, 22 Feb 2024 22:30:31 GMT   (726kb,D)

Title: Improving Code Generation by Training with Natural Language Feedback
Authors: Angelica Chen, J\'er\'emy Scheurer, Tomasz Korbak, Jon Ander Campos,
  Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, Ethan Perez
Categories: cs.SE cs.AI cs.CL cs.LG
Comments: Published in (and superceded by) TMLR:
  https://openreview.net/forum?id=xo3hI5MwvU
\\ ( https://arxiv.org/abs/2303.16749 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05949 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 05:04:33 GMT   (15179kb,D)

Title: CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic
  inference and learning
Authors: Nihal Sanjay Singh, Keito Kobayashi, Qixuan Cao, Kemal Selcuk, Tianrui
  Hu, Shaila Niazi, Navid Anjum Aadit, Shun Kanai, Hideo Ohno, Shunsuke Fukami,
  and Kerem Y. Camsari
Categories: cond-mat.mes-hall cs.AI cs.ET cs.LG
\\ ( https://arxiv.org/abs/2304.05949 ,  15179kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04366
replaced with revised version Fri, 23 Feb 2024 07:45:42 GMT   (3107kb,D)

Title: Enhancing Worker Recruitment in Collaborative Mobile Crowdsourcing: A
  Graph Neural Network Trust Evaluation Approach
Authors: Zhongwei Zhan, Yingjie Wang, Peiyong Duan, Akshita Maradapu Vera
  Venkata Sai, Zhaowei Liu, Chaocan Xiang, Xiangrong Tong, Weilong Wang,
  Zhipeng Cai
Categories: cs.SI cs.AI cs.HC cs.LG
\\ ( https://arxiv.org/abs/2306.04366 ,  3107kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08672
replaced with revised version Fri, 23 Feb 2024 01:10:41 GMT   (68kb,D)

Title: FedDefender: Backdoor Attack Defense in Federated Learning
Authors: Waris Gill (1), Ali Anwar (2), Muhammad Ali Gulzar (1) ((1) Virginia
  Tech, (2) University of Minnesota Twin Cities)
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: Published in SE4SafeML 2023 (co-located with FSE 2023). See
  https://dl.acm.org/doi/abs/10.1145/3617574.3617858
DOI: 10.1145/3617574.3617858
\\ ( https://arxiv.org/abs/2307.08672 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08784
replaced with revised version Fri, 23 Feb 2024 04:56:37 GMT   (8383kb,D)

Title: CodeCoT: Tackling Code Syntax Errors in CoT Reasoning for Code
  Generation
Authors: Dong Huang, Qingwen Bu, Yuhao Qing, Heming Cui
Categories: cs.SE cs.AI
Comments: Title changed
\\ ( https://arxiv.org/abs/2308.08784 ,  8383kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09051
replaced with revised version Fri, 23 Feb 2024 07:51:31 GMT   (23724kb,D)

Title: GenDOM: Generalizable One-shot Deformable Object Manipulation with
  Parameter-Aware Policy
Authors: So Kuroki, Jiaxian Guo, Tatsuya Matsushima, Takuya Okubo, Masato
  Kobayashi, Yuya Ikeda, Ryosuke Takanami, Paul Yoo, Yutaka Matsuo, Yusuke
  Iwasawa
Categories: cs.RO cs.AI
Comments: Extended version of arXiv:2306.09872
\\ ( https://arxiv.org/abs/2309.09051 ,  23724kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10092
replaced with revised version Thu, 22 Feb 2024 21:48:12 GMT   (27489kb,D)

Title: Conformal Temporal Logic Planning using Large Language Models
Authors: Jun Wang, Jiaming Tong, Kaiyuan Tan, Yevgeniy Vorobeychik, Yiannis
  Kantaros
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2309.10092 ,  27489kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04218
replaced with revised version Fri, 23 Feb 2024 10:57:57 GMT   (88kb,D)

Title: A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence
  Classes with the same Skeleton
Authors: Vidya Sagar Sharma
Categories: cs.DS cs.AI cs.LG
Comments: 75 pages, 2 Figures
\\ ( https://arxiv.org/abs/2310.04218 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02181 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 09:12:44 GMT   (1474kb,D)

Title: Joint Problems in Learning Multiple Dynamical Systems
Authors: Mengjia Niu and Xiaoyu He and Petr Ry\v{s}av\'y and Quan Zhou and
  Jakub Marecek
Categories: math.OC cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.02181 ,  1474kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10788
replaced with revised version Thu, 22 Feb 2024 20:51:22 GMT   (20451kb,D)

Title: Efficient Temporally-Aware DeepFake Detection using H.264 Motion Vectors
Authors: Peter Gr\"onquist, Yufan Ren, Qingyi He, Alessio Verardo, Sabine
  S\"usstrunk
Categories: cs.CV cs.AI
ACM-class: I.5.4; I.4.8; I.2.10; I.4.2
\\ ( https://arxiv.org/abs/2311.10788 ,  20451kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12943
replaced with revised version Fri, 23 Feb 2024 15:29:14 GMT   (5985kb,D)

Title: InteRACT: Transformer Models for Human Intent Prediction Conditioned on
  Robot Actions
Authors: Kushal Kedia, Atiksh Bhardwaj, Prithwish Dan, Sanjiban Choudhury
Categories: cs.RO cs.AI cs.LG cs.MA
\\ ( https://arxiv.org/abs/2311.12943 ,  5985kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02677
replaced with revised version Fri, 23 Feb 2024 14:30:57 GMT   (4326kb,D)

Title: Contact Energy Based Hindsight Experience Prioritization
Authors: Erdi Sayar, Zhenshan Bing, Carlo D'Eramo, Ozgur S. Oguz, Alois Knoll
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2312.02677 ,  4326kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11234
replaced with revised version Fri, 23 Feb 2024 13:41:18 GMT   (6855kb,D)

Title: Perceptual Musical Features for Interpretable Audio Tagging
Authors: Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos and Giorgos
  Stamou
Categories: cs.SD cs.AI eess.AS
Comments: Github Repository:
  https://github.com/vaslyb/perceptible-music-tagging
\\ ( https://arxiv.org/abs/2312.11234 ,  6855kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07278
replaced with revised version Fri, 23 Feb 2024 10:09:24 GMT   (9285kb,D)

Title: Semi-Supervised Semantic Segmentation using Redesigned Self-Training for
  White Blood Cells
Authors: Vinh Quoc Luu, Duy Khanh Le, Huy Thanh Nguyen, Minh Thanh Nguyen,
  Thinh Tien Nguyen, Vinh Quang Dinh
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.07278 ,  9285kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10935
replaced with revised version Fri, 23 Feb 2024 04:36:51 GMT   (29694kb,D)

Title: SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents
Authors: Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing
  Zhang, Zhiyong Wu
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2401.10935 ,  29694kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12686
replaced with revised version Fri, 23 Feb 2024 10:04:14 GMT   (1994kb,D)

Title: Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach
Authors: Christian Fabian, Kai Cui, Heinz Koeppl
Categories: cs.MA cs.AI cs.GT cs.LG
Comments: accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2401.12686 ,  1994kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17842
replaced with revised version Fri, 23 Feb 2024 09:11:37 GMT   (16484kb,D)

Title: Explainable Benchmarking for Iterative Optimization Heuristics
Authors: Niki van Stein, Diederick Vermetten, Anna V. Kononova, Thomas B\"ack
Categories: cs.NE cs.AI
Comments: Submitted to ACM TELO
\\ ( https://arxiv.org/abs/2401.17842 ,  16484kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00025
replaced with revised version Thu, 22 Feb 2024 20:38:47 GMT   (1519kb,D)

Title: Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with
  SplitK work decomposition
Authors: Adnan Hoque, Less Wright, Chih-Chieh Yang, Mudhakar Srivatsa, Raghu
  Ganti
Categories: cs.DC cs.AI
\\ ( https://arxiv.org/abs/2402.00025 ,  1519kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05980
replaced with revised version Fri, 23 Feb 2024 05:48:48 GMT   (414kb,D)

Title: Do Large Code Models Understand Programming Concepts? A Black-box
  Approach
Authors: Ashish Hooda, Mihai Christodorescu, Miltiadis Allamanis, Aaron Wilson,
  Kassem Fawaz, Somesh Jha
Categories: cs.SE cs.AI cs.LG cs.PL
\\ ( https://arxiv.org/abs/2402.05980 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07939
replaced with revised version Fri, 23 Feb 2024 12:21:14 GMT   (20283kb,D)

Title: UFO: A UI-Focused Agent for Windows OS Interaction
Authors: Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua
  Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
Categories: cs.HC cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.07939 ,  20283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12928
replaced with revised version Fri, 23 Feb 2024 14:40:01 GMT   (4213kb,D)

Title: A Literature Review of Literature Reviews in Pattern Analysis and
  Machine Intelligence
Authors: Penghai Zhao, Xin Zhang, Ming-Ming Cheng, Jian Yang, Xiang Li
Categories: cs.DL cs.AI cs.CV
Comments: 20 pages,9 figures, 5 tables. [February 19, 2024] 20 pages,9 figures,
  5 tables. Typos fixed. [February 23, 2024]
\\ ( https://arxiv.org/abs/2402.12928 ,  4213kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13326 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 21:25:42 GMT   (4246kb,D)

Title: Deep Hedging with Market Impact
Authors: Andrei Neagu and Fr\'ed\'eric Godin and Clarence Simard and Leila
  Kosseim
Categories: q-fin.CP cs.AI
Comments: 13 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.13326 ,  4246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13352 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 08:55:48 GMT   (1210kb,D)

Title: KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
Authors: Boran Apak, Medina Bandic, Aritra Sarkar and Sebastian Feld
Categories: quant-ph cs.AI cs.ET cs.LG
\\ ( https://arxiv.org/abs/2402.13352 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14096
replaced with revised version Fri, 23 Feb 2024 03:53:32 GMT   (9138kb,D)

Title: EyeTrans: Merging Human and Machine Attention for Neural Code
  Summarization
Authors: Yifan Zhang, Jiliang Li, Zachary Karas, Aakash Bansal, Toby Jia-Jun
  Li, Collin McMillan, Kevin Leach, Yu Huang
Categories: cs.SE cs.AI cs.HC
DOI: 10.1145/3643732
\\ ( https://arxiv.org/abs/2402.14096 ,  9138kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14323
replaced with revised version Fri, 23 Feb 2024 02:53:20 GMT   (1470kb,D)

Title: REPOFUSE: Repository-Level Code Completion with Fused Dual Context
Authors: Ming Liang, Xiaoheng Xie, Gehao Zhang, Xunjin Zheng, Peng Di, wei
  jiang, Hongwei Chen, Chengpeng Wang, Gang Fan
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2402.14323 ,  1470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14601
replaced with revised version Fri, 23 Feb 2024 04:38:37 GMT   (1043kb,D)

Title: Bringing Generative AI to Adaptive Learning in Education
Authors: Hang Li, Tianlong Xu, Chaoli Zhang, Eason Chen, Jing Liang, Xing Fan,
  Haoyang Li, Jiliang Tang, Qingsong Wen
Categories: cs.CY cs.AI cs.HC cs.LG
Comments: 14 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.14601 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04695
replaced with revised version Thu, 22 Feb 2024 19:11:46 GMT   (18804kb,D)

Title: ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image
  Diffusion Models
Authors: Maitreya Patel and Tejas Gokhale and Chitta Baral and Yezhou Yang
Categories: cs.CV cs.CL cs.LG
Comments: Accepted at AAAI'24 | Project page: https://conceptbed.github.io
\\ ( https://arxiv.org/abs/2306.04695 ,  18804kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03128
replaced with revised version Fri, 23 Feb 2024 13:19:52 GMT   (6832kb,D)

Title: MetaTool Benchmark for Large Language Models: Deciding Whether to Use
  Tools and Which to Use
Authors: Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and
  Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong
  and Lichao Sun
Categories: cs.SE cs.CL
\\ ( https://arxiv.org/abs/2310.03128 ,  6832kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06310
replaced with revised version Thu, 22 Feb 2024 20:35:02 GMT   (22023kb,D)

Title: ViSAGe: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image
  Generation
Authors: Akshita Jha, Vinodkumar Prabhakaran, Remi Denton, Sarah Laszlo, Shachi
  Dave, Rida Qadri, Chandan K. Reddy, Sunipa Dev
Categories: cs.CV cs.CL cs.CY
\\ ( https://arxiv.org/abs/2401.06310 ,  22023kb)
------------------------------------------------------------------------------
\\
arXiv:1710.00095 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 17:39:40 GMT   (169kb,D)

Title: User-friendly guarantees for the Langevin Monte Carlo with inaccurate
  gradient
Authors: Arnak S. Dalalyan and Avetik G. Karagulyan
Categories: math.ST cs.LG math.PR stat.CO stat.ML stat.TH
Journal-ref: Stochastic Processes and their Applications, Volume 129, Issue 12,
  December 2019, Pages 5278-5311
DOI: 10.1016/j.spa.2019.02.016
\\ ( https://arxiv.org/abs/1710.00095 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2102.04363 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 10:47:23 GMT   (74kb,D)

Title: Efficient Data-Driven Optimization with Noisy Data
Authors: Bart P.G. Van Parys
Categories: math.OC cs.LG math.ST stat.TH
MSC-class: 90C15, 62C99
DOI: 10.1016/j.orl.2024.107089
\\ ( https://arxiv.org/abs/2102.04363 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2104.10561
replaced with revised version Fri, 23 Feb 2024 17:27:50 GMT   (2388kb,D)

Title: Turning Federated Learning Systems Into Covert Channels
Authors: Gabriele Costa, Fabio Pinelli, Simone Soderi, Gabriele Tolomei
Categories: cs.CR cs.LG
Journal-ref: IEEE Access, vol. 10, pp. 130642-130656, 2022
DOI: 10.1109/ACCESS.2022.3229124
\\ ( https://arxiv.org/abs/2104.10561 ,  2388kb)
------------------------------------------------------------------------------
\\
arXiv:2105.08620 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 01:23:23 GMT   (3186kb,D)

Title: Adversarial Examples Detection with Bayesian Neural Network
Authors: Yao Li, Tongyi Tang, Cho-Jui Hsieh, Thomas C. M. Lee
Categories: stat.ML cs.CV cs.LG
\\ ( https://arxiv.org/abs/2105.08620 ,  3186kb)
------------------------------------------------------------------------------
\\
arXiv:2202.05650 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 16:04:18 GMT   (5178kb,D)

Title: Bernstein Flows for Flexible Posteriors in Variational Bayes
Authors: Oliver D\"urr and Stephan H\"orling and Daniel Dold and Ivonne Kovylov
  and Beate Sick
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2202.05650 ,  5178kb)
------------------------------------------------------------------------------
\\
arXiv:2203.13847 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 11:16:10 GMT   (828kb,D)

Title: Cluster Algebras: Network Science and Machine Learning
Authors: Pierre-Philippe Dechant, Yang-Hui He, Elli Heyes, Edward Hirst
Categories: math.CO cs.LG hep-th math.AG
Comments: 38 pages, 27 figures
Report-no: LIMS-2022-011
Journal-ref: J.Comput.Algebra 8 (2023) 100008
DOI: 10.1016/j.jaca.2023.100008
\\ ( https://arxiv.org/abs/2203.13847 ,  828kb)
------------------------------------------------------------------------------
\\
arXiv:2206.04277 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 21:09:01 GMT   (752kb,D)

Title: On Hypothesis Transfer Learning of Functional Linear Models
Authors: Haotian Lin, Matthew Reimherr
Categories: stat.ML cs.LG
Comments: The results are extended to functional GLM
\\ ( https://arxiv.org/abs/2206.04277 ,  752kb)
------------------------------------------------------------------------------
\\
arXiv:2209.11924 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 21:44:44 GMT   (213kb,D)

Title: Interventional Causal Representation Learning
Authors: Kartik Ahuja, Divyat Mahajan, Yixin Wang, Yoshua Bengio
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2209.11924 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16311 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 08:32:00 GMT   (177kb,D)

Title: Simultaneous off-the-grid learning of mixtures issued from a continuous
  dictionary
Authors: Cristina Butucea (CREST, FAIRPLAY), Jean-Fran\c{c}ois Delmas
  (CERMICS), Anne Dutfoy (EDF R&D), Cl\'ement Hardy (CERMICS, EDF R&D)
Categories: stat.ML cs.LG math.PR math.ST stat.TH
\\ ( https://arxiv.org/abs/2210.16311 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2211.16943 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 07:59:23 GMT   (1262kb,D)

Title: Predicting Properties of Quantum Systems with Conditional Generative
  Models
Authors: Haoxiang Wang, Maurice Weber, Josh Izaac, Cedric Yen-Yu Lin
Categories: quant-ph cs.LG
Comments: 12 pages, 14 figures, 5 pages appendix. Open-source code is available
  at https://github.com/PennyLaneAI/generative-quantum-states
\\ ( https://arxiv.org/abs/2211.16943 ,  1262kb)
------------------------------------------------------------------------------
\\
arXiv:2301.03553
replaced with revised version Thu, 22 Feb 2024 23:10:14 GMT   (2699kb,D)

Title: FedDebug: Systematic Debugging for Federated Learning Applications
Authors: Waris Gill, Ali Anwar, Muhammad Ali Gulzar
Categories: cs.SE cs.CV cs.DC cs.LG
Comments: Published at ICSE 2023. Link
  https://ieeexplore.ieee.org/document/10172839
Journal-ref: In 2023 IEEE/ACM 45th International Conference on Software
  Engineering (ICSE) (pp. 456-789). IEEE (2023)
DOI: 10.1109/ICSE48619.2023.00053
\\ ( https://arxiv.org/abs/2301.03553 ,  2699kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08028
replaced with revised version Thu, 22 Feb 2024 20:44:58 GMT   (2861kb,D)

Title: EdgeServe: A Streaming System for Decentralized Model Serving
Authors: Ted Shaowang, Sanjay Krishnan
Categories: cs.DB cs.DC cs.LG
Comments: 19 pages, 15 figures
\\ ( https://arxiv.org/abs/2303.08028 ,  2861kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07164
replaced with revised version Thu, 22 Feb 2024 21:53:40 GMT   (1593kb,D)

Title: Learning-Augmented Online Packet Scheduling with Deadlines
Authors: Ya-Chun Liang and Clifford Stein and Hao-Ting Wei
Categories: cs.DS cs.LG cs.NI
\\ ( https://arxiv.org/abs/2305.07164 ,  1593kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11792 (*cross-listing*)
replaced with revised version Thu, 22 Feb 2024 19:44:52 GMT   (1665kb,D)

Title: Quantum Convolutional Neural Networks with Interaction Layers for
  Classification of Classical Data
Authors: Jishnu Mahmud, Raisa Mashtura, Shaikh Anowarul Fattah, Mohammad Saquib
Categories: quant-ph cs.LG
Comments: 31 pages, 13 figures, 6 tables
Journal-ref: Quantum Machine Intelligence 6, 11 (2024)
DOI: 10.1007/s42484-024-00145-4
\\ ( https://arxiv.org/abs/2307.11792 ,  1665kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01054 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 07:39:14 GMT   (3509kb,D)

Title: Simulation-based inference using surjective sequential neural likelihood
  estimation
Authors: Simon Dirmeier, Carlo Albert, Fernando Perez-Cruz
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2308.01054 ,  3509kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09814 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 14:23:45 GMT   (90kb)

Title: Convolutional Deep Kernel Machines
Authors: Edward Milsom, Ben Anson, Laurence Aitchison
Categories: stat.ML cs.LG
Comments: ICLR 2024 Camera Ready Version
\\ ( https://arxiv.org/abs/2309.09814 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01362
replaced with revised version Fri, 23 Feb 2024 03:51:51 GMT   (19575kb,D)

Title: Robot Fleet Learning via Policy Merging
Authors: Lirui Wang, Kaiqing Zhang, Allan Zhou, Max Simchowitz, Russ Tedrake
Categories: cs.RO cs.LG
Comments: See the code https://github.com/liruiw/Fleet-Tools for more details
\\ ( https://arxiv.org/abs/2310.01362 ,  19575kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14340 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 06:38:35 GMT   (951kb,D)

Title: Estimation of partially known Gaussian graphical models with score-based
  structural priors
Authors: Mart\'in Sevilla, Antonio Garc\'ia Marques, Santiago Segarra
Categories: stat.ML cs.LG
Comments: 17 pages, 7 figures, AISTATS 2024
\\ ( https://arxiv.org/abs/2401.14340 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15236
replaced with revised version Fri, 23 Feb 2024 15:07:38 GMT   (9349kb,D)

Title: Adaptive Deep Learning for Efficient Visual Pose Estimation aboard
  Ultra-low-power Nano-drones
Authors: Beatrice Alessandra Motetti, Luca Crupi, Mustafa Omer Mohammed Elamin
  Elshaigi, Matteo Risso, Daniele Jahier Pagliari, Daniele Palossi, Alessio
  Burrello
Categories: cs.CV cs.LG
Comments: Accepted for publication in the 2024 Design, Automation and Test in
  Europe (DATE) conference
\\ ( https://arxiv.org/abs/2401.15236 ,  9349kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03808 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 05:50:14 GMT   (1877kb,D)

Title: SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal
  Denoising
Authors: Yu-Tung Liu, Kuan-Chen Wang, Kai-Chun Liu, Sheng-Yu Peng, Yu Tsao
Categories: eess.SP cs.LG
Comments: This paper is accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2402.03808 ,  1877kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08082 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 17:51:20 GMT   (462kb)

Title: Score-based generative models break the curse of dimensionality in
  learning a family of sub-Gaussian probability distributions
Authors: Frank Cole, Yulong Lu
Categories: stat.ML cs.LG
Comments: 33 pages, to appear in the proceedings of 12th International
  Conference on Learning Representations
\\ ( https://arxiv.org/abs/2402.08082 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08576
replaced with revised version Thu, 22 Feb 2024 19:20:51 GMT   (244kb,D)

Title: Regret Minimization in Stackelberg Games with Side Information
Authors: Keegan Harris, Zhiwei Steven Wu, Maria-Florina Balcan
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2402.08576 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12531
replaced with revised version Thu, 22 Feb 2024 23:49:53 GMT   (7489kb,D)

Title: Improving Deep Generative Models on Many-To-One Image-to-Image
  Translation
Authors: Sagar Saxena, Mohammad Nayeem Teli
Categories: cs.CV cs.LG
Comments: 11 pages, 6 figures; template format corrected
\\ ( https://arxiv.org/abs/2402.12531 ,  7489kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13005 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 16:45:39 GMT   (614kb,D)

Title: SzCORE: A Seizure Community Open-source Research Evaluation framework
  for the validation of EEG-based automated seizure detection algorithms
Authors: Jonathan Dan, Una Pale, Alireza Amirshahi, William Cappelletti, Thorir
  Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Adriano Bernini, Luca
  Benini, S\'andor Beniczky, David Atienza, Philippe Ryvlin
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2402.13005 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13277
replaced with revised version Thu, 22 Feb 2024 19:17:37 GMT   (782kb,D)

Title: MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek
  in WSNs
Authors: Md. Alamin Talukder, Selina Sharmin, Md Ashraf Uddin, Md Manowarul
  Islam and Sunil Aryal
Categories: cs.CR cs.LG
Comments: International Journal of Information Security, Springer Journal - Q1,
  Scopus, ISI, SCIE, IF: 3.2 - Accepted on Jan 17, 2024
\\ ( https://arxiv.org/abs/2402.13277 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13291
replaced with revised version Fri, 23 Feb 2024 17:26:06 GMT   (109kb,D)

Title: DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language
  Models
Authors: Berkay Berabi, Alexey Gronskiy, Veselin Raychev, Gishor Sivanrupan,
  Victor Chibotaru, Martin Vechev
Categories: cs.CR cs.LG cs.PL cs.SE
Comments: 26 pages, 13 figures (v2, small fix in author affiliations)
\\ ( https://arxiv.org/abs/2402.13291 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13699
replaced with revised version Fri, 23 Feb 2024 18:31:45 GMT   (3494kb,D)

Title: Explainable Classification Techniques for Quantum Dot Device
  Measurements
Authors: Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok
  Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak
Categories: cs.CV cond-mat.mes-hall cs.LG
Comments: 5 pages, 3 figures
Journal-ref: Proceedings of the XAI4Sci: Explainable machine learning for
  sciences workshop at AAAI 2024, Vancouver, Canada
\\ ( https://arxiv.org/abs/2402.13699 ,  3494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14148 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 10:42:19 GMT   (2315kb,D)

Title: Neural Networks and Friction: Slide, Hold, Learn
Authors: Joaquin Garcia-Suarez
Categories: physics.geo-ph cs.LG
Comments: 10 paged, 10 figures, 2 tables
\\ ( https://arxiv.org/abs/2402.14148 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14285
replaced with revised version Fri, 23 Feb 2024 02:15:32 GMT   (1758kb,D)

Title: Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
Authors: Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang,
  Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, Yisong Yue
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2402.14285 ,  1758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14349 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 07:52:02 GMT   (3367kb,D)

Title: Uncertainty-driven and Adversarial Calibration Learning for Epicardial
  Adipose Tissue Segmentation
Authors: Kai Zhao, Zhiming Liu, Jiaqi Liu, Jingbiao Zhou, Bihong Liao, Huifang
  Tang, Qiuyu Wang, Chunquan Li
Categories: eess.IV cs.CV cs.LG
Comments: 13 pages,7 figuers
\\ ( https://arxiv.org/abs/2402.14349 ,  3367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14434 (*cross-listing*)
replaced with revised version Fri, 23 Feb 2024 05:14:06 GMT   (568kb,D)

Title: Parallelized Midpoint Randomization for Langevin Monte Carlo
Authors: Lu Yu, Arnak Dalalyan
Categories: math.ST cs.LG math.PR stat.CO stat.TH
Comments: arXiv admin note: substantial text overlap with arXiv:2306.08494
\\ ( https://arxiv.org/abs/2402.14434 ,  568kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
