Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80009 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月1日 17:26
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 30 Jan 24 19:00:00 GMT  to  Wed 31 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.17436
Date: Tue, 30 Jan 2024 20:51:42 GMT   (746kb,D)

Title: Difficulty Modelling in Mobile Puzzle Games: An Empirical Study on
  Different Methods to Combine Player Analytics and Simulated Data
Authors: Jeppe Theiss Kristensen, Paolo Burelli
Categories: cs.AI
\\
  Difficulty is one of the key drivers of player engagement and it is often one
of the aspects that designers tweak most to optimise the player experience;
operationalising it is, therefore, a crucial task for game development studios.
A common practice consists of creating metrics out of data collected by player
interactions with the content; however, this allows for estimation only after
the content is released and does not consider the characteristics of potential
future players.
  In this article, we present a number of potential solutions for the
estimation of difficulty under such conditions, and we showcase the results of
a comparative study intended to understand which method and which types of data
perform better in different scenarios.
  The results reveal that models trained on a combination of cohort statistics
and simulated data produce the most accurate estimations of difficulty in all
scenarios. Furthermore, among these models, artificial neural networks show the
most consistent results.
\\ ( https://arxiv.org/abs/2401.17436 ,  746kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17511
Date: Wed, 31 Jan 2024 00:08:44 GMT   (525kb,D)

Title: Linguistically Communicating Uncertainty in Patient-Facing Risk
  Prediction Models
Authors: Adarsa Sivaprasad and Ehud Reiter
Categories: cs.AI cs.CL
\\
  This paper addresses the unique challenges associated with uncertainty
quantification in AI models when applied to patient-facing contexts within
healthcare. Unlike traditional eXplainable Artificial Intelligence (XAI)
methods tailored for model developers or domain experts, additional
considerations of communicating in natural language, its presentation and
evaluating understandability are necessary. We identify the challenges in
communication model performance, confidence, reasoning and unknown knowns using
natural language in the context of risk prediction. We propose a design aimed
at addressing these challenges, focusing on the specific application of
in-vitro fertilisation outcome prediction.
\\ ( https://arxiv.org/abs/2401.17511 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17527
Date: Wed, 31 Jan 2024 01:09:40 GMT   (850kb,D)

Title: Learning to Stop Cut Generation for Efficient Mixed-Integer Linear
  Programming
Authors: Haotian Ling, Zhihai Wang, Jie Wang
Categories: cs.AI
\\
  Cutting planes (cuts) play an important role in solving mixed-integer linear
programs (MILPs), as they significantly tighten the dual bounds and improve the
solving performance. A key problem for cuts is when to stop cuts generation,
which is important for the efficiency of solving MILPs. However, many modern
MILP solvers employ hard-coded heuristics to tackle this problem, which tends
to neglect underlying patterns among MILPs from certain applications. To
address this challenge, we formulate the cuts generation stopping problem as a
reinforcement learning problem and propose a novel hybrid graph representation
model (HYGRO) to learn effective stopping strategies. An appealing feature of
HYGRO is that it can effectively capture both the dynamic and static features
of MILPs, enabling dynamic decision-making for the stopping strategies. To the
best of our knowledge, HYGRO is the first data-driven method to tackle the cuts
generation stopping problem. By integrating our approach with modern solvers,
experiments demonstrate that HYGRO significantly improves the efficiency of
solving MILPs compared to competitive baselines, achieving up to 31%
improvement.
\\ ( https://arxiv.org/abs/2401.17527 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17710
Date: Wed, 31 Jan 2024 09:59:59 GMT   (16988kb,D)

Title: Aesthetic Preference Prediction in Interior Design: Fuzzy Approach
Authors: Ayana Adilova and Pakizar Shamoi
Categories: cs.AI
Comments: Submitted to IEEE conference for consideration
\\
  Interior design is all about creating spaces that look and feel good.
However, the subjective nature of aesthetic preferences presents a significant
challenge in defining and quantifying what makes an interior design visually
appealing. The current paper addresses this gap by introducing a novel
methodology for quantifying and predicting aesthetic preferences in interior
design. Our study combines fuzzy logic with image processing techniques. We
collected a dataset of interior design images from social media platforms,
focusing on essential visual attributes such as color harmony, lightness, and
complexity. We integrate these features using weighted average to compute a
general aesthetic score. Our approach considers individual color preferences in
calculating the overall aesthetic preference. We initially gather user ratings
for primary colors like red, brown, and others to understand their preferences.
Then, we use the pixel count of the top five dominant colors in the image to
get the color scheme preference. The color scheme preference and the aesthetic
score are then passed as inputs to the fuzzy inference system to calculate an
overall preference score. This score represents a comprehensive measure of the
user's preference for a particular interior design, considering their color
choices and general aesthetic appeal. We used the 2AFC (Two-Alternative Forced
Choice) method to validate our methodology, achieving a notable hit rate of
0.7. This study can help designers and professionals better understand and meet
people's interior design preferences, especially in a world that relies heavily
on digital media.
\\ ( https://arxiv.org/abs/2401.17710 ,  16988kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17749
Date: Wed, 31 Jan 2024 11:14:29 GMT   (25750kb,D)

Title: SwarmBrain: Embodied agent for real-time strategy game StarCraft II via
  large language models
Authors: Xiao Shao, Weifu Jiang, Fei Zuo, Mengqing Liu
Categories: cs.AI
\\
  Large language models (LLMs) have recently garnered significant
accomplishments in various exploratory tasks, even surpassing the performance
of traditional reinforcement learning-based methods that have historically
dominated the agent-based field. The purpose of this paper is to investigate
the efficacy of LLMs in executing real-time strategy war tasks within the
StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an
embodied agent leveraging LLM for real-time strategy implementation in the
StarCraft II game environment. The SwarmBrain comprises two key components: 1)
a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed
to orchestrate macro-level strategies from a high-level perspective. This
matrix emulates the overarching consciousness of the Zerg intelligence brain,
synthesizing strategic foresight with the aim of allocating resources,
directing expansion, and coordinating multi-pronged assaults. 2) a Swarm
ReflexNet, which is agile counterpart to the calculated deliberation of the
Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the
Swarm ReflexNet employs a condition-response state machine framework, enabling
expedited tactical responses for fundamental Zerg unit maneuvers. In the
experimental setup, SwarmBrain is in control of the Zerg race in confrontation
with an Computer-controlled Terran adversary. Experimental results show the
capacity of SwarmBrain to conduct economic augmentation, territorial expansion,
and tactical formulation, and it shows the SwarmBrain is capable of achieving
victory against Computer players set at different difficulty levels.
\\ ( https://arxiv.org/abs/2401.17749 ,  25750kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17783
Date: Wed, 31 Jan 2024 12:26:59 GMT   (5395kb,D)

Title: SDRDPy: An application to graphically visualize the knowledge obtained
  with supervised descriptive rule algorithms
Authors: M.A. Padilla-Rascon, P. Gonzalez, C.J. Carmona
Categories: cs.AI
\\
  SDRDPy is a desktop application that allows experts an intuitive graphic and
tabular representation of the knowledge extracted by any supervised descriptive
rule discovery algorithm. The application is able to provide an analysis of the
data showing the relevant information of the data set and the relationship
between the rules, data and the quality measures associated for each rule
regardless of the tool where algorithm has been executed. All of the
information is presented in a user-friendly application in order to facilitate
expert analysis and also the exportation of reports in different formats.
\\ ( https://arxiv.org/abs/2401.17783 ,  5395kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17373
Date: Tue, 30 Jan 2024 19:01:24 GMT   (1296kb)

Title: Arabic Tweet Act: A Weighted Ensemble Pre-Trained Transformer Model for
  Classifying Arabic Speech Acts on Twitter
Authors: Khadejaa Alshehri, Areej Alhothali and Nahed Alowidi
Categories: cs.CL cs.AI
Comments: 16 pages, 6 figures
\\
  Speech acts are a speakers actions when performing an utterance within a
conversation, such as asking, recommending, greeting, or thanking someone,
expressing a thought, or making a suggestion. Understanding speech acts helps
interpret the intended meaning and actions behind a speakers or writers words.
This paper proposes a Twitter dialectal Arabic speech act classification
approach based on a transformer deep learning neural network. Twitter and
social media, are becoming more and more integrated into daily life. As a
result, they have evolved into a vital source of information that represents
the views and attitudes of their users. We proposed a BERT based weighted
ensemble learning approach to integrate the advantages of various BERT models
in dialectal Arabic speech acts classification. We compared the proposed model
against several variants of Arabic BERT models and sequence-based models. We
developed a dialectal Arabic tweet act dataset by annotating a subset of a
large existing Arabic sentiment analysis dataset (ASAD) based on six speech act
categories. We also evaluated the models on a previously developed Arabic Tweet
Act dataset (ArSAS). To overcome the class imbalance issue commonly observed in
speech act problems, a transformer-based data augmentation model was
implemented to generate an equal proportion of speech act categories. The
results show that the best BERT model is araBERTv2-Twitter models with a
macro-averaged F1 score and an accuracy of 0.73 and 0.84, respectively. The
performance improved using a BERT-based ensemble method with a 0.74 and 0.85
averaged F1 score and accuracy on our dataset, respectively.
\\ ( https://arxiv.org/abs/2401.17373 ,  1296kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17377
Date: Tue, 30 Jan 2024 19:03:49 GMT   (6464kb,D)

Title: Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion
  Tokens
Authors: Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh
  Hajishirzi
Categories: cs.CL cs.AI cs.IR
\\
  Are n-gram language models still relevant in this era of neural large
language models (LLMs)? Our answer is yes, and we show their values in both
text analysis and improving neural LLMs. Yet this necessitates modernizing
n-gram models in two aspects. First, we train them at the same data scale as
neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever
built. Second, existing n-gram models use small n which hinders their
performance; we instead allow n to be arbitrarily large, by introducing a new
$\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables
(which would be very expensive), we develop an engine named infini-gram --
powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram
with arbitrary n) probabilities with millisecond-level latency. The
$\infty$-gram framework and infini-gram engine enable us to conduct many novel
and interesting analyses of human-written and machine-generated text: we find
that the $\infty$-gram LM has fairly high accuracy for next-token prediction
(47%), and can complement neural LLMs to greatly reduce their language modeling
perplexities. When analyzing machine-generated text, we also observe
irregularities in the machine--$\infty$-gram agreement level with respect to
the suffix length, which indicates deficiencies in neural LLM pretraining and
the positional embeddings of Transformers. We open-source our infini-gram
engine in the hopes of enabling more study on how to best use verbatim
information retrieved from large text corpora.
\\ ( https://arxiv.org/abs/2401.17377 ,  6464kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17390
Date: Tue, 30 Jan 2024 19:13:12 GMT   (650kb,D)

Title: Customizing Language Model Responses with Contrastive In-Context
  Learning
Authors: Xiang Gao, Kamalika Das
Categories: cs.CL cs.AI
Comments: Accepted to appear at AAAI 2024
\\
  Large language models (LLMs) are becoming increasingly important for machine
learning applications. However, it can be challenging to align LLMs with our
intent, particularly when we want to generate content that is preferable over
others or when we want the LLM to respond in a certain style or tone that is
hard to describe. To address this challenge, we propose an approach that uses
contrastive examples to better describe our intent. This involves providing
positive examples that illustrate the true intent, along with negative examples
that show what characteristics we want LLMs to avoid. The negative examples can
be retrieved from labeled data, written by a human, or generated by the LLM
itself. Before generating an answer, we ask the model to analyze the examples
to teach itself what to avoid. This reasoning step provides the model with the
appropriate articulation of the user's need and guides it towards generting a
better answer. We tested our approach on both synthesized and real-world
datasets, including StackExchange and Reddit, and found that it significantly
improves performance compared to standard few-shot prompting
\\ ( https://arxiv.org/abs/2401.17390 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17396
Date: Tue, 30 Jan 2024 19:27:04 GMT   (560kb)

Title: Fine-tuning Transformer-based Encoder for Turkish Language Understanding
  Tasks
Authors: Savas Yildirim
Categories: cs.CL cs.AI
\\
  Deep learning-based and lately Transformer-based language models have been
dominating the studies of natural language processing in the last years. Thanks
to their accurate and fast fine-tuning characteristics, they have outperformed
traditional machine learning-based approaches and achieved state-of-the-art
results for many challenging natural language understanding (NLU) problems.
Recent studies showed that the Transformer-based models such as BERT, which is
Bidirectional Encoder Representations from Transformers, have reached
impressive achievements on many tasks. Moreover, thanks to their transfer
learning capacity, these architectures allow us to transfer pre-built models
and fine-tune them to specific NLU tasks such as question answering. In this
study, we provide a Transformer-based model and a baseline benchmark for the
Turkish Language. We successfully fine-tuned a Turkish BERT model, namely
BERTurk that is trained with base settings, to many downstream tasks and
evaluated with a the Turkish Benchmark dataset. We showed that our studies
significantly outperformed other existing baseline approaches for Named-Entity
Recognition, Sentiment Analysis, Question Answering and Text Classification in
Turkish Language. We publicly released these four fine-tuned models and
resources in reproducibility and with the view of supporting other Turkish
researchers and applications.
\\ ( https://arxiv.org/abs/2401.17396 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17461
Date: Tue, 30 Jan 2024 21:49:30 GMT   (7654kb,D)

Title: Synthetic Dialogue Dataset Generation using LLM Agents
Authors: Yelaman Abdullin, Diego Molla-Aliod, Bahadorreza Ofoghi, John
  Yearwood, Qingyang Li
Categories: cs.CL cs.AI
Comments: GEM Workshop @ EMNLP 2023
\\
  Linear programming (LP) problems are pervasive in real-life applications.
However, despite their apparent simplicity, an untrained user may find it
difficult to determine the linear model of their specific problem. We envisage
the creation of a goal-oriented conversational agent that will engage in
conversation with the user to elicit all information required so that a
subsequent agent can generate the linear model. In this paper, we present an
approach for the generation of sample dialogues that can be used to develop and
train such a conversational agent. Using prompt engineering, we develop two
agents that "talk" to each other, one acting as the conversational agent, and
the other acting as the user. Using a set of text descriptions of linear
problems from NL4Opt available to the user only, the agent and the user engage
in conversation until the agent has retrieved all key information from the
original problem description. We also propose an extrinsic evaluation of the
dialogues by assessing how well the summaries generated by the dialogues match
the original problem descriptions. We conduct human and automatic evaluations,
including an evaluation approach that uses GPT-4 to mimic the human evaluation
metrics. The evaluation results show an overall good quality of the dialogues,
though research is still needed to improve the quality of the GPT-4 evaluation
metrics. The resulting dialogues, including the human annotations of a subset,
are available to the research community. The conversational agent used for the
generation of the dialogues can be used as a baseline.
\\ ( https://arxiv.org/abs/2401.17461 ,  7654kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17464
Date: Tue, 30 Jan 2024 21:53:30 GMT   (734kb,D)

Title: Efficient Tool Use with Chain-of-Abstraction Reasoning
Authors: Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth
  Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut,
  Tianlu Wang
Categories: cs.CL
\\
  To achieve faithful reasoning that aligns with human expectations, large
language models (LLMs) need to ground their reasoning to real-world knowledge
(e.g., web facts, math and physical rules). Tools help LLMs access this
external knowledge, but there remains challenges for fine-tuning LLM agents
(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where
inter-connected tool calls require holistic and efficient tool usage planning.
  In this work, we propose a new method for LLMs to better leverage tools in
multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to
first decode reasoning chains with abstract placeholders, and then call domain
tools to reify each reasoning chain by filling in specific knowledge. This
planning with abstract chains enables LLMs to learn more general reasoning
strategies, which are robust to shifts of domain knowledge (e.g., math results)
relevant to different reasoning questions. It also allows LLMs to perform
decoding and calling of external tools in parallel, which avoids the inference
delay caused by waiting for tool responses. In mathematical reasoning and Wiki
QA domains, we show that our method consistently outperforms previous
chain-of-thought and tool-augmented baselines on both in-distribution and
out-of-distribution test sets, with an average ~6% absolute QA accuracy
improvement. LLM agents trained with our method also show more efficient tool
use, with inference speed being on average ~1.4x faster than baseline
tool-augmented LLMs.
\\ ( https://arxiv.org/abs/2401.17464 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17477
Date: Tue, 30 Jan 2024 22:22:55 GMT   (1161kb)

Title: Detecting mental disorder on social media: a ChatGPT-augmented
  explainable approach
Authors: Loris Belcastro, Riccardo Cantini, Fabrizio Marozzo, Domenico Talia,
  Paolo Trunfio
Categories: cs.CL cs.AI cs.LG cs.SI
\\
  In the digital era, the prevalence of depressive symptoms expressed on social
media has raised serious concerns, necessitating advanced methodologies for
timely detection. This paper addresses the challenge of interpretable
depression detection by proposing a novel methodology that effectively combines
Large Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and
conversational agents like ChatGPT. In our methodology, explanations are
achieved by integrating BERTweet, a Twitter-specific variant of BERT, into a
novel self-explanatory model, namely BERT-XDD, capable of providing both
classification and explanations via masked attention. The interpretability is
further enhanced using ChatGPT to transform technical explanations into
human-readable commentaries. By introducing an effective and modular approach
for interpretable depression detection, our methodology can contribute to the
development of socially responsible digital platforms, fostering early
intervention and support for mental health challenges under the guidance of
qualified healthcare professionals.
\\ ( https://arxiv.org/abs/2401.17477 ,  1161kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17498
Date: Tue, 30 Jan 2024 23:08:26 GMT   (721kb,D)

Title: Improving QA Model Performance with Cartographic Inoculation
Authors: Allen Chen (UT Austin), Okan Tankirulu (UT Austin)
Categories: cs.CL
Comments: 9 pages, 6 figures
ACM-class: I.2.7
\\
  QA models are faced with complex and open-ended contextual reasoning
problems, but can often learn well-performing solution heuristics by exploiting
dataset-specific patterns in their training data. These patterns, or "dataset
artifacts", reduce the model's ability to generalize to real-world QA problems.
Utilizing an ElectraSmallDiscriminator model trained for QA, we analyze the
impacts and incidence of dataset artifacts using an adversarial challenge set
designed to confuse models reliant on artifacts for prediction. Extending
existing work on methods for mitigating artifact impacts, we propose
cartographic inoculation, a novel method that fine-tunes models on an optimized
subset of the challenge data to reduce model reliance on dataset artifacts. We
show that by selectively fine-tuning a model on ambiguous adversarial examples
from a challenge set, significant performance improvements can be made on the
full challenge dataset with minimal loss of model generalizability to other
challenging environments and QA datasets.
\\ ( https://arxiv.org/abs/2401.17498 ,  721kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17514
Date: Wed, 31 Jan 2024 00:15:34 GMT   (220kb,D)

Title: FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation
Authors: Rheeya Uppaal, Yixuan Li, Junjie Hu
Categories: cs.CL
\\
  A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled
data from both source and target domains to learn domain-invariant
representations for adaptation. However, these methods showcase certain
limitations, encouraging the use of self-supervised learning through continued
pre-training. The necessity of continued pre-training or learning
domain-invariant representations is still unclear in the prompt-based
classification framework, where an input example is modified by a template and
then fed into a language model (LM) to generate a label string. To examine this
new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy
UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and
labeled examples using two different instruction-tuning tasks. Specifically,
the first task trains the LM on unlabeled texts from both domains via masked
language modeling (MLM), and the other uses supervised instruction-tuning on
source-labeled data for classification. We conduct extensive experiments on 24
real-world domain pairs to show the effectiveness of our method over strong
domain-invariant learning methods. Our analysis sheds light on why masked
language modeling improves target-domain classification performance in
prompt-based UDA. We discover that MLM helps the model learn both semantic and
background knowledge of a domain, which are both beneficial for downstream
classification.
\\ ( https://arxiv.org/abs/2401.17514 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17536
Date: Wed, 31 Jan 2024 01:37:33 GMT   (466kb,D)

Title: PipeNet: Question Answering with Semantic Pruning over Knowledge Graphs
Authors: Ying Su, Jipeng Zhang, Yangqiu Song, Tong Zhang
Categories: cs.CL
Comments: 8 pages, 4 figures
\\
  It is well acknowledged that incorporating explicit knowledge graphs (KGs)
can benefit question answering. Existing approaches typically follow a
grounding-reasoning pipeline in which entity nodes are first grounded for the
query (question and candidate answers), and then a reasoning module reasons
over the matched multi-hop subgraph for answer prediction. Although the
pipeline largely alleviates the issue of extracting essential information from
giant KGs, efficiency is still an open challenge when scaling up hops in
grounding the subgraphs. In this paper, we target at finding semantically
related entity nodes in the subgraph to improve the efficiency of graph
reasoning with KG. We propose a grounding-pruning-reasoning pipeline to prune
noisy nodes, remarkably reducing the computation cost and memory usage while
also obtaining decent subgraph representation. In detail, the pruning module
first scores concept nodes based on the dependency distance between matched
spans and then prunes the nodes according to score ranks. To facilitate the
evaluation of pruned subgraphs, we also propose a graph attention network (GAT)
based module to reason with the subgraph data. Experimental results on
CommonsenseQA and OpenBookQA demonstrate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2401.17536 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17574
Date: Wed, 31 Jan 2024 03:39:07 GMT   (2255kb,D)

Title: Scavenging Hyena: Distilling Transformers into Long Convolution Models
Authors: Tokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad
  Sami Nur Islam, Wassim Jabbour, Laurence Liang
Categories: cs.CL cs.LG
Comments: 9 pages, 2 figures
\\
  The rapid evolution of Large Language Models (LLMs), epitomized by
architectures like GPT-4, has reshaped the landscape of natural language
processing. This paper introduces a pioneering approach to address the
efficiency concerns associated with LLM pre-training, proposing the use of
knowledge distillation for cross-architecture transfer. Leveraging insights
from the efficient Hyena mechanism, our method replaces attention heads in
transformer models by Hyena, offering a cost-effective alternative to
traditional pre-training while confronting the challenge of processing long
contextual information, inherent in quadratic attention mechanisms. Unlike
conventional compression-focused methods, our technique not only enhances
inference speed but also surpasses pre-training in terms of both accuracy and
efficiency. In the era of evolving LLMs, our work contributes to the pursuit of
sustainable AI solutions, striking a balance between computational power and
environmental impact.
\\ ( https://arxiv.org/abs/2401.17574 ,  2255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17585
Date: Wed, 31 Jan 2024 04:12:59 GMT   (12992kb,D)

Title: Propagation and Pitfalls: Reasoning-based Assessment of Knowledge
  Editing through Counterfactual Tasks
Authors: Wenyue Hua, Jiang Guo, Mingwen Dong, Henghui Zhu, Patrick Ng, Zhiguo
  Wang
Categories: cs.CL cs.AI cs.LG stat.ME
Comments: 22 pages, 14 figures, 5 tables
\\
  Current approaches of knowledge editing struggle to effectively propagate
updates to interconnected facts. In this work, we delve into the barriers that
hinder the appropriate propagation of updated knowledge within these models for
accurate reasoning. To support our analysis, we introduce a novel
reasoning-based benchmark -- ReCoE (Reasoning-based Counterfactual Editing
dataset) -- which covers six common reasoning schemes in real world. We conduct
a thorough analysis of existing knowledge editing techniques, including input
augmentation, finetuning, and locate-and-edit. We found that all model editing
methods show notably low performance on this dataset, especially in certain
reasoning schemes. Our analysis over the chain-of-thought generation of edited
models further uncover key reasons behind the inadequacy of existing knowledge
editing methods from a reasoning standpoint, involving aspects on fact-wise
editing, fact recall ability, and coherence in generation. We will make our
benchmark publicly available.
\\ ( https://arxiv.org/abs/2401.17585 ,  12992kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17588
Date: Wed, 31 Jan 2024 04:19:22 GMT   (8001kb,D)

Title: Local and Global Contexts for Conversation
Authors: Zuoquan Lin and Xinyi Shen
Categories: cs.CL
Comments: 11 pages, 3 figures
\\
  The context in conversation is the dialog history crucial for multi-turn
dialogue. Learning from the relevant contexts in dialog history for grounded
conversation is a challenging problem. Local context is the most neighbor and
more sensitive to the subsequent response, and global context is relevant to a
whole conversation far beyond neighboring utterances. Currently, pretrained
transformer models for conversation challenge capturing the correlation and
connection between local and global contexts. We introduce a local and global
conversation model (LGCM) for general-purpose conversation in open domain. It
is a local-global hierarchical transformer model that excels at accurately
discerning and assimilating the relevant contexts necessary for generating
responses. It employs a local encoder to grasp the local context at the level
of individual utterances and a global encoder to understand the broader context
at the dialogue level. The seamless fusion of these locally and globally
contextualized encodings ensures a comprehensive comprehension of the
conversation. Experiments on popular datasets show that LGCM outperforms the
existing conversation models on the performance of automatic metrics with
significant margins.
\\ ( https://arxiv.org/abs/2401.17588 ,  8001kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17597
Date: Wed, 31 Jan 2024 04:50:00 GMT   (398kb,D)

Title: SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization
Authors: Sangwoo Cho, Kaiqiang Song, Chao Zhao, Xiaoyang Wang, Dong Yu
Categories: cs.CL
Comments: 11 pages, 2 figures
\\
  Multi-turn dialogues are characterized by their extended length and the
presence of turn-taking conversations. Traditional language models often
overlook the distinct features of these dialogues by treating them as regular
text. In this paper, we propose a speaker-enhanced pre-training method for long
dialogue summarization, which leverages the inherent structure of multiple-turn
dialogues. To support our study, we curate a diverse dataset that includes
transcripts from real-world scenarios, movie or TV show transcripts, and
dialogues generated by a Large Language Model. We then perform a pre-training,
which encompasses the detection of speaker changes, and masked utterance
generation. Experimental results of fine-tuned models demonstrate that our
model achieves state-of-the-art performance on downstream benchmarks with long
context, surpassing baseline models and highlighting the effectiveness of our
approach. Our findings highlight the importance of curating pre-training
datasets that exhibit diversity and variations in length distribution to ensure
effective alignment with downstream datasets.
\\ ( https://arxiv.org/abs/2401.17597 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17600
Date: Wed, 31 Jan 2024 04:57:12 GMT   (14711kb,D)

Title: Good at captioning, bad at counting: Benchmarking GPT-4V on Earth
  observation data
Authors: Chenhui Zhang, Sherrie Wang
Categories: cs.CL cs.AI cs.CV
Comments: 62 pages; work in progress
\\
  Large Vision-Language Models (VLMs) have demonstrated impressive performance
on complex tasks involving visual input with natural language instructions.
However, it remains unclear to what extent capabilities on natural images
transfer to Earth observation (EO) data, which are predominantly satellite and
aerial images less common in VLM training data. In this work, we propose a
comprehensive benchmark to gauge the progress of VLMs toward being useful tools
for EO data by assessing their abilities on scene understanding, localization
and counting, and change detection tasks. Motivated by real-world applications,
our benchmark includes scenarios like urban monitoring, disaster relief, land
use, and conservation. We discover that, although state-of-the-art VLMs like
GPT-4V possess extensive world knowledge that leads to strong performance on
open-ended tasks like location understanding and image captioning, their poor
spatial reasoning limits usefulness on object localization and counting tasks.
Our benchmark will be made publicly available at https://vleo.danielz.ch/ and
on Hugging Face at
https://huggingface.co/collections/mit-ei/vleo-benchmark-datasets-65b789b0466555489cce0d70
for easy model evaluation.
\\ ( https://arxiv.org/abs/2401.17600 ,  14711kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17602
Date: Wed, 31 Jan 2024 05:11:00 GMT   (583kb,D)

Title: Assertion Detection Large Language Model In-context Learning LoRA
  Fine-tuning
Authors: Yuelyu Ji, Zeshui Yu and Yanshan Wang
Categories: cs.CL
\\
  In this study, we aim to address the task of assertion detection when
extracting medical concepts from clinical notes, a key process in clinical
natural language processing (NLP). Assertion detection in clinical NLP usually
involves identifying assertion types for medical concepts in the clinical text,
namely certainty (whether the medical concept is positive, negated, possible,
or hypothetical), temporality (whether the medical concept is for present or
the past history), and experiencer (whether the medical concept is described
for the patient or a family member). These assertion types are essential for
healthcare professionals to quickly and clearly understand the context of
medical conditions from unstructured clinical texts, directly influencing the
quality and outcomes of patient care. Although widely used, traditional
methods, particularly rule-based NLP systems and machine learning or deep
learning models, demand intensive manual efforts to create patterns and tend to
overlook less common assertion types, leading to an incomplete understanding of
the context. To address this challenge, our research introduces a novel
methodology that utilizes Large Language Models (LLMs) pre-trained on a vast
array of medical data for assertion detection. We enhanced the current method
with advanced reasoning techniques, including Tree of Thought (ToT), Chain of
Thought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank
Adaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010
assertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11
improvements over the previous works. To further assess the generalizability of
our approach, we extended our evaluation to a local dataset that focused on
sleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31
higher than the previous method.
\\ ( https://arxiv.org/abs/2401.17602 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17623
Date: Wed, 31 Jan 2024 06:49:36 GMT   (561kb,D)

Title: Neighboring Perturbations of Knowledge Editing on Large Language Models
Authors: Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, Zhen-Hua Ling
Categories: cs.CL
\\
  Despite their exceptional capabilities, large language models (LLMs) are
prone to generating unintended text due to false or outdated knowledge. Given
the resource-intensive nature of retraining LLMs, there has been a notable
increase in the development of knowledge editing. However, current approaches
and evaluations rarely explore the perturbation of editing on neighboring
knowledge. This paper studies whether updating new knowledge to LLMs perturbs
the neighboring knowledge encapsulated within them. Specifically, we seek to
figure out whether appending a new answer into an answer list to a factual
question leads to catastrophic forgetting of original correct answers in this
list, as well as unintentional inclusion of incorrect answers. A metric of
additivity is introduced and a benchmark dubbed as Perturbation Evaluation of
Appending Knowledge (PEAK) is constructed to evaluate the degree of
perturbation to neighboring knowledge when appending new knowledge. Besides, a
plug-and-play framework termed Appending via Preservation and Prevention (APP)
is proposed to mitigate the neighboring perturbation by maintaining the
integrity of the answer list. Experiments demonstrate the effectiveness of APP
coupling with four editing methods on three LLMs.
\\ ( https://arxiv.org/abs/2401.17623 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17632
Date: Wed, 31 Jan 2024 07:23:22 GMT   (124kb,D)

Title: What Do Self-Supervised Speech and Speaker Models Learn? New Findings
  From a Cross Model Layer-Wise Analysis
Authors: Takanori Ashihara, Marc Delcroix, Takafumi Moriya, Kohei Matsuura,
  Taichi Asami, Yusuke Ijima
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at ICASSP 2024
\\
  Self-supervised learning (SSL) has attracted increased attention for learning
meaningful speech representations. Speech SSL models, such as WavLM, employ
masked prediction training to encode general-purpose representations. In
contrast, speaker SSL models, exemplified by DINO-based models, adopt
utterance-level training objectives primarily for speaker representation.
Understanding how these models represent information is essential for refining
model efficiency and effectiveness. Unlike the various analyses of speech SSL,
there has been limited investigation into what information speaker SSL captures
and how its representation differs from speech SSL or other fully-supervised
speaker models. This paper addresses these fundamental questions. We explore
the capacity to capture various speech properties by applying SUPERB evaluation
probing tasks to speech and speaker SSL models. We also examine which layers
are predominantly utilized for each task to identify differences in how speech
is represented. Furthermore, we conduct direct comparisons to measure the
similarities between layers within and across models. Our analysis unveils that
1) the capacity to represent content information is somewhat unrelated to
enhanced speaker representation, 2) specific layers of speech SSL models would
be partly specialized in capturing linguistic information, and 3) speaker SSL
models tend to disregard linguistic information but exhibit more sophisticated
speaker representation.
\\ ( https://arxiv.org/abs/2401.17632 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17633
Date: Wed, 31 Jan 2024 07:26:47 GMT   (7807kb,D)

Title: Navigating the OverKill in Large Language Models
Authors: Chenyu Shi, Xiao Wang, Qiming Ge, Songyang Gao, Xianjun Yang, Tao Gui,
  Qi Zhang, Xuanjing Huang, Xun Zhao, Dahua Lin
Categories: cs.CL cs.AI
\\
  Large language models are meticulously aligned to be both helpful and
harmless. However, recent research points to a potential overkill which means
models may refuse to answer benign queries. In this paper, we investigate the
factors for overkill by exploring how models handle and determine the safety of
queries. Our findings reveal the presence of shortcuts within models, leading
to an over-attention of harmful words like 'kill' and prompts emphasizing
safety will exacerbate overkill. Based on these insights, we introduce
Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic
strategy, to alleviate this phenomenon. We first extract such over-attention by
amplifying the difference in the model's output distributions when responding
to system prompts that either include or omit an emphasis on safety. Then we
determine the final next-token predictions by downplaying the over-attention
from the model via contrastive decoding. Empirical results indicate that our
method has achieved an average reduction of the refusal rate by 20\% while
having almost no impact on safety.
\\ ( https://arxiv.org/abs/2401.17633 ,  7807kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17658
Date: Wed, 31 Jan 2024 08:28:06 GMT   (7958kb,D)

Title: Document Structure in Long Document Transformers
Authors: Jan Buchmann, Max Eichler, Jan-Micha Bodensohn, Ilia Kuznetsov, Iryna
  Gurevych
Categories: cs.CL
Comments: Accepted at EACL 2024. Code and data:
  http://github.com/UKPLab/eacl2024-doc-structure
\\
  Long documents often exhibit structure with hierarchically organized elements
of different functions, such as section headers and paragraphs. Despite the
omnipresence of document structure, its role in natural language processing
(NLP) remains opaque. Do long-document Transformer models acquire an internal
representation of document structure during pre-training? How can structural
information be communicated to a model after pre-training, and how does it
influence downstream performance? To answer these questions, we develop a novel
suite of probing tasks to assess structure-awareness of long-document
Transformers, propose general-purpose structure infusion methods, and evaluate
the effects of structure infusion on QASPER and Evidence Inference, two
challenging long-document NLP tasks. Results on LED and LongT5 suggest that
they acquire implicit understanding of document structure during pre-training,
which can be further enhanced by structure infusion, leading to improved
end-task performance. To foster research on the role of document structure in
NLP modeling, we make our data and code publicly available.
\\ ( https://arxiv.org/abs/2401.17658 ,  7958kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17671
Date: Wed, 31 Jan 2024 08:48:35 GMT   (4224kb,D)

Title: Contextual Feature Extraction Hierarchies Converge in Large Language
  Models and the Brain
Authors: Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta and
  Nima Mesgarani
Categories: cs.CL cs.AI q-bio.NC
Comments: 19 pages, 5 figures and 4 supplementary figures
\\
  Recent advancements in artificial intelligence have sparked interest in the
parallels between large language models (LLMs) and human neural processing,
particularly in language comprehension. While prior research has established
similarities in the representation of LLMs and the brain, the underlying
computational principles that cause this convergence, especially in the context
of evolving LLMs, remain elusive. Here, we examined a diverse selection of
high-performance LLMs with similar parameter sizes to investigate the factors
contributing to their alignment with the brain's language processing
mechanisms. We find that as LLMs achieve higher performance on benchmark tasks,
they not only become more brain-like as measured by higher performance when
predicting neural responses from LLM embeddings, but also their hierarchical
feature extraction pathways map more closely onto the brain's while using fewer
layers to do the same encoding. We also compare the feature extraction pathways
of the LLMs to each other and identify new ways in which high-performing models
have converged toward similar hierarchical processing mechanisms. Finally, we
show the importance of contextual information in improving model performance
and brain similarity. Our findings reveal the converging aspects of language
processing in the brain and LLMs and offer new directions for developing models
that align more closely with human cognitive processing.
\\ ( https://arxiv.org/abs/2401.17671 ,  4224kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17686
Date: Wed, 31 Jan 2024 09:16:35 GMT   (8133kb,D)

Title: Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought
  Reasoning
Authors: Tinghui Zhu, Kai Zhang, Jian Xie, Yu Su
Categories: cs.CL
\\
  Recent advancements have significantly augmented the reasoning capabilities
of Large Language Models (LLMs) through various methodologies, especially
chain-of-thought (CoT) reasoning. However, previous methods fail to address
reasoning errors in intermediate steps, leading to accumulative errors.In this
paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT
and deductive reasoning with step-wise beam search for LLMs. Our approach
deploys a verifier, verifying the deducibility of a reasoning step and its
premises, thus alleviating the error accumulation. Furthermore, we introduce a
scalable and labor-free data construction method to amplify our model's
verification capabilities. Extensive experiments demonstrate that our approach
significantly enhances the base performance of LLMs of various scales (7B, 13B,
70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres,
including arithmetic, commonsense, and symbolic. Moreover, our analysis proves
DBS's capability of detecting diverse and subtle reasoning errors and
robustness on different model scales.
\\ ( https://arxiv.org/abs/2401.17686 ,  8133kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17692
Date: Wed, 31 Jan 2024 09:28:06 GMT   (229kb,D)

Title: Mitigating the Problem of Strong Priors in LMs with Context
  Extrapolation
Authors: Raymond Douglas, Andis Draguns, Tom\'a\v{s} Gaven\v{c}iak
Categories: cs.CL
Comments: 12 pages, 4 figures
\\
  Language models (LMs) have become important tools in a variety of
applications, from data processing to the creation of instruction-following
assistants. But despite their advantages, LMs have certain idiosyncratic
limitations such as the problem of `strong priors', where a model learns to
output typical continuations in response to certain, usually local, portions of
the input regardless of any earlier instructions. For example, prompt injection
attacks can induce models to ignore explicit directives. In some cases, larger
models have been shown to be more susceptible to these problems than similar
smaller models, an example of the phenomenon of `inverse scaling'. We develop a
new technique for mitigating the problem of strong priors: we take the original
set of instructions, produce a weakened version of the original prompt that is
even more susceptible to the strong priors problem, and then extrapolate the
continuation away from the weakened prompt. This lets us infer how the model
would continue a hypothetical strengthened set of instructions. Our technique
conceptualises LMs as mixture models which combine a family of data generation
processes, reinforcing the desired elements of the mixture. Our approach works
at inference time, removing any need for retraining. We apply it to eleven
models including GPT-2, GPT-3, Llama 2, and Mistral on four tasks, and find
improvements in 41/44. Across all 44 combinations the median increase in
proportion of tasks completed is 40%.
\\ ( https://arxiv.org/abs/2401.17692 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17703
Date: Wed, 31 Jan 2024 09:49:22 GMT   (8626kb,D)

Title: WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts
Authors: Pardis Sadat Zahraei, Ali Emami
Categories: cs.CL cs.CY
Comments: Accepted for publication in main proceedings of EACL 2024 conference,
  22 pages, 16 figures
ACM-class: I.2.7; K.4.1
\\
  The Winograd Schema Challenge (WSC) serves as a prominent benchmark for
evaluating machine understanding. While Large Language Models (LLMs) excel at
answering WSC questions, their ability to generate such questions remains less
explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting
method which enhances the generation of WSC instances (50% valid cases vs. 10%
in recent methods). Using this approach, we introduce WSC+, a novel dataset
comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework
by incorporating new 'ambiguous' and 'offensive' categories, providing a deeper
insight into model overconfidence and bias. Our analysis reveals nuances in
generation-evaluation consistency, suggesting that LLMs may not always
outperform in evaluating their own generated questions when compared to those
crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an
accuracy of 68.7%, significantly below the human benchmark of 95.1%.
\\ ( https://arxiv.org/abs/2401.17703 ,  8626kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17716
Date: Wed, 31 Jan 2024 10:20:01 GMT   (265kb,D)

Title: Enhancing Large Language Model with Decomposed Reasoning for Emotion
  Cause Pair Extraction
Authors: Jialiang Wu, Yi Shen, Ziheng Zhang, Longjun Cai
Categories: cs.CL
Comments: 13 pages, 5 figures
\\
  Emotion-Cause Pair Extraction (ECPE) involves extracting clause pairs
representing emotions and their causes in a document. Existing methods tend to
overfit spurious correlations, such as positional bias in existing benchmark
datasets, rather than capturing semantic features. Inspired by recent work, we
explore leveraging large language model (LLM) to address ECPE task without
additional training. Despite strong capabilities, LLMs suffer from
uncontrollable outputs, resulting in mediocre performance. To address this, we
introduce chain-of-thought to mimic human cognitive process and propose the
Decomposed Emotion-Cause Chain (DECC) framework. Combining inducing inference
and logical pruning, DECC guides LLMs to tackle ECPE task. We further enhance
the framework by incorporating in-context learning. Experiment results
demonstrate the strength of DECC compared to state-of-the-art supervised
fine-tuning methods. Finally, we analyze the effectiveness of each component
and the robustness of the method in various scenarios, including different LLM
bases, rebalanced datasets, and multi-pair extraction.
\\ ( https://arxiv.org/abs/2401.17716 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17755
Date: Wed, 31 Jan 2024 11:30:24 GMT   (440kb,D)

Title: CauESC: A Causal Aware Model for Emotional Support Conversation
Authors: Wei Chen, Hengxu Lin, Qun Zhang, Xiaojin Zhang, Xiang Bai, Xuanjing
  Huang, Zhongyu Wei
Categories: cs.CL
Comments: 15 pages, 5 figures
ACM-class: I.2.7
\\
  Emotional Support Conversation aims at reducing the seeker's emotional
distress through supportive response. Existing approaches have two limitations:
(1) They ignore the emotion causes of the distress, which is important for
fine-grained emotion understanding; (2) They focus on the seeker's own mental
state rather than the emotional dynamics during interaction between speakers.
To address these issues, we propose a novel framework CauESC, which firstly
recognizes the emotion causes of the distress, as well as the emotion effects
triggered by the causes, and then understands each strategy of verbal grooming
independently and integrates them skillfully. Experimental results on the
benchmark dataset demonstrate the effectiveness of our approach and show the
benefits of emotion understanding from cause to effect and
independent-integrated strategy modeling.
\\ ( https://arxiv.org/abs/2401.17755 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17809
Date: Wed, 31 Jan 2024 13:08:45 GMT   (206kb,D)

Title: SWEA: Changing Factual Knowledge in Large Language Models via Subject
  Word Embedding Altering
Authors: Xiaopeng Li, Shasha Li, Bin Ji, Shezheng Song, Xi Wang, Jun Ma, Jie
  Yu, Xiaodong Liu, Jing Wang and Weimin Zhang
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress; Our code will be released
\\
  Model editing has recently gained widespread attention. Current model editing
methods primarily involve modifying model parameters or adding additional
modules to the existing model. However, the former causes irreversible damage
to LLMs, while the latter incurs additional inference overhead and fuzzy vector
matching is not always reliable. To address these issues, we propose an
expandable Subject Word Embedding Altering (SWEA) framework, which modifies the
representation of subjects and achieve the goal of editing knowledge during the
inference stage. SWEA uses precise key matching outside the model and performs
reliable subject word embedding altering, thus protecting the original weights
of the model without increasing inference overhead. We then propose optimizing
then suppressing fusion method, which first optimizes the embedding vector for
the editing target and then suppresses the Knowledge Embedding Dimension (KED)
to obtain the final fused embedding. We thus propose SWEAOS method for editing
factual knowledge in LLMs. We demonstrate the state-of-the-art performance of
SWEAOS on the COUNTERFACT and zsRE datasets. To further validate the reasoning
ability of SWEAOS in editing knowledge, we evaluate it on the more complex
RIPPLEEDITS benchmark. The results on two subdatasets demonstrate that our
SWEAOS possesses state-of-the-art reasoning ability.
\\ ( https://arxiv.org/abs/2401.17809 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17824
Date: Wed, 31 Jan 2024 13:35:07 GMT   (1438kb,D)

Title: A Survey of Pre-trained Language Models for Processing Scientific Text
Authors: Xanh Ho, Anh Khoa Duong Nguyen, An Tuan Dao, Junfeng Jiang, Yuki
  Chida, Kaito Sugimoto, Huy Quoc To, Florian Boudin and Akiko Aizawa
Categories: cs.CL
Comments: Resources are available at https://github.com/Alab-NII/Awesome-SciLM
\\
  The number of Language Models (LMs) dedicated to processing scientific text
is on the rise. Keeping pace with the rapid growth of scientific LMs (SciLMs)
has become a daunting task for researchers. To date, no comprehensive surveys
on SciLMs have been undertaken, leaving this issue unaddressed. Given the
constant stream of new SciLMs, appraising the state-of-the-art and how they
compare to each other remain largely unknown. This work fills that gap and
provides a comprehensive review of SciLMs, including an extensive analysis of
their effectiveness across different domains, tasks and datasets, and a
discussion on the challenges that lie ahead.
\\ ( https://arxiv.org/abs/2401.17824 ,  1438kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17827
Date: Wed, 31 Jan 2024 13:40:00 GMT   (282kb,D)

Title: Neural Machine Translation for Malayalam Paraphrase Generation
Authors: Christeena Varghese, Sergey Koshelev, Ivan P. Yamshchikov
Categories: cs.CL cs.AI
ACM-class: I.7.0; I.2.7
\\
  This study explores four methods of generating paraphrases in Malayalam,
utilizing resources available for English paraphrasing and pre-trained Neural
Machine Translation (NMT) models. We evaluate the resulting paraphrases using
both automated metrics, such as BLEU, METEOR, and cosine similarity, as well as
human annotation. Our findings suggest that automated evaluation measures may
not be fully appropriate for Malayalam, as they do not consistently align with
human judgment. This discrepancy underscores the need for more nuanced
paraphrase evaluation approaches especially for highly agglutinative languages.
\\ ( https://arxiv.org/abs/2401.17827 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17839
Date: Wed, 31 Jan 2024 13:57:24 GMT   (6685kb,D)

Title: Global-Liar: Factuality of LLMs over Time and Geographic Regions
Authors: Shujaat Mirza, Bruno Coelho, Yuyuan Cui, Christina P\"opper, Damon
  McCoy
Categories: cs.CL cs.AI cs.IR
Comments: 24 pages, 12 figures, 9 tables
\\
  The increasing reliance on AI-driven solutions, particularly Large Language
Models (LLMs) like the GPT series, for information retrieval highlights the
critical need for their factuality and fairness, especially amidst the rampant
spread of misinformation and disinformation online. Our study evaluates the
factual accuracy, stability, and biases in widely adopted GPT models, including
GPT-3.5 and GPT-4, contributing to reliability and integrity of AI-mediated
information dissemination.
  We introduce 'Global-Liar,' a dataset uniquely balanced in terms of
geographic and temporal representation, facilitating a more nuanced evaluation
of LLM biases. Our analysis reveals that newer iterations of GPT models do not
always equate to improved performance. Notably, the GPT-4 version from March
demonstrates higher factual accuracy than its subsequent June release.
Furthermore, a concerning bias is observed, privileging statements from the
Global North over the Global South, thus potentially exacerbating existing
informational inequities. Regions such as Africa and the Middle East are at a
disadvantage, with much lower factual accuracy. The performance fluctuations
over time suggest that model updates may not consistently benefit all regions
equally.
  Our study also offers insights into the impact of various LLM configuration
settings, such as binary decision forcing, model re-runs and temperature, on
model's factuality. Models constrained to binary (true/false) choices exhibit
reduced factuality compared to those allowing an 'unclear' option. Single
inference at a low temperature setting matches the reliability of majority
voting across various configurations. The insights gained highlight the need
for culturally diverse and geographically inclusive model training and
evaluation. This approach is key to achieving global equity in technology,
distributing AI benefits fairly worldwide.
\\ ( https://arxiv.org/abs/2401.17839 ,  6685kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17858
Date: Wed, 31 Jan 2024 14:19:03 GMT   (1308kb,D)

Title: Probing Language Models' Gesture Understanding for Enhanced Human-AI
  Interaction
Authors: Philipp Wicke
Categories: cs.CL
Comments: Preprint
\\
  The rise of Large Language Models (LLMs) has affected various disciplines
that got beyond mere text generation. Going beyond their textual nature, this
project proposal aims to investigate the interaction between LLMs and
non-verbal communication, specifically focusing on gestures. The proposal sets
out a plan to examine the proficiency of LLMs in deciphering both explicit and
implicit non-verbal cues within textual prompts and their ability to associate
these gestures with various contextual factors. The research proposes to test
established psycholinguistic study designs to construct a comprehensive dataset
that pairs textual prompts with detailed gesture descriptions, encompassing
diverse regional variations, and semantic labels. To assess LLMs' comprehension
of gestures, experiments are planned, evaluating their ability to simulate
human behaviour in order to replicate psycholinguistic experiments. These
experiments consider cultural dimensions and measure the agreement between
LLM-identified gestures and the dataset, shedding light on the models'
contextual interpretation of non-verbal cues (e.g. gestures).
\\ ( https://arxiv.org/abs/2401.17858 ,  1308kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17882
Date: Wed, 31 Jan 2024 14:41:23 GMT   (224kb,D)

Title: I Think, Therefore I am: Awareness in Large Language Models
Authors: Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan and Lichao Sun
Categories: cs.CL
\\
  Do large language models (LLMs) exhibit any forms of awareness similar to
humans? In this paper, we introduce the concept of awareness to LLMs, arguing
that awareness is an essential aspect of trustworthiness for LLMs to enhance
their interaction with humans while ensuring ethical responses. We define
awareness in LLMs as the ability to perceive and understand themselves as AI
models and to exhibit social intelligence. We identify four key dimensions of
awareness: capability, mission, emotion, and perspective. To assess LLMs on
these dimensions, we introduce a specialized dataset, AwareLLM dataset. Our
findings reveal that LLMs demonstrate a decent degree of awareness, though they
still lack substantial capability awareness.
\\ ( https://arxiv.org/abs/2401.17882 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17897
Date: Wed, 31 Jan 2024 15:04:01 GMT   (113kb)

Title: Employing Label Models on ChatGPT Answers Improves Legal Text Entailment
  Performance
Authors: Chau Nguyen and Le-Minh Nguyen
Categories: cs.CL
Comments: 15 pages
\\
  The objective of legal text entailment is to ascertain whether the assertions
in a legal query logically follow from the information provided in one or
multiple legal articles. ChatGPT, a large language model, is robust in many
natural language processing tasks, including legal text entailment: when we set
the temperature = 0 (the ChatGPT answers are deterministic) and prompt the
model, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms
the previous SOTA of 67.89%. On the other hand, if the temperature is larger
than zero, ChatGPT answers are not deterministic, leading to inconsistent
answers and fluctuating results. We propose to leverage label models (a
fundamental component of weak supervision techniques) to integrate the
provisional answers by ChatGPT into consolidated labels. By that way, we treat
ChatGPT provisional answers as noisy predictions which can be consolidated by
label models. The experimental results demonstrate that this approach can
attain an accuracy of 76.15%, marking a significant improvement of 8.26% over
the prior state-of-the-art benchmark. Additionally, we perform an analysis of
the instances where ChatGPT produces incorrect answers, then we classify the
errors, offering insights that could guide potential enhancements for future
research endeavors.
\\ ( https://arxiv.org/abs/2401.17897 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17911
Date: Wed, 31 Jan 2024 15:16:25 GMT   (5429kb,D)

Title: SNNLP: Energy-Efficient Natural Language Processing Using Spiking Neural
  Networks
Authors: R. Alexander Knipper, Kaniz Mishty, Mehdi Sadi, Shubhra Kanti Karmaker
  Santu
Categories: cs.CL
\\
  As spiking neural networks receive more attention, we look toward
applications of this computing paradigm in fields other than computer vision
and signal processing. One major field, underexplored in the neuromorphic
setting, is Natural Language Processing (NLP), where most state-of-the-art
solutions still heavily rely on resource-consuming and power-hungry traditional
deep learning architectures. Therefore, it is compelling to design NLP models
for neuromorphic architectures due to their low energy requirements, with the
additional benefit of a more human-brain-like operating model for processing
information. However, one of the biggest issues with bringing NLP to the
neuromorphic setting is in properly encoding text into a spike train so that it
can be seamlessly handled by both current and future SNN architectures. In this
paper, we compare various methods of encoding text as spikes and assess each
method's performance in an associated SNN on a downstream NLP task, namely,
sentiment analysis. Furthermore, we go on to propose a new method of encoding
text as spikes that outperforms a widely-used rate-coding technique, Poisson
rate-coding, by around 13\% on our benchmark NLP tasks. Subsequently, we
demonstrate the energy efficiency of SNNs implemented in hardware for the
sentiment analysis task compared to traditional deep neural networks, observing
an energy efficiency increase of more than 32x during inference and 60x during
training while incurring the expected energy-performance tradeoff.
\\ ( https://arxiv.org/abs/2401.17911 ,  5429kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17919
Date: Wed, 31 Jan 2024 15:33:37 GMT   (15405kb,D)

Title: LOCOST: State-Space Models for Long Document Abstractive Summarization
Authors: Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen,
  Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick
  Gallinari
Categories: cs.CL cs.LG
Comments: 9 pages, 5 figures, 7 tables, EACL 2024 conference
\\
  State-space models are a low-complexity alternative to transformers for
encoding long sequences and capturing long-term dependencies. We propose
LOCOST: an encoder-decoder architecture based on state-space models for
conditional text generation with long context inputs. With a computational
complexity of $O(L \log L)$, this architecture can handle significantly longer
sequences than state-of-the-art models that are based on sparse attention
patterns. We evaluate our model on a series of long document abstractive
summarization tasks. The model reaches a performance level that is 93-96%
comparable to the top-performing sparse transformers of the same size while
saving up to 50% memory during training and up to 87% during inference.
Additionally, LOCOST effectively handles input texts exceeding 600K tokens at
inference time, setting new state-of-the-art results on full-book summarization
and opening new perspectives for long input processing.
\\ ( https://arxiv.org/abs/2401.17919 ,  15405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17922
Date: Wed, 31 Jan 2024 15:35:21 GMT   (6627kb,D)

Title: [Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference
  Annotation with LLMs
Authors: Rebecca M. M. Hicke and David Mimno
Categories: cs.CL
Comments: Accepted to LaTeCH-CLfL 2024
\\
  Coreference annotation and resolution is a vital component of computational
literary studies. However, it has previously been difficult to build high
quality systems for fiction. Coreference requires complicated structured
outputs, and literary text involves subtle inferences and highly varied
language. New language-model-based seq2seq systems present the opportunity to
solve both these problems by learning to directly generate a copy of an input
sentence with markdown-like annotations. We create, evaluate, and release
several trained models for coreference, as well as a workflow for training new
models.
\\ ( https://arxiv.org/abs/2401.17922 ,  6627kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17974
Date: Wed, 31 Jan 2024 16:30:50 GMT   (9927kb,D)

Title: GUMsley: Evaluating Entity Salience in Summarization for 12 English
  Genres
Authors: Jessica Lin, Amir Zeldes
Categories: cs.CL
Comments: Camera-ready for EACL 2024
\\
  As NLP models become increasingly capable of understanding documents in terms
of coherent entities rather than strings, obtaining the most salient entities
for each document is not only an important end task in itself but also vital
for Information Retrieval (IR) and other downstream applications such as
controllable summarization. In this paper, we present and evaluate GUMsley, the
first entity salience dataset covering all named and non-named salient entities
for 12 genres of English text, aligned with entity types, Wikification links
and full coreference resolution annotations. We promote a strict definition of
salience using human summaries and demonstrate high inter-annotator agreement
for salience based on whether a source entity is mentioned in the summary. Our
evaluation shows poor performance by pre-trained SOTA summarization models and
zero-shot LLM prompting in capturing salient entities in generated summaries.
We also show that predicting or providing salient entities to several model
architectures enhances performance and helps derive higher-quality summaries by
alleviating the entity hallucination problem in existing abstractive
summarization.
\\ ( https://arxiv.org/abs/2401.17974 ,  9927kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17979
Date: Wed, 31 Jan 2024 16:34:10 GMT   (7563kb,D)

Title: Entity Linking in the Job Market Domain
Authors: Mike Zhang and Rob van der Goot and Barbara Plank
Categories: cs.CL
Comments: Accepted at EACL 2024 Findings
\\
  In Natural Language Processing, entity linking (EL) has centered around
Wikipedia, but yet remains underexplored for the job market domain.
Disambiguating skill mentions can help us get insight into the current labor
market demands. In this work, we are the first to explore EL in this domain,
specifically targeting the linkage of occupational skills to the ESCO taxonomy
(le Vrang et al., 2014). Previous efforts linked coarse-grained (full)
sentences to a corresponding ESCO skill. In this work, we link more
fine-grained span-level mentions of skills. We tune two high-performing neural
EL models, a bi-encoder (Wu et al., 2020) and an autoregressive model (Cao et
al., 2021), on a synthetically generated mention--skill pair dataset and
evaluate them on a human-annotated skill-linking benchmark. Our findings reveal
that both models are capable of linking implicit mentions of skills to their
correct taxonomy counterparts. Empirically, BLINK outperforms GENRE in strict
evaluation, but GENRE performs better in loose evaluation (accuracy@$k$).
\\ ( https://arxiv.org/abs/2401.17979 ,  7563kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18001
Date: Wed, 31 Jan 2024 17:02:31 GMT   (6698kb,D)

Title: Desiderata for the Context Use of Question Answering Systems
Authors: Sagi Shaier, Lawrence E Hunter, Katharina von der Wense
Categories: cs.CL
Comments: Accepted to EACL 2024
\\
  Prior work has uncovered a set of common problems in state-of-the-art
context-based question answering (QA) systems: a lack of attention to the
context when the latter conflicts with a model's parametric knowledge, little
robustness to noise, and a lack of consistency with their answers. However,
most prior work focus on one or two of those problems in isolation, which makes
it difficult to see trends across them. We aim to close this gap, by first
outlining a set of -- previously discussed as well as novel -- desiderata for
QA models. We then survey relevant analysis and methods papers to provide an
overview of the state of the field. The second part of our work presents
experiments where we evaluate 15 QA systems on 5 datasets according to all
desiderata at once. We find many novel trends, including (1) systems that are
less susceptible to noise are not necessarily more consistent with their
answers when given irrelevant context; (2) most systems that are more
susceptible to noise are more likely to correctly answer according to a context
that conflicts with their parametric knowledge; and (3) the combination of
conflicting knowledge and noise can reduce system performance by up to 96%. As
such, our desiderata help increase our understanding of how these models work
and reveal potential avenues for improvements.
\\ ( https://arxiv.org/abs/2401.18001 ,  6698kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18028
Date: Wed, 31 Jan 2024 17:43:04 GMT   (112kb)

Title: Supporting Anticipatory Governance using LLMs: Evaluating and Aligning
  Large Language Models with the News Media to Anticipate the Negative Impacts
  of AI
Authors: Mowafak Allaham, Nicholas Diakopoulos
Categories: cs.CL cs.AI cs.CY
Comments: 14 pages + research ethics and social impact statement, references,
  and appendix. Under conference review
\\
  Anticipating the negative impacts of emerging AI technologies is a challenge,
especially in the early stages of development. An understudied approach to such
anticipation is the use of LLMs to enhance and guide this process. Despite
advancements in LLMs and evaluation metrics to account for biases in generated
text, it is unclear how well these models perform in anticipatory tasks.
Specifically, the use of LLMs to anticipate AI impacts raises questions about
the quality and range of categories of negative impacts these models are
capable of generating. In this paper we leverage news media, a diverse data
source that is rich with normative assessments of emerging technologies, to
formulate a taxonomy of impacts to act as a baseline for comparing against. By
computationally analyzing thousands of news articles published by hundreds of
online news domains around the world, we develop a taxonomy consisting of ten
categories of AI impacts. We then evaluate both instruction-based (GPT-4 and
Mistral-7B-Instruct) and fine-tuned completion models (Mistral-7B and GPT-3)
using a sample from this baseline. We find that the generated impacts using
Mistral-7B, fine-tuned on impacts from the news media, tend to be qualitatively
on par with impacts generated using a larger scale model such as GPT-4.
Moreover, we find that these LLMs generate impacts that largely reflect the
taxonomy of negative impacts identified in the news media, however the impacts
produced by instruction-based models had gaps in the production of certain
categories of impacts in comparison to fine-tuned models. This research
highlights a potential bias in state-of-the-art LLMs when used for anticipating
impacts and demonstrates the advantages of aligning smaller LLMs with a diverse
range of impacts, such as those reflected in the news media, to better reflect
such impacts during anticipatory exercises.
\\ ( https://arxiv.org/abs/2401.18028 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18034
Date: Wed, 31 Jan 2024 17:58:10 GMT   (6220kb)

Title: Paramanu: A Family of Novel Efficient Indic Generative Foundation
  Language Models
Authors: Mitodru Niyogi and Arnab Bhattacharya
Categories: cs.CL cs.AI
\\
  We present Gyan AI Paramanu ("atom"), a family of novel language models for
Indian languages. It is a collection of auto-regressive monolingual, bilingual,
and multilingual Indic language models pretrained from scratch on a single GPU
for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi,
Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia,
Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are
pretrained with a context size of 1024 on a single GPU. The models are very
efficient, small, fast, and powerful. We have also developed an efficient most
advanced Indic tokenizer that can even tokenize unseen languages. In order to
avoid the "curse of multi-linguality" in our multilingual mParamanu model, we
pretrained on comparable corpora by typological grouping using the same script.
We performed human evaluation of our pretrained models for open end text
generation on grammar, coherence, creativity, and factuality metrics for
Bangla, Hindi, and Sanskrit. Our Bangla, Hindi, and Sanskrit models
outperformed GPT-3.5-Turbo (ChatGPT), Bloom 7B, LLaMa-2 7B, OPT 6.7B, GPT-J 6B,
GPTNeo 1.3B, GPT2-XL large language models (LLMs) by a large margin despite
being smaller in size by 66 to 20 times compared to standard 7B LLMs. To run
inference on our pretrained models, CPU is enough, and GPU is not needed. We
also instruction-tuned our pretrained Bangla, Hindi, Marathi, Tamil, and Telugu
models on 23k instructions in respective languages. Our pretrained and
instruction-tuned models which are first of its kind, most powerful efficient
small generative language models ever developed for Indic languages, and the
various results lead to the conclusion that high quality generative language
models are possible without high amount of compute power and humongous number
of parameters. We plan to release our models at https://www.bharatgpts.com.
\\ ( https://arxiv.org/abs/2401.18034 ,  6220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18040
Date: Wed, 31 Jan 2024 18:03:39 GMT   (336kb)

Title: Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic
  Motivation Reinforcement Learning Algorithms for Improved Training and
  Adaptability
Authors: Navin Kamuni, Hardik Shah, Sathishkumar Chintala, Naveen Kunchakuri,
  Sujatha Alla Old Dominion
Categories: cs.CL cs.AI
Comments: 6 pages, 1 figure, 18th IEEE International Conference on Semantic
  Computing
\\
  End-to-end multi-task dialogue systems are usually designed with separate
modules for the dialogue pipeline. Among these, the policy module is essential
for deciding what to do in response to user input. This policy is trained by
reinforcement learning algorithms by taking advantage of an environment in
which an agent receives feedback in the form of a reward signal. The current
dialogue systems, however, only provide meagre and simplistic rewards.
Investigating intrinsic motivation reinforcement learning algorithms is the
goal of this study. Through this, the agent can quickly accelerate training and
improve its capacity to judge the quality of its actions by teaching it an
internal incentive system. In particular, we adapt techniques for random
network distillation and curiosity-driven reinforcement learning to measure the
frequency of state visits and encourage exploration by using semantic
similarity between utterances. Experimental results on MultiWOZ, a
heterogeneous dataset, show that intrinsic motivation-based debate systems
outperform policies that depend on extrinsic incentives. By adopting random
network distillation, for example, which is trained using semantic similarity
between user-system dialogues, an astounding average success rate of 73% is
achieved. This is a significant improvement over the baseline Proximal Policy
Optimization (PPO), which has an average success rate of 60%. In addition,
performance indicators such as booking rates and completion rates show a 10%
rise over the baseline. Furthermore, these intrinsic incentive models help
improve the system's policy's resilience in an increasing amount of domains.
This implies that they could be useful in scaling up to settings that cover a
wider range of domains.
\\ ( https://arxiv.org/abs/2401.18040 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18045
Date: Wed, 31 Jan 2024 18:06:29 GMT   (232kb,D)

Title: SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition
Authors: Yihan Wu, Soumi Maiti, Yifan Peng, Wangyou Zhang, Chenda Li, Yuyue
  Wang, Xihua Wang, Shinji Watanabe, Ruihua Song
Categories: cs.CL cs.AI cs.SD eess.AS
Comments: 11 pages, 2 figures
\\
  Recent advancements in language models have significantly enhanced
performance in multiple speech-related tasks. Existing speech language models
typically utilize task-dependent prompt tokens to unify various speech tasks in
a single model. However, this design omits the intrinsic connections between
different speech tasks, which can potentially boost the performance of each
task. In this work, we propose a novel decoder-only speech language model,
SpeechComposer, that can unify common speech tasks by composing a fixed set of
prompt tokens. Built upon four primary tasks -- speech synthesis, speech
recognition, speech language modeling, and text language modeling --
SpeechComposer can easily extend to more speech tasks via compositions of
well-designed prompt tokens, like voice conversion and speech enhancement. The
unification of prompt tokens also makes it possible for knowledge sharing among
different speech tasks in a more structured manner. Experimental results
demonstrate that our proposed SpeechComposer can improve the performance of
both primary tasks and composite tasks, showing the effectiveness of the shared
prompt tokens. Remarkably, the unified decoder-only model achieves a comparable
and even better performance than the baselines which are expert models designed
for single tasks.
\\ ( https://arxiv.org/abs/2401.18045 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18046
Date: Wed, 31 Jan 2024 18:07:12 GMT   (1302kb,D)

Title: Multipath parsing in the brain
Authors: Berta Franzluebbers, Donald Dunagan, Milo\v{s} Stanojevi\'c, Jan Buys,
  John T. Hale
Categories: cs.CL
Comments: 15 pages
\\
  Humans understand sentences word-by-word, in the order that they hear them.
This incrementality entails resolving temporary ambiguities about syntactic
relationships. We investigate how humans process these syntactic ambiguities by
correlating predictions from incremental generative dependency parsers with
timecourse data from people undergoing functional neuroimaging while listening
to an audiobook. In particular, we compare competing hypotheses regarding the
number of developing syntactic analyses in play during word-by-word
comprehension: one vs more than one. This comparison involves evaluating
syntactic surprisal from a state-of-the-art dependency parser with LLM-adapted
encodings against an existing fMRI dataset. In both English and Chinese data,
we find evidence for multipath parsing. Brain regions associated with this
multipath effect include bilateral superior temporal gyrus.
\\ ( https://arxiv.org/abs/2401.18046 ,  1302kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18058
Date: Wed, 31 Jan 2024 18:29:39 GMT   (664kb,D)

Title: LongAlign: A Recipe for Long Context Alignment of Large Language Models
Authors: Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang,
  Yuxiao Dong, Juanzi Li
Categories: cs.CL cs.LG
\\
  Extending large language models to effectively handle long contexts requires
instruction fine-tuning on input sequences of similar length. To address this,
we present LongAlign -- a recipe of the instruction data, training, and
evaluation for long context alignment. First, we construct a long
instruction-following dataset using Self-Instruct. To ensure the data
diversity, it covers a broad range of tasks from various long context sources.
Second, we adopt the packing and sorted batching strategies to speed up
supervised fine-tuning on data with varied length distributions. Additionally,
we develop a loss weighting method to balance the contribution to the loss
across different sequences during packing training. Third, we introduce the
LongBench-Chat benchmark for evaluating instruction-following capabilities on
queries of 10k-100k in length. Experiments show that LongAlign outperforms
existing recipes for LLMs in long context tasks by up to 30\%, while also
maintaining their proficiency in handling short, generic tasks. The code, data,
and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.
\\ ( https://arxiv.org/abs/2401.18058 ,  664kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18059
Date: Wed, 31 Jan 2024 18:30:21 GMT   (2334kb,D)

Title: RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval
Authors: Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie,
  Christopher D. Manning
Categories: cs.CL cs.LG
\\
  Retrieval-augmented language models can better adapt to changes in world
state and incorporate long-tail knowledge. However, most existing methods
retrieve only short contiguous chunks from a retrieval corpus, limiting
holistic understanding of the overall document context. We introduce the novel
approach of recursively embedding, clustering, and summarizing chunks of text,
constructing a tree with differing levels of summarization from the bottom up.
At inference time, our RAPTOR model retrieves from this tree, integrating
information across lengthy documents at different levels of abstraction.
Controlled experiments show that retrieval with recursive summaries offers
significant improvements over traditional retrieval-augmented LMs on several
tasks. On question-answering tasks that involve complex, multi-step reasoning,
we show state-of-the-art results; for example, by coupling RAPTOR retrieval
with the use of GPT-4, we can improve the best performance on the QuALITY
benchmark by 20% in absolute accuracy.
\\ ( https://arxiv.org/abs/2401.18059 ,  2334kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18070
Date: Wed, 31 Jan 2024 18:48:20 GMT   (1023kb,D)

Title: Do Language Models Exhibit the Same Cognitive Biases in Problem Solving
  as Human Learners?
Authors: Andreas Opedal, Alessandro Stolfo, Haruki Shirakami, Ying Jiao, Ryan
  Cotterell, Bernhard Sch\"olkopf, Abulhair Saparov, Mrinmaya Sachan
Categories: cs.CL cs.AI cs.LG
Comments: Preprint
\\
  There is increasing interest in employing large language models (LLMs) as
cognitive models. For such purposes, it is central to understand which
cognitive properties are well-modeled by LLMs, and which are not. In this work,
we study the biases of LLMs in relation to those known in children when solving
arithmetic word problems. Surveying the learning science literature, we posit
that the problem-solving process can be split into three distinct steps: text
comprehension, solution planning and solution execution. We construct tests for
each one in order to understand which parts of this process can be faithfully
modeled by current state-of-the-art LLMs. We generate a novel set of word
problems for each of these tests, using a neuro-symbolic method that enables
fine-grained control over the problem features. We find evidence that LLMs,
with and without instruction-tuning, exhibit human-like biases in both the
text-comprehension and the solution-planning steps of the solving process, but
not during the final step which relies on the problem's arithmetic expressions
(solution execution).
\\ ( https://arxiv.org/abs/2401.18070 ,  1023kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17342
Date: Tue, 30 Jan 2024 13:41:12 GMT   (1208kb,D)

Title: A Latent Space Metric for Enhancing Prediction Confidence in Earth
  Observation Data
Authors: Ioannis Pitsiorlas, Argyro Tsantalidou, George Arvanitakis, Marios
  Kountouris, Charalambos Kontoes
Categories: cs.LG cs.AI
\\
  This study presents a new approach for estimating confidence in machine
learning model predictions, specifically in regression tasks utilizing Earth
Observation (EO) data, with a particular focus on mosquito abundance (MA)
estimation. We take advantage of a Variational AutoEncoder architecture, to
derive a confidence metric by the latent space representations of EO datasets.
This methodology is pivotal in establishing a correlation between the Euclidean
distance in latent representations and the Absolute Error (AE) in individual MA
predictions. Our research focuses on EO datasets from the Veneto region in
Italy and the Upper Rhine Valley in Germany, targeting areas significantly
affected by mosquito populations. A key finding is a notable correlation of
0.46 between the AE of MA predictions and the proposed confidence metric. This
correlation signifies a robust, new metric for quantifying the reliability and
enhancing the trustworthiness of the AI model's predictions in the context of
both EO data analysis and mosquito abundance studies.
\\ ( https://arxiv.org/abs/2401.17342 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17350
Date: Tue, 30 Jan 2024 17:57:07 GMT   (24356kb,D)

Title: Timeseries Suppliers Allocation Risk Optimization via Deep Black
  Litterman Model
Authors: Jiayuan Luo, Wentao Zhang, Yuchen Fang, Xiaowei Gao, Dingyi Zhuang,
  Hao Chen, Xinke Jiang
Categories: cs.LG cs.AI
Comments: version 1
\\
  We introduce the BL model and the Perspective Matrix to optimize supplier
selection and order allocation, focusing on both temporal and spatial dynamics.
Our development of a Supplier Relationship Network, using a Spatio-Temporal
Graph Neural Network, enhances the understanding of complex supplier
interdependencies. Additionally, we address credibility issues in zero-order
scenarios with a Masked Ranking Mechanism, improving supplier ranking
efficiency. Our model demonstrates superior results on two datasets compared to
the traditional models. Our evaluations using real-world datasets highlight
DBLM's superiority in providing accurate predictions and precise confidence
intervals, particularly in high-resolution scenarios.
\\ ( https://arxiv.org/abs/2401.17350 ,  24356kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17401
Date: Tue, 30 Jan 2024 19:35:43 GMT   (2088kb,D)

Title: Step-size Optimization for Continual Learning
Authors: Thomas Degris, Khurram Javed, Arsalan Sharifnassab, Yuxin Liu, Richard
  Sutton
Categories: cs.LG cs.AI
\\
  In continual learning, a learner has to keep learning from the data over its
whole life time. A key issue is to decide what knowledge to keep and what
knowledge to let go. In a neural network, this can be implemented by using a
step-size vector to scale how much gradient samples change network weights.
Common algorithms, like RMSProp and Adam, use heuristics, specifically
normalization, to adapt this step-size vector. In this paper, we show that
those heuristics ignore the effect of their adaptation on the overall objective
function, for example by moving the step-size vector away from better step-size
vectors. On the other hand, stochastic meta-gradient descent algorithms, like
IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to
the overall objective function. On simple problems, we show that IDBD is able
to consistently improve step-size vectors, where RMSProp and Adam do not. We
explain the differences between the two approaches and their respective
limitations. We conclude by suggesting that combining both approaches could be
a promising future direction to improve the performance of neural networks in
continual learning.
\\ ( https://arxiv.org/abs/2401.17401 ,  2088kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17408
Date: Tue, 30 Jan 2024 19:52:02 GMT   (3707kb,D)

Title: Solving Boltzmann Optimization Problems with Deep Learning
Authors: Fiona Knoll, John T. Daly, Jess J. Meyer
Categories: cs.LG cs.ET math.OC
\\
  Decades of exponential scaling in high performance computing (HPC) efficiency
is coming to an end. Transistor based logic in complementary metal-oxide
semiconductor (CMOS) technology is approaching physical limits beyond which
further miniaturization will be impossible. Future HPC efficiency gains will
necessarily rely on new technologies and paradigms of compute. The Ising model
shows particular promise as a future framework for highly energy efficient
computation. Ising systems are able to operate at energies approaching
thermodynamic limits for energy consumption of computation. Ising systems can
function as both logic and memory. Thus, they have the potential to
significantly reduce energy costs inherent to CMOS computing by eliminating
costly data movement. The challenge in creating Ising-based hardware is in
optimizing useful circuits that produce correct results on fundamentally
nondeterministic hardware. The contribution of this paper is a novel machine
learning approach, a combination of deep neural networks and random forests,
for efficiently solving optimization problems that minimize sources of error in
the Ising model. In addition, we provide a process to express a Boltzmann
probability optimization problem as a supervised machine learning problem.
\\ ( https://arxiv.org/abs/2401.17408 ,  3707kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17426
Date: Tue, 30 Jan 2024 20:29:06 GMT   (8864kb,D)

Title: Superiority of Multi-Head Attention in In-Context Linear Regression
Authors: Yingqian Cui, Jie Ren, Pengfei He, Jiliang Tang, Yue Xing
Categories: cs.LG cs.AI stat.ML
\\
  We present a theoretical analysis of the performance of transformer with
softmax attention in in-context learning with linear regression tasks. While
the existing literature predominantly focuses on the convergence of
transformers with single-/multi-head attention, our research centers on
comparing their performance. We conduct an exact theoretical analysis to
demonstrate that multi-head attention with a substantial embedding dimension
performs better than single-head attention. When the number of in-context
examples D increases, the prediction loss using single-/multi-head attention is
in O(1/D), and the one for multi-head attention has a smaller multiplicative
constant. In addition to the simplest data distribution setting, we consider
more scenarios, e.g., noisy labels, local examples, correlated features, and
prior knowledge. We observe that, in general, multi-head attention is preferred
over single-head attention. Our results verify the effectiveness of the design
of multi-head attention in the transformer architecture.
\\ ( https://arxiv.org/abs/2401.17426 ,  8864kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17435
Date: Tue, 30 Jan 2024 20:49:47 GMT   (587kb,D)

Title: Can Large Language Models Replace Economic Choice Prediction Labs?
Authors: Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz
Categories: cs.LG cs.AI cs.CL cs.GT cs.HC
\\
  Economic choice prediction is an essential challenging task, often
constrained by the difficulties in acquiring human choice data. Indeed,
experimental economics studies had focused mostly on simple choice settings.
The AI community has recently contributed to that effort in two ways:
considering whether LLMs can substitute for humans in the above-mentioned
simple choice prediction settings, and the study through ML lens of more
elaborated but still rigorous experimental economics settings, employing
incomplete information, repetitive play, and natural language communication,
notably language-based persuasion games. This leaves us with a major
inspiration: can LLMs be used to fully simulate the economic environment and
generate data for efficient human choice prediction, substituting for the
elaborated economic lab studies? We pioneer the study of this subject,
demonstrating its feasibility. In particular, we show that a model trained
solely on LLM-generated data can effectively predict human behavior in a
language-based persuasion game, and can even outperform models trained on
actual human data.
\\ ( https://arxiv.org/abs/2401.17435 ,  587kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17441
Date: Tue, 30 Jan 2024 21:02:21 GMT   (8310kb,D)

Title: Explaining Predictive Uncertainty by Exposing Second-Order Effects
Authors: Florian Bley and Sebastian Lapuschkin and Wojciech Samek and
  Gr\'egoire Montavon
Categories: cs.LG cs.AI stat.ML
Comments: 12 pages + supplement
\\
  Explainable AI has brought transparency into complex ML blackboxes, enabling,
in particular, to identify which features these models use for their
predictions. So far, the question of explaining predictive uncertainty, i.e.
why a model 'doubts', has been scarcely studied. Our investigation reveals that
predictive uncertainty is dominated by second-order effects, involving single
features or product interactions between them. We contribute a new method for
explaining predictive uncertainty based on these second-order effects.
Computationally, our method reduces to a simple covariance computation over a
collection of first-order explanations. Our method is generally applicable,
allowing for turning common attribution techniques (LRP, Gradient x Input,
etc.) into powerful second-order uncertainty explainers, which we call CovLRP,
CovGI, etc. The accuracy of the explanations our method produces is
demonstrated through systematic quantitative evaluations, and the overall
usefulness of our method is demonstrated via two practical showcases.
\\ ( https://arxiv.org/abs/2401.17441 ,  8310kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17443
Date: Tue, 30 Jan 2024 21:11:35 GMT   (6080kb,D)

Title: Liquid Democracy for Low-Cost Ensemble Pruning
Authors: Ben Armstrong, Kate Larson
Categories: cs.LG cs.AI cs.MA
Comments: 30 pages, 20 figures. Extended abstract to appear at AAMAS 2024
\\
  We argue that there is a strong connection between ensemble learning and a
delegative voting paradigm -- liquid democracy -- that can be leveraged to
reduce ensemble training costs. We present an incremental training procedure
that identifies and removes redundant classifiers from an ensemble via
delegation mechanisms inspired by liquid democracy. Through both analysis and
extensive experiments we show that this process greatly reduces the
computational cost of training compared to training a full ensemble. By
carefully selecting the underlying delegation mechanism, weight centralization
in the classifier population is avoided, leading to higher accuracy than some
boosting methods. Furthermore, this work serves as an exemplar of how
frameworks from computational social choice literature can be applied to
problems in nontraditional domains.
\\ ( https://arxiv.org/abs/2401.17443 ,  6080kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17460
Date: Tue, 30 Jan 2024 21:46:09 GMT   (341kb,D)

Title: Rendering Wireless Environments Useful for Gradient Estimators: A
  Zero-Order Stochastic Federated Learning Method
Authors: Elissa Mhanna and Mohamad Assaad
Categories: cs.LG cs.DC cs.MA math.OC
\\
  Federated learning (FL) is a novel approach to machine learning that allows
multiple edge devices to collaboratively train a model without disclosing their
raw data. However, several challenges hinder the practical implementation of
this approach, especially when devices and the server communicate over wireless
channels, as it suffers from communication and computation bottlenecks in this
case. By utilizing a communication-efficient framework, we propose a novel
zero-order (ZO) method with a one-point gradient estimator that harnesses the
nature of the wireless communication channel without requiring the knowledge of
the channel state coefficient. It is the first method that includes the
wireless channel in the learning algorithm itself instead of wasting resources
to analyze it and remove its impact. The two main difficulties of this work are
that in FL, the objective function is usually not convex, which makes the
extension of FL to ZO methods challenging, and that including the impact of
wireless channels requires extra attention. However, we overcome these
difficulties and comprehensively analyze the proposed zero-order federated
learning (ZOFL) framework. We establish its convergence theoretically, and we
prove a convergence rate of $O(\frac{1}{\sqrt[3]{K}})$ in the nonconvex
setting. We further demonstrate the potential of our algorithm with
experimental results, taking into account independent and identically
distributed (IID) and non-IID device data distributions.
\\ ( https://arxiv.org/abs/2401.17460 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17504
Date: Tue, 30 Jan 2024 23:39:40 GMT   (2035kb,D)

Title: CaMU: Disentangling Causal Effects in Deep Model Unlearning
Authors: Shaofei Shen, Chenhao Zhang, Alina Bialkowski, Weitong Chen, Miao Xu
Categories: cs.LG stat.ME
Comments: Full version of the paper accepted for the SDM 24 conference
\\
  Machine unlearning requires removing the information of forgetting data while
keeping the necessary information of remaining data. Despite recent
advancements in this area, existing methodologies mainly focus on the effect of
removing forgetting data without considering the negative impact this can have
on the information of the remaining data, resulting in significant performance
degradation after data removal. Although some methods try to repair the
performance of remaining data after removal, the forgotten information can also
return after repair. Such an issue is due to the intricate intertwining of the
forgetting and remaining data. Without adequately differentiating the influence
of these two kinds of data on the model, existing algorithms take the risk of
either inadequate removal of the forgetting data or unnecessary loss of
valuable information from the remaining data. To address this shortcoming, the
present study undertakes a causal analysis of the unlearning and introduces a
novel framework termed Causal Machine Unlearning (CaMU). This framework adds
intervention on the information of remaining data to disentangle the causal
effects between forgetting data and remaining data. Then CaMU eliminates the
causal impact associated with forgetting data while concurrently preserving the
causal relevance of the remaining data. Comprehensive empirical results on
various datasets and models suggest that CaMU enhances performance on the
remaining data and effectively minimizes the influences of forgetting data.
Notably, this work is the first to interpret deep model unlearning tasks from a
new perspective of causality and provide a solution based on causal analysis,
which opens up new possibilities for future research in deep model unlearning.
\\ ( https://arxiv.org/abs/2401.17504 ,  2035kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17505
Date: Tue, 30 Jan 2024 23:46:35 GMT   (1422kb,D)

Title: Arrows of Time for Large Language Models
Authors: Vassilis Papadopoulos, J\'er\'emie Wenger, Cl\'ement Hongler
Categories: cs.LG cs.AI cs.CL
\\
  We study the probabilistic modeling performed by Autoregressive Large
Language Models through the angle of time directionality. We empirically find a
time asymmetry exhibited by such models in their ability to model natural
language: a difference in the average log-perplexity when trying to predict the
next token versus when trying to predict the previous one. This difference is
at the same time subtle and very consistent across various modalities
(language, model size, training time, ...). Theoretically, this is surprising:
from an information-theoretic point of view, there should be no such
difference. We provide a theoretical framework to explain how such an asymmetry
can appear from sparsity and computational complexity considerations, and
outline a number of perspectives opened by our results.
\\ ( https://arxiv.org/abs/2401.17505 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17523
Date: Wed, 31 Jan 2024 00:43:30 GMT   (2352kb,D)

Title: Game-Theoretic Unlearnable Example Generator
Authors: Shuang Liu and Yihan Wang and Xiao-Shan Gao
Categories: cs.LG cs.CR stat.ML
\\
  Unlearnable example attacks are data poisoning attacks aiming to degrade the
clean test accuracy of deep learning by adding imperceptible perturbations to
the training samples, which can be formulated as a bi-level optimization
problem. However, directly solving this optimization problem is intractable for
deep neural networks. In this paper, we investigate unlearnable example attacks
from a game-theoretic perspective, by formulating the attack as a nonzero sum
Stackelberg game. First, the existence of game equilibria is proved under the
normal setting and the adversarial training setting. It is shown that the game
equilibrium gives the most powerful poison attack in that the victim has the
lowest test accuracy among all networks within the same hypothesis space, when
certain loss functions are used. Second, we propose a novel attack method,
called the Game Unlearnable Example (GUE), which has three main gradients. (1)
The poisons are obtained by directly solving the equilibrium of the Stackelberg
game with a first-order algorithm. (2) We employ an autoencoder-like generative
network model as the poison attacker. (3) A novel payoff function is introduced
to evaluate the performance of the poison. Comprehensive experiments
demonstrate that GUE can effectively poison the model in various scenarios.
Furthermore, the GUE still works by using a relatively small percentage of the
training data to train the generator, and the poison generator can generalize
to unseen data well. Our implementation code can be found at
https://github.com/hong-xian/gue.
\\ ( https://arxiv.org/abs/2401.17523 ,  2352kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17539
Date: Wed, 31 Jan 2024 01:51:29 GMT   (2274kb,D)

Title: Enhancing Score-Based Sampling Methods with Ensembles
Authors: Tobias Bischoff, Bryan Riel
Categories: cs.LG stat.CO
\\
  We introduce ensembles within score-based sampling methods to develop
gradient-free approximate sampling techniques that leverage the collective
dynamics of particle ensembles to compute approximate reverse diffusion drifts.
We introduce the underlying methodology, emphasizing its relationship with
generative diffusion models and the previously introduced F\"ollmer sampler. We
demonstrate the efficacy of ensemble strategies through various examples,
ranging from low- to medium-dimensionality sampling problems, including
multi-modal and highly non-Gaussian probability distributions, and provide
comparisons to traditional methods like NUTS. Our findings highlight the
potential of ensemble strategies for modeling complex probability distributions
in situations where gradients are unavailable. Finally, we showcase its
application in the context of Bayesian inversion problems within the
geophysical sciences.
\\ ( https://arxiv.org/abs/2401.17539 ,  2274kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17541
Date: Wed, 31 Jan 2024 02:08:43 GMT   (253kb,D)

Title: Towards Understanding Variants of Invariant Risk Minimization through
  the Lens of Calibration
Authors: Kotaro Yoshida, Hiroki Naganuma
Categories: cs.LG
\\
  Machine learning models traditionally assume that training and test data are
independently and identically distributed. However, in real-world applications,
the test distribution often differs from training. This problem, known as
out-of-distribution generalization, challenges conventional models. Invariant
Risk Minimization (IRM) emerges as a solution, aiming to identify features
invariant across different environments to enhance out-of-distribution
robustness. However, IRM's complexity, particularly its bi-level optimization,
has led to the development of various approximate methods. Our study
investigates these approximate IRM techniques, employing the Expected
Calibration Error (ECE) as a key metric. ECE, which measures the reliability of
model prediction, serves as an indicator of whether models effectively capture
environment-invariant features. Through a comparative analysis of datasets with
distributional shifts, we observe that Information Bottleneck-based IRM, which
condenses representational information, achieves a balance in improving ECE
while preserving accuracy relatively. This finding is pivotal, as it
demonstrates a feasible path to maintaining robustness without compromising
accuracy. Nonetheless, our experiments also caution against
over-regularization, which can diminish accuracy. This underscores the
necessity for a systematic approach in evaluating out-of-distribution
generalization metrics, one that beyond mere accuracy to address the nuanced
interplay between accuracy and calibration.
\\ ( https://arxiv.org/abs/2401.17541 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17542
Date: Wed, 31 Jan 2024 02:09:21 GMT   (2045kb,D)

Title: Data-Effective Learning: A Comprehensive Medical Benchmark
Authors: Wenxuan Yang, Weimin Tan, Yuqi Sun, Bo Yan
Categories: cs.LG cs.AI cs.CV
\\
  Data-effective learning aims to use data in the most impactful way to train
AI models, which involves strategies that focus on data quality rather than
quantity, ensuring the data used for training has high informational value.
Data-effective learning plays a profound role in accelerating AI training,
reducing computational costs, and saving data storage, which is very important
as the volume of medical data in recent years has grown beyond many people's
expectations. However, due to the lack of standards and comprehensive
benchmark, research on medical data-effective learning is poorly studied. To
address this gap, our paper introduces a comprehensive benchmark specifically
for evaluating data-effective learning in the medical field. This benchmark
includes a dataset with millions of data samples from 31 medical centers
(DataDEL), a baseline method for comparison (MedDEL), and a new evaluation
metric (NormDEL) to objectively measure data-effective learning performance.
Our extensive experimental results show the baseline MedDEL can achieve
performance comparable to the original large dataset with only 5% of the data.
Establishing such an open data-effective learning benchmark is crucial for the
medical AI research community because it facilitates efficient data use,
promotes collaborative breakthroughs, and fosters the development of
cost-effective, scalable, and impactful healthcare solutions. The project can
be accessed at
https://github.com/shadow2469/Data-Effective-Learning-A-Comprehensive-Medical-Benchmark.git.
\\ ( https://arxiv.org/abs/2401.17542 ,  2045kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17544
Date: Wed, 31 Jan 2024 02:18:27 GMT   (438kb,D)

Title: Trainable Fixed-Point Quantization for Deep Learning Acceleration on
  FPGAs
Authors: Dingyi Dai, Yichi Zhang, Jiahao Zhang, Zhanqiu Hu, Yaohui Cai, Qi Sun,
  Zhiru Zhang
Categories: cs.LG cs.CV
\\
  Quantization is a crucial technique for deploying deep learning models on
resource-constrained devices, such as embedded FPGAs. Prior efforts mostly
focus on quantizing matrix multiplications, leaving other layers like BatchNorm
or shortcuts in floating-point form, even though fixed-point arithmetic is more
efficient on FPGAs. A common practice is to fine-tune a pre-trained model to
fixed-point for FPGA deployment, but potentially degrading accuracy.
  This work presents QFX, a novel trainable fixed-point quantization approach
that automatically learns the binary-point position during model training.
Additionally, we introduce a multiplier-free quantization strategy within QFX
to minimize DSP usage. QFX is implemented as a PyTorch-based library that
efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a
differentiable manner during backpropagation. With minimal effort, models
trained with QFX can readily be deployed through HLS, producing the same
numerical results as their software counterparts. Our evaluation shows that
compared to post-training quantization, QFX can quantize models trained with
element-wise layers quantized to fewer bits and achieve higher accuracy on both
CIFAR-10 and ImageNet datasets. We further demonstrate the efficacy of
multiplier-free quantization using a state-of-the-art binarized neural network
accelerator designed for an embedded FPGA (AMD Xilinx Ultra96 v2). We plan to
release QFX in open-source format.
\\ ( https://arxiv.org/abs/2401.17544 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17548
Date: Wed, 31 Jan 2024 02:26:09 GMT   (3728kb,D)

Title: Rethinking Channel Dependence for Multivariate Time Series Forecasting:
  Learning from Leading Indicators
Authors: Lifan Zhao, Yanyan Shen
Categories: cs.LG cs.AI
Comments: Accepted to ICLR 2024. Preprint version
\\
  Recently, channel-independent methods have achieved state-of-the-art
performance in multivariate time series (MTS) forecasting. Despite reducing
overfitting risks, these methods miss potential opportunities in utilizing
channel dependence for accurate predictions. We argue that there exist locally
stationary lead-lag relationships between variates, i.e., some lagged variates
may follow the leading indicators within a short time period. Exploiting such
channel dependence is beneficial since leading indicators offer advance
information that can be used to reduce the forecasting difficulty of the lagged
variates. In this paper, we propose a new method named LIFT that first
efficiently estimates leading indicators and their leading steps at each time
step and then judiciously allows the lagged variates to utilize the advance
information from leading indicators. LIFT plays as a plugin that can be
seamlessly collaborated with arbitrary time series forecasting methods.
Extensive experiments on six real-world datasets demonstrate that LIFT improves
the state-of-the-art methods by 5.5% in average forecasting performance.
\\ ( https://arxiv.org/abs/2401.17548 ,  3728kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17580
Date: Wed, 31 Jan 2024 03:51:30 GMT   (1905kb,D)

Title: Graph Contrastive Learning with Cohesive Subgraph Awareness
Authors: Yucheng Wu, Leye Wang, Xiao Han, and Han-Jia Ye
Categories: cs.LG
\\
  Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy
for learning representations of diverse graphs including social and biomedical
networks. GCL widely uses stochastic graph topology augmentation, such as
uniform node dropping, to generate augmented graphs. However, such stochastic
augmentations may severely damage the intrinsic properties of a graph and
deteriorate the following representation learning process. We argue that
incorporating an awareness of cohesive subgraphs during the graph augmentation
and learning processes has the potential to enhance GCL performance. To this
end, we propose a novel unified framework called CTAug, to seamlessly integrate
cohesion awareness into various existing GCL mechanisms. In particular, CTAug
comprises two specialized modules: topology augmentation enhancement and graph
learning enhancement. The former module generates augmented graphs that
carefully preserve cohesion properties, while the latter module bolsters the
graph encoder's ability to discern subgraph patterns. Theoretical analysis
shows that CTAug can strictly improve existing GCL mechanisms. Empirical
experiments verify that CTAug can achieve state-of-the-art performance for
graph representation learning, especially for graphs with high degrees. The
code is available at https://doi.org/10.5281/zenodo.10594093, or
https://github.com/wuyucheng2002/CTAug.
\\ ( https://arxiv.org/abs/2401.17580 ,  1905kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17612
Date: Wed, 31 Jan 2024 05:52:11 GMT   (4651kb,D)

Title: IGCN: Integrative Graph Convolutional Networks for Multi-modal Data
Authors: Cagri Ozdemir, Mohammad Al Olaimat, Yashu Vashishath, Serdar Bozdag
  and Alzheimer's Disease Neuroimaging Initiative
Categories: cs.LG
\\
  Recent advances in Graph Neural Networks (GNN) have led to a considerable
growth in graph data modeling for multi-modal data which contains various types
of nodes and edges. Although some integrative prediction solutions have been
developed recently for network-structured data, these methods have some
restrictions. For a node classification task involving multi-modal data,
certain data modalities may perform better when predicting one class, while
others might excel in predicting a different class. Thus, to obtain a better
learning representation, advanced computational methodologies are required for
the integrative analysis of multi-modal data. Moreover, existing integrative
tools lack a comprehensive and cohesive understanding of the rationale behind
their specific predictions, making them unsuitable for enhancing model
interpretability. Addressing these restrictions, we introduce a novel
integrative neural network approach for multi-modal data networks, named
Integrative Graph Convolutional Networks (IGCN). IGCN learns node embeddings
from multiple topologies and fuses the multiple node embeddings into a weighted
form by assigning attention coefficients to the node embeddings. Our proposed
attention mechanism helps identify which types of data receive more emphasis
for each sample to predict a certain class. Therefore, IGCN has the potential
to unravel previously unknown characteristics within different node
classification tasks. We benchmarked IGCN on several datasets from different
domains, including a multi-omics dataset to predict cancer subtypes and a
multi-modal clinical dataset to predict the progression of Alzheimer's disease.
Experimental results show that IGCN outperforms or is on par with the
state-of-the-art and baseline methods.
\\ ( https://arxiv.org/abs/2401.17612 ,  4651kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17615
Date: Wed, 31 Jan 2024 05:59:38 GMT   (1595kb,D)

Title: Graph Multi-Similarity Learning for Molecular Property Prediction
Authors: Hao Xu, Zhengyang Zhou, Pengyu Hong
Categories: cs.LG cs.CE
\\
  Effective molecular representation learning is essential for molecular
property prediction. Contrastive learning, a prominent self-supervised approach
for molecular representation learning, relies on establishing positive and
negative pairs. However, this binary similarity categorization oversimplifies
the nature of complex molecular relationships and overlooks the degree of
relative similarities among molecules, posing challenges to the effectiveness
and generality of representation learning. In response to this challenge, we
propose the Graph Multi-Similarity Learning for Molecular Property Prediction
(GraphMSL) framework. GraphMSL incorporates a generalized multi-similarity
metric in a continuous scale, capturing self-similarity and relative
similarities. The unimodal multi-similarity metrics are derived from various
chemical modalities, and the fusion of these metrics into a multimodal form
significantly enhances the effectiveness of GraphMSL. In addition, the
flexibility of fusion function can reshape the focus of the model to convey
different chemical semantics. GraphMSL proves effective in drug discovery
evaluations through various downstream tasks and post-hoc analysis of learnt
representations. Its notable performance suggests significant potential for the
exploration of new drug candidates.
\\ ( https://arxiv.org/abs/2401.17615 ,  1595kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17653
Date: Wed, 31 Jan 2024 08:13:35 GMT   (327kb)

Title: A primer on synthetic health data
Authors: Jennifer Anne Bartell, Sander Boisen Valentin, Anders Krogh, Henning
  Langberg, and Martin B{\o}gsted
Categories: cs.LG
\\
  Recent advances in deep generative models have greatly expanded the potential
to create realistic synthetic health datasets. These synthetic datasets aim to
preserve the characteristics, patterns, and overall scientific conclusions
derived from sensitive health datasets without disclosing patient identity or
sensitive information. Thus, synthetic data can facilitate safe data sharing
that supports a range of initiatives including the development of new
predictive models, advanced health IT platforms, and general project ideation
and hypothesis development. However, many questions and challenges remain,
including how to consistently evaluate a synthetic dataset's similarity and
predictive utility in comparison to the original real dataset and risk to
privacy when shared. Additional regulatory and governance issues have not been
widely addressed. In this primer, we map the state of synthetic health data,
including generation and evaluation methods and tools, existing examples of
deployment, the regulatory and ethical landscape, access and governance
options, and opportunities for further development.
\\ ( https://arxiv.org/abs/2401.17653 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17657
Date: Wed, 31 Jan 2024 08:21:35 GMT   (803kb)

Title: An attempt to generate new bridge types from latent space of
  energy-based model
Authors: Hongjun Zhang
Categories: cs.LG cs.AI
Comments: 8 pages, 6 figures
\\
  Use energy-based model for bridge-type innovation. The loss function is
explained by the game theory, the logic is clear and the formula is simple and
clear. Thus avoid the use of maximum likelihood estimation to explain the loss
function and eliminate the need for Monte Carlo methods to solve the normalized
denominator. Assuming that the bridge-type population follows a Boltzmann
distribution, a neural network is constructed to represent the energy function.
Use Langevin dynamics technology to generate a new sample with low energy
value, thus a generative model of bridge-type based on energy is established.
Train energy function on symmetric structured image dataset of three span beam
bridge, arch bridge, cable-stayed bridge, and suspension bridge to accurately
calculate the energy values of real and fake samples. Sampling from latent
space, using gradient descent algorithm, the energy function transforms the
sampling points into low energy score samples, thereby generating new bridge
types different from the dataset. Due to unstable and slow training in this
attempt, the possibility of generating new bridge types is rare and the image
definition of generated images is low.
\\ ( https://arxiv.org/abs/2401.17657 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17695
Date: Wed, 31 Jan 2024 09:31:28 GMT   (10121kb,D)

Title: Datacube segmentation via Deep Spectral Clustering
Authors: Alessandro Bombini and Fernando Garc\'ia-Avello Bof\'ias and Caterina
  Bracci and Michele Ginolfi and Chiara Ruberto
Categories: cs.LG cs.CV physics.app-ph
Comments: 20 pages, 10 figures, doi for code repository, dataset and trained
  model available and reported in the paper
\\
  Extended Vision techniques are ubiquitous in physics. However, the data cubes
steaming from such analysis often pose a challenge in their interpretation, due
to the intrinsic difficulty in discerning the relevant information from the
spectra composing the data cube.
  Furthermore, the huge dimensionality of data cube spectra poses a complex
task in its statistical interpretation; nevertheless, this complexity contains
a massive amount of statistical information that can be exploited in an
unsupervised manner to outline some essential properties of the case study at
hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering
of data-cube's spectra, performed in a suitably defined low-dimensional
embedding space.
  To tackle this topic, we explore the possibility of applying unsupervised
clustering methods in encoded space, i.e. perform deep clustering on the
spectral properties of datacube pixels. A statistical dimensional reduction is
performed by an ad hoc trained (Variational) AutoEncoder, in charge of mapping
spectra into lower dimensional metric spaces, while the clustering process is
performed by a (learnable) iterative K-Means clustering algorithm.
  We apply this technique to two different use cases, of different physical
origins: a set of Macro mapping X-Ray Fluorescence (MA-XRF) synthetic data on
pictorial artworks, and a dataset of simulated astrophysical observations.
\\ ( https://arxiv.org/abs/2401.17695 ,  10121kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17705
Date: Wed, 31 Jan 2024 09:49:46 GMT   (405kb)

Title: Predicting suicidal behavior among Indian adults using childhood trauma,
  mental health questionnaires and machine learning cascade ensembles
Authors: Akash K Rao, Gunjan Y Trivedi, Riri G Trivedi, Anshika Bajpai, Gajraj
  Singh Chauhan, Vishnu K Menon, Kathirvel Soundappan, Hemalatha Ramani, Neha
  Pandya, Varun Dutt
Categories: cs.LG cs.HC
Comments: 11 pages, presnted at the 4th International Conference on Frontiers
  in Computing and Systems (COMSYS 2023), Himachal Pradesh, October 2023
\\
  Among young adults, suicide is India's leading cause of death, accounting for
an alarming national suicide rate of around 16%. In recent years, machine
learning algorithms have emerged to predict suicidal behavior using various
behavioral traits. But to date, the efficacy of machine learning algorithms in
predicting suicidal behavior in the Indian context has not been explored in
literature. In this study, different machine learning algorithms and ensembles
were developed to predict suicide behavior based on childhood trauma, different
mental health parameters, and other behavioral factors. The dataset was
acquired from 391 individuals from a wellness center in India. Information
regarding their childhood trauma, psychological wellness, and other mental
health issues was acquired through standardized questionnaires. Results
revealed that cascade ensemble learning methods using a support vector machine,
decision trees, and random forest were able to classify suicidal behavior with
an accuracy of 95.04% using data from childhood trauma and mental health
questionnaires. The study highlights the potential of using these machine
learning ensembles to identify individuals with suicidal tendencies so that
targeted interinterventions could be provided efficiently.
\\ ( https://arxiv.org/abs/2401.17705 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17743
Date: Wed, 31 Jan 2024 11:02:45 GMT   (5841kb,D)

Title: Algorithmic Robust Forecast Aggregation
Authors: Yongkang Guo, Jason D. Hartline, Zhihuan Huang, Yuqing Kong, Anant
  Shah, Fang-Yi Yu
Categories: cs.LG cs.GT
\\
  Forecast aggregation combines the predictions of multiple forecasters to
improve accuracy. However, the lack of knowledge about forecasters' information
structure hinders optimal aggregation. Given a family of information
structures, robust forecast aggregation aims to find the aggregator with
minimal worst-case regret compared to the omniscient aggregator. Previous
approaches for robust forecast aggregation rely on heuristic observations and
parameter tuning. We propose an algorithmic framework for robust forecast
aggregation. Our framework provides efficient approximation schemes for general
information aggregation with a finite family of possible information
structures. In the setting considered by Arieli et al. (2018) where two agents
receive independent signals conditioned on a binary state, our framework also
provides efficient approximation schemes by imposing Lipschitz conditions on
the aggregator or discrete conditions on agents' reports. Numerical experiments
demonstrate the effectiveness of our method by providing a nearly optimal
aggregator in the setting considered by Arieli et al. (2018).
\\ ( https://arxiv.org/abs/2401.17743 ,  5841kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17752
Date: Wed, 31 Jan 2024 11:26:03 GMT   (3156kb,D)

Title: PF-GNN: Differentiable particle filtering based approximation of
  universal graph representations
Authors: Mohammed Haroon Dupty, Yanfei Dong, Wee Sun Lee
Categories: cs.LG cs.AI
Comments: Published as a conference paper at ICLR 2022
\\
  Message passing Graph Neural Networks (GNNs) are known to be limited in
expressive power by the 1-WL color-refinement test for graph isomorphism. Other
more expressive models either are computationally expensive or need
preprocessing to extract structural features from the graph. In this work, we
propose to make GNNs universal by guiding the learning process with exact
isomorphism solver techniques which operate on the paradigm of
Individualization and Refinement (IR), a method to artificially introduce
asymmetry and further refine the coloring when 1-WL stops. Isomorphism solvers
generate a search tree of colorings whose leaves uniquely identify the graph.
However, the tree grows exponentially large and needs hand-crafted pruning
techniques which are not desirable from a learning perspective. We take a
probabilistic view and approximate the search tree of colorings (i.e.
embeddings) by sampling multiple paths from root to leaves of the search tree.
To learn more discriminative representations, we guide the sampling process
with particle filter updates, a principled approach for sequential state
estimation. Our algorithm is end-to-end differentiable, can be applied with any
GNN as backbone and learns richer graph representations with only linear
increase in runtime. Experimental evaluation shows that our approach
consistently outperforms leading GNN models on both synthetic benchmarks for
isomorphism detection as well as real-world datasets.
\\ ( https://arxiv.org/abs/2401.17752 ,  3156kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17780
Date: Wed, 31 Jan 2024 12:23:24 GMT   (149kb,D)

Title: A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with
  Uniform PAC Guarantees
Authors: Toshinori Kitamura, Tadashi Kozuno, Masahiro Kato, Yuki Ichihara,
  Soichiro Nishimori, Akiyoshi Sannai, Sho Sonoda, Wataru Kumagai, Yutaka
  Matsuo
Categories: cs.LG
\\
  We study a primal-dual reinforcement learning (RL) algorithm for the online
constrained Markov decision processes (CMDP) problem, wherein the agent
explores an optimal policy that maximizes return while satisfying constraints.
Despite its widespread practical use, the existing theoretical literature on
primal-dual RL algorithms for this problem only provides sublinear regret
guarantees and fails to ensure convergence to optimal policies. In this paper,
we introduce a novel policy gradient primal-dual algorithm with uniform
probably approximate correctness (Uniform-PAC) guarantees, simultaneously
ensuring convergence to optimal policies, sublinear regret, and polynomial
sample complexity for any target accuracy. Notably, this represents the first
Uniform-PAC algorithm for the online CMDP problem. In addition to the
theoretical guarantees, we empirically demonstrate in a simple CMDP that our
algorithm converges to optimal policies, while an existing algorithm exhibits
oscillatory performance and constraint violation.
\\ ( https://arxiv.org/abs/2401.17780 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17790
Date: Wed, 31 Jan 2024 12:32:18 GMT   (503kb,D)

Title: RADIN: Souping on a Budget
Authors: Thibaut Menes and Olivier Risser-Maroix
Categories: cs.LG cs.CV
\\
  Model Soups, extending Stochastic Weights Averaging (SWA), combine models
fine-tuned with different hyperparameters. Yet, their adoption is hindered by
computational challenges due to subset selection issues. In this paper, we
propose to speed up model soups by approximating soups performance using
averaged ensemble logits performances. Theoretical insights validate the
congruence between ensemble logits and weight averaging soups across any mixing
ratios. Our Resource ADjusted soups craftINg (RADIN) procedure stands out by
allowing flexible evaluation budgets, enabling users to adjust his budget of
exploration adapted to his resources while increasing performance at lower
budget compared to previous greedy approach (up to 4% on ImageNet).
\\ ( https://arxiv.org/abs/2401.17790 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17791
Date: Wed, 31 Jan 2024 12:33:31 GMT   (2257kb,D)

Title: Graph Transformers without Positional Encodings
Authors: Ayush Garg
Categories: cs.LG cs.AI
Comments: Independent Research
\\
  Recently, Transformers for graph representation learning have become
increasingly popular, achieving state-of-the-art performance on a wide-variety
of datasets, either alone or in combination with message-passing graph neural
networks (MP-GNNs). Infusing graph inductive-biases in the innately
structure-agnostic transformer architecture in the form of structural or
positional encodings (PEs) is key to achieving these impressive results.
However, designing such encodings is tricky and disparate attempts have been
made to engineer such encodings including Laplacian eigenvectors, relative
random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge
encodings etc. In this work, we argue that such encodings may not be required
at all, provided the attention mechanism itself incorporates information about
the graph structure. We introduce Eigenformer, which uses a novel
spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the
graph, and empirically show that it achieves performance comparable to SOTA
MP-GNN architectures and Graph Transformers on a number of standard GNN
benchmark datasets, even surpassing the SOTA on some datasets. We also find
that our architecture is much faster to train in terms of number of epochs,
presumably due to the innate graph inductive biases.
\\ ( https://arxiv.org/abs/2401.17791 ,  2257kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17802
Date: Wed, 31 Jan 2024 12:52:10 GMT   (928kb,D)

Title: Distillation Enhanced Time Series Forecasting Network with Momentum
  Contrastive Learning
Authors: Haozhi Gao, Qianqian Ren, Jinbao Li
Categories: cs.LG cs.AI
\\
  Contrastive representation learning is crucial in time series analysis as it
alleviates the issue of data noise and incompleteness as well as sparsity of
supervision signal. However, existing constrastive learning frameworks usually
focus on intral-temporal features, which fails to fully exploit the intricate
nature of time series data. To address this issue, we propose DE-TSMCL, an
innovative distillation enhanced framework for long sequence time series
forecasting. Specifically, we design a learnable data augmentation mechanism
which adaptively learns whether to mask a timestamp to obtain optimized
sub-sequences. Then, we propose a contrastive learning task with momentum
update to explore inter-sample and intra-temporal correlations of time series
to learn the underlying structure feature on the unlabeled time series.
Meanwhile, we design a supervised task to learn more robust representations and
facilitate the contrastive learning process. Finally, we jointly optimize the
above two tasks. By developing model loss from multiple tasks, we can learn
effective representations for downstream forecasting task. Extensive
experiments, in comparison with state-of-the-arts, well demonstrate the
effectiveness of DE-TSMCL, where the maximum improvement can reach to 27.3%.
\\ ( https://arxiv.org/abs/2401.17802 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17823
Date: Wed, 31 Jan 2024 13:28:07 GMT   (151kb,D)

Title: Privacy-preserving data release leveraging optimal transport and
  particle gradient descent
Authors: Konstantin Donhauser and Javier Abad and Neha Hulkund and Fanny Yang
Categories: cs.LG cs.CR
\\
  We present a novel approach for differentially private data synthesis of
protected tabular datasets, a relevant task in highly sensitive domains such as
healthcare and government. Current state-of-the-art methods predominantly use
marginal-based approaches, where a dataset is generated from private estimates
of the marginals. In this paper, we introduce PrivPGD, a new generation method
for marginal-based private data synthesis, leveraging tools from optimal
transport and particle gradient descent. Our algorithm outperforms existing
methods on a large range of datasets while being highly scalable and offering
the flexibility to incorporate additional domain-specific constraints.
\\ ( https://arxiv.org/abs/2401.17823 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17835
Date: Wed, 31 Jan 2024 13:52:11 GMT   (1881kb,D)

Title: Predicting the Future with Simple World Models
Authors: Tankred Saanum, Peter Dayan, Eric Schulz
Categories: cs.LG
\\
  World models can represent potentially high-dimensional pixel observations in
compact latent spaces, making it tractable to model the dynamics of the
environment. However, the latent dynamics inferred by these models may still be
highly complex. Abstracting the dynamics of the environment with simple models
can have several benefits. If the latent dynamics are simple, the model may
generalize better to novel transitions, and discover useful latent
representations of environment states. We propose a regularization scheme that
simplifies the world model's latent dynamics. Our model, the Parsimonious
Latent Space Model (PLSM), minimizes the mutual information between latent
states and the dynamics that arise between them. This makes the dynamics softly
state-invariant, and the effects of the agent's actions more predictable. We
combine the PLSM with three different model classes used for i) future latent
state prediction, ii) video prediction, and iii) planning. We find that our
regularization improves accuracy, generalization, and performance in downstream
tasks.
\\ ( https://arxiv.org/abs/2401.17835 ,  1881kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17838
Date: Wed, 31 Jan 2024 13:56:08 GMT   (315kb,D)

Title: A Cross-View Hierarchical Graph Learning Hypernetwork for Skill
  Demand-Supply Joint Prediction
Authors: Wenshuo Chao, Zhaopeng Qiu, Likang Wu, Zhuoning Guo, Zhi Zheng,
  Hengshu Zhu, Hao Liu
Categories: cs.LG cs.AI
Comments: 11 pages, 7 figures, AAAI24
\\
  The rapidly changing landscape of technology and industries leads to dynamic
skill requirements, making it crucial for employees and employers to anticipate
such shifts to maintain a competitive edge in the labor market. Existing
efforts in this area either rely on domain-expert knowledge or regarding skill
evolution as a simplified time series forecasting problem. However, both
approaches overlook the sophisticated relationships among different skills and
the inner-connection between skill demand and supply variations. In this paper,
we propose a Cross-view Hierarchical Graph learning Hypernetwork (CHGH)
framework for joint skill demand-supply prediction. Specifically, CHGH is an
encoder-decoder network consisting of i) a cross-view graph encoder to capture
the interconnection between skill demand and supply, ii) a hierarchical graph
encoder to model the co-evolution of skills from a cluster-wise perspective,
and iii) a conditional hyper-decoder to jointly predict demand and supply
variations by incorporating historical demand-supply gaps. Extensive
experiments on three real-world datasets demonstrate the superiority of the
proposed framework compared to seven baselines and the effectiveness of the
three modules.
\\ ( https://arxiv.org/abs/2401.17838 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17865
Date: Wed, 31 Jan 2024 14:23:51 GMT   (2821kb,D)

Title: Manipulating Predictions over Discrete Inputs in Machine Teaching
Authors: Xiaodong Wu, Yufei Han, Hayssam Dahrouj, Jianbing Ni, Zhenwen Liang,
  Xiangliang Zhang
Categories: cs.LG cs.AI
Comments: 8 pages, 2 figures
ACM-class: I.2.6
\\
  Machine teaching often involves the creation of an optimal (typically
minimal) dataset to help a model (referred to as the `student') achieve
specific goals given by a teacher. While abundant in the continuous domain, the
studies on the effectiveness of machine teaching in the discrete domain are
relatively limited. This paper focuses on machine teaching in the discrete
domain, specifically on manipulating student models' predictions based on the
goals of teachers via changing the training data efficiently. We formulate this
task as a combinatorial optimization problem and solve it by proposing an
iterative searching algorithm. Our algorithm demonstrates significant numerical
merit in the scenarios where a teacher attempts at correcting erroneous
predictions to improve the student's models, or maliciously manipulating the
model to misclassify some specific samples to the target class aligned with his
personal profits. Experimental results show that our proposed algorithm can
have superior performance in effectively and efficiently manipulating the
predictions of the model, surpassing conventional baselines.
\\ ( https://arxiv.org/abs/2401.17865 ,  2821kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17870
Date: Wed, 31 Jan 2024 14:27:35 GMT   (2123kb,D)

Title: Efficient Subseasonal Weather Forecast using Teleconnection-informed
  Transformers
Authors: Shan Zhao, Zhitong Xiong, Xiao Xiang Zhu
Categories: cs.LG cs.AI
Comments: Submitted to IGARSS 2024
\\
  Subseasonal forecasting, which is pivotal for agriculture, water resource
management, and early warning of disasters, faces challenges due to the chaotic
nature of the atmosphere. Recent advances in machine learning (ML) have
revolutionized weather forecasting by achieving competitive predictive skills
to numerical models. However, training such foundation models requires
thousands of GPU days, which causes substantial carbon emissions and limits
their broader applicability. Moreover, ML models tend to fool the pixel-wise
error scores by producing smoothed results which lack physical consistency and
meteorological meaning. To deal with the aforementioned problems, we propose a
teleconnection-informed transformer. Our architecture leverages the pretrained
Pangu model to achieve good initial weights and integrates a
teleconnection-informed temporal module to improve predictability in an
extended temporal range. Remarkably, by adjusting 1.1% of the Pangu model's
parameters, our method enhances predictability on four surface and five
upper-level atmospheric variables at a two-week lead time. Furthermore, the
teleconnection-filtered features improve the spatial granularity of outputs
significantly, indicating their potential physical consistency. Our research
underscores the importance of atmospheric and oceanic teleconnections in
driving future weather conditions. Besides, it presents a resource-efficient
pathway for researchers to leverage existing foundation models on versatile
downstream tasks.
\\ ( https://arxiv.org/abs/2401.17870 ,  2123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17975
Date: Wed, 31 Jan 2024 16:31:54 GMT   (18351kb,D)

Title: Understanding polysemanticity in neural networks through coding theory
Authors: Simon C. Marshall and Jan H. Kirchner
Categories: cs.LG cs.AI
\\
  Despite substantial efforts, neural network interpretability remains an
elusive goal, with previous research failing to provide succinct explanations
of most single neurons' impact on the network output. This limitation is due to
the polysemantic nature of most neurons, whereby a given neuron is involved in
multiple unrelated network states, complicating the interpretation of that
neuron. In this paper, we apply tools developed in neuroscience and information
theory to propose both a novel practical approach to network interpretability
and theoretical insights into polysemanticity and the density of codes. We
infer levels of redundancy in the network's code by inspecting the
eigenspectrum of the activation's covariance matrix. Furthermore, we show how
random projections can reveal whether a network exhibits a smooth or
non-differentiable code and hence how interpretable the code is. This same
framework explains the advantages of polysemantic neurons to learning
performance and explains trends found in recent results by Elhage et
al.~(2022). Our approach advances the pursuit of interpretability in neural
networks, providing insights into their underlying structure and suggesting new
avenues for circuit-level interpretability.
\\ ( https://arxiv.org/abs/2401.17975 ,  18351kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18018
Date: Wed, 31 Jan 2024 17:28:24 GMT   (3000kb,D)

Title: Prompt-Driven LLM Safeguarding via Directed Representation Optimization
Authors: Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei
  Chang, Minlie Huang, Nanyun Peng
Categories: cs.LG cs.AI cs.CL
\\
  Prepending model inputs with safety prompts is a common practice of
safeguarding large language models (LLMs) from complying with queries that
contain harmful intents. However, the working mechanisms of safety prompts have
not yet been fully understood, which hinders the potential for automatically
optimizing them for improved LLM safety. Motivated by this problem, we
investigate the impact of safety prompts from the perspective of model
representations. We find that in models' representation space, harmful and
harmless queries can be largely distinguished, but this is not noticeably
enhanced by safety prompts. Instead, the queries' representations are moved by
different safety prompts in similar directions, where models become more prone
to refusal (i.e., refusing to provide assistance) even when the queries are
harmless. Inspired by these findings, we propose a method called DRO (Directed
Representation Optimization) for automatic safety prompt optimization. DRO
treats safety prompts as continuous, trainable embeddings and learns to move
the representations of harmful/harmless queries along/opposite the direction in
which the model's refusal probability increases. We demonstrate that DRO
remarkably improves the safeguarding performance of human-crafted safety
prompts and outperforms strong baselines, as evaluated on out-of-domain
benchmarks, without compromising the general model capability.
\\ ( https://arxiv.org/abs/2401.18018 ,  3000kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18035
Date: Wed, 31 Jan 2024 17:59:57 GMT   (1585kb,D)

Title: Optimizing contrastive learning for cortical folding pattern detection
Authors: Aymeric Gaudin (1), Louise Guillon (1), Clara Fischer (1), Arnaud
  Cachia (2), Denis Rivi\`ere (1), Jean-Fran\c{c}ois Mangin (1), Jo\"el Chavas
  (1) ((1) Neurospin, Gif-sur-Yvette, France, (2) LaPsyD\'e, Laboratoire
  A.Binet-Sorbonne, Paris, France)
Categories: cs.LG
Comments: 9 pages, 6 figures, 1 table, SPIE Imaging 2024
\\
  The human cerebral cortex has many bumps and grooves called gyri and sulci.
Even though there is a high inter-individual consistency for the main cortical
folds, this is not the case when we examine the exact shapes and details of the
folding patterns. Because of this complexity, characterizing the cortical
folding variability and relating them to subjects' behavioral characteristics
or pathologies is still an open scientific problem. Classical approaches
include labeling a few specific patterns, either manually or
semi-automatically, based on geometric distances, but the recent availability
of MRI image datasets of tens of thousands of subjects makes modern
deep-learning techniques particularly attractive. Here, we build a
self-supervised deep-learning model to detect folding patterns in the cingulate
region. We train a contrastive self-supervised model (SimCLR) on both Human
Connectome Project (1101 subjects) and UKBioBank (21070 subjects) datasets with
topological-based augmentations on the cortical skeletons, which are
topological objects that capture the shape of the folds. We explore several
backbone architectures (convolutional network, DenseNet, and PointNet) for the
SimCLR. For evaluation and testing, we perform a linear classification task on
a database manually labeled for the presence of the "double-parallel" folding
pattern in the cingulate region, which is related to schizophrenia
characteristics. The best model, giving a test AUC of 0.76, is a convolutional
network with 6 layers, a 10-dimensional latent space, a linear projection head,
and using the branch-clipping augmentation. This is the first time that a
self-supervised deep learning model has been applied to cortical skeletons on
such a large dataset and quantitatively evaluated. We can now envisage the next
step: applying it to other brain regions to detect other biomarkers.
\\ ( https://arxiv.org/abs/2401.18035 ,  1585kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18047
Date: Wed, 31 Jan 2024 18:08:06 GMT   (698kb,D)

Title: Epidemic Modeling using Hybrid of Time-varying SIRD, Particle Swarm
  Optimization, and Deep Learning
Authors: Naresh Kumar, Seba Susan
Categories: cs.LG cs.NE physics.soc-ph
Comments: Accepted in ICCCNT 2023
DOI: 10.1109/ICCCNT56998.2023.10308066
\\
  Epidemiological models are best suitable to model an epidemic if the spread
pattern is stationary. To deal with non-stationary patterns and multiple waves
of an epidemic, we develop a hybrid model encompassing epidemic modeling,
particle swarm optimization, and deep learning. The model mainly caters to
three objectives for better prediction: 1. Periodic estimation of the model
parameters. 2. Incorporating impact of all the aspects using data fitting and
parameter optimization 3. Deep learning based prediction of the model
parameters. In our model, we use a system of ordinary differential equations
(ODEs) for Susceptible-Infected-Recovered-Dead (SIRD) epidemic modeling,
Particle Swarm Optimization (PSO) for model parameter optimization, and
stacked-LSTM for forecasting the model parameters. Initial or one time
estimation of model parameters is not able to model multiple waves of an
epidemic. So, we estimate the model parameters periodically (weekly). We use
PSO to identify the optimum values of the model parameters. We next train the
stacked-LSTM on the optimized parameters, and perform forecasting of the model
parameters for upcoming four weeks. Further, we fed the LSTM forecasted
parameters into the SIRD model to forecast the number of COVID-19 cases. We
evaluate the model for highly affected three countries namely; the USA, India,
and the UK. The proposed hybrid model is able to deal with multiple waves, and
has outperformed existing methods on all the three datasets.
\\ ( https://arxiv.org/abs/2401.18047 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18057
Date: Wed, 31 Jan 2024 18:29:10 GMT   (1873kb,D)

Title: Rank Supervised Contrastive Learning for Time Series Classification
Authors: Qianying Ren, Dongsheng Luo, Dongjin Song
Categories: cs.LG
\\
  Recently, various contrastive learning techniques have been developed to
categorize time series data and exhibit promising performance. A general
paradigm is to utilize appropriate augmentations and construct feasible
positive samples such that the encoder can yield robust and discriminative
representations by mapping similar data points closer together in the feature
space while pushing dissimilar data points farther apart. Despite its efficacy,
the fine-grained relative similarity (e.g., rank) information of positive
samples is largely ignored, especially when labeled samples are limited. To
this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform
time series classification. Different from conventional contrastive learning
frameworks, RankSCL augments raw data in a targeted way in the embedding space
and adopts certain filtering rules to select more informative positive and
negative pairs of samples. Moreover, a novel rank loss is developed to assign
different weights for different levels of positive samples, enable the encoder
to extract the fine-grained information of the same class, and produce a clear
boundary among different classes. Thoroughly empirical studies on 128 UCR
datasets and 30 UEA datasets demonstrate that the proposed RankSCL can achieve
state-of-the-art performance compared to existing baseline methods.
\\ ( https://arxiv.org/abs/2401.18057 ,  1873kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18079
Date: Wed, 31 Jan 2024 18:58:14 GMT   (1474kb,D)

Title: KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
  Quantization
Authors: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney,
  Yakun Sophia Shao, Kurt Keutzer, Amir Gholami
Categories: cs.LG
\\
  LLMs are seeing growing use for applications such as document analysis and
summarization which require large context windows, and with these large context
windows KV cache activations surface as the dominant contributor to memory
consumption during inference. Quantization is a promising approach for
compressing KV cache activations; however, existing solutions fail to represent
activations accurately in ultra-low precisions, such as sub-4-bit. In this
work, we present KVQuant, which addresses this problem by incorporating novel
methods for quantizing cached KV activations, including: (i) Per-Channel Key
Quantization, where we adjust the dimension along which we quantize the Key
activations to better match the distribution; (ii) Pre-RoPE Key Quantization,
where we quantize Key activations before the rotary positional embedding to
mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,
where we derive per-layer sensitivity-weighted non-uniform datatypes that
better represent the distributions; (iv) Per-Vector Dense-and-Sparse
Quantization, where we isolate outliers separately for each vector to minimize
skews in quantization ranges; and (v) Q-Norm, where we normalize quantization
centroids in order to mitigate distribution shift, providing additional
benefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,
and Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit
quantization on both Wikitext-2 and C4, outperforming existing approaches. Our
method enables serving the LLaMA-7B model with a context length of up to 1
million on a single A100-80GB GPU and up to 10 million on an 8-GPU system.
\\ ( https://arxiv.org/abs/2401.18079 ,  1474kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.17319 (*cross-listing*)
Date: Thu, 25 Jan 2024 23:35:47 GMT   (4157kb,D)

Title: Decentralized Federated Learning: A Survey on Security and Privacy
Authors: Ehsan Hallaji and Roozbeh Razavi-Far and Mehrdad Saif and Boyu Wang
  and Qiang Yang
Categories: cs.CR cs.AI cs.LG stat.ML
Comments: Accepted for publication in IEEE Transactions on Big Data
\\
  Federated learning has been rapidly evolving and gaining popularity in recent
years due to its privacy-preserving features, among other advantages.
Nevertheless, the exchange of model updates and gradients in this architecture
provides new attack surfaces for malicious users of the network which may
jeopardize the model performance and user and data privacy. For this reason,
one of the main motivations for decentralized federated learning is to
eliminate server-related threats by removing the server from the network and
compensating for it through technologies such as blockchain. However, this
advantage comes at the cost of challenging the system with new privacy threats.
Thus, performing a thorough security analysis in this new paradigm is
necessary. This survey studies possible variations of threats and adversaries
in decentralized federated learning and overviews the potential defense
mechanisms. Trustability and verifiability of decentralized federated learning
are also considered in this study.
\\ ( https://arxiv.org/abs/2401.17319 ,  4157kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17343 (*cross-listing*)
Date: Tue, 30 Jan 2024 14:18:37 GMT   (17298kb,D)

Title: YTCommentQA: Video Question Answerability in Instructional Videos
Authors: Saelyne Yang, Sunghyun Park, Yunseok Jang, Moontae Lee
Categories: cs.CV cs.AI
Comments: AAAI 2024
\\
  Instructional videos provide detailed how-to guides for various tasks, with
viewers often posing questions regarding the content. Addressing these
questions is vital for comprehending the content, yet receiving immediate
answers is difficult. While numerous computational models have been developed
for Video Question Answering (Video QA) tasks, they are primarily trained on
questions generated based on video content, aiming to produce answers from
within the content. However, in real-world situations, users may pose questions
that go beyond the video's informational boundaries, highlighting the necessity
to determine if a video can provide the answer. Discerning whether a question
can be answered by video content is challenging due to the multi-modal nature
of videos, where visual and verbal information are intertwined. To bridge this
gap, we present the YTCommentQA dataset, which contains naturally-generated
questions from YouTube, categorized by their answerability and required
modality to answer -- visual, script, or both. Experiments with answerability
classification tasks demonstrate the complexity of YTCommentQA and emphasize
the need to comprehend the combined role of visual and script information in
video reasoning. The dataset is available at
https://github.com/lgresearch/YTCommentQA.
\\ ( https://arxiv.org/abs/2401.17343 ,  17298kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17417 (*cross-listing*)
Date: Tue, 30 Jan 2024 20:17:51 GMT   (876kb,D)

Title: Through-Wall Imaging based on WiFi Channel State Information
Authors: Julian Strohmayer, Rafael Sterzinger, Christian Stippel, Martin Kampel
Categories: cs.CV cs.AI cs.LG
\\
  This work presents a seminal approach for synthesizing images from WiFi
Channel State Information (CSI) in through-wall scenarios. Leveraging the
strengths of WiFi, such as cost-effectiveness, illumination invariance, and
wall-penetrating capabilities, our approach enables visual monitoring of indoor
environments beyond room boundaries and without the need for cameras. More
generally, it improves the interpretability of WiFi CSI by unlocking the option
to perform image-based downstream tasks, e.g., visual activity recognition. In
order to achieve this crossmodal translation from WiFi CSI to images, we rely
on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics.
We extensively evaluate our proposed methodology through an ablation study on
architecture configuration and a quantitative/qualitative assessment of
reconstructed images. Our results demonstrate the viability of our method and
highlight its potential for practical applications.
\\ ( https://arxiv.org/abs/2401.17417 ,  876kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17424 (*cross-listing*)
Date: Tue, 30 Jan 2024 20:27:28 GMT   (1202kb,D)

Title: Application of Neural Networks for the Reconstruction of Supernova
  Neutrino Energy Spectra Following Fast Neutrino Flavor Conversions
Authors: Sajad Abbar, Meng-Ru Wu, and Zewei Xiong
Categories: astro-ph.HE cs.AI cs.LG
Comments: 13 pages, 6 figures, submitted to PRD. arXiv admin note: text overlap
  with arXiv:2311.15656
Report-no: MPP-2024-12
\\
  Neutrinos can undergo fast flavor conversions (FFCs) within extremely dense
astrophysical environments such as core-collapse supernovae (CCSNe) and neutron
star mergers (NSMs). In this study, we explore FFCs in a \emph{multi-energy}
neutrino gas, revealing that when the FFC growth rate significantly exceeds
that of the vacuum Hamiltonian, all neutrinos (regardless of energy) share a
common survival probability dictated by the energy-integrated neutrino
spectrum. We then employ physics-informed neural networks (PINNs) to predict
the asymptotic outcomes of FFCs within such a multi-energy neutrino gas. These
predictions are based on the first two moments of neutrino angular
distributions for each energy bin, typically available in state-of-the-art CCSN
and NSM simulations. Our PINNs achieve errors as low as $\lesssim6\%$ and
$\lesssim 18\%$ for predicting the number of neutrinos in the electron channel
and the relative absolute error in the neutrino moments, respectively.
\\ ( https://arxiv.org/abs/2401.17424 ,  1202kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17434 (*cross-listing*)
Date: Tue, 30 Jan 2024 20:45:49 GMT   (264kb)

Title: Integrating Generative AI in Hackathons: Opportunities, Challenges, and
  Educational Implications
Authors: Ramteja Sajja, Carlos Erazo, Zhouyayan Li, Bekir Z. Demiray, Yusuf
  Sermet and Ibrahim Demir
Categories: cs.CY cs.AI cs.HC
Comments: 8491 words, 23 pages, 12 figures
\\
  Hackathons and software competitions, increasingly pivotal in the software
industry, serve as vital catalysts for innovation and skill development for
both organizations and students. These platforms enable companies to prototype
ideas swiftly, while students gain enriched learning experiences, enhancing
their practical skills. Over the years, hackathons have transitioned from mere
competitive events to significant educational tools, fusing theoretical
knowledge with real-world problem-solving. The integration of hackathons into
computer science and software engineering curricula aims to align educational
proficiencies within a collaborative context, promoting peer connectivity and
enriched learning via industry-academia collaborations. However, the infusion
of advanced technologies, notably artificial intelligence (AI), and machine
learning, into hackathons is revolutionizing their structure and outcomes. This
evolution brings forth both opportunities, like enhanced learning experiences,
and challenges, such as ethical concerns. This study delves into the impact of
generative AI, examining its influence on student's technological choices based
on a case study on the University of Iowa 2023 event. The exploration provides
insights into AI's role in hackathons, and its educational implications, and
offers a roadmap for the integration of such technologies in future events,
ensuring innovation is balanced with ethical and educational considerations.
\\ ( https://arxiv.org/abs/2401.17434 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17455 (*cross-listing*)
Date: Tue, 30 Jan 2024 21:33:05 GMT   (3476kb,D)

Title: Multiscale Parallel Tempering for Fast Sampling on Redistricting Plans
Authors: Gabriel Chuang, Gregory Herschlag, Jonathan C. Mattingly
Categories: physics.soc-ph cs.AI cs.SI math.PR
Comments: 26 pages with appendix; 11 figures
MSC-class: 60J10, 91G60
ACM-class: G.3.8; E.1.3
\\
  When auditing a redistricting plan, a persuasive method is to compare the
plan with an ensemble of neutrally drawn redistricting plans. Ensembles are
generated via algorithms that sample distributions on balanced graph
partitions. To audit the partisan difference between the ensemble and a given
plan, one must ensure that the non-partisan criteria are matched so that we may
conclude that partisan differences come from bias rather than, for example,
levels of compactness or differences in community preservation. Certain
sampling algorithms allow one to explicitly state the policy-based probability
distribution on plans, however, these algorithms have shown poor mixing times
for large graphs (i.e. redistricting spaces) for all but a few specialized
measures. In this work, we generate a multiscale parallel tempering approach
that makes local moves at each scale. The local moves allow us to adopt a wide
variety of policy-based measures. We examine our method in the state of
Connecticut and succeed at achieving fast mixing on a policy-based distribution
that has never before been sampled at this scale. Our algorithm shows promise
to expand to a significantly wider class of measures that will (i) allow for
more principled and situation-based comparisons and (ii) probe for the typical
partisan impact that policy can have on redistricting.
\\ ( https://arxiv.org/abs/2401.17455 ,  3476kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17459 (*cross-listing*)
Date: Tue, 30 Jan 2024 21:42:59 GMT   (36kb)

Title: A Preliminary Study on Using Large Language Models in Software
  Pentesting
Authors: Kumar Shashwat, Francis Hahn, Xinming Ou, Dmitry Goldgof, Lawrence
  Hall, Jay Ligatti, S. Raj Rajgopalan, Armin Ziaie Tabari
Categories: cs.CR cs.AI
\\
  Large language models (LLM) are perceived to offer promising potentials for
automating security tasks, such as those found in security operation centers
(SOCs). As a first step towards evaluating this perceived potential, we
investigate the use of LLMs in software pentesting, where the main task is to
automatically identify software security vulnerabilities in source code. We
hypothesize that an LLM-based AI agent can be improved over time for a specific
security task as human operators interact with it. Such improvement can be
made, as a first step, by engineering prompts fed to the LLM based on the
responses produced, to include relevant contexts and structures so that the
model provides more accurate results. Such engineering efforts become
sustainable if the prompts that are engineered to produce better results on
current tasks, also produce better results on future unknown tasks. To examine
this hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains
2,740 hand-crafted source code test cases containing various types of
vulnerabilities. We divide the test cases into training and testing data, where
we engineer the prompts based on the training data (only), and evaluate the
final system on the testing data. We compare the AI agent's performance on the
testing data against the performance of the agent without the prompt
engineering. We also compare the AI agent's results against those from
SonarQube, a widely used static code analyzer for security testing. We built
and tested multiple versions of the AI agent using different off-the-shelf LLMs
-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with
both chat completion and assistant APIs). The results show that using LLMs is a
viable approach to build an AI agent for software pentesting that can improve
through repeated use and prompt engineering.
\\ ( https://arxiv.org/abs/2401.17459 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17500 (*cross-listing*)
Date: Tue, 30 Jan 2024 23:18:35 GMT   (28047kb,D)

Title: LeTO: Learning Constrained Visuomotor Policy with Differentiable
  Trajectory Optimization
Authors: Zhengtong Xu, Yu She
Categories: cs.RO cs.AI
Comments: 8 pages, 5 figures
\\
  This paper introduces LeTO, a method for learning constrained visuomotor
policy via differentiable trajectory optimization. Our approach uniquely
integrates a differentiable optimization layer into the neural network. By
formulating the optimization layer as a trajectory optimization problem, we
enable the model to end-to-end generate actions in a safe and controlled
fashion without extra modules. Our method allows for the introduction of
constraints information during the training process, thereby balancing the
training objectives of satisfying constraints, smoothing the trajectories, and
minimizing errors with demonstrations. This "gray box" method marries the
optimization-based safety and interpretability with the powerful
representational abilities of neural networks. We quantitatively evaluate LeTO
in simulation and on the real robot. In simulation, LeTO achieves a success
rate comparable to state-of-the-art imitation learning methods, but the
generated trajectories are of less uncertainty, higher quality, and smoother.
In real-world experiments, we deployed LeTO to handle constraints-critical
tasks. The results show the effectiveness of LeTO comparing with
state-of-the-art imitation learning approaches. We release our code at
https://github.com/ZhengtongXu/LeTO.
\\ ( https://arxiv.org/abs/2401.17500 ,  28047kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17513 (*cross-listing*)
Date: Wed, 31 Jan 2024 00:15:31 GMT   (501kb,D)

Title: A PNP ion channel deep learning solver with local neural network and
  finite element input data
Authors: Hwi Lee, Zhen Chao, Harris Cobb, Yingjie Liu, Dexuan Xie
Categories: physics.bio-ph cs.AI physics.comp-ph
Comments: 16 pages, 4 figures, 5 tables
MSC-class: 92-08
\\
  In this paper, a deep learning method for solving an improved one-dimensional
Poisson-Nernst-Planck ion channel (PNPic) model, called the PNPic deep learning
solver, is presented. In particular, it combines a novel local neural network
scheme with an effective PNPic finite element solver. Since the input data of
the neural network scheme only involves a small local patch of coarse grid
solutions, which the finite element solver can quickly produce, the PNPic deep
learning solver can be trained much faster than any corresponding conventional
global neural network solvers. After properly trained, it can output a
predicted PNPic solution in a much higher degree of accuracy than the low cost
coarse grid solutions and can reflect different perturbation cases on the
parameters, ion channel subregions, and interface and boundary values, etc.
Consequently, the PNPic deep learning solver can generate a numerical solution
with high accuracy for a family of PNPic models. As an initial study, two types
of numerical tests were done by perturbing one and two parameters of the PNPic
model, respectively, as well as the tests done by using a few perturbed
interface positions of the model as training samples. These tests demonstrate
that the PNPic deep learning solver can generate highly accurate PNPic
numerical solutions.
\\ ( https://arxiv.org/abs/2401.17513 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17583 (*cross-listing*)
Date: Wed, 31 Jan 2024 03:58:28 GMT   (12475kb,D)

Title: Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion
Authors: Tairan He, Chong Zhang, Wenli Xiao, Guanqi He, Changliu Liu, Guanya
  Shi
Categories: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY
Comments: Project website: https://agile-but-safe.github.io/
\\
  Legged robots navigating cluttered environments must be jointly agile for
efficient task execution and safe to avoid collisions with obstacles or humans.
Existing studies either develop conservative controllers (< 1.0 m/s) to ensure
safety, or focus on agility without considering potentially fatal collisions.
This paper introduces Agile But Safe (ABS), a learning-based control framework
that enables agile and collision-free locomotion for quadrupedal robots. ABS
involves an agile policy to execute agile motor skills amidst obstacles and a
recovery policy to prevent failures, collaboratively achieving high-speed and
collision-free navigation. The policy switch in ABS is governed by a learned
control-theoretic reach-avoid value network, which also guides the recovery
policy as an objective function, thereby safeguarding the robot in a closed
loop. The training process involves the learning of the agile policy, the
reach-avoid value network, the recovery policy, and an exteroception
representation network, all in simulation. These trained modules can be
directly deployed in the real world with onboard sensing and computation,
leading to high-speed and collision-free navigation in confined indoor and
outdoor spaces with both static and dynamic obstacles.
\\ ( https://arxiv.org/abs/2401.17583 ,  12475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17592 (*cross-listing*)
Date: Wed, 31 Jan 2024 04:32:41 GMT   (9179kb,D)

Title: Local Feature Matching Using Deep Learning: A Survey
Authors: Shibiao Xu, Shunpeng Chen, Rongtao Xu, Changwei Wang, Peng Lu, Li Guo
Categories: cs.CV cs.AI
\\
  Local feature matching enjoys wide-ranging applications in the realm of
computer vision, encompassing domains such as image retrieval, 3D
reconstruction, and object recognition. However, challenges persist in
improving the accuracy and robustness of matching due to factors like viewpoint
and lighting variations. In recent years, the introduction of deep learning
models has sparked widespread exploration into local feature matching
techniques. The objective of this endeavor is to furnish a comprehensive
overview of local feature matching methods. These methods are categorized into
two key segments based on the presence of detectors. The Detector-based
category encompasses models inclusive of Detect-then-Describe, Joint Detection
and Description, Describe-then-Detect, as well as Graph Based techniques. In
contrast, the Detector-free category comprises CNN Based, Transformer Based,
and Patch Based methods. Our study extends beyond methodological analysis,
incorporating evaluations of prevalent datasets and metrics to facilitate a
quantitative comparison of state-of-the-art techniques. The paper also explores
the practical application of local feature matching in diverse domains such as
Structure from Motion, Remote Sensing Image Registration, and Medical Image
Registration, underscoring its versatility and significance across various
fields. Ultimately, we endeavor to outline the current challenges faced in this
domain and furnish future research directions, thereby serving as a reference
for researchers involved in local feature matching and its interconnected
domains.
\\ ( https://arxiv.org/abs/2401.17592 ,  9179kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17617 (*cross-listing*)
Date: Wed, 31 Jan 2024 06:12:28 GMT   (9061kb,D)

Title: Unveiling the Power of Self-supervision for Multi-view Multi-human
  Association and Tracking
Authors: Wei Feng, Feifan Wang, Ruize Han, Zekun Qian and Song Wang
Categories: cs.CV cs.AI
\\
  Multi-view multi-human association and tracking (MvMHAT), is a new but
important problem for multi-person scene video surveillance, aiming to track a
group of people over time in each view, as well as to identify the same person
across different views at the same time, which is different from previous MOT
and multi-camera MOT tasks only considering the over-time human tracking. This
way, the videos for MvMHAT require more complex annotations while containing
more information for self learning. In this work, we tackle this problem with a
self-supervised learning aware end-to-end network. Specifically, we propose to
take advantage of the spatial-temporal self-consistency rationale by
considering three properties of reflexivity, symmetry and transitivity. Besides
the reflexivity property that naturally holds, we design the self-supervised
learning losses based on the properties of symmetry and transitivity, for both
appearance feature learning and assignment matrix optimization, to associate
the multiple humans over time and across views. Furthermore, to promote the
research on MvMHAT, we build two new large-scale benchmarks for the network
training and testing of different algorithms. Extensive experiments on the
proposed benchmarks verify the effectiveness of our method. We have released
the benchmark and code to the public.
\\ ( https://arxiv.org/abs/2401.17617 ,  9061kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17626 (*cross-listing*)
Date: Wed, 31 Jan 2024 06:58:26 GMT   (359kb)

Title: Generative AI to Generate Test Data Generators
Authors: Benoit Baudry, Khashayar Etemadi, Sen Fang, Yogya Gamage, Yi Liu,
  Yuxin Liu, Martin Monperrus, Javier Ron, Andr\'e Silva, Deepika Tiwari
Categories: cs.SE cs.AI cs.LG
\\
  Generating fake data is an essential dimension of modern software testing, as
demonstrated by the number and significance of data faking libraries. Yet,
developers of faking libraries cannot keep up with the wide range of data to be
generated for different natural languages and domains. In this paper, we assess
the ability of generative AI for generating test data in different domains. We
design three types of prompts for Large Language Models (LLMs), which perform
test data generation tasks at different levels of integrability: 1) raw test
data generation, 2) synthesizing programs in a specific language that generate
useful test data, and 3) producing programs that use state-of-the-art faker
libraries. We evaluate our approach by prompting LLMs to generate test data for
11 domains. The results show that LLMs can successfully generate realistic test
data generators in a wide range of domains at all three levels of
integrability.
\\ ( https://arxiv.org/abs/2401.17626 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17645 (*cross-listing*)
Date: Wed, 31 Jan 2024 07:58:54 GMT   (665kb,D)

Title: ReSLLM: Large Language Models are Strong Resource Selectors for
  Federated Search
Authors: Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon
Categories: cs.IR cs.AI
\\
  Federated search, which involves integrating results from multiple
independent search engines, will become increasingly pivotal in the context of
Retrieval-Augmented Generation pipelines empowering LLM-based applications such
as chatbots. These systems often distribute queries among various search
engines, ranging from specialized (e.g., PubMed) to general (e.g., Google),
based on the nature of user utterances. A critical aspect of federated search
is resource selection - the selection of appropriate resources prior to issuing
the query to ensure high-quality and rapid responses, and contain costs
associated with calling the external search engines. However, current SOTA
resource selection methodologies primarily rely on feature-based learning
approaches. These methods often involve the labour intensive and expensive
creation of training labels for each resource. In contrast, LLMs have exhibited
strong effectiveness as zero-shot methods across NLP and IR tasks. We
hypothesise that in the context of federated search LLMs can assess the
relevance of resources without the need for extensive predefined labels or
features. In this paper, we propose ReSLLM. Our ReSLLM method exploits LLMs to
drive the selection of resources in federated search in a zero-shot setting. In
addition, we devise an unsupervised fine tuning protocol, the Synthetic Label
Augmentation Tuning (SLAT), where the relevance of previously logged queries
and snippets from resources is predicted using an off-the-shelf LLM and then in
turn used to fine-tune ReSLLM with respect to resource selection. Our empirical
evaluation and analysis details the factors influencing the effectiveness of
LLMs in this context. The results showcase the merits of ReSLLM for resource
selection: not only competitive effectiveness in the zero-shot setting, but
also obtaining large when fine-tuned using SLAT-protocol.
\\ ( https://arxiv.org/abs/2401.17645 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17661 (*cross-listing*)
Date: Wed, 31 Jan 2024 08:31:08 GMT   (2566kb,D)

Title: Towards the implementation of Industry 4.0: A methodology-based approach
  oriented to the customer life cycle
Authors: V\'ictor Julio Ram\'irez-Dur\'an, Idoia Berges, Arantza Illarramendi
Categories: cs.SE cs.AI
Comments: Accepted version of paper: V\'ictor Julio Ram\'irez-Dur\'an, Idoia
  Berges, Arantza Illarramendi: Towards the implementation of Industry 4.0: A
  methodology-based approach oriented to the customer life cycle. Comput. Ind.
  126: 103403 (2021). DOI: 10.1016/j.compind.2021.103403
Journal-ref: Comput. Ind. 126: 103403 (2021)
DOI: 10.1016/j.compind.2021.103403
\\
  Many different worldwide initiatives are promoting the transformation from
machine dominant manufacturing to digital manufacturing. Thus, to achieve a
successful transformation to Industry 4.0 standard, manufacturing enterprises
are required to implement a clear roadmap. However, Small and Medium
Manufacturing Enterprises (SMEs) encounter many barriers and difficulties
(economical, technical, cultural, etc.) in the implementation of Industry 4.0.
Although several works deal with the incorporation of Industry 4.0 technologies
in the area of the product and supply chain life cycles, which SMEs could use
as reference, this is not the case for the customer life cycle. Thus, we
present two contributions that can help the software engineers of those SMEs to
incorporate Industry 4.0 technologies in the context of the customer life
cycle. The first contribution is a methodology that can help those software
engineers in the task of creating new software services, aligned with Industry
4.0, that allow to change how customers interact with enterprises and the
experiences they have while interacting with them. The methodology details a
set of stages that are divided into phases which in turn are made up of
activities. It places special emphasis on the incorporation of semantics
descriptions and 3D visualization in the implementation of those new services.
The second contribution is a system developed for a real manufacturing
scenario, using the proposed methodology, which allows to observe the
possibilities that this kind of systems can offer to SMEs in two phases of the
customer life cycle: Discover & Shop, and Use & Service.
\\ ( https://arxiv.org/abs/2401.17661 ,  2566kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17690 (*cross-listing*)
Date: Wed, 31 Jan 2024 09:23:16 GMT   (1810kb,D)

Title: EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for
  Automated Audio Captioning
Authors: Jaeyeon Kim, Jaeyoon Jung, Jinjoo Lee, Sang Hoon Woo
Categories: eess.AS cs.AI cs.SD
Comments: Accepted to ICASSP 2024
\\
  We propose EnCLAP, a novel framework for automated audio captioning. EnCLAP
employs two acoustic representation models, EnCodec and CLAP, along with a
pretrained language model, BART. We also introduce a new training objective
called masked codec modeling that improves acoustic awareness of the pretrained
language model. Experimental results on AudioCaps and Clotho demonstrate that
our model surpasses the performance of baseline models. Source code will be
available at https://github.com/jaeyeonkim99/EnCLAP . An online demo is
available at https://huggingface.co/spaces/enclap-team/enclap .
\\ ( https://arxiv.org/abs/2401.17690 ,  1810kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17700 (*cross-listing*)
Date: Wed, 31 Jan 2024 09:45:03 GMT   (233kb)

Title: Classification of executive functioning performance post-longitudinal
  tDCS using functional connectivity and machine learning methods
Authors: Akash K Rao, Vishnu K Menon, Shashank Uttrani, Ayushman Dixit,
  Dipanshu Verma, Varun Dutt
Categories: cs.HC cs.AI
Comments: 7 pages, presented at the IEEE 20th India Council International
  Conference (INDICON 2023), Hyderabad, India, December 2023
\\
  Executive functioning is a cognitive process that enables humans to plan,
organize, and regulate their behavior in a goal-directed manner. Understanding
and classifying the changes in executive functioning after longitudinal
interventions (like transcranial direct current stimulation (tDCS)) has not
been explored in the literature. This study employs functional connectivity and
machine learning algorithms to classify executive functioning performance
post-tDCS. Fifty subjects were divided into experimental and placebo control
groups. EEG data was collected while subjects performed an executive
functioning task on Day 1. The experimental group received tDCS during task
training from Day 2 to Day 8, while the control group received sham tDCS. On
Day 10, subjects repeated the tasks specified on Day 1. Different functional
connectivity metrics were extracted from EEG data and eventually used for
classifying executive functioning performance using different machine learning
algorithms. Results revealed that a novel combination of partial directed
coherence and multi-layer perceptron (along with recursive feature elimination)
resulted in a high classification accuracy of 95.44%. We discuss the
implications of our results in developing real-time neurofeedback systems for
assessing and enhancing executive functioning performance post-tDCS
administration.
\\ ( https://arxiv.org/abs/2401.17700 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17711 (*cross-listing*)
Date: Wed, 31 Jan 2024 10:03:27 GMT   (385kb)

Title: Prediction of multitasking performance post-longitudinal tDCS via
  EEG-based functional connectivity and machine learning methods
Authors: Akash K Rao, Shashank Uttrani, Vishnu K Menon, Darshil Shah, Arnav
  Bhavsar, Shubhajit Roy Chowdhury, Varun Dutt
Categories: cs.HC cs.AI
Comments: 16 pages, presented at the 30th International Conference on Neural
  Information Processing (ICONIP2023), Changsha, China, November 2023
\\
  Predicting and understanding the changes in cognitive performance, especially
after a longitudinal intervention, is a fundamental goal in neuroscience.
Longitudinal brain stimulation-based interventions like transcranial direct
current stimulation (tDCS) induce short-term changes in the resting membrane
potential and influence cognitive processes. However, very little research has
been conducted on predicting these changes in cognitive performance
post-intervention. In this research, we intend to address this gap in the
literature by employing different EEG-based functional connectivity analyses
and machine learning algorithms to predict changes in cognitive performance in
a complex multitasking task. Forty subjects were divided into experimental and
active-control conditions. On Day 1, all subjects executed a multitasking task
with simultaneous 32-channel EEG being acquired. From Day 2 to Day 7, subjects
in the experimental condition undertook 15 minutes of 2mA anodal tDCS
stimulation during task training. Subjects in the active-control condition
undertook 15 minutes of sham stimulation during task training. On Day 10, all
subjects again executed the multitasking task with EEG acquisition.
Source-level functional connectivity metrics, namely phase lag index and
directed transfer function, were extracted from the EEG data on Day 1 and Day
10. Various machine learning models were employed to predict changes in
cognitive performance. Results revealed that the multi-layer perceptron and
directed transfer function recorded a cross-validation training RMSE of 5.11%
and a test RMSE of 4.97%. We discuss the implications of our results in
developing real-time cognitive state assessors for accurately predicting
cognitive performance in dynamic and complex tasks post-tDCS intervention
\\ ( https://arxiv.org/abs/2401.17711 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17733 (*cross-listing*)
Date: Wed, 31 Jan 2024 10:54:34 GMT   (938kb)

Title: Towards Physical Plausibility in Neuroevolution Systems
Authors: Gabriel Cort\^es, Nuno Louren\c{c}o, Penousal Machado
Categories: cs.NE cs.AI cs.LG
\\
  The increasing usage of Artificial Intelligence (AI) models, especially Deep
Neural Networks (DNNs), is increasing the power consumption during training and
inference, posing environmental concerns and driving the need for more
energy-efficient algorithms and hardware solutions. This work addresses the
growing energy consumption problem in Machine Learning (ML), particularly
during the inference phase. Even a slight reduction in power usage can lead to
significant energy savings, benefiting users, companies, and the environment.
Our approach focuses on maximizing the accuracy of Artificial Neural Network
(ANN) models using a neuroevolutionary framework whilst minimizing their power
consumption. To do so, power consumption is considered in the fitness function.
We introduce a new mutation strategy that stochastically reintroduces modules
of layers, with power-efficient modules having a higher chance of being chosen.
We introduce a novel technique that allows training two separate models in a
single training step whilst promoting one of them to be more power efficient
than the other while maintaining similar accuracy. The results demonstrate a
reduction in power consumption of ANN models by up to 29.2% without a
significant decrease in predictive performance.
\\ ( https://arxiv.org/abs/2401.17733 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17739 (*cross-listing*)
Date: Wed, 31 Jan 2024 10:59:57 GMT   (6236kb,D)

Title: Operator learning without the adjoint
Authors: Nicolas Boull\'e, Diana Halikias, Samuel E. Otto, Alex Townsend
Categories: math.NA cs.AI cs.LG cs.NA
Comments: 49 pages, 5 figures
\\
  There is a mystery at the heart of operator learning: how can one recover a
non-self-adjoint operator from data without probing the adjoint? Current
practical approaches suggest that one can accurately recover an operator while
only using data generated by the forward action of the operator without access
to the adjoint. However, naively, it seems essential to sample the action of
the adjoint. In this paper, we partially explain this mystery by proving that
without querying the adjoint, one can approximate a family of non-self-adjoint
infinite-dimensional compact operators via projection onto a Fourier basis. We
then apply the result to recovering Green's functions of elliptic partial
differential operators and derive an adjoint-free sample complexity bound.
While existing theory justifies low sample complexity in operator learning,
ours is the first adjoint-free analysis that attempts to close the gap between
theory and practice.
\\ ( https://arxiv.org/abs/2401.17739 ,  6236kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17741 (*cross-listing*)
Date: Wed, 31 Jan 2024 11:00:26 GMT   (16997kb,D)

Title: Haris: an Advanced Autonomous Mobile Robot for Smart Parking Assistance
Authors: Layth Hamad, Muhammad Asif Khan, Hamid Menouar, Fethi Filali, Amr
  Mohamed
Categories: cs.RO cs.AI
Comments: Accepted in 2024 IEEE International Conference on Consumer
  Electronics (ICCE), Las Vegas, NV, USA, 2024
\\
  This paper presents Haris, an advanced autonomous mobile robot system for
tracking the location of vehicles in crowded car parks using license plate
recognition. The system employs simultaneous localization and mapping (SLAM)
for autonomous navigation and precise mapping of the parking area, eliminating
the need for GPS dependency. In addition, the system utilizes a sophisticated
framework using computer vision techniques for object detection and automatic
license plate recognition (ALPR) for reading and associating license plate
numbers with location data. This information is subsequently synchronized with
a back-end service and made accessible to users via a user-friendly mobile app,
offering effortless vehicle location and alleviating congestion within the
parking facility. The proposed system has the potential to improve the
management of short-term large outdoor parking areas in crowded places such as
sports stadiums. The demo of the robot can be found on
https://youtu.be/ZkTCM35fxa0?si=QjggJuN7M1o3oifx.
\\ ( https://arxiv.org/abs/2401.17741 ,  16997kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17776 (*cross-listing*)
Date: Wed, 31 Jan 2024 12:16:39 GMT   (3486kb,D)

Title: Double InfoGAN for Contrastive Analysis
Authors: Florence Carton, Robin Louiset, Pietro Gori
Categories: cs.CV cs.AI stat.ML
Comments: Accepted at AISTATS 2024
\\
  Contrastive Analysis (CA) deals with the discovery of what is common and what
is distinctive of a target domain compared to a background one. This is of
great interest in many applications, such as medical imaging. Current
state-of-the-art (SOTA) methods are latent variable models based on VAE
(CA-VAEs). However, they all either ignore important constraints or they don't
enforce fundamental assumptions. This may lead to sub-optimal solutions where
distinctive factors are mistaken for common ones (or viceversa). Furthermore,
the generated images have a rather poor quality, typical of VAEs, decreasing
their interpretability and usefulness. Here, we propose Double InfoGAN, the
first GAN based method for CA that leverages the high-quality synthesis of GAN
and the separation power of InfoGAN. Experimental results on four visual
datasets, from simple synthetic examples to complex medical images, show that
the proposed method outperforms SOTA CA-VAEs in terms of latent separation and
image quality. Datasets and code are available online.
\\ ( https://arxiv.org/abs/2401.17776 ,  3486kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17805 (*cross-listing*)
Date: Wed, 31 Jan 2024 13:04:34 GMT   (1000kb,D)

Title: Biospheric AI
Authors: Marcin Korecki
Categories: cs.CY cs.AI
\\
  The dominant paradigm in AI ethics and value alignment is highly
anthropocentric. The focus of these disciplines is strictly on human values
which limits the depth and breadth of their insights. Recently, attempts to
expand to a sentientist perspective have been initiated. We argue that neither
of these outlooks is sufficient to capture the actual complexity of the
biosphere and ensure that AI does not damage it. Thus, we propose a new
paradigm -- Biospheric AI that assumes an ecocentric perspective. We discuss
hypothetical ways in which such an AI might be designed. Moreover, we give
directions for research and application of the modern AI models that would be
consistent with the biospheric interests. All in all, this work attempts to
take first steps towards a comprehensive program of research that focuses on
the interactions between AI and the biosphere.
\\ ( https://arxiv.org/abs/2401.17805 ,  1000kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17812 (*cross-listing*)
Date: Wed, 31 Jan 2024 13:12:42 GMT   (923kb,D)

Title: Deterministic Computing Power Networking: Architecture, Technologies and
  Prospects
Authors: Qingmin Jia, Yujiao Hu, Xiaomao Zhou, Qianpiao Ma, Kai Guo, Huayu
  Zhang, Renchao Xie, Tao Huang, Yunjie Liu
Categories: cs.NI cs.AI
\\
  With the development of new Internet services such as computation-intensive
and delay-sensitive tasks, the traditional "Best Effort" network transmission
mode has been greatly challenged. The network system is urgently required to
provide end-to-end transmission determinacy and computing determinacy for new
applications to ensure the safe and efficient operation of services. Based on
the research of the convergence of computing and networking, a new network
paradigm named deterministic computing power networking (Det-CPN) is proposed.
In this article, we firstly introduce the research advance of computing power
networking. And then the motivations and scenarios of Det-CPN are analyzed.
Following that, we present the system architecture, technological capabilities,
workflow as well as key technologies for Det-CPN. Finally, the challenges and
future trends of Det-CPN are analyzed and discussed.
\\ ( https://arxiv.org/abs/2401.17812 ,  923kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17828 (*cross-listing*)
Date: Wed, 31 Jan 2024 13:41:17 GMT   (20667kb,D)

Title: Leveraging Swin Transformer for Local-to-Global Weakly Supervised
  Semantic Segmentation
Authors: Rozhan Ahmadi, Shohreh Kasaei
Categories: cs.CV cs.AI
Comments: 7 pages, 4 figures, 3 tables
\\
  In recent years, weakly supervised semantic segmentation using image-level
labels as supervision has received significant attention in the field of
computer vision. Most existing methods have addressed the challenges arising
from the lack of spatial information in these labels by focusing on
facilitating supervised learning through the generation of pseudo-labels from
class activation maps (CAMs). Due to the localized pattern detection of
Convolutional Neural Networks (CNNs), CAMs often emphasize only the most
discriminative parts of an object, making it challenging to accurately
distinguish foreground objects from each other and the background. Recent
studies have shown that Vision Transformer (ViT) features, due to their global
view, are more effective in capturing the scene layout than CNNs. However, the
use of hierarchical ViTs has not been extensively explored in this field. This
work explores the use of Swin Transformer by proposing "SWTformer" to enhance
the accuracy of the initial seed CAMs by bringing local and global views
together. SWTformer-V1 generates class probabilities and CAMs using only the
patch tokens as features. SWTformer-V2 incorporates a multi-scale feature
fusion mechanism to extract additional information and utilizes a
background-aware mechanism to generate more accurate localization maps with
improved cross-object discrimination. Based on experiments on the PascalVOC
2012 dataset, SWTformer-V1 achieves a 0.98% mAP higher localization accuracy,
outperforming state-of-the-art models. It also yields comparable performance by
0.82% mIoU on average higher than other methods in generating initial
localization maps, depending only on the classification network. SWTformer-V2
further improves the accuracy of the generated seed CAMs by 5.32% mIoU, further
proving the effectiveness of the local-to-global view provided by the Swin
transformer.
\\ ( https://arxiv.org/abs/2401.17828 ,  20667kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17842 (*cross-listing*)
Date: Wed, 31 Jan 2024 14:02:26 GMT   (17068kb,D)

Title: Explainable Benchmarking for Iterative Optimization Heuristics
Authors: Niki van Stein, Diederick Vermetten, Anna V. Kononova, Thomas B\"ack
Categories: cs.NE cs.AI
Comments: Will be submitted to ACM TELO
\\
  Benchmarking heuristic algorithms is vital to understand under which
conditions and on what kind of problems certain algorithms perform well. In
most current research into heuristic optimization algorithms, only a very
limited number of scenarios, algorithm configurations and hyper-parameter
settings are explored, leading to incomplete and often biased insights and
results. This paper presents a novel approach we call explainable benchmarking.
Introducing the IOH-Xplainer software framework, for analyzing and
understanding the performance of various optimization algorithms and the impact
of their different components and hyper-parameters. We showcase the framework
in the context of two modular optimization frameworks. Through this framework,
we examine the impact of different algorithmic components and configurations,
offering insights into their performance across diverse scenarios. We provide a
systematic method for evaluating and interpreting the behaviour and efficiency
of iterative optimization heuristics in a more transparent and comprehensible
manner, allowing for better benchmarking and algorithm design.
\\ ( https://arxiv.org/abs/2401.17842 ,  17068kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17866 (*cross-listing*)
Date: Wed, 31 Jan 2024 14:25:05 GMT   (437kb)

Title: Making Sense of Knowledge Intensive Processes: an Oil & Gas Industry
  Scenario
Authors: Juliana Jansen Ferreira, Vin\'icius Segura, Ana Fucs, Rog\'erio de
  Paula
Categories: cs.HC cs.AI
Comments: 9 pages. This paper was presented at the Sensemaking in a Senseless
  World workshop during the 2018 ACM CHI Conference on Human Factors in
  Computing Systems
\\
  Sensemaking is a constant and ongoing process by which people associate
meaning to experiences. It can be an individual process, known as abduction, or
a group process by which people give meaning to collective experiences. The
sensemaking of a group is influenced by the abduction process of each person
about the experience. Every collaborative process needs some level of
sensemaking to show results. For a knowledge intensive process, sensemaking is
central and related to most of its tasks. We present findings from a fieldwork
executed in knowledge intensive process from the Oil and Gas industry. Our
findings indicated that different types of knowledge can be combined to compose
the result of a sensemaking process (e.g. decision, the need for more
discussion, etc.). This paper presents an initial set of knowledge types that
can be combined to compose the result of the sensemaking of a collaborative
decision making process. We also discuss ideas for using systems powered by
Artificial Intelligence to support sensemaking processes.
\\ ( https://arxiv.org/abs/2401.17866 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17895 (*cross-listing*)
Date: Wed, 31 Jan 2024 15:02:26 GMT   (42283kb,D)

Title: ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural
  Radiance Fields
Authors: Edward Bartrum and Thu Nguyen-Phuoc and Chris Xie and Zhengqin Li and
  Numair Khan and Armen Avetisyan and Douglas Lanman and Lei Xiao
Categories: cs.CV cs.AI cs.GR
Comments: For our project page, see https://replaceanything3d.github.io/
\\
  We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene
editing method that enables the replacement of specific objects within a scene.
Given multi-view images of a scene, a text prompt describing the object to
replace, and a text prompt describing the new object, our Erase-and-Replace
approach can effectively swap objects in the scene with newly generated content
while maintaining 3D consistency across multiple viewpoints. We demonstrate the
versatility of ReplaceAnything3D by applying it to various realistic 3D scenes,
showcasing results of modified foreground objects that are well-integrated with
the rest of the scene without affecting its overall integrity.
\\ ( https://arxiv.org/abs/2401.17895 ,  42283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17914 (*cross-listing*)
Date: Wed, 31 Jan 2024 15:24:13 GMT   (696kb,D)

Title: Attention Graph for Multi-Robot Social Navigation with Deep
  Reinforcement Learning
Authors: Erwan Escudie and Laetitia Matignon and Jacques Saraydaryan
Categories: cs.RO cs.AI
\\
  Learning robot navigation strategies among pedestrian is crucial for domain
based applications. Combining perception, planning and prediction allows us to
model the interactions between robots and pedestrians, resulting in impressive
outcomes especially with recent approaches based on deep reinforcement learning
(RL). However, these works do not consider multi-robot scenarios. In this
paper, we present MultiSoc, a new method for learning multi-agent socially
aware navigation strategies using RL. Inspired by recent works on multi-agent
deep RL, our method leverages graph-based representation of agent interactions,
combining the positions and fields of view of entities (pedestrians and
agents). Each agent uses a model based on two Graph Neural Network combined
with attention mechanisms. First an edge-selector produces a sparse graph, then
a crowd coordinator applies node attention to produce a graph representing the
influence of each entity on the others. This is incorporated into a model-free
RL framework to learn multi-agent policies. We evaluate our approach on
simulation and provide a series of experiments in a set of various conditions
(number of agents / pedestrians). Empirical results show that our method learns
faster than social navigation deep RL mono-agent techniques, and enables
efficient multi-agent implicit coordination in challenging crowd navigation
with multiple heterogeneous humans. Furthermore, by incorporating customizable
meta-parameters, we can adjust the neighborhood density to take into account in
our navigation strategy.
\\ ( https://arxiv.org/abs/2401.17914 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17972 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:27:47 GMT   (9867kb,D)

Title: MelNet: A Real-Time Deep Learning Algorithm for Object Detection
Authors: Yashar Azadvatan and Murat Kurt
Categories: cs.CV cs.AI cs.LG
Comments: 11 pages, 9 figures, 5 tables
\\
  In this study, a novel deep learning algorithm for object detection, named
MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset
for object detection. Following 300 training epochs, MelNet attained an mAP
(mean average precision) score of 0.732. Additionally, three alternative models
-YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI
dataset and juxtaposed with MelNet for object detection.
  The outcomes underscore the efficacy of employing transfer learning in
certain instances. Notably, preexisting models trained on prominent datasets
(e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding
underscores the viability of creating a new model tailored to a specific
scenario and training it on a specific dataset. This investigation demonstrates
that training MelNet exclusively on the KITTI dataset also surpasses
EfficientDet after 150 epochs. Consequently, post-training, MelNet's
performance closely aligns with that of other pre-trained models.
\\ ( https://arxiv.org/abs/2401.17972 ,  9867kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17976 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:33:12 GMT   (115kb,D)

Title: Circuit Partitioning for Multi-Core Quantum Architectures with Deep
  Reinforcement Learning
Authors: Arnau Pastor, Pau Escofet, Sahar Ben Rached, Eduard Alarc\'on, Pere
  Barlet-Ros, and Sergi Abadal
Categories: quant-ph cs.AI
\\
  Quantum computing holds immense potential for solving classically intractable
problems by leveraging the unique properties of quantum mechanics. The
scalability of quantum architectures remains a significant challenge.
Multi-core quantum architectures are proposed to solve the scalability problem,
arising a new set of challenges in hardware, communications and compilation,
among others. One of these challenges is to adapt a quantum algorithm to fit
within the different cores of the quantum computer. This paper presents a novel
approach for circuit partitioning using Deep Reinforcement Learning,
contributing to the advancement of both quantum computing and graph
partitioning. This work is the first step in integrating Deep Reinforcement
Learning techniques into Quantum Circuit Mapping, opening the door to a new
paradigm of solutions to such problems.
\\ ( https://arxiv.org/abs/2401.17976 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17981 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:38:32 GMT   (2381kb,D)

Title: Enhancing Multimodal Large Language Models with Vision Detection Models:
  An Empirical Study
Authors: Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen
Categories: cs.CV cs.AI
\\
  Despite the impressive capabilities of Multimodal Large Language Models
(MLLMs) in integrating text and image modalities, challenges remain in
accurately interpreting detailed visual elements. This paper presents an
empirical study on enhancing MLLMs with state-of-the-art (SOTA) object
detection and Optical Character Recognition models to improve fine-grained
image understanding and reduce hallucination in responses. Our research
investigates the embedding-based infusion of detection information, the impact
of such infusion on the MLLMs' original abilities, and the interchangeability
of detection models. We conduct systematic experiments with models such as
LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines
MLLMs' performance in specific visual tasks but also maintains their original
strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10
benchmarks, achieving an improvement of up to 12.99% on the normalized average
score, marking a notable advancement in multimodal understanding. We release
our codes to facilitate further exploration into the fine-grained multimodal
dialogue capabilities of MLLMs.
\\ ( https://arxiv.org/abs/2401.17981 ,  2381kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17985 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:44:20 GMT   (31719kb,D)

Title: Shrub of a thousand faces: an individual segmentation from satellite
  images using deep learning
Authors: Rohaifa Khaldi, Siham Tabik, Sergio Puertas-Ruiz, Julio Pe\~nas de
  Giles, Jos\'e Antonio H\'odar Correa, Regino Zamora, Domingo Alcaraz Segura
Categories: cs.CV cs.AI
Comments: 39 pages, 20 figures
\\
  Monitoring the distribution and size structure of long-living shrubs, such as
Juniperus communis, can be used to estimate the long-term effects of climate
change on high-mountain and high latitude ecosystems. Historical aerial
very-high resolution imagery offers a retrospective tool to monitor shrub
growth and distribution at high precision. Currently, deep learning models
provide impressive results for detecting and delineating the contour of objects
with defined shapes. However, adapting these models to detect natural objects
that express complex growth patterns, such as junipers, is still a challenging
task.
  This research presents a novel approach that leverages remotely sensed RGB
imagery in conjunction with Mask R-CNN-based instance segmentation models to
individually delineate Juniperus shrubs above the treeline in Sierra Nevada
(Spain). In this study, we propose a new data construction design that consists
in using photo interpreted (PI) and field work (FW) data to respectively
develop and externally validate the model. We also propose a new shrub-tailored
evaluation algorithm based on a new metric called Multiple Intersections over
Ground Truth Area (MIoGTA) to assess and optimize the model shrub delineation
performance. Finally, we deploy the developed model for the first time to
generate a wall-to-wall map of Juniperus individuals.
  The experimental results demonstrate the efficiency of our dual data
construction approach in overcoming the limitations associated with traditional
field survey methods. They also highlight the robustness of MIoGTA metric in
evaluating instance segmentation models on species with complex growth patterns
showing more resilience against data annotation uncertainty. Furthermore, they
show the effectiveness of employing Mask R-CNN with ResNet101-C4 backbone in
delineating PI and FW shrubs, achieving an F1-score of 87,87% and 76.86%,
respectively.
\\ ( https://arxiv.org/abs/2401.17985 ,  31719kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17902 (*cross-listing*)
Date: Wed, 31 Jan 2024 15:06:34 GMT   (115kb,D)

Title: Revisiting speech segmentation and lexicon learning with better features
Authors: Herman Kamper, Benjamin van Niekerk
Categories: eess.AS cs.CL cs.SD
Comments: 2 pages
\\
  We revisit a self-supervised method that segments unlabelled speech into
word-like segments. We start from the two-stage duration-penalised dynamic
programming method that performs zero-resource segmentation without learning an
explicit lexicon. In the first acoustic unit discovery stage, we replace
contrastive predictive coding features with HuBERT. After word segmentation in
the second stage, we get an acoustic word embedding for each segment by
averaging HuBERT features. These embeddings are clustered using K-means to get
a lexicon. The result is good full-coverage segmentation with a lexicon that
achieves state-of-the-art performance on the ZeroSpeech benchmarks.
\\ ( https://arxiv.org/abs/2401.17902 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17345 (*cross-listing*)
Date: Tue, 30 Jan 2024 15:44:14 GMT   (729kb)

Title: Reproducibility, energy efficiency and performance of pseudorandom
  number generators in machine learning: a comparative study of python, numpy,
  tensorflow, and pytorch implementations
Authors: Benjamin Antunes, David R.C Hill
Categories: cs.MS cs.LG
Comments: 20 pages, 10 tables, 1 figure
\\
  Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine
learning technologies because they are interesting for numerous methods. The
field of machine learning holds the potential for substantial advancements
across various domains, as exemplified by recent breakthroughs in Large
Language Models (LLMs). However, despite the growing interest, persistent
concerns include issues related to reproducibility and energy consumption.
Reproducibility is crucial for robust scientific inquiry and explainability,
while energy efficiency underscores the imperative to conserve finite global
resources. This study delves into the investigation of whether the leading
Pseudo-Random Number Generators (PRNGs) employed in machine learning languages,
libraries, and frameworks uphold statistical quality and numerical
reproducibility when compared to the original C implementation of the
respective PRNG algorithms. Additionally, we aim to evaluate the time
efficiency and energy consumption of various implementations. Our experiments
encompass Python, NumPy, TensorFlow, and PyTorch, utilizing the Mersenne
Twister, PCG, and Philox algorithms. Remarkably, we verified that the temporal
performance of machine learning technologies closely aligns with that of
C-based implementations, with instances of achieving even superior
performances. On the other hand, it is noteworthy that ML technologies consumed
only 10% more energy than their C-implementation counterparts. However, while
statistical quality was found to be comparable, achieving numerical
reproducibility across different platforms for identical seeds and algorithms
was not achieved.
\\ ( https://arxiv.org/abs/2401.17345 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17484 (*cross-listing*)
Date: Tue, 30 Jan 2024 22:37:24 GMT   (7802kb,D)

Title: Pixel to Elevation: Learning to Predict Elevation Maps at Long Range
  using Images for Autonomous Offroad Navigation
Authors: Chanyoung Chung, Georgios Georgakis, Patrick Spieler, Curtis Padgett,
  Shehryar Khattak
Categories: cs.RO cs.CV cs.LG
Comments: 8 pages, 6 figures, Under review
\\
  Understanding terrain topology at long-range is crucial for the success of
off-road robotic missions, especially when navigating at high-speeds. LiDAR
sensors, which are currently heavily relied upon for geometric mapping, provide
sparse measurements when mapping at greater distances. To address this
challenge, we present a novel learning-based approach capable of predicting
terrain elevation maps at long-range using only onboard egocentric images in
real-time. Our proposed method is comprised of three main elements. First, a
transformer-based encoder is introduced that learns cross-view associations
between the egocentric views and prior bird-eye-view elevation map predictions.
Second, an orientation-aware positional encoding is proposed to incorporate the
3D vehicle pose information over complex unstructured terrain with multi-view
visual image features. Lastly, a history-augmented learn-able map embedding is
proposed to achieve better temporal consistency between elevation map
predictions to facilitate the downstream navigational tasks. We experimentally
validate the applicability of our proposed approach for autonomous offroad
robotic navigation in complex and unstructured terrain using real-world offroad
driving data. Furthermore, the method is qualitatively and quantitatively
compared against the current state-of-the-art methods. Extensive field
experiments demonstrate that our method surpasses baseline models in accurately
predicting terrain elevation while effectively capturing the overall terrain
topology at long-ranges. Finally, ablation studies are conducted to highlight
and understand the effect of key components of the proposed approach and
validate their suitability to improve offroad robotic navigation capabilities.
\\ ( https://arxiv.org/abs/2401.17484 ,  7802kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17546 (*cross-listing*)
Date: Wed, 31 Jan 2024 02:20:21 GMT   (1805kb)

Title: Effective Multi-Stage Training Model For Edge Computing Devices In
  Intrusion Detection
Authors: Thua Huynh Trong, Thanh Nguyen Hoang
Categories: cs.CR cs.LG
DOI: 10.5121/ijcnc.2024.16102
\\
  Intrusion detection poses a significant challenge within expansive and
persistently interconnected environments. As malicious code continues to
advance and sophisticated attack methodologies proliferate, various advanced
deep learning-based detection approaches have been proposed. Nevertheless, the
complexity and accuracy of intrusion detection models still need further
enhancement to render them more adaptable to diverse system categories,
particularly within resource-constrained devices, such as those embedded in
edge computing systems. This research introduces a three-stage training
paradigm, augmented by an enhanced pruning methodology and model compression
techniques. The objective is to elevate the system's effectiveness,
concurrently maintaining a high level of accuracy for intrusion detection.
Empirical assessments conducted on the UNSW-NB15 dataset evince that this
solution notably reduces the model's dimensions, while upholding accuracy
levels equivalent to similar proposals.
\\ ( https://arxiv.org/abs/2401.17546 ,  1805kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17573 (*cross-listing*)
Date: Wed, 31 Jan 2024 03:35:08 GMT   (1434kb)

Title: Tensor-based process control and monitoring for semiconductor
  manufacturing with unstable disturbances
Authors: Yanrong Li, Juan Du, Fugee Tsung, Wei Jiang
Categories: stat.ML cs.LG cs.SY eess.IV eess.SY
Comments: 30 pages, 5 figures
\\
  With the development and popularity of sensors installed in manufacturing
systems, complex data are collected during manufacturing processes, which
brings challenges for traditional process control methods. This paper proposes
a novel process control and monitoring method for the complex structure of
high-dimensional image-based overlay errors (modeled in tensor form), which are
collected in semiconductor manufacturing processes. The proposed method aims to
reduce overlay errors using limited control recipes. We first build a
high-dimensional process model and propose different tensor-on-vector
regression algorithms to estimate parameters in the model to alleviate the
curse of dimensionality. Then, based on the estimate of tensor parameters, the
exponentially weighted moving average (EWMA) controller for tensor data is
designed whose stability is theoretically guaranteed. Considering the fact that
low-dimensional control recipes cannot compensate for all high-dimensional
disturbances on the image, control residuals are monitored to prevent
significant drifts of uncontrollable high-dimensional disturbances. Through
extensive simulations and real case studies, the performances of parameter
estimation algorithms and the EWMA controller in tensor space are evaluated.
Compared with existing image-based feedback controllers, the superiority of our
method is verified especially when disturbances are not stable.
\\ ( https://arxiv.org/abs/2401.17573 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17629 (*cross-listing*)
Date: Wed, 31 Jan 2024 07:11:01 GMT   (46102kb,D)

Title: Spatial-and-Frequency-aware Restoration method for Images based on
  Diffusion Models
Authors: Kyungsung Lee, Donggyu Lee, Myungjoo Kang
Categories: cs.CV cs.LG
\\
  Diffusion models have recently emerged as a promising framework for Image
Restoration (IR), owing to their ability to produce high-quality
reconstructions and their compatibility with established methods. Existing
methods for solving noisy inverse problems in IR, considers the pixel-wise
data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware
diffusion model for IR with Gaussian noise. Our model encourages images to
preserve data-fidelity in both the spatial and frequency domains, resulting in
enhanced reconstruction quality. We comprehensively evaluate the performance of
our model on a variety of noisy inverse problems, including inpainting,
denoising, and super-resolution. Our thorough evaluation demonstrates that
SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and
FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS
and FID metrics.
\\ ( https://arxiv.org/abs/2401.17629 ,  46102kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17675 (*cross-listing*)
Date: Wed, 31 Jan 2024 08:52:45 GMT   (32kb)

Title: Convergence analysis of t-SNE as a gradient flow for point cloud on a
  manifold
Authors: Seonghyeon Jeong, Hau-Tieng Wu
Categories: stat.ML cs.DS cs.LG
MSC-class: 90C26, 90C30
ACM-class: F.2.2; F.2.0; G.4
\\
  We present a theoretical foundation regarding the boundedness of the t-SNE
algorithm. t-SNE employs gradient descent iteration with Kullback-Leibler (KL)
divergence as the objective function, aiming to identify a set of points that
closely resemble the original data points in a high-dimensional space,
minimizing KL divergence. Investigating t-SNE properties such as perplexity and
affinity under a weak convergence assumption on the sampled dataset, we examine
the behavior of points generated by t-SNE under continuous gradient flow.
Demonstrating that points generated by t-SNE remain bounded, we leverage this
insight to establish the existence of a minimizer for KL divergence.
\\ ( https://arxiv.org/abs/2401.17675 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17737 (*cross-listing*)
Date: Wed, 31 Jan 2024 10:58:13 GMT   (590kb,D)

Title: Hierarchical Bias-Driven Stratification for Interpretable Causal Effect
  Estimation
Authors: Lucile Ter-Minassian, Liran Szlak, Ehud Karavani, Chris Holmes and
  Yishai Shimoni
Categories: stat.ME cs.LG stat.ML
\\
  Interpretability and transparency are essential for incorporating causal
effect models from observational data into policy decision-making. They can
provide trust for the model in the absence of ground truth labels to evaluate
the accuracy of such models. To date, attempts at transparent causal effect
estimation consist of applying post hoc explanation methods to black-box
models, which are not interpretable. Here, we present BICauseTree: an
interpretable balancing method that identifies clusters where natural
experiments occur locally. Our approach builds on decision trees with a
customized objective function to improve balancing and reduce treatment
allocation bias. Consequently, it can additionally detect subgroups presenting
positivity violations, exclude them, and provide a covariate-based definition
of the target population we can infer from and generalize to. We evaluate the
method's performance using synthetic and realistic datasets, explore its
bias-interpretability tradeoff, and show that it is comparable with existing
approaches.
\\ ( https://arxiv.org/abs/2401.17737 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17738 (*cross-listing*)
Date: Wed, 31 Jan 2024 10:58:59 GMT   (2076kb,D)

Title: Harnessing Smartwatch Microphone Sensors for Cough Detection and
  Classification
Authors: Pranay Jaiswal, Haroon R. Lone
Categories: cs.SD cs.HC cs.LG eess.AS
Comments: 7 pages
\\
  This study investigates the potential of using smartwatches with built-in
microphone sensors for monitoring coughs and detecting various cough types. We
conducted a study involving 32 participants and collected 9 hours of audio data
in a controlled manner. Afterward, we processed this data using a structured
approach, resulting in 223 positive cough samples. We further improved the
dataset through augmentation techniques and employed a specialized 1D CNN
model. This model achieved an impressive accuracy rate of 98.49% while
non-walking and 98.2% while walking, showing smartwatches can detect cough.
Moreover, our research successfully identified four distinct types of coughs
using clustering techniques.
\\ ( https://arxiv.org/abs/2401.17738 ,  2076kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17760 (*cross-listing*)
Date: Wed, 31 Jan 2024 11:37:14 GMT   (3053kb)

Title: Regularized Linear Discriminant Analysis Using a Nonlinear Covariance
  Matrix Estimator
Authors: Maaz Mahadi, Tarig Ballal, Muhammad Moinuddin, Tareq Y. Al-Naffouri,
  and Ubaid M. Al-Saggaf
Categories: stat.ML cs.LG eess.SP
\\
  Linear discriminant analysis (LDA) is a widely used technique for data
classification. The method offers adequate performance in many classification
problems, but it becomes inefficient when the data covariance matrix is
ill-conditioned. This often occurs when the feature space's dimensionality is
higher than or comparable to the training data size. Regularized LDA (RLDA)
methods based on regularized linear estimators of the data covariance matrix
have been proposed to cope with such a situation. The performance of RLDA
methods is well studied, with optimal regularization schemes already proposed.
In this paper, we investigate the capability of a positive semidefinite
ridge-type estimator of the inverse covariance matrix that coincides with a
nonlinear (NL) covariance matrix estimator. The estimator is derived by
reformulating the score function of the optimal classifier utilizing linear
estimation methods, which eventually results in the proposed NL-RLDA
classifier. We derive asymptotic and consistent estimators of the proposed
technique's misclassification rate under the assumptions of a double-asymptotic
regime and multivariate Gaussian model for the classes. The consistent
estimator, coupled with a one-dimensional grid search, is used to set the value
of the regularization parameter required for the proposed NL-RLDA classifier.
Performance evaluations based on both synthetic and real data demonstrate the
effectiveness of the proposed classifier. The proposed technique outperforms
state-of-art methods over multiple datasets. When compared to state-of-the-art
methods across various datasets, the proposed technique exhibits superior
performance.
\\ ( https://arxiv.org/abs/2401.17760 ,  3053kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17781 (*cross-listing*)
Date: Wed, 31 Jan 2024 12:23:55 GMT   (13422kb,D)

Title: Vision-Assisted Digital Twin Creation for mmWave Beam Management
Authors: Maximilian Arnold, Bence Major, Fabio Valerio Massoli, Joseph B.
  Soriaga, Arash Behboodi
Categories: eess.SP cs.LG
Comments: ICC2024 accepted paper. Copyright IEEE
\\
  In the context of communication networks, digital twin technology provides a
means to replicate the radio frequency (RF) propagation environment as well as
the system behaviour, allowing for a way to optimize the performance of a
deployed system based on simulations. One of the key challenges in the
application of Digital Twin technology to mmWave systems is the prevalent
channel simulators' stringent requirements on the accuracy of the 3D Digital
Twin, reducing the feasibility of the technology in real applications. We
propose a practical Digital Twin creation pipeline and a channel simulator,
that relies only on a single mounted camera and position information. We
demonstrate the performance benefits compared to methods that do not explicitly
model the 3D environment, on downstream sub-tasks in beam acquisition, using
the real-world dataset of the DeepSense6G challenge
\\ ( https://arxiv.org/abs/2401.17781 ,  13422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17789 (*cross-listing*)
Date: Wed, 31 Jan 2024 12:32:17 GMT   (3365kb,D)

Title: Robustly overfitting latents for flexible neural image compression
Authors: Yura Perugachi-Diaz, Arwin Gansekoele, Sandjai Bhulai
Categories: cs.CV cs.LG stat.ML
\\
  Neural image compression has made a great deal of progress. State-of-the-art
models are based on variational autoencoders and are outperforming classical
models. Neural compression models learn to encode an image into a quantized
latent representation that can be efficiently sent to the decoder, which
decodes the quantized latent into a reconstructed image. While these models
have proven successful in practice, they lead to sub-optimal results due to
imperfect optimization and limitations in the encoder and decoder capacity.
Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the
latents of pre-trained neural image compression models. We extend this idea by
introducing SGA+, which contains three different methods that build upon SGA.
Further, we give a detailed analysis of our proposed methods, show how they
improve performance, and show that they are less sensitive to hyperparameter
choices. Besides, we show how each method can be extended to three- instead of
two-class rounding. Finally, we show how refinement of the latents with our
best-performing method improves the compression performance on the Tecnick
dataset and how it can be deployed to partly move along the rate-distortion
curve.
\\ ( https://arxiv.org/abs/2401.17789 ,  3365kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17868 (*cross-listing*)
Date: Wed, 31 Jan 2024 14:27:07 GMT   (15145kb,D)

Title: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment
  Anything Model
Authors: Zihan Zhong, Zhiqiang Tang, Tong He, Haoyang Fang, Chun Yuan
Categories: cs.CV cs.LG
Comments: Accepted at ICLR 2024 Conference
\\
  The Segment Anything Model (SAM) stands as a foundational framework for image
segmentation. While it exhibits remarkable zero-shot generalization in typical
scenarios, its advantage diminishes when applied to specialized domains like
medical imagery and remote sensing. To address this limitation, this paper
introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning
approach. By integrating ultra-lightweight convolutional parameters into
Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases
into the plain ViT encoder, further reinforcing SAM's local prior assumption.
Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge
but also revives its capacity of learning high-level image semantics, which is
constrained by SAM's foreground-background segmentation pretraining.
Comprehensive experimentation across diverse benchmarks spanning multiple
domains underscores Conv-LoRA's superiority in adapting SAM to real-world
semantic segmentation tasks.
\\ ( https://arxiv.org/abs/2401.17868 ,  15145kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17880 (*cross-listing*)
Date: Wed, 31 Jan 2024 14:37:06 GMT   (3633kb,D)

Title: Graph Attention-based Reinforcement Learning for Trajectory Design and
  Resource Assignment in Multi-UAV Assisted Communication
Authors: Zikai Feng, Di Wu, Mengxing Huang, Chau Yuen
Categories: cs.MA cs.IT cs.LG math.IT
Comments: 13 pages
MSC-class: 68M11
ACM-class: I.2.11
\\
  In the multiple unmanned aerial vehicle (UAV)- assisted downlink
communication, it is challenging for UAV base stations (UAV BSs) to realize
trajectory design and resource assignment in unknown environments. The
cooperation and competition between UAV BSs in the communication network leads
to a Markov game problem. Multi-agent reinforcement learning is a significant
solution for the above decision-making. However, there are still many common
issues, such as the instability of the system and low utilization of historical
data, that limit its application. In this paper, a novel graph-attention
multi-agent trust region (GA-MATR) reinforcement learning framework is proposed
to solve the multi-UAV assisted communication problem. Graph recurrent network
is introduced to process and analyze complex topology of the communication
network, so as to extract useful information and patterns from observational
information. The attention mechanism provides additional weighting for conveyed
information, so that the critic network can accurately evaluate the value of
behavior for UAV BSs. This provides more reliable feedback signals and helps
the actor network update the strategy more effectively. Ablation simulations
indicate that the proposed approach attains improved convergence over the
baselines. UAV BSs learn the optimal communication strategies to achieve their
maximum cumulative rewards. Additionally, multi-agent trust region method with
monotonic convergence provides an estimated Nash equilibrium for the multi-UAV
assisted communication Markov game.
\\ ( https://arxiv.org/abs/2401.17880 ,  3633kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17958 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:07:44 GMT   (48kb)

Title: Convergence Analysis for General Probability Flow ODEs of Diffusion
  Models in Wasserstein Distances
Authors: Xuefeng Gao, Lingjiong Zhu
Categories: stat.ML cs.LG math.PR
Comments: 47 pages, 3 tables. arXiv admin note: text overlap with
  arXiv:2311.11003
\\
  Score-based generative modeling with probability flow ordinary differential
equations (ODEs) has achieved remarkable success in a variety of applications.
While various fast ODE-based samplers have been proposed in the literature and
employed in practice, the theoretical understandings about convergence
properties of the probability flow ODE are still quite limited. In this paper,
we provide the first non-asymptotic convergence analysis for a general class of
probability flow ODE samplers in 2-Wasserstein distance, assuming accurate
score estimates. We then consider various examples and establish results on the
iteration complexity of the corresponding ODE-based samplers.
\\ ( https://arxiv.org/abs/2401.17958 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17967 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:16:48 GMT   (771kb,D)

Title: CONCORD: Towards a DSL for Configurable Graph Code Representation
Authors: Mootez Saad and Tushar Sharma
Categories: cs.SE cs.LG
\\
  Deep learning is widely used to uncover hidden patterns in large code
corpora. To achieve this, constructing a format that captures the relevant
characteristics and features of source code is essential. Graph-based
representations have gained attention for their ability to model structural and
semantic information. However, existing tools lack flexibility in constructing
graphs across different programming languages, limiting their use.
Additionally, the output of these tools often lacks interoperability and
results in excessively large graphs, making graph-based neural networks
training slower and less scalable.
  We introduce CONCORD, a domain-specific language to build customizable graph
representations. It implements reduction heuristics to reduce graphs' size
complexity. We demonstrate its effectiveness in code smell detection as an
illustrative use case and show that: first, CONCORD can produce code
representations automatically per the specified configuration, and second, our
heuristics can achieve comparable performance with significantly reduced size.
CONCORD will help researchers a) create and experiment with customizable
graph-based code representations for different software engineering tasks
involving DL, b) reduce the engineering work to generate graph representations,
c) address the issue of scalability in GNN models, and d) enhance the
reproducibility of experiments in research through a standardized approach to
code representation and analysis.
\\ ( https://arxiv.org/abs/2401.17967 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17992 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:52:19 GMT   (6064kb,D)

Title: Multilinear Operator Networks
Authors: Yixin Cheng, Grigorios G. Chrysos, Markos Georgopoulos, Volkan Cevher
Categories: cs.CV cs.LG
Comments: International Conference on Learning Representations Poster(2024)
\\
  Despite the remarkable capabilities of deep neural networks in image
recognition, the dependence on activation functions remains a largely
unexplored area and has yet to be eliminated. On the other hand, Polynomial
Networks is a class of models that does not require activation functions, but
have yet to perform on par with modern architectures. In this work, we aim
close this gap and propose MONet, which relies solely on multilinear operators.
The core layer of MONet, called Mu-Layer, captures multiplicative interactions
of the elements of the input token. MONet captures high-degree interactions of
the input elements and we demonstrate the efficacy of our approach on a series
of image recognition and scientific computing benchmarks. The proposed model
outperforms prior polynomial networks and performs on par with modern
architectures. We believe that MONet can inspire further research on models
that use entirely multilinear operations.
\\ ( https://arxiv.org/abs/2401.17992 ,  6064kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18006 (*cross-listing*)
Date: Wed, 31 Jan 2024 17:08:34 GMT   (1936kb,D)

Title: EEG-GPT: Exploring Capabilities of Large Language Models for EEG
  Classification and Interpretation
Authors: Jonathan W. Kim and Ahmed Alaa and Danilo Bernardo
Categories: eess.SP cs.LG q-bio.QM
\\
  In conventional machine learning (ML) approaches applied to
electroencephalography (EEG), this is often a limited focus, isolating specific
brain activities occurring across disparate temporal scales (from transient
spikes in milliseconds to seizures lasting minutes) and spatial scales (from
localized high-frequency oscillations to global sleep activity). This siloed
approach limits the development EEG ML models that exhibit multi-scale
electrophysiological understanding and classification capabilities. Moreover,
typical ML EEG approaches utilize black-box approaches, limiting their
interpretability and trustworthiness in clinical contexts. Thus, we propose
EEG-GPT, a unifying approach to EEG classification that leverages advances in
large language models (LLM). EEG-GPT achieves excellent performance comparable
to current state-of-the-art deep learning methods in classifying normal from
abnormal EEG in a few-shot learning paradigm utilizing only 2% of training
data. Furthermore, it offers the distinct advantages of providing intermediate
reasoning steps and coordinating specialist EEG tools across multiple scales in
its operation, offering transparent and interpretable step-by-step
verification, thereby promoting trustworthiness in clinical contexts.
\\ ( https://arxiv.org/abs/2401.18006 ,  1936kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18012 (*cross-listing*)
Date: Wed, 31 Jan 2024 17:20:28 GMT   (609kb,D)

Title: Causal Coordinated Concurrent Reinforcement Learning
Authors: Tim Tse, Isaac Chan, Zhitang Chen
Categories: stat.ML cs.LG
\\
  In this work, we propose a novel algorithmic framework for data sharing and
coordinated exploration for the purpose of learning more data-efficient and
better performing policies under a concurrent reinforcement learning (CRL)
setting. In contrast to other work which make the assumption that all agents
act under identical environments, we relax this restriction and instead
consider the formulation where each agent acts within an environment which
shares a global structure but also exhibits individual variations. Our
algorithm leverages a causal inference algorithm in the form of Additive Noise
Model - Mixture Model (ANM-MM) in extracting model parameters governing
individual differentials via independence enforcement. We propose a new data
sharing scheme based on a similarity measure of the extracted model parameters
and demonstrate superior learning speeds on a set of autoregressive, pendulum
and cart-pole swing-up tasks and finally, we show the effectiveness of diverse
action selection between common agents under a sparse reward setting. To the
best of our knowledge, this is the first work in considering non-identical
environments in CRL and one of the few works which seek to integrate causal
inference with reinforcement learning (RL).
\\ ( https://arxiv.org/abs/2401.18012 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18017 (*cross-listing*)
Date: Wed, 31 Jan 2024 17:28:05 GMT   (46kb)

Title: Causal Discovery by Kernel Deviance Measures with Heterogeneous
  Transforms
Authors: Tim Tse, Zhitang Chen, Shengyu Zhu, Yue Liu
Categories: stat.ML cs.LG
\\
  The discovery of causal relationships in a set of random variables is a
fundamental objective of science and has also recently been argued as being an
essential component towards real machine intelligence. One class of causal
discovery techniques are founded based on the argument that there are inherent
structural asymmetries between the causal and anti-causal direction which could
be leveraged in determining the direction of causation. To go about capturing
these discrepancies between cause and effect remains to be a challenge and many
current state-of-the-art algorithms propose to compare the norms of the kernel
mean embeddings of the conditional distributions. In this work, we argue that
such approaches based on RKHS embeddings are insufficient in capturing
principal markers of cause-effect asymmetry involving higher-order structural
variabilities of the conditional distributions. We propose Kernel Intrinsic
Invariance Measure with Heterogeneous Transform (KIIM-HT) which introduces a
novel score measure based on heterogeneous transformation of RKHS embeddings to
extract relevant higher-order moments of the conditional densities for causal
discovery. Inference is made via comparing the score of each hypothetical
cause-effect direction. Tests and comparisons on a synthetic dataset, a
two-dimensional synthetic dataset and the real-world benchmark dataset
T\"ubingen Cause-Effect Pairs verify our approach. In addition, we conduct a
sensitivity analysis to the regularization parameter to faithfully compare
previous work to our method and an experiment with trials on varied
hyperparameter values to showcase the robustness of our algorithm.
\\ ( https://arxiv.org/abs/2401.18017 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18039 (*cross-listing*)
Date: Wed, 31 Jan 2024 18:01:36 GMT   (2329kb)

Title: Variable selection for Na\"ive Bayes classification
Authors: Rafael Blanquero, Emilio Carrizosa, Pepa Ram\'irez-Cobo, M. Remedios
  Sillero-Denamiel
Categories: stat.ML cs.LG
Journal-ref: Computers & Operations Research, Volume 135, 2021, 105456,
DOI: 10.1016/j.cor.2021.105456
\\
  The Na\"ive Bayes has proven to be a tractable and efficient method for
classification in multivariate analysis. However, features are usually
correlated, a fact that violates the Na\"ive Bayes' assumption of conditional
independence, and may deteriorate the method's performance. Moreover, datasets
are often characterized by a large number of features, which may complicate the
interpretation of the results as well as slow down the method's execution.
  In this paper we propose a sparse version of the Na\"ive Bayes classifier
that is characterized by three properties. First, the sparsity is achieved
taking into account the correlation structure of the covariates. Second,
different performance measures can be used to guide the selection of features.
Third, performance constraints on groups of higher interest can be included.
Our proposal leads to a smart search, which yields competitive running times,
whereas the flexibility in terms of performance measure for classification is
integrated. Our findings show that, when compared against well-referenced
feature selection approaches, the proposed sparse Na\"ive Bayes obtains
competitive results regarding accuracy, sparsity and running times for balanced
datasets. In the case of datasets with unbalanced (or with different
importance) classes, a better compromise between classification rates for the
different classes is achieved.
\\ ( https://arxiv.org/abs/2401.18039 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18054 (*cross-listing*)
Date: Wed, 31 Jan 2024 18:20:42 GMT   (4238kb,D)

Title: Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based
  Action Recognition
Authors: Wei Wei, Tom De Schepper, Kevin Mets
Categories: cs.CV cs.LG
Comments: This work is accepted at VISAPP 2024 as a short paper
\\
  Continual learning (CL) is the research field that aims to build machine
learning models that can accumulate knowledge continuously over different tasks
without retraining from scratch. Previous studies have shown that pre-training
graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)
after fine-tuning, a setting which is closely related to CL. Thus, we focus on
studying GNN in the continual graph learning (CGL) setting. We propose the
first continual graph learning benchmark for spatio-temporal graphs and use it
to benchmark well-known CGL methods in this novel setting. The benchmark is
based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action
recognition. Beyond benchmarking for standard performance metrics, we study the
class and task-order sensitivity of CGL methods, i.e., the impact of learning
order on each class/task's performance, and the architectural sensitivity of
CGL methods with backbone GNN at various widths and depths. We reveal that
task-order robust methods can still be class-order sensitive and observe
results that contradict previous empirical observations on architectural
sensitivity in CL.
\\ ( https://arxiv.org/abs/2401.18054 ,  4238kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2209.08316
replaced with revised version Wed, 31 Jan 2024 15:49:34 GMT   (1161kb,D)

Title: An Empathetic AI Coach for Self-Attachment Therapy
Authors: Lisa Alazraki, Ali Ghachem, Neophytos Polydorou, Foaad Khosmood and
  Abbas Edalat
Categories: cs.AI cs.CL cs.LG
Journal-ref: 2021 IEEE Third International Conference on Cognitive Machine
  Intelligence (CogMI), 2021, pp. 78-87
DOI: 10.1109/CogMI52975.2021.00019
\\ ( https://arxiv.org/abs/2209.08316 ,  1161kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08775
replaced with revised version Wed, 31 Jan 2024 04:11:42 GMT   (4968kb,D)

Title: GEAR: Augmenting Language Models with Generalizable and Efficient Tool
  Resolution
Authors: Yining Lu and Haoping Yu and Daniel Khashabi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2307.08775 ,  4968kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15176
replaced with revised version Wed, 31 Jan 2024 12:40:40 GMT   (1642kb,D)

Title: RCT Rejection Sampling for Causal Estimation Evaluation
Authors: Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg,
  Rohit Bhattacharya
Categories: cs.AI cs.CL cs.LG stat.ME
Comments: Code and data at https://github.com/kakeith/rct_rejection_sampling
Journal-ref: Transactions on Machine Learning Research (TMLR) 2023
\\ ( https://arxiv.org/abs/2307.15176 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11234
replaced with revised version Wed, 31 Jan 2024 12:59:20 GMT   (2721kb,D)

Title: Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding
Authors: Zhe Chen, Daniel Harabor, Jiaoyang Li, Peter J. Stuckey
Categories: cs.AI cs.MA cs.RO
Comments: The paper was accepted for publication at AAAI 2024
\\ ( https://arxiv.org/abs/2308.11234 ,  2721kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13192
replaced with revised version Wed, 31 Jan 2024 01:16:00 GMT   (4760kb)

Title: Generative Design of Crystal Structures by Point Cloud Representations
  and Diffusion Model
Authors: Zhelin Li, Rami Mrad, Runxian Jiao, Guan Huang, Jun Shan, Shibing Chu
  and Yuanping Chen
Categories: cs.AI cond-mat.mtrl-sci cs.LG physics.comp-ph
Comments: I have submitted to a journal
\\ ( https://arxiv.org/abs/2401.13192 ,  4760kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15443
replaced with revised version Wed, 31 Jan 2024 02:50:41 GMT   (1096kb,D)

Title: DiffuserLite: Towards Real-time Diffusion Planning
Authors: Zibin Dong, Jianye Hao, Yifu Yuan, Fei Ni, Yitian Wang, Pengyi Li and
  Yan Zheng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.15443 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10558
replaced with revised version Wed, 31 Jan 2024 13:14:02 GMT   (508kb,D)

Title: On-the-fly Denoising for Data Augmentation in Natural Language
  Understanding
Authors: Tianqing Fang, Wenxuan Zhou, Fangyu Liu, Hongming Zhang, Yangqiu Song,
  Muhao Chen
Categories: cs.CL cs.AI
Comments: Findings of EACL 2024
\\ ( https://arxiv.org/abs/2212.10558 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10767
replaced with revised version Wed, 31 Jan 2024 04:10:30 GMT   (41kb,D)

Title: How Does Beam Search improve Span-Level Confidence Estimation in
  Generative Sequence Labeling?
Authors: Kazuma Hashimoto and Iftekhar Naim and Karthik Raman
Categories: cs.CL
Comments: UncertaiNLP 2024 (an EACL 2024 workshop:
  https://uncertainlp.github.io/)
\\ ( https://arxiv.org/abs/2212.10767 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14205
replaced with revised version Wed, 31 Jan 2024 13:28:58 GMT   (8368kb,D)

Title: $\mu$PLAN: Summarizing using a Content Plan as Cross-Lingual Bridge
Authors: Fantine Huot, Joshua Maynez, Chris Alberti, Reinald Kim Amplayo,
  Priyanka Agrawal, Constanza Fierro, Shashi Narayan, Mirella Lapata
Categories: cs.CL
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2305.14205 ,  8368kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14341
replaced with revised version Wed, 31 Jan 2024 02:32:19 GMT   (5960kb,D)

Title: APPLS: Evaluating Evaluation Metrics for Plain Language Summarization
Authors: Yue Guo, Tal August, Gondy Leroy, Trevor Cohen, Lucy Lu Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.14341 ,  5960kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15002
replaced with revised version Wed, 31 Jan 2024 03:56:22 GMT   (7690kb,D)

Title: A RelEntLess Benchmark for Modelling Graded Relations between Named
  Entities
Authors: Asahi Ushio and Jose Camacho Collados and Steven Schockaert
Categories: cs.CL cs.LG
Comments: EACL 2024 main conference
\\ ( https://arxiv.org/abs/2305.15002 ,  7690kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00162
replaced with revised version Wed, 31 Jan 2024 05:00:25 GMT   (13661kb,D)

Title: What Do Self-Supervised Speech Models Know About Words?
Authors: Ankita Pasad, Chung-Ming Chien, Shane Settle, Karen Livescu
Categories: cs.CL cs.LG eess.AS
Comments: Pre-MIT Press publication version
\\ ( https://arxiv.org/abs/2307.00162 ,  13661kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08648
replaced with revised version Wed, 31 Jan 2024 02:36:48 GMT   (5633kb,D)

Title: MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings
Authors: Yonchanok Khaokaew, Hao Xue, Flora D. Salim
Categories: cs.CL cs.AI
DOI: 10.1145/3643514
\\ ( https://arxiv.org/abs/2309.08648 ,  5633kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08929
replaced with revised version Wed, 31 Jan 2024 14:25:15 GMT   (7366kb,D)

Title: Leveraging Multi-lingual Positive Instances in Contrastive Learning to
  Improve Sentence Embedding
Authors: Kaiyan Zhao, Qiyu Wu, Xin-Qiang Cai, Yoshimasa Tsuruoka
Categories: cs.CL
Comments: Accepted to EACL 2024, main conference
\\ ( https://arxiv.org/abs/2309.08929 ,  7366kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08958
replaced with revised version Wed, 31 Jan 2024 03:42:04 GMT   (39kb,D)

Title: Monolingual or Multilingual Instruction Tuning: Which Makes a Better
  Alpaca
Authors: Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov, Barry
  Haddow, Kenneth Heafield
Categories: cs.CL cs.AI
Comments: Accepted to Findings of ACL: EACL 2024. Added human evaluation and
  shortened writing
\\ ( https://arxiv.org/abs/2309.08958 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09048
replaced with revised version Tue, 30 Jan 2024 20:21:00 GMT   (14458kb,D)

Title: GRASP: A novel benchmark for evaluating language GRounding And Situated
  Physics understanding in multimodal language models
Authors: Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia
  Ohmer, Elia Bruni
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09048 ,  14458kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15623
replaced with revised version Wed, 31 Jan 2024 06:58:51 GMT   (2808kb,D)

Title: Injecting linguistic knowledge into BERT for Dialogue State Tracking
Authors: Xiaohan Feng, Xixin Wu, Helen Meng
Categories: cs.CL cs.AI cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2311.15623 ,  2808kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00949
replaced with revised version Tue, 30 Jan 2024 21:32:31 GMT   (4105kb,D)

Title: Hyperparameter Optimization for Large Language Model Instruction-Tuning
Authors: Christophe Tribes, Sacha Benarroch-Lelong, Peng Lu, Ivan Kobyzev
Categories: cs.CL math.OC
\\ ( https://arxiv.org/abs/2312.00949 ,  4105kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03863
replaced with revised version Wed, 31 Jan 2024 11:29:40 GMT   (816kb,D)

Title: Efficient Large Language Models: A Survey
Authors: Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu,
  Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang
Categories: cs.CL cs.AI
Comments: Version 3: Added more latest papers
\\ ( https://arxiv.org/abs/2312.03863 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07518
replaced with revised version Wed, 31 Jan 2024 06:20:32 GMT   (2768kb,D)

Title: Survey of Natural Language Processing for Education: Taxonomy,
  Systematic Review, and Future Trends
Authors: Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian,
  Aoying Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.07518 ,  2768kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08694
replaced with revised version Tue, 30 Jan 2024 21:59:08 GMT   (327kb,D)

Title: Combining Confidence Elicitation and Sample-based Methods for
  Uncertainty Quantification in Misinformation Mitigation
Authors: Mauricio Rivera, Jean-Fran\c{c}ois Godbout, Reihaneh Rabbany, Kellin
  Pelrine
Categories: cs.CL cs.AI
Comments: 12 pages, 11 figures
\\ ( https://arxiv.org/abs/2401.08694 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11864
replaced with revised version Wed, 31 Jan 2024 03:50:07 GMT   (8638kb,D)

Title: Improving Small Language Models' Mathematical Reasoning via
  Equation-of-Thought Distillation
Authors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.11864 ,  8638kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12292
replaced with revised version Wed, 31 Jan 2024 06:44:42 GMT   (14979kb,D)

Title: GRATH: Gradual Self-Truthifying for Large Language Models
Authors: Weixin Chen, Dawn Song, Bo Li
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.12292 ,  14979kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14440
replaced with revised version Wed, 31 Jan 2024 10:52:52 GMT   (7082kb,D)

Title: Semantic Sensitivities and Inconsistent Predictions: Measuring the
  Fragility of NLI Models
Authors: Erik Arakelyan, Zhaoqi Liu, Isabelle Augenstein
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2401.14440 ,  7082kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15378
replaced with revised version Wed, 31 Jan 2024 12:39:06 GMT   (875kb)

Title: A RAG-based Question Answering System Proposal for Understanding Islam:
  MufassirQAS LLM
Authors: Ahmet Yusuf Alan, Enis Karaarslan, \"Omer Aydin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.15378 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15496
replaced with revised version Wed, 31 Jan 2024 17:36:29 GMT   (2176kb,D)

Title: Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue
  Summarization
Authors: Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, Yiyong Xiao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.15496 ,  2176kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16092
replaced with revised version Wed, 31 Jan 2024 08:33:37 GMT   (19141kb,D)

Title: Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and
  Prompt Engineering May Not Help You
Authors: Felix Friedrich, Katharina H\"ammerl, Patrick Schramowski, Jindrich
  Libovicky, Kristian Kersting, Alexander Fraser
Categories: cs.CL cs.CY cs.LG
\\ ( https://arxiv.org/abs/2401.16092 ,  19141kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16403
replaced with revised version Wed, 31 Jan 2024 07:59:16 GMT   (7914kb,D)

Title: ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media
  Text
Authors: Thanh-Nhi Nguyen, Thanh-Phong Le, Kiet Van Nguyen
Categories: cs.CL
Comments: Accepted at the EACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2401.16403 ,  7914kb)
------------------------------------------------------------------------------
\\
arXiv:2105.11309
replaced with revised version Wed, 31 Jan 2024 14:39:49 GMT   (1704kb)

Title: Efficiently Solving High-Order and Nonlinear ODEs with Rational Fraction
  Polynomial: the Ratio Net
Authors: Chenxin Qin, Ruhao Liu, Maocai Li, Shengyuan Li, Yi Liu, and Chichun
  Zhou
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2105.11309 ,  1704kb)
------------------------------------------------------------------------------
\\
arXiv:2201.09196
replaced with revised version Wed, 31 Jan 2024 05:30:08 GMT   (44172kb,D)

Title: Learning to Predict Gradients for Semi-Supervised Continual Learning
Authors: Yan Luo, Yongkang Wong, Mohan Kankanhalli, Qi Zhao
Categories: cs.LG cs.CV
Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
Journal-ref: IEEE Transactions on Neural Networks and Learning Systems, 2024
\\ ( https://arxiv.org/abs/2201.09196 ,  44172kb)
------------------------------------------------------------------------------
\\
arXiv:2205.09622
replaced with revised version Wed, 31 Jan 2024 09:51:15 GMT   (496kb,D)

Title: What Is Fairness? On the Role of Protected Attributes and Fictitious
  Worlds
Authors: Ludwig Bothmann, Kristina Peters, Bernd Bischl
Categories: cs.LG cs.AI cs.CY
\\ ( https://arxiv.org/abs/2205.09622 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2205.15523
replaced with revised version Wed, 31 Jan 2024 05:30:22 GMT   (15355kb,D)

Title: Variational Transfer Learning using Cross-Domain Latent Modulation
Authors: Jinyong Hou, Jeremiah D. Deng, Stephen Cranefield, Xuejie Din
Categories: cs.LG cs.AI cs.CV
Comments: Under review. Extended version of a previous WACV paper
  (arXiv:2012.11727). 13 pages, 8 figures
\\ ( https://arxiv.org/abs/2205.15523 ,  15355kb)
------------------------------------------------------------------------------
\\
arXiv:2206.05239
replaced with revised version Tue, 30 Jan 2024 22:21:04 GMT   (894kb,D)

Title: StructCoder: Structure-Aware Transformer for Code Generation
Authors: Sindhu Tipirneni, Ming Zhu, Chandan K. Reddy
Categories: cs.LG cs.SE
\\ ( https://arxiv.org/abs/2206.05239 ,  894kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06225
replaced with revised version Wed, 31 Jan 2024 15:50:17 GMT   (116kb,D)

Title: On the Generalizability of ECG-based Stress Detection Models
Authors: Pooja Prajod, Elisabeth Andr\'e
Categories: cs.LG cs.AI cs.CV
Comments: Published in Proceedings of 2022 21st IEEE International Conference
  on Machine Learning and Applications (ICMLA)
DOI: 10.1109/ICMLA55696.2022.00090
\\ ( https://arxiv.org/abs/2210.06225 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11673
replaced with revised version Wed, 31 Jan 2024 03:02:06 GMT   (2480kb,D)

Title: Bayesian Self-Supervised Contrastive Learning
Authors: Bin Liu, Bang Wang, Tianrui Li
Categories: cs.LG
Comments: 24 pages
\\ ( https://arxiv.org/abs/2301.11673 ,  2480kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13530
replaced with revised version Wed, 31 Jan 2024 17:29:26 GMT   (8201kb,D)

Title: Domain-Generalizable Multiple-Domain Clustering
Authors: Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum
Categories: cs.LG cs.CV
Comments: 13 pages, 3 figures
\\ ( https://arxiv.org/abs/2301.13530 ,  8201kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12095
replaced with revised version Wed, 31 Jan 2024 13:05:35 GMT   (33604kb,D)

Title: CARD: Channel Aligned Robust Blend Transformer for Time Series
  Forecasting
Authors: Wang Xue, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, Rong Jin
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.12095 ,  33604kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09882
replaced with revised version Wed, 31 Jan 2024 01:46:58 GMT   (25318kb,D)

Title: Uncertainty Quantification via Spatial-Temporal Tweedie Model for
  Zero-inflated and Long-tail Travel Demand Prediction
Authors: Xinke Jiang, Dingyi Zhuang, Xianghui Zhang, Hao Chen, Jiayuan Luo,
  Xiaowei Gao
Categories: cs.LG stat.ML stat.OT
Comments: In proceeding of CIKM 2023. Doi:
  https://dl.acm.org/doi/10.1145/3583780.3615215
DOI: 10.1145/3583780.3615215
\\ ( https://arxiv.org/abs/2306.09882 ,  25318kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06555
replaced with revised version Wed, 31 Jan 2024 17:57:17 GMT   (2826kb,D)

Title: Deep Network Approximation: Beyond ReLU to Diverse Activation Functions
Authors: Shijun Zhang, Jianfeng Lu, Hongkai Zhao
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2307.06555 ,  2826kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04836
replaced with revised version Wed, 31 Jan 2024 03:29:10 GMT   (3623kb,D)

Title: Beyond Surprise: Improving Exploration Through Surprise Novelty
Authors: Hung Le, Kien Do, Dung Nguyen, Svetha Venkatesh
Categories: cs.LG
Comments: 17 pages including Appendix
\\ ( https://arxiv.org/abs/2308.04836 ,  3623kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12612
replaced with revised version Wed, 31 Jan 2024 15:52:01 GMT   (513kb,D)

Title: Try with Simpler -- An Evaluation of Improved Principal Component
  Analysis in Log-based Anomaly Detection
Authors: Lin Yang, Junjie Chen, Shutao Gao, Zhihao Gong, Hongyu Zhang, Yue
  Kang, Huaan Li
Categories: cs.LG cs.SE
Comments: Accepted by TOSEM
\\ ( https://arxiv.org/abs/2308.12612 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13352
replaced with revised version Wed, 31 Jan 2024 14:53:18 GMT   (1403kb,D)

Title: A Generic Machine Learning Framework for Fully-Unsupervised Anomaly
  Detection with Contaminated Data
Authors: Markus Ulmer, Jannik Zgraggen, and Lilach Goren Huber
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.13352 ,  1403kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12862
replaced with revised version Wed, 31 Jan 2024 01:05:14 GMT   (14170kb,D)

Title: Associative Transformer
Authors: Yuwei Sun, Hideya Ochiai, Zhirong Wu, Stephen Lin, Ryota Kanai
Categories: cs.LG cs.CV cs.NE
\\ ( https://arxiv.org/abs/2309.12862 ,  14170kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15375
replaced with revised version Tue, 30 Jan 2024 22:13:06 GMT   (3950kb,D)

Title: PPG-to-ECG Signal Translation for Continuous Atrial Fibrillation
  Detection via Attention-based Deep State-Space Modeling
Authors: Khuong Vo, Mostafa El-Khamy, Yoojin Choi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.15375 ,  3950kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13139
replaced with revised version Wed, 31 Jan 2024 02:00:13 GMT   (29kb)

Title: Graph Neural Networks with polynomial activations have limited
  expressivity
Authors: Sammy Khalife
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.13139 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14087
replaced with revised version Tue, 30 Jan 2024 20:23:03 GMT   (7774kb,D)

Title: A Specialized Semismooth Newton Method for Kernel-Based Optimal
  Transport
Authors: Tianyi Lin, Marco Cuturi and Michael I. Jordan
Categories: cs.LG math.OC
Comments: Accepted by AISTATS 2024; Fix some inaccuracy in the definition and
  proof; 24 pages, 36 figures
\\ ( https://arxiv.org/abs/2310.14087 ,  7774kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16592
replaced with revised version Wed, 31 Jan 2024 02:49:52 GMT   (1059kb,D)

Title: Over-the-air Federated Policy Gradient
Authors: Huiwen Yang, Lingying Huang, Subhrakanti Dey, Ling Shi
Categories: cs.LG cs.DC eess.SP
Comments: To appear at IEEE ICC 2024
\\ ( https://arxiv.org/abs/2310.16592 ,  1059kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20703
replaced with revised version Wed, 31 Jan 2024 12:39:06 GMT   (16827kb,D)

Title: Vanishing Gradients in Reinforcement Finetuning of Language Models
Authors: Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley,
  Preetum Nakkiran, Joshua Susskind, Etai Littwin
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.20703 ,  16827kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06643
replaced with revised version Wed, 31 Jan 2024 18:06:16 GMT   (4018kb,D)

Title: Privacy Risks Analysis and Mitigation in Federated Learning for Medical
  Images
Authors: Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu
Categories: cs.LG
Comments: V1
\\ ( https://arxiv.org/abs/2311.06643 ,  4018kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06353
replaced with revised version Wed, 31 Jan 2024 11:49:06 GMT   (1823kb,D)

Title: Federated Full-Parameter Tuning of Billion-Sized Language Models with
  Communication Cost under 18 Kilobytes
Authors: Zhen Qin, Daoyuan Chen, Bingchen Qian, Bolin Ding, Yaliang Li,
  Shuiguang Deng
Categories: cs.LG cs.DC
Comments: Codes are available at
  https://github.com/alibaba/FederatedScope/tree/FedKSeed. We will continuously
  update the codebase and arXiv version
\\ ( https://arxiv.org/abs/2312.06353 ,  1823kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12784
replaced with revised version Wed, 31 Jan 2024 08:07:23 GMT   (7096kb,D)

Title: Fast Cell Library Characterization for Design Technology Co-Optimization
  Based on Graph Neural Networks
Authors: Tianliang Ma, Zhihui Deng, Xuguang Sun, Leilai Shao, Kainlu Low
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.12784 ,  7096kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11143
replaced with revised version Wed, 31 Jan 2024 01:22:43 GMT   (4157kb,D)

Title: Gaussian Adaptive Attention is All You Need: Robust Contextual
  Representations Across Multiple Modalities
Authors: Georgios Ioannides, Aman Chadha, Aaron Elkins
Categories: cs.LG cs.AI cs.CL cs.CV cs.SD eess.AS eess.SP
\\ ( https://arxiv.org/abs/2401.11143 ,  4157kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15292
replaced with revised version Wed, 31 Jan 2024 01:52:46 GMT   (687kb,D)

Title: Adaptive Block Sparse Regularization under Arbitrary Linear Transform
Authors: Takanobu Furuhashi, Hidekata Hontani, Tatsuya Yokota
Categories: cs.LG eess.SP
Comments: 5 pages, 3 figures
\\ ( https://arxiv.org/abs/2401.15292 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15469
replaced with revised version Wed, 31 Jan 2024 10:17:28 GMT   (13019kb,D)

Title: Wind speed super-resolution and validation: from ERA5 to CERRA via
  diffusion models
Authors: Fabio Merizzi, Andrea Asperti, Stefano Colamonaco
Categories: cs.LG cs.AI physics.ao-ph
\\ ( https://arxiv.org/abs/2401.15469 ,  13019kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16729
replaced with revised version Wed, 31 Jan 2024 05:11:54 GMT   (720kb,D)

Title: Widely Linear Matched Filter: A Lynchpin towards the Interpretability of
  Complex-valued CNNs
Authors: Qingchen Wang, Zhe Li, Zdenka Babic, Wei Deng, Ljubi\v{s}a
  Stankovi\'c, Danilo P. Mandic
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.16729 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2104.13130
replaced with revised version Wed, 31 Jan 2024 05:46:26 GMT   (9076kb)

Title: Secure and Efficient Federated Learning Through Layering and Sharding
  Blockchain
Authors: Shuo Yuan, Bin Cao, Yao Sun, Zhiguo Wan, Mugen Peng
Categories: cs.CR cs.AI cs.IT math.IT
Comments: Accepted by IEEE Transactions on Network Science and Engineering
DOI: 10.1109/TNSE.2024.3361458
\\ ( https://arxiv.org/abs/2104.13130 ,  9076kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12445
replaced with revised version Tue, 30 Jan 2024 23:09:40 GMT   (1744kb,D)

Title: SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic
  Spaces
Authors: Ivan Vall\'es-P\'erez, Grzegorz Beringer, Piotr Bilinski, Gary Cook,
  Roberto Barra-Chicote
Categories: cs.SD cs.AI eess.AS
Comments: In proceedings of the 26th European Conference on Artificial
  Intelligence ECAI 2023. 8 pages + 1 appendix page
\\ ( https://arxiv.org/abs/2307.12445 ,  1744kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02126
replaced with revised version Wed, 31 Jan 2024 10:36:02 GMT   (2698kb,D)

Title: Cognitive TransFuser: Semantics-guided Transformer-based Sensor Fusion
  for Improved Waypoint Prediction
Authors: Hwan-Soo Choi, Jongoh Jeong, Young Hoo Cho, Kuk-Jin Yoon, and
  Jong-Hwan Kim
Categories: cs.RO cs.AI
Comments: Accepted to RiTA 2023
\\ ( https://arxiv.org/abs/2308.02126 ,  2698kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11199
replaced with revised version Wed, 31 Jan 2024 14:11:56 GMT   (785kb,D)

Title: ConcatPlexer: Additional Dim1 Batching for Faster ViTs
Authors: Donghoon Han, Seunghyeon Seo, Donghyeon Jeon, Jiho Jang, Chaerin Kong
  and Nojun Kwak
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2308.11199 ,  785kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02169
replaced with revised version Wed, 31 Jan 2024 06:18:14 GMT   (0kb,I)

Title: Dual Relation Alignment for Composed Image Retrieval
Authors: Xintong Jiang, Yaxiong Wang, Yujiao Wu, Meng Wang, Xueming Qian
Categories: cs.CV cs.AI
Comments: The architecture of our model changes, hence methodolgy and
  experiments changes a lot, We have significantly revised the original
  manuscript of the paper, so a withdraw of our original script is needed
\\ ( https://arxiv.org/abs/2309.02169 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05930
replaced with revised version Wed, 31 Jan 2024 16:11:27 GMT   (2160kb,D)

Title: Combining Deep Learning and Street View Imagery to Map Smallholder Crop
  Types
Authors: Jordi Laguarta Soler, Thomas Friedel, Sherrie Wang
Categories: cs.CV cs.AI
Comments: Accepted to AAAI-24: Special Track on AI for Social Impact
\\ ( https://arxiv.org/abs/2309.05930 ,  2160kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04645 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 09:54:43 GMT   (5096kb,D)

Title: Do self-supervised speech and language models extract similar
  representations as human brain?
Authors: Peili Chen, Linyang He, Li Fu, Lu Fan, Edward F. Chang, Yuanning Li
Categories: q-bio.NC cs.AI cs.CL eess.AS
Comments: To appear in 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing
\\ ( https://arxiv.org/abs/2310.04645 ,  5096kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08873
replaced with revised version Wed, 31 Jan 2024 03:25:37 GMT   (55047kb,D)

Title: Interactive Navigation in Environments with Traversable Obstacles Using
  Large Language and Vision-Language Models
Authors: Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, and K. W.
  Samuel Au
Categories: cs.RO cs.AI
Comments: Accepted by ICRA 2024, 7 pages, 8 figures
\\ ( https://arxiv.org/abs/2310.08873 ,  55047kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13786 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 14:04:54 GMT   (244kb)

Title: Fundamental Limits of Membership Inference Attacks on Machine Learning
  Models
Authors: Eric Aubinais, Elisabeth Gassiat, Pablo Piantanida
Categories: stat.ML cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.13786 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19620
replaced with revised version Wed, 31 Jan 2024 11:22:46 GMT   (4800kb,D)

Title: Large Trajectory Models are Scalable Motion Predictors and Planners
Authors: Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo,
  Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao
Categories: cs.RO cs.AI cs.CV
\\ ( https://arxiv.org/abs/2310.19620 ,  4800kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20381
replaced with revised version Tue, 30 Jan 2024 19:32:19 GMT   (36565kb,D)

Title: A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical
  Image Analysis
Authors: Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lei Wang, Lingqiao
  Liu, Leyang Cui, Zhaopeng Tu, Longyue Wang, Luping Zhou
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.20381 ,  36565kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11319
replaced with revised version Tue, 30 Jan 2024 22:51:22 GMT   (5339kb,D)

Title: GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for
  Automated Segmentation of Mobility Infrastructure
Authors: Rafi Ibn Sultan, Chengyin Li, Hui Zhu, Prashant Khanduri, Marco
  Brocanelli, Dongxiao Zhu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.11319 ,  5339kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06712
replaced with revised version Wed, 31 Jan 2024 18:44:22 GMT   (41767kb,D)

Title: Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion
  Models
Authors: Zhipeng Bao and Yijun Li and Krishna Kumar Singh and Yu-Xiong Wang and
  Martial Hebert
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.06712 ,  41767kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06550
replaced with revised version Wed, 31 Jan 2024 18:13:53 GMT   (10625kb)

Title: Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery
  and Geographical Prior
Authors: Chuanji Shi, Yingying Zhang, Jiaotuan Wang, Xin Guo and Qiqi Zhu
Categories: cs.CV cs.AI
Comments: 9 pages, 9 figures
MSC-class: 68T99
ACM-class: I.4.9
\\ ( https://arxiv.org/abs/2401.06550 ,  10625kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06633
replaced with revised version Wed, 31 Jan 2024 11:07:32 GMT   (582kb,D)

Title: Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential
  Recommendations
Authors: Lei Li, Jianxun Lian, Xiao Zhou, Xing Xie
Categories: cs.IR cs.AI
Comments: 9 pages, Accepted to AAAI2024
\\ ( https://arxiv.org/abs/2401.06633 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07031
replaced with revised version Tue, 30 Jan 2024 20:50:56 GMT   (2863kb,D)

Title: Code Security Vulnerability Repair Using Reinforcement Learning with
  Large Language Models
Authors: Nafis Tanveer Islam, Mohammad Bahrami Karkevandi, Peyman Najafirad
Categories: cs.CR cs.AI cs.SE
\\ ( https://arxiv.org/abs/2401.07031 ,  2863kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14489
replaced with revised version Tue, 30 Jan 2024 21:26:09 GMT   (27008kb,D)

Title: The Case for Co-Designing Model Architectures with Hardware
Authors: Quentin Anthony, Jacob Hatef, Deepak Narayanan, Stella Biderman, Stas
  Bekman, Junqi Yin, Aamir Shafi, Hari Subramoni, Dhabaleswar Panda
Categories: cs.DC cs.AI
\\ ( https://arxiv.org/abs/2401.14489 ,  27008kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15970
replaced with revised version Wed, 31 Jan 2024 02:11:46 GMT   (9645kb,D)

Title: HEQuant: Marrying Homomorphic Encryption and Quantization for
  Communication-Efficient Private Inference
Authors: Tianshi Xu, Meng Li, Runsheng Wang
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2401.15970 ,  9645kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17053
replaced with revised version Wed, 31 Jan 2024 14:53:22 GMT   (7793kb,D)

Title: BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane
  Extrapolation
Authors: Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang,
  Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, and Pan Ji
Categories: cs.CV cs.AI cs.GR
Comments: Video: https://www.youtube.com/watch?v=PxIBtd6G0mA
\\ ( https://arxiv.org/abs/2401.17053 ,  7793kb)
------------------------------------------------------------------------------
\\
arXiv:2006.02570 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 21:39:38 GMT   (81104kb,D)

Title: Exploration of Interpretability Techniques for Deep COVID-19
  Classification using Chest X-ray Images
Authors: Soumick Chatterjee, Fatima Saad, Chompunuch Sarasaen, Suhita Ghosh,
  Valerie Krug, Rupali Khatun, Rahul Mishra, Nirja Desai, Petia Radeva, Georg
  Rose, Sebastian Stober, Oliver Speck, Andreas N\"urnberger
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2006.02570 ,  81104kb)
------------------------------------------------------------------------------
\\
arXiv:2112.10953
replaced with revised version Tue, 30 Jan 2024 21:36:42 GMT   (823kb,D)

Title: An adaptation of InfoMap to absorbing random walks using
  absorption-scaled graphs
Authors: Esteban Vargas Bernal, Mason A. Porter, Joseph H. Tien
Categories: cs.SI cs.LG math.PR nlin.AO physics.soc-ph
\\ ( https://arxiv.org/abs/2112.10953 ,  823kb)
------------------------------------------------------------------------------
\\
arXiv:2203.01327 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 00:40:51 GMT   (3024kb,D)

Title: Hyperspectral Pixel Unmixing with Latent Dirichlet Variational
  Autoencoder
Authors: Kiran Mantripragada and Faisal Z. Qureshi
Categories: eess.IV cs.CV cs.LG
DOI: 10.1109/TGRS.2024.3357589
\\ ( https://arxiv.org/abs/2203.01327 ,  3024kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17615 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 21:09:48 GMT   (7026kb,D)

Title: Utilizing Reinforcement Learning for de novo Drug Design
Authors: Hampus Gummesson Svensson, Christian Tyrchan, Ola Engkvist, Morteza
  Haghir Chehreghani
Categories: q-bio.BM cs.LG
\\ ( https://arxiv.org/abs/2303.17615 ,  7026kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17323 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 03:35:44 GMT   (1434kb,D)

Title: Some Primal-Dual Theory for Subgradient Methods for Strongly Convex
  Optimization
Authors: Benjamin Grimmer, Danlin Li
Categories: math.OC cs.LG
Comments: 22 pages, major revision shortened the write-up and unified the
  analysis to be done just once in a single "super" setting
\\ ( https://arxiv.org/abs/2305.17323 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05318 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 15:54:58 GMT   (4338kb,D)

Title: Predicting small molecules solubilities on endpoint devices using deep
  ensemble neural networks
Authors: Mayk Caldas Ramos and Andrew D. White
Categories: physics.chem-ph cs.LG
\\ ( https://arxiv.org/abs/2307.05318 ,  4338kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10895
replaced with revised version Wed, 31 Jan 2024 12:40:51 GMT   (27595kb,D)

Title: Variational Autoencoding of Dental Point Clouds
Authors: Johan Ziruo Ye, Thomas {\O}rkild, Peter Lempel S{\o}ndergaard,
  S{\o}ren Hauberg
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.10895 ,  27595kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12459 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 11:56:57 GMT   (418kb,D)

Title: Consistent Signal Reconstruction from Streaming Multivariate Time Series
Authors: Emilio Ruiz-Moreno, Luis Miguel L\'opez-Ramos, Baltasar
  Beferull-Lozano
Categories: eess.SP cs.LG
Comments: 11 pages, 8 figures
\\ ( https://arxiv.org/abs/2308.12459 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12831
replaced with revised version Wed, 31 Jan 2024 03:53:31 GMT   (33179kb,D)

Title: ECNR: Efficient Compressive Neural Representation of Time-Varying
  Volumetric Datasets
Authors: Kaiyuan Tang and Chaoli Wang
Categories: cs.CV cs.GR cs.LG
Comments: Accepted by IEEE PacificVis 2024 (conference papers track)
\\ ( https://arxiv.org/abs/2311.12831 ,  33179kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16699 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 19:33:29 GMT   (74kb,D)

Title: Computational Tradeoffs of Optimization-Based Bound Tightening in ReLU
  Networks
Authors: Fabian Badilla, Marcos Goycoolea, Gonzalo Mu\~noz, Thiago Serra
Categories: math.OC cs.LG
Comments: 8 pages, 4 figures
\\ ( https://arxiv.org/abs/2312.16699 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17019 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 14:08:52 GMT   (1571kb,D)

Title: Efficient Learning of Long-Range and Equivariant Quantum Systems
Authors: \v{S}t\v{e}p\'an \v{S}m\'id, Roberto Bondesan
Categories: quant-ph cs.LG
Comments: 51 pages
\\ ( https://arxiv.org/abs/2312.17019 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01145 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 05:50:05 GMT   (2592kb,D)

Title: HAAQI-Net: A non-intrusive neural music quality assessment model for
  hearing aids
Authors: Dyah A. M. G. Wisnu, Epri W. Pratiwi, Stefano Rini, Ryandhimas E.
  Zezario, Hsin-Min Wang, Yu Tsao
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2401.01145 ,  2592kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04286 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 23:12:06 GMT   (35kb)

Title: Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax
  Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes
Authors: Hyunouk Ko and Xiaoming Huo
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2401.04286 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08404 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 20:56:07 GMT   (1534kb)

Title: Training and Comparison of nnU-Net and DeepMedic Methods for
  Autosegmentation of Pediatric Brain Tumors
Authors: Arastoo Vossough, Nastaran Khalili, Ariana M. Familiar, Deep Gandhi,
  Karthik Viswanathan, Wenxin Tu, Debanjan Haldar, Sina Bagheri, Hannah
  Anderson, Shuvanjan Haldar, Phillip B. Storm, Adam Resnick, Jeffrey B. Ware,
  Ali Nabavizadeh, Anahita Fathi Kazerooni
Categories: eess.IV cs.CV cs.LG physics.med-ph
\\ ( https://arxiv.org/abs/2401.08404 ,  1534kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14442 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 07:12:52 GMT   (48763kb,D)

Title: Improving Antibody Humanness Prediction using Patent Data
Authors: Talip Ucar, Aubin Ramon, Dino Oglic, Rebecca Croasdale-Wood, Tom
  Diethe, Pietro Sormanni
Categories: q-bio.QM cs.LG stat.ML
Comments: 13 pages, 6 figures, Code: https://github.com/AstraZeneca/SelfPAD
\\ ( https://arxiv.org/abs/2401.14442 ,  48763kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14577
replaced with revised version Wed, 31 Jan 2024 01:53:05 GMT   (4900kb,D)

Title: An Algorithm for Streaming Differentially Private Data
Authors: Girish Kumar, Thomas Strohmer, and Roman Vershynin
Categories: cs.DB cs.IT cs.LG math.IT math.ST stat.TH
\\ ( https://arxiv.org/abs/2401.14577 ,  4900kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15294
replaced with revised version Wed, 31 Jan 2024 05:23:56 GMT   (75kb)

Title: Integral Operator Approaches for Scattered Data Fitting on Spheres
Authors: Shao-Bo Lin
Categories: math.NA cs.LG cs.NA
\\ ( https://arxiv.org/abs/2401.15294 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16468
replaced with revised version Wed, 31 Jan 2024 18:54:15 GMT   (42471kb,D)

Title: High-Quality Image Restoration Following Human Instructions
Authors: Marcos V. Conde, Gregor Geigle, Radu Timofte
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2401.16468 ,  42471kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16655 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 02:14:18 GMT   (13kb)

Title: Rademacher Complexity of Neural ODEs via Chen-Fliess Series
Authors: Joshua Hanson, Maxim Raginsky
Categories: stat.ML cs.LG cs.SY eess.SY math.OC
Comments: 14 pages; submitted to L4DC 2024
\\ ( https://arxiv.org/abs/2401.16655 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16986 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 11:01:06 GMT   (880kb,D)

Title: Causal Machine Learning for Cost-Effective Allocation of Development Aid
Authors: Milan Kuzmanovic, Dennis Frauen, Tobias Hatt, Stefan Feuerriegel
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2401.16986 ,  880kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
