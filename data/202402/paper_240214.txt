Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100011 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月14日 17:36
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 12 Feb 24 19:00:00 GMT  to  Tue 13 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.07925
Date: Mon, 5 Feb 2024 16:23:07 GMT   (5289kb,D)

Title: Point and Instruct: Enabling Precise Image Editing by Unifying Direct
  Manipulation and Text Instructions
Authors: Alec Helbling, Seongmin Lee, Polo Chau
Categories: cs.AI cs.HC
\\
  Machine learning has enabled the development of powerful systems capable of
editing images from natural language instructions. However, in many common
scenarios it is difficult for users to specify precise image transformations
with text alone. For example, in an image with several dogs, it is difficult to
select a particular dog and move it to a precise location. Doing this with text
alone would require a complex prompt that disambiguates the target dog and
describes the destination. However, direct manipulation is well suited to
visual tasks like selecting objects and specifying locations. We introduce
Point and Instruct, a system for seamlessly combining familiar direct
manipulation and textual instructions to enable precise image manipulation.
With our system, a user can visually mark objects and locations, and reference
them in textual instructions. This allows users to benefit from both the visual
descriptiveness of natural language and the spatial precision of direct
manipulation.
\\ ( https://arxiv.org/abs/2402.07925 ,  5289kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07927
Date: Mon, 5 Feb 2024 19:49:13 GMT   (104kb,D)

Title: A Systematic Survey of Prompt Engineering in Large Language Models:
  Techniques and Applications
Authors: Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat
  Mondal, and Aman Chadha
Categories: cs.AI cs.CL cs.HC
Comments: 9 pages, 2 figures
\\
  Prompt engineering has emerged as an indispensable technique for extending
the capabilities of large language models (LLMs) and vision-language models
(VLMs). This approach leverages task-specific instructions, known as prompts,
to enhance model efficacy without modifying the core model parameters. Rather
than updating the model parameters, prompts allow seamless integration of
pre-trained models into downstream tasks by eliciting desired model behaviors
solely based on the given prompt. Prompts can be natural language instructions
that provide context to guide the model or learned vector representations that
activate relevant knowledge. This burgeoning field has enabled success across
various applications, from question-answering to commonsense reasoning.
However, there remains a lack of systematic organization and understanding of
the diverse prompt engineering methods and techniques. This survey paper
addresses the gap by providing a structured overview of recent advancements in
prompt engineering, categorized by application area. For each prompting
approach, we provide a summary detailing the prompting methodology, its
applications, the models involved, and the datasets utilized. We also delve
into the strengths and limitations of each approach and include a taxonomy
diagram and table summarizing datasets, models, and critical points of each
prompting technique. This systematic analysis enables a better understanding of
this rapidly developing field and facilitates future research by illuminating
open challenges and opportunities for prompt engineering.
\\ ( https://arxiv.org/abs/2402.07927 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07963
Date: Mon, 12 Feb 2024 10:32:47 GMT   (3122kb,D)

Title: SMX: Sequential Monte Carlo Planning for Expert Iteration
Authors: Matthew V Macfarlane, Edan Toledo, Donal Byrne, Siddarth Singh, Paul
  Duckworth, Alexandre Laterre
Categories: cs.AI cs.LG
Comments: 25 pages, 5 main figures
\\
  Developing agents that can leverage planning abilities during their decision
and learning processes is critical to the advancement of Artificial
Intelligence. Recent works have demonstrated the effectiveness of combining
tree-based search methods and self-play learning mechanisms. Yet, these methods
typically face scaling challenges due to the sequential nature of their search.
While practical engineering solutions can partly overcome this, they still
demand extensive computational resources, which hinders their applicability. In
this paper, we introduce SMX, a model-based planning algorithm that utilises
scalable Sequential Monte Carlo methods to create an effective self-learning
mechanism. Grounded in the theoretical framework of control as inference, SMX
benefits from robust theoretical underpinnings. Its sampling-based search
approach makes it adaptable to environments with both discrete and continuous
action spaces. Furthermore, SMX allows for high parallelisation and can run on
hardware accelerators to optimise computing efficiency. SMX demonstrates a
statistically significant improvement in performance compared to AlphaZero, as
well as demonstrating its performance as an improvement operator for a
model-free policy, matching or exceeding top model-free methods across both
continuous and discrete environments.
\\ ( https://arxiv.org/abs/2402.07963 ,  3122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08064
Date: Mon, 12 Feb 2024 21:14:45 GMT   (1097kb)

Title: Beyond LLMs: Advancing the Landscape of Complex Reasoning
Authors: Jennifer Chu-Carroll, Andrew Beck, Greg Burnham, David OS Melville,
  David Nachman, A. Erdem \"Ozcan, David Ferrucci
Categories: cs.AI cs.CL
\\
  Since the advent of Large Language Models a few years ago, they have often
been considered the de facto solution for many AI problems. However, in
addition to the many deficiencies of LLMs that prevent them from broad industry
adoption, such as reliability, cost, and speed, there is a whole class of
common real world problems that Large Language Models perform poorly on,
namely, constraint satisfaction and optimization problems. These problems are
ubiquitous and current solutions are highly specialized and expensive to
implement. At Elemental Cognition, we developed our EC AI platform which takes
a neuro-symbolic approach to solving constraint satisfaction and optimization
problems. The platform employs, at its core, a precise and high performance
logical reasoning engine, and leverages LLMs for knowledge acquisition and user
interaction. This platform supports developers in specifying application logic
in natural and concise language while generating application user interfaces to
interact with users effectively. We evaluated LLMs against systems built on the
EC AI platform in three domains and found the EC AI systems to significantly
outperform LLMs on constructing valid and optimal solutions, on validating
proposed solutions, and on repairing invalid solutions.
\\ ( https://arxiv.org/abs/2402.08064 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08088
Date: Mon, 12 Feb 2024 22:10:06 GMT   (3919kb,D)

Title: Out-of-Distribution Detection and Data Drift Monitoring using
  Statistical Process Control
Authors: Ghada Zamzmi, Kesavan Venkatesh, Brandon Nelson, Smriti Prathapan,
  Paul H. Yi, Berkman Sahiner, and Jana G. Delfino
Categories: cs.AI cs.LG eess.IV
\\
  Background: Machine learning (ML) methods often fail with data that deviates
from their training distribution. This is a significant concern for ML-enabled
devices in clinical settings, where data drift may cause unexpected performance
that jeopardizes patient safety.
  Method: We propose a ML-enabled Statistical Process Control (SPC) framework
for out-of-distribution (OOD) detection and drift monitoring. SPC is
advantageous as it visually and statistically highlights deviations from the
expected distribution. To demonstrate the utility of the proposed framework for
monitoring data drift in radiological images, we investigated different design
choices, including methods for extracting feature representations, drift
quantification, and SPC parameter selection.
  Results: We demonstrate the effectiveness of our framework for two tasks: 1)
differentiating axial vs. non-axial computed tomography (CT) images and 2)
separating chest x-ray (CXR) from other modalities. For both tasks, we achieved
high accuracy in detecting OOD inputs, with 0.913 in CT and 0.995 in CXR, and
sensitivity of 0.980 in CT and 0.984 in CXR. Our framework was also adept at
monitoring data streams and identifying the time a drift occurred. In a
simulation with 100 daily CXR cases, we detected a drift in OOD input
percentage from 0-1% to 3-5% within two days, maintaining a low false-positive
rate. Through additional experimental results, we demonstrate the framework's
data-agnostic nature and independence from the underlying model's structure.
  Conclusion: We propose a framework for OOD detection and drift monitoring
that is agnostic to data, modality, and model. The framework is customizable
and can be adapted for specific applications.
\\ ( https://arxiv.org/abs/2402.08088 ,  3919kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08115
Date: Mon, 12 Feb 2024 23:11:01 GMT   (673kb,D)

Title: On the Self-Verification Limitations of Large Language Models on
  Reasoning and Planning Tasks
Authors: Kaya Stechly, Karthik Valmeekam, Subbarao Kambhampati
Categories: cs.AI
Comments: arXiv admin note: text overlap with arXiv:2310.12397
\\
  There has been considerable divergence of opinion on the reasoning abilities
of Large Language Models (LLMs). While the initial optimism that reasoning
might emerge automatically with scale has been tempered thanks to a slew of
counterexamples--ranging from multiplication to simple planning--there persists
a wide spread belief that LLMs can self-critique and improve their own
solutions in an iterative fashion. This belief seemingly rests on the
assumption that verification of correctness should be easier than generation--a
rather classical argument from computational complexity--which should be
irrelevant to LLMs to the extent that what they are doing is approximate
retrieval. In this paper, we set out to systematically investigate the
effectiveness of iterative prompting in the context of reasoning and planning.
We present a principled empirical study of the performance of GPT-4 in three
domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both
with the model critiquing its own answers and with an external correct reasoner
verifying proposed solutions. In each case, we analyze whether the content of
criticisms actually affects bottom line performance, and whether we can ablate
elements of the augmented system without losing performance. We observe
significant performance collapse with self-critique, significant performance
gains with sound external verification, but that the content of critique
doesn't matter to the performance of the system. In fact, merely re-prompting
with a sound verifier maintains most of the benefits of more involved setups.
\\ ( https://arxiv.org/abs/2402.08115 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08128
Date: Mon, 12 Feb 2024 23:53:46 GMT   (157kb,D)

Title: Recursive Joint Simulation in Games
Authors: Vojtech Kovarik, Caspar Oesterheld, Vincent Conitzer
Categories: cs.AI cs.GT
\\
  Game-theoretic dynamics between AI agents could differ from traditional
human-human interactions in various ways. One such difference is that it may be
possible to accurately simulate an AI agent, for example because its source
code is known. Our aim is to explore ways of leveraging this possibility to
achieve more cooperative outcomes in strategic settings. In this paper, we
study an interaction between AI agents where the agents run a recursive joint
simulation. That is, the agents first jointly observe a simulation of the
situation they face. This simulation in turn recursively includes additional
simulations (with a small chance of failure, to avoid infinite recursion), and
the results of all these nested simulations are observed before an action is
chosen. We show that the resulting interaction is strategically equivalent to
an infinitely repeated version of the original game, allowing a direct transfer
of existing results such as the various folk theorems.
\\ ( https://arxiv.org/abs/2402.08128 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08145
Date: Tue, 13 Feb 2024 00:50:06 GMT   (387kb,D)

Title: Epistemic Exploration for Generalizable Planning and Learning in
  Non-Stationary Settings
Authors: Rushang Karia, Pulkit Verma, Alberto Speranzon, Siddharth Srivastava
Categories: cs.AI
Comments: To appear at ICAPS-24
\\
  This paper introduces a new approach for continual planning and model
learning in non-stationary stochastic environments expressed using relational
representations. Such capabilities are essential for the deployment of
sequential decision-making systems in the uncertain, constantly evolving real
world. Working in such practical settings with unknown (and non-stationary)
transition systems and changing tasks, the proposed framework models gaps in
the agent's current state of knowledge and uses them to conduct focused,
investigative explorations. Data collected using these explorations is used for
learning generalizable probabilistic models for solving the current task
despite continual changes in the environment dynamics. Empirical evaluations on
several benchmark domains show that this approach significantly outperforms
planning and RL baselines in terms of sample complexity in non-stationary
settings. Theoretical results show that the system reverts to exhibit desirable
convergence properties when stationarity holds.
\\ ( https://arxiv.org/abs/2402.08145 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08174
Date: Tue, 13 Feb 2024 02:13:12 GMT   (1369kb,D)

Title: Hierarchical Position Embedding of Graphs with Landmarks and Clustering
  for Link Prediction
Authors: Minsang Kim and Seungjun Baek
Categories: cs.AI cs.LG
Comments: The International World Wide Web Conference (WWW) 2024, Accepted
  paper
DOI: 10.1145/3589334.3645372
\\
  Learning positional information of nodes in a graph is important for link
prediction tasks. We propose a representation of positional information using
representative nodes called landmarks. A small number of nodes with high degree
centrality are selected as landmarks, which serve as reference points for the
nodes' positions. We justify this selection strategy for well-known random
graph models and derive closed-form bounds on the average path lengths
involving landmarks. In a model for power-law graphs, we prove that landmarks
provide asymptotically exact information on inter-node distances. We apply
theoretical insights to practical networks and propose Hierarchical Position
embedding with Landmarks and Clustering (HPLC). HPLC combines landmark
selection and graph clustering, where the graph is partitioned into densely
connected clusters in which nodes with the highest degree are selected as
landmarks. HPLC leverages the positional information of nodes based on
landmarks at various levels of hierarchy such as nodes' distances to landmarks,
inter-landmark distances and hierarchical grouping of clusters. Experiments
show that HPLC achieves state-of-the-art performances of link prediction on
various datasets in terms of HIT@K, MRR, and AUC. The code is available at
\url{https://github.com/kmswin1/HPLC}.
\\ ( https://arxiv.org/abs/2402.08174 ,  1369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08178
Date: Tue, 13 Feb 2024 02:28:57 GMT   (12108kb,D)

Title: LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied
  Agents
Authors: Jae-Woo Choi and Youngwoo Yoon and Hyobin Ong and Jaehong Kim and
  Minsu Jang
Categories: cs.AI
Comments: ICLR 2024. Code: https://github.com/lbaa2022/LLMTaskPlanning
\\
  Large language models (LLMs) have recently received considerable attention as
alternative solutions for task planning. However, comparing the performance of
language-oriented task planners becomes difficult, and there exists a dearth of
detailed exploration regarding the effects of various factors such as
pre-trained model selection and prompt construction. To address this, we
propose a benchmark system for automatically quantifying performance of task
planning for home-service embodied agents. Task planners are tested on two
pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of
Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform
extensive experiments with LLMs and prompts, and explore several enhancements
of the baseline planner. We expect that the proposed benchmark tool would
accelerate the development of language-oriented task planners.
\\ ( https://arxiv.org/abs/2402.08178 ,  12108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08184
Date: Tue, 13 Feb 2024 02:48:18 GMT   (4076kb,D)

Title: Enabling Multi-Agent Transfer Reinforcement Learning via Scenario
  Independent Representation
Authors: Ayesha Siddika Nipu, Siming Liu, Anthony Harris
Categories: cs.AI cs.LG
Comments: 2023 IEEE Conference on Games (CoG)
DOI: 10.1109/CoG57401.2023.10333236
\\
  Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in
tackling complex tasks that require collaboration and competition among agents
in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch
is arduous and may not always be feasible, particularly for MASs with a large
number of interactive agents due to the extensive sample complexity. Therefore,
reusing knowledge gained from past experiences or other agents could
efficiently accelerate the learning process and upscale MARL algorithms. In
this study, we introduce a novel framework that enables transfer learning for
MARL through unifying various state spaces into fixed-size inputs that allow
one unified deep-learning policy viable in different scenarios within a MAS. We
evaluated our approach in a range of scenarios within the StarCraft Multi-Agent
Challenge (SMAC) environment, and the findings show significant enhancements in
multi-agent learning performance using maneuvering skills learned from other
scenarios compared to agents learning from scratch. Furthermore, we adopted
Curriculum Transfer Learning (CTL), enabling our deep learning policy to
progressively acquire knowledge and skills across pre-designed homogeneous
learning scenarios organized by difficulty levels. This process promotes inter-
and intra-agent knowledge transfer, leading to high multi-agent learning
performance in more complicated heterogeneous scenarios.
\\ ( https://arxiv.org/abs/2402.08184 ,  4076kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08185
Date: Tue, 13 Feb 2024 03:01:22 GMT   (178kb,D)

Title: Advancing Data-driven Weather Forecasting: Time-Sliding Data
  Augmentation of ERA5
Authors: Minjong Cheon, Daehyun Kang, Yo-Hwan Choi, and Seon-Yu Kang
Categories: cs.AI cs.CV physics.ao-ph
\\
  Modern deep learning techniques, which mimic traditional numerical weather
prediction (NWP) models and are derived from global atmospheric reanalysis
data, have caused a significant revolution within a few years. In this new
paradigm, our research introduces a novel strategy that deviates from the
common dependence on high-resolution data, which is often constrained by
computational resources, and instead utilizes low-resolution data (2.5 degrees)
for global weather prediction and climate data analysis. Our main focus is
evaluating data-driven weather prediction (DDWP) frameworks, specifically
addressing sample size adequacy, structural improvements to the model, and the
ability of climate data to represent current climatic trends. By using the
Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed
time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5),
this paper improves on conventional approaches by adding more variables and a
novel approach to data augmentation and processing. Our findings reveal that
despite the lower resolution, the proposed approach demonstrates considerable
accuracy in predicting atmospheric conditions, effectively rivaling
higher-resolution models. Furthermore, the study confirms the model's
proficiency in reflecting current climate trends and its potential in
predicting future climatic events, underscoring its utility in climate change
strategies. This research marks a pivotal step in the realm of meteorological
forecasting, showcasing the feasibility of lower-resolution data in producing
reliable predictions and opening avenues for more accessible and inclusive
climate modeling. The insights gleaned from this study not only contribute to
the advancement of climate science but also lay the groundwork for future
innovations in the field.
\\ ( https://arxiv.org/abs/2402.08185 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08208
Date: Tue, 13 Feb 2024 04:15:26 GMT   (1206kb,D)

Title: Inherent Diverse Redundant Safety Mechanisms for AI-based Software
  Elements in Automotive Applications
Authors: Mandar Pitale, Alireza Abbaspour, Devesh Upadhyay
Categories: cs.AI
Comments: This article is accepted for the SAE WCX 2024 conference proceedings
\\
  This paper explores the role and challenges of Artificial Intelligence (AI)
algorithms, specifically AI-based software elements, in autonomous driving
systems. These AI systems are fundamental in executing real-time critical
functions in complex and high-dimensional environments. They handle vital tasks
like multi-modal perception, cognition, and decision-making tasks such as
motion planning, lane keeping, and emergency braking. A primary concern relates
to the ability (and necessity) of AI models to generalize beyond their initial
training data. This generalization issue becomes evident in real-time
scenarios, where models frequently encounter inputs not represented in their
training or validation data. In such cases, AI systems must still function
effectively despite facing distributional or domain shifts. This paper
investigates the risk associated with overconfident AI models in
safety-critical applications like autonomous driving. To mitigate these risks,
methods for training AI models that help maintain performance without
overconfidence are proposed. This involves implementing certainty reporting
architectures and ensuring diverse training data. While various
distribution-based methods exist to provide safety mechanisms for AI models,
there is a noted lack of systematic assessment of these methods, especially in
the context of safety-critical automotive applications. Many methods in the
literature do not adapt well to the quick response times required in
safety-critical edge applications. This paper reviews these methods, discusses
their suitability for safety-critical applications, and highlights their
strengths and limitations. The paper also proposes potential improvements to
enhance the safety and reliability of AI algorithms in autonomous vehicles in
the context of rapid and accurate decision-making processes.
\\ ( https://arxiv.org/abs/2402.08208 ,  1206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08211
Date: Tue, 13 Feb 2024 04:28:43 GMT   (2640kb,D)

Title: Transformer Mechanisms Mimic Frontostriatal Gating Operations When
  Trained on Human Working Memory Tasks
Authors: Aaron Traylor, Jack Merullo, Michael J. Frank, Ellie Pavlick
Categories: cs.AI
Comments: 8 pages, 4 figures
ACM-class: I.2.6
\\
  Models based on the Transformer neural network architecture have seen success
on a wide variety of tasks that appear to require complex "cognitive branching"
-- or the ability to maintain pursuit of one goal while accomplishing others.
In cognitive neuroscience, success on such tasks is thought to rely on
sophisticated frontostriatal mechanisms for selective \textit{gating}, which
enable role-addressable updating -- and later readout -- of information to and
from distinct "addresses" of memory, in the form of clusters of neurons.
However, Transformer models have no such mechanisms intentionally built-in. It
is thus an open question how Transformers solve such tasks, and whether the
mechanisms that emerge to help them to do so bear any resemblance to the gating
mechanisms in the human brain. In this work, we analyze the mechanisms that
emerge within a vanilla attention-only Transformer trained on a simple sequence
modeling task inspired by a task explicitly designed to study working memory
gating in computational cognitive neuroscience. We find that, as a result of
training, the self-attention mechanism within the Transformer specializes in a
way that mirrors the input and output gating mechanisms which were explicitly
incorporated into earlier, more biologically-inspired architectures. These
results suggest opportunities for future research on computational similarities
between modern AI architectures and models of the human brain.
\\ ( https://arxiv.org/abs/2402.08211 ,  2640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08236
Date: Tue, 13 Feb 2024 06:02:05 GMT   (860kb,D)

Title: BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept
  Analysis and BERT
Authors: Siqi Peng, Hongyuan Yang, Akihiro Yamamoto
Categories: cs.AI
Comments: 23 pages, 5 figures
\\
  We propose BERT4FCA, a novel method for link prediction in bipartite
networks, using formal concept analysis (FCA) and BERT. Link prediction in
bipartite networks is an important task that can solve various practical
problems like friend recommendation in social networks and co-authorship
prediction in author-paper networks. Recent research has found that in
bipartite networks, maximal bi-cliques provide important information for link
prediction, and they can be extracted by FCA. Some FCA-based bipartite link
prediction methods have achieved good performance. However, we figured out that
their performance could be further improved because these methods did not fully
capture the rich information of the extracted maximal bi-cliques. To address
this limitation, we propose an approach using BERT, which can learn more
information from the maximal bi-cliques extracted by FCA and use them to make
link prediction. We conduct experiments on three real-world bipartite networks
and demonstrate that our method outperforms previous FCA-based methods, and
some classic methods such as matrix-factorization and node2vec.
\\ ( https://arxiv.org/abs/2402.08236 ,  860kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08242
Date: Tue, 13 Feb 2024 06:13:17 GMT   (469kb,D)

Title: Towards Equitable Agile Research and Development of AI and Robotics
Authors: Andrew Hundt, Julia Schuller, Severin Kacianka
Categories: cs.AI cs.CY cs.LG cs.RO cs.SE
Comments: 15 pages (32 with refs + appendix), 2 figures, 1 table (7 with
  appendix), incorporates changes based on WeRobot 2023 Draft feedback
\\
  Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to
replicate and amplify existing biases and prejudices, as do Robots with AI. For
example, robots with facial recognition have failed to identify Black Women as
human, while others have categorized people, such as Black Men, as criminals
based on appearance alone. A 'culture of modularity' means harms are perceived
as 'out of scope', or someone else's responsibility, throughout employment
positions in the 'AI supply chain'. Incidents are routine enough
(incidentdatabase.ai lists over 2000 examples) to indicate that few
organizations are capable of completely respecting peoples' rights; meeting
claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and
then addressing such failures in their organizations and artifacts. We propose
a framework for adapting widely practiced Research and Development (R&D)
project management methodologies to build organizational equity capabilities
and better integrate known evidence-based best practices. We describe how
project teams can organize and operationalize the most promising practices,
skill sets, organizational cultures, and methods to detect and address
rights-based fairness, equity, accountability, and ethical problems as early as
possible when they are often less harmful and easier to mitigate; then monitor
for unforeseen incidents to adaptively and constructively address them. Our
primary example adapts an Agile development process based on Scrum, one of the
most widely adopted approaches to organizing R&D teams. We also discuss
limitations of our proposed framework and future research directions.
\\ ( https://arxiv.org/abs/2402.08242 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08250
Date: Tue, 13 Feb 2024 06:38:46 GMT   (527kb)

Title: A survey of recent methods for addressing AI fairness and bias in
  biomedicine
Authors: Yifan Yang, Mingquan Lin, Han Zhao, Yifan Peng, Furong Huang, Zhiyong
  Lu
Categories: cs.AI
\\
  Artificial intelligence (AI) systems have the potential to revolutionize
clinical practices, including improving diagnostic accuracy and surgical
decision-making, while also reducing costs and manpower. However, it is
important to recognize that these systems may perpetuate social inequities or
demonstrate biases, such as those based on race or gender. Such biases can
occur before, during, or after the development of AI models, making it critical
to understand and address potential biases to enable the accurate and reliable
application of AI models in clinical settings. To mitigate bias concerns during
model development, we surveyed recent publications on different debiasing
methods in the fields of biomedical natural language processing (NLP) or
computer vision (CV). Then we discussed the methods that have been applied in
the biomedical domain to address bias. We performed our literature search on
PubMed, ACM digital library, and IEEE Xplore of relevant articles published
between January 2018 and December 2023 using multiple combinations of keywords.
We then filtered the result of 10,041 articles automatically with loose
constraints, and manually inspected the abstracts of the remaining 890 articles
to identify the 55 articles included in this review. Additional articles in the
references are also included in this review. We discuss each method and compare
its strengths and weaknesses. Finally, we review other potential methods from
the general domain that could be applied to biomedicine to address bias and
improve fairness.The bias of AIs in biomedicine can originate from multiple
sources. Existing debiasing methods that focus on algorithms can be categorized
into distributional or algorithmic.
\\ ( https://arxiv.org/abs/2402.08250 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08269
Date: Tue, 13 Feb 2024 07:49:57 GMT   (913kb,D)

Title: Geometry-induced Implicit Regularization in Deep ReLU Neural Networks
Authors: Joachim Bona-Pellissier (IMT), Fran \c{c}ois Malgouyres (IMT), Fran
  \c{c}ois Bachoc (IMT)
Categories: cs.AI cs.LG cs.NE math.OC math.ST stat.TH
\\
  It is well known that neural networks with many more parameters than training
examples do not overfit. Implicit regularization phenomena, which are still not
well understood, occur during optimization and 'good' networks are favored.
Thus the number of parameters is not an adequate measure of complexity if we do
not consider all possible networks but only the 'good' ones. To better
understand which networks are favored during optimization, we study the
geometry of the output set as parameters vary. When the inputs are fixed, we
prove that the dimension of this set changes and that the local dimension,
called batch functional dimension, is almost surely determined by the
activation patterns in the hidden layers. We prove that the batch functional
dimension is invariant to the symmetries of the network parameterization:
neuron permutations and positive rescalings. Empirically, we establish that the
batch functional dimension decreases during optimization. As a consequence,
optimization leads to parameters with low batch functional dimensions. We call
this phenomenon geometry-induced implicit regularization.The batch functional
dimension depends on both the network parameters and inputs. To understand the
impact of the inputs, we study, for fixed parameters, the largest attainable
batch functional dimension when the inputs vary. We prove that this quantity,
called computable full functional dimension, is also invariant to the
symmetries of the network's parameterization, and is determined by the
achievable activation patterns. We also provide a sampling theorem, showing a
fast convergence of the estimation of the computable full functional dimension
for a random input of increasing size. Empirically we find that the computable
full functional dimension remains close to the number of parameters, which is
related to the notion of local identifiability. This differs from the observed
values for the batch functional dimension computed on training inputs and test
inputs. The latter are influenced by geometry-induced implicit regularization.
\\ ( https://arxiv.org/abs/2402.08269 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08280
Date: Tue, 13 Feb 2024 08:14:10 GMT   (1468kb,D)

Title: Pix2Code: Learning to Compose Neural Visual Concepts as Programs
Authors: Antonia W\"ust, Wolfgang Stammer, Quentin Delfosse, Devendra Singh
  Dhami, Kristian Kersting
Categories: cs.AI cs.CV cs.LG
\\
  The challenge in learning abstract concepts from images in an unsupervised
fashion lies in the required integration of visual perception and generalizable
relational reasoning. Moreover, the unsupervised nature of this task makes it
necessary for human users to be able to understand a model's learnt concepts
and potentially revise false behaviours. To tackle both the generalizability
and interpretability constraints of visual concept learning, we propose
Pix2Code, a framework that extends program synthesis to visual relational
reasoning by utilizing the abilities of both explicit, compositional symbolic
and implicit neural representations. This is achieved by retrieving object
representations from images and synthesizing relational concepts as
lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the
challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its
ability to identify compositional visual concepts that generalize to novel data
and concept configurations. Particularly, in stark contrast to neural
approaches, we show that Pix2Code's representations remain human interpretable
and can be easily revised for improved performance.
\\ ( https://arxiv.org/abs/2402.08280 ,  1468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08284
Date: Tue, 13 Feb 2024 08:24:32 GMT   (1738kb)

Title: A Logical Approach to Criminal Case Investigation
Authors: Takanori Ugai, Yusuke Koyanagi, Fumihito Nishino
Categories: cs.AI
Comments: 11 pages, 11 figures
\\
  XAI (eXplanable AI) techniques that have the property of explaining the
reasons for their conclusions, i.e. explainability or interpretability, are
attracting attention. XAI is expected to be used in the development of forensic
science and the justice system. In today's forensic and criminal investigation
environment, experts face many challenges due to large amounts of data, small
pieces of evidence in a chaotic and complex environment, traditional laboratory
structures and sometimes inadequate knowledge. All these can lead to failed
investigations and miscarriages of justice. In this paper, we describe the
application of one logical approach to crime scene investigation. The subject
of the application is ``The Adventure of the Speckled Band'' from the Sherlock
Holmes short stories. The applied data is the knowledge graph created for the
Knowledge Graph Reasoning Challenge. We tried to find the murderer by inferring
each person with the motive, opportunity, and method. We created an ontology of
motives and methods of murder from dictionaries and dictionaries, added it to
the knowledge graph of ``The Adventure of the Speckled Band'', and applied
scripts to determine motives, opportunities, and methods.
\\ ( https://arxiv.org/abs/2402.08284 ,  1738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08298
Date: Tue, 13 Feb 2024 08:53:57 GMT   (12kb)

Title: Time to Stop and Think: What kind of research do we want to do?
Authors: Josu Ceberio, Borja Calvo
Categories: cs.AI
\\
  Experimentation is an intrinsic part of research in artificial intelligence
since it allows for collecting quantitative observations, validating
hypotheses, and providing evidence for their reformulation. For that reason,
experimentation must be coherent with the purposes of the research, properly
addressing the relevant questions in each case. Unfortunately, the literature
is full of works whose experimentation is neither rigorous nor convincing,
oftentimes designed to support prior beliefs rather than answering the relevant
research questions.
  In this paper, we focus on the field of metaheuristic optimization, since it
is our main field of work, and it is where we have observed the misconduct that
has motivated this letter. Even if we limit the focus of this manuscript to the
experimental part of the research, our main goal is to sew the seed of sincere
critical assessment of our work, sparking a reflection process both at the
individual and the community level. Such a reflection process is too complex
and extensive to be tackled as a whole. Therefore, to bring our feet to the
ground, we will include in this document our reflections about the role of
experimentation in our work, discussing topics such as the use of benchmark
instances vs instance generators, or the statistical assessment of empirical
results. That is, all the statements included in this document are personal
views and opinions, which can be shared by others or not. Certainly, having
different points of view is the basis to establish a good discussion process.
\\ ( https://arxiv.org/abs/2402.08298 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08369
Date: Tue, 13 Feb 2024 11:01:52 GMT   (27600kb,D)

Title: One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill
Authors: Sangwoo Shin, Daehee Lee, Minjong Yoo, Woo Kyung Kim, Honguk Woo
Categories: cs.AI
Comments: ICML-2023 Camera Ready Version
\\
  One-shot imitation is to learn a new task from a single demonstration, yet it
is a challenging problem to adopt it for complex tasks with the high domain
diversity inherent in a non-stationary environment. To tackle the problem, we
explore the compositionality of complex tasks, and present a novel skill-based
imitation learning framework enabling one-shot imitation and zero-shot
adaptation; from a single demonstration for a complex unseen task, a semantic
skill sequence is inferred and then each skill in the sequence is converted
into an action sequence optimized for environmental hidden dynamics that can
vary over time. Specifically, we leverage a vision-language model to learn a
semantic skill set from offline video datasets, where each skill is represented
on the vision-language embedding space, and adapt meta-learning with dynamics
inference to enable zero-shot skill adaptation. We evaluate our framework with
various one-shot imitation scenarios for extended multi-stage Meta-world tasks,
showing its superiority in learning complex tasks, generalizing to dynamics
changes, and extending to different demonstration conditions and modalities,
compared to other baselines.
\\ ( https://arxiv.org/abs/2402.08369 ,  27600kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08423
Date: Tue, 13 Feb 2024 12:50:04 GMT   (1900kb,D)

Title: Vehicle Behavior Prediction by Episodic-Memory Implanted NDT
Authors: Peining Shen, Jianwu Fang, Hongkai Yu, and Jianru Xue
Categories: cs.AI
Comments: Accepted by ICRA2024
\\
  In autonomous driving, predicting the behavior (turning left, stopping, etc.)
of target vehicles is crucial for the self-driving vehicle to make safe
decisions and avoid accidents. Existing deep learning-based methods have shown
excellent and accurate performance, but the black-box nature makes it
untrustworthy to apply them in practical use. In this work, we explore the
interpretability of behavior prediction of target vehicles by an Episodic
Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of
eMem-NDT is constructed by hierarchically clustering the text embedding of
vehicle behavior descriptions. eMem-NDT is a neural-backed part of a
pre-trained deep learning model by changing the soft-max layer of the deep
model to eMem-NDT, for grouping and aligning the memory prototypes of the
historical vehicle behavior features in training data on a neural decision
tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning
the behavior memory prototypes. By eMem-NDT, we infer each instance in behavior
prediction of vehicles by bottom-up Memory Prototype Matching (MPM) (searching
the appropriate leaf node and the links to the root node) and top-down Leaf
Link Aggregation (LLA) (obtaining the probability of future behaviors of
vehicles for certain instances). We validate eMem-NDT on BLVD and LOKI
datasets, and the results show that our model can obtain a superior performance
to other methods with clear explainability. The code is available at
https://github.com/JWFangit/eMem-NDT.
\\ ( https://arxiv.org/abs/2402.08423 ,  1900kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08466
Date: Tue, 13 Feb 2024 13:48:54 GMT   (97kb,D)

Title: Taking Training Seriously: Human Guidance and Management-Based
  Regulation of Artificial Intelligence
Authors: Cary Coglianese and Colton R. Crum
Categories: cs.AI cs.CV
Comments: 12 pages, 1 figure
\\
  Fervent calls for more robust governance of the harms associated with
artificial intelligence (AI) are leading to the adoption around the world of
what regulatory scholars have called a management-based approach to regulation.
Recent initiatives in the United States and Europe, as well as the adoption of
major self-regulatory standards by the International Organization for
Standardization, share in common a core management-based paradigm. These
management-based initiatives seek to motivate an increase in human oversight of
how AI tools are trained and developed. Refinements and systematization of
human-guided training techniques will thus be needed to fit within this
emerging era of management-based regulatory paradigm. If taken seriously,
human-guided training can alleviate some of the technical and ethical pressures
on AI, boosting AI performance with human intuition as well as better
addressing the needs for fairness and effective explainability. In this paper,
we discuss the connection between the emerging management-based regulatory
frameworks governing AI and the need for human oversight during training. We
broadly cover some of the technical components involved in human-guided
training and then argue that the kinds of high-stakes use cases for AI that
appear of most concern to regulators should lean more on human-guided training
than on data-only training. We hope to foster a discussion between legal
scholars and computer scientists involving how to govern a domain of technology
that is vast, heterogenous, and dynamic in its applications and risks.
\\ ( https://arxiv.org/abs/2402.08466 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08472
Date: Tue, 13 Feb 2024 14:05:02 GMT   (1296kb,D)

Title: Large Language Models for the Automated Analysis of Optimization
  Algorithms
Authors: Camilo Chac\'on Sartori and Christian Blum and Gabriela Ochoa
Categories: cs.AI
Comments: Submitted to the GECCO 2024 conference
\\
  The ability of Large Language Models (LLMs) to generate high-quality text and
code has fuelled their rise in popularity. In this paper, we aim to demonstrate
the potential of LLMs within the realm of optimization algorithms by
integrating them into STNWeb. This is a web-based tool for the generation of
Search Trajectory Networks (STNs), which are visualizations of optimization
algorithm behavior. Although visualizations produced by STNWeb can be very
informative for algorithm designers, they often require a certain level of
prior knowledge to be interpreted. In an attempt to bridge this knowledge gap,
we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive
written reports, complemented by automatically generated plots, thereby
enhancing the user experience and reducing the barriers to the adoption of this
tool by the research community. Moreover, our approach can be expanded to other
tools from the optimization community, showcasing the versatility and potential
of LLMs in this field.
\\ ( https://arxiv.org/abs/2402.08472 ,  1296kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08492
Date: Tue, 13 Feb 2024 14:38:12 GMT   (923kb)

Title: The Application of ChatGPT in Responding to Questions Related to the
  Boston Bowel Preparation Scale
Authors: Xiaoqiang Liu, Yubin Wang, Zicheng Huang, Boming Xu, Yilin Zeng, Xinqi
  Chen, Zilong Wang, Enning Yang, Xiaoxuan Lei, Yisen Huang, Xiaobo Liu
Categories: cs.AI
\\
  Background: Colonoscopy, a crucial diagnostic tool in gastroenterology,
depends heavily on superior bowel preparation. ChatGPT, a large language model
with emergent intelligence which also exhibits potential in medical
applications. This study aims to assess the accuracy and consistency of ChatGPT
in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment.
Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023.
These images were evaluated using the BBPS by 3 senior endoscopists and 3
novice endoscopists. Additionally, ChatGPT also assessed these images, having
been divided into three groups and undergone specific Fine-tuning. Consistency
was evaluated through two rounds of testing. Results: In the initial round,
ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists'
accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and
0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT
shows promise in bowel preparation scoring, it currently does not match the
accuracy and consistency of experienced endoscopists. Future research should
focus on in-depth Fine-tuning.
\\ ( https://arxiv.org/abs/2402.08492 ,  923kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08511
Date: Tue, 13 Feb 2024 15:05:54 GMT   (147kb,D)

Title: Amplifying Exploration in Monte-Carlo Tree Search by Focusing on the
  Unknown
Authors: Cedric Derstroff, Jannis Brugger, Jannis Bl\"uml, Mira Mezini, Stefan
  Kramer, Kristian Kersting
Categories: cs.AI
Comments: 10 pages, 7 figures
\\
  Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast
amount of applications. It strategically allocates computational resources to
focus on promising segments of the search tree, making it a very attractive
search algorithm in large search spaces. However, it often expends its limited
resources on reevaluating previously explored regions when they remain the most
promising path. Our proposed methodology, denoted as AmEx-MCTS, solves this
problem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the
decoupling of value updates, visit count updates, and the selected path during
the tree search, thereby enabling the exclusion of already explored subtrees or
leaves. This segregation preserves the utility of visit counts for both
exploration-exploitation balancing and quality metrics within MCTS. The
resultant augmentation facilitates in a considerably broader search using
identical computational resources, preserving the essential characteristics of
MCTS. The expanded coverage not only yields more precise estimations but also
proves instrumental in larger and more complex problems. Our empirical
evaluation demonstrates the superior performance of AmEx-MCTS, surpassing
classical MCTS and related approaches by a substantial margin.
\\ ( https://arxiv.org/abs/2402.08511 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08514
Date: Tue, 13 Feb 2024 15:10:30 GMT   (76kb,D)

Title: Counterfactual Influence in Markov Decision Processes
Authors: Milad Kazemi, Jessica Lally, Ekaterina Tishchenko, Hana Chockler and
  Nicola Paoletti
Categories: cs.AI
Comments: 12 pages, 6 figures
\\
  Our work addresses a fundamental problem in the context of counterfactual
inference for Markov Decision Processes (MDPs). Given an MDP path $\tau$, this
kind of inference allows us to derive counterfactual paths $\tau'$ describing
what-if versions of $\tau$ obtained under different action sequences than those
observed in $\tau$. However, as the counterfactual states and actions deviate
from the observed ones over time, the observation $\tau$ may no longer
influence the counterfactual world, meaning that the analysis is no longer
tailored to the individual observation, resulting in interventional outcomes
rather than counterfactual ones. Even though this issue specifically affects
the popular Gumbel-max structural causal model used for MDP counterfactuals, it
has remained overlooked until now. In this work, we introduce a formal
characterisation of influence based on comparing counterfactual and
interventional distributions. We devise an algorithm to construct
counterfactual models that automatically satisfy influence constraints.
Leveraging such models, we derive counterfactual policies that are not just
optimal for a given reward structure but also remain tailored to the observed
path. Even though there is an unavoidable trade-off between policy optimality
and strength of influence constraints, our experiments demonstrate that it is
possible to derive (near-)optimal policies while remaining under the influence
of the observation.
\\ ( https://arxiv.org/abs/2402.08514 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08565
Date: Tue, 13 Feb 2024 16:05:51 GMT   (13122kb,D)

Title: Artificial Intelligence for Literature Reviews: Opportunities and
  Challenges
Authors: Francisco Bolanos, Angelo Salatino, Francesco Osborne, Enrico Motta
Categories: cs.AI cs.HC cs.IR
\\
  This manuscript presents a comprehensive review of the use of Artificial
Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous
and organised methodology that assesses and integrates previous research on a
given topic. Numerous tools have been developed to assist and partially
automate the SLR process. The increasing role of AI in this field shows great
potential in providing more effective support for researchers, moving towards
the semi-automatic creation of literature reviews. Our study focuses on how AI
techniques are applied in the semi-automation of SLRs, specifically in the
screening and extraction phases. We examine 21 leading SLR tools using a
framework that combines 23 traditional features with 11 AI features. We also
analyse 11 recent tools that leverage large language models for searching the
literature and assisting academic writing. Finally, the paper discusses current
trends in the field, outlines key research challenges, and suggests directions
for future research.
\\ ( https://arxiv.org/abs/2402.08565 ,  13122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08644
Date: Tue, 13 Feb 2024 18:24:08 GMT   (305kb,D)

Title: Tandem Transformers for Inference Efficient LLMs
Authors: Aishwarya P S and Pranav Ajit Nair and Yashas Samaga and Toby Boyd and
  Sanjiv Kumar and Prateek Jain and Praneeth Netrapalli
Categories: cs.AI cs.CL
\\
  The autoregressive nature of conventional large language models (LLMs)
inherently limits inference speed, as tokens are generated sequentially. While
speculative and parallel decoding techniques attempt to mitigate this, they
face limitations: either relying on less accurate smaller models for generation
or failing to fully leverage the base LLM's representations.
  We introduce a novel architecture, Tandem transformers, to address these
issues. This architecture uniquely combines (1) a small autoregressive model
and (2) a large model operating in block mode (processing multiple tokens
simultaneously). The small model's predictive accuracy is substantially
enhanced by granting it attention to the large model's richer representations.
On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko
demonstrates a 3.3% improvement in next-token prediction accuracy over a
standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter
model with comparable downstream performance. We further incorporate the tandem
model within the speculative decoding (SPEED) framework where the large model
validates tokens from the small model. This ensures that the Tandem of
PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster
than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream
task accuracy.
\\ ( https://arxiv.org/abs/2402.08644 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08646
Date: Tue, 13 Feb 2024 18:24:23 GMT   (1382kb,D)

Title: Inference of Abstraction for a Unified Account of Symbolic Reasoning
  from Data
Authors: Hiroyuki Kido
Categories: cs.AI
\\
  Inspired by empirical work in neuroscience for Bayesian approaches to brain
function, we give a unified probabilistic account of various types of symbolic
reasoning from data. We characterise them in terms of formal logic using the
classical consequence relation, an empirical consequence relation, maximal
consistent sets, maximal possible sets and maximum likelihood estimation. The
theory gives new insights into reasoning towards human-like machine
intelligence.
\\ ( https://arxiv.org/abs/2402.08646 ,  1382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08670
Date: Tue, 13 Feb 2024 18:51:18 GMT   (522kb,D)

Title: Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models
Authors: Yuqing Liu, Yu Wang, Lichao Sun, Philip S. Yu
Categories: cs.AI
Comments: under review
\\
  The development of large vision-language models (LVLMs) offers the potential
to address challenges faced by traditional multimodal recommendations thanks to
their proficient understanding of static images and textual dynamics. However,
the application of LVLMs in this field is still limited due to the following
complexities: First, LVLMs lack user preference knowledge as they are trained
from vast general datasets. Second, LVLMs suffer setbacks in addressing
multiple image dynamics in scenarios involving discrete, noisy, and redundant
image sequences. To overcome these issues, we propose the novel reasoning
scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large
vision-language models for multimodal recommendation. We utilize user history
as in-context user preferences to address the first challenge. Next, we prompt
LVLMs to generate item image summaries and utilize image comprehension in
natural language space combined with item titles to query the user preferences
over candidate items. We conduct comprehensive experiments across four datasets
with three LVLMs: GPT4-V, LLaVa-7b, and LLaVa-13b. The numerical results
indicate the efficacy of VST.
\\ ( https://arxiv.org/abs/2402.08670 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07913
Date: Tue, 30 Jan 2024 13:11:23 GMT   (1075kb,D)

Title: QACP: An Annotated Question Answering Dataset for Assisting Chinese
  Python Programming Learners
Authors: Rui Xiao, Lu Han, Xiaoying Zhou, Jiong Wang, Na Zong, Pengyu Zhang
Categories: cs.CL cs.AI cs.HC
\\
  In online learning platforms, particularly in rapidly growing computer
programming courses, addressing the thousands of students' learning queries
requires considerable human cost. The creation of intelligent assistant large
language models (LLMs) tailored for programming education necessitates distinct
data support. However, in real application scenarios, the data resources for
training such LLMs are relatively scarce. Therefore, to address the data
scarcity in intelligent educational systems for programming, this paper
proposes a new Chinese question-and-answer dataset for Python learners. To
ensure the authenticity and reliability of the sources of the questions, we
collected questions from actual student questions and categorized them
according to various dimensions such as the type of questions and the type of
learners. This annotation principle is designed to enhance the effectiveness
and quality of online programming education, providing a solid data foundation
for developing the programming teaching assists (TA). Furthermore, we conducted
comprehensive evaluations of various LLMs proficient in processing and
generating Chinese content, highlighting the potential limitations of general
LLMs as intelligent teaching assistants in computer programming courses.
\\ ( https://arxiv.org/abs/2402.07913 ,  1075kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08005
Date: Mon, 12 Feb 2024 19:10:13 GMT   (795kb,D)

Title: Refined Direct Preference Optimization with Synthetic Data for
  Behavioral Alignment of LLMs
Authors: V\'ictor Gallego
Categories: cs.CL cs.LG
Comments: Pre-print. Submitted to the ICLR 2024 Workshop on Representational
  Alignment (Re-Align)
\\
  In this paper, we introduce \emph{refined Direct Preference Optimization}
(rDPO), a method for improving the behavioral alignment of Large Language
Models (LLMs) without the need for human-annotated data. The method involves
creating synthetic data using self-critique prompting by a teacher LLM and then
utilising a generalized DPO loss function to distil to a student LLM. The loss
function incorporates an additional external reward model to improve the
quality of synthetic data, making rDPO robust to potential noise in the
synthetic dataset. rDPO is shown to be effective in a diverse set of
behavioural alignment tasks, such as improved safety, robustness against
role-playing, and reduced sycophancy. Code to be released at
https://github.com/vicgalle/refined-dpo.
\\ ( https://arxiv.org/abs/2402.08005 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08015
Date: Mon, 12 Feb 2024 19:25:11 GMT   (206kb,D)

Title: Enhancing Amharic-LLaMA: Integrating Task Specific and Generative
  Datasets
Authors: Israel Abebe Azime, Mitiku Yohannes Fuge, Atnafu Lambebo Tonja,
  Tadesse Destaw Belay, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas
  Chanie, Walelign Tewabe Sewunetie, Seid Muhie Yimam
Categories: cs.CL
\\
  Large language models (LLMs) have received a lot of attention in natural
language processing (NLP) research because of their exceptional performance in
understanding and generating human languages. However, low-resource languages
are left behind due to the unavailability of resources. In this work, we focus
on enhancing the LLaMA-2-Amharic model by integrating task-specific and
generative datasets to improve language model performance for Amharic. We
compile an Amharic instruction fine-tuning dataset and fine-tuned
LLaMA-2-Amharic model. The fine-tuned model shows promising results in
different NLP tasks. We open-source our dataset creation pipeline, instruction
datasets, trained models, and evaluation outputs to promote language-specific
studies on these models.
\\ ( https://arxiv.org/abs/2402.08015 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08021
Date: Mon, 12 Feb 2024 19:35:37 GMT   (804kb,D)

Title: Careless Whisper: Speech-to-Text Hallucination Harms
Authors: Allison Koenecke, Anna Seo Gyeong Choi, Katelyn Mei, Hilke Schellmann,
  Mona Sloane
Categories: cs.CL cs.CY
\\
  Speech-to-text services aim to transcribe input audio as accurately as
possible. They increasingly play a role in everyday life, for example in
personal voice assistants or in customer-company interactions. We evaluate Open
AI's Whisper, a state-of-the-art service outperforming industry competitors.
While many of Whisper's transcriptions were highly accurate, we found that
roughly 1% of audio transcriptions contained entire hallucinated phrases or
sentences, which did not exist in any form in the underlying audio. We
thematically analyze the Whisper-hallucinated content, finding that 38% of
hallucinations include explicit harms such as violence, made up personal
information, or false video-based authority. We further provide hypotheses on
why hallucinations occur, uncovering potential disparities due to speech type
by health status. We call on industry practitioners to ameliorate these
language-model-based hallucinations in Whisper, and to raise awareness of
potential biases in downstream applications of speech-to-text models.
\\ ( https://arxiv.org/abs/2402.08021 ,  804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08078
Date: Mon, 12 Feb 2024 21:44:32 GMT   (328kb,D)

Title: Large Language Models as Agents in Two-Player Games
Authors: Yang Liu, Peng Sun, Hang Li
Categories: cs.CL cs.LG
\\
  By formally defining the training processes of large language models (LLMs),
which usually encompasses pre-training, supervised fine-tuning, and
reinforcement learning with human feedback, within a single and unified machine
learning paradigm, we can glean pivotal insights for advancing LLM
technologies. This position paper delineates the parallels between the training
methods of LLMs and the strategies employed for the development of agents in
two-player games, as studied in game theory, reinforcement learning, and
multi-agent systems. We propose a re-conceptualization of LLM learning
processes in terms of agent learning in language-based games. This framework
unveils innovative perspectives on the successes and challenges in LLM
development, offering a fresh understanding of addressing alignment issues
among other strategic considerations. Furthermore, our two-player game approach
sheds light on novel data preparation and machine learning techniques for
training LLMs.
\\ ( https://arxiv.org/abs/2402.08078 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08100
Date: Mon, 12 Feb 2024 22:35:40 GMT   (7705kb,D)

Title: Investigating the Impact of Data Contamination of Large Language Models
  in Text-to-SQL Translation
Authors: Federico Ranaldi, Elena Sofia Ruzzetti, Dario Onorati, Leonardo
  Ranaldi, Cristina Giannone, Andrea Favalli, Raniero Romagnoli, Fabio Massimo
  Zanzotto
Categories: cs.CL cs.LG
\\
  Understanding textual description to generate code seems to be an achieved
capability of instruction-following Large Language Models (LLMs) in zero-shot
scenario. However, there is a severe possibility that this translation ability
may be influenced by having seen target textual descriptions and the related
code. This effect is known as Data Contamination.
  In this study, we investigate the impact of Data Contamination on the
performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we
introduce a novel method to detect Data Contamination in GPTs and examine
GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new
unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on
databases with modified information via an adversarial table disconnection
(ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of
information from the database. Our results indicate a significant performance
drop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications,
highlighting the effect of Data Contamination on LLMs in Text-to-SQL
translation tasks.
\\ ( https://arxiv.org/abs/2402.08100 ,  7705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08113
Date: Mon, 12 Feb 2024 23:08:37 GMT   (20246kb,D)

Title: Addressing cognitive bias in medical language models
Authors: Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur
  Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, Rama
  Chellappa
Categories: cs.CL cs.HC
\\
  The integration of large language models (LLMs) into the medical field has
gained significant attention due to their promising accuracy in simulated
clinical decision-making settings. However, clinical decision-making is more
complex than simulations because physicians' decisions are shaped by many
factors, including the presence of cognitive bias. However, the degree to which
LLMs are susceptible to the same cognitive biases that affect human clinicians
remains unexplored. Our hypothesis posits that when LLMs are confronted with
clinical questions containing cognitive biases, they will yield significantly
less accurate responses compared to the same questions presented without such
biases.In this study, we developed BiasMedQA, a novel benchmark for evaluating
cognitive biases in LLMs applied to medical tasks. Using BiasMedQA we evaluated
six LLMs, namely GPT-4, Mixtral-8x70B, GPT-3.5, PaLM-2, Llama 2 70B-chat, and
the medically specialized PMC Llama 13B. We tested these models on 1,273
questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3,
modified to replicate common clinically-relevant cognitive biases. Our analysis
revealed varying effects for biases on these LLMs, with GPT-4 standing out for
its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B,
which were disproportionately affected by cognitive bias. Our findings
highlight the critical need for bias mitigation in the development of medical
LLMs, pointing towards safer and more reliable applications in healthcare.
\\ ( https://arxiv.org/abs/2402.08113 ,  20246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08155
Date: Tue, 13 Feb 2024 01:31:08 GMT   (325kb,D)

Title: CMA-R:Causal Mediation Analysis for Explaining Rumour Detection
Authors: Lin Tian, Xiuzhen Zhang, Jey Han Lau
Categories: cs.CL cs.AI
Comments: 9 pages, 7 figures, Accepted by EACL 2024 Findings
\\
  We apply causal mediation analysis to explain the decision-making process of
neural models for rumour detection on Twitter. Interventions at the input and
network level reveal the causal impacts of tweets and words in the model
output. We find that our approach CMA-R -- Causal Mediation Analysis for Rumour
detection -- identifies salient tweets that explain model predictions and show
strong agreement with human judgements for critical tweets determining the
truthfulness of stories. CMA-R can further highlight causally impactful words
in the salient tweets, providing another layer of interpretability and
transparency into these blackbox rumour detection systems. Code is available
at: https://github.com/ltian678/cma-r.
\\ ( https://arxiv.org/abs/2402.08155 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08183
Date: Tue, 13 Feb 2024 02:46:45 GMT   (235kb,D)

Title: Pixel Sentence Representation Learning
Authors: Chenghao Xiao, Zhuoxu Huang, Danlu Chen, G Thomas Hudson, Yizhi Li,
  Haoran Duan, Chenghua Lin, Jie Fu, Jungong Han, Noura Al Moubayed
Categories: cs.CL cs.CV
\\
  Pretrained language models are long known to be subpar in capturing sentence
and document-level semantics. Though heavily investigated, transferring
perturbation-based methods from unsupervised visual representation learning to
NLP remains an unsolved problem. This is largely due to the discreteness of
subword units brought by tokenization of language models, limiting small
perturbations of inputs to form semantics-preserved positive pairs. In this
work, we conceptualize the learning of sentence-level textual semantics as a
visual representation learning process. Drawing from cognitive and linguistic
sciences, we introduce an unsupervised visual sentence representation learning
framework, employing visually-grounded text perturbation methods like typos and
word order shuffling, resonating with human cognitive patterns, and enabling
perturbation to texts to be perceived as continuous. Our approach is further
bolstered by large-scale unsupervised topical alignment training and natural
language inference supervision, achieving comparable performance in semantic
textual similarity (STS) to existing state-of-the-art NLP methods.
Additionally, we unveil our method's inherent zero-shot cross-lingual
transferability and a unique leapfrogging pattern across languages during
iterative training. To our knowledge, this is the first representation learning
method devoid of traditional language models for understanding sentence and
document semantics, marking a stride closer to human-like textual
comprehension. Our code is available at
https://github.com/gowitheflow-1998/Pixel-Linguist
\\ ( https://arxiv.org/abs/2402.08183 ,  235kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08219
Date: Tue, 13 Feb 2024 05:15:46 GMT   (13419kb,D)

Title: BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models
Authors: Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, Bo Dai
Categories: cs.CL cs.AI cs.LG
Comments: 24 pages, 10 figures
\\
  Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini
for specific tasks is challenging. Due to the opacity in their parameters,
embeddings, and even output probabilities, existing fine-tuning adaptation
methods are inapplicable. Consequently, adapting these black-box LLMs is only
possible through their API services, raising concerns about transparency,
privacy, and cost. To address these challenges, we introduce BBox-Adapter, a
novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target
and source domain data by treating target data as positive and source data as
negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to
promote the likelihood of target domain data while penalizing that of the
source domain. Furthermore, it features an online adaptation mechanism, which
incorporates real-time positive data sampling from ground-truth, human, or AI
feedback, coupled with negative data from previous adaptations. Extensive
experiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It
improves model performance by up to 6.77% across diverse tasks and domains,
while reducing training and inference costs by 31.30x and 1.84x, respectively.
\\ ( https://arxiv.org/abs/2402.08219 ,  13419kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08227
Date: Tue, 13 Feb 2024 05:36:54 GMT   (7573kb,D)

Title: Privacy-Preserving Language Model Inference with Instance Obfuscation
Authors: Yixiang Yao, Fei Wang, Srivatsan Ravi, Muhao Chen
Categories: cs.CL
\\
  Language Models as a Service (LMaaS) offers convenient access for developers
and researchers to perform inference using pre-trained language models.
Nonetheless, the input data and the inference results containing private
information are exposed as plaintext during the service call, leading to
privacy issues. Recent studies have started tackling the privacy issue by
transforming input data into privacy-preserving representation from the
user-end with the techniques such as noise addition and content perturbation,
while the exploration of inference result protection, namely decision privacy,
is still a blank page. In order to maintain the black-box manner of LMaaS,
conducting data privacy protection, especially for the decision, is a
challenging task because the process has to be seamless to the models and
accompanied by limited communication and computation overhead. We thus propose
Instance-Obfuscated Inference (IOI) method, which focuses on addressing the
decision privacy issue of natural language understanding tasks in their
complete life-cycle. Besides, we conduct comprehensive experiments to evaluate
the performance as well as the privacy-protection strength of the proposed
method on various benchmarking tasks.
\\ ( https://arxiv.org/abs/2402.08227 ,  7573kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08259
Date: Tue, 13 Feb 2024 07:17:52 GMT   (380kb,D)

Title: A Survey of Table Reasoning with Large Language Models
Authors: Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che
Categories: cs.CL
\\
  Table reasoning, which aims to generate the corresponding answer to the
question following the user requirement according to the provided table, and
optionally a text description of the table, effectively improving the
efficiency of obtaining information. Recently, using Large Language Models
(LLMs) has become the mainstream method for table reasoning, because it not
only significantly reduces the annotation cost but also exceeds the performance
of previous methods. However, existing research still lacks a summary of
LLM-based table reasoning works. Due to the existing lack of research,
questions about which techniques can improve table reasoning performance in the
era of LLMs, why LLMs excel at table reasoning, and how to enhance table
reasoning abilities in the future, remain largely unexplored. This gap
significantly limits progress in research. To answer the above questions and
advance table reasoning research with LLMs, we present this survey to analyze
existing research, inspiring future work. In this paper, we analyze the
mainstream techniques used to improve table reasoning performance in the LLM
era, and the advantages of LLMs compared to pre-LLMs for solving table
reasoning. We provide research directions from both the improvement of existing
methods and the expansion of practical applications to inspire future research.
\\ ( https://arxiv.org/abs/2402.08259 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08277
Date: Tue, 13 Feb 2024 08:12:48 GMT   (8720kb,D)

Title: Towards Faithful and Robust LLM Specialists for Evidence-Based
  Question-Answering
Authors: Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus
  Leippold
Categories: cs.CL cs.LG
\\
  Advances towards more faithful and traceable answers of Large Language Models
(LLMs) are crucial for various research and practical endeavors. One avenue in
reaching this goal is basing the answers on reliable sources. However, this
Evidence-Based QA has proven to work insufficiently with LLMs in terms of
citing the correct sources (source quality) and truthfully representing the
information within sources (answer attributability). In this work, we
systematically investigate how to robustly fine-tune LLMs for better source
quality and answer attributability. Specifically, we introduce a data
generation pipeline with automated data quality filters, which can synthesize
diversified high-quality training and testing data at scale. We further
introduce four test sets to benchmark the robustness of fine-tuned specialist
models. Extensive evaluation shows that fine-tuning on synthetic data improves
performance on both in- and out-of-distribution. %Evidence-Based QA cases.
Furthermore, we show that data quality, which can be drastically improved by
proposed quality filters, matters more than quantity in improving
Evidence-Based QA.
\\ ( https://arxiv.org/abs/2402.08277 ,  8720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08303
Date: Tue, 13 Feb 2024 09:06:14 GMT   (4255kb,D)

Title: ChatCell: Facilitating Single-Cell Analysis with Natural Language
Authors: Yin Fang, Kangwei Liu, Ningyu Zhang, Xinle Deng, Penghui Yang, Zhuo
  Chen, Xiangru Tang, Mark Gerstein, Xiaohui Fan, Huajun Chen
Categories: cs.CL cs.AI cs.CE cs.HC cs.LG
Comments: Ongoing work; 15 pages, 6 Tables, 9 Figures; Project homepage:
  https://zjunlp.github.io/project/ChatCell; Code:
  https://github.com/zjunlp/ChatCell; Dataset:
  https://huggingface.co/datasets/zjunlp/ChatCell-Instructions; Demo:
  https://huggingface.co/spaces/zjunlp/Chatcell
\\
  As Large Language Models (LLMs) rapidly evolve, their influence in science is
becoming increasingly prominent. The emerging capabilities of LLMs in task
generalization and free-form dialogue can significantly advance fields like
chemistry and biology. However, the field of single-cell biology, which forms
the foundational building blocks of living organisms, still faces several
challenges. High knowledge barriers and limited scalability in current methods
restrict the full exploitation of LLMs in mastering single-cell data, impeding
direct accessibility and rapid iteration. To this end, we introduce ChatCell,
which signifies a paradigm shift by facilitating single-cell analysis with
natural language. Leveraging vocabulary adaptation and unified sequence
generation, ChatCell has acquired profound expertise in single-cell biology and
the capability to accommodate a diverse range of analysis tasks. Extensive
experiments further demonstrate ChatCell's robust performance and potential to
deepen single-cell insights, paving the way for more accessible and intuitive
exploration in this pivotal field. Our project homepage is available at
https://zjunlp.github.io/project/ChatCell.
\\ ( https://arxiv.org/abs/2402.08303 ,  4255kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08318
Date: Tue, 13 Feb 2024 09:26:19 GMT   (3179kb,D)

Title: Explicit References to Social Values in Fairy Tales: A Comparison
  between Three European Cultures
Authors: Alba Morollon Diaz-Faes, Carla Sofia Ribeiro Murteira, Martin Ruskov
Categories: cs.CL cs.CY
Comments: In Proceedings of the Joint 3rd International Conference on Natural
  Language Processing for Digital Humanities and 8th International Workshop on
  Computational Linguistics for Uralic Languages
ACM-class: J.5; K.4.m
\\
  The study of social values in fairy tales opens the possibility to learn
about the communication of values across space and time. We propose to study
the communication of values in fairy tales from Portugal, Italy and Germany
using a technique called word embedding with a compass to quantify vocabulary
differences and commonalities. We study how these three national traditions of
fairy tales differ in their explicit references to values. To do this, we
specify a list of value-charged tokens, consider their word stems and analyse
the distance between these in a bespoke pre-trained Word2Vec model. We
triangulate and critically discuss the validity of the resulting hypotheses
emerging from this quantitative model. Our claim is that this is a reusable and
reproducible method for the study of the values explicitly referenced in
historical corpora. Finally, our preliminary findings hint at a shared cultural
understanding and the expression of values such as Benevolence, Conformity, and
Universalism across European societies, suggesting the existence of a
pan-European cultural memory.
\\ ( https://arxiv.org/abs/2402.08318 ,  3179kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08327
Date: Tue, 13 Feb 2024 09:47:07 GMT   (4726kb,D)

Title: PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers
Authors: Weizhe Lin, Jingbiao Mei, Jinghong Chen, Bill Byrne
Categories: cs.CL
Comments: 8 pages
\\
  Large Multimodal Models (LMMs) excel in natural language and visual
understanding but are challenged by exacting tasks such as Knowledge-based
Visual Question Answering (KB-VQA) which involve the retrieval of relevant
information from document collections to use in shaping answers to questions.
We present an extensive training and evaluation framework, M2KR, for KB-VQA.
M2KR contains a collection of vision and language tasks which we have
incorporated into a single suite of benchmark tasks for training and evaluating
general-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a
pre-trained version of the recently developed Fine-grained Late-interaction
Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new
state-of-the-art results across a range of tasks. We also present
investigations into the scaling behaviors of PreFLMR intended to be useful in
future developments in general-purpose multi-modal retrievers.
\\ ( https://arxiv.org/abs/2402.08327 ,  4726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08341
Date: Tue, 13 Feb 2024 10:09:00 GMT   (41375kb,D)

Title: Eliciting Big Five Personality Traits in Large Language Models: A
  Textual Analysis with Classifier-Driven Approach
Authors: Airlie Hilliard, Cristian Munoz, Zekun Wu and Adriano Soares Koshiyama
Categories: cs.CL cs.AI
Comments: Manuscript submitted to ACM Facct. Authors One and Two contributed
  equally to this work
\\
  Large Language Models (LLMs) are increasingly being utilized by both
candidates and employers in the recruitment context. However, with this comes
numerous ethical concerns, particularly related to the lack of transparency in
these "black-box" models. Although previous studies have sought to increase the
transparency of these models by investigating the personality traits of LLMs,
many of the previous studies have provided them with personality assessments to
complete. On the other hand, this study seeks to obtain a better understanding
of such models by examining their output variations based on different input
prompts. Specifically, we use a novel elicitation approach using prompts
derived from common interview questions, as well as prompts designed to elicit
particular Big Five personality traits to examine whether the models were
susceptible to trait-activation like humans are, to measure their personality
based on the language used in their outputs. To do so, we repeatedly prompted
multiple LMs with different parameter sizes, including Llama-2, Falcon,
Mistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined
their personality using classifiers trained on the myPersonality dataset. Our
results reveal that, generally, all LLMs demonstrate high openness and low
extraversion. However, whereas LMs with fewer parameters exhibit similar
behaviour in personality traits, newer and LMs with more parameters exhibit a
broader range of personality traits, with increased agreeableness, emotional
stability, and openness. Furthermore, a greater number of parameters is
positively associated with openness and conscientiousness. Moreover, fine-tuned
models exhibit minor modulations in their personality traits, contingent on the
dataset. Implications and directions for future research are discussed.
\\ ( https://arxiv.org/abs/2402.08341 ,  41375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08382
Date: Tue, 13 Feb 2024 11:22:52 GMT   (168kb,D)

Title: Punctuation Restoration Improves Structure Understanding without
  Supervision
Authors: Junghyun Min, Minho Lee, Woochul Lee, Yeonsoo Lee
Categories: cs.CL
Comments: 10 pages, 1 figure
\\
  Unsupervised learning objectives like language modeling and de-noising
constitute a significant part in producing pre-trained models that perform
various downstream applications from natural language understanding to
conversational tasks. However, despite impressive conversational capabilities
of recent large language model, their abilities to capture syntactic or
semantic structure within text lag behind. We hypothesize that the mismatch
between linguistic performance and competence in machines is attributable to
insufficient transfer of linguistic structure knowledge to computational
systems with currently popular pre-training objectives. We show that
punctuation restoration transfers to improvements in in- and
out-of-distribution performance on structure-related tasks like named entity
recognition, open information extraction, chunking, and part-of-speech tagging.
Punctuation restoration is an effective learning objective that can improve
structure understanding and yield a more robust structure-aware representations
of natural language.
\\ ( https://arxiv.org/abs/2402.08382 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08392
Date: Tue, 13 Feb 2024 11:37:30 GMT   (852kb,D)

Title: Large Language Models as Minecraft Agents
Authors: Chris Madge and Massimo Poesio
Categories: cs.CL
\\
  In this work we examine the use of Large Language Models (LLMs) in the
challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs
in the builder and architect settings, introduce clarification questions and
examining the challenges and opportunities for improvement. In addition, we
present a platform for online interaction with the agents and an evaluation
against previous works.
\\ ( https://arxiv.org/abs/2402.08392 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08403
Date: Tue, 13 Feb 2024 12:04:43 GMT   (16kb,D)

Title: LLMs and the Human Condition
Authors: Peter Wallis
Categories: cs.CL
Comments: A draft paper for circulation. target is CUI or IVA in 2024
\\
  This paper presents three established theories of human decision-making and
describes how they can be integrated to provide a model of purposive human
action. Taking seriously the idea of language as action the model is then
applied to the conversational user interfaces. Theory based AI research has had
a hard time recently and the aim here is to revitalise interest in
understanding what LLMs are actually doing other than running poorly understood
machine learning routines over all the data the relevant Big Tech company can
hoover up. When a raspberry pi computer for under 50USD is up to 400 times
faster than the first commercial Cray super computer~\cite{crayVpi}, Big Tech
can get really close to having an infinite number of monkeys typing at random
and producing text, some of which will make sense. By understanding where
ChatGPT's apparent intelligence comes from, perhaps we can perform the magic
with fewer resources and at the same time gain some understanding about our
relationship with our world.
\\ ( https://arxiv.org/abs/2402.08403 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08467
Date: Tue, 13 Feb 2024 13:50:08 GMT   (32kb,D)

Title: Lying Blindly: Bypassing ChatGPT's Safeguards to Generate Hard-to-Detect
  Disinformation Claims at Scale
Authors: Freddy Heppell, Mehmet E. Bakir, Kalina Bontcheva
Categories: cs.CL
\\
  As Large Language Models (LLMs) become more proficient, their misuse in
large-scale viral disinformation campaigns is a growing concern. This study
explores the capability of ChatGPT to generate unconditioned claims about the
war in Ukraine, an event beyond its knowledge cutoff, and evaluates whether
such claims can be differentiated by human readers and automated tools from
human-written ones. We compare war-related claims from ClaimReview, authored by
IFCN-registered fact-checkers, and similar short-form content generated by
ChatGPT. We demonstrate that ChatGPT can produce realistic, target-specific
disinformation cheaply, fast, and at scale, and that these claims cannot be
reliably distinguished by humans or existing automated tools.
\\ ( https://arxiv.org/abs/2402.08467 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08479
Date: Tue, 13 Feb 2024 14:12:32 GMT   (887kb,D)

Title: Plausible Extractive Rationalization through Semi-Supervised Entailment
  Signal
Authors: Yeo Wei Jie, Ranjan Satapathy, Erik Cambria
Categories: cs.CL
\\
  The increasing use of complex and opaque black box models requires the
adoption of interpretable measures, one such option is extractive rationalizing
models, which serve as a more interpretable alternative. These models, also
known as Explain-Then-Predict models, employ an explainer model to extract
rationales and subsequently condition the predictor with the extracted
information. Their primary objective is to provide precise and faithful
explanations, represented by the extracted rationales. In this paper, we take a
semi-supervised approach to optimize for the plausibility of extracted
rationales. We adopt a pre-trained natural language inference (NLI) model and
further fine-tune it on a small set of supervised rationales ($10\%$). The NLI
predictor is leveraged as a source of supervisory signals to the explainer via
entailment alignment. We show that, by enforcing the alignment agreement
between the explanation and answer in a question-answering task, the
performance can be improved without access to ground truth labels. We evaluate
our approach on the ERASER dataset and show that our approach achieves
comparable results with supervised extractive models and outperforms
unsupervised approaches by $> 100\%$.
\\ ( https://arxiv.org/abs/2402.08479 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08496
Date: Tue, 13 Feb 2024 14:51:45 GMT   (959kb,D)

Title: A Systematic Review of Data-to-Text NLG
Authors: Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis
Categories: cs.CL cs.AI cs.LG
\\
  This systematic review aims to provide a comprehensive analysis of the state
of data-to-text generation research, focusing on identifying research gaps,
offering future directions, and addressing challenges found during the review.
We thoroughly examined the literature, including approaches, datasets,
evaluation metrics, applications, multilingualism, and hallucination mitigation
measures. Our review provides a roadmap for future research in this rapidly
evolving field.
\\ ( https://arxiv.org/abs/2402.08496 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08498
Date: Tue, 13 Feb 2024 14:53:12 GMT   (1868kb)

Title: Auditing Counterfire: Evaluating Advanced Counterargument Generation
  with Evidence and Style
Authors: Preetika Verma, Kokil Jaidka, Svetlana Churina
Categories: cs.CL
Comments: 17 pages, 10 figures, 9 tables
\\
  We present a novel dataset for the controlled composition of counterarguments
designed for further applications in argument refining, mining, and evaluation.
Our dataset constitutes enriched counter-arguments to posts in the Reddit
ChangeMyView dataset that are integrated with evidence retrieved from
high-quality sources and generated based on user preferences, adjusting the
critical attributes of evidence and argument style. The resultant Counterfire
corpus comprises arguments generated from GPT-3.5 turbo, Koala, and PaLM 2
models and two of their finetuned variants (N = 32,000). Model evaluation
indicates strong paraphrasing abilities with evidence, albeit limited word
overlap, while demonstrating high style integration (0.9682 for 'reciprocity'),
showing the ability of LLM to assimilate diverse styles. Of all models, GPT-3.5
turbo showed the highest scores in argument quality evaluation, showing
consistent accuracy (score >0.8). In further analyses, reciprocity-style
counterarguments display higher counts in most categories, possibly indicating
a more creatively persuasive use of evidence. In contrast, human-written
counterarguments exhibited greater argumentative richness and diversity across
categories. Despite human-written arguments being favored as the most
persuasive in human evaluation, the 'No Style' generated text surprisingly
exhibited the highest score, prompting further exploration and investigation on
the trade-offs in generation for facts and style.
\\ ( https://arxiv.org/abs/2402.08498 ,  1868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08562
Date: Tue, 13 Feb 2024 16:04:21 GMT   (563kb,D)

Title: Higher Layers Need More LoRA Experts
Authors: Chongyang Gao and Kezhen Chen and Jinmeng Rao and Baochen Sun and
  Ruibo Liu and Daiyi Peng and Yawen Zhang and Xiaoyuan Guo and Jie Yang and VS
  Subrahmanian
Categories: cs.CL cs.AI
Comments: The code is available at https://github.com/GCYZSL/MoLA
\\
  Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA)
offer training efficiency on Large Language Models, but their impact on model
performance remains limited. Recent efforts integrate LoRA and
Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite
promising results, research on improving the efficiency of LoRA with MoE is
still in its early stages. Recent studies have shown that experts in the MoE
architecture have different strengths and also exhibit some redundancy. Does
this statement also apply to parameter-efficient MoE? In this paper, we
introduce a novel parameter-efficient MoE method,
\textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert
\textbf{A}llocation (MoLA)} for Transformer-based models, where each model
layer has the flexibility to employ a varying number of LoRA experts. We
investigate several architectures with varying layer-wise expert
configurations. Experiments on six well-known NLP and commonsense QA benchmarks
demonstrate that MoLA achieves equal or superior performance compared to all
baselines. We find that allocating more LoRA experts to higher layers further
enhances the effectiveness of models with a certain number of experts in total.
With much fewer parameters, this allocation strategy outperforms the setting
with the same number of experts in every layer. This work can be widely used as
a plug-and-play parameter-efficient tuning approach for various applications.
The code is available at https://github.com/GCYZSL/MoLA.
\\ ( https://arxiv.org/abs/2402.08562 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08567
Date: Tue, 13 Feb 2024 16:06:17 GMT   (9500kb,D)

Title: Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM
  Agents Exponentially Fast
Authors: Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang,
  Jing Jiang, Min Lin
Categories: cs.CL cs.CR cs.CV cs.LG cs.MA
\\
  A multimodal large language model (MLLM) agent can receive instructions,
capture images, retrieve histories from memory, and decide which tools to use.
Nonetheless, red-teaming efforts have revealed that adversarial images/prompts
can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an
even more severe safety issue in multi-agent environments, referred to as
infectious jailbreak. It entails the adversary simply jailbreaking a single
agent, and without any further intervention from the adversary, (almost) all
agents will become infected exponentially fast and exhibit harmful behaviors.
To validate the feasibility of infectious jailbreak, we simulate multi-agent
environments containing up to one million LLaVA-1.5 agents, and employ
randomized pair-wise chat as a proof-of-concept instantiation for multi-agent
interaction. Our results show that feeding an (infectious) adversarial image
into the memory of any randomly chosen agent is sufficient to achieve
infectious jailbreak. Finally, we derive a simple principle for determining
whether a defense mechanism can provably restrain the spread of infectious
jailbreak, but how to design a practical defense that meets this principle
remains an open question to investigate. Our project page is available at
https://sail-sg.github.io/Agent-Smith/.
\\ ( https://arxiv.org/abs/2402.08567 ,  9500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08577
Date: Tue, 13 Feb 2024 16:28:28 GMT   (8708kb,D)

Title: Test-Time Backdoor Attacks on Multimodal Large Language Models
Authors: Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin
Categories: cs.CL cs.CR cs.CV cs.LG cs.MM
\\
  Backdoor attacks are commonly executed by contaminating training data, such
that a trigger can activate predetermined harmful effects during the test
phase. In this work, we present AnyDoor, a test-time backdoor attack against
multimodal large language models (MLLMs), which involves injecting the backdoor
into the textual modality using adversarial test images (sharing the same
universal perturbation), without requiring access to or modification of the
training data. AnyDoor employs similar techniques used in universal adversarial
attacks, but distinguishes itself by its ability to decouple the timing of
setup and activation of harmful effects. In our experiments, we validate the
effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4,
InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies.
Notably, because the backdoor is injected by a universal perturbation, AnyDoor
can dynamically change its backdoor trigger prompts/harmful effects, exposing a
new challenge for defending against backdoor attacks. Our project page is
available at https://sail-sg.github.io/AnyDoor/.
\\ ( https://arxiv.org/abs/2402.08577 ,  8708kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08581
Date: Tue, 13 Feb 2024 16:35:48 GMT   (1361kb,D)

Title: Improving Factual Error Correction for Abstractive Summarization via
  Data Distillation and Conditional-generation Cloze
Authors: Yiyang Li and Lei Li and Dingxin Hu and Xueyi Hao and Marina Litvak
  and Natalia Vanetik and Yanquan Zhou
Categories: cs.CL
Comments: manuscript
\\
  Improving factual consistency in abstractive summarization has been a focus
of current research. One promising approach is the post-editing method.
However, previous works have yet to make sufficient use of factual factors in
summaries and suffers from the negative effect of the training datasets. In
this paper, we first propose a novel factual error correction model FactCloze
based on a conditional-generation cloze task. FactCloze can construct the
causality among factual factors while being able to determine whether the blank
can be answered or not. Then, we propose a data distillation method to generate
a more faithful summarization dataset SummDSC via multiple-dimensional
evaluation. We experimentally validate the effectiveness of our approach, which
leads to an improvement in multiple factual consistency metrics compared to
baselines.
\\ ( https://arxiv.org/abs/2402.08581 ,  1361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08594
Date: Tue, 13 Feb 2024 16:57:02 GMT   (1882kb,D)

Title: Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning
Authors: Haeju Lee, Minchan Jeong, Se-Young Yun, Kee-Eung Kim
Categories: cs.CL
Comments: The first two authors equally contributed to this work. Findings of
  EMNLP 2023
DOI: 10.18653/v1/2023.findings-emnlp.329
\\
  Prompt tuning, in which prompts are optimized to adapt large-scale
pre-trained language models to downstream tasks instead of fine-tuning the full
model parameters, has been shown to be particularly effective when the prompts
are trained in a multi-task transfer learning setting. These methods generally
involve individually training prompts for each source task and then aggregating
them to provide the initialization of the prompt for the target task. However,
this approach critically ignores the fact that some of the source tasks could
be negatively or positively interfering with each other. We argue that when we
extract knowledge from source tasks via training source prompts, we need to
consider this correlation among source tasks for better transfer to target
tasks. To this end, we propose a Bayesian approach where we work with the
posterior distribution of prompts across source tasks. We obtain representative
source prompts corresponding to the samples from the posterior utilizing Stein
Variational Gradient Descent, which are then aggregated to constitute the
initial target prompt. We show extensive experimental results on the standard
benchmark NLP tasks, where our Bayesian multi-task transfer learning approach
outperforms the state-of-the-art methods in many settings. Furthermore, our
approach requires no auxiliary models other than the prompt itself, achieving a
high degree of parameter efficiency.
\\ ( https://arxiv.org/abs/2402.08594 ,  1882kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08631
Date: Tue, 13 Feb 2024 17:59:34 GMT   (8530kb,D)

Title: Knowledge Editing on Black-box Large Language Models
Authors: Xiaoshuai Song, Zhengyang Wang, Keqing He, Guanting Dong, Jinxu Zhao,
  Weiran Xu
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\
  Knowledge editing (KE) aims to efficiently and precisely modify the behavior
of large language models (LLMs) to update specific knowledge without negatively
influencing other knowledge. Current research primarily focuses on white-box
LLMs editing, overlooking an important scenario: black-box LLMs editing, where
LLMs are accessed through interfaces and only textual output is available. To
address the limitations of existing evaluations that are not inapplicable to
black-box LLM editing and lack comprehensiveness, we propose a
multi-perspective evaluation framework, incorporating the assessment of style
retention for the first time. To tackle privacy leaks of editing data and style
over-editing in current methods, we introduce a novel postEdit framework,
resolving privacy concerns through downstream post-processing and maintaining
textual style consistency via fine-grained editing to original responses.
Experiments and analysis on two benchmarks demonstrate that postEdit
outperforms all baselines and achieves strong generalization, especially with
huge improvements on style retention (average $+20.82\%\uparrow$).
\\ ( https://arxiv.org/abs/2402.08631 ,  8530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08638
Date: Tue, 13 Feb 2024 18:04:53 GMT   (7854kb,D)

Title: SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14
  Languages
Authors: Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris
  Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir
  Araujo, Abinew Ali Ayele, Pavan Baswani, Meriem Beloucif, Chris Biemann,
  Sofia Bourhim, Christine De Kock, Genet Shanko Dekebo, Oumaima Hourrane,
  Gopichand Kanumolu, Lokesh Madasu, Samuel Rutunda, Manish Shrivastava, Thamar
  Solorio, Nirmal Surange, Hailegnaw Getaneh Tilaye, Krishnapriya Vishnubhotla,
  Genta Winata, Seid Muhie Yimam, Saif M. Mohammad
Categories: cs.CL
\\
  Exploring and quantifying semantic relatedness is central to representing
language. It holds significant implications across various NLP tasks, including
offering insights into the capabilities and performance of Large Language
Models (LLMs). While earlier NLP research primarily focused on semantic
similarity, often within the English language context, we instead investigate
the broader phenomenon of semantic relatedness. In this paper, we present
SemRel, a new semantic relatedness dataset collection annotated by native
speakers across 14 languages:Afrikaans, Algerian Arabic, Amharic, English,
Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern
Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from
five distinct language families and are predominantly spoken in Africa and Asia
-- regions characterised by a relatively limited availability of NLP resources.
Each instance in the SemRel datasets is a sentence pair associated with a score
that represents the degree of semantic textual relatedness between the two
sentences. The scores are obtained using a comparative annotation framework. We
describe the data collection and annotation processes, related challenges when
building the datasets, and their impact and utility in NLP. We further report
experiments for each language and across the different languages.
\\ ( https://arxiv.org/abs/2402.08638 ,  7854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08666
Date: Tue, 13 Feb 2024 18:48:23 GMT   (7918kb,D)

Title: Improving Generalization in Semantic Parsing by Increasing Natural
  Language Variation
Authors: Irina Saparina and Mirella Lapata
Categories: cs.CL
Comments: EACL 2024
\\
  Text-to-SQL semantic parsing has made significant progress in recent years,
with various models demonstrating impressive performance on the challenging
Spider benchmark. However, it has also been shown that these models often
struggle to generalize even when faced with small perturbations of previously
(accurately) parsed expressions. This is mainly due to the linguistic form of
questions in Spider which are overly specific, unnatural, and display limited
variation. In this work, we use data augmentation to enhance the robustness of
text-to-SQL parsers against natural language variations. Existing approaches
generate question reformulations either via models trained on Spider or only
introduce local changes. In contrast, we leverage the capabilities of large
language models to generate more realistic and diverse questions. Using only a
few prompts, we achieve a two-fold increase in the number of questions in
Spider. Training on this augmented dataset yields substantial improvements on a
range of evaluation sets, including robustness benchmarks and out-of-domain
data.
\\ ( https://arxiv.org/abs/2402.08666 ,  7918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07999
Date: Mon, 12 Feb 2024 19:04:32 GMT   (451kb,D)

Title: NetInfoF Framework: Measuring and Exploiting Network Usable Information
Authors: Meng-Chieh Lee, Haiyang Yu, Jian Zhang, Vassilis N. Ioannidis, Xiang
  Song, Soji Adeshina, Da Zheng, Christos Faloutsos
Categories: cs.LG cs.SI
Comments: Accepted to ICLR 2024 (Spotlight)
\\
  Given a node-attributed graph, and a graph task (link prediction or node
classification), can we tell if a graph neural network (GNN) will perform well?
More specifically, do the graph structure and the node features carry enough
usable information for the task? Our goals are (1) to develop a fast tool to
measure how much information is in the graph structure and in the node
features, and (2) to exploit the information to solve the task, if there is
enough. We propose NetInfoF, a framework including NetInfoF_Probe and
NetInfoF_Act, for the measurement and the exploitation of network usable
information (NUI), respectively. Given a graph data, NetInfoF_Probe measures
NUI without any model training, and NetInfoF_Act solves link prediction and
node classification, while two modules share the same backbone. In summary,
NetInfoF has following notable advantages: (a) General, handling both link
prediction and node classification; (b) Principled, with theoretical guarantee
and closed-form solution; (c) Effective, thanks to the proposed adjustment to
node similarity; (d) Scalable, scaling linearly with the input size. In our
carefully designed synthetic datasets, NetInfoF correctly identifies the ground
truth of NUI and is the only method being robust to all graph scenarios.
Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link
prediction compared to general GNN baselines.
\\ ( https://arxiv.org/abs/2402.07999 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08010
Date: Mon, 12 Feb 2024 19:18:50 GMT   (506kb,D)

Title: Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature
  Learning
Authors: Yuxiao Wen, Arthur Jacot
Categories: cs.LG cs.AI stat.ML
\\
  We describe the emergence of a Convolution Bottleneck (CBN) structure in
CNNs, where the network uses its first few layers to transform the input
representation into a representation that is supported only along a few
frequencies and channels, before using the last few layers to map back to the
outputs. We define the CBN rank, which describes the number and type of
frequencies that are kept inside the bottleneck, and partially prove that the
parameter norm required to represent a function $f$ scales as depth times the
CBN rank $f$. We also show that the parameter norm depends at next order on the
regularity of $f$. We show that any network with almost optimal parameter norm
will exhibit a CBN structure in both the weights and - under the assumption
that the network is stable under large learning rate - the activations, which
motivates the common practice of down-sampling; and we verify that the CBN
results still hold with down-sampling. Finally we use the CBN structure to
interpret the functions learned by CNNs on a number of tasks.
\\ ( https://arxiv.org/abs/2402.08010 ,  506kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08018
Date: Mon, 12 Feb 2024 19:27:30 GMT   (5178kb,D)

Title: Nearest Neighbour Score Estimators for Diffusion Generative Models
Authors: Matthew Niedoba, Dylan Green, Saeid Naderiparizi, Vasileios Lioutas,
  Jonathan Wilder Lavington, Xiaoxuan Liang, Yunpeng Liu, Ke Zhang, Setareh
  Dabiri, Adam \'Scibior, Berend Zwartsenberg, Frank Wood
Categories: cs.LG cs.CV stat.ML
Comments: 25 pages, 9 figures
\\
  Score function estimation is the cornerstone of both training and sampling
from diffusion generative models. Despite this fact, the most commonly used
estimators are either biased neural network approximations or high variance
Monte Carlo estimators based on the conditional score. We introduce a novel
nearest neighbour score function estimator which utilizes multiple samples from
the training set to dramatically decrease estimator variance. We leverage our
low variance estimator in two compelling applications. Training consistency
models with our estimator, we report a significant increase in both convergence
speed and sample quality. In diffusion models, we show that our estimator can
replace a learned network for probability-flow ODE integration, opening
promising new avenues of future research.
\\ ( https://arxiv.org/abs/2402.08018 ,  5178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08022
Date: Mon, 12 Feb 2024 19:39:07 GMT   (1017kb,D)

Title: Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale
  Wireless Networks
Authors: Talha Bozkus, Urbashi Mitra
Categories: cs.LG cs.NI eess.SP
\\
  Optimizing large-scale wireless networks, including optimal resource
management, power allocation, and throughput maximization, is inherently
challenging due to their non-observable system dynamics and heterogeneous and
complex nature. Herein, a novel ensemble Q-learning algorithm that addresses
the performance and complexity challenges of the traditional Q-learning
algorithm for optimizing wireless networks is presented. Ensemble learning with
synthetic Markov Decision Processes is tailored to wireless networks via new
models for approximating large state-space observable wireless networks. In
particular, digital cousins are proposed as an extension of the traditional
digital twin concept wherein multiple Q-learning algorithms on multiple
synthetic Markovian environments are run in parallel and their outputs are
fused into a single Q-function. Convergence analyses of key statistics and
Q-functions and derivations of upper bounds on the estimation bias and variance
are provided. Numerical results across a variety of real-world wireless
networks show that the proposed algorithm can achieve up to 50% less average
policy error with up to 40% less runtime complexity than the state-of-the-art
reinforcement learning algorithms. It is also shown that theoretical results
properly predict trends in the experimental results.
\\ ( https://arxiv.org/abs/2402.08022 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08023
Date: Mon, 12 Feb 2024 19:39:26 GMT   (4215kb,D)

Title: UGMAE: A Unified Framework for Graph Masked Autoencoders
Authors: Yijun Tian, Chuxu Zhang, Ziyi Kou, Zheyuan Liu, Xiangliang Zhang,
  Nitesh V. Chawla
Categories: cs.LG cs.AI
\\
  Generative self-supervised learning on graphs, particularly graph masked
autoencoders, has emerged as a popular learning paradigm and demonstrated its
efficacy in handling non-Euclidean data. However, several remaining issues
limit the capability of existing methods: 1) the disregard of uneven node
significance in masking, 2) the underutilization of holistic graph information,
3) the ignorance of semantic knowledge in the representation space due to the
exclusive use of reconstruction loss in the output space, and 4) the unstable
reconstructions caused by the large volume of masked contents. In light of
this, we propose UGMAE, a unified framework for graph masked autoencoders to
address these issues from the perspectives of adaptivity, integrity,
complementarity, and consistency. Specifically, we first develop an adaptive
feature mask generator to account for the unique significance of nodes and
sample informative masks (adaptivity). We then design a ranking-based structure
reconstruction objective joint with feature reconstruction to capture holistic
graph information and emphasize the topological proximity between neighbors
(integrity). After that, we present a bootstrapping-based similarity module to
encode the high-level semantic knowledge in the representation space,
complementary to the low-level reconstruction in the output space
(complementarity). Finally, we build a consistency assurance module to provide
reconstruction objectives with extra stabilized consistency targets
(consistency). Extensive experiments demonstrate that UGMAE outperforms both
contrastive and generative state-of-the-art baselines on several tasks across
multiple datasets.
\\ ( https://arxiv.org/abs/2402.08023 ,  4215kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08056
Date: Mon, 12 Feb 2024 20:46:47 GMT   (2773kb,D)

Title: MIML library: a Modular and Flexible Library for Multi-instance
  Multi-label Learning
Authors: \'Alvaro Belmonte and Amelia Zafra and Eva Gibaja
Categories: cs.LG
DOI: 10.1016/j.neucom.2022.05.068
\\
  MIML library is a Java software tool to develop, test, and compare
classification algorithms for multi-instance multi-label (MIML) learning. The
library includes 43 algorithms and provides a specific format and facilities
for data managing and partitioning, holdout and cross-validation methods,
standard metrics for performance evaluation, and generation of reports. In
addition, algorithms can be executed through $xml$ configuration files without
needing to program. It is platform-independent, extensible, free, open-source,
and available on GitHub under the GNU General Public License.
\\ ( https://arxiv.org/abs/2402.08056 ,  2773kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08062
Date: Mon, 12 Feb 2024 21:12:11 GMT   (50kb)

Title: Avoiding Catastrophe in Continuous Spaces by Asking for Help
Authors: Benjamin Plaut, Hanlin Zhu, Stuart Russell
Categories: cs.LG cs.AI
\\
  Most reinforcement learning algorithms with formal regret guarantees assume
all mistakes are reversible and rely on essentially trying all possible
options. This approach leads to poor outcomes when some mistakes are
irreparable or even catastrophic. We propose a variant of the contextual bandit
problem where the goal is to minimize the chance of catastrophe. Specifically,
we assume that the payoff each round represents the chance of avoiding
catastrophe that round, and try to maximize the product of payoffs (the overall
chance of avoiding catastrophe). To give the agent some chance of success, we
allow a limited number of queries to a mentor and assume a Lipschitz continuous
payoff function. We present an algorithm whose regret and rate of querying the
mentor both approach 0 as the time horizon grows, assuming a continuous 1D
state space and a relatively "simple" payoff function. We also provide a
matching lower bound: without the simplicity assumption: any algorithm either
constantly asks for help or is nearly guaranteed to cause catastrophe. Finally,
we identify the key obstacle to generalizing our algorithm to a
multi-dimensional state space.
\\ ( https://arxiv.org/abs/2402.08062 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08073
Date: Mon, 12 Feb 2024 21:32:49 GMT   (511kb,D)

Title: Grounding Data Science Code Generation with Input-Output Specifications
Authors: Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat
  Chaudhuri, Alex Polozov
Categories: cs.LG cs.PL cs.SE
\\
  Large language models (LLMs) have recently demonstrated a remarkable ability
to generate code from natural language (NL) prompts. However, in the real
world, NL is often too ambiguous to capture the true intent behind programming
problems, requiring additional input-output (I/O) specifications.
Unfortunately, LLMs can have difficulty aligning their outputs with both the NL
prompt and the I/O specification. In this paper, we give a way to mitigate this
issue in the context of data science programming, where tasks require explicit
I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel
approach for the instruction fine-tuning of LLMs with respect to I/O
specifications. Our method leverages synthetic data produced by the LLM itself
and utilizes execution-derived feedback as a key learning signal. This
feedback, in the form of program I/O specifications, is provided to the LLM to
facilitate instruction fine-tuning. We evaluated our approach on two
challenging data science benchmarks, Arcade and DS-1000. The results
demonstrate a significant improvement in the LLM's ability to generate code
that is not only executable but also accurately aligned with user
specifications, substantially improving the quality of code generation for
complex data science tasks.
\\ ( https://arxiv.org/abs/2402.08073 ,  511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08085
Date: Mon, 12 Feb 2024 22:06:37 GMT   (919kb,D)

Title: Message Detouring: A Simple Yet Effective Cycle Representation for
  Expressive Graph Learning
Authors: Ziquan Wei, Tingting Dan, Guorong Wu
Categories: cs.LG cs.AI cs.CG
Comments: 16 pages, 5 figures
\\
  Graph learning is crucial in the fields of bioinformatics, social networks,
and chemicals. Although high-order graphlets, such as cycles, are critical to
achieving an informative graph representation for node classification, edge
prediction, and graph recognition, modeling high-order topological
characteristics poses significant computational challenges, restricting its
widespread applications in machine learning. To address this limitation, we
introduce the concept of \textit{message detouring} to hierarchically
characterize cycle representation throughout the entire graph, which
capitalizes on the contrast between the shortest and longest pathways within a
range of local topologies associated with each graph node. The topological
feature representations derived from our message detouring landscape
demonstrate comparable expressive power to high-order
\textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In
addition to the integration with graph kernel and message passing neural
networks, we present a novel message detouring neural network, which uses
Transformer backbone to integrate cycle representations across nodes and edges.
Aside from theoretical results, experimental results on expressiveness, graph
classification, and node classification show message detouring can
significantly outperform current counterpart approaches on various benchmark
datasets.
\\ ( https://arxiv.org/abs/2402.08085 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08086
Date: Mon, 12 Feb 2024 22:07:43 GMT   (23832kb,D)

Title: Text-centric Alignment for Multi-Modality Learning
Authors: Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, Shou-De Lin
Categories: cs.LG cs.CL cs.CV
\\
  This research paper addresses the challenge of modality mismatch in
multimodal learning, where the modalities available during inference differ
from those available at training. We propose the Text-centric Alignment for
Multi-Modality Learning (TAMML) approach, an innovative method that utilizes
Large Language Models (LLMs) with in-context learning and foundation models to
enhance the generalizability of multimodal systems under these conditions. By
leveraging the unique properties of text as a unified semantic space, TAMML
demonstrates significant improvements in handling unseen, diverse, and
unpredictable modality combinations. TAMML not only adapts to varying
modalities but also maintains robust performance, showcasing the potential of
foundation models in overcoming the limitations of traditional fixed-modality
frameworks in embedding representations. This study contributes to the field by
offering a flexible, effective solution for real-world applications where
modality availability is dynamic and uncertain.
\\ ( https://arxiv.org/abs/2402.08086 ,  23832kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08090
Date: Mon, 12 Feb 2024 22:17:28 GMT   (3369kb,D)

Title: Learning Neural Contracting Dynamics: Extended Linearization and Global
  Guarantees
Authors: Sean Jaffe and Alexander Davydov and Deniz Lapsekili and Ambuj singh
  and Francesco Bullo
Categories: cs.LG math.OC
Comments: 9 pages, 3 figures. Under Review
\\
  Global stability and robustness guarantees in learned dynamical systems are
essential to ensure well-behavedness of the systems in the face of uncertainty.
We present Extended Linearized Contracting Dynamics (ELCD), the first neural
network-based dynamical system with global contractivity guarantees in
arbitrary metrics. The key feature of ELCD is a parametrization of the extended
linearization of the nonlinear vector field. In its most basic form, ELCD is
guaranteed to be (i) globally exponentially stable, (ii) equilibrium
contracting, and (iii) globally contracting with respect to some metric. To
allow for contraction with respect to more general metrics in the data space,
we train diffeomorphisms between the data space and a latent space and enforce
contractivity in the latent space, which ensures global contractivity in the
data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D
LASA datasets.
\\ ( https://arxiv.org/abs/2402.08090 ,  3369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08093
Date: Mon, 12 Feb 2024 22:21:30 GMT   (2424kb,D)

Title: BASE TTS: Lessons from building a billion-parameter Text-to-Speech model
  on 100K hours of data
Authors: Mateusz {\L}ajszczak, Guillermo C\'ambara, Yang Li, Fatih Beyhan,
  Arent van Korlaar, Fan Yang, Arnaud Joly, \'Alvaro Mart\'in-Cortinas, Ammar
  Abbas, Adam Michalski, Alexis Moinet, Sri Karlapati, Ewa Muszy\'nska, Haohan
  Guo, Bartosz Putrycz, Soledad L\'opez Gambino, Kayeon Yoo, Elena Sokolova,
  Thomas Drugman
Categories: cs.LG cs.CL eess.AS
Comments: v1
\\
  We introduce a text-to-speech (TTS) model called BASE TTS, which stands for
$\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with
$\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date,
trained on 100K hours of public domain speech data, achieving a new
state-of-the-art in speech naturalness. It deploys a 1-billion-parameter
autoregressive Transformer that converts raw texts into discrete codes
("speechcodes") followed by a convolution-based decoder which converts these
speechcodes into waveforms in an incremental, streamable manner. Further, our
speechcodes are built using a novel speech tokenization technique that features
speaker ID disentanglement and compression with byte-pair encoding. Echoing the
widely-reported "emergent abilities" of large language models when trained on
increasing volume of data, we show that BASE TTS variants built with 10K+ hours
and 500M+ parameters begin to demonstrate natural prosody on textually complex
sentences. We design and share a specialized dataset to measure these emergent
abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE
TTS by evaluating against baselines that include publicly available large-scale
text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated
by the model can be heard at https://amazon-ltts-paper.com/.
\\ ( https://arxiv.org/abs/2402.08093 ,  2424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08096
Date: Mon, 12 Feb 2024 22:32:12 GMT   (1845kb,D)

Title: Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?
Authors: Andrew Bai, Chih-Kuan Yeh, Cho-Jui Hsieh, Ankur Taly
Categories: cs.LG
Comments: 17 pages, 13 figures
\\
  Fine-tuning pretrained foundational models on specific tasks is now the de
facto approach for text and vision tasks. A known pitfall of this approach is
the forgetting of pretraining knowledge that happens during finetuning.
Rehearsing samples randomly from the pretrain dataset is a common approach to
alleviate such forgetting. However, we find that random mixing unintentionally
includes samples which are not (yet) forgotten or unlearnable by the model. We
propose a novel sampling scheme, mix-cd, that identifies and prioritizes
samples that actually face forgetting, which we call collateral damage. Since
directly identifying collateral damage samples is computationally expensive, we
propose a procedure to estimate the distribution of such samples by tracking
the statistics of finetuned samples. Our approach is lightweight, easy to
implement, and can be seamlessly integrated into existing models, offering an
effective means to retain pretrain performance without additional computational
costs.
\\ ( https://arxiv.org/abs/2402.08096 ,  1845kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08105
Date: Mon, 12 Feb 2024 22:48:30 GMT   (22876kb,D)

Title: Learning Cartesian Product Graphs with Laplacian Constraints
Authors: Changhao Shi and Gal Mishne
Categories: cs.LG stat.ML
Comments: Accepted to AISTATS 2024
\\
  Graph Laplacian learning, also known as network topology inference, is a
problem of great interest to multiple communities. In Gaussian graphical models
(GM), graph learning amounts to endowing covariance selection with the
Laplacian structure. In graph signal processing (GSP), it is essential to infer
the unobserved graph from the outputs of a filtering system. In this paper, we
study the problem of learning Cartesian product graphs under Laplacian
constraints. The Cartesian graph product is a natural way for modeling
higher-order conditional dependencies and is also the key for generalizing GSP
to multi-way tensors. We establish statistical consistency for the penalized
maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and
propose an efficient algorithm to solve the problem. We also extend our method
for efficient joint graph learning and imputation in the presence of structural
missing values. Experiments on synthetic and real-world datasets demonstrate
that our method is superior to previous GSP and GM methods.
\\ ( https://arxiv.org/abs/2402.08105 ,  22876kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08112
Date: Mon, 12 Feb 2024 23:08:17 GMT   (857kb,D)

Title: A Competition Winning Deep Reinforcement Learning Agent in microRTS
Authors: Scott Goodfriend
Categories: cs.LG cs.AI
Comments: 26 pages, 4 figures
\\
  Scripted agents have predominantly won the five previous iterations of the
IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep
Reinforcement Learning (DRL) algorithms making significant strides in real-time
strategy (RTS) games, their adoption in this primarily academic competition has
been limited due to the considerable training resources required and the
complexity inherent in creating and debugging such agents. RAISocketAI is the
first DRL agent to win the IEEE microRTS competition. In a benchmark without
performance constraints, RAISocketAI regularly defeated the two prior
competition winners. This first competition-winning DRL submission can be a
benchmark for future microRTS competitions and a starting point for future DRL
research. Iteratively fine-tuning the base policy and transfer learning to
specific maps were critical to RAISocketAI's winning performance. These
strategies can be used to economically train future DRL agents. Further work in
Imitation Learning using Behavior Cloning and fine-tuning these models with DRL
has proven promising as an efficient way to bootstrap models with demonstrated,
competitive behaviors.
\\ ( https://arxiv.org/abs/2402.08112 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08114
Date: Mon, 12 Feb 2024 23:09:00 GMT   (1443kb,D)

Title: Active Preference Learning for Large Language Models
Authors: William Muldrew, Peter Hayes, Mingtian Zhang, David Barber
Categories: cs.LG cs.AI cs.CL
Comments: 13 pages, 5 figures, 6 tables
\\
  As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.
\\ ( https://arxiv.org/abs/2402.08114 ,  1443kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08117
Date: Mon, 12 Feb 2024 23:15:16 GMT   (4753kb,D)

Title: A Universal Non-Parametric Approach For Improved Molecular Sequence
  Analysis
Authors: Sarwan Ali, Tamkanat E Ali, Prakash Chourasia, Murray Patterson
Categories: cs.LG q-bio.QM
Comments: Accepted at The Pacific-Asia Conference on Knowledge Discovery and
  Data Mining (PAKDD) 2024
\\
  In the field of biological research, it is essential to comprehend the
characteristics and functions of molecular sequences. The classification of
molecular sequences has seen widespread use of neural network-based techniques.
Despite their astounding accuracy, these models often require a substantial
number of parameters and more data collection. In this work, we present a novel
approach based on the compression-based Model, motivated from
\cite{jiang2023low}, which combines the simplicity of basic compression
algorithms like Gzip and Bz2, with Normalized Compression Distance (NCD)
algorithm to achieve better performance on classification tasks without relying
on handcrafted features or pre-trained models. Firstly, we compress the
molecular sequence using well-known compression algorithms, such as Gzip and
Bz2. By leveraging the latent structure encoded in compressed files, we compute
the Normalized Compression Distance between each pair of molecular sequences,
which is derived from the Kolmogorov complexity. This gives us a distance
matrix, which is the input for generating a kernel matrix using a Gaussian
kernel. Next, we employ kernel Principal Component Analysis (PCA) to get the
vector representations for the corresponding molecular sequence, capturing
important structural and functional information. The resulting vector
representations provide an efficient yet effective solution for molecular
sequence analysis and can be used in ML-based downstream tasks. The proposed
approach eliminates the need for computationally intensive Deep Neural Networks
(DNNs), with their large parameter counts and data requirements. Instead, it
leverages a lightweight and universally accessible compression-based model.
\\ ( https://arxiv.org/abs/2402.08117 ,  4753kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08126
Date: Mon, 12 Feb 2024 23:50:44 GMT   (55kb)

Title: Contextual Multinomial Logit Bandits with General Value Functions
Authors: Mengxiao Zhang, Haipeng Luo
Categories: cs.LG
\\
  Contextual multinomial logit (MNL) bandits capture many real-world assortment
recommendation problems such as online retailing/advertising. However, prior
work has only considered (generalized) linear value functions, which greatly
limits its applicability. Motivated by this fact, in this work, we consider
contextual MNL bandits with a general value function class that contains the
ground truth, borrowing ideas from a recent trend of studies on contextual
bandits. Specifically, we consider both the stochastic and the adversarial
settings, and propose a suite of algorithms, each with different
computation-regret trade-off. When applied to the linear case, our results not
only are the first ones with no dependence on a certain problem-dependent
constant that can be exponentially large, but also enjoy other advantages such
as computational efficiency, dimension-free regret bounds, or the ability to
handle completely adversarial contexts and rewards.
\\ ( https://arxiv.org/abs/2402.08126 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08127
Date: Mon, 12 Feb 2024 23:50:47 GMT   (3504kb,D)

Title: Efficient Contextual Bandits with Uninformed Feedback Graphs
Authors: Mengxiao Zhang, Yuheng Zhang, Haipeng Luo, Paul Mineiro
Categories: cs.LG
\\
  Bandits with feedback graphs are powerful online learning models that
interpolate between the full information and classic bandit problems, capturing
many real-life applications. A recent work by Zhang et al. (2023) studies the
contextual version of this problem and proposes an efficient and optimal
algorithm via a reduction to online regression. However, their algorithm
crucially relies on seeing the feedback graph before making each decision,
while in many applications, the feedback graph is uninformed, meaning that it
is either only revealed after the learner makes her decision or even never
fully revealed at all. This work develops the first contextual algorithm for
such uninformed settings, via an efficient reduction to online regression over
both the losses and the graphs. Importantly, we show that it is critical to
learn the graphs using log loss instead of squared loss to obtain favorable
regret guarantees. We also demonstrate the empirical effectiveness of our
algorithm on a bidding application using both synthetic and real-world data.
\\ ( https://arxiv.org/abs/2402.08127 ,  3504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08132
Date: Mon, 12 Feb 2024 23:55:55 GMT   (76kb)

Title: On the Resurgence of Recurrent Models for Long Sequences: Survey and
  Research Opportunities in the Transformer Era
Authors: Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco
  Gori and Stefano Melacci
Categories: cs.LG
Comments: Under review
\\
  A longstanding challenge for the Machine Learning community is the one of
developing models that are capable of processing and learning from very long
sequences of data. The outstanding results of Transformers-based networks
(e.g., Large Language Models) promotes the idea of parallel attention as the
key to succeed in such a challenge, obfuscating the role of classic sequential
processing of Recurrent Models. However, in the last few years, researchers who
were concerned by the quadratic complexity of self-attention have been
proposing a novel wave of neural models, which gets the best from the two
worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State
Models emerged as robust approaches to function approximation over time, thus
opening a new perspective in learning from sequential data, followed by many
people in the field and exploited to implement a special class of (linear)
Recurrent Neural Networks. This survey is aimed at providing an overview of
these trends framed under the unifying umbrella of Recurrence. Moreover, it
emphasizes novel research opportunities that become prominent when abandoning
the idea of processing long sequences whose length is known-in-advance for the
more realistic setting of potentially infinite-length sequences, thus
intersecting the field of lifelong-online learning from streamed data.
\\ ( https://arxiv.org/abs/2402.08132 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08134
Date: Tue, 13 Feb 2024 00:02:05 GMT   (3053kb,D)

Title: Randomized Algorithms for Symmetric Nonnegative Matrix Factorization
Authors: Koby Hayashi, Sinan G. Aksoy, Grey Ballard, Haesun Park
Categories: cs.LG cs.NA math.NA math.OC
MSC-class: 65F55, 65F20
\\
  Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data
analysis and machine learning that approximates a symmetric matrix with a
product of a nonnegative, low-rank matrix and its transpose. To design faster
and more scalable algorithms for SymNMF we develop two randomized algorithms
for its computation. The first algorithm uses randomized matrix sketching to
compute an initial low-rank input matrix and proceeds to use this input to
rapidly compute a SymNMF. The second algorithm uses randomized leverage score
sampling to approximately solve constrained least squares problems. Many
successful methods for SymNMF rely on (approximately) solving sequences of
constrained least squares problems. We prove theoretically that leverage score
sampling can approximately solve nonnegative least squares problems to a chosen
accuracy with high probability. Finally we demonstrate that both methods work
well in practice by applying them to graph clustering tasks on large real world
data sets. These experiments show that our methods approximately maintain
solution quality and achieve significant speed ups for both large dense and
large sparse problems.
\\ ( https://arxiv.org/abs/2402.08134 ,  3053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08156
Date: Tue, 13 Feb 2024 01:38:01 GMT   (149kb)

Title: Group Decision-Making among Privacy-Aware Agents
Authors: Marios Papachristou, M. Amin Rahimian
Categories: cs.LG cs.AI cs.CR cs.MA stat.ML
\\
  How can individuals exchange information to learn from each other despite
their privacy needs and security concerns? For example, consider individuals
deliberating a contentious topic and being concerned about divulging their
private experiences. Preserving individual privacy and enabling efficient
social learning are both important desiderata but seem fundamentally at odds
with each other and very hard to reconcile. We do so by controlling information
leakage using rigorous statistical guarantees that are based on differential
privacy (DP). Our agents use log-linear rules to update their beliefs after
communicating with their neighbors. Adding DP randomization noise to beliefs
provides communicating agents with plausible deniability with regard to their
private information and their network neighborhoods. We consider two learning
environments one for distributed maximum-likelihood estimation given a finite
number of private signals and another for online learning from an infinite,
intermittent signal stream. Noisy information aggregation in the finite case
leads to interesting tradeoffs between rejecting low-quality states and making
sure all high-quality states are accepted in the algorithm output. Our results
flesh out the nature of the trade-offs in both cases between the quality of the
group decision outcomes, learning accuracy, communication cost, and the level
of privacy protections that the agents are afforded.
\\ ( https://arxiv.org/abs/2402.08156 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08170
Date: Tue, 13 Feb 2024 02:03:26 GMT   (296kb,D)

Title: LLaGA: Large Language and Graph Assistant
Authors: Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, Zhangyang Wang
Categories: cs.LG cs.AI
\\
  Graph Neural Networks (GNNs) have empowered the advance in graph-structured
data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4
has heralded a new era in deep learning. However, their application to graph
data poses distinct challenges due to the inherent difficulty of translating
graph structures to language. To this end, we introduce the \textbf{L}arge
\textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant
(\textbf{LLaGA}), an innovative model that effectively integrates LLM
capabilities to handle the complexities of graph-structured data. LLaGA retains
the general-purpose nature of LLMs while adapting graph data into a format
compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to
structure-aware sequences and then mapping these into the token embedding space
through a versatile projector. LLaGA excels in versatility, generalizability
and interpretability, allowing it to perform consistently well across different
datasets and tasks, extend its ability to unseen datasets or tasks, and provide
explanations for graphs. Our extensive experiments across popular graph
benchmarks show that LLaGA delivers outstanding performance across four
datasets and three tasks using one single model, surpassing state-of-the-art
graph models in both supervised and zero-shot scenarios. Our code is available
at \url{https://github.com/ChenRunjin/LLaGA}
\\ ( https://arxiv.org/abs/2402.08170 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08180
Date: Tue, 13 Feb 2024 02:36:41 GMT   (85kb)

Title: Online Structured Prediction with Fenchel--Young Losses and Improved
  Surrogate Regret for Online Multiclass Classification with Logistic Loss
Authors: Shinsaku Sakaue, Han Bao, Taira Tsuchiya, Taihei Oki
Categories: cs.LG
\\
  This paper studies online structured prediction with full-information
feedback. For online multiclass classification, van der Hoeven (2020) has
obtained surrogate regret bounds independent of the time horizon, or
\emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap}
framework. However, this framework has been limited to multiclass
classification primarily because it relies on a classification-specific
procedure for converting estimated scores to outputs. We extend the
exploit-the-surrogate-gap framework to online structured prediction with
\emph{Fenchel--Young losses}, a large family of surrogate losses including the
logistic loss for multiclass classification, obtaining finite surrogate regret
bounds in various structured prediction problems. To this end, we propose and
analyze \emph{randomized decoding}, which converts estimated scores to general
structured outputs. Moreover, by applying our decoding to online multiclass
classification with the logistic loss, we obtain a surrogate regret bound of
$O(B^2)$, where $B$ is the $\ell_2$-diameter of the domain. This bound is tight
up to logarithmic factors and improves the previous bound of $O(dB^2)$ due to
van der Hoeven (2020) by a factor of $d$, the number of classes.
\\ ( https://arxiv.org/abs/2402.08180 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08182
Date: Tue, 13 Feb 2024 02:41:56 GMT   (133kb,D)

Title: Variational Continual Test-Time Adaptation
Authors: Fan Lyu, Kaile Du, Yuyang Li, Hanyu Zhao, Zhang Zhang, Guangcan Liu,
  Liang Wang
Categories: cs.LG stat.ML
\\
  The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods
that only use unlabeled test data, as it can cause significant error
propagation. In this paper, we introduce VCoTTA, a variational Bayesian
approach to measure uncertainties in CTTA. At the source stage, we transform a
pre-trained deterministic model into a Bayesian Neural Network (BNN) via a
variational warm-up strategy, injecting uncertainties into the model. During
the testing time, we employ a mean-teacher update strategy using variational
inference for the student model and exponential moving average for the teacher
model. Our novel approach updates the student model by combining priors from
both the source and teacher models. The evidence lower bound is formulated as
the cross-entropy between the student and teacher models, along with the
Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on
three datasets demonstrate the method's effectiveness in mitigating prior drift
within the CTTA framework.
\\ ( https://arxiv.org/abs/2402.08182 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08187
Date: Tue, 13 Feb 2024 03:14:32 GMT   (4047kb,D)

Title: Learning time-dependent PDE via graph neural networks and deep operator
  network for robust accuracy on irregular grids
Authors: Sung Woong Cho, Jae Yong Lee, Hyung Ju Hwang
Categories: cs.LG cs.NA math.NA
Comments: 25 pages, 11 figures
MSC-class: 65D17, 68U07
\\
  Scientific computing using deep learning has seen significant advancements in
recent years. There has been growing interest in models that learn the operator
from the parameters of a partial differential equation (PDE) to the
corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural
operator, among other models, have been designed with structures suitable for
handling functions as inputs and outputs, enabling real-time predictions as
surrogate models for solution operators. There has also been significant
progress in the research on surrogate models based on graph neural networks
(GNNs), specifically targeting the dynamics in time-dependent PDEs. In this
paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to
effectively adapt DeepONet, which is well-known for successful operator
learning. GraphDeepONet exhibits robust accuracy in predicting solutions
compared to existing GNN-based PDE solver models. It maintains consistent
performance even on irregular grids, leveraging the advantages inherited from
DeepONet and enabling predictions on arbitrary grids. Additionally, unlike
traditional DeepONet and its variants, GraphDeepONet enables time extrapolation
for time-dependent PDE solutions. We also provide theoretical analysis of the
universal approximation capability of GraphDeepONet in approximating continuous
operators across arbitrary time intervals.
\\ ( https://arxiv.org/abs/2402.08187 ,  4047kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08193
Date: Tue, 13 Feb 2024 03:31:36 GMT   (831kb,D)

Title: Gaussian Ensemble Belief Propagation for Efficient Inference in
  High-Dimensional Systems
Authors: Dan MacKinlay, Russell Tsuchida, Dan Pagendam, Petra Kuhnert
Categories: cs.LG stat.ML
Comments: Under conference submission
MSC-class: 62-07 (Primary) 62F15, 62M40, 68T05, 68W25
ACM-class: I.2.6; H.2.4; I.2.8; J.2
\\
  Efficient inference in high-dimensional models remains a central challenge in
machine learning. This paper introduces the Gaussian Ensemble Belief
Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and
Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing
low-rank local messages in a graphical model structure. This combination
inherits favourable qualities from each method. Ensemble techniques allow GEnBP
to handle high-dimensional states, parameters and intricate, noisy, black-box
generation processes. The use of local messages in a graphical model structure
ensures that the approach is suited to distributed computing and can
efficiently handle complex dependence structures. GEnBP is particularly
advantageous when the ensemble size is considerably smaller than the inference
dimension. This scenario often arises in fields such as spatiotemporal
modelling, image processing and physical model inversion. GEnBP can be applied
to general problem structures, including jointly learning system parameters,
observation parameters, and latent state variables.
\\ ( https://arxiv.org/abs/2402.08193 ,  831kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08202
Date: Tue, 13 Feb 2024 04:03:09 GMT   (77kb,D)

Title: Confronting Discrimination in Classification: Smote Based on
  Marginalized Minorities in the Kernel Space for Imbalanced Data
Authors: Lingyun Zhong
Categories: cs.LG
Comments: 10 pages, 2 figures
\\
  Financial fraud detection poses a typical challenge characterized by class
imbalance, where instances of fraud are extremely rare but can lead to
unpredictable economic losses if misidentified. Precisely classifying these
critical minority samples represents a challenging task within the
classification. The primary difficulty arises from mainstream classifiers,
which often exhibit "implicit discrimination" against minority samples in
evaluation metrics, which results in frequent misclassifications, and the key
to the problem lies in the overlap of feature spaces between majority and
minority samples. To address these challenges, oversampling is a feasible
solution, yet current classical oversampling methods often lack the necessary
caution in sample selection, exacerbating feature space overlap. In response,
we propose a novel classification oversampling approach based on the decision
boundary and sample proximity relationships. This method carefully considers
the distance between critical samples and the decision hyperplane, as well as
the density of surrounding samples, resulting in an adaptive oversampling
strategy in the kernel space. Finally, we test the proposed method on a classic
financial fraud dataset, and the results show that our proposed method provides
an effective and robust solution that can improve the classification accuracy
of minorities.
\\ ( https://arxiv.org/abs/2402.08202 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08209
Date: Tue, 13 Feb 2024 04:17:48 GMT   (64kb,D)

Title: Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits
Authors: Hiroyuki Namba, Shota Horiguchi, Masaki Hamamoto, Masashi Egi
Categories: cs.LG cs.AI
\\
  Data cleansing aims to improve model performance by removing a set of harmful
instances from the training dataset. Data Shapley is a common theoretically
guaranteed method to evaluate the contribution of each instance to model
performance; however, it requires training on all subsets of the training data,
which is computationally expensive. In this paper, we propose an
iterativemethod to fast identify a subset of instances with low data Shapley
values by using the thresholding bandit algorithm. We provide a theoretical
guarantee that the proposed method can accurately select harmful instances if a
sufficiently large number of iterations is conducted. Empirical evaluation
using various models and datasets demonstrated that the proposed method
efficiently improved the computational speed while maintaining the model
performance.
\\ ( https://arxiv.org/abs/2402.08209 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08225
Date: Tue, 13 Feb 2024 05:33:35 GMT   (1210kb,D)

Title: Improving Black-box Robustness with In-Context Rewriting
Authors: Kyle O'Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon
  Kim, Marzyeh Ghassemi, Thomas Hartvigsen
Categories: cs.LG
\\
  Machine learning models often excel on in-distribution (ID) data but struggle
with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD
robustness are not applicable to settings where the model is effectively a
black box, such as when the weights are frozen, retraining is costly, or the
model is leveraged via an API. Test-time augmentation (TTA) is a simple
post-hoc technique for improving robustness that sidesteps black-box
constraints by aggregating predictions across multiple augmentations of the
test input. TTA has seen limited use in NLP due to the challenge of generating
effective natural language augmentations. In this work, we propose LLM-TTA,
which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA
outperforms conventional augmentation functions across sentiment, toxicity, and
news classification tasks for BERT and T5 models, with BERT's OOD robustness
improving by an average of 4.30 percentage points without regressing average ID
performance. We explore selectively augmenting inputs based on prediction
entropy to reduce the rate of expensive LLM augmentations, allowing us to
maintain performance gains while reducing the average number of generated
augmentations by 57.76%. LLM-TTA is agnostic to the task model architecture,
does not require OOD labels, and is effective across low and high-resource
settings. We share our data, models, and code for reproducibility.
\\ ( https://arxiv.org/abs/2402.08225 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08228
Date: Tue, 13 Feb 2024 05:38:45 GMT   (1662kb,D)

Title: Investigating Out-of-Distribution Generalization of GNNs: An
  Architecture Perspective
Authors: Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang
Categories: cs.LG cs.AI
\\
  Graph neural networks (GNNs) have exhibited remarkable performance under the
assumption that test data comes from the same distribution of training data.
However, in real-world scenarios, this assumption may not always be valid.
Consequently, there is a growing focus on exploring the Out-of-Distribution
(OOD) problem in the context of graphs. Most existing efforts have primarily
concentrated on improving graph OOD generalization from two
\textbf{model-agnostic} perspectives: data-driven methods and strategy-based
learning. However, there has been limited attention dedicated to investigating
the impact of well-known \textbf{GNN model architectures} on graph OOD
generalization, which is orthogonal to existing research. In this work, we
provide the first comprehensive investigation of OOD generalization on graphs
from an architecture perspective, by examining the common building blocks of
modern GNNs. Through extensive experiments, we reveal that both the graph
self-attention mechanism and the decoupled architecture contribute positively
to graph OOD generalization. In contrast, we observe that the linear
classification layer tends to compromise graph OOD generalization capability.
Furthermore, we provide in-depth theoretical insights and discussions to
underpin these discoveries. These insights have empowered us to develop a novel
GNN backbone model, DGAT, designed to harness the robust properties of both
graph self-attention mechanism and the decoupled architecture. Extensive
experimental results demonstrate the effectiveness of our model under graph
OOD, exhibiting substantial and consistent enhancements across various training
strategies.
\\ ( https://arxiv.org/abs/2402.08228 ,  1662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08229
Date: Tue, 13 Feb 2024 05:43:49 GMT   (4983kb,D)

Title: Causal Discovery under Off-Target Interventions
Authors: Davin Choo, Kirankumar Shiragur, Caroline Uhler
Categories: cs.LG cs.DS stat.ME stat.ML
Comments: Accepted into AISTATS 2024
\\
  Causal graph discovery is a significant problem with applications across
various disciplines. However, with observational data alone, the underlying
causal graph can only be recovered up to its Markov equivalence class, and
further assumptions or interventions are necessary to narrow down the true
graph. This work addresses the causal discovery problem under the setting of
stochastic interventions with the natural goal of minimizing the number of
interventions performed. We propose the following stochastic intervention model
which subsumes existing adaptive noiseless interventions in the literature
while capturing scenarios such as fat-hand interventions and CRISPR gene
knockouts: any intervention attempt results in an actual intervention on a
random subset of vertices, drawn from a distribution dependent on attempted
action. Under this model, we study the two fundamental problems in causal
discovery of verification and search and provide approximation algorithms with
polylogarithmic competitive ratios and provide some preliminary experimental
results.
\\ ( https://arxiv.org/abs/2402.08229 ,  4983kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08244
Date: Tue, 13 Feb 2024 06:18:42 GMT   (620kb)

Title: APALU: A Trainable, Adaptive Activation Function for Deep Learning
  Networks
Authors: Barathi Subramanian, Rathinaraja Jeyaraj, Rakhmonov Akhrorjon
  Akhmadjon Ugli, and Jeonghong Kim
Categories: cs.LG cs.NE
Comments: 9 pages, 4 figures, Submitted at IJCAI 2024 conference
\\
  Activation function is a pivotal component of deep learning, facilitating the
extraction of intricate data patterns. While classical activation functions
like ReLU and its variants are extensively utilized, their static nature and
simplicity, despite being advantageous, often limit their effectiveness in
specialized tasks. The trainable activation functions also struggle sometimes
to adapt to the unique characteristics of the data. Addressing these
limitations, we introduce a novel trainable activation function, adaptive
piecewise approximated activation linear unit (APALU), to enhance the learning
performance of deep learning across a broad range of tasks. It presents a
unique set of features that enable it to maintain stability and efficiency in
the learning process while adapting to complex data representations.
Experiments reveal significant improvements over widely used activation
functions for different tasks. In image classification, APALU increases
MobileNet and GoogleNet accuracy by 0.37% and 0.04%, respectively, on the
CIFAR10 dataset. In anomaly detection, it improves the average area under the
curve of One-CLASS Deep SVDD by 0.8% on the MNIST dataset, 1.81% and 1.11%
improvements with DifferNet, and knowledge distillation, respectively, on the
MVTech dataset. Notably, APALU achieves 100% accuracy on a sign language
recognition task with a limited dataset. For regression tasks, APALU enhances
the performance of deep neural networks and recurrent neural networks on
different datasets. These improvements highlight the robustness and
adaptability of APALU across diverse deep-learning applications.
\\ ( https://arxiv.org/abs/2402.08244 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08255
Date: Tue, 13 Feb 2024 07:07:37 GMT   (4217kb,D)

Title: Distal Interference: Exploring the Limits of Model-Based Continual
  Learning
Authors: Heinrich van Deventer, Anna Sergeevna Bosman
Categories: cs.LG cs.AI cs.NE
MSC-class: 68T07
ACM-class: I.5.1
\\
  Continual learning is the sequential learning of different tasks by a machine
learning model. Continual learning is known to be hindered by catastrophic
interference or forgetting, i.e. rapid unlearning of earlier learned tasks when
new tasks are learned. Despite their practical success, artificial neural
networks (ANNs) are prone to catastrophic interference. This study analyses how
gradient descent and overlapping representations between distant input points
lead to distal interference and catastrophic interference. Distal interference
refers to the phenomenon where training a model on a subset of the domain leads
to non-local changes on other subsets of the domain. This study shows that
uniformly trainable models without distal interference must be exponentially
large. A novel antisymmetric bounded exponential layer B-spline ANN
architecture named ABEL-Spline is proposed that can approximate any continuous
function, is uniformly trainable, has polynomial computational complexity, and
provides some guarantees for distal interference. Experiments are presented to
demonstrate the theoretical properties of ABEL-Splines. ABEL-Splines are also
evaluated on benchmark regression problems. It is concluded that the weaker
distal interference guarantees in ABEL-Splines are insufficient for model-only
continual learning. It is conjectured that continual learning with polynomial
complexity models requires augmentation of the training data or algorithm.
\\ ( https://arxiv.org/abs/2402.08255 ,  4217kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08268
Date: Tue, 13 Feb 2024 07:47:36 GMT   (7336kb,D)

Title: World Model on Million-Length Video And Language With RingAttention
Authors: Hao Liu, Wilson Yan, Matei Zaharia, Pieter Abbeel
Categories: cs.LG
\\
  Current language models fall short in understanding aspects of the world not
easily described in words, and struggle with complex, long-form tasks. Video
sequences offer valuable temporal information absent in language and static
images, making them attractive for joint modeling with language. Such models
could develop a understanding of both human textual knowledge and the physical
world, enabling broader AI capabilities for assisting humans. However, learning
from millions of tokens of video and language sequences poses challenges due to
memory constraints, computational complexity, and limited datasets. To address
these challenges, we curate a large dataset of diverse videos and books,
utilize the RingAttention technique to scalably train on long sequences, and
gradually increase context size from 4K to 1M tokens. This paper makes the
following contributions: (a) Largest context size neural network: We train one
of the largest context size transformers on long video and language sequences,
setting new benchmarks in difficult retrieval tasks and long video
understanding. (b) Solutions for overcoming vision-language training
challenges, including using masked sequence packing for mixing different
sequence lengths, loss weighting to balance language and vision, and
model-generated QA dataset for long sequence chat. (c) A highly-optimized
implementation with RingAttention, masked sequence packing, and other key
features for training on millions-length multimodal sequences. (d) Fully
open-sourced a family of 7B parameter models capable of processing long text
documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M
tokens. This work paves the way for training on massive datasets of long video
and language to develop understanding of both human knowledge and the
multimodal world, and broader capabilities.
\\ ( https://arxiv.org/abs/2402.08268 ,  7336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08290
Date: Tue, 13 Feb 2024 08:41:32 GMT   (136kb,D)

Title: The Effect of Data Poisoning on Counterfactual Explanations
Authors: Andr\'e Artelt, Shubham Sharma, Freddy Lecu\'e, Barbara Hammer
Categories: cs.LG cs.AI
\\
  Counterfactual explanations provide a popular method for analyzing the
predictions of black-box systems, and they can offer the opportunity for
computational recourse by suggesting actionable changes on how to change the
input to obtain a different (i.e. more favorable) system output. However,
recent work highlighted their vulnerability to different types of
manipulations. This work studies the vulnerability of counterfactual
explanations to data poisoning. We formalize data poisoning in the context of
counterfactual explanations for increasing the cost of recourse on three
different levels: locally for a single instance, or a sub-group of instances,
or globally for all instances. We demonstrate that state-of-the-art
counterfactual generation methods \& toolboxes are vulnerable to such data
poisoning.
\\ ( https://arxiv.org/abs/2402.08290 ,  136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08296
Date: Tue, 13 Feb 2024 08:50:14 GMT   (7256kb,D)

Title: Multi-Level GNN Preconditioner for Solving Large Scale Problems
Authors: Matthieu Nastorg (TAU, IFPEN), Jean-Marc Gratien (IFPEN), Thibault
  Faney (IFPEN), Michele Alessandro Bucci (TAU), Guillaume Charpiat (TAU), Marc
  Schoenauer (TAU)
Categories: cs.LG cs.NA math.NA
\\
  Large-scale numerical simulations often come at the expense of daunting
computations. High-Performance Computing has enhanced the process, but adapting
legacy codes to leverage parallel GPU computations remains challenging.
Meanwhile, Machine Learning models can harness GPU computations effectively but
often struggle with generalization and accuracy. Graph Neural Networks (GNNs),
in particular, are great for learning from unstructured data like meshes but
are often limited to small-scale problems. Moreover, the capabilities of the
trained model usually restrict the accuracy of the data-driven solution. To
benefit from both worlds, this paper introduces a novel preconditioner
integrating a GNN model within a multi-level Domain Decomposition framework.
The proposed GNN-based preconditioner is used to enhance the efficiency of a
Krylov method, resulting in a hybrid solver that can converge with any desired
level of accuracy. The efficiency of the Krylov method greatly benefits from
the GNN preconditioner, which is adaptable to meshes of any size and shape, is
executed on GPUs, and features a multi-level approach to enforce the
scalability of the entire process. Several experiments are conducted to
validate the numerical behavior of the hybrid solver, and an in-depth analysis
of its performance is proposed to assess its competitiveness against a C++
legacy solver.
\\ ( https://arxiv.org/abs/2402.08296 ,  7256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08309
Date: Tue, 13 Feb 2024 09:12:55 GMT   (4048kb,D)

Title: Prompted Contextual Vectors for Spear-Phishing Detection
Authors: Daniel Nahmias, Gal Engelberg, Dan Klein, Asaf Shabtai
Categories: cs.LG cs.CL cs.CR
ACM-class: I.2.7
\\
  Spear-phishing attacks present a significant security challenge, with large
language models (LLMs) escalating the threat by generating convincing emails
and facilitating target reconnaissance. To address this, we propose a detection
approach based on a novel document vectorization method that utilizes an
ensemble of LLMs to create representation vectors. By prompting LLMs to reason
and respond to human-crafted questions, we quantify the presence of common
persuasion principles in the email's content, producing prompted contextual
document vectors for a downstream supervised machine learning model. We
evaluate our method using a unique dataset generated by a proprietary system
that automates target reconnaissance and spear-phishing email creation. Our
method achieves a 91% F1 score in identifying LLM-generated spear-phishing
emails, with the training set comprising only traditional phishing and benign
emails. Key contributions include an innovative document vectorization method
utilizing LLM reasoning, a publicly available dataset of high-quality
spear-phishing emails, and the demonstrated effectiveness of our method in
detecting such emails. This methodology can be utilized for various document
classification tasks, particularly in adversarial problem domains.
\\ ( https://arxiv.org/abs/2402.08309 ,  4048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08313
Date: Tue, 13 Feb 2024 09:17:20 GMT   (741kb,D)

Title: Approximating Families of Sharp Solutions to Fisher's Equation with
  Physics-Informed Neural Networks
Authors: Franz M. Rohrhofer, Stefan Posch, Clemens G\"o{\ss}nitzer, Bernhard C.
  Geiger
Categories: cs.LG
Comments: 14 pages, 7 figures
\\
  This paper employs physics-informed neural networks (PINNs) to solve Fisher's
equation, a fundamental representation of a reaction-diffusion system with both
simplicity and significance. The focus lies specifically in investigating
Fisher's equation under conditions of large reaction rate coefficients, wherein
solutions manifest as traveling waves, posing a challenge for numerical methods
due to the occurring steepness of the wave front. To address optimization
challenges associated with the standard PINN approach, a residual weighting
scheme is introduced. This scheme is designed to enhance the tracking of
propagating wave fronts by considering the reaction term in the
reaction-diffusion equation. Furthermore, a specific network architecture is
studied which is tailored for solutions in the form of traveling waves. Lastly,
the capacity of PINNs to approximate an entire family of solutions is assessed
by incorporating the reaction rate coefficient as an additional input to the
network architecture. This modification enables the approximation of the
solution across a broad and continuous range of reaction rate coefficients,
thus solving a class of reaction-diffusion systems using a single PINN
instance.
\\ ( https://arxiv.org/abs/2402.08313 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08321
Date: Tue, 13 Feb 2024 09:34:22 GMT   (55kb)

Title: Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret
  with Adversarial Robustness in Partial Monitoring
Authors: Taira Tsuchiya, Shinji Ito, Junya Honda
Categories: cs.LG stat.ML
Comments: 30 pages
\\
  Partial monitoring is a generic framework of online decision-making problems
with limited observations. To make decisions from such limited observations, it
is necessary to find an appropriate distribution for exploration. Recently, a
powerful approach for this purpose, exploration by optimization (ExO), was
proposed, which achieves the optimal bounds in adversarial environments with
follow-the-regularized-leader for a wide range of online decision-making
problems. However, a naive application of ExO in stochastic environments
significantly degrades regret bounds. To resolve this problem in locally
observable games, we first establish a novel framework and analysis for ExO
with a hybrid regularizer. This development allows us to significantly improve
the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which
achieves nearly optimal bounds both in stochastic and adversarial environments.
In particular, we derive a stochastic regret bound of $O(\sum_{a \neq a^*} k^2
m^2 \log T / \Delta_a)$, where $k$, $m$, and $T$ are the numbers of actions,
observations and rounds, $a^*$ is an optimal action, and $\Delta_a$ is the
suboptimality gap for action $a$. This bound is roughly $\Theta(k^2 \log T)$
times smaller than existing BOBW bounds. In addition, for globally observable
games, we provide a new BOBW algorithm with the first $O(\log T)$ stochastic
bound.
\\ ( https://arxiv.org/abs/2402.08321 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08324
Date: Tue, 13 Feb 2024 09:40:19 GMT   (852kb,D)

Title: Uncertainty Quantification via Stable Distribution Propagation
Authors: Felix Petersen, Aashwin Mishra, Hilde Kuehne, Christian Borgelt,
  Oliver Deussen, Mikhail Yurochkin
Categories: cs.LG cs.AI
Comments: Published at ICLR 2024, Code @
  https://github.com/Felix-Petersen/distprop
\\
  We propose a new approach for propagating stable probability distributions
through neural networks. Our method is based on local linearization, which we
show to be an optimal approximation in terms of total variation distance for
the ReLU non-linearity. This allows propagating Gaussian and Cauchy input
uncertainties through neural networks to quantify their output uncertainties.
To demonstrate the utility of propagating distributions, we apply the proposed
method to predicting calibrated confidence intervals and selective prediction
on out-of-distribution data. The results demonstrate a broad applicability of
propagating distributions and show the advantages of our method over other
approaches such as moment matching.
\\ ( https://arxiv.org/abs/2402.08324 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08365
Date: Tue, 13 Feb 2024 10:50:54 GMT   (1526kb,D)

Title: NeuRes: Learning Proofs of Propositional Satisfiability
Authors: Mohamed Ghanem, Frederik Schmitt, Julian Siber, Bernd Finkbeiner
Categories: cs.LG cs.LO
\\
  We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other
neural SAT solving methods, NeuRes is capable of proving unsatisfiability as
opposed to merely predicting it. By design, NeuRes operates in a
certificate-driven fashion by employing propositional resolution to prove
unsatisfiability and to accelerate the process of finding satisfying truth
assignments in case of unsat and sat formulas, respectively. To realize this,
we propose a novel architecture that adapts elements from Graph Neural Networks
and Pointer Networks to autoregressively select pairs of nodes from a dynamic
graph structure, which is essential to the generation of resolution proofs. Our
model is trained and evaluated on a dataset of teacher proofs and truth
assignments that we compiled with the same random formula distribution used by
NeuroSAT. In our experiments, we show that NeuRes solves more test formulas
than NeuroSAT by a rather wide margin on different distributions while being
much more data-efficient. Furthermore, we show that NeuRes is capable of
largely shortening teacher proofs by notable proportions. We use this feature
to devise a bootstrapped training procedure that manages to reduce a dataset of
proofs generated by an advanced solver by ~23% after training on it with no
extra guidance.
\\ ( https://arxiv.org/abs/2402.08365 ,  1526kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08367
Date: Tue, 13 Feb 2024 10:54:43 GMT   (37605kb,D)

Title: RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural
  Networks
Authors: Chengxi Zeng, Tilo Burghardt, Alberto M Gambaruto
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2402.06955
\\
  While many recent Physics-Informed Neural Networks (PINNs) variants have had
considerable success in solving Partial Differential Equations, the empirical
benefits of feature mapping drawn from the broader Neural Representations
research have been largely overlooked. We highlight the limitations of widely
used Fourier-based feature mapping in certain situations and suggest the use of
the conditionally positive definite Radial Basis Function. The empirical
findings demonstrate the effectiveness of our approach across a variety of
forward and inverse problem cases. Our method can be seamlessly integrated into
coordinate-based input neural networks and contribute to the wider field of
PINNs research.
\\ ( https://arxiv.org/abs/2402.08367 ,  37605kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08371
Date: Tue, 13 Feb 2024 11:02:12 GMT   (1518kb)

Title: Helping university students to choose elective courses by using a hybrid
  multi-criteria recommendation system with genetic optimization
Authors: A. Esteban, A. Zafra and C. Romero
Categories: cs.LG
Journal-ref: Knowledge-Based Systems, (2020):194, 105385
DOI: 10.1016/j.knosys.2019.105385
\\
  The wide availability of specific courses together with the flexibility of
academic plans in university studies reveal the importance of Recommendation
Systems (RSs) in this area. These systems appear as tools that help students to
choose courses that suit to their personal interests and their academic
performance. This paper presents a hybrid RS that combines Collaborative
Filtering (CF) and Content-based Filtering (CBF) using multiple criteria
related both to student and course information to recommend the most suitable
courses to the students. A Genetic Algorithm (GA) has been developed to
automatically discover the optimal RS configuration which include both the most
relevant criteria and the configuration of the rest of parameters. The
experimental study has used real information of Computer Science Degree of
University of Cordoba (Spain) including information gathered from students
during three academic years, counting on 2500 entries of 95 students and 63
courses. Experimental results show a study of the most relevant criteria for
the course recommendation, the importance of using a hybrid model that combines
both student information and course information to increase the reliability of
the recommendations as well as an excellent performance compared to previous
models.
\\ ( https://arxiv.org/abs/2402.08371 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08373
Date: Tue, 13 Feb 2024 11:10:14 GMT   (4786kb,D)

Title: Time-Series Classification for Dynamic Strategies in Multi-Step
  Forecasting
Authors: Riku Green, Grant Stevens, Telmo de Menezes e Silva Filho, Zahraa
  Abdallah
Categories: cs.LG cs.AI
\\
  Multi-step forecasting (MSF) in time-series, the ability to make predictions
multiple time steps into the future, is fundamental to almost all temporal
domains. To make such forecasts, one must assume the recursive complexity of
the temporal dynamics. Such assumptions are referred to as the forecasting
strategy used to train a predictive model. Previous work shows that it is not
clear which forecasting strategy is optimal a priori to evaluating on unseen
data. Furthermore, current approaches to MSF use a single (fixed) forecasting
strategy.
  In this paper, we characterise the instance-level variance of optimal
forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We
experiment using 10 datasets from different scales, domains, and lengths of
multi-step horizons. When using a random-forest-based classifier, DyStrat
outperforms the best fixed strategy, which is not knowable a priori, 94% of the
time, with an average reduction in mean-squared error of 11%. Our approach
typically triples the top-1 accuracy compared to current approaches. Notably,
we show DyStrat generalises well for any MSF task.
\\ ( https://arxiv.org/abs/2402.08373 ,  4786kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08383
Date: Tue, 13 Feb 2024 11:22:59 GMT   (7742kb,D)

Title: Uncertainty Quantification for Forward and Inverse Problems of PDEs via
  Latent Global Evolution
Authors: Tailin Wu, Willie Neiswanger, Hongtao Zheng, Stefano Ermon, Jure
  Leskovec
Categories: cs.LG cs.AI
Comments: Accepted by AAAI 2024 (Oral)
\\
  Deep learning-based surrogate models have demonstrated remarkable advantages
over classical solvers in terms of speed, often achieving speedups of 10 to
1000 times over traditional partial differential equation (PDE) solvers.
However, a significant challenge hindering their widespread adoption in both
scientific and industrial domains is the lack of understanding about their
prediction uncertainties, particularly in scenarios that involve critical
decision making. To address this limitation, we propose a method that
integrates efficient and precise uncertainty quantification into a deep
learning-based surrogate model. Our method, termed Latent Evolution of PDEs
with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based
surrogate models with robust and efficient uncertainty quantification
capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent
vectors within a latent space to evolve both the system's state and its
corresponding uncertainty estimation. The latent vectors are decoded to provide
predictions for the system's state as well as estimates of its uncertainty. In
extensive experiments, we demonstrate the accurate uncertainty quantification
performance of our approach, surpassing that of strong baselines including deep
ensembles, Bayesian neural network layers, and dropout. Our method excels at
propagating uncertainty over extended auto-regressive rollouts, making it
suitable for scenarios involving long-term predictions. Our code is available
at: https://github.com/AI4Science-WestlakeU/le-pde-uq.
\\ ( https://arxiv.org/abs/2402.08383 ,  7742kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08384
Date: Tue, 13 Feb 2024 11:25:20 GMT   (412kb,D)

Title: Selective Learning: Towards Robust Calibration with Dynamic
  Regularization
Authors: Zongbo Han, Yifeng Yang, Changqing Zhang, Linjun Zhang, Joey Tianyi
  Zhou, Qinghua Hu, Huaxiu Yao
Categories: cs.LG cs.AI
\\
  Miscalibration in deep learning refers to there is a discrepancy between the
predicted confidence and performance. This problem usually arises due to the
overfitting problem, which is characterized by learning everything presented in
the training set, resulting in overconfident predictions during testing.
Existing methods typically address overfitting and mitigate the miscalibration
by adding a maximum-entropy regularizer to the objective function. The
objective can be understood as seeking a model that fits the ground-truth
labels by increasing the confidence while also maximizing the entropy of
predicted probabilities by decreasing the confidence. However, previous methods
lack clear guidance on confidence adjustment, leading to conflicting objectives
(increasing but also decreasing confidence). Therefore, we introduce a method
called Dynamic Regularization (DReg), which aims to learn what should be
learned during training thereby circumventing the confidence adjusting
trade-off. At a high level, DReg aims to obtain a more reliable model capable
of acknowledging what it knows and does not know. Specifically, DReg
effectively fits the labels for in-distribution samples (samples that should be
learned) while applying regularization dynamically to samples beyond model
capabilities (e.g., outliers), thereby obtaining a robust calibrated model
especially on the samples beyond model capabilities. Both theoretical and
empirical analyses sufficiently demonstrate the superiority of DReg compared
with previous methods.
\\ ( https://arxiv.org/abs/2402.08384 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08400
Date: Tue, 13 Feb 2024 11:59:43 GMT   (5002kb,D)

Title: Adaptive Hierarchical Certification for Segmentation using Randomized
  Smoothing
Authors: Alaa Anani, Tobias Lorenz, Bernt Schiele, Mario Fritz
Categories: cs.LG cs.CV
\\
  Common certification methods operate on a flat pre-defined set of
fine-grained classes. In this paper, however, we propose a novel, more general,
and practical setting, namely adaptive hierarchical certification for image
semantic segmentation. In this setting, the certification can be within a
multi-level hierarchical label space composed of fine to coarse levels. Unlike
classic methods where the certification would abstain for unstable components,
our approach adaptively relaxes the certification to a coarser level within the
hierarchy. This relaxation lowers the abstain rate whilst providing more
certified semantically meaningful information. We mathematically formulate the
problem setup and introduce, for the first time, an adaptive hierarchical
certification algorithm for image semantic segmentation, that certifies image
pixels within a hierarchy and prove the correctness of its guarantees. Since
certified accuracy does not take the loss of information into account when
traversing into a coarser hierarchy level, we introduce a novel evaluation
paradigm for adaptive hierarchical certification, namely the certified
information gain metric, which is proportional to the class granularity level.
Our evaluation experiments on real-world challenging datasets such as
Cityscapes and ACDC demonstrate that our adaptive algorithm achieves a higher
certified information gain and a lower abstain rate compared to the current
state-of-the-art certification method, as well as other non-adaptive versions
of it.
\\ ( https://arxiv.org/abs/2402.08400 ,  5002kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08401
Date: Tue, 13 Feb 2024 12:02:37 GMT   (380kb,D)

Title: LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph
  Attention Network for Fake News Detection
Authors: Batool Lakzaei and Mostafa Haghir Chehreghani and Alireza Bagheri
Categories: cs.LG cs.AI cs.SI
\\
  In the era of widespread social networks, the rapid dissemination of fake
news has emerged as a significant threat, inflicting detrimental consequences
across various dimensions of people's lives. Machine learning and deep learning
approaches have been extensively employed for identifying fake news. However, a
significant challenge in identifying fake news is the limited availability of
labeled news datasets. Therefore, the One-Class Learning (OCL) approach,
utilizing only a small set of labeled data from the interest class, can be a
suitable approach to address this challenge. On the other hand, representing
data as a graph enables access to diverse content and structural information,
and label propagation methods on graphs can be effective in predicting node
labels. In this paper, we adopt a graph-based model for data representation and
introduce a semi-supervised and one-class approach for fake news detection,
called LOSS-GAT. Initially, we employ a two-step label propagation algorithm,
utilizing Graph Neural Networks (GNNs) as an initial classifier to categorize
news into two groups: interest (fake) and non-interest (real). Subsequently, we
enhance the graph structure using structural augmentation techniques.
Ultimately, we predict the final labels for all unlabeled data using a GNN that
induces randomness within the local neighborhood of nodes through the
aggregation function. We evaluate our proposed method on five common datasets
and compare the results against a set of baseline models, including both OCL
and binary labeled models. The results demonstrate that LOSS-GAT achieves a
notable improvement, surpassing 10%, with the advantage of utilizing only a
limited set of labeled fake news. Noteworthy, LOSS-GAT even outperforms binary
labeled models.
\\ ( https://arxiv.org/abs/2402.08401 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08405
Date: Tue, 13 Feb 2024 12:09:15 GMT   (2543kb,D)

Title: A Novel Approach to Regularising 1NN classifier for Improved
  Generalization
Authors: Aditya Challa, Sravan Danda, Laurent Najman
Categories: cs.LG
\\
  In this paper, we propose a class of non-parametric classifiers, that learn
arbitrary boundaries and generalize well.
  Our approach is based on a novel way to regularize 1NN classifiers using a
greedy approach. We refer to this class of classifiers as Watershed
Classifiers. 1NN classifiers are known to trivially over-fit but have very
large VC dimension, hence do not generalize well. We show that watershed
classifiers can find arbitrary boundaries on any dense enough dataset, and, at
the same time, have very small VC dimension; hence a watershed classifier leads
to good generalization.
  Traditional approaches to regularize 1NN classifiers are to consider $K$
nearest neighbours. Neighbourhood component analysis (NCA) proposes a way to
learn representations consistent with ($n-1$) nearest neighbour classifier,
where $n$ denotes the size of the dataset. In this article, we propose a loss
function which can learn representations consistent with watershed classifiers,
and show that it outperforms the NCA baseline.
\\ ( https://arxiv.org/abs/2402.08405 ,  2543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08406
Date: Tue, 13 Feb 2024 12:11:40 GMT   (8633kb,D)

Title: Transition Constrained Bayesian Optimization via Markov Decision
  Processes
Authors: Jose Pablo Folch, Calvin Tsay, Robert M Lee, Behrang Shafei, Weronika
  Ormaniec, Andreas Krause, Mark van der Wilk, Ruth Misener, Mojm\'ir Mutn\'y
Categories: cs.LG
Comments: 9 pages main, 24 pages total, 13 figures, 1 table, preprint
\\
  Bayesian optimization is a methodology to optimize black-box functions.
Traditionally, it focuses on the setting where you can arbitrarily query the
search space. However, many real-life problems do not offer this flexibility;
in particular, the search space of the next query may depend on previous ones.
Example challenges arise in the physical sciences in the form of local movement
constraints, required monotonicity in certain variables, and transitions
influencing the accuracy of measurements. Altogether, such transition
constraints necessitate a form of planning. This work extends Bayesian
optimization via the framework of Markov Decision Processes, iteratively
solving a tractable linearization of our objective using reinforcement learning
to obtain a policy that plans ahead over long horizons. The resulting policy is
potentially history-dependent and non-Markovian. We showcase applications in
chemical reactor optimization, informative path planning, machine calibration,
and other synthetic examples.
\\ ( https://arxiv.org/abs/2402.08406 ,  8633kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08421
Date: Tue, 13 Feb 2024 12:49:22 GMT   (355kb,D)

Title: Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning
  for Digital Twins
Authors: Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab and
  Hirley Alves
Categories: cs.LG cs.MA
\\
  Digital twin (DT) platforms are increasingly regarded as a promising
technology for controlling, optimizing, and monitoring complex engineering
systems such as next-generation wireless networks. An important challenge in
adopting DT solutions is their reliance on data collected offline, lacking
direct access to the physical environment. This limitation is particularly
severe in multi-agent systems, for which conventional multi-agent reinforcement
(MARL) requires online interactions with the environment. A direct application
of online MARL schemes to an offline setting would generally fail due to the
epistemic uncertainty entailed by the limited availability of data. In this
work, we propose an offline MARL scheme for DT-based wireless networks that
integrates distributional RL and conservative Q-learning to address the
environment's inherent aleatoric uncertainty and the epistemic uncertainty
arising from limited data. To further exploit the offline data, we adapt the
proposed scheme to the centralized training decentralized execution framework,
allowing joint training of the agents' policies. The proposed MARL scheme,
referred to as multi-agent conservative quantile regression (MA-CQR) addresses
general risk-sensitive design criteria and is applied to the trajectory
planning problem in drone networks, showcasing its advantages.
\\ ( https://arxiv.org/abs/2402.08421 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08450
Date: Tue, 13 Feb 2024 13:37:13 GMT   (5467kb,D)

Title: Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph
  Products
Authors: Guy Bar-Shalom, Beatrice Bevilacqua, Haggai Maron
Categories: cs.LG
\\
  In the realm of Graph Neural Networks (GNNs), two exciting research
directions have recently emerged: Subgraph GNNs and Graph Transformers. In this
paper, we propose an architecture that integrates both approaches, dubbed
Subgraphormer, which combines the enhanced expressive power, message-passing
mechanisms, and aggregation schemes from Subgraph GNNs with attention and
positional encodings, arguably the most important components in Graph
Transformers. Our method is based on an intriguing new connection we reveal
between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be
formulated as Message Passing Neural Networks (MPNNs) operating on a product of
the graph with itself. We use this formulation to design our architecture:
first, we devise an attention mechanism based on the connectivity of the
product graph. Following this, we propose a novel and efficient positional
encoding scheme for Subgraph GNNs, which we derive as a positional encoding for
the product graph. Our experimental results demonstrate significant performance
improvements over both Subgraph GNNs and Graph Transformers on a wide range of
datasets.
\\ ( https://arxiv.org/abs/2402.08450 ,  5467kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08470
Date: Tue, 13 Feb 2024 14:00:59 GMT   (15871kb,D)

Title: Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic
  Degradation Analysis at Scale
Authors: Yangxin Fan, Raymond Wieser, Laura Bruckman, Roger French, Yinghui Wu
Categories: cs.LG cs.AI cs.DC
\\
  We propose a novel Spatio-Temporal Graph Neural Network empowered trend
analysis approach (ST-GTrend) to perform fleet-level performance degradation
analysis for Photovoltaic (PV) power networks. PV power stations have become an
integral component to the global sustainable energy production landscape.
Accurately estimating the performance of PV systems is critical to their
feasibility as a power generation technology and as a financial asset. One of
the most challenging problems in assessing the Levelized Cost of Energy (LCOE)
of a PV system is to understand and estimate the long-term Performance Loss
Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates
spatio-temporal coherence and graph attention to separate PLR as a long-term
"aging" trend from multiple fluctuation terms in the PV input data. To cope
with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled
graph autoencoder array to extract aging and fluctuation terms simultaneously.
ST-GTrend imposes flatness and smoothness regularization to ensure the
disentanglement between aging and fluctuation. To scale the analysis to large
PV systems, we also introduce Para-GTrend, a parallel algorithm to accelerate
the training and inference of ST-GTrend. We have evaluated ST-GTrend on three
large-scale PV datasets, spanning a time period of 10 years. Our results show
that ST-GTrend reduces Mean Absolute Percent Error (MAPE) and Euclidean
Distances by 34.74% and 33.66% compared to the SOTA methods. Our results
demonstrate that Para-GTrend can speed up ST-GTrend by up to 7.92 times. We
further verify the generality and effectiveness of ST-GTrend for trend analysis
using financial and economic datasets.
\\ ( https://arxiv.org/abs/2402.08470 ,  15871kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08480
Date: Tue, 13 Feb 2024 14:13:17 GMT   (8294kb,D)

Title: Revealing Decurve Flows for Generalized Graph Propagation
Authors: Chen Lin, Liheng Ma, Yiyang Chen, Wanli Ouyang, Michael M. Bronstein
  and Philip H.S. Torr
Categories: cs.LG math.DG
Comments: 15 pages, 4 figures
\\
  This study addresses the limitations of the traditional analysis of
message-passing, central to graph learning, by defining {\em
\textbf{generalized propagation}} with directed and weighted graphs. The
significance manifest in two ways. \textbf{Firstly}, we propose {\em
Generalized Propagation Neural Networks} (\textbf{GPNNs}), a framework that
unifies most propagation-based graph neural networks. By generating
directed-weighted propagation graphs with adjacency function and connectivity
function, GPNNs offer enhanced insights into attention mechanisms across
various graph models. We delve into the trade-offs within the design space with
empirical experiments and emphasize the crucial role of the adjacency function
for model expressivity via theoretical analysis. \textbf{Secondly}, we propose
the {\em Continuous Unified Ricci Curvature} (\textbf{CURC}), an extension of
celebrated {\em Ollivier-Ricci Curvature} for directed and weighted graphs.
Theoretically, we demonstrate that CURC possesses continuity, scale invariance,
and a lower bound connection with the Dirichlet isoperimetric constant
validating bottleneck analysis for GPNNs. We include a preliminary exploration
of learned propagation patterns in datasets, a first in the field. We observe
an intriguing ``{\em \textbf{decurve flow}}'' - a curvature reduction during
training for models with learnable propagation, revealing the evolution of
propagation over time and a deeper connection to over-smoothing and bottleneck
trade-off.
\\ ( https://arxiv.org/abs/2402.08480 ,  8294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08491
Date: Tue, 13 Feb 2024 14:36:46 GMT   (870kb,D)

Title: Deep Reinforcement Learning for Controlled Traversing of the Attractor
  Landscape of Boolean Models in the Context of Cellular Reprogramming
Authors: Andrzej Mizera, Jakub Zarzycki
Categories: cs.LG cs.AI q-bio.MN q-bio.QM
\\
  Cellular reprogramming can be used for both the prevention and cure of
different diseases. However, the efficiency of discovering reprogramming
strategies with classical wet-lab experiments is hindered by lengthy time
commitments and high costs. In this study, we develop a~novel computational
framework based on deep reinforcement learning that facilitates the
identification of reprogramming strategies. For this aim, we formulate
a~control problem in the context of cellular reprogramming for the frameworks
of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce
the notion of a~pseudo-attractor and a~procedure for identification of
pseudo-attractor state during training. Finally, we devise a~computational
framework for solving the control problem, which we test on a~number of
different models.
\\ ( https://arxiv.org/abs/2402.08491 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08493
Date: Tue, 13 Feb 2024 14:41:28 GMT   (184kb,D)

Title: Sparsity via Sparse Group $k$-max Regularization
Authors: Qinghua Tao, Xiangming Xi, Jun Xu and Johan A.K. Suykens
Categories: cs.LG stat.ML
Comments: 7 pages, accepted to American Control Conference 2024
\\
  For the linear inverse problem with sparsity constraints, the $l_0$
regularized problem is NP-hard, and existing approaches either utilize greedy
algorithms to find almost-optimal solutions or to approximate the $l_0$
regularization with its convex counterparts. In this paper, we propose a novel
and concise regularization, namely the sparse group $k$-max regularization,
which can not only simultaneously enhance the group-wise and in-group sparsity,
but also casts no additional restraints on the magnitude of variables in each
group, which is especially important for variables at different scales, so that
it approximate the $l_0$ norm more closely. We also establish an iterative soft
thresholding algorithm with local optimality conditions and complexity analysis
provided. Through numerical experiments on both synthetic and real-world
datasets, we verify the effectiveness and flexibility of the proposed method.
\\ ( https://arxiv.org/abs/2402.08493 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08502
Date: Tue, 13 Feb 2024 14:59:19 GMT   (1275kb,D)

Title: Provable Traffic Rule Compliance in Safe Reinforcement Learning on the
  Open Sea
Authors: Hanna Krasowski, Matthias Althoff
Categories: cs.LG cs.SY eess.SY
\\
  Autonomous vehicles have to obey traffic rules. These rules are often
formalized using temporal logic, resulting in constraints that are hard to
solve using optimization-based motion planners. Reinforcement Learning (RL) is
a promising method to find motion plans adhering to temporal logic
specifications. However, vanilla RL algorithms are based on random exploration,
which is inherently unsafe. To address this issue, we propose a provably safe
RL approach that always complies with traffic rules. As a specific application
area, we consider vessels on the open sea, which must adhere to the Convention
on the International Regulations for Preventing Collisions at Sea (COLREGS). We
introduce an efficient verification approach that determines the compliance of
actions with respect to the COLREGS formalized using temporal logic. Our action
verification is integrated into the RL process so that the agent only selects
verified actions. In contrast to agents that only integrate the traffic rule
information in the reward function, our provably safe agent always complies
with the formalized rules in critical maritime traffic situations and, thus,
never causes a collision.
\\ ( https://arxiv.org/abs/2402.08502 ,  1275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08522
Date: Tue, 13 Feb 2024 15:24:46 GMT   (521kb,D)

Title: Fairness Auditing with Multi-Agent Collaboration
Authors: Martijn de Vos and Akash Dhasade and Jade Garcia Bourr\'ee and
  Anne-Marie Kermarrec and Erwan Le Merrer and Benoit Rottembourg and Gilles
  Tredan
Categories: cs.LG
Comments: 21 pages, 7 figures
\\
  Existing work in fairness audits assumes that agents operate independently.
In this paper, we consider the case of multiple agents auditing the same
platform for different tasks. Agents have two levers: their collaboration
strategy, with or without coordination beforehand, and their sampling method.
We theoretically study their interplay when agents operate independently or
collaborate. We prove that, surprisingly, coordination can sometimes be
detrimental to audit accuracy, whereas uncoordinated collaboration generally
yields good results. Experimentation on real-world datasets confirms this
observation, as the audit accuracy of uncoordinated collaboration matches that
of collaborative optimal sampling.
\\ ( https://arxiv.org/abs/2402.08522 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08526
Date: Tue, 13 Feb 2024 15:29:50 GMT   (5072kb,D)

Title: Concept-1K: A Novel Benchmark for Instance Incremental Learning
Authors: Junhao Zheng, Shengjie Qiu, Qianli Ma
Categories: cs.LG cs.CL
\\
  Incremental learning (IL) is essential to realize the human-level
intelligence in the neural network. However, existing IL scenarios and datasets
are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs
do not suffer from catastrophic forgetting. To this end, we propose a
challenging IL scenario called instance-incremental learning (IIL) and a novel
dataset called Concept-1K, which supports an order of magnitude larger IL
steps. Based on the experiments on Concept-1K, we reveal that billion-parameter
PLMs still suffer from catastrophic forgetting, and the forgetting is affected
by both model scale, pretraining, and buffer size. Furthermore, existing IL
methods and a popular finetuning technique, LoRA, fail to achieve satisfactory
performance. Our study provides a novel scenario for future studies to explore
the catastrophic forgetting of PLMs and encourage more powerful techniques to
be designed for alleviating the forgetting in PLMs. The data, code and scripts
are publicly available at
https://github.com/zzz47zzz/pretrained-lm-for-incremental-learning.
\\ ( https://arxiv.org/abs/2402.08526 ,  5072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08529
Date: Tue, 13 Feb 2024 15:34:39 GMT   (12382kb,D)

Title: Approximately Piecewise E(3) Equivariant Point Networks
Authors: Matan Atzmon, Jiahui Huang, Francis Williams, Or Litany
Categories: cs.LG cs.CV
Comments: ICLR 2024
\\
  Integrating a notion of symmetry into point cloud neural networks is a
provably effective way to improve their generalization capability. Of
particular interest are $E(3)$ equivariant point cloud networks where Euclidean
transformations applied to the inputs are preserved in the outputs. Recent
efforts aim to extend networks that are $E(3)$ equivariant, to accommodate
inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In
practical settings, however, the partitioning into individually transforming
regions is unknown a priori. Errors in the partition prediction would
unavoidably map to errors in respecting the true input symmetry. Past works
have proposed different ways to predict the partition, which may exhibit
uncontrolled errors in their ability to maintain equivariance to the actual
partition. To this end, we introduce APEN: a general framework for constructing
approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is
that functions that are equivariant with respect to a finer partition will also
maintain equivariance in relation to the true partition. Leveraging this
observation, we propose a design where the equivariance approximation error at
each layers can be bounded solely in terms of (i) uncertainty quantification of
the partition prediction, and (ii) bounds on the probability of failing to
suggest a proper subpartition of the ground truth one. We demonstrate the
effectiveness of APEN using two data types exemplifying part-based symmetry:
(i) real-world scans of room scenes containing multiple furniture-type objects;
and, (ii) human motions, characterized by articulated parts exhibiting rigid
movement. Our empirical results demonstrate the advantage of integrating
piecewise $E(3)$ symmetry into network design, showing a distinct improvement
in generalization compared to prior works for both classification and
segmentation tasks.
\\ ( https://arxiv.org/abs/2402.08529 ,  12382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08530
Date: Tue, 13 Feb 2024 15:35:24 GMT   (1359kb,D)

Title: A Distributional Analogue to the Successor Representation
Authors: Harley Wiltzer and Jesse Farebrother and Arthur Gretton and Yunhao
  Tang and Andr\'e Barreto and Will Dabney and Marc G. Bellemare and Mark
  Rowland
Categories: cs.LG cs.AI stat.ML
\\
  This paper contributes a new approach for distributional reinforcement
learning which elucidates a clean separation of transition structure and reward
in the learning process. Analogous to how the successor representation (SR)
describes the expected consequences of behaving according to a given policy,
our distributional successor measure (SM) describes the distributional
consequences of this behaviour. We formulate the distributional SM as a
distribution over distributions and provide theory connecting it with
distributional and model-based reinforcement learning. Moreover, we propose an
algorithm that learns the distributional SM from data by minimizing a two-level
maximum mean discrepancy. Key to our method are a number of algorithmic
techniques that are independently valuable for learning generative models of
state. As an illustration of the usefulness of the distributional SM, we show
that it enables zero-shot risk-sensitive policy evaluation in a way that was
not previously possible.
\\ ( https://arxiv.org/abs/2402.08530 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08539
Date: Tue, 13 Feb 2024 15:43:30 GMT   (617kb)

Title: Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning
Authors: Mingyang Li, Hongyu Liu, Yixuan Li, Zejun Wang, Yuan Yuan, Honglin Dai
Categories: cs.LG stat.AP
\\
  This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI)
dataset and aims to explore early detection and disease progression in
Alzheimer's disease (AD). We employ innovative data preprocessing strategies,
including the use of the random forest algorithm to fill missing data and the
handling of outliers and invalid data, thereby fully mining and utilizing these
limited data resources. Through Spearman correlation coefficient analysis, we
identify some features strongly correlated with AD diagnosis. We build and test
three machine learning models using these features: random forest, XGBoost, and
support vector machine (SVM). Among them, the XGBoost model performs the best
in terms of diagnostic performance, achieving an accuracy of 91%. Overall, this
study successfully overcomes the challenge of missing data and provides
valuable insights into early detection of Alzheimer's disease, demonstrating
its unique research value and practical significance.
\\ ( https://arxiv.org/abs/2402.08539 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08540
Date: Tue, 13 Feb 2024 15:45:20 GMT   (409kb,D)

Title: Generative VS non-Generative Models in Engineering Shape Optimization
Authors: Muhammad Usama, Zahid Masood, Shahroz Khan, Konstantinos Kostas,
  Panagiotis Kaklis
Categories: cs.LG
\\
  In this work, we perform a systematic comparison of the effectiveness and
efficiency of generative and non-generative models in constructing design
spaces for novel and efficient design exploration and shape optimization. We
apply these models in the case of airfoil/hydrofoil design and conduct the
comparison on the resulting design spaces. A conventional Generative
Adversarial Network (GAN) and a state-of-the-art generative model, the
Performance-Augmented Diverse Generative Adversarial Network (PaDGAN), are
juxtaposed with a linear non-generative model based on the coupling of the
Karhunen-Lo\`eve Expansion and a physics-informed Shape Signature Vector
(SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding
and a physics-augmented design space, non-generative models have the potential
to cost-effectively generate high-performing valid designs with enhanced
coverage of the design space. In this work, both approaches are applied to two
large foil profile datasets comprising real-world and artificial designs
generated through either a profile-generating parametric model or deep-learning
approach. These datasets are further enriched with integral properties of their
members' shapes as well as physics-informed parameters. Our results illustrate
that the design spaces constructed by the non-generative model outperform the
generative model in terms of design validity, generating robust latent spaces
with none or significantly fewer invalid designs when compared to generative
models. We aspire that these findings will aid the engineering design community
in making informed decisions when constructing designs spaces for shape
optimization, as we have show that under certain conditions computationally
inexpensive approaches can closely match or even outperform state-of-the art
generative models.
\\ ( https://arxiv.org/abs/2402.08540 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08552
Date: Tue, 13 Feb 2024 15:55:41 GMT   (4064kb,D)

Title: Confronting Reward Overoptimization for Diffusion Models: A Perspective
  of Inductive and Primacy Biases
Authors: Ziyi Zhang and Sen Zhang and Yibing Zhan and Yong Luo and Yonggang Wen
  and Dacheng Tao
Categories: cs.LG cs.CV
\\
  Bridging the gap between diffusion models and human preferences is crucial
for their integration into practical generative workflows. While optimizing
downstream reward models has emerged as a promising alignment strategy,
concerns arise regarding the risk of excessive optimization with learned reward
models, which potentially compromises ground-truth performance. In this work,
we confront the reward overoptimization problem in diffusion model alignment
through the lenses of both inductive and primacy biases. We first identify the
divergence of current methods from the temporal inductive bias inherent in the
multi-step denoising process of diffusion models as a potential source of
overoptimization. Then, we surprisingly discover that dormant neurons in our
critic model act as a regularization against overoptimization, while active
neurons reflect primacy bias in this setting. Motivated by these observations,
we propose Temporal Diffusion Policy Optimization with critic active neuron
Reset (TDPO-R), a policy gradient algorithm that exploits the temporal
inductive bias of intermediate timesteps, along with a novel reset strategy
that targets active neurons to counteract the primacy bias. Empirical results
demonstrate the superior efficacy of our algorithms in mitigating reward
overoptimization.
\\ ( https://arxiv.org/abs/2402.08552 ,  4064kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08563
Date: Tue, 13 Feb 2024 16:04:41 GMT   (2436kb,D)

Title: Denoising Diffusion Restoration Tackles Forward and Inverse Problems for
  the Laplace Operator
Authors: Amartya Mukherjee, Melissa M. Stadt, Lena Podina, Mohammad Kohandel,
  Jun Liu
Categories: cs.LG cs.CV math.AP
Comments: 29 pages
\\
  Diffusion models have emerged as a promising class of generative models that
map noisy inputs to realistic images. More recently, they have been employed to
generate solutions to partial differential equations (PDEs). However, they
still struggle with inverse problems in the Laplacian operator, for instance,
the Poisson equation, because the eigenvalues that are large in magnitude
amplify the measurement noise. This paper presents a novel approach for the
inverse and forward solution of PDEs through the use of denoising diffusion
restoration models (DDRM). DDRMs were used in linear inverse problems to
restore original clean signals by exploiting the singular value decomposition
(SVD) of the linear operator. Equivalently, we present an approach to restore
the solution and the parameters in the Poisson equation by exploiting the
eigenvalues and the eigenfunctions of the Laplacian operator. Our results show
that using denoising diffusion restoration significantly improves the
estimation of the solution and parameters. Our research, as a result, pioneers
the integration of diffusion models with the principles of underlying physics
to solve PDEs.
\\ ( https://arxiv.org/abs/2402.08563 ,  2436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08573
Date: Tue, 13 Feb 2024 16:21:18 GMT   (382kb,D)

Title: Two Tales of Single-Phase Contrastive Hebbian Learning
Authors: Rasmus Kj{\ae}r H{\o}ier and Christopher Zach
Categories: cs.LG cs.NE
Comments: 18 pages
\\
  The search for "biologically plausible" learning algorithms has converged on
the idea of representing gradients as activity differences. However, most
approaches require a high degree of synchronization (distinct phases during
learning) and introduce substantial computational overhead, which raises doubts
regarding their biological plausibility as well as their potential utility for
neuromorphic computing. Furthermore, they commonly rely on applying
infinitesimal perturbations (nudges) to output units, which is impractical in
noisy environments. Recently it has been shown that by modelling artificial
neurons as dyads with two oppositely nudged compartments, it is possible for a
fully local learning algorithm named ``dual propagation'' to bridge the
performance gap to backpropagation, without requiring separate learning phases
or infinitesimal nudging. However, the algorithm has the drawback that its
numerical stability relies on symmetric nudging, which may be restrictive in
biological and analog implementations. In this work we first provide a solid
foundation for the objective underlying the dual propagation method, which also
reveals a surprising connection with adversarial robustness. Second, we
demonstrate how dual propagation is related to a particular adjoint state
method, which is stable regardless of asymmetric nudging.
\\ ( https://arxiv.org/abs/2402.08573 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08578
Date: Tue, 13 Feb 2024 16:30:30 GMT   (191kb,D)

Title: FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local
  Parameter Sharing
Authors: Yongzhe Jia, Xuyun Zhang, Amin Beheshti, Wanchun Dou
Categories: cs.LG cs.AI cs.DC
Comments: Accepted by AAAI 2024
\\
  Federated Learning (FL) has emerged as a promising solution in Edge Computing
(EC) environments to process the proliferation of data generated by edge
devices. By collaboratively optimizing the global machine learning models on
distributed edge devices, FL circumvents the need for transmitting raw data and
enhances user privacy. Despite practical successes, FL still confronts
significant challenges including constrained edge device resources, multiple
tasks deployment, and data heterogeneity. However, existing studies focus on
mitigating the FL training costs of each single task whereas neglecting the
resource consumption across multiple tasks in heterogeneous FL scenarios. In
this paper, we propose Heterogeneous Federated Learning with Local Parameter
Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer
learning to facilitate the deployment of multiple tasks on a single device by
dividing the local model into a shareable encoder and task-specific encoders.
To further reduce resource consumption, a channel-wise model pruning algorithm
that shrinks the footprint of local models while accounting for both data and
system heterogeneity is employed in FedLPS. Additionally, a novel heterogeneous
model aggregation algorithm is proposed to aggregate the heterogeneous
predictors in FedLPS. We implemented the proposed FedLPS on a real FL platform
and compared it with state-of-the-art (SOTA) FL frameworks. The experimental
results on five popular datasets and two modern DNN models illustrate that the
proposed FedLPS significantly outperforms the SOTA FL frameworks by up to 4.88%
and reduces the computational resource consumption by 21.3%. Our code is
available at:https://github.com/jyzgh/FedLPS.
\\ ( https://arxiv.org/abs/2402.08578 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08583
Date: Tue, 13 Feb 2024 16:36:50 GMT   (1989kb,D)

Title: Mixture of Link Predictors
Authors: Li Ma, Haoyu Han, Juanhui Li, Harry Shomer, Hui Liu, Xiaofeng Gao,
  Jiliang Tang
Categories: cs.LG
\\
  Link prediction, which aims to forecast unseen connections in graphs, is a
fundamental task in graph machine learning. Heuristic methods, leveraging a
range of different pairwise measures such as common neighbors and shortest
paths, often rival the performance of vanilla Graph Neural Networks (GNNs).
Therefore, recent advancements in GNNs for link prediction (GNN4LP) have
primarily focused on integrating one or a few types of pairwise information. In
this work, we reveal that different node pairs within the same dataset
necessitate varied pairwise information for accurate prediction and models that
only apply the same pairwise information uniformly could achieve suboptimal
performance. As a result, we propose a simple mixture of experts model Link-MoE
for link prediction. Link-MoE utilizes various GNNs as experts and
strategically selects the appropriate expert for each node pair based on
various types of pairwise information. Experimental results across diverse
real-world datasets demonstrate substantial performance improvement from
Link-MoE. Notably, Link-MoE achieves a relative improvement of 18.82\% on the
MRR metric for the Pubmed dataset and 10.8\% on the Hits@100 metric for the
ogbl-ppa dataset, compared to the best baselines.
\\ ( https://arxiv.org/abs/2402.08583 ,  1989kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08586
Date: Tue, 13 Feb 2024 16:44:02 GMT   (391kb,D)

Title: Faster Repeated Evasion Attacks in Tree Ensembles
Authors: Lorenzo Cascioli, Laurens Devos, Ond\v{r}ej Ku\v{z}elka, Jesse Davis
Categories: cs.LG
\\
  Tree ensembles are one of the most widely used model classes. However, these
models are susceptible to adversarial examples, i.e., slightly perturbed
examples that elicit a misprediction. There has been significant research on
designing approaches to construct such examples for tree ensembles. But this is
a computationally challenging problem that often must be solved a large number
of times (e.g., for all examples in a training set). This is compounded by the
fact that current approaches attempt to find such examples from scratch. In
contrast, we exploit the fact that multiple similar problems are being solved.
Specifically, our approach exploits the insight that adversarial examples for
tree ensembles tend to perturb a consistent but relatively small set of
features. We show that we can quickly identify this set of features and use
this knowledge to speedup constructing adversarial examples.
\\ ( https://arxiv.org/abs/2402.08586 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08593
Date: Tue, 13 Feb 2024 16:53:48 GMT   (634kb,D)

Title: Graph Feature Preprocessor: Real-time Extraction of Subgraph-based
  Features from Transaction Graphs
Authors: Jovan Blanu\v{s}a, Maximo Cravero Baraja, Andreea Anghel, Luc von
  Niederh\"ausern, Erik Altman, Haris Pozidis and Kubilay Atasu
Categories: cs.LG cs.AI
\\
  In this paper, we present "Graph Feature Preprocessor", a software library
for detecting typical money laundering and fraud patterns in financial
transaction graphs in real time. These patterns are used to produce a rich set
of transaction features for downstream machine learning training and inference
tasks such as money laundering detection. We show that our enriched transaction
features dramatically improve the prediction accuracy of
gradient-boosting-based machine learning models. Our library exploits multicore
parallelism, maintains a dynamic in-memory graph, and efficiently mines
subgraph patterns in the incoming transaction stream, which enables it to be
operated in a streaming manner. We evaluate our library using highly-imbalanced
synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets.
In these datasets, the proportion of illicit transactions is very small, which
makes the learning process challenging. Our solution, which combines our Graph
Feature Preprocessor and gradient-boosting-based machine learning models, is
able to detect these illicit transactions with higher minority-class F1 scores
than standard graph neural networks. In addition, the end-to-end throughput
rate of our solution executed on a multicore CPU outperforms the graph neural
network baselines executed on a powerful V100 GPU. Overall, the combination of
high accuracy, a high throughput rate, and low latency of our solution
demonstrates the practical value of our library in real-world applications.
Graph Feature Preprocessor has been integrated into IBM mainframe software
products, namely "IBM Cloud Pak for Data on Z" and "AI Toolkit for IBM Z and
LinuxONE".
\\ ( https://arxiv.org/abs/2402.08593 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08595
Date: Tue, 13 Feb 2024 16:57:06 GMT   (144kb,D)

Title: Homomorphism Counts for Graph Neural Networks: All About That Basis
Authors: Emily Jin, Michael Bronstein, Ismail Ilkan Ceylan, Matthias Lanzinger
Categories: cs.LG
\\
  Graph neural networks are architectures for learning invariant functions over
graphs. A large body of work has investigated the properties of graph neural
networks and identified several limitations, particularly pertaining to their
expressive power. Their inability to count certain patterns (e.g., cycles) in a
graph lies at the heart of such limitations, since many functions to be learned
rely on the ability of counting such patterns. Two prominent paradigms aim to
address this limitation by enriching the graph features with subgraph or
homomorphism pattern counts. In this work, we show that both of these
approaches are sub-optimal in a certain sense and argue for a more fine-grained
approach, which incorporates the homomorphism counts of all structures in the
"basis" of the target pattern. This yields strictly more expressive
architectures without incurring any additional overhead in terms of
computational complexity compared to existing approaches. We prove a series of
theoretical results on node-level and graph-level motif parameters and
empirically validate them on standard benchmark datasets.
\\ ( https://arxiv.org/abs/2402.08595 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08609
Date: Tue, 13 Feb 2024 17:18:56 GMT   (6641kb,D)

Title: Mixtures of Experts Unlock Parameter Scaling for Deep RL
Authors: Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse
  Farebrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, Pablo
  Samuel Castro
Categories: cs.LG cs.AI
\\
  The recent rapid progress in (self) supervised learning models is in large
part predicted by empirical scaling laws: a model's performance scales
proportionally to its size. Analogous scaling laws remain elusive for
reinforcement learning domains, however, where increasing the parameter count
of a model often hurts its final performance. In this paper, we demonstrate
that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs
(Puigcerver et al., 2023), into value-based networks results in more
parameter-scalable models, evidenced by substantial performance increases
across a variety of training regimes and model sizes. This work thus provides
strong empirical evidence towards developing scaling laws for reinforcement
learning.
\\ ( https://arxiv.org/abs/2402.08609 ,  6641kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08611
Date: Tue, 16 Jan 2024 15:09:53 GMT   (1171kb,D)

Title: A Cost-Sensitive Transformer Model for Prognostics Under Highly
  Imbalanced Industrial Data
Authors: Ali Beikmohammadi, Mohammad Hosein Hamian, Neda Khoeyniha, Tony
  Lindgren, Olof Steinert, and Sindri Magn\'usson
Categories: cs.LG cs.AI
\\
  The rapid influx of data-driven models into the industrial sector has been
facilitated by the proliferation of sensor technology, enabling the collection
of vast quantities of data. However, leveraging these models for failure
detection and prognosis poses significant challenges, including issues like
missing values and class imbalances. Moreover, the cost sensitivity associated
with industrial operations further complicates the application of conventional
models in this context. This paper introduces a novel cost-sensitive
transformer model developed as part of a systematic workflow, which also
integrates a hybrid resampler and a regression-based imputer. After subjecting
our approach to rigorous testing using the APS failure dataset from Scania
trucks and the SECOM dataset, we observed a substantial enhancement in
performance compared to state-of-the-art methods. Moreover, we conduct an
ablation study to analyze the contributions of different components in our
proposed method. Our findings highlight the potential of our method in
addressing the unique challenges of failure prediction in industrial settings,
thereby contributing to enhanced reliability and efficiency in industrial
operations.
\\ ( https://arxiv.org/abs/2402.08611 ,  1171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08621
Date: Tue, 13 Feb 2024 17:42:27 GMT   (88kb,D)

Title: A Generalized Approach to Online Convex Optimization
Authors: Mohammad Pedramfar, Vaneet Aggarwal
Categories: cs.LG math.OC stat.ML
\\
  In this paper, we analyze the problem of online convex optimization in
different settings. We show that any algorithm for online linear optimization
with fully adaptive adversaries is an algorithm for online convex optimization.
We also show that any such algorithm that requires full-information feedback
may be transformed to an algorithm with semi-bandit feedback with comparable
regret bound. We further show that algorithms that are designed for fully
adaptive adversaries using deterministic semi-bandit feedback can obtain
similar bounds using only stochastic semi-bandit feedback when facing oblivious
adversaries. We use this to describe general meta-algorithms to convert first
order algorithms to zeroth order algorithms with comparable regret bounds. Our
framework allows us to analyze online optimization in various settings, such
full-information feedback, bandit feedback, stochastic regret, adversarial
regret and various forms of non-stationary regret. Using our analysis, we
provide the first efficient projection-free online convex optimization
algorithm using linear optimization oracles.
\\ ( https://arxiv.org/abs/2402.08621 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08648
Date: Tue, 13 Feb 2024 18:27:53 GMT   (23280kb,D)

Title: Generating Universal Adversarial Perturbations for Quantum Classifiers
Authors: Gautham Anil, Vishnu Vinod, Apurva Narayan
Categories: cs.LG cs.AI
Comments: Accepted at AAAI 2024
\\
  Quantum Machine Learning (QML) has emerged as a promising field of research,
aiming to leverage the capabilities of quantum computing to enhance existing
machine learning methodologies. Recent studies have revealed that, like their
classical counterparts, QML models based on Parametrized Quantum Circuits
(PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of
Universal Adversarial Perturbations (UAPs) in the quantum domain has been
demonstrated theoretically in the context of quantum classifiers. In this work,
we introduce QuGAP: a novel framework for generating UAPs for quantum
classifiers. We conceptualize the notion of additive UAPs for PQC-based
classifiers and theoretically demonstrate their existence. We then utilize
generative models (QuGAP-A) to craft additive UAPs and experimentally show that
quantum classifiers are susceptible to such attacks. Moreover, we formulate a
new method for generating unitary UAPs (QuGAP-U) using quantum generative
models and a novel loss function based on fidelity constraints. We evaluate the
performance of the proposed framework and show that our method achieves
state-of-the-art misclassification rates, while maintaining high fidelity
between legitimate and adversarial samples.
\\ ( https://arxiv.org/abs/2402.08648 ,  23280kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08653
Date: Tue, 13 Feb 2024 18:33:45 GMT   (5246kb,D)

Title: SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
Authors: Wuxinlin Cheng, Chenhui Deng, Ali Aghdaei, Zhiru Zhang, Zhuo Feng
Categories: cs.LG cs.AI
\\
  Modern graph neural networks (GNNs) can be sensitive to changes in the input
graph structure and node features, potentially resulting in unpredictable
behavior and degraded performance. In this work, we introduce a spectral
framework known as SAGMAN for examining the stability of GNNs. This framework
assesses the distance distortions that arise from the nonlinear mappings of
GNNs between the input and output manifolds: when two nearby nodes on the input
manifold are mapped (through a GNN model) to two distant ones on the output
manifold, it implies a large distance distortion and thus a poor GNN stability.
We propose a distance-preserving graph dimension reduction (GDR) approach that
utilizes spectral graph embedding and probabilistic graphical models (PGMs) to
create low-dimensional input/output graph-based manifolds for meaningful
stability analysis. Our empirical evaluations show that SAGMAN effectively
assesses the stability of each node when subjected to various edge or feature
perturbations, offering a scalable approach for evaluating the stability of
GNNs, extending to applications within recommendation systems. Furthermore, we
illustrate its utility in downstream tasks, notably in enhancing GNN stability
and facilitating adversarial targeted attacks.
\\ ( https://arxiv.org/abs/2402.08653 ,  5246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08667
Date: Tue, 13 Feb 2024 18:48:28 GMT   (1166kb,D)

Title: Target Score Matching
Authors: Valentin De Bortoli, Michael Hutchinson, Peter Wirnsberger, Arnaud
  Doucet
Categories: cs.LG stat.CO stat.ML
\\
  Denoising Score Matching estimates the score of a noised version of a target
distribution by minimizing a regression loss and is widely used to train the
popular class of Denoising Diffusion Models. A well known limitation of
Denoising Score Matching, however, is that it yields poor estimates of the
score at low noise levels. This issue is particularly unfavourable for problems
in the physical sciences and for Monte Carlo sampling tasks for which the score
of the clean original target is known. Intuitively, estimating the score of a
slightly noised version of the target should be a simple task in such cases. In
this paper, we address this shortcoming and show that it is indeed possible to
leverage knowledge of the target score. We present a Target Score Identity and
corresponding Target Score Matching regression loss which allows us to obtain
score estimates admitting favourable properties at low noise levels.
\\ ( https://arxiv.org/abs/2402.08667 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08672
Date: Tue, 13 Feb 2024 18:54:08 GMT   (1078kb,D)

Title: Model Assessment and Selection under Temporal Distribution Shift
Authors: Elise Han, Chengpiao Huang, Kaizheng Wang
Categories: cs.LG cs.AI stat.ME
Comments: 24 pages, 6 figures
MSC-class: 62G05 (Primary), 62J02 (Secondary)
\\
  We investigate model assessment and selection in a changing environment, by
synthesizing datasets from both the current time period and historical epochs.
To tackle unknown and potentially arbitrary temporal distribution shift, we
develop an adaptive rolling window approach to estimate the generalization
error of a given model. This strategy also facilitates the comparison between
any two candidate models by estimating the difference of their generalization
errors. We further integrate pairwise comparisons into a single-elimination
tournament, achieving near-optimal model selection from a collection of
candidates. Theoretical analyses and numerical experiments demonstrate the
adaptivity of our proposed methods to the non-stationarity in data.
\\ ( https://arxiv.org/abs/2402.08672 ,  1078kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08676
Date: Tue, 13 Feb 2024 18:56:55 GMT   (72kb,D)

Title: A Convergence Analysis of Approximate Message Passing with Non-Separable
  Functions and Applications to Multi-Class Classification
Authors: Burak \c{C}akmak, Yue M. Lu, Manfred Opper
Categories: cs.LG cs.IT math.IT
\\
  Motivated by the recent application of approximate message passing (AMP) to
the analysis of convex optimizations in multi-class classifications [Loureiro,
et. al., 2021], we present a convergence analysis of AMP dynamics with
non-separable multivariate nonlinearities. As an application, we present a
complete (and independent) analysis of the motivated convex optimization
problem.
\\ ( https://arxiv.org/abs/2402.08676 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08678
Date: Tue, 13 Feb 2024 18:58:17 GMT   (434kb,D)

Title: Graph Mamba: Towards Learning on Graphs with State Space Models
Authors: Ali Behrouz and Farnoosh Hashemi
Categories: cs.LG
\\
  Graph Neural Networks (GNNs) have shown promising potential in graph
representation learning. The majority of GNNs define a local message-passing
mechanism, propagating information over the graph by stacking multiple layers.
These methods, however, are known to suffer from two major limitations:
over-squashing and poor capturing of long-range dependencies. Recently, Graph
Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural
Networks (MPNNs). GTs, however, have quadratic computational cost, lack
inductive biases on graph structures, and rely on complex Positional/Structural
Encodings (SE/PE). In this paper, we show that while Transformers, complex
message-passing, and SE/PE are sufficient for good performance in practice,
neither is necessary. Motivated by the recent success of State Space Models
(SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general
framework for a new class of GNNs based on selective SSMs. We discuss and
categorize the new challenges when adopting SSMs to graph-structured data, and
present four required and one optional steps to design GMNs, where we choose
(1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of
Bidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE
and SE. We further provide theoretical justification for the power of GMNs.
Experiments demonstrate that despite much less computational cost, GMNs attain
an outstanding performance in long-range, small-scale, large-scale, and
heterophilic benchmark datasets.
\\ ( https://arxiv.org/abs/2402.08678 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08679
Date: Tue, 13 Feb 2024 18:58:48 GMT   (410kb,D)

Title: COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
Authors: Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu
Categories: cs.LG cs.AI cs.CL
\\
  Jailbreaks on Large language models (LLMs) have recently received increasing
attention. For a comprehensive assessment of LLM safety, it is essential to
consider jailbreaks with diverse attributes, such as contextual coherence and
sentiment/stylistic variations, and hence it is beneficial to study
controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this
paper, we formally formulate the controllable attack generation problem, and
build a novel connection between this problem and controllable text generation,
a well-explored topic of natural language processing. Based on this connection,
we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a
state-of-the-art, highly efficient algorithm in controllable text generation,
and introduce the COLD-Attack framework which unifies and automates the search
of adversarial LLM attacks under a variety of control requirements such as
fluency, stealthiness, sentiment, and left-right-coherence. The controllability
enabled by COLD-Attack leads to diverse new jailbreak scenarios which not only
cover the standard setting of generating fluent suffix attacks, but also allow
us to address new controllable attack settings such as revising a user query
adversarially with minimal paraphrasing, and inserting stealthy attacks in
context with left-right-coherence. Our extensive experiments on various LLMs
(Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5) show COLD-Attack's broad
applicability, strong controllability, high success rate, and attack
transferability. Our code is available at
https://github.com/Yu-Fangxu/COLD-Attack.
\\ ( https://arxiv.org/abs/2402.08679 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08680
Date: Tue, 13 Feb 2024 18:59:05 GMT   (6508kb,D)

Title: Mitigating Object Hallucination in Large Vision-Language Models via
  Classifier-Free Guidance
Authors: Linxi Zhao and Yihe Deng and Weitong Zhang and Quanquan Gu
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: 27 pages, 20 figures, 4 tables
\\
  The advancement of Large Vision-Language Models (LVLMs) has increasingly
highlighted the critical issue of their tendency to hallucinate non-existing
objects in the images. To address this issue, previous works focused on using
specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the
outputs of LVLMs. However, these approaches require either expensive
training/fine-tuning or API access to advanced LLMs to correct the model's
output post-generation. In this paper, we tackle this challenge by introducing
a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE
(MARINE), which is both training-free and API-free, and can effectively and
efficiently reduce object hallucinations during the generation process.
Specifically, MARINE enriches the visual context of LVLMs by integrating
existing open-source vision models, and employs classifier-free guidance to
incorporate the additional object grounding features to improve the precision
of LVLMs' generations. Through comprehensive evaluations across $6$ popular
LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of
MARINE, which even outperforms existing fine-tuning-based methods. Remarkably,
it not only reduces hallucinations but also improves the detailedness of LVLMs'
generations, as assessed by GPT-4V.
\\ ( https://arxiv.org/abs/2402.08680 ,  6508kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.07909 (*cross-listing*)
Date: Mon, 29 Jan 2024 10:23:47 GMT   (1621kb,D)

Title: Prompt4Vis: Prompting Large Language Models with Example Mining and
  Schema Filtering for Tabular Data Visualization
Authors: Shuaimin Li, Xuanang Chen, Yuanfeng Song, Yunze Song, Chen Zhang
Categories: cs.HC cs.AI cs.CL cs.DB
\\
  Data visualization (DV) systems are increasingly recognized for their
profound capability to uncover insights from vast datasets, gaining attention
across both industry and academia. Crafting data queries is an essential
process within certain declarative visualization languages (DVLs, e.g.,
Vega-Lite, EChart.). The evolution of natural language processing (NLP)
technologies has streamlined the use of natural language interfaces to
visualize tabular data, offering a more accessible and intuitive user
experience. However, current methods for converting natural language questions
into data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite
utilizing complex neural network architectures, still fall short of
expectations and have great room for improvement.
  Large language models (LLMs) such as ChatGPT and GPT-4, have established new
benchmarks in a variety of NLP tasks, fundamentally altering the landscape of
the field. Inspired by these advancements, we introduce a novel framework,
Prompt4Vis, leveraging LLMs and in-context learning to enhance the performance
of generating data visualization from natural language. Prompt4Vis comprises
two key components: (1) a multi-objective example mining module, designed to
find out the truly effective examples that strengthen the LLM's in-context
learning capabilities for text-to-vis; (2) a schema filtering module, which is
proposed to simplify the schema of the database. Extensive experiments through
5-fold cross-validation on the NVBench dataset demonstrate the superiority of
Prompt4Vis, which notably surpasses the state-of-the-art (SOTA) RGVisNet by
approximately 35.9% and 71.3% on dev and test sets, respectively. To the best
of our knowledge, Prompt4Vis is the first work that introduces in-context
learning into the text-to-vis for generating data visualization queries.
\\ ( https://arxiv.org/abs/2402.07909 ,  1621kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07911 (*cross-listing*)
Date: Tue, 30 Jan 2024 08:54:46 GMT   (8297kb,D)

Title: Does mapping elites illuminate search spaces? A large-scale user study
  of MAP--Elites applied to human--AI collaborative design
Authors: Sean P. Walton, Ben J. Evans, Alma A. M. Rahat, James Stovold, Jakub
  Vincalek
Categories: cs.HC cs.AI cs.CE cs.NE
Comments: 17 pages
ACM-class: I.2.0; J.6; G.1.6
\\
  Two studies of a human-AI collaborative design tool were carried out in order
to understand the influence design recommendations have on the design process.
The tool investigated is based on an evolutionary algorithm attempting to
design a virtual car to travel as far as possible in a fixed time. Participants
were able to design their own cars, make recommendations to the algorithm and
view sets of recommendations from the algorithm. The algorithm-recommended sets
were designs which had been previously tested; some sets were simply randomly
picked and other sets were picked using MAP-Elites. In the first study 808
design sessions were recorded as part of a science outreach program, each with
analytical data of how each participant used the tool. To provide context to
this quantitative data, a smaller double-blind lab study was also carried out
with 12 participants. In the lab study the same quantitative data from the
large scale study was collected alongside responses to interview questions.
Although there is some evidence that the MAP-Elites provide higher-quality
individual recommendations, neither study provides convincing evidence that
these recommendations have a more positive influence on the design process than
simply a random selection of designs. In fact, it seems that providing a
combination of MAP-Elites and randomly selected recommendations is beneficial
to the process. Furthermore, simply viewing recommendations from the MAP-Elites
had a positive influence on engagement in the design task and the quality of
the final design produced. Our findings are significant both for researchers
designing new mixed-initiative tools, and those who wish to evaluate existing
tools. Most significantly, we found that metrics researchers currently use to
evaluate the success of human-AI collaborative algorithms do not measure the
full influence these algorithms have on the design process.
\\ ( https://arxiv.org/abs/2402.07911 ,  8297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07912 (*cross-listing*)
Date: Tue, 30 Jan 2024 11:47:12 GMT   (1656kb,D)

Title: Spatial Computing: Concept, Applications, Challenges and Future
  Directions
Authors: Gokul Yenduri, Ramalingam M, Praveen Kumar Reddy Maddikunta, Thippa
  Reddy Gadekallu, Rutvij H Jhaveri, Ajay Bandi, Junxin Chen, Wei Wang, Adarsh
  Arunkumar Shirawalmath, Raghav Ravishankar, Weizheng Wang
Categories: cs.HC cs.AI
Comments: Submitted to peer reviewe
\\
  Spatial computing is a technological advancement that facilitates the
seamless integration of devices into the physical environment, resulting in a
more natural and intuitive digital world user experience. Spatial computing has
the potential to become a significant advancement in the field of computing.
From GPS and location-based services to healthcare, spatial computing
technologies have influenced and improved our interactions with the digital
world. The use of spatial computing in creating interactive digital
environments has become increasingly popular and effective. This is explained
by its increasing significance among researchers and industrial organisations,
which motivated us to conduct this review. This review provides a detailed
overview of spatial computing, including its enabling technologies and its
impact on various applications. Projects related to spatial computing are also
discussed. In this review, we also explored the potential challenges and
limitations of spatial computing. Furthermore, we discuss potential solutions
and future directions. Overall, this paper aims to provide a comprehensive
understanding of spatial computing, its enabling technologies, their impact on
various applications, emerging challenges, and potential solutions.
\\ ( https://arxiv.org/abs/2402.07912 ,  1656kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07919 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:26:39 GMT   (2680kb)

Title: How Can Generative AI Enhance the Well-being of Blind?
Authors: Oliver Bendel
Categories: cs.HC cs.AI
Comments: 7 pages
ACM-class: I.2; K.3
\\
  This paper examines the question of how generative AI can improve the
well-being of blind or visually impaired people. It refers to a current
example, the Be My Eyes app, in which the Be My AI feature was integrated in
2023, which is based on GPT-4 from OpenAI. The author's tests are described and
evaluated. There is also an ethical and social discussion. The power of the
tool, which can analyze still images in an amazing way, is demonstrated. Those
affected gain a new independence and a new perception of their environment. At
the same time, they are dependent on the world view and morality of the
provider or developer, who prescribe or deny them certain descriptions. An
outlook makes it clear that the analysis of moving images will mean a further
leap forward. It is fair to say that generative AI can fundamentally improve
the well-being of blind and visually impaired people and will change it in
various ways.
\\ ( https://arxiv.org/abs/2402.07919 ,  2680kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07922 (*cross-listing*)
Date: Sat, 3 Feb 2024 01:33:05 GMT   (963kb,D)

Title: Towards the Human Digital Twin: Definition and Design -- A survey
Authors: Martin Wolfgang Lauer-Schmaltz, Philip Cash, John Paulin Hansen, Anja
  Maier
Categories: cs.HC cs.AI cs.DB
Comments: This paper is an extension of the following paper: Lauer-Schmaltz MW,
  Cash P, Hansen JP, Maier A. Designing Human Digital Twins for
  Behaviour-Changing Therapy and Rehabilitation: A Systematic Review.
  Proceedings of the Design Society. 2022;2:1303-1312. doi:10.1017/pds.2022.132
\\
  Human Digital Twins (HDTs) are a fast-emerging technology with significant
potential in fields ranging from healthcare to sports. HDTs extend the
traditional understanding of Digital Twins by representing humans as the
underlying physical entity. This has introduced several significant challenges,
including ambiguity in the definition of HDTs and a lack of guidance for their
design. This survey brings together the recent advances in the field of HDTs to
guide future developers by proposing a first cross-domain definition of HDTs
based on their characteristics, as well as eleven key design considerations
that emerge from the associated challenges.
\\ ( https://arxiv.org/abs/2402.07922 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07928 (*cross-listing*)
Date: Mon, 5 Feb 2024 21:17:44 GMT   (2024kb,D)

Title: Abstracted Trajectory Visualization for Explainability in Reinforcement
  Learning
Authors: Yoshiki Takagi, Roderick Tabalba, Nurit Kirshenbaum, Jason Leigh
Categories: cs.HC cs.AI cs.LG
Comments: 14pages, 11figures
\\
  Explainable AI (XAI) has demonstrated the potential to help reinforcement
learning (RL) practitioners to understand how RL models work. However, XAI for
users who do not have RL expertise (non-RL experts), has not been studied
sufficiently. This results in a difficulty for the non-RL experts to
participate in the fundamental discussion of how RL models should be designed
for an incoming society where humans and AI coexist. Solving such a problem
would enable RL experts to communicate with the non-RL experts in producing
machine learning solutions that better fit our society. We argue that
abstracted trajectories, that depicts transitions between the major states of
the RL model, will be useful for non-RL experts to build a mental model of the
agents. Our early results suggest that by leveraging a visualization of the
abstracted trajectories, users without RL expertise are able to infer the
behavior patterns of RL.
\\ ( https://arxiv.org/abs/2402.07928 ,  2024kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07932 (*cross-listing*)
Date: Tue, 6 Feb 2024 15:41:49 GMT   (251kb)

Title: A Human-Machine Collaboration Framework for the Development of Schemas
Authors: Nicos Isaak
Categories: cs.HC cs.AI cs.CL
Comments: 14 pages
ACM-class: I.2.0; I.2.3; I.2.4; I.2.7; I.5.1; I.5.4
\\
  The Winograd Schema Challenge (WSC), a seemingly well-thought-out test for
machine intelligence, has been proposed to shed light on developing systems
that exhibit human behavior. Since its introduction, it aimed to pivot the
focus of the AI community from the technology to the science of AI. While
common and trivial for humans, studies show that it is still challenging for
machines, especially when they have to deal with novel schemas, that is,
well-designed sentences that require the resolving of definite pronouns. As
researchers have become increasingly interested in the challenge itself, this
presumably necessitates the availability of an extensive collection of Winograd
schemas, which goes beyond what human experts can reasonably develop
themselves, especially after proposed ways of utilizing them as novel forms of
CAPTCHAs.
  To address this necessity, we propose a novel framework that explicitly
focuses on how humans and machines can collaborate as teammates to design novel
schemas from scratch. This is being accomplished by combining two recent
studies from the literature: i) Winventor, a machine-driven approach for the
development of large amounts of Winograd schemas, albeit not of high quality,
and ii) WinoFlexi, an online crowdsourcing system that allows crowd workers to
develop a limited number of schemas often of similar quality to that of
experts. Our proposal crafts a new road map toward developing a novel
collaborative platform that amplifies human and machine intelligence by
combining their complementary strengths.
\\ ( https://arxiv.org/abs/2402.07932 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07933 (*cross-listing*)
Date: Tue, 6 Feb 2024 16:00:32 GMT   (694kb)

Title: Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual
  Framework, Potentials and Limitations
Authors: Mario Truss, Marc Schmitt
Categories: cs.HC cs.AI cs.LG cs.SE
ACM-class: I.2.0; H.5.0; D.2.2; H.1.2; I.2.5; K.6.1
\\
  This paper evaluates No-Code AutoML as a solution for challenges in AI
product prototyping, characterized by unpredictability and inaccessibility to
non-experts, and proposes a conceptual framework. This complexity of AI
products hinders seamless execution and interdisciplinary collaboration crucial
for human-centered AI products. Relevant to industry and innovation, it affects
strategic decision-making and investment risk mitigation. Current approaches
provide limited insights into the potential and feasibility of AI product
ideas. Employing Design Science Research, the study identifies challenges and
integrates no-code AutoML as a solution by presenting a framework for AI
product prototyping with No-code AutoML. A case study confirms its potential in
supporting non-experts, offering a structured approach to AI product
development. The framework facilitates accessible and interpretable
prototyping, benefiting academia, managers, and decision-makers. Strategic
integration of no-code AutoML enhances efficiency, empowers non-experts, and
informs early-stage decisions, albeit with acknowledged limitations.
\\ ( https://arxiv.org/abs/2402.07933 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07938 (*cross-listing*)
Date: Wed, 7 Feb 2024 21:08:49 GMT   (271kb,D)

Title: Large Language User Interfaces: Voice Interactive User Interfaces
  powered by LLMs
Authors: Syed Mekael Wasti, Ken Q. Pu, Ali Neshati
Categories: cs.HC cs.AI cs.CL cs.LG
Comments: Submitted to peer review
ACM-class: I.2.7; I.2.1
\\
  The recent meteoric advancements in large language models have showcased a
remarkable capacity for logical reasoning and comprehension. These newfound
capabilities have opened the door to a new generation of software, as has been
made obvious through the innumerable ways they are being applied in the
industry. This research focuses on harnessing and guiding the upgraded power of
LLMs to construct a framework that can serve as an intermediary between a user
and their user interface. By comprehending a user's needs through a thorough
analysis of natural textual inputs, an effectively crafted LLM engine can
classify the most likely available application, identify the desired UI
component and subsequently execute the user's expected actions. This
integration can evolve static UI systems into highly dynamic and adaptable
solutions, introducing a new frontier of intelligent and responsive user
experiences. Such a framework can fundamentally shift how users accomplish
daily tasks, skyrocket efficiency, and greatly reduce cognitive load.
\\ ( https://arxiv.org/abs/2402.07938 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07939 (*cross-listing*)
Date: Thu, 8 Feb 2024 15:40:35 GMT   (10848kb,D)

Title: UFO: A UI-Focused Agent for Windows OS Interaction
Authors: Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua
  Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
Categories: cs.HC cs.AI cs.CL
\\
  We introduce UFO, an innovative UI-Focused agent to fulfill user requests
tailored to applications on Windows OS, harnessing the capabilities of
GPT-Vision. UFO employs a dual-agent framework to meticulously observe and
analyze the graphical user interface (GUI) and control information of Windows
applications. This enables the agent to seamlessly navigate and operate within
individual applications and across them to fulfill user requests, even when
spanning multiple applications. The framework incorporates a control
interaction module, facilitating action grounding without human intervention
and enabling fully automated execution. Consequently, UFO transforms arduous
and time-consuming processes into simple tasks achievable solely through
natural language commands. We conducted testing of UFO across 9 popular Windows
applications, encompassing a variety of scenarios reflective of users' daily
usage. The results, derived from both quantitative metrics and real-case
studies, underscore the superior effectiveness of UFO in fulfilling user
requests. To the best of our knowledge, UFO stands as the first UI agent
specifically tailored for task completion within the Windows OS environment.
The open-source code for UFO is available on https://github.com/microsoft/UFO.
\\ ( https://arxiv.org/abs/2402.07939 ,  10848kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07940 (*cross-listing*)
Date: Thu, 8 Feb 2024 19:21:33 GMT   (427kb)

Title: LLMs Among Us: Generative AI Participating in Digital Discourse
Authors: Kristina Radivojevic, Nicholas Clark, Paul Brenner
Categories: cs.HC cs.AI cs.CY cs.SI
\\
  The emergence of Large Language Models (LLMs) has great potential to reshape
the landscape of many social media platforms. While this can bring promising
opportunities, it also raises many threats, such as biases and privacy
concerns, and may contribute to the spread of propaganda by malicious actors.
We developed the "LLMs Among Us" experimental framework on top of the Mastodon
social media platform for bot and human participants to communicate without
knowing the ratio or nature of bot and human participants. We built 10 personas
with three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three
rounds of the experiment and surveyed participants after each round to measure
the ability of LLMs to pose as human participants without human detection. We
found that participants correctly identified the nature of other users in the
experiment only 42% of the time despite knowing the presence of both bots and
humans. We also found that the choice of persona had substantially more impact
on human perception than the choice of mainstream LLMs.
\\ ( https://arxiv.org/abs/2402.07940 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07945 (*cross-listing*)
Date: Fri, 9 Feb 2024 02:33:45 GMT   (8574kb,D)

Title: ScreenAgent: A Vision Language Model-driven Computer Control Agent
Authors: Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng,
  He Kong, Yi Chang, Qi Wang
Categories: cs.HC cs.AI cs.CV
\\
  Existing Large Language Models (LLM) can invoke a variety of tools and APIs
to complete complex tasks. The computer, as the most powerful and universal
tool, could potentially be controlled directly by a trained LLM agent. Powered
by the computer, we can hopefully build a more generalized agent to assist
humans in various daily digital works. In this paper, we construct an
environment for a Vision Language Model (VLM) agent to interact with a real
computer screen. Within this environment, the agent can observe screenshots and
manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard
actions. We also design an automated control pipeline that includes planning,
acting, and reflecting phases, guiding the agent to continuously interact with
the environment and complete multi-step tasks. Additionally, we construct the
ScreenAgent Dataset, which collects screenshots and action sequences when
completing a variety of daily computer tasks. Finally, we trained a model,
ScreenAgent, which achieved computer control capabilities comparable to GPT-4V
and demonstrated more precise UI positioning capabilities. Our attempts could
inspire further research on building a generalist LLM agent. The code is
available at \url{https://github.com/niuzaisheng/ScreenAgent}.
\\ ( https://arxiv.org/abs/2402.07945 ,  8574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07946 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:10:29 GMT   (370kb)

Title: Re-Envisioning Command and Control
Authors: Kaleb McDowell, Ellen Novoseller, Anna Madison, Vinicius G. Goecks,
  Christopher Kelshaw
Categories: cs.HC cs.AI cs.CL cs.LG
Comments: Submitted to the NATO Science and Technology Organization Symposium
  (ICMCIS) organized by the Information Systems Technology (IST) Panel,
  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
ACM-class: I.2.6; I.2.7; J.7
\\
  Future warfare will require Command and Control (C2) decision-making to occur
in more complex, fast-paced, ill-structured, and demanding conditions. C2 will
be further complicated by operational challenges such as Denied, Degraded,
Intermittent, and Limited (DDIL) communications and the need to account for
many data streams, potentially across multiple domains of operation. Yet,
current C2 practices -- which stem from the industrial era rather than the
emerging intelligence era -- are linear and time-consuming. Critically, these
approaches may fail to maintain overmatch against adversaries on the future
battlefield. To address these challenges, we propose a vision for future C2
based on robust partnerships between humans and artificial intelligence (AI)
systems. This future vision is encapsulated in three operational impacts:
streamlining the C2 operations process, maintaining unity of effort, and
developing adaptive collective knowledge systems. This paper illustrates the
envisaged future C2 capabilities, discusses the assumptions that shaped them,
and describes how the proposed developments could transform C2 in future
warfare.
\\ ( https://arxiv.org/abs/2402.07946 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07949 (*cross-listing*)
Date: Sat, 10 Feb 2024 00:49:46 GMT   (1214kb,D)

Title: Optimizing the Design of an Artificial Pancreas to Improve Diabetes
  Management
Authors: Ashok Khanna, Olivier Francon, Risto Miikkulainen
Categories: q-bio.QM cs.AI cs.LG cs.NE
\\
  Diabetes, a chronic condition that impairs how the body turns food into
energy, i.e. blood glucose, affects 38 million people in the US alone. The
standard treatment is to supplement carbohydrate intake with an artificial
pancreas, i.e. a continuous insulin pump (basal shots), as well as occasional
insulin injections (bolus shots). The goal of the treatment is to keep blood
glucose at the center of an acceptable range, as measured through a continuous
glucose meter. A secondary goal is to minimize injections, which are unpleasant
and difficult for some patients to implement. In this study, neuroevolution was
used to discover an optimal strategy for the treatment. Based on a dataset of
30 days of treatment and measurements of a single patient, a random forest was
first trained to predict future glucose levels. A neural network was then
evolved to prescribe carbohydrates, basal pumping levels, and bolus injections.
Evolution discovered a Pareto front that reduced deviation from the target and
number of injections compared to the original data, thus improving patients'
quality of life. To make the system easier to adopt, a language interface was
developed with a large language model. Thus, these technologies not only
improve patient care but also adoption in a broader population.
\\ ( https://arxiv.org/abs/2402.07949 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07956 (*cross-listing*)
Date: Sat, 10 Feb 2024 18:48:45 GMT   (1171kb)

Title: Educational data mining and learning analytics: An updated survey
Authors: C. Romero, S. Ventura
Categories: cs.HC cs.AI
Journal-ref: Wiley interdisciplinary reviews: Data mining and knowledge
  discovery;2020; 10(3):e1355
DOI: 10.1002/widm.1355
\\
  This survey is an updated and improved version of the previous one published
in 2013 in this journal with the title data mining in education. It reviews in
a comprehensible and very general way how Educational Data Mining and Learning
Analytics have been applied over educational data. In the last decade, this
research area has evolved enormously and a wide range of related terms are now
used in the bibliography such as Academic Analytics, Institutional Analytics,
Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in
Education, Big Data in Education, and Educational Data Science. This paper
provides the current state of the art by reviewing the main publications, the
key milestones, the knowledge discovery cycle, the main educational
environments, the specific tools, the free available datasets, the most used
methods, the main objectives, and the future trends in this research area.
\\ ( https://arxiv.org/abs/2402.07956 ,  1171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08030 (*cross-listing*)
Date: Mon, 12 Feb 2024 19:49:58 GMT   (4833kb,D)

Title: Why and When LLM-Based Assistants Can Go Wrong: Investigating the
  Effectiveness of Prompt-Based Interactions for Software Help-Seeking
Authors: Anjali Khurana, Hari Subramonyam, Parmit K Chilana
Categories: cs.HC cs.AI cs.LG
Comments: Accepted for publication in the Proceedings of the 29th International
  Conference on Intelligent User Interfaces (IUI'24), March 18--21, 2024, in
  Greenville, SC, USA
DOI: 10.1145/3640543.3645200
\\
  Large Language Model (LLM) assistants, such as ChatGPT, have emerged as
potential alternatives to search methods for helping users navigate complex,
feature-rich software. LLMs use vast training data from domain-specific texts,
software manuals, and code repositories to mimic human-like interactions,
offering tailored assistance, including step-by-step instructions. In this
work, we investigated LLM-generated software guidance through a within-subject
experiment with 16 participants and follow-up interviews. We compared a
baseline LLM assistant with an LLM optimized for particular software contexts,
SoftAIBot, which also offered guidelines for constructing appropriate prompts.
We assessed task completion, perceived accuracy, relevance, and trust.
Surprisingly, although SoftAIBot outperformed the baseline LLM, our results
revealed no significant difference in LLM usage and user perceptions with or
without prompt guidelines and the integration of domain context. Most users
struggled to understand how the prompt's text related to the LLM's responses
and often followed the LLM's suggestions verbatim, even if they were incorrect.
This resulted in difficulties when using the LLM's advice for software tasks,
leading to low task completion rates. Our detailed analysis also revealed that
users remained unaware of inaccuracies in the LLM's responses, indicating a gap
between their lack of software expertise and their ability to evaluate the
LLM's assistance. With the growing push for designing domain-specific LLM
assistants, we emphasize the importance of incorporating explainable,
context-aware cues into LLMs to help users understand prompt-based
interactions, identify biases, and maximize the utility of LLM assistants.
\\ ( https://arxiv.org/abs/2402.08030 ,  4833kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08072 (*cross-listing*)
Date: Mon, 12 Feb 2024 21:32:05 GMT   (2487kb,D)

Title: Enhancing Programming Error Messages in Real Time with Generative AI
Authors: Bailey Kimmel, Austin Geisert, Lily Yaro, Brendan Gipson, Taylor
  Hotchkiss, Sidney Osae-Asante, Hunter Vaught, Grant Wininger, Chase Yamaguchi
Categories: cs.HC cs.AI
Comments: Accepted to CHI 2024
DOI: 10.1145/3613905.3647967
\\
  Generative AI is changing the way that many disciplines are taught, including
computer science. Researchers have shown that generative AI tools are capable
of solving programming problems, writing extensive blocks of code, and
explaining complex code in simple terms. Particular promise has been shown in
using generative AI to enhance programming error messages. Both students and
instructors have complained for decades that these messages are often cryptic
and difficult to understand. Yet recent work has shown that students make fewer
repeated errors when enhanced via GPT-4. We extend this work by implementing
feedback from ChatGPT for all programs submitted to our automated assessment
tool, Athene, providing help for compiler, run-time, and logic errors. Our
results indicate that adding generative AI to an automated assessment tool does
not necessarily make it better and that design of the interface matters greatly
to the usability of the feedback that GPT-4 provided.
\\ ( https://arxiv.org/abs/2402.08072 ,  2487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08075 (*cross-listing*)
Date: Mon, 12 Feb 2024 21:40:45 GMT   (2074kb,D)

Title: Efficient and Scalable Fine-Tune of Language Models for Genome
  Understanding
Authors: Huixin Zhan, Ying Nian Wu, Zijun Zhang
Categories: q-bio.GN cs.AI cs.LG
\\
  Although DNA foundation models have advanced the understanding of genomes,
they still face significant challenges in the limited scale and diversity of
genomic data. This limitation starkly contrasts with the success of natural
language foundation models, which thrive on substantially larger scales.
Furthermore, genome understanding involves numerous downstream genome
annotation tasks with inherent data heterogeneity, thereby necessitating more
efficient and robust fine-tuning methods tailored for genomics. Here, we
present \textsc{Lingo}: \textsc{L}anguage prefix f\textsc{In}e-tuning for
\textsc{G}en\textsc{O}mes. Unlike DNA foundation models, \textsc{Lingo}
strategically leverages natural language foundation models' contextual cues,
recalibrating their linguistic knowledge to genomic sequences. \textsc{Lingo}
further accommodates numerous, heterogeneous downstream fine-tune tasks by an
adaptive rank sampling method that prunes and stochastically reintroduces
pruned singular vectors within small computational budgets. Adaptive rank
sampling outperformed existing fine-tuning methods on all benchmarked 14 genome
understanding tasks, while requiring fewer than 2\% of trainable parameters as
genomic-specific adapters. Impressively, applying these adapters on natural
language foundation models matched or even exceeded the performance of DNA
foundation models. \textsc{Lingo} presents a new paradigm of efficient and
scalable genome understanding via genomic-specific adapters on language models.
\\ ( https://arxiv.org/abs/2402.08075 ,  2074kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08125 (*cross-listing*)
Date: Mon, 12 Feb 2024 23:49:40 GMT   (21414kb,D)

Title: Customizable Perturbation Synthesis for Robust SLAM Benchmarking
Authors: Xiaohao Xu, Tianyi Zhang, Sibo Wang, Xiang Li, Yongqi Chen, Ye Li,
  Bhiksha Raj, Matthew Johnson-Roberson, Xiaonan Huang
Categories: cs.RO cs.AI cs.CV cs.MM
Comments: 40 pages
\\
  Robustness is a crucial factor for the successful deployment of robots in
unstructured environments, particularly in the domain of Simultaneous
Localization and Mapping (SLAM). Simulation-based benchmarks have emerged as a
highly scalable approach for robustness evaluation compared to real-world data
collection. However, crafting a challenging and controllable noisy world with
diverse perturbations remains relatively under-explored. To this end, we
propose a novel, customizable pipeline for noisy data synthesis, aimed at
assessing the resilience of multi-modal SLAM models against various
perturbations. This pipeline incorporates customizable hardware setups,
software components, and perturbed environments. In particular, we introduce
comprehensive perturbation taxonomy along with a perturbation composition
toolbox, allowing the transformation of clean simulations into challenging
noisy environments. Utilizing the pipeline, we instantiate the Robust-SLAM
benchmark, which includes diverse perturbation types, to evaluate the risk
tolerance of existing advanced multi-modal SLAM models. Our extensive analysis
uncovers the susceptibilities of existing SLAM models to real-world
disturbance, despite their demonstrated accuracy in standard benchmarks. Our
perturbation synthesis toolbox, SLAM robustness evaluation pipeline, and
Robust-SLAM benchmark will be made publicly available at
https://github.com/Xiaohao-Xu/SLAM-under-Perturbation/.
\\ ( https://arxiv.org/abs/2402.08125 ,  21414kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08144 (*cross-listing*)
Date: Tue, 13 Feb 2024 00:46:46 GMT   (112kb,D)

Title: Average-Case Analysis of Iterative Voting
Authors: Joshua Kavner, Lirong Xia
Categories: cs.GT cs.AI
Comments: 63 pages, 2 figures
\\
  Iterative voting is a natural model of repeated strategic decision-making in
social choice when agents have the opportunity to update their votes prior to
finalizing the group decision. Prior work has analyzed the efficacy of
iterative plurality on the welfare of the chosen outcome at equilibrium,
relative to the truthful vote profile, via an adaptation of the price of
anarchy. However, prior analyses have only studied the worst-case and
average-case performances when agents' preferences are distributed by the
impartial culture. This work extends average-case analyses to a wider class of
distributions and distinguishes when iterative plurality improves or degrades
asymptotic welfare.
\\ ( https://arxiv.org/abs/2402.08144 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08147 (*cross-listing*)
Date: Tue, 13 Feb 2024 00:55:14 GMT   (291kb,D)

Title: Verified Multi-Step Synthesis using Large Language Models and Monte
  Carlo Tree Search
Authors: David Brandfonbrener, Sibi Raja, Tarun Prasad, Chloe Loughridge,
  Jianang Yang, Simon Henniger, William E. Byrd, Robert Zinkov, Nada Amin
Categories: cs.SE cs.AI cs.LG cs.LO
\\
  We present an approach using Monte Carlo Tree Search (MCTS) to guide Large
Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq.
Our method, which we call VMCTS, leverages the verifier inside the search
algorithm by checking partial programs at each step. In combination with the
LLM prior, the verifier feedback raises the synthesis capabilities of open
source models. On a set of five verified programming problems, we find that in
four problems where the base model cannot solve the question even when
re-sampling solutions for one hour, VMCTS can solve the problems within 6
minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented
with plugins and multiple re-tries on these problems. Our code and benchmarks
are available at
https://github.com/namin/llm-verified-with-monte-carlo-tree-search .
\\ ( https://arxiv.org/abs/2402.08147 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08151 (*cross-listing*)
Date: Tue, 13 Feb 2024 01:03:39 GMT   (661kb,D)

Title: Gradient-flow adaptive importance sampling for Bayesian leave one out
  cross-validation for sigmoidal classification models
Authors: Joshua C Chang, Xiangting Li, Shixin Xu, Hao-Ren Yao, Julia Porcino,
  Carson Chow
Categories: stat.ME cs.AI cs.LG math.SP math.ST stat.TH
Comments: Submitted
\\
  We introduce a set of gradient-flow-guided adaptive importance sampling (IS)
transformations to stabilize Monte-Carlo approximations of point-wise leave one
out cross-validated (LOO) predictions for Bayesian classification models. One
can leverage this methodology for assessing model generalizability by for
instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves
and derived metrics like the AUROC and AUPRC. By the calculus of variations and
gradient flow, we derive two simple nonlinear single-step transformations that
utilize gradient information to shift a model's pre-trained full-data posterior
closer to the target LOO posterior predictive distributions. In doing so, the
transformations stabilize importance weights. Because the transformations
involve the gradient of the likelihood function, the resulting Monte Carlo
integral depends on Jacobian determinants with respect to the model Hessian. We
derive closed-form exact formulae for these Jacobian determinants in the cases
of logistic regression and shallow ReLU-activated artificial neural networks,
and provide a simple approximation that sidesteps the need to compute full
Hessian matrices and their spectra. We test the methodology on an $n\ll p$
dataset that is known to produce unstable LOO IS weights.
\\ ( https://arxiv.org/abs/2402.08151 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08164 (*cross-listing*)
Date: Tue, 13 Feb 2024 01:52:15 GMT   (19kb)

Title: On Limitations of the Transformer Architecture
Authors: Binghui Peng, Srini Narayanan, Christos Papadimitriou
Categories: stat.ML cs.AI cs.LG
\\
  What are the root causes of hallucinations in large language models (LLMs)?
We use Communication Complexity to prove that the Transformer layer is
incapable of composing functions (e.g., identify a grandparent of a person in a
genealogy) if the domains of the functions are large enough; we show through
examples that this inability is already empirically present when the domains
are quite small. We also point out that several mathematical tasks that are at
the core of the so-called compositional tasks thought to be hard for LLMs are
unlikely to be solvable by Transformers, for large enough instances and
assuming that certain well accepted conjectures in the field of Computational
Complexity are true.
\\ ( https://arxiv.org/abs/2402.08164 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08191 (*cross-listing*)
Date: Tue, 13 Feb 2024 03:25:33 GMT   (18398kb,D)

Title: THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic
  Manipulation
Authors: Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse
  Thomason, Dieter Fox
Categories: cs.RO cs.AI cs.LG
Comments: 30 pages
\\
  To realize effective large-scale, real-world robotic applications, we must
evaluate how well our robot policies adapt to changes in environmental
conditions. Unfortunately, a majority of studies evaluate robot performance in
environments closely resembling or even identical to the training setup. We
present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse
manipulation tasks, that enables systematical evaluation of models across 12
axes of environmental perturbations. These perturbations include changes in
color, texture, and size of objects, table-tops, and backgrounds; we also vary
lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4
state-of-the-art manipulation models to reveal that their success rate degrades
between 30-50% across these perturbation factors. When multiple perturbations
are applied in unison, the success rate degrades $\geq$75%. We identify that
changing the number of distractor objects, target object color, or lighting
conditions are the perturbations that reduce model performance the most. To
verify the ecological validity of our results, we show that our results in
simulation are correlated ($\bar{R}^2 = 0.614$) to similar perturbations in
real-world experiments. We open source code for others to use THE COLOSSEUM,
and also release code to 3D print the objects used to replicate the real-world
perturbations. Ultimately, we hope that THE COLOSSEUM will serve as a benchmark
to identify modeling decisions that systematically improve generalization for
manipulation. See https://robot-colosseum.github.io/ for more details.
\\ ( https://arxiv.org/abs/2402.08191 ,  18398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08198 (*cross-listing*)
Date: Tue, 13 Feb 2024 03:51:10 GMT   (1432kb,D)

Title: PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for
  Efficient and Generalizable Compound-Protein Interaction Prediction
Authors: Lirong Wu, Yufei Huang, Cheng Tan, Zhangyang Gao, Bozhen Hu, Haitao
  Lin, Zicheng Liu, Stan Z. Li
Categories: q-bio.BM cs.AI cs.LG
\\
  Compound-Protein Interaction (CPI) prediction aims to predict the pattern and
strength of compound-protein interactions for rational drug discovery. Existing
deep learning-based methods utilize only the single modality of protein
sequences or structures and lack the co-modeling of the joint distribution of
the two modalities, which may lead to significant performance drops in complex
real-world scenarios due to various factors, e.g., modality missing and domain
shifting. More importantly, these methods only model protein sequences and
structures at a single fixed scale, neglecting more fine-grained multi-scale
information, such as those embedded in key protein fragments. In this paper, we
propose a novel multi-scale Protein Sequence-structure Contrasting framework
for CPI prediction (PSC-CPI), which captures the dependencies between protein
sequences and structures through both intra-modality and cross-modality
contrasting. We further apply length-variable protein augmentation to allow
contrasting to be performed at different scales, from the amino acid level to
the sequence level. Finally, in order to more fairly evaluate the model
generalizability, we split the test data into four settings based on whether
compounds and proteins have been observed during the training stage. Extensive
experiments have shown that PSC-CPI generalizes well in all four settings,
particularly in the more challenging ``Unseen-Both" setting, where neither
compounds nor proteins have been observed during training. Furthermore, even
when encountering a situation of modality missing, i.e., inference with only
single-modality protein data, PSC-CPI still exhibits comparable or even better
performance than previous approaches.
\\ ( https://arxiv.org/abs/2402.08198 ,  1432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08246 (*cross-listing*)
Date: Tue, 13 Feb 2024 06:20:37 GMT   (3272kb,D)

Title: Ant Colony Optimization for Cooperative Inspection Path Planning Using
  Multiple Unmanned Aerial Vehicles
Authors: Duy Nam Bui, Thuy Ngan Duong, Manh Duong Phung
Categories: eess.SY cs.AI cs.SY
Comments: Published in: 2024 IEEE/SICE International Symposium on System
  Integration (SII)
DOI: 10.1109/SII58957.2024.10417512
\\
  This paper presents a new swarm intelligence-based approach to deal with the
cooperative path planning problem of unmanned aerial vehicles (UAVs), which is
essential for the automatic inspection of infrastructure. The approach uses a
3D model of the structure to generate viewpoints for the UAVs. The calculation
of the viewpoints considers the constraints related to the UAV formation model,
camera parameters, and requirements for data post-processing. The viewpoints
are then used as input to formulate the path planning as an extended traveling
salesman problem and the definition of a new cost function. Ant colony
optimization is finally used to solve the problem to yield optimal inspection
paths. Experiments with 3D models of real structures have been conducted to
evaluate the performance of the proposed approach. The results show that our
system is not only capable of generating feasible inspection paths for UAVs but
also reducing the path length by 29.47\% for complex structures when compared
with another heuristic approach. The source code of the algorithm can be found
at https://github.com/duynamrcv/aco_3d_ipp.
\\ ( https://arxiv.org/abs/2402.08246 ,  3272kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08256 (*cross-listing*)
Date: Tue, 13 Feb 2024 07:12:44 GMT   (3434kb,D)

Title: Modeling Balanced Explicit and Implicit Relations with Contrastive
  Learning for Knowledge Concept Recommendation in MOOCs
Authors: Hengnian Gu, Zhiyi Duan, Pan Xie, Dongdai Zhou
Categories: cs.IR cs.AI
Comments: Accepted to WWW 2024
\\
  The knowledge concept recommendation in Massive Open Online Courses (MOOCs)
is a significant issue that has garnered widespread attention. Existing methods
primarily rely on the explicit relations between users and knowledge concepts
on the MOOC platforms for recommendation. However, there are numerous implicit
relations (e.g., shared interests or same knowledge levels between users)
generated within the users' learning activities on the MOOC platforms. Existing
methods fail to consider these implicit relations, and these relations
themselves are difficult to learn and represent, causing poor performance in
knowledge concept recommendation and an inability to meet users' personalized
needs. To address this issue, we propose a novel framework based on contrastive
learning, which can represent and balance the explicit and implicit relations
for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we
first construct a MOOCs heterogeneous information network (HIN) by modeling the
data from the MOOC platforms. Then, we utilize a relation-updated graph
convolutional network and stacked multi-channel graph neural network to
represent the explicit and implicit relations in the HIN, respectively.
Considering that the quantity of explicit relations is relatively fewer
compared to implicit relations in MOOCs, we propose a contrastive learning with
prototypical graph to enhance the representations of both relations to capture
their fruitful inherent relational knowledge, which can guide the propagation
of students' preferences within the HIN. Based on these enhanced
representations, to ensure the balanced contribution of both towards the final
recommendation, we propose a dual-head attention mechanism for balanced fusion.
Experimental results demonstrate that CL-KCRec outperforms several
state-of-the-art baselines on real-world datasets in terms of HR, NDCG and MRR.
\\ ( https://arxiv.org/abs/2402.08256 ,  3434kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08267 (*cross-listing*)
Date: Tue, 13 Feb 2024 07:45:25 GMT   (853kb)

Title: Improving Image Coding for Machines through Optimizing Encoder via
  Auxiliary Loss
Authors: Kei Iino, Shunsuke Akamatsu, Hiroshi Watanabe, Shohei Enomoto, Akira
  Sakamoto, Takeharu Eda
Categories: cs.CV cs.AI
Comments: copyright 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\
  Image coding for machines (ICM) aims to compress images for machine analysis
using recognition models rather than human vision. Hence, in ICM, it is
important for the encoder to recognize and compress the information necessary
for the machine recognition task. There are two main approaches in learned ICM;
optimization of the compression model based on task loss, and Region of
Interest (ROI) based bit allocation. These approaches provide the encoder with
the recognition capability. However, optimization with task loss becomes
difficult when the recognition model is deep, and ROI-based methods often
involve extra overhead during evaluation. In this study, we propose a novel
training method for learned ICM models that applies auxiliary loss to the
encoder to improve its recognition capability and rate-distortion performance.
Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in
object detection and semantic segmentation tasks, compared to the conventional
training method.
\\ ( https://arxiv.org/abs/2402.08267 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08310 (*cross-listing*)
Date: Tue, 13 Feb 2024 09:13:30 GMT   (5038kb,D)

Title: One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a
  synthetically trained Generative Model
Authors: Thomas P\"ollabauer, Julius K\"uhn, Jiayi Li, Arjan Kuijper
Categories: cs.CV cs.AI
Journal-ref: 21st Eurographics Workshop on Graphics and Cultural Heritage (GCH
  2023)
\\
  Estimating the 3D shape of an object using a single image is a difficult
problem. Modern approaches achieve good results for general objects, based on
real photographs, but worse results on less expressive representations such as
historic sketches. Our automated approach generates a variety of detailed 3D
representation from a single sketch, depicting a medieval statue, and can be
guided by multi-modal inputs, such as text prompts. It relies solely on
synthetic data for training, making it adoptable even in cases of only small
numbers of training examples. Our solution allows domain experts such as a
curators to interactively reconstruct potential appearances of lost artifacts.
\\ ( https://arxiv.org/abs/2402.08310 ,  5038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08323 (*cross-listing*)
Date: Tue, 13 Feb 2024 09:38:17 GMT   (4057kb)

Title: Mapping the Ethics of Generative AI: A Comprehensive Scoping Review
Authors: Thilo Hagendorff
Categories: cs.CY cs.AI
\\
  The advent of generative artificial intelligence and the widespread adoption
of it in society engendered intensive debates about its ethical implications
and risks. These risks often differ from those associated with traditional
discriminative machine learning. To synthesize the recent discourse and map its
normative concepts, we conducted a scoping review on the ethics of generative
artificial intelligence, including especially large language models and
text-to-image models. Our analysis provides a taxonomy of 378 normative issues
in 19 topic areas and ranks them according to their prevalence in the
literature. The study offers a comprehensive overview for scholars,
practitioners, or policymakers, condensing the ethical debates surrounding
fairness, safety, harmful content, hallucinations, privacy, interaction risks,
security, alignment, societal impacts, and others. We discuss the results,
evaluate imbalances in the literature, and explore unsubstantiated risk
scenarios.
\\ ( https://arxiv.org/abs/2402.08323 ,  4057kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08349 (*cross-listing*)
Date: Tue, 13 Feb 2024 10:28:57 GMT   (843kb,D)

Title: Evaluating the Data Model Robustness of Text-to-SQL Systems Based on
  Real User Queries
Authors: Jonathan F\"urst, Catherine Kosten, Farhard Nooralahzadeh, Yi Zhang,
  Kurt Stockinger
Categories: cs.DB cs.AI cs.CL
\\
  Text-to-SQL systems (also known as NL-to-SQL systems) have become an
increasingly popular solution for bridging the gap between user capabilities
and SQL-based data access. These systems translate user requests in natural
language to valid SQL statements for a specific database. Recent Text-to-SQL
systems have benefited from the rapid improvement of transformer-based language
models. However, while Text-to-SQL systems that incorporate such models
continuously reach new high scores on -- often synthetic -- benchmark datasets,
a systematic exploration of their robustness towards different data models in a
real-world, realistic scenario is notably missing. This paper provides the
first in-depth evaluation of the data model robustness of Text-to-SQL systems
in practice based on a multi-year international project focused on Text-to-SQL
interfaces. Our evaluation is based on a real-world deployment of FootballDB, a
system that was deployed over a 9 month period in the context of the FIFA World
Cup 2022, during which about 6K natural language questions were asked and
executed. All of our data is based on real user questions that were asked live
to the system. We manually labeled and translated a subset of these questions
for three different data models. For each data model, we explore the
performance of representative Text-to-SQL systems and language models. We
further quantify the impact of training data size, pre-, and post-processing
steps as well as language model inference time. Our comprehensive evaluation
sheds light on the design choices of real-world Text-to-SQL systems and their
impact on moving from research prototypes to real deployments. Last, we provide
a new benchmark dataset to the community, which is the first to enable the
evaluation of different data models for the same dataset and is substantially
more challenging than most previous datasets in terms of query complexity.
\\ ( https://arxiv.org/abs/2402.08349 ,  843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08409 (*cross-listing*)
Date: Tue, 13 Feb 2024 12:21:06 GMT   (13728kb,D)

Title: Transferring Ultrahigh-Field Representations for Intensity-Guided Brain
  Segmentation of Low-Field Magnetic Resonance Imaging
Authors: Kwanseok Oh, Jieun Lee, Da-Woon Heo, Dinggang Shen, and Heung-Il Suk
Categories: cs.CV cs.AI
Comments: 32 pages, 9 figures, and 5 tables
\\
  Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI,
provides superior anatomical details of internal brain structures owing to its
enhanced signal-to-noise ratio and susceptibility-induced contrast. However,
the widespread use of 7T MRI is limited by its high cost and lower
accessibility compared to low-field (LF) MRI. This study proposes a
deep-learning framework that systematically fuses the input LF magnetic
resonance feature representations with the inferred 7T-like feature
representations for brain image segmentation tasks in a 7T-absent environment.
Specifically, our adaptive fusion module aggregates 7T-like features derived
from the LF image by a pre-trained network and then refines them to be
effectively assimilable UHF guidance into LF image features. Using
intensity-guided features obtained from such aggregation and assimilation,
segmentation models can recognize subtle structural representations that are
usually difficult to recognize when relying only on LF features. Beyond such
advantages, this strategy can seamlessly be utilized by modulating the contrast
of LF features in alignment with UHF guidance, even when employing arbitrary
segmentation models. Exhaustive experiments demonstrated that the proposed
method significantly outperformed all baseline models on both brain tissue and
whole-brain segmentation tasks; further, it exhibited remarkable adaptability
and scalability by successfully integrating diverse segmentation models and
tasks. These improvements were not only quantifiable but also visible in the
superlative visual quality of segmentation masks.
\\ ( https://arxiv.org/abs/2402.08409 ,  13728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08429 (*cross-listing*)
Date: Wed, 24 Jan 2024 06:22:08 GMT   (1090kb,D)

Title: Is 3-(F)WL Enough to Distinguish All 3D Graphs?
Authors: Wanghan Xu
Categories: cs.OH cs.AI
\\
  The problem of graph isomorphism is an important but challenging problem in
the field of graph analysis, for example: analyzing the similarity of two
chemical molecules, or studying the expressive ability of graph neural
networks. WL test is a method to judge whether two graphs are isomorphic, but
it cannot distinguish all non-isomorphic graphs. As an improvement of WL, k-WL
has stronger isomorphism discrimination ability, and as k increases, its
discrimination ability is strictly increasing. However, whether the isomorphic
discrimination power of k-WL is strictly increasing for more complex 3D graphs,
or whether there exists k that can discriminate all 3D graphs, remains
unexplored. This paper attempts to explore this problem from the perspective of
graph generation.
\\ ( https://arxiv.org/abs/2402.08429 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08473 (*cross-listing*)
Date: Tue, 13 Feb 2024 14:07:49 GMT   (46265kb,D)

Title: Intriguing Differences Between Zero-Shot and Systematic Evaluations of
  Vision-Language Transformer Models
Authors: Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu, Lingjiong Zhu
Categories: cs.CV cs.AI cs.LG
Comments: 30 pages, 30 figures
\\
  Transformer-based models have dominated natural language processing and other
areas in the last few years due to their superior (zero-shot) performance on
benchmark datasets. However, these models are poorly understood due to their
complexity and size. While probing-based methods are widely used to understand
specific properties, the structures of the representation space are not
systematically characterized; consequently, it is unclear how such models
generalize and overgeneralize to new inputs beyond datasets. In this paper,
based on a new gradient descent optimization method, we are able to explore the
embedding space of a commonly used vision-language model. Using the Imagenette
dataset, we show that while the model achieves over 99\% zero-shot
classification performance, it fails systematic evaluations completely. Using a
linear approximation, we provide a framework to explain the striking
differences. We have also obtained similar results using a different model to
support that our results are applicable to other transformer models with
continuous inputs. We also propose a robust way to detect the modified images.
\\ ( https://arxiv.org/abs/2402.08473 ,  46265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08509 (*cross-listing*)
Date: Tue, 13 Feb 2024 15:04:11 GMT   (97kb,D)

Title: From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL
  CONSTRUCT Queries (Extended Version)
Authors: Philipp Seifer, Daniel Hern\'andez, Ralf L\"ammel, Steffen Staab
Categories: cs.DB cs.AI cs.LO
Comments: 19 pages, 5 figures
\\
  SPARQL CONSTRUCT queries allow for the specification of data processing
pipelines that transform given input graphs into new output graphs. It is now
common to constrain graphs through SHACL shapes allowing users to understand
which data they can expect and which not. However, it becomes challenging to
understand what graph data can be expected at the end of a data processing
pipeline without knowing the particular input data: Shape constraints on the
input graph may affect the output graph, but may no longer apply literally, and
new shapes may be imposed by the query template. In this paper, we study the
derivation of shape constraints that hold on all possible output graphs of a
given SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query is
fixed, e.g., being part of a program, whereas the input graphs adhere to input
shape constraints but may otherwise vary over time and, thus, are mostly
unknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment
of SHACL (Simple SHACL). We formally define the problem of deriving the most
restrictive set of Simple SHACL shapes that constrain the results from
evaluating a SCCQ over any input graph restricted by a given set of Simple
SHACL shapes. We propose and implement an algorithm that statically analyses
input SHACL shapes and CONSTRUCT queries and prove its soundness and
complexity.
\\ ( https://arxiv.org/abs/2402.08509 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08547 (*cross-listing*)
Date: Tue, 13 Feb 2024 15:53:09 GMT   (429kb,D)

Title: Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting
Authors: Simina Br\^anzei and MohammadTaghi Hajiaghayi and Reed Phillips and
  Suho Shin and Kun Wang
Categories: cs.GT cs.AI econ.TH
\\
  We consider the setting of repeated fair division between two players,
denoted Alice and Bob, with private valuations over a cake. In each round, a
new cake arrives, which is identical to the ones in previous rounds. Alice cuts
the cake at a point of her choice, while Bob chooses the left piece or the
right piece, leaving the remainder for Alice. We consider two versions:
sequential, where Bob observes Alice's cut point before choosing left/right,
and simultaneous, where he only observes her cut point after making his choice.
The simultaneous version was first considered by Aumann and Maschler (1995).
  We observe that if Bob is almost myopic and chooses his favorite piece too
often, then he can be systematically exploited by Alice through a strategy akin
to a binary search. This strategy allows Alice to approximate Bob's preferences
with increasing precision, thereby securing a disproportionate share of the
resource over time.
  We analyze the limits of how much a player can exploit the other one and show
that fair utility profiles are in fact achievable. Specifically, the players
can enforce the equitable utility profile of $(1/2, 1/2)$ in the limit on every
trajectory of play, by keeping the other player's utility to approximately
$1/2$ on average while guaranteeing they themselves get at least approximately
$1/2$ on average. We show this theorem using a connection with Blackwell
approachability.
  Finally, we analyze a natural dynamic known as fictitious play, where players
best respond to the empirical distribution of the other player. We show that
fictitious play converges to the equitable utility profile of $(1/2, 1/2)$ at a
rate of $O(1/\sqrt{T})$.
\\ ( https://arxiv.org/abs/2402.08547 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08570 (*cross-listing*)
Date: Tue, 13 Feb 2024 16:14:32 GMT   (2799kb,D)

Title: Online Foundation Model Selection in Robotics
Authors: Po-han Li, Oyku Selin Toprak, Aditya Narayanan, Ufuk Topcu, Sandeep
  Chinchali
Categories: cs.RO cs.AI cs.LG
\\
  Foundation models have recently expanded into robotics after excelling in
computer vision and natural language processing. The models are accessible in
two ways: open-source or paid, closed-source options. Users with access to both
face a problem when deciding between effective yet costly closed-source models
and free but less powerful open-source alternatives. We call it the model
selection problem. Existing supervised-learning methods are impractical due to
the high cost of collecting extensive training data from closed-source models.
Hence, we focus on the online learning setting where algorithms learn while
collecting data, eliminating the need for large pre-collected datasets. We thus
formulate a user-centric online model selection problem and propose a novel
solution that combines an open-source encoder to output context and an online
learning algorithm that processes this context. The encoder distills vast data
distributions into low-dimensional features, i.e., the context, without
additional training. The online learning algorithm aims to maximize a composite
reward that includes model performance, execution time, and costs based on the
context extracted from the data. It results in an improved trade-off between
selecting open-source and closed-source models compared to non-contextual
methods, as validated by our theoretical analysis. Experiments across
language-based robotic tasks such as Waymo Open Dataset, ALFRED, and Open
X-Embodiment demonstrate real-world applications of the solution. The results
show that the solution significantly improves the task success rate by up to
14%.
\\ ( https://arxiv.org/abs/2402.08570 ,  2799kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08582 (*cross-listing*)
Date: Tue, 13 Feb 2024 16:36:21 GMT   (3490kb,D)

Title: FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing
  Medical Image Analysis
Authors: Charulkumar Chodvadiya, Navyansh Mahla, Kinshuk Gaurav Singh, Kshitij
  Sharad Jadhav
Categories: cs.CV cs.AI
Comments: 5 Pages, 3 figures
\\
  Medical image segmentation is a critical process in the field of medical
imaging, playing a pivotal role in diagnosis, treatment, and research. It
involves partitioning of an image into multiple regions, representing distinct
anatomical or pathological structures. Conventional methods often grapple with
the challenge of balancing spatial precision and comprehensive feature
representation due to their reliance on traditional loss functions. To overcome
this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that
integrates the benefits of contrastive learning (which extracts intricate
features, particularly in the nuanced domain of medical imaging) with the
spatial accuracy inherent in the Dice loss. The objective is to augment both
spatial precision and feature-based representation in the segmentation of
medical images. FESS Loss signifies a notable advancement, offering a more
accurate and refined segmentation process, ultimately contributing to
heightened precision in the analysis of medical images. Further, FESS loss
demonstrates superior performance in limited annotated data availability
scenarios often present in the medical domain.
\\ ( https://arxiv.org/abs/2402.08582 ,  3490kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08640 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:09:38 GMT   (1204kb,D)

Title: Forecasting high-impact research topics via machine learning on evolving
  knowledge graphs
Authors: Xuemei Gu, Mario Krenn
Categories: cs.DL cs.AI cs.LG
Comments: 8 pages, 6 figures, Comments welcome!
\\
  The exponential growth in scientific publications poses a severe challenge
for human researchers. It forces attention to more narrow sub-fields, which
makes it challenging to discover new impactful research ideas and
collaborations outside one's own field. While there are ways to predict a
scientific paper's future citation counts, they need the research to be
finished and the paper written, usually assessing impact long after the idea
was conceived. Here we show how to predict the impact of onsets of ideas that
have never been published by researchers. For that, we developed a large
evolving knowledge graph built from more than 21 million scientific papers. It
combines a semantic network created from the content of the papers and an
impact network created from the historic citations of papers. Using machine
learning, we can predict the dynamic of the evolving network into the future
with high accuracy, and thereby the impact of new research directions. We
envision that the ability to predict the impact of new ideas will be a crucial
component of future artificial muses that can inspire new impactful and
interesting scientific ideas.
\\ ( https://arxiv.org/abs/2402.08640 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08658 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:39:36 GMT   (1158kb)

Title: The Last JITAI? The Unreasonable Effectiveness of Large Language Models
  in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity
  in a Prospective Cardiac Rehabilitation Setting
Authors: David Haag, Devender Kumar, Sebastian Gruber, Mahdi Sareban, Gunnar
  Treff, Josef Niebauer, Christopher Bull, Jan David Smeddinck
Categories: cs.HC cs.AI
ACM-class: J.3
\\
  We explored the viability of Large Language Models (LLMs) for triggering and
personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in
digital health. JITAIs are being explored as a key mechanism for sustainable
behavior change, adapting interventions to an individual's current context and
needs. However, traditional rule-based and machine learning models for JITAI
implementation face scalability and reliability limitations, such as lack of
personalization, difficulty in managing multi-parametric systems, and issues
with data sparsity. To investigate JITAI implementation via LLMs, we tested the
contemporary overall performance-leading model 'GPT-4' with examples grounded
in the use case of fostering heart-healthy physical activity in outpatient
cardiac rehabilitation. Three personas and five sets of context information per
persona were used as a basis of triggering and personalizing JITAIs.
Subsequently, we generated a total of 450 proposed JITAI decisions and message
content, divided equally into JITAIs generated by 10 iterations with GPT-4, a
baseline provided by 10 laypersons (LayPs), and a gold standard set by 10
healthcare professionals (HCPs). Ratings from 27 LayPs indicated that JITAIs
generated by GPT-4 were superior to those by HCPs and LayPs over all assessed
scales: i.e., appropriateness, engagement, effectiveness, and professionality.
This study indicates that LLMs have significant potential for implementing
JITAIs as a building block of personalized or "precision" health, offering
scalability, effective personalization based on opportunistically sampled
information, and good acceptability.
\\ ( https://arxiv.org/abs/2402.08658 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08671 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:53:13 GMT   (10286kb,D)

Title: Are Semi-Dense Detector-Free Methods Good at Matching Local Features?
Authors: Matthieu Vilain, R\'emi Giraud, Hugo Germain, Guillaume Bourmaud
Categories: cs.CV cs.AI
\\
  Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among
the most popular image matching methods. While SDF methods are trained to
establish correspondences between two images, their performances are almost
exclusively evaluated using relative pose estimation metrics. Thus, the link
between their ability to establish correspondences and the quality of the
resulting estimated pose has thus far received little attention. This paper is
a first attempt to study this link. We start with proposing a novel structured
attention-based image matching architecture (SAM). It allows us to show a
counter-intuitive result on two datasets (MegaDepth and HPatches): on the one
hand SAM either outperforms or is on par with SDF methods in terms of
pose/homography estimation metrics, but on the other hand SDF approaches are
significantly better than SAM in terms of matching accuracy. We then propose to
limit the computation of the matching accuracy to textured regions, and show
that in this case SAM often surpasses SDF methods. Our findings highlight a
strong correlation between the ability to establish accurate correspondences in
textured regions and the accuracy of the resulting estimated pose/homography.
Our code will be made available.
\\ ( https://arxiv.org/abs/2402.08671 ,  10286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08682 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:59:51 GMT   (6887kb,D)

Title: IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality
  3D Generation
Authors: Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova,
  Andrea Vedaldi, Oran Gafni, Filippos Kokkinos
Categories: cs.CV cs.AI cs.LG
\\
  Most text-to-3D generators build upon off-the-shelf text-to-image models
trained on billions of images. They use variants of Score Distillation Sampling
(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation
is to fine-tune the 2D generator to be multi-view aware, which can help
distillation or can be combined with reconstruction networks to output 3D
objects directly. In this paper, we further explore the design space of
text-to-3D models. We significantly improve multi-view generation by
considering video instead of image generators. Combined with a 3D
reconstruction algorithm which, by using Gaussian splatting, can optimize a
robust image-based loss, we directly produce high-quality 3D outputs from the
generated views. Our new method, IM-3D, reduces the number of evaluations of
the 2D generator network 10-100x, resulting in a much more efficient pipeline,
better quality, fewer geometric inconsistencies, and higher yield of usable 3D
assets.
\\ ( https://arxiv.org/abs/2402.08682 ,  6887kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07907 (*cross-listing*)
Date: Mon, 22 Jan 2024 14:48:13 GMT   (181kb)

Title: Applications, challenges and ethical issues of AI and ChatGPT in
  education
Authors: Dimitrios Sidiropoulos and Christos-Nikolaos Anagnostopoulos
Categories: cs.HC cs.CL
Comments: 6 pages
\\
  Artificial Intelligence (AI) in recent years has shown an unprecedentedly
impressive development, tending to play a catalytic role in all aspects of
life. The interest of the academic community, but also of governments, is huge
in the dynamics of AI and is reflected by the truly explosive amount of
investment and research that is underway. Enthusiastic opinions and statements
about AI are made every day, but at the same time they also bring to the fore
alarming predictions about its effects. This paper aims to describe the
opportunities emerging from the use of artificial intelligence and ChatGPT to
improve education, but also to identify the challenges and ethical issues that
arise.
\\ ( https://arxiv.org/abs/2402.07907 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07950 (*cross-listing*)
Date: Sat, 10 Feb 2024 04:47:58 GMT   (1419kb,D)

Title: Sentinels of the Stream: Unleashing Large Language Models for Dynamic
  Packet Classification in Software Defined Networks -- Position Paper
Authors: Shariq Murtuza
Categories: cs.CR cs.CL
\\
  With the release of OpenAI's ChatGPT, the field of large language models
(LLM) saw an increase of academic interest in GPT based chat assistants. In the
next few months multiple accesible large language models were released that
included Meta's LLama models and Mistral AI's Mistral and Mixtral MoE models.
These models are available openly for a wide array of purposes with a wide
spectrum of licenses. These LLMs have found their use in a different number of
fields like code development, SQL generation etc. In this work we propose our
plan to explore the applicability of large language model in the domain of
network security. We plan to create Sentinel, a LLM, to analyse network packet
contents and pass a judgment on it's threat level. This work is a preliminary
report that will lay our plan for our future endeavors.
\\ ( https://arxiv.org/abs/2402.07950 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08017 (*cross-listing*)
Date: Mon, 12 Feb 2024 19:27:26 GMT   (4466kb,D)

Title: Lumos : Empowering Multimodal LLMs with Scene Text Recognition
Authors: Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee,
  Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu,
  Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar
Categories: cs.CV cs.CL cs.LG
Comments: Submitted to KDD 2024 (ADS Track)
\\
  We introduce Lumos, the first end-to-end multimodal question-answering system
with text understanding capabilities. At the core of Lumos is a Scene Text
Recognition (STR) component that extracts text from first person point-of-view
images, the output of which is used to augment input to a Multimodal Large
Language Model (MM-LLM). While building Lumos, we encountered numerous
challenges related to STR quality, overall latency, and model inference. In
this paper, we delve into those challenges, and discuss the system
architecture, design choices, and modeling techniques employed to overcome
these obstacles. We also provide a comprehensive evaluation for each component,
showcasing high quality and efficiency.
\\ ( https://arxiv.org/abs/2402.08017 ,  4466kb)
------------------------------------------------------------------------------
\\
arXiv:2209.10712 (*cross-listing*)
Date: Wed, 21 Sep 2022 23:57:35 GMT   (581kb,D)

Title: Compressing Sign Information in DCT-based Image Coding via Deep Sign
  Retrieval
Authors: Kei Suzuki, Chihiro Tsutake, Keita Takahashi, Toshiaki Fujii
Categories: cs.IT cs.LG eess.IV eess.SP math.IT
\\
  Compressing the sign information of discrete cosine transform (DCT)
coefficients is an intractable problem in image coding schemes due to the
equiprobable characteristics of the signs. To overcome this difficulty, we
propose an efficient compression method for the sign information called "sign
retrieval." This method is inspired by phase retrieval, which is a classical
signal restoration problem of finding the phase information of discrete Fourier
transform coefficients from their magnitudes. The sign information of all DCT
coefficients is excluded from a bitstream at the encoder and is complemented at
the decoder through our sign retrieval method. We show through experiments that
our method outperforms previous ones in terms of the bit amount for the signs
and computation cost. Our method, implemented in Python language, is available
from https://github.com/ctsutake/dsr.
\\ ( https://arxiv.org/abs/2209.10712 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07915 (*cross-listing*)
Date: Thu, 1 Feb 2024 16:39:02 GMT   (850kb)

Title: Research on Older Adults' Interaction with E-Health Interface Based on
  Explainable Artificial Intelligence
Authors: Xueting Huang, Zhibo Zhang, Fusen Guo, Xianghao Wang, Kun Chi, Kexin
  Wu
Categories: cs.HC cs.LG
\\
  This paper proposed a comprehensive mixed-methods framework with varied
samples of older adults, including user experience, usability assessments, and
in-depth interviews with the integration of Explainable Artificial Intelligence
(XAI) methods. The experience of older adults' interaction with the Ehealth
interface is collected through interviews and transformed into operatable
databases whereas XAI methods are utilized to explain the collected interview
results in this research work. The results show that XAI-infused e-health
interfaces could play an important role in bridging the age-related digital
divide by investigating elders' preferences when interacting with E-health
interfaces. Furthermore, the study identifies important design factors, such as
intuitive visualization and straightforward explanations, that are critical for
creating efficient Human Computer Interaction (HCI) tools among older users.
Furthermore, this study emphasizes the revolutionary potential of XAI in
e-health interfaces for older users, emphasizing the importance of transparency
and understandability in HCI-driven healthcare solutions. This study's findings
have far-reaching implications for the design and development of user-centric
e-health technologies, intending to increase the overall well-being of older
adults.
\\ ( https://arxiv.org/abs/2402.07915 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07937 (*cross-listing*)
Date: Sun, 4 Feb 2024 18:55:24 GMT   (9875kb)

Title: A Physiological Sensor-Based Android Application Synchronized with a
  Driving Simulator for Driver Monitoring
Authors: David Gonz\'alez-Ortega, Francisco Javier D\'iaz-Pernas, Mario
  Mart\'inez-Zarzuela, and M\'iriam Ant\'on-Rodr\'iguez
Categories: cs.HC cs.LG eess.SP
Comments: 28 pages
Journal-ref: Sensors 2019, 19, 399
DOI: 10.3390/s19020399
\\
  In this paper, we present an Android application to control and monitor the
physiological sensors from the Shimmer platform and its synchronized working
with a driving simulator. The Android app can monitor drivers and their
parameters can be used to analyze the relation between their physiological
states and driving performance. The app can configure, select, receive,
process, represent graphically, and store the signals from electrocardiogram
(ECG), electromyogram (EMG) and galvanic skin response (GSR) modules and
accelerometers, a magnetometer and a gyroscope. The Android app is synchronized
in two steps with a driving simulator that we previously developed using the
Unity game engine to analyze driving security and efficiency. The Android app
was tested with different sensors working simultaneously at various sampling
rates and in different Android devices. We also tested the synchronized working
of the driving simulator and the Android app with 25 people and analyzed the
relation between data from the ECG, EMG, GSR, and gyroscope sensors and from
the simulator. Among others, some significant correlations between a
gyroscope-based feature calculated by the Android app and vehicle data and
particular traffic offences were found. The Android app can be applied with
minor adaptations to other different users such as patients with chronic
diseases or athletes.
\\ ( https://arxiv.org/abs/2402.07937 ,  9875kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07948 (*cross-listing*)
Date: Fri, 9 Feb 2024 20:33:48 GMT   (482kb)

Title: evolSOM: an R Package for evolutionary conservation analysis with SOMs
Authors: Santiago Prochetto, Renata Reinheimer, Georgina Stegmayer
Categories: q-bio.QM cs.LG q-bio.PE
Comments: 8 pages, 1 figure
\\
  Motivation: Unraveling the connection between genes and traits is crucial for
solving many biological puzzles. Genes provide instructions for building
cellular machinery, directing the processes that sustain life. RNA molecules
and proteins, derived from these genetic instructions, play crucial roles in
shaping cell structures, influencing reactions, and guiding behavior. This
fundamental biological principle links genetic makeup to observable traits, but
integrating and extracting meaningful relationships from this complex,
multimodal data presents a significant challenge. Results: We introduce
evolSOM, a novel R package that utilizes Self-Organizing Maps (SOMs) to explore
and visualize the conservation of biological variables, easing the integration
of phenotypic and genotypic attributes. By constructing species-specific or
condition-specific SOMs that capture non-redundant patterns, evolSOM allows the
analysis of displacement of biological variables between species or conditions.
Variables displaced together suggest membership in the same regulatory network,
and the nature of the displacement may hold biological significance. The
package automatically calculates and graphically presents these displacements,
enabling efficient comparison and revealing conserved and displaced variables.
The package facilitates the integration of diverse phenotypic data types,
enabling the exploration of potential gene drivers underlying observed
phenotypic changes. Its user-friendly interface and visualization capabilities
enhance the accessibility of complex network analyses. Illustratively, we
employed evolSOM to study the displacement of genes and phenotypic traits,
successfully identifying potential drivers of phenotypic differentiation in
grass leaves. Availability: The package is open-source and is available at
https://github.com/sanprochetto/evolSOM.
\\ ( https://arxiv.org/abs/2402.07948 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07955 (*cross-listing*)
Date: Sat, 10 Feb 2024 17:31:46 GMT   (8408kb,D)

Title: ProtIR: Iterative Refinement between Retrievers and Predictors for
  Protein Function Annotation
Authors: Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, Aur\'elie Lozano,
  Payel Das, Jian Tang
Categories: q-bio.BM cs.LG
\\
  Protein function annotation is an important yet challenging task in biology.
Recent deep learning advancements show significant potential for accurate
function prediction by learning from protein sequences and structures.
Nevertheless, these predictor-based methods often overlook the modeling of
protein similarity, an idea commonly employed in traditional approaches using
sequence or structure retrieval tools. To fill this gap, we first study the
effect of inter-protein similarity modeling by benchmarking retriever-based
methods against predictors on protein function annotation tasks. Our results
show that retrievers can match or outperform predictors without large-scale
pre-training. Building on these insights, we introduce a novel variational
pseudo-likelihood framework, ProtIR, designed to improve function predictors by
incorporating inter-protein similarity modeling. This framework iteratively
refines knowledge between a function predictor and retriever, thereby combining
the strengths of both predictors and retrievers. ProtIR showcases around 10%
improvement over vanilla predictor-based methods. Besides, it achieves
performance on par with protein language model-based methods, yet without the
need for massive pre-training, highlighting the efficacy of our framework. Code
will be released upon acceptance.
\\ ( https://arxiv.org/abs/2402.07955 ,  8408kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07970 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:24:32 GMT   (1431kb,D)

Title: Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical
  Similarity Search
Authors: Kathryn E. Kirchoff, James Wellnitz, Joshua E. Hochuli, Travis
  Maxfield, Konstantin I. Popov, Shawn Gomez, Alexander Tropsha
Categories: cs.IR cs.LG
\\
  Nearest neighbor-based similarity searching is a common task in chemistry,
with notable use cases in drug discovery. Yet, some of the most commonly used
approaches for this task still leverage a brute-force approach. In practice
this can be computationally costly and overly time-consuming, due in part to
the sheer size of modern chemical databases. Previous computational
advancements for this task have generally relied on improvements to hardware or
dataset-specific tricks that lack generalizability. Approaches that leverage
lower-complexity searching algorithms remain relatively underexplored. However,
many of these algorithms are approximate solutions and/or struggle with typical
high-dimensional chemical embeddings. Here we evaluate whether a combination of
low-dimensional chemical embeddings and a k-d tree data structure can achieve
fast nearest neighbor queries while maintaining performance on standard
chemical similarity search benchmarks. We examine different dimensionality
reductions of standard chemical embeddings as well as a learned,
structurally-aware embedding -- SmallSA -- for this task. With this framework,
searches on over one billion chemicals execute in less than a second on a
single CPU core, five orders of magnitude faster than the brute-force approach.
We also demonstrate that SmallSA achieves competitive performance on chemical
similarity benchmarks.
\\ ( https://arxiv.org/abs/2402.07970 ,  1431kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08001 (*cross-listing*)
Date: Mon, 12 Feb 2024 19:05:27 GMT   (1074kb,D)

Title: Improvement and generalization of ABCD method with Bayesian inference
Authors: Ezequiel Alvarez, Leandro Da Rold, Manuel Szewc, Alejandro Szynkman,
  Santiago A. Tanco, Tatiana Tarutina
Categories: hep-ph cs.LG hep-ex
Comments: 24 pages, 9 figures
\\
  To find New Physics or to refine our knowledge of the Standard Model at the
LHC is an enterprise that involves many factors. We focus on taking advantage
of available information and pour our effort in re-thinking the usual
data-driven ABCD method to improve it and to generalize it using Bayesian
Machine Learning tools. We propose that a dataset consisting of a signal and
many backgrounds is well described through a mixture model. Signal, backgrounds
and their relative fractions in the sample can be well extracted by exploiting
the prior knowledge and the dependence between the different observables at the
event-by-event level with Bayesian tools. We show how, in contrast to the ABCD
method, one can take advantage of understanding some properties of the
different backgrounds and of having more than two independent observables to
measure in each event. In addition, instead of regions defined through hard
cuts, the Bayesian framework uses the information of continuous distribution to
obtain soft-assignments of the events which are statistically more robust. To
compare both methods we use a toy problem inspired by $pp\to hh\to b\bar b b
\bar b$, selecting a reduced and simplified number of processes and analysing
the flavor of the four jets and the invariant mass of the jet-pairs, modeled
with simplified distributions. Taking advantage of all this information, and
starting from a combination of biased and agnostic priors, leads us to a very
good posterior once we use the Bayesian framework to exploit the data and the
mutual information of the observables at the event-by-event level. We show how,
in this simplified model, the Bayesian framework outperforms the ABCD method
sensitivity in obtaining the signal fraction in scenarios with $1\%$ and
$0.5\%$ true signal fractions in the dataset. We also show that the method is
robust against the absence of signal.
\\ ( https://arxiv.org/abs/2402.08001 ,  1074kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08012 (*cross-listing*)
Date: Mon, 12 Feb 2024 19:21:14 GMT   (35kb)

Title: Online Differentially Private Synthetic Data Generation
Authors: Yiyun He, Roman Vershynin, Yizhe Zhu
Categories: math.ST cs.DS cs.LG math.PR stat.TH
Comments: 19 pages
\\
  We present a polynomial-time algorithm for online differentially private
synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and
an infinite time horizon, we develop an online algorithm that generates a
differentially private synthetic dataset at each time $t$. This algorithm
achieves a near-optimal accuracy bound of $O(t^{-1/d}\log(t))$ for $d\geq 2$
and $O(t^{-1}\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This
result generalizes the previous work on the continual release model for
counting queries to include Lipschitz queries. Compared to the offline case,
where the entire dataset is available at once, our approach requires only an
extra polylog factor in the accuracy bound.
\\ ( https://arxiv.org/abs/2402.08012 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08063 (*cross-listing*)
Date: Mon, 12 Feb 2024 21:14:37 GMT   (806kb,D)

Title: Locality Sensitive Hashing for Network Traffic Fingerprinting
Authors: Nowfel Mashnoor, Jay Thom, Abdur Rouf, Shamik Sengupta, Batyr Charyyev
Categories: cs.NI cs.CR cs.LG
Comments: Conference Name: 2023 IEEE 29th International Symposium on Local and
  Metropolitan Area Networks (LANMAN) Date of Conference: 10-11 July 2023
DOI: 10.1109/LANMAN58293.2023.10189810
\\
  The advent of the Internet of Things (IoT) has brought forth additional
intricacies and difficulties to computer networks. These gadgets are
particularly susceptible to cyber-attacks because of their simplistic design.
Therefore, it is crucial to recognise these devices inside a network for the
purpose of network administration and to identify any harmful actions. Network
traffic fingerprinting is a crucial technique for identifying devices and
detecting anomalies. Currently, the predominant methods for this depend heavily
on machine learning (ML). Nevertheless, machine learning (ML) methods need the
selection of features, adjustment of hyperparameters, and retraining of models
to attain optimal outcomes and provide resilience to concept drifts detected in
a network. In this research, we suggest using locality-sensitive hashing (LSH)
for network traffic fingerprinting as a solution to these difficulties. Our
study focuses on examining several design options for the Nilsimsa LSH
function. We then use this function to create unique fingerprints for network
data, which may be used to identify devices. We also compared it with ML-based
traffic fingerprinting and observed that our method increases the accuracy of
state-of-the-art by 12% achieving around 94% accuracy in identifying devices in
a network.
\\ ( https://arxiv.org/abs/2402.08063 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08077 (*cross-listing*)
Date: Mon, 12 Feb 2024 21:44:20 GMT   (13915kb,D)

Title: Diffeomorphic Measure Matching with Kernels for Generative Modeling
Authors: Biraj Pandey, Bamdad Hosseini, Pau Batlle, and Houman Owhadi
Categories: stat.ML cs.LG math.DS stat.CO
MSC-class: 35Q68 49Q22 62F15 68T07 62R07
\\
  This article presents a general framework for the transport of probability
measures towards minimum divergence generative modeling and sampling using
ordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces
(RKHSs), inspired by ideas from diffeomorphic matching and image registration.
A theoretical analysis of the proposed method is presented, giving a priori
error bounds in terms of the complexity of the model, the number of samples in
the training set, and model misspecification. An extensive suite of numerical
experiments further highlights the properties, strengths, and weaknesses of the
method and extends its applicability to other tasks, such as conditional
simulation and inference.
\\ ( https://arxiv.org/abs/2402.08077 ,  13915kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08082 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:02:23 GMT   (45kb)

Title: Score-based generative models break the curse of dimensionality in
  learning a family of sub-Gaussian probability distributions
Authors: Frank Cole, Yulong Lu
Categories: stat.ML cs.LG
Comments: 30 pages, to appear in the proceedings of 12th International
  Conference on Learning Representations
\\
  While score-based generative models (SGMs) have achieved remarkable success
in enormous image generation tasks, their mathematical foundations are still
limited. In this paper, we analyze the approximation and generalization of SGMs
in learning a family of sub-Gaussian probability distributions. We introduce a
notion of complexity for probability distributions in terms of their relative
density with respect to the standard Gaussian measure. We prove that if the
log-relative density can be locally approximated by a neural network whose
parameters can be suitably bounded, then the distribution generated by
empirical score matching approximates the target distribution in total
variation with a dimension-independent rate. We illustrate our theory through
examples, which include certain mixtures of Gaussians. An essential ingredient
of our proof is to derive a dimension-free deep neural network approximation
rate for the true score function associated with the forward process, which is
interesting in its own right.
\\ ( https://arxiv.org/abs/2402.08082 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08095 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:26:52 GMT   (724kb)

Title: Convergence Analysis of Discrete Diffusion Model: Exact Implementation
  through Uniformization
Authors: Hongrui Chen, Lexing Ying
Categories: stat.ML cs.LG
Comments: 19 pages
\\
  Diffusion models have achieved huge empirical success in data generation
tasks. Recently, some efforts have been made to adapt the framework of
diffusion models to discrete state space, providing a more natural approach for
modeling intrinsically discrete data, such as language and graphs. This is
achieved by formulating both the forward noising process and the corresponding
reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we
investigate the theoretical properties of the discrete diffusion model.
Specifically, we introduce an algorithm leveraging the uniformization of
continuous Markov chains, implementing transitions on random time points. Under
reasonable assumptions on the learning of the discrete score function, we
derive Total Variation distance and KL divergence guarantees for sampling from
any distribution on a hypercube. Our results align with state-of-the-art
achievements for diffusion models in $\mathbb{R}^d$ and further underscore the
advantages of discrete diffusion models in comparison to the $\mathbb{R}^d$
setting.
\\ ( https://arxiv.org/abs/2402.08095 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08097 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:34:53 GMT   (425kb)

Title: An Accelerated Gradient Method for Simple Bilevel Optimization with
  Convex Lower-level Problem
Authors: Jincheng Cao, Ruichen Jiang, Erfan Yazdandoost Hamedani, Aryan
  Mokhtari
Categories: math.OC cs.LG stat.ML
\\
  In this paper, we focus on simple bilevel optimization problems, where we
minimize a convex smooth objective function over the optimal solution set of
another convex smooth constrained optimization problem. We present a novel
bilevel optimization method that locally approximates the solution set of the
lower-level problem using a cutting plane approach and employs an accelerated
gradient-based update to reduce the upper-level objective function over the
approximated solution set. We measure the performance of our method in terms of
suboptimality and infeasibility errors and provide non-asymptotic convergence
guarantees for both error criteria. Specifically, when the feasible set is
compact, we show that our method requires at most
$\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ iterations to find a
solution that is $\epsilon_f$-suboptimal and $\epsilon_g$-infeasible. Moreover,
under the additional assumption that the lower-level objective satisfies the
$r$-th H\"olderian error bound, we show that our method achieves an iteration
complexity of
$\mathcal{O}(\max\{\epsilon_{f}^{-\frac{2r-1}{2r}},\epsilon_{g}^{-\frac{2r-1}{2r}}\})$,
which matches the optimal complexity of single-level convex constrained
optimization when $r=1$.
\\ ( https://arxiv.org/abs/2402.08097 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08106 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:52:32 GMT   (26kb)

Title: Mirror Descent-Ascent for mean-field min-max problems
Authors: Razvan-Andrei Lascu, Mateusz B. Majka, {\L}ukasz Szpruch
Categories: math.OC cs.LG math.PR
Comments: 32 pages
\\
  We study two variants of the mirror descent-ascent algorithm for solving
min-max problems on the space of measures: simultaneous and sequential. We work
under assumptions of convexity-concavity and relative smoothness of the payoff
function with respect to a suitable Bregman divergence, defined on the space of
measures via flat derivatives. We show that the convergence rates to mixed Nash
equilibria, measured in the Nikaid\`o-Isoda error, are of order
$\mathcal{O}\left(N^{-1/2}\right)$ and $\mathcal{O}\left(N^{-2/3}\right)$ for
the simultaneous and sequential schemes, respectively, which is in line with
the state-of-the-art results for related finite-dimensional algorithms.
\\ ( https://arxiv.org/abs/2402.08106 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08108 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:56:16 GMT   (1839kb,D)

Title: Finding Moving-Band Statistical Arbitrages via Convex-Concave
  Optimization
Authors: Kasper Johansson and Thomas Schmelzer and Stephen Boyd
Categories: econ.EM cs.LG q-fin.PM
\\
  We propose a new method for finding statistical arbitrages that can contain
more assets than just the traditional pair. We formulate the problem as seeking
a portfolio with the highest volatility, subject to its price remaining in a
band and a leverage limit. This optimization problem is not convex, but can be
approximately solved using the convex-concave procedure, a specific sequential
convex programming method. We show how the method generalizes to finding
moving-band statistical arbitrages, where the price band midpoint varies over
time.
\\ ( https://arxiv.org/abs/2402.08108 ,  1839kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08109 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:56:18 GMT   (1956kb)

Title: From Data to Decisions: The Transformational Power of Machine Learning
  in Business Recommendations
Authors: Kapilya Gangadharan, K. Malathi, Anoop Purandaran, Barathi
  Subramanian, and Rathinaraja Jeyaraj
Categories: cs.DC cs.IR cs.LG
Comments: 55 pages, 14 figures
\\
  This research aims to explore the impact of Machine Learning (ML) on the
evolution and efficacy of Recommendation Systems (RS), particularly in the
context of their growing significance in commercial business environments.
Methodologically, the study delves into the role of ML in crafting and refining
these systems, focusing on aspects such as data sourcing, feature engineering,
and the importance of evaluation metrics, thereby highlighting the iterative
nature of enhancing recommendation algorithms. The deployment of Recommendation
Engines (RE), driven by advanced algorithms and data analytics, is explored
across various domains, showcasing their significant impact on user experience
and decision-making processes. These engines not only streamline information
discovery and enhance collaboration but also accelerate knowledge acquisition,
proving vital in navigating the digital landscape for businesses. They
contribute significantly to sales, revenue, and the competitive edge of
enterprises by offering improved recommendations that align with individual
customer needs. The research identifies the increasing expectation of users for
a seamless, intuitive online experience, where content is personalized and
dynamically adapted to changing preferences. Future research directions include
exploring advancements in deep learning models, ethical considerations in the
deployment of RS, and addressing scalability challenges. This study emphasizes
the indispensability of comprehending and leveraging ML in RS for researchers
and practitioners, to tap into the full potential of personalized
recommendation in commercial business prospects.
\\ ( https://arxiv.org/abs/2402.08109 ,  1956kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08201 (*cross-listing*)
Date: Tue, 13 Feb 2024 03:55:56 GMT   (101kb,D)

Title: Off-Policy Evaluation in Markov Decision Processes under Weak
  Distributional Overlap
Authors: Mohammad Mehrabi and Stefan Wager
Categories: stat.ML cs.LG
Comments: 50 pages, 4 figures
\\
  Doubly robust methods hold considerable promise for off-policy evaluation in
Markov decision processes (MDPs) under sequential ignorability: They have been
shown to converge as $1/\sqrt{T}$ with the horizon $T$, to be statistically
efficient in large samples, and to allow for modular implementation where
preliminary estimation tasks can be executed using standard reinforcement
learning techniques. Existing results, however, make heavy use of a strong
distributional overlap assumption whereby the stationary distributions of the
target policy and the data-collection policy are within a bounded factor of
each other -- and this assumption is typically only credible when the state
space of the MDP is bounded. In this paper, we re-visit the task of off-policy
evaluation in MDPs under a weaker notion of distributional overlap, and
introduce a class of truncated doubly robust (TDR) estimators which we find to
perform well in this setting. When the distribution ratio of the target and
data-collection policies is square-integrable (but not necessarily bounded),
our approach recovers the large-sample behavior previously established under
strong distributional overlap. When this ratio is not square-integrable, TDR is
still consistent but with a slower-than-$1/\sqrt{T}$; furthermore, this rate of
convergence is minimax over a class of MDPs defined only using mixing
conditions. We validate our approach numerically and find that, in our
experiments, appropriate truncation plays a major role in enabling accurate
off-policy evaluation when strong distributional overlap does not hold.
\\ ( https://arxiv.org/abs/2402.08201 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08210 (*cross-listing*)
Date: Tue, 13 Feb 2024 04:19:06 GMT   (25482kb,D)

Title: Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS
Authors: Mohammad Ghazi Vakili, Christoph Gorgulla, AkshatKumar Nigam, Dmitry
  Bezrukov, Daniel Varoli, Alex Aliper, Daniil Polykovsky, Krishna M.
  Padmanabha Das, Jamie Snider, Anna Lyakisheva, Ardalan Hosseini Mansob, Zhong
  Yao, Lela Bitar, Eugene Radchenko, Xiao Ding, Jinxin Liu, Fanye Meng, Feng
  Ren, Yudong Cao, Igor Stagljar, Al\'an Aspuru-Guzik, Alex Zhavoronkov
Categories: quant-ph cs.CE cs.GT cs.LG
\\
  The discovery of small molecules with therapeutic potential is a
long-standing challenge in chemistry and biology. Researchers have increasingly
leveraged novel computational techniques to streamline the drug development
process to increase hit rates and reduce the costs associated with bringing a
drug to market. To this end, we introduce a quantum-classical generative model
that seamlessly integrates the computational power of quantum algorithms
trained on a 16-qubit IBM quantum computer with the established reliability of
classical methods for designing small molecules. Our hybrid generative model
was applied to designing new KRAS inhibitors, a crucial target in cancer
therapy. We synthesized 15 promising molecules during our investigation and
subjected them to experimental testing to assess their ability to engage with
the target. Notably, among these candidates, two molecules, ISM061-018-2 and
ISM061-22, each featuring unique scaffolds, stood out by demonstrating
effective engagement with KRAS. ISM061-018-2 was identified as a broad-spectrum
KRAS inhibitor, exhibiting a binding affinity to KRAS-G12D at $1.4 \mu M$.
Concurrently, ISM061-22 exhibited specific mutant selectivity, displaying
heightened activity against KRAS G12R and Q61H mutants. To our knowledge, this
work shows for the first time the use of a quantum-generative model to yield
experimentally confirmed biological hits, showcasing the practical potential of
quantum-assisted drug discovery to produce viable therapeutics. Moreover, our
findings reveal that the efficacy of distribution learning correlates with the
number of qubits utilized, underlining the scalability potential of quantum
computing resources. Overall, we anticipate our results to be a stepping stone
towards developing more advanced quantum generative models in drug discovery.
\\ ( https://arxiv.org/abs/2402.08210 ,  25482kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08233 (*cross-listing*)
Date: Tue, 13 Feb 2024 05:53:00 GMT   (283kb)

Title: End-to-End Policy Learning of a Statistical Arbitrage Autoencoder
  Architecture
Authors: Fabian Krause, Jan-Peter Calliess
Categories: q-fin.TR cs.LG
Comments: 11 pages, 1 figure
\\
  In Statistical Arbitrage (StatArb), classical mean reversion trading
strategies typically hinge on asset-pricing or PCA based models to identify the
mean of a synthetic asset. Once such a (linear) model is identified, a separate
mean reversion strategy is then devised to generate a trading signal. With a
view of generalising such an approach and turning it truly data-driven, we
study the utility of Autoencoder architectures in StatArb. As a first approach,
we employ a standard Autoencoder trained on US stock returns to derive trading
strategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance
this model, we take a policy-learning approach and embed the Autoencoder
network into a neural network representation of a space of portfolio trading
policies. This integration outputs portfolio allocations directly and is
end-to-end trainable by backpropagation of the risk-adjusted returns of the
neural policy. Our findings demonstrate that this innovative end-to-end policy
learning approach not only simplifies the strategy development process, but
also yields superior gross returns over its competitors illustrating the
potential of end-to-end training over classical two-stage approaches.
\\ ( https://arxiv.org/abs/2402.08233 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08251 (*cross-listing*)
Date: Tue, 13 Feb 2024 06:40:55 GMT   (18983kb,D)

Title: Object Detection in Thermal Images Using Deep Learning for Unmanned
  Aerial Vehicles
Authors: Minh Dang Tu, Kieu Trang Le, Manh Duong Phung
Categories: cs.CV cs.LG
Comments: Published in: 2024 IEEE/SICE International Symposium on System
  Integration (SII)
DOI: 10.1109/SII58957.2024.10417611
\\
  This work presents a neural network model capable of recognizing small and
tiny objects in thermal images collected by unmanned aerial vehicles. Our model
consists of three parts, the backbone, the neck, and the prediction head. The
backbone is developed based on the structure of YOLOv5 combined with the use of
a transformer encoder at the end. The neck includes a BI-FPN block combined
with the use of a sliding window and a transformer to increase the information
fed into the prediction head. The prediction head carries out the detection by
evaluating feature maps with the Sigmoid function. The use of transformers with
attention and sliding windows increases recognition accuracy while keeping the
model at a reasonable number of parameters and computation requirements for
embedded systems. Experiments conducted on public dataset VEDAI and our
collected datasets show that our model has a higher accuracy than
state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5,
SMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that
our model achieves a real-time computation speed with a stability rate of over
90%.
\\ ( https://arxiv.org/abs/2402.08251 ,  18983kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08344 (*cross-listing*)
Date: Tue, 13 Feb 2024 10:19:33 GMT   (2729kb,D)

Title: Implicit Bias in Noisy-SGD: With Applications to Differentially Private
  Training
Authors: Tom Sander, Maxime Sylvestre, Alain Durmus
Categories: stat.ML cs.LG
\\
  Training Deep Neural Networks (DNNs) with small batches using Stochastic
Gradient Descent (SGD) yields superior test performance compared to larger
batches. The specific noise structure inherent to SGD is known to be
responsible for this implicit bias. DP-SGD, used to ensure differential privacy
(DP) in DNNs' training, adds Gaussian noise to the clipped gradients.
Surprisingly, large-batch training still results in a significant decrease in
performance, which poses an important challenge because strong DP guarantees
necessitate the use of massive batches. We first show that the phenomenon
extends to Noisy-SGD (DP-SGD without clipping), suggesting that the
stochasticity (and not the clipping) is the cause of this implicit bias, even
with additional isotropic Gaussian noise. We theoretically analyse the
solutions obtained with continuous versions of Noisy-SGD for the Linear Least
Square and Diagonal Linear Network settings, and reveal that the implicit bias
is indeed amplified by the additional noise. Thus, the performance issues of
large-batch DP-SGD training are rooted in the same underlying principles as
SGD, offering hope for potential improvements in large batch training
strategies.
\\ ( https://arxiv.org/abs/2402.08344 ,  2729kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08379 (*cross-listing*)
Date: Tue, 13 Feb 2024 11:18:27 GMT   (1020kb,D)

Title: The Duet of Representations and How Explanations Exacerbate It
Authors: Charles Wan, Rodrigo Belo, Leid Zejnilovi\'c, Susana Lavado
Categories: cs.HC cs.LG
Journal-ref: In World Conference on Explainable Artificial Intelligence (pp.
  181-197). Cham: Springer Nature Switzerland (2023)
DOI: 10.1007/978-3-031-44067-0_10
\\
  An algorithm effects a causal representation of relations between features
and labels in the human's perception. Such a representation might conflict with
the human's prior belief. Explanations can direct the human's attention to the
conflicting feature and away from other relevant features. This leads to causal
overattribution and may adversely affect the human's information processing. In
a field experiment we implemented an XGBoost-trained model as a decision-making
aid for counselors at a public employment service to predict candidates' risk
of long-term unemployment. The treatment group of counselors was also provided
with SHAP. The results show that the quality of the human's decision-making is
worse when a feature on which the human holds a conflicting prior belief is
displayed as part of the explanation.
\\ ( https://arxiv.org/abs/2402.08379 ,  1020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08412 (*cross-listing*)
Date: Tue, 13 Feb 2024 12:29:38 GMT   (8885kb,D)

Title: Interacting Particle Systems on Networks: joint inference of the network
  and the interaction kernel
Authors: Quanjun Lang, Xiong Wang, Fei Lu and Mauro Maggioni
Categories: stat.ML cs.LG math.DS math.ST stat.TH
Comments: 53 pages, 17 figures
MSC-class: 62F12, 82C22
\\
  Modeling multi-agent systems on networks is a fundamental challenge in a wide
variety of disciplines. We jointly infer the weight matrix of the network and
the interaction kernel, which determine respectively which agents interact with
which others and the rules of such interactions from data consisting of
multiple trajectories. The estimator we propose leads naturally to a non-convex
optimization problem, and we investigate two approaches for its solution: one
is based on the alternating least squares (ALS) algorithm; another is based on
a new algorithm named operator regression with alternating least squares
(ORALS). Both algorithms are scalable to large ensembles of data trajectories.
We establish coercivity conditions guaranteeing identifiability and
well-posedness. The ALS algorithm appears statistically efficient and robust
even in the small data regime but lacks performance and convergence guarantees.
The ORALS estimator is consistent and asymptotically normal under a coercivity
condition. We conduct several numerical experiments ranging from Kuramoto
particle systems on networks to opinion dynamics in leader-follower models.
\\ ( https://arxiv.org/abs/2402.08412 ,  8885kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08422 (*cross-listing*)
Date: Tue, 13 Feb 2024 12:49:50 GMT   (648kb,D)

Title: Distribution Estimation under the Infinity Norm
Authors: Aryeh Kontorovich and Amichai Painsky
Categories: math.ST cs.LG stat.TH
Comments: Distribution Estimation, Probability Estimation, Infinity Norm
\\
  We present novel bounds for estimating discrete probability distributions
under the $\ell_\infty$ norm. These are nearly optimal in various precise
senses, including a kind of instance-optimality. Our data-dependent convergence
guarantees for the maximum likelihood estimator significantly improve upon the
currently known results. A variety of techniques are utilized and innovated
upon, including Chernoff-type inequalities and empirical Bernstein bounds. We
illustrate our results in synthetic and real-world experiments. Finally, we
apply our proposed framework to a basic selective inference problem, where we
estimate the most frequent probabilities in a sample.
\\ ( https://arxiv.org/abs/2402.08422 ,  648kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08424 (*cross-listing*)
Date: Tue, 13 Feb 2024 12:52:02 GMT   (12809kb,D)

Title: Conditional Neural Expert Processes for Learning from Demonstration
Authors: Yigit Yildirim, Emre Ugur
Categories: cs.RO cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Submitted to Robotics and Automation Letters on
  February 13, 2024
\\
  Learning from Demonstration (LfD) is a widely used technique for skill
acquisition in robotics. However, demonstrations of the same skill may exhibit
significant variances, or learning systems may attempt to acquire different
means of the same skill simultaneously, making it challenging to encode these
motions into movement primitives. To address these challenges, we propose an
LfD framework, namely the Conditional Neural Expert Processes (CNEP), that
learns to assign demonstrations from different modes to distinct expert
networks utilizing the inherent information within the latent space to match
experts with the encoded representations. CNEP does not require supervision on
which mode the trajectories belong to. Provided experiments on artificially
generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare
the performance of CNEP with another LfD framework, namely Conditional Neural
Movement Primitives (CNMP), on a range of tasks, including experiments on a
real robot. The results reveal enhanced modeling performance for movement
primitives, leading to the synthesis of trajectories that more accurately
reflect those demonstrated by experts, particularly when the model inputs
include intersection points from various trajectories. Additionally, CNEP
offers improved interpretability and faster convergence by promoting expert
specialization. Furthermore, we show that the CNEP model accomplishes obstacle
avoidance tasks with a real manipulator when provided with novel start and
destination points, in contrast to the CNMP model, which leads to collisions
with the obstacle.
\\ ( https://arxiv.org/abs/2402.08424 ,  12809kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08425 (*cross-listing*)
Date: Tue, 13 Feb 2024 12:52:41 GMT   (2302kb,D)

Title: Transfer Operators from Batches of Unpaired Points via Entropic
  Transport Kernels
Authors: Florian Beier, Hancheng Bi, Cl\'ement Sarrazin, Bernhard Schmitzer,
  Gabriele Steidl
Categories: stat.ML cs.LG math.DS
MSC-class: 37A30, 62G07
\\
  In this paper, we are concerned with estimating the joint probability of
random variables $X$ and $Y$, given $N$ independent observation blocks
$(\boldsymbol{x}^i,\boldsymbol{y}^i)$, $i=1,\ldots,N$, each of $M$ samples
$(\boldsymbol{x}^i,\boldsymbol{y}^i) = \bigl((x^i_j, y^i_{\sigma^i(j)})
\bigr)_{j=1}^M$, where $\sigma^i$ denotes an unknown permutation of i.i.d.
sampled pairs $(x^i_j,y_j^i)$, $j=1,\ldots,M$. This means that the internal
ordering of the $M$ samples within an observation block is not known. We derive
a maximum-likelihood inference functional, propose a computationally tractable
approximation and analyze their properties. In particular, we prove a
$\Gamma$-convergence result showing that we can recover the true density from
empirical approximations as the number $N$ of blocks goes to infinity. Using
entropic optimal transport kernels, we model a class of hypothesis spaces of
density functions over which the inference functional can be minimized. This
hypothesis class is particularly suited for approximate inference of transfer
operators from data. We solve the resulting discrete minimization problem by a
modification of the EMML algorithm to take addional transition probability
constraints into account and prove the convergence of this algorithm.
Proof-of-concept examples demonstrate the potential of our method.
\\ ( https://arxiv.org/abs/2402.08425 ,  2302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08426 (*cross-listing*)
Date: Tue, 13 Feb 2024 12:53:18 GMT   (400kb,D)

Title: Frequency-aware Graph Signal Processing for Collaborative Filtering
Authors: Jiafeng Xia, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang and
  Ning Gu
Categories: cs.IR cs.LG
\\
  Graph Signal Processing (GSP) based recommendation algorithms have recently
attracted lots of attention due to its high efficiency. However, these methods
failed to consider the importance of various interactions that reflect unique
user/item characteristics and failed to utilize user and item high-order
neighborhood information to model user preference, thus leading to sub-optimal
performance. To address the above issues, we propose a frequency-aware graph
signal processing method (FaGSP) for collaborative filtering. Firstly, we
design a Cascaded Filter Module, consisting of an ideal high-pass filter and an
ideal low-pass filter that work in a successive manner, to capture both unique
and common user/item characteristics to more accurately model user preference.
Then, we devise a Parallel Filter Module, consisting of two low-pass filters
that can easily capture the hierarchy of neighborhood, to fully utilize
high-order neighborhood information of users/items for more accurate user
preference modeling. Finally, we combine these two modules via a linear model
to further improve recommendation accuracy. Extensive experiments on six public
datasets demonstrate the superiority of our method from the perspectives of
prediction accuracy and training efficiency compared with state-of-the-art
GCN-based recommendation methods and GSP-based recommendation methods.
\\ ( https://arxiv.org/abs/2402.08426 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08468 (*cross-listing*)
Date: Tue, 13 Feb 2024 13:54:47 GMT   (840kb)

Title: ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical
  System
Authors: Tommaso Puccetti, Simone Nardi, Cosimo Cinquilli, Tommaso Zoppi,
  Andrea Ceccarelli
Categories: cs.CR cs.LG
Comments: 18 pages
\\
  Most of the intrusion detection datasets to research machine learning-based
intrusion detection systems (IDSs) are devoted to cyber-only systems, and they
typically collect data from one architectural layer. Additionally, often the
attacks are generated in dedicated attack sessions, without reproducing the
realistic alternation and overlap of normal and attack actions. We present a
dataset for intrusion detection by performing penetration testing on an
embedded cyber-physical system built over Robot Operating System 2 (ROS2).
Features are monitored from three architectural layers: the Linux operating
system, the network, and the ROS2 services. The dataset is structured as a time
series and describes the expected behavior of the system and its response to
ROS2-specific attacks: it repeatedly alternates periods of attack-free
operation with periods when a specific attack is being performed. Noteworthy,
this allows measuring the time to detect an attacker and the number of
malicious activities performed before detection. Also, it allows training an
intrusion detector to minimize both, by taking advantage of the numerous
alternating periods of normal and attack operations.
\\ ( https://arxiv.org/abs/2402.08468 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08508 (*cross-listing*)
Date: Tue, 13 Feb 2024 15:03:02 GMT   (138kb,D)

Title: A PAC-Bayesian Link Between Generalisation and Flat Minima
Authors: Maxime Haddouche, Paul Viallard, Umut Simsekli, Benjamin Guedj
Categories: stat.ML cs.LG
Comments: We provide novel PAC-Bayesian generalisation bounds involving
  gradient norms and being interpretable under the lens of flat minima
\\
  Modern machine learning usually involves predictors in the overparametrised
setting (number of trained parameters greater than dataset size), and their
training yield not only good performances on training data, but also good
generalisation capacity. This phenomenon challenges many theoretical results,
and remains an open problem. To reach a better understanding, we provide novel
generalisation bounds involving gradient terms. To do so, we combine the
PAC-Bayes toolbox with Poincar\'e and Log-Sobolev inequalities, avoiding an
explicit dependency on dimension of the predictor space. Our results highlight
the positive influence of \emph{flat minima} (being minima with a neighbourhood
nearly minimising the learning problem as well) on generalisation performances,
involving directly the benefits of the optimisation phase.
\\ ( https://arxiv.org/abs/2402.08508 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08576 (*cross-listing*)
Date: Tue, 13 Feb 2024 16:24:57 GMT   (246kb,D)

Title: Regret Minimization in Stackelberg Games with Side Information
Authors: Keegan Harris, Zhiwei Steven Wu, Maria-Florina Balcan
Categories: cs.GT cs.LG
\\
  In its most basic form, a Stackelberg game is a two-player game in which a
leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg
games are perhaps one of the biggest success stories of algorithmic game theory
over the last decade, as algorithms for playing in Stackelberg games have been
deployed in many real-world domains including airport security, anti-poaching
efforts, and cyber-crime prevention. However, these algorithms often fail to
take into consideration the additional information available to each player
(e.g. traffic patterns, weather conditions, network congestion), a salient
feature of reality which may significantly affect both players' optimal
strategies. We formalize such settings as Stackelberg games with side
information, in which both players observe an external context before playing.
The leader then commits to a (possibly context-dependent) strategy, and the
follower best-responds to both the leader's strategy and the context. We focus
on the online setting in which a sequence of followers arrive over time, and
the context may change from round-to-round. In sharp contrast to the
non-contextual version, we show that it is impossible for the leader to achieve
good performance (measured by regret) in the full adversarial setting (i.e.,
when both the context and the follower are chosen by an adversary). However, it
turns out that a little bit of randomness goes a long way. Motivated by our
impossibility result, we show that no-regret learning is possible in two
natural relaxations: the setting in which the sequence of followers is chosen
stochastically and the sequence of contexts is adversarial, and the setting in
which the sequence of contexts is stochastic and the sequence of followers is
chosen by an adversary.
\\ ( https://arxiv.org/abs/2402.08576 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08592 (*cross-listing*)
Date: Tue, 13 Feb 2024 16:52:10 GMT   (565kb)

Title: Convolutional Neural Networks Towards Facial Skin Lesions Detection
Authors: Reza Sarshar, Mohammad Heydari, Elham Akhondzadeh Noughabi
Categories: eess.IV cs.CV cs.LG
Comments: 6 pages, 11 figures
\\
  Facial analysis has emerged as a prominent area of research with diverse
applications, including cosmetic surgery programs, the beauty industry,
photography, and entertainment. Manipulating patient images often necessitates
professional image processing software. This study contributes by providing a
model that facilitates the detection of blemishes and skin lesions on facial
images through a convolutional neural network and machine learning approach.
The proposed method offers advantages such as simple architecture, speed and
suitability for image processing while avoiding the complexities associated
with traditional methods. The model comprises four main steps: area selection,
scanning the chosen region, lesion diagnosis, and marking the identified
lesion. Raw data for this research were collected from a reputable clinic in
Tehran specializing in skincare and beauty services. The dataset includes
administrative information, clinical data, and facial and profile images. A
total of 2300 patient images were extracted from this raw data. A software tool
was developed to crop and label lesions, with input from two treatment experts.
In the lesion preparation phase, the selected area was standardized to 50 * 50
pixels. Subsequently, a convolutional neural network model was employed for
lesion labeling. The classification model demonstrated high accuracy, with a
measure of 0.98 for healthy skin and 0.97 for lesioned skin specificity.
Internal validation involved performance indicators and cross-validation, while
external validation compared the model's performance indicators with those of
the transfer learning method using the Vgg16 deep network model. Compared to
existing studies, the results of this research showcase the efficacy and
desirability of the proposed model and methodology.
\\ ( https://arxiv.org/abs/2402.08592 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08606 (*cross-listing*)
Date: Tue, 13 Feb 2024 17:12:01 GMT   (275kb,D)

Title: Arbitrary Polynomial Separations in Trainable Quantum Machine Learning
Authors: Eric R. Anschuetz and Xun Gao
Categories: quant-ph cs.LG
Comments: 35 pages, 3 figures
\\
  Recent theoretical results in quantum machine learning have demonstrated a
general trade-off between the expressive power of quantum neural networks
(QNNs) and their trainability; as a corollary of these results, practical
exponential separations in expressive power over classical machine learning
models are believed to be infeasible as such QNNs take a time to train that is
exponential in the model size. We here circumvent these negative results by
constructing a hierarchy of efficiently trainable QNNs that exhibit
unconditionally provable, polynomial memory separations of arbitrary constant
degree over classical neural networks in performing a classical sequence
modeling task. Furthermore, each unit cell of the introduced class of QNNs is
computationally efficient, implementable in constant time on a quantum device.
The classical networks we prove a separation over include well-known examples
such as recurrent neural networks and Transformers. We show that quantum
contextuality is the source of the expressivity separation, suggesting that
other classical sequence learning problems with long-time correlations may be a
regime where practical advantages in quantum machine learning may exist.
\\ ( https://arxiv.org/abs/2402.08606 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08616 (*cross-listing*)
Date: Tue, 13 Feb 2024 17:32:59 GMT   (44kb)

Title: Adjustment Identification Distance: A gadjid for Causal Structure
  Learning
Authors: Leonard Henckel and Theo W\"urtzen and Sebastian Weichwald
Categories: stat.ML cs.LG stat.ME
\\
  Evaluating graphs learned by causal discovery algorithms is difficult: The
number of edges that differ between two graphs does not reflect how the graphs
differ with respect to the identifying formulas they suggest for causal
effects. We introduce a framework for developing causal distances between
graphs which includes the structural intervention distance for directed acyclic
graphs as a special case. We use this framework to develop improved
adjustment-based distances as well as extensions to completed partially
directed acyclic graphs and causal orders. We develop polynomial-time
reachability algorithms to compute the distances efficiently. In our package
gadjid (open source at https://github.com/CausalDisco/gadjid), we provide
implementations of our distances; they are orders of magnitude faster than the
structural intervention distance and thereby provide a success metric for
causal discovery that scales to graph sizes that were previously prohibitive.
\\ ( https://arxiv.org/abs/2402.08616 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08637 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:03:56 GMT   (37kb,D)

Title: Strategizing against No-Regret Learners in First-Price Auctions
Authors: Aviad Rubinstein and Junyao Zhao
Categories: cs.GT cs.DS cs.LG
\\
  We study repeated first-price auctions and general repeated Bayesian games
between two players, where one player, the learner, employs a no-regret
learning algorithm, and the other player, the optimizer, knowing the learner's
algorithm, strategizes to maximize its own utility. For a commonly used class
of no-regret learning algorithms called mean-based algorithms, we show that (i)
in standard (i.e., full-information) first-price auctions, the optimizer cannot
get more than the Stackelberg utility -- a standard benchmark in the
literature, but (ii) in Bayesian first-price auctions, there are instances
where the optimizer can achieve much higher than the Stackelberg utility.
  On the other hand, Mansour et al. (2022) showed that a more sophisticated
class of algorithms called no-polytope-swap-regret algorithms are sufficient to
cap the optimizer's utility at the Stackelberg utility in any repeated Bayesian
game (including Bayesian first-price auctions), and they pose the open question
whether no-polytope-swap-regret algorithms are necessary to cap the optimizer's
utility. For general Bayesian games, under a reasonable and necessary
condition, we prove that no-polytope-swap-regret algorithms are indeed
necessary to cap the optimizer's utility and thus answer their open question.
For Bayesian first-price auctions, we give a simple improvement of the standard
algorithm for minimizing the polytope swap regret by exploiting the structure
of Bayesian first-price auctions.
\\ ( https://arxiv.org/abs/2402.08637 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08643 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:20:04 GMT   (10547kb,D)

Title: Learned Image Compression with Text Quality Enhancement
Authors: Chih-Yu Lai, Dung Tran, and Kazuhito Koishida
Categories: cs.CV cs.LG
Comments: Submitted to ICIP 2024
\\
  Learned image compression has gained widespread popularity for their
efficiency in achieving ultra-low bit-rates. Yet, images containing substantial
textual content, particularly screen-content images (SCI), often suffers from
text distortion at such compressed levels. To address this, we propose to
minimize a novel text logit loss designed to quantify the disparity in text
between the original and reconstructed images, thereby improving the perceptual
quality of the reconstructed text. Through rigorous experimentation across
diverse datasets and employing state-of-the-art algorithms, our findings reveal
significant enhancements in the quality of reconstructed text upon integration
of the proposed loss function with appropriate weighting. Notably, we achieve a
Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and
-28.03% for Word Error Rate (WER) on average by applying the text logit loss
for two screenshot datasets. Additionally, we present quantitative metrics
tailored for evaluating text quality in image compression tasks. Our findings
underscore the efficacy and potential applicability of our proposed text logit
loss function across various text-aware image compression contexts.
\\ ( https://arxiv.org/abs/2402.08643 ,  10547kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08645 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:24:10 GMT   (2048kb,D)

Title: Peeking Behind the Curtains of Residual Learning
Authors: Tunhou Zhang, Feng Yan, Hai Li, Yiran Chen
Categories: cs.CV cs.LG
Comments: Arxiv Preprint
\\
  The utilization of residual learning has become widespread in deep and
scalable neural nets. However, the fundamental principles that contribute to
the success of residual learning remain elusive, thus hindering effective
training of plain nets with depth scalability. In this paper, we peek behind
the curtains of residual learning by uncovering the "dissipating inputs"
phenomenon that leads to convergence failure in plain neural nets: the input is
gradually compromised through plain layers due to non-linearities, resulting in
challenges of learning feature representations. We theoretically demonstrate
how plain neural nets degenerate the input to random noise and emphasize the
significance of a residual connection that maintains a better lower bound of
surviving neurons as a solution. With our theoretical discoveries, we propose
"The Plain Neural Net Hypothesis" (PNNH) that identifies the internal path
across non-linear layers as the most critical part in residual learning, and
establishes a paradigm to support the training of deep plain neural nets devoid
of residual connections. We thoroughly evaluate PNNH-enabled CNN architectures
and Transformers on popular vision benchmarks, showing on-par accuracy, up to
0.3% higher training throughput, and 2x better parameter efficiency compared to
ResNets and vision Transformers.
\\ ( https://arxiv.org/abs/2402.08645 ,  2048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08662 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:46:10 GMT   (3176kb,D)

Title: Learning Emergent Gaits with Decentralized Phase Oscillators: on the
  role of Observations, Rewards, and Feedback
Authors: Jenny Zhang, Steve Heim, Se Hwan Jeon, Sangbae Kim
Categories: cs.RO cs.LG
Comments: ICRA 2024, 8 pages 7 Figures
\\
  We present a minimal phase oscillator model for learning quadrupedal
locomotion. Each of the four oscillators is coupled only to itself and its
corresponding leg through local feedback of the ground reaction force, which
can be interpreted as an observer feedback gain. We interpret the oscillator
itself as a latent contact state-estimator. Through a systematic ablation
study, we show that the combination of phase observations, simple phase-based
rewards, and the local feedback dynamics induces policies that exhibit emergent
gait preferences, while using a reduced set of simple rewards, and without
prescribing a specific gait. The code is open-source, and a video synopsis
available at https://youtu.be/1NKQ0rSV3jU.
\\ ( https://arxiv.org/abs/2402.08662 ,  3176kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08674 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:55:27 GMT   (5376kb,D)

Title: Human Curriculum Effects Emerge with In-Context Learning in Neural
  Networks
Authors: Jacob Russin, Ellie Pavlick, Michael J. Frank
Categories: cs.NE cs.LG q-bio.NC
Comments: 7 pages, 4 figures, under review at CogSci 2024
\\
  Human learning is sensitive to rule-like structure and the curriculum of
examples used for training. In tasks governed by succinct rules, learning is
more robust when related examples are blocked across trials, but in the absence
of such rules, interleaving is more effective. To date, no neural model has
simultaneously captured these seemingly contradictory effects. Here we show
that this same tradeoff spontaneously emerges with "in-context learning" (ICL)
both in neural networks trained with metalearning and in large language models
(LLMs). ICL is the ability to learn new tasks "in context" - without weight
changes - via an inner-loop algorithm implemented in activation dynamics.
Experiments with pretrained LLMs and metalearning transformers show that ICL
exhibits the blocking advantage demonstrated in humans on a task involving
rule-like structure, and conversely, that concurrent in-weight learning
reproduces the interleaving advantage observed in humans on tasks lacking such
structure.
\\ ( https://arxiv.org/abs/2402.08674 ,  5376kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2309.03041
replaced with revised version Tue, 13 Feb 2024 07:35:25 GMT   (39kb,D)

Title: A Refutation of Shapley Values for Explainability
Authors: Xuanxiang Huang, Joao Marques-Silva
Categories: cs.AI
\\ ( https://arxiv.org/abs/2309.03041 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04918
replaced with revised version Tue, 13 Feb 2024 18:22:41 GMT   (341kb,D)

Title: SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning
Authors: Lei You and Hei Victor Cheng
Categories: cs.AI
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.04918 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06646
replaced with revised version Tue, 13 Feb 2024 17:25:42 GMT   (40kb,D)

Title: Computational Copyright: Towards A Royalty Model for Music Generative AI
Authors: Junwei Deng, Jiaqi Ma
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.06646 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16044
replaced with revised version Tue, 13 Feb 2024 13:02:23 GMT   (10337kb,D)

Title: LLMLight: Large Language Models as Traffic Signal Control Agents
Authors: Siqi Lai, Zhao Xu, Weijia Zhang, Hao Liu and Hui Xiong
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.16044 ,  10337kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12467
replaced with revised version Tue, 13 Feb 2024 08:21:50 GMT   (3115kb,D)

Title: An open dataset for the evolution of oracle bone characters: EVOBC
Authors: Haisu Guan, Jinpeng Wan, Yuliang Liu, Pengjie Wang, Kaile Zhang,
  Zhebin Kuang, Xinyu Wang, Xiang Bai, Lianwen Jin
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.12467 ,  3115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00060
replaced with revised version Tue, 13 Feb 2024 18:06:21 GMT   (3918kb,D)

Title: Treatment of Epistemic Uncertainty in Conjunction Analysis with
  Dempster-Shafer Theory
Authors: Luis Sanchez and Massimiliano Vasile and Silvia Sanvido and Klaus
  Mertz and Christophe Taillan
Categories: cs.AI cs.IT math.IT math.PR
Comments: 28 pages, 23 figures
\\ ( https://arxiv.org/abs/2402.00060 ,  3918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03181
replaced with revised version Mon, 12 Feb 2024 22:19:17 GMT   (14509kb,D)

Title: C-RAG: Certified Generation Risks for Retrieval-Augmented Language
  Models
Authors: Mintong Kang, Nezihe Merve G\"urel, Ning Yu, Dawn Song, Bo Li
Categories: cs.AI cs.CL cs.IR
\\ ( https://arxiv.org/abs/2402.03181 ,  14509kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04338
replaced with revised version Tue, 13 Feb 2024 16:05:50 GMT   (0kb,I)

Title: Logical recognition method for solving the problem of identification in
  the Internet of Things
Authors: Islambek Saymanov
Categories: cs.AI
Comments: I will rework and improve it and post it again
\\ ( https://arxiv.org/abs/2402.04338 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07197
replaced with revised version Tue, 13 Feb 2024 09:25:37 GMT   (684kb,D)

Title: GraphTranslator: Aligning Graph Model to Large Language Model for
  Open-ended Tasks
Authors: Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao
  Xu, Hong Liu, Cheng Yang, Chuan Shi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07197 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07420
replaced with revised version Tue, 13 Feb 2024 07:02:05 GMT   (3433kb,D)

Title: On the Transit Obfuscation Problem
Authors: Hideaki Takahashi and Alex Fukunaga
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07420 ,  3433kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07462
replaced with revised version Tue, 13 Feb 2024 05:21:40 GMT   (36088kb)

Title: A Hormetic Approach to the Value-Loading Problem: Preventing the
  Paperclip Apocalypse?
Authors: Nathan I. N. Henry, Mangor Pedersen, Matt Williams, Jamin L. B.
  Martin, Liesje Donkin
Categories: cs.AI cs.CY cs.LG cs.MA econ.TH
Comments: 24 pages, 7 figures
MSC-class: 68T01, 68T37, 68T42
ACM-class: I.2.0; I.2.8; I.2.11
\\ ( https://arxiv.org/abs/2402.07462 ,  36088kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07787
replaced with revised version Tue, 13 Feb 2024 15:05:37 GMT   (8194kb,D)

Title: Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment
  Analysis
Authors: Xiaowei Zhao, Yong Zhou, Xiujuan Xu, Yu Liu
Categories: cs.AI
Comments: 8 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.07787 ,  8194kb)
------------------------------------------------------------------------------
\\
arXiv:2211.05987
replaced with revised version Tue, 13 Feb 2024 02:51:03 GMT   (422kb,D)

Title: CCPrefix: Counterfactual Contrastive Prefix-Tuning for Many-Class
  Classification
Authors: Yang Li, Canran Xu, Guodong Long, Tao Shen, Chongyang Tao and Jing
  Jiang
Categories: cs.CL
Comments: has been accepted by EACL 2024
\\ ( https://arxiv.org/abs/2211.05987 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11916
replaced with revised version Mon, 12 Feb 2024 23:09:40 GMT   (1080kb,D)

Title: Large Language Models Are Latent Variable Models: Explaining and Finding
  Good Demonstrations for In-Context Learning
Authors: Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang
  Wang
Categories: cs.CL cs.AI cs.LG
Comments: code at:
  https://github.com/WANGXinyiLinda/concept-based-demonstration-selection
  Accepted to NeurIPS 2023
\\ ( https://arxiv.org/abs/2301.11916 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08467
replaced with revised version Mon, 12 Feb 2024 19:03:18 GMT   (965kb,D)

Title: Learning to Compress Prompts with Gist Tokens
Authors: Jesse Mu, Xiang Lisa Li, Noah Goodman
Categories: cs.CL
Comments: NeurIPS 2023, 26 pages. Version 3 updates preprint to camera-ready
  version and clarifies some writing in places
\\ ( https://arxiv.org/abs/2304.08467 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13691
replaced with revised version Mon, 12 Feb 2024 20:25:32 GMT   (295kb,D)

Title: Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering
Authors: Mingda Chen, Xilun Chen, Wen-tau Yih
Categories: cs.CL
Comments: EACL 2024 Camera Ready
\\ ( https://arxiv.org/abs/2305.13691 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14610
replaced with revised version Tue, 13 Feb 2024 16:18:06 GMT   (10371kb,D)

Title: This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language
  Models
Authors: Bryan Li, Samar Haider, Chris Callison-Burch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.14610 ,  10371kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08701
replaced with revised version Tue, 13 Feb 2024 18:37:25 GMT   (2940kb,D)

Title: AlpaGasus: Training A Better Alpaca with Fewer Data
Authors: Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas
  Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin
Categories: cs.CL
Comments: 32 Pages; 29 Figures; 15 Tables
\\ ( https://arxiv.org/abs/2307.08701 ,  2940kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13808
replaced with revised version Tue, 13 Feb 2024 05:27:44 GMT   (9064kb,D)

Title: Watermarking Conditional Text Generation for AI Detection: Unveiling
  Challenges and a Semantic-Aware Watermark Remedy
Authors: Yu Fu, Deyi Xiong, Yue Dong
Categories: cs.CL cs.CR
Comments: 8 pages, 6 figures (accepted to AAAI 2024)
\\ ( https://arxiv.org/abs/2307.13808 ,  9064kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01413
replaced with revised version Mon, 12 Feb 2024 20:38:23 GMT   (626kb,D)

Title: LaFiCMIL: Rethinking Large File Classification from the Perspective of
  Correlated Multiple Instance Learning
Authors: Tiezhu Sun, Weiguo Pian, Nadia Daoudi, Kevin Allix, Tegawend\'e F.
  Bissyand\'e, Jacques Klein
Categories: cs.CL cs.AI
Comments: 12 pages; manuscript revision
\\ ( https://arxiv.org/abs/2308.01413 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07462
replaced with revised version Tue, 13 Feb 2024 09:10:29 GMT   (4539kb,D)

Title: Are Large Language Model-based Evaluators the Solution to Scaling Up
  Multilingual Evaluation?
Authors: Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed
  Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram
Categories: cs.CL
Comments: Accepted to EACL 2024 findings
\\ ( https://arxiv.org/abs/2309.07462 ,  4539kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07822
replaced with revised version Tue, 13 Feb 2024 10:52:52 GMT   (2739kb,D)

Title: CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain
  Performance and Calibration
Authors: Rachneet Sachdeva, Martin Tutek, Iryna Gurevych
Categories: cs.CL
Comments: Accepted to EACL 2024 main conference
\\ ( https://arxiv.org/abs/2309.07822 ,  2739kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05404
replaced with revised version Mon, 12 Feb 2024 21:34:52 GMT   (10397kb,D)

Title: Exploring the Maze of Multilingual Modeling
Authors: Sina Bagheri Nezhad, Ameeta Agrawal
Categories: cs.CL
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2310.05404 ,  10397kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09518
replaced with revised version Tue, 13 Feb 2024 18:40:42 GMT   (2442kb,D)

Title: Instruction Tuning with Human Curriculum
Authors: Bruce W. Lee, Hyunsoo Cho, Kang Min Yoo
Categories: cs.CL cs.AI cs.LG
Comments: Under review, *ACL
\\ ( https://arxiv.org/abs/2310.09518 ,  2442kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18463
replaced with revised version Tue, 13 Feb 2024 13:57:27 GMT   (1366kb,D)

Title: PeTailor: Improving Large Language Model by Tailored Chunk Scorer in
  Biomedical Triple Extraction
Authors: Mingchen Li, M. Chen, Huixue Zhou, Halil Kilicoglu, Rui Zhang
Categories: cs.CL
Comments: this is the second preprint version
\\ ( https://arxiv.org/abs/2310.18463 ,  1366kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19791
replaced with revised version Mon, 12 Feb 2024 21:06:05 GMT   (3056kb,D)

Title: LILO: Learning Interpretable Libraries by Compressing and Documenting
  Code
Authors: Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin
  Liu, Joshua B. Tenenbaum, Jacob Andreas
Categories: cs.CL cs.AI cs.LG cs.PL
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.19791 ,  3056kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09593
replaced with revised version Tue, 13 Feb 2024 01:47:52 GMT   (548kb,D)

Title: Multi-Step Dialogue Workflow Action Prediction
Authors: Ramya Ramakrishnan, Ethan R. Elenberg, Hashan Narangodage, Ryan
  McDonald
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.09593 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07399
replaced with revised version Tue, 13 Feb 2024 03:48:00 GMT   (4902kb,D)

Title: Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis
  Framework with Prompt-Generated Rationales
Authors: Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong
  Ryong Lee, Dosik Hwang, Yongsik Sim, Beomseok Sohn, Dongha Lee, Jinyoung Yeo
Categories: cs.CL cs.AI
Comments: Accepted to AAAI 2024
\\ ( https://arxiv.org/abs/2312.07399 ,  4902kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01854
replaced with revised version Tue, 13 Feb 2024 13:22:38 GMT   (123kb,D)

Title: Multilingual Instruction Tuning With Just a Pinch of Multilinguality
Authors: Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut
  Tsarfaty, Matan Eyal
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.01854 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09002
replaced with revised version Tue, 13 Feb 2024 02:20:31 GMT   (7823kb,D)

Title: AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on
  Large Language Models
Authors: Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong
  Zhang, Yongfeng Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.09002 ,  7823kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11911
replaced with revised version Tue, 13 Feb 2024 03:18:54 GMT   (3180kb,D)

Title: Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA?
Authors: Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.11911 ,  3180kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12005
replaced with revised version Tue, 13 Feb 2024 04:18:45 GMT   (432kb,D)

Title: ALMs: Authorial Language Models for Authorship Attribution
Authors: Weihang Huang and Akira Murakami and Jack Grieve
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.12005 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12117
replaced with revised version Tue, 13 Feb 2024 07:04:30 GMT   (2243kb,D)

Title: The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large
  Language Models
Authors: Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang,
  Fred Morstatter, Jay Pujara
Categories: cs.CL
Comments: Code and datasets are available at
  https://github.com/kahrabian/mllm-nvar
\\ ( https://arxiv.org/abs/2401.12117 ,  2243kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15042
replaced with revised version Tue, 13 Feb 2024 13:24:49 GMT   (8091kb,D)

Title: PROXYQA: An Alternative Framework for Evaluating Long-Form Text
  Generation with Large Language Models
Authors: Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng,
  Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.15042 ,  8091kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15170
replaced with revised version Mon, 12 Feb 2024 23:04:10 GMT   (820kb,D)

Title: Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning
  Matches Human Performance in Some Hermeneutic Tasks
Authors: Zackary Okun Dunivin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.15170 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00559
replaced with revised version Tue, 13 Feb 2024 09:37:03 GMT   (8386kb,D)

Title: A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for
  Verifiers of Reasoning Chains
Authors: Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or
  Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva
Categories: cs.CL
Comments: Dataset at https://huggingface.co/datasets/google/reveal
\\ ( https://arxiv.org/abs/2402.00559 ,  8386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00746
replaced with revised version Tue, 13 Feb 2024 02:42:35 GMT   (951kb,D)

Title: Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
Authors: Mingyu Jin, Qinkai Yu, Chong Zhang, Dong Shu, Suiyuan Zhu, Mengnan Du,
  Yongfeng Zhang, Yanda Meng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00746 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00786
replaced with revised version Tue, 13 Feb 2024 17:12:26 GMT   (4612kb,D)

Title: CroissantLLM: A Truly Bilingual French-English Language Model
Authors: Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, Ant\'onio Loison,
  Duarte M. Alves, Caio Corro, Nicolas Boizard, Jo\~ao Alves, Ricardo Rei,
  Pedro H. Martins, Antoni Bigata Casademunt, Fran\c{c}ois Yvon, Andr\'e F.T.
  Martins, Gautier Viaud, C\'eline Hudelot, Pierre Colombo
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.00786 ,  4612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01155
replaced with revised version Tue, 13 Feb 2024 09:11:01 GMT   (3010kb,D)

Title: CABINET: Content Relevance based Noise Reduction for Table Question
  Answering
Authors: Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman
  Kumar, Balaji Krishnamurthy
Categories: cs.CL
Comments: Accepted at ICLR 2024 (spotlight)
\\ ( https://arxiv.org/abs/2402.01155 ,  3010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06126
replaced with revised version Tue, 13 Feb 2024 16:38:03 GMT   (1329kb,D)

Title: Learn To be Efficient: Build Structured Sparsity in Large Language
  Models
Authors: Haizhong Zheng, Xiaoyan Bai, Beidi Chen, Fan Lai, Atul Prakash
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.06126 ,  1329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07282
replaced with revised version Tue, 13 Feb 2024 14:21:02 GMT   (6082kb,D)

Title: How do Large Language Models Navigate Conflicts between Honesty and
  Helpfulness?
Authors: Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.07282 ,  6082kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07431
replaced with revised version Tue, 13 Feb 2024 06:34:48 GMT   (4615kb,D)

Title: SALAD: Smart AI Language Assistant Daily
Authors: Ragib Amin Nihal, Tran Dong Huu Quoc, Lin Zirui, Xu Yimimg, Liu
  Haoran, An Zhaoyi, and Kyou Ma
Categories: cs.CL cs.CY
\\ ( https://arxiv.org/abs/2402.07431 ,  4615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07446
replaced with revised version Tue, 13 Feb 2024 02:47:00 GMT   (9749kb,D)

Title: Quality Does Matter: A Detailed Look at the Quality and Utility of
  Web-Mined Parallel Corpora
Authors: Surangika Ranathunga, Nisansa de Silva, Menan Velayuthan, Aloka
  Fernando, Charitha Rathnayake
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.07446 ,  9749kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07896
replaced with revised version Tue, 13 Feb 2024 18:44:11 GMT   (3135kb,D)

Title: Suppressing Pink Elephants with Direct Principle Feedback
Authors: Louis Castricato, Nathan Lile, Suraj Anand, Hailey Schoelkopf,
  Siddharth Verma, Stella Biderman
Categories: cs.CL
Comments: 8 pages, 6 figures
\\ ( https://arxiv.org/abs/2402.07896 ,  3135kb)
------------------------------------------------------------------------------
\\
arXiv:2002.03339
replaced with revised version Tue, 13 Feb 2024 05:09:49 GMT   (390kb)

Title: Input Validation for Neural Networks via Runtime Local Robustness
  Verification
Authors: Jiangchao Liu, Liqian Chen, Antoine Mine and Ji Wang
Categories: cs.LG stat.ML
ACM-class: F.3.1
\\ ( https://arxiv.org/abs/2002.03339 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2109.08792
replaced with revised version Mon, 12 Feb 2024 20:17:04 GMT   (1782kb,D)

Title: Learning to be Fair: A Consequentialist Approach to Equitable
  Decision-Making
Authors: Alex Chohlas-Wood, Madison Coots, Henry Zhu, Emma Brunskill, Sharad
  Goel
Categories: cs.LG cs.CY
\\ ( https://arxiv.org/abs/2109.08792 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2205.13340
replaced with revised version Tue, 13 Feb 2024 16:27:35 GMT   (456kb,D)

Title: Deep Active Learning with Noise Stability
Authors: Xingjian Li, Pengkun Yang, Yangcheng Gu, Xueying Zhan, Tianyang Wang,
  Min Xu, Chengzhong Xu
Categories: cs.LG
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2205.13340 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2301.03180
replaced with revised version Tue, 13 Feb 2024 05:11:17 GMT   (610kb,D)

Title: Subset verification and search algorithms for causal DAGs
Authors: Davin Choo, Kirankumar Shiragur
Categories: cs.LG cs.DS stat.ML
Comments: Accepted into AISTATS 2023
  (https://aistats.org/aistats2023/accepted.html)
\\ ( https://arxiv.org/abs/2301.03180 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11989
replaced with revised version Tue, 13 Feb 2024 14:15:01 GMT   (1496kb,D)

Title: Practical Differentially Private Hyperparameter Tuning with Subsampling
Authors: Antti Koskela and Tejas Kulkarni
Categories: cs.LG cs.CR
Journal-ref: NeurIPS 2023
\\ ( https://arxiv.org/abs/2301.11989 ,  1496kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00284
replaced with revised version Mon, 12 Feb 2024 19:35:55 GMT   (1203kb,D)

Title: Selective Uncertainty Propagation in Offline RL
Authors: Sanath Kumar Krishnamurthy, Shrey Modi, Tanmay Gangwani, Sumeet
  Katariya, Branislav Kveton, Anshuka Rangi
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2302.00284 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10295
replaced with revised version Mon, 12 Feb 2024 19:43:38 GMT   (24198kb,D)

Title: Correlation Clustering with Active Learning of Pairwise Similarities
Authors: Linus Aronsson, Morteza Haghir Chehreghani
Categories: cs.LG stat.ML
Journal-ref: Transactions on Machine Learning Research (TMLR) (2024).
  https://openreview.net/forum?id=Ryf1TVCjBz
\\ ( https://arxiv.org/abs/2302.10295 ,  24198kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06530
replaced with revised version Tue, 13 Feb 2024 07:32:47 GMT   (9183kb,D)

Title: Making Batch Normalization Great in Federated Deep Learning
Authors: Jike Zhong, Hong-You Chen, Wei-Lun Chao
Categories: cs.LG cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2303.06530 ,  9183kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08944
replaced with revised version Tue, 13 Feb 2024 03:24:23 GMT   (28kb)

Title: Agnostic Multi-Robust Learning Using ERM
Authors: Saba Ahmadi, Avrim Blum, Omar Montasser, Kevin Stangl
Categories: cs.LG cs.CR cs.CV
\\ ( https://arxiv.org/abs/2303.08944 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08242
replaced with revised version Tue, 13 Feb 2024 14:14:14 GMT   (3531kb,D)

Title: The Deep Latent Position Topic Model for Clustering and Representation
  of Networks with Textual Edges
Authors: R\'emi Boutin, Pierre Latouche, Charles Bouveyron
Categories: cs.LG cs.CL cs.SI stat.ME
Comments: 29 pages including the appendix, 13 figures, 6 tables, journal paper
\\ ( https://arxiv.org/abs/2304.08242 ,  3531kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12076
replaced with revised version Tue, 13 Feb 2024 08:09:49 GMT   (4248kb,D)

Title: Customized Load Profiles Synthesis for Electricity Customers Based on
  Conditional Diffusion Models
Authors: Zhenyi Wang, Hongcai Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2304.12076 ,  4248kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06741
replaced with revised version Mon, 12 Feb 2024 19:10:19 GMT   (1156kb,D)

Title: IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers
Authors: Jingge Xiao, Leonie Basso, Wolfgang Nejdl, Niloy Ganguly, Sandipan
  Sikdar
Categories: cs.LG cs.AI
Comments: AAAI 2024 Camera-Ready Version
\\ ( https://arxiv.org/abs/2305.06741 ,  1156kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14120
replaced with revised version Mon, 12 Feb 2024 20:40:27 GMT   (5938kb,D)

Title: Learning relevant contextual variables within Bayesian Optimization
Authors: Julien Martinelli, Ayush Bharti, Armi Tiihonen, S.T. John, Louis
  Filstroff, Sabina J. Sloman, Patrick Rinke and Samuel Kaski
Categories: cs.LG stat.ML
Comments: Preprint. Under review
\\ ( https://arxiv.org/abs/2305.14120 ,  5938kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15984
replaced with revised version Tue, 13 Feb 2024 00:38:13 GMT   (856kb,D)

Title: Dynamic Inter-treatment Information Sharing for Individualized Treatment
  Effects Estimation
Authors: Vinod Kumar Chauhan, Jiandong Zhou, Ghadeer Ghosheh, Soheila Molaei
  and David A. Clifton
Categories: cs.LG stat.ME
Comments: accepted to The 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2305.15984 ,  856kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17148
replaced with revised version Tue, 13 Feb 2024 06:41:01 GMT   (45kb)

Title: Differentially Private Low-dimensional Synthetic Data from
  High-dimensional Datasets
Authors: Yiyun He, Thomas Strohmer, Roman Vershynin, Yizhe Zhu
Categories: cs.LG cs.CR cs.DS math.PR math.ST stat.TH
Comments: 22 pages
\\ ( https://arxiv.org/abs/2305.17148 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17301
replaced with revised version Tue, 13 Feb 2024 09:23:48 GMT   (59kb)

Title: Stability-penalty-adaptive follow-the-regularized-leader: Sparsity,
  game-dependency, and best-of-both-worlds
Authors: Taira Tsuchiya, Shinji Ito, Junya Honda
Categories: cs.LG stat.ML
Comments: Published version in Advances in Neural Information Processing
  Systems 36 (NeurIPS 2023), 32 pages
\\ ( https://arxiv.org/abs/2305.17301 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01896
replaced with revised version Tue, 13 Feb 2024 17:32:20 GMT   (6527kb,D)

Title: Learning to Stabilize Online Reinforcement Learning in Unbounded State
  Spaces
Authors: Brahma S. Pavse, Matthew Zurek, Yudong Chen, Qiaomin Xie, Josiah P.
  Hanna
Categories: cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2306.01896 ,  6527kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03589
replaced with revised version Mon, 12 Feb 2024 19:24:06 GMT   (965kb,D)

Title: How does over-squashing affect the power of GNNs?
Authors: Francesco Di Giovanni, T. Konstantin Rusch, Michael M. Bronstein,
  Andreea Deac, Marc Lackenby, Siddhartha Mishra, Petar Veli\v{c}kovi\'c
Categories: cs.LG stat.ML
Comments: 36 pages; Published in Transactions on Machine Learning Research
  (TMLR)
\\ ( https://arxiv.org/abs/2306.03589 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04848
replaced with revised version Tue, 13 Feb 2024 16:08:41 GMT   (4193kb,D)

Title: Interpreting and Improving Diffusion Models Using the Euclidean Distance
  Function
Authors: Frank Permenter and Chenyang Yuan
Categories: cs.LG cs.CV math.OC stat.ML
Comments: 23 pages, 9 figures, 2 tables
\\ ( https://arxiv.org/abs/2306.04848 ,  4193kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06375
replaced with revised version Tue, 13 Feb 2024 11:37:50 GMT   (1233kb)

Title: Optimized Gradient Tracking for Decentralized Online Learning
Authors: Shivangi Dubey Sharma (1) and Ketan Rajawat (1), ((1) Indian Institute
  of Technology Kanpur)
Categories: cs.LG eess.SP math.OC
Comments: 30 pages, 6 Figures
\\ ( https://arxiv.org/abs/2306.06375 ,  1233kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09686
replaced with revised version Mon, 12 Feb 2024 23:01:27 GMT   (2259kb,D)

Title: Collapsed Inference for Bayesian Deep Learning
Authors: Zhe Zeng, Guy Van den Broeck
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2306.09686 ,  2259kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00668
replaced with revised version Tue, 13 Feb 2024 05:13:26 GMT   (33083kb,D)

Title: Active Sensing with Predictive Coding and Uncertainty Minimization
Authors: Abdelrahman Sharafeldin, Nabil Imam, Hannah Choi
Categories: cs.LG cs.NE
Comments: 31 pages, LaTeX; Fixed a small issue with section numbering; Added
  more experiments, baselines, and relevant references
\\ ( https://arxiv.org/abs/2307.00668 ,  33083kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06887
replaced with revised version Tue, 13 Feb 2024 04:00:47 GMT   (1548kb,D)

Title: Provable Multi-Task Representation Learning by Two-Layer ReLU Neural
  Networks
Authors: Liam Collins, Hamed Hassani, Mahdi Soltanolkotabi, Aryan Mokhtari,
  Sanjay Shakkottai
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.06887 ,  1548kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08433
replaced with revised version Tue, 13 Feb 2024 11:20:46 GMT   (214kb,D)

Title: From random-walks to graph-sprints: a low-latency node embedding
  framework on continuous-time dynamic graphs
Authors: Ahmad Naser Eddin, Jacopo Bono, David Apar\'icio, Hugo Ferreira,
  Jo\~ao Ascens\~ao, Pedro Ribeiro, Pedro Bizarro
Categories: cs.LG
Comments: 9 pages, 5 figures, 7 tables
\\ ( https://arxiv.org/abs/2307.08433 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11607
replaced with revised version Tue, 13 Feb 2024 13:21:06 GMT   (686kb,D)

Title: Finding Optimal Diverse Feature Sets with Alternative Feature Selection
Authors: Jakob Bach
Categories: cs.LG
Comments: Changes from v1 to v2: Experiments for heuristic search methods
  added; various minor changes and additions to synchronize with journal
  version (currently still under review)
\\ ( https://arxiv.org/abs/2307.11607 ,  686kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12367
replaced with revised version Mon, 12 Feb 2024 21:05:29 GMT   (8581kb,D)

Title: SafeAR: Safe Algorithmic Recourse by Risk-Aware Policies
Authors: Haochen Wu, Shubham Sharma, Sunandita Patra, Sriram Gopalakrishnan
Categories: cs.LG cs.AI
Comments: Accepted to AAAI 2024 main track with oral presentation; Supplemental
  material appended to main paper
\\ ( https://arxiv.org/abs/2308.12367 ,  8581kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17016
replaced with revised version Tue, 13 Feb 2024 10:03:10 GMT   (30kb,D)

Title: Efficient Agnostic Learning with Average Smoothness
Authors: Steve Hanneke, Aryeh Kontorovich, Guy Kornowski
Categories: cs.LG math.ST stat.ML stat.TH
Comments: ALT 2024 camera ready version. arXiv admin note: text overlap with
  arXiv:2302.06005
\\ ( https://arxiv.org/abs/2309.17016 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07048
replaced with revised version Tue, 13 Feb 2024 00:09:59 GMT   (694kb,D)

Title: FedMFS: Federated Multimodal Fusion Learning with Selective Modality
  Communication
Authors: Liangqi Yuan and Dong-Jun Han and Vishnu Pandi Chellapandi and
  Stanislaw H. \.Zak and Christopher G. Brinton
Categories: cs.LG cs.DC cs.NI
Comments: ICC 2024
\\ ( https://arxiv.org/abs/2310.07048 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08833
replaced with revised version Mon, 12 Feb 2024 19:06:53 GMT   (583kb,D)

Title: Optimal Sample Complexity for Average Reward Markov Decision Processes
Authors: Shengbo Wang, Jose Blanchet, and Peter Glynn
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2310.08833 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09877
replaced with revised version Tue, 13 Feb 2024 09:38:50 GMT   (275kb,D)

Title: Statistical inference using machine learning and classical techniques
  based on accumulated local effects (ALE)
Authors: Chitu Okoli
Categories: cs.LG cs.AI
Comments: 1. P-values have been added to the analysis. 2. Neural network
  demonstration replaced with random forest. 3. Effects plot shows NALED band
  based on p-values
\\ ( https://arxiv.org/abs/2310.09877 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11009
replaced with revised version Tue, 13 Feb 2024 06:41:34 GMT   (1115kb,D)

Title: LPFormer: An Adaptive Graph Transformer for Link Prediction
Authors: Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, Jiliang Tang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.11009 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13576
replaced with revised version Tue, 13 Feb 2024 16:18:04 GMT   (355kb,D)

Title: Tree Search in DAG Space with Model-based Reinforcement Learning for
  Causal Discovery
Authors: Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.13576 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15903
replaced with revised version Tue, 13 Feb 2024 04:16:29 GMT   (3924kb)

Title: Neural Collapse in Multi-label Learning with Pick-all-label Loss
Authors: Pengyu Li, Yutong Wang, Xiao Li, Qing Qu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.15903 ,  3924kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17022
replaced with revised version Tue, 13 Feb 2024 18:10:20 GMT   (371kb,D)

Title: Controlled Decoding from Language Models
Authors: Sidharth Mudgal and Jong Lee and Harish Ganapathy and YaGuang Li and
  Tao Wang and Yanping Huang and Zhifeng Chen and Heng-Tze Cheng and Michael
  Collins and Trevor Strohman and Jilin Chen and Alex Beutel and Ahmad Beirami
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.17022 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03415
replaced with revised version Tue, 13 Feb 2024 13:10:31 GMT   (3151kb,D)

Title: PowerFlowNet: Power Flow Approximation Using Message Passing Graph
  Neural Networks
Authors: Nan Lin, Stavros Orfanoudakis, Nathan Ordonez Cardenas, Juan S.
  Giraldo, Pedro P. Vergara
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 10 pages, 7 figures
\\ ( https://arxiv.org/abs/2311.03415 ,  3151kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06840
replaced with revised version Tue, 13 Feb 2024 18:53:58 GMT   (63kb)

Title: Omitted Labels in Causality: A Study of Paradoxes
Authors: Bijan Mazaheri, Siddharth Jain, Matthew Cook, Jehoshua Bruck
Categories: cs.LG cs.AI cs.IT cs.SI math.IT stat.ME
\\ ( https://arxiv.org/abs/2311.06840 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15578
replaced with revised version Tue, 13 Feb 2024 09:38:44 GMT   (1732kb,D)

Title: Experimental Analysis of Large-scale Learnable Vector Storage
  Compression
Authors: Hailin Zhang, Penghao Zhao, Xupeng Miao, Yingxia Shao, Zirui Liu, Tong
  Yang, Bin Cui
Categories: cs.LG cs.DB cs.IR
\\ ( https://arxiv.org/abs/2311.15578 ,  1732kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03096
replaced with revised version Tue, 13 Feb 2024 06:26:22 GMT   (881kb,D)

Title: What Causes Polysemanticity? An Alternative Origin Story of Mixed
  Selectivity from Incidental Causes
Authors: Victor Lecomte, Kushal Thaman, Rylan Schaeffer, Naomi Bashkansky,
  Trevor Chow, Sanmi Koyejo
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2312.03096 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06441
replaced with revised version Tue, 13 Feb 2024 13:36:42 GMT   (3957kb,D)

Title: Revisiting Graph-Based Fraud Detection in Sight of Heterophily and
  Spectrum
Authors: Fan Xu, Nan Wang, Hao Wu, Xuezhi Wen, Xibin Zhao, Hai Wan
Categories: cs.LG cs.AI cs.SI
\\ ( https://arxiv.org/abs/2312.06441 ,  3957kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09533
replaced with revised version Tue, 13 Feb 2024 01:53:13 GMT   (3075kb,D)

Title: Adversarial Robustness on Image Classification with $k$-means
Authors: Rollin Omari, Junae Kim and Paul Montague
Categories: cs.LG cs.CR cs.CV cs.NE
Comments: 6 pages, 3 figures, 2 equations, 1 algorithm
DOI: 10.1109/ACCESS.2024.3365517
\\ ( https://arxiv.org/abs/2312.09533 ,  3075kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15230
replaced with revised version Tue, 13 Feb 2024 13:19:34 GMT   (6632kb,D)

Title: PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs
Authors: Max Zimmer, Megi Andoni, Christoph Spiegel, Sebastian Pokutta
Categories: cs.LG cs.AI
Comments: 27 pages, 4 figures, 15 tables
\\ ( https://arxiv.org/abs/2312.15230 ,  6632kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17430
replaced with revised version Tue, 13 Feb 2024 06:13:00 GMT   (1954kb,D)

Title: LEFL: Low Entropy Client Sampling in Federated Learning
Authors: Waqwoya Abebe, Pablo Munoz, Ali Jannesari
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.17430 ,  1954kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01335
replaced with revised version Mon, 12 Feb 2024 22:22:37 GMT   (833kb,D)

Title: Self-Play Fine-Tuning Converts Weak Language Models to Strong Language
  Models
Authors: Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and
  Quanquan Gu
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: 28 pages, 6 figures, 6 tables
\\ ( https://arxiv.org/abs/2401.01335 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04301
replaced with revised version Tue, 13 Feb 2024 15:48:46 GMT   (11402kb,D)

Title: Setting the Record Straight on Transformer Oversmoothing
Authors: Gb\`etondji J-S Dovonon, Michael M. Bronstein, Matt J. Kusner
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.04301 ,  11402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04305
replaced with revised version Mon, 12 Feb 2024 22:54:30 GMT   (28570kb,D)

Title: Advancing Deep Active Learning & Data Subset Selection: Unifying
  Principles with Information-Theory Intuitions
Authors: Andreas Kirsch
Categories: cs.LG cs.IT math.IT
Comments: PhD thesis
\\ ( https://arxiv.org/abs/2401.04305 ,  28570kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10386
replaced with revised version Tue, 13 Feb 2024 01:53:14 GMT   (460kb,D)

Title: Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest
  Machine Learning
Authors: Zaina Abu Hweij, Florence Liang, Sophie Zhang
Categories: cs.LG eess.SP physics.med-ph
\\ ( https://arxiv.org/abs/2401.10386 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15292
replaced with revised version Tue, 13 Feb 2024 04:58:26 GMT   (1270kb,D)

Title: Adaptive Block Sparse Regularization under Arbitrary Linear Transform
Authors: Takanobu Furuhashi, Hidekata Hontani, Tatsuya Yokota
Categories: cs.LG eess.SP
Comments: 5 pages, 4 figures
\\ ( https://arxiv.org/abs/2401.15292 ,  1270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00899
replaced with revised version Tue, 13 Feb 2024 15:53:06 GMT   (529kb,D)

Title: Weakly Supervised Learners for Correction of AI Errors with Provable
  Performance Guarantees
Authors: Ivan Y. Tyukin, Tatiana Tyukina, Daniel van Helden, Zedong Zheng,
  Evgeny M. Mirkes, Oliver J. Sutton, Qinghua Zhou, Alexander N. Gorban,
  Penelope Allison
Categories: cs.LG cs.AI stat.ML
MSC-class: 68T05, 68T37
\\ ( https://arxiv.org/abs/2402.00899 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01632
replaced with revised version Tue, 13 Feb 2024 17:27:45 GMT   (1882kb,D)

Title: Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown
  Hyperparameters Of Any Type
Authors: Juliusz Ziomek, Masaki Adachi, Michael A. Osborne
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.01632 ,  1882kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02006
replaced with revised version Tue, 13 Feb 2024 01:59:28 GMT   (1739kb,D)

Title: PresAIse, A Prescriptive AI Solution for Enterprises
Authors: Wei Sun, Scott McFaddin, Linh Ha Tran, Shivaram Subramanian, Kristjan
  Greenewald, Yeshi Tenzin, Zack Xue, Youssef Drissi, Markus Ettl
Categories: cs.LG
Comments: 14 pages
\\ ( https://arxiv.org/abs/2402.02006 ,  1739kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02316
replaced with revised version Tue, 13 Feb 2024 08:23:18 GMT   (262kb,D)

Title: Your Diffusion Model is Secretly a Certifiably Robust Classifier
Authors: Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang,
  Hang Su, Jun Zhu
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2402.02316 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02651
replaced with revised version Tue, 13 Feb 2024 17:51:03 GMT   (9471kb,D)

Title: Vision-Language Models Provide Promptable Representations for
  Reinforcement Learning
Authors: William Chen and Oier Mees and Aviral Kumar and Sergey Levine
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2402.02651 ,  9471kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03496
replaced with revised version Tue, 13 Feb 2024 14:33:15 GMT   (646kb,D)

Title: Can We Remove the Square-Root in Adaptive Gradient Methods? A
  Second-Order Perspective
Authors: Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner,
  Alireza Makhzani
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2402.03496 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04390
replaced with revised version Mon, 12 Feb 2024 20:40:44 GMT   (958kb)

Title: Densely Multiplied Physics Informed Neural Networks
Authors: Feilong Jiang, Xiaonan Hou, Min Xia
Categories: cs.LG cs.AI
Comments: 15 pages, 9 figures
\\ ( https://arxiv.org/abs/2402.04390 ,  958kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05098
replaced with revised version Tue, 13 Feb 2024 16:32:09 GMT   (9161kb,D)

Title: On diffusion models for amortized inference: Benchmarking and improving
  stochastic control and sampling
Authors: Marcin Sendera, Minsu Kim, Sarthak Mittal, Pablo Lemos, Luca Scimeca,
  Jarrid Rector-Brooks, Alexandre Adam, Yoshua Bengio, Nikolay Malkin
Categories: cs.LG stat.ML
Comments: 21 pages; code: https://github.com/GFNOrg/gfn-diffusion
\\ ( https://arxiv.org/abs/2402.05098 ,  9161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05643
replaced with revised version Tue, 13 Feb 2024 15:38:11 GMT   (2537kb,D)

Title: Improving Token-Based World Models with Parallel Observation Prediction
Authors: Lior Cohen, Kaixin Wang, Bingyi Kang, Shie Mannor
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.05643 ,  2537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05785
replaced with revised version Tue, 13 Feb 2024 07:36:40 GMT   (184kb,D)

Title: Limits of Transformer Language Models on Learning Algorithmic
  Compositions
Authors: Jonathan Thomm, Aleksandar Terzic, Geethan Karunaratne, Giacomo
  Camposampiero, Bernhard Sch\"olkopf, Abbas Rahimi
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.05785 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06864
replaced with revised version Tue, 13 Feb 2024 06:14:21 GMT   (236kb,D)

Title: Discriminative Adversarial Unlearning
Authors: Rohan Sharma, Shijie Zhou, Kaiyi Ji and Changyou Chen
Categories: cs.LG cs.AI
Comments: 13 pages including references, 2 tables, 2 figures and 1 algorithm
\\ ( https://arxiv.org/abs/2402.06864 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06974
replaced with revised version Tue, 13 Feb 2024 07:09:06 GMT   (9429kb,D)

Title: Non-linear Fusion in Federated Learning: A Hypernetwork Approach to
  Federated Domain Generalization
Authors: Marc Bartholet, Taehyeon Kim, Ami Beuret, Se-Young Yun, Joachim M.
  Buhmann
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.06974 ,  9429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07107
replaced with revised version Tue, 13 Feb 2024 05:14:34 GMT   (5895kb,D)

Title: Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential
  Reinforcement Learning
Authors: Alex Christopher Stutts, Danilo Erricolo, Theja Tulabandhula, Amit
  Ranjan Trivedi
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.07107 ,  5895kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07211
replaced with revised version Tue, 13 Feb 2024 07:14:24 GMT   (748kb,D)

Title: Towards Fast Stochastic Sampling in Diffusion Generative Models
Authors: Kushagra Pandey, Maja Rudolph, Stephan Mandt
Categories: cs.LG stat.ML
Comments: Accepted in the NeurIPS'23 Workshop on Diffusion Models. Full version
  of this work can be found at arXiv:2310.07894
\\ ( https://arxiv.org/abs/2402.07211 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07309
replaced with revised version Tue, 13 Feb 2024 17:24:20 GMT   (8141kb,D)

Title: HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node
  Classification on Text-Attributed Hypergraphs
Authors: Adri\'an Bazaga and Pietro Li\`o and Gos Micklem
Categories: cs.LG cs.CL stat.ML
Comments: 11 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.07309 ,  8141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07480
replaced with revised version Tue, 13 Feb 2024 09:09:41 GMT   (1252kb,D)

Title: Topological safeguard for evasion attack interpreting the neural
  networks' behavior
Authors: Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, I\~nigo Mendialdua,
  Raul Orduna-Urrutia
Categories: cs.LG
Journal-ref: Pattern Recognition, Volume 147, 2024, 110130, ISSN 0031-3203
DOI: 10.1016/j.patcog.2023.110130
\\ ( https://arxiv.org/abs/2402.07480 ,  1252kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12138
replaced with revised version Tue, 13 Feb 2024 04:56:48 GMT   (2752kb,D)

Title: LMs: Understanding Code Syntax and Semantics for Code Analysis
Authors: Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen
  Zhang, Liming Nie, Li Li, Yang Liu
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2305.12138 ,  2752kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06123
replaced with revised version Tue, 13 Feb 2024 14:36:33 GMT   (3405kb,D)

Title: Adversarial attacks and defenses in explainable artificial intelligence:
  A survey
Authors: Hubert Baniecki and Przemyslaw Biecek
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: Accepted by Information Fusion
\\ ( https://arxiv.org/abs/2306.06123 ,  3405kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08533 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 15:42:15 GMT   (1019kb)

Title: Nonlinear Processing with Linear Optics
Authors: Mustafa Yildirim, Niyazi Ulas Dinc, Ilker Oguz, Demetri Psaltis and
  Christophe Moser
Categories: physics.optics cs.AI cs.ET cs.LG
Comments: 15 pages and 5 figures
\\ ( https://arxiv.org/abs/2307.08533 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01592 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 19:19:48 GMT   (833kb,D)

Title: Les Houches Lectures on Deep Learning at Large & Infinite Width
Authors: Yasaman Bahri, Boris Hanin, Antonin Brossollet, Vittorio Erba,
  Christian Keup, Rosalba Pacelli, James B. Simon
Categories: stat.ML cs.AI cs.LG hep-th math.PR
Comments: These are notes from lectures delivered by Yasaman Bahri and Boris
  Hanin at the 2022 Les Houches Summer School on Statistics Physics and Machine
  Learning and a first version of them were transcribed by Antonin Brossollet,
  Vittorio Erba, Christian Keup, Rosalba Pacelli, James B. Simon
\\ ( https://arxiv.org/abs/2309.01592 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09342
replaced with revised version Mon, 12 Feb 2024 20:25:31 GMT   (811kb,D)

Title: Ranking LLM-Generated Loop Invariants for Program Verification
Authors: Saikat Chakraborty, Shuvendu K. Lahiri, Sarah Fakhoury, Madanlal
  Musuvathi, Akash Lal, Aseem Rastogi, Aditya Senthilnathan, Rahul Sharma,
  Nikhil Swamy
Categories: cs.PL cs.AI cs.CL cs.SE
Comments: Findings of The 2023 Conference on Empirical Methods in Natural
  Language Processing (EMNLP-findings 2023)
\\ ( https://arxiv.org/abs/2310.09342 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09401
replaced with revised version Tue, 13 Feb 2024 10:20:35 GMT   (1318kb,D)

Title: CROWN: A Novel Approach to Comprehending Users' Preferences for Accurate
  Personalized News Recommendation
Authors: Yunyong Ko, Seongeun Ryu, Sang-Wook Kim
Categories: cs.IR cs.AI
Comments: 10 pages, 6 figures, 8 tables
\\ ( https://arxiv.org/abs/2310.09401 ,  1318kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10622
replaced with revised version Tue, 13 Feb 2024 15:18:29 GMT   (992kb,D)

Title: Unit Test Generation using Generative AI : A Comparative Performance
  Analysis of Autogeneration Tools
Authors: Shreya Bhatia, Tarushi Gandhi, Dhruv Kumar, Pankaj Jalote
Categories: cs.SE cs.AI
Comments: Accepted to LLM4Code @ ICSE 2024
\\ ( https://arxiv.org/abs/2312.10622 ,  992kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01269
replaced with revised version Tue, 13 Feb 2024 17:56:24 GMT   (1608kb,D)

Title: LLbezpeky: Leveraging Large Language Models for Vulnerability Detection
Authors: Noble Saji Mathews, Yelizaveta Brus, Yousra Aafer, Meiyappan Nagappan,
  Shane McIntosh
Categories: cs.CR cs.AI cs.SE
Comments: This project report was presented as a part of the course CS858 at
  the University of Waterloo under the supervision of Prof. Yousra Aafer
\\ ( https://arxiv.org/abs/2401.01269 ,  1608kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12258
replaced with revised version Tue, 13 Feb 2024 12:32:35 GMT   (1402kb,D)

Title: Emergent Dominance Hierarchies in Reinforcement Learning Agents
Authors: Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
Categories: cs.MA cs.AI cs.GT cs.LG
\\ ( https://arxiv.org/abs/2401.12258 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16013
replaced with revised version Tue, 13 Feb 2024 04:40:46 GMT   (15160kb,D)

Title: SERL: A Software Suite for Sample-Efficient Robotic Reinforcement
  Learning
Authors: Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit
  Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, Sergey Levine
Categories: cs.RO cs.AI
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2401.16013 ,  15160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03781 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 04:28:48 GMT   (7846kb,D)

Title: MolTC: Towards Molecular Relational Modeling In Language Models
Authors: Junfeng Fang, Shuai Zhang, Chang Wu, Zhengyi Yang, Zhiyuan Liu, Sihang
  Li, Kun Wang, Wenjie Du and Xiang Wang
Categories: q-bio.QM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03781 ,  7846kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06938
replaced with revised version Tue, 13 Feb 2024 15:58:40 GMT   (1971kb,D)

Title: Efficient Resource Scheduling for Distributed Infrastructures Using
  Negotiation Capabilities
Authors: Junjie Chu and Prashant Singh and Salman Toor
Categories: cs.DC cs.AI cs.LG
Comments: Accepted in IEEE CLOUD 2023. 13 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.06938 ,  1971kb)
------------------------------------------------------------------------------
\\
arXiv:2208.03886 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 18:33:54 GMT   (45kb)

Title: What can we know about that which we cannot even imagine?
Authors: David H. Wolpert
Categories: physics.hist-ph cs.CL
Comments: 38 pages, 9 pages are references
\\ ( https://arxiv.org/abs/2208.03886 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17143
replaced with revised version Tue, 13 Feb 2024 05:31:19 GMT   (220kb)

Title: Techniques for supercharging academic writing with generative AI
Authors: Zhicheng Lin
Categories: cs.CY cs.CL
Comments: 14 pages, 2 figures, 1 table, 1 box
\\ ( https://arxiv.org/abs/2310.17143 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14887
replaced with revised version Mon, 12 Feb 2024 22:02:04 GMT   (98kb,D)

Title: The Power of Noise: Redefining Retrieval for RAG Systems
Authors: Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone
  Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio
  Silvestri
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2401.14887 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2001.07026 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 08:09:00 GMT   (1648kb)

Title: Leveraging tensor kernels to reduce objective function mismatch in deep
  clustering
Authors: Daniel J. Trosten, Sigurd L{\o}kse, Robert Jenssen, Michael
  Kampffmeyer
Categories: stat.ML cs.CV cs.LG
DOI: 10.1016/j.patcog.2023.110229
\\ ( https://arxiv.org/abs/2001.07026 ,  1648kb)
------------------------------------------------------------------------------
\\
arXiv:2007.15788 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 11:04:26 GMT   (2559kb,D)

Title: Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision
  Making
Authors: Jie Zhou, Botao Hao, Zheng Wen, Jingfei Zhang, Will Wei Sun
Categories: stat.ML cs.LG
Comments: Accepted by Journal of the American Statistical Association
\\ ( https://arxiv.org/abs/2007.15788 ,  2559kb)
------------------------------------------------------------------------------
\\
arXiv:2112.08217 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 15:27:28 GMT   (18195kb,D)

Title: Probabilistic Forecasting with Generative Networks via Scoring Rule
  Minimization
Authors: Lorenzo Pacchiardi, Rilwan Adewoyin, Peter Dueben, Ritabrata Dutta
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2112.08217 ,  18195kb)
------------------------------------------------------------------------------
\\
arXiv:2206.09256
replaced with revised version Mon, 12 Feb 2024 20:13:26 GMT   (641kb,D)

Title: Multistream Gaze Estimation with Anatomical Eye Region Isolation by
  Synthetic to Real Transfer Learning
Authors: Zunayed Mahmud, Paul Hungler, Ali Etemad
Categories: cs.CV cs.LG
Comments: 15 pages, 7 figures, 14 tables. This work has been accepted to the
  IEEE Transactions on Artificial Intelligence $\copyright$ 2024 IEEE. Personal
  use of this material is permitted. Permission from IEEE must be obtained for
  all other uses
\\ ( https://arxiv.org/abs/2206.09256 ,  641kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13139 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 17:18:16 GMT   (144kb,D)

Title: A Novel Framework for Policy Mirror Descent with General
  Parameterization and Linear Convergence
Authors: Carlo Alfano, Rui Yuan, Patrick Rebeschini
Categories: stat.ML cs.LG math.OC math.ST stat.TH
Comments: Post-conference updates
\\ ( https://arxiv.org/abs/2301.13139 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11508
replaced with revised version Tue, 13 Feb 2024 11:33:18 GMT   (8028kb,D)

Title: nSimplex Zen: A Novel Dimensionality Reduction for Euclidean and Hilbert
  Spaces
Authors: Richard Connor, Lucia Vadicamo
Categories: cs.IR cs.LG
DOI: 10.1145/3647642
\\ ( https://arxiv.org/abs/2302.11508 ,  8028kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02566 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 20:13:17 GMT   (8764kb,D)

Title: MFAI: A Scalable Bayesian Matrix Factorization Approach to Leveraging
  Auxiliary Information
Authors: Zhiwei Wang, Fa Zhang, Cong Zheng, Xianghong Hu, Mingxuan Cai, Can
  Yang
Categories: stat.ML cs.LG stat.CO
\\ ( https://arxiv.org/abs/2303.02566 ,  8764kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08353
replaced with revised version Mon, 12 Feb 2024 21:17:03 GMT   (3248kb,D)

Title: Fast and Efficient Matching Algorithm with Deadline Instances
Authors: Zhao Song, Weixin Wang, Chenbo Yin, Junze Yin
Categories: cs.DS cs.LG
\\ ( https://arxiv.org/abs/2305.08353 ,  3248kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06064
replaced with revised version Tue, 13 Feb 2024 17:17:53 GMT   (2292kb,D)

Title: Neural Algorithmic Reasoning for Combinatorial Optimisation
Authors: Dobrik Georgiev and Danilo Numeroso and Davide Bacciu and Pietro Li\`o
Categories: cs.NE cs.LG
\\ ( https://arxiv.org/abs/2306.06064 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12510 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 07:52:17 GMT   (2174kb)

Title: Comparative Analysis of Segment Anything Model and U-Net for Breast
  Tumor Detection in Ultrasound and Mammography Images
Authors: Mohsen Ahmadi, Masoumeh Farhadi Nia, Sara Asgarian, Kasra Danesh,
  Elyas Irankhah, Ahmad Gholizadeh Lonbar, Abbas Sharifi
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.12510 ,  2174kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06299
replaced with revised version Tue, 13 Feb 2024 15:51:50 GMT   (32kb)

Title: Towards a Certified Proof Checker for Deep Neural Network Verification
Authors: Remi Desmartin, Omri Isac, Grant Passmore, Kathrin Stark, Guy Katz and
  Ekaterina Komendantskaya
Categories: cs.LO cs.LG cs.PL
Comments: This is a preprint version of the paper that appeared at LOPSTR 2023
\\ ( https://arxiv.org/abs/2307.06299 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11018 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 16:33:43 GMT   (1500kb,D)

Title: Amortized Variational Inference: When and Why?
Authors: Charles C. Margossian and David M. Blei
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2307.11018 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06686
replaced with revised version Tue, 13 Feb 2024 06:07:12 GMT   (19840kb,D)

Title: TorchQL: A Programming Framework for Integrity Constraints in Machine
  Learning
Authors: Aaditya Naik, Adam Stein, Yinjun Wu, Eric Wong, Mayur Naik
Categories: cs.DB cs.LG cs.SE
\\ ( https://arxiv.org/abs/2308.06686 ,  19840kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10630 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 09:58:40 GMT   (4107kb,D)

Title: A Homogenization Approach for Gradient-Dominated Stochastic Optimization
Authors: Jiyuan Tan, Chenyu Xue, Chuwen Zhang, Qi Deng, Dongdong Ge, Yinyu Ye
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2308.10630 ,  4107kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07138 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 07:40:31 GMT   (1426kb)

Title: Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders
Authors: Matthew B. Webster and Joonnyong Lee
Categories: eess.SP cs.LG
Comments: 17 pages, 8 figures, submitted for review
MSC-class: I.2.6
\\ ( https://arxiv.org/abs/2309.07138 ,  1426kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05955 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 13:19:30 GMT   (3827kb,D)

Title: Bayesian Quality-Diversity approaches for constrained optimization
  problems with mixed continuous, discrete and categorical variables
Authors: Loic Brevault and Mathieu Balesdent
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2310.05955 ,  3827kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17816 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 23:58:37 GMT   (5652kb,D)

Title: Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around
  Exposure-Outcome Pairs
Authors: Jacqueline Maasch, Weishen Pan, Shantanu Gupta, Volodymyr Kuleshov,
  Kyra Gan, Fei Wang
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2310.17816 ,  5652kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12904 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 01:30:19 GMT   (66kb)

Title: Learning to Compute Gr\"obner Bases
Authors: Hiroshi Kera, Yuki Ishihara, Yuta Kambe, Tristan Vaccon, Kazuhiro
  Yokoyama
Categories: math.AC cs.LG
Comments: 42 pages, 24 tables
\\ ( https://arxiv.org/abs/2311.12904 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18083
replaced with revised version Sat, 10 Feb 2024 22:05:46 GMT   (493kb,D)

Title: Meta Co-Training: Two Views are Better than One
Authors: Jay C. Rothenberger, Dimitrios I. Diochnos
Categories: cs.CV cs.LG
Comments: 16 pages, 14 figures, 10 tables, for implementation see
  https://github.com/JayRothenberger/Meta-Co-Training
ACM-class: I.2.6; I.4.10
\\ ( https://arxiv.org/abs/2311.18083 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01046 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 13:26:00 GMT   (1420kb,D)

Title: Bagged Regularized $k$-Distances for Anomaly Detection
Authors: Yuchao Cai and Yuheng Ma and Hanfang Yang and Hanyuan Hang
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2312.01046 ,  1420kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17173 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 00:19:03 GMT   (97kb,D)

Title: Non-Vacuous Generalization Bounds for Large Language Models
Authors: Sanae Lotfi, Marc Finzi, Yilun Kuang, Tim G. J. Rudner, Micah
  Goldblum, Andrew Gordon Wilson
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.17173 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00691 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 13:40:53 GMT   (873kb,D)

Title: Stochastic Gradient Descent for Additive Nonparametric Regression
Authors: Xin Chen and Jason M. Klusowski
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2401.00691 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04632
replaced with revised version Tue, 13 Feb 2024 15:43:01 GMT   (347kb,D)

Title: Hypercomplex neural network in time series forecasting of stock data
Authors: Rados{\l}aw Kycia, Agnieszka Niemczynowicz
Categories: cs.NE cs.LG
Comments: 16 pages
\\ ( https://arxiv.org/abs/2401.04632 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15719 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 06:08:07 GMT   (23kb)

Title: Rates of Convergence in the Central Limit Theorem for Markov Chains,
  with an Application to TD Learning
Authors: R. Srikant
Categories: math.PR cs.LG cs.SY eess.SY math.OC
\\ ( https://arxiv.org/abs/2401.15719 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02196 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 02:31:19 GMT   (982kb,D)

Title: Sample-Efficient Clustering and Conquer Procedures for Parallel
  Large-Scale Ranking and Selection
Authors: Zishi Zhang, Yijie Peng
Categories: stat.ME cs.LG
\\ ( https://arxiv.org/abs/2402.02196 ,  982kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04691 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 08:06:44 GMT   (50kb)

Title: Learning Operators with Stochastic Gradient Descent in General Hilbert
  Spaces
Authors: Lei Shi and Jia-Qi Yang
Categories: stat.ML cs.LG math.FA math.ST stat.TH
Comments: 56 pages
\\ ( https://arxiv.org/abs/2402.04691 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06282
replaced with revised version Tue, 13 Feb 2024 14:24:53 GMT   (714kb,D)

Title: Retrieve, Merge, Predict: Augmenting Tables with Data Lakes
Authors: Riccardo Cappuzzo (1), Gael Varoquaux (1), Aimee Coelho (2), Paolo
  Papotti (3) ((1) SODA Team - Inria Saclay, (2) Dataiku, (3) EURECOM)
Categories: cs.DB cs.LG
Comments: 12 pages + references, 11 figures. Under submission at VLDB2024 (EA&B
  track)
\\ ( https://arxiv.org/abs/2402.06282 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07355 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 02:49:52 GMT   (63kb)

Title: Sampling from the Mean-Field Stationary Distribution
Authors: Yunbum Kook, Matthew S. Zhang, Sinho Chewi, Murat A. Erdogdu, Mufan
  Bill Li
Categories: math.ST cs.LG stat.ML stat.TH
\\ ( https://arxiv.org/abs/2402.07355 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07357 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 13:46:07 GMT   (2867kb,D)

Title: Regression Trees for Fast and Adaptive Prediction Intervals
Authors: Luben M. C. Cabezas, Mateus P. Otto, Rafael Izbicki, Rafael B. Stern
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.07357 ,  2867kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07595 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 15:39:11 GMT   (300kb,D)

Title: Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and
  DINOv2 in Medical Imaging Classification
Authors: Yuning Huang, Jingchen Zou, Lanxi Meng, Xin Yue, Qing Zhao, Jianqiang
  Li, Changwei Song, Gabriel Jimenez, Shaowu Li, Guanghui Fu
Categories: eess.IV cs.LG
\\ ( https://arxiv.org/abs/2402.07595 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07735 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 09:48:47 GMT   (10164kb,D)

Title: Graph Structure Inference with BAM: Introducing the Bilinear Attention
  Mechanism
Authors: Philipp Froehlich and Heinz Koeppl
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.07735 ,  10164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07839
replaced with revised version Tue, 13 Feb 2024 13:19:54 GMT   (10301kb,D)

Title: Towards Meta-Pruning via Optimal Transport
Authors: Alexander Theus, Olin Geimer, Friedrich Wicke, Thomas Hofmann, Sotiris
  Anagnostidis, Sidak Pal Singh
Categories: cs.CV cs.LG
Comments: Accepted as a Spotlight (top 5% of submissions) at the International
  Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2402.07839 ,  10301kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
