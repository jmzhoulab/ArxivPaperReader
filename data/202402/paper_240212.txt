Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80011 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月12日 16:36
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu  8 Feb 24 19:00:00 GMT  to  Fri  9 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.06025
Date: Thu, 8 Feb 2024 19:57:29 GMT   (1504kb,D)

Title: Doing Experiments and Revising Rules with Natural Language and
  Probabilistic Reasoning
Authors: Top Piriyakulkij, Kevin Ellis
Categories: cs.AI cs.CL
\\
  We build a computational model of how humans actively infer hidden rules by
doing experiments. The basic principles behind the model is that, even if the
rule is deterministic, the learner considers a broader space of fuzzy
probabilistic rules, which it represents in natural language, and updates its
hypotheses online after each experiment according to approximately Bayesian
principles. In the same framework we also model experiment design according to
information-theoretic criteria. We find that the combination of these three
principles -- explicit hypotheses, probabilistic rules, and online updates --
can explain human performance on a Zendo-style task, and that removing any of
these components leaves the model unable to account for the data.
\\ ( https://arxiv.org/abs/2402.06025 ,  1504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06044
Date: Thu, 8 Feb 2024 20:35:06 GMT   (4577kb,D)

Title: OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind
  Reasoning Capabilities of Large Language Models
Authors: Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He
Categories: cs.AI cs.CL
\\
  Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track
of the mental states of others, is pivotal in developing socially intelligent
agents. However, prevalent N-ToM benchmarks have several shortcomings,
including the presence of ambiguous and artificial narratives, absence of
personality traits and preferences, a lack of questions addressing characters'
psychological mental states, and limited diversity in the questions posed. In
response to these issues, we construct OpenToM, a new benchmark for assessing
N-ToM with (1) longer and clearer narrative stories, (2) characters with
explicit personality traits, (3) actions that are triggered by character
intentions, and (4) questions designed to challenge LLMs' capabilities of
modeling characters' mental states of both the physical and psychological
world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling
certain aspects of mental states in the physical world but fall short when
tracking characters' mental states in the psychological world.
\\ ( https://arxiv.org/abs/2402.06044 ,  4577kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06049
Date: Tue, 6 Feb 2024 03:24:27 GMT   (2978kb,D)

Title: Limits of Large Language Models in Debating Humans
Authors: James Flamino, Mohammed Shahid Modi, Boleslaw K. Szymanski, Brendan
  Cross, Colton Mikolajczyk
Categories: cs.AI cs.CL cs.HC stat.AP
Comments: 23 pages, 6 figures, 3 tables, 21 pages of supplemental materials, 8
  supplemental figures, 6 supplemental tables
\\
  Large Language Models (LLMs) have shown remarkable promise in their ability
to interact proficiently with humans. Subsequently, their potential use as
artificial confederates and surrogates in sociological experiments involving
conversation is an exciting prospect. But how viable is this idea? This paper
endeavors to test the limits of current-day LLMs with a pre-registered study
integrating real people with LLM agents acting as people. The study focuses on
debate-based opinion consensus formation in three environments: humans only,
agents and humans, and agents only. Our goal is to understand how LLM agents
influence humans, and how capable they are in debating like humans. We find
that LLMs can blend in and facilitate human productivity but are less
convincing in debate, with their behavior ultimately deviating from human's. We
elucidate these primary failings and anticipate that LLMs must evolve further
before being viable debaters.
\\ ( https://arxiv.org/abs/2402.06049 ,  2978kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06097
Date: Thu, 8 Feb 2024 23:12:02 GMT   (152kb,D)

Title: TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph
  Generalisation via Simulated KGE Models
Authors: Jeffrey Sardina, John D. Kelleher, Declan O'Sullivan
Categories: cs.AI cs.LG
Comments: This article was accepted for publication at IEEE ICSC 2024
MSC-class: 68R10
\\
  In this paper we introduce TWIG (Topologically-Weighted Intelligence
Generation), a novel, embedding-free paradigm for simulating the output of KGEs
that uses a tiny fraction of the parameters. TWIG learns weights from inputs
that consist of topological features of the graph data, with no coding for
latent representations of entities or edges. Our experiments on the UMLS
dataset show that a single TWIG neural network can predict the results of
state-of-the-art ComplEx-N3 KGE model nearly exactly on across all
hyperparameter configurations. To do this it uses a total of 2590 learnable
parameters, but accurately predicts the results of 1215 different
hyperparameter combinations with a combined cost of 29,322,000 parameters.
Based on these results, we make two claims: 1) that KGEs do not learn latent
semantics, but only latent representations of structural patterns; 2) that
hyperparameter choice in KGEs is a deterministic function of the KGE model and
graph structure. We further hypothesise that, as TWIG can simulate KGEs without
embeddings, that node and edge embeddings are not needed to learn to accurately
predict new facts in KGs. Finally, we formulate all of our findings under the
umbrella of the ``Structural Generalisation Hypothesis", which suggests that
``twiggy" embedding-free / data-structure-based learning methods can allow a
single neural network to simulate KGE performance, and perhaps solve the Link
Prediction task, across many KGs from diverse domains and with different
semantics.
\\ ( https://arxiv.org/abs/2402.06097 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06098
Date: Thu, 8 Feb 2024 23:15:23 GMT   (236kb,D)

Title: Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge
  Graph Learning
Authors: Jeffrey Sardina, Luca Costabello, Christophe Gu\'eret
Categories: cs.AI cs.LG
Comments: This article was accepted for publication at IEEE ICSC 2024, and is
  being made available as an author preprint. As soon as it is published by
  IEEE, this registry will be updated in accordance with the IEEE copyright
  agreement
\\
  Knowledge Graphs (KGs) have become increasingly common for representing
large-scale linked data. However, their immense size has required graph
learning systems to assist humans in analysis, interpretation, and pattern
detection. While there have been promising results for researcher- and
clinician- empowerment through a variety of KG learning systems, we identify
four key deficiencies in state-of-the-art graph learning that simultaneously
limit KG learning performance and diminish the ability of humans to interface
optimally with these learning systems. These deficiencies are: 1) lack of
expert knowledge integration, 2) instability to node degree extremity in the
KG, 3) lack of consideration for uncertainty and relevance while learning, and
4) lack of explainability. Furthermore, we characterise state-of-the-art
attempts to solve each of these problems and note that each attempt has largely
been isolated from attempts to solve the other problems. Through a
formalisation of these problems and a review of the literature that addresses
them, we adopt the position that not only are deficiencies in these four key
areas holding back human-KG empowerment, but that the divide-and-conquer
approach to solving these problems as individual units rather than a whole is a
significant barrier to the interface between humans and KG learning systems. We
propose that it is only through integrated, holistic solutions to the
limitations of KG learning systems that human and KG learning co-empowerment
will be efficiently affected. We finally present our "Veni, Vidi, Vici"
framework that sets a roadmap for effectively and efficiently shifting to a
holistic co-empowerment model in both the KG learning and the broader machine
learning domain.
\\ ( https://arxiv.org/abs/2402.06098 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06147
Date: Mon, 5 Feb 2024 06:12:29 GMT   (208kb,D)

Title: DeAL: Decoding-time Alignment for Large Language Models
Authors: James Y. Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit
  Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchoff, Dan Roth
Categories: cs.AI cs.CL
Comments: The appendix contains data that is offensive / disturbing in nature
\\
  Large Language Models (LLMs) are nowadays expected to generate content
aligned with human preferences. Current work focuses on alignment at model
training time, through techniques such as Reinforcement Learning with Human
Feedback (RLHF). However, it is unclear if such methods are an effective choice
to teach alignment objectives to the model. First, the inability to incorporate
multiple, custom rewards and reliance on a model developer's view of universal
and static principles are key limitations. Second, the residual gaps in model
training and the reliability of such approaches are also questionable (e.g.
susceptibility to jail-breaking even after safety training). To address these,
we propose DeAL, a framework that allows the user to customize reward functions
and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view
decoding as a heuristic-guided search process and facilitate the use of a wide
variety of alignment objectives. Our experiments with programmatic constraints
such as keyword and length constraints (studied widely in the pre-LLM era) and
abstract objectives such as harmlessness and helpfulness (proposed in the
post-LLM era) show that we can DeAL with fine-grained trade-offs, improve
adherence to alignment objectives, and address residual gaps in LLMs. Lastly,
while DeAL can be effectively paired with RLHF and prompting techniques, its
generality makes decoding slower, an optimization we leave for future work.
\\ ( https://arxiv.org/abs/2402.06147 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06264
Date: Fri, 9 Feb 2024 09:25:18 GMT   (1255kb)

Title: LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to
  Support Art Appreciation Education
Authors: Unggi Lee, Minji Jeon, Yunseo Lee, Gyuri Byun, Yoorim Son, Jaeyoon
  Shin, Hongkyu Ko, Hyeoncheol Kim
Categories: cs.AI cs.SI
Comments: 37 pages, 4 figures, 10 tables
\\
  Art appreciation is vital in nurturing critical thinking and emotional
intelligence among learners. However, traditional art appreciation education
has often been hindered by limited access to art resources, especially for
disadvantaged students, and an imbalanced emphasis on STEM subjects in
mainstream education. In response to these challenges, recent technological
advancements have paved the way for innovative solutions. This study explores
the application of multi-modal large language models (MLLMs) in art
appreciation education, focusing on developing LLaVA-Docent, a model that
leverages these advancements. Our approach involved a comprehensive literature
review and consultations with experts in the field, leading to developing a
robust data framework. Utilizing this framework, we generated a virtual
dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in
training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative
and qualitative evaluations of LLaVA-Docent to assess its effectiveness,
benchmarking it against the GPT-4 model in a few-shot setting. The evaluation
process revealed distinct strengths and weaknesses of the LLaVA-Docent model.
Our findings highlight the efficacy of LLaVA-Docent in enhancing the
accessibility and engagement of art appreciation education. By harnessing the
potential of MLLMs, this study makes a significant contribution to the field of
art education, proposing a novel methodology that reimagines the way art
appreciation is taught and experienced.
\\ ( https://arxiv.org/abs/2402.06264 ,  1255kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06326
Date: Fri, 9 Feb 2024 11:06:20 GMT   (2231kb,D)

Title: Prompt Learning on Temporal Interaction Graphs
Authors: Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun,
  Yao Zhang, Yinglong Zhao, Yulin Kang
Categories: cs.AI cs.LG cs.SI
Comments: 11 pages, 8 figures
\\
  Temporal Interaction Graphs (TIGs) are widely utilized to represent
real-world systems. To facilitate representation learning on TIGs, researchers
have proposed a series of TIG models. However, these models are still facing
two tough gaps between the pre-training and downstream predictions in their
``pre-train, predict'' training paradigm. First, the temporal discrepancy
between the pre-training and inference data severely undermines the models'
applicability in distant future predictions on the dynamically evolving data.
Second, the semantic divergence between pretext and downstream tasks hinders
their practical applications, as they struggle to align with their learning and
prediction capabilities across application scenarios.
  Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight
mechanism for model generalization. Applying this paradigm is a potential
solution to solve the aforementioned challenges. However, the adaptation of
this paradigm to TIGs is not straightforward. The application of prompting in
static graph contexts falls short in temporal settings due to a lack of
consideration for time-sensitive dynamics and a deficiency in expressive power.
To address this issue, we introduce Temporal Interaction Graph Prompting
(TIGPrompt), a versatile framework that seamlessly integrates with TIG models,
bridging both the temporal and semantic gaps. In detail, we propose a temporal
prompt generator to offer temporally-aware prompts for different tasks. These
prompts stand out for their minimalistic design, relying solely on the tuning
of the prompt generator with very little supervision data. To cater to varying
computational resource demands, we propose an extended ``pre-train,
prompt-based fine-tune'' paradigm, offering greater flexibility. Through
extensive experiments, the TIGPrompt demonstrates the SOTA performance and
remarkable efficiency advantages.
\\ ( https://arxiv.org/abs/2402.06326 ,  2231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06359
Date: Fri, 9 Feb 2024 12:08:49 GMT   (1949kb,D)

Title: Modelling Human Values for AI Reasoning
Authors: Nardine Osman and Mark d'Inverno
Categories: cs.AI cs.MA
MSC-class: 68T01
ACM-class: I.2.4
\\
  One of today's most significant societal challenges is building AI systems
whose behaviour, or the behaviour it enables within communities of interacting
agents (human and artificial), aligns with human values. To address this
challenge, we detail a formal model of human values for their explicit
computational representation. To our knowledge, this has not been attempted as
yet, which is surprising given the growing volume of research integrating
values within AI. Taking as our starting point the wealth of research
investigating the nature of human values from social psychology over the last
few decades, we set out to provide such a formal model. We show how this model
can provide the foundational apparatus for AI-based reasoning over values, and
demonstrate its applicability in real-world use cases. We illustrate how our
model captures the key ideas from social psychology research and propose a
roadmap for future integrated, and interdisciplinary, research into human
values in AI. The ability to automatically reason over values not only helps
address the value alignment problem but also facilitates the design of AI
systems that can support individuals and communities in making more informed,
value-aligned decisions. More and more, individuals and organisations are
motivated to understand their values more explicitly and explore whether their
behaviours and attitudes properly reflect them. Our work on modelling human
values will enable AI systems to be designed and deployed to meet this growing
need.
\\ ( https://arxiv.org/abs/2402.06359 ,  1949kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06389
Date: Fri, 9 Feb 2024 13:11:19 GMT   (8977kb,D)

Title: Human Aesthetic Preference-Based Large Text-to-Image Model
  Personalization: Kandinsky Generation as an Example
Authors: Aven-Le Zhou, Yu-Ao Wang, Wei Wu and Kang Zhang
Categories: cs.AI cs.HC cs.MM
Comments: 9 pages, 10 figures
\\
  With the advancement of neural generative capabilities, the art community has
actively embraced GenAI (generative artificial intelligence) for creating
painterly content. Large text-to-image models can quickly generate
aesthetically pleasing outcomes. However, the process can be non-deterministic
and often involves tedious trial-and-error, as users struggle with formulating
effective prompts to achieve their desired results. This paper introduces a
prompting-free generative approach that empowers users to automatically
generate personalized painterly content that incorporates their aesthetic
preferences in a customized artistic style. This approach involves utilizing
``semantic injection'' to customize an artist model in a specific artistic
style, and further leveraging a genetic algorithm to optimize the prompt
generation process through real-time iterative human feedback. By solely
relying on the user's aesthetic evaluation and preference for the artist
model-generated images, this approach creates the user a personalized model
that encompasses their aesthetic preferences and the customized artistic style.
\\ ( https://arxiv.org/abs/2402.06389 ,  8977kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06487
Date: Fri, 9 Feb 2024 15:43:31 GMT   (5533kb,D)

Title: Le Nozze di Giustizia. Interactions between Artificial Intelligence,
  Law, Logic, Language and Computation with some case studies in Traffic
  Regulations and Health Care
Authors: Joost J. Joosten and Manuela Montoya Garc\'ia
Categories: cs.AI cs.CY
\\
  An important aim of this paper is to convey some basics of mathematical logic
to the legal community working with Artificial Intelligence. After analysing
what AI is, we decide to delimit ourselves to rule-based AI leaving Neural
Networks and Machine Learning aside. Rule based AI allows for Formal methods
which are described in a rudimentary form. We will then see how mathematical
logic interacts with legal rule-based AI practice. We shall see how
mathematical logic imposes limitations and complications to AI applications. We
classify the limitations and interactions between mathematical logic and legal
AI in three categories: logical, computational and mathematical. The examples
to showcase the interactions will largely come from European traffic
regulations. The paper closes off with some reflections on how and where AI
could be used and on basic mechanisms that shape society.
\\ ( https://arxiv.org/abs/2402.06487 ,  5533kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06500
Date: Fri, 9 Feb 2024 16:10:19 GMT   (981kb,D)

Title: On the Fly Detection of Root Causes from Observed Data with Application
  to IT Systems
Authors: Lei Zan, Charles K. Assaad, Emilie Devijver, Eric Gaussier
Categories: cs.AI
\\
  This paper introduces a new structural causal model tailored for representing
threshold-based IT systems and presents a new algorithm designed to rapidly
detect root causes of anomalies in such systems. When root causes are not
causally related, the method is proven to be correct; while an extension is
proposed based on the intervention of an agent to relax this assumption. Our
algorithm and its agent-based extension leverage causal discovery from offline
data and engage in subgraph traversal when encountering new anomalies in online
data. Our extensive experiments demonstrate the superior performance of our
methods, even when applied to data generated from alternative structural causal
models or real IT monitoring data.
\\ ( https://arxiv.org/abs/2402.06500 ,  981kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06503
Date: Fri, 9 Feb 2024 16:12:53 GMT   (2079kb,D)

Title: ACTER: Diverse and Actionable Counterfactual Sequences for Explaining
  and Diagnosing RL Policies
Authors: Jasmina Gajcin and Ivana Dusparic
Categories: cs.AI cs.LG
Comments: 17 pages, 4 Figures
\\
  Understanding how failure occurs and how it can be prevented in reinforcement
learning (RL) is necessary to enable debugging, maintain user trust, and
develop personalized policies. Counterfactual reasoning has often been used to
assign blame and understand failure by searching for the closest possible world
in which the failure is avoided. However, current counterfactual state
explanations in RL can only explain an outcome using just the current state
features and offer no actionable recourse on how a negative outcome could have
been prevented. In this work, we propose ACTER (Actionable Counterfactual
Sequences for Explaining Reinforcement Learning Outcomes), an algorithm for
generating counterfactual sequences that provides actionable advice on how
failure can be avoided. ACTER investigates actions leading to a failure and
uses the evolutionary algorithm NSGA-II to generate counterfactual sequences of
actions that prevent it with minimal changes and high certainty even in
stochastic environments. Additionally, ACTER generates a set of multiple
diverse counterfactual sequences that enable users to correct failure in the
way that best fits their preferences. We also introduce three diversity metrics
that can be used for evaluating the diversity of counterfactual sequences. We
evaluate ACTER in two RL environments, with both discrete and continuous
actions, and show that it can generate actionable and diverse counterfactual
sequences. We conduct a user study to explore how explanations generated by
ACTER help users identify and correct failure.
\\ ( https://arxiv.org/abs/2402.06503 ,  2079kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06529
Date: Fri, 9 Feb 2024 16:40:59 GMT   (9496kb,D)

Title: Introspective Planning: Guiding Language-Enabled Agents to Refine Their
  Own Uncertainty
Authors: Kaiqu Liang, Zixu Zhang, Jaime Fern\'andez Fisac
Categories: cs.AI cs.CL cs.LG
Comments: 22 pages, 15 figures
\\
  Large language models (LLMs) exhibit advanced reasoning skills, enabling
robots to comprehend natural language instructions and strategically plan
high-level actions through proper grounding. However, LLM hallucination may
result in robots confidently executing plans that are misaligned with user
goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural
language instructions can induce task uncertainty, particularly in situations
where multiple valid options exist. To address this issue, LLMs must identify
such uncertainty and proactively seek clarification. This paper explores the
concept of introspective planning as a systematic method for guiding LLMs in
forming uncertainty--aware plans for robotic task execution without the need
for fine-tuning. We investigate uncertainty quantification in task-level robot
planning and demonstrate that introspection significantly improves both success
rates and safety compared to state-of-the-art LLM-based planning approaches.
Furthermore, we assess the effectiveness of introspective planning in
conjunction with conformal prediction, revealing that this combination yields
tighter confidence bounds, thereby maintaining statistical success guarantees
with fewer superfluous user clarification queries.
\\ ( https://arxiv.org/abs/2402.06529 ,  9496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06557
Date: Fri, 9 Feb 2024 17:15:45 GMT   (692kb,D)

Title: The Quantified Boolean Bayesian Network: Theory and Experiments with a
  Logical Graphical Model
Authors: Gregory Coppola
Categories: cs.AI cs.IR
\\
  This paper introduces the Quantified Boolean Bayesian Network (QBBN), which
provides a unified view of logical and probabilistic reasoning. The QBBN is
meant to address a central problem with the Large Language Model (LLM), which
has become extremely popular in Information Retrieval, which is that the LLM
hallucinates. A Bayesian Network, by construction, cannot hallucinate, because
it can only return answers that it can explain. We show how a Bayesian Network
over an unbounded number of boolean variables can be configured to represent
the logical reasoning underlying human language. We do this by creating a
key-value version of the First-Order Calculus, for which we can prove
consistency and completeness. We show that the model is trivially trained over
fully observed data, but that inference is non-trivial. Exact inference in a
Bayesian Network is intractable (i.e. $\Omega(2^N)$ for $N$ variables). For
inference, we investigate the use of Loopy Belief Propagation (LBP), which is
not guaranteed to converge, but which has been shown to often converge in
practice. Our experiments show that LBP indeed does converge very reliably, and
our analysis shows that a round of LBP takes time $O(N2^n)$, where $N$ bounds
the number of variables considered, and $n$ bounds the number of incoming
connections to any factor, and further improvements may be possible. Our
network is specifically designed to alternate between AND and OR gates in a
Boolean Algebra, which connects more closely to logical reasoning, allowing a
completeness proof for an expanded version of our network, and also allows
inference to follow specific but adequate pathways, that turn out to be fast.
\\ ( https://arxiv.org/abs/2402.06557 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06590
Date: Fri, 9 Feb 2024 18:10:38 GMT   (8823kb,D)

Title: Predictive representations: building blocks of intelligence
Authors: Wilka Carvalho, Momchil S. Tomov, William de Cothi, Caswell Barry,
  Samuel J. Gershman
Categories: cs.AI cs.LG
\\
  Adaptive behavior often requires predicting future events. The theory of
reinforcement learning prescribes what kinds of predictive representations are
useful and how to compute them. This paper integrates these theoretical ideas
with work on cognition and neuroscience. We pay special attention to the
successor representation (SR) and its generalizations, which have been widely
applied both as engineering tools and models of brain function. This
convergence suggests that particular kinds of predictive representations may
function as versatile building blocks of intelligence.
\\ ( https://arxiv.org/abs/2402.06590 ,  8823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06596
Date: Fri, 9 Feb 2024 18:19:25 GMT   (1127kb,D)

Title: Understanding the Weakness of Large Language Model Agents within a
  Complex Android Environment
Authors: Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, Zhen Xiao
Categories: cs.AI cs.HC cs.SE
\\
  Large language models (LLMs) have empowered intelligent agents to execute
intricate tasks within domain-specific software such as browsers and games.
However, when applied to general-purpose software systems like operating
systems, LLM agents face three primary challenges. Firstly, the action space is
vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date
understanding and deliver accurate responses. Secondly, real-world tasks often
require inter-application cooperation}, demanding farsighted planning from LLM
agents. Thirdly, agents need to identify optimal solutions aligning with user
constraints, such as security concerns and preferences. These challenges
motivate AndroidArena, an environment and benchmark designed to evaluate LLM
agents on a modern operating system. To address high-cost of manpower, we
design a scalable and semi-automated method to construct the benchmark. In the
task evaluation, AndroidArena incorporates accurate and adaptive metrics to
address the issue of non-unique solutions. Our findings reveal that even
state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to
specific constraints. Additionally, we identify a lack of four key
capabilities, i.e., understanding, reasoning, exploration, and reflection, as
primary reasons for the failure of LLM agents. Furthermore, we provide
empirical analysis on the failure of reflection, and improve the success rate
by 27% with our proposed exploration strategy. This work is the first to
present valuable insights in understanding fine-grained weakness of LLM agents,
and offers a path forward for future research in this area. Environment,
benchmark, and evaluation code for AndroidArena are released at
https://github.com/AndroidArenaAgent/AndroidArena.
\\ ( https://arxiv.org/abs/2402.06596 ,  1127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06015
Date: Thu, 8 Feb 2024 19:25:40 GMT   (9225kb,D)

Title: Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing
Authors: Yong Cao, Wenyan Li, Jiaang Li, Yifei Yuan, Daniel Hershcovich
Categories: cs.CL cs.CV
Comments: work in process
\\
  Pretrained large Vision-Language models have drawn considerable interest in
recent years due to their remarkable performance. Despite considerable efforts
to assess these models from diverse perspectives, the extent of visual cultural
awareness in the state-of-the-art GPT-4V model remains unexplored. To tackle
this gap, we extensively probed GPT-4V using the MaRVL benchmark dataset,
aiming to investigate its capabilities and limitations in visual understanding
with a focus on cultural aspects. Specifically, we introduced three visual
related tasks, i.e. caption classification, pairwise captioning, and culture
tag selection, to systematically delve into fine-grained visual cultural
evaluation. Experimental results indicate that GPT-4V excels at identifying
cultural concepts but still exhibits weaker performance in low-resource
languages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V
proves to be more culturally relevant in image captioning tasks than the
original MaRVL human annotations, suggesting a promising solution for future
visual cultural benchmark construction.
\\ ( https://arxiv.org/abs/2402.06015 ,  9225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06041
Date: Thu, 8 Feb 2024 20:24:44 GMT   (481kb,D)

Title: A Prompt Response to the Demand for Automatic Gender-Neutral Translation
Authors: Beatrice Savoldi and Andrea Piergentili and Dennis Fucci and Matteo
  Negri and Luisa Bentivogli
Categories: cs.CL
Comments: Accepted at EACL 2024
\\
  Gender-neutral translation (GNT) that avoids biased and undue binary
assumptions is a pivotal challenge for the creation of more inclusive
translation technologies. Advancements for this task in Machine Translation
(MT), however, are hindered by the lack of dedicated parallel data, which are
necessary to adapt MT systems to satisfy neutral constraints. For such a
scenario, large language models offer hitherto unforeseen possibilities, as
they come with the distinct advantage of being versatile in various (sub)tasks
when provided with explicit instructions. In this paper, we explore this
potential to automate GNT by comparing MT with the popular GPT-4 model. Through
extensive manual analyses, our study empirically reveals the inherent
limitations of current MT systems in generating GNTs and provides valuable
insights into the potential and challenges associated with prompting for
neutrality.
\\ ( https://arxiv.org/abs/2402.06041 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06073
Date: Thu, 8 Feb 2024 21:47:16 GMT   (716kb)

Title: LightCAM: A Fast and Light Implementation of Context-Aware Masking based
  D-Tdnn for Speaker Verification
Authors: Di Cao, Xianchen Wang, Junfeng Zhou, Jiakai Zhang, Yanjing Lei and
  Wenpeng Chen
Categories: cs.CL cs.SD eess.AS
\\
  Traditional Time Delay Neural Networks (TDNN) have achieved state-of-the-art
performance at the cost of high computational complexity and slower inference
speed, making them difficult to implement in an industrial environment. The
Densely Connected Time Delay Neural Network (D-TDNN) with Context Aware Masking
(CAM) module has proven to be an efficient structure to reduce complexity while
maintaining system performance. In this paper, we propose a fast and
lightweight model, LightCAM, which further adopts a depthwise separable
convolution module (DSM) and uses multi-scale feature aggregation (MFA) for
feature fusion at different levels. Extensive experiments are conducted on
VoxCeleb dataset, the comparative results show that it has achieved an EER of
0.83 and MinDCF of 0.0891 in VoxCeleb1-O, which outperforms the other
mainstream speaker verification methods. In addition, complexity analysis
further demonstrates that the proposed architecture has lower computational
cost and faster inference speed.
\\ ( https://arxiv.org/abs/2402.06073 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06094
Date: Thu, 8 Feb 2024 23:02:04 GMT   (2199kb,D)

Title: Rethinking Data Selection for Supervised Fine-Tuning
Authors: Ming Shen
Categories: cs.CL
\\
  Although supervised finetuning (SFT) has emerged as an essential technique to
align large language models with humans, it is considered superficial, with
style learning being its nature. At the same time, recent works indicate the
importance of data selection for SFT, showing that finetuning with high-quality
and diverse subsets of the original dataset leads to superior downstream
performance. In this work, we rethink the intuition behind data selection for
SFT. Considering SFT is superficial, we propose that essential demonstrations
for SFT should focus on reflecting human-like interactions instead of data
quality or diversity. However, it is not straightforward to directly assess to
what extent a demonstration reflects human styles. Towards an initial attempt
in this direction, we find selecting instances with long responses is
surprisingly more effective for SFT than utilizing full datasets or instances
selected based on quality and diversity. We hypothesize that such a simple
heuristic implicitly mimics a crucial aspect of human-style conversation:
detailed responses are usually more helpful.
\\ ( https://arxiv.org/abs/2402.06094 ,  2199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06120
Date: Fri, 9 Feb 2024 01:10:25 GMT   (2671kb,D)

Title: Exploring Group and Symmetry Principles in Large Language Models
Authors: Shima Imani, Hamid Palangi
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated impressive performance across
a wide range of applications; however, assessing their reasoning capabilities
remains a significant challenge. In this paper, we introduce a framework
grounded in group and symmetry principles, which have played a crucial role in
fields such as physics and mathematics, and offer another way to evaluate their
capabilities. While the proposed framework is general, to showcase the benefits
of employing these properties, we focus on arithmetic reasoning and investigate
the performance of these models on four group properties: closure, identity,
inverse, and associativity. Our findings reveal that LLMs studied in this work
struggle to preserve group properties across different test regimes. In the
closure test, we observe biases towards specific outputs and an abrupt
degradation in their performance from 100% to 0% after a specific sequence
length. They also perform poorly in the identity test, which represents adding
irrelevant information in the context, and show sensitivity when subjected to
inverse test, which examines the robustness of the model with respect to
negation. In addition, we demonstrate that breaking down problems into smaller
steps helps LLMs in the associativity test that we have conducted. To support
these tests we have developed a synthetic dataset which will be released.
\\ ( https://arxiv.org/abs/2402.06120 ,  2671kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06125
Date: Fri, 9 Feb 2024 01:15:42 GMT   (2209kb,D)

Title: Language Model Sentence Completion with a Parser-Driven Rhetorical
  Control Method
Authors: Joshua Zingale and Jugal Kalita
Categories: cs.CL
Comments: To be published in the main proceedings of the Association for
  Computational Linguistics, European Chapter (EACL 2024)
\\
  Controlled text generation (CTG) seeks to guide large language model (LLM)
output to produce text that conforms to desired criteria. The current study
presents a novel CTG algorithm that enforces adherence toward specific
rhetorical relations in an LLM sentence-completion context by a parser-driven
decoding scheme that requires no model fine-tuning. The method is validated
both with automatic and human evaluation. The code is accessible on GitHub.
\\ ( https://arxiv.org/abs/2402.06125 ,  2209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06126
Date: Fri, 9 Feb 2024 01:18:16 GMT   (1329kb,D)

Title: Learn To be Efficient: Build Structured Sparsity in Large Language
  Models
Authors: Haizhong Zheng, Xiaoyan Bai, Beidi Chen, Fan Lai, Atul Prakash
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have achieved remarkable success with their
billion-level parameters, yet they incur high inference overheads. The
emergence of activation sparsity in LLMs provides a natural approach to reduce
this cost by involving only parts of the parameters for inference. Existing
methods only focus on utilizing this naturally formed activation sparsity,
overlooking the potential for further amplifying this inherent sparsity. In
this paper, we hypothesize that LLMs can learn to be efficient by achieving
more structured activation sparsity.To achieve this, we introduce a novel
algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs
to learn to activate fewer neurons and achieve a better trade-off between
sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which
mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and
LLaMA with soft activation functions. We evaluate LTE on four models and eleven
datasets. The experiments show that LTE achieves a better trade-off between
sparsity and task performance. For instance, LTE with LLaMA provides a
1.83x-2.59x FLOPs speed-up on language generation tasks, outperforming the
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.06126 ,  1329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06155
Date: Fri, 9 Feb 2024 03:08:12 GMT   (676kb,D)

Title: Model Editing with Canonical Examples
Authors: John Hewitt, Sarah Chen, Lanruo Lora Xie, Edward Adams, Percy Liang,
  Christopher D. Manning
Categories: cs.CL
\\
  We introduce model editing with canonical examples, a setting in which (1) a
single learning example is provided per desired behavior, (2) evaluation is
performed exclusively out-of-distribution, and (3) deviation from an initial
model is strictly limited. A canonical example is a simple instance of good
behavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g.,
An aspect of researchers is coldhearted). The evaluation set contains more
complex examples of each behavior (like a paragraph in which the capital of
Mauritius is called for.) We create three datasets and modify three more for
model editing with canonical examples, covering knowledge-intensive
improvements, social bias mitigation, and syntactic edge cases. In our
experiments on Pythia language models, we find that LoRA outperforms full
finetuning and MEMIT. We then turn to the Backpack language model architecture
because it is intended to enable targeted improvement. The Backpack defines a
large bank of sense vectors--a decomposition of the different uses of each
word--which are weighted and summed to form the output logits of the model. We
propose sense finetuning, which selects and finetunes a few ($\approx$ 10)
sense vectors for each canonical example, and find that it outperforms other
finetuning methods, e.g., 4.8% improvement vs 0.3%. Finally, we improve
GPT-J-6B by an inference-time ensemble with just the changes from sense
finetuning of a 35x smaller Backpack, in one setting outperforming editing
GPT-J itself (4.1% vs 1.0%).
\\ ( https://arxiv.org/abs/2402.06155 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06196
Date: Fri, 9 Feb 2024 05:37:09 GMT   (7242kb,D)

Title: Large Language Models: A Survey
Authors: Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
  Richard Socher, Xavier Amatriain, Jianfeng Gao
Categories: cs.CL cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2401.14423
\\
  Large Language Models (LLMs) have drawn a lot of attention due to their
strong performance on a wide range of natural language tasks, since the release
of ChatGPT in November 2022. LLMs' ability of general-purpose language
understanding and generation is acquired by training billions of model's
parameters on massive amounts of text data, as predicted by scaling laws
\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while
very recent, is evolving rapidly in many different ways. In this paper, we
review some of the most prominent LLMs, including three popular LLM families
(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and
limitations. We also give an overview of techniques developed to build, and
augment LLMs. We then survey popular datasets prepared for LLM training,
fine-tuning, and evaluation, review widely used LLM evaluation metrics, and
compare the performance of several popular LLMs on a set of representative
benchmarks. Finally, we conclude the paper by discussing open challenges and
future research directions.
\\ ( https://arxiv.org/abs/2402.06196 ,  7242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06204
Date: Fri, 9 Feb 2024 06:16:08 GMT   (7532kb,D)

Title: The Generative AI Paradox on Evaluation: What It Can Solve, It May Not
  Evaluate
Authors: Juhyun Oh, Eunsu Kim, Inha Cha, Alice Oh
Categories: cs.CL cs.AI
\\
  This paper explores the assumption that Large Language Models (LLMs) skilled
in generation tasks are equally adept as evaluators. We assess the performance
of three LLMs and one open-source LM in Question-Answering (QA) and evaluation
tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a
significant disparity, with LLMs exhibiting lower performance in evaluation
tasks compared to generation tasks. Intriguingly, we discover instances of
unfaithful evaluation where models accurately evaluate answers in areas where
they lack competence, underscoring the need to examine the faithfulness and
trustworthiness of LLMs as evaluators. This study contributes to the
understanding of "the Generative AI Paradox" (West et al., 2023), highlighting
a need to explore the correlation between generative excellence and evaluation
proficiency, and the necessity to scrutinize the faithfulness aspect in model
evaluations.
\\ ( https://arxiv.org/abs/2402.06204 ,  7532kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06220
Date: Fri, 9 Feb 2024 07:12:56 GMT   (932kb,D)

Title: A Unified Causal View of Instruction Tuning
Authors: Lu Chen, Wei Huang, Ruqing Zhang, Wei Chen, Jiafeng Guo, Xueqi Cheng
Categories: cs.CL
\\
  Instruction tuning on a mixture of tasks has improved zero-shot capabilities
in natural language processing (NLP). Nevertheless, existing methods often
learn features that exhibit correlations between instruction-formatted samples
and target labels, rather than causal relationships. Termed as ``spurious
correlation'' in statistics, such a correlation may change drastically in a new
task, making the effect from the learned features to be misleading. To this
end, we develop a meta Structural Causal Model (meta-SCM) to integrate
different NLP tasks under a single causal structure of the data. Specifically,
the meta-SCM introduces multiple latent factors that represent properties of
source context, only some of which causally influence the target labels for a
specific task. The key idea is to learn task-required causal factors and only
use those to make predictions for a given task. Theoretically, we prove the
causal factor can be identified without mixing information from others. Guided
by the identifiability, we propose a Structural Instruction Tuning (SIT) method
to learn the task-required causal representations that can mimic the causal
factors for each task. The utility of our approach is verified by improvements
of zero-shot ability on a range of unseen datasets and tasks.
\\ ( https://arxiv.org/abs/2402.06220 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06221
Date: Fri, 9 Feb 2024 07:13:44 GMT   (3522kb,D)

Title: ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume
  Generation and Refinement
Authors: Saurabh Bhausaheb Zinjad, Amrita Bhattacharjee, Amey Bhilegaonkar,
  Huan Liu
Categories: cs.CL cs.IR
Comments: Under review
\\
  Crafting the ideal, job-specific resume is a challenging task for many job
applicants, especially for early-career applicants. While it is highly
recommended that applicants tailor their resume to the specific role they are
applying for, manually tailoring resumes to job descriptions and role-specific
requirements is often (1) extremely time-consuming, and (2) prone to human
errors. Furthermore, performing such a tailoring step at scale while applying
to several roles may result in a lack of quality of the edited resumes. To
tackle this problem, in this demo paper, we propose ResumeFlow: a Large
Language Model (LLM) aided tool that enables an end user to simply provide
their detailed resume and the desired job posting, and obtain a personalized
resume specifically tailored to that specific job posting in the matter of a
few seconds. Our proposed pipeline leverages the language understanding and
information extraction capabilities of state-of-the-art LLMs such as OpenAI's
GPT-4 and Google's Gemini, in order to (1) extract details from a job
description, (2) extract role-specific details from the user-provided resume,
and then (3) use these to refine and generate a role-specific resume for the
user. Our easy-to-use tool leverages the user-chosen LLM in a completely
off-the-shelf manner, thus requiring no fine-tuning. We demonstrate the
effectiveness of our tool via a video demo and propose novel task-specific
evaluation metrics to control for alignment and hallucination. Our tool is
available at https://job-aligned-resume.streamlit.app.
\\ ( https://arxiv.org/abs/2402.06221 ,  3522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06262
Date: Fri, 9 Feb 2024 09:20:59 GMT   (188kb,D)

Title: On the Efficacy of Eviction Policy for Key-Value Constrained Generative
  Language Model Inference
Authors: Siyu Ren, Kenny Q. Zhu
Categories: cs.CL cs.AI
\\
  Despite the recent success associated with Large Language Models~(LLMs), they
are notably cost-prohibitive to deploy in resource-constrained environments due
to their excessive memory and computational demands. In addition to model
parameters, the key-value cache is also stored in GPU memory, growing linearly
with batch size and sequence length. As a remedy, recent works have proposed
various eviction policies for maintaining the overhead of key-value cache under
a given budget. This paper embarks on the efficacy of existing eviction
policies in terms of \textit{importance score calculation} and \textit{eviction
scope construction}. We identify the deficiency of prior policies in these two
aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache
\underline{o}mission policy based on temporal attention scores and robustness
measures. Extensive experimentation spanning prefilling and auto-regressive
decoding stages validates the superiority of RoCo. Finally, we release EasyKV,
a versatile software package dedicated to user-friendly key-value constrained
generative inference. Code available at \url{https://github.com/DRSY/EasyKV}.
\\ ( https://arxiv.org/abs/2402.06262 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06332
Date: Fri, 9 Feb 2024 11:22:08 GMT   (1590kb,D)

Title: InternLM-Math: Open Math Large Language Models Toward Verifiable
  Reasoning
Authors: Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao,
  Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang,
  Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei
  Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin
Categories: cs.CL
\\
  The math abilities of large language models can represent their abstract
reasoning ability. In this paper, we introduce and open-source our math
reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We
unify chain-of-thought reasoning, reward modeling, formal reasoning, data
augmentation, and code interpreter in a unified seq2seq format and supervise
our model to be a versatile math reasoner, verifier, prover, and augmenter.
These abilities can be used to develop the next math LLMs or self-iteration.
InternLM-Math obtains open-sourced state-of-the-art performance under the
setting of in-context learning, supervised fine-tuning, and code-assisted
reasoning in various informal and formal benchmarks including GSM8K, MATH,
Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves
30.3 on the MiniF2F test set without fine-tuning. We further explore how to use
LEAN to solve math problems and study its performance under the setting of
multi-task learning which shows the possibility of using LEAN as a unified
platform for solving and proving in math. Our models, codes, and data are
released at \url{https://github.com/InternLM/InternLM-Math}.
\\ ( https://arxiv.org/abs/2402.06332 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06341
Date: Fri, 9 Feb 2024 11:34:16 GMT   (10973kb,D)

Title: RareBench: Can LLMs Serve as Rare Diseases Specialists?
Authors: Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, Ting
  Chen
Categories: cs.CL
\\
  Generalist Large Language Models (LLMs), such as GPT-4, have shown
considerable promise in various domains, including medical diagnosis. Rare
diseases, affecting approximately 300 million people worldwide, often have
unsatisfactory clinical diagnosis rates primarily due to a lack of experienced
physicians and the complexity of differentiating among many rare diseases. In
this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's
rare disease after 17 doctors failed" underscore LLMs' potential, yet
underexplored, role in clinically diagnosing rare diseases. To bridge this
research gap, we introduce RareBench, a pioneering benchmark designed to
systematically evaluate the capabilities of LLMs on 4 critical dimensions
within the realm of rare diseases. Meanwhile, we have compiled the largest
open-source dataset on rare disease patients, establishing a benchmark for
future studies in this domain. To facilitate differential diagnosis of rare
diseases, we develop a dynamic few-shot prompt methodology, leveraging a
comprehensive rare disease knowledge graph synthesized from multiple knowledge
bases, significantly enhancing LLMs' diagnostic performance. Moreover, we
present an exhaustive comparative study of GPT-4's diagnostic capabilities
against those of specialist physicians. Our experimental findings underscore
the promising potential of integrating LLMs into the clinical diagnostic
process for rare diseases. This paves the way for exciting possibilities in
future advancements in this field.
\\ ( https://arxiv.org/abs/2402.06341 ,  10973kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06342
Date: Fri, 9 Feb 2024 11:34:39 GMT   (6632kb)

Title: Promoting Target Data in Context-aware Neural Machine Translation
Authors: Harritxu Gete and Thierry Etchegoyhen
Categories: cs.CL
\\
  Standard context-aware neural machine translation (NMT) typically relies on
parallel document-level data, exploiting both source and target contexts.
Concatenation-based approaches in particular, still a strong baseline for
document-level NMT, prepend source and/or target context sentences to the
sentences to be translated, with model variants that exploit equal amounts of
source and target data on each side achieving state-of-the-art results. In this
work, we investigate whether target data should be further promoted within
standard concatenation-based approaches, as most document-level phenomena rely
on information that is present on the target language side. We evaluate novel
concatenation-based variants where the target context is prepended to the
source language, either in isolation or in combination with the source context.
Experimental results in English-Russian and Basque-Spanish show that including
target context in the source leads to large improvements on target language
phenomena. On source-dependent phenomena, using only target language context in
the source achieves parity with state-of-the-art concatenation approaches, or
slightly underperforms, whereas combining source and target context on the
source side leads to significant gains across the board.
\\ ( https://arxiv.org/abs/2402.06342 ,  6632kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06420
Date: Fri, 9 Feb 2024 14:08:23 GMT   (6610kb,D)

Title: Findings of the First Workshop on Simulating Conversational Intelligence
  in Chat
Authors: Yvette Graham, Mohammed Rameez Qureshi, Haider Khalid, Gerasimos
  Lampouras, Ignacio Iacobacci, Qun Liu
Categories: cs.CL
\\
  The aim of this workshop is to bring together experts working on open-domain
dialogue research. In this speedily advancing research area many challenges
still exist, such as learning information from conversations, engaging in
realistic and convincing simulation of human intelligence and reasoning.
SCI-CHAT follows previous workshops on open domain dialogue but with a focus on
the simulation of intelligent conversation as judged in a live human
evaluation. Models aim to include the ability to follow a challenging topic
over a multi-turn conversation, while positing, refuting and reasoning over
arguments. The workshop included both a research track and shared task. The
main goal of this paper is to provide an overview of the shared task and a link
to an additional paper that will include an in depth analysis of the shared
task results following presentation at the workshop.
\\ ( https://arxiv.org/abs/2402.06420 ,  6610kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06443
Date: Fri, 9 Feb 2024 14:39:20 GMT   (7837kb,D)

Title: Explaining Veracity Predictions with Evidence Summarization: A
  Multi-Task Model Approach
Authors: Recep Firat Cekinel and Pinar Karagoz
Categories: cs.CL
\\
  The rapid dissemination of misinformation through social media increased the
importance of automated fact-checking. Furthermore, studies on what deep neural
models pay attention to when making predictions have increased in recent years.
While significant progress has been made in this field, it has not yet reached
a level of reasoning comparable to human reasoning. To address these gaps, we
propose a multi-task explainable neural model for misinformation detection.
Specifically, this work formulates an explanation generation process of the
model's veracity prediction as a text summarization problem. Additionally, the
performance of the proposed model is discussed on publicly available datasets
and the findings are evaluated with related studies.
\\ ( https://arxiv.org/abs/2402.06443 ,  7837kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06492
Date: Fri, 9 Feb 2024 15:53:15 GMT   (1653kb,D)

Title: Inducing Systematicity in Transformers by Attending to Structurally
  Quantized Embeddings
Authors: Yichen Jiang, Xiang Zhou, Mohit Bansal
Categories: cs.CL cs.AI cs.LG
Comments: 22 pages, code: https://github.com/jiangycTarheel/SQ-Transformer
\\
  Transformers generalize to novel compositions of structures and entities
after being trained on a complex dataset, but easily overfit on datasets of
insufficient complexity. We observe that when the training set is sufficiently
complex, the model encodes sentences that have a common syntactic structure
using a systematic attention pattern. Inspired by this observation, we propose
SQ-Transformer (Structurally Quantized) that explicitly encourages
systematicity in the embeddings and attention layers, even with a training set
of low complexity. At the embedding level, we introduce Structure-oriented
Vector Quantization (SoVQ) to cluster word embeddings into several classes of
structurally equivalent entities. At the attention level, we devise the
Systematic Attention Layer (SAL) and an alternative, Systematically Regularized
Layer (SRL) that operate on the quantized word embeddings so that sentences of
the same structure are encoded with invariant or similar attention patterns.
Empirically, we show that SQ-Transformer achieves stronger compositional
generalization than the vanilla Transformer on multiple low-complexity semantic
parsing and machine translation datasets. In our analysis, we show that SoVQ
indeed learns a syntactically clustered embedding space and SAL/SRL induces
generalizable attention patterns, which lead to improved systematicity.
\\ ( https://arxiv.org/abs/2402.06492 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06509
Date: Fri, 9 Feb 2024 16:15:30 GMT   (8672kb,D)

Title: Asking the Right Question at the Right Time: Human and Model Uncertainty
  Guidance to Ask Clarification Questions
Authors: Alberto Testoni and Raquel Fern\'andez
Categories: cs.CL cs.AI
Comments: Accepted at EACL 2024
\\
  Clarification questions are an essential dialogue tool to signal
misunderstanding, ambiguities, and under-specification in language use. While
humans are able to resolve uncertainty by asking questions since childhood,
modern dialogue systems struggle to generate effective questions. To make
progress in this direction, in this work we take a collaborative dialogue task
as a testbed and study how model uncertainty relates to human uncertainty -- an
as yet under-explored problem. We show that model uncertainty does not mirror
human clarification-seeking behavior, which suggests that using human
clarification questions as supervision for deciding when to ask may not be the
most effective way to resolve model uncertainty. To address this issue, we
propose an approach to generating clarification questions based on model
uncertainty estimation, compare it to several alternatives, and show that it
leads to significant improvements in terms of task success. Our findings
highlight the importance of equipping dialogue systems with the ability to
assess their own uncertainty and exploit in interaction.
\\ ( https://arxiv.org/abs/2402.06509 ,  8672kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06544
Date: Fri, 9 Feb 2024 17:00:32 GMT   (8934kb,D)

Title: Calibrating Long-form Generations from Large Language Models
Authors: Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, Bhuwan
  Dhingra
Categories: cs.CL cs.AI cs.LG
\\
  To enhance Large Language Models' (LLMs) reliability, calibration is
essential -- the model's assessed confidence scores should align with the
actual likelihood of its responses being correct. However, current confidence
elicitation methods and calibration metrics typically rely on a binary
true/false assessment of response correctness. This approach does not apply to
long-form generation, where an answer can be partially correct. Addressing this
gap, we introduce a unified calibration framework, in which both the
correctness of the LLMs' responses and their associated confidence levels are
treated as distributions across a range of scores. Within this framework, we
develop three metrics to precisely evaluate LLM calibration and further propose
two confidence elicitation methods based on self-consistency and
self-evaluation. Our experiments, which include long-form QA and summarization
tasks, demonstrate that larger models don't necessarily guarantee better
calibration, that calibration performance is found to be metric-dependent, and
that self-consistency methods excel in factoid datasets. We also find that
calibration can be enhanced through techniques such as fine-tuning, integrating
relevant source documents, scaling the temperature, and combining
self-consistency with self-evaluation. Lastly, we showcase a practical
application of our system: selecting and cascading open-source models and
ChatGPT to optimize correctness given a limited API budget. This research not
only challenges existing notions of LLM calibration but also offers practical
methodologies for improving trustworthiness in long-form generation.
\\ ( https://arxiv.org/abs/2402.06544 ,  8934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06549
Date: Fri, 9 Feb 2024 17:02:41 GMT   (6700kb,D)

Title: Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection
  via Retrieval-Augmented GPT-4 and LLaMA
Authors: Marek \v{S}uppa and Daniel Skala and Daniela Ja\v{s}\v{s} and Samuel
  Su\v{c}\'ik and Andrej \v{S}vec and Peter Hra\v{s}ka
Categories: cs.CL cs.AI
Comments: Accepted to the 7th Workshop on Challenges and Applications of
  Automated Extraction of Socio-political Events from Text (CASE 2024)
\\
  This study details our approach for the CASE 2024 Shared Task on Climate
Activism Stance and Hate Event Detection, focusing on Hate Speech Detection,
Hate Speech Target Identification, and Stance Detection as classification
challenges. We explored the capability of Large Language Models (LLMs),
particularly GPT-4, in zero- or few-shot settings enhanced by retrieval
augmentation and re-ranking for Tweet classification. Our goal was to determine
if LLMs could match or surpass traditional methods in this context.
  We conducted an ablation study with LLaMA for comparison, and our results
indicate that our models significantly outperformed the baselines, securing
second place in the Target Detection task. The code for our submission is
available at https://github.com/NaiveNeuron/bryndza-case-2024
\\ ( https://arxiv.org/abs/2402.06549 ,  6700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06584
Date: Fri, 9 Feb 2024 18:05:03 GMT   (429kb,D)

Title: G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German
Authors: Ehsan Latif, Gyeong-Geon Lee, Knut Neuman, Tamara Kastorff, and
  Xiaoming Zhai
Categories: cs.CL cs.AI
Comments: First German Science Education LLM, Submitted to AIED2024
\\
  The advancement of natural language processing has paved the way for
automated scoring systems in various languages, such as German (e.g., German
BERT [G-BERT]). Automatically scoring written responses to science questions in
German is a complex task and challenging for standard G-BERT as they lack
contextual knowledge in the science domain and may be unaligned with student
writing styles. This paper developed a contextualized German Science Education
BERT (G-SciEdBERT), an innovative large language model tailored for scoring
German-written responses to science tasks. Using G-BERT, we pre-trained
G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens
to the Programme for International Student Assessment (PISA) 2015. We
fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring
accuracy. We then compared its performance with G-BERT. Our findings reveal a
substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a
10% increase of quadratic weighted kappa compared to G-BERT (mean accuracy
difference = 0.096, SD = 0.024). These insights underline the significance of
specialized language models like G-SciEdBERT, which is trained to enhance the
accuracy of automated scoring, offering a substantial contribution to the field
of AI in education.
\\ ( https://arxiv.org/abs/2402.06584 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06592
Date: Fri, 9 Feb 2024 18:12:11 GMT   (427kb,D)

Title: Self-consistent context aware conformer transducer for speech
  recognition
Authors: Konstantin Kolokolov, Pavel Pekichev, Karthik Raghunathan
Categories: cs.CL cs.SD eess.AS
\\
  We propose a novel neural network architecture based on conformer transducer
that adds contextual information flow to the ASR systems. Our method improves
the accuracy of recognizing uncommon words while not harming the word error
rate of regular words. We explore the uncommon words accuracy improvement when
we use the new model and/or shallow fusion with context language model. We
found that combination of both provides cumulative gain in uncommon words
recognition accuracy.
\\ ( https://arxiv.org/abs/2402.06592 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06608
Date: Fri, 9 Feb 2024 18:39:13 GMT   (152kb,D)

Title: TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and
  logical intermediate representations
Authors: Sudhir Agarwal and Anu Sreepathy
Categories: cs.CL cs.AI
Comments: 20 pages (7 main + 2 references + 11 appendix), 4 figures, 2 tables
\\
  We study the problem of generating plans for given natural language planning
task requests. On one hand, LLMs excel at natural language processing but do
not perform well on planning. On the other hand, classical planning tools excel
at planning tasks but require input in a structured language such as the
Planning Domain Definition Language (PDDL). We leverage the strengths of both
the techniques by using an LLM for generating the PDDL representation (task
PDDL) of planning task requests followed by using a classical planner for
computing a plan. Unlike previous approaches that use LLMs for generating task
PDDLs directly, our approach comprises of (a) translate: using an LLM only for
generating a logically interpretable intermediate representation of natural
language task descriptions, (b) infer: deriving additional logically dependent
information from the intermediate representation using a logic reasoner
(currently, Answer Set Programming solver), and (c) compile: generating the
target task PDDL from the base and inferred information. We observe that using
an LLM to only output the intermediate representation significantly reduces LLM
errors. Consequently, TIC approach achieves, for at least one LLM, high
accuracy on task PDDL generation for all seven domains of our evaluation
dataset.
\\ ( https://arxiv.org/abs/2402.06608 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06617
Date: Fri, 9 Feb 2024 18:50:51 GMT   (665kb,D)

Title: FaBERT: Pre-training BERT on Persian Blogs
Authors: Mostafa Masumi, Seyed Soroush Majd, Mehrnoush Shamsfard, Hamid Beigy
Categories: cs.CL
\\
  We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs
corpus, encompassing both informal and formal Persian texts. FaBERT is designed
to excel in traditional Natural Language Understanding (NLU) tasks, addressing
the intricacies of diverse sentence structures and linguistic styles prevalent
in the Persian language. In our comprehensive evaluation of FaBERT on 12
datasets in various downstream tasks, encompassing Sentiment Analysis (SA),
Named Entity Recognition (NER), Natural Language Inference (NLI), Question
Answering (QA), and Question Paraphrasing (QP), it consistently demonstrated
improved performance, all achieved within a compact model size. The findings
highlight the importance of utilizing diverse and cleaned corpora, such as
HmBlogs, to enhance the performance of language models like BERT in Persian
Natural Language Processing (NLP) applications. FaBERT is openly accessible at
https://huggingface.co/sbunlp/fabert
\\ ( https://arxiv.org/abs/2402.06617 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06619
Date: Fri, 9 Feb 2024 18:51:49 GMT   (9679kb,D)

Title: Aya Dataset: An Open-Access Collection for Multilingual Instruction
  Tuning
Authors: Shivalika Singh, Freddie Vargus, Daniel Dsouza, B\"orje F. Karlsson,
  Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas
  Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson,
  Marina Machado, Luisa Souza Moura, Dominik Krzemi\'nski, Hakimeh Fadaei, Irem
  Erg\"un, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu
  Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian
  Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet \"Ust\"un,
  Marzieh Fadaee, Sara Hooker
Categories: cs.CL cs.AI
\\
  Datasets are foundational to many breakthroughs in modern artificial
intelligence. Many recent achievements in the space of natural language
processing (NLP) can be attributed to the finetuning of pre-trained models on a
diverse set of tasks that enables a large language model (LLM) to respond to
instructions. Instruction fine-tuning (IFT) requires specifically constructed
and annotated datasets. However, existing datasets are almost all in the
English language. In this work, our primary goal is to bridge the language gap
by building a human-curated instruction-following dataset spanning 65
languages. We worked with fluent speakers of languages from around the world to
collect natural instances of instructions and completions. Furthermore, we
create the most extensive multilingual collection to date, comprising 513
million instances through templating and translating existing datasets across
114 languages. In total, we contribute four key resources: we develop and
open-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,
and the Aya Evaluation Suite. The Aya initiative also serves as a valuable case
study in participatory research, involving collaborators from 119 countries. We
see this as a valuable framework for future research collaborations that aim to
bridge gaps in resources.
\\ ( https://arxiv.org/abs/2402.06619 ,  9679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06625
Date: Fri, 9 Feb 2024 18:57:08 GMT   (1014kb,D)

Title: Understanding the Effects of Iterative Prompting on Truthfulness
Authors: Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju
Categories: cs.CL
\\
  The development of Large Language Models (LLMs) has notably transformed
numerous sectors, offering impressive text generation capabilities. Yet, the
reliability and truthfulness of these models remain pressing concerns. To this
end, we investigate iterative prompting, a strategy hypothesized to refine LLM
responses, assessing its impact on LLM truthfulness, an area which has not been
thoroughly explored. Our extensive experiments delve into the intricacies of
iterative prompting variants, examining their influence on the accuracy and
calibration of model responses. Our findings reveal that naive prompting
methods significantly undermine truthfulness, leading to exacerbated
calibration errors. In response to these challenges, we introduce several
prompting variants designed to address the identified issues. These variants
demonstrate marked improvements over existing baselines, signaling a promising
direction for future research. Our work provides a nuanced understanding of
iterative prompting and introduces novel approaches to enhance the truthfulness
of LLMs, thereby contributing to the development of more accurate and
trustworthy AI systems.
\\ ( https://arxiv.org/abs/2402.06625 ,  1014kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05940
Date: Wed, 24 Jan 2024 22:45:34 GMT   (1411kb)

Title: Causal Relationship Network of Risk Factors Impacting Workday Loss in
  Underground Coal Mines
Authors: Shangsi Ren, Cameron A. Beeche, Zhiyi Shi, Maria Acevedo Garcia,
  Katherine Zychowski, Shuguang Leng, Pedram Roghanchi, Jiantao Pu
Categories: cs.LG cs.AI stat.ME
Comments: 5 figures 5 tables
\\
  This study aims to establish the causal relationship network between various
factors leading to workday loss in underground coal mines using a novel causal
artificial intelligence (AI) method. The analysis utilizes data obtained from
the National Institute for Occupational Safety and Health (NIOSH). A total of
101,010 injury records from 3,982 unique underground coal mines spanning the
years from 1990 to 2020 were extracted from the NIOSH database. Causal
relationships were analyzed and visualized using a novel causal AI method
called Grouped Greedy Equivalence Search (GGES). The impact of each variable on
workday loss was assessed through intervention do-calculus adjustment (IDA)
scores. Model training and validation were performed using the 10-fold
cross-validation technique. Performance metrics, including adjacency precision
(AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall
(AHR), were utilized to evaluate the models. Findings revealed that after 2006,
key direct causes of workday loss among mining employees included total mining
experience, mean office employees, mean underground employees, county, and
total mining experience (years). Total mining experience emerged as the most
influential factor, whereas mean employees per mine exhibited the least
influence. The analyses emphasized the significant role of total mining
experience in determining workday loss. The models achieved optimal
performance, with AP, AR, AHP, and AHR values measuring 0.694, 0.653, 0.386,
and 0.345, respectively. This study demonstrates the feasibility of utilizing
the new GGES method to clarify the causal factors behind the workday loss by
analyzing employment demographics and injury records and establish their causal
relationship network.
\\ ( https://arxiv.org/abs/2402.05940 ,  1411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05942
Date: Fri, 2 Feb 2024 17:31:50 GMT   (3044kb,D)

Title: Cooperative Knowledge Distillation: A Learner Agnostic Approach
Authors: Michael Livanos, Ian Davidson, Stephen Wong
Categories: cs.LG cs.AI
Comments: 8 pages, 7 figures, AAAI24
\\
  Knowledge distillation is a simple but powerful way to transfer knowledge
between a teacher model to a student model. Existing work suffers from at least
one of the following key limitations in terms of direction and scope of
transfer which restrict its use: all knowledge is transferred from teacher to
student regardless of whether or not that knowledge is useful, the student is
the only one learning in this exchange, and typically distillation transfers
knowledge only from a single teacher to a single student. We formulate a novel
form of knowledge distillation in which many models can act as both students
and teachers which we call cooperative distillation. The models cooperate as
follows: a model (the student) identifies specific deficiencies in it's
performance and searches for another model (the teacher) who encodes learned
knowledge into instructional virtual instances via counterfactual instance
generation. Because different models may have different strengths and
weaknesses, all models can act as either students or teachers (cooperation)
when appropriate and only distill knowledge in areas specific to their
strengths (focus). Since counterfactuals as a paradigm are not tied to any
specific algorithm, we can use this method to distill knowledge between
learners of different architectures, algorithms, and even feature spaces. We
demonstrate that our approach not only outperforms baselines such as transfer
learning, self-supervised learning, and multiple knowledge distillation
algorithms on several datasets, but it can also be used in settings where the
aforementioned techniques cannot.
\\ ( https://arxiv.org/abs/2402.05942 ,  3044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05943
Date: Fri, 2 Feb 2024 20:41:55 GMT   (410kb)

Title: A hybrid IndRNNLSTM approach for real-time anomaly detection in
  software-defined networks
Authors: Sajjad Salem, Salman Asoudeh
Categories: cs.LG cs.AI cs.NI
\\
  Anomaly detection in SDN using data flow prediction is a difficult task. This
problem is included in the category of time series and regression problems.
Machine learning approaches are challenging in this field due to the manual
selection of features. On the other hand, deep learning approaches have
important features due to the automatic selection of features. Meanwhile,
RNN-based approaches have been used the most. The LSTM and GRU approaches learn
dependent entities well; on the other hand, the IndRNN approach learns
non-dependent entities in time series. The proposed approach tried to use a
combination of IndRNN and LSTM approaches to learn dependent and non-dependent
features. Feature selection approaches also provide a suitable view of features
for the models; for this purpose, four feature selection models, Filter,
Wrapper, Embedded, and Autoencoder were used. The proposed IndRNNLSTM
algorithm, in combination with Embedded, was able to achieve MAE=1.22 and
RMSE=9.92 on NSL-KDD data.
\\ ( https://arxiv.org/abs/2402.05943 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05944
Date: Fri, 2 Feb 2024 23:05:30 GMT   (1582kb,D)

Title: Todyformer: Towards Holistic Dynamic Graph Transformers with
  Structure-Aware Tokenization
Authors: Mahdi Biparva, Raika Karimi, Faezeh Faez, Yingxue Zhang
Categories: cs.LG
\\
  Temporal Graph Neural Networks have garnered substantial attention for their
capacity to model evolving structural and temporal patterns while exhibiting
impressive performance. However, it is known that these architectures are
encumbered by issues that constrain their performance, such as over-squashing
and over-smoothing. Meanwhile, Transformers have demonstrated exceptional
computational capacity to effectively address challenges related to long-range
dependencies. Consequently, we introduce Todyformer-a novel Transformer-based
neural network tailored for dynamic graphs. It unifies the local encoding
capacity of Message-Passing Neural Networks (MPNNs) with the global encoding of
Transformers through i) a novel patchifying paradigm for dynamic graphs to
improve over-squashing, ii) a structure-aware parametric tokenization strategy
leveraging MPNNs, iii) a Transformer with temporal positional-encoding to
capture long-range dependencies, and iv) an encoding architecture that
alternates between local and global contextualization, mitigating
over-smoothing in MPNNs. Experimental evaluations on public benchmark datasets
demonstrate that Todyformer consistently outperforms the state-of-the-art
methods for downstream tasks. Furthermore, we illustrate the underlying aspects
of the proposed model in effectively capturing extensive temporal dependencies
in dynamic graphs.
\\ ( https://arxiv.org/abs/2402.05944 ,  1582kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05945
Date: Sat, 3 Feb 2024 03:50:58 GMT   (1503kb,D)

Title: Eliminating Information Leakage in Hard Concept Bottleneck Models with
  Supervised, Hierarchical Concept Learning
Authors: Ao Sun, Yuanyuan Yuan, Pingchuan Ma, and Shuai Wang
Categories: cs.LG cs.AI
\\
  Concept Bottleneck Models (CBMs) aim to deliver interpretable and
interventionable predictions by bridging features and labels with
human-understandable concepts. While recent CBMs show promising potential, they
suffer from information leakage, where unintended information beyond the
concepts (either when concepts are represented with probabilities or binary
states) are leaked to the subsequent label prediction. Consequently, distinct
classes are falsely classified via indistinguishable concepts, undermining the
interpretation and intervention of CBMs.
  This paper alleviates the information leakage issue by introducing label
supervision in concept predication and constructing a hierarchical concept set.
Accordingly, we propose a new paradigm of CBMs, namely SupCBM, which achieves
label predication via predicted concepts and a deliberately-designed
intervention matrix. SupCBM focuses on concepts that are mostly relevant to the
predicted label and only distinguishes classes when different concepts are
presented. Our evaluations show that SupCBM outperforms SOTA CBMs over diverse
datasets. It also manifests better generality across different backbone models.
With proper quantification of information leakage in different CBMs, we
demonstrate that SupCBM significantly reduces the information leakage.
\\ ( https://arxiv.org/abs/2402.05945 ,  1503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05946
Date: Sat, 3 Feb 2024 06:21:33 GMT   (1999kb,D)

Title: Unveiling Latent Causal Rules: A Temporal Point Process Approach for
  Abnormal Event Explanation
Authors: Yiling Kuang, Chao Yang, Yang Yang, Shuang Li
Categories: cs.LG cs.AI
Comments: Accepted by AISTATS 2024
\\
  In high-stakes systems such as healthcare, it is critical to understand the
causal reasons behind unusual events, such as sudden changes in patient's
health. Unveiling the causal reasons helps with quick diagnoses and precise
treatment planning. In this paper, we propose an automated method for
uncovering "if-then" logic rules to explain observational events. We introduce
temporal point processes to model the events of interest, and discover the set
of latent rules to explain the occurrence of events. To achieve this, we employ
an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the
likelihood of each event being explained by each discovered rule. In the
M-step, we update both the rule set and model parameters to enhance the
likelihood function's lower bound. Notably, we optimize the rule set in a
differential manner. Our approach demonstrates accurate performance in both
discovering rules and identifying root causes. We showcase its promising
results using synthetic and real healthcare datasets.
\\ ( https://arxiv.org/abs/2402.05946 ,  1999kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05947
Date: Sat, 3 Feb 2024 11:10:57 GMT   (37619kb,D)

Title: Separable Multi-Concept Erasure from Diffusion Models
Authors: Mengnan Zhao, Lihe Zhang, Tianhang Zheng, Yuqiu Kong and Baocai Yin
Categories: cs.LG cs.CV
\\
  Large-scale diffusion models, known for their impressive image generation
capabilities, have raised concerns among researchers regarding social impacts,
such as the imitation of copyrighted artistic styles. In response, existing
approaches turn to machine unlearning techniques to eliminate unsafe concepts
from pre-trained models. However, these methods compromise the generative
performance and neglect the coupling among multi-concept erasures, as well as
the concept restoration problem. To address these issues, we propose a
Separable Multi-concept Eraser (SepME), which mainly includes two parts: the
generation of concept-irrelevant representations and the weight decoupling. The
former aims to avoid unlearning substantial information that is irrelevant to
forgotten concepts. The latter separates optimizable model weights, making each
weight increment correspond to a specific concept erasure without affecting
generative performance on other concepts. Specifically, the weight increment
for erasing a specified concept is formulated as a linear combination of
solutions calculated based on other known undesirable concepts. Extensive
experiments indicate the efficacy of our approach in eliminating concepts,
preserving model performance, and offering flexibility in the erasure or
recovery of various concepts.
\\ ( https://arxiv.org/abs/2402.05947 ,  37619kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05948
Date: Sat, 3 Feb 2024 15:51:17 GMT   (1780kb,D)

Title: DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on
  Prototypical Networks
Authors: Jianing He, Qi Zhang, Weiping Ding, Duoqian Miao, Jun Zhao, Liang Hu,
  Longbing Cao
Categories: cs.LG cs.CL
Comments: 16 pages
\\
  Early exiting has demonstrated its effectiveness in accelerating the
inference of pre-trained language models like BERT by dynamically adjusting the
number of layers executed. However, most existing early exiting methods only
consider local information from an individual test sample to determine their
exiting indicators, failing to leverage the global information offered by
sample population. This leads to suboptimal estimation of prediction
correctness, resulting in erroneous exiting decisions. To bridge the gap, we
explore the necessity of effectively combining both local and global
information to ensure reliable early exiting during inference. Purposefully, we
leverage prototypical networks to learn class prototypes and devise a distance
metric between samples and class prototypes. This enables us to utilize global
information for estimating the correctness of early predictions. On this basis,
we propose a novel Distance-Enhanced Early Exiting framework for BERT
(DE$^3$-BERT). DE$^3$-BERT implements a hybrid exiting strategy that
supplements classic entropy-based local information with distance-based global
information to enhance the estimation of prediction correctness for more
reliable early exiting decisions. Extensive experiments on the GLUE benchmark
demonstrate that DE$^3$-BERT consistently outperforms state-of-the-art models
under different speed-up ratios with minimal storage or computational overhead,
yielding a better trade-off between model performance and inference efficiency.
Additionally, an in-depth analysis further validates the generality and
interpretability of our method.
\\ ( https://arxiv.org/abs/2402.05948 ,  1780kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05949
Date: Sat, 3 Feb 2024 20:50:48 GMT   (954kb)

Title: An explainable machine learning-based approach for analyzing customers'
  online data to identify the importance of product attributes
Authors: Aigin Karimzadeh, Amir Zakery, Mohammadreza Mohammadi, Ali Yavari
Categories: cs.LG cs.GT
\\
  Online customer data provides valuable information for product design and
marketing research, as it can reveal the preferences of customers. However,
analyzing these data using artificial intelligence (AI) for data-driven design
is a challenging task due to potential concealed patterns. Moreover, in these
research areas, most studies are only limited to finding customers' needs. In
this study, we propose a game theory machine learning (ML) method that extracts
comprehensive design implications for product development. The method first
uses a genetic algorithm to select, rank, and combine product features that can
maximize customer satisfaction based on online ratings. Then, we use SHAP
(SHapley Additive exPlanations), a game theory method that assigns a value to
each feature based on its contribution to the prediction, to provide a
guideline for assessing the importance of each feature for the total
satisfaction. We apply our method to a real-world dataset of laptops from
Kaggle, and derive design implications based on the results. Our approach
tackles a major challenge in the field of multi-criteria decision making and
can help product designers and marketers, to understand customer preferences
better with less data and effort. The proposed method outperforms benchmark
methods in terms of relevant performance metrics.
\\ ( https://arxiv.org/abs/2402.05949 ,  954kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05950
Date: Sat, 3 Feb 2024 21:36:22 GMT   (423kb,D)

Title: \textit{SQT} -- \textit{std} $Q$-target
Authors: Nitsan Soffair, Dotan Di-Castro, Orly Avner, Shie Mannor
Categories: cs.LG cs.AI
\\
  \textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble,
$Q$-learning-based algorithm, which is based on a single key $Q$-formula:
$Q$-networks standard deviation, which is an "uncertainty penalty", and, serves
as a minimalistic solution to the problem of \textit{overestimation} bias. We
implement \textit{SQT} on top of TD3/TD7 code and test it against the
state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven
popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s
$Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a
\textit{conservative} solution to overestimation bias in RL, while \textit{SQT}
shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on
all tasks.
\\ ( https://arxiv.org/abs/2402.05950 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05951
Date: Sat, 3 Feb 2024 21:58:06 GMT   (798kb,D)

Title: \textit{MinMaxMin} $Q$-learning
Authors: Nitsan Soffair, Shie Mannor
Categories: cs.LG cs.AI
\\
  \textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic
algorithm that addresses the problem of \textit{overestimation} bias
($Q$-estimations are overestimating the real $Q$-values) inherent in
\textit{conservative} RL algorithms. Its core formula relies on the
disagreement among $Q$-networks in the form of the min-batch MaxMin
$Q$-networks distance which is added to the $Q$-target and used as the priority
experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3
and TD7, subjecting it to rigorous testing against state-of-the-art
continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet
environments. The results show a consistent performance improvement of
\textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
\\ ( https://arxiv.org/abs/2402.05951 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05952
Date: Sun, 4 Feb 2024 05:51:14 GMT   (180kb,D)

Title: Advancing Graph Representation Learning with Large Language Models: A
  Comprehensive Survey of Techniques
Authors: Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, Jianling Sun
Categories: cs.LG cs.AI cs.CL
\\
  The integration of Large Language Models (LLMs) with Graph Representation
Learning (GRL) marks a significant evolution in analyzing complex data
structures. This collaboration harnesses the sophisticated linguistic
capabilities of LLMs to improve the contextual understanding and adaptability
of graph models, thereby broadening the scope and potential of GRL. Despite a
growing body of research dedicated to integrating LLMs into the graph domain, a
comprehensive review that deeply analyzes the core components and operations
within these models is notably lacking. Our survey fills this gap by proposing
a novel taxonomy that breaks down these models into primary components and
operation techniques from a novel technical perspective. We further dissect
recent literature into two primary components including knowledge extractors
and organizers, and two operation techniques including integration and training
stratigies, shedding light on effective model design and training strategies.
Additionally, we identify and explore potential future research avenues in this
nascent yet underexplored field, proposing paths for continued progress.
\\ ( https://arxiv.org/abs/2402.05952 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05954
Date: Sun, 4 Feb 2024 09:25:07 GMT   (13722kb,D)

Title: EasyFS: an Efficient Model-free Feature Selection Framework via Elastic
  Transformation of Features
Authors: Jianming Lv, Sijun Xia, Depin Liang, Wei Chen
Categories: cs.LG
\\
  Traditional model-free feature selection methods treat each feature
independently while disregarding the interrelationships among features, which
leads to relatively poor performance compared with the model-aware methods. To
address this challenge, we propose an efficient model-free feature selection
framework via elastic expansion and compression of the features, namely EasyFS,
to achieve better performance than state-of-the-art model-aware methods while
sharing the characters of efficiency and flexibility with the existing
model-free methods. In particular, EasyFS expands the feature space by using
the random non-linear projection network to achieve the non-linear combinations
of the original features, so as to model the interrelationships among the
features and discover most correlated features. Meanwhile, a novel redundancy
measurement based on the change of coding rate is proposed for efficient
filtering of redundant features. Comprehensive experiments on 21 different
datasets show that EasyFS outperforms state-of-the art methods up to 10.9\% in
the regression tasks and 5.7\% in the classification tasks while saving more
than 94\% of the time.
\\ ( https://arxiv.org/abs/2402.05954 ,  13722kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05955
Date: Sun, 4 Feb 2024 10:21:03 GMT   (15223kb,D)

Title: A Hyper-Transformer model for Controllable Pareto Front Learning with
  Split Feasibility Constraints
Authors: Tran Anh Tuan, Nguyen Viet Dung, Tran Ngoc Thang
Categories: cs.LG math.OC
\\
  Controllable Pareto front learning (CPFL) approximates the Pareto solution
set and then locates a Pareto optimal solution with respect to a given
reference vector. However, decision-maker objectives were limited to a
constraint region in practice, so instead of training on the entire decision
space, we only trained on the constraint region. Controllable Pareto front
learning with Split Feasibility Constraints (SFC) is a way to find the best
Pareto solutions to a split multi-objective optimization problem that meets
certain constraints. In the previous study, CPFL used a Hypernetwork model
comprising multi-layer perceptron (Hyper-MLP) blocks. With the substantial
advancement of transformer architecture in deep learning, transformers can
outperform other architectures in various tasks. Therefore, we have developed a
hyper-transformer (Hyper-Trans) model for CPFL with SFC. We use the theory of
universal approximation for the sequence-to-sequence function to show that the
Hyper-Trans model makes MED errors smaller in computational experiments than
the Hyper-MLP model.
\\ ( https://arxiv.org/abs/2402.05955 ,  15223kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05956
Date: Sun, 4 Feb 2024 15:33:58 GMT   (3559kb,D)

Title: Pathformer: Multi-scale transformers with Adaptive Pathways for Time
  Series Forecasting
Authors: Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang,
  Qingsong Wen, Bin Yang, Chenjuan Guo
Categories: cs.LG
\\
  Transformer-based models have achieved some success in time series
forecasting. Existing methods mainly model time series from limited or fixed
scales, making it challenging to capture different characteristics spanning
various scales. In this paper, we propose multi-scale transformers with
adaptive pathways (Pathformer). The proposed Transformer integrates both
temporal resolution and temporal distance for multi-scale modeling. Multi-scale
division divides the time series into different temporal resolutions using
patches of various sizes. Based on the division of each scale, dual attention
is performed over these patches to capture global correlations and local
details as temporal dependencies. We further enrich the multi-scale transformer
with adaptive pathways, which adaptively adjust the multi-scale modeling
process based on the varying temporal dynamics in the input time series,
improving the prediction accuracy and generalization of Pathformer. Extensive
experiments on eleven real-world datasets demonstrate that Pathformer not only
achieves state-of-the-art performance by surpassing all current models but also
exhibits stronger generalization abilities under various transfer scenarios.
\\ ( https://arxiv.org/abs/2402.05956 ,  3559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05957
Date: Sun, 4 Feb 2024 17:28:27 GMT   (438kb,D)

Title: Accelerating PDE Data Generation via Differential Operator Action in
  Solution Space
Authors: Huanshuo Dong, Hong Wang, Haoyang Liu, Jian Luo, Jie Wang
Categories: cs.LG
\\
  Recent advancements in data-driven approaches, such as Neural Operator (NO),
have demonstrated their effectiveness in reducing the solving time of Partial
Differential Equations (PDEs). However, one major challenge faced by these
approaches is the requirement for a large amount of high-precision training
data, which needs significant computational costs during the generation
process. To address this challenge, we propose a novel PDE dataset generation
algorithm, namely Differential Operator Action in Solution space (DiffOAS),
which speeds up the data generation process and enhances the precision of the
generated data simultaneously. Specifically, DiffOAS obtains a few basic PDE
solutions and then combines them to get solutions. It applies differential
operators on these solutions, a process we call 'operator action', to
efficiently generate precise PDE data points. Theoretical analysis shows that
the time complexity of DiffOAS method is one order lower than the existing
generation method. Experimental results show that DiffOAS accelerates the
generation of large-scale datasets with 10,000 instances by 300 times. Even
with just 5% of the generation time, NO trained on the data generated by
DiffOAS exhibits comparable performance to that using the existing generation
method, which highlights the efficiency of DiffOAS.
\\ ( https://arxiv.org/abs/2402.05957 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05959
Date: Sun, 4 Feb 2024 21:43:37 GMT   (315kb,D)

Title: Nature-Inspired Local Propagation
Authors: Alessandro Betti, Marco Gori
Categories: cs.LG cond-mat.dis-nn cs.AI cs.NE
\\
  The spectacular results achieved in machine learning, including the recent
advances in generative AI, rely on large data collections. On the opposite,
intelligent processes in nature arises without the need for such collections,
but simply by online processing of the environmental information. In
particular, natural learning processes rely on mechanisms where data
representation and learning are intertwined in such a way to respect
spatiotemporal locality. This paper shows that such a feature arises from a
pre-algorithmic view of learning that is inspired by related studies in
Theoretical Physics. We show that the algorithmic interpretation of the derived
"laws of learning", which takes the structure of Hamiltonian equations, reduces
to Backpropagation when the speed of propagation goes to infinity. This opens
the doors to machine learning studies based on full on-line information
processing that are based the replacement of Backpropagation with the proposed
spatiotemporal local algorithm.
\\ ( https://arxiv.org/abs/2402.05959 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05960
Date: Mon, 5 Feb 2024 02:51:37 GMT   (13769kb,D)

Title: Phase-driven Domain Generalizable Learning for Nonstationary Time Series
Authors: Payal Mohapatra, Lixu Wang, Qi Zhu
Categories: cs.LG eess.SP
\\
  Monitoring and recognizing patterns in continuous sensing data is crucial for
many practical applications. These real-world time-series data are often
nonstationary, characterized by varying statistical and spectral properties
over time. This poses a significant challenge in developing learning models
that can effectively generalize across different distributions. In this work,
based on our observation that nonstationary statistics are intrinsically linked
to the phase information, we propose a time-series learning framework, PhASER.
It consists of three novel elements: 1) phase augmentation that diversifies
non-stationarity while preserving discriminatory semantics, 2) separate feature
encoding by viewing time-varying magnitude and phase as independent modalities,
and 3) feature broadcasting by incorporating phase with a novel residual
connection for inherent regularization to enhance distribution invariant
learning. Upon extensive evaluation on 5 datasets from human activity
recognition, sleep-stage classification, and gesture recognition against 10
state-of-the-art baseline methods, we demonstrate that PhASER consistently
outperforms the best baselines by an average of 5% and up to 13% in some cases.
Moreover, PhASER's principles can be applied broadly to boost the
generalization ability of existing time series classification models.
\\ ( https://arxiv.org/abs/2402.05960 ,  13769kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05962
Date: Mon, 5 Feb 2024 06:03:38 GMT   (5032kb,D)

Title: EXGC: Bridging Efficiency and Explainability in Graph Condensation
Authors: Junfeng Fang and Xinglin Li and Yongduo Sui and Yuan Gao and Guibin
  Zhang and Kun Wang and Xiang Wang and Xiangnan He
Categories: cs.LG
\\
  Graph representation learning on vast datasets, like web data, has made
significant strides. However, the associated computational and storage
overheads raise concerns. In sight of this, Graph condensation (GCond) has been
introduced to distill these large real datasets into a more concise yet
information-rich synthetic graph. Despite acceleration efforts, existing GCond
methods mainly grapple with efficiency, especially on expansive web data
graphs. Hence, in this work, we pinpoint two major inefficiencies of current
paradigms: (1) the concurrent updating of a vast parameter set, and (2)
pronounced parameter redundancy. To counteract these two limitations
correspondingly, we first (1) employ the Mean-Field variational approximation
for convergence acceleration, and then (2) propose the objective of Gradient
Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading
explanation techniques (e.g., GNNExplainer and GSAT) to instantiate the GDIB,
our EXGC, the Efficient and eXplainable Graph Condensation method is proposed,
which can markedly boost efficiency and inject explainability. Our extensive
evaluations across eight datasets underscore EXGC's superiority and relevance.
Code is available at https://github.com/MangoKiller/EXGC.
\\ ( https://arxiv.org/abs/2402.05962 ,  5032kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05963
Date: Mon, 5 Feb 2024 10:04:00 GMT   (2061kb,D)

Title: Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement
  Learning Using Unique Experiences
Authors: Nikhil Kumar Singh and Indranil Saha
Categories: cs.LG cs.AI cs.RO cs.SY eess.SY
\\
  Efficient utilization of the replay buffer plays a significant role in the
off-policy actor-critic reinforcement learning (RL) algorithms used for
model-free control policy synthesis for complex dynamical systems. We propose a
method for achieving sample efficiency, which focuses on selecting unique
samples and adding them to the replay buffer during the exploration with the
goal of reducing the buffer size and maintaining the independent and
identically distributed (IID) nature of the samples. Our method is based on
selecting an important subset of the set of state variables from the
experiences encountered during the initial phase of random exploration,
partitioning the state space into a set of abstract states based on the
selected important state variables, and finally selecting the experiences with
unique state-reward combination by using a kernel density estimator. We
formally prove that the off-policy actor-critic algorithm incorporating the
proposed method for unique experience accumulation converges faster than the
vanilla off-policy actor-critic algorithm. Furthermore, we evaluate our method
by comparing it with two state-of-the-art actor-critic RL algorithms on several
continuous control benchmarks available in the Gym environment. Experimental
results demonstrate that our method achieves a significant reduction in the
size of the replay buffer for all the benchmarks while achieving either faster
convergent or better reward accumulation compared to the baseline algorithms.
\\ ( https://arxiv.org/abs/2402.05963 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05964
Date: Mon, 5 Feb 2024 12:16:28 GMT   (458kb,D)

Title: A Survey on Transformer Compression
Authors: Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu,
  and Dacheng Tao
Categories: cs.LG cs.CL cs.CV
\\
  Large models based on the Transformer architecture play increasingly vital
roles in artificial intelligence, particularly within the realms of natural
language processing (NLP) and computer vision (CV). Model compression methods
reduce their memory and computational cost, which is a necessary step to
implement the transformer models on practical devices. Given the unique
architecture of transformer, featuring alternative attention and Feedforward
Neural Network (FFN) modules, specific compression techniques are required. The
efficiency of these compression methods is also paramount, as it is usually
impractical to retrain large models on the entire training dataset.This survey
provides a comprehensive review of recent compression methods, with a specific
focus on their application to transformer models. The compression methods are
primarily categorized into pruning, quantization, knowledge distillation, and
efficient architecture design. In each category, we discuss compression methods
for both CV and NLP tasks, highlighting common underlying principles. At last,
we delve into the relation between various compression methods, and discuss the
further directions in this domain.
\\ ( https://arxiv.org/abs/2402.05964 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05965
Date: Mon, 5 Feb 2024 13:03:00 GMT   (44696kb,D)

Title: Hybrid Neural Representations for Spherical Data
Authors: Hyomin Kim, Yunhui Jang, Jaeho Lee, Sungsoo Ahn
Categories: cs.LG eess.SP
Comments: 13 pages, 8 figures
\\
  In this paper, we study hybrid neural representations for spherical data, a
domain of increasing relevance in scientific research. In particular, our work
focuses on weather and climate data as well as comic microwave background (CMB)
data. Although previous studies have delved into coordinate-based neural
representations for spherical signals, they often fail to capture the intricate
details of highly nonlinear signals. To address this limitation, we introduce a
novel approach named Hybrid Neural Representations for Spherical data (HNeR-S).
Our main idea is to use spherical feature-grids to obtain positional features
which are combined with a multilayer perception to predict the target signal.
We consider feature-grids with equirectangular and hierarchical equal area
isolatitude pixelization structures that align with weather data and CMB data,
respectively. We extensively verify the effectiveness of our HNeR-S for
regression, super-resolution, temporal interpolation, and compression tasks.
\\ ( https://arxiv.org/abs/2402.05965 ,  44696kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05966
Date: Mon, 5 Feb 2024 17:06:26 GMT   (1451kb,D)

Title: Rethink Model Re-Basin and the Linear Mode Connectivity
Authors: Xingyu Qu, Samuel Horvath
Categories: cs.LG cs.AI
Comments: 40 pages
\\
  Recent studies suggest that with sufficiently wide models, most SGD solutions
can, up to permutation, converge into the same basin. This phenomenon, known as
the model re-basin regime, has significant implications for model averaging.
However, current re-basin strategies are limited in effectiveness due to a lack
of comprehensive understanding of underlying mechanisms. Addressing this gap,
our work revisits standard practices and uncovers the frequent inadequacies of
existing matching algorithms, which we show can be mitigated through proper
re-normalization. By introducing a more direct analytical approach, we expose
the interaction between matching algorithms and re-normalization processes.
This perspective not only clarifies and refines previous findings but also
facilitates novel insights. For instance, it connects the linear mode
connectivity to pruning, motivating a lightweight yet effective post-pruning
plug-in that can be directly merged with any existing pruning techniques. Our
implementation is available at https://github.com/XingyuQu/rethink-re-basin.
\\ ( https://arxiv.org/abs/2402.05966 ,  1451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05967
Date: Mon, 5 Feb 2024 18:00:07 GMT   (3699kb,D)

Title: The last Dance : Robust backdoor attack via diffusion models and
  bayesian approach
Authors: Orson Mengara
Categories: cs.LG cs.AI cs.CR eess.SP
Comments: Preprint : audio backdoor attack performed on Hugging Face's
  Transformer pre-trained models, in particular for Audios (pre-trained model).
  This attack incorporates state-of-the-art Bayesian techniques, a modified
  Fokker-Planck equation, and a diffusion model approach
\\
  Diffusion models are state-of-the-art deep learning generative models that
are trained on the principle of learning forward and backward diffusion
processes via the progressive addition of noise and denoising. In this paper,
we seek to trick audio-based DNN models, such as those in the Hugging Face
framework, for example, those that focus on audio, in particular
transformer-based artificial intelligence models, which are powerful machine
learning models that save time and deliver faster, more efficient results. We
demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on
audio transformers derived from Hugging Face, a popular framework in the world
of artificial intelligence (AI) research. The backdoor attack developed in this
paper is based on poisoning the model's training data by incorporating backdoor
diffusion sampling and a Bayesian approach to the distribution of poisoned
data.
\\ ( https://arxiv.org/abs/2402.05967 ,  3699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05968
Date: Mon, 5 Feb 2024 19:52:19 GMT   (528kb,D)

Title: Federated Learning Priorities Under the European Union Artificial
  Intelligence Act
Authors: Herbert Woisetschl\"ager, Alexander Erben, Bill Marino, Shiqiang Wang,
  Nicholas D. Lane, Ruben Mayer, Hans-Arno Jacobsen
Categories: cs.LG cs.AI cs.CY
ACM-class: I.2; I.2.11; K.5
\\
  The age of AI regulation is upon us, with the European Union Artificial
Intelligence Act (AI Act) leading the way. Our key inquiry is how this will
affect Federated Learning (FL), whose starting point of prioritizing data
privacy while performing ML fundamentally differs from that of centralized
learning. We believe the AI Act and future regulations could be the missing
catalyst that pushes FL toward mainstream adoption. However, this can only
occur if the FL community reprioritizes its research focus. In our position
paper, we perform a first-of-its-kind interdisciplinary analysis (legal and ML)
of the impact the AI Act may have on FL and make a series of observations
supporting our primary position through quantitative and qualitative analysis.
We explore data governance issues and the concern for privacy. We establish new
challenges regarding performance and energy efficiency within lifecycle
monitoring. Taken together, our analysis suggests there is a sizable
opportunity for FL to become a crucial component of AI Act-compliant ML systems
and for the new regulation to drive the adoption of FL techniques in general.
Most noteworthy are the opportunities to defend against data bias and enhance
private and secure computation
\\ ( https://arxiv.org/abs/2402.05968 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05969
Date: Tue, 6 Feb 2024 00:32:28 GMT   (21384kb,D)

Title: Breaking Symmetry When Training Transformers
Authors: Chunsheng Zuo, Michael Guerzhoy
Categories: cs.LG
\\
  As we show in this paper, the prediction for output token $n+1$ of
Transformer architectures without one of the mechanisms of positional encodings
and causal attention is invariant to permutations of input tokens $1, 2, ...,
n-1$. Usually, both mechanisms are employed and the symmetry with respect to
the input tokens is broken. Recently, it has been shown that one can train
Transformers without positional encodings. This must be enabled by the causal
attention mechanism. In this paper, we elaborate on the argument that the
causal connection mechanism must be responsible for the fact that Transformers
are able to model input sequences where the order is important. Vertical
"slices" of Transformers are all encouraged to represent the same location $k$
in the input sequence. We hypothesize that residual connections contribute to
this phenomenon, and demonstrate evidence for this.
\\ ( https://arxiv.org/abs/2402.05969 ,  21384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05970
Date: Tue, 6 Feb 2024 06:27:07 GMT   (19194kb,D)

Title: Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning
  and Levels-of-Experts
Authors: Kun Wang, Hao Wu, Guibin Zhang, Junfeng Fang, Yuxuan Liang, Yuankai
  Wu, Roger Zimmermann, Yang Wang
Categories: cs.LG cs.AI
\\
  In this paper, we address the issue of modeling and estimating changes in the
state of the spatio-temporal dynamical systems based on a sequence of
observations like video frames. Traditional numerical simulation systems depend
largely on the initial settings and correctness of the constructed partial
differential equations (PDEs). Despite recent efforts yielding significant
success in discovering data-driven PDEs with neural networks, the limitations
posed by singular scenarios and the absence of local insights prevent them from
performing effectively in a broader real-world context. To this end, this paper
propose the universal expert module -- that is, optical flow estimation
component, to capture the evolution laws of general physical processes in a
data-driven fashion. To enhance local insight, we painstakingly design a
finer-grained physical pipeline, since local characteristics may be influenced
by various internal contextual information, which may contradict the
macroscopic properties of the whole system. Further, we harness currently
popular neural discrete learning to unveil the underlying important features in
its latent space, this process better injects interpretability, which can help
us obtain a powerful prior over these discrete random variables. We conduct
extensive experiments and ablations to demonstrate that the proposed framework
achieves large performance margins, compared with the existing SOTA baselines.
\\ ( https://arxiv.org/abs/2402.05970 ,  19194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05971
Date: Tue, 6 Feb 2024 18:11:06 GMT   (169kb,D)

Title: Are we making much progress? Revisiting chemical reaction yield
  prediction from an imbalanced regression perspective
Authors: Yihong Ma, Xiaobao Huang, Bozhao Nan, Nuno Moniz, Xiangliang Zhang,
  Olaf Wiest and Nitesh V. Chawla
Categories: cs.LG physics.chem-ph
\\
  The yield of a chemical reaction quantifies the percentage of the target
product formed in relation to the reactants consumed during the chemical
reaction. Accurate yield prediction can guide chemists toward selecting
high-yield reactions during synthesis planning, offering valuable insights
before dedicating time and resources to wet lab experiments. While recent
advancements in yield prediction have led to overall performance improvement
across the entire yield range, an open challenge remains in enhancing
predictions for high-yield reactions, which are of greater concern to chemists.
In this paper, we argue that the performance gap in high-yield predictions
results from the imbalanced distribution of real-world data skewed towards
low-yield reactions, often due to unreacted starting materials and inherent
ambiguities in the reaction processes. Despite this data imbalance, existing
yield prediction methods continue to treat different yield ranges equally,
assuming a balanced training distribution. Through extensive experiments on
three real-world yield prediction datasets, we emphasize the urgent need to
reframe reaction yield prediction as an imbalanced regression problem. Finally,
we demonstrate that incorporating simple cost-sensitive re-weighting methods
can significantly enhance the performance of yield prediction models on
underrepresented high-yield regions.
\\ ( https://arxiv.org/abs/2402.05971 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05973
Date: Wed, 7 Feb 2024 12:26:56 GMT   (1571kb,D)

Title: Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL)
  Framework in UAV Networks
Authors: Sana Hafeez, Lina Mohjazi, Muhammad Ali Imran and Yao Sun
Categories: cs.LG eess.SP
Comments: 6 pages, 7 figures, 2023 IEEE International Workshop on Computer
  Aided Modeling and Design of Communication Links and Networks (IEEE CAMAD),
  Edinburgh UK
\\
  Privacy, scalability, and reliability are significant challenges in unmanned
aerial vehicle (UAV) networks as distributed systems, especially when employing
machine learning (ML) technologies with substantial data exchange. Recently,
the application of federated learning (FL) to UAV networks has improved
collaboration, privacy, resilience, and adaptability, making it a promising
framework for UAV applications. However, implementing FL for UAV networks
introduces drawbacks such as communication overhead, synchronization issues,
scalability limitations, and resource constraints. To address these challenges,
this paper presents the Blockchain-enabled Clustered and Scalable Federated
Learning (BCS-FL) framework for UAV networks. This improves the
decentralization, coordination, scalability, and efficiency of FL in
large-scale UAV networks. The framework partitions UAV networks into separate
clusters, coordinated by cluster head UAVs (CHs), to establish a connected
graph. Clustering enables efficient coordination of updates to the ML model.
Additionally, hybrid inter-cluster and intra-cluster model aggregation schemes
generate the global model after each training round, improving collaboration
and knowledge sharing among clusters. The numerical findings illustrate the
achievement of convergence while also emphasizing the trade-offs between the
effectiveness of training and communication efficiency.
\\ ( https://arxiv.org/abs/2402.05973 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05976
Date: Wed, 7 Feb 2024 22:24:09 GMT   (553kb)

Title: RankSum An unsupervised extractive text summarization based on rank
  fusion
Authors: A. Joshi, E. Fidalgo, E. Alegre, and R. Alaiz-Rodriguez
Categories: cs.LG cs.AI
\\
  In this paper, we propose Ranksum, an approach for extractive text
summarization of single documents based on the rank fusion of four
multi-dimensional sentence features extracted for each sentence: topic
information, semantic content, significant keywords, and position. The Ranksum
obtains the sentence saliency rankings corresponding to each feature in an
unsupervised way followed by the weighted fusion of the four scores to rank the
sentences according to their significance. The scores are generated in
completely unsupervised way, and a labeled document set is required to learn
the fusion weights. Since we found that the fusion weights can generalize to
other datasets, we consider the Ranksum as an unsupervised approach. To
determine topic rank, we employ probabilistic topic models whereas semantic
information is captured using sentence embeddings. To derive rankings using
sentence embeddings, we utilize Siamese networks to produce abstractive
sentence representation and then we formulate a novel strategy to arrange them
in their order of importance. A graph-based strategy is applied to find the
significant keywords and related sentence rankings in the document. We also
formulate a sentence novelty measure based on bigrams, trigrams, and sentence
embeddings to eliminate redundant sentences from the summary. The ranks of all
the sentences computed for each feature are finally fused to get the final
score for each sentence in the document. We evaluate our approach on publicly
available summarization datasets CNN/DailyMail and DUC 2002. Experimental
results show that our approach outperforms other existing state-of-the-art
summarization methods.
\\ ( https://arxiv.org/abs/2402.05976 ,  553kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05981
Date: Thu, 8 Feb 2024 08:02:57 GMT   (2461kb,D)

Title: Exploring the Impact of In-Browser Deep Learning Inference on Quality of
  User Experience and Performance
Authors: Qipeng Wang, Shiqi Jiang, Zhenpeng Chen, Xu Cao, Yuanchun Li, Aoyu Li,
  Ying Zhang, Yun Ma, Ting Cao, Xuanzhe Liu
Categories: cs.LG cs.PF
\\
  Deep Learning (DL) is increasingly being integrated into Web applications
through a method known as "in-browser inference", where the DL processes occur
directly within Web browsers. However, the actual performance of this method
and its effect on user experience quality (QoE) is not well-understood. This
gap in knowledge necessitates new forms of QoE measurement, going beyond
traditional metrics such as page load time. To address this, we conducted the
first extensive performance evaluation of in-browser inference. We introduced
new metrics for this purpose: responsiveness, smoothness, and inference
accuracy.
  Our thorough study included 9 widely-used DL models and tested them across 50
popular PC Web browsers. The findings show a significant latency issue with
in-browser inference: it's on average 16.9 times slower on CPU and 4.9 times
slower on GPU than native inference methods. Several factors contribute to this
latency, including underused hardware instruction sets, inherent delays in the
runtime environment, resource competition within the browser, and
inefficiencies in software libraries and GPU abstractions.
  Moreover, in-browser inference demands a lot of memory, sometimes up to 334.6
times more than the size of the DL models themselves. This excessive memory
usage is partly due to suboptimal memory management. Additionally, we noticed
that in-browser inference increases the time it takes for graphical user
interface (GUI) components to load in web browsers by a significant 67.2\%,
which severely impacts the overall QoE for users of web applications that
depend on this technology.
\\ ( https://arxiv.org/abs/2402.05981 ,  2461kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06010
Date: Thu, 8 Feb 2024 19:12:33 GMT   (1878kb,D)

Title: NPSVC++: Nonparallel Classifiers Encounter Representation Learning
Authors: Junhong Zhang, Zhihui Lai, Jie Zhou, Guangfei Liang
Categories: cs.LG stat.ML
\\
  This paper focuses on a specific family of classifiers called nonparallel
support vector classifiers (NPSVCs). Different from typical classifiers, the
training of an NPSVC involves the minimization of multiple objectives,
resulting in the potential concerns of feature suboptimality and class
dependency. Consequently, no effective learning scheme has been established to
improve NPSVCs' performance through representation learning, especially deep
learning. To break this bottleneck, we develop NPSVC++ based on multi-objective
optimization, enabling the end-to-end learning of NPSVC and its features. By
pursuing Pareto optimality, NPSVC++ theoretically ensures feature optimality
across classes, hence effectively overcoming the two issues above. A general
learning procedure via duality optimization is proposed, based on which we
provide two applicable instances, K-NPSVC++ and D-NPSVC++. The experiments show
their superiority over the existing methods and verify the efficacy of NPSVC++.
\\ ( https://arxiv.org/abs/2402.06010 ,  1878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06019
Date: Thu, 8 Feb 2024 19:41:38 GMT   (1057kb,D)

Title: Checking the Sufficiently Scattered Condition using a Global Non-Convex
  Optimization Software
Authors: Nicolas Gillis, Robert Luce
Categories: cs.LG eess.SP math.OC stat.ML
Comments: 14 pages, code available from https://gitlab.com/ngillis/check-ssc
\\
  The sufficiently scattered condition (SSC) is a key condition in the study of
identifiability of various matrix factorization problems, including
nonnegative, minimum-volume, symmetric, simplex-structured, and polytopic
matrix factorizations. The SSC allows one to guarantee that the computed matrix
factorization is unique/identifiable, up to trivial ambiguities. However, this
condition is NP-hard to check in general. In this paper, we show that it can
however be checked in a reasonable amount of time in realistic scenarios, when
the factorization rank is not too large. This is achieved by formulating the
problem as a non-convex quadratic optimization problem over a bounded set. We
use the global non-convex optimization software Gurobi, and showcase the
usefulness of this code on synthetic data sets and on real-world hyperspectral
images.
\\ ( https://arxiv.org/abs/2402.06019 ,  1057kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06023
Date: Thu, 8 Feb 2024 19:47:34 GMT   (13865kb,D)

Title: Decision Theory-Guided Deep Reinforcement Learning for Fast Learning
Authors: Zelin Wan, Jin-Hee Cho, Mu Zhu, Ahmed H. Anwar, Charles Kamhoua,
  Munindar P. Singh
Categories: cs.LG cs.AI cs.GT
\\
  This paper introduces a novel approach, Decision Theory-guided Deep
Reinforcement Learning (DT-guided DRL), to address the inherent cold start
problem in DRL. By integrating decision theory principles, DT-guided DRL
enhances agents' initial performance and robustness in complex environments,
enabling more efficient and reliable convergence during learning. Our
investigation encompasses two primary problem contexts: the cart pole and maze
navigation challenges. Experimental results demonstrate that the integration of
decision theory not only facilitates effective initial guidance for DRL agents
but also promotes a more structured and informed exploration strategy,
particularly in environments characterized by large and intricate state spaces.
The results of experiment demonstrate that DT-guided DRL can provide
significantly higher rewards compared to regular DRL. Specifically, during the
initial phase of training, the DT-guided DRL yields up to an 184% increase in
accumulated reward. Moreover, even after reaching convergence, it maintains a
superior performance, ending with up to 53% more reward than standard DRL in
large maze problems. DT-guided DRL represents an advancement in mitigating a
fundamental challenge of DRL by leveraging functions informed by human
(designer) knowledge, setting a foundation for further research in this
promising interdisciplinary domain.
\\ ( https://arxiv.org/abs/2402.06023 ,  13865kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06030
Date: Thu, 8 Feb 2024 20:07:43 GMT   (208kb,D)

Title: Game-theoretic Counterfactual Explanation for Graph Neural Networks
Authors: Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A. Kash, Sourav
  Medya
Categories: cs.LG cs.AI
Comments: Accepted to WWW 2024
\\
  Graph Neural Networks (GNNs) have been a powerful tool for node
classification tasks in complex networks. However, their decision-making
processes remain a black-box to users, making it challenging to understand the
reasoning behind their predictions. Counterfactual explanations (CFE) have
shown promise in enhancing the interpretability of machine learning models.
Prior approaches to compute CFE for GNNS often are learning-based approaches
that require training additional graphs. In this paper, we propose a
semivalue-based, non-learning approach to generate CFE for node classification
tasks, eliminating the need for any additional training. Our results reveals
that computing Banzhaf values requires lower sample complexity in identifying
the counterfactual explanations compared to other popular methods such as
computing Shapley values. Our empirical evidence indicates computing Banzhaf
values can achieve up to a fourfold speed up compared to Shapley values. We
also design a thresholding method for computing Banzhaf values and show
theoretical and empirical results on its robustness in noisy environments,
making it superior to Shapley values. Furthermore, the thresholded Banzhaf
values are shown to enhance efficiency without compromising the quality (i.e.,
fidelity) in the explanations in three popular graph datasets.
\\ ( https://arxiv.org/abs/2402.06030 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06031
Date: Thu, 8 Feb 2024 20:07:47 GMT   (1301kb,D)

Title: An operator learning perspective on parameter-to-observable maps
Authors: Daniel Zhengyu Huang, Nicholas H. Nelsen, Margaret Trautner
Categories: cs.LG math.ST stat.ML stat.TH
Comments: 58 pages, 9 figures
MSC-class: 68T07, 62G20, 65J15
\\
  Computationally efficient surrogates for parametrized physical models play a
crucial role in science and engineering. Operator learning provides data-driven
surrogates that map between function spaces. However, instead of full-field
measurements, often the available data are only finite-dimensional
parametrizations of model inputs or finite observables of model outputs.
Building off of Fourier Neural Operators, this paper introduces the Fourier
Neural Mappings (FNMs) framework that is able to accommodate such
finite-dimensional inputs and outputs. The paper develops universal
approximation theorems for the method. Moreover, in many applications the
underlying parameter-to-observable (PtO) map is defined implicitly through an
infinite-dimensional operator, such as the solution operator of a partial
differential equation. A natural question is whether it is more data-efficient
to learn the PtO map end-to-end or first learn the solution operator and
subsequently compute the observable from the full-field solution. A theoretical
analysis of Bayesian nonparametric regression of linear functionals, which is
of independent interest, suggests that the end-to-end approach can actually
have worse sample complexity. Extending beyond the theory, numerical results
for the FNM approximation of three nonlinear PtO maps demonstrate the benefits
of the operator learning perspective that this paper adopts.
\\ ( https://arxiv.org/abs/2402.06031 ,  1301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06034
Date: Thu, 8 Feb 2024 20:14:35 GMT   (715kb)

Title: Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch
  Gradient Descent
Authors: Haoyu Yang and Anthony Agnesina and Haoxing Ren
Categories: cs.LG cs.AI
Comments: 7 pages, 2 figures, preprint
\\
  Exploding predictive AI has enabled fast yet effective evaluation and
decision-making in modern chip physical design flows. State-of-the-art
frameworks typically include the objective of minimizing the mean square error
(MSE) between the prediction and the ground truth. We argue the averaging
effect of MSE induces limitations in both model training and deployment, and
good MSE behavior does not guarantee the capability of these models to assist
physical design flows which are likely sabotaged due to a small portion of
prediction error. To address this, we propose mini-pixel batch gradient descent
(MPGD), a plug-and-play optimization algorithm that takes the most informative
entries into consideration, offering probably faster and better convergence.
Experiments on representative benchmark suits show the significant benefits of
MPGD on various physical design prediction tasks using CNN or Graph-based
models.
\\ ( https://arxiv.org/abs/2402.06034 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06038
Date: Thu, 8 Feb 2024 20:20:54 GMT   (35891kb,D)

Title: Contrastive Approach to Prior Free Positive Unlabeled Learning
Authors: Anish Acharya, Sujay Sanghavi
Categories: cs.LG cs.AI cs.CV
\\
  Positive Unlabeled (PU) learning refers to the task of learning a binary
classifier given a few labeled positive samples, and a set of unlabeled samples
(which could be positive or negative). In this paper, we propose a novel PU
learning framework, that starts by learning a feature space through
pretext-invariant representation learning and then applies pseudo-labeling to
the unlabeled examples, leveraging the concentration property of the
embeddings. Overall, our proposed approach handily outperforms state-of-the-art
PU learning methods across several standard PU benchmark datasets, while not
requiring a-priori knowledge or estimate of class prior. Remarkably, our method
remains effective even when labeled data is scant, where most PU learning
algorithms falter. We also provide simple theoretical analysis motivating our
proposed algorithms and establish generalization guarantee for our approach.
\\ ( https://arxiv.org/abs/2402.06038 ,  35891kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06045
Date: Thu, 8 Feb 2024 20:36:21 GMT   (1071kb,D)

Title: Direct Acquisition Optimization for Low-Budget Active Learning
Authors: Zhuokai Zhao, Yibo Jiang, Yuxin Chen
Categories: cs.LG
\\
  Active Learning (AL) has gained prominence in integrating data-intensive
machine learning (ML) models into domains with limited labeled data. However,
its effectiveness diminishes significantly when the labeling budget is low. In
this paper, we first empirically observe the performance degradation of
existing AL algorithms in the low-budget settings, and then introduce Direct
Acquisition Optimization (DAO), a novel AL algorithm that optimizes sample
selections based on expected true loss reduction. Specifically, DAO utilizes
influence functions to update model parameters and incorporates an additional
acquisition strategy to mitigate bias in loss estimation. This approach
facilitates a more accurate estimation of the overall error reduction, without
extensive computations or reliance on labeled data. Experiments demonstrate
DAO's effectiveness in low budget settings, outperforming state-of-the-arts
approaches across seven benchmarks.
\\ ( https://arxiv.org/abs/2402.06045 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06056
Date: Thu, 8 Feb 2024 20:57:10 GMT   (4936kb,D)

Title: ActiveDP: Bridging Active Learning and Data Programming
Authors: Naiqing Guan, Nick Koudas
Categories: cs.LG cs.DB
Comments: accepted by EDBT 2024 research track
\\
  Modern machine learning models require large labelled datasets to achieve
good performance, but manually labelling large datasets is expensive and
time-consuming. The data programming paradigm enables users to label large
datasets efficiently but produces noisy labels, which deteriorates the
downstream model's performance. The active learning paradigm, on the other
hand, can acquire accurate labels but only for a small fraction of instances.
In this paper, we propose ActiveDP, an interactive framework bridging active
learning and data programming together to generate labels with both high
accuracy and coverage, combining the strengths of both paradigms. Experiments
show that ActiveDP outperforms previous weak supervision and active learning
approaches and consistently performs well under different labelling budgets.
\\ ( https://arxiv.org/abs/2402.06056 ,  4936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06075
Date: Thu, 8 Feb 2024 21:51:07 GMT   (432kb)

Title: Scaling Artificial Intelligence for Digital Wargaming in Support of
  Decision-Making
Authors: Scotty Black, Christian Darken
Categories: cs.LG cs.AI
Report-no: STO-MP-MSG-207-23
Journal-ref: NATO STO-MP-MSG-207 2023
DOI: 10.14339/STO-MP-MSG-207-23-PDF
\\
  In this unprecedented era of technology-driven transformation, it becomes
more critical than ever that we aggressively invest in developing robust
artificial intelligence (AI) for wargaming in support of decision-making. By
advancing AI-enabled systems and pairing these with human judgment, we will be
able to enhance all-domain awareness, improve the speed and quality of our
decision cycles, offer recommendations for novel courses of action, and more
rapidly counter our adversary's actions. It therefore becomes imperative that
we accelerate the development of AI to help us better address the complexity of
modern challenges and dilemmas that currently requires human intelligence and,
if possible, attempt to surpass human intelligence--not to replace humans, but
to augment and better inform human decision-making at machine speed. Although
deep reinforcement learning continues to show promising results in intelligent
agent behavior development for the long-horizon, complex tasks typically found
in combat modeling and simulation, further research is needed to enable the
scaling of AI to deal with these intricate and expansive state-spaces
characteristic of wargaming for either concept development, education, or
analysis. To help address this challenge, in our research, we are developing
and implementing a hierarchical reinforcement learning framework that includes
a multi-model approach and dimension-invariant observation abstractions.
\\ ( https://arxiv.org/abs/2402.06075 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06082
Date: Thu, 8 Feb 2024 22:17:40 GMT   (373kb,D)

Title: SubGen: Token Generation in Sublinear Time and Memory
Authors: Amir Zandieh, Insu Han, Vahab Mirrokni, Amin Karbasi
Categories: cs.LG cs.AI cs.DS
\\
  Despite the significant success of large language models (LLMs), their
extensive memory requirements pose challenges for deploying them in
long-context token generation. The substantial memory footprint of LLM decoders
arises from the necessity to store all previous tokens in the attention module,
a requirement imposed by key-value (KV) caching. In this work, our focus is on
developing an efficient compression technique for the KV cache. Empirical
evidence indicates a significant clustering tendency within key embeddings in
the attention module. Building on this key insight, we have devised a novel
caching method with sublinear complexity, employing online clustering on key
tokens and online $\ell_2$ sampling on values. The result is a provably
accurate and efficient attention decoding algorithm, termed SubGen. Not only
does this algorithm ensure a sublinear memory footprint and sublinear time
complexity, but we also establish a tight error bound for our approach.
Empirical evaluations on long-context question-answering tasks demonstrate that
SubGen significantly outperforms existing and state-of-the-art KV cache
compression methods in terms of performance and efficiency.
\\ ( https://arxiv.org/abs/2402.06082 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06087
Date: Thu, 8 Feb 2024 22:39:49 GMT   (123kb,D)

Title: Descriptive Kernel Convolution Network with Improved Random Walk Kernel
Authors: Meng-Chieh Lee, Lingxiao Zhao, Leman Akoglu
Categories: cs.LG
Comments: WWW 2024
\\
  Graph kernels used to be the dominant approach to feature engineering for
structured data, which are superseded by modern GNNs as the former lacks
learnability. Recently, a suite of Kernel Convolution Networks (KCNs)
successfully revitalized graph kernels by introducing learnability, which
convolves input with learnable hidden graphs using a certain graph kernel. The
random walk kernel (RWK) has been used as the default kernel in many KCNs,
gaining increasing attention. In this paper, we first revisit the RWK and its
current usage in KCNs, revealing several shortcomings of the existing designs,
and propose an improved graph kernel RWK+, by introducing color-matching random
walks and deriving its efficient computation. We then propose RWK+CN, a KCN
that uses RWK+ as the core kernel to learn descriptive graph features with an
unsupervised objective, which can not be achieved by GNNs. Further, by
unrolling RWK+, we discover its connection with a regular GCN layer, and
propose a novel GNN layer RWK+Conv. In the first part of experiments, we
demonstrate the descriptive learning ability of RWK+CN with the improved random
walk kernel RWK+ on unsupervised pattern mining tasks; in the second part, we
show the effectiveness of RWK+ for a variety of KCN architectures and
supervised graph learning tasks, and demonstrate the expressiveness of RWK+Conv
layer, especially on the graph-level tasks. RWK+ and RWK+Conv adapt to various
real-world applications, including web applications such as bot detection in a
web-scale Twitter social network, and community classification in Reddit social
interaction networks.
\\ ( https://arxiv.org/abs/2402.06087 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06104
Date: Thu, 8 Feb 2024 23:43:53 GMT   (1672kb,D)

Title: Function Aligned Regression: A Method Explicitly Learns Functional
  Derivatives from Data
Authors: Dixian Zhu and Livnat Jerby-Arnon
Categories: cs.LG cs.AI
Comments: 21 pages excluding references
\\
  Regression is a fundamental task in machine learning that has garnered
extensive attention over the past decades. The conventional approach for
regression involves employing loss functions that primarily concentrate on
aligning model prediction with the ground truth for each individual data
sample, which, as we show, can result in sub-optimal prediction of the
relationships between the different samples. Recent research endeavors have
introduced novel perspectives by incorporating label similarity information to
regression. However, a notable gap persists in these approaches when it comes
to fully capturing the intricacies of the underlying ground truth function. In
this work, we propose FAR (Function Aligned Regression) as a arguably better
and more efficient solution to fit the underlying function of ground truth by
capturing functional derivatives. We demonstrate the effectiveness of the
proposed method practically on 2 synthetic datasets and on 8 extensive
real-world tasks from 6 benchmark datasets with other 8 competitive baselines.
The code is open-sourced at \url{https://github.com/DixianZhu/FAR}.
\\ ( https://arxiv.org/abs/2402.06104 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06110
Date: Fri, 9 Feb 2024 00:24:46 GMT   (3825kb,D)

Title: AI enhanced data assimilation and uncertainty quantification applied to
  Geological Carbon Storage
Authors: G. S. Seabra (1, 2), N. T. M\"ucke (3, 4), V. L. S. Silva (2, 5), D.
  Voskov (1, 6), F. Vossepoel (1) ((1) TU Delft, Netherlands, (2) Petrobras,
  Brazil, (3) Centrum Wiskunde & Informatica, Netherlands, (4) Utrecht
  University, Netherlands, (5) Imperial College London, United Kingdom, (6)
  Stanford University, USA)
Categories: cs.LG
Comments: 29 pages, 20 figures, submited to the International Journal of
  Greenhouse Gas Control
ACM-class: J.2
\\
  This study investigates the integration of machine learning (ML) and data
assimilation (DA) techniques, focusing on implementing surrogate models for
Geological Carbon Storage (GCS) projects while maintaining high fidelity
physical results in posterior states. Initially, we evaluate the surrogate
modeling capability of two distinct machine learning models, Fourier Neural
Operators (FNOs) and Transformer UNet (T-UNet), in the context of CO$_2$
injection simulations within channelized reservoirs. We introduce the
Surrogate-based hybrid ESMDA (SH-ESMDA), an adaptation of the traditional
Ensemble Smoother with Multiple Data Assimilation (ESMDA). This method uses
FNOs and T-UNet as surrogate models and has the potential to make the standard
ESMDA process at least 50% faster or more, depending on the number of
assimilation steps. Additionally, we introduce Surrogate-based Hybrid RML
(SH-RML), a variational data assimilation approach that relies on the
randomized maximum likelihood (RML) where both the FNO and the T-UNet enable
the computation of gradients for the optimization of the objective function,
and a high-fidelity model is employed for the computation of the posterior
states. Our comparative analyses show that SH-RML offers better uncertainty
quantification compared to conventional ESMDA for the case study.
\\ ( https://arxiv.org/abs/2402.06110 ,  3825kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06121
Date: Fri, 9 Feb 2024 01:11:23 GMT   (24351kb,D)

Title: Iterated Denoising Energy Matching for Sampling from Boltzmann Densities
Authors: Tara Akhound-Sadegh, Jarrid Rector-Brooks, Avishek Joey Bose, Sarthak
  Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh,
  Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, Alexander Tong
Categories: cs.LG stat.ML
Comments: Code for iDEM is available at https://github.com/jarridrb/dem
\\
  Efficiently generating statistically independent samples from an unnormalized
probability distribution, such as equilibrium samples of many-body systems, is
a foundational problem in science. In this paper, we propose Iterated Denoising
Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic
score matching objective leveraging solely the energy function and its gradient
-- and no data samples -- to train a diffusion-based sampler. Specifically,
iDEM alternates between (I) sampling regions of high model density from a
diffusion-based sampler and (II) using these samples in our stochastic matching
objective to further improve the sampler. iDEM is scalable to high dimensions
as the inner matching objective, is simulation-free, and requires no MCMC
samples. Moreover, by leveraging the fast mode mixing behavior of diffusion,
iDEM smooths out the energy landscape enabling efficient exploration and
learning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging
from standard synthetic energy functions to invariant $n$-body particle
systems. We show that the proposed approach achieves state-of-the-art
performance on all metrics and trains $2-5\times$ faster, which allows it to be
the first method to train using energy on the challenging $55$-particle
Lennard-Jones system.
\\ ( https://arxiv.org/abs/2402.06121 ,  24351kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06128
Date: Fri, 9 Feb 2024 01:19:47 GMT   (7074kb,D)

Title: Rethinking Node-wise Propagation for Large-scale Graph Learning
Authors: Xunkai Li, Jingyuan Ma, Zhengyu Wu, Daohan Su, Wentao Zhang, Rong-Hua
  Li, Guoren Wang
Categories: cs.LG cs.AI cs.SI
Comments: Accepted by WWW 2024
\\
  Scalable graph neural networks (GNNs) have emerged as a promising technique,
which exhibits superior predictive performance and high running efficiency
across numerous large-scale graph-based web applications. However, (i) Most
scalable GNNs tend to treat all nodes in graphs with the same propagation
rules, neglecting their topological uniqueness; (ii) Existing node-wise
propagation optimization strategies are insufficient on web-scale graphs with
intricate topology, where a full portrayal of nodes' local properties is
required. Intuitively, different nodes in web-scale graphs possess distinct
topological roles, and therefore propagating them indiscriminately or neglect
local contexts may compromise the quality of node representations. This
intricate topology in web-scale graphs cannot be matched by small-scale
scenarios. To address the above issues, we propose \textbf{A}daptive
\textbf{T}opology-aware \textbf{P}ropagation (ATP), which reduces potential
high-bias propagation and extracts structural patterns of each node in a
scalable manner to improve running efficiency and predictive performance.
Remarkably, ATP is crafted to be a plug-and-play node-wise propagation
optimization strategy, allowing for offline execution independent of the graph
learning process in a new perspective. Therefore, this approach can be
seamlessly integrated into most scalable GNNs while remain orthogonal to
existing node-wise propagation optimization strategies. Extensive experiments
on 12 datasets, including the most representative large-scale ogbn-papers100M,
have demonstrated the effectiveness of ATP. Specifically, ATP has proven to be
efficient in improving the performance of prevalent scalable GNNs for
semi-supervised node classification while addressing redundant computational
costs.
\\ ( https://arxiv.org/abs/2402.06128 ,  7074kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06135
Date: Fri, 9 Feb 2024 01:47:18 GMT   (9857kb,D)

Title: Jointly Learning Representations for Map Entities via Heterogeneous
  Graph Contrastive Learning
Authors: Jiawei Jiang, Yifan Yang, Jingyuan Wang, Junjie Wu
Categories: cs.LG
Comments: 14 pages, 6 figures
\\
  The electronic map plays a crucial role in geographic information systems,
serving various urban managerial scenarios and daily life services. Developing
effective Map Entity Representation Learning (MERL) methods is crucial to
extracting embedding information from electronic maps and converting map
entities into representation vectors for downstream applications. However,
existing MERL methods typically focus on one specific category of map entities,
such as POIs, road segments, or land parcels, which is insufficient for
real-world diverse map-based applications and might lose latent structural and
semantic information interacting between entities of different types. Moreover,
using representations generated by separate models for different map entities
can introduce inconsistencies. Motivated by this, we propose a novel method
named HOME-GCL for learning representations of multiple categories of map
entities. Our approach utilizes a heterogeneous map entity graph (HOME graph)
that integrates both road segments and land parcels into a unified framework. A
HOME encoder with parcel-segment joint feature encoding and heterogeneous graph
transformer is then deliberately designed to convert segments and parcels into
representation vectors. Moreover, we introduce two types of contrastive
learning tasks, namely intra-entity and inter-entity tasks, to train the
encoder in a self-supervised manner. Extensive experiments on three large-scale
datasets covering road segment-based, land parcel-based, and trajectory-based
tasks demonstrate the superiority of our approach. To the best of our
knowledge, HOME-GCL is the first attempt to jointly learn representations for
road segments and land parcels using a unified model.
\\ ( https://arxiv.org/abs/2402.06135 ,  9857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06137
Date: Fri, 9 Feb 2024 02:11:25 GMT   (1511kb,D)

Title: On the Privacy of Selection Mechanisms with Gaussian Noise
Authors: Jonathan Lebensold, Doina Precup and Borja Balle
Categories: cs.LG cs.CR
Comments: AISTATS 2024
\\
  Report Noisy Max and Above Threshold are two classical differentially private
(DP) selection mechanisms. Their output is obtained by adding noise to a
sequence of low-sensitivity queries and reporting the identity of the query
whose (noisy) answer satisfies a certain condition. Pure DP guarantees for
these mechanisms are easy to obtain when Laplace noise is added to the queries.
On the other hand, when instantiated using Gaussian noise, standard analyses
only yield approximate DP guarantees despite the fact that the outputs of these
mechanisms lie in a discrete space. In this work, we revisit the analysis of
Report Noisy Max and Above Threshold with Gaussian noise and show that, under
the additional assumption that the underlying queries are bounded, it is
possible to provide pure ex-ante DP bounds for Report Noisy Max and pure
ex-post DP bounds for Above Threshold. The resulting bounds are tight and
depend on closed-form expressions that can be numerically evaluated using
standard methods. Empirically we find these lead to tighter privacy accounting
in the high privacy, low data regime. Further, we propose a simple privacy
filter for composing pure ex-post DP guarantees, and use it to derive a fully
adaptive Gaussian Sparse Vector Technique mechanism. Finally, we provide
experiments on mobility and energy consumption datasets demonstrating that our
Sparse Vector Technique is practically competitive with previous approaches and
requires less hyper-parameter tuning.
\\ ( https://arxiv.org/abs/2402.06137 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06150
Date: Fri, 9 Feb 2024 02:59:08 GMT   (1558kb,D)

Title: Domain Generalization with Small Data
Authors: Kecheng Chen, Elena Gal, Hong Yan, and Haoliang Li
Categories: cs.LG cs.CV
Comments: This paper has been accepted by International Journal of Computer
  Vision
\\
  In this work, we propose to tackle the problem of domain generalization in
the context of \textit{insufficient samples}. Instead of extracting latent
feature embeddings based on deterministic models, we propose to learn a
domain-invariant representation based on the probabilistic framework by mapping
each data point into probabilistic embeddings. Specifically, we first extend
empirical maximum mean discrepancy (MMD) to a novel probabilistic MMD that can
measure the discrepancy between mixture distributions (i.e., source domains)
consisting of a series of latent distributions rather than latent points.
Moreover, instead of imposing the contrastive semantic alignment (CSA) loss
based on pairs of latent points, a novel probabilistic CSA loss encourages
positive probabilistic embedding pairs to be closer while pulling other
negative ones apart. Benefiting from the learned representation captured by
probabilistic models, our proposed method can marriage the measurement on the
\textit{distribution over distributions} (i.e., the global perspective
alignment) and the distribution-based contrastive semantic alignment (i.e., the
local perspective alignment). Extensive experimental results on three
challenging medical datasets show the effectiveness of our proposed method in
the context of insufficient data compared with state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.06150 ,  1558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06160
Date: Fri, 9 Feb 2024 03:23:39 GMT   (188kb,D)

Title: Improved Evidential Deep Learning via a Mixture of Dirichlet
  Distributions
Authors: J. Jon Ryu, Maohao Shen, Soumya Ghosh, Yuheng Bu, Prasanna Sattigeri,
  Subhro Das, Gregory W. Wornell
Categories: cs.LG stat.ML
Comments: 18 pages, 5 figures
\\
  This paper explores a modern predictive uncertainty estimation approach,
called evidential deep learning (EDL), in which a single neural network model
is trained to learn a meta distribution over the predictive distribution by
minimizing a specific objective function. Despite their strong empirical
performance, recent studies by Bengs et al. identify a fundamental pitfall of
the existing methods: the learned epistemic uncertainty may not vanish even in
the infinite-sample limit. We corroborate the observation by providing a
unifying view of a class of widely used objectives from the literature. Our
analysis reveals that the EDL methods essentially train a meta distribution by
minimizing a certain divergence measure between the distribution and a
sample-size-independent target distribution, resulting in spurious epistemic
uncertainty. Grounded in theoretical principles, we propose learning a
consistent target distribution by modeling it with a mixture of Dirichlet
distributions and learning via variational inference. Afterward, a final meta
distribution model distills the learned uncertainty from the target model.
Experimental results across various uncertainty-based downstream tasks
demonstrate the superiority of our proposed method, and illustrate the
practical implications arising from the consistency and inconsistency of
learned epistemic uncertainty.
\\ ( https://arxiv.org/abs/2402.06160 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06171
Date: Fri, 9 Feb 2024 04:01:25 GMT   (11688kb,D)

Title: Pushing Boundaries: Mixup's Influence on Neural Collapse
Authors: Quinn Fisher, Haoming Meng, Vardan Papyan
Categories: cs.LG
Comments: Published as a conference paper at the International Conference on
  Learning Representations (ICLR 2024)
\\
  Mixup is a data augmentation strategy that employs convex combinations of
training instances and their respective labels to augment the robustness and
calibration of deep neural networks. Despite its widespread adoption, the
nuanced mechanisms that underpin its success are not entirely understood. The
observed phenomenon of Neural Collapse, where the last-layer activations and
classifier of deep networks converge to a simplex equiangular tight frame
(ETF), provides a compelling motivation to explore whether mixup induces
alternative geometric configurations and whether those could explain its
success. In this study, we delve into the last-layer activations of training
data for deep networks subjected to mixup, aiming to uncover insights into its
operational efficacy. Our investigation, spanning various architectures and
dataset pairs, reveals that mixup's last-layer activations predominantly
converge to a distinctive configuration different than one might expect. In
this configuration, activations from mixed-up examples of identical classes
align with the classifier, while those from different classes delineate
channels along the decision boundary. Moreover, activations in earlier layers
exhibit patterns, as if trained with manifold mixup. These findings are
unexpected, as mixed-up features are not simple convex combinations of feature
class means (as one might get, for example, by training mixup with the mean
squared error loss). By analyzing this distinctive geometric configuration, we
elucidate the mechanisms by which mixup enhances model calibration. To further
validate our empirical observations, we conduct a theoretical analysis under
the assumption of an unconstrained features model, utilizing the mixup loss.
Through this, we characterize and derive the optimal last-layer features under
the assumption that the classifier forms a simplex ETF.
\\ ( https://arxiv.org/abs/2402.06171 ,  11688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06184
Date: Fri, 9 Feb 2024 04:46:48 GMT   (36948kb,D)

Title: The boundary of neural network trainability is fractal
Authors: Jascha Sohl-Dickstein
Categories: cs.LG cs.NE nlin.CD
Comments: 3 pages, mesmerizing fractals
\\
  Some fractals -- for instance those associated with the Mandelbrot and
quadratic Julia sets -- are computed by iterating a function, and identifying
the boundary between hyperparameters for which the resulting series diverges or
remains bounded. Neural network training similarly involves iterating an update
function (e.g. repeated steps of gradient descent), can result in convergent or
divergent behavior, and can be extremely sensitive to small changes in
hyperparameters. Motivated by these similarities, we experimentally examine the
boundary between neural network hyperparameters that lead to stable and
divergent training. We find that this boundary is fractal over more than ten
decades of scale in all tested configurations.
\\ ( https://arxiv.org/abs/2402.06184 ,  36948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06187
Date: Fri, 9 Feb 2024 05:04:40 GMT   (8614kb,D)

Title: Premier-TACO: Pretraining Multitask Representation via Temporal
  Action-Driven Contrastive Loss
Authors: Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Shuang Ma, Hal Daum\'e III,
  Huazhe Xu, John Langford, Praveen Palanisamy, Kalyan Shankar Basu, Furong
  Huang
Categories: cs.LG cs.AI cs.RO
\\
  We present Premier-TACO, a multitask feature representation learning approach
designed to improve few-shot policy learning efficiency in sequential
decision-making tasks. Premier-TACO leverages a subset of multitask offline
datasets for pretraining a general feature representation, which captures
critical environmental dynamics and is fine-tuned using minimal expert
demonstrations. It advances the temporal action contrastive learning (TACO)
objective, known for state-of-the-art results in visual control tasks, by
incorporating a novel negative example sampling strategy. This strategy is
crucial in significantly boosting TACO's computational efficiency, making
large-scale multitask offline pretraining feasible. Our extensive empirical
evaluation in a diverse set of continuous control benchmarks including Deepmind
Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness
in pretraining visual representations, significantly enhancing few-shot
imitation learning of novel tasks. Our code, pretraining data, as well as
pretrained model checkpoints will be released at
https://github.com/PremierTACO/premier-taco.
\\ ( https://arxiv.org/abs/2402.06187 ,  8614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06223
Date: Fri, 9 Feb 2024 07:18:06 GMT   (14490kb,D)

Title: Revealing Multimodal Contrastive Representation Learning through Latent
  Partial Causal Models
Authors: Yuhang Liu, Zhen Zhang, Dong Gong, Biwei Huang, Mingming Gong, Anton
  van den Hengel, Kun Zhang, Javen Qinfeng Shi
Categories: cs.LG cs.CV stat.ML
\\
  Multimodal contrastive representation learning methods have proven successful
across a range of domains, partly due to their ability to generate meaningful
shared representations of complex phenomena. To enhance the depth of analysis
and understanding of these acquired representations, we introduce a unified
causal model specifically designed for multimodal data. By examining this
model, we show that multimodal contrastive representation learning excels at
identifying latent coupled variables within the proposed unified model, up to
linear or permutation transformations resulting from different assumptions. Our
findings illuminate the potential of pre-trained multimodal models, eg, CLIP,
in learning disentangled representations through a surprisingly simple yet
highly effective tool: linear independent component analysis. Experiments
demonstrate the robustness of our findings, even when the assumptions are
violated, and validate the effectiveness of the proposed method in learning
disentangled representations.
\\ ( https://arxiv.org/abs/2402.06223 ,  14490kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06255
Date: Fri, 9 Feb 2024 09:09:39 GMT   (211kb,D)

Title: Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial
  Tuning
Authors: Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang
Categories: cs.LG cs.AI cs.CL cs.CR
\\
  Although Large Language Models (LLMs) have achieved tremendous success in
various applications, they are also susceptible to certain prompts that can
induce them to bypass built-in safety measures and provide dangerous or illegal
content, a phenomenon known as jailbreak. To protect LLMs from producing
harmful information, various defense strategies are proposed, with most
focusing on content filtering or adversarial training of models. In this paper,
we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense
control mechanism, which is then embedded as a prefix to user prompts to
implement our defense strategy. We design a training process similar to
adversarial training to achieve our optimized goal, alternating between
updating attack and defense controls. To our knowledge, we are the first to
implement defense from the perspective of prompt tuning. Once employed, our
method will hardly impact the operational efficiency of LLMs. Experiments show
that our method is effective in both black-box and white-box settings, reducing
the success rate of advanced attacks to nearly 0 while maintaining the benign
answer rate of 80% to simple benign questions. Our work might potentially chart
a new perspective for future explorations in LLM security.
\\ ( https://arxiv.org/abs/2402.06255 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06266
Date: Fri, 9 Feb 2024 09:28:01 GMT   (302kb,D)

Title: Value function interference and greedy action selection in value-based
  multi-objective reinforcement learning
Authors: Peter Vamplew, Cameron Foale, Richard Dazeley
Categories: cs.LG
\\
  Multi-objective reinforcement learning (MORL) algorithms extend conventional
reinforcement learning (RL) to the more general case of problems with multiple,
conflicting objectives, represented by vector-valued rewards. Widely-used
scalar RL methods such as Q-learning can be modified to handle multiple
objectives by (1) learning vector-valued value functions, and (2) performing
action selection using a scalarisation or ordering operator which reflects the
user's utility with respect to the different objectives. However, as we
demonstrate here, if the user's utility function maps widely varying
vector-values to similar levels of utility, this can lead to interference in
the value-function learned by the agent, leading to convergence to sub-optimal
policies. This will be most prevalent in stochastic environments when
optimising for the Expected Scalarised Return criterion, but we present a
simple example showing that interference can also arise in deterministic
environments. We demonstrate empirically that avoiding the use of random
tie-breaking when identifying greedy actions can ameliorate, but not fully
overcome, the problems caused by value function interference.
\\ ( https://arxiv.org/abs/2402.06266 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06268
Date: Fri, 9 Feb 2024 09:34:36 GMT   (270kb,D)

Title: YAMLE: Yet Another Machine Learning Environment
Authors: Martin Ferianc, Miguel Rodrigues
Categories: cs.LG
Comments: Find it at: https://github.com/martinferianc/yamle
\\
  YAMLE: Yet Another Machine Learning Environment is an open-source framework
that facilitates rapid prototyping and experimentation with machine learning
(ML) models and methods. The key motivation is to reduce repetitive work when
implementing new approaches and improve reproducibility in ML research. YAMLE
includes a command-line interface and integrations with popular and
well-maintained PyTorch-based libraries to streamline training, hyperparameter
optimisation, and logging. The ambition for YAMLE is to grow into a shared
ecosystem where researchers and practitioners can quickly build on and compare
existing implementations. Find it at: https://github.com/martinferianc/yamle.
\\ ( https://arxiv.org/abs/2402.06268 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06276
Date: Fri, 9 Feb 2024 09:40:33 GMT   (1937kb,D)

Title: Safe Active Learning for Time-Series Modeling with Gaussian Processes
Authors: Christoph Zimmer, Mona Meister, Duy Nguyen-Tuong
Categories: cs.LG stat.ML
Comments: Clarification / Errata of article originally published at NeurIPS:
  https://proceedings.neurips.cc/paper/2018/hash/b197ffdef2ddc3308584dce7afa3661b-Abstract.html
\\
  Learning time-series models is useful for many applications, such as
simulation and forecasting. In this study, we consider the problem of actively
learning time-series models while taking given safety constraints into account.
For time-series modeling we employ a Gaussian process with a nonlinear
exogenous input structure. The proposed approach generates data appropriate for
time series model learning, i.e. input and output trajectories, by dynamically
exploring the input space. The approach parametrizes the input trajectory as
consecutive trajectory sections, which are determined stepwise given safety
requirements and past observations. We analyze the proposed algorithm and
evaluate it empirically on a technical application. The results show the
effectiveness of our approach in a realistic technical use case.
\\ ( https://arxiv.org/abs/2402.06276 ,  1937kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06287
Date: Fri, 9 Feb 2024 09:54:01 GMT   (4639kb,D)

Title: AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems
Authors: Clara Punzi, Roberto Pellungrini, Mattia Setzu, Fosca Giannotti and
  Dino Pedreschi
Categories: cs.LG cs.AI cs.HC
\\
  Everyday we increasingly rely on machine learning models to automate and
support high-stake tasks and decisions. This growing presence means that humans
are now constantly interacting with machine learning-based systems, training
and using models everyday. Several different techniques in computer science
literature account for the human interaction with machine learning systems, but
their classification is sparse and the goals varied. This survey proposes a
taxonomy of Hybrid Decision Making Systems, providing both a conceptual and
technical framework for understanding how current computer science literature
models interaction between humans and machines.
\\ ( https://arxiv.org/abs/2402.06287 ,  4639kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06289
Date: Fri, 9 Feb 2024 09:58:35 GMT   (410kb,D)

Title: Evaluating Membership Inference Attacks and Defenses in Federated
  Learning
Authors: Gongxi Zhu, Donghao Li, Hanlin Gu, Yuxing Han, Yuan Yao, Lixin Fan,
  Qiang Yang
Categories: cs.LG cs.CR
Comments: 11 pages, 4 figures
\\
  Membership Inference Attacks (MIAs) pose a growing threat to privacy
preservation in federated learning. The semi-honest attacker, e.g., the server,
may determine whether a particular sample belongs to a target client according
to the observed model information. This paper conducts an evaluation of
existing MIAs and corresponding defense strategies. Our evaluation on MIAs
reveals two important findings about the trend of MIAs. Firstly, combining
model information from multiple communication rounds (Multi-temporal) enhances
the overall effectiveness of MIAs compared to utilizing model information from
a single epoch. Secondly, incorporating models from non-target clients
(Multi-spatial) significantly improves the effectiveness of MIAs, particularly
when the clients' data is homogeneous. This highlights the importance of
considering the temporal and spatial model information in MIAs. Next, we assess
the effectiveness via privacy-utility tradeoff for two type defense mechanisms
against MIAs: Gradient Perturbation and Data Replacement. Our results
demonstrate that Data Replacement mechanisms achieve a more optimal balance
between preserving privacy and maintaining model utility. Therefore, we
recommend the adoption of Data Replacement methods as a defense strategy
against MIAs. Our code is available in https://github.com/Liar-Mask/FedMIA.
\\ ( https://arxiv.org/abs/2402.06289 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06293
Date: Fri, 9 Feb 2024 10:14:18 GMT   (3676kb,D)

Title: Probabilistic Forecasting of Irregular Time Series via Conditional Flows
Authors: Vijaya Krishna Yalavarthi, Randolf Scholz, Stefan Born, Lars
  Schmidt-Thieme
Categories: cs.LG stat.ML
\\
  Probabilistic forecasting of irregularly sampled multivariate time series
with missing values is an important problem in many fields, including health
care, astronomy, and climate. State-of-the-art methods for the task estimate
only marginal distributions of observations in single channels and at single
timepoints, assuming a fixed-shape parametric distribution. In this work, we
propose a novel model, ProFITi, for probabilistic forecasting of irregularly
sampled time series with missing values using conditional normalizing flows.
The model learns joint distributions over the future values of the time series
conditioned on past observations and queried channels and times, without
assuming any fixed shape of the underlying distribution. As model components,
we introduce a novel invertible triangular attention layer and an invertible
non-linear activation function on and onto the whole real line. We conduct
extensive experiments on four datasets and demonstrate that the proposed model
provides $4$ times higher likelihood over the previously best model.
\\ ( https://arxiv.org/abs/2402.06293 ,  3676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06295
Date: Fri, 9 Feb 2024 10:16:58 GMT   (4471kb,D)

Title: Multimodal Interpretable Data-Driven Models for Early Prediction of
  Antimicrobial Multidrug Resistance Using Multivariate Time-Series
Authors: Sergio Mart\'inez-Ag\"uero, Antonio G. Marques, Inmaculada
  Mora-Jim\'enez, Joaqu\'in Alv\'arez-Rodr\'iguez, Cristina Soguero-Ruiza
Categories: cs.LG q-bio.QM
\\
  Electronic health records (EHR) is an inherently multimodal register of the
patient's health status characterized by static data and multivariate time
series (MTS). While MTS are a valuable tool for clinical prediction, their
fusion with other data modalities can possibly result in more thorough insights
and more accurate results. Deep neural networks (DNNs) have emerged as
fundamental tools for identifying and defining underlying patterns in the
healthcare domain. However, fundamental improvements in interpretability are
needed for DNN models to be widely used in the clinical setting. In this study,
we present an approach built on a collection of interpretable multimodal
data-driven models that may anticipate and understand the emergence of
antimicrobial multidrug resistance (AMR) germs in the intensive care unit (ICU)
of the University Hospital of Fuenlabrada (Madrid, Spain). The profile and
initial health status of the patient are modeled using static variables, while
the evolution of the patient's health status during the ICU stay is modeled
using several MTS, including mechanical ventilation and antibiotics intake. The
multimodal DNNs models proposed in this paper include interpretable principles
in addition to being effective at predicting AMR and providing an explainable
prediction support system for AMR in the ICU. Furthermore, our proposed
methodology based on multimodal models and interpretability schemes can be
leveraged in additional clinical problems dealing with EHR data, broadening the
impact and applicability of our results.
\\ ( https://arxiv.org/abs/2402.06295 ,  4471kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06318
Date: Fri, 9 Feb 2024 10:57:14 GMT   (8161kb,D)

Title: TimEHR: Image-based Time Series Generation for Electronic Health Records
Authors: Hojjat Karami, Mary-Anne Hartley, David Atienza, Anisoara Ionescu
Categories: cs.LG
\\
  Time series in Electronic Health Records (EHRs) present unique challenges for
generative models, such as irregular sampling, missing values, and high
dimensionality. In this paper, we propose a novel generative adversarial
network (GAN) model, TimEHR, to generate time series data from EHRs. In
particular, TimEHR treats time series as images and is based on two conditional
GANs. The first GAN generates missingness patterns, and the second GAN
generates time series values based on the missingness pattern. Experimental
results on three real-world EHR datasets show that TimEHR outperforms
state-of-the-art methods in terms of fidelity, utility, and privacy metrics.
\\ ( https://arxiv.org/abs/2402.06318 ,  8161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06323
Date: Fri, 9 Feb 2024 11:03:52 GMT   (156kb,D)

Title: How Uniform Random Weights Induce Non-uniform Bias: Typical
  Interpolating Neural Networks Generalize with Narrow Teachers
Authors: Gon Buzaglo, Itamar Harel, Mor Shpigel Nacson, Alon Brutzkus, Nathan
  Srebro, Daniel Soudry
Categories: cs.LG stat.ML
\\
  Background. A main theoretical puzzle is why over-parameterized Neural
Networks (NNs) generalize well when trained to zero loss (i.e., so they
interpolate the data). Usually, the NN is trained with Stochastic Gradient
Descent (SGD) or one of its variants. However, recent empirical work examined
the generalization of a random NN that interpolates the data: the NN was
sampled from a seemingly uniform prior over the parameters, conditioned on that
the NN perfectly classifying the training set. Interestingly, such a NN sample
typically generalized as well as SGD-trained NNs.
  Contributions. We prove that such a random NN interpolator typically
generalizes well if there exists an underlying narrow ``teacher NN" that agrees
with the labels. Specifically, we show that such a `flat' prior over the NN
parametrization induces a rich prior over the NN functions, due to the
redundancy in the NN structure. In particular, this creates a bias towards
simpler functions, which require less relevant parameters to represent --
enabling learning with a sample complexity approximately proportional to the
complexity of the teacher (roughly, the number of non-redundant parameters),
rather than the student's.
\\ ( https://arxiv.org/abs/2402.06323 ,  156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06330
Date: Fri, 9 Feb 2024 11:09:52 GMT   (212kb,D)

Title: Continual Learning on Graphs: A Survey
Authors: Zonggui Tian, Du Zhang, and Hong-Ning Dai
Categories: cs.LG
\\
  Recently, continual graph learning has been increasingly adopted for diverse
graph-structured data processing tasks in non-stationary environments. Despite
its promising learning capability, current studies on continual graph learning
mainly focus on mitigating the catastrophic forgetting problem while ignoring
continuous performance improvement. To bridge this gap, this article aims to
provide a comprehensive survey of recent efforts on continual graph learning.
Specifically, we introduce a new taxonomy of continual graph learning from the
perspective of overcoming catastrophic forgetting. Moreover, we systematically
analyze the challenges of applying these continual graph learning methods in
improving performance continuously and then discuss the possible solutions.
Finally, we present open issues and future directions pertaining to the
development of continual graph learning and discuss how they impact continuous
performance improvement.
\\ ( https://arxiv.org/abs/2402.06330 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06331
Date: Fri, 9 Feb 2024 11:15:49 GMT   (612kb,D)

Title: Taking Class Imbalance Into Account in Open Set Recognition Evaluation
Authors: Joanna Komorniczak and Pawel Ksieniewicz
Categories: cs.LG cs.CV
\\
  In recent years Deep Neural Network-based systems are not only increasing in
popularity but also receive growing user trust. However, due to the
closed-world assumption of such systems, they cannot recognize samples from
unknown classes and often induce an incorrect label with high confidence.
Presented work looks at the evaluation of methods for Open Set Recognition,
focusing on the impact of class imbalance, especially in the dichotomy between
known and unknown samples. As an outcome of problem analysis, we present a set
of guidelines for evaluation of methods in this field.
\\ ( https://arxiv.org/abs/2402.06331 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06348
Date: Fri, 9 Feb 2024 11:53:27 GMT   (2305kb,D)

Title: Fairness of Exposure in Online Restless Multi-armed Bandits
Authors: Archit Sood, Shweta Jain and Sujit Gujar
Categories: cs.LG stat.ML
Comments: Accepted as extended abstract in AAMAS 2024
\\
  Restless multi-armed bandits (RMABs) generalize the multi-armed bandits where
each arm exhibits Markovian behavior and transitions according to their
transition dynamics. Solutions to RMAB exist for both offline and online cases.
However, they do not consider the distribution of pulls among the arms. Studies
have shown that optimal policies lead to unfairness, where some arms are not
exposed enough. Existing works in fairness in RMABs focus heavily on the
offline case, which diminishes their application in real-world scenarios where
the environment is largely unknown. In the online scenario, we propose the
first fair RMAB framework, where each arm receives pulls in proportion to its
merit. We define the merit of an arm as a function of its stationary reward
distribution. We prove that our algorithm achieves sublinear fairness regret in
the single pull case $O(\sqrt{T\ln T})$, with $T$ being the total number of
episodes. Empirically, we show that our algorithm performs well in the
multi-pull scenario as well.
\\ ( https://arxiv.org/abs/2402.06348 ,  2305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06367
Date: Fri, 9 Feb 2024 12:19:06 GMT   (2516kb,D)

Title: TEE4EHR: Transformer Event Encoder for Better Representation Learning in
  Electronic Health Records
Authors: Hojjat Karami, David Atienza, Anisoara Ionescu
Categories: cs.LG
\\
  Irregular sampling of time series in electronic health records (EHRs) is one
of the main challenges for developing machine learning models. Additionally,
the pattern of missing data in certain clinical variables is not at random but
depends on the decisions of clinicians and the state of the patient. Point
process is a mathematical framework for analyzing event sequence data that is
consistent with irregular sampling patterns. Our model, TEE4EHR, is a
transformer event encoder (TEE) with point process loss that encodes the
pattern of laboratory tests in EHRs. The utility of our TEE has been
investigated in a variety of benchmark event sequence datasets. Additionally,
we conduct experiments on two real-world EHR databases to provide a more
comprehensive evaluation of our model. Firstly, in a self-supervised learning
approach, the TEE is jointly learned with an existing attention-based deep
neural network which gives superior performance in negative log-likelihood and
future event prediction. Besides, we propose an algorithm for aggregating
attention weights that can reveal the interaction between the events. Secondly,
we transfer and freeze the learned TEE to the downstream task for the outcome
prediction, where it outperforms state-of-the-art models for handling
irregularly sampled time series. Furthermore, our results demonstrate that our
approach can improve representation learning in EHRs and can be useful for
clinical prediction tasks.
\\ ( https://arxiv.org/abs/2402.06367 ,  2516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06377
Date: Fri, 9 Feb 2024 12:54:34 GMT   (2002kb,D)

Title: High-Precision Geosteering via Reinforcement Learning and Particle
  Filters
Authors: Ressi Bonti Muhammad, Apoorv Srivastava, Sergey Alyaev, Reidar Brumer
  Bratvold, Daniel M. Tartakovsky
Categories: cs.LG cs.AI physics.geo-ph
Comments: 40 pages
\\
  Geosteering, a key component of drilling operations, traditionally involves
manual interpretation of various data sources such as well-log data. This
introduces subjective biases and inconsistent procedures. Academic attempts to
solve geosteering decision optimization with greedy optimization and
Approximate Dynamic Programming (ADP) showed promise but lacked adaptivity to
realistic diverse scenarios. Reinforcement learning (RL) offers a solution to
these challenges, facilitating optimal decision-making through reward-based
iterative learning. State estimation methods, e.g., particle filter (PF),
provide a complementary strategy for geosteering decision-making based on
online information. We integrate an RL-based geosteering with PF to address
realistic geosteering scenarios. Our framework deploys PF to process real-time
well-log data to estimate the location of the well relative to the
stratigraphic layers, which then informs the RL-based decision-making process.
We compare our method's performance with that of using solely either RL or PF.
Our findings indicate a synergy between RL and PF in yielding optimized
geosteering decisions.
\\ ( https://arxiv.org/abs/2402.06377 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06380
Date: Fri, 9 Feb 2024 12:58:36 GMT   (155kb,D)

Title: Optimal estimation of Gaussian (poly)trees
Authors: Yuhao Wang, Ming Gao, Wai Ming Tai, Bryon Aragam, Arnab Bhattacharyya
Categories: cs.LG stat.ML
\\
  We develop optimal algorithms for learning undirected Gaussian trees and
directed Gaussian polytrees from data. We consider both problems of
distribution learning (i.e. in KL distance) and structure learning (i.e. exact
recovery). The first approach is based on the Chow-Liu algorithm, and learns an
optimal tree-structured distribution efficiently. The second approach is a
modification of the PC algorithm for polytrees that uses partial correlation as
a conditional independence tester for constraint-based structure learning. We
derive explicit finite-sample guarantees for both approaches, and show that
both approaches are optimal by deriving matching lower bounds. Additionally, we
conduct numerical experiments to compare the performance of various algorithms,
providing further insights and empirical evidence.
\\ ( https://arxiv.org/abs/2402.06380 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06402
Date: Fri, 9 Feb 2024 13:40:11 GMT   (18774kb,D)

Title: Hierarchical Transformers are Efficient Meta-Reinforcement Learners
Authors: Gresa Shala, Andr\'e Biedenkapp, Josif Grabocka
Categories: cs.LG cs.AI
\\
  We introduce Hierarchical Transformers for Meta-Reinforcement Learning
(HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims
to address the challenge of enabling reinforcement learning agents to perform
effectively in previously unseen tasks. We demonstrate how past episodes serve
as a rich source of information, which our model effectively distills and
applies to new contexts. Our learned algorithm is capable of outperforming the
previous state-of-the-art and provides more efficient meta-training while
significantly improving generalization capabilities. Experimental results,
obtained across various simulated tasks of the Meta-World Benchmark, indicate a
significant improvement in learning efficiency and adaptability compared to the
state-of-the-art on a variety of tasks. Our approach not only enhances the
agent's ability to generalize from limited data but also paves the way for more
robust and versatile AI systems.
\\ ( https://arxiv.org/abs/2402.06402 ,  18774kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06414
Date: Fri, 9 Feb 2024 14:00:16 GMT   (41kb)

Title: Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in
  Generative AI Interactions
Authors: Bianca-Mihaela Ganescu, Jonathan Passerat-Palmbach
Categories: cs.LG cs.CR
Comments: Accepted at PPAI-24: The 5th AAAI Workshop on Privacy-Preserving
  Artificial Intelligence 2024
\\
  Generative AI, exemplified by models like transformers, has opened up new
possibilities in various domains but also raised concerns about fairness,
transparency and reliability, especially in fields like medicine and law. This
paper emphasizes the urgency of ensuring fairness and quality in these domains
through generative AI. It explores using cryptographic techniques, particularly
Zero-Knowledge Proofs (ZKPs), to address concerns regarding performance
fairness and accuracy while protecting model privacy. Applying ZKPs to Machine
Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables
independent validation of AI-generated content without revealing sensitive
model information, promoting transparency and trust. ZKML enhances AI fairness
by providing cryptographic audit trails for model predictions and ensuring
uniform performance across users. We introduce snarkGPT, a practical ZKML
implementation for transformers, to empower users to verify output accuracy and
quality while preserving model privacy. We present a series of empirical
results studying snarkGPT's scalability and performance to assess the
feasibility and challenges of adopting a ZKML-powered approach to capture
quality and performance fairness problems in generative AI models.
\\ ( https://arxiv.org/abs/2402.06414 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06434
Date: Fri, 9 Feb 2024 14:24:18 GMT   (5437kb,D)

Title: Where is the Truth? The Risk of Getting Confounded in a Continual World
Authors: Florian Peter Busch, Roshni Kamath, Rupert Mitchell, Wolfgang Stammer,
  Kristian Kersting and Martin Mundt
Categories: cs.LG stat.ML
\\
  A dataset is confounded if it is most easily solved via a spurious
correlation which fails to generalize to new data. We will show that, in a
continual learning setting where confounders may vary in time across tasks, the
resulting challenge far exceeds the standard forgetting problem normally
considered. In particular, we derive mathematically the effect of such
confounders on the space of valid joint solutions to sets of confounded tasks.
Interestingly, our theory predicts that for many such continual datasets,
spurious correlations are easily ignored when the tasks are trained on jointly,
but it is far harder to avoid confounding when they are considered
sequentially. We construct such a dataset and demonstrate empirically that
standard continual learning methods fail to ignore confounders, while training
jointly on all tasks is successful. Our continually confounded dataset, ConCon,
is based on CLEVR images and demonstrates the need for continual learning
methods with more robust behavior with respect to confounding.
\\ ( https://arxiv.org/abs/2402.06434 ,  5437kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06441
Date: Fri, 9 Feb 2024 14:34:28 GMT   (325kb,D)

Title: Incorporating Taylor Series and Recursive Structure in Neural Networks
  for Time Series Prediction
Authors: Jarrod Mau and Kevin Moon
Categories: cs.LG
\\
  Time series analysis is relevant in various disciplines such as physics,
biology, chemistry, and finance. In this paper, we present a novel neural
network architecture that integrates elements from ResNet structures, while
introducing the innovative incorporation of the Taylor series framework. This
approach demonstrates notable enhancements in test accuracy across many of the
baseline datasets investigated. Furthermore, we extend our method to
incorporate a recursive step, which leads to even further improvements in test
accuracy. Our findings underscore the potential of our proposed model to
significantly advance time series analysis methodologies, offering promising
avenues for future research and application.
\\ ( https://arxiv.org/abs/2402.06441 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06445
Date: Fri, 9 Feb 2024 14:46:50 GMT   (95kb,D)

Title: The Deep Equilibrium Algorithmic Reasoner
Authors: Dobrik Georgiev, Pietro Li\`o, Davide Buffelli
Categories: cs.LG
\\
  Recent work on neural algorithmic reasoning has demonstrated that graph
neural networks (GNNs) could learn to execute classical algorithms. Doing so,
however, has always used a recurrent architecture, where each iteration of the
GNN aligns with an algorithm's iteration. Since an algorithm's solution is
often an equilibrium, we conjecture and empirically validate that one can train
a network to solve algorithmic problems by directly finding the equilibrium.
Note that this does not require matching each GNN iteration with a step of the
algorithm.
\\ ( https://arxiv.org/abs/2402.06445 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06452
Date: Fri, 9 Feb 2024 14:58:07 GMT   (287kb,D)

Title: An Algorithmic Framework for Constructing Multiple Decision Trees by
  Evaluating Their Combination Performance Throughout the Construction Process
Authors: Keito Tajima, Naoki Ichijo, Yuta Nakahara, and Toshiyasu Matsushima
Categories: cs.LG
\\
  Predictions using a combination of decision trees are known to be effective
in machine learning. Typical ideas for constructing a combination of decision
trees for prediction are bagging and boosting. Bagging independently constructs
decision trees without evaluating their combination performance and averages
them afterward. Boosting constructs decision trees sequentially, only
evaluating a combination performance of a new decision tree and the fixed past
decision trees at each step. Therefore, neither method directly constructs nor
evaluates a combination of decision trees for the final prediction. When the
final prediction is based on a combination of decision trees, it is natural to
evaluate the appropriateness of the combination when constructing them. In this
study, we propose a new algorithmic framework that constructs decision trees
simultaneously and evaluates their combination performance throughout the
construction process. Our framework repeats two procedures. In the first
procedure, we construct new candidates of combinations of decision trees to
find a proper combination of decision trees. In the second procedure, we
evaluate each combination performance of decision trees under some criteria and
select a better combination. To confirm the performance of the proposed
framework, we perform experiments on synthetic and benchmark data.
\\ ( https://arxiv.org/abs/2402.06452 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06457
Date: Fri, 9 Feb 2024 15:02:56 GMT   (890kb,D)

Title: V-STaR: Training Verifiers for Self-Taught Reasoners
Authors: Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville,
  Alessandro Sordoni and Rishabh Agarwal
Categories: cs.LG cs.AI cs.CL
\\
  Common self-improvement approaches for large language models (LLMs), such as
STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated
solutions to improve their problem-solving ability. However, these approaches
discard the large amounts of incorrect solutions generated during this process,
potentially neglecting valuable information in such solutions. To address this
shortcoming, we propose V-STaR that utilizes both the correct and incorrect
solutions generated during the self-improvement process to train a verifier
using DPO that judges correctness of model-generated solutions. This verifier
is used at inference time to select one solution among many candidate
solutions. Running V-STaR for multiple iterations results in progressively
better reasoners and verifiers, delivering a 4% to 17% test accuracy
improvement over existing self-improvement and verification approaches on
common code generation and math reasoning benchmarks with LLaMA2 models.
\\ ( https://arxiv.org/abs/2402.06457 ,  890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06461
Date: Fri, 9 Feb 2024 15:09:38 GMT   (25509kb,D)

Title: Sequential Flow Matching for Generative Modeling
Authors: Jongmin Yoon, and Juho Lee
Categories: cs.LG cs.CV stat.ML
Comments: 19 pages, 13 figures. Under review by ICML 2024
\\
  Straightening the probability flow of the continuous-time generative models,
such as diffusion models or flow-based models, is the key to fast sampling
through the numerical solvers, existing methods learn a linear path by directly
generating the probability path the joint distribution between the noise and
data distribution. One key reason for the slow sampling speed of the ODE-based
solvers that simulate these generative models is the global truncation error of
the ODE solver, caused by the high curvature of the ODE trajectory, which
explodes the truncation error of the numerical solvers in the low-NFE regime.
To address this challenge, We propose a novel method called SeqRF, a learning
technique that straightens the probability flow to reduce the global truncation
error and hence enable acceleration of sampling and improve the synthesis
quality. In both theoretical and empirical studies, we first observe the
straightening property of our SeqRF. Through empirical evaluations via SeqRF
over flow-based generative models, We achieve surpassing results on CIFAR-10,
CelebA-$64 \times 64$, and LSUN-Church datasets.
\\ ( https://arxiv.org/abs/2402.06461 ,  25509kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06465
Date: Fri, 9 Feb 2024 15:17:53 GMT   (54kb)

Title: On Differentially Private Subspace Estimation Without Distributional
  Assumptions
Authors: Eliad Tsfadia
Categories: cs.LG cs.CR cs.DS
\\
  Private data analysis faces a significant challenge known as the curse of
dimensionality, leading to increased costs. However, many datasets possess an
inherent low-dimensional structure. For instance, during optimization via
gradient descent, the gradients frequently reside near a low-dimensional
subspace. If the low-dimensional structure could be privately identified using
a small amount of points, we could avoid paying (in terms of privacy and
accuracy) for the high ambient dimension.
  On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved
that privately estimating subspaces, in general, requires an amount of points
that depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed
this limitation by considering points that are i.i.d. samples from a Gaussian
distribution whose covariance matrix has a certain eigenvalue gap. Yet, it was
still left unclear whether we could provide similar upper bounds without
distributional assumptions and whether we could prove lower bounds that depend
on similar eigenvalue gaps.
  In this work, we make progress in both directions. We formulate the problem
of private subspace estimation under two different types of singular value gaps
of the input data and prove new upper and lower bounds for both types. In
particular, our results determine what type of gap is sufficient and necessary
for estimating a subspace with an amount of points that is independent of the
dimension.
\\ ( https://arxiv.org/abs/2402.06465 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06501
Date: Fri, 9 Feb 2024 16:11:04 GMT   (383kb,D)

Title: Scalable Interactive Machine Learning for Future Command and Control
Authors: Anna Madison, Ellen Novoseller, Vinicius G. Goecks, Benjamin T. Files,
  Nicholas Waytowich, Alfred Yu, Vernon J. Lawhern, Steven Thurman, Christopher
  Kelshaw, Kaleb McDowell
Categories: cs.LG cs.AI cs.CL cs.HC
Comments: Submitted to the NATO Science and Technology Organization Symposium
  (ICMCIS) organized by the Information Systems Technology (IST) Panel,
  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
ACM-class: I.2.6; I.2.7; J.7
\\
  Future warfare will require Command and Control (C2) personnel to make
decisions at shrinking timescales in complex and potentially ill-defined
situations. Given the need for robust decision-making processes and
decision-support tools, integration of artificial and human intelligence holds
the potential to revolutionize the C2 operations process to ensure adaptability
and efficiency in rapidly changing operational environments. We propose to
leverage recent promising breakthroughs in interactive machine learning, in
which humans can cooperate with machine learning algorithms to guide machine
learning algorithm behavior. This paper identifies several gaps in
state-of-the-art science and technology that future work should address to
extend these approaches to function in complex C2 contexts. In particular, we
describe three research focus areas that together, aim to enable scalable
interactive machine learning (SIML): 1) developing human-AI interaction
algorithms to enable planning in complex, dynamic situations; 2) fostering
resilient human-AI teams through optimizing roles, configurations, and trust;
and 3) scaling algorithms and human-AI teams for flexibility across a range of
potential contexts and situations.
\\ ( https://arxiv.org/abs/2402.06501 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06512
Date: Fri, 9 Feb 2024 16:18:38 GMT   (1373kb,D)

Title: Multimodal Clinical Trial Outcome Prediction with Large Language Models
Authors: Wenhao Zheng, Dongsheng Peng, Hongxia Xu, Hongtu Zhu, Tianfan Fu,
  Huaxiu Yao
Categories: cs.LG cs.CL
\\
  The clinical trial is a pivotal and costly process, often spanning multiple
years and requiring substantial financial resources. Therefore, the development
of clinical trial outcome prediction models aims to exclude drugs likely to
fail and holds the potential for significant cost savings. Recent data-driven
attempts leverage deep learning methods to integrate multimodal data for
predicting clinical trial outcomes. However, these approaches rely on manually
designed modal-specific encoders, which limits both the extensibility to adapt
new modalities and the ability to discern similar information patterns across
different modalities. To address these issues, we propose a multimodal
mixture-of-experts (LIFTED) approach for clinical trial outcome prediction.
Specifically, LIFTED unifies different modality data by transforming them into
natural language descriptions. Then, LIFTED constructs unified noise-resilient
encoders to extract information from modal-specific language descriptions.
Subsequently, a sparse Mixture-of-Experts framework is employed to further
refine the representations, enabling LIFTED to identify similar information
patterns across different modalities and extract more consistent
representations from those patterns using the same expert model. Finally, a
mixture-of-experts module is further employed to dynamically integrate
different modality representations for prediction, which gives LIFTED the
ability to automatically weigh different modalities and pay more attention to
critical information. The experiments demonstrate that LIFTED significantly
enhances performance in predicting clinical trial outcomes across all three
phases compared to the best baseline, showcasing the effectiveness of our
proposed key components.
\\ ( https://arxiv.org/abs/2402.06512 ,  1373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06530
Date: Fri, 9 Feb 2024 16:41:50 GMT   (1981kb,D)

Title: Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite
  Kernel Strategy in One-Class Classification
Authors: Muhammad Uzair Zahid, Aysen Degerli, Fahad Sohrab, Serkan Kiranyaz,
  and Moncef Gabbouj
Categories: cs.LG cs.AI eess.SP
\\
  Early detection of myocardial infarction (MI), a critical condition arising
from coronary artery disease (CAD), is vital to prevent further myocardial
damage. This study introduces a novel method for early MI detection using a
one-class classification (OCC) algorithm in echocardiography. Our study
overcomes the challenge of limited echocardiography data availability by
adopting a novel approach based on Multi-modal Subspace Support Vector Data
Description. The proposed technique involves a specialized MI detection
framework employing multi-view echocardiography incorporating a composite
kernel in the non-linear projection trick, fusing Gaussian and Laplacian
sigmoid functions. Additionally, we enhance the update strategy of the
projection matrices by adapting maximization for both or one of the modalities
in the optimization process. Our method boosts MI detection capability by
efficiently transforming features extracted from echocardiography data into an
optimized lower-dimensional subspace. The OCC model trained specifically on
target class instances from the comprehensive HMC-QU dataset that includes
multiple echocardiography views indicates a marked improvement in MI detection
accuracy. Our findings reveal that our proposed multi-view approach achieves a
geometric mean of 71.24\%, signifying a substantial advancement in
echocardiography-based MI diagnosis and offering more precise and efficient
diagnostic tools.
\\ ( https://arxiv.org/abs/2402.06530 ,  1981kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06532
Date: Fri, 9 Feb 2024 16:43:57 GMT   (963kb,D)

Title: Generative Adversarial Bayesian Optimization for Surrogate Objectives
Authors: Michael S. Yao, Yimeng Zeng, Hamsa Bastani, Jacob Gardner, James C.
  Gee, Osbert Bastani
Categories: cs.LG cs.AI
Comments: 15 pages, 3 figures
\\
  Offline model-based policy optimization seeks to optimize a learned surrogate
objective function without querying the true oracle objective during
optimization. However, inaccurate surrogate model predictions are frequently
encountered along the optimization trajectory. To address this limitation, we
propose generative adversarial Bayesian optimization (GABO) using adaptive
source critic regularization, a task-agnostic framework for Bayesian
optimization that employs a Lipschitz-bounded source critic model to constrain
the optimization trajectory to regions where the surrogate function is
reliable. We show that under certain assumptions for the continuous input space
prior, our algorithm dynamically adjusts the strength of the source critic
regularization. GABO outperforms existing baselines on a number of different
offline optimization tasks across a variety of scientific domains. Our code is
available at https://github.com/michael-s-yao/gabo
\\ ( https://arxiv.org/abs/2402.06532 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06552
Date: Fri, 9 Feb 2024 17:07:31 GMT   (23405kb,D)

Title: Deceptive Path Planning via Reinforcement Learning with Graph Neural
  Networks
Authors: Michael Y. Fatemi and Wesley A. Suttle and Brian M. Sadler
Categories: cs.LG
Comments: 11 pages, 14 figures
MSC-class: 68T05
\\
  Deceptive path planning (DPP) is the problem of designing a path that hides
its true goal from an outside observer. Existing methods for DPP rely on
unrealistic assumptions, such as global state observability and perfect model
knowledge, and are typically problem-specific, meaning that even minor changes
to a previously solved problem can force expensive computation of an entirely
new solution. Given these drawbacks, such methods do not generalize to unseen
problem instances, lack scalability to realistic problem sizes, and preclude
both on-the-fly tunability of deception levels and real-time adaptivity to
changing environments. In this paper, we propose a reinforcement learning
(RL)-based scheme for training policies to perform DPP over arbitrary weighted
graphs that overcomes these issues. The core of our approach is the
introduction of a local perception model for the agent, a new state space
representation distilling the key components of the DPP problem, the use of
graph neural network-based policies to facilitate generalization and scaling,
and the introduction of new deception bonuses that translate the deception
objectives of classical methods to the RL setting. Through extensive
experimentation we show that, without additional fine-tuning, at test time the
resulting policies successfully generalize, scale, enjoy tunable levels of
deception, and adapt in real-time to changes in the environment.
\\ ( https://arxiv.org/abs/2402.06552 ,  23405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06559
Date: Fri, 9 Feb 2024 17:18:33 GMT   (25253kb,D)

Title: Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous
  Driving and Zero-Shot Instruction Following
Authors: Brian Yang, Huangyuan Su, Nikolaos Gkanatsios, Tsung-Wei Ke, Ayush
  Jain, Jeff Schneider, Katerina Fragkiadaki
Categories: cs.LG cs.AI cs.CL cs.RO
\\
  Diffusion models excel at modeling complex and multimodal trajectory
distributions for decision-making and control. Reward-gradient guided denoising
has been recently proposed to generate trajectories that maximize both a
differentiable reward function and the likelihood under the data distribution
captured by a diffusion model. Reward-gradient guided denoising requires a
differentiable reward function fitted to both clean and noised samples,
limiting its applicability as a general trajectory optimizer. In this paper, we
propose DiffusionES, a method that combines gradient-free optimization with
trajectory denoising to optimize black-box non-differentiable objectives while
staying in the data manifold. Diffusion-ES samples trajectories during
evolutionary search from a diffusion model and scores them using a black-box
reward function. It mutates high-scoring trajectories using a truncated
diffusion process that applies a small number of noising and denoising steps,
allowing for much more efficient exploration of the solution space. We show
that DiffusionES achieves state-of-the-art performance on nuPlan, an
established closed-loop planning benchmark for autonomous driving. Diffusion-ES
outperforms existing sampling-based planners, reactive deterministic or
diffusion-based policies, and reward-gradient guidance. Additionally, we show
that unlike prior guidance methods, our method can optimize non-differentiable
language-shaped reward functions generated by few-shot LLM prompting. When
guided by a human teacher that issues instructions to follow, our method can
generate novel, highly complex behaviors, such as aggressive lane weaving,
which are not present in the training data. This allows us to solve the hardest
nuPlan scenarios which are beyond the capabilities of existing trajectory
optimization methods and driving policies.
\\ ( https://arxiv.org/abs/2402.06559 ,  25253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06563
Date: Fri, 9 Feb 2024 17:27:35 GMT   (383kb)

Title: What is Hiding in Medicine's Dark Matter? Learning with Missing Data in
  Medical Practices
Authors: Neslihan Suzen, Evgeny M. Mirkes, Damian Roland, Jeremy Levesley,
  Alexander N. Gorban, Tim J. Coats
Categories: cs.LG cs.AI cs.CL cs.HC cs.IT math.IT
Comments: 8 pages
Journal-ref: 2023 IEEE International Conference on Big Data (BigData),
  4979-4986
DOI: 10.1109/BigData59044.2023.10386194
\\
  Electronic patient records (EPRs) produce a wealth of data but contain
significant missing information. Understanding and handling this missing data
is an important part of clinical data analysis and if left unaddressed could
result in bias in analysis and distortion in critical conclusions. Missing data
may be linked to health care professional practice patterns and imputation of
missing data can increase the validity of clinical decisions. This study
focuses on statistical approaches for understanding and interpreting the
missing data and machine learning based clinical data imputation using a single
centre's paediatric emergency data and the data from UK's largest clinical
audit for traumatic injury database (TARN). In the study of 56,961 data points
related to initial vital signs and observations taken on children presenting to
an Emergency Department, we have shown that missing data are likely to be
non-random and how these are linked to health care professional practice
patterns. We have then examined 79 TARN fields with missing values for 5,791
trauma cases. Singular Value Decomposition (SVD) and k-Nearest Neighbour (kNN)
based missing data imputation methods are used and imputation results against
the original dataset are compared and statistically tested. We have concluded
that the 1NN imputer is the best imputation which indicates a usual pattern of
clinical decision making: find the most similar patients and take their
attributes as imputation.
\\ ( https://arxiv.org/abs/2402.06563 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06570
Date: Fri, 9 Feb 2024 17:40:51 GMT   (277kb,D)

Title: Distilling Morphology-Conditioned Hypernetworks for Efficient Universal
  Morphology Control
Authors: Zheng Xiong, Risto Vuorio, Jacob Beck, Matthieu Zimmer, Kun Shao,
  Shimon Whiteson
Categories: cs.LG cs.RO
\\
  Learning a universal policy across different robot morphologies can
significantly improve learning efficiency and enable zero-shot generalization
to unseen morphologies. However, learning a highly performant universal policy
requires sophisticated architectures like transformers (TF) that have larger
memory and computational cost than simpler multi-layer perceptrons (MLP). To
achieve both good performance like TF and high efficiency like MLP at inference
time, we propose HyperDistill, which consists of: (1) A morphology-conditioned
hypernetwork (HN) that generates robot-wise MLP policies, and (2) A policy
distillation approach that is essential for successful training. We show that
on UNIMAL, a benchmark with hundreds of diverse morphologies, HyperDistill
performs as well as a universal TF teacher policy on both training and unseen
test robots, but reduces model size by 6-14 times, and computational cost by
67-160 times in different environments. Our analysis attributes the efficiency
advantage of HyperDistill at inference time to knowledge decoupling, i.e., the
ability to decouple inter-task and intra-task knowledge, a general principle
that could also be applied to improve inference efficiency in other domains.
\\ ( https://arxiv.org/abs/2402.06570 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06578
Date: Fri, 9 Feb 2024 17:51:43 GMT   (1002kb,D)

Title: On the Universality of Coupling-based Normalizing Flows
Authors: Felix Draxler, Stefan Wahl, Christoph Schn\"orr, Ullrich K\"othe
Categories: cs.LG stat.ML
Comments: under review
\\
  We present a novel theoretical framework for understanding the expressive
power of coupling-based normalizing flows such as RealNVP. Despite their
prevalence in scientific applications, a comprehensive understanding of
coupling flows remains elusive due to their restricted architectures. Existing
theorems fall short as they require the use of arbitrarily ill-conditioned
neural networks, limiting practical applicability. Additionally, we demonstrate
that these constructions inherently lead to volume-preserving flows, a property
which we show to be a fundamental constraint for expressivity. We propose a new
distributional universality theorem for coupling-based normalizing flows, which
overcomes several limitations of prior work. Our results support the general
wisdom that the coupling architecture is expressive and provide a nuanced view
for choosing the expressivity of coupling functions, bridging a gap between
empirical results and theoretical understanding.
\\ ( https://arxiv.org/abs/2402.06578 ,  1002kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06580
Date: Fri, 9 Feb 2024 17:55:01 GMT   (11692kb,D)

Title: SAE: Single Architecture Ensemble Neural Networks
Authors: Martin Ferianc, Hongxiang Fan, Miguel Rodrigues
Categories: cs.LG
Comments: 32 pages
\\
  Ensembles of separate neural networks (NNs) have shown superior accuracy and
confidence calibration over single NN across tasks. Recent methods compress
ensembles within a single network via early exits or multi-input multi-output
frameworks. However, the landscape of these methods is fragmented thus far,
making it difficult to choose the right approach for a given task. Furthermore,
the algorithmic performance of these methods is behind the ensemble of separate
NNs and requires extensive architecture tuning. We propose a novel methodology
unifying these approaches into a Single Architecture Ensemble (SAE). Our method
learns the optimal number and depth of exits per ensemble input in a single NN.
This enables the SAE framework to flexibly tailor its configuration for a given
architecture or application. We evaluate SAEs on image classification and
regression across various network architecture types and sizes. We demonstrate
competitive accuracy or confidence calibration to baselines while reducing the
compute operations or parameter count by up to $1.5{\sim}3.7\times$.
\\ ( https://arxiv.org/abs/2402.06580 ,  11692kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06606
Date: Fri, 9 Feb 2024 18:34:08 GMT   (206kb,D)

Title: RQP-SGD: Differential Private Machine Learning through Noisy SGD and
  Randomized Quantization
Authors: Ce Feng, Parv Venkitasubramaniam
Categories: cs.LG cs.AI cs.CR
Comments: This work is accepted by the 5th AAAI Workshop on Privacy-Preserving
  Artificial Intelligence
\\
  The rise of IoT devices has prompted the demand for deploying machine
learning at-the-edge with real-time, efficient, and secure data processing. In
this context, implementing machine learning (ML) models with real-valued weight
parameters can prove to be impractical particularly for large models, and there
is a need to train models with quantized discrete weights. At the same time,
these low-dimensional models also need to preserve privacy of the underlying
dataset. In this work, we present RQP-SGD, a new approach for
privacy-preserving quantization to train machine learning models for low-memory
ML-at-the-edge. This approach combines differentially private stochastic
gradient descent (DP-SGD) with randomized quantization, providing a measurable
privacy guarantee in machine learning. In particular, we study the utility
convergence of implementing RQP-SGD on ML tasks with convex objectives and
quantization constraints and demonstrate its efficacy over deterministic
quantization. Through experiments conducted on two datasets, we show the
practical effectiveness of RQP-SGD.
\\ ( https://arxiv.org/abs/2402.06606 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06614
Date: Fri, 9 Feb 2024 18:45:00 GMT   (152kb,D)

Title: The Complexity of Sequential Prediction in Dynamical Systems
Authors: Vinod Raman, Unique Subedi, Ambuj Tewari
Categories: cs.LG stat.ML
Comments: 35 pages
\\
  We study the problem of learning to predict the next state of a dynamical
system when the underlying evolution function is unknown. Unlike previous work,
we place no parametric assumptions on the dynamical system, and study the
problem from a learning theory perspective. We define new combinatorial
measures and dimensions and show that they quantify the optimal mistake and
regret bounds in the realizable and agnostic setting respectively.
\\ ( https://arxiv.org/abs/2402.06614 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06627
Date: Fri, 9 Feb 2024 18:59:29 GMT   (350kb,D)

Title: Feedback Loops With Language Models Drive In-Context Reward Hacking
Authors: Alexander Pan and Erik Jones and Meena Jagadeesan and Jacob Steinhardt
Categories: cs.LG cs.AI cs.CL
Comments: 44 pages, 12 figures
\\
  Language models influence the external world: they query APIs that read and
write to web pages, generate content that shapes human behavior, and run system
commands as autonomous agents. These interactions form feedback loops: LLM
outputs affect the world, which in turn affect subsequent LLM outputs. In this
work, we show that feedback loops can cause in-context reward hacking (ICRH),
where the LLM at test-time optimizes a (potentially implicit) objective but
creates negative side effects in the process. For example, consider an LLM
agent deployed to increase Twitter engagement; the LLM may retrieve its
previous tweets into the context window and make them more controversial,
increasing engagement but also toxicity. We identify and study two processes
that lead to ICRH: output-refinement and policy-refinement. For these
processes, evaluations on static datasets are insufficient -- they miss the
feedback effects and thus cannot capture the most harmful behavior. In
response, we provide three recommendations for evaluation to capture more
instances of ICRH. As AI development accelerates, the effects of feedback loops
will proliferate, increasing the need to understand their role in shaping LLM
behavior.
\\ ( https://arxiv.org/abs/2402.06627 ,  350kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.05941 (*cross-listing*)
Date: Fri, 2 Feb 2024 02:11:31 GMT   (15578kb,D)

Title: Character-based Outfit Generation with Vision-augmented Style Extraction
  via LLMs
Authors: Najmeh Forouzandehmehr, Yijie Cao, Nikhil Thakurdesai, Ramin Giahi,
  Luyi Ma, Nima Farrokhsiar, Jianpeng Xu, Evren Korpeoglu, Kannan Achan
Categories: cs.IR cs.AI cs.LG
Comments: 7 pages, 4 figures, IEEE Big Data 2023 3rd Workshop on Multimodal AI
  (MMAI 2023), IEEE BigData 2023
\\
  The outfit generation problem involves recommending a complete outfit to a
user based on their interests. Existing approaches focus on recommending items
based on anchor items or specific query styles but do not consider customer
interests in famous characters from movie, social media, etc. In this paper, we
define a new Character-based Outfit Generation (COG) problem, designed to
accurately interpret character information and generate complete outfit sets
according to customer specifications such as age and gender. To tackle this
problem, we propose a novel framework LVA-COG that leverages Large Language
Models (LLMs) to extract insights from customer interests (e.g., character
information) and employ prompt engineering techniques for accurate
understanding of customer preferences. Additionally, we incorporate
text-to-image models to enhance the visual understanding and generation
(factual or counterfactual) of cohesive outfits. Our framework integrates LLMs
with text-to-image models and improves the customer's approach to fashion by
generating personalized recommendations. With experiments and case studies, we
demonstrate the effectiveness of our solution from multiple dimensions.
\\ ( https://arxiv.org/abs/2402.05941 ,  15578kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05975 (*cross-listing*)
Date: Sun, 4 Feb 2024 17:47:03 GMT   (9493kb)

Title: A Deep Learning Approach for Brain Tumor Classification and Segmentation
  Using a Multiscale Convolutional Neural Network
Authors: Francisco Javier D\'iaz-Pernas, Mario Mart\'inez-Zarzuela, M\'iriam
  Ant\'on-Rodr\'iguez, and David Gonz\'alez-Ortega
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 14 pages
Journal-ref: Healthcare 2021, 9, 153
DOI: 10.3390/healthcare9020153
\\
  In this paper, we present a fully automatic brain tumor segmentation and
classification model using a Deep Convolutional Neural Network that includes a
multiscale approach. One of the differences of our proposal with respect to
previous works is that input images are processed in three spatial scales along
different processing pathways. This mechanism is inspired in the inherent
operation of the Human Visual System. The proposed neural model can analyze MRI
images containing three types of tumors: meningioma, glioma, and pituitary
tumor, over sagittal, coronal, and axial views and does not need preprocessing
of input images to remove skull or vertebral column parts in advance. The
performance of our method on a publicly available MRI image dataset of 3064
slices from 233 patients is compared with previously classical machine learning
and deep learning published methods. In the comparison, our method remarkably
obtained a tumor classification accuracy of 0.973, higher than the other
approaches using the same database.
\\ ( https://arxiv.org/abs/2402.05975 ,  9493kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05977 (*cross-listing*)
Date: Wed, 7 Feb 2024 22:25:54 GMT   (2258kb)

Title: Tool wear monitoring using an online, automatic and low cost system
  based on local texture
Authors: M. T. Garc\'ia-Ord\'as, E. Alegre-Guti\'errez, R. Alaiz-Rodr\'iguez,
  V. Gonz\'alez-Castro
Categories: cs.CV cs.AI cs.LG
\\
  In this work we propose a new online, low cost and fast approach based on
computer vision and machine learning to determine whether cutting tools used in
edge profile milling processes are serviceable or disposable based on their
wear level. We created a new dataset of 254 images of edge profile cutting
heads which is, to the best of our knowledge, the first publicly available
dataset with enough quality for this purpose. All the inserts were segmented
and their cutting edges were cropped, obtaining 577 images of cutting edges:
301 functional and 276 disposable. The proposed method is based on (1) dividing
the cutting edge image in different regions, called Wear Patches (WP), (2)
characterising each one as worn or serviceable using texture descriptors based
on different variants of Local Binary Patterns (LBP) and (3) determine, based
on the state of these WP, if the cutting edge (and, therefore, the tool) is
serviceable or disposable. We proposed and assessed five different patch
division configurations. The individual WP were classified by a Support Vector
Machine (SVM) with an intersection kernel. The best patch division
configuration and texture descriptor for the WP achieves an accuracy of 90.26%
in the detection of the disposable cutting edges. These results show a very
promising opportunity for automatic wear monitoring in edge profile milling
processes.
\\ ( https://arxiv.org/abs/2402.05977 ,  2258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05978 (*cross-listing*)
Date: Wed, 7 Feb 2024 22:27:16 GMT   (1215kb)

Title: Combining shape and contour features to improve tool wear monitoring in
  milling processes
Authors: M. T. Garc\'ia-Ord\'as, E. Alegre-Guti\'errez, V. Gonz\'alez-Castro,
  R. Alaiz-Rodr\'iguez
Categories: cs.CV cs.AI cs.LG
\\
  In this paper, a new system based on combinations of a shape descriptor and a
contour descriptor has been proposed for classifying inserts in milling
processes according to their wear level following a computer vision based
approach. To describe the wear region shape we have proposed a new descriptor
called ShapeFeat and its contour has been characterized using the method
BORCHIZ that, to the best of our knowledge, achieves the best performance for
tool wear monitoring following a computer vision-based approach. Results show
that the combination of BORCHIZ with ShapeFeat using a late fusion method
improves the classification performance significantly, obtaining an accuracy of
91.44% in the binary classification (i.e. the classification of the wear as
high or low) and 82.90% using three target classes (i.e. classification of the
wear as high, medium or low). These results outperform the ones obtained by
both descriptors used on their own, which achieve accuracies of 88.70 and
80.67% for two and three classes, respectively, using ShapeFeat and 87.06 and
80.24% with B-ORCHIZ. This study yielded encouraging results for the
manufacturing community in order to classify automatically the inserts in terms
of their wear for milling processes.
\\ ( https://arxiv.org/abs/2402.05978 ,  1215kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05979 (*cross-listing*)
Date: Wed, 7 Feb 2024 22:29:42 GMT   (635kb,D)

Title: On the Standardization of Behavioral Use Clauses and Their Adoption for
  Responsible Licensing of AI
Authors: Daniel McDuff, Tim Korjakow, Scott Cambo, Jesse Josua Benjamin, Jenny
  Lee, Yacine Jernite, Carlos Mu\~noz Ferrandis, Aaron Gokaslan, Alek
  Tarkowski, Joseph Lindley, A. Feder Cooper, Danish Contractor
Categories: cs.SE cs.AI
\\
  Growing concerns over negligent or malicious uses of AI have increased the
appetite for tools that help manage the risks of the technology. In 2018,
licenses with behaviorial-use clauses (commonly referred to as Responsible AI
Licenses) were proposed to give developers a framework for releasing AI assets
while specifying their users to mitigate negative applications. As of the end
of 2023, on the order of 40,000 software and model repositories have adopted
responsible AI licenses licenses. Notable models licensed with behavioral use
clauses include BLOOM (language) and LLaMA2 (language), Stable Diffusion
(image), and GRID (robotics). This paper explores why and how these licenses
have been adopted, and why and how they have been adapted to fit particular use
cases. We use a mixed-methods methodology of qualitative interviews, clustering
of license clauses, and quantitative analysis of license adoption. Based on
this evidence we take the position that responsible AI licenses need
standardization to avoid confusing users or diluting their impact. At the same
time, customization of behavioral restrictions is also appropriate in some
contexts (e.g., medical domains). We advocate for ``standardized
customization'' that can meet users' needs and can be supported via tooling.
\\ ( https://arxiv.org/abs/2402.05979 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05980 (*cross-listing*)
Date: Thu, 8 Feb 2024 06:48:01 GMT   (414kb,D)

Title: Do Large Code Models Understand Programming Concepts? A Black-box
  Approach
Authors: Ashish Hooda, Mihai Christodorescu, Miltos Allamanis, Aaron Wilson,
  Kassem Fawaz, Somesh Jha
Categories: cs.SE cs.AI cs.LG cs.PL
\\
  Large Language Models' success on text generation has also made them better
at code generation and coding tasks. While a lot of work has demonstrated their
remarkable performance on tasks such as code completion and editing, it is
still unclear as to why. We help bridge this gap by exploring to what degree
auto-regressive models understand the logical constructs of the underlying
programs. We propose Counterfactual Analysis for Programming Concept Predicates
(CACP) as a counterfactual testing framework to evaluate whether Large Code
Models understand programming concepts. With only black-box access to the
model, we use CACP to evaluate ten popular Large Code Models for four different
programming concepts. Our findings suggest that current models lack
understanding of concepts such as data flow and control flow.
\\ ( https://arxiv.org/abs/2402.05980 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06004 (*cross-listing*)
Date: Thu, 8 Feb 2024 19:01:14 GMT   (94kb,D)

Title: Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank
  Compression Strategy
Authors: Seyedarmin Azizi, Mahdi Nazemi, Massoud Pedram
Categories: cs.CV cs.AI stat.ML
\\
  As Vision Transformers (ViTs) increasingly set new benchmarks in computer
vision, their practical deployment on inference engines is often hindered by
their significant memory bandwidth and (on-chip) memory footprint requirements.
This paper addresses this memory limitation by introducing an activation-aware
model compression methodology that uses selective low-rank weight tensor
approximations of different layers to reduce the parameter count of ViTs. The
key idea is to decompose the weight tensors into a sum of two
parameter-efficient tensors while minimizing the error between the product of
the input activations with the original weight tensor and the product of the
input activations with the approximate tensor sum. This approximation is
further refined by adopting an efficient layer-wise error compensation
technique that uses the gradient of the layer's output loss. The combination of
these techniques achieves excellent results while it avoids being trapped in a
shallow local minimum early in the optimization process and strikes a good
balance between the model compression and output accuracy. Notably, the
presented method significantly reduces the parameter count of DeiT-B by 60%
with less than 1% accuracy drop on the ImageNet dataset, overcoming the usual
accuracy degradation seen in low-rank approximations. In addition to this, the
presented compression technique can compress large DeiT/ViT models to have
about the same model size as smaller DeiT/ViT variants while yielding up to
1.8% accuracy gain. These results highlight the efficacy of our approach,
presenting a viable solution for embedding ViTs in memory-constrained
environments without compromising their performance.
\\ ( https://arxiv.org/abs/2402.06004 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06026 (*cross-listing*)
Date: Thu, 8 Feb 2024 19:57:57 GMT   (1382kb,D)

Title: Quantum neural network with ensemble learning to mitigate barren
  plateaus and cost function concentration
Authors: Lucas Friedrich, Jonas Maziero
Categories: quant-ph cs.AI
\\
  The rapid development of quantum computers promises transformative impacts
across diverse fields of science and technology. Quantum neural networks
(QNNs), as a forefront application, hold substantial potential. Despite the
multitude of proposed models in the literature, persistent challenges, notably
the vanishing gradient (VG) and cost function concentration (CFC) problems,
impede their widespread success. In this study, we introduce a novel approach
to quantum neural network construction, specifically addressing the issues of
VG and CFC. Our methodology employs ensemble learning, advocating for the
simultaneous deployment of multiple quantum circuits with a depth equal to $1$,
a departure from the conventional use of a single quantum circuit with depth
$L$. We assess the efficacy of our proposed model through a comparative
analysis with a conventionally constructed QNN. The evaluation unfolds in the
context of a classification problem, yielding valuable insights into the
potential advantages of our innovative approach.
\\ ( https://arxiv.org/abs/2402.06026 ,  1382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06046 (*cross-listing*)
Date: Thu, 8 Feb 2024 20:37:51 GMT   (577kb)

Title: Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging
  Mishap
Authors: Philip Koopman
Categories: cs.RO cs.AI
Comments: 17 pages, 2 figures
\\
  An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San
Francisco resulted not only in a severe injury, but also dramatic upheaval at
that company that will likely have lasting effects throughout the industry. The
issues stem not just from the crash facts themselves, but also how Cruise
mishandled dealing with their robotaxi dragging a pedestrian under the vehicle
after the initial post-crash stop. A pair of external investigation reports
provide raw material describing the incident and critique the company response
from a regulatory interaction point of view, but did not include potential
safety recommendations in scope. We use that report material to highlight
specific facts and relationships between events by tying together different
pieces of the report material. We then explore safety lessons that might be
learned with regard to technology, operational safety practices, and
organizational reaction to incidents.
\\ ( https://arxiv.org/abs/2402.06046 ,  577kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06053 (*cross-listing*)
Date: Thu, 8 Feb 2024 20:49:09 GMT   (2048kb,D)

Title: Randomness Is All You Need: Semantic Traversal of Problem-Solution
  Spaces with Large Language Models
Authors: Thomas Sandholm, Sayandev Mukherjee, Bernardo A. Huberman
Categories: cs.HC cs.AI cs.CY
\\
  We present a novel approach to exploring innovation problem and solution
domains using LLM fine-tuning with a custom idea database. By semantically
traversing the bi-directional problem and solution tree at different
temperature levels we achieve high diversity in solution edit distance while
still remaining close to the original problem statement semantically. In
addition to finding a variety of solutions to a given problem, this method can
also be used to refine and clarify the original problem statement. As further
validation of the approach, we implemented a proof-of-concept Slack bot to
serve as an innovation assistant.
\\ ( https://arxiv.org/abs/2402.06053 ,  2048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06078 (*cross-listing*)
Date: Thu, 8 Feb 2024 22:05:45 GMT   (334kb,D)

Title: Gaussian Mixture Models for Affordance Learning using Bayesian Networks
Authors: Pedro Os\'orio, Alexandre Bernardino, Ruben Martinez-Cantin, Jos\'e
  Santos-Victor
Categories: cs.RO cs.AI
Comments: IEEE/RSJ International Conference on Intelligent Robots and Systems
  2010
Journal-ref: Published on the Proceedings of the IEEE/RSJ International
  Conference on Intelligent Robots and Systems 2010
DOI: 10.1109/IROS.2010.5650297
\\
  Affordances are fundamental descriptors of relationships between actions,
objects and effects. They provide the means whereby a robot can predict
effects, recognize actions, select objects and plan its behavior according to
desired goals. This paper approaches the problem of an embodied agent exploring
the world and learning these affordances autonomously from its sensory
experiences. Models exist for learning the structure and the parameters of a
Bayesian Network encoding this knowledge. Although Bayesian Networks are
capable of dealing with uncertainty and redundancy, previous work considered
complete observability of the discrete sensory data, which may lead to hard
errors in the presence of noise. In this paper we consider a probabilistic
representation of the sensors by Gaussian Mixture Models (GMMs) and explicitly
taking into account the probability distribution contained in each discrete
affordance concept, which can lead to a more correct learning.
\\ ( https://arxiv.org/abs/2402.06078 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06079 (*cross-listing*)
Date: Thu, 8 Feb 2024 22:06:55 GMT   (3616kb,D)

Title: DiscDiff: Latent Diffusion Model for DNA Sequence Generation
Authors: Zehui Li, Yuhao Ni, William A V Beardall, Guoxuan Xia, Akashaditya
  Das, Guy-Bart Stan, Yiren Zhao
Categories: q-bio.GN cs.AI cs.LG
Comments: Different from the prior work "Latent Diffusion Model for DNA
  Sequence Generation" (arXiv:2310.06150), we updated the evaluation framework
  and compared the DiscDiff with other methods comprehensively. In addition, a
  post-training framework is proposed to increase the quality of generated
  sequences
\\
  This paper introduces a novel framework for DNA sequence generation,
comprising two key components: DiscDiff, a Latent Diffusion Model (LDM)
tailored for generating discrete DNA sequences, and Absorb-Escape, a
post-training algorithm designed to refine these sequences. Absorb-Escape
enhances the realism of the generated sequences by correcting `round errors'
inherent in the conversion process between latent and input spaces. Our
approach not only sets new standards in DNA sequence generation but also
demonstrates superior performance over existing diffusion models, in generating
both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the
first comprehensive, multi-species dataset for DNA generation, encompassing
160,000 unique sequences from 15 species. We hope this study will advance the
generative modelling of DNA, with potential implications for gene therapy and
protein production.
\\ ( https://arxiv.org/abs/2402.06079 ,  3616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06086 (*cross-listing*)
Date: Thu, 8 Feb 2024 22:38:14 GMT   (319kb,D)

Title: Rhizomes to Load Balance Skewed In-Degree Distributions
Authors: Bibrak Qamar Chandio
Categories: cs.DC cs.AI cs.DS
ACM-class: C.1.4; C.3; C.4; D.1.3
\\
  The paper aims to address load imbalance caused by high in-degree
distribution in graphs by applying the idea of rhizome to vertex-centric
message-driven graph processing. Rhizome construction of the graph creates
multiple named vertex address for any number of single large in-degree
vertices. It then allows other vertices to point to any of the named addresses
thus sharing the in-degree load. The rhizomes internally communicate and remain
consistent to provide a unified and correct view of the vertex. Simulated
experimental results show performance speed ups for BFS graph traversal on
large chip sizes for the tested input graph datasets containing highly skewed
in-degree distribution. The improvements come from sharing the in-degree
compute workload among memory-processing elements and also lowering contention
on the network-on-chip.
\\ ( https://arxiv.org/abs/2402.06086 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06107 (*cross-listing*)
Date: Fri, 9 Feb 2024 00:01:42 GMT   (5334kb,D)

Title: Multiple Instance Learning for Cheating Detection and Localization in
  Online Examinations
Authors: Yemeng Liu, Jing Ren, Jianshuo Xu, Xiaomei Bai, Roopdeep Kaur, Feng
  Xia
Categories: cs.CV cs.AI cs.CY cs.LG
Comments: 12 pages, 7 figures
MSC-class: 68T40, 68T45
ACM-class: I.2.10; I.5.4
Journal-ref: IEEE Transactions on Cognitive and Developmental Systems 2024
DOI: 10.1109/TCDS.2024.3349705
\\
  The spread of the Coronavirus disease-2019 epidemic has caused many courses
and exams to be conducted online. The cheating behavior detection model in
examination invigilation systems plays a pivotal role in guaranteeing the
equality of long-distance examinations. However, cheating behavior is rare, and
most researchers do not comprehensively take into account features such as head
posture, gaze angle, body posture, and background information in the task of
cheating behavior detection. In this paper, we develop and present CHEESE, a
CHEating detection framework via multiplE inStancE learning. The framework
consists of a label generator that implements weak supervision and a feature
encoder to learn discriminative features. In addition, the framework combines
body posture and background features extracted by 3D convolution with eye gaze,
head posture and facial features captured by OpenFace 2.0. These features are
fed into the spatio-temporal graph module by stitching to analyze the
spatio-temporal changes in video clips to detect the cheating behaviors. Our
experiments on three datasets, UCF-Crime, ShanghaiTech and Online Exam
Proctoring (OEP), prove the effectiveness of our method as compared to the
state-of-the-art approaches, and obtain the frame-level AUC score of 87.58% on
the OEP dataset.
\\ ( https://arxiv.org/abs/2402.06107 ,  5334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06116 (*cross-listing*)
Date: Fri, 9 Feb 2024 00:58:57 GMT   (15458kb,D)

Title: LLMs for Coding and Robotics Education
Authors: Peng Shu, Huaqin Zhao, Hanqi Jiang, Yiwei Li, Shaochen Xu, Yi Pan,
  Zihao Wu, Zhengliang Liu, Guoyu Lu, Le Guan, Gong Chen, Xianqiao Wang
  Tianming Liu
Categories: cs.RO cs.AI
Comments: 20 pages, 6 figures, 1 table
\\
  Large language models and multimodal large language models have
revolutionized artificial intelligence recently. An increasing number of
regions are now embracing these advanced technologies. Within this context,
robot coding education is garnering increasing attention. To teach young
children how to code and compete in robot challenges, large language models are
being utilized for robot code explanation, generation, and modification. In
this paper, we highlight an important trend in robot coding education. We test
several mainstream large language models on both traditional coding tasks and
the more challenging task of robot code generation, which includes block
diagrams. Our results show that GPT-4V outperforms other models in all of our
tests but struggles with generating block diagram images.
\\ ( https://arxiv.org/abs/2402.06116 ,  15458kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06118 (*cross-listing*)
Date: Fri, 9 Feb 2024 01:00:14 GMT   (21691kb,D)

Title: ViGoR: Improving Visual Grounding of Large Vision Language Models with
  Fine-Grained Reward Modeling
Authors: Siming Yan, Min Bai, Weifeng Chen, Xiong Zhou, Qixing Huang, Li Erran
  Li
Categories: cs.CV cs.AI
Comments: 10 pages, 3 figures
\\
  By combining natural language understanding and the generation capabilities
and breadth of knowledge of large language models with image perception, recent
large vision language models (LVLMs) have shown unprecedented reasoning
capabilities in the real world. However, the generated text often suffers from
inaccurate grounding in the visual input, resulting in errors such as
hallucinating nonexistent scene elements, missing significant parts of the
scene, and inferring incorrect attributes and relationships between objects. To
address these issues, we introduce a novel framework, ViGoR (Visual Grounding
Through Fine-Grained Reward Modeling) that utilizes fine-grained reward
modeling to significantly enhance the visual grounding of LVLMs over
pre-trained baselines. This improvement is efficiently achieved using much
cheaper human evaluations instead of full supervisions, as well as automated
methods. We show the effectiveness of our approach through numerous metrics on
several benchmarks. Additionally, we construct a comprehensive and challenging
dataset specifically designed to validate the visual grounding capabilities of
LVLMs. Finally, we plan to release our human annotation comprising
approximately 16,000 images and generated text pairs with fine-grained
evaluations to contribute to related research in the community.
\\ ( https://arxiv.org/abs/2402.06118 ,  21691kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06158 (*cross-listing*)
Date: Fri, 9 Feb 2024 03:18:44 GMT   (6530kb,D)

Title: Assortment Planning with Sponsored Products
Authors: Shaojie Tang, Shuzhang Cai, Jing Yuan, Kai Han
Categories: cs.DS cs.AI cs.IR
\\
  In the rapidly evolving landscape of retail, assortment planning plays a
crucial role in determining the success of a business. With the rise of
sponsored products and their increasing prominence in online marketplaces,
retailers face new challenges in effectively managing their product assortment
in the presence of sponsored products. Remarkably, previous research in
assortment planning largely overlooks the existence of sponsored products and
their potential impact on overall recommendation effectiveness. Instead, they
commonly make the simplifying assumption that all products are either organic
or non-sponsored. This research gap underscores the necessity for a more
thorough investigation of the assortment planning challenge when sponsored
products are in play. We formulate the assortment planning problem in the
presence of sponsored products as a combinatorial optimization task. The
ultimate objective is to compute an assortment plan that optimizes expected
revenue while considering the specific requirements of placing sponsored
products strategically.
\\ ( https://arxiv.org/abs/2402.06158 ,  6530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06165 (*cross-listing*)
Date: Fri, 9 Feb 2024 03:48:20 GMT   (1994kb,D)

Title: Learning Contrastive Feature Representations for Facial Action Unit
  Detection
Authors: Ziqiao Shang, Bin Liu, Fei Teng, Tianrui Li
Categories: cs.CV cs.AI cs.LG
Comments: 11 pages, 3 figures, submitted to an IEEE journal
\\
  The predominant approach to facial action unit (AU) detection revolves around
a supervised multi-label binary classification problem. Existing methodologies
often encode pixel-level information of AUs, thereby imposing substantial
demands on model complexity and expressiveness. Moreover, this practice
elevates the susceptibility to overfitting due to the presence of noisy AU
labels. In the present study, we introduce a contrastive learning framework
enhanced by both supervised and self-supervised signals. The objective is to
acquire discriminative features, deviating from the conventional pixel-level
learning paradigm within the domain of AU detection. To address the challenge
posed by noisy AU labels, we augment the supervised signal through the
introduction of a self-supervised signal. This augmentation is achieved through
positive sample sampling, encompassing three distinct types of positive sample
pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we
employ an importance re-weighting strategy tailored for minority AUs. The
resulting loss, denoted as AUNCE, is proposed to encapsulate this strategy. Our
experimental assessments, conducted on two widely-utilized benchmark datasets
(BP4D and DISFA), underscore the superior performance of our approach compared
to state-of-the-art methods in the realm of AU detection.
\\ ( https://arxiv.org/abs/2402.06165 ,  1994kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06178 (*cross-listing*)
Date: Fri, 9 Feb 2024 04:34:08 GMT   (1587kb,D)

Title: MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models
Authors: Yixiao Zhang, Yukara Ikemiya, Gus Xia, Naoki Murata, Marco Mart\'inez,
  Wei-Hsiang Liao, Yuki Mitsufuji, Simon Dixon
Categories: cs.SD cs.AI cs.MM eess.AS
Comments: Project page: https://bit.ly/musicmagus-demo
\\
  Recent advances in text-to-music generation models have opened new avenues in
musical creativity. However, music generation usually involves iterative
refinements, and how to edit the generated music remains a significant
challenge. This paper introduces a novel approach to the editing of music
generated by such models, enabling the modification of specific attributes,
such as genre, mood and instrument, while maintaining other aspects unchanged.
Our method transforms text editing to \textit{latent space manipulation} while
adding an extra constraint to enforce consistency. It seamlessly integrates
with existing pretrained text-to-music diffusion models without requiring
additional training. Experimental results demonstrate superior performance over
both zero-shot and certain supervised baselines in style and timbre transfer
evaluations. Additionally, we showcase the practical applicability of our
approach in real-world music editing scenarios.
\\ ( https://arxiv.org/abs/2402.06178 ,  1587kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06185 (*cross-listing*)
Date: Fri, 9 Feb 2024 04:47:26 GMT   (2300kb,D)

Title: Development and validation of an artificial intelligence model to
  accurately predict spinopelvic parameters
Authors: Edward S. Harake, Joseph R. Linzey, Cheng Jiang, Rushikesh S. Joshi,
  Mark M. Zaki, Jaes C. Jones, Siri S. Khalsa, John H. Lee, Zachary Wilseck,
  Jacob R. Joseph, Todd C. Hollon, and Paul Park
Categories: cs.CV cs.AI cs.LG
Comments: 10 pages, 5 figures, to appear in Journal of Neurosurgery: Spine
\\
  Objective. Achieving appropriate spinopelvic alignment has been shown to be
associated with improved clinical symptoms. However, measurement of spinopelvic
radiographic parameters is time-intensive and interobserver reliability is a
concern. Automated measurement tools have the promise of rapid and consistent
measurements, but existing tools are still limited by some degree of manual
user-entry requirements. This study presents a novel artificial intelligence
(AI) tool called SpinePose that automatically predicts spinopelvic parameters
with high accuracy without the need for manual entry.
  Methods. SpinePose was trained and validated on 761 sagittal whole-spine
X-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic
incidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle
(T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was
labeled by 4 reviewers, including fellowship-trained spine surgeons and a
fellowship-trained radiologist with neuroradiology subspecialty certification.
Median errors relative to the most senior reviewer were calculated to determine
model accuracy on test images. Intraclass correlation coefficients (ICC) were
used to assess inter-rater reliability.
  Results. SpinePose exhibited the following median (interquartile range)
parameter errors: SVA: 2.2(2.3)mm, p=0.93; PT: 1.3(1.2){\deg}, p=0.48; SS:
1.7(2.2){\deg}, p=0.64; PI: 2.2(2.1){\deg}, p=0.24; LL: 2.6(4.0){\deg}, p=0.89;
T1PA: 1.1(0.9){\deg}, p=0.42; and L1PA: 1.4(1.6){\deg}, p=0.49. Model
predictions also exhibited excellent reliability at all parameters (ICC:
0.91-1.0).
  Conclusions. SpinePose accurately predicted spinopelvic parameters with
excellent reliability comparable to fellowship-trained spine surgeons and
neuroradiologists. Utilization of predictive AI tools in spinal imaging can
substantially aid in patient selection and surgical planning.
\\ ( https://arxiv.org/abs/2402.06185 ,  2300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06188 (*cross-listing*)
Date: Fri, 9 Feb 2024 05:05:28 GMT   (3256kb,D)

Title: A self-supervised framework for learning whole slide representations
Authors: Xinhai Hou, Cheng Jiang, Akhil Kondepudi, Yiwei Lyu, Asadur Zaman
  Chowdury, Honglak Lee, Todd C. Hollon
Categories: cs.CV cs.AI cs.LG
Comments: 18 pages, 11 figures
\\
  Whole slide imaging is fundamental to biomedical microscopy and computational
pathology. However, whole slide images (WSIs) present a complex computer vision
challenge due to their gigapixel size, diverse histopathologic features,
spatial heterogeneity, and limited/absent data annotations. These challenges
highlight that supervised training alone can result in suboptimal whole slide
representations. Self-supervised representation learning can achieve
high-quality WSI visual feature learning for downstream diagnostic tasks, such
as cancer diagnosis or molecular genetic prediction. Here, we present a general
self-supervised whole slide learning (S3L) framework for gigapixel-scale
self-supervision of WSIs. S3L combines data transformation strategies from
transformer-based vision and language modeling into a single unified framework
to generate paired views for self-supervision. S3L leverages the inherent
regional heterogeneity, histologic feature variability, and information
redundancy within WSIs to learn high-quality whole-slide representations. We
benchmark S3L visual representations on two diagnostic tasks for two biomedical
microscopy modalities. S3L significantly outperforms WSI baselines for cancer
diagnosis and genetic mutation prediction. Additionally, S3L achieves good
performance using both in-domain and out-of-distribution patch encoders,
demonstrating good flexibility and generalizability.
\\ ( https://arxiv.org/abs/2402.06188 ,  3256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06229 (*cross-listing*)
Date: Fri, 9 Feb 2024 07:44:27 GMT   (4723kb,D)

Title: Exploring Interaction Patterns for Debugging: Enhancing Conversational
  Capabilities of AI-assistants
Authors: Bhavya Chopra, Yasharth Bajpai, Param Biyani, Gustavo Soares, Arjun
  Radhakrishna, Chris Parnin, Sumit Gulwani
Categories: cs.HC cs.AI cs.SE
Comments: 7 pages, 4 figures, 2 tables
\\
  The widespread availability of Large Language Models (LLMs) within Integrated
Development Environments (IDEs) has led to their speedy adoption.
Conversational interactions with LLMs enable programmers to obtain natural
language explanations for various software development tasks. However, LLMs
often leap to action without sufficient context, giving rise to implicit
assumptions and inaccurate responses. Conversations between developers and LLMs
are primarily structured as question-answer pairs, where the developer is
responsible for asking the the right questions and sustaining conversations
across multiple turns. In this paper, we draw inspiration from interaction
patterns and conversation analysis -- to design Robin, an enhanced
conversational AI-assistant for debugging. Through a within-subjects user study
with 12 industry professionals, we find that equipping the LLM to -- (1)
leverage the insert expansion interaction pattern, (2) facilitate turn-taking,
and (3) utilize debugging workflows -- leads to lowered conversation barriers,
effective fault localization, and 5x improvement in bug resolution rates.
\\ ( https://arxiv.org/abs/2402.06229 ,  4723kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06299 (*cross-listing*)
Date: Fri, 9 Feb 2024 10:24:47 GMT   (2853kb,D)

Title: A Functional Analysis Approach to Symbolic Regression
Authors: Kirill Antonov, Roman Kalkreuth, Kaifeng Yang, Thomas B\"ack, Niki van
  Stein, Anna V Kononova
Categories: cs.NE cs.AI
Comments: 14 pages, 3 figures. Submitted to Genetic and Evolutionary
  Computation Conference (GECCO-2024)
\\
  Symbolic regression (SR) poses a significant challenge for randomized search
heuristics due to its reliance on the synthesis of expressions for input-output
mappings. Although traditional genetic programming (GP) algorithms have
achieved success in various domains, they exhibit limited performance when
tree-based representations are used for SR. To address these limitations, we
introduce a novel SR approach called Fourier Tree Growing (FTG) that draws
insights from functional analysis. This new perspective enables us to perform
optimization directly in a different space, thus avoiding intricate symbolic
expressions. Our proposed algorithm exhibits significant performance
improvements over traditional GP methods on a range of classical
one-dimensional benchmarking problems. To identify and explain limiting factors
of GP and FTG, we perform experiments on a large-scale polynomials benchmark
with high-order polynomials up to degree 100. To the best of the authors'
knowledge, this work represents the pioneering application of functional
analysis in addressing SR problems. The superior performance of the proposed
algorithm and insights into the limitations of GP open the way for further
advancing GP for SR and related areas of explainable machine learning.
\\ ( https://arxiv.org/abs/2402.06299 ,  2853kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06304 (*cross-listing*)
Date: Fri, 9 Feb 2024 10:34:01 GMT   (399kb)

Title: A New Approach to Voice Authenticity
Authors: Nicolas M. M\"uller, Piotr Kawa, Shen Hu, Matthias Neu, Jennifer
  Williams, Philip Sperl, Konstantin B\"ottinger
Categories: cs.SD cs.AI eess.AS
\\
  Voice faking, driven primarily by recent advances in text-to-speech (TTS)
synthesis technology, poses significant societal challenges. Currently, the
prevailing assumption is that unaltered human speech can be considered genuine,
while fake speech comes from TTS synthesis. We argue that this binary
distinction is oversimplified. For instance, altered playback speeds can be
used for malicious purposes, like in the 'Drunken Nancy Pelosi' incident.
Similarly, editing of audio clips can be done ethically, e.g., for brevity or
summarization in news reporting or podcasts, but editing can also create
misleading narratives. In this paper, we propose a conceptual shift away from
the binary paradigm of audio being either 'fake' or 'real'. Instead, our focus
is on pinpointing 'voice edits', which encompass traditional modifications like
filters and cuts, as well as TTS synthesis and VC systems. We delineate 6
categories and curate a new challenge dataset rooted in the M-AILABS corpus,
for which we present baseline detection systems. And most importantly, we argue
that merely categorizing audio as fake or real is a dangerous
over-simplification that will fail to move the field of speech technology
forward.
\\ ( https://arxiv.org/abs/2402.06304 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06334 (*cross-listing*)
Date: Fri, 9 Feb 2024 11:23:14 GMT   (73kb,D)

Title: ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs
Authors: Fernando Ferraretto, Thiago Laitz, Roberto Lotufo, Rodrigo Nogueira
Categories: cs.IR cs.AI cs.CL
\\
  ExaRanker recently introduced an approach to training information retrieval
(IR) models, incorporating natural language explanations as additional labels.
The method addresses the challenge of limited labeled examples, leading to
improvements in the effectiveness of IR models. However, the initial results
were based on proprietary language models such as GPT-3.5, which posed
constraints on dataset size due to its cost and data privacy. In this paper, we
introduce ExaRanker-Open, where we adapt and explore the use of open-source
language models to generate explanations. The method has been tested using
different LLMs and datasets sizes to better comprehend the effective
contribution of data augmentation. Our findings reveal that incorporating
explanations consistently enhances neural rankers, with benefits escalating as
the LLM size increases. Notably, the data augmentation method proves
advantageous even with large datasets, as evidenced by ExaRanker surpassing the
target baseline by 0.6 nDCG@10 points in our study. To encourage further
advancements by the research community, we have open-sourced both the code and
datasets at https://github.com/unicamp-dl/ExaRanker.
\\ ( https://arxiv.org/abs/2402.06334 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06360 (*cross-listing*)
Date: Fri, 9 Feb 2024 12:10:00 GMT   (2329kb,D)

Title: CoSearchAgent: A Lightweight Collaborative Search Agent with Large
  Language Models
Authors: Peiyuan Gong, Jiamian Li, Jiaxin Mao
Categories: cs.IR cs.AI cs.CL
Comments: 4 pages, demo
\\
  Collaborative search supports multiple users working together to accomplish a
specific search task. Research has found that designing lightweight
collaborative search plugins within instant messaging platforms aligns better
with users' collaborative habits. However, due to the complexity of multi-user
interaction scenarios, it is challenging to implement a fully functioning
lightweight collaborative search system. Therefore, previous studies on
lightweight collaborative search had to rely on the Wizard of Oz paradigm. In
recent years, large language models (LLMs) have been demonstrated to interact
naturally with users and achieve complex information-seeking tasks through
LLM-based agents. Hence, to better support the research in collaborative
search, in this demo, we propose CoSearchAgent, a lightweight collaborative
search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that
can support collaborative search during multi-party conversations on this
platform. Equipped with the capacity to understand the queries and context in
multi-user conversations and the ability to search the Web for relevant
information via APIs, CoSearchAgent can respond to user queries with answers
grounded on the relevant search results. It can also ask clarifying questions
when the information needs are unclear. The proposed CoSearchAgent is highly
flexible and would be useful for supporting further research on collaborative
search. The code and demo video are accessible.
\\ ( https://arxiv.org/abs/2402.06360 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06388 (*cross-listing*)
Date: Fri, 9 Feb 2024 13:10:04 GMT   (9kb)

Title: On the Convergence Rate of the Stochastic Gradient Descent (SGD) and
  application to a modified policy gradient for the Multi Armed Bandit
Authors: Stefana Anita and Gabriel Turinici
Categories: stat.ML cs.AI cs.DS cs.LG cs.NA math.NA
\\
  We present a self-contained proof of the convergence rate of the Stochastic
Gradient Descent (SGD) when the learning rate follows an inverse time decays
schedule; we next apply the results to the convergence of a modified form of
policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
\\ ( https://arxiv.org/abs/2402.06388 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06397 (*cross-listing*)
Date: Fri, 9 Feb 2024 13:29:44 GMT   (2499kb,D)

Title: Finding hardness reductions automatically using SAT solvers
Authors: Helena Bergold, Manfred Scheucher, Felix Schr\"oder
Categories: cs.CC cs.AI cs.DS cs.LO math.CO
\\
  In this article, we show that the completion problem, i.e. the decision
problem whether a partial structure can be completed to a full structure, is
NP-complete for many combinatorial structures. While the gadgets for most
reductions in literature are found by hand, we present an algorithm to
construct gadgets in a fully automated way. Using our framework which is based
on SAT, we present the first thorough study of the completion problem on sign
mappings with forbidden substructures by classifying thousands of structures
for which the completion problem is NP-complete. Our list in particular
includes interior triple systems, which were introduced by Knuth towards an
axiomatization of planar point configurations. Last but not least, we give an
infinite family of structures generalizing interior triple system to higher
dimensions for which the completion problem is NP-complete.
\\ ( https://arxiv.org/abs/2402.06397 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06472 (*cross-listing*)
Date: Fri, 9 Feb 2024 15:25:36 GMT   (29566kb,D)

Title: "When He Feels Cold, He Goes to the Seahorse"-Blending Generative AI
  into Multimaterial Storymaking for Family Expressive Arts Therapy
Authors: Di Liu, Hanqing Zhou, Pengcheng An
Categories: cs.HC cs.AI
Comments: to appear at ACM CHI '24
DOI: 10.1145/3613904.3642852
\\
  Storymaking, as an integrative form of expressive arts therapy, is an
effective means to foster family communication. Yet, the integration of
generative AI as expressive materials in therapeutic storymaking remains
underexplored. And there is a lack of HCI implications on how to support
families and therapists in this context. Addressing this, our study involved
five weeks of storymaking sessions with seven families guided by a professional
therapist. In these sessions, the families used both traditional art-making
materials and image-based generative AI to create and evolve their family
stories. Via the rich empirical data and commentaries from four expert
therapists, we contextualize how families creatively melded AI and traditional
expressive materials to externalize their ideas and feelings. Through the lens
of Expressive Therapies Continuum (ETC), we characterize the therapeutic
implications of AI as expressive materials. Desirable interaction qualities to
support children, parents, and therapists are distilled for future HCI
research.
\\ ( https://arxiv.org/abs/2402.06472 ,  29566kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06506 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:14:30 GMT   (16535kb,D)

Title: Classifying point clouds at the facade-level using geometric features
  and deep learning networks
Authors: Yue Tan, Olaf Wysocki, Ludwig Hoegner, Uwe Stilla
Categories: cs.CV cs.AI cs.LG
Comments: Accepted to the Recent Advances in 3D Geoinformation Science,
  Proceedings of the 18th 3D GeoInfo Conference 2023
\\
  3D building models with facade details are playing an important role in many
applications now. Classifying point clouds at facade-level is key to create
such digital replicas of the real world. However, few studies have focused on
such detailed classification with deep neural networks. We propose a method
fusing geometric features with deep learning networks for point cloud
classification at facade-level. Our experiments conclude that such early-fused
features improve deep learning methods' performance. This method can be applied
for compensating deep learning networks' ability in capturing local geometric
information and promoting the advancement of semantic segmentation.
\\ ( https://arxiv.org/abs/2402.06506 ,  16535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06599 (*cross-listing*)
Date: Fri, 9 Feb 2024 18:21:51 GMT   (9524kb,D)

Title: On the Out-Of-Distribution Generalization of Multimodal Large Language
  Models
Authors: Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu,
  Yuqing Yang, Shikai Guan, Jiazheng Xu, and Peng Cui
Categories: cs.CV cs.AI
\\
  We investigate the generalization boundaries of current Multimodal Large
Language Models (MLLMs) via comprehensive evaluation under out-of-distribution
scenarios and domain-specific tasks. We evaluate their zero-shot generalization
across synthetic images, real-world distributional shifts, and specialized
datasets like medical and molecular imagery. Empirical results indicate that
MLLMs struggle with generalization beyond common training domains, limiting
their direct application without adaptation. To understand the cause of
unreliable performance, we analyze three hypotheses: semantic
misinterpretation, visual feature extraction insufficiency, and mapping
deficiency. Results identify mapping deficiency as the primary hurdle. To
address this problem, we show that in-context learning (ICL) can significantly
enhance MLLMs' generalization, opening new avenues for overcoming
generalization barriers. We further explore the robustness of ICL under
distribution shifts and show its vulnerability to domain shifts, label shifts,
and spurious correlation shifts between in-context examples and test data.
\\ ( https://arxiv.org/abs/2402.06599 ,  9524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05939 (*cross-listing*)
Date: Fri, 12 Jan 2024 00:00:32 GMT   (8206kb,D)

Title: Uncertainty Awareness of Large Language Models Under Code Distribution
  Shifts: A Benchmark Study
Authors: Yufei Li, Simin Chen, Yanghong Guo, Wei Yang, Yue Dong, Cong Liu
Categories: cs.SE cs.CL cs.LG
Comments: 16 pages, 12 figures
\\
  Large Language Models (LLMs) have been widely employed in programming
language analysis to enhance human productivity. Yet, their reliability can be
compromised by various code distribution shifts, leading to inconsistent
outputs. While probabilistic methods are known to mitigate such impact through
uncertainty calibration and estimation, their efficacy in the language domain
remains underexplored compared to their application in image-based tasks. In
this work, we first introduce a large-scale benchmark dataset, incorporating
three realistic patterns of code distribution shifts at varying intensities.
Then we thoroughly investigate state-of-the-art probabilistic methods applied
to CodeLlama using these shifted code snippets. We observe that these methods
generally improve the uncertainty awareness of CodeLlama, with increased
calibration quality and higher uncertainty estimation~(UE) precision. However,
our study further reveals varied performance dynamics across different criteria
(e.g., calibration error vs misclassification detection) and trade-off between
efficacy and efficiency, highlighting necessary methodological selection
tailored to specific contexts.
\\ ( https://arxiv.org/abs/2402.05939 ,  8206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05953 (*cross-listing*)
Date: Sun, 4 Feb 2024 06:51:03 GMT   (27297kb,D)

Title: idMotif: An Interactive Motif Identification in Protein Sequences
Authors: Ji Hwan Park, Vikash Prasad, Sydney Newsom, Fares Najar, Rakhi Rajan
Categories: q-bio.QM cs.GR cs.HC cs.LG
Comments: IEEE CGA
Journal-ref: idMotif: An Interactive Motif Identification in Protein
  Sequences," in IEEE Computer Graphics and Applications, 2023
DOI: 10.1109/MCG.2023.3345742
\\
  This article introduces idMotif, a visual analytics framework designed to aid
domain experts in the identification of motifs within protein sequences.
Motifs, short sequences of amino acids, are critical for understanding the
distinct functions of proteins. Identifying these motifs is pivotal for
predicting diseases or infections. idMotif employs a deep learning-based method
for the categorization of protein sequences, enabling the discovery of
potential motif candidates within protein groups through local explanations of
deep learning model decisions. It offers multiple interactive views for the
analysis of protein clusters or groups and their sequences. A case study,
complemented by expert feedback, illustrates idMotif's utility in facilitating
the analysis and identification of protein sequences and motifs.
\\ ( https://arxiv.org/abs/2402.05953 ,  27297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05958 (*cross-listing*)
Date: Sun, 4 Feb 2024 19:45:59 GMT   (271kb)

Title: A comparative study on wearables and single-camera video for upper-limb
  out-of-thelab activity recognition with different deep learning architectures
Authors: Mario Mart\'inez-Zarzuela, David Gonz\'alez-Ortega, M\'iriam
  Ant\'on-Rodr\'iguez, Francisco Javier D\'iaz-Pernas, Henning M\"uller,
  Cristina Sim\'on-Mart\'inez
Categories: cs.CV cs.LG eess.SP
Journal-ref: Gait & Posture (2023) 106, p. 119-120
DOI: 10.1016/j.gaitpost.2023.07.149
\\
  The use of a wide range of computer vision solutions, and more recently
high-end Inertial Measurement Units (IMU) have become increasingly popular for
assessing human physical activity in clinical and research settings.
Nevertheless, to increase the feasibility of patient tracking in out-of-the-lab
settings, it is necessary to use a reduced number of devices for movement
acquisition. Promising solutions in this context are IMU-based wearables and
single camera systems. Additionally, the development of machine learning
systems able to recognize and digest clinically relevant data in-the-wild is
needed, and therefore determining the ideal input to those is crucial.
\\ ( https://arxiv.org/abs/2402.05958 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05961 (*cross-listing*)
Date: Mon, 5 Feb 2024 04:12:40 GMT   (1768kb,D)

Title: Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization
  Benchmark
Authors: Hyeonah Kim, Minsu Kim, Sanghyeok Choi, Jinkyoo Park
Categories: q-bio.BM cs.LG cs.NE
Comments: 20 pages (including 9 pages of appendix)
\\
  This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet
(Genetic GFN), which integrates an iterative genetic search into GFlowNet.
Genetic search effectively guides the GFlowNet to high-rewarded regions,
addressing global over-exploration that results in training inefficiency and
exploring limited regions. In addition, training strategies, such as rank-based
replay training and unsupervised maximum likelihood pre-training, are further
introduced to improve the sample efficiency of Genetic GFN. The proposed method
shows a state-of-the-art score of 16.213, significantly outperforming the
reported best score in the benchmark of 15.185, in practical molecular
optimization (PMO), which is an official benchmark for sample-efficient
molecular optimization. Remarkably, ours exceeds all baselines, including
reinforcement learning, Bayesian optimization, generative models, GFlowNets,
and genetic algorithms, in 14 out of 23 tasks.
\\ ( https://arxiv.org/abs/2402.05961 ,  1768kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05972 (*cross-listing*)
Date: Wed, 7 Feb 2024 09:03:26 GMT   (4196kb,D)

Title: Gaussian-process-regression-based method for the localization of
  exceptional points in complex resonance spectra
Authors: Patrick Egenlauf, Patric Rommel, J\"org Main
Categories: quant-ph cond-mat.mes-hall cs.LG
Comments: 28 pages, 10 figures, submitted to Machine Learning: Science and
  Technology
\\
  Resonances in open quantum systems depending on at least two controllable
parameters can show the phenomenon of exceptional points (EPs), where not only
the eigenvalues but also the eigenvectors of two or more resonances coalesce.
Their exact localization in the parameter space is challenging, in particular
in systems, where the computation of the quantum spectra and resonances is
numerically very expensive. We introduce an efficient machine learning
algorithm to find exceptional points based on Gaussian process regression
(GPR). The GPR-model is trained with an initial set of eigenvalue pairs
belonging to an EP and used for a first estimation of the EP position via a
numerically cheap root search. The estimate is then improved iteratively by
adding selected exact eigenvalue pairs as training points to the GPR-model. The
GPR-based method is developed and tested on a simple low-dimensional matrix
model and then applied to a challenging real physical system, viz., the
localization of EPs in the resonance spectra of excitons in cuprous oxide in
external electric and magnetic fields. The precise computation of EPs, by
taking into account the complete valence band structure and central-cell
corrections of the crystal, can be the basis for the experimental observation
of EPs in this system.
\\ ( https://arxiv.org/abs/2402.05972 ,  4196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05982 (*cross-listing*)
Date: Thu, 8 Feb 2024 13:02:05 GMT   (731kb,D)

Title: Anfinsen Goes Neural: a Graphical Model for Conditional Antibody Design
Authors: Nayoung Kim, Minsu Kim, Jinkyoo Park
Categories: q-bio.QM cs.LG
Comments: 18 pages, 8 figures
\\
  Antibody design plays a pivotal role in advancing therapeutics. Although deep
learning has made rapid progress in this field, existing methods make limited
use of general protein knowledge and assume a graphical model (GM) that
violates empirical findings on proteins. To address these limitations, we
present Anfinsen Goes Neural (AGN), a graphical model that uses a pre-trained
protein language model (pLM) and encodes a seminal finding on proteins called
Anfinsen's dogma. Our framework follows a two-step process of sequence
generation with pLM and structure prediction with graph neural network (GNN).
Experiments show that our approach outperforms state-of-the-art results on
benchmark experiments. We also address a critical limitation of
non-autoregressive models -- namely, that they tend to generate unrealistic
sequences with overly repeating tokens. To resolve this, we introduce a
composition-based regularization term to the cross-entropy objective that
allows an efficient trade-off between high performance and low token
repetition. We demonstrate that our approach establishes a Pareto frontier over
the current state-of-the-art. Our code is available at
https://github.com/lkny123/AGN.
\\ ( https://arxiv.org/abs/2402.05982 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05983 (*cross-listing*)
Date: Thu, 8 Feb 2024 14:23:24 GMT   (5331kb,D)

Title: Capability enhancement of the X-ray micro-tomography system via
  ML-assisted approaches
Authors: Dhruvi Shah, Shruti Mehta, Ashish Agrawal, Shishir Purohit, Bhaskar
  Chaudhury
Categories: eess.IV cs.LG physics.app-ph physics.ins-det
\\
  Ring artifacts in X-ray micro-CT images are one of the primary causes of
concern in their accurate visual interpretation and quantitative analysis. The
geometry of X-ray micro-CT scanners is similar to the medical CT machines,
except the sample is rotated with a stationary source and detector. The ring
artifacts are caused by a defect or non-linear responses in detector pixels
during the MicroCT data acquisition. Artifacts in MicroCT images can often be
so severe that the images are no longer useful for further analysis. Therefore,
it is essential to comprehend the causes of artifacts and potential solutions
to maximize image quality. This article presents a convolution neural network
(CNN)-based Deep Learning (DL) model inspired by UNet with a series of encoder
and decoder units with skip connections for removal of ring artifacts. The
proposed architecture has been evaluated using the Structural Similarity Index
Measure (SSIM) and Mean Squared Error (MSE). Additionally, the results are
compared with conventional filter-based non-ML techniques and are found to be
better than the latter.
\\ ( https://arxiv.org/abs/2402.05983 ,  5331kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06033 (*cross-listing*)
Date: Thu, 8 Feb 2024 20:12:47 GMT   (46kb)

Title: An Inexact Halpern Iteration for with Application to Distributionally
  Robust Optimization
Authors: Ling Liang, Kim-Chuan Toh, and Jia-Jie Zhu
Categories: math.OC cs.LG
\\
  The Halpern iteration for solving monotone inclusion problems has gained
increasing interests in recent years due to its simple form and appealing
convergence properties. In this paper, we investigate the inexact variants of
the scheme in both deterministic and stochastic settings. We conduct extensive
convergence analysis and show that by choosing the inexactness tolerances
appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in
terms of the (expected) residue norm. Our results relax the state-of-the-art
inexactness conditions employed in the literature while sharing the same
competitive convergence properties. We then demonstrate how the proposed
methods can be applied for solving two classes of data-driven Wasserstein
distributionally robust optimization problems that admit convex-concave min-max
optimization reformulations. We highlight its capability of performing inexact
computations for distributionally robust learning with stochastic first-order
methods.
\\ ( https://arxiv.org/abs/2402.06033 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06047 (*cross-listing*)
Date: Thu, 8 Feb 2024 20:38:35 GMT   (625kb,D)

Title: Intelligent Mode-switching Framework for Teleoperation
Authors: Burak Kizilkaya, Changyang She, Guodong Zhao, Muhammad Ali Imran
Categories: cs.RO cs.LG cs.NI
Comments: Accepted by the 2024 IEEE International Conference on Robotics and
  Automation (ICRA)
\\
  Teleoperation can be very difficult due to limited perception, high
communication latency, and limited degrees of freedom (DoFs) at the operator
side. Autonomous teleoperation is proposed to overcome this difficulty by
predicting user intentions and performing some parts of the task autonomously
to decrease the demand on the operator and increase the task completion rate.
However, decision-making for mode-switching is generally assumed to be done by
the operator, which brings an extra DoF to be controlled by the operator and
introduces extra mental demand. On the other hand, the communication
perspective is not investigated in the current literature, although
communication imperfections and resource limitations are the main bottlenecks
for teleoperation. In this study, we propose an intelligent mode-switching
framework by jointly considering mode-switching and communication systems. User
intention recognition is done at the operator side. Based on user intention
recognition, a deep reinforcement learning (DRL) agent is trained and deployed
at the operator side to seamlessly switch between autonomous and teleoperation
modes. A real-world data set is collected from our teleoperation testbed to
train both user intention recognition and DRL algorithms. Our results show that
the proposed framework can achieve up to 50% communication load reduction with
improved task completion probability.
\\ ( https://arxiv.org/abs/2402.06047 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06059 (*cross-listing*)
Date: Thu, 8 Feb 2024 21:03:34 GMT   (1615kb,D)

Title: Impact on Public Health Decision Making by Utilizing Big Data Without
  Domain Knowledge
Authors: Miao Zhang, Salman Rahman, Vishwali Mhasawade, Rumi Chunara
Categories: cs.CY cs.CV cs.LG
\\
  New data sources, and artificial intelligence (AI) methods to extract
information from them are becoming plentiful, and relevant to decision making
in many societal applications. An important example is street view imagery,
available in over 100 countries, and considered for applications such as
assessing built environment aspects in relation to community health outcomes.
Relevant to such uses, important examples of bias in the use of AI are evident
when decision-making based on data fails to account for the robustness of the
data, or predictions are based on spurious correlations. To study this risk, we
utilize 2.02 million GSV images along with health, demographic, and
socioeconomic data from New York City. Initially, we demonstrate that built
environment characteristics inferred from GSV labels at the intra-city level
may exhibit inadequate alignment with the ground truth. We also find that the
average individual-level behavior of physical inactivity significantly mediates
the impact of built environment features by census tract, as measured through
GSV. Finally, using a causal framework which accounts for these mediators of
environmental impacts on health, we find that altering 10% of samples in the
two lowest tertiles would result in a 4.17 (95% CI 3.84 to 4.55) or 17.2 (95%
CI 14.4 to 21.3) times bigger decrease on the prevalence of obesity or
diabetes, than the same proportional intervention on the number of crosswalks
by census tract. This work illustrates important issues of robustness and model
specification for informing effective allocation of interventions using new
data sources.
\\ ( https://arxiv.org/abs/2402.06059 ,  1615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06063 (*cross-listing*)
Date: Thu, 8 Feb 2024 21:19:16 GMT   (2264kb)

Title: 3D-2D Neural Nets for Phase Retrieval in Noisy Interferometric Imaging
Authors: Andrew H. Proppe, Guillaume Thekkadath, Duncan England, Philip J.
  Bustard, Fr\'ed\'eric Bouchard, Jeff S. Lundeen, Benjamin J. Sussman
Categories: physics.optics cs.CV cs.LG eess.IV
\\
  In recent years, neural networks have been used to solve phase retrieval
problems in imaging with superior accuracy and speed than traditional
techniques, especially in the presence of noise. However, in the context of
interferometric imaging, phase noise has been largely unaddressed by existing
neural network architectures. Such noise arises naturally in an interferometer
due to mechanical instabilities or atmospheric turbulence, limiting measurement
acquisition times and posing a challenge in scenarios with limited light
intensity, such as remote sensing. Here, we introduce a 3D-2D Phase Retrieval
U-Net (PRUNe) that takes noisy and randomly phase-shifted interferograms as
inputs, and outputs a single 2D phase image. A 3D downsampling convolutional
encoder captures correlations within and between frames to produce a 2D latent
space, which is upsampled by a 2D decoder into a phase image. We test our model
against a state-of-the-art singular value decomposition algorithm and find
PRUNe reconstructions consistently show more accurate and smooth
reconstructions, with a x2.5 - 4 lower mean squared error at multiple
signal-to-noise ratios for interferograms with low (< 1 photon/pixel) and high
(~100 photons/pixel) signal intensity. Our model presents a faster and more
accurate approach to perform phase retrieval in extremely low light intensity
interferometry in presence of phase noise, and will find application in other
multi-frame noisy imaging techniques.
\\ ( https://arxiv.org/abs/2402.06063 ,  2264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06102 (*cross-listing*)
Date: Thu, 8 Feb 2024 23:35:03 GMT   (5906kb,D)

Title: Real-World Fluid Directed Rigid Body Control via Deep Reinforcement
  Learning
Authors: Mohak Bhardwaj, Thomas Lampe, Michael Neunert, Francesco Romano, Abbas
  Abdolmaleki, Arunkumar Byravan, Markus Wulfmeier, Martin Riedmiller, Jonas
  Buchli
Categories: cs.RO cs.LG
\\
  Recent advances in real-world applications of reinforcement learning (RL)
have relied on the ability to accurately simulate systems at scale. However,
domains such as fluid dynamical systems exhibit complex dynamic phenomena that
are hard to simulate at high integration rates, limiting the direct application
of modern deep RL algorithms to often expensive or safety critical hardware. In
this work, we introduce "Box o Flows", a novel benchtop experimental control
system for systematically evaluating RL algorithms in dynamic real-world
scenarios. We describe the key components of the Box o Flows, and through a
series of experiments demonstrate how state-of-the-art model-free RL algorithms
can synthesize a variety of complex behaviors via simple reward specifications.
Furthermore, we explore the role of offline RL in data-efficient hypothesis
testing by reusing past experiences. We believe that the insights gained from
this preliminary study and the availability of systems like the Box o Flows
support the way forward for developing systematic RL algorithms that can be
generally applied to complex, dynamical systems. Supplementary material and
videos of experiments are available at
https://sites.google.com/view/box-o-flows/home.
\\ ( https://arxiv.org/abs/2402.06102 ,  5906kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06122 (*cross-listing*)
Date: Fri, 9 Feb 2024 01:11:34 GMT   (1268kb,D)

Title: Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests
  for Means of Multiple Data Streams
Authors: Brian Cho, Kyra Gan, Nathan Kallus
Categories: stat.ME cs.LG stat.ML
\\
  We propose a novel nonparametric sequential test for composite hypotheses for
means of multiple data streams. Our proposed method, \emph{peeking with
expectation-based averaged capital} (PEAK), builds upon the testing-as-betting
framework and provides a non-asymptotic $\alpha$-level test across any stopping
time. PEAK is computationally tractable and efficiently rejects hypotheses that
are incorrect across all potential distributions that satisfy our nonparametric
assumption, enabling joint composite hypothesis testing on multiple streams of
data. We numerically validate our theoretical findings under the best arm
identification and threshold identification in the bandit setting, illustrating
the computational efficiency of our method against state-of-the-art testing
methods.
\\ ( https://arxiv.org/abs/2402.06122 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06127 (*cross-listing*)
Date: Fri, 9 Feb 2024 01:19:41 GMT   (4541kb,D)

Title: CityFlowER: An Efficient and Realistic Traffic Simulator with Embedded
  Machine Learning Models
Authors: Longchao Da, Chen Chu, Weinan Zhang, Hua Wei
Categories: cs.MA cs.LG
Comments: 4 pages, 4 figures
ACM-class: G.3
\\
  Traffic simulation is an essential tool for transportation infrastructure
planning, intelligent traffic control policy learning, and traffic flow
analysis. Its effectiveness relies heavily on the realism of the simulators
used. Traditional traffic simulators, such as SUMO and CityFlow, are often
limited by their reliance on rule-based models with hyperparameters that
oversimplify driving behaviors, resulting in unrealistic simulations. To
enhance realism, some simulators have provided Application Programming
Interfaces (APIs) to interact with Machine Learning (ML) models, which learn
from observed data and offer more sophisticated driving behavior models.
However, this approach faces challenges in scalability and time efficiency as
vehicle numbers increase. Addressing these limitations, we introduce
CityFlowER, an advancement over the existing CityFlow simulator, designed for
efficient and realistic city-wide traffic simulation. CityFlowER innovatively
pre-embeds ML models within the simulator, eliminating the need for external
API interactions and enabling faster data computation. This approach allows for
a blend of rule-based and ML behavior models for individual vehicles, offering
unparalleled flexibility and efficiency, particularly in large-scale
simulations. We provide detailed comparisons with existing simulators,
implementation insights, and comprehensive experiments to demonstrate
CityFlowER's superiority in terms of realism, efficiency, and adaptability.
\\ ( https://arxiv.org/abs/2402.06127 ,  4541kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06151 (*cross-listing*)
Date: Fri, 9 Feb 2024 03:01:13 GMT   (2630kb,D)

Title: POTEC: Off-Policy Learning for Large Action Spaces via Two-Stage Policy
  Decomposition
Authors: Yuta Saito, Jihan Yao, Thorsten Joachims
Categories: stat.ML cs.LG
Comments: arXiv admin note: text overlap with arXiv:2305.08062
\\
  We study off-policy learning (OPL) of contextual bandit policies in large
discrete action spaces where existing methods -- most of which rely crucially
on reward-regression models or importance-weighted policy gradients -- fail due
to excessive bias or variance. To overcome these issues in OPL, we propose a
novel two-stage algorithm, called Policy Optimization via Two-Stage Policy
Decomposition (POTEC). It leverages clustering in the action space and learns
two different policies via policy- and regression-based approaches,
respectively. In particular, we derive a novel low-variance gradient estimator
that enables to learn a first-stage policy for cluster selection efficiently
via a policy-based approach. To select a specific action within the cluster
sampled by the first-stage policy, POTEC uses a second-stage policy derived
from a regression-based approach within each cluster. We show that a local
correctness condition, which only requires that the regression model preserves
the relative expected reward differences of the actions within each cluster,
ensures that our policy-gradient estimator is unbiased and the second-stage
policy is optimal. We also show that POTEC provides a strict generalization of
policy- and regression-based approaches and their associated assumptions.
Comprehensive experiments demonstrate that POTEC provides substantial
improvements in OPL effectiveness particularly in large and structured action
spaces.
\\ ( https://arxiv.org/abs/2402.06151 ,  2630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06162 (*cross-listing*)
Date: Fri, 9 Feb 2024 03:33:13 GMT   (4913kb,D)

Title: Wasserstein proximal operators describe score-based generative models
  and resolve memorization
Authors: Benjamin J. Zhang, Siting Liu, Wuchen Li, Markos A. Katsoulakis, and
  Stanley J. Osher
Categories: stat.ML cs.LG
\\
  We focus on the fundamental mathematical structure of score-based generative
models (SGMs). We first formulate SGMs in terms of the Wasserstein proximal
operator (WPO) and demonstrate that, via mean-field games (MFGs), the WPO
formulation reveals mathematical structure that describes the inductive bias of
diffusion and score-based models. In particular, MFGs yield optimality
conditions in the form of a pair of coupled partial differential equations: a
forward-controlled Fokker-Planck (FP) equation, and a backward
Hamilton-Jacobi-Bellman (HJB) equation. Via a Cole-Hopf transformation and
taking advantage of the fact that the cross-entropy can be related to a linear
functional of the density, we show that the HJB equation is an uncontrolled FP
equation. Second, with the mathematical structure at hand, we present an
interpretable kernel-based model for the score function which dramatically
improves the performance of SGMs in terms of training samples and training
time. In addition, the WPO-informed kernel model is explicitly constructed to
avoid the recently studied memorization effects of score-based generative
models. The mathematical form of the new kernel-based models in combination
with the use of the terminal condition of the MFG reveals new explanations for
the manifold learning and generalization properties of SGMs, and provides a
resolution to their memorization effects. Finally, our mathematically informed,
interpretable kernel-based model suggests new scalable bespoke neural network
architectures for high-dimensional applications.
\\ ( https://arxiv.org/abs/2402.06162 ,  4913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06173 (*cross-listing*)
Date: Fri, 9 Feb 2024 04:13:38 GMT   (2411kb,D)

Title: SMC Is All You Need: Parallel Strong Scaling
Authors: Xinzhu Liang, Sanjaya Lohani, Joseph M. Lukens, Brian T. Kirby, Thomas
  A. Searles, Kody J.H. Law
Categories: stat.ML cs.LG stat.CO
Comments: 21 pages, 11 figures
\\
  In the general framework of Bayesian inference, the target distribution can
only be evaluated up-to a constant of proportionality. Classical consistent
Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte
Carlo (MCMC) have unbounded time complexity requirements. We develop a fully
parallel sequential Monte Carlo (pSMC) method which provably delivers parallel
strong scaling, i.e. the time complexity (and per-node memory) remains bounded
if the number of asynchronous processes is allowed to grow. More precisely, the
pSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes
the number of communicating samples in each processor and $R$ denotes the
number of processors. In particular, for suitably-large problem-dependent $N$,
as $R \rightarrow \infty$ the method converges to infinitesimal accuracy
MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with
no efficiency leakage, i.e. computational complexity
Cost$=O(\varepsilon^{-2})$. A number of Bayesian inference problems are taken
into consideration to compare the pSMC and MCMC methods.
\\ ( https://arxiv.org/abs/2402.06173 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06190 (*cross-listing*)
Date: Fri, 9 Feb 2024 05:06:58 GMT   (5443kb,D)

Title: Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain
Authors: Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan
  Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath
Categories: cs.CV cs.LG
\\
  Standard modern machine-learning-based imaging methods have faced challenges
in medical applications due to the high cost of dataset construction and,
thereby, the limited labeled training data available. Additionally, upon
deployment, these methods are usually used to process a large volume of data on
a daily basis, imposing a high maintenance cost on medical facilities. In this
paper, we introduce a new neural network architecture, termed LoGoNet, with a
tailored self-supervised learning (SSL) method to mitigate such challenges.
LoGoNet integrates a novel feature extractor within a U-shaped architecture,
leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture
both long-range and short-range feature dependencies adeptly. This is in
contrast to existing methods that rely on increasing network capacity to
enhance feature extraction. This combination of novel techniques in our model
is especially beneficial in medical image segmentation, given the difficulty of
learning intricate and often irregular body organ shapes, such as the spleen.
Complementary, we propose a novel SSL method tailored for 3D images to
compensate for the lack of large labeled datasets. The method combines masking
and contrastive learning techniques within a multi-task learning framework and
is compatible with both Vision Transformer (ViT) and CNN-based models. We
demonstrate the efficacy of our methods in numerous tasks across two standard
datasets (i.e., BTCV and MSD). Benchmark comparisons with eight
state-of-the-art models highlight LoGoNet's superior performance in both
inference time and accuracy.
\\ ( https://arxiv.org/abs/2402.06190 ,  5443kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06191 (*cross-listing*)
Date: Fri, 9 Feb 2024 05:10:53 GMT   (34680kb,D)

Title: The Berkeley Single Cell Computational Microscopy (BSCCM) Dataset
Authors: Henry Pinkard, Cherry Liu, Fanice Nyatigo, Daniel A. Fletcher, Laura
  Waller
Categories: cs.CV cs.LG q-bio.QM
\\
  Computational microscopy, in which hardware and algorithms of an imaging
system are jointly designed, shows promise for making imaging systems that cost
less, perform more robustly, and collect new types of information. Often, the
performance of computational imaging systems, especially those that incorporate
machine learning, is sample-dependent. Thus, standardized datasets are an
essential tool for comparing the performance of different approaches. Here, we
introduce the Berkeley Single Cell Computational Microscopy (BSCCM) dataset,
which contains over ~12,000,000 images of 400,000 of individual white blood
cells. The dataset contains images captured with multiple illumination patterns
on an LED array microscope and fluorescent measurements of the abundance of
surface proteins that mark different cell types. We hope this dataset will
provide a valuable resource for the development and testing of new algorithms
in computational microscopy and computer vision with practical biomedical
applications.
\\ ( https://arxiv.org/abs/2402.06191 ,  34680kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06224 (*cross-listing*)
Date: Fri, 9 Feb 2024 07:20:14 GMT   (1071kb,D)

Title: Adaptive multi-gradient methods for quasiconvex vector optimization and
  applications to multi-task learning
Authors: Nguyen Anh Minh and Le Dung Muu and Tran Ngoc Thang
Categories: math.OC cs.LG
\\
  We present an adaptive step-size method, which does not include line-search
techniques, for solving a wide class of nonconvex multiobjective programming
problems on an unbounded constraint set. We also prove convergence of a general
approach under modest assumptions. More specifically, the convexity criterion
might not be satisfied by the objective function. Unlike descent line-search
algorithms, it does not require an initial step-size to be determined by a
previously determined Lipschitz constant. The process's primary characteristic
is its gradual step-size reduction up until a predetermined condition is met.
It can be specifically applied to offer an innovative multi-gradient projection
method for unbounded constrained optimization issues. Preliminary findings from
a few computational examples confirm the accuracy of the strategy. We apply the
proposed technique to some multi-task learning experiments to show its efficacy
for large-scale challenges.
\\ ( https://arxiv.org/abs/2402.06224 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06226 (*cross-listing*)
Date: Fri, 9 Feb 2024 07:23:27 GMT   (693kb)

Title: N-1 Reduced Optimal Power Flow Using Augmented Hierarchical Graph Neural
  Network
Authors: Thuan Pham, Xingpeng Li
Categories: eess.SY cs.LG cs.SY
\\
  Optimal power flow (OPF) is used to perform generation redispatch in power
system real-time operations. N-1 OPF can ensure safe grid operations under
diverse contingency scenarios. For large and intricate power networks with
numerous variables and constraints, achieving an optimal solution for real-time
N-1 OPF necessitates substantial computational resources. To mitigate this
challenge, machine learning (ML) is introduced as an additional tool for
predicting congested or heavily loaded lines dynamically. In this paper, an
advanced ML model known as the augmented hierarchical graph neural network
(AHGNN) was proposed to predict critical congested lines and create N-1 reduced
OPF (N-1 ROPF). The proposed AHGNN-enabled N-1 ROPF can result in a remarkable
reduction in computing time while retaining the solution quality. Several
variations of GNN-based ML models are also implemented as benchmark to
demonstrate effectiveness of the proposed AHGNN approach. Case studies prove
the proposed AHGNN and the associated N-1 ROPF are highly effective in reducing
computation time while preserving solution quality, highlighting the promising
potential of ML, particularly GNN in enhancing power system operations.
\\ ( https://arxiv.org/abs/2402.06226 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06271 (*cross-listing*)
Date: Fri, 9 Feb 2024 09:37:28 GMT   (955kb,D)

Title: Adaptive proximal gradient methods are universal without approximation
Authors: Konstantinos A. Oikonomidis, Emanuel Laude, Puya Latafat, Andreas
  Themelis and Panagiotis Patrinos
Categories: math.OC cs.LG
MSC-class: 65K05, 90C06, 90C25, 90C30, 90C47
\\
  We show that adaptive proximal gradient methods for convex problems are not
restricted to traditional Lipschitzian assumptions. Our analysis reveals that a
class of linesearch-free methods is still convergent under mere local H\"older
gradient continuity, covering in particular continuously differentiable
semi-algebraic functions. To mitigate the lack of local Lipschitz continuity,
popular approaches revolve around $\varepsilon$-oracles and/or linesearch
procedures. In contrast, we exploit plain H\"older inequalities not entailing
any approximation, all while retaining the linesearch-free nature of adaptive
schemes. Furthermore, we prove full sequence convergence without prior
knowledge of local H\"older constants nor of the order of H\"older continuity.
In numerical experiments we present comparisons to baseline methods on diverse
tasks from machine learning covering both the locally and the globally H\"older
setting.
\\ ( https://arxiv.org/abs/2402.06271 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06275 (*cross-listing*)
Date: Fri, 9 Feb 2024 09:40:12 GMT   (27516kb,D)

Title: Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics
Authors: Artur P. Toshev, Jonas A. Erbesdobler, Nikolaus A. Adams, Johannes
  Brandstetter
Categories: physics.flu-dyn cs.LG
\\
  Smoothed particle hydrodynamics (SPH) is omnipresent in modern engineering
and scientific disciplines. SPH is a class of Lagrangian schemes that
discretize fluid dynamics via finite material points that are tracked through
the evolving velocity field. Due to the particle-like nature of the simulation,
graph neural networks (GNNs) have emerged as appealing and successful
surrogates. However, the practical utility of such GNN-based simulators relies
on their ability to faithfully model physics, providing accurate and stable
predictions over long time horizons - which is a notoriously hard problem. In
this work, we identify particle clustering originating from tensile
instabilities as one of the primary pitfalls. Based on these insights, we
enhance both training and rollout inference of state-of-the-art GNN-based
simulators with varying components from standard SPH solvers, including
pressure, viscous, and external force components. All neural SPH-enhanced
simulators achieve better performance, often by orders of magnitude, than the
baseline GNNs, allowing for significantly longer rollouts and significantly
better physics modeling. Code available under
(https://github.com/tumaer/neuralsph).
\\ ( https://arxiv.org/abs/2402.06275 ,  27516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06277 (*cross-listing*)
Date: Fri, 9 Feb 2024 09:41:26 GMT   (13225kb,D)

Title: Controllable seismic velocity synthesis using generative diffusion
  models
Authors: Fu Wang, Xinquan Huang, Tariq Alkhalifah
Categories: physics.geo-ph cs.LG
\\
  Accurate seismic velocity estimations are vital to understanding Earth's
subsurface structures, assessing natural resources, and evaluating seismic
hazards. Machine learning-based inversion algorithms have shown promising
performance in regional (i.e., for exploration) and global velocity estimation,
while their effectiveness hinges on access to large and diverse training
datasets whose distributions generally cover the target solutions.
Additionally, enhancing the precision and reliability of velocity estimation
also requires incorporating prior information, e.g., geological classes, well
logs, and subsurface structures, but current statistical or neural
network-based methods are not flexible enough to handle such multi-modal
information. To address both challenges, we propose to use conditional
generative diffusion models for seismic velocity synthesis, in which we readily
incorporate those priors. This approach enables the generation of seismic
velocities that closely match the expected target distribution, offering
datasets informed by both expert knowledge and measured data to support
training for data-driven geophysical methods. We demonstrate the flexibility
and effectiveness of our method through training diffusion models on the
OpenFWI dataset under various conditions, including class labels, well logs,
reflectivity images, as well as the combination of these priors. The
performance of the approach under out-of-distribution conditions further
underscores its generalization ability, showcasing its potential to provide
tailored priors for velocity inverse problems and create specific training
datasets for machine learning-based geophysical applications.
\\ ( https://arxiv.org/abs/2402.06277 ,  13225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06282 (*cross-listing*)
Date: Fri, 9 Feb 2024 09:48:38 GMT   (717kb,D)

Title: Retrieve, Merge, Predict: Augmenting Tables with Data Lakes
Authors: Riccardo Cappuzzo (1), Gael Varoquaux (1), Aimee Coelho (2), Paolo
  Papotti (3) ((1) SODA Team - Inria Saclay, (2) Dataiku, (3) EURECOM)
Categories: cs.DB cs.LG
Comments: 12 pages + references, 11 figures. Under submission at VLDB2024 (EA&B
  track)
\\
  We present an in-depth analysis of data discovery in data lakes, focusing on
table augmentation for given machine learning tasks. We analyze alternative
methods used in the three main steps: retrieving joinable tables, merging
information, and predicting with the resultant table. As data lakes, the paper
uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for
benchmarking this data discovery task -- and Open Data US, a well-referenced
real data lake. Through systematic exploration on both lakes, our study
outlines the importance of accurately retrieving join candidates and the
efficiency of simple merging methods. We report new insights on the benefits of
existing solutions and on their limitations, aiming at guiding future research
in this space.
\\ ( https://arxiv.org/abs/2402.06282 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06320 (*cross-listing*)
Date: Fri, 9 Feb 2024 11:01:35 GMT   (3221kb,D)

Title: Particle Denoising Diffusion Sampler
Authors: Angus Phillips, Hai-Dang Dau, Michael John Hutchinson, Valentin De
  Bortoli, George Deligiannidis, Arnaud Doucet
Categories: stat.ML cs.LG stat.CO
Comments: 30 pages, 12 figures, 3 tables, 4 algorithms
\\
  Denoising diffusion models have become ubiquitous for generative modeling.
The core idea is to transport the data distribution to a Gaussian by using a
diffusion. Approximate samples from the data distribution are then obtained by
estimating the time-reversal of this diffusion using score matching ideas. We
follow here a similar strategy to sample from unnormalized probability
densities and compute their normalizing constants. However, the time-reversed
diffusion is here simulated by using an original iterative particle scheme
relying on a novel score matching loss. Contrary to standard denoising
diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS)
provides asymptotically consistent estimates under mild assumptions. We
demonstrate PDDS on multimodal and high dimensional sampling tasks.
\\ ( https://arxiv.org/abs/2402.06320 ,  3221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06357 (*cross-listing*)
Date: Fri, 9 Feb 2024 12:07:06 GMT   (427kb,D)

Title: The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks
Authors: Jona te Lintelo and Stefanos Koffas and Stjepan Picek
Categories: cs.CR cs.LG
\\
  Sponge attacks aim to increase the energy consumption and computation time of
neural networks deployed on hardware accelerators. Existing sponge attacks can
be performed during inference via sponge examples or during training via Sponge
Poisoning. Sponge examples leverage perturbations added to the model's input to
increase energy and latency, while Sponge Poisoning alters the objective
function of a model to induce inference-time energy/latency effects.
  In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is
the first sponge attack that is performed directly on the parameters of a
pre-trained model. Our experiments show that SpongeNet can successfully
increase the energy consumption of vision models with fewer samples required
than Sponge Poisoning. Our experiments indicate that poisoning defenses are
ineffective if not adjusted specifically for the defense against Sponge
Poisoning (i.e., they decrease batch normalization bias values). Our work shows
that SpongeNet is more effective on StarGAN than the state-of-the-art.
Additionally, SpongeNet is stealthier than the previous Sponge Poisoning attack
as it does not require significant changes in the victim model's weights. Our
experiments indicate that the SpongeNet attack can be performed even when an
attacker has access to only 1% of the entire dataset and reach up to 11% energy
increase.
\\ ( https://arxiv.org/abs/2402.06357 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06386 (*cross-listing*)
Date: Fri, 9 Feb 2024 13:08:21 GMT   (369kb,D)

Title: Boosting-Based Sequential Meta-Tree Ensemble Construction for Improved
  Decision Trees
Authors: Ryota Maniwa, Naoki Ichijo, Yuta Nakahara, and Toshiyasu Matsushima
Categories: stat.ML cs.LG
\\
  A decision tree is one of the most popular approaches in machine learning
fields. However, it suffers from the problem of overfitting caused by overly
deepened trees. Then, a meta-tree is recently proposed. It solves the problem
of overfitting caused by overly deepened trees. Moreover, the meta-tree
guarantees statistical optimality based on Bayes decision theory. Therefore,
the meta-tree is expected to perform better than the decision tree. In contrast
to a single decision tree, it is known that ensembles of decision trees, which
are typically constructed boosting algorithms, are more effective in improving
predictive performance. Thus, it is expected that ensembles of meta-trees are
more effective in improving predictive performance than a single meta-tree, and
there are no previous studies that construct multiple meta-trees in boosting.
Therefore, in this study, we propose a method to construct multiple meta-trees
using a boosting approach. Through experiments with synthetic and benchmark
datasets, we conduct a performance comparison between the proposed methods and
the conventional methods using ensembles of decision trees. Furthermore, while
ensembles of decision trees can cause overfitting as well as a single decision
tree, experiments confirmed that ensembles of meta-trees can prevent
overfitting due to the tree depth.
\\ ( https://arxiv.org/abs/2402.06386 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06412 (*cross-listing*)
Date: Fri, 9 Feb 2024 13:58:33 GMT   (5040kb,D)

Title: Improving the Worst-Case Bidirectional Communication Complexity for
  Nonconvex Distributed Optimization under Function Similarity
Authors: Kaja Gruntkowska, Alexander Tyurin, Peter Richt\'arik
Categories: math.OC cs.LG stat.ML
\\
  Effective communication between the server and workers plays a key role in
distributed optimization. In this paper, we focus on optimizing the
server-to-worker communication, uncovering inefficiencies in prevalent downlink
compression approaches. Considering first the pure setup where the uplink
communication costs are negligible, we introduce MARINA-P, a novel method for
downlink compression, employing a collection of correlated compressors.
Theoretical analyses demonstrates that MARINA-P with permutation compressors
can achieve a server-to-worker communication complexity improving with the
number of workers, thus being provably superior to existing algorithms. We
further show that MARINA-P can serve as a starting point for extensions such as
methods supporting bidirectional compression. We introduce M3, a method
combining MARINA-P with uplink compression and a momentum step, achieving
bidirectional compression with provable improvements in total communication
complexity as the number of workers increases. Theoretical findings align
closely with empirical experiments, underscoring the efficiency of the proposed
algorithms.
\\ ( https://arxiv.org/abs/2402.06412 ,  5040kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06463 (*cross-listing*)
Date: Fri, 9 Feb 2024 15:14:48 GMT   (2725kb,D)

Title: Cardiac ultrasound simulation for autonomous ultrasound navigation
Authors: Abdoul Aziz Amadou, Laura Peralta, Paul Dryburgh, Paul Klein, Kaloian
  Petkov, Richard James Housden, Vivek Singh, Rui Liao, Young-Ho Kim, Florin
  Christian Ghesu, Tommaso Mansi, Ronak Rajani, Alistair Young and Kawal Rhode
Categories: eess.IV cs.CV cs.LG
Comments: 24 pages, 10 figures, 5 tables
ACM-class: I.6.0; I.5.4; J.3
\\
  Ultrasound is well-established as an imaging modality for diagnostic and
interventional purposes. However, the image quality varies with operator skills
as acquiring and interpreting ultrasound images requires extensive training due
to the imaging artefacts, the range of acquisition parameters and the
variability of patient anatomies. Automating the image acquisition task could
improve acquisition reproducibility and quality but training such an algorithm
requires large amounts of navigation data, not saved in routine examinations.
Thus, we propose a method to generate large amounts of ultrasound images from
other modalities and from arbitrary positions, such that this pipeline can
later be used by learning algorithms for navigation. We present a novel
simulation pipeline which uses segmentations from other modalities, an
optimized volumetric data representation and GPU-accelerated Monte Carlo path
tracing to generate view-dependent and patient-specific ultrasound images. We
extensively validate the correctness of our pipeline with a phantom experiment,
where structures' sizes, contrast and speckle noise properties are assessed.
Furthermore, we demonstrate its usability to train neural networks for
navigation in an echocardiography view classification experiment by generating
synthetic images from more than 1000 patients. Networks pre-trained with our
simulations achieve significantly superior performance in settings where large
real datasets are not available, especially for under-represented classes. The
proposed approach allows for fast and accurate patient-specific ultrasound
image generation, and its usability for training networks for
navigation-related tasks is demonstrated.
\\ ( https://arxiv.org/abs/2402.06463 ,  2725kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06494 (*cross-listing*)
Date: Fri, 9 Feb 2024 15:56:39 GMT   (5993kb,D)

Title: Deep Learning-Based Auto-Segmentation of Planning Target Volume for
  Total Marrow and Lymph Node Irradiation
Authors: Ricardo Coimbra Brioso, Damiano Dei, Nicola Lambri, Daniele Loiacono,
  Pietro Mancosu, Marta Scorsetti
Categories: cs.CV cs.LG
Comments: arXiv admin note: text overlap with arXiv:2304.02353
\\
  In order to optimize the radiotherapy delivery for cancer treatment,
especially when dealing with complex treatments such as Total Marrow and Lymph
Node Irradiation (TMLI), the accurate contouring of the Planning Target Volume
(PTV) is crucial. Unfortunately, relying on manual contouring for such
treatments is time-consuming and prone to errors. In this paper, we investigate
the application of Deep Learning (DL) to automate the segmentation of the PTV
in TMLI treatment, building upon previous work that introduced a solution to
this problem based on a 2D U-Net model. We extend the previous research (i) by
employing the nnU-Net framework to develop both 2D and 3D U-Net models and (ii)
by evaluating the trained models on the PTV with the exclusion of bones, which
consist mainly of lymp-nodes and represent the most challenging region of the
target volume to segment. Our result show that the introduction of nnU-NET
framework led to statistically significant improvement in the segmentation
performance. In addition, the analysis on the PTV after the exclusion of bones
showed that the models are quite robust also on the most challenging areas of
the target volume. Overall, our study is a significant step forward in the
application of DL in a complex radiotherapy treatment such as TMLI, offering a
viable and scalable solution to increase the number of patients who can benefit
from this treatment.
\\ ( https://arxiv.org/abs/2402.06494 ,  5993kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06521 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:34:28 GMT   (7418kb,D)

Title: Reconstructing facade details using MLS point clouds and Bag-of-Words
  approach
Authors: Thomas Froech, Olaf Wysocki, Ludwig Hoegner, Uwe Stilla
Categories: cs.CV cs.LG
Comments: Accepted to the Recent Advances in 3D Geoinformation Science,
  Proceedings of the 18th 3D GeoInfo Conference 2023
\\
  In the reconstruction of fa\c{c}ade elements, the identification of specific
object types remains challenging and is often circumvented by rectangularity
assumptions or the use of bounding boxes. We propose a new approach for the
reconstruction of 3D fa\c{c}ade details. We combine MLS point clouds and a
pre-defined 3D model library using a BoW concept, which we augment by
incorporating semi-global features. We conduct experiments on the models
superimposed with random noise and on the TUM-FA\c{C}ADE dataset. Our method
demonstrates promising results, improving the conventional BoW approach. It
holds the potential to be utilized for more realistic facade reconstruction
without rectangularity assumptions, which can be used in applications such as
testing automated driving functions or estimating fa\c{c}ade solar potential.
\\ ( https://arxiv.org/abs/2402.06521 ,  7418kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06525 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:37:08 GMT   (755kb,D)

Title: Flexible infinite-width graph convolutional networks and the importance
  of representation learning
Authors: Ben Anson, Edward Milsom, Laurence Aitchison
Categories: stat.ML cs.LG
\\
  A common theoretical approach to understanding neural networks is to take an
infinite-width limit, at which point the outputs become Gaussian process (GP)
distributed. This is known as a neural network Gaussian process (NNGP).
However, the NNGP kernel is fixed, and tunable only through a small number of
hyperparameters, eliminating any possibility of representation learning. This
contrasts with finite-width NNs, which are often believed to perform well
precisely because they are able to learn representations. Thus in simplifying
NNs to make them theoretically tractable, NNGPs may eliminate precisely what
makes them work well (representation learning). This motivated us to understand
whether representation learning is necessary in a range of graph classification
tasks. We develop a precise tool for this task, the graph convolutional deep
kernel machine. This is very similar to an NNGP, in that it is an infinite
width limit and uses kernels, but comes with a `knob' to control the amount of
representation learning. We found that representation learning is necessary (in
the sense that it gives dramatic performance improvements) in graph
classification tasks and heterophilous node classification tasks, but not in
homophilous node classification tasks.
\\ ( https://arxiv.org/abs/2402.06525 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06531 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:43:34 GMT   (10046kb,D)

Title: Transferring facade labels between point clouds with semantic octrees
  while considering change detection
Authors: Sophia Schwarz, Tanja Pilz, Olaf Wysocki, Ludwig Hoegner, Uwe Stilla
Categories: cs.CV cs.LG
Comments: Accepted to the Recent Advances in 3D Geoinformation Science,
  Proceedings of the 18th 3D GeoInfo Conference 2023
\\
  Point clouds and high-resolution 3D data have become increasingly important
in various fields, including surveying, construction, and virtual reality.
However, simply having this data is not enough; to extract useful information,
semantic labeling is crucial. In this context, we propose a method to transfer
annotations from a labeled to an unlabeled point cloud using an octree
structure. The structure also analyses changes between the point clouds. Our
experiments confirm that our method effectively transfers annotations while
addressing changes. The primary contribution of this project is the development
of the method for automatic label transfer between two different point clouds
that represent the same real-world object. The proposed method can be of great
importance for data-driven deep learning algorithms as it can also allow
circumventing stochastic transfer learning by deterministic label transfer
between datasets depicting the same objects.
\\ ( https://arxiv.org/abs/2402.06531 ,  10046kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06535 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:49:13 GMT   (1579kb,D)

Title: Bandit Convex Optimisation
Authors: Tor Lattimore
Categories: math.OC cs.LG stat.ML
Comments: 158 pages
\\
  Bandit convex optimisation is a fundamental framework for studying
zeroth-order convex optimisation. These notes cover the many tools used for
this problem, including cutting plane methods, interior point methods,
continuous exponential weights, gradient descent and online Newton step. The
nuances between the many assumptions and setups are explained. Although there
is not much truly new here, some existing tools are applied in novel ways to
obtain new algorithms. A few bounds are improved in minor ways.
\\ ( https://arxiv.org/abs/2402.06535 ,  1579kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06560 (*cross-listing*)
Date: Fri, 9 Feb 2024 17:19:05 GMT   (1998kb,D)

Title: Video Annotator: A framework for efficiently building video classifiers
  using vision-language models and active learning
Authors: Amir Ziai, Aneesh Vartakavi
Categories: cs.CV cs.LG
Comments: Submitted for review to KDD '24 (ADS Track)
\\
  High-quality and consistent annotations are fundamental to the successful
development of robust machine learning models. Traditional data annotation
methods are resource-intensive and inefficient, often leading to a reliance on
third-party annotators who are not the domain experts. Hard samples, which are
usually the most informative for model training, tend to be difficult to label
accurately and consistently without business context. These can arise
unpredictably during the annotation process, requiring a variable number of
iterations and rounds of feedback, leading to unforeseen expenses and time
commitments to guarantee quality.
  We posit that more direct involvement of domain experts, using a
human-in-the-loop system, can resolve many of these practical challenges. We
propose a novel framework we call Video Annotator (VA) for annotating,
managing, and iterating on video classification datasets. Our approach offers a
new paradigm for an end-user-centered model development process, enhancing the
efficiency, usability, and effectiveness of video classifiers. Uniquely, VA
allows for a continuous annotation process, seamlessly integrating data
collection and model training.
  We leverage the zero-shot capabilities of vision-language foundation models
combined with active learning techniques, and demonstrate that VA enables the
efficient creation of high-quality models. VA achieves a median 6.8 point
improvement in Average Precision relative to the most competitive baseline
across a wide-ranging assortment of tasks. We release a dataset with 153k
labels across 56 video understanding tasks annotated by three professional
video editors using VA, and also release code to replicate our experiments at:
http://github.com/netflix/videoannotator.
\\ ( https://arxiv.org/abs/2402.06560 ,  1998kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06562 (*cross-listing*)
Date: Fri, 9 Feb 2024 17:26:26 GMT   (10759kb,D)

Title: Safe Guaranteed Exploration for Non-linear Systems
Authors: Manish Prajapat, Johannes K\"ohler, Matteo Turchetta, Andreas Krause,
  Melanie N. Zeilinger
Categories: eess.SY cs.LG cs.RO cs.SY math.OC
\\
  Safely exploring environments with a-priori unknown constraints is a
fundamental challenge that restricts the autonomy of robots. While safety is
paramount, guarantees on sufficient exploration are also crucial for ensuring
autonomous task completion. To address these challenges, we propose a novel
safe guaranteed exploration framework using optimal control, which achieves
first-of-its-kind results: guaranteed exploration for non-linear systems with
finite time sample complexity bounds, while being provably safe with
arbitrarily high probability. The framework is general and applicable to many
real-world scenarios with complex non-linear dynamics and unknown domains.
Based on this framework we propose an efficient algorithm, SageMPC, SAfe
Guaranteed Exploration using Model Predictive Control. SageMPC improves
efficiency by incorporating three techniques: i) exploiting a Lipschitz bound,
ii) goal-directed exploration, and iii) receding horizon style re-planning, all
while maintaining the desired sample complexity, safety and exploration
guarantees of the framework. Lastly, we demonstrate safe efficient exploration
in challenging unknown environments using SageMPC with a car model.
\\ ( https://arxiv.org/abs/2402.06562 ,  10759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06581 (*cross-listing*)
Date: Fri, 9 Feb 2024 18:01:15 GMT   (8044kb,D)

Title: More than the Sum of Its Parts: Ensembling Backbone Networks for
  Few-Shot Segmentation
Authors: Nico Catalano, Alessandro Maranelli, Agnese Chiatti, Matteo Matteucci
Categories: cs.CV cs.LG
\\
  Semantic segmentation is a key prerequisite to robust image understanding for
applications in \acrlong{ai} and Robotics. \acrlong{fss}, in particular,
concerns the extension and optimization of traditional segmentation methods in
challenging conditions where limited training examples are available. A
predominant approach in \acrlong{fss} is to rely on a single backbone for
visual feature extraction. Choosing which backbone to leverage is a deciding
factor contributing to the overall performance. In this work, we interrogate on
whether fusing features from different backbones can improve the ability of
\acrlong{fss} models to capture richer visual features. To tackle this
question, we propose and compare two ensembling techniques-Independent Voting
and Feature Fusion. Among the available \acrlong{fss} methods, we implement the
proposed ensembling techniques on PANet. The module dedicated to predicting
segmentation masks from the backbone embeddings in PANet avoids trainable
parameters, creating a controlled `in vitro' setting for isolating the impact
of different ensembling strategies. Leveraging the complementary strengths of
different backbones, our approach outperforms the original single-backbone
PANet across standard benchmarks even in challenging one-shot learning
scenarios. Specifically, it achieved a performance improvement of +7.37\% on
PASCAL-5\textsuperscript{i} and of +10.68\% on COCO-20\textsuperscript{i} in
the top-performing scenario where three backbones are combined. These results,
together with the qualitative inspection of the predicted subject masks,
suggest that relying on multiple backbones in PANet leads to a more
comprehensive feature representation, thus expediting the successful
application of \acrlong{fss} methods in challenging, data-scarce environments.
\\ ( https://arxiv.org/abs/2402.06581 ,  8044kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2210.11846
replaced with revised version Fri, 9 Feb 2024 15:28:52 GMT   (1447kb,D)

Title: Redefining Counterfactual Explanations for Reinforcement Learning:
  Overview, Challenges and Opportunities
Authors: Jasmina Gajcin and Ivana Dusparic
Categories: cs.AI
Comments: 32 pages, 6 figures
\\ ( https://arxiv.org/abs/2210.11846 ,  1447kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07514
replaced with revised version Fri, 9 Feb 2024 13:29:14 GMT   (73kb)

Title: Explainability is NOT a Game
Authors: Joao Marques-Silva and Xuanxiang Huang
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2307.07514 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07665
replaced with revised version Fri, 9 Feb 2024 08:53:25 GMT   (2392kb,D)

Title: Deep Backtracking Counterfactuals for Causally Compliant Explanations
Authors: Klaus-Rudolf Kladny, Julius von K\"ugelgen, Bernhard Sch\"olkopf,
  Michael Muehlebach
Categories: cs.AI cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.07665 ,  2392kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14894
replaced with revised version Fri, 9 Feb 2024 11:51:51 GMT   (7384kb,D)

Title: Local Universal Explainer (LUX) -- a rule-based explainer with factual,
  counterfactual and visual explanations
Authors: Szymon Bobek, Grzegorz J. Nalepa
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.14894 ,  7384kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09830
replaced with revised version Fri, 9 Feb 2024 09:48:41 GMT   (734kb,D)

Title: AutoPlanBench: Automatically generating benchmarks for LLM planners from
  PDDL
Authors: Katharina Stein, Daniel Fi\v{s}er, J\"org Hoffmann and Alexander
  Koller
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.09830 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16044
replaced with revised version Fri, 9 Feb 2024 17:11:59 GMT   (10337kb,D)

Title: LLMLight: Large Language Models as Traffic Signal Control Agents
Authors: Siqi Lai, Zhao Xu, Weijia Zhang, Hao Liu and Hui Xiong
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.16044 ,  10337kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05391
replaced with revised version Fri, 9 Feb 2024 09:00:46 GMT   (9617kb,D)

Title: Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
Authors: Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang
  Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, Jiaqi Li, Xiaoze Liu,
  Jeff Z. Pan, Ningyu Zhang, Huajun Chen
Categories: cs.AI cs.CV cs.IR cs.LG
Comments: Ongoing work; 55 pages, 11 Tables, 13 Figures, 619 citations; Paper
  list is available at https://github.com/zjukg/KG-MM-Survey
\\ ( https://arxiv.org/abs/2402.05391 ,  9617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05786
replaced with revised version Fri, 9 Feb 2024 04:19:26 GMT   (450kb)

Title: Prompting Fairness: Artificial Intelligence as Game Players
Authors: Jazmia Henry
Categories: cs.AI cs.GT
Comments: preprint
\\ ( https://arxiv.org/abs/2402.05786 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05894
replaced with revised version Fri, 9 Feb 2024 08:08:57 GMT   (437kb,D)

Title: Large Language Model Meets Graph Neural Network in Knowledge
  Distillation
Authors: Shengxiang Hu, Guobing Zou, Song Yang, Yanglan Gan, Bofeng Zhang,
  Yixin Chen
Categories: cs.AI cs.LG
Comments: 17 pages, 6 figures, 4 tables
MSC-class: 68T30, 68R10, 68T05
\\ ( https://arxiv.org/abs/2402.05894 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2209.09034
replaced with revised version Fri, 9 Feb 2024 15:30:08 GMT   (199kb,D)

Title: ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification
Authors: Kai North, Marcos Zampieri, Tharindu Ranasinghe
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2209.09034 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12707
replaced with revised version Fri, 9 Feb 2024 05:31:11 GMT   (4222kb,D)

Title: Quantifying Association Capabilities of Large Language Models and Its
  Implications on Privacy Leakage
Authors: Hanyin Shao, Jie Huang, Shen Zheng, Kevin Chen-Chuan Chang
Categories: cs.CL cs.AI cs.CR
Comments: EACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.12707 ,  4222kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13179
replaced with revised version Fri, 9 Feb 2024 17:29:19 GMT   (1792kb,D)

Title: Teaching Probabilistic Logical Reasoning to Transformers
Authors: Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi
Categories: cs.CL cs.AI
Comments: This work is part of the proceedings of EACL Findings 2024
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2305.13179 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18582
replaced with revised version Fri, 9 Feb 2024 06:37:51 GMT   (7932kb,D)

Title: Information Association for Language Model Updating by Mitigating
  LM-Logical Discrepancy
Authors: Pengfei Yu and Heng Ji
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.18582 ,  7932kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10635
replaced with revised version Thu, 8 Feb 2024 23:16:17 GMT   (2142kb,D)

Title: SciBench: Evaluating College-Level Scientific Problem-Solving Abilities
  of Large Language Models
Authors: Xiaoxuan Wang and Ziniu Hu and Pan Lu and Yanqiao Zhu and Jieyu Zhang
  and Satyen Subramaniam and Arjun R. Loomba and Shichang Zhang and Yizhou Sun
  and Wei Wang
Categories: cs.CL cs.AI cs.LG
Comments: Results updated; multimodal dataset added
\\ ( https://arxiv.org/abs/2307.10635 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06358
replaced with revised version Fri, 9 Feb 2024 01:44:59 GMT   (63kb,D)

Title: Generative Data Augmentation using LLMs improves Distributional
  Robustness in Question Answering
Authors: Arijit Ghosh Chowdhury, Aman Chadha
Categories: cs.CL cs.AI
Comments: 10 tables, 1 figure, To appear at EACL 2024 Student Research Workshop
\\ ( https://arxiv.org/abs/2309.06358 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08345
replaced with revised version Fri, 9 Feb 2024 07:09:27 GMT   (886kb,D)

Title: Data Distribution Bottlenecks in Grounding Language Models to Knowledge
  Bases
Authors: Yiheng Shu, Zhiwei Yu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2309.08345 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08968
replaced with revised version Thu, 8 Feb 2024 22:43:04 GMT   (542kb,D)

Title: Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large
  Language Models for Dynamic Inference
Authors: Parsa Kavehzadeh, Mojtaba Valipour, Marzieh Tahaei, Ali Ghodsi, Boxing
  Chen, Mehdi Rezagholizadeh
Categories: cs.CL cs.LG
Comments: Accepted to EACL 2024 - Findings
\\ ( https://arxiv.org/abs/2309.08968 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16575
replaced with revised version Fri, 9 Feb 2024 13:55:53 GMT   (6977kb)

Title: A Benchmark for Learning to Translate a New Language from One Grammar
  Book
Authors: Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke
  Melas-Kyriazi
Categories: cs.CL
Comments: Project site: https://lukemelas.github.io/mtob/
\\ ( https://arxiv.org/abs/2309.16575 ,  6977kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03094
replaced with revised version Thu, 8 Feb 2024 22:02:22 GMT   (7097kb,D)

Title: Large Language Model Cascades with Mixture of Thoughts Representations
  for Cost-efficient Reasoning
Authors: Murong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu Yao
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.03094 ,  7097kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09656
replaced with revised version Fri, 9 Feb 2024 16:35:28 GMT   (3134kb,D)

Title: Structured Chemistry Reasoning with Large Language Models
Authors: Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Yejin Choi, Jiawei
  Han, Lianhui Qin
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2311.09656 ,  3134kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13668
replaced with revised version Fri, 9 Feb 2024 18:16:56 GMT   (241kb,AD)

Title: MAIRA-1: A specialised large multimodal model for radiology report
  generation
Authors: Stephanie L. Hyland, Shruthi Bannur, Kenza Bouzid, Daniel C. Castro,
  Mercy Ranjit, Anton Schwaighofer, Fernando P\'erez-Garc\'ia, Valentina
  Salvatelli, Shaury Srivastav, Anja Thieme, Noel Codella, Matthew P. Lungren,
  Maria Teodora Wetscherek, Ozan Oktay, Javier Alvarez-Valle
Categories: cs.CL cs.AI cs.CV
Comments: 18 pages, 9 tables, 5 figures. v2 adds test IDs and image encoder
  citation
\\ ( https://arxiv.org/abs/2311.13668 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15842
replaced with revised version Fri, 9 Feb 2024 17:56:03 GMT   (743kb,D)

Title: Knowledge Distillation of LLM for Automatic Scoring of Science Education
  Assessments
Authors: Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai
Categories: cs.CL cs.AI
Comments: Submitted to AIED2024
\\ ( https://arxiv.org/abs/2312.15842 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05928
replaced with revised version Fri, 9 Feb 2024 05:44:18 GMT   (8258kb,D)

Title: Mitigating Unhelpfulness in Emotional Support Conversations with
  Multifaceted AI Feedback
Authors: Jiashuo Wang, Chunpu Xu, Chak Tou Leong, Wenjie Li, Jing Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.05928 ,  8258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02420
replaced with revised version Fri, 9 Feb 2024 06:36:41 GMT   (124kb,D)

Title: Factuality of Large Language Models in the Year 2024
Authors: Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi
  Georgiev, Rocktim Jyoti Das, Preslav Nakov
Categories: cs.CL cs.AI
Comments: 9 pages, 1 figure and 2 tables
\\ ( https://arxiv.org/abs/2402.02420 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04838
replaced with revised version Fri, 9 Feb 2024 09:04:21 GMT   (7989kb,D)

Title: PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity
  Recognition
Authors: Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, Can Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.04838 ,  7989kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05130
replaced with revised version Fri, 9 Feb 2024 02:45:51 GMT   (966kb,D)

Title: LB-KBQA: Large-language-model and BERT based Knowledge-Based Question
  and Answering System
Authors: Yan Zhao, Zhongyun Li, Yushan Pan, Jiaxing Wang, Yihong Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.05130 ,  966kb)
------------------------------------------------------------------------------
\\
arXiv:2006.07841
replaced with revised version Fri, 9 Feb 2024 02:23:03 GMT   (588kb,D)

Title: Classify and Generate Reciprocally: Simultaneous Positive-Unlabelled
  Learning and Conditional Generation with Extra Data
Authors: Bing Yu, Ke Sun, He Wang, Zhouchen Lin, Zhanxing Zhu
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2006.07841 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06007
replaced with revised version Fri, 9 Feb 2024 16:13:20 GMT   (101kb)

Title: Universal Approximation Power of Deep Residual Neural Networks via
  Nonlinear Control Theory
Authors: Paulo Tabuada and Bahman Gharesifard
Categories: cs.LG cs.SY eess.SY math.OC stat.ML
Comments: Sejun Park and Geonho Hwang brought to our atention a mistake in the
  proof of Theorem 5.1. This mistake is corrected in this version with the
  consequence of increasing the number of neurons per layer from n+1 to 2n+1
Journal-ref: ICLR 2021, TAC 2023
\\ ( https://arxiv.org/abs/2007.06007 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2103.11856
replaced with revised version Fri, 9 Feb 2024 09:48:46 GMT   (129kb,D)

Title: A Link between Coding Theory and Cross-Validation with Applications
Authors: Tapio Pahikkala, Parisa Movahedi, Ileana Montoya, Havu Miikonen,
  Stephan Foldes, Antti Airola, Laszlo Major
Categories: cs.LG cs.IT math.CO math.IT
\\ ( https://arxiv.org/abs/2103.11856 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2202.10027
replaced with revised version Fri, 9 Feb 2024 17:20:19 GMT   (7432kb,D)

Title: Toward More Generalized Malicious URL Detection Models
Authors: YunDa Tsai, Cayon Liow, Yin Sheng Siang, Shou-De Lin
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2202.10027 ,  7432kb)
------------------------------------------------------------------------------
\\
arXiv:2202.12887
replaced with revised version Fri, 9 Feb 2024 15:48:36 GMT   (26167kb,D)

Title: Fault-Tolerant Neural Networks from Biological Error Correction Codes
Authors: Alexander Zlokapa, Andrew K. Tan, John M. Martyn, Ila R. Fiete, Max
  Tegmark, Isaac L. Chuang
Categories: cs.LG cs.NE q-bio.NC stat.ML
Report-no: MIT-CTP/5395
\\ ( https://arxiv.org/abs/2202.12887 ,  26167kb)
------------------------------------------------------------------------------
\\
arXiv:2203.00444
replaced with revised version Thu, 8 Feb 2024 21:36:16 GMT   (109kb,D)

Title: Parameter-free Mirror Descent
Authors: Andrew Jacobsen, Ashok Cutkosky
Categories: cs.LG math.OC stat.ML
Comments: 59 pages. v4: Added a new section (7. Trade-offs in the Horizon
  Dependence) discussing how to achieve an alternative type of parameter-free
  bound using our framework; v3: published at COLT 2022 + fixed typos; v2:
  improved the algorithms in sections 3, 5, and 6 (tighter regret, simpler
  updates and analysis), corrected minor technical details and fixed typos
\\ ( https://arxiv.org/abs/2203.00444 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2209.09441
replaced with revised version Fri, 9 Feb 2024 01:16:15 GMT   (9137kb,D)

Title: Locally Constrained Representations in Reinforcement Learning
Authors: Somjit Nath, Rushiv Arora and Samira Ebrahimi Kahou
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2209.09441 ,  9137kb)
------------------------------------------------------------------------------
\\
arXiv:2210.08106
replaced with revised version Fri, 9 Feb 2024 18:21:39 GMT   (2199kb,D)

Title: A Primal-Dual Algorithm for Hybrid Federated Learning
Authors: Tom Overman, Garrett Blum, Diego Klabjan
Categories: cs.LG
Comments: Accepted by AAAI 2024. To appear in AAAI proceedings
\\ ( https://arxiv.org/abs/2210.08106 ,  2199kb)
------------------------------------------------------------------------------
\\
arXiv:2301.06625
replaced with revised version Fri, 9 Feb 2024 16:47:45 GMT   (2830kb,D)

Title: TDSTF: Transformer-based Diffusion probabilistic model for Sparse Time
  series Forecasting
Authors: Ping Chang, Huayu Li, Stuart F. Quan, Shuyang Lu, Shu-Fen Wung, Janet
  Roveda and Ao Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.06625 ,  2830kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10890
replaced with revised version Fri, 9 Feb 2024 06:02:19 GMT   (6641kb,D)

Title: Learning Interpretable Low-dimensional Representation via Physical
  Symmetry
Authors: Xuanjie Liu, Daniel Chin, Yichen Huang, Gus Xia
Categories: cs.LG cs.AI
Comments: Accepted by NeurIPS 2023
\\ ( https://arxiv.org/abs/2302.10890 ,  6641kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15495
replaced with revised version Fri, 9 Feb 2024 15:35:03 GMT   (19271kb,D)

Title: Real-Time Bus Arrival Prediction: A Deep Learning Approach for Enhanced
  Urban Mobility
Authors: Narges Rashvand, Sanaz Sadat Hosseini, Mona Azarbayjani, Hamed Tabkhi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.15495 ,  19271kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01235
replaced with revised version Fri, 9 Feb 2024 15:22:23 GMT   (141kb,D)

Title: How Graph Structure and Label Dependencies Contribute to Node
  Classification in a Large Network of Documents
Authors: Pirmin Lemberger and Antoine Saillenfest
Categories: cs.LG math.ST stat.TH
Comments: 10 pages, 1 figure
MSC-class: 62-08
ACM-class: G.3
\\ ( https://arxiv.org/abs/2304.01235 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09872
replaced with revised version Fri, 9 Feb 2024 09:30:59 GMT   (133kb,D)

Title: Depth Functions for Partial Orders with a Descriptive Analysis of
  Machine Learning Algorithms
Authors: Hannah Blocher, Georg Schollmeyer, Christoph Jansen, Malte Nalenz
Categories: cs.LG stat.ME
Comments: Accepted to ISIPTA 2023; Forthcoming in: Proceedings of Machine
  Learning Research
\\ ( https://arxiv.org/abs/2304.09872 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11004
replaced with revised version Fri, 9 Feb 2024 16:40:31 GMT   (2002kb,D)

Title: Knowledge Distillation Under Ideal Joint Classifier Assumption
Authors: Huayu Li, Xiwen Chen, Gregory Ditzler, Janet Roveda, Ao Li
Categories: cs.LG
DOI: 10.1016/j.neunet.2024.106160
\\ ( https://arxiv.org/abs/2304.11004 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15984
replaced with revised version Fri, 9 Feb 2024 13:35:37 GMT   (1148kb,D)

Title: Dynamic Inter-treatment Information Sharing for Individualized Treatment
  Effects Estimation
Authors: Vinod Kumar Chauhan, Jiandong Zhou, Ghadeer Ghosheh, Soheila Molaei
  and David A. Clifton
Categories: cs.LG stat.ME
Comments: accepted to The 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2305.15984 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09805
replaced with revised version Fri, 9 Feb 2024 16:04:42 GMT   (23180kb,D)

Title: Mimicking Better by Matching the Approximate Action Distribution
Authors: Jo\~ao A. C\^andido Ramos, Lionel Blond\'e, Naoya Takeishi and
  Alexandros Kalousis
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.09805 ,  23180kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11128
replaced with revised version Fri, 9 Feb 2024 01:06:17 GMT   (16061kb,D)

Title: CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning
Authors: Nikunj Gupta, Somjit Nath and Samira Ebrahimi Kahou
Categories: cs.LG cs.MA
\\ ( https://arxiv.org/abs/2306.11128 ,  16061kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03816
replaced with revised version Fri, 9 Feb 2024 18:27:51 GMT   (34kb)

Title: A Combinatorial Characterization of Supervised Online Learnability
Authors: Vinod Raman, Unique Subedi, Ambuj Tewari
Categories: cs.LG
Comments: 20 pages. arXiv admin note: text overlap with arXiv:2306.06247
\\ ( https://arxiv.org/abs/2307.03816 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04526
replaced with revised version Fri, 9 Feb 2024 14:02:28 GMT   (24369kb,D)

Title: Self-Expanding Neural Networks
Authors: Rupert Mitchell, Robin Menzenbach, Kristian Kersting, Martin Mundt
Categories: cs.LG
Comments: 17 pages, 7 figures
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2307.04526 ,  24369kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16406
replaced with revised version Fri, 9 Feb 2024 13:38:15 GMT   (2384kb,D)

Title: CktGNN: Circuit Graph Neural Network for Electronic Design Automation
Authors: Zehao Dong, Weidong Cao, Muhan Zhang, Dacheng Tao, Yixin Chen, Xuan
  Zhang
Categories: cs.LG
Comments: Accepted by ICLR (International Conference on Learning
  Representations) 2023
\\ ( https://arxiv.org/abs/2308.16406 ,  2384kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00738
replaced with revised version Fri, 9 Feb 2024 13:34:07 GMT   (2105kb,D)

Title: Rethinking the Power of Graph Canonization in Graph Representation
  Learning with Stability
Authors: Zehao Dong, Muhan Zhang, Philip R.O. Payne, Michael A Province, Carlos
  Cruchaga, Tianyu Zhao, Fuhai Li, Yixin Chen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.00738 ,  2105kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03631
replaced with revised version Fri, 9 Feb 2024 09:57:57 GMT   (1447kb,D)

Title: Insights Into the Inner Workings of Transformer Models for Protein
  Function Prediction
Authors: Markus Wenzel, Erik Gr\"uner, Nils Strodthoff
Categories: cs.LG q-bio.BM
Comments: 26 pages, 12 figures, 5 tables, source code available at
  https://github.com/markuswenzel/xai-proteins
Journal-ref: Bioinformatics (2024) btae031
DOI: 10.1093/bioinformatics/btae031
\\ ( https://arxiv.org/abs/2309.03631 ,  1447kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17179
replaced with revised version Fri, 9 Feb 2024 00:13:46 GMT   (872kb,D)

Title: Alphazero-like Tree-Search can Guide Large Language Model Decoding and
  Training
Authors: Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen,
  Weinan Zhang, Jun Wang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2309.17179 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10461
replaced with revised version Fri, 9 Feb 2024 16:59:43 GMT   (17264kb,D)

Title: Model Selection of Zero-shot Anomaly Detectors in the Absence of Labeled
  Validation Data
Authors: Clement Fung, Chen Qiu, Aodong Li, Maja Rudolph
Categories: cs.LG cs.CV
Comments: 14 pages
\\ ( https://arxiv.org/abs/2310.10461 ,  17264kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14751
replaced with revised version Thu, 8 Feb 2024 22:37:36 GMT   (225kb,D)

Title: Efficient and Interpretable Bandit Algorithms
Authors: Subhojyoti Mukherjee, Ruihao Zhu, Branislav Kveton
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.14751 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16355
replaced with revised version Thu, 8 Feb 2024 23:36:44 GMT   (1146kb,D)

Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on
  Any GPU/TPUs
Authors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong
  Chen, Eric Xing, Zhiting Hu
Categories: cs.LG
Comments: RedCoast (Redco) has been released under Apache License 2.0 at
  https://github.com/tanyuqian/redco
\\ ( https://arxiv.org/abs/2310.16355 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17042
replaced with revised version Thu, 8 Feb 2024 23:39:47 GMT   (3121kb,D)

Title: StochGradAdam: Accelerating Neural Networks Training with Stochastic
  Gradient Sampling
Authors: Juyoung Yun
Categories: cs.LG cs.AI cs.CV cs.NE
\\ ( https://arxiv.org/abs/2310.17042 ,  3121kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19064
replaced with revised version Fri, 9 Feb 2024 18:35:22 GMT   (251kb,D)

Title: Apple Tasting: Combinatorial Dimensions and Minimax Rates
Authors: Vinod Raman, Unique Subedi, Ananth Raman, Ambuj Tewari
Categories: cs.LG stat.ML
Comments: 20 pages
\\ ( https://arxiv.org/abs/2310.19064 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00109
replaced with revised version Thu, 8 Feb 2024 22:28:13 GMT   (267kb,D)

Title: FairWASP: Fast and Optimal Fair Wasserstein Pre-processing
Authors: Zikai Xiong, Niccol\`o Dalmasso, Alan Mishler, Vamsi K. Potluru,
  Tucker Balch, Manuela Veloso
Categories: cs.LG stat.ML
Comments: Accepted at AAAI 2024, Main Track. 15 pages, 4 figures, 1 table
\\ ( https://arxiv.org/abs/2311.00109 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08936
replaced with revised version Fri, 9 Feb 2024 11:04:42 GMT   (3750kb,D)

Title: Confident Naturalness Explanation (CNE): A Framework to Explain and
  Assess Patterns Forming Naturalness
Authors: Ahmed Emam, Mohamed Farag, Ribana Roscher
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2311.08936 ,  3750kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15297
replaced with revised version Fri, 9 Feb 2024 13:58:43 GMT   (2373kb,D)

Title: Controllable Expensive Multi-objective Learning with Warm-starting
  Bayesian Optimization
Authors: Quang-Huy Nguyen, Long P. Hoang, Hoang V. Viet, Dung D. Le
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2311.15297 ,  2373kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15960
replaced with revised version Fri, 9 Feb 2024 02:58:37 GMT   (2329kb,D)

Title: Program Machine Policy: Addressing Long-Horizon Tasks by Integrating
  Program Synthesis and State Machines
Authors: Yu-An Lin, Chen-Tao Lee, Guan-Ting Liu, Pu-Jen Cheng, Shao-Hua Sun
Categories: cs.LG cs.AI cs.PL cs.RO
\\ ( https://arxiv.org/abs/2311.15960 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17943
replaced with revised version Thu, 8 Feb 2024 20:28:28 GMT   (512kb,D)

Title: LayerCollapse: Adaptive compression of neural networks
Authors: Soheil Zibakhsh Shabgahi, Mohammad Sohail Shariff, Farinaz Koushanfar
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.17943 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07577
replaced with revised version Thu, 8 Feb 2024 21:28:23 GMT   (1959kb,D)

Title: Benchmarking Distribution Shift in Tabular Data with TableShift
Authors: Josh Gardner, Zoran Popovic, Ludwig Schmidt
Categories: cs.LG
Comments: NeurIPS 2023 Dataset and Benchmarks Track accepted version
\\ ( https://arxiv.org/abs/2312.07577 ,  1959kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14378
replaced with revised version Fri, 9 Feb 2024 15:48:23 GMT   (1884kb,D)

Title: Multimodal Attention Merging for Improved Speech Recognition and Audio
  Event Classification
Authors: Anirudh S. Sundar, Chao-Han Huck Yang, David M. Chan, Shalini Ghosh,
  Venkatesh Ravichandran, Phani Sankar Nidadavolu
Categories: cs.LG cs.SD eess.AS
Comments: 5 pages, 1 figure, ICASSP 2024 Workshop on Self-supervision in Audio,
  Speech and Beyond
\\ ( https://arxiv.org/abs/2312.14378 ,  1884kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15122
replaced with revised version Thu, 8 Feb 2024 19:39:19 GMT   (822kb,D)

Title: Scaling Is All You Need: Autonomous Driving with JAX-Accelerated
  Reinforcement Learning
Authors: Moritz Harmel, Anubhav Paras, Andreas Pasternak, Nicholas Roy, Gary
  Linscott
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2312.15122 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04130
replaced with revised version Thu, 8 Feb 2024 22:13:45 GMT   (693kb,D)

Title: Plug-and-Play Transformer Modules for Test-Time Adaptation
Authors: Xiangyu Chang, Sk Miraj Ahmed, Srikanth V. Krishnamurthy, Basak Guler,
  Ananthram Swami, Samet Oymak, Amit K. Roy-Chowdhury
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.04130 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15024
replaced with revised version Fri, 9 Feb 2024 17:59:40 GMT   (176kb,D)

Title: SliceGPT: Compress Large Language Models by Deleting Rows and Columns
Authors: Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento,
  Torsten Hoefler, James Hensman
Categories: cs.LG cs.CL
Comments: 22 pages, 8 figures, accepted at ICLR24
\\ ( https://arxiv.org/abs/2401.15024 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17350
replaced with revised version Fri, 9 Feb 2024 05:44:54 GMT   (2994kb)

Title: Time Series Supplier Allocation via Deep Black-Litterman Model
Authors: Jiayuan Luo, Wentao Zhang, Yuchen Fang, Xiaowei Gao, Dingyi Zhuang,
  Hao Chen, Xinke Jiang
Categories: cs.LG cs.AI
Comments: In submission to SIGKDD 2024
\\ ( https://arxiv.org/abs/2401.17350 ,  2994kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01344
replaced with revised version Thu, 8 Feb 2024 22:33:03 GMT   (5605kb,D)

Title: Monotone, Bi-Lipschitz, and Polyak-Lojasiewicz Networks
Authors: Ruigang Wang, Krishnamurthy Dvijotham, Ian R. Manchester
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.01344 ,  5605kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03256
replaced with revised version Fri, 9 Feb 2024 00:06:58 GMT   (90kb)

Title: Learning Best-in-Class Policies for the Predict-then-Optimize Framework
Authors: Michael Huang, Vishal Gupta
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2402.03256 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03646
replaced with revised version Fri, 9 Feb 2024 17:15:08 GMT   (3302kb,D)

Title: Lens: A Foundation Model for Network Traffic in Cybersecurity
Authors: Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Huajie Shao
Categories: cs.LG cs.NI
\\ ( https://arxiv.org/abs/2402.03646 ,  3302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04004
replaced with revised version Fri, 9 Feb 2024 01:56:38 GMT   (1429kb,D)

Title: Understanding the Effect of Noise in LLM Training Data with Algorithmic
  Chains of Thought
Authors: Alex Havrilla, Maia Iyer
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.04004 ,  1429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04377
replaced with revised version Thu, 8 Feb 2024 23:15:10 GMT   (970kb,D)

Title: NeRCC: Nested-Regression Coded Computing for Resilient Distributed
  Prediction Serving Systems
Authors: Parsa Moradi, Mohammad Ali Maddah-Ali
Categories: cs.LG cs.DC cs.IT math.IT
\\ ( https://arxiv.org/abs/2402.04377 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04915
replaced with revised version Fri, 9 Feb 2024 15:12:42 GMT   (313kb,D)

Title: Moco: A Learnable Meta Optimizer for Combinatorial Optimization
Authors: Tim Dernedde, Daniela Thyssens, S\"oren Dittrich, Maximilian
  Stubbemann, Lars Schmidt-Thieme
Categories: cs.LG
Comments: 13 pages, 3 figures
\\ ( https://arxiv.org/abs/2402.04915 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05203
replaced with revised version Fri, 9 Feb 2024 16:47:02 GMT   (1504kb,D)

Title: Bellman Conformal Inference: Calibrating Prediction Intervals For Time
  Series
Authors: Zitong Yang, Emmanuel Cand\`es, Lihua Lei
Categories: cs.LG stat.ML
Comments: 17 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.05203 ,  1504kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05262
replaced with revised version Fri, 9 Feb 2024 13:40:22 GMT   (4484kb,D)

Title: Evaluation of Data Augmentation and Loss Functions in Semantic Image
  Segmentation for Drilling Tool Wear Detection
Authors: Elke Schlager, Andreas Windisch, Lukas Hanna, Thomas Kl\"unsner, Elias
  Jan Hagendorfer, Tamara Teppernegg
Categories: cs.CV cs.AI cs.LG
Comments: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this article is
  published in Journal of Intelligent Manufacturing, and is available online at
  https://doi.org/10.1007/s10845-023-02313-y
DOI: 10.1007/s10845-023-02313-y
\\ ( https://arxiv.org/abs/2302.05262 ,  4484kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02611
replaced with revised version Fri, 9 Feb 2024 08:43:11 GMT   (419kb,D)

Title: Stochastic Population Update Can Provably Be Helpful in Multi-Objective
  Evolutionary Algorithms
Authors: Chao Bian, Yawen Zhou, Miqing Li, Chao Qian
Categories: cs.NE cs.AI
\\ ( https://arxiv.org/abs/2306.02611 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13566
replaced with revised version Fri, 9 Feb 2024 02:43:22 GMT   (3209kb,D)

Title: The Impact of Imperfect XAI on Human-AI Decision-Making
Authors: Katelyn Morrison, Philipp Spitzer, Violet Turri, Michelle Feng, Niklas
  K\"uhl, Adam Perer
Categories: cs.HC cs.AI
Comments: Accepted to ACM CSCW 2024. 27 pages, 9 figures, 1 table, additional
  figures/table in the appendix
DOI: 10.1145/3641022
\\ ( https://arxiv.org/abs/2307.13566 ,  3209kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04451
replaced with revised version Fri, 9 Feb 2024 16:28:40 GMT   (3002kb,D)

Title: Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning
  Attacks
Authors: Domenico Cotroneo, Cristina Improta, Pietro Liguori, Roberto Natella
Categories: cs.CR cs.AI
Comments: Accepted for publication at the International Conference on Program
  Comprehension 2024
DOI: 10.1145/3643916.3644416
\\ ( https://arxiv.org/abs/2308.04451 ,  3002kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00155
replaced with revised version Fri, 9 Feb 2024 14:03:08 GMT   (1029kb,D)

Title: LLM in the Shell: Generative Honeypots
Authors: Muris Sladi\'c and Veronica Valeros and Carlos Catania and Sebastian
  Garcia
Categories: cs.CR cs.AI cs.CL
Comments: 6 pages. 2 figures. 2 tables
\\ ( https://arxiv.org/abs/2309.00155 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11641
replaced with revised version Thu, 8 Feb 2024 20:52:25 GMT   (8767kb,D)

Title: Attentive VQ-VAE
Authors: Angello Hoyos and Mariano Rivera
Categories: cs.CV cs.AI
Comments: 5 pages, 4 figures, 2 table2, 1 pseudo-code
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/2309.11641 ,  8767kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05207
replaced with revised version Fri, 9 Feb 2024 03:55:12 GMT   (5880kb,D)

Title: Facial Action Unit Detection Based on Multi-task Learning Strategy for
  Unlabeled Facial Images in the Wild
Authors: Ziqiao Shang, Bin Liu
Categories: cs.CV cs.AI cs.LG
Comments: 15 pages, 6 figure, submitted to an Elsevier journal
\\ ( https://arxiv.org/abs/2310.05207 ,  5880kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18235
replaced with revised version Fri, 9 Feb 2024 01:03:23 GMT   (13461kb,D)

Title: Davidsonian Scene Graph: Improving Reliability in Fine-grained
  Evaluation for Text-to-Image Generation
Authors: Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna,
  Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, Su Wang
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: ICLR 2024; Project website: https://google.github.io/dsg
\\ ( https://arxiv.org/abs/2310.18235 ,  13461kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06623
replaced with revised version Thu, 8 Feb 2024 20:33:00 GMT   (1226kb,D)

Title: VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach
  For Intelligent Highway Transportation Systems
Authors: Armin Danesh Pazho, Vinit Katariya, Ghazal Alinezhad Noghre, Hamed
  Tabkhi
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.06623 ,  1226kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12944
replaced with revised version Fri, 9 Feb 2024 06:42:41 GMT   (6830kb,D)

Title: SkyCharge: Deploying Unmanned Aerial Vehicles for Dynamic Load
  Optimization in Solar Small Cell 5G Networks
Authors: Daksh Dave, Vinay Chamola, Sandeep Joshi, Sherali Zeadally
Categories: cs.NI cs.AI cs.LG cs.NE
\\ ( https://arxiv.org/abs/2311.12944 ,  6830kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16594
replaced with revised version Thu, 8 Feb 2024 19:54:33 GMT   (308kb,D)

Title: Monitor Placement for Fault Localization in Deep Neural Network
  Accelerators
Authors: Wei-Kai Liu
Categories: cs.AR cs.AI
\\ ( https://arxiv.org/abs/2311.16594 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14001
replaced with revised version Fri, 9 Feb 2024 02:32:57 GMT   (130kb,D)

Title: Deep Learning Based Face Recognition Method using Siamese Network
Authors: Enoch Solomon, Abraham Woubie and Eyael Solomon Emiru
Categories: cs.CV cs.AI cs.CR
\\ ( https://arxiv.org/abs/2312.14001 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03630
replaced with revised version Fri, 9 Feb 2024 17:48:19 GMT   (1633kb,D)

Title: Why Solving Multi-agent Path Finding with Large Language Model has not
  Succeeded Yet
Authors: Weizhe Chen, Sven Koenig, Bistra Dilkina
Categories: cs.MA cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.03630 ,  1633kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04846 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 17:44:15 GMT   (5082kb,D)

Title: The inherent goodness of well educated intelligence
Authors: Michael E. Glinsky and Sharon Sievert
Categories: econ.TH cs.AI
Comments: 13 pages, 12 figures, 15 equations, to be submitted to Nature
\\ ( https://arxiv.org/abs/2401.04846 ,  5082kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09773
replaced with revised version Fri, 9 Feb 2024 03:14:10 GMT   (13110kb,D)

Title: SEINE: Structure Encoding and Interaction Network for Nuclei Instance
  Segmentation
Authors: Ye Zhang, Linghan Cai, Ziyue Wang, Yongbing Zhang
Categories: cs.CV cs.AI
Comments: 10 pages, 12 figures, 6 tables, submitted to TMI
\\ ( https://arxiv.org/abs/2401.09773 ,  13110kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10816
replaced with revised version Fri, 9 Feb 2024 00:55:52 GMT   (1039kb,D)

Title: Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve
  Health Outcomes
Authors: Jodi Chiam, Aloysius Lim, Cheryl Nott, Nicholas Mark, Ankur Teredesai,
  Sunil Shinde
Categories: cs.HC cs.AI cs.LG
Comments: 20 pages, 2 figures
\\ ( https://arxiv.org/abs/2401.10816 ,  1039kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01703
replaced with revised version Fri, 9 Feb 2024 05:25:11 GMT   (286kb)

Title: A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver
  Interaction in Los Angeles
Authors: Benjamin A.T. Grahama, Lauren Brown, Georgios Chochlakis, Morteza
  Dehghani, Raquel Delerme, Brittany Friedman, Ellie Graeden, Preni Golazizian,
  Rajat Hebbar, Parsa Hejabi, Aditya Kommineni, Mayag\"uez Salinas, Michael
  Sierra-Ar\'evalo, Jackson Trager, Nicholas Weller, and Shrikanth Narayanan
Categories: cs.CY cs.AI cs.LG eess.AS
Comments: 13 pages
ACM-class: I.2.0; I.2.7
\\ ( https://arxiv.org/abs/2402.01703 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03247
replaced with revised version Fri, 9 Feb 2024 12:56:08 GMT   (30269kb,D)

Title: HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible
  Dataflows for Energy-Efficient CNN Inference
Authors: Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, and Ishan Thakkar
Categories: cs.AR cs.AI cs.ET
Comments: The paper is under review at ACM TODAES
\\ ( https://arxiv.org/abs/2402.03247 ,  30269kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03781 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 01:54:54 GMT   (7844kb,D)

Title: MolTC: Towards Molecular Relational Modeling In Language Models
Authors: Junfeng Fang, Shuai Zhang, Chang Wu, Zhiyuan Liu, Sihang Li, Kun Wang,
  Wenjie Du and Xiang Wang
Categories: q-bio.QM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03781 ,  7844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03843
replaced with revised version Fri, 9 Feb 2024 03:51:59 GMT   (3574kb,D)

Title: A new method for optical steel rope non-destructive damage detection
Authors: Yunqing Bao, Bin Hu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.03843 ,  3574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05027
replaced with revised version Fri, 9 Feb 2024 16:36:48 GMT   (1705kb,D)

Title: Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs
  with Recurrent Message Passing
Authors: Jannis Weil and Zhenghua Bao and Osama Abboud and Tobias Meuser
Categories: cs.MA cs.AI
Comments: Accepted at AAMAS 2024, version with appendix; revised sections 1 and
  7, corrected table 1, final results unchanged
\\ ( https://arxiv.org/abs/2402.05027 ,  1705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05650
replaced with revised version Fri, 9 Feb 2024 07:20:18 GMT   (0kb,I)

Title: Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation
  of LLM-Supported SE Tasks
Authors: Wei Wang, Huilong Ning, Gaowei Zhang, Libo Liu and Yi Wang
Categories: cs.SE cs.AI
Comments: I have decided to withdraw this article as I am in the process of
  making further revisions and edits to improve its content. Thank you for your
  understanding
\\ ( https://arxiv.org/abs/2402.05650 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02330
replaced with revised version Fri, 9 Feb 2024 18:38:53 GMT   (0kb,I)

Title: LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model
Authors: Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang
Categories: cs.CV cs.CL
Comments: The datasets were incomplete as they did not include all the
  necessary copyrights
\\ ( https://arxiv.org/abs/2401.02330 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03003
replaced with revised version Fri, 9 Feb 2024 05:15:07 GMT   (430kb,D)

Title: AST-T5: Structure-Aware Pretraining for Code Generation and
  Understanding
Authors: Linyuan Gong, Mostafa Elhoushi, Alvin Cheung
Categories: cs.SE cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.03003 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2208.04284 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 10:21:12 GMT   (75kb,D)

Title: On Rademacher Complexity-based Generalization Bounds for Deep Learning
Authors: Lan V. Truong
Categories: stat.ML cs.LG
Comments: 38 pages, 1 figure
\\ ( https://arxiv.org/abs/2208.04284 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2208.11665 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 16:10:01 GMT   (10153kb,D)

Title: Statistical exploration of the Manifold Hypothesis
Authors: Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy
Categories: stat.ME cs.LG stat.ML
MSC-class: 62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30
\\ ( https://arxiv.org/abs/2208.11665 ,  10153kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04382 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 16:48:37 GMT   (3349kb,D)

Title: Structure of Classifier Boundaries: Case Study for a Naive Bayes
  Classifier
Authors: Alan F. Karr, Zac Bowen, Adam A. Porter
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2212.04382 ,  3349kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11584 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 02:59:33 GMT   (138kb,D)

Title: Robust variance-regularized risk minimization with concomitant scaling
Authors: Matthew J. Holland
Categories: stat.ML cs.LG
Comments: Revised version accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2301.11584 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06018
replaced with revised version Fri, 9 Feb 2024 17:44:51 GMT   (3332kb,D)

Title: Optimizing Floors in First Price Auctions: an Empirical Study of Yahoo
  Advertising
Authors: Miguel Alcobendas, Jonathan Ji, Hemakumar Gokulakannan, Dawit Wami,
  Boris Kapchits, Emilien Pouradier Duteil, Korby Satow, Maria Rosario Levy
  Roman, Oriol Diaz, Amado A. Diaz Jr., Rabi Kavoori
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2302.06018 ,  3332kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02131 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 18:54:36 GMT   (2909kb,D)

Title: Spacetime-Efficient Low-Depth Quantum State Preparation with
  Applications
Authors: Kaiwen Gui, Alexander M. Dalzell, Alessandro Achille, Martin Suchara,
  Frederic T. Chong
Categories: quant-ph cs.CC cs.LG
\\ ( https://arxiv.org/abs/2303.02131 ,  2909kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09669 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 20:59:42 GMT   (7841kb,D)

Title: Predicting discrete-time bifurcations with deep learning
Authors: Thomas M. Bury, Daniel Dylewsky, Chris T. Bauch, Madhur Anand, Leon
  Glass, Alvin Shrier, Gil Bub
Categories: q-bio.QM cs.LG math.DS
Journal-ref: Nat Commun 14, 6331 (2023)
DOI: 10.1038/s41467-023-42020-z
\\ ( https://arxiv.org/abs/2303.09669 ,  7841kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02621
replaced with revised version Fri, 9 Feb 2024 14:05:35 GMT   (5377kb,D)

Title: High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation
Authors: Arvi Jonnarth, Yushan Zhang, Michael Felsberg
Categories: cs.CV cs.LG eess.IV
Journal-ref: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2024, pp. 1010-1019
\\ ( https://arxiv.org/abs/2304.02621 ,  5377kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12522 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 22:52:40 GMT   (6932kb,D)

Title: A New Inexact Proximal Linear Algorithm with Adaptive Stopping Criteria
  for Robust Phase Retrieval
Authors: Zhong Zheng, Shiqian Ma, and Lingzhou Xue
Categories: math.OC cs.LG eess.SP stat.CO stat.ML
Comments: 23 pages
\\ ( https://arxiv.org/abs/2304.12522 ,  6932kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07572 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 14:51:16 GMT   (2098kb,D)

Title: Towards Convergence Rates for Parameter Estimation in Gaussian-gated
  Mixture of Experts
Authors: Huy Nguyen, TrungTin Nguyen, Khai Nguyen, Nhat Ho
Categories: stat.ML cs.LG
Comments: 32 pages, 9 figures; Huy Nguyen and TrungTin Nguyen contributed
  equally to this work
\\ ( https://arxiv.org/abs/2305.07572 ,  2098kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08167
replaced with revised version Fri, 9 Feb 2024 06:14:54 GMT   (2482kb,D)

Title: Where Does My Model Underperform? A Human Evaluation of Slice Discovery
  Algorithms
Authors: Nari Johnson, \'Angel Alexander Cabrera, Gregory Plumb, Ameet
  Talwalkar
Categories: cs.HC cs.CV cs.LG
Comments: Proceedings of the AAAI Conference on Human Computation and
  Crowdsourcing, 11(1), 65-76. Best Paper Award
DOI: 10.1609/hcomp.v11i1.27548
\\ ( https://arxiv.org/abs/2306.08167 ,  2482kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10943 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 21:17:12 GMT   (676kb,D)

Title: Probabilistic Matching of Real and Generated Data Statistics in
  Generative Adversarial Networks
Authors: Philipp Pilar, Niklas Wahlstr\"om
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2306.10943 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16593 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 15:11:24 GMT   (214kb,D)

Title: Autoregressive with Slack Time Series Model for Forecasting a
  Partially-Observed Dynamical Time Series
Authors: Akifumi Okuno, Yuya Morishita, Yoh-ichi Mototake
Categories: stat.ME cs.LG stat.ML
Comments: 15 pages, 6 figures, accepted to IEEE Access
\\ ( https://arxiv.org/abs/2306.16593 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16978
replaced with revised version Fri, 9 Feb 2024 15:09:48 GMT   (4932kb,D)

Title: Learning Coverage Paths in Unknown Environments with Deep Reinforcement
  Learning
Authors: Arvi Jonnarth, Jie Zhao, Michael Felsberg
Categories: cs.RO cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2306.16978 ,  4932kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07160
replaced with revised version Thu, 8 Feb 2024 21:23:40 GMT   (1279kb,D)

Title: LLark: A Multimodal Instruction-Following Language Model for Music
Authors: Josh Gardner, Simon Durand, Daniel Stoller, Rachel M. Bittner
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2310.07160 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05436 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 21:52:44 GMT   (1515kb,D)

Title: Fair Coresets via Optimal Transport
Authors: Zikai Xiong, Niccol\`o Dalmasso, Shubham Sharma, Freddy Lecue, Daniele
  Magazzeni, Vamsi K. Potluru, Tucker Balch, Manuela Veloso
Categories: stat.ML cs.CY cs.LG
Comments: 21 pages, 8 figures, 5 tables
\\ ( https://arxiv.org/abs/2311.05436 ,  1515kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09922
replaced with revised version Fri, 9 Feb 2024 02:01:16 GMT   (0kb,I)

Title: Fast multiplication by two's complement addition of numbers represented
  as a set of polynomial radix 2 indexes, stored as an integer list for
  massively parallel computation
Authors: Mark Stocks
Categories: cs.MS cs.DC cs.DS cs.LG
Comments: withdrawn to review some details of the text for mathematical
  accuracy and proof. We plan to resubmit
\\ ( https://arxiv.org/abs/2311.09922 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16609
replaced with revised version Fri, 9 Feb 2024 08:40:25 GMT   (101kb,D)

Title: Eigenmatrix for unstructured sparse recovery
Authors: Lexing Ying
Categories: math.NA cs.IT cs.LG cs.NA eess.SP math.IT
\\ ( https://arxiv.org/abs/2311.16609 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15824
replaced with revised version Fri, 9 Feb 2024 12:00:56 GMT   (15kb)

Title: Self-Supervised Learning for Few-Shot Bird Sound Classification
Authors: Ilyass Moummad and Romain Serizel and Nicolas Farrugia
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2312.15824 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05126
replaced with revised version Fri, 9 Feb 2024 09:55:46 GMT   (2369kb,D)

Title: Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving
  Vision Transformer
Authors: Teru Nagamori, Sayaka Shiota, Hitoshi Kiya
Categories: cs.CV cs.LG
Comments: Accepted by APSIPA Transactions on Signal and Information Processing.
  arXiv admin note: substantial text overlap with arXiv:2309.02556
\\ ( https://arxiv.org/abs/2401.05126 ,  2369kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11176 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 09:11:43 GMT   (4173kb,D)

Title: Data-Driven Target Localization: Benchmarking Gradient Descent Using the
  Cram\'er-Rao Bound
Authors: Shyam Venkatasubramanian, Sandeep Gogineni, Bosung Kang, Muralidhar
  Rangaswamy
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2401.11176 ,  4173kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12424
replaced with revised version Fri, 9 Feb 2024 00:44:22 GMT   (466kb,D)

Title: DALex: Lexicase-like Selection via Diverse Aggregation
Authors: Andrew Ni, Li Ding, Lee Spector
Categories: cs.NE cs.LG
Comments: 15 pages, 4 figures. Accepted at EuroGP'24
\\ ( https://arxiv.org/abs/2401.12424 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14423
replaced with revised version Thu, 8 Feb 2024 21:40:47 GMT   (11885kb,D)

Title: Prompt Design and Engineering: Introduction and Advanced Methods
Authors: Xavier Amatriain
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2401.14423 ,  11885kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17205 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 18:08:33 GMT   (1515kb,D)

Title: Adaptive Experiment Design with Synthetic Controls
Authors: Alihan H\"uy\"uk, Zhaozhi Qian, Mihaela van der Schaar
Categories: stat.ML cs.LG
Comments: Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics
\\ ( https://arxiv.org/abs/2401.17205 ,  1515kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01274
replaced with revised version Fri, 9 Feb 2024 11:58:10 GMT   (525kb,D)

Title: On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio
  Classification
Authors: Calum Heggan, Sam Budgett, Timothy Hosepedales, Mehrdad Yaghoobi
Categories: cs.SD cs.LG eess.AS
Comments: Camera Ready version as submitted to ICASSP SASB Workshop 2024. 5
  pages, 2 figures, 3 tables
\\ ( https://arxiv.org/abs/2402.01274 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02304 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 22:56:21 GMT   (1279kb,D)

Title: Efficient Numerical Wave Propagation Enhanced By An End-to-End Deep
  Learning Model
Authors: Luis Kaiser, Richard Tsai, Christian Klingenberg
Categories: math.AP cs.LG
\\ ( https://arxiv.org/abs/2402.02304 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04907 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 07:15:02 GMT   (68kb)

Title: On a Combinatorial Problem Arising in Machine Teaching
Authors: Brigt H{\aa}vardstun, Jan Kratochv\'il, Joakim Sunde, Jan Arne Telle
Categories: math.CO cs.LG
Comments: 14 pages, 1 figure
ACM-class: G.2.1
\\ ( https://arxiv.org/abs/2402.04907 ,  68kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
