Gmail jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 26
send mail ONLY to cs <no-reply@arxiv.org> 2024年2月5日 15:40
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu  1 Feb 24 19:00:00 GMT  to  Fri  2 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.00901
Date: Wed, 31 Jan 2024 23:22:13 GMT   (1438kb)

Title: Real Sparks of Artificial Intelligence and the Importance of Inner
  Interpretability
Authors: Alex Grzankowski
Categories: cs.AI
\\
  The present paper looks at one of the most thorough articles on the
intelligence of GPT, research conducted by engineers at Microsoft. Although
there is a great deal of value in their work, I will argue that, for familiar
philosophical reasons, their methodology, !Blackbox Interpretability"#is
wrongheaded. But there is a better way. There is an exciting and emerging
discipline of !Inner Interpretability"#(and specifically Mechanistic
Interpretability) that aims to uncover the internal activations and weights of
models in order to understand what they represent and the algorithms they
implement. In my view, a crucial mistake in Black-box Interpretability is the
failure to appreciate that how processes are carried out matters when it comes
to intelligence and understanding. I can#t pretend to have a full story that
provides both necessary and sufficient conditions for being intelligent, but I
do think that Inner Interpretability dovetails nicely with plausible
philosophical views of what intelligence requires. So the conclusion is modest,
but the important point in my view is seeing how to get the research on the
right track. Towards the end of the paper, I will show how some of the
philosophical concepts can be used to further refine how Inner Interpretability
is approached, so the paper helps draw out a profitable, future two-way
exchange between Philosophers and Computer Scientists.
\\ ( https://arxiv.org/abs/2402.00901 ,  1438kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01118
Date: Fri, 2 Feb 2024 03:22:12 GMT   (6889kb,D)

Title: Pok\'eLLMon: A Human-Parity Agent for Pok\'emon Battles with Large
  Language Models
Authors: Sihao Hu, Tiansheng Huang, Ling Liu
Categories: cs.AI cs.CL
Comments: 10 pages
\\
  We introduce \textsc{Pok\'eLLMon}, the first LLM-embodied agent that achieves
human-parity performance in tactical battle games, as demonstrated in Pok\'emon
battles. The design of \textsc{Pok\'eLLMon} incorporates three key strategies:
(i) In-context reinforcement learning that instantly consumes text-based
feedback derived from battles to iteratively refine the policy; (ii)
Knowledge-augmented generation that retrieves external knowledge to counteract
hallucination and enables the agent to act timely and properly; (iii)
Consistent action generation to mitigate the \textit{panic switching}
phenomenon when the agent faces a powerful opponent and wants to elude the
battle. We show that online battles against human demonstrates
\textsc{Pok\'eLLMon}'s human-like battle strategies and just-in-time decision
making, achieving 49\% of win rate in the Ladder competitions and 56\% of win
rate in the invited battles. Our implementation and playable battle logs are
available at: \url{https://github.com/git-disl/PokeLLMon}.
\\ ( https://arxiv.org/abs/2402.01118 ,  6889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01276
Date: Fri, 2 Feb 2024 10:05:25 GMT   (3505kb,D)

Title: Federated Unlearning: a Perspective of Stability and Fairness
Authors: Jiaqi Shao, Tao Lin, Xuanyu Cao, Bing Luo
Categories: cs.AI
\\
  This paper explores the multifaceted consequences of federated unlearning
(FU) with data heterogeneity. We introduce key metrics for FU assessment,
concentrating on verification, global stability, and local fairness, and
investigate the inherent trade-offs. Furthermore, we formulate the unlearning
process with data heterogeneity through an optimization framework. Our key
contribution lies in a comprehensive theoretical analysis of the trade-offs in
FU and provides insights into data heterogeneity's impacts on FU. Leveraging
these insights, we propose FU mechanisms to manage the trade-offs, guiding
further development for FU mechanisms. We empirically validate that our FU
mechanisms effectively balance trade-offs, confirming insights derived from our
theoretical analysis.
\\ ( https://arxiv.org/abs/2402.01276 ,  3505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01292
Date: Fri, 2 Feb 2024 10:28:24 GMT   (2614kb,D)

Title: Towards the new XAI: A Hypothesis-Driven Approach to Decision Support
  Using Evidence
Authors: Thao Le, Tim Miller, Ronal Singh, Liz Sonenberg
Categories: cs.AI cs.HC
Comments: 21 pages
\\
  Prior research on AI-assisted human decision-making has explored several
different explainable AI (XAI) approaches. A recent paper has proposed a
paradigm shift calling for hypothesis-driven XAI through a conceptual framework
called evaluative AI that gives people evidence that supports or refutes
hypotheses without necessarily giving a decision-aid recommendation. In this
paper we describe and evaluate an approach for hypothesis-driven XAI based on
the Weight of Evidence (WoE) framework, which generates both positive and
negative evidence for a given hypothesis. Through human behavioural
experiments, we show that our hypothesis-driven approach increases decision
accuracy, reduces reliance compared to a recommendation-driven approach and an
AI-explanation-only baseline, but with a small increase in under-reliance
compared to the recommendation-driven approach. Further, we show that
participants used our hypothesis-driven approach in a materially different way
to the two baselines.
\\ ( https://arxiv.org/abs/2402.01292 ,  2614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01499
Date: Fri, 2 Feb 2024 15:31:08 GMT   (6037kb,D)

Title: Developing and Evaluating a Design Method for Positive Artificial
  Intelligence
Authors: Willem van der Maden, Derek Lomas, Paul Hekkert
Categories: cs.AI
\\
  As artificial intelligence (AI) continues advancing, ensuring positive
societal impacts becomes critical, especially as AI systems become increasingly
ubiquitous in various aspects of life. However, developing "AI for good" poses
substantial challenges around aligning systems with complex human values.
Presently, we lack mature methods for addressing these challenges. This article
presents and evaluates the Positive AI design method aimed at addressing this
gap. The method provides a human-centered process to translate wellbeing
aspirations into concrete practices. First, we explain the method's four key
steps: contextualizing, operationalizing, optimizing, and implementing
wellbeing supported by continuous measurement for feedback cycles. We then
present a multiple case study where novice designers applied the method,
revealing strengths and weaknesses related to efficacy and usability. Next, an
expert evaluation study assessed the quality of the resulting concepts, rating
them moderately high for feasibility, desirability, and plausibility of
achieving intended wellbeing benefits. Together, these studies provide
preliminary validation of the method's ability to improve AI design, while
surfacing areas needing refinement like developing support for complex steps.
Proposed adaptations such as examples and evaluation heuristics could address
weaknesses. Further research should examine sustained application over multiple
projects. This human-centered approach shows promise for realizing the vision
of 'AI for Wellbeing' that does not just avoid harm, but actively benefits
humanity.
\\ ( https://arxiv.org/abs/2402.01499 ,  6037kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01602
Date: Fri, 2 Feb 2024 18:00:35 GMT   (957kb,D)

Title: Foundation Model Sherpas: Guiding Foundation Models through Knowledge
  and Reasoning
Authors: Debarun Bhattacharjya, Junkyu Lee, Don Joven Agravante, Balaji
  Ganesan, Radu Marinescu
Categories: cs.AI
Comments: 9 pages
\\
  Foundation models (FMs) such as large language models have revolutionized the
field of AI by showing remarkable performance in various tasks. However, they
exhibit numerous limitations that prevent their broader adoption in many
real-world systems, which often require a higher bar for trustworthiness and
usability. Since FMs are trained using loss functions aimed at reconstructing
the training corpus in a self-supervised manner, there is no guarantee that the
model's output aligns with users' preferences for a specific task at hand. In
this survey paper, we propose a conceptual framework that encapsulates
different modes by which agents could interact with FMs and guide them suitably
for a set of tasks, particularly through knowledge augmentation and reasoning.
Our framework elucidates agent role categories such as updating the underlying
FM, assisting with prompting the FM, and evaluating the FM output. We also
categorize several state-of-the-art approaches into agent interaction
protocols, highlighting the nature and extent of involvement of the various
agent roles. The proposed framework provides guidance for future directions to
further realize the power of FMs in practical AI systems.
\\ ( https://arxiv.org/abs/2402.01602 ,  957kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01607
Date: Fri, 2 Feb 2024 18:11:43 GMT   (25518kb,D)

Title: Natural Counterfactuals With Necessary Backtracking
Authors: Guang-Yuan Hao, Jiji Zhang, Biwei Huang, Hao Wang, Kun Zhang
Categories: cs.AI cs.LG stat.ME
\\
  Counterfactual reasoning is pivotal in human cognition and especially
important for providing explanations and making decisions. While Judea Pearl's
influential approach is theoretically elegant, its generation of a
counterfactual scenario often requires interventions that are too detached from
the real scenarios to be feasible. In response, we propose a framework of
natural counterfactuals and a method for generating counterfactuals that are
natural with respect to the actual world's data distribution. Our methodology
refines counterfactual reasoning, allowing changes in causally preceding
variables to minimize deviations from realistic scenarios. To generate natural
counterfactuals, we introduce an innovative optimization framework that permits
but controls the extent of backtracking with a naturalness criterion. Empirical
experiments indicate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.01607 ,  25518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00888
Date: Tue, 30 Jan 2024 04:00:54 GMT   (602kb,D)

Title: Security and Privacy Challenges of Large Language Models: A Survey
Authors: Badhan Chandra Das, M. Hadi Amini, Yanzhao Wu
Categories: cs.CL cs.AI cs.CR
\\
  Large Language Models (LLMs) have demonstrated extraordinary capabilities and
contributed to multiple fields, such as generating and summarizing text,
language translation, and question-answering. Nowadays, LLM is becoming a very
popular tool in computerized language processing tasks, with the capability to
analyze complicated linguistic patterns and provide relevant and appropriate
responses depending on the context. While offering significant advantages,
these models are also vulnerable to security and privacy attacks, such as
jailbreaking attacks, data poisoning attacks, and Personally Identifiable
Information (PII) leakage attacks. This survey provides a thorough review of
the security and privacy challenges of LLMs for both training data and users,
along with the application-based risks in various domains, such as
transportation, education, and healthcare. We assess the extent of LLM
vulnerabilities, investigate emerging security and privacy attacks for LLMs,
and review the potential defense mechanisms. Additionally, the survey outlines
existing research gaps in this domain and highlights future research
directions.
\\ ( https://arxiv.org/abs/2402.00888 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00956
Date: Thu, 1 Feb 2024 19:25:50 GMT   (595kb,D)

Title: Exploring Spatial Schema Intuitions in Large Language and Vision Models
Authors: Philipp Wicke and Lennart Wachowiak
Categories: cs.CL
Comments: Preprint
\\
  Despite the ubiquity of large language models (LLMs) in AI research, the
question of embodiment in LLMs remains underexplored, distinguishing them from
embodied systems in robotics where sensory perception directly informs physical
action. Our investigation navigates the intriguing terrain of whether LLMs,
despite their non-embodied nature, effectively capture implicit human
intuitions about fundamental, spatial building blocks of language. We employ
insights from spatial cognitive foundations developed through early
sensorimotor experiences, guiding our exploration through the reproduction of
three psycholinguistic experiments. Surprisingly, correlations between model
outputs and human responses emerge, revealing adaptability without a tangible
connection to embodied experiences. Notable distinctions include polarized
language model responses and reduced correlations in vision language models.
This research contributes to a nuanced understanding of the interplay between
language, spatial experiences, and the computations made by large language
models. More at https://cisnlp.github.io/Spatial_Schemas/
\\ ( https://arxiv.org/abs/2402.00956 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00969
Date: Thu, 1 Feb 2024 19:38:32 GMT   (711kb,D)

Title: SPARQL Generation with Entity Pre-trained GPT for KG Question Answering
Authors: Diego Bustamante, Hideaki Takeda
Categories: cs.CL cs.AI cs.DB cs.IR
Comments: 7 pages, 1 figure, 2 tables. For the implementation, see
  https://github.com/DiegoEmilio01/SPARQL-generation-with-entity-pre-trained-GPT-for-KG-Question-Answering
MSC-class: 68P20, 68T50
ACM-class: H.2.3; H.3.3; I.2.7
\\
  Knowledge Graphs popularity has been rapidly growing in last years. All that
knowledge is available for people to query it through the many online databases
on the internet. Though, it would be a great achievement if non-programmer
users could access whatever information they want to know. There has been a lot
of effort oriented to solve this task using natural language processing tools
and creativity encouragement by way of many challenges. Our approach focuses on
assuming a correct entity linking on the natural language questions and
training a GPT model to create SPARQL queries from them. We managed to isolate
which property of the task can be the most difficult to solve at few or
zero-shot and we proposed pre-training on all entities (under CWA) to improve
the performance. We obtained a 62.703% accuracy of exact SPARQL matches on
testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of
0.009 on the question answering challenge.
\\ ( https://arxiv.org/abs/2402.00969 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00978
Date: Thu, 1 Feb 2024 19:49:44 GMT   (10266kb,D)

Title: An Information-Theoretic Approach to Analyze NLP Classification Tasks
Authors: Luran Wang, Mark Gales, Vatsal Raina
Categories: cs.CL cs.AI cs.IT math.IT
Comments: 21 pages, 10 figures, 11 tables
\\
  Understanding the importance of the inputs on the output is useful across
many tasks. This work provides an information-theoretic framework to analyse
the influence of inputs for text classification tasks. Natural language
processing (NLP) tasks take either a single element input or multiple element
inputs to predict an output variable, where an element is a block of text. Each
text element has two components: an associated semantic meaning and a
linguistic realization. Multiple-choice reading comprehension (MCRC) and
sentiment classification (SC) are selected to showcase the framework. For MCRC,
it is found that the context influence on the output compared to the question
influence reduces on more challenging datasets. In particular, more challenging
contexts allow a greater variation in complexity of questions. Hence, test
creators need to carefully consider the choice of the context when designing
multiple-choice questions for assessment. For SC, it is found the semantic
meaning of the input text dominates (above 80\% for all datasets considered)
compared to its linguistic realisation when determining the sentiment. The
framework is made available at:
https://github.com/WangLuran/nlp-element-influence
\\ ( https://arxiv.org/abs/2402.00978 ,  10266kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01018
Date: Thu, 1 Feb 2024 21:10:44 GMT   (3369kb,D)

Title: HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent
Authors: Weijie Xu, Zicheng Huang, Wenxiang Hu, Xi Fang, Rajesh Kumar
  Cherukuri, Naumaan Nayyar, Lorenzo Malandri, Srinivasan H. Sengamedu
Categories: cs.CL cs.AI
Comments: 13 pages, 9 figures
MSC-class: 68T50
ACM-class: I.2.7
Journal-ref: EACL 2024
\\
  Recent advancements in Large Language Models (LLMs) have been reshaping
Natural Language Processing (NLP) task in several domains. Their use in the
field of Human Resources (HR) has still room for expansions and could be
beneficial for several time consuming tasks. Examples such as time-off
submissions, medical claims filing, and access requests are noteworthy, but
they are by no means the sole instances. However, the aforementioned
developments must grapple with the pivotal challenge of constructing a
high-quality training dataset. On one hand, most conversation datasets are
solving problems for customers not employees. On the other hand, gathering
conversations with HR could raise privacy concerns. To solve it, we introduce
HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR
domains to evaluate LLM Agent. Our work has the following contributions: (1) It
is the first labeled open-sourced conversation dataset in the HR domain for NLP
research. (2) It provides a detailed recipe for the data generation procedure
along with data analysis and human evaluations. The data generation pipeline is
transferable and can be easily adapted for labeled conversation data generation
in other domains. (3) The proposed data-collection pipeline is mostly based on
LLMs with minimal human involvement for annotation, which is time and
cost-efficient.
\\ ( https://arxiv.org/abs/2402.01018 ,  3369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01019
Date: Thu, 1 Feb 2024 21:13:04 GMT   (9514kb,D)

Title: Domain-Independent Deception: A New Taxonomy and Linguistic Analysis
Authors: Rakesh M. Verma, Nachum Dershowitz, Victor Zeng, Dainis Boumber,
  Xuting Liu
Categories: cs.CL cs.CR cs.CY
Comments: 33 pages. arXiv admin note: text overlap with arXiv:2207.01738
\\
  Internet-based economies and societies are drowning in deceptive attacks.
These attacks take many forms, such as fake news, phishing, and job scams,
which we call ``domains of deception.'' Machine-learning and
natural-language-processing researchers have been attempting to ameliorate this
precarious situation by designing domain-specific detectors. Only a few recent
works have considered domain-independent deception. We collect these disparate
threads of research and investigate domain-independent deception. First, we
provide a new computational definition of deception and break down deception
into a new taxonomy. Then, we analyze the debate on linguistic cues for
deception and supply guidelines for systematic reviews. Finally, we investigate
common linguistic features and give evidence for knowledge transfer across
different forms of deception.
\\ ( https://arxiv.org/abs/2402.01019 ,  9514kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01025
Date: Thu, 1 Feb 2024 21:27:19 GMT   (10395kb,D)

Title: Graph-based Clustering for Detecting Semantic Change Across Time and
  Languages
Authors: Xianghe Ma, Michael Strube, Wei Zhao
Categories: cs.CL
Comments: EACL2024 Camera Ready (20 pages)
\\
  Despite the predominance of contextualized embeddings in NLP, approaches to
detect semantic change relying on these embeddings and clustering methods
underperform simpler counterparts based on static word embeddings. This stems
from the poor quality of the clustering methods to produce sense clusters --
which struggle to capture word senses, especially those with low frequency.
This issue hinders the next step in examining how changes in word senses in one
language influence another. To address this issue, we propose a graph-based
clustering approach to capture nuanced changes in both high- and low-frequency
word senses across time and languages, including the acquisition and loss of
these senses over time. Our experimental results show that our approach
substantially surpasses previous approaches in the SemEval2020 binary
classification task across four languages. Moreover, we showcase the ability of
our approach as a versatile visualization tool to detect semantic changes in
both intra-language and inter-language setups. We make our code and data
publicly available.
\\ ( https://arxiv.org/abs/2402.01025 ,  10395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01030
Date: Thu, 1 Feb 2024 21:38:58 GMT   (8761kb,D)

Title: Executable Code Actions Elicit Better LLM Agents
Authors: Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao
  Peng, Heng Ji
Categories: cs.CL cs.AI
Comments: Code, data, model, and demo are available at
  https://github.com/xingyaoww/code-act
\\
  Large Language Model (LLM) agents, capable of performing a broad range of
actions, such as invoking tools and controlling robots, show great potential in
tackling real-world challenges. LLM agents are typically prompted to produce
actions by generating JSON or text in a pre-defined format, which is usually
limited by constrained action space (e.g., the scope of pre-defined tools) and
restricted flexibility (e.g., inability to compose multiple tools). This work
proposes to use executable Python code to consolidate LLM agents' actions into
a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct
can execute code actions and dynamically revise prior actions or emit new
actions upon new observations through multi-turn interactions. Our extensive
analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that
CodeAct outperforms widely used alternatives (up to 20% higher success rate).
The encouraging performance of CodeAct motivates us to build an open-source LLM
agent that interacts with environments by executing interpretable code and
collaborates with users using natural language. To this end, we collect an
instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn
interactions using CodeAct. We show that it can be used with existing data to
improve models in agent-oriented tasks without compromising their general
capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with
Python interpreter and uniquely tailored to perform sophisticated tasks (e.g.,
model training) using existing libraries and autonomously self-debug.
\\ ( https://arxiv.org/abs/2402.01030 ,  8761kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01035
Date: Thu, 1 Feb 2024 21:49:34 GMT   (1376kb,D)

Title: Getting the most out of your tokenizer for pre-training and domain
  adaptation
Authors: Gautier Dagan, Gabriele Synnaeve, Baptiste Rozi\`ere
Categories: cs.CL
\\
  Tokenization is an understudied and often neglected component of modern LLMs.
Most published works use a single tokenizer for all experiments, often borrowed
from another model, without performing ablations or analysis to optimize
tokenization. Moreover, the tokenizer is generally kept unchanged when
fine-tuning a base model. In this paper, we show that the size,
pre-tokenization regular expression, and training data of a tokenizer can
significantly impact the model's generation speed, effective context size,
memory usage, and downstream performance. We train specialized Byte-Pair
Encoding code tokenizers, and conduct extensive ablations on the impact of
tokenizer design on the performance of LLMs for code generation tasks such as
HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters
selection and switching the tokenizer in a pre-trained LLM. We perform our
experiments on models trained from scratch and from pre-trained models,
verifying their applicability to a wide range of use-cases. We find that when
fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of
a pre-trained LLM to obtain large gains in generation speed and effective
context size.
\\ ( https://arxiv.org/abs/2402.01035 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01051
Date: Thu, 1 Feb 2024 22:54:31 GMT   (1679kb,D)

Title: Generation, Distillation and Evaluation of Motivational
  Interviewing-Style Reflections with a Foundational Language Model
Authors: Andrew Brown, Jiading Zhu, Mohamed Abdelwahab, Alec Dong, Cindy Wang,
  Jonathan Rose
Categories: cs.CL
Comments: Accepted to EACL 2024 Long Paper
\\
  Large Foundational Language Models are capable of performing many tasks at a
high level but are difficult to deploy in many applications because of their
size and proprietary ownership. Many will be motivated to distill specific
capabilities of foundational models into smaller models that can be owned and
controlled. In the development of a therapeutic chatbot, we wish to distill a
capability known as reflective listening, in which a therapist produces
reflections of client speech. These reflections either restate what a client
has said, or connect what was said to a relevant observation, idea or guess
that encourages and guides the client to continue contemplation. In this paper,
we present a method for distilling the generation of reflections from a
Foundational Language Model (GPT-4) into smaller models. We first show that
GPT-4, using zero-shot prompting, can generate reflections at near 100% success
rate, superior to all previous methods. Using reflections generated by GPT-4,
we fine-tune different sizes of the GPT-2 family. The GPT-2-small model
achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90%
success. We also show that GPT-4 can help in the labor-intensive task of
evaluating the quality of the distilled models, using it as a zero-shot
classifier. Using triple-human review as a guide, the classifier achieves a
Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.
\\ ( https://arxiv.org/abs/2402.01051 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01053
Date: Thu, 1 Feb 2024 22:56:39 GMT   (767kb,D)

Title: Plan-Grounded Large Language Models for Dual Goal Conversational
  Settings
Authors: Diogo Gl\'oria-Silva, Rafael Ferreira, Diogo Tavares, David Semedo,
  Jo\~ao Magalh\~aes
Categories: cs.CL cs.AI
\\
  Training Large Language Models (LLMs) to follow user instructions has been
shown to supply the LLM with ample capacity to converse fluently while being
aligned with humans. Yet, it is not completely clear how an LLM can lead a
plan-grounded conversation in mixed-initiative settings where instructions flow
in both directions of the conversation, i.e. both the LLM and the user provide
instructions to one another. In this paper, we tackle a dual goal
mixed-initiative conversational setting where the LLM not only grounds the
conversation on an arbitrary plan but also seeks to satisfy both a procedural
plan and user instructions. The LLM is then responsible for guiding the user
through the plan and, at the same time, adapting to new circumstances,
answering questions, and activating safety guardrails when needed. We propose a
novel LLM that grounds the dialogue on a procedural plan, can take the dialogue
initiative, and enforces guardrails on the system's behavior, while also
improving the LLM's responses to unexpected user behavior. Experiments in
controlled settings and with real users show that the best-performing model,
which we call PlanLLM, achieves a 2.1x improvement over a strong baseline.
Moreover, experiments also show good generalization to unseen domains.
\\ ( https://arxiv.org/abs/2402.01053 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01065
Date: Thu, 1 Feb 2024 23:46:05 GMT   (48kb,D)

Title: Evaluation Methodology for Large Language Models for Multilingual
  Document Question and Answer
Authors: Adar Kahana, Jaya Susan Mathew, Said Bleik, Jeremy Reynolds, Oren
  Elisha
Categories: cs.CL cs.AI
\\
  With the widespread adoption of Large Language Models (LLMs), in this paper
we investigate the multilingual capability of these models. Our preliminary
results show that, translating the native language context, question and answer
into a high resource language produced the best results.
\\ ( https://arxiv.org/abs/2402.01065 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01091
Date: Fri, 2 Feb 2024 01:39:00 GMT   (240kb,D)

Title: Reading Between the Tweets: Deciphering Ideological Stances of
  Interconnected Mixed-Ideology Communities
Authors: Zihao He, Ashwin Rao, Siyi Guo, Negar Mokhberian, Kristina Lerman
Categories: cs.CL cs.CY cs.SI
\\
  Recent advances in NLP have improved our ability to understand the nuanced
worldviews of online communities. Existing research focused on probing
ideological stances treats liberals and conservatives as separate groups.
However, this fails to account for the nuanced views of the organically formed
online communities and the connections between them. In this paper, we study
discussions of the 2020 U.S. election on Twitter to identify complex
interacting communities. Capitalizing on this interconnectedness, we introduce
a novel approach that harnesses message passing when finetuning language models
(LMs) to probe the nuanced ideologies of these communities. By comparing the
responses generated by LMs and real-world survey results, our method shows
higher alignment than existing baselines, highlighting the potential of using
LMs in revealing complex ideologies within and across interconnected
mixed-ideology communities.
\\ ( https://arxiv.org/abs/2402.01091 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01097
Date: Fri, 2 Feb 2024 02:12:46 GMT   (7643kb,D)

Title: Let's Negotiate! A Survey of Negotiation Dialogue Systems
Authors: Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma, Zhuang
  Li, Lizhen Qu, Zhaleh Semnani Azad, Ingrid Zukerman, Gholamreza Haffari
Categories: cs.CL
Comments: Accepted by EACL 2024 (findings). arXiv admin note: substantial text
  overlap with arXiv:2212.09072
\\
  Negotiation is a crucial ability in human communication. Recently, there has
been a resurgent research interest in negotiation dialogue systems, whose goal
is to create intelligent agents that can assist people in resolving conflicts
or reaching agreements. Although there have been many explorations into
negotiation dialogue systems, a systematic review of this task has not been
performed to date. We aim to fill this gap by investigating recent studies in
the field of negotiation dialogue systems, and covering benchmarks, evaluations
and methodologies within the literature. We also discuss potential future
directions, including multi-modal, multi-party and cross-cultural negotiation
scenarios. Our goal is to provide the community with a systematic overview of
negotiation dialogue systems and to inspire future research.
\\ ( https://arxiv.org/abs/2402.01097 ,  7643kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01108
Date: Fri, 2 Feb 2024 02:53:11 GMT   (175kb,D)

Title: Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and
  Human-Centered Solutions
Authors: Pouya Pezeshkpour, Eser Kandogan, Nikita Bhutani, Sajjadur Rahman, Tom
  Mitchell, Estevam Hruschka
Categories: cs.CL cs.LG
\\
  Remarkable performance of large language models (LLMs) in a variety of tasks
brings forth many opportunities as well as challenges of utilizing them in
production settings. Towards practical adoption of LLMs, multi-agent systems
hold great promise to augment, integrate, and orchestrate LLMs in the larger
context of enterprise platforms that use existing proprietary data and models
to tackle complex real-world tasks. Despite the tremendous success of these
systems, current approaches rely on narrow, single-focus objectives for
optimization and evaluation, often overlooking potential constraints in
real-world scenarios, including restricted budgets, resources and time.
Furthermore, interpreting, analyzing, and debugging these systems requires
different components to be evaluated in relation to one another. This demand is
currently not feasible with existing methodologies. In this postion paper, we
introduce the concept of reasoning capacity as a unifying criterion to enable
integration of constraints during optimization and establish connections among
different components within the system, which also enable a more holistic and
comprehensive approach to evaluation. We present a formal definition of
reasoning capacity and illustrate its utility in identifying limitations within
each component of the system. We then argue how these limitations can be
addressed with a self-reflective process wherein human-feedback is used to
alleviate shortcomings in reasoning and enhance overall consistency of the
system.
\\ ( https://arxiv.org/abs/2402.01108 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01115
Date: Fri, 2 Feb 2024 03:15:13 GMT   (2271kb,D)

Title: Interpretation of Intracardiac Electrograms Through Textual
  Representations
Authors: William Jongwon Han, Diana Gomez, Avi Alok, Chaojing Duan, Michael A.
  Rosenberg, Douglas Weber, Emerson Liu, Ding Zhao
Categories: cs.CL eess.SP
Comments: 16 pages, 7 figures
ACM-class: I.2.7; J.3
\\
  Understanding the irregular electrical activity of atrial fibrillation (AFib)
has been a key challenge in electrocardiography. For serious cases of AFib,
catheter ablations are performed to collect intracardiac electrograms (EGMs).
EGMs offer intricately detailed and localized electrical activity of the heart
and are an ideal modality for interpretable cardiac studies. Recent
advancements in artificial intelligence (AI) has allowed some works to utilize
deep learning frameworks to interpret EGMs during AFib. Additionally, language
models (LMs) have shown exceptional performance in being able to generalize to
unseen domains, especially in healthcare. In this study, we are the first to
leverage pretrained LMs for finetuning of EGM interpolation and AFib
classification via masked language modeling. We formulate the EGM as a textual
sequence and present competitive performances on AFib classification compared
against other representations. Lastly, we provide a comprehensive
interpretability study to provide a multi-perspective intuition of the model's
behavior, which could greatly benefit the clinical use.
\\ ( https://arxiv.org/abs/2402.01115 ,  2271kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01117
Date: Fri, 2 Feb 2024 03:21:00 GMT   (193kb,D)

Title: DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models
Authors: Mohammadreza Pourreza and Davood Rafiei
Categories: cs.CL cs.DB cs.HC
\\
  Leading models for the text-to-SQL task heavily rely on proprietary Large
Language Models (LLMs), posing concerns over data privacy. Closing the
performance gap between small open-source models and large proprietary models
is crucial to mitigate this reliance. To this end, we introduce a novel
two-stage fine-tuning approach that decomposes the task into two simpler tasks.
Through comprehensive evaluation on two large cross-domain datasets and two
small LLMs, we show that this approach improves execution accuracy by 3 to 7
percent, effectively aligning the performance of open-source models with their
proprietary counterparts.
\\ ( https://arxiv.org/abs/2402.01117 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01152
Date: Fri, 2 Feb 2024 05:38:59 GMT   (21331kb,D)

Title: AccentFold: A Journey through African Accents for Zero-Shot ASR
  Adaptation to Target Accents
Authors: Abraham Toluwase Owodunni, Aditya Yadavalli, Chris Chinenye Emezue,
  Tobi Olatunji, Clinton C Mbataku
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to EACL Findings 2024
\\
  Despite advancements in speech recognition, accented speech remains
challenging. While previous approaches have focused on modeling techniques or
creating accented speech datasets, gathering sufficient data for the multitude
of accents, particularly in the African context, remains impractical due to
their sheer diversity and associated budget constraints. To address these
challenges, we propose \textit{AccentFold}, a method that exploits spatial
relationships between learned accent embeddings to improve downstream Automatic
Speech Recognition (ASR). Our exploratory analysis of speech embeddings
representing 100+ African accents reveals interesting spatial accent
relationships highlighting geographic and genealogical similarities, capturing
consistent phonological, and morphological regularities, all learned
empirically from speech. Furthermore, we discover accent relationships
previously uncharacterized by the Ethnologue. Through empirical evaluation, we
demonstrate the effectiveness of AccentFold by showing that, for
out-of-distribution (OOD) accents, sampling accent subsets for training based
on AccentFold information outperforms strong baselines a relative WER
improvement of 4.6%. AccentFold presents a promising approach for improving ASR
performance on accented speech, particularly in the context of African accents,
where data scarcity and budget constraints pose significant challenges. Our
findings emphasize the potential of leveraging linguistic relationships to
improve zero-shot ASR adaptation to target accents.
\\ ( https://arxiv.org/abs/2402.01152 ,  21331kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01155
Date: Fri, 2 Feb 2024 05:48:39 GMT   (3010kb,D)

Title: CABINET: Content Relevance based Noise Reduction for Table Question
  Answering
Authors: Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumita Bhatia, Yaman
  Kumar, Balaji Krishnamurthy
Categories: cs.CL
Comments: Accepted at ICLR 2024 (spotlight)
\\
  Table understanding capability of Large Language Models (LLMs) has been
extensively studied through the task of question-answering (QA) over tables.
Typically, only a small part of the whole table is relevant to derive the
answer for a given question. The irrelevant parts act as noise and are
distracting information, resulting in sub-optimal performance due to the
vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content
RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to
enable LLMs to focus on relevant tabular data by suppressing extraneous
information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained
differentially with the QA LLM, that weighs the table content based on its
relevance to the input question before feeding it to the question-answering LLM
(QA LLM). To further aid the relevance scorer, CABINET employs a weakly
supervised module that generates a parsing statement describing the criteria of
rows and columns relevant to the question and highlights the content of
corresponding table cells. CABINET significantly outperforms various tabular
LLM baselines, as well as GPT3-based in-context learning methods, is more
robust to noise, maintains outperformance on tables of varying sizes, and
establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We
release our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.
\\ ( https://arxiv.org/abs/2402.01155 ,  3010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01158
Date: Fri, 2 Feb 2024 05:54:12 GMT   (406kb,D)

Title: LLM-Detector: Improving AI-Generated Chinese Text Detection with
  Open-Source LLM Instruction Tuning
Authors: Rongsheng Wang and Haoming Chen and Ruizhe Zhou and Han Ma and Yaofei
  Duan and Yanlan Kang and Songhua Yang and Baoyu Fan and Tao Tan
Categories: cs.CL
Comments: 17 pages, 13 tables, 7 figures
\\
  ChatGPT and other general large language models (LLMs) have achieved
remarkable success, but they have also raised concerns about the misuse of
AI-generated texts. Existing AI-generated text detection models, such as based
on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor
out-of-domain (OOD) detection performance. In this paper, we first collected
Chinese text responses generated by human experts and 9 types of LLMs, for
which to multiple domains questions, and further created a dataset that mixed
human-written sentences and sentences polished by LLMs. We then proposed
LLM-Detector, a novel method for both document-level and sentence-level text
detection through Instruction Tuning of LLMs. Our method leverages the wealth
of knowledge LLMs acquire during pre-training, enabling them to detect the text
they generate. Instruction tuning aligns the model's responses with the user's
expected text detection tasks. Experimental results show that previous methods
struggle with sentence-level AI-generated text detection and OOD detection. In
contrast, our proposed method not only significantly outperforms baseline
methods in both sentence-level and document-level text detection but also
demonstrates strong generalization capabilities. Furthermore, since
LLM-Detector is trained based on open-source LLMs, it is easy to customize for
deployment.
\\ ( https://arxiv.org/abs/2402.01158 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01172
Date: Fri, 2 Feb 2024 06:31:50 GMT   (11897kb,D)

Title: Streaming Sequence Transduction through Dynamic Compression
Authors: Weiting Tan, Yunmo Chen, Tongfei Chen, Guanghui Qin, Haoran Xu, Heidi
  C. Zhang, Benjamin Van Durme, Philipp Koehn
Categories: cs.CL cs.SD eess.AS
\\
  We introduce STAR (Stream Transduction with Anchor Representations), a novel
Transformer-based model designed for efficient sequence-to-sequence
transduction over streams. STAR dynamically segments input streams to create
compressed anchor representations, achieving nearly lossless compression (12x)
in Automatic Speech Recognition (ASR) and outperforming existing methods.
Moreover, STAR demonstrates superior segmentation and latency-quality
trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory
footprint, and quality.
\\ ( https://arxiv.org/abs/2402.01172 ,  11897kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01173
Date: Fri, 2 Feb 2024 06:34:11 GMT   (154kb,D)

Title: Efficient Prompt Caching via Embedding Similarity
Authors: Hanlin Zhu, Banghua Zhu, Jiantao Jiao
Categories: cs.CL cs.LG
Comments: 21 pages, 3 figures
\\
  Large language models (LLMs) have achieved huge success in numerous natural
language process (NLP) tasks. However, it faces the challenge of significant
resource consumption during inference. In this paper, we aim to improve the
inference efficiency of LLMs by prompt caching, i.e., if the current prompt can
be answered by the same response of a previous prompt, one can directly utilize
that previous response without calling the LLM. Specifically, we focus on the
prediction accuracy of prompt caching for single-round question-answering tasks
via embedding similarity. The existing embeddings of prompts mostly focus on
whether two prompts are semantically similar, which is not necessarily
equivalent to whether the same response can answer them. Therefore, we propose
a distillation-based method to fine-tune the existing embeddings for better
caching prediction. Theoretically, we provide finite-sample guarantees for the
convergence of our method under different types of loss functions. Empirically,
we carefully construct a hard dataset based on Kwiatkowski et al. (2019) where
the existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.
We then fine-tune the above embedding model, which significantly improves the
AUC of caching prediction from 0.51 to 0.81. We also conduct simulations
demonstrating that our trained models achieve better caching efficiency than
the previous embedding model.
\\ ( https://arxiv.org/abs/2402.01173 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01176
Date: Fri, 2 Feb 2024 06:44:22 GMT   (475kb,D)

Title: Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing
  External Corpus
Authors: Xiaoxi Li, Zhicheng Dou, Yujia Zhou, Fangchao Liu
Categories: cs.CL cs.IR
\\
  The advent of large language models (LLMs) has showcased their efficacy
across various domains, yet they often hallucinate, especially in
knowledge-intensive tasks that require external knowledge sources. To improve
factual accuracy of language models, retrieval-augmented generation (RAG) has
emerged as a popular solution. However, traditional retrieval modules often
rely on large-scale document indexes, which can be disconnected from generative
tasks. Through generative retrieval (GR) approach, language models can achieve
superior retrieval performance by directly generating relevant document
identifiers (DocIDs). However, the relationship between GR and downstream
tasks, as well as the potential of LLMs in GR, remains unexplored. In this
paper, we present a unified language model that utilizes external corpus to
handle various knowledge-intensive tasks by seamlessly integrating generative
retrieval, closed-book generation, and RAG. In order to achieve effective
retrieval and generation through a unified continuous decoding process, we
introduce the following mechanisms: (1) a ranking-oriented DocID decoding
strategy, which improves ranking ability by directly learning from a DocID
ranking list; (2) a continuous generation strategy to facilitate effective and
efficient RAG; (3) well-designed auxiliary DocID understanding tasks to enhance
the model's comprehension of DocIDs and their relevance to downstream tasks.
Our approach is evaluated on the widely used KILT benchmark using two variants
of backbone models: an encoder-decoder T5 model and a decoder-only LLM, Llama2.
Experimental results showcase the superior performance of our models in both
retrieval and downstream knowledge-intensive tasks.
\\ ( https://arxiv.org/abs/2402.01176 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01182
Date: Fri, 2 Feb 2024 06:57:53 GMT   (277kb,D)

Title: In-Context Learning for Few-Shot Nested Named Entity Recognition
Authors: Meishan Zhang, Bin Wang, Hao Fei, Min Zhang
Categories: cs.CL
Comments: 5 figures
Journal-ref: ICASSP 2024
\\
  In nested Named entity recognition (NER), entities are nested with each
other, and thus requiring more data annotations to address. This leads to the
development of few-shot nested NER, where the prevalence of pretrained language
models with in-context learning (ICL) offers promising solutions. In this work,
we introduce an effective and innovative ICL framework for the setting of
few-shot nested NER. We improve the ICL prompt by devising a novel example
demonstration selection mechanism, EnDe retriever. In EnDe retriever, we employ
contrastive learning to perform three types of representation learning, in
terms of semantic similarity, boundary similarity, and label similarity, to
generate high-quality demonstration examples. Extensive experiments over three
nested NER and four flat NER datasets demonstrate the efficacy of our system.
\\ ( https://arxiv.org/abs/2402.01182 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01267
Date: Fri, 2 Feb 2024 09:41:51 GMT   (177kb)

Title: The Human and the Mechanical: logos, truthfulness, and ChatGPT
Authors: Anastasia Giannakidou and Alda Mari
Categories: cs.CL cs.AI
Comments: Under submission
\\
  The paper addresses the question of whether it is appropriate to talk about
`mechanical minds' at all, and whether ChatGPT models can indeed be thought of
as realizations of that. Our paper adds a semantic argument to the current
debate. The act of human assertion requires the formation of a veridicality
judgment. Modification of assertions with modals (John must be at home) and the
use of subjective elements (John is obviously at home) indicate that the
speaker is manipulating her judgments and, in a cooperative context, intends
her epistemic state to be transparent to the addressee. Veridicality judgments
are formed on the basis of two components: (i) evidence that relates to reality
(exogenous evidence) and (ii) endogenous evidence, such as preferences and
private beliefs. `Mechanical minds' lack these two components: (i) they do not
relate to reality and (ii) do not have endogenous evidence. Therefore they lack
the ability to form a belief about the world and a veridicality judgments
altogether. They can only mimic that judgment, but the output is not ground in
the very foundations for it.
\\ ( https://arxiv.org/abs/2402.01267 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01300
Date: Fri, 2 Feb 2024 10:42:06 GMT   (16kb,D)

Title: Two Approaches to Diachronic Normalization of Polish Texts
Authors: Kacper Dudzic, Filip Grali\'nski, Krzysztof Jassem, Marek Kubis, Piotr
  Wierzcho\'n
Categories: cs.CL
Comments: Accepted to the LaTeCH-CLfL 2024 workshop
\\
  This paper discusses two approaches to the diachronic normalization of Polish
texts: a rule-based solution that relies on a set of handcrafted patterns, and
a neural normalization model based on the text-to-text transfer transformer
architecture. The training and evaluation data prepared for the task are
discussed in detail, along with experiments conducted to compare the proposed
normalization solutions. A quantitative and qualitative analysis is made. It is
shown that at the current stage of inquiry into the problem, the rule-based
solution outperforms the neural one on 3 out of 4 variants of the prepared
dataset, although in practice both approaches have distinct advantages and
disadvantages.
\\ ( https://arxiv.org/abs/2402.01300 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01349
Date: Fri, 2 Feb 2024 12:07:00 GMT   (289kb,D)

Title: Beyond the Answers: Reviewing the Rationality of Multiple Choice
  Question Answering for the Evaluation of Large Language Models
Authors: Haochun Wang, Sendong Zhao, Zewen Qiang, Bing Qin, Ting Liu
Categories: cs.CL cs.AI
Comments: 13 pages, 4 figures
\\
  In the field of natural language processing (NLP), Large Language Models
(LLMs) have precipitated a paradigm shift, markedly enhancing performance in
natural language generation tasks. Despite these advancements, the
comprehensive evaluation of LLMs remains an inevitable challenge for the
community. Recently, the utilization of Multiple Choice Question Answering
(MCQA) as a benchmark for LLMs has gained considerable traction. This study
investigates the rationality of MCQA as an evaluation method for LLMs. If LLMs
genuinely understand the semantics of questions, their performance should
exhibit consistency across the varied configurations derived from the same
questions. Contrary to this expectation, our empirical findings suggest a
notable disparity in the consistency of LLM responses, which we define as
REsponse VAriability Syndrome (REVAS) of the LLMs, indicating that current
MCQA-based benchmarks may not adequately capture the true capabilities of LLMs,
which underscores the need for more robust evaluation mechanisms in assessing
the performance of LLMs.
\\ ( https://arxiv.org/abs/2402.01349 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01352
Date: Fri, 2 Feb 2024 12:11:16 GMT   (3506kb,D)

Title: Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting
  the Variation in Human Signals during Visuo-Linguistic Processes
Authors: Ece Takmaz, Sandro Pezzelle, Raquel Fern\'andez
Categories: cs.CL cs.AI cs.CV
Comments: To appear in EACL 2024
\\
  There is an intricate relation between the properties of an image and how
humans behave while describing the image. This behavior shows ample variation,
as manifested in human signals such as eye movements and when humans start to
describe the image. Despite the value of such signals of visuo-linguistic
variation, they are virtually disregarded in the training of current pretrained
models, which motivates further investigation. Using a corpus of Dutch image
descriptions with concurrently collected eye-tracking data, we explore the
nature of the variation in visuo-linguistic signals, and find that they
correlate with each other. Given this result, we hypothesize that variation
stems partly from the properties of the images, and explore whether image
representations encoded by pretrained vision encoders can capture such
variation. Our results indicate that pretrained models do so to a
weak-to-moderate degree, suggesting that the models lack biases about what
makes a stimulus complex for humans and what leads to variations in human
outputs.
\\ ( https://arxiv.org/abs/2402.01352 ,  3506kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01360
Date: Fri, 2 Feb 2024 12:27:58 GMT   (7629kb,D)

Title: What Makes Medical Claims (Un)Verifiable? Analyzing Entity and Relation
  Properties for Fact Verification
Authors: Amelie W\"uhrl and Yarik Menchaca Resendiz and Lara Grimminger and
  Roman Klinger
Categories: cs.CL
Comments: Accepted at EACL 2024
\\
  Biomedical claim verification fails if no evidence can be discovered. In
these cases, the fact-checking verdict remains unknown and the claim is
unverifiable. To improve upon this, we have to understand if there are any
claim properties that impact its verifiability. In this work we assume that
entities and relations define the core variables in a biomedical claim's
anatomy and analyze if their properties help us to differentiate verifiable
from unverifiable claims. In a study with trained annotation experts we prompt
them to find evidence for biomedical claims, and observe how they refine search
queries for their evidence search. This leads to the first corpus for
scientific fact verification annotated with subject-relation-object triplets,
evidence documents, and fact-checking verdicts (the BEAR-Fact corpus). We find
(1) that discovering evidence for negated claims (e.g., X-does-not-cause-Y) is
particularly challenging. Further, we see that annotators process queries
mostly by adding constraints to the search and by normalizing entities to
canonical names. (2) We compare our in-house annotations with a small
crowdsourcing setting where we employ medical experts and laypeople. We find
that domain expertise does not have a substantial effect on the reliability of
annotations. Finally, (3), we demonstrate that it is possible to reliably
estimate the success of evidence retrieval purely from the claim text~(.82\F),
whereas identifying unverifiable claims proves more challenging (.27\F). The
dataset is available at http://www.ims.uni-stuttgart.de/data/bioclaim.
\\ ( https://arxiv.org/abs/2402.01360 ,  7629kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01364
Date: Fri, 2 Feb 2024 12:34:09 GMT   (1653kb,D)

Title: Continual Learning for Large Language Models: A Survey
Authors: Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu,
  Gholamreza Haffari
Categories: cs.CL cs.LG
\\
  Large language models (LLMs) are not amenable to frequent re-training, due to
high training costs arising from their massive scale. However, updates are
necessary to endow LLMs with new skills and keep them up-to-date with rapidly
evolving human knowledge. This paper surveys recent works on continual learning
for LLMs. Due to the unique nature of LLMs, we catalog continue learning
techniques in a novel multi-staged categorization scheme, involving continual
pretraining, instruction tuning, and alignment. We contrast continual learning
for LLMs with simpler adaptation methods used in smaller models, as well as
with other enhancement strategies like retrieval-augmented generation and model
editing. Moreover, informed by a discussion of benchmarks and evaluation, we
identify several challenges and future work directions for this crucial task.
\\ ( https://arxiv.org/abs/2402.01364 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01375
Date: Fri, 2 Feb 2024 12:59:27 GMT   (12303kb,D)

Title: Dive into the Chasm: Probing the Gap between In- and Cross-Topic
  Generalization
Authors: Andreas Waldis, Yufang Hou, Iryna Gurevych
Categories: cs.CL
Comments: EACL 2024
\\
  Pre-trained language models (LMs) perform well in In-Topic setups, where
training and testing data come from the same topics. However, they face
challenges in Cross-Topic scenarios where testing data is derived from distinct
topics -- such as Gun Control. This study analyzes various LMs with three
probing-based experiments to shed light on the reasons behind the In- vs.
Cross-Topic generalization gap. Thereby, we demonstrate, for the first time,
that generalization gaps and the robustness of the embedding space vary
significantly across LMs. Additionally, we assess larger LMs and underscore the
relevance of our analysis for recent models. Overall, diverse pre-training
objectives, architectural regularization, or data deduplication contribute to
more robust LMs and diminish generalization gaps. Our research contributes to a
deeper understanding and comparison of language models across different
generalization scenarios.
\\ ( https://arxiv.org/abs/2402.01375 ,  12303kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01376
Date: Fri, 2 Feb 2024 13:00:38 GMT   (503kb)

Title: LoTR: Low Tensor Rank Weight Adaptation
Authors: Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev and Ivan Oseledets
Categories: cs.CL cs.AI cs.LG
Comments: Submitted
\\
  In this paper we generalize and extend an idea of low-rank adaptation (LoRA)
of large language models (LLMs) based on Transformer architecture. Widely used
LoRA-like methods of fine-tuning LLMs are based on matrix factorization of
gradient update. We introduce LoTR, a novel approach for parameter-efficient
fine-tuning of LLMs which represents a gradient update to parameters in a form
of tensor decomposition. Low-rank adapter for each layer is constructed as a
product of three matrices, and tensor structure arises from sharing left and
right multipliers of this product among layers. Simultaneous compression of a
sequence of layers with low-rank tensor representation allows LoTR to archive
even better parameter efficiency then LoRA especially for deep models.
Moreover, the core tensor does not depend on original weight dimension and can
be made arbitrary small, which allows for extremely cheap and fast downstream
fine-tuning.
\\ ( https://arxiv.org/abs/2402.01376 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01383
Date: Fri, 2 Feb 2024 13:06:35 GMT   (364kb,D)

Title: LLM-based NLG Evaluation: Current Status and Challenges
Authors: Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan
Categories: cs.CL
\\
  Evaluating natural language generation (NLG) is a vital but challenging
problem in artificial intelligence. Traditional evaluation metrics mainly
capturing content (e.g. n-gram) overlap between system outputs and references
are far from satisfactory, and large language models (LLMs) such as ChatGPT
have demonstrated great potential in NLG evaluation in recent years. Various
automatic evaluation methods based on LLMs have been proposed, including
metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled
evaluation data. In this survey, we first give a taxonomy of LLM-based NLG
evaluation methods, and discuss their pros and cons, respectively. We also
discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several
open problems in this area and point out future research directions.
\\ ( https://arxiv.org/abs/2402.01383 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01404
Date: Fri, 2 Feb 2024 13:37:07 GMT   (7013kb,D)

Title: On Measuring Context Utilization in Document-Level MT Systems
Authors: Wafaa Mohammed, Vlad Niculae
Categories: cs.CL
\\
  Document-level translation models are usually evaluated using general metrics
such as BLEU, which are not informative about the benefits of context. Current
work on context-aware evaluation, such as contrastive methods, only measure
translation accuracy on words that need context for disambiguation. Such
measures cannot reveal whether the translation model uses the correct
supporting context. We propose to complement accuracy-based evaluation with
measures of context utilization. We find that perturbation-based analysis
(comparing models' performance when provided with correct versus random
context) is an effective measure of overall context utilization. For a
finer-grained phenomenon-specific evaluation, we propose to measure how much
the supporting context contributes to handling context-dependent discourse
phenomena. We show that automatically-annotated supporting context gives
similar conclusions to human-annotated context and can be used as alternative
for cases where human annotations are not available. Finally, we highlight the
importance of using discourse-rich datasets when assessing context utilization.
\\ ( https://arxiv.org/abs/2402.01404 ,  7013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01416
Date: Fri, 2 Feb 2024 13:55:37 GMT   (9695kb,D)

Title: Sequence Shortening for Context-Aware Machine Translation
Authors: Pawe{\l} M\k{a}ka, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis
Categories: cs.CL cs.AI cs.LG
Comments: Findings of the ACL: EACL 2024
\\
  Context-aware Machine Translation aims to improve translations of sentences
by incorporating surrounding sentences as context. Towards this task, two main
architectures have been applied, namely single-encoder (based on concatenation)
and multi-encoder models. In this study, we show that a special case of
multi-encoder architecture, where the latent representation of the source
sentence is cached and reused as the context in the next step, achieves higher
accuracy on the contrastive datasets (where the models have to rank the correct
translation among the provided sentences) and comparable BLEU and COMET scores
as the single- and multi-encoder approaches. Furthermore, we investigate the
application of Sequence Shortening to the cached representations. We test three
pooling-based shortening techniques and introduce two novel methods - Latent
Grouping and Latent Selecting, where the network learns to group tokens or
selects the tokens to be cached as context. Our experiments show that the two
methods achieve competitive BLEU and COMET scores and accuracies on the
contrastive datasets to the other tested methods while potentially allowing for
higher interpretability and reducing the growth of memory requirements with
increased context size.
\\ ( https://arxiv.org/abs/2402.01416 ,  9695kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01423
Date: Fri, 2 Feb 2024 14:08:34 GMT   (1838kb,D)

Title: Different Tastes of Entities: Investigating Human Label Variation in
  Named Entity Annotations
Authors: Siyao Peng, Zihang Sun, Sebastian Loftus, Barbara Plank
Categories: cs.CL
Comments: 9 pages; Accepted at UnImplicit workshop at EACL 2024
\\
  Named Entity Recognition (NER) is a key information extraction task with a
long-standing tradition. While recent studies address and aim to correct
annotation errors via re-labeling efforts, little is known about the sources of
human label variation, such as text ambiguity, annotation error, or guideline
divergence. This is especially the case for high-quality datasets and beyond
English CoNLL03. This paper studies disagreements in expert-annotated named
entity datasets for three languages: English, Danish, and Bavarian. We show
that text ambiguity and artificial guideline changes are dominant factors for
diverse annotations among high-quality revisions. We survey student annotations
on a subset of difficult entities and substantiate the feasibility and
necessity of manifold annotations for understanding named entity ambiguities
from a distributional perspective.
\\ ( https://arxiv.org/abs/2402.01423 ,  1838kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01427
Date: Fri, 2 Feb 2024 14:15:01 GMT   (10647kb,D)

Title: The effect of diversity on group decision-making
Authors: Georgi Karadzhov, Andreas Vlachos, Tom Stafford
Categories: cs.CL
\\
  We explore different aspects of cognitive diversity and its effect on the
success of group deliberation. To evaluate this, we use 500 dialogues from
small, online groups discussing the Wason Card Selection task - the DeliData
corpus. Leveraging the corpus, we perform quantitative analysis evaluating
three different measures of cognitive diversity. First, we analyse the effect
of group size as a proxy measure for diversity. Second, we evaluate the effect
of the size of the initial idea pool. Finally, we look into the content of the
discussion by analysing discussed solutions, discussion patterns, and how
conversational probing can improve those characteristics.
  Despite the reputation of groups for compounding bias, we show that small
groups can, through dialogue, overcome intuitive biases and improve individual
decision-making. Across a large sample and different operationalisations, we
consistently find that greater cognitive diversity is associated with more
successful group deliberation. Code and data used for the analysis are
available in the anonymised repository: https://anonymous.4open.science/
r/cogsci24-FD6D
\\ ( https://arxiv.org/abs/2402.01427 ,  10647kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01453
Date: Fri, 2 Feb 2024 14:42:09 GMT   (7734kb,D)

Title: The Queen of England is not England's Queen: On the Lack of Factual
  Coherency in PLMs
Authors: Paul Youssef, J\"org Schl\"otterer, Christin Seifert
Categories: cs.CL
Comments: Accepted to EACL Findings 2024
\\
  Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches
their representations and justifies their use as knowledge bases. Previous work
has focused on probing PLMs for factual knowledge by measuring how often they
can correctly predict an object entity given a subject and a relation, and
improving fact retrieval by optimizing the prompts used for querying PLMs. In
this work, we consider a complementary aspect, namely the coherency of factual
knowledge in PLMs, i.e., how often can PLMs predict the subject entity given
its initial prediction of the object entity. This goes beyond evaluating how
much PLMs know, and focuses on the internal state of knowledge inside them. Our
results indicate that PLMs have low coherency using manually written, optimized
and paraphrased prompts, but including an evidence paragraph leads to
substantial improvement. This shows that PLMs fail to model inverse relations
and need further enhancements to be able to handle retrieving facts from their
parameters in a coherent manner, and to be considered as knowledge bases.
\\ ( https://arxiv.org/abs/2402.01453 ,  7734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01469
Date: Fri, 2 Feb 2024 14:56:48 GMT   (1800kb,D)

Title: AMOR: A Recipe for Building Adaptable Modular Knowledge Agents Through
  Process Feedback
Authors: Jian Guan, Wei Wu, Zujie Wen, Peng Xu, Hongning Wang, Minlie Huang
Categories: cs.CL
Comments: Work in progress
\\
  The notable success of large language models (LLMs) has sparked an upsurge in
building language agents to complete various complex tasks. We present AMOR, an
agent framework based on open-source LLMs, which reasons with external
knowledge bases and adapts to specific domains through human supervision to the
reasoning process. AMOR builds reasoning logic over a finite state machine
(FSM) that solves problems through autonomous executions and transitions over
disentangled modules. This allows humans to provide direct feedback to the
individual modules, and thus naturally forms process supervision. Based on this
reasoning and feedback framework, we develop AMOR through two-stage
fine-tuning: warm-up and adaptation. The former fine-tunes the LLM with
examples automatically constructed from various public datasets and enables
AMOR to generalize across different knowledge environments, while the latter
tailors AMOR to specific domains using process feedback. Extensive experiments
across multiple domains demonstrate the advantage of AMOR to strong baselines,
thanks to its FSM-based reasoning and process feedback mechanism.
\\ ( https://arxiv.org/abs/2402.01469 ,  1800kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01495
Date: Fri, 2 Feb 2024 15:26:39 GMT   (7721kb,D)

Title: A Comparative Analysis of Conversational Large Language Models in
  Knowledge-Based Text Generation
Authors: Phillip Schneider, Manuel Klettner, Elena Simperl, Florian Matthes
Categories: cs.CL
Comments: Accepted to EACL 2024
\\
  Generating natural language text from graph-structured data is essential for
conversational information seeking. Semantic triples derived from knowledge
graphs can serve as a valuable source for grounding responses from
conversational agents by providing a factual basis for the information they
communicate. This is especially relevant in the context of large language
models, which offer great potential for conversational interaction but are
prone to hallucinating, omitting, or producing conflicting information. In this
study, we conduct an empirical analysis of conversational large language models
in generating natural language text from semantic triples. We compare four
large language models of varying sizes with different prompting techniques.
Through a series of benchmark experiments on the WebNLG dataset, we analyze the
models' performance and identify the most common issues in the generated
predictions. Our findings show that the capabilities of large language models
in triple verbalization can be significantly improved through few-shot
prompting, post-processing, and efficient fine-tuning techniques, particularly
for smaller models that exhibit lower zero-shot performance.
\\ ( https://arxiv.org/abs/2402.01495 ,  7721kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01505
Date: Fri, 2 Feb 2024 15:38:47 GMT   (7918kb,D)

Title: Code-Switched Language Identification is Harder Than You Think
Authors: Laurie Burchell, Alexandra Birch, Robert P. Thompson, Kenneth Heafield
Categories: cs.CL
Comments: EACL 2024
\\
  Code switching (CS) is a very common phenomenon in written and spoken
communication but one that is handled poorly by many natural language
processing applications. Looking to the application of building CS corpora, we
explore CS language identification (LID) for corpus building. We make the task
more realistic by scaling it to more languages and considering models with
simpler architectures for faster inference. We also reformulate the task as a
sentence-level multi-label tagging problem to make it more tractable. Having
defined the task, we investigate three reasonable models for this task and
define metrics which better reflect desired performance. We present empirical
evidence that no current approach is adequate and finally provide
recommendations for future work in this area.
\\ ( https://arxiv.org/abs/2402.01505 ,  7918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01510
Date: Fri, 2 Feb 2024 15:44:28 GMT   (970kb)

Title: A Hybrid Strategy for Chat Transcript Summarization
Authors: Pratik K. Biswas
Categories: cs.CL
Comments: Journal Paper (13 Pages, 7 Figures, 4 Tables). arXiv admin note: text
  overlap with arXiv:2103.10599
MSC-class: 68
ACM-class: I.7
\\
  Text summarization is the process of condensing a piece of text to fewer
sentences, while still preserving its content. Chat transcript, in this
context, is a textual copy of a digital or online conversation between a
customer (caller) and agent(s). This paper presents an indigenously (locally)
developed hybrid method that first combines extractive and abstractive
summarization techniques in compressing ill-punctuated or un-punctuated chat
transcripts to produce more readable punctuated summaries and then optimizes
the overall quality of summarization through reinforcement learning. Extensive
testing, evaluations, comparisons, and validation have demonstrated the
efficacy of this approach for large-scale deployment of chat transcript
summarization, in the absence of manually generated reference (annotated)
summaries.
\\ ( https://arxiv.org/abs/2402.01510 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01512
Date: Fri, 2 Feb 2024 15:53:31 GMT   (503kb,D)

Title: Distractor Generation for Multiple-Choice Questions: A Survey of
  Methods, Datasets, and Evaluation
Authors: Elaf Alhazmi, Quan Z. Sheng, Wei Emma Zhang, Munazza Zaib, Ahoud
  Alhazmi
Categories: cs.CL
\\
  Distractors are important in learning evaluation. This paper surveys
distractor generation tasks using English multiple-choice question datasets for
textual and multimodal contexts. In particular, this paper presents a thorough
literature review of the recent studies on distractor generation tasks,
discusses multiple choice components and their characteristics, analyzes the
related datasets, and summarizes the evaluation metrics of distractor
generation. Our investigation reveals that more than half of datasets are
human-generated from educational sources in specific domains such as Science
and English, which are largely text-based, with a lack of open domain and
multimodal datasets.
\\ ( https://arxiv.org/abs/2402.01512 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01513
Date: Fri, 2 Feb 2024 15:54:19 GMT   (6909kb,D)

Title: Multilingual Gradient Word-Order Typology from Universal Dependencies
Authors: Emi Baylor and Esther Ploeger and Johannes Bjerva
Categories: cs.CL
Comments: EACL 2024
\\
  While information from the field of linguistic typology has the potential to
improve performance on NLP tasks, reliable typological data is a prerequisite.
Existing typological databases, including WALS and Grambank, suffer from
inconsistencies primarily caused by their categorical format. Furthermore,
typological categorisations by definition differ significantly from the
continuous nature of phenomena, as found in natural language corpora. In this
paper, we introduce a new seed dataset made up of continuous-valued data,
rather than categorical data, that can better reflect the variability of
language. While this initial dataset focuses on word-order typology, we also
present the methodology used to create the dataset, which can be easily adapted
to generate data for a broader set of features and languages.
\\ ( https://arxiv.org/abs/2402.01513 ,  6909kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01521
Date: Fri, 2 Feb 2024 16:07:05 GMT   (2013kb,D)

Title: K-Level Reasoning with Large Language Models
Authors: Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, Furu
  Wei
Categories: cs.CL cs.AI
\\
  While Large Language Models (LLMs) have demonstrated their proficiency in
complex reasoning tasks, their performance in dynamic, interactive, and
competitive scenarios - such as business strategy and stock market analysis -
remains underexplored. To bridge this gap, we formally explore the dynamic
reasoning capabilities of LLMs for decision-making in rapidly evolving
environments. We introduce two game theory-based pilot challenges that mirror
the complexities of real-world dynamic decision-making. These challenges are
well-defined, enabling clear, controllable, and precise evaluation of LLMs'
dynamic reasoning abilities. Through extensive experiments, we find that
existing reasoning methods tend to falter in dynamic settings that require
k-level thinking - a key concept not tackled by previous works. To address
this, we propose a novel reasoning approach for LLMs, named "K-Level
Reasoning". This approach adopts the perspective of rivals to recursively
employ k-level thinking based on available historical information, which
significantly improves the prediction accuracy of rivals' subsequent moves and
informs more strategic decision-making. This research not only sets a robust
quantitative benchmark for the assessment of dynamic reasoning but also
markedly enhances the proficiency of LLMs in dynamic contexts.
\\ ( https://arxiv.org/abs/2402.01521 ,  2013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01535
Date: Fri, 2 Feb 2024 16:26:52 GMT   (7589kb,D)

Title: An Empirical Analysis of Diversity in Argument Summarization
Authors: Michiel van der Meer, Piek Vossen, Catholijn M. Jonker, Pradeep K.
  Murukannaiah
Categories: cs.CL cs.AI
Comments: Accepted at EACL2024 (main proceedings)
\\
  Presenting high-level arguments is a crucial task for fostering participation
in online societal discussions. Current argument summarization approaches miss
an important facet of this task -- capturing diversity -- which is important
for accommodating multiple perspectives. We introduce three aspects of
diversity: those of opinions, annotators, and sources. We evaluate approaches
to a popular argument summarization task called Key Point Analysis, which shows
how these approaches struggle to (1) represent arguments shared by few people,
(2) deal with data from various sources, and (3) align with subjectivity in
human-provided annotations. We find that both general-purpose LLMs and
dedicated KPA models exhibit this behavior, but have complementary strengths.
Further, we observe that diversification of training data may ameliorate
generalization. Addressing diversity in argument summarization requires a mix
of strategies to deal with subjectivity.
\\ ( https://arxiv.org/abs/2402.01535 ,  7589kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01582
Date: Fri, 2 Feb 2024 17:20:16 GMT   (382kb)

Title: Automating Sound Change Prediction for Phylogenetic Inference: A
  Tukanoan Case Study
Authors: Kalvin Chang, Nathaniel R. Robinson, Anna Cai, Ting Chen, Annie Zhang,
  David R. Mortensen
Categories: cs.CL
Comments: Accepted to LChange 2023
\\
  We describe a set of new methods to partially automate linguistic
phylogenetic inference given (1) cognate sets with their respective protoforms
and sound laws, (2) a mapping from phones to their articulatory features and
(3) a typological database of sound changes. We train a neural network on these
sound change data to weight articulatory distances between phones and predict
intermediate sound change steps between historical protoforms and their modern
descendants, replacing a linguistic expert in part of a parsimony-based
phylogenetic inference algorithm. In our best experiments on Tukanoan
languages, this method produces trees with a Generalized Quartet Distance of
0.12 from a tree that used expert annotations, a significant improvement over
other semi-automated baselines. We discuss potential benefits and drawbacks to
our neural approach and parsimony-based tree prediction. We also experiment
with a minimal generalization learner for automatic sound law induction,
finding it comparably effective to sound laws from expert annotation. Our code
is publicly available at https://github.com/cmu-llab/aiscp.
\\ ( https://arxiv.org/abs/2402.01582 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01586
Date: Fri, 2 Feb 2024 17:26:23 GMT   (19156kb,D)

Title: TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent
  Constitution
Authors: Wenyue Hua, Xianjun Yang, Zelong Li, Cheng Wei, Yongfeng Zhang
Categories: cs.CL cs.AI cs.LG cs.MA
Comments: 16 pages, 3 figures, 5 tables, comments and suggestions are welcome
\\
  The emergence of LLM-based agents has garnered considerable attention, yet
their trustworthiness remains an under-explored area. As agents can directly
interact with the physical environment, their reliability and safety is
critical. This paper presents an Agent-Constitution-based agent framework,
TrustAgent, an initial investigation into improving the safety dimension of
trustworthiness in LLM-based agents. This framework consists of threefold
strategies: pre-planning strategy which injects safety knowledge to the model
prior to plan generation, in-planning strategy which bolsters safety during
plan generation, and post-planning strategy which ensures safety by
post-planning inspection. Through experimental analysis, we demonstrate how
these approaches can effectively elevate an LLM agent's safety by identifying
and preventing potential dangers. Furthermore, we explore the intricate
relationships between safety and helpfulness, and between the model's reasoning
ability and its efficacy as a safe agent. This paper underscores the imperative
of integrating safety awareness and trustworthiness into the design and
deployment of LLM-based agents, not only to enhance their performance but also
to ensure their responsible integration into human-centric environments. Data
and code are available at https://github.com/agiresearch/TrustAgent.
\\ ( https://arxiv.org/abs/2402.01586 ,  19156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01592
Date: Fri, 2 Feb 2024 17:35:49 GMT   (512kb,D)

Title: Towards Sustainable Workplace Mental Health: A Novel Approach to Early
  Intervention and Support
Authors: David W. Vinson, Mihael Arcan, David-Paul Niland, Fionn Delahunty
Categories: cs.CL
\\
  Employee well-being is a critical concern in the contemporary workplace, as
highlighted by the American Psychological Association's 2021 report, indicating
that 71% of employees experience stress or tension. This stress contributes
significantly to workplace attrition and absenteeism, with 61% of attrition and
16% of sick days attributed to poor mental health. A major challenge for
employers is that employees often remain unaware of their mental health issues
until they reach a crisis point, resulting in limited utilization of corporate
well-being benefits. This research addresses this challenge by presenting a
groundbreaking stress detection algorithm that provides real-time support
preemptively. Leveraging automated chatbot technology, the algorithm
objectively measures mental health levels by analyzing chat conversations,
offering personalized treatment suggestions in real-time based on linguistic
biomarkers. The study explores the feasibility of integrating these innovations
into practical learning applications within real-world contexts and introduces
a chatbot-style system integrated into the broader employee experience
platform. This platform, encompassing various features, aims to enhance overall
employee well-being, detect stress in real time, and proactively engage with
individuals to improve support effectiveness, demonstrating a 22% increase when
assistance is provided early. Overall, the study emphasizes the importance of
fostering a supportive workplace environment for employees' mental health.
\\ ( https://arxiv.org/abs/2402.01592 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01613
Date: Fri, 2 Feb 2024 18:23:18 GMT   (3769kb,D)

Title: Nomic Embed: Training a Reproducible Long Context Text Embedder
Authors: Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy
  Mulyar
Categories: cs.CL cs.AI
\\
  This technical report describes the training of nomic-embed-text-v1, the
first fully reproducible, open-source, open-weights, open-data, 8192 context
length English text embedding model that outperforms both OpenAI Ada-002 and
OpenAI text-embedding-3-small on short and long-context tasks. We release the
training code and model weights under an Apache 2 license. In contrast with
other open-source models, we release a training data loader with 235 million
curated text pairs that allows for the full replication of nomic-embed-text-v1.
You can find code and data to replicate the model at
https://github.com/nomic-ai/contrastors
\\ ( https://arxiv.org/abs/2402.01613 ,  3769kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01618
Date: Fri, 2 Feb 2024 18:31:15 GMT   (700kb,D)

Title: Style Vectors for Steering Generative Large Language Model
Authors: Kai Konen, Sophie Jentzsch, Diaoul\'e Diallo, Peer Sch\"utt, Oliver
  Bensch, Roxanne El Baff, Dominik Opitz, Tobias Hecking
Categories: cs.CL
Comments: Will be published as findings paper at EACL2024 - 18th Conference of
  the European Chapter of the Association for Computational Linguistics
\\
  This research explores strategies for steering the output of large language
models (LLMs) towards specific styles, such as sentiment, emotion, or writing
style, by adding style vectors to the activations of hidden layers during text
generation. We show that style vectors can be simply computed from recorded
layer activations for input texts in a specific style in contrast to more
complex training-based approaches. Through a series of experiments, we
demonstrate the effectiveness of activation engineering using such style
vectors to influence the style of generated text in a nuanced and
parameterisable way, distinguishing it from prompt engineering. The presented
research constitutes a significant step towards developing more adaptive and
effective AI-empowered interactive systems.
\\ ( https://arxiv.org/abs/2402.01618 ,  700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01619
Date: Fri, 2 Feb 2024 18:32:24 GMT   (426kb,D)

Title: KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce
  Programs over Low-resourced Knowledge Bases
Authors: Jiajie Zhang, Shulin Cao, Linmei Hu, Ling Feng, Lei Hou, Juanzi Li
Categories: cs.CL
\\
  Program induction (PI) has become a promising paradigm for using knowledge
bases (KBs) to help large language models (LLMs) answer complex
knowledge-intensive questions. Nonetheless, PI typically relies on a large
number of parallel question-program pairs to make the LLM aware of the schema
of the given KB, and is thus challenging for many low-resourced KBs that lack
annotated data. To this end, we propose KB-Plugin, a plug-and-play framework
that enables LLMs to induce programs over any low-resourced KB. Firstly,
KB-Plugin adopts self-supervised learning to encode the detailed schema
information of a given KB into a pluggable module, namely schema plugin.
Secondly, KB-Plugin utilizes abundant annotated data from a rich-resourced KB
to train another pluggable module, namely PI plugin, which can help the LLM
extract question-relevant schema information from the schema plugin of any KB
and utilize this information to induce programs over this KB. Experiments on
five heterogeneous KBQA datasets show that KB-Plugin achieves better or
comparable performance with 25$\times$ smaller backbone LLM compared to SoTA PI
methods for low-resourced KBs, and even approaches the performance of
supervised methods. Our code and data are available at
https://github.com/THU-KEG/KB-Plugin.
\\ ( https://arxiv.org/abs/2402.01619 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01620
Date: Fri, 2 Feb 2024 18:35:14 GMT   (1316kb,D)

Title: MAGDi: Structured Distillation of Multi-Agent Interaction Graphs
  Improves Reasoning in Smaller Language Models
Authors: Justin Chih-Yao Chen, Swarnadeep Saha, Elias Stengel-Eskin, Mohit
  Bansal
Categories: cs.CL
Comments: 15 pages; First two authors contributed equally; GitHub:
  https://github.com/dinobby/MAGDi
\\
  Multi-agent interactions between Large Language Model (LLM) agents have shown
major improvements on diverse reasoning tasks. However, these involve long
generations from multiple models across several rounds, making them expensive.
Moreover, these multi-agent approaches fail to provide a final, single model
for efficient inference. To address this, we introduce MAGDi, a new method for
structured distillation of the reasoning interactions between multiple LLMs
into smaller LMs. MAGDi teaches smaller models by representing multi-agent
interactions as graphs, augmenting a base student model with a graph encoder,
and distilling knowledge using three objective functions: next-token
prediction, a contrastive loss between correct and incorrect reasoning, and a
graph-based objective to model the interaction structure. Experiments on seven
widely-used commonsense and math reasoning benchmarks show that MAGDi improves
the reasoning capabilities of smaller models, outperforming several methods
that distill from a single teacher and multiple teachers. Moreover, MAGDi also
demonstrates an order of magnitude higher efficiency over its teachers. We
conduct extensive analyses to show that MAGDi (1) enhances the generalizability
to out-of-domain tasks, (2) scales positively with the size and strength of the
base student model, and (3) obtains larger improvements (via our multi-teacher
training) when applying self-consistency - an inference technique that relies
on model diversity.
\\ ( https://arxiv.org/abs/2402.01620 ,  1316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01622
Date: Fri, 2 Feb 2024 18:39:51 GMT   (2873kb,D)

Title: TravelPlanner: A Benchmark for Real-World Planning with Language Agents
Authors: Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong
  Tian, Yanghua Xiao, Yu Su
Categories: cs.CL
Comments: Work in progress
\\
  Planning has been part of the core pursuit for artificial intelligence since
its conception, but earlier AI agents mostly focused on constrained settings
because many of the cognitive substrates necessary for human-level planning
have been lacking. Recently, language agents powered by large language models
(LLMs) have shown interesting capabilities such as tool use and reasoning. Are
these language agents capable of planning in more complex settings that are out
of the reach of prior AI agents? To advance this investigation, we propose
TravelPlanner, a new planning benchmark that focuses on travel planning, a
common real-world planning scenario. It provides a rich sandbox environment,
various tools for accessing nearly four million data records, and 1,225
meticulously curated planning intents and reference plans. Comprehensive
evaluations show that the current language agents are not yet capable of
handling such complex planning tasks-even GPT-4 only achieves a success rate of
0.6%. Language agents struggle to stay on task, use the right tools to collect
information, or keep track of multiple constraints. However, we note that the
mere possibility for language agents to tackle such a complex problem is in
itself non-trivial progress. TravelPlanner provides a challenging yet
meaningful testbed for future language agents.
\\ ( https://arxiv.org/abs/2402.01622 ,  2873kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01629
Date: Fri, 2 Feb 2024 18:44:37 GMT   (60kb)

Title: Position Paper: Generalized grammar rules and structure-based
  generalization beyond classical equivariance for lexical tasks and
  transduction
Authors: Mircea Petrache, Shubhendu Trivedi
Categories: cs.CL cs.LG stat.ML
Comments: 12 pages
\\
  Compositional generalization is one of the main properties which
differentiates lexical learning in humans from state-of-art neural networks. We
propose a general framework for building models that can generalize
compositionally using the concept of Generalized Grammar Rules (GGRs), a class
of symmetry-based compositional constraints for transduction tasks, which we
view as a transduction analogue of equivariance constraints in physics-inspired
tasks. Besides formalizing generalized notions of symmetry for language
transduction, our framework is general enough to contain many existing works as
special cases. We present ideas on how GGRs might be implemented, and in the
process draw connections to reinforcement learning and other areas of research.
\\ ( https://arxiv.org/abs/2402.01629 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00893
Date: Wed, 31 Jan 2024 03:52:32 GMT   (616kb,D)

Title: MoDE: A Mixture-of-Experts Model with Mutual Distillation among the
  Experts
Authors: Zhitian Xie, Yinger Zhang, Chenyi Zhuang, Qitao Shi, Zhining Liu,
  Jinjie Gu, Guannan Zhang
Categories: cs.LG cs.AI
Comments: Accepted by AAAI-24
\\
  The application of mixture-of-experts (MoE) is gaining popularity due to its
ability to improve model's performance. In an MoE structure, the gate layer
plays a significant role in distinguishing and routing input features to
different experts. This enables each expert to specialize in processing their
corresponding sub-tasks. However, the gate's routing mechanism also gives rise
to narrow vision: the individual MoE's expert fails to use more samples in
learning the allocated sub-task, which in turn limits the MoE to further
improve its generalization ability. To effectively address this, we propose a
method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual
distillation among experts to enable each expert to pick up more features
learned by other experts and gain more accurate perceptions on their original
allocated sub-tasks. We conduct plenty experiments including tabular, NLP and
CV datasets, which shows MoDE's effectiveness, universality and robustness.
Furthermore, we develop a parallel study through innovatively constructing
"expert probing", to experimentally prove why MoDE works: moderate distilling
knowledge can improve each individual expert's test performances on their
assigned tasks, leading to MoE's overall performance improvement.
\\ ( https://arxiv.org/abs/2402.00893 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00899
Date: Wed, 31 Jan 2024 20:36:13 GMT   (528kb,D)

Title: Weakly Supervised Learners for Correction of AI Errors with Provable
  Performance Guarantees
Authors: Ivan Y. Tyukin, Tatiana Tyukina, Daniel van Helden, Zedong Zhang,
  Evgeny M. Mirkes, Oliver J. Sutton, Qinghua Zhou, Alexander N. Gorban,
  Penelope Allison
Categories: cs.LG cs.AI stat.ML
MSC-class: 68T05, 68T37
\\
  We present a new methodology for handling AI errors by introducing weakly
supervised AI error correctors with a priori performance guarantees. These AI
correctors are auxiliary maps whose role is to moderate the decisions of some
previously constructed underlying classifier by either approving or rejecting
its decisions. The rejection of a decision can be used as a signal to suggest
abstaining from making a decision. A key technical focus of the work is in
providing performance guarantees for these new AI correctors through bounds on
the probabilities of incorrect decisions. These bounds are distribution
agnostic and do not rely on assumptions on the data dimension. Our empirical
example illustrates how the framework can be applied to improve the performance
of an image classifier in a challenging real-world task where training data are
scarce.
\\ ( https://arxiv.org/abs/2402.00899 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00904
Date: Thu, 1 Feb 2024 02:44:32 GMT   (31kb)

Title: Graph Domain Adaptation: Challenges, Progress and Prospects
Authors: Boshen Shi, Yongqing Wang, Fangda Guo, Bingbing Xu, Huawei Shen, Xueqi
  Cheng
Categories: cs.LG cs.AI
\\
  As graph representation learning often suffers from label scarcity problems
in real-world applications, researchers have proposed graph domain adaptation
(GDA) as an effective knowledge-transfer paradigm across graphs. In particular,
to enhance model performance on target graphs with specific tasks, GDA
introduces a bunch of task-related graphs as source graphs and adapts the
knowledge learnt from source graphs to the target graphs. Since GDA combines
the advantages of graph representation learning and domain adaptation, it has
become a promising direction of transfer learning on graphs and has attracted
an increasing amount of research interest in recent years. In this paper, we
comprehensively overview the studies of GDA and present a detailed survey of
recent advances. Specifically, we outline the research status and challenges,
propose a taxonomy, introduce the details of representative works, and discuss
the prospects. To the best of our knowledge, this paper is the first survey for
graph domain adaptation. A detailed paper list is available at
https://github.com/Skyorca/Awesome-Graph-Domain-Adaptation-Papers.
\\ ( https://arxiv.org/abs/2402.00904 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00907
Date: Thu, 1 Feb 2024 03:47:05 GMT   (2370kb,D)

Title: AlphaRank: An Artificial Intelligence Approach for Ranking and Selection
  Problems
Authors: Ruihan Zhou, L. Jeff Hong and Yijie Peng
Categories: cs.LG stat.ME
\\
  We introduce AlphaRank, an artificial intelligence approach to address the
fixed-budget ranking and selection (R&S) problems. We formulate the sequential
sampling decision as a Markov decision process and propose a Monte Carlo
simulation-based rollout policy that utilizes classic R&S procedures as base
policies for efficiently learning the value function of stochastic dynamic
programming. We accelerate online sample-allocation by using deep reinforcement
learning to pre-train a neural network model offline based on a given prior. We
also propose a parallelizable computing framework for large-scale problems,
effectively combining "divide and conquer" and "recursion" for enhanced
scalability and efficiency. Numerical experiments demonstrate that the
performance of AlphaRank is significantly improved over the base policies,
which could be attributed to AlphaRank's superior capability on the trade-off
among mean, variance, and induced correlation overlooked by many existing
policies.
\\ ( https://arxiv.org/abs/2402.00907 ,  2370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00910
Date: Thu, 1 Feb 2024 09:24:36 GMT   (401kb,D)

Title: Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
Authors: Ahmed Radwan, Layan Zaafarani, Jetana Abudawood, Faisal AlZahrani,
  Fares Fourat
Categories: cs.LG cs.AI
\\
  Addressing biases in AI models is crucial for ensuring fair and accurate
predictions. However, obtaining large, unbiased datasets for training can be
challenging. This paper proposes a comprehensive approach using multiple
methods to remove bias in AI models, with only a small dataset and a
potentially biased pretrained model. We train multiple models with the
counter-bias of the pre-trained model through data splitting, local training,
and regularized fine-tuning, gaining potentially counter-biased models. Then,
we employ ensemble learning for all models to reach unbiased predictions. To
further accelerate the inference time of our ensemble model, we conclude our
solution with knowledge distillation that results in a single unbiased neural
network. We demonstrate the effectiveness of our approach through experiments
on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work
contributes to the ongoing effort to create more unbiased and reliable AI
models, even with limited data availability.
\\ ( https://arxiv.org/abs/2402.00910 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00912
Date: Thu, 1 Feb 2024 10:18:43 GMT   (40372kb,D)

Title: Can we Constrain Concept Bottleneck Models to Learn Semantically
  Meaningful Input Features?
Authors: Jack Furby, Daniel Cunnington, Dave Braines, Alun Preece
Categories: cs.LG cs.AI cs.CV
Comments: Main paper: 7 pages, 8 figures, Appendix: 15 pages, 22 figures. This
  paper is a preprint
\\
  Concept Bottleneck Models (CBMs) are considered inherently interpretable
because they first predict a set of human-defined concepts before using these
concepts to predict the output of a downstream task. For inherent
interpretability to be fully realised, and ensure trust in a model's output, we
need to guarantee concepts are predicted based on semantically mapped input
features. For example, one might expect the pixels representing a broken bone
in an image to be used for the prediction of a fracture. However, current
literature indicates this is not the case, as concept predictions are often
mapped to irrelevant input features. We hypothesise that this occurs when
concept annotations are inaccurate or how input features should relate to
concepts is unclear. In general, the effect of dataset labelling on concept
representations in CBMs remains an understudied area. Therefore, in this paper,
we examine how CBMs learn concepts from datasets with fine-grained concept
annotations. We demonstrate that CBMs can learn concept representations with
semantic mapping to input features by removing problematic concept
correlations, such as two concepts always appearing together. To support our
evaluation, we introduce a new synthetic image dataset based on a playing cards
domain, which we hope will serve as a benchmark for future CBM research. For
validation, we provide empirical evidence on a real-world dataset of chest
X-rays, to demonstrate semantically meaningful concepts can be learned in
real-world applications.
\\ ( https://arxiv.org/abs/2402.00912 ,  40372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00955
Date: Thu, 1 Feb 2024 19:24:45 GMT   (334kb,D)

Title: FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with
  Contrastive Learning in Multimodal Electronic Health Records
Authors: Yuqing Wang, Malvika Pillai, Yun Zhao, Catherine Curtin, Tina
  Hernandez-Boussard
Categories: cs.LG cs.CY
Comments: 16 pages, in submission
\\
  In the high-stakes realm of healthcare, ensuring fairness in predictive
models is crucial. Electronic Health Records (EHRs) have become integral to
medical decision-making, yet existing methods for enhancing model fairness
restrict themselves to unimodal data and fail to address the multifaceted
social biases intertwined with demographic factors in EHRs. To mitigate these
biases, we present FairEHR-CLP: a general framework for Fairness-aware Clinical
Predictions with Contrastive Learning in EHRs. FairEHR-CLP operates through a
two-stage process, utilizing patient demographics, longitudinal data, and
clinical notes. First, synthetic counterparts are generated for each patient,
allowing for diverse demographic identities while preserving essential health
information. Second, fairness-aware predictions employ contrastive learning to
align patient representations across sensitive attributes, jointly optimized
with an MLP classifier with a softmax layer for clinical classification tasks.
Acknowledging the unique challenges in EHRs, such as varying group sizes and
class imbalance, we introduce a novel fairness metric to effectively measure
error rate disparities across subgroups. Extensive experiments on three diverse
EHR datasets on three tasks demonstrate the effectiveness of FairEHR-CLP in
terms of fairness and utility compared with competitive baselines. FairEHR-CLP
represents an advancement towards ensuring both accuracy and equity in
predictive healthcare models.
\\ ( https://arxiv.org/abs/2402.00955 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00957
Date: Thu, 1 Feb 2024 19:25:58 GMT   (136kb,D)

Title: Credal Learning Theory
Authors: Michele Caprio, Maryam Sultana, Eleni Elia, Fabio Cuzzolin
Categories: cs.LG cs.AI stat.ML
Comments: 14 pages, 1 figure
\\
  Statistical learning theory is the foundation of machine learning, providing
theoretical bounds for the risk of models learnt from a (single) training set,
assumed to issue from an unknown probability distribution. In actual
deployment, however, the data distribution may (and often does) vary, causing
domain adaptation/generalization issues. In this paper we lay the foundations
for a `credal' theory of learning, using convex sets of probabilities (credal
sets) to model the variability in the data-generating distribution. Such credal
sets, we argue, may be inferred from a finite sample of training sets. Bounds
are derived for the case of finite hypotheses spaces (both assuming
realizability or not) as well as infinite model spaces, which directly
generalize classical results.
\\ ( https://arxiv.org/abs/2402.00957 ,  136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00965
Date: Thu, 1 Feb 2024 19:31:51 GMT   (2111kb,D)

Title: Multi-Modal Machine Learning Framework for Automated Seizure Detection
  in Laboratory Rats
Authors: Aaron Mullen, Samuel E. Armstrong, Jasmine Perdeh, Bjorn Bauer,
  Jeffrey Talbert, V.K. Cody Bumgardner
Categories: cs.LG cs.CV eess.SP
\\
  A multi-modal machine learning system uses multiple unique data sources and
types to improve its performance. This article proposes a system that combines
results from several types of models, all of which are trained on different
data signals. As an example to illustrate the efficacy of the system, an
experiment is described in which multiple types of data are collected from rats
suffering from seizures. This data includes electrocorticography readings,
piezoelectric motion sensor data, and video recordings. Separate models are
trained on each type of data, with the goal of classifying each time frame as
either containing a seizure or not. After each model has generated its
classification predictions, these results are combined. While each data signal
works adequately on its own for prediction purposes, the significant imbalance
in class labels leads to increased numbers of false positives, which can be
filtered and removed by utilizing all data sources. This paper will demonstrate
that, after postprocessing and combination techniques, classification accuracy
is improved with this multi-modal system when compared to the performance of
each individual data source.
\\ ( https://arxiv.org/abs/2402.00965 ,  2111kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00972
Date: Thu, 1 Feb 2024 19:41:04 GMT   (6332kb,D)

Title: Closure Discovery for Coarse-Grained Partial Differential Equations
  using Multi-Agent Reinforcement Learning
Authors: Jan-Philipp von Bassewitz, Sebastian Kaltenbach, Petros Koumoutsakos
Categories: cs.LG cs.MA physics.comp-ph
Comments: 18 pages, 12 figures
\\
  Reliable predictions of critical phenomena, such as weather, wildfires and
epidemics are often founded on models described by Partial Differential
Equations (PDEs). However, simulations that capture the full range of
spatio-temporal scales in such PDEs are often prohibitively expensive.
Consequently, coarse-grained simulations that employ heuristics and empirical
closure terms are frequently utilized as an alternative. We propose a novel and
systematic approach for identifying closures in under-resolved PDEs using
Multi-Agent Reinforcement Learning (MARL). The MARL formulation incorporates
inductive bias and exploits locality by deploying a central policy represented
efficiently by Convolutional Neural Networks (CNN). We demonstrate the
capabilities and limitations of MARL through numerical solutions of the
advection equation and the Burgers' equation. Our results show accurate
predictions for in- and out-of-distribution test cases as well as a significant
speedup compared to resolving all scales.
\\ ( https://arxiv.org/abs/2402.00972 ,  6332kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00976
Date: Thu, 1 Feb 2024 19:47:31 GMT   (142kb)

Title: Recurrent Transformers with Dynamic Halt
Authors: Jishnu Ray Chowdhury, Cornelia Caragea
Categories: cs.LG cs.AI cs.NE
\\
  In this paper, we study the inductive biases of two major approaches to
augmenting Transformers with a recurrent mechanism - (1) the approach of
incorporating a depth-wise recurrence similar to Universal Transformers; and
(2) the approach of incorporating a chunk-wise temporal recurrence like
Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways
to extend and combine the above methods - for example, we propose a global
mean-based dynamic halting mechanism for Universal Transformer and an
augmentation of Temporal Latent Bottleneck with elements from Universal
Transformer. We compare the models and probe their inductive biases in several
diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling,
ListOps, and Logical Inference.
\\ ( https://arxiv.org/abs/2402.00976 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00987
Date: Thu, 1 Feb 2024 20:05:04 GMT   (824kb,D)

Title: Self-Supervised Contrastive Pre-Training for Multivariate Point
  Processes
Authors: Xiao Shou, Dharmashankar Subramanian, Debarun Bhattacharjya, Tian Gao,
  Kristin P. Bennet
Categories: cs.LG
\\
  Self-supervision is one of the hallmarks of representation learning in the
increasingly popular suite of foundation models including large language models
such as BERT and GPT-3, but it has not been pursued in the context of
multivariate event streams, to the best of our knowledge. We introduce a new
paradigm for self-supervised learning for multivariate point processes using a
transformer encoder. Specifically, we design a novel pre-training strategy for
the encoder where we not only mask random event epochs but also insert randomly
sampled "void" epochs where an event does not occur; this differs from the
typical discrete-time pretext tasks such as word-masking in BERT but expands
the effectiveness of masking to better capture continuous-time dynamics. To
improve downstream tasks, we introduce a contrasting module that compares real
events to simulated void instances. The pre-trained model can subsequently be
fine-tuned on a potentially much smaller event dataset, similar conceptually to
the typical transfer of popular pre-trained language models. We demonstrate the
effectiveness of our proposed paradigm on the next-event prediction task using
synthetic datasets and 3 real applications, observing a relative performance
boost of as high as up to 20% compared to state-of-the-art models.
\\ ( https://arxiv.org/abs/2402.00987 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01032
Date: Thu, 1 Feb 2024 21:44:11 GMT   (2337kb,D)

Title: Repeat After Me: Transformers are Better than State Space Models at
  Copying
Authors: Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach
Categories: cs.LG cs.AI cs.CL
\\
  Transformers are the dominant architecture for sequence modeling, but there
is growing interest in models that use a fixed-size latent state that does not
depend on the sequence length, which we refer to as "generalized state space
models" (GSSMs). In this paper we show that while GSSMs are promising in terms
of inference-time efficiency, they are limited compared to transformer models
on tasks that require copying from the input context. We start with a
theoretical analysis of the simple task of string copying and prove that a two
layer transformer can copy strings of exponential length while GSSMs are
fundamentally limited by their fixed-size latent state. Empirically, we find
that transformers outperform GSSMs in terms of efficiency and generalization on
synthetic tasks that require copying the context. Finally, we evaluate
pretrained large language models and find that transformer models dramatically
outperform state space models at copying and retrieving information from
context. Taken together, these results suggest a fundamental gap between
transformers and GSSMs on tasks of practical interest.
\\ ( https://arxiv.org/abs/2402.01032 ,  2337kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01045
Date: Thu, 1 Feb 2024 22:24:58 GMT   (4185kb,D)

Title: LatticeGraphNet: A two-scale graph neural operator for simulating
  lattice structures
Authors: Ayush Jain, Ehsan Haghighat, Sai Nelaturi
Categories: cs.LG cs.CE
\\
  This study introduces a two-scale Graph Neural Operator (GNO), namely,
LatticeGraphNet (LGN), designed as a surrogate model for costly nonlinear
finite-element simulations of three-dimensional latticed parts and structures.
LGN has two networks: LGN-i, learning the reduced dynamics of lattices, and
LGN-ii, learning the mapping from the reduced representation onto the
tetrahedral mesh. LGN can predict deformation for arbitrary lattices, therefore
the name operator. Our approach significantly reduces inference time while
maintaining high accuracy for unseen simulations, establishing the use of GNOs
as efficient surrogate models for evaluating mechanical responses of lattices
and structures.
\\ ( https://arxiv.org/abs/2402.01045 ,  4185kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01047
Date: Thu, 1 Feb 2024 22:32:39 GMT   (155kb,D)

Title: Ultra Fast Transformers on FPGAs for Particle Physics Experiments
Authors: Zhixing Jiang, Dennis Yin, Elham E Khoda, Vladimir Loncar, Ekaterina
  Govorkova, Eric Moreno, Philip Harris, Scott Hauck, Shih-Chieh Hsu
Categories: cs.LG cs.AR hep-ex
Comments: 6 pages, 2 figures
Journal-ref: Machine Learning and the Physical Sciences Workshop, NeurIPS 2023
\\
  This work introduces a highly efficient implementation of the transformer
architecture on a Field-Programmable Gate Array (FPGA) by using the
\texttt{hls4ml} tool. Given the demonstrated effectiveness of transformer
models in addressing a wide range of problems, their application in
experimental triggers within particle physics becomes a subject of significant
interest. In this work, we have implemented critical components of a
transformer model, such as multi-head attention and softmax layers. To evaluate
the effectiveness of our implementation, we have focused on a particle physics
jet flavor tagging problem, employing a public dataset. We recorded latency
under 2 $\mu$s on the Xilinx UltraScale+ FPGA, which is compatible with
hardware trigger requirements at the CERN Large Hadron Collider experiments.
\\ ( https://arxiv.org/abs/2402.01047 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01055
Date: Thu, 1 Feb 2024 23:03:53 GMT   (64kb,D)

Title: Multiclass Learning from Noisy Labels for Non-decomposable Performance
  Measures
Authors: Mingyuan Zhang, Shivani Agarwal
Categories: cs.LG stat.ML
Comments: Accepted to AISTATS 2024
\\
  There has been much interest in recent years in learning good classifiers
from data with noisy labels. Most work on learning from noisy labels has
focused on standard loss-based performance measures. However, many machine
learning problems require using non-decomposable performance measures which
cannot be expressed as the expectation or sum of a loss on individual examples;
these include for example the H-mean, Q-mean and G-mean in class imbalance
settings, and the Micro $F_1$ in information retrieval. In this paper, we
design algorithms to learn from noisy labels for two broad classes of
multiclass non-decomposable performance measures, namely, monotonic convex and
ratio-of-linear, which encompass all the above examples. Our work builds on the
Frank-Wolfe and Bisection based methods of Narasimhan et al. (2015). In both
cases, we develop noise-corrected versions of the algorithms under the widely
studied family of class-conditional noise models. We provide regret (excess
risk) bounds for our algorithms, establishing that even though they are trained
on noisy data, they are Bayes consistent in the sense that their performance
converges to the optimal performance w.r.t. the clean (non-noisy) distribution.
Our experiments demonstrate the effectiveness of our algorithms in handling
label noise.
\\ ( https://arxiv.org/abs/2402.01055 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01057
Date: Thu, 1 Feb 2024 23:06:19 GMT   (2337kb,D)

Title: Expert Proximity as Surrogate Rewards for Single Demonstration Imitation
  Learning
Authors: Chia-Cheng Chiang, Li-Cheng Lan, Wei-Fang Sun, Chien Feng, Cho-Jui
  Hsieh, Chun-Yi Lee
Categories: cs.LG
\\
  In this paper, we focus on single-demonstration imitation learning (IL), a
practical approach for real-world applications where obtaining numerous expert
demonstrations is costly or infeasible. In contrast to typical IL settings with
multiple demonstrations, single-demonstration IL involves an agent having
access to only one expert trajectory. We highlight the issue of sparse reward
signals in this setting and propose to mitigate this issue through our proposed
Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed
to address reward sparsity by introducing a denser surrogate reward function
that considers environmental dynamics. This surrogate reward function
encourages the agent to navigate towards states that are proximal to expert
states. In practice, TDIL trains a transition discriminator to differentiate
between valid and non-valid transitions in a given environment to compute the
surrogate rewards. The experiments demonstrate that TDIL outperforms existing
IL approaches and achieves expert-level performance in the single-demonstration
IL setting across five widely adopted MuJoCo benchmarks as well as the "Adroit
Door" environment.
\\ ( https://arxiv.org/abs/2402.01057 ,  2337kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01058
Date: Thu, 1 Feb 2024 23:06:50 GMT   (54kb)

Title: Towards an Algebraic Framework For Approximating Functions Using Neural
  Network Polynomials
Authors: Shakil Rafi, Joshua Lee Padgett, and Ukash Nakarmi
Categories: cs.LG cs.NA cs.NE math.CO math.NA
Comments: 56 pages
MSC-class: 62M45
\\
  We make the case for neural network objects and extend an already existing
neural network calculus explained in detail in Chapter 2 on \cite{bigbook}. Our
aim will be to show that, yes, indeed, it makes sense to talk about neural
network polynomials, neural network exponentials, sine, and cosines in the
sense that they do indeed approximate their real number counterparts subject to
limitations on certain of their parameters, $q$, and $\varepsilon$. While doing
this, we show that the parameter and depth growth are only polynomial on their
desired accuracy (defined as a 1-norm difference over $\mathbb{R}$), thereby
showing that this approach to approximating, where a neural network in some
sense has the structural properties of the function it is approximating is not
entire intractable.
\\ ( https://arxiv.org/abs/2402.01058 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01070
Date: Fri, 2 Feb 2024 00:03:51 GMT   (4555kb,D)

Title: FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via
  Weight Shift Aggregation
Authors: Jungwon Seo, Chunming Rong, Minhoe Kim
Categories: cs.LG
\\
  Federated Learning (FL) offers a compelling method for training machine
learning models with a focus on preserving data privacy. The presence of system
heterogeneity and statistical heterogeneity, recognized challenges in FL,
arises from the diversity of client hardware, network, and dataset
distribution. This diversity can critically affect the training pace and the
performance of models. While many studies address either system or statistical
heterogeneity by introducing communication-efficient or stable convergence
algorithms, addressing these challenges in isolation often leads to compromises
due to unaddressed heterogeneity. In response, this paper introduces FedShift,
a novel algorithm designed to enhance both the training speed and the models'
accuracy in a dual heterogeneity scenario. Our solution can improve client
engagement through quantization and mitigate the adverse effects on performance
typically associated with quantization by employing a shifting technique. This
technique has proven to enhance accuracy by an average of 3.9% in diverse
heterogeneity environments.
\\ ( https://arxiv.org/abs/2402.01070 ,  4555kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01071
Date: Fri, 2 Feb 2024 00:16:45 GMT   (9662kb,D)

Title: Chameleon: Foundation Models for Fairness-aware Multi-modal Data
  Augmentation to Enhance Coverage of Minorities
Authors: Mahdi Erfanian and H. V. Jagadish and Abolfazl Asudeh
Categories: cs.LG cs.CY cs.DB
\\
  The potential harms of the under-representation of minorities in training
data, particularly in multi-modal settings, is a well-recognized concern. While
there has been extensive effort in detecting such under-representation,
resolution has remained a challenge. With recent advancements in generative AI,
large language models and foundation models have emerged as versatile tools
across various domains. In this paper, we propose Chameleon, a system that
efficiently utilizes these tools to augment a data set with a minimal addition
of synthetically generated tuples, in order to enhance the coverage of the
under-represented groups. Our system follows a rejection sampling approach to
ensure the generated tuples have a high quality and follow the underlying
distribution. In order to minimize the rejection chance of the generated
tuples, we propose multiple strategies for providing a guide for the foundation
model. Our experiment results, in addition to confirming the efficiency of our
proposed algorithms, illustrate the effectiveness of our approach, as the
unfairness of the model in a downstream task significantly dropped after data
repair using Chameleon.
\\ ( https://arxiv.org/abs/2402.01071 ,  9662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01076
Date: Fri, 2 Feb 2024 00:28:19 GMT   (3694kb,D)

Title: DoseGNN: Improving the Performance of Deep Learning Models in Adaptive
  Dose-Volume Histogram Prediction through Graph Neural Networks
Authors: Zehao Dong, Yixin Chen, Tianyu Zhao
Categories: cs.LG
\\
  Dose-Volume Histogram (DVH) prediction is fundamental in radiation therapy
that facilitate treatment planning, dose evaluation, plan comparison and etc.
It helps to increase the ability to deliver precise and effective radiation
treatments while managing potential toxicities to healthy tissues as needed to
reduce the risk of complications. This paper extends recently disclosed
research findings presented on AAPM (AAPM 65th Annual Meeting $\&$ Exhibition)
and includes necessary technique details. The objective is to design efficient
deep learning models for DVH prediction on general radiotherapy platform
equipped with high performance CBCT system, where input CT images and target
dose images to predict may have different origins, spacing and sizes. Deep
learning models widely-adopted in DVH prediction task are evaluated on the
novel radiotherapy platform, and graph neural networks (GNNs) are shown to be
the ideal architecture to construct a plug-and-play framework to improve
predictive performance of base deep learning models in the adaptive setting.
\\ ( https://arxiv.org/abs/2402.01076 ,  3694kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01077
Date: Fri, 2 Feb 2024 00:31:01 GMT   (139kb)

Title: Recent Advances in Predictive Modeling with Electronic Health Records
Authors: Jiaqi Wang, Junyu Luo, Muchao Ye, Xiaochen Wang, Yuan Zhong, Aofei
  Chang, Guanjie Huang, Ziyi Yin, Cao Xiao, Jimeng Sun, Fenglong Ma
Categories: cs.LG cs.AI
\\
  The development of electronic health records (EHR) systems has enabled the
collection of a vast amount of digitized patient data. However, utilizing EHR
data for predictive modeling presents several challenges due to its unique
characteristics. With the advancements in machine learning techniques, deep
learning has demonstrated its superiority in various applications, including
healthcare. This survey systematically reviews recent advances in deep
learning-based predictive models using EHR data. Specifically, we begin by
introducing the background of EHR data and providing a mathematical definition
of the predictive modeling task. We then categorize and summarize predictive
deep models from multiple perspectives. Furthermore, we present benchmarks and
toolkits relevant to predictive modeling in healthcare. Finally, we conclude
this survey by discussing open challenges and suggesting promising directions
for future research.
\\ ( https://arxiv.org/abs/2402.01077 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01093
Date: Fri, 2 Feb 2024 01:45:18 GMT   (137kb,D)

Title: Specialized Language Models with Cheap Inference from Limited Domain
  Data
Authors: David Grangier, Angelos Katharopoulos, Pierre Ablin, Awni Hannun
Categories: cs.LG cs.CL
\\
  Large language models have emerged as a versatile tool but are challenging to
apply to tasks lacking large inference budgets and large in-domain training
sets. This work formalizes these constraints and distinguishes four important
variables: the pretraining budget (for training before the target domain is
known), the specialization budget (for training after the target domain is
known), the inference budget, and the in-domain training set size. Across these
settings, we compare different approaches from the machine learning literature.
Limited by inference cost, we find better alternatives to the standard practice
of training very large vanilla transformer models. In particular, we show that
hyper-networks and mixture of experts have better perplexity for large
pretraining budgets, while small models trained on importance sampled datasets
are attractive for large specialization budgets.
\\ ( https://arxiv.org/abs/2402.01093 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01095
Date: Fri, 2 Feb 2024 01:58:16 GMT   (9741kb,D)

Title: How many views does your deep neural network use for prediction?
Authors: Keisuke Kawano and Takuro Kutsuna and Keisuke Sano
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: 20 pages
\\
  The generalization ability of Deep Neural Networks (DNNs) is still not fully
understood, despite numerous theoretical and empirical analyses. Recently,
Allen-Zhu & Li (2023) introduced the concept of multi-views to explain the
generalization ability of DNNs, but their main target is ensemble or distilled
models, and no method for estimating multi-views used in a prediction of a
specific input is discussed. In this paper, we propose Minimal Sufficient Views
(MSVs), which is similar to multi-views but can be efficiently computed for
real images. MSVs is a set of minimal and distinct features in an input, each
of which preserves a model's prediction for the input. We empirically show that
there is a clear relationship between the number of MSVs and prediction
accuracy across models, including convolutional and transformer models,
suggesting that a multi-view like perspective is also important for
understanding the generalization ability of (non-ensemble or non-distilled)
DNNs.
\\ ( https://arxiv.org/abs/2402.01095 ,  9741kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01096
Date: Fri, 2 Feb 2024 01:58:58 GMT   (329kb,D)

Title: Trustworthy Distributed AI Systems: Robustness, Privacy, and Governance
Authors: Wenqi Wei and Ling Liu
Categories: cs.LG cs.AI cs.CR cs.DC
Comments: Manuscript accepted to ACM Computing Surveys
\\
  Emerging Distributed AI systems are revolutionizing big data computing and
data processing capabilities with growing economic and societal impact.
However, recent studies have identified new attack surfaces and risks caused by
security, privacy, and fairness issues in AI systems. In this paper, we review
representative techniques, algorithms, and theoretical foundations for
trustworthy distributed AI through robustness guarantee, privacy protection,
and fairness awareness in distributed learning. We first provide a brief
overview of alternative architectures for distributed learning, discuss
inherent vulnerabilities for security, privacy, and fairness of AI algorithms
in distributed learning, and analyze why these problems are present in
distributed learning regardless of specific architectures. Then we provide a
unique taxonomy of countermeasures for trustworthy distributed AI, covering (1)
robustness to evasion attacks and irregular queries at inference, and
robustness to poisoning attacks, Byzantine attacks, and irregular data
distribution during training; (2) privacy protection during distributed
learning and model inference at deployment; and (3) AI fairness and governance
with respect to both data and models. We conclude with a discussion on open
challenges and future research directions toward trustworthy distributed AI,
such as the need for trustworthy AI policy guidelines, the AI
responsibility-utility co-design, and incentives and compliance.
\\ ( https://arxiv.org/abs/2402.01096 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01098
Date: Fri, 2 Feb 2024 02:21:06 GMT   (699kb,D)

Title: Bayesian Deep Learning for Remaining Useful Life Estimation via Stein
  Variational Gradient Descent
Authors: Luca Della Libera, Jacopo Andreoli, Davide Dalle Pezze, Mirco
  Ravanelli, Gian Antonio Susto
Categories: cs.LG stat.ML
Comments: 26 pages, 3 figures
\\
  A crucial task in predictive maintenance is estimating the remaining useful
life of physical systems. In the last decade, deep learning has improved
considerably upon traditional model-based and statistical approaches in terms
of predictive performance. However, in order to optimally plan maintenance
operations, it is also important to quantify the uncertainty inherent to the
predictions. This issue can be addressed by turning standard frequentist neural
networks into Bayesian neural networks, which are naturally capable of
providing confidence intervals around the estimates. Several methods exist for
training those models. Researchers have focused mostly on parametric
variational inference and sampling-based techniques, which notoriously suffer
from limited approximation power and large computational burden, respectively.
In this work, we use Stein variational gradient descent, a recently proposed
algorithm for approximating intractable distributions that overcomes the
drawbacks of the aforementioned techniques. In particular, we show through
experimental studies on simulated run-to-failure turbofan engine degradation
data that Bayesian deep learning models trained via Stein variational gradient
descent consistently outperform with respect to convergence speed and
predictive performance both the same models trained via parametric variational
inference and their frequentist counterparts trained via backpropagation.
Furthermore, we propose a method to enhance performance based on the
uncertainty information provided by the Bayesian models. We release the source
code at https://github.com/lucadellalib/bdl-rul-svgd.
\\ ( https://arxiv.org/abs/2402.01098 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01103
Date: Fri, 2 Feb 2024 02:40:51 GMT   (10353kb,D)

Title: Compositional Generative Modeling: A Single Model is Not All You Need
Authors: Yilun Du, Leslie Kaelbling
Categories: cs.LG cs.AI cs.CV cs.RO
\\
  Large monolithic generative models trained on massive amounts of data have
become an increasingly dominant approach in AI research. In this paper, we
argue that we should instead construct large generative systems by composing
smaller generative models together. We show how such a compositional generative
approach enables us to learn distributions in a more data-efficient manner,
enabling generalization to parts of the data distribution unseen at training
time. We further show how this enables us to program and construct new
generative models for tasks completely unseen at training. Finally, we show
that in many cases, we can discover separate compositional components from
data.
\\ ( https://arxiv.org/abs/2402.01103 ,  10353kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01105
Date: Fri, 2 Feb 2024 02:44:59 GMT   (771kb,D)

Title: A Survey for Foundation Models in Autonomous Driving
Authors: Haoxiang Gao and Yaqian Li and Kaiwen Long and Ming Yang and Yiqing
  Shen
Categories: cs.LG cs.CV cs.RO
\\
  The advent of foundation models has revolutionized the fields of natural
language processing and computer vision, paving the way for their application
in autonomous driving (AD). This survey presents a comprehensive review of more
than 40 research papers, demonstrating the role of foundation models in
enhancing AD. Large language models contribute to planning and simulation in
AD, particularly through their proficiency in reasoning, code generation and
translation. In parallel, vision foundation models are increasingly adapted for
critical tasks such as 3D object detection and tracking, as well as creating
realistic driving scenarios for simulation and testing. Multi-modal foundation
models, integrating diverse inputs, exhibit exceptional visual understanding
and spatial reasoning, crucial for end-to-end AD. This survey not only provides
a structured taxonomy, categorizing foundation models based on their modalities
and functionalities within the AD domain but also delves into the methods
employed in current research. It identifies the gaps between existing
foundation models and cutting-edge AD approaches, thereby charting future
research directions and proposing a roadmap for bridging these gaps.
\\ ( https://arxiv.org/abs/2402.01105 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01107
Date: Fri, 2 Feb 2024 02:48:03 GMT   (267kb,D)

Title: Simulation of Graph Algorithms with Looped Transformers
Authors: Artur Back de Luca and Kimon Fountoulakis
Categories: cs.LG cs.AI cs.DS
Comments: 45 pages, 2 figures
\\
  The execution of graph algorithms using neural networks has recently
attracted significant interest due to promising empirical progress. This
motivates further understanding of how neural networks can replicate reasoning
steps with relational data. In this work, we study the ability of transformer
networks to simulate algorithms on graphs from a theoretical perspective. The
architecture that we utilize is a looped transformer with extra attention heads
that interact with the graph. We prove by construction that this architecture
can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth-
and Depth-First Search, and Kosaraju's strongly connected components algorithm.
The width of the network does not increase with the size of the input graph,
which implies that the network can simulate the above algorithms for any graph.
Despite this property, we show that there is a limit to simulation in our
solution due to finite precision. Finally, we show a Turing Completeness result
with constant width when the extra attention heads are utilized.
\\ ( https://arxiv.org/abs/2402.01107 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01109
Date: Fri, 2 Feb 2024 02:56:50 GMT   (1005kb,D)

Title: Vaccine: Perturbation-aware Alignment for Large Language Model
Authors: Tiansheng Huang, Sihao Hu, Ling Liu
Categories: cs.LG cs.CL
\\
  The new paradigm of finetuning-as-a-service introduces a new attack surface
for Large Language Models (LLMs): a few harmful data uploaded by users can
easily trick the finetuning to produce an alignment-broken model. We conduct an
empirical analysis and uncover a \textit{harmful embedding drift} phenomenon,
showing a probable cause of the alignment-broken effect. Inspired by our
findings, we propose Vaccine, a perturbation-aware alignment technique to
mitigate the security risk of users finetuning. The core idea of Vaccine is to
produce invariant hidden embeddings by progressively adding crafted
perturbation to them in the alignment phase. This enables the embeddings to
withstand harmful perturbation from un-sanitized user data in the finetuning
phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)
demonstrate that Vaccine can boost the robustness of alignment against harmful
prompts induced embedding drift while reserving reasoning ability towards
benign prompts. Our code is available at
\url{https://github.com/git-disl/Vaccine}.
\\ ( https://arxiv.org/abs/2402.01109 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01111
Date: Fri, 2 Feb 2024 03:00:40 GMT   (64kb)

Title: Near-Optimal Reinforcement Learning with Self-Play under Adaptivity
  Constraints
Authors: Dan Qiao, Yu-Xiang Wang
Categories: cs.LG cs.AI cs.MA stat.ML
\\
  We study the problem of multi-agent reinforcement learning (MARL) with
adaptivity constraints -- a new problem motivated by real-world applications
where deployments of new policies are costly and the number of policy updates
must be minimized. For two-player zero-sum Markov Games, we design a (policy)
elimination based algorithm that achieves a regret of $\widetilde{O}(\sqrt{H^3
S^2 ABK})$, while the batch complexity is only $O(H+\log\log K)$. In the above,
$S$ denotes the number of states, $A,B$ are the number of actions for the two
players respectively, $H$ is the horizon and $K$ is the number of episodes.
Furthermore, we prove a batch complexity lower bound
$\Omega(\frac{H}{\log_{A}K}+\log\log K)$ for all algorithms with
$\widetilde{O}(\sqrt{K})$ regret bound, which matches our upper bound up to
logarithmic factors. As a byproduct, our techniques naturally extend to
learning bandit games and reward-free MARL within near optimal batch
complexity. To the best of our knowledge, these are the first line of results
towards understanding MARL with low adaptivity.
\\ ( https://arxiv.org/abs/2402.01111 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01114
Date: Fri, 2 Feb 2024 03:14:37 GMT   (1235kb,D)

Title: Double-Dip: Thwarting Label-Only Membership Inference Attacks with
  Transfer Learning and Randomization
Authors: Arezoo Rajabi, Reeya Pimple, Aiswarya Janardhanan, Surudhi Asokraj,
  Bhaskar Ramasubramanian, Radha Poovendran
Categories: cs.LG cs.AI cs.CR
\\
  Transfer learning (TL) has been demonstrated to improve DNN model performance
when faced with a scarcity of training samples. However, the suitability of TL
as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is
unexplored. A class of privacy attacks called membership inference attacks
(MIAs) aim to determine whether a given sample belongs to the training dataset
(member) or not (nonmember). We introduce Double-Dip, a systematic empirical
study investigating the use of TL (Stage-1) combined with randomization
(Stage-2) to thwart MIAs on overfitted DNNs without degrading classification
accuracy. Our study examines the roles of shared feature space and parameter
values between source and target models, number of frozen layers, and
complexity of pretrained models. We evaluate Double-Dip on three (Target,
Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii)
(CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a)
VGG-19, (b) ResNet-18, (c) Swin-T, and (d) FaceNet. Our experiments demonstrate
that Stage-1 reduces adversary success while also significantly increasing
classification accuracy of nonmembers against an adversary with either
white-box or black-box DNN model access, attempting to carry out SOTA
label-only MIAs. After Stage-2, success of an adversary carrying out a
label-only MIA is further reduced to near 50%, bringing it closer to a random
guess and showing the effectiveness of Double-Dip. Stage-2 of Double-Dip also
achieves lower ASR and higher classification accuracy than regularization and
differential privacy-based methods.
\\ ( https://arxiv.org/abs/2402.01114 ,  1235kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01140
Date: Fri, 2 Feb 2024 04:43:06 GMT   (3871kb,D)

Title: Root Cause Analysis In Microservice Using Neural Granger Causal
  Discovery
Authors: Cheng-Ming Lin, Ching Chang, Wei-Yao Wang, Kuang-Da Wang, Wen-Chih
  Peng
Categories: cs.LG cs.AI cs.DC
Comments: AAAI 2024 Main Track
\\
  In recent years, microservices have gained widespread adoption in IT
operations due to their scalability, maintenance, and flexibility. However, it
becomes challenging for site reliability engineers (SREs) to pinpoint the root
cause due to the complex relationships in microservices when facing system
malfunctions. Previous research employed structured learning methods (e.g.,
PC-algorithm) to establish causal relationships and derive root causes from
causal graphs. Nevertheless, they ignored the temporal order of time series
data and failed to leverage the rich information inherent in the temporal
relationships. For instance, in cases where there is a sudden spike in CPU
utilization, it can lead to an increase in latency for other microservices.
However, in this scenario, the anomaly in CPU utilization occurs before the
latency increase, rather than simultaneously. As a result, the PC-algorithm
fails to capture such characteristics. To address these challenges, we propose
RUN, a novel approach for root cause analysis using neural Granger causal
discovery with contrastive learning. RUN enhances the backbone encoder by
integrating contextual information from time series, and leverages a time
series forecasting model to conduct neural Granger causal discovery. In
addition, RUN incorporates Pagerank with a personalization vector to
efficiently recommend the top-k root causes. Extensive experiments conducted on
the synthetic and real-world microservice-based datasets demonstrate that RUN
noticeably outperforms the state-of-the-art root cause analysis methods.
Moreover, we provide an analysis scenario for the sock-shop case to showcase
the practicality and efficacy of RUN in microservice-based applications. Our
code is publicly available at https://github.com/zmlin1998/RUN.
\\ ( https://arxiv.org/abs/2402.01140 ,  3871kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01143
Date: Fri, 2 Feb 2024 04:52:52 GMT   (6164kb,D)

Title: Learning Network Representations with Disentangled Graph Auto-Encoder
Authors: Di Fan, Chuanhou Gao
Categories: cs.LG cs.AI stat.ML
Comments: 61 pages, 13 figures
\\
  The (variational) graph auto-encoder is extensively employed for learning
representations of graph-structured data. However, the formation of real-world
graphs is a complex and heterogeneous process influenced by latent factors.
Existing encoders are fundamentally holistic, neglecting the entanglement of
latent factors. This not only makes graph analysis tasks less effective but
also makes it harder to understand and explain the representations. Learning
disentangled graph representations with (variational) graph auto-encoder poses
significant challenges, and remains largely unexplored in the existing
literature. In this article, we introduce the Disentangled Graph Auto-Encoder
(DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that
leverage generative models to learn disentangled representations. Specifically,
we first design a disentangled graph convolutional network with multi-channel
message-passing layers, as the encoder aggregating information related to each
disentangled latent factor. Subsequently, a component-wise flow is applied to
each channel to enhance the expressive capabilities of disentangled variational
graph auto-encoder. Additionally, we design a factor-wise decoder, considering
the characteristics of disentangled representations. In order to further
enhance the independence among representations, we introduce independence
constraints on mapping channels for different latent factors. Empirical
experiments on both synthetic and real-world datasets show the superiority of
our proposed method compared to several state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.01143 ,  6164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01146
Date: Fri, 2 Feb 2024 05:21:50 GMT   (61kb,D)

Title: Limited Memory Online Gradient Descent for Kernelized Pairwise Learning
  with Dynamic Averaging
Authors: Hilal AlQuabeh, William de Vazelhes, Bin Gu
Categories: cs.LG
Comments: Accepted in AAAI 2024
\\
  Pairwise learning, an important domain within machine learning, addresses
loss functions defined on pairs of training examples, including those in metric
learning and AUC maximization. Acknowledging the quadratic growth in
computation complexity accompanying pairwise loss as the sample size grows,
researchers have turned to online gradient descent (OGD) methods for enhanced
scalability. Recently, an OGD algorithm emerged, employing gradient computation
involving prior and most recent examples, a step that effectively reduces
algorithmic complexity to $O(T)$, with $T$ being the number of received
examples. This approach, however, confines itself to linear models while
assuming the independence of example arrivals. We introduce a lightweight OGD
algorithm that does not require the independence of examples and generalizes to
kernel pairwise learning. Our algorithm builds the gradient based on a random
example and a moving average representing the past data, which results in a
sub-linear regret bound with a complexity of $O(T)$. Furthermore, through the
integration of $O(\sqrt{T}{\log{T}})$ random Fourier features, the complexity
of kernel calculations is effectively minimized. Several experiments with
real-world datasets show that the proposed technique outperforms kernel and
linear algorithms in offline and online scenarios.
\\ ( https://arxiv.org/abs/2402.01146 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01147
Date: Fri, 2 Feb 2024 05:22:41 GMT   (1490kb,D)

Title: Efficient Reinforcement Learning for Routing Jobs in Heterogeneous
  Queueing Systems
Authors: Neharika Jali, Guannan Qu, Weina Wang, Gauri Joshi
Categories: cs.LG cs.PF
Comments: Accepted to AISTATS 2024
\\
  We consider the problem of efficiently routing jobs that arrive into a
central queue to a system of heterogeneous servers. Unlike homogeneous systems,
a threshold policy, that routes jobs to the slow server(s) when the queue
length exceeds a certain threshold, is known to be optimal for the
one-fast-one-slow two-server system. But an optimal policy for the multi-server
system is unknown and non-trivial to find. While Reinforcement Learning (RL)
has been recognized to have great potential for learning policies in such
cases, our problem has an exponentially large state space size, rendering
standard RL inefficient. In this work, we propose ACHQ, an efficient policy
gradient based algorithm with a low dimensional soft threshold policy
parameterization that leverages the underlying queueing structure. We provide
stationary-point convergence guarantees for the general case and despite the
low-dimensional parameterization prove that ACHQ converges to an approximate
global optimum for the special case of two servers. Simulations demonstrate an
improvement in expected response time of up to ~30% over the greedy policy that
routes to the fastest available server.
\\ ( https://arxiv.org/abs/2402.01147 ,  1490kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01160
Date: Fri, 2 Feb 2024 05:59:48 GMT   (76kb,D)

Title: Truncated Non-Uniform Quantization for Distributed SGD
Authors: Guangfeng Yan, Tan Li, Yuanzhang Xiao, Congduan Li and Linqi Song
Categories: cs.LG cs.DC
\\
  To address the communication bottleneck challenge in distributed learning,
our work introduces a novel two-stage quantization strategy designed to enhance
the communication efficiency of distributed Stochastic Gradient Descent (SGD).
The proposed method initially employs truncation to mitigate the impact of
long-tail noise, followed by a non-uniform quantization of the post-truncation
gradients based on their statistical characteristics. We provide a
comprehensive convergence analysis of the quantized distributed SGD,
establishing theoretical guarantees for its performance. Furthermore, by
minimizing the convergence error, we derive optimal closed-form solutions for
the truncation threshold and non-uniform quantization levels under given
communication constraints. Both theoretical insights and extensive experimental
evaluations demonstrate that our proposed algorithm outperforms existing
quantization schemes, striking a superior balance between communication
efficiency and convergence performance.
\\ ( https://arxiv.org/abs/2402.01160 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01195
Date: Fri, 2 Feb 2024 07:44:26 GMT   (10752kb,D)

Title: Conditional Normalizing Flows for Active Learning of Coarse-Grained
  Molecular Representations
Authors: Henrik Schopmans, Pascal Friederich
Categories: cs.LG cs.AI physics.chem-ph
ACM-class: I.2.0
\\
  Efficient sampling of the Boltzmann distribution of molecular systems is a
long-standing challenge. Recently, instead of generating long molecular
dynamics simulations, generative machine learning methods such as normalizing
flows have been used to learn the Boltzmann distribution directly, without
samples. However, this approach is susceptible to mode collapse and thus often
does not explore the full configurational space. In this work, we address this
challenge by separating the problem into two levels, the fine-grained and
coarse-grained degrees of freedom. A normalizing flow conditioned on the
coarse-grained space yields a probabilistic connection between the two levels.
To explore the configurational space, we employ coarse-grained simulations with
active learning which allows us to update the flow and make all-atom potential
energy evaluations only when necessary. Using alanine dipeptide as an example,
we show that our methods obtain a speedup to molecular dynamics simulations of
approximately 15.9 to 216.2 compared to the speedup of 4.5 of the current
state-of-the-art machine learning approach.
\\ ( https://arxiv.org/abs/2402.01195 ,  10752kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01201
Date: Fri, 2 Feb 2024 08:05:35 GMT   (1760kb,D)

Title: Few-Shot Class-Incremental Learning with Prior Knowledge
Authors: Wenhao Jiang, Duo Li, Menghan Hu, Guangtao Zhai, Xiaokang Yang,
  Xiao-Ping Zhang
Categories: cs.LG cs.AI
\\
  To tackle the issues of catastrophic forgetting and overfitting in few-shot
class-incremental learning (FSCIL), previous work has primarily concentrated on
preserving the memory of old knowledge during the incremental phase. The role
of pre-trained model in shaping the effectiveness of incremental learning is
frequently underestimated in these studies. Therefore, to enhance the
generalization ability of the pre-trained model, we propose Learning with Prior
Knowledge (LwPK) by introducing nearly free prior knowledge from a few
unlabeled data of subsequent incremental classes. We cluster unlabeled
incremental class samples to produce pseudo-labels, then jointly train these
with labeled base class samples, effectively allocating embedding space for
both old and new class data. Experimental results indicate that LwPK
effectively enhances the model resilience against catastrophic forgetting, with
theoretical analysis based on empirical risk minimization and class distance
measurement corroborating its operational principles. The source code of LwPK
is publicly available at: \url{https://github.com/StevenJ308/LwPK}.
\\ ( https://arxiv.org/abs/2402.01201 ,  1760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01203
Date: Fri, 2 Feb 2024 08:13:18 GMT   (17719kb,D)

Title: Structured World Modeling via Semantic Vector Quantization
Authors: Yi-Fu Wu, Minseung Lee, Sungjin Ahn
Categories: cs.LG cs.CV
Comments: Accepted in ICLR 2024
\\
  Neural discrete representations are crucial components of modern neural
networks. However, their main limitation is that the primary strategies such as
VQ-VAE can only provide representations at the patch level. Therefore, one of
the main goals of representation learning, acquiring structured, semantic, and
compositional abstractions such as the color and shape of an object, remains
elusive. In this paper, we present the first approach to semantic neural
discrete representation learning. The proposed model, called Semantic
Vector-Quantized Variational Autoencoder (SVQ), leverages recent advances in
unsupervised object-centric learning to address this limitation. Specifically,
we observe that a simple approach quantizing at the object level poses a
significant challenge and propose constructing scene representations
hierarchically, from low-level discrete concept schemas to object
representations. Additionally, we suggest a novel method for structured
semantic world modeling by training a prior over these representations,
enabling the ability to generate images by sampling the semantic properties of
the objects in the scene. In experiments on various 2D and 3D object-centric
datasets, we find that our model achieves superior generation performance
compared to non-semantic vector quantization methods such as VQ-VAE and
previous object-centric generative models. Furthermore, we find that the
semantic discrete representations can solve downstream scene understanding
tasks that require reasoning about the properties of different objects in the
scene.
\\ ( https://arxiv.org/abs/2402.01203 ,  17719kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01204
Date: Fri, 2 Feb 2024 08:17:41 GMT   (295kb,D)

Title: A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
Authors: Wei-Yao Wang, Wei-Wei Du, Derek Xu, Wei Wang, Wen-Chih Peng
Categories: cs.LG cs.AI
Comments: The paper list can be found at
  https://github.com/wwweiwei/awesome-self-supervised-learning-for-tabular-data
\\
  Self-supervised learning (SSL) has been incorporated into many
state-of-the-art models in various domains, where SSL defines pretext tasks
based on unlabeled datasets to learn contextualized and robust representations.
Recently, SSL has been a new trend in exploring the representation learning
capability in the realm of tabular data, which is more challenging due to not
having explicit relations for learning descriptive representations. This survey
aims to systematically review and summarize the recent progress and challenges
of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal
definition of NS-TD and clarify its correlation to related studies. Then, these
approaches are categorized into three groups -- predictive learning,
contrastive learning, and hybrid learning, with their motivations and strengths
of representative methods within each direction. On top of this, application
issues of SSL4NS-TD are presented, including automatic data engineering,
cross-table transferability, and domain knowledge integration. In addition, we
elaborate on existing benchmarks and datasets for NS-TD applications to discuss
the performance of existing tabular models. Finally, we discuss the challenges
of SSL4NS-TD and provide potential directions for future research. We expect
our work to be useful in terms of encouraging more research on lowering the
barrier to entry SSL for the tabular domain and improving the foundations for
implicit tabular data.
\\ ( https://arxiv.org/abs/2402.01204 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01206
Date: Fri, 2 Feb 2024 08:25:28 GMT   (1117kb,D)

Title: Comparative Evaluation of Weather Forecasting using Machine Learning
  Models
Authors: Md Saydur Rahman, Farhana Akter Tumpa, Md Shazid Islam, Abul Al Arabi,
  Md Sanzid Bin Hossain, Md Saad Ul Haque
Categories: cs.LG
\\
  Gaining a deeper understanding of weather and being able to predict its
future conduct have always been considered important endeavors for the growth
of our society. This research paper explores the advancements in understanding
and predicting nature's behavior, particularly in the context of weather
forecasting, through the application of machine learning algorithms. By
leveraging the power of machine learning, data mining, and data analysis
techniques, significant progress has been made in this field. This study
focuses on analyzing the contributions of various machine learning algorithms
in predicting precipitation and temperature patterns using a 20-year dataset
from a single weather station in Dhaka city. Algorithms such as Gradient
Boosting, AdaBoosting, Artificial Neural Network, Stacking Random Forest,
Stacking Neural Network, and Stacking KNN are evaluated and compared based on
their performance metrics, including Confusion matrix measurements. The
findings highlight remarkable achievements and provide valuable insights into
their performances and features correlation.
\\ ( https://arxiv.org/abs/2402.01206 ,  1117kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01207
Date: Fri, 2 Feb 2024 08:25:32 GMT   (169kb,D)

Title: Efficient Causal Graph Discovery Using Large Language Models
Authors: Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, Yoshua
  Bengio
Categories: cs.LG cs.AI stat.ME
\\
  We propose a novel framework that leverages LLMs for full causal graph
discovery. While previous LLM-based methods have used a pairwise query
approach, this requires a quadratic number of queries which quickly becomes
impractical for larger causal graphs. In contrast, the proposed framework uses
a breadth-first search (BFS) approach which allows it to use only a linear
number of queries. We also show that the proposed method can easily incorporate
observational data when available, to improve performance. In addition to being
more time and data-efficient, the proposed framework achieves state-of-the-art
results on real-world causal graphs of varying sizes. The results demonstrate
the effectiveness and efficiency of the proposed method in discovering causal
relationships, showcasing its potential for broad applicability in causal graph
discovery tasks across different domains.
\\ ( https://arxiv.org/abs/2402.01207 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01208
Date: Fri, 2 Feb 2024 08:26:42 GMT   (832kb,D)

Title: Location Agnostic Adaptive Rain Precipitation Prediction using Deep
  Learning
Authors: Md Shazid Islam, Md Saydur Rahman, Md Saad Ul Haque, Farhana Akter
  Tumpa, Md Sanzid Bin Hossain, Abul Al Arabi
Categories: cs.LG cs.AI
\\
  Rain precipitation prediction is a challenging task as it depends on weather
and meteorological features which vary from location to location. As a result,
a prediction model that performs well at one location does not perform well at
other locations due to the distribution shifts. In addition, due to global
warming, the weather patterns are changing very rapidly year by year which
creates the possibility of ineffectiveness of those models even at the same
location as time passes. In our work, we have proposed an adaptive deep
learning-based framework in order to provide a solution to the aforementioned
challenges. Our method can generalize the model for the prediction of
precipitation for any location where the methods without adaptation fail. Our
method has shown 43.51%, 5.09%, and 38.62% improvement after adaptation using a
deep neural network for predicting the precipitation of Paris, Los Angeles, and
Tokyo, respectively.
\\ ( https://arxiv.org/abs/2402.01208 ,  832kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01226
Date: Fri, 2 Feb 2024 08:45:38 GMT   (2575kb,D)

Title: HW-SW Optimization of DNNs for Privacy-preserving People Counting on
  Low-resolution Infrared Arrays
Authors: Matteo Risso, Chen Xie, Francesco Daghero, Alessio Burrello,
  Seyedmorteza Mollaei, Marco Castellano, Enrico Macii, Massimo Poncino,
  Daniele Jahier Pagliari
Categories: cs.LG cs.AR
Comments: This paper has been accepted for publication in the DATE 2024
  conference IEEE
\\
  Low-resolution infrared (IR) array sensors enable people counting
applications such as monitoring the occupancy of spaces and people flows while
preserving privacy and minimizing energy consumption. Deep Neural Networks
(DNNs) have been shown to be well-suited to process these sensor data in an
accurate and efficient manner. Nevertheless, the space of DNNs' architectures
is huge and its manual exploration is burdensome and often leads to sub-optimal
solutions. To overcome this problem, in this work, we propose a highly
automated full-stack optimization flow for DNNs that goes from neural
architecture search, mixed-precision quantization, and post-processing, down to
the realization of a new smart sensor prototype, including a Microcontroller
with a customized instruction set. Integrating these cross-layer optimizations,
we obtain a large set of Pareto-optimal solutions in the 3D-space of energy,
memory, and accuracy. Deploying such solutions on our hardware platform, we
improve the state-of-the-art achieving up to 4.2x model size reduction, 23.8x
code size reduction, and 15.38x energy reduction at iso-accuracy.
\\ ( https://arxiv.org/abs/2402.01226 ,  2575kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01231
Date: Fri, 2 Feb 2024 08:55:23 GMT   (2320kb,D)

Title: Unveiling Delay Effects in Traffic Forecasting: A Perspective from
  Spatial-Temporal Delay Differential Equations
Authors: Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang,
  Yuanchun Zhou
Categories: cs.LG
Comments: 11 pages, 7 figures
\\
  Traffic flow forecasting is a fundamental research issue for transportation
planning and management, which serves as a canonical and typical example of
spatial-temporal predictions. In recent years, Graph Neural Networks (GNNs) and
Recurrent Neural Networks (RNNs) have achieved great success in capturing
spatial-temporal correlations for traffic flow forecasting. Yet, two
non-ignorable issues haven't been well solved: 1) The message passing in GNNs
is immediate, while in reality the spatial message interactions among
neighboring nodes can be delayed. The change of traffic flow at one node will
take several minutes, i.e., time delay, to influence its connected neighbors.
2) Traffic conditions undergo continuous changes. The prediction frequency for
traffic flow forecasting may vary based on specific scenario requirements. Most
existing discretized models require retraining for each prediction horizon,
restricting their applicability. To tackle the above issues, we propose a
neural Spatial-Temporal Delay Differential Equation model, namely STDDE. It
includes both delay effects and continuity into a unified delay differential
equation framework, which explicitly models the time delay in spatial
information propagation. Furthermore, theoretical proofs are provided to show
its stability. Then we design a learnable traffic-graph time-delay estimator,
which utilizes the continuity of the hidden states to achieve the gradient
backward process. Finally, we propose a continuous output module, allowing us
to accurately predict traffic flow at various frequencies, which provides more
flexibility and adaptability to different scenarios. Extensive experiments show
the superiority of the proposed STDDE along with competitive computational
efficiency.
\\ ( https://arxiv.org/abs/2402.01231 ,  2320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01238
Date: Fri, 2 Feb 2024 09:03:38 GMT   (967kb,D)

Title: Flexible Variational Information Bottleneck: Achieving Diverse
  Compression with a Single Training
Authors: Sota Kudo, Naoaki Ono, Shigehiko Kanaya, Ming Huang
Categories: cs.LG cs.AI cs.IT math.IT
\\
  Information Bottleneck (IB) is a widely used framework that enables the
extraction of information related to a target random variable from a source
random variable. In the objective function, IB controls the trade-off between
data compression and predictiveness through the Lagrange multiplier $\beta$.
Traditionally, to find the trade-off to be learned, IB requires a search for
$\beta$ through multiple training cycles, which is computationally expensive.
In this study, we introduce Flexible Variational Information Bottleneck (FVIB),
an innovative framework for classification task that can obtain optimal models
for all values of $\beta$ with single, computationally efficient training. We
theoretically demonstrate that across all values of reasonable $\beta$, FVIB
can simultaneously maximize an approximation of the objective function for
Variational Information Bottleneck (VIB), the conventional IB method. Then we
empirically show that FVIB can learn the VIB objective as effectively as VIB.
Furthermore, in terms of calibration performance, FVIB outperforms other IB and
calibration methods by enabling continuous optimization of $\beta$. Our codes
are available at https://github.com/sotakudo/fvib.
\\ ( https://arxiv.org/abs/2402.01238 ,  967kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01242
Date: Fri, 2 Feb 2024 09:10:35 GMT   (9888kb,D)

Title: Two Heads Are Better Than One: Boosting Graph Sparse Training via
  Semantic and Topological Awareness
Authors: Guibin Zhang, Yanwei Yue, Kun Wang, Junfeng Fang, Yongduo Sui, Kai
  Wang, Yuxuan Liang, Dawei Cheng, Shirui Pan, Tianlong Chen
Categories: cs.LG
\\
  Graph Neural Networks (GNNs) excel in various graph learning tasks but face
computational challenges when applied to large-scale graphs. A promising
solution is to remove non-essential edges to reduce the computational overheads
in GNN. Previous literature generally falls into two categories:
topology-guided and semantic-guided. The former maintains certain graph
topological properties yet often underperforms on GNNs due to low integration
with neural network training. The latter performs well at lower sparsity on
GNNs but faces performance collapse at higher sparsity levels. With this in
mind, we take the first step to propose a new research line and concept termed
Graph Sparse Training (GST), which dynamically manipulates sparsity at the data
level. Specifically, GST initially constructs a topology & semantic anchor at a
low training cost, followed by performing dynamic sparse training to align the
sparse graph with the anchor. We introduce the Equilibria Sparsification
Principle to guide this process, effectively balancing the preservation of both
topological and semantic information. Ultimately, GST produces a sparse graph
with maximum topological integrity and no performance degradation. Extensive
experiments on 6 datasets and 5 backbones showcase that GST (I) identifies
subgraphs at higher graph sparsity levels (1.67%~15.85% $\uparrow$) than
state-of-the-art sparsification methods, (II) preserves more key spectral
properties, (III) achieves 1.27-3.42$\times$ speedup in GNN inference and (IV)
successfully helps graph adversarial defense and graph lottery tickets.
\\ ( https://arxiv.org/abs/2402.01242 ,  9888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01252
Date: Fri, 2 Feb 2024 09:19:45 GMT   (5665kb,D)

Title: Target inductive methods for zero-shot regression
Authors: Miriam Fdez-D\'iaz, Jos\'e Ram\'on Quevedo, Elena Monta\~n\'es
Categories: cs.LG
Journal-ref: Information Sciences ISSN: 0020-0255 2022 Volumen: 599 P\'aginas:
  44-63
DOI: 10.1016/j.ins.2022.03.075
\\
  This research arises from the need to predict the amount of air pollutants in
meteorological stations. Air pollution depends on the location of the stations
(weather conditions and activities in the surroundings). Frequently, the
surrounding information is not considered in the learning process. This
information is known beforehand in the absence of unobserved weather conditions
and remains constant for the same station. Considering the surrounding
information as side information facilitates the generalization for predicting
pollutants in new stations, leading to a zero-shot regression scenario.
Available methods in zero-shot typically lean towards classification, and are
not easily extensible to regression. This paper proposes two zero-shot methods
for regression. The first method is a similarity based approach that learns
models from features and aggregates them using side information. However,
potential knowledge of the feature models may be lost in the aggregation. The
second method overcomes this drawback by replacing the aggregation procedure
and learning the correspondence between side information and feature-induced
models, instead. Both proposals are compared with a baseline procedure using
artificial datasets, UCI repository communities and crime datasets, and the
pollutants. Both approaches outperform the baseline method, but the parameter
learning approach manifests its superiority over the similarity based method.
\\ ( https://arxiv.org/abs/2402.01252 ,  5665kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01261
Date: Fri, 2 Feb 2024 09:32:03 GMT   (10716kb,D)

Title: TEDDY: Trimming Edges with Degree-based Discrimination strategY
Authors: Hyunjin Seo, Jihun Yun, Eunho Yang
Categories: cs.LG cs.AI
\\
  Since the pioneering work on the lottery ticket hypothesis for graph neural
networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph
lottery tickets (GLT) has become one of the pivotal focus in the GNN community,
inspiring researchers to discover sparser GLT while achieving comparable
performance to original dense networks. In parallel, the graph structure has
gained substantial attention as a crucial factor in GNN training dynamics, also
elucidated by several recent studies. Despite this, contemporary studies on
GLT, in general, have not fully exploited inherent pathways in the graph
structure and identified tickets in an iterative manner, which is
time-consuming and inefficient. To address these limitations, we introduce
TEDDY, a one-shot edge sparsification framework that leverages structural
information by incorporating edge-degree information. Following edge
sparsification, we encourage the parameter sparsity during training via simple
projected gradient descent on the $\ell_0$ ball. Given the target sparsity
levels for both the graph structure and the model parameters, our TEDDY
facilitates efficient and rapid realization of GLT within a single training.
Remarkably, our experimental results demonstrate that TEDDY significantly
surpasses conventional iterative approaches in generalization, even when
conducting one-shot sparsification that solely utilizes graph structures,
without taking node features into account.
\\ ( https://arxiv.org/abs/2402.01261 ,  10716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01262
Date: Fri, 2 Feb 2024 09:33:07 GMT   (515kb,D)

Title: Cascaded Scaling Classifier: class incremental learning with probability
  scaling
Authors: Jary Pomponi, Alessio Devoto, Simone Scardapane
Categories: cs.LG cs.CV
\\
  Humans are capable of acquiring new knowledge and transferring learned
knowledge into different domains, incurring a small forgetting. The same
ability, called Continual Learning, is challenging to achieve when operating
with neural networks due to the forgetting affecting past learned tasks when
learning new ones. This forgetting can be mitigated by replaying stored samples
from past tasks, but a large memory size may be needed for long sequences of
tasks; moreover, this could lead to overfitting on saved samples. In this
paper, we propose a novel regularisation approach and a novel incremental
classifier called, respectively, Margin Dampening and Cascaded Scaling
Classifier. The first combines a soft constraint and a knowledge distillation
approach to preserve past learned knowledge while allowing the model to learn
new patterns effectively. The latter is a gated incremental classifier, helping
the model modify past predictions without directly interfering with them. This
is achieved by modifying the output of the model with auxiliary scaling
functions. We empirically show that our approach performs well on multiple
benchmarks against well-established baselines, and we also study each component
of our proposal and how the combinations of such components affect the final
results.
\\ ( https://arxiv.org/abs/2402.01262 ,  515kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01263
Date: Fri, 2 Feb 2024 09:34:49 GMT   (1559kb,D)

Title: A Differentiable POGLM with Forward-Backward Message Passing
Authors: Chengrui Li, Weihan Li, Yule Wang, and Anqi Wu
Categories: cs.LG q-bio.NC
\\
  The partially observable generalized linear model (POGLM) is a powerful tool
for understanding neural connectivity under the assumption of existing hidden
neurons. With spike trains only recorded from visible neurons, existing works
use variational inference to learn POGLM meanwhile presenting the difficulty of
learning this latent variable model. There are two main issues: (1) the sampled
Poisson hidden spike count hinders the use of the pathwise gradient estimator
in VI; and (2) the existing design of the variational model is neither
expressive nor time-efficient, which further affects the performance. For (1),
we propose a new differentiable POGLM, which enables the pathwise gradient
estimator, better than the score function gradient estimator used in existing
works. For (2), we propose the forward-backward message-passing sampling scheme
for the variational model. Comprehensive experiments show that our
differentiable POGLMs with our forward-backward message passing produce a
better performance on one synthetic and two real-world datasets. Furthermore,
our new method yields more interpretable parameters, underscoring its
significance in neuroscience.
\\ ( https://arxiv.org/abs/2402.01263 ,  1559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01264
Date: Fri, 2 Feb 2024 09:36:06 GMT   (2873kb,D)

Title: Direct side information learning for zero-shot regression
Authors: Miriam Fdez-D\'iaz, Elena Monta\~n\'es, Jos\'e Ram\'on Quevedo
Categories: cs.LG
Journal-ref: Neurocomputing 2023 Volumen 561 126873
DOI: 10.1016/j.neucom.2023.126873
\\
  Zero-shot learning provides models for targets for which instances are not
available, commonly called unobserved targets. The availability of target side
information becomes crucial in this context in order to properly induce models
for these targets. The literature is plenty of strategies to cope with this
scenario, but specifically designed on the basis of a zero-shot classification
scenario, mostly in computer vision and image classification, but they are
either not applicable or easily extensible for a zero-shot regression framework
for which a continuos value is required to be predicted rather than a label. In
fact, there is a considerable lack of methods for zero-shot regression in the
literature. Two approaches for zero-shot regression that work in a two-phase
procedure were recently proposed. They first learn the observed target models
through a classical regression learning ignoring the target side information.
Then, they aggregate those observed target models afterwards exploiting the
target side information and the models for the unobserved targets are induced.
Despite both have shown quite good performance because of the different
treatment they grant to the common features and to the side information, they
exploit features and side information separately, avoiding a global
optimization for providing the unobserved target models. The proposal of this
paper is a novel method that jointly takes features and side information in a
one-phase learning process, but treating side information properly and in a
more deserving way than as common features. A specific kernel that properly
merges features and side information is proposed for this purpose resulting in
a novel approach that exhibits better performance over both artificial and real
datasets.
\\ ( https://arxiv.org/abs/2402.01264 ,  2873kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01293
Date: Fri, 2 Feb 2024 10:30:05 GMT   (44945kb,D)

Title: Can MLLMs Perform Text-to-Image In-Context Learning?
Authors: Yuchen Zeng, Wonjun Kang, Yicong Chen, Hyung Il Koo, Kangwook Lee
Categories: cs.LG cs.CL
\\
  The evolution from Large Language Models (LLMs) to Multimodal Large Language
Models (MLLMs) has spurred research into extending In-Context Learning (ICL) to
its multimodal counterpart. Existing such studies have primarily concentrated
on image-to-text ICL. However, the Text-to-Image ICL (T2I-ICL), with its unique
characteristics and potential applications, remains underexplored. To address
this gap, we formally define the task of T2I-ICL and present CoBSAT, the first
T2I-ICL benchmark dataset, encompassing ten tasks. Utilizing our dataset to
benchmark six state-of-the-art MLLMs, we uncover considerable difficulties
MLLMs encounter in solving T2I-ICL. We identify the primary challenges as the
inherent complexity of multimodality and image generation. To overcome these
challenges, we explore strategies like fine-tuning and Chain-of-Thought
prompting, demonstrating notable improvements. Our code and dataset are
available at \url{https://github.com/UW-Madison-Lee-Lab/CoBSAT}.
\\ ( https://arxiv.org/abs/2402.01293 ,  44945kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01295
Date: Fri, 2 Feb 2024 10:34:13 GMT   (6385kb,D)

Title: ExtremeCast: Boosting Extreme Value Prediction for Global Weather
  Forecast
Authors: Wanghan Xu, Kang Chen, Tao Han, Hao Chen, Wanli Ouyang, Lei Bai
Categories: cs.LG cs.AI
\\
  Data-driven weather forecast based on machine learning (ML) has experienced
rapid development and demonstrated superior performance in the global
medium-range forecast compared to traditional physics-based dynamical models.
However, most of these ML models struggle with accurately predicting extreme
weather, which is closely related to the extreme value prediction. Through
mathematical analysis, we prove that the use of symmetric losses, such as the
Mean Squared Error (MSE), leads to biased predictions and underestimation of
extreme values. To address this issue, we introduce Exloss, a novel loss
function that performs asymmetric optimization and highlights extreme values to
obtain accurate extreme weather forecast. Furthermore, we introduce a
training-free extreme value enhancement strategy named ExEnsemble, which
increases the variance of pixel values and improves the forecast robustness.
Combined with an advanced global weather forecast model, extensive experiments
show that our solution can achieve state-of-the-art performance in extreme
weather prediction, while maintaining the overall forecast accuracy comparable
to the top medium-range forecast models.
\\ ( https://arxiv.org/abs/2402.01295 ,  6385kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01296
Date: Fri, 2 Feb 2024 10:35:05 GMT   (1630kb,D)

Title: Bi-CryptoNets: Leveraging Different-Level Privacy for Encrypted
  Inference
Authors: Man-Jie Yuan, Zheng Zou, Wei Gao
Categories: cs.LG cs.CR cs.CV
\\
  Privacy-preserving neural networks have attracted increasing attention in
recent years, and various algorithms have been developed to keep the balance
between accuracy, computational complexity and information security from the
cryptographic view. This work takes a different view from the input data and
structure of neural networks. We decompose the input data (e.g., some images)
into sensitive and insensitive segments according to importance and privacy.
The sensitive segment includes some important and private information such as
human faces and we take strong homomorphic encryption to keep security, whereas
the insensitive one contains some background and we add perturbations. We
propose the bi-CryptoNets, i.e., plaintext and ciphertext branches, to deal
with two segments, respectively, and ciphertext branch could utilize the
information from plaintext branch by unidirectional connections. We adopt
knowledge distillation for our bi-CryptoNets by transferring representations
from a well-trained teacher neural network. Empirical studies show the
effectiveness and decrease of inference latency for our bi-CryptoNets.
\\ ( https://arxiv.org/abs/2402.01296 ,  1630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01297
Date: Fri, 2 Feb 2024 10:36:53 GMT   (116kb,D)

Title: Characterizing Overfitting in Kernel Ridgeless Regression Through the
  Eigenspectrum
Authors: Tin Sum Cheng and Aurelien Lucchi and Anastasis Kratsios and David
  Belius
Categories: cs.LG stat.ML
\\
  We derive new bounds for the condition number of kernel matrices, which we
then use to enhance existing non-asymptotic test error bounds for kernel
ridgeless regression in the over-parameterized regime for a fixed input
dimension. For kernels with polynomial spectral decay, we recover the bound
from previous work; for exponential decay, our bound is non-trivial and novel.
  Our conclusion on overfitting is two-fold: (i) kernel regressors whose
eigenspectrum decays polynomially must generalize well, even in the presence of
noisy labeled training data; these models exhibit so-called tempered
overfitting; (ii) if the eigenspectrum of any kernel ridge regressor decays
exponentially, then it generalizes poorly, i.e., it exhibits catastrophic
overfitting. This adds to the available characterization of kernel ridge
regressors exhibiting benign overfitting as the extremal case where the
eigenspectrum of the kernel decays sub-polynomially. Our analysis combines new
random matrix theory (RMT) techniques with recent tools in the kernel ridge
regression (KRR) literature.
\\ ( https://arxiv.org/abs/2402.01297 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01302
Date: Fri, 2 Feb 2024 10:44:42 GMT   (748kb,D)

Title: A Unified Framework for Gradient-based Clustering of Distributed Data
Authors: Aleksandar Armacki, Dragana Bajovi\'c, Du\v{s}an Jakoveti\'c, Soummya
  Kar
Categories: cs.LG cs.DC cs.MA
Comments: 35 pages, 5 figures, 6 tables
\\
  We develop a family of distributed clustering algorithms that work over
networks of users. In the proposed scenario, users contain a local dataset and
communicate only with their immediate neighbours, with the aim of finding a
clustering of the full, joint data. The proposed family, termed Distributed
Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$,
controling the proximity of users' center estimates, with $\mathcal{F}$
determining the clustering loss. Specialized to popular clustering losses like
$K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel
distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a
novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We
provide a unified analysis and establish several strong results, under mild
assumptions. First, the sequence of centers generated by the methods converges
to a well-defined notion of fixed point, under any center initialization and
value of $\rho$. Second, as $\rho$ increases, the family of fixed points
produced by DGC-$\mathcal{F}_\rho$ converges to a notion of consensus fixed
points. We show that consensus fixed points of DGC-$\mathcal{F}_{\rho}$ are
equivalent to fixed points of gradient clustering over the full data,
guaranteeing a clustering of the full data is produced. For the special case of
Bregman losses, we show that our fixed points converge to the set of Lloyd
points. Numerical experiments on real data confirm our theoretical findings and
demonstrate strong performance of the methods.
\\ ( https://arxiv.org/abs/2402.01302 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01306
Date: Fri, 2 Feb 2024 10:53:36 GMT   (857kb,D)

Title: KTO: Model Alignment as Prospect Theoretic Optimization
Authors: Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe
  Kiela
Categories: cs.LG cs.AI
Comments: preprint
\\
  Kahneman & Tversky's $\textit{prospect theory}$ tells us that humans perceive
random variables in a biased but well-defined manner; for example, humans are
famously loss-averse. We show that objectives for aligning LLMs with human
feedback implicitly incorporate many of these biases -- the success of these
objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed
to them being $\textit{human-aware loss functions}$ (HALOs). However, the
utility functions these methods attribute to humans still differ from those in
the prospect theory literature. Using a Kahneman-Tversky model of human
utility, we propose a HALO that directly maximizes the utility of generations
instead of maximizing the log-likelihood of preferences, as current methods do.
We call this approach Kahneman-Tversky Optimization (KTO), and it matches or
exceeds the performance of preference-based methods at scales from 1B to 30B.
Crucially, KTO does not need preferences -- only a binary signal of whether an
output is desirable or undesirable for a given input. This makes it far easier
to use in the real world, where preference data is scarce and expensive.
\\ ( https://arxiv.org/abs/2402.01306 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01327
Date: Fri, 2 Feb 2024 11:26:18 GMT   (161kb,D)

Title: Supervised Algorithmic Fairness in Distribution Shifts: A Survey
Authors: Yujie Lin, Dong Li, Chen Zhao, Xintao Wu, Qin Tian, Minglai Shao
Categories: cs.LG cs.AI cs.CY
\\
  Supervised fairness-aware machine learning under distribution shifts is an
emerging field that addresses the challenge of maintaining equitable and
unbiased predictions when faced with changes in data distributions from source
to target domains. In real-world applications, machine learning models are
often trained on a specific dataset but deployed in environments where the data
distribution may shift over time due to various factors. This shift can lead to
unfair predictions, disproportionately affecting certain groups characterized
by sensitive attributes, such as race and gender. In this survey, we provide a
summary of various types of distribution shifts and comprehensively investigate
existing methods based on these shifts, highlighting six commonly used
approaches in the literature. Additionally, this survey lists publicly
available datasets and evaluation metrics for empirical studies. We further
explore the interconnection with related research fields, discuss the
significant challenges, and identify potential directions for future studies.
\\ ( https://arxiv.org/abs/2402.01327 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01340
Date: Fri, 2 Feb 2024 11:53:27 GMT   (2829kb)

Title: SignSGD with Federated Defense: Harnessing Adversarial Attacks through
  Gradient Sign Decoding
Authors: Chanho Park, Namyoon Lee
Categories: cs.LG cs.CR eess.SP
\\
  Distributed learning is an effective approach to accelerate model training
using multiple workers. However, substantial communication delays emerge
between workers and a parameter server due to massive costs associated with
communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple
yet effective optimizer that reduces communication costs through one-bit
quantization, yet the convergence rates considerably decrease as adversarial
workers increase. In this paper, we show that the convergence rate is invariant
as the number of adversarial workers increases, provided that the number of
adversarial workers is smaller than that of benign workers. The key idea
showing this counter-intuitive result is our novel signSGD with federated
defense (signSGD-FD). Unlike the traditional approaches, signSGD-FD exploits
the gradient information sent by adversarial workers with the proper weights,
which are obtained through gradient sign decoding. Experimental results
demonstrate signSGD-FD achieves superior convergence rates over traditional
algorithms in various adversarial attack scenarios.
\\ ( https://arxiv.org/abs/2402.01340 ,  2829kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01341
Date: Fri, 2 Feb 2024 11:55:57 GMT   (33kb)

Title: Fundamental Properties of Causal Entropy and Information Gain
Authors: Francisco N. F. Q. Simoes, Mehdi Dastani, Thijs van Ommen
Categories: cs.LG cs.IT math.IT stat.ML
Comments: Accepted for the conference CLeaR (Causal Learning and Reasoning)
  2024. To appear in its proceedings
\\
  Recent developments enable the quantification of causal control given a
structural causal model (SCM). This has been accomplished by introducing
quantities which encode changes in the entropy of one variable when intervening
on another. These measures, named causal entropy and causal information gain,
aim to address limitations in existing information theoretical approaches for
machine learning tasks where causality plays a crucial role. They have not yet
been properly mathematically studied. Our research contributes to the formal
understanding of the notions of causal entropy and causal information gain by
establishing and analyzing fundamental properties of these concepts, including
bounds and chain rules. Furthermore, we elucidate the relationship between
causal entropy and stochastic interventions. We also propose definitions for
causal conditional entropy and causal conditional information gain. Overall,
this exploration paves the way for enhancing causal machine learning tasks
through the study of recently-proposed information theoretic quantities
grounded in considerations about causality.
\\ ( https://arxiv.org/abs/2402.01341 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01342
Date: Fri, 2 Feb 2024 11:57:50 GMT   (627kb,D)

Title: Training-time Neuron Alignment through Permutation Subspace for
  Improving Linear Mode Connectivity and Model Fusion
Authors: Zexi Li, Zhiqi Li, Jie Lin, Tao Shen, Tao Lin, Chao Wu
Categories: cs.LG stat.ML
Comments: preprint
\\
  In deep learning, stochastic gradient descent often yields functionally
similar yet widely scattered solutions in the weight space even under the same
initialization, causing barriers in the Linear Mode Connectivity (LMC)
landscape. Overcoming these barriers is crucial for understanding deep learning
dynamics and enhancing model-fusion algorithms. Previous studies highlight the
role of permutation symmetry in reducing post-training barriers through network
permutation. However, these post-hoc methods, demanding extra computations, are
less effective for larger, complex models (e.g., ViT, LLM) due to numerous
permutation matrices. Thus, in this paper, we study training-time neuron
alignment. Our hypothesis suggests that training-time permutation subspace can
reduce LMC barriers for free. We find that pruning at initialization supports
this. Beyond pruning, we introduce TNA-PFN, a simple yet lossless algorithm
using a partial gradient mask during training. TNA-PFN is theoretically and
empirically validated for reducing LMC barriers. It excels in wide model fusion
applications, especially in federated learning, two algorithms based on TNA-FPN
that are proposed to show its prospects even under heterogeneous datasets.
Moreover, TNA-PFN can enhance the generalization of model soup for vision
transformers and ColD fusion for pretrained language models.
\\ ( https://arxiv.org/abs/2402.01342 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01343
Date: Fri, 2 Feb 2024 11:57:53 GMT   (3063kb,D)

Title: Shapelet-based Model-agnostic Counterfactual Local Explanations for Time
  Series Classification
Authors: Qi Huang, Wei Chen, Thomas B\"ack, Niki van Stein
Categories: cs.LG
Comments: The paper has been accepted by the XAI4Sci workshop of AAAI 2024
\\
  In this work, we propose a model-agnostic instance-based post-hoc
explainability method for time series classification. The proposed algorithm,
namely Time-CF, leverages shapelets and TimeGAN to provide counterfactual
explanations for arbitrary time series classifiers. We validate the proposed
method on several real-world univariate time series classification tasks from
the UCR Time Series Archive. The results indicate that the counterfactual
instances generated by Time-CF when compared to state-of-the-art methods,
demonstrate better performance in terms of four explainability metrics:
closeness, sensibility, plausibility, and sparsity.
\\ ( https://arxiv.org/abs/2402.01343 ,  3063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01344
Date: Fri, 2 Feb 2024 12:02:42 GMT   (5613kb,D)

Title: Monotone, Bi-Lipschitz, and Polyak-\L{}ojasiewicz Networks
Authors: Ruigang Wang, Krishnamurthy Dvijotham, Ian R. Manchester
Categories: cs.LG
\\
  This paper presents a new \emph{bi-Lipschitz} invertible neural network, the
BiLipNet, which has the ability to control both its \emph{Lipschitzness}
(output sensitivity to input perturbations) and \emph{inverse Lipschitzness}
(input distinguishability from different outputs). The main contribution is a
novel invertible residual layer with certified strong monotonicity and
Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz
networks. The certification is based on incremental quadratic constraints,
which achieves much tighter bounds compared to spectral normalization.
Moreover, we formulate the model inverse calculation as a three-operator
splitting problem, for which fast algorithms are known. Based on the proposed
bi-Lipschitz network, we introduce a new scalar-output network, the PLNet,
which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn
non-convex surrogate losses with favourable properties, e.g., a unique and
efficiently-computable global minimum.
\\ ( https://arxiv.org/abs/2402.01344 ,  5613kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01348
Date: Fri, 2 Feb 2024 12:04:44 GMT   (4198kb,D)

Title: CORE: Mitigating Catastrophic Forgetting in Continual Learning through
  Cognitive Replay
Authors: Jianshu Zhang, Yankai Fu, Ziheng Peng, Dongyu Yao, Kun He
Categories: cs.LG cs.AI
\\
  This paper introduces a novel perspective to significantly mitigate
catastrophic forgetting in continuous learning (CL), which emphasizes models'
capacity to preserve existing knowledge and assimilate new information. Current
replay-based methods treat every task and data sample equally and thus can not
fully exploit the potential of the replay buffer. In response, we propose
COgnitive REplay (CORE), which draws inspiration from human cognitive review
processes. CORE includes two key strategies: Adaptive Quantity Allocation and
Quality-Focused Data Selection. The former adaptively modulates the replay
buffer allocation for each task based on its forgetting rate, while the latter
guarantees the inclusion of representative data that best encapsulates the
characteristics of each task within the buffer. Our approach achieves an
average accuracy of 37.95% on split-CIFAR10, surpassing the best baseline
method by 6.52%. Additionally, it significantly enhances the accuracy of the
poorest-performing task by 6.30% compared to the top baseline.
\\ ( https://arxiv.org/abs/2402.01348 ,  4198kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01350
Date: Fri, 2 Feb 2024 12:09:20 GMT   (1804kb,D)

Title: FedMoE: Data-Level Personalization with Mixture of Experts for
  Model-Heterogeneous Personalized Federated Learning
Authors: Liping Yi, Han Yu, Chao Ren, Heng Zhang, Gang Wang, Xiaoguang Liu,
  Xiaoxiao Li
Categories: cs.LG cs.DC
\\
  Federated learning (FL) is widely employed for collaborative training on
decentralized data but faces challenges like data, system, and model
heterogeneity. This prompted the emergency of model-heterogeneous personalized
federated learning (MHPFL). However, concerns persist regarding data and model
privacy, model performance, communication, and computational costs in current
MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous
personalized Federated learning algorithm (FedMoE) with the Mixture of Experts
(MoE), renowned for enhancing large language models (LLMs). It assigns a shared
homogeneous small feature extractor and a local gating network for each
client's local heterogeneous large model. (1) During local training, the local
heterogeneous model's feature extractor acts as a local expert for personalized
feature (representation) extraction, while the shared homogeneous small feature
extractor serves as a global expert for generalized feature extraction. The
local gating network produces personalized weights for extracted
representations from both experts on each data sample. The three models form a
local heterogeneous MoE. The weighted mixed representation fuses global
generalized and local personalized features and is processed by the local
heterogeneous large model's header with personalized prediction information for
output. The MoE and prediction header are updated synchronously. (2) The
trained local homogeneous small feature extractors are sent to the server for
cross-client information fusion via aggregation. Briefly, FedMoE first enhances
local model personalization at a fine-grained data level while supporting model
heterogeneity.
\\ ( https://arxiv.org/abs/2402.01350 ,  1804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01359
Date: Fri, 2 Feb 2024 12:27:32 GMT   (10361kb,D)

Title: TESSERACT: Eliminating Experimental Bias in Malware Classification
  across Space and Time (Extended Version)
Authors: Zeliang Kan, Shae McFadden, Daniel Arp, Feargus Pendlebury, Roberto
  Jordaney, Johannes Kinder, Fabio Pierazzi, Lorenzo Cavallaro
Categories: cs.LG cs.CR cs.PF
Comments: 35 pages, submitted to ACM ToPS, under reviewing. arXiv admin note:
  text overlap with arXiv:1807.07838
\\
  Machine learning (ML) plays a pivotal role in detecting malicious software.
Despite the high F1-scores reported in numerous studies reaching upwards of
0.99, the issue is not completely solved. Malware detectors often experience
performance decay due to constantly evolving operating systems and attack
methods, which can render previously learned knowledge insufficient for
accurate decision-making on new inputs. This paper argues that commonly
reported results are inflated due to two pervasive sources of experimental bias
in the detection task: spatial bias caused by data distributions that are not
representative of a real-world deployment; and temporal bias caused by
incorrect time splits of data, leading to unrealistic configurations. To
address these biases, we introduce a set of constraints for fair experiment
design, and propose a new metric, AUT, for classifier robustness in real-world
settings. We additionally propose an algorithm designed to tune training data
to enhance classifier performance. Finally, we present TESSERACT, an
open-source framework for realistic classifier comparison. Our evaluation
encompasses both traditional ML and deep learning methods, examining published
works on an extensive Android dataset with 259,230 samples over a five-year
span. Additionally, we conduct case studies in the Windows PE and PDF domains.
Our findings identify the existence of biases in previous studies and reveal
that significant performance enhancements are possible through appropriate,
periodic tuning. We explore how mitigation strategies may support in achieving
a more stable and better performance over time by employing multiple strategies
to delay performance decay.
\\ ( https://arxiv.org/abs/2402.01359 ,  10361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01361
Date: Fri, 2 Feb 2024 12:29:18 GMT   (197kb,D)

Title: To the Max: Reinventing Reward in Reinforcement Learning
Authors: Grigorii Veviurko, Wendelin B\"ohmer, Mathijs de Weerdt
Categories: cs.LG
\\
  In reinforcement learning (RL), different rewards can define the same optimal
policy but result in drastically different learning performance. For some, the
agent gets stuck with a suboptimal behavior, and for others, it solves the task
efficiently. Choosing a good reward function is hence an extremely important
yet challenging problem. In this paper, we explore an alternative approach to
using rewards for learning. We introduce max-reward RL, where an agent
optimizes the maximum rather than the cumulative reward. Unlike earlier works,
our approach works for deterministic and stochastic environments and can be
easily combined with state-of-the-art RL algorithms. In the experiments, we
study the performance of max-reward RL algorithms in two goal-reaching
environments from Gymnasium-Robotics and demonstrate its benefits over standard
RL. The code is publicly available.
\\ ( https://arxiv.org/abs/2402.01361 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01369
Date: Fri, 2 Feb 2024 12:39:49 GMT   (4273kb,D)

Title: Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with
  Multi-Modal Priors
Authors: Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian
  Yu
Categories: cs.LG cs.CR cs.CV
Comments: 10 figures
\\
  Diffusion models have been widely deployed in various image generation tasks,
demonstrating an extraordinary connection between image and text modalities.
However, they face challenges of being maliciously exploited to generate
harmful or sensitive images by appending a specific suffix to the original
prompt. Existing works mainly focus on using single-modal information to
conduct attacks, which fails to utilize multi-modal features and results in
less than satisfactory performance. Integrating multi-modal priors (MMP), i.e.
both text and image features, we propose a targeted attack method named
MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a
target object into the image content while simultaneously removing the original
object. The MMP-Attack shows a notable advantage over existing works with
superior universality and transferability, which can effectively attack
commercial text-to-image (T2I) models such as DALL-E 3. To the best of our
knowledge, this marks the first successful attempt of transfer-based attack to
commercial T2I models. Our code is publicly available at
\url{https://github.com/ydc123/MMP-Attack}.
\\ ( https://arxiv.org/abs/2402.01369 ,  4273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01371
Date: Fri, 2 Feb 2024 12:48:49 GMT   (233kb,D)

Title: Critic-Actor for Average Reward MDPs with Function Approximation: A
  Finite-Time Analysis
Authors: Prashansa Panda and Shalabh Bhatnagar
Categories: cs.LG
\\
  In recent years, there has been a lot of research work activity focused on
carrying out asymptotic and non-asymptotic convergence analyses for
two-timescale actor critic algorithms where the actor updates are performed on
a timescale that is slower than that of the critic. In a recent work, the
critic-actor algorithm has been presented for the infinite horizon discounted
cost setting in the look-up table case where the timescales of the actor and
the critic are reversed and asymptotic convergence analysis has been presented.
In our work, we present the first critic-actor algorithm with function
approximation and in the long-run average reward setting and present the first
finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal
learning rates and prove that our algorithm achieves a sample complexity of
$\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the
critic to be upper bounded by $\epsilon$ which is better than the one obtained
for actor-critic in a similar setting. We also show the results of numerical
experiments on three benchmark settings and observe that the critic-actor
algorithm competes well with the actor-critic algorithm.
\\ ( https://arxiv.org/abs/2402.01371 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01379
Date: Fri, 2 Feb 2024 13:03:15 GMT   (183kb,D)

Title: Regularized boosting with an increasing coefficient magnitude stop
  criterion as meta-learner in hyperparameter optimization stacking ensemble
Authors: Laura Fdez-D\'iaz, Jos\'e Ram\'on Quevedo, Elena Monta\~n\'es
Categories: cs.LG
Journal-ref: Neurocomputing 2023 Volume 551 126516
DOI: 10.1016/j.neucom.2023.126516
\\
  In Hyperparameter Optimization (HPO), only the hyperparameter configuration
with the best performance is chosen after performing several trials, then,
discarding the effort of training all the models with every hyperparameter
configuration trial and performing an ensemble of all them. This ensemble
consists of simply averaging the model predictions or weighting the models by a
certain probability. Recently, other more sophisticated ensemble strategies,
such as the Caruana method or the stacking strategy has been proposed. On the
one hand, the Caruana method performs well in HPO ensemble, since it is not
affected by the effects of multicollinearity, which is prevalent in HPO. It
just computes the average over a subset of predictions with replacement. But it
does not benefit from the generalization power of a learning process. On the
other hand, stacking methods include a learning procedure since a meta-learner
is required to perform the ensemble. Yet, one hardly finds advice about which
meta-learner is adequate. Besides, some meta-learners may suffer from the
effects of multicollinearity or need to be tuned to reduce them. This paper
explores meta-learners for stacking ensemble in HPO, free of hyperparameter
tuning, able to reduce the effects of multicollinearity and considering the
ensemble learning process generalization power. At this respect, the boosting
strategy seems promising as a stacking meta-learner. In fact, it completely
removes the effects of multicollinearity. This paper also proposes an implicit
regularization in the classical boosting method and a novel non-parametric stop
criterion suitable only for boosting and specifically designed for HPO. The
synergy between these two improvements over boosting exhibits competitive and
promising predictive power performance compared to other existing meta-learners
and ensemble approaches for HPO other than the stacking ensemble.
\\ ( https://arxiv.org/abs/2402.01379 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01399
Date: Fri, 2 Feb 2024 13:31:17 GMT   (5134kb,D)

Title: A Probabilistic Model to explain Self-Supervised Representation Learning
Authors: Alice Bizeul, Bernhard Sch\"olkopf and Carl Allen
Categories: cs.LG cs.AI stat.ML
\\
  Self-supervised learning (SSL) learns representations by leveraging an
auxiliary unsupervised task, such as classifying semantically related samples,
e.g. different data augmentations or modalities. Of the many approaches to SSL,
contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for
learning representations that achieve downstream performance close to that of
supervised learning. However, a theoretical understanding of the mechanism
behind these methods eludes. We propose a generative latent variable model for
the data and show that several families of discriminative self-supervised
algorithms, including contrastive methods, approximately induce its latent
structure over representations, providing a unifying theoretical framework. We
also justify links to mutual information and the use of a projection head.
Fitting our model generatively, as SimVE, improves performance over previous
VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows
the gap to discriminative methods on _content_ classification and, as our
analysis predicts, outperforms them where _style_ information is required,
taking a step toward task-agnostic representations.
\\ ( https://arxiv.org/abs/2402.01399 ,  5134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01401
Date: Fri, 2 Feb 2024 13:33:30 GMT   (3055kb,D)

Title: Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
Authors: Jack Foster, Kyle Fogarty, Stefan Schoepf, Cengiz \"Oztireli,
  Alexandra Brintrup
Categories: cs.LG cs.AI stat.ML
\\
  To comply with AI and data regulations, the need to forget private or
copyrighted information from trained machine learning models is increasingly
important. The key challenge in unlearning is forgetting the necessary data in
a timely manner, while preserving model performance. In this work, we address
the zero-shot unlearning scenario, whereby an unlearning algorithm must be able
to remove data given only a trained model and the data to be forgotten. Under
such a definition, existing state-of-the-art methods are insufficient. Building
on the concepts of Lipschitz continuity, we present a method that induces
smoothing of the forget sample's output, with respect to perturbations of that
sample. We show this smoothing successfully results in forgetting while
preserving general model performance. We perform extensive empirical evaluation
of our method over a range of contemporary benchmarks, verifying that our
method achieves state-of-the-art performance under the strict constraints of
zero-shot unlearning.
\\ ( https://arxiv.org/abs/2402.01401 ,  3055kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01408
Date: Fri, 2 Feb 2024 13:42:12 GMT   (5661kb,D)

Title: Climbing the Ladder of Interpretability with Counterfactual Concept
  Bottleneck Models
Authors: Gabriele Dominici, Pietro Barbiero, Francesco Giannini, Martin
  Gjoreski, Giuseppe Marra and Marc Langheinrich
Categories: cs.LG cs.AI
\\
  Current deep learning models are not designed to simultaneously address three
fundamental questions: predict class labels to solve a given classification
task (the "What?"), explain task predictions (the "Why?"), and imagine
alternative scenarios that could result in different predictions (the "What
if?"). The inability to answer these questions represents a crucial gap in
deploying reliable AI agents, calibrating human trust, and deepening
human-machine interaction. To bridge this gap, we introduce CounterFactual
Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently
address the above queries all at once without the need to run post-hoc
searches. Our results show that CF-CBMs produce: accurate predictions (the
"What?"), simple explanations for task predictions (the "Why?"), and
interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or
estimate the most probable counterfactual to: (i) explain the effect of concept
interventions on tasks, (ii) show users how to get a desired class label, and
(iii) propose concept interventions via "task-driven" interventions.
\\ ( https://arxiv.org/abs/2402.01408 ,  5661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01415
Date: Fri, 2 Feb 2024 13:53:29 GMT   (255kb,D)

Title: SMLP: Symbolic Machine Learning Prover
Authors: Franz Brau{\ss}e, Zurab Khasidashvili, Konstantin Korovin
Categories: cs.LG cs.AI cs.LO cs.SC math.OC
Comments: 12 pages, 4 figures. (submitted)
\\
  Symbolic Machine Learning Prover (SMLP) is a tool and a library for system
exploration based on data samples obtained by simulating or executing the
system on a number of input vectors. SMLP aims at exploring the system based on
this data by taking a grey-box approach: SMLP combines statistical methods of
data exploration with building and exploring machine learning models in close
feedback loop with the system's response, and exploring these models by
combining probabilistic and formal methods. SMLP has been applied in industrial
setting at Intel for analyzing and optimizing hardware designs at the analog
level. SMLP is a general purpose tool and can be applied to systems that can be
sampled and modeled by machine learning models.
\\ ( https://arxiv.org/abs/2402.01415 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01431
Date: Fri, 2 Feb 2024 14:20:04 GMT   (2329kb,D)

Title: Approximate Control for Continuous-Time POMDPs
Authors: Yannick Eich, Bastian Alt, Heinz Koeppl
Categories: cs.LG cs.SY eess.SY q-bio.QM
Comments: To be published in AISTATS 2024
\\
  This work proposes a decision-making framework for partially observable
systems in continuous time with discrete state and action spaces. As optimal
decision-making becomes intractable for large state spaces we employ
approximation methods for the filtering and the control problem that scale well
with an increasing number of states. Specifically, we approximate the
high-dimensional filtering distribution by projecting it onto a parametric
family of distributions, and integrate it into a control heuristic based on the
fully observable system to obtain a scalable policy. We demonstrate the
effectiveness of our approach on several partially observed systems, including
queueing systems and chemical reaction networks.
\\ ( https://arxiv.org/abs/2402.01431 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01439
Date: Fri, 2 Feb 2024 14:30:48 GMT   (6764kb,D)

Title: From Words to Molecules: A Survey of Large Language Models in Chemistry
Authors: Chang Liao, Yemin Yu, Yu Mei, Ying Wei
Categories: cs.LG cs.AI q-bio.BM q-bio.QM
Comments: Submitted to IJCAI 2024 survey track
\\
  In recent years, Large Language Models (LLMs) have achieved significant
success in natural language processing (NLP) and various interdisciplinary
areas. However, applying LLMs to chemistry is a complex task that requires
specialized domain knowledge. This paper provides a thorough exploration of the
nuanced methodologies employed in integrating LLMs into the field of chemistry,
delving into the complexities and innovations at this interdisciplinary
juncture. Specifically, our analysis begins with examining how molecular
information is fed into LLMs through various representation and tokenization
methods. We then categorize chemical LLMs into three distinct groups based on
the domain and modality of their input data, and discuss approaches for
integrating these inputs for LLMs. Furthermore, this paper delves into the
pretraining objectives with adaptations to chemical LLMs. After that, we
explore the diverse applications of LLMs in chemistry, including novel
paradigms for their application in chemistry tasks. Finally, we identify
promising research directions, including further integration with chemical
knowledge, advancements in continual learning, and improvements in model
interpretability, paving the way for groundbreaking developments in the field.
\\ ( https://arxiv.org/abs/2402.01439 ,  6764kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01440
Date: Fri, 2 Feb 2024 14:32:42 GMT   (856kb,D)

Title: Few-Shot Learning on Graphs: from Meta-learning to Pre-training and
  Prompting
Authors: Xingtong Yu, Yuan Fang, Zemin Liu, Yuxia Wu, Zhihao Wen, Jianyuan Bo,
  Xinming Zhang and Steven C.H. Hoi
Categories: cs.LG cs.AI cs.SI
\\
  Graph representation learning, a critical step in graph-centric tasks, has
seen significant advancements. Earlier techniques often operate in an
end-to-end setting, where performance heavily relies on the availability of
ample labeled data. This constraint has spurred the emergence of few-shot
learning on graphs, where only a few task-specific labels are available for
each task. Given the extensive literature in this field, this survey endeavors
to synthesize recent developments, provide comparative insights, and identify
future directions. We systematically categorize existing studies into three
major families: meta-learning approaches, pre-training approaches, and hybrid
approaches, with a finer-grained classification in each family to aid readers
in their method selection process. Within each category, we analyze the
relationships among these methods and compare their strengths and limitations.
Finally, we outline prospective future directions for few-shot learning on
graphs to catalyze continued innovation in this field.
\\ ( https://arxiv.org/abs/2402.01440 ,  856kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01444
Date: Fri, 2 Feb 2024 14:36:50 GMT   (12194kb,D)

Title: Mission Critical -- Satellite Data is a Distinct Modality in Machine
  Learning
Authors: Esther Rolf, Konstantin Klemmer, Caleb Robinson, Hannah Kerner
Categories: cs.LG cs.AI cs.CV
Comments: 15 pages, 5 figures
\\
  Satellite data has the potential to inspire a seismic shift for machine
learning -- one in which we rethink existing practices designed for traditional
data modalities. As machine learning for satellite data (SatML) gains traction
for its real-world impact, our field is at a crossroads. We can either continue
applying ill-suited approaches, or we can initiate a new research agenda that
centers around the unique characteristics and challenges of satellite data.
This position paper argues that satellite data constitutes a distinct modality
for machine learning research and that we must recognize it as such to advance
the quality and impact of SatML research across theory, methods, and
deployment. We outline critical discussion questions and actionable suggestions
to transform SatML from merely an intriguing application area to a dedicated
research discipline that helps move the needle on big challenges for machine
learning and society.
\\ ( https://arxiv.org/abs/2402.01444 ,  12194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01450
Date: Fri, 2 Feb 2024 14:39:39 GMT   (203kb,D)

Title: Improving importance estimation in covariate shift for providing
  accurate prediction error
Authors: Laura Fdez-D\'iaz, Sara Gonz\'alez Tomillo, Elena Monta\~n\'es, Jos\'e
  Ram\'on Quevedo
Categories: cs.LG stat.ML
Journal-ref: Expert Systems With Applications 2022 Volume 193 116376
DOI: 10.1016/j.eswa.2021.116376
\\
  In traditional Machine Learning, the algorithms predictions are based on the
assumption that the data follows the same distribution in both the training and
the test datasets. However, in real world data this condition does not hold
and, for instance, the distribution of the covariates changes whereas the
conditional distribution of the targets remains unchanged. This situation is
called covariate shift problem where standard error estimation may be no longer
accurate. In this context, the importance is a measure commonly used to
alleviate the influence of covariate shift on error estimations. The main
drawback is that it is not easy to compute. The Kullback-Leibler Importance
Estimation Procedure (KLIEP) is capable of estimating importance in a promising
way. Despite its good performance, it fails to ignore target information, since
it only includes the covariates information for computing the importance. In
this direction, this paper explores the potential performance improvement if
target information is considered in the computation of the importance. Then, a
redefinition of the importance arises in order to be generalized in this way.
Besides the potential improvement in performance, including target information
make possible the application to a real application about plankton
classification that motivates this research and characterized by its great
dimensionality, since considering targets rather than covariates reduces the
computation and the noise in the covariates. The impact of taking target
information is also explored when Logistic Regression (LR), Kernel Mean
Matching (KMM), Ensemble Kernel Mean Matching (EKMM) and the naive predecessor
of KLIEP called Kernel Density Estimation (KDE) methods estimate the
importance. The experimental results lead to a more accurate error estimation
using target information, especially in case of the more promising method
KLIEP.
\\ ( https://arxiv.org/abs/2402.01450 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01454
Date: Fri, 2 Feb 2024 14:43:19 GMT   (7236kb,D)

Title: Integrating Large Language Models in Causal Discovery: A Statistical
  Causal Approach
Authors: Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue,
  Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai
Categories: cs.LG cs.AI stat.ME stat.ML
\\
  In practical statistical causal discovery (SCD), embedding domain expert
knowledge as constraints into the algorithm is widely accepted as significant
for creating consistent meaningful causal models, despite the recognized
challenges in systematic acquisition of the background knowledge. To overcome
these challenges, this paper proposes a novel methodology for causal inference,
in which SCD methods and knowledge based causal inference (KBCI) with a large
language model (LLM) are synthesized through "statistical causal prompting
(SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have
revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result
with prior knowledge from LLM-KBCI to approach the ground truth, and that the
SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has
been clarified that an LLM can improve SCD with its background knowledge, even
if the LLM does not contain information on the dataset. The proposed approach
can thus address challenges such as dataset biases and limitations,
illustrating the potential of LLMs to improve data-driven causal inference
across diverse scientific domains.
\\ ( https://arxiv.org/abs/2402.01454 ,  7236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01476
Date: Fri, 2 Feb 2024 15:05:13 GMT   (327kb,D)

Title: Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian
  Processes
Authors: Yingyi Chen, Qinghua Tao, Francesco Tonin, Johan A.K. Suykens
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: We propose Kernel-Eigen Pair Sparse Variational Gaussian Processes
  (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry
  of attention kernel is tackled by KSVD and a reduced time complexity is
  acquired
\\
  While the great capability of Transformers significantly boosts prediction
accuracy, it could also yield overconfident predictions and require calibrated
uncertainty estimation, which can be commonly tackled by Gaussian processes
(GPs). Existing works apply GPs with symmetric kernels under variational
inference to the attention kernel; however, omitting the fact that attention
kernels are in essence asymmetric. Moreover, the complexity of deriving the GP
posteriors remains high for large-scale data. In this work, we propose
Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building
uncertainty-aware self-attention where the asymmetry of attention kernels is
tackled by Kernel SVD (KSVD) and a reduced complexity is acquired. Through
KEP-SVGP, i) the SVGP pair induced by the two sets of singular vectors from
KSVD w.r.t. the attention kernel fully characterizes the asymmetry; ii) using
only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP
posteriors can be based on the inversion of a diagonal matrix containing
singular values, contributing to a reduction in time complexity; iii) an
evidence lower bound is derived so that variational parameters can be optimized
towards this objective. Experiments verify our excellent performances and
efficiency on in-distribution, distribution-shift and out-of-distribution
benchmarks.
\\ ( https://arxiv.org/abs/2402.01476 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01481
Date: Fri, 2 Feb 2024 15:07:09 GMT   (4855kb,D)

Title: Multi-level protein pre-training with Vabs-Net
Authors: Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, Shuqi Lu
Categories: cs.LG cs.AI q-bio.BM
\\
  In recent years, there has been a surge in the development of 3D
structure-based pre-trained protein models, representing a significant
advancement over pre-trained protein language models in various downstream
tasks. However, most existing structure-based pre-trained models primarily
focus on the residue level, i.e., alpha carbon atoms, while ignoring other
atoms like side chain atoms. We argue that modeling proteins at both residue
and atom levels is important since the side chain atoms can also be crucial for
numerous downstream tasks, for example, molecular docking. Nevertheless, we
find that naively combining residue and atom information during pre-training
typically fails. We identify a key reason is the information leakage caused by
the inclusion of atom structure in the input, which renders residue-level
pre-training tasks trivial and results in insufficiently expressive residue
representations. To address this issue, we introduce a span mask pre-training
strategy on 3D protein chains to learn meaningful representations of both
residues and atoms. This leads to a simple yet effective approach to learning
protein representation suitable for diverse downstream tasks. Extensive
experimental results on binding site prediction and function prediction tasks
demonstrate our proposed pre-training approach significantly outperforms other
methods. Our code will be made public.
\\ ( https://arxiv.org/abs/2402.01481 ,  4855kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01484
Date: Fri, 2 Feb 2024 15:12:16 GMT   (9440kb,D)

Title: Connecting the Dots: Is Mode-Connectedness the Key to Feasible
  Sample-Based Inference in Bayesian Neural Networks?
Authors: Emanuel Sommer, Lisa Wimmer, Theodore Papamarkou, Ludwig Bothmann,
  Bernd Bischl, David R\"ugamer
Categories: cs.LG stat.CO stat.ML
\\
  A major challenge in sample-based inference (SBI) for Bayesian neural
networks is the size and structure of the networks' parameter space. Our work
shows that successful SBI is possible by embracing the characteristic
relationship between weight and function space, uncovering a systematic link
between overparameterization and the difficulty of the sampling problem.
Through extensive experiments, we establish practical guidelines for sampling
and convergence diagnosis. As a result, we present a Bayesian deep ensemble
approach as an effective solution with competitive performance and uncertainty
quantification.
\\ ( https://arxiv.org/abs/2402.01484 ,  9440kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01514
Date: Fri, 2 Feb 2024 15:54:53 GMT   (4552kb,D)

Title: Mapping the Multiverse of Latent Representations
Authors: Jeremy Wayland, Corinna Coupette, Bastian Rieck
Categories: cs.LG math.AT stat.ML
\\
  Echoing recent calls to counter reliability and robustness concerns in
machine learning via multiverse analysis, we present PRESTO, a principled
framework for mapping the multiverse of machine-learning models that rely on
latent representations. Although such models enjoy widespread adoption, the
variability in their embeddings remains poorly understood, resulting in
unnecessary complexity and untrustworthy representations. Our framework uses
persistent homology to characterize the latent spaces arising from different
combinations of diverse machine-learning methods, (hyper)parameter
configurations, and datasets, allowing us to measure their pairwise
(dis)similarity and statistically reason about their distributions. As we
demonstrate both theoretically and empirically, our pipeline preserves
desirable properties of collections of latent representations, and it can be
leveraged to perform sensitivity analysis, detect anomalous embeddings, or
efficiently and effectively navigate hyperparameter search spaces.
\\ ( https://arxiv.org/abs/2402.01514 ,  4552kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01515
Date: Fri, 2 Feb 2024 15:55:25 GMT   (2300kb,D)

Title: Enhancing Stochastic Gradient Descent: A Unified Framework and Novel
  Acceleration Methods for Faster Convergence
Authors: Yichuan Deng, Zhao Song, Chiwun Yang
Categories: cs.LG cs.AI math.OC
\\
  Based on SGD, previous works have proposed many algorithms that have improved
convergence speed and generalization in stochastic optimization, such as SGDm,
AdaGrad, Adam, etc. However, their convergence analysis under non-convex
conditions is challenging. In this work, we propose a unified framework to
address this issue. For any first-order methods, we interpret the updated
direction $g_t$ as the sum of the stochastic subgradient $\nabla f_t(x_t)$ and
an additional acceleration term $\frac{2|\langle v_t, \nabla f_t(x_t)
\rangle|}{\|v_t\|_2^2} v_t$, thus we can discuss the convergence by analyzing
$\langle v_t, \nabla f_t(x_t) \rangle$. Through our framework, we have
discovered two plug-and-play acceleration methods: \textbf{Reject Accelerating}
and \textbf{Random Vector Accelerating}, we theoretically demonstrate that
these two methods can directly lead to an improvement in convergence rate.
\\ ( https://arxiv.org/abs/2402.01515 ,  2300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01528
Date: Fri, 2 Feb 2024 16:15:24 GMT   (176kb,D)

Title: Decoding Speculative Decoding
Authors: Minghao Yan, Saurabh Agarwal, Shivaram Venkataraman
Categories: cs.LG cs.CL
\\
  Speculative Decoding is a widely used technique to speed up inference for
Large Language Models (LLMs) without modifying its outcome. When performing
inference on an LLM, speculative decoding uses a smaller draft model which
generates speculative tokens and then uses the target LLM to verify those draft
tokens. The speedup provided by speculative decoding heavily depends on the
choice of the draft model. It has been widely suggested to select a draft model
that provides a high probability of the generated token being accepted by the
LLM to achieve the highest throughput. However, our experiments indicate the
contrary with throughput diminishing as the probability of generated tokens to
be accepted by the target model increases. To understand this phenomenon, we
perform extensive experiments to characterize the different factors that affect
speculative decoding and how those factors interact and affect the speedups.
Based on our experiments we describe an analytical model which can be used to
decide the right draft model for a given workload. Further, using our insights
we design a new draft model for LLaMA-65B which can provide 30% higher
throughput than existing draft models.
\\ ( https://arxiv.org/abs/2402.01528 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01543
Date: Fri, 2 Feb 2024 16:35:51 GMT   (347kb,D)

Title: Adaptive Optimization for Prediction with Missing Data
Authors: Dimitris Bertsimas, Arthur Delarue, and Jean Pauphilet
Categories: cs.LG stat.ML
Comments: arXiv admin note: text overlap with arXiv:2104.03158
\\
  When training predictive models on data with missing entries, the most widely
used and versatile approach is a pipeline technique where we first impute
missing entries and then compute predictions. In this paper, we view prediction
with missing data as a two-stage adaptive optimization problem and propose a
new class of models, adaptive linear regression models, where the regression
coefficients adapt to the set of observed features. We show that some adaptive
linear regression models are equivalent to learning an imputation rule and a
downstream linear regression model simultaneously instead of sequentially. We
leverage this joint-impute-then-regress interpretation to generalize our
framework to non-linear models. In settings where data is strongly not missing
at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.
\\ ( https://arxiv.org/abs/2402.01543 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01546
Date: Fri, 2 Feb 2024 16:39:08 GMT   (1762kb,D)

Title: Privacy-Preserving Distributed Learning for Residential Short-Term Load
  Forecasting
Authors: Yi Dong, Yingjie Wang, Mariana Gama, Mustafa A. Mustafa, Geert
  Deconinck, Xiaowei Huang
Categories: cs.LG cs.AI cs.CR cs.DC cs.MA cs.SY eess.SY
\\
  In the realm of power systems, the increasing involvement of residential
users in load forecasting applications has heightened concerns about data
privacy. Specifically, the load data can inadvertently reveal the daily
routines of residential users, thereby posing a risk to their property
security. While federated learning (FL) has been employed to safeguard user
privacy by enabling model training without the exchange of raw data, these FL
models have shown vulnerabilities to emerging attack techniques, such as Deep
Leakage from Gradients and poisoning attacks. To counteract these, we initially
employ a Secure-Aggregation (SecAgg) algorithm that leverages multiparty
computation cryptographic techniques to mitigate the risk of gradient leakage.
However, the introduction of SecAgg necessitates the deployment of additional
sub-center servers for executing the multiparty computation protocol, thereby
escalating computational complexity and reducing system robustness, especially
in scenarios where one or more sub-centers are unavailable. To address these
challenges, we introduce a Markovian Switching-based distributed training
framework, the convergence of which is substantiated through rigorous
theoretical analysis. The Distributed Markovian Switching (DMS) topology shows
strong robustness towards the poisoning attacks as well. Case studies employing
real-world power system load data validate the efficacy of our proposed
algorithm. It not only significantly minimizes communication complexity but
also maintains accuracy levels comparable to traditional FL methods, thereby
enhancing the scalability of our load forecasting algorithm.
\\ ( https://arxiv.org/abs/2402.01546 ,  1762kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01567
Date: Fri, 2 Feb 2024 17:00:17 GMT   (77kb,D)

Title: Understanding Adam Optimizer via Online Learning of Updates: Adam is
  FTRL in Disguise
Authors: Kwangjun Ahn, Zhiyu Zhang, Yunbum Kook, Yan Dai
Categories: cs.LG math.OC
Comments: Comments would be appreciated!
\\
  Despite the success of the Adam optimizer in practice, the theoretical
understanding of its algorithmic components still remains limited. In
particular, most existing analyses of Adam show the convergence rate that can
be simply achieved by non-adative algorithms like SGD. In this work, we provide
a different perspective based on online learning that underscores the
importance of Adam's algorithmic components. Inspired by Cutkosky et al.
(2023), we consider the framework called online learning of updates, where we
choose the updates of an optimizer based on an online learner. With this
framework, the design of a good optimizer is reduced to the design of a good
online learner. Our main observation is that Adam corresponds to a principled
online learning framework called Follow-the-Regularized-Leader (FTRL). Building
on this observation, we study the benefits of its algorithmic components from
the online learning perspective.
\\ ( https://arxiv.org/abs/2402.01567 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01608
Date: Fri, 2 Feb 2024 18:14:16 GMT   (1452kb)

Title: Contingency Analysis of a Grid of Connected EVs for Primary Frequency
  Control of an Industrial Microgrid Using Efficient Control Scheme
Authors: J.N. Sabhahit, S.S. Solanke, V.K. Jadoun, H. Malik, F.P. Garc\'ia
  M\'arquez, J.M. Pinar-P\'erez
Categories: cs.LG cs.SY eess.SY
Comments: Published in energies (MDPI) 2022
Journal-ref: Energies 2022, 15, 3102
DOI: 10.3390/en15093102
\\
  After over a century of internal combustion engines ruling the transport
sector, electric vehicles appear to be on the verge of gaining traction due to
a slew of advantages, including lower operating costs and lower CO2 emissions.
By using the Vehicle-to-Grid (or Grid-to-Vehicle if Electric vehicles (EVs) are
utilized as load) approach, EVs can operate as both a load and a source.
Primary frequency regulation and congestion management are two essential
characteristics of this technology that are added to an industrial microgrid.
Industrial Microgrids are made up of different energy sources such as wind
farms and PV farms, storage systems, and loads. EVs have gained a lot of
interest as a technique for frequency management because of their ability to
regulate quickly. Grid reliability depends on this quick reaction. Different
contingency, state of charge of the electric vehicles, and a varying number of
EVs in an EV fleet are considered in this work, and a proposed control scheme
for frequency management is presented. This control scheme enables
bidirectional power flow, allowing for primary frequency regulation during the
various scenarios that an industrial microgrid may encounter over the course of
a 24-h period. The presented controller will provide dependable frequency
regulation support to the industrial microgrid during contingencies, as will be
demonstrated by simulation results, achieving a more reliable system. However,
simulation results will show that by increasing a number of the EVs in a fleet
for the Vehicle-to-Grid approach, an industrial microgrid\'s frequency can be
enhanced even further.
\\ ( https://arxiv.org/abs/2402.01608 ,  1452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01614
Date: Fri, 2 Feb 2024 18:24:37 GMT   (1416kb,D)

Title: L2G2G: a Scalable Local-to-Global Network Embedding with Graph
  Autoencoders
Authors: Ruikang Ouyang, Andrew Elliott, Stratis Limnios, Mihai Cucuringu,
  Gesine Reinert
Categories: cs.LG cs.AI cs.SI stat.ML
Comments: 13 pages, 4 figures, Complex Networks 2023, Volume I, SCI 1141
\\
  For analysing real-world networks, graph representation learning is a popular
tool. These methods, such as a graph autoencoder (GAE), typically rely on
low-dimensional representations, also called embeddings, which are obtained
through minimising a loss function; these embeddings are used with a decoder
for downstream tasks such as node classification and edge prediction. While
GAEs tend to be fairly accurate, they suffer from scalability issues. For
improved speed, a Local2Global approach, which combines graph patch embeddings
based on eigenvector synchronisation, was shown to be fast and achieve good
accuracy. Here we propose L2G2G, a Local2Global method which improves GAE
accuracy without sacrificing scalability. This improvement is achieved by
dynamically synchronising the latent node representations, while training the
GAEs. It also benefits from the decoder computing an only local patch loss.
Hence, aligning the local embeddings in each epoch utilises more information
from the graph than a single post-training alignment does, while maintaining
scalability. We illustrate on synthetic benchmarks, as well as real-world
examples, that L2G2G achieves higher accuracy than the standard Local2Global
approach and scales efficiently on the larger data sets. We find that for large
and dense networks, it even outperforms the slow, but assumed more accurate,
GAEs.
\\ ( https://arxiv.org/abs/2402.01614 ,  1416kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01621
Date: Fri, 2 Feb 2024 18:39:40 GMT   (3190kb,D)

Title: Stochastic Two Points Method for Deep Model Zeroth-order Optimization
Authors: Yijiang Pang, Jiayu Zhou
Categories: cs.LG
\\
  Large foundation models, such as large language models, have performed
exceptionally well in various application scenarios. Building or fully
fine-tuning such large models is usually prohibitive due to either hardware
budget or lack of access to backpropagation. The zeroth-order methods offer a
promising direction for tackling this challenge, where only forward passes are
needed to update the model. This paper introduces an efficient Stochastic
Two-Point (S2P) approach within the gradient-free regime. We present the
theoretical convergence properties of S2P under the general and relaxed
smoothness assumptions. The theoretical properties also shed light on a faster
and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new
convergence properties that better represent the dynamics of deep models in
training. Our comprehensive empirical results show that AS2P is highly
effective in optimizing objectives for large deep models, including language
models, and outperforms standard methods across various model types and scales,
with 2 $\times$ speed-up in training over most conducted tasks.
\\ ( https://arxiv.org/abs/2402.01621 ,  3190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01632
Date: Fri, 2 Feb 2024 18:52:16 GMT   (1882kb,D)

Title: Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown
  Hyperparameters Of Any Type
Authors: Juliusz Ziomek, Masaki Adachi, Michael A. Osborne
Categories: cs.LG stat.ML
\\
  Bayesian optimisation requires fitting a Gaussian process model, which in
turn requires specifying hyperparameters - most of the theoretical literature
assumes those hyperparameters are known. The commonly used maximum likelihood
estimator for hyperparameters of the Gaussian process is consistent only if the
data fills the space uniformly, which does not have to be the case in Bayesian
optimisation. Since no guarantees exist regarding the correctness of
hyperparameter estimation, and those hyperparameters can significantly affect
the Gaussian process fit, theoretical analysis of Bayesian optimisation with
unknown hyperparameters is very challenging. Previously proposed algorithms
with the no-regret property were only able to handle the special case of
unknown lengthscales, reproducing kernel Hilbert space norm and applied only to
the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the
first algorithm enjoying the no-regret property in the case of unknown
hyperparameters of arbitrary form, and which supports both Bayesian and
frequentist settings. Our proof idea is novel and can easily be extended to
other variants of Bayesian optimisation. We show this by extending our
algorithm to the adversarially robust optimisation setting under unknown
hyperparameters. Finally, we empirically evaluate our algorithm on a set of toy
problems and show that it can outperform the maximum likelihood estimator.
\\ ( https://arxiv.org/abs/2402.01632 ,  1882kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.00874 (*cross-listing*)
Date: Wed, 10 Jan 2024 17:21:20 GMT   (862kb)

Title: dRG-MEC: Decentralized Reinforced Green Offloading for MEC-enabled Cloud
  Network
Authors: Asad Aftab and Semeen Rehman
Categories: cs.NI cs.AI eess.SP
\\
  Multi-access-Mobile Edge Computing (MEC) is a promising solution for
computationally demanding rigorous applications, that can meet 6G network
service requirements. However, edge servers incur high computation costs during
task processing. In this paper, we proposed a technique to minimize the total
computation and communication overhead for optimal resource utilization with
joint computational offloading that enables a green environment. Our
optimization problem is NP-hard; thus, we proposed a decentralized
Reinforcement Learning (dRL) approach where we eliminate the problem of
dimensionality and over-estimation of the value functions. Compared to baseline
schemes our technique achieves a 37.03% reduction in total system costs.
\\ ( https://arxiv.org/abs/2402.00874 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00876 (*cross-listing*)
Date: Wed, 10 Jan 2024 19:43:52 GMT   (6340kb,D)

Title: Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud
Authors: Siavash Alamouti, Fay Arjomandi, Michel Burger and Dr. Bashar
  Altakouri
Categories: cs.NI cs.AI
\\
  As we transition from the mobile internet to the 'Cognitive Internet,' a
significant shift occurs in how we engage with technology and intelligence. We
contend that the Cognitive Internet goes beyond the Cognitive Internet of
Things (Cognitive IoT), enabling connected objects to independently acquire
knowledge and understanding. Unlike the Mobile Internet and Cognitive IoT, the
Cognitive Internet integrates collaborative intelligence throughout the
network, blending the cognitive IoT realm with system-wide collaboration and
human intelligence. This integrated intelligence facilitates interactions
between devices, services, entities, and individuals across diverse domains
while preserving decision-making autonomy and accommodating various identities.
  The paper delves into the foundational elements, distinct characteristics,
benefits, and industrial impact of the 'Cognitive Internet' paradigm. It
highlights the importance of adaptable AI infrastructures and hybrid edge cloud
(HEC) platforms in enabling this shift. This evolution brings forth cognitive
services, a Knowledge as a Service (KaaS) economy, enhanced decision-making
autonomy, sustainable digital progress, advancements in data management,
processing techniques, and a stronger emphasis on privacy. In essence, this
paper serves as a crucial resource for understanding and leveraging the
transformative potential of HEC for Cognitive Internet. Supported by case
studies, forward-looking perspectives, and real-world applications, it provides
comprehensive insights into this emerging paradigm.
\\ ( https://arxiv.org/abs/2402.00876 ,  6340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00881 (*cross-listing*)
Date: Sat, 20 Jan 2024 16:10:31 GMT   (2879kb,D)

Title: On the Interplay of Artificial Intelligence and Space-Air-Ground
  Integrated Networks: A Survey
Authors: Adilya Bakambekova, Nour Kouzayha and Tareq Al-Naffouri
Categories: cs.NI cs.AI
\\
  Space-Air-Ground Integrated Networks (SAGINs), which incorporate space and
aerial networks with terrestrial wireless systems, are vital enablers of the
emerging sixth-generation (6G) wireless networks. Besides bringing significant
benefits to various applications and services, SAGINs are envisioned to extend
high-speed broadband coverage to remote areas, such as small towns or mining
sites, or areas where terrestrial infrastructure cannot reach, such as
airplanes or maritime use cases. However, due to the limited power and storage
resources, as well as other constraints introduced by the design of terrestrial
networks, SAGINs must be intelligently configured and controlled to satisfy the
envisioned requirements. Meanwhile, Artificial Intelligence (AI) is another
critical enabler of 6G. Due to massive amounts of available data, AI has been
leveraged to address pressing challenges of current and future wireless
networks. By adding AI and facilitating the decision-making and prediction
procedures, SAGINs can effectively adapt to their surrounding environment, thus
enhancing the performance of various metrics. In this work, we aim to
investigate the interplay of AI and SAGINs by providing a holistic overview of
state-of-the-art research in AI-enabled SAGINs. Specifically, we present a
comprehensive overview of some potential applications of AI in SAGINs. We also
cover open issues in employing AI and detail the contributions of SAGINs in the
development of AI. Finally, we highlight some limitations of the existing
research works and outline potential future research directions.
\\ ( https://arxiv.org/abs/2402.00881 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00891 (*cross-listing*)
Date: Tue, 30 Jan 2024 16:55:25 GMT   (311kb,D)

Title: Large Language Models in Cybersecurity: State-of-the-Art
Authors: Farzad Nourmohammadzadeh Motlagh, Mehrdad Hajizadeh, Mehryar Majd,
  Pejman Najafi, Feng Cheng, Christoph Meinel
Categories: cs.CR cs.AI cs.CL cs.LG
\\
  The rise of Large Language Models (LLMs) has revolutionized our comprehension
of intelligence bringing us closer to Artificial Intelligence. Since their
introduction, researchers have actively explored the applications of LLMs
across diverse fields, significantly elevating capabilities. Cybersecurity,
traditionally resistant to data-driven solutions and slow to embrace machine
learning, stands out as a domain. This study examines the existing literature,
providing a thorough characterization of both defensive and adversarial
applications of LLMs within the realm of cybersecurity. Our review not only
surveys and categorizes the current landscape but also identifies critical
research gaps. By evaluating both offensive and defensive applications, we aim
to provide a holistic understanding of the potential risks and opportunities
associated with LLM-driven cybersecurity.
\\ ( https://arxiv.org/abs/2402.00891 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00892 (*cross-listing*)
Date: Wed, 31 Jan 2024 03:31:03 GMT   (1141kb,D)

Title: EVA-GAN: Enhanced Various Audio Generation via Scalable Generative
  Adversarial Networks
Authors: Shijia Liao, Shiyi Lan, Arun George Zachariah
Categories: cs.SD cs.AI cs.LG eess.AS
\\
  The advent of Large Models marks a new era in machine learning, significantly
outperforming smaller models by leveraging vast datasets to capture and
synthesize complex patterns. Despite these advancements, the exploration into
scaling, especially in the audio generation domain, remains limited, with
previous efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and
suffering from both spectral discontinuities and blurriness in the
high-frequency domain, alongside a lack of robustness against out-of-domain
data. These limitations restrict the applicability of models to diverse use
cases, including music and singing generation. Our work introduces Enhanced
Various Audio Generation via Scalable Generative Adversarial Networks
(EVA-GAN), yields significant improvements over previous state-of-the-art in
spectral and high-frequency reconstruction and robustness in out-of-domain data
performance, enabling the generation of HiFi audios by employing an extensive
dataset of 36,000 hours of 44.1kHz audio, a context-aware module, a
Human-In-The-Loop artifact measurement toolkit, and expands the model to
approximately 200 million parameters. Demonstrations of our work are available
at https://double-blind-eva-gan.cc.
\\ ( https://arxiv.org/abs/2402.00892 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00896 (*cross-listing*)
Date: Wed, 31 Jan 2024 13:30:20 GMT   (983kb,D)

Title: Privacy and Security Implications of Cloud-Based AI Services : A Survey
Authors: Alka Luqman, Riya Mahesh, Anupam Chattopadhyay
Categories: cs.CR cs.AI cs.LG
\\
  This paper details the privacy and security landscape in today's cloud
ecosystem and identifies that there is a gap in addressing the risks introduced
by machine learning models. As machine learning algorithms continue to evolve
and find applications across diverse domains, the need to categorize and
quantify privacy and security risks becomes increasingly critical. With the
emerging trend of AI-as-a-Service (AIaaS), machine learned AI models (or ML
models) are deployed on the cloud by model providers and used by model
consumers. We first survey the AIaaS landscape to document the various kinds of
liabilities that ML models, especially Deep Neural Networks pose and then
introduce a taxonomy to bridge this gap by holistically examining the risks
that creators and consumers of ML models are exposed to and their known
defences till date. Such a structured approach will be beneficial for ML model
providers to create robust solutions. Likewise, ML model consumers will find it
valuable to evaluate such solutions and understand the implications of their
engagement with such services. The proposed taxonomies provide a foundational
basis for solutions in private, secure and robust ML, paving the way for more
transparent and resilient AI systems.
\\ ( https://arxiv.org/abs/2402.00896 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00913 (*cross-listing*)
Date: Thu, 1 Feb 2024 10:58:10 GMT   (576kb,D)

Title: Institutional Platform for Secure Self-Service Large Language Model
  Exploration
Authors: V. K. Cody Bumgardner, Mitchell A. Klusty, W. Vaiden Logan, Samuel E.
  Armstrong, Caylin Hickey and Jeff Talbert
Categories: cs.CR cs.AI cs.CL
Comments: 10 pages 11 figures, 5 listings, 4 tables
\\
  This paper introduces a user-friendly platform developed by the University of
Kentucky Center for Applied AI, designed to make large, customized language
models (LLMs) more accessible. By capitalizing on recent advancements in
multi-LoRA inference, the system efficiently accommodates custom adapters for a
diverse range of users and projects. The paper outlines the system's
architecture and key features, encompassing dataset curation, model training,
secure inference, and text-based feature extraction.
  We illustrate the establishment of a tenant-aware computational network using
agent-based methods, securely utilizing islands of isolated resources as a
unified system. The platform strives to deliver secure LLM services,
emphasizing process and data isolation, end-to-end encryption, and role-based
resource authentication. This contribution aligns with the overarching goal of
enabling simplified access to cutting-edge AI models and technology in support
of scientific discovery.
\\ ( https://arxiv.org/abs/2402.00913 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00918 (*cross-listing*)
Date: Thu, 1 Feb 2024 13:47:23 GMT   (3779kb,D)

Title: MUSTAN: Multi-scale Temporal Context as Attention for Robust Video
  Foreground Segmentation
Authors: Praveen Kumar Pokala, Jaya Sai Kiran Patibandla, Naveen Kumar Pandey,
  and Balakrishna Reddy Pailla
Categories: cs.CV cs.AI
Comments: 10 pages, 8 figures
\\
  Video foreground segmentation (VFS) is an important computer vision task
wherein one aims to segment the objects under motion from the background. Most
of the current methods are image-based, i.e., rely only on spatial cues while
ignoring motion cues. Therefore, they tend to overfit the training data and
don't generalize well to out-of-domain (OOD) distribution. To solve the above
problem, prior works exploited several cues such as optical flow, background
subtraction mask, etc. However, having a video data with annotations like
optical flow is a challenging task. In this paper, we utilize the temporal
information and the spatial cues from the video data to improve OOD
performance. However, the challenge lies in how we model the temporal
information given the video data in an interpretable way creates a very
noticeable difference. We therefore devise a strategy that integrates the
temporal context of the video in the development of VFS. Our approach give rise
to deep learning architectures, namely MUSTAN1 and MUSTAN2 and they are based
on the idea of multi-scale temporal context as an attention, i.e., aids our
models to learn better representations that are beneficial for VFS. Further, we
introduce a new video dataset, namely Indoor Surveillance Dataset (ISD) for
VFS. It has multiple annotations on a frame level such as foreground binary
mask, depth map, and instance semantic annotations. Therefore, ISD can benefit
other computer vision tasks. We validate the efficacy of our architectures and
compare the performance with baselines. We demonstrate that proposed methods
significantly outperform the benchmark methods on OOD. In addition, the
performance of MUSTAN2 is significantly improved on certain video categories on
OOD data due to ISD.
\\ ( https://arxiv.org/abs/2402.00918 ,  3779kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00944 (*cross-listing*)
Date: Thu, 1 Feb 2024 19:00:55 GMT   (1368kb,D)

Title: NCoder -- A Quantum Field Theory approach to encoding data
Authors: David S. Berman, Marc S. Klinger, Alexander G. Stapleton
Categories: hep-th cond-mat.dis-nn cs.AI
\\
  In this paper we present a novel approach to interpretable AI inspired by
Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified
autoencoder neural network whose latent layer is prescribed to be a subset of
$n$-point correlation functions. Regarding images as draws from a lattice field
theory, this architecture mimics the task of perturbatively constructing the
effective action of the theory order by order in an expansion using Feynman
diagrams. Alternatively, the NCoder may be regarded as simulating the procedure
of statistical inference whereby high dimensional data is first summarized in
terms of several lower dimensional summary statistics (here the $n$-point
correlation functions), and subsequent out-of-sample data is generated by
inferring the data generating distribution from these statistics. In this way
the NCoder suggests a fascinating correspondence between perturbative
renormalizability and the sufficiency of models. We demonstrate the efficacy of
the NCoder by applying it to the generation of MNIST images, and find that
generated images can be correctly classified using only information from the
first three $n$-point functions of the image distribution.
\\ ( https://arxiv.org/abs/2402.00944 ,  1368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01002 (*cross-listing*)
Date: Thu, 1 Feb 2024 20:32:14 GMT   (11412kb,D)

Title: AI-generated faces free from racial and gender stereotypes
Authors: Nouar AlDahoul, Talal Rahwan, Yasir Zaki
Categories: cs.CV cs.AI
Comments: 26 pages, 6 figures
\\
  Text-to-image generative AI models such as Stable Diffusion are used daily by
millions worldwide. However, many have raised concerns regarding how these
models amplify racial and gender stereotypes. To study this phenomenon, we
develop a classifier to predict the race, gender, and age group of any given
face image, and show that it achieves state-of-the-art performance. Using this
classifier, we quantify biases in Stable Diffusion across six races, two
genders, five age groups, 32 professions, and eight attributes. We then propose
novel debiasing solutions that outperform state-of-the-art alternatives.
Additionally, we examine the degree to which Stable Diffusion depicts
individuals of the same race as being similar to one another. This analysis
reveals a high degree of stereotyping, e.g., depicting most middle eastern
males as being dark-skinned, bearded, and wearing a traditional headdress. We
address these limitations by proposing yet another novel solution that
increases facial diversity across genders and racial groups. Our solutions are
open-sourced and made publicly available.
\\ ( https://arxiv.org/abs/2402.01002 ,  11412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01020 (*cross-listing*)
Date: Thu, 1 Feb 2024 21:15:55 GMT   (142kb,D)

Title: Quantifying analogy of concepts via ologs and wiring diagrams
Authors: Jason Lo
Categories: cs.LO cs.AI cs.DM math.CO math.CT
Comments: 30 pages
MSC-class: 68T30 (Primary) 68T20, 68P05, 68T40 (Secondary)
ACM-class: I.2.4; I.2.8
\\
  We build on the theory of ontology logs (ologs) created by Spivak and Kent,
and define a notion of wiring diagrams. In this article, a wiring diagram is a
finite directed labelled graph. The labels correspond to types in an olog; they
can also be interpreted as readings of sensors in an autonomous system. As
such, wiring diagrams can be used as a framework for an autonomous system to
form abstract concepts. We show that the graphs underlying skeleton wiring
diagrams form a category. This allows skeleton wiring diagrams to be compared
and manipulated using techniques from both graph theory and category theory. We
also extend the usual definition of graph edit distance to the case of wiring
diagrams by using operations only available to wiring diagrams, leading to a
metric on the set of all skeleton wiring diagrams. In the end, we give an
extended example on calculating the distance between two concepts represented
by wiring diagrams, and explain how to apply our framework to any application
domain.
\\ ( https://arxiv.org/abs/2402.01020 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01134 (*cross-listing*)
Date: Fri, 2 Feb 2024 04:17:02 GMT   (19610kb,D)

Title: DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping
Authors: Zequan Chen, Jianping Li, Qusheng Li, Bisheng Yang, Zhen Dong
Categories: cs.CV cs.AI
\\
  Automated Aerial Triangulation (AAT), aiming to restore image pose and
reconstruct sparse points simultaneously, plays a pivotal role in earth
observation. With its rich research heritage spanning several decades in
photogrammetry, AAT has evolved into a fundamental process widely applied in
large-scale Unmanned Aerial Vehicle (UAV) based mapping. Despite its
advancements, classic AAT methods still face challenges like low efficiency and
limited robustness. This paper introduces DeepAAT, a deep learning network
designed specifically for AAT of UAV imagery. DeepAAT considers both spatial
and spectral characteristics of imagery, enhancing its capability to resolve
erroneous matching pairs and accurately predict image poses. DeepAAT marks a
significant leap in AAT's efficiency, ensuring thorough scene coverage and
precision. Its processing speed outpaces incremental AAT methods by hundreds of
times and global AAT methods by tens of times while maintaining a comparable
level of reconstruction accuracy. Additionally, DeepAAT's scene clustering and
merging strategy facilitate rapid localization and pose determination for
large-scale UAV images, even under constrained computing resources. The
experimental results demonstrate DeepAAT's substantial improvements over
conventional AAT methods, highlighting its potential in the efficiency and
accuracy of UAV-based 3D reconstruction tasks. To benefit the photogrammetry
society, the code of DeepAAT will be released at:
https://github.com/WHU-USI3DV/DeepAAT.
\\ ( https://arxiv.org/abs/2402.01134 ,  19610kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01145 (*cross-listing*)
Date: Fri, 2 Feb 2024 05:04:51 GMT   (172kb,D)

Title: ReEvo: Large Language Models as Hyper-Heuristics with Reflective
  Evolution
Authors: Haoran Ye, Jiarui Wang, Zhiguang Cao, Guojie Song
Categories: cs.NE cs.AI
\\
  The omnipresence of NP-hard combinatorial optimization problems (COPs)
compels domain experts to engage in trial-and-error heuristic design process.
The long-standing endeavor of design automation has gained new momentum with
the rise of large language models (LLMs). This paper introduces Language
Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages
LLMs for heuristic generation, featuring minimal manual intervention and
open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution
(ReEvo), a generic searching framework that emulates the reflective design
approach of human experts while far surpassing human capabilities with its
scalable LLM inference, Internet-scale domain knowledge, and powerful
evolutionary search. Evaluations across 12 COP settings show that 1) verbal
reflections for evolution lead to smoother fitness landscapes, explicit
inference of black-box COP settings, and better search results; 2) heuristics
generated by ReEvo in minutes can outperform state-of-the-art human designs and
neural solvers; 3) LHHs enable efficient algorithm design automation even when
challenged with black-box COPs, demonstrating its potential for complex and
novel real-world applications. Our code is available:
https://github.com/ai4co/LLM-as-HH.
\\ ( https://arxiv.org/abs/2402.01145 ,  172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01162 (*cross-listing*)
Date: Fri, 2 Feb 2024 06:05:18 GMT   (6235kb,D)

Title: 2AFC Prompting of Large Multimodal Models for Image Quality Assessment
Authors: Hanwei Zhu, Xiangjie Sui, Baoliang Chen, Xuelin Liu, Peilin Chen,
  Yuming Fang, and Shiqi Wang
Categories: cs.CV cs.AI
\\
  While abundant research has been conducted on improving high-level visual
understanding and reasoning capabilities of large multimodal models~(LMMs),
their visual quality assessment~(IQA) ability has been relatively
under-explored. Here we take initial steps towards this goal by employing the
two-alternative forced choice~(2AFC) prompting, as 2AFC is widely regarded as
the most reliable way of collecting human opinions of visual quality.
Subsequently, the global quality score of each image estimated by a particular
LMM can be efficiently aggregated using the maximum a posterior estimation.
Meanwhile, we introduce three evaluation criteria: consistency, accuracy, and
correlation, to provide comprehensive quantifications and deeper insights into
the IQA capability of five LMMs. Extensive experiments show that existing LMMs
exhibit remarkable IQA ability on coarse-grained quality comparison, but there
is room for improvement on fine-grained quality discrimination. The proposed
dataset sheds light on the future development of IQA models based on LMMs. The
codes will be made publicly available at https://github.com/h4nwei/2AFC-LMMs.
\\ ( https://arxiv.org/abs/2402.01162 ,  6235kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01166 (*cross-listing*)
Date: Fri, 2 Feb 2024 06:20:44 GMT   (140kb,D)

Title: A Comprehensive Survey on 3D Content Generation
Authors: Jian Liu, Xiaoshui Huang, Tianyu Huang, Lu Chen, Yuenan Hou, Shixiang
  Tang, Ziwei Liu, Wanli Ouyang, Wangmeng Zuo, Junjun Jiang, Xianming Liu
Categories: cs.CV cs.AI
\\
  Recent years have witnessed remarkable advances in artificial intelligence
generated content(AIGC), with diverse input modalities, e.g., text, image,
video, audio and 3D. The 3D is the most close visual modality to real-world 3D
environment and carries enormous knowledge. The 3D content generation shows
both academic and practical values while also presenting formidable technical
challenges. This review aims to consolidate developments within the burgeoning
domain of 3D content generation. Specifically, a new taxonomy is proposed that
categorizes existing approaches into three types: 3D native generative methods,
2D prior-based 3D generative methods, and hybrid 3D generative methods. The
survey covers approximately 60 papers spanning the major techniques. Besides,
we discuss limitations of current 3D content generation techniques, and point
out open challenges as well as promising directions for future work.
Accompanied with this survey, we have established a project website where the
resources on 3D content generation research are provided. The project page is
available at https://github.com/hitcslj/Awesome-AIGC-3D.
\\ ( https://arxiv.org/abs/2402.01166 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01169 (*cross-listing*)
Date: Fri, 2 Feb 2024 06:23:00 GMT   (23kb)

Title: Faster Inference of Integer SWIN Transformer by Removing the GELU
  Activation
Authors: Mohammadreza Tayaranian, Seyyed Hasan Mozafari, James J. Clark, Brett
  Meyer, Warren Gross
Categories: cs.CV cs.AI
Comments: 5 pages, 1 figure. Submitted to Edge Intelligence Workshop III, an
  AAAI 2024 workshop
\\
  SWIN transformer is a prominent vision transformer model that has
state-of-the-art accuracy in image classification tasks. Despite this success,
its unique architecture causes slower inference compared with similar deep
neural networks. Integer quantization of the model is one of the methods used
to improve its inference latency. However, state-of-the-art has not been able
to fully quantize the model. In this work, we improve upon the inference
latency of the state-of-the-art methods by removing the floating-point
operations, which are associated with the GELU activation in Swin Transformer.
While previous work proposed to replace the non-integer operations with linear
approximation functions, we propose to replace GELU with ReLU activation. The
advantage of ReLU over previous methods is its low memory and computation
complexity. We use iterative knowledge distillation to compensate for the lost
accuracy due to replacing GELU with ReLU. We quantize our GELU-less SWIN
transformer and show that on an RTX 4090 NVIDIA GPU we can improve the
inference latency of the quantized SWIN transformer by at least $11\%$ while
maintaining an accuracy drop of under $0.5\%$ on the ImageNet evaluation
dataset.
\\ ( https://arxiv.org/abs/2402.01169 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01219 (*cross-listing*)
Date: Fri, 2 Feb 2024 08:41:15 GMT   (109kb,D)

Title: AI Code Generators for Security: Friend or Foe?
Authors: Roberto Natella, Pietro Liguori, Cristina Improta, Bojan Cukic,
  Domenico Cotroneo
Categories: cs.CR cs.AI cs.SE
Comments: Dataset available at: https://github.com/dessertlab/violent-python
Journal-ref: IEEE Security & Privacy, Early Access, February 2024
DOI: 10.1109/MSEC.2024.3355713
\\
  Recent advances of artificial intelligence (AI) code generators are opening
new opportunities in software security research, including misuse by malicious
actors. We review use cases for AI code generators for security and introduce
an evaluation benchmark.
\\ ( https://arxiv.org/abs/2402.01219 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01227 (*cross-listing*)
Date: Fri, 2 Feb 2024 08:46:57 GMT   (3551kb,D)

Title: STAA-Net: A Sparse and Transferable Adversarial Attack for Speech
  Emotion Recognition
Authors: Yi Chang, Zhao Ren, Zixing Zhang, Xin Jing, Kun Qian, Xi Shao, Bin Hu,
  Tanja Schultz, Bj\"orn W. Schuller
Categories: cs.SD cs.AI cs.HC eess.AS
\\
  Speech contains rich information on the emotions of humans, and Speech
Emotion Recognition (SER) has been an important topic in the area of
human-computer interaction. The robustness of SER models is crucial,
particularly in privacy-sensitive and reliability-demanding domains like
private healthcare. Recently, the vulnerability of deep neural networks in the
audio domain to adversarial attacks has become a popular area of research.
However, prior works on adversarial attacks in the audio domain primarily rely
on iterative gradient-based techniques, which are time-consuming and prone to
overfitting the specific threat model. Furthermore, the exploration of sparse
perturbations, which have the potential for better stealthiness, remains
limited in the audio domain. To address these challenges, we propose a
generator-based attack method to generate sparse and transferable adversarial
examples to deceive SER models in an end-to-end and efficient manner. We
evaluate our method on two widely-used SER datasets, Database of Elicited Mood
in Speech (DEMoS) and Interactive Emotional dyadic MOtion CAPture (IEMOCAP),
and demonstrate its ability to generate successful sparse adversarial examples
in an efficient manner. Moreover, our generated adversarial examples exhibit
model-agnostic transferability, enabling effective adversarial attacks on
advanced victim models.
\\ ( https://arxiv.org/abs/2402.01227 ,  3551kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01239 (*cross-listing*)
Date: Fri, 2 Feb 2024 09:07:00 GMT   (156kb,D)

Title: PRIME: Protect Your Videos From Malicious Editing
Authors: Guanlin Li, Shuai Yang, Jie Zhang, Tianwei Zhang
Categories: cs.CV cs.AI
\\
  With the development of generative models, the quality of generated content
keeps increasing. Recently, open-source models have made it surprisingly easy
to manipulate and edit photos and videos, with just a few simple prompts. While
these cutting-edge technologies have gained popularity, they have also given
rise to concerns regarding the privacy and portrait rights of individuals.
Malicious users can exploit these tools for deceptive or illegal purposes.
Although some previous works focus on protecting photos against generative
models, we find there are still gaps between protecting videos and images in
the aspects of efficiency and effectiveness. Therefore, we introduce our
protection method, PRIME, to significantly reduce the time cost and improve the
protection performance. Moreover, to evaluate our proposed protection method,
we consider both objective metrics and human subjective metrics. Our evaluation
results indicate that PRIME only costs 8.3% GPU hours of the cost of the
previous state-of-the-art method and achieves better protection results on both
human evaluation and objective metrics. Code can be found in
https://github.com/GuanlinLee/prime.
\\ ( https://arxiv.org/abs/2402.01239 ,  156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01240 (*cross-listing*)
Date: Fri, 2 Feb 2024 09:07:09 GMT   (2295kb,D)

Title: Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser
  Web Tracker Classification in an Imbalanced Setting
Authors: Wolf Rieder, Philip Raschke, Thomas Cory
Categories: cs.CR cs.AI cs.LG
\\
  The World Wide Web's connectivity is greatly attributed to the HTTP protocol,
with HTTP messages offering informative header fields that appeal to
disciplines like web security and privacy, especially concerning web tracking.
Despite existing research employing HTTP/S request messages to identify web
trackers, HTTP/S response headers are often overlooked. This study endeavors to
design effective machine learning classifiers for web tracker detection using
HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers,
obtained through the traffic monitoring browser extension T.EX, serves as our
data set. Eleven supervised models were trained on Chrome data and tested
across all browsers. The results demonstrated high accuracy, F1-score,
precision, recall, and minimal log-loss error for Chrome and Firefox, but
subpar performance on Brave, potentially due to its distinct data distribution
and feature set. The research suggests that these classifiers are viable for
detecting web trackers in Chrome and Firefox. However, real-world application
testing remains pending, and the distinction between tracker types and broader
label sources could be explored in future studies.
\\ ( https://arxiv.org/abs/2402.01240 ,  2295kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01241 (*cross-listing*)
Date: Fri, 2 Feb 2024 09:09:23 GMT   (31529kb,D)

Title: Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D
  Diffusion?
Authors: Cristian Sbrolli, Paolo Cudrano, Matteo Matteucci
Categories: cs.CV cs.AI
Comments: 10 pages, 6 figures
\\
  Recent advancements in deep generative models, particularly with the
application of CLIP (Contrastive Language Image Pretraining) to Denoising
Diffusion Probabilistic Models (DDPMs), have demonstrated remarkable
effectiveness in text to image generation. The well structured embedding space
of CLIP has also been extended to image to shape generation with DDPMs,
yielding notable results. Despite these successes, some fundamental questions
arise: Does CLIP ensure the best results in shape generation from images? Can
we leverage conditioning to bring explicit 3D knowledge into the generative
process and obtain better quality? This study introduces CISP (Contrastive
Image Shape Pre training), designed to enhance 3D shape synthesis guided by 2D
images. CISP aims to enrich the CLIP framework by aligning 2D images with 3D
shapes in a shared embedding space, specifically capturing 3D characteristics
potentially overlooked by CLIP's text image focus. Our comprehensive analysis
assesses CISP's guidance performance against CLIP guided models, focusing on
generation quality, diversity, and coherence of the produced shapes with the
conditioning image. We find that, while matching CLIP in generation quality and
diversity, CISP substantially improves coherence with input images,
underscoring the value of incorporating 3D knowledge into generative models.
These findings suggest a promising direction for advancing the synthesis of 3D
visual content by integrating multimodal systems with 3D representations.
\\ ( https://arxiv.org/abs/2402.01241 ,  31529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01259 (*cross-listing*)
Date: Fri, 2 Feb 2024 09:30:27 GMT   (2390kb,D)

Title: Position Aware 60 GHz mmWave Beamforming for V2V Communications
  Utilizing Deep Learning
Authors: Muhammad Baqer Mollah, Honggang Wang, and Hua Fang
Categories: cs.NI cs.AI cs.LG cs.SI cs.SY eess.SY
Comments: 2024 IEEE International Conference on Communications (ICC), Denver,
  CO, USA
\\
  Beamforming techniques are considered as essential parts to compensate the
severe path loss in millimeter-wave (mmWave) communications by adopting large
antenna arrays and formulating narrow beams to obtain satisfactory received
powers. However, performing accurate beam alignment over such narrow beams for
efficient link configuration by traditional beam selection approaches, mainly
relied on channel state information, typically impose significant latency and
computing overheads, which is often infeasible in vehicle-to-vehicle (V2V)
communications like highly dynamic scenarios. In contrast, utilizing
out-of-band contextual information, such as vehicular position information, is
a potential alternative to reduce such overheads. In this context, this paper
presents a deep learning-based solution on utilizing the vehicular position
information for predicting the optimal beams having sufficient mmWave received
powers so that the best V2V line-of-sight links can be ensured proactively.
After experimental evaluation of the proposed solution on real-world measured
mmWave sensing and communications datasets, the results show that the solution
can achieve up to 84.58% of received power of link status on average, which
confirm a promising solution for beamforming in mmWave at 60 GHz enabled V2V
communications.
\\ ( https://arxiv.org/abs/2402.01259 ,  2390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01298 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:39:58 GMT   (1395kb,D)

Title: Learning Semantic Information from Raw Audio Signal Using Both
  Contextual and Phonetic Representations
Authors: Jaeyeon Kim, Injune Hwang, Kyogu Lee
Categories: eess.AS cs.AI cs.SD
Comments: Accepted to ICASSP 2024
\\
  We propose a framework to learn semantics from raw audio signals using two
types of representations, encoding contextual and phonetic information
respectively. Specifically, we introduce a speech-to-unit processing pipeline
that captures two types of representations with different time resolutions. For
the language model, we adopt a dual-channel architecture to incorporate both
types of representation. We also present new training objectives, masked
context reconstruction and masked context prediction, that push models to learn
semantics effectively. Experiments on the sSIMI metric of Zero Resource Speech
Benchmark 2021 and Fluent Speech Command dataset show our framework learns
semantics better than models trained with only one type of representation.
\\ ( https://arxiv.org/abs/2402.01298 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01335 (*cross-listing*)
Date: Fri, 2 Feb 2024 11:40:27 GMT   (12814kb,D)

Title: Simulator-Free Visual Domain Randomization via Video Games
Authors: Chintan Trivedi, Nemanja Ra\v{s}ajski, Konstantinos Makantasis,
  Antonios Liapis and Georgios N. Yannakakis
Categories: cs.CV cs.AI
\\
  Domain randomization is an effective computer vision technique for improving
transferability of vision models across visually distinct domains exhibiting
similar content. Existing approaches, however, rely extensively on tweaking
complex and specialized simulation engines that are difficult to construct,
subsequently affecting their feasibility and scalability. This paper introduces
BehAVE, a video understanding framework that uniquely leverages the plethora of
existing commercial video games for domain randomization, without requiring
access to their simulation engines. Under BehAVE (1) the inherent rich visual
diversity of video games acts as the source of randomization and (2) player
behavior -- represented semantically via textual descriptions of actions --
guides the *alignment* of videos with similar content. We test BehAVE on 25
games of the first-person shooter (FPS) genre across various video and text
foundation models and we report its robustness for domain randomization. BehAVE
successfully aligns player behavioral patterns and is able to zero-shot
transfer them to multiple unseen FPS games when trained on just one FPS game.
In a more challenging setting, BehAVE manages to improve the zero-shot
transferability of foundation models to unseen FPS games (up to 22%) even when
trained on a game of a different genre (Minecraft). Code and dataset can be
found at https://github.com/nrasajski/BehAVE.
\\ ( https://arxiv.org/abs/2402.01335 ,  12814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01345 (*cross-listing*)
Date: Fri, 2 Feb 2024 12:02:46 GMT   (388kb,D)

Title: Skip $\textbackslash n$: A simple method to reduce hallucination in
  Large Vision-Language Models
Authors: Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike
  Zheng Shou
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Technical Report
\\
  Recent advancements in large vision-language models (LVLMs) have demonstrated
impressive capability in visual information understanding with human language.
Despite these advances, LVLMs still face challenges with multimodal
hallucination, such as generating text descriptions of objects that are not
present in the visual information. However, the underlying fundamental reasons
of multimodal hallucinations remain poorly explored. In this paper, we propose
a new perspective, suggesting that the inherent biases in LVLMs might be a key
factor in hallucinations. Specifically, we systematically identify a semantic
shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'),
where the content before and after '$\textbackslash n\textbackslash n$' in the
training data frequently exhibit significant semantic changes. This pattern
leads the model to infer that the contents following '$\textbackslash
n\textbackslash n$' should be obviously different from the preceding contents
with less hallucinatory descriptions, thereby increasing the probability of
hallucinatory descriptions subsequent to the '$\textbackslash n\textbackslash
n$'. We have validated this hypothesis on multiple publicly available LVLMs.
Besides, we find that deliberately inserting '$\textbackslash n\textbackslash
n$' at the generated description can induce more hallucinations. A simple
method is proposed to effectively mitigate the hallucination of LVLMs by
skipping the output of `\textbackslash n'.
\\ ( https://arxiv.org/abs/2402.01345 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01353 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:13:09 GMT   (76kb)

Title: Efficient compilation of expressive problem space specifications to
  neural network solvers
Authors: Matthew L. Daggitt, Wen Kokke, Robert Atkey
Categories: cs.LO cs.AI cs.LG
\\
  Recent work has described the presence of the embedding gap in neural network
verification. On one side of the gap is a high-level specification about the
network's behaviour, written by a domain expert in terms of the interpretable
problem space. On the other side are a logically-equivalent set of
satisfiability queries, expressed in the uninterpretable embedding space in a
form suitable for neural network solvers. In this paper we describe an
algorithm for compiling the former to the latter. We explore and overcome
complications that arise from targeting neural network solvers as opposed to
standard SMT solvers.
\\ ( https://arxiv.org/abs/2402.01353 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01355 (*cross-listing*)
Date: Fri, 2 Feb 2024 12:22:41 GMT   (3631kb,D)

Title: FindingEmo: An Image Dataset for Emotion Recognition in the Wild
Authors: Laurent Mertens and Elahe' Yargholi and Hans Op de Beeck and Jan Van
  den Stock and Joost Vennekens
Categories: cs.CV cs.AI
Comments: 30 pages, 21 figures, 12 tables
\\
  We introduce FindingEmo, a new image dataset containing annotations for 25k
images, specifically tailored to Emotion Recognition. Contrary to existing
datasets, it focuses on complex scenes depicting multiple people in various
naturalistic, social settings, with images being annotated as a whole, thereby
going beyond the traditional focus on faces or single individuals. Annotated
dimensions include Valence, Arousal and Emotion label, with annotations
gathered using Prolific. Together with the annotations, we release the list of
URLs pointing to the original images, as well as all associated source code.
\\ ( https://arxiv.org/abs/2402.01355 ,  3631kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01410 (*cross-listing*)
Date: Fri, 2 Feb 2024 13:42:45 GMT   (5202kb,D)

Title: XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision
Authors: Miguel Correia, Alceu Bissoto, Carlos Santiago, Catarina Barata
Categories: cs.CV cs.AI cs.LG
Comments: Accepted in the iMIMIC Workshop @ MICCAI 2023
\\
  Skin cancer detection through dermoscopy image analysis is a critical task.
However, existing models used for this purpose often lack interpretability and
reliability, raising the concern of physicians due to their black-box nature.
In this paper, we propose a novel approach for the diagnosis of melanoma using
an interpretable prototypical-part model. We introduce a guided supervision
based on non-expert feedback through the incorporation of: 1) binary masks,
obtained automatically using a segmentation network; and 2) user-refined
prototypes. These two distinct information pathways aim to ensure that the
learned prototypes correspond to relevant areas within the skin lesion,
excluding confounding factors beyond its boundaries. Experimental results
demonstrate that, even without expert supervision, our approach achieves
superior performance and generalization compared to non-interpretable models.
\\ ( https://arxiv.org/abs/2402.01410 ,  5202kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01446 (*cross-listing*)
Date: Fri, 2 Feb 2024 14:38:04 GMT   (8700kb,D)

Title: Guidance Graph Optimization for Lifelong Multi-Agent Path Finding
Authors: Yulun Zhang, He Jiang, Varun Bhatt, Stefanos Nikolaidis, Jiaoyang Li
Categories: cs.MA cs.AI cs.RO
\\
  We study how to use guidance to improve the throughput of lifelong
Multi-Agent Path Finding (MAPF). Previous studies have demonstrated that while
incorporating guidance, such as highways, can accelerate MAPF algorithms, this
often results in a trade-off with solution quality. In addition, how to
generate good guidance automatically remains largely unexplored, with current
methods falling short of surpassing manually designed ones. In this work, we
introduce the directed guidance graph as a versatile representation of guidance
for lifelong MAPF, framing Guidance Graph Optimization (GGO) as the task of
optimizing its edge weights. We present two GGO algorithms to automatically
generate guidance for arbitrary lifelong MAPF algorithms and maps. The first
method directly solves GGO by employing CMA-ES, a black-box optimization
algorithm. The second method, PIU, optimizes an update model capable of
generating guidance, demonstrating the ability to transfer optimized guidance
graphs to larger maps with similar layouts. Empirically, we show that (1) our
guidance graphs improve the throughput of three representative lifelong MAPF
algorithms in four benchmark maps, and (2) our update model can generate
guidance graphs for as large as $93 \times 91$ maps and as many as 3000 agents.
\\ ( https://arxiv.org/abs/2402.01446 ,  8700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01467 (*cross-listing*)
Date: Fri, 2 Feb 2024 14:55:51 GMT   (22016kb,D)

Title: Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents
Authors: Jiyi Wang, Likai Tang, Huimiao Chen, Sen Song
Categories: eess.SY cs.AI cs.CE cs.NE cs.SY q-bio.NC
\\
  Can replay, as a widely observed neural activity pattern in brain regions,
particularly in the hippocampus and neocortex, emerge in an artificial agent?
If yes, does it contribute to the tasks? In this work, without heavy dependence
on complex assumptions, we discover naturally emergent replay under
task-optimized paradigm using a recurrent neural network-based reinforcement
learning model, which mimics the hippocampus and prefrontal cortex, as well as
their intercommunication and the sensory cortex input. The emergent replay in
the hippocampus, which results from the episodic memory and cognitive map as
well as environment observations, well resembles animal experimental data and
serves as an effective indicator of high task performance. The model also
successfully reproduces local and nonlocal replay, which matches the human
experimental data. Our work provides a new avenue for understanding the
mechanisms behind replay.
\\ ( https://arxiv.org/abs/2402.01467 ,  22016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01536 (*cross-listing*)
Date: Fri, 2 Feb 2024 16:27:11 GMT   (454kb,D)

Title: Homogenization Effects of Large Language Models on Human Creative
  Ideation
Authors: Barrett R. Anderson, Jash Hemant Shah, Max Kreminski
Categories: cs.HC cs.AI
Comments: 20 pages, 7 figures
\\
  Large language models (LLMs) are now being used in a wide variety of
contexts, including as creativity support tools (CSTs) intended to help their
users come up with new ideas. But do LLMs actually support user creativity? We
hypothesized that the use of an LLM as a CST might make the LLM's users feel
more creative, and even broaden the range of ideas suggested by each individual
user, but also homogenize the ideas suggested by different users. We conducted
a 36-participant comparative user study and found, in accordance with the
homogenization hypothesis, that different users tended to produce less
semantically distinct ideas with ChatGPT than with an alternative CST.
Additionally, ChatGPT users generated a greater number of more detailed ideas,
but felt less responsible for the ideas they generated. We discuss potential
implications of these findings for users, designers, and developers of
LLM-based CSTs.
\\ ( https://arxiv.org/abs/2402.01536 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01537 (*cross-listing*)
Date: Fri, 2 Feb 2024 16:27:45 GMT   (3683kb,D)

Title: Closing the Gap in Human Behavior Analysis: A Pipeline for Synthesizing
  Trimodal Data
Authors: Christian Stippel, Thomas Heitzinger, Rafael Sterzinger, Martin Kampel
Categories: cs.CV cs.AI cs.LG
\\
  In pervasive machine learning, especially in Human Behavior Analysis (HBA),
RGB has been the primary modality due to its accessibility and richness of
information. However, linked with its benefits are challenges, including
sensitivity to lighting conditions and privacy concerns. One possibility to
overcome these vulnerabilities is to resort to different modalities. For
instance, thermal is particularly adept at accentuating human forms, while
depth adds crucial contextual layers. Despite their known benefits, only a few
HBA-specific datasets that integrate these modalities exist. To address this
shortage, our research introduces a novel generative technique for creating
trimodal, i.e., RGB, thermal, and depth, human-focused datasets. This technique
capitalizes on human segmentation masks derived from RGB images, combined with
thermal and depth backgrounds that are sourced automatically. With these two
ingredients, we synthesize depth and thermal counterparts from existing RGB
data utilizing conditional image-to-image translation. By employing this
approach, we generate trimodal data that can be leveraged to train models for
settings with limited data, bad lightning conditions, or privacy-sensitive
areas.
\\ ( https://arxiv.org/abs/2402.01537 ,  3683kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01566 (*cross-listing*)
Date: Fri, 2 Feb 2024 16:59:48 GMT   (16468kb,D)

Title: Boximator: Generating Rich and Controllable Motions for Video Synthesis
Authors: Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping
  Yuan, Hang Li
Categories: cs.CV cs.AI
Comments: 16 pages, 9 figures
\\
  Generating rich and controllable motion is a pivotal challenge in video
synthesis. We propose Boximator, a new approach for fine-grained motion
control. Boximator introduces two constraint types: hard box and soft box.
Users select objects in the conditional frame using hard boxes and then use
either type of boxes to roughly or rigorously define the object's position,
shape, or motion path in future frames. Boximator functions as a plug-in for
existing video diffusion models. Its training process preserves the base
model's knowledge by freezing the original weights and training only the
control module. To address training challenges, we introduce a novel
self-tracking technique that greatly simplifies the learning of box-object
correlations. Empirically, Boximator achieves state-of-the-art video quality
(FVD) scores, improving on two base models, and further enhanced after
incorporating box constraints. Its robust motion controllability is validated
by drastic increases in the bounding box alignment metric. Human evaluation
also shows that users favor Boximator generation results over the base model.
\\ ( https://arxiv.org/abs/2402.01566 ,  16468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01580 (*cross-listing*)
Date: Fri, 2 Feb 2024 17:19:20 GMT   (3108kb,D)

Title: Generative AI for Education (GAIED): Advances, Opportunities, and
  Challenges
Authors: Paul Denny, Sumit Gulwani, Neil T. Heffernan, Tanja K\"aser, Steven
  Moore, Anna N. Rafferty, Adish Singla
Categories: cs.CY cs.AI
\\
  This survey article has grown out of the GAIED (pronounced "guide") workshop
organized by the authors at the NeurIPS 2023 conference. We organized the GAIED
workshop as part of a community-building effort to bring together researchers,
educators, and practitioners to explore the potential of generative AI for
enhancing education. This article aims to provide an overview of the workshop
activities and highlight several future research directions in the area of
GAIED.
\\ ( https://arxiv.org/abs/2402.01580 ,  3108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01591 (*cross-listing*)
Date: Fri, 2 Feb 2024 17:34:53 GMT   (4020kb,D)

Title: BAT: Learning to Reason about Spatial Sounds with Large Language Models
Authors: Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol Choi, David
  Harwath
Categories: eess.AS cs.AI cs.CL cs.SD
Comments: Preprint, work in progress
\\
  Spatial sound reasoning is a fundamental human skill, enabling us to navigate
and interpret our surroundings based on sound. In this paper we present BAT,
which combines the spatial sound perception ability of a binaural acoustic
scene analysis model with the natural language reasoning capabilities of a
large language model (LLM) to replicate this innate ability. To address the
lack of existing datasets of in-the-wild spatial sounds, we synthesized a
binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed
SpatialSoundQA, a spatial sound-based question-answering dataset, offering a
range of QA tasks that train BAT in various aspects of spatial sound perception
and reasoning. The acoustic front end encoder of BAT is a novel spatial audio
encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by
itself achieves strong performance across sound event detection, spatial
localization, and distance estimation. By integrating Spatial-AST with LLaMA-2
7B model, BAT transcends standard Sound Event Localization and Detection (SELD)
tasks, enabling the model to reason about the relationships between the sounds
in its environment. Our experiments demonstrate BAT's superior performance on
both spatial sound perception and reasoning, showcasing the immense potential
of LLMs in navigating and interpreting complex spatial audio environments.
\\ ( https://arxiv.org/abs/2402.01591 ,  4020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00898 (*cross-listing*)
Date: Wed, 31 Jan 2024 19:52:00 GMT   (464kb,D)

Title: An Early Categorization of Prompt Injection Attacks on Large Language
  Models
Authors: Sippo Rossi, Alisia Marianne Michel, Raghava Rao Mukkamala and Jason
  Bennett Thatcher
Categories: cs.CR cs.CL cs.LG
Comments: 21 pages double spacing
\\
  Large language models and AI chatbots have been at the forefront of
democratizing artificial intelligence. However, the releases of ChatGPT and
other similar tools have been followed by growing concerns regarding the
difficulty of controlling large language models and their outputs. Currently,
we are witnessing a cat-and-mouse game where users attempt to misuse the models
with a novel attack called prompt injections. In contrast, the developers
attempt to discover the vulnerabilities and block the attacks simultaneously.
In this paper, we provide an overview of these emergent threats and present a
categorization of prompt injections, which can guide future research on prompt
injections and act as a checklist of vulnerabilities in the development of LLM
interfaces. Moreover, based on previous literature and our own empirical
research, we discuss the implications of prompt injections to LLM end users,
developers, and researchers.
\\ ( https://arxiv.org/abs/2402.00898 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01135 (*cross-listing*)
Date: Fri, 2 Feb 2024 04:20:13 GMT   (1029kb,D)

Title: A Multi-Agent Conversational Recommender System
Authors: Jiabao Fang, Shen Gao, Pengjie Ren, Xiuying Chen, Suzan Verberne,
  Zhaochun Ren
Categories: cs.IR cs.CL
\\
  Due to strong capabilities in conducting fluent, multi-turn conversations
with users, Large Language Models (LLMs) have the potential to further improve
the performance of Conversational Recommender System (CRS). Unlike the aimless
chit-chat that LLM excels at, CRS has a clear target. So it is imperative to
control the dialogue flow in the LLM to successfully recommend appropriate
items to the users. Furthermore, user feedback in CRS can assist the system in
better modeling user preferences, which has been ignored by existing studies.
However, simply prompting LLM to conduct conversational recommendation cannot
address the above two key challenges.
  In this paper, we propose Multi-Agent Conversational Recommender System
(MACRS) which contains two essential modules. First, we design a multi-agent
act planning framework, which can control the dialogue flow based on four
LLM-based agents. This cooperative multi-agent framework will generate various
candidate responses based on different dialogue acts and then choose the most
appropriate response as the system response, which can help MACRS plan suitable
dialogue acts. Second, we propose a user feedback-aware reflection mechanism
which leverages user feedback to reason errors made in previous turns to adjust
the dialogue act planning, and higher-level user information from implicit
semantics. We conduct extensive experiments based on user simulator to
demonstrate the effectiveness of MACRS in recommendation and user preferences
collection. Experimental results illustrate that MACRS demonstrates an
improvement in user interaction experience compared to directly using LLMs.
\\ ( https://arxiv.org/abs/2402.01135 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01391 (*cross-listing*)
Date: Fri, 2 Feb 2024 13:14:31 GMT   (323kb,D)

Title: StepCoder: Improve Code Generation with Reinforcement Learning from
  Compiler Feedback
Authors: Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Junjie
  Shan, Caishuang Huang, Wei Shen, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji,
  Rui Zheng, Qi Zhang, Xuanjing Huang, Tao Gui
Categories: cs.SE cs.CL
Comments: 13 pages, 5 figures
\\
  The advancement of large language models (LLMs) has significantly propelled
the field of code generation. Previous work integrated reinforcement learning
(RL) with compiler feedback for exploring the output space of LLMs to enhance
code generation quality. However, the lengthy code generated by LLMs in
response to complex human requirements makes RL exploration a challenge. Also,
since the unit tests may not cover the complicated code, optimizing LLMs by
using these unexecuted code snippets is ineffective. To tackle these
challenges, we introduce StepCoder, a novel RL framework for code generation,
consisting of two main components: CCCS addresses the exploration challenge by
breaking the long sequences code generation task into a Curriculum of Code
Completion Subtasks, while FGO only optimizes the model by masking the
unexecuted code segments to provide Fine-Grained Optimization. In addition, we
furthermore construct the APPS+ dataset for RL training, which is manually
verified to ensure the correctness of unit tests. Experimental results show
that our method improves the ability to explore the output space and
outperforms state-of-the-art approaches in corresponding benchmarks.
\\ ( https://arxiv.org/abs/2402.01391 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01577 (*cross-listing*)
Date: Fri, 2 Feb 2024 17:16:23 GMT   (3174kb,D)

Title: Deep Active Learning for Data Mining from Conflict Text Corpora
Authors: Mihai Croicu
Categories: cs.CY cs.CL stat.ML
Comments: 40 pages, 6 figures. Paper presented at the Using LLMs and
  Text-as-Data in Political Science Research Workshop at the University of
  Barcelona, 29 January 2024
\\
  High-resolution event data on armed conflict and related processes have
revolutionized the study of political contention with datasets like UCDP GED,
ACLED etc. However, most of these datasets limit themselves to collecting
spatio-temporal (high-resolution) and intensity data. Information on dynamics,
such as targets, tactics, purposes etc. are rarely collected owing to the
extreme workload of collecting data. However, most datasets rely on a rich
corpus of textual data allowing further mining of further information connected
to each event. This paper proposes one such approach that is inexpensive and
high performance, leveraging active learning - an iterative process of
improving a machine learning model based on sequential (guided) human input.
Active learning is employed to then step-wise train (fine-tuning) of a large,
encoder-only language model adapted for extracting sub-classes of events
relating to conflict dynamics. The approach shows performance similar to human
(gold-standard) coding while reducing the amount of required human annotation
by as much as 99%.
\\ ( https://arxiv.org/abs/2402.01577 ,  3174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01579 (*cross-listing*)
Date: Fri, 2 Feb 2024 17:17:42 GMT   (773kb)

Title: How Paralingual are Paralinguistic Representations? A Case Study in
  Speech Emotion Recognition
Authors: Orchid Chetia Phukan, Gautam Siddharth Kashyap, Arun Balaji Buduru,
  Rajesh Sharma
Categories: eess.AS cs.CL cs.SD
\\
  Pre-trained Models (PTMs) have facilitated substantial progress in the field
of Speech Emotion Recognition (SER). SER is an area with applications ranging
from HumanComputer Interaction to Healthcare. Recent studies have leveraged
various PTM representations as input features for downstream models for SER.
PTM specifically pre-trained for paralinguistic tasks have obtained
state-of-the-art (SOTA) performance for SER. However, such PTM haven't been
evaluated for SER in multilingual settings and experimented only with English.
So, we fill this gap, by performing a comprehensive comparative study of five
PTMs (TRILLsson, wav2vec2, XLS-R, x-vector, Whisper) for assessing the
effectiveness of paralingual PTM (TRILLsson) for SER across multiple languages.
Representations from TRILLsson achieved the best performance among all the
PTMs. This demonstrates that TRILLsson is able to effectively capture the
various paralinguistic features from speech data for better SER. We also show
that downstream models using TRILLsson representations achieve SOTA performance
in terms of accuracy across various multi-lingual datasets.
\\ ( https://arxiv.org/abs/2402.01579 ,  773kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00878 (*cross-listing*)
Date: Fri, 12 Jan 2024 14:56:45 GMT   (5371kb,D)

Title: Radio Map Estimation -- An Open Dataset with Directive Transmitter
  Antennas and Initial Experiments
Authors: Fabian Jaensch, Giuseppe Caire, Beg\"um Demir
Categories: cs.NI cs.LG eess.SP
Comments: 13 pages, 121 figures, This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\
  Over the last years, several works have explored the application of deep
learning algorithms to determine the large-scale signal fading (also referred
to as ``path loss'') between transmitter and receiver pairs in urban
communication networks. The central idea is to replace costly measurement
campaigns, inaccurate statistical models or computationally expensive
ray-tracing simulations by machine learning models which, once trained, produce
accurate predictions almost instantly. Although the topic has attracted
attention from many researchers, there are few open benchmark datasets and
codebases that would allow everyone to test and compare the developed methods
and algorithms. We take a step towards filling this gap by releasing a publicly
available dataset of simulated path loss radio maps together with realistic
city maps from real-world locations and aerial images from open datasources.
Initial experiments regarding model architectures, input feature design and
estimation of radio maps from aerial images are presented and the code is made
available.
\\ ( https://arxiv.org/abs/2402.00878 ,  5371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00879 (*cross-listing*)
Date: Mon, 15 Jan 2024 22:23:06 GMT   (3479kb,D)

Title: Graph Representation Learning for Contention and Interference Management
  in Wireless Networks
Authors: Zhouyou Gu, Branka Vucetic, Kishore Chikkam, Pasquale Aliberti, Wibowo
  Hardjawana
Categories: cs.NI cs.LG eess.SP
Comments: This work has been accepted in the IEEE/ACM Transactions on
  Networking. Copyright may be transferred without notice, after which this
  version may no longer be accessible
\\
  Restricted access window (RAW) in Wi-Fi 802.11ah networks manages contention
and interference by grouping users and allocating periodic time slots for each
group's transmissions. We will find the optimal user grouping decisions in RAW
to maximize the network's worst-case user throughput. We review existing user
grouping approaches and highlight their performance limitations in the above
problem. We propose formulating user grouping as a graph construction problem
where vertices represent users and edge weights indicate the contention and
interference. This formulation leverages the graph's max cut to group users and
optimizes edge weights to construct the optimal graph whose max cut yields the
optimal grouping decisions. To achieve this optimal graph construction, we
design an actor-critic graph representation learning (AC-GRL) algorithm.
Specifically, the actor neural network (NN) is trained to estimate the optimal
graph's edge weights using path losses between users and access points. A graph
cut procedure uses semidefinite programming to solve the max cut efficiently
and return the grouping decisions for the given weights. The critic NN
approximates user throughput achieved by the above-returned decisions and is
used to improve the actor. Additionally, we present an architecture that uses
the online-measured throughput and path losses to fine-tune the decisions in
response to changes in user populations and their locations. Simulations show
that our methods achieve $30\%\sim80\%$ higher worst-case user throughput than
the existing approaches and that the proposed architecture can further improve
the worst-case user throughput by $5\%\sim30\%$ while ensuring timely updates
of grouping decisions.
\\ ( https://arxiv.org/abs/2402.00879 ,  3479kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00897 (*cross-listing*)
Date: Wed, 31 Jan 2024 19:20:08 GMT   (1892kb)

Title: Screening method for early dementia using sound objects as voice
  biomarkers
Authors: Adam Pluta, Zbigniew Pioch, J\k{e}drzej Kardach, Piotr Zio{\l}o,
  Tomasz Kr\k{e}cicki, El\.zbieta Trypka
Categories: cs.SD cs.LG eess.AS q-bio.QM
\\
  Introduction: We present a screening method for early dementia using features
based on sound objects as voice biomarkers.
  Methods: The final dataset used for machine learning models consisted of 266
observations, with a distribution of 186 healthy individuals, 46 diagnosed with
Alzheimer's, and 34 with MCI. This method is based on six-second recordings of
the sustained vowel /a/ spoken by the subject. The main original contribution
of this work is the use of carefully crafted features based on sound objects.
This approach allows one to first represent the sound spectrum in a more
accurate way than the standard spectrum, and then build interpretable features
containing relevant information about subjects' control over their voice.
  Results: ROC AUC obtained in this work for distinguishing healthy subjects
from those with MCI was 0.85, while accuracy was 0.76. For distinguishing
between healthy subjects and those with either MCI or Alzheimer's the results
were 0.84, 0.77, respectively.
  Conclusion: The use of features based on sound objects enables screening for
early dementia even on very short recordings of language-independent voice
samples.
\\ ( https://arxiv.org/abs/2402.00897 ,  1892kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00906 (*cross-listing*)
Date: Thu, 1 Feb 2024 03:16:40 GMT   (975kb,D)

Title: BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic
  Architectures against Model Inversion Attacks
Authors: Hamed Poursiami, Ihsen Alouani, Maryam Parsa
Categories: cs.CR cs.LG cs.NE
Comments: 7 pages, 4 figures
\\
  With the mainstream integration of machine learning into security-sensitive
domains such as healthcare and finance, concerns about data privacy have
intensified. Conventional artificial neural networks (ANNs) have been found
vulnerable to several attacks that can leak sensitive data. Particularly, model
inversion (MI) attacks enable the reconstruction of data samples that have been
used to train the model. Neuromorphic architectures have emerged as a paradigm
shift in neural computing, enabling asynchronous and energy-efficient
computation. However, little to no existing work has investigated the privacy
of neuromorphic architectures against model inversion. Our study is motivated
by the intuition that the non-differentiable aspect of spiking neural networks
(SNNs) might result in inherent privacy-preserving properties, especially
against gradient-based attacks. To investigate this hypothesis, we propose a
thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we
develop novel inversion attack strategies that are comprehensively designed to
target SNNs, offering a comparative analysis with their conventional ANN
counterparts. Our experiments, conducted on diverse event-based and static
datasets, demonstrate the effectiveness of the proposed attack strategies and
therefore questions the assumption of inherent privacy-preserving in
neuromorphic architectures.
\\ ( https://arxiv.org/abs/2402.00906 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00920 (*cross-listing*)
Date: Thu, 1 Feb 2024 14:33:24 GMT   (427kb)

Title: Deep Learning Approaches for Network Traffic Classification in the
  Internet of Things (IoT): A Survey
Authors: Jawad Hussain Kalwar, Sania Bhatti
Categories: cs.CR cs.LG cs.NI
\\
  The Internet of Things (IoT) has witnessed unprecedented growth, resulting in
a massive influx of diverse network traffic from interconnected devices.
Effectively classifying this network traffic is crucial for optimizing resource
allocation, enhancing security measures, and ensuring efficient network
management in IoT systems. Deep learning has emerged as a powerful technique
for network traffic classification due to its ability to automatically learn
complex patterns and representations from raw data. This survey paper aims to
provide a comprehensive overview of the existing deep learning approaches
employed in network traffic classification specifically tailored for IoT
environments. By systematically analyzing and categorizing the latest research
contributions in this domain, we explore the strengths and limitations of
various deep learning models in handling the unique challenges posed by IoT
network traffic. Through this survey, we aim to offer researchers and
practitioners valuable insights, identify research gaps, and provide directions
for future research to further enhance the effectiveness and efficiency of deep
learning-based network traffic classification in IoT.
\\ ( https://arxiv.org/abs/2402.00920 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00926 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:17:36 GMT   (2325kb,D)

Title: A Comparative Analysis of Gene Expression Profiling by Statistical and
  Machine Learning Approaches
Authors: Myriam Bontonou, Ana\"is Haget, Maria Boulougouri, Benjamin Audit,
  Pierre Borgnat, Jean-Michel Arbona
Categories: q-bio.GN cs.LG
\\
  Many machine learning models have been proposed to classify phenotypes from
gene expression data. In addition to their good performance, these models can
potentially provide some understanding of phenotypes by extracting explanations
for their decisions. These explanations often take the form of a list of genes
ranked in order of importance for the predictions, the highest-ranked genes
being interpreted as linked to the phenotype. We discuss the biological and the
methodological limitations of such explanations. Experiments are performed on
several datasets gathering cancer and healthy tissue samples from the TCGA,
GTEx and TARGET databases. A collection of machine learning models including
logistic regression, multilayer perceptron, and graph neural network are
trained to classify samples according to their cancer type. Gene rankings are
obtained from explainability methods adapted to these models, and compared to
the ones from classical statistical feature selection methods such as mutual
information, DESeq2, and EdgeR. Interestingly, on simple tasks, we observe that
the information learned by black-box neural networks is related to the notion
of differential expression. In all cases, a small set containing the
best-ranked genes is sufficient to achieve a good classification. However,
these genes differ significantly between the methods and similar classification
performance can be achieved with numerous lower ranked genes. In conclusion,
although these methods enable the identification of biomarkers characteristic
of certain pathologies, our results question the completeness of the selected
gene sets and thus of explainability by the identification of the underlying
biological processes.
\\ ( https://arxiv.org/abs/2402.00926 ,  2325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00943 (*cross-listing*)
Date: Thu, 1 Feb 2024 19:00:40 GMT   (457kb,D)

Title: Approximate Nearest Neighbor Search with Window Filters
Authors: Joshua Engels, Benjamin Landrum, Shangdi Yu, Laxman Dhulipala, Julian
  Shun
Categories: cs.DS cs.IR cs.LG
Comments: Code available: https://github.com/JoshEngels/RangeFilteredANN
\\
  We define and investigate the problem of $\textit{c-approximate window
search}$: approximate nearest neighbor search where each point in the dataset
has a numeric label, and the goal is to find nearest neighbors to queries
within arbitrary label ranges. Many semantic search problems, such as image and
document search with timestamp filters, or product search with cost filters,
are natural examples of this problem. We propose and theoretically analyze a
modular tree-based framework for transforming an index that solves the
traditional c-approximate nearest neighbor problem into a data structure that
solves window search. On standard nearest neighbor benchmark datasets equipped
with random label values, adversarially constructed embeddings, and image
search embeddings with real timestamps, we obtain up to a $75\times$ speedup
over existing solutions at the same level of recall.
\\ ( https://arxiv.org/abs/2402.00943 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00949 (*cross-listing*)
Date: Thu, 1 Feb 2024 19:06:06 GMT   (780kb,D)

Title: Geometry of Polynomial Neural Networks
Authors: Kaie Kubjas, Jiayi Li, Maximilian Wiesmann
Categories: math.AG cs.LG stat.ML
MSC-class: 68T07, 14P10, 14N07, 14M12
\\
  We study the expressivity and learning process for polynomial neural networks
(PNNs) with monomial activation functions. The weights of the network
parametrize the neuromanifold. In this paper, we study certain neuromanifolds
using tools from algebraic geometry: we give explicit descriptions as
semialgebraic sets and characterize their Zariski closures, called
neurovarieties. We study their dimension and associate an algebraic degree, the
learning degree, to the neurovariety. The dimension serves as a geometric
measure for the expressivity of the network, the learning degree is a measure
for the complexity of training the network and provides upper bounds on the
number of learnable functions. These theoretical results are accompanied with
experiments.
\\ ( https://arxiv.org/abs/2402.00949 ,  780kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00994 (*cross-listing*)
Date: Thu, 1 Feb 2024 20:18:06 GMT   (913kb)

Title: A Cost-Efficient Approach for Creating Virtual Fitting Room using
  Generative Adversarial Networks (GANs)
Authors: Kirolos Attallah, Girgis Zaky, Nourhan Abdelrhim, Kyrillos Botros,
  Amjad Dife, and Nermin Negied
Categories: cs.CV cs.LG
Journal-ref: International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 15 Issue 1, 2024
DOI: 10.14569/IJACSA.2024.0150132
\\
  Customers all over the world want to see how the clothes fit them or not
before purchasing. Therefore, customers by nature prefer brick-and-mortar
clothes shopping so they can try on products before purchasing them. But after
the Pandemic of COVID19 many sellers either shifted to online shopping or
closed their fitting rooms which made the shopping process hesitant and
doubtful. The fact that the clothes may not be suitable for their buyers after
purchase led us to think about using new AI technologies to create an online
platform or a virtual fitting room (VFR) in the form of a mobile application
and a deployed model using a webpage that can be embedded later to any online
store where they can try on any number of cloth items without physically trying
them. Besides, it will save much searching time for their needs. Furthermore,
it will reduce the crowding and headache in the physical shops by applying the
same technology using a special type of mirror that will enable customers to
try on faster. On the other hand, from business owners' perspective, this
project will highly increase their online sales, besides, it will save the
quality of the products by avoiding physical trials issues. The main approach
used in this work is applying Generative Adversarial Networks (GANs) combined
with image processing techniques to generate one output image from two input
images which are the person image and the cloth image. This work achieved
results that outperformed the state-of-the-art approaches found in literature.
\\ ( https://arxiv.org/abs/2402.00994 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01000 (*cross-listing*)
Date: Thu, 1 Feb 2024 20:27:19 GMT   (728kb,D)

Title: Multivariate Probabilistic Time Series Forecasting with Correlated
  Errors
Authors: Vincent Zhihao Zheng, Lijun Sun
Categories: stat.ML cs.LG
Comments: arXiv admin note: text overlap with arXiv:2305.17028
\\
  Modeling the correlations among errors is closely associated with how
accurately the model can quantify predictive uncertainty in probabilistic time
series forecasting. Recent multivariate models have made significant progress
in accounting for contemporaneous correlations among errors, while a common
assumption on these errors is that they are temporally independent for the sake
of statistical simplicity. However, real-world observations often deviate from
this assumption, since errors usually exhibit substantial autocorrelation due
to various factors such as the exclusion of temporally correlated covariates.
In this work, we propose an efficient method, based on a low-rank-plus-diagonal
parameterization of the covariance matrix, which can effectively characterize
the autocorrelation of errors. The proposed method possesses several desirable
properties: the complexity does not scale with the number of time series, the
resulting covariance can be used for calibrating predictions, and it can
seamlessly integrate with any model with Gaussian-distributed errors. We
empirically demonstrate these properties using two distinct neural forecasting
models -- GPVar and Transformer. Our experimental results confirm the
effectiveness of our method in enhancing predictive accuracy and the quality of
uncertainty quantification on multiple real-world datasets.
\\ ( https://arxiv.org/abs/2402.01000 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01036 (*cross-listing*)
Date: Thu, 1 Feb 2024 21:49:50 GMT   (1447kb,D)

Title: Fisher information dissipation for time inhomogeneous stochastic
  differential equations
Authors: Qi Feng, Xinzhe Zuo, Wuchen Li
Categories: math.PR cs.LG stat.ML
Comments: 9 figures, 36 pages
\\
  We provide a Lyapunov convergence analysis for time-inhomogeneous variable
coefficient stochastic differential equations (SDEs). Three typical examples
include overdamped, irreversible drift, and underdamped Langevin dynamics. We
first formula the probability transition equation of Langevin dynamics as a
modified gradient flow of the Kullback-Leibler divergence in the probability
space with respect to time-dependent optimal transport metrics. This
formulation contains both gradient and non-gradient directions depending on a
class of time-dependent target distribution. We then select a time-dependent
relative Fisher information functional as a Lyapunov functional. We develop a
time-dependent Hessian matrix condition, which guarantees the convergence of
the probability density function of the SDE. We verify the proposed conditions
for several time-inhomogeneous Langevin dynamics. For the overdamped Langevin
dynamics, we prove the $O(t^{-1/2})$ convergence in $L^1$ distance for the
simulated annealing dynamics with a strongly convex potential function. For the
irreversible drift Langevin dynamics, we prove an improved convergence towards
the target distribution in an asymptotic regime. We also verify the convergence
condition for the underdamped Langevin dynamics. Numerical examples demonstrate
the convergence results for the time-dependent Langevin dynamics.
\\ ( https://arxiv.org/abs/2402.01036 ,  1447kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01050 (*cross-listing*)
Date: Thu, 1 Feb 2024 22:43:55 GMT   (192kb,D)

Title: Distributed MCMC inference for Bayesian Non-Parametric Latent Block
  Model
Authors: Reda Khoufache, Anisse Belhadj, Hanene Azzag, Mustapha Lebbah
Categories: stat.ML cs.LG stat.CO
Comments: Accepted to PaKDD 2024
\\
  In this paper, we introduce a novel Distributed Markov Chain Monte Carlo
(MCMC) inference method for the Bayesian Non-Parametric Latent Block Model
(DisNPLBM), employing the Master/Worker architecture. Our non-parametric
co-clustering algorithm divides observations and features into partitions using
latent multivariate Gaussian block distributions. The workload on rows is
evenly distributed among workers, who exclusively communicate with the master
and not among themselves. DisNPLBM demonstrates its impact on cluster labeling
accuracy and execution times through experimental results. Moreover, we present
a real-use case applying our approach to co-cluster gene expression data. The
code source is publicly available at
https://github.com/redakhoufache/Distributed-NPLBM.
\\ ( https://arxiv.org/abs/2402.01050 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01052 (*cross-listing*)
Date: Thu, 1 Feb 2024 22:54:45 GMT   (3711kb,D)

Title: Weakly Convex Regularisers for Inverse Problems: Convergence of Critical
  Points and Primal-Dual Optimisation
Authors: Zakhar Shumaylov, Jeremy Budd, Subhadip Mukherjee, Carola-Bibiane
  Sch\"onlieb
Categories: math.OC cs.CV cs.LG stat.ML
Comments: 26 pages, 4 figures, preprint
\\
  Variational regularisation is the primary method for solving inverse
problems, and recently there has been considerable work leveraging deeply
learned regularisation for enhanced performance. However, few results exist
addressing the convergence of such regularisation, particularly within the
context of critical points as opposed to global minima. In this paper, we
present a generalised formulation of convergent regularisation in terms of
critical points, and show that this is achieved by a class of weakly convex
regularisers. We prove convergence of the primal-dual hybrid gradient method
for the associated variational problem, and, given a Kurdyka-Lojasiewicz
condition, an $\mathcal{O}(\log{k}/k)$ ergodic convergence rate. Finally,
applying this theory to learned regularisation, we prove universal
approximation for input weakly convex neural networks (IWCNN), and show
empirically that IWCNNs can lead to improved performance of learned adversarial
regularisers for computed tomography (CT) reconstruction.
\\ ( https://arxiv.org/abs/2402.01052 ,  3711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01054 (*cross-listing*)
Date: Thu, 1 Feb 2024 22:58:21 GMT   (2072kb,D)

Title: Unconditional Latent Diffusion Models Memorize Patient Imaging Data
Authors: Salman Ul Hassan Dar, Marvin Seyfarth, Jannik Kahmann, Isabelle Ayx,
  Theano Papavassiliu, Stefan O. Schoenberg, Sandy Engelhardt
Categories: eess.IV cs.CV cs.LG
\\
  Generative latent diffusion models hold a wide range of applications in the
medical imaging domain. A noteworthy application is privacy-preserved open-data
sharing by proposing synthetic data as surrogates of real patient data. Despite
the promise, these models are susceptible to patient data memorization, where
models generate patient data copies instead of novel synthetic samples. This
undermines the whole purpose of preserving patient data and may even result in
patient re-identification. Considering the importance of the problem,
surprisingly it has received relatively little attention in the medical imaging
community. To this end, we assess memorization in latent diffusion models for
medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR,
and X-ray datasets for synthetic data generation. Afterwards, we examine the
amount of training data memorized utilizing self-supervised models and further
investigate various factors that can possibly lead to memorization by training
models in different settings. We observe a surprisingly large amount of data
memorization among all datasets, with up to 41.7%, 19.6%, and 32.6% of the
training data memorized in CT, MRI, and X-ray datasets respectively. Further
analyses reveal that increasing training data size and using data augmentation
reduce memorization, while over-training enhances it. Overall, our results
suggest a call for memorization-informed evaluation of synthetic data prior to
open-data sharing.
\\ ( https://arxiv.org/abs/2402.01054 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01062 (*cross-listing*)
Date: Thu, 1 Feb 2024 23:26:16 GMT   (4214kb)

Title: Bio-Inspired Compensatory Strategies for Damage to Flapping Robotic
  Propulsors
Authors: Meredith L. Hooper, Isabel Scherl, and Morteza Gharib
Categories: cs.RO cs.LG
\\
  To maintain full autonomy, autonomous robotic systems must have the ability
to self-repair. Self-repairing via compensatory mechanisms appears in nature:
for example, some fish can lose even 76% of their propulsive surface without
loss of thrust by altering stroke mechanics. However, direct transference of
these alterations from an organism to a robotic flapping propulsor may not be
optimal due to irrelevant evolutionary pressures. We instead seek to determine
what alterations to stroke mechanics are optimal for a damaged robotic system
via artificial evolution. To determine whether natural and machine-learned
optima differ, we employ a cyber-physical system using a Covariance Matrix
Adaptation Evolutionary Strategy to seek the most efficient trajectory for a
given force. We implement an online optimization with hardware-in-the-loop,
performing experimental function evaluations with an actuated flexible flat
plate. To recoup thrust production following partial amputation, the most
efficient learned strategy was to increase amplitude, increase frequency,
increase the amplitude of angle of attack, and phase shift the angle of attack
by approximately 110 degrees. In fish, only an amplitude increase is reported
by majority in the literature. To recoup side-force production, a more
challenging optimization landscape is encountered. Nesting of optimal angle of
attack traces is found in the resultant-based reference frame, but no clear
trend in amplitude or frequency are exhibited -- in contrast to the increase in
frequency reported in insect literature. These results suggest that how
mechanical flapping propulsors most efficiently adjust to damage of a flapping
propulsor may not align with natural swimmers and flyers.
\\ ( https://arxiv.org/abs/2402.01062 ,  4214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01067 (*cross-listing*)
Date: Thu, 1 Feb 2024 23:53:12 GMT   (890kb,D)

Title: Assessing Patient Eligibility for Inspire Therapy through Machine
  Learning and Deep Learning Models
Authors: Mohsena Chowdhury, Tejas Vyas, Rahul Alapati, Andr\'es M Bur, Guanghui
  Wang
Categories: eess.IV cs.CV cs.LG
\\
  Inspire therapy is an FDA-approved internal neurostimulation treatment for
obstructive sleep apnea. However, not all patients respond to this therapy,
posing a challenge even for experienced otolaryngologists to determine
candidacy. This paper makes the first attempt to leverage both machine learning
and deep learning techniques in discerning patient responsiveness to Inspire
therapy using medical data and videos captured through Drug-Induced Sleep
Endoscopy (DISE), an essential procedure for Inspire therapy. To achieve this,
we gathered and annotated three datasets from 127 patients. Two of these
datasets comprise endoscopic videos focused on the Base of the Tongue and
Velopharynx. The third dataset composes the patient's clinical information. By
utilizing these datasets, we benchmarked and compared the performance of six
deep learning models and five classical machine learning algorithms. The
results demonstrate the potential of employing machine learning and deep
learning techniques to determine a patient's eligibility for Inspire therapy,
paving the way for future advancements in this field.
\\ ( https://arxiv.org/abs/2402.01067 ,  890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01082 (*cross-listing*)
Date: Fri, 2 Feb 2024 00:48:27 GMT   (737kb,D)

Title: Salsa Fresca: Angular Embeddings and Pre-Training for ML Attacks on
  Learning With Errors
Authors: Samuel Stevens, Emily Wenger, Cathy Li, Niklas Nolte, Eshika Saxena,
  Fran\c{c}ois Charton, Kristin Lauter
Categories: cs.CR cs.LG
Comments: 8 pages (main text)
\\
  Learning with Errors (LWE) is a hard math problem underlying recently
standardized post-quantum cryptography (PQC) systems for key exchange and
digital signatures. Prior work proposed new machine learning (ML)-based attacks
on LWE problems with small, sparse secrets, but these attacks require millions
of LWE samples to train on and take days to recover secrets. We propose three
key methods -- better preprocessing, angular embeddings and model pre-training
-- to improve these attacks, speeding up preprocessing by $25\times$ and
improving model sample efficiency by $10\times$. We demonstrate for the first
time that pre-training improves and reduces the cost of ML attacks on LWE. Our
architecture improvements enable scaling to larger-dimension LWE problems: this
work is the first instance of ML attacks recovering sparse binary secrets in
dimension $n=1024$, the smallest dimension used in practice for homomorphic
encryption applications of LWE where sparse binary secrets are proposed.
\\ ( https://arxiv.org/abs/2402.01082 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01089 (*cross-listing*)
Date: Fri, 2 Feb 2024 01:13:16 GMT   (214kb,D)

Title: No Free Prune: Information-Theoretic Barriers to Pruning at
  Initialization
Authors: Tanishq Kumar, Kevin Luo, Mark Sellke
Categories: stat.ML cs.LG
\\
  The existence of "lottery tickets" arXiv:1803.03635 at or near initialization
raises the tantalizing question of whether large models are necessary in deep
learning, or whether sparse networks can be quickly identified and trained
without ever training the dense models that contain them. However, efforts to
find these sparse subnetworks without training the dense model ("pruning at
initialization") have been broadly unsuccessful arXiv:2009.08576. We put
forward a theoretical explanation for this, based on the model's effective
parameter count, $p_\text{eff}$, given by the sum of the number of non-zero
weights in the final network and the mutual information between the sparsity
mask and the data. We show the Law of Robustness of arXiv:2105.12806 extends to
sparse networks with the usual parameter count replaced by $p_\text{eff}$,
meaning a sparse neural network which robustly interpolates noisy data requires
a heavily data-dependent mask. We posit that pruning during and after training
outputs masks with higher mutual information than those produced by pruning at
initialization. Thus two networks may have the same sparsities, but differ in
effective parameter count based on how they were trained. This suggests that
pruning near initialization may be infeasible and explains why lottery tickets
exist, but cannot be found fast (i.e. without training the full network).
Experiments on neural networks confirm that information gained during training
may indeed affect model capacity.
\\ ( https://arxiv.org/abs/2402.01089 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01090 (*cross-listing*)
Date: Fri, 2 Feb 2024 01:18:48 GMT   (563kb,D)

Title: Scalable Higher-Order Tensor Product Spline Models
Authors: David R\"ugamer
Categories: stat.ML cs.LG stat.CO
Comments: Accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024. arXiv admin note: substantial
  text overlap with arXiv:2205.14515
\\
  In the current era of vast data and transparent machine learning, it is
essential for techniques to operate at a large scale while providing a clear
mathematical comprehension of the internal workings of the method. Although
there already exist interpretable semi-parametric regression methods for
large-scale applications that take into account non-linearity in the data, the
complexity of the models is still often limited. One of the main challenges is
the absence of interactions in these models, which are left out for the sake of
better interpretability but also due to impractical computational costs. To
overcome this limitation, we propose a new approach using a factorization
method to derive a highly scalable higher-order tensor product spline model.
Our method allows for the incorporation of all (higher-order) interactions of
non-linear feature effects while having computational costs proportional to a
model without interactions. We further develop a meaningful penalization scheme
and examine the induced optimization problem. We conclude by evaluating the
predictive and estimation performance of our method.
\\ ( https://arxiv.org/abs/2402.01090 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01092 (*cross-listing*)
Date: Fri, 2 Feb 2024 01:41:38 GMT   (11663kb,D)

Title: A Dynamical Model of Neural Scaling Laws
Authors: Blake Bordelon, Alexander Atanasov, Cengiz Pehlevan
Categories: stat.ML cond-mat.dis-nn cs.LG
Comments: 34 pages, 9 figures, in submission
\\
  On a variety of tasks, the performance of neural networks predictably
improves with training time, dataset size and model size across many orders of
magnitude. This phenomenon is known as a neural scaling law. Of fundamental
importance is the compute-optimal scaling law, which reports the performance as
a function of units of compute when choosing model sizes optimally. We analyze
a random feature model trained with gradient descent as a solvable model of
network training and generalization. This reproduces many observations about
neural scaling laws. First, our model makes a prediction about why the scaling
of performance with training time and with model size have different power law
exponents. Consequently, the theory predicts an asymmetric compute-optimal
scaling rule where the number of training steps are increased faster than model
parameters, consistent with recent empirical observations. Second, it has been
observed that early in training, networks converge to their infinite-width
dynamics at a rate $1/\textit{width}$ but at late time exhibit a rate
$\textit{width}^{-c}$, where $c$ depends on the structure of the architecture
and task. We show that our model exhibits this behavior. Lastly, our theory
shows how the gap between training and test loss can gradually build up over
time due to repeated reuse of data.
\\ ( https://arxiv.org/abs/2402.01092 ,  11663kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01116 (*cross-listing*)
Date: Fri, 2 Feb 2024 03:19:54 GMT   (723kb,D)

Title: Scalable Multi-modal Model Predictive Control via Duality-based
  Interaction Predictions
Authors: Hansung Kim, Siddharth H. Nair, Francesco Borrelli
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: Submitted to IEEE Intelligent Vehicles Symposium 2024
\\
  We propose a hierarchical architecture designed for scalable real-time Model
Predictive Control (MPC) in complex, multi-modal traffic scenarios. This
architecture comprises two key components: 1) RAID-Net, a novel attention-based
Recurrent Neural Network that predicts relevant interactions along the MPC
prediction horizon between the autonomous vehicle and the surrounding vehicles
using Lagrangian duality, and 2) a reduced Stochastic MPC problem that
eliminates irrelevant collision avoidance constraints, enhancing computational
efficiency. Our approach is demonstrated in a simulated traffic intersection
with interactive surrounding vehicles, showcasing a 12x speed-up in solving the
motion planning problem. A video demonstrating the proposed architecture in
multiple complex traffic scenarios can be found here:
https://youtu.be/-TcMeolCLWc
\\ ( https://arxiv.org/abs/2402.01116 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01138 (*cross-listing*)
Date: Fri, 2 Feb 2024 04:30:58 GMT   (2542kb,D)

Title: Graph Neural Networks in EEG-based Emotion Recognition: A Survey
Authors: Chenyu Liu, Xinliang Zhou, Yihao Wu, Ruizhi Yang, Liming Zhai, Ziyu
  Jia and Yang Liu
Categories: eess.SP cs.LG
\\
  Compared to other modalities, EEG-based emotion recognition can intuitively
respond to the emotional patterns in the human brain and, therefore, has become
one of the most concerning tasks in the brain-computer interfaces field. Since
dependencies within brain regions are closely related to emotion, a significant
trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion
recognition. However, brain region dependencies in emotional EEG have
physiological bases that distinguish GNNs in this field from those in other
time series fields. Besides, there is neither a comprehensive review nor
guidance for constructing GNNs in EEG-based emotion recognition. In the survey,
our categorization reveals the commonalities and differences of existing
approaches under a unified framework of graph construction. We analyze and
categorize methods from three stages in the framework to provide clear guidance
on constructing GNNs in EEG-based emotion recognition. In addition, we discuss
several open challenges and future directions, such as Temporal full-connected
graph and Graph condensation.
\\ ( https://arxiv.org/abs/2402.01138 ,  2542kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01139 (*cross-listing*)
Date: Fri, 2 Feb 2024 04:42:09 GMT   (345kb,D)

Title: Online conformal prediction with decaying step sizes
Authors: Anastasios N. Angelopoulos and Rina Foygel Barber and Stephen Bates
Categories: stat.ML cs.LG stat.ME
\\
  We introduce a method for online conformal prediction with decaying step
sizes. Like previous methods, ours possesses a retrospective guarantee of
coverage for arbitrary sequences. However, unlike previous methods, we can
simultaneously estimate a population quantile when it exists. Our theory and
experiments indicate substantially improved practical properties: in
particular, when the distribution is stable, the coverage is close to the
desired level for every time point, not just on average over the observed
sequence.
\\ ( https://arxiv.org/abs/2402.01139 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01148 (*cross-listing*)
Date: Fri, 2 Feb 2024 05:23:34 GMT   (2125kb,D)

Title: The Optimality of Kernel Classifiers in Sobolev Space
Authors: Jianfa Lai, Zhifan Li, Dongming Huang, Qian Lin
Categories: math.ST cs.LG stat.ML stat.TH
Comments: 21 pages, 2 figures
MSC-class: 62G08 (Primary), 68T07, 46E22 (secondary)
ACM-class: G.3
\\
  Kernel methods are widely used in machine learning, especially for
classification problems. However, the theoretical analysis of kernel
classification is still limited. This paper investigates the statistical
performances of kernel classifiers. With some mild assumptions on the
conditional probability $\eta(x)=\mathbb{P}(Y=1\mid X=x)$, we derive an upper
bound on the classification excess risk of a kernel classifier using recent
advances in the theory of kernel regression. We also obtain a minimax lower
bound for Sobolev spaces, which shows the optimality of the proposed
classifier. Our theoretical results can be extended to the generalization error
of overparameterized neural network classifiers. To make our theoretical
results more applicable in realistic settings, we also propose a simple method
to estimate the interpolation smoothness of $2\eta(x)-1$ and apply the method
to real datasets.
\\ ( https://arxiv.org/abs/2402.01148 ,  2125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01258 (*cross-listing*)
Date: Fri, 2 Feb 2024 09:29:40 GMT   (137kb,D)

Title: Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field
  Dynamics on the Attention Landscape
Authors: Juno Kim and Taiji Suzuki
Categories: stat.ML cs.LG
Comments: 32 pages, 1 figure
\\
  Large language models based on the Transformer architecture have demonstrated
impressive capabilities to learn in context. However, existing theoretical
studies on how this phenomenon arises are limited to the dynamics of a single
layer of attention trained on linear regression tasks. In this paper, we study
the optimization of a Transformer consisting of a fully connected layer
followed by a linear attention layer. The MLP acts as a common nonlinear
representation or feature map, greatly enhancing the power of in-context
learning. We prove in the mean-field and two-timescale limit that the
infinite-dimensional loss landscape for the distribution of parameters, while
highly nonconvex, becomes quite benign. We also analyze the second-order
stability of mean-field dynamics and show that Wasserstein gradient flow almost
always avoids saddle points. Furthermore, we establish novel methods for
obtaining concrete improvement rates both away from and near critical points.
This represents the first saddle point analysis of mean-field dynamics in
general and the techniques are of independent interest.
\\ ( https://arxiv.org/abs/2402.01258 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01274 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:00:51 GMT   (525kb,D)

Title: On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio
  Classification
Authors: Calum Heggan, Sam Budgett, Timothy Hosepedales, Mehrdad Yeghoobi
Categories: cs.SD cs.LG eess.AS
Comments: Camera Ready version as submitted to ICASSP SASB Workshop 2024. 5
  pages, 2 figures, 3 tables
\\
  In recent years, self-supervised learning has excelled for its capacity to
learn robust feature representations from unlabelled data. Networks pretrained
through self-supervision serve as effective feature extractors for downstream
tasks, including Few-Shot Learning. While the evaluation of unsupervised
approaches for few-shot learning is well-established in imagery, it is notably
absent in acoustics. This study addresses this gap by assessing large-scale
self-supervised models' performance in few-shot audio classification.
Additionally, we explore the relationship between a model's few-shot learning
capability and other downstream task benchmarks. Our findings reveal
state-of-the-art performance in some few-shot problems such as
SpeechCommandsv2, as well as strong correlations between speech-based few-shot
problems and various downstream audio tasks.
\\ ( https://arxiv.org/abs/2402.01274 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01275 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:04:29 GMT   (5924kb,D)

Title: Parametric-Task MAP-Elites
Authors: Timoth\'ee Anne, Jean-Baptiste Mouret
Categories: cs.NE cs.LG
\\
  Optimizing a set of functions simultaneously by leveraging their similarity
is called multi-task optimization. Current black-box multi-task algorithms only
solve a finite set of tasks, even when the tasks originate from a continuous
space. In this paper, we introduce Parametric-task MAP-Elites (PT-ME), a novel
black-box algorithm to solve continuous multi-task optimization problems. This
algorithm (1) solves a new task at each iteration, effectively covering the
continuous space, and (2) exploits a new variation operator based on local
linear regression. The resulting dataset of solutions makes it possible to
create a function that maps any task parameter to its optimal solution. We show
on two parametric-task toy problems and a more realistic and challenging
robotic problem in simulation that PT-ME outperforms all baselines, including
the deep reinforcement learning algorithm PPO.
\\ ( https://arxiv.org/abs/2402.01275 ,  5924kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01282 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:16:10 GMT   (1775kb,D)

Title: Differentiable and accelerated wavelet transforms on the sphere and ball
Authors: Matthew A. Price, Alicja Polanska, Jessica Whitney, Jason D. McEwen
Categories: astro-ph.IM cs.LG physics.comp-ph
\\
  Directional wavelet dictionaries are hierarchical representations which
efficiently capture and segment information across scale, location and
orientation. Such representations demonstrate a particular affinity to physical
signals, which often exhibit highly anisotropic, localised multiscale
structure. Many physically important signals are observed over spherical
domains, such as the celestial sky in cosmology. Leveraging recent advances in
computational harmonic analysis, we design new highly distributable and
automatically differentiable directional wavelet transforms on the
$2$-dimensional sphere $\mathbb{S}^2$ and $3$-dimensional ball $\mathbb{B}^3 =
\mathbb{R}^+ \times \mathbb{S}^2$ (the space formed by augmenting the sphere
with the radial half-line). We observe up to a $300$-fold and $21800$-fold
acceleration for signals on the sphere and ball, respectively, compared to
existing software, whilst maintaining 64-bit machine precision. Not only do
these algorithms dramatically accelerate existing spherical wavelet transforms,
the gradient information afforded by automatic differentiation unlocks many
data-driven analysis techniques previously not possible for these spaces. We
publicly release both S2WAV and S2BALL, open-sourced JAX libraries for our
transforms that are automatically differentiable and readily deployable both on
and over clusters of hardware accelerators (e.g. GPUs & TPUs).
\\ ( https://arxiv.org/abs/2402.01282 ,  1775kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01287 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:23:03 GMT   (657kb,D)

Title: Spiking CenterNet: A Distillation-boosted Spiking Neural Network for
  Object Detection
Authors: Lennard Bodden, Franziska Schwaiger, Duc Bach Ha, Lars Kreuzberg, Sven
  Behnke
Categories: cs.CV cs.LG cs.NE
Comments: 8 pages, 5 figures. Submitted to WCCI-2024
\\
  In the era of AI at the edge, self-driving cars, and climate change, the need
for energy-efficient, small, embedded AI is growing. Spiking Neural Networks
(SNNs) are a promising approach to address this challenge, with their
event-driven information flow and sparse activations. We propose Spiking
CenterNet for object detection on event data. It combines an SNN CenterNet
adaptation with an efficient M2U-Net-based decoder. Our model significantly
outperforms comparable previous work on Prophesee's challenging GEN1 Automotive
Detection Dataset while using less than half the energy. Distilling the
knowledge of a non-spiking teacher into our SNN further increases performance.
To the best of our knowledge, our work is the first approach that takes
advantage of knowledge distillation in the field of spiking object detection.
\\ ( https://arxiv.org/abs/2402.01287 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01338 (*cross-listing*)
Date: Fri, 2 Feb 2024 11:47:56 GMT   (4739kb,D)

Title: Inferring the Langevin Equation with Uncertainty via Bayesian Neural
  Networks
Authors: Youngkyoung Bae, Seungwoong Ha, Hawoong Jeong
Categories: cond-mat.stat-mech cond-mat.soft cs.LG physics.bio-ph
Comments: 30 pages, 17 figures
\\
  Pervasive across diverse domains, stochastic systems exhibit fluctuations in
processes ranging from molecular dynamics to climate phenomena. The Langevin
equation has served as a common mathematical model for studying such systems,
enabling predictions of their temporal evolution and analyses of thermodynamic
quantities, including absorbed heat, work done on the system, and entropy
production. However, inferring the Langevin equation from observed trajectories
remains challenging, particularly for nonlinear and high-dimensional systems.
In this study, we present a comprehensive framework that employs Bayesian
neural networks for inferring Langevin equations in both overdamped and
underdamped regimes. Our framework first provides the drift force and diffusion
matrix separately and then combines them to construct the Langevin equation. By
providing a distribution of predictions instead of a single value, our approach
allows us to assess prediction uncertainties, which can prevent potential
misunderstandings and erroneous decisions about the system. We demonstrate the
effectiveness of our framework in inferring Langevin equations for various
scenarios including a neuron model and microscopic engine, highlighting its
versatility and potential impact.
\\ ( https://arxiv.org/abs/2402.01338 ,  4739kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01382 (*cross-listing*)
Date: Fri, 2 Feb 2024 13:06:33 GMT   (292kb,D)

Title: Emergence of heavy tails in homogenized stochastic gradient descent
Authors: Zhe Jiao, Martin Keller-Ressel
Categories: stat.ML cs.LG
MSC-class: 60H30, 68Txx
ACM-class: G.3; I.2.6
\\
  It has repeatedly been observed that loss minimization by stochastic gradient
descent (SGD) leads to heavy-tailed distributions of neural network parameters.
Here, we analyze a continuous diffusion approximation of SGD, called
homogenized stochastic gradient descent, show that it behaves asymptotically
heavy-tailed, and give explicit upper and lower bounds on its tail-index. We
validate these bounds in numerical experiments and show that they are typically
close approximations to the empirical tail-index of SGD iterates. In addition,
their explicit form enables us to quantify the interplay between optimization
parameters and the tail-index. Doing so, we contribute to the ongoing
discussion on links between heavy tails and the generalization performance of
neural networks as well as the ability of SGD to avoid suboptimal local minima.
\\ ( https://arxiv.org/abs/2402.01382 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01393 (*cross-listing*)
Date: Fri, 2 Feb 2024 13:17:19 GMT   (4019kb,D)

Title: ALERT-Transformer: Bridging Asynchronous and Synchronous Machine
  Learning for Real-Time Event-based Spatio-Temporal Data
Authors: Carmen Martin-Turrero, Maxence Bouvier, Manuel Breitenstein, Pietro
  Zanuttigh, Vincent Parret
Categories: cs.CV cs.LG cs.NE
Comments: Preprint version. 8 pages, 7 figures, under review
MSC-class: 68T05
ACM-class: I.2.6; I.2.10; I.4.8; I.4.10; D.2.2; D.1.4
\\
  We seek to enable classic processing of continuous ultra-sparse
spatiotemporal data generated by event-based sensors with dense machine
learning models. We propose a novel hybrid pipeline composed of asynchronous
sensing and synchronous processing that combines several ideas: (1) an
embedding based on PointNet models -- the ALERT module -- that can continuously
integrate new and dismiss old events thanks to a leakage mechanism, (2) a
flexible readout of the embedded data that allows to feed any downstream model
with always up-to-date features at any sampling rate, (3) exploiting the input
sparsity in a patch-based approach inspired by Vision Transformer to optimize
the efficiency of the method. These embeddings are then processed by a
transformer model trained for object and gesture recognition. Using this
approach, we achieve performances at the state-of-the-art with a lower latency
than competitors. We also demonstrate that our asynchronous model can operate
at any desired sampling rate.
\\ ( https://arxiv.org/abs/2402.01393 ,  4019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01400 (*cross-listing*)
Date: Fri, 2 Feb 2024 13:31:24 GMT   (307kb,D)

Title: Query-Efficient Correlation Clustering with Noisy Oracle
Authors: Yuko Kuroki, Atsushi Miyauchi, Francesco Bonchi, Wei Chen
Categories: stat.ML cs.DS cs.LG
\\
  We study a general clustering setting in which we have $n$ elements to be
clustered, and we aim to perform as few queries as possible to an oracle that
returns a noisy sample of the similarity between two elements. Our setting
encompasses many application domains in which the similarity function is costly
to compute and inherently noisy. We propose two novel formulations of online
learning problems rooted in the paradigm of Pure Exploration in Combinatorial
Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For
both settings, we design algorithms that combine a sampling strategy with a
classic approximation algorithm for correlation clustering and study their
theoretical guarantees. Our results are the first examples of polynomial-time
algorithms that work for the case of PE-CMAB in which the underlying offline
optimization problem is NP-hard.
\\ ( https://arxiv.org/abs/2402.01400 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01412 (*cross-listing*)
Date: Fri, 2 Feb 2024 13:44:47 GMT   (991kb,D)

Title: Bass Accompaniment Generation via Latent Diffusion
Authors: Marco Pasini, Maarten Grachten, Stefan Lattner
Categories: cs.SD cs.LG eess.AS
Comments: ICASSP 2024
\\
  The ability to automatically generate music that appropriately matches an
arbitrary input track is a challenging task. We present a novel controllable
system for generating single stems to accompany musical mixes of arbitrary
length. At the core of our method are audio autoencoders that efficiently
compress audio waveform samples into invertible latent representations, and a
conditional latent diffusion model that takes as input the latent encoding of a
mix and generates the latent encoding of a corresponding stem. To provide
control over the timbre of generated samples, we introduce a technique to
ground the latent space to a user-provided reference style during diffusion
sampling. For further improving audio quality, we adapt classifier-free
guidance to avoid distortions at high guidance strengths when generating an
unbounded latent space. We train our model on a dataset of pairs of mixes and
matching bass stems. Quantitative experiments demonstrate that, given an input
mix, the proposed system can generate basslines with user-specified timbres.
Our controllable conditional audio generation framework represents a
significant step forward in creating generative AI tools to assist musicians in
music production.
\\ ( https://arxiv.org/abs/2402.01412 ,  991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01413 (*cross-listing*)
Date: Fri, 2 Feb 2024 13:45:42 GMT   (686kb,D)

Title: Objective and subjective evaluation of speech enhancement methods in the
  UDASE task of the 7th CHiME challenge
Authors: Simon Leglaive, Matthieu Fraticelli, Hend ElGhazaly, L\'eonie Borne,
  Mostafa Sadeghi, Scott Wisdom, Manuel Pariente, John R. Hershey, Daniel
  Pressnitzer, Jon P. Barker
Categories: cs.SD cs.LG eess.AS
\\
  Supervised models for speech enhancement are trained using artificially
generated mixtures of clean speech and noise signals. However, the synthetic
training conditions may not accurately reflect real-world conditions
encountered during testing. This discrepancy can result in poor performance
when the test domain significantly differs from the synthetic training domain.
To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to
leverage real-world noisy speech recordings from the test domain for
unsupervised domain adaptation of speech enhancement models. Specifically, this
test domain corresponds to the CHiME-5 dataset, characterized by real
multi-speaker and conversational speech recordings made in noisy and
reverberant domestic environments, for which ground-truth clean speech signals
are not available. In this paper, we present the objective and subjective
evaluations of the systems that were submitted to the CHiME-7 UDASE task, and
we provide an analysis of the results. This analysis reveals a limited
correlation between subjective ratings and several supervised nonintrusive
performance metrics recently proposed for speech enhancement. Conversely, the
results suggest that more traditional intrusive objective metrics can be used
for in-domain performance evaluation using the reverberant LibriCHiME-5 dataset
developed for the challenge. The subjective evaluation indicates that all
systems successfully reduced the background noise, but always at the expense of
increased distortion. Out of the four speech enhancement methods evaluated
subjectively, only one demonstrated an improvement in overall quality compared
to the unprocessed noisy speech, highlighting the difficulty of the task. The
tools and audio material created for the CHiME-7 UDASE task are shared with the
community.
\\ ( https://arxiv.org/abs/2402.01413 ,  686kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01424 (*cross-listing*)
Date: Fri, 2 Feb 2024 14:11:23 GMT   (802kb,D)

Title: A Data-Driven Analysis of Robust Automatic Piano Transcription
Authors: Drew Edwards, Simon Dixon, Emmanouil Benetos, Akira Maezawa, Yuta
  Kusaka
Categories: cs.SD cs.LG eess.AS
Comments: Accepted for publication in IEEE Signal Processing Letters on 31
  Janurary, 2024
\\
  Algorithms for automatic piano transcription have improved dramatically in
recent years due to new datasets and modeling techniques. Recent developments
have focused primarily on adapting new neural network architectures, such as
the Transformer and Perceiver, in order to yield more accurate systems. In this
work, we study transcription systems from the perspective of their training
data. By measuring their performance on out-of-distribution annotated piano
data, we show how these models can severely overfit to acoustic properties of
the training data. We create a new set of audio for the MAESTRO dataset,
captured automatically in a professional studio recording environment via
Yamaha Disklavier playback. Using various data augmentation techniques when
training with the original and re-performed versions of the MAESTRO dataset, we
achieve state-of-the-art note-onset accuracy of 88.4 F1-score on the MAPS
dataset, without seeing any of its training data. We subsequently analyze these
data augmentation techniques in a series of ablation studies to better
understand their influence on the resulting models.
\\ ( https://arxiv.org/abs/2402.01424 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01434 (*cross-listing*)
Date: Fri, 2 Feb 2024 14:26:32 GMT   (1846kb,D)

Title: Conditioning non-linear and infinite-dimensional diffusion processes
Authors: Elizabeth Louise Baker, Gefan Yang, Michael L. Severinsen, Christy
  Anna Hipsley, Stefan Sommer
Categories: stat.ML cs.LG stat.CO
\\
  Generative diffusion models and many stochastic models in science and
engineering naturally live in infinite dimensions before discretisation. To
incorporate observed data for statistical and learning tasks, one needs to
condition on observations. While recent work has treated conditioning linear
processes in infinite dimensions, conditioning non-linear processes in infinite
dimensions has not been explored. This paper conditions function valued
stochastic processes without prior discretisation. To do so, we use an
infinite-dimensional version of Girsanov's theorem to condition a
function-valued stochastic process, leading to a stochastic differential
equation (SDE) for the conditioned process involving the score. We apply this
technique to do time series analysis for shapes of organisms in evolutionary
biology, where we discretise via the Fourier basis and then learn the
coefficients of the score function with score matching methods.
\\ ( https://arxiv.org/abs/2402.01434 ,  1846kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01441 (*cross-listing*)
Date: Fri, 2 Feb 2024 14:34:22 GMT   (1411kb)

Title: Learning the Market: Sentiment-Based Ensemble Trading Agents
Authors: Andrew Ye, James Xu, Yi Wang, Yifan Yu, Daniel Yan, Ryan Chen, Bosheng
  Dong, Vipin Chaudhary, Shuai Xu
Categories: q-fin.TR cs.LG
\\
  We propose the integration of sentiment analysis and deep-reinforcement
learning ensemble algorithms for stock trading, and design a strategy capable
of dynamically altering its employed agent given concurrent market sentiment.
In particular, we create a simple-yet-effective method for extracting news
sentiment and combine this with general improvements upon existing works,
resulting in automated trading agents that effectively consider both
qualitative market factors and quantitative stock data. We show that our
approach results in a strategy that is profitable, robust, and risk-minimal --
outperforming the traditional ensemble strategy as well as single agent
algorithms and market metrics. Our findings determine that the conventional
practice of switching ensemble agents every fixed-number of months is
sub-optimal, and that a dynamic sentiment-based framework greatly unlocks
additional performance within these agents. Furthermore, as we have designed
our algorithm with simplicity and efficiency in mind, we hypothesize that the
transition of our method from historical evaluation towards real-time trading
with live data should be relatively simple.
\\ ( https://arxiv.org/abs/2402.01441 ,  1411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01460 (*cross-listing*)
Date: Fri, 2 Feb 2024 14:52:10 GMT   (944kb,D)

Title: Deep Conditional Generative Learning: Model and Error Analysis
Authors: Jinyuan Chang, Zhao Ding, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang
Categories: stat.ML cs.LG
\\
  We introduce an Ordinary Differential Equation (ODE) based deep generative
method for learning a conditional distribution, named the Conditional Follmer
Flow. Starting from a standard Gaussian distribution, the proposed flow could
efficiently transform it into the target conditional distribution at time 1.
For effective implementation, we discretize the flow with Euler's method where
we estimate the velocity field nonparametrically using a deep neural network.
Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein
distance between the distribution of the learned samples and the target
distribution, providing the first comprehensive end-to-end error analysis for
conditional distribution learning via ODE flow. Our numerical experiments
showcase its effectiveness across a range of scenarios, from standard
nonparametric conditional density estimation problems to more intricate
challenges involving image data, illustrating its superiority over various
existing conditional density estimation methods.
\\ ( https://arxiv.org/abs/2402.01460 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01493 (*cross-listing*)
Date: Fri, 2 Feb 2024 15:22:06 GMT   (8222kb,D)

Title: Sliced-Wasserstein Estimation with Spherical Harmonics as Control
  Variates
Authors: R\'emi Leluc, Aymeric Dieuleveut, Fran\c{c}ois Portier, Johan Segers
  and Aigerim Zhuman
Categories: stat.ML cs.LG
MSC-class: 65C05 (Primary) 65D30, 68Txx, 68Wxx (Secondary)
\\
  The Sliced-Wasserstein (SW) distance between probability measures is defined
as the average of the Wasserstein distances resulting for the associated
one-dimensional projections. As a consequence, the SW distance can be written
as an integral with respect to the uniform measure on the sphere and the Monte
Carlo framework can be employed for calculating the SW distance. Spherical
harmonics are polynomials on the sphere that form an orthonormal basis of the
set of square-integrable functions on the sphere. Putting these two facts
together, a new Monte Carlo method, hereby referred to as Spherical Harmonics
Control Variates (SHCV), is proposed for approximating the SW distance using
spherical harmonics as control variates. The resulting approach is shown to
have good theoretical properties, e.g., a no-error property for Gaussian
measures under a certain form of linear dependency between the variables.
Moreover, an improved rate of convergence, compared to Monte Carlo, is
established for general measures. The convergence analysis relies on the
Lipschitz property associated to the SW integrand. Several numerical
experiments demonstrate the superior performance of SHCV against
state-of-the-art methods for SW distance computation.
\\ ( https://arxiv.org/abs/2402.01493 ,  8222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01502 (*cross-listing*)
Date: Fri, 2 Feb 2024 15:36:43 GMT   (5847kb,D)

Title: Why do Random Forests Work? Understanding Tree Ensembles as
  Self-Regularizing Adaptive Smoothers
Authors: Alicia Curth and Alan Jeffares and Mihaela van der Schaar
Categories: stat.ML cs.LG
\\
  Despite their remarkable effectiveness and broad application, the drivers of
success underlying ensembles of trees are still not fully understood. In this
paper, we highlight how interpreting tree ensembles as adaptive and
self-regularizing smoothers can provide new intuition and deeper insight to
this topic. We use this perspective to show that, when studied as smoothers,
randomized tree ensembles not only make predictions that are quantifiably more
smooth than the predictions of the individual trees they consist of, but also
further regulate their smoothness at test-time based on the dissimilarity
between testing and training inputs. First, we use this insight to revisit,
refine and reconcile two recent explanations of forest success by providing a
new way of quantifying the conjectured behaviors of tree ensembles objectively
by measuring the effective degree of smoothing they imply. Then, we move beyond
existing explanations for the mechanisms by which tree ensembles improve upon
individual trees and challenge the popular wisdom that the superior performance
of forests should be understood as a consequence of variance reduction alone.
We argue that the current high-level dichotomy into bias- and
variance-reduction prevalent in statistics is insufficient to understand tree
ensembles -- because the prevailing definition of bias does not capture
differences in the expressivity of the hypothesis classes formed by trees and
forests. Instead, we show that forests can improve upon trees by three distinct
mechanisms that are usually implicitly entangled. In particular, we demonstrate
that the smoothing effect of ensembling can reduce variance in predictions due
to noise in outcome generation, reduce variability in the quality of the
learned function given fixed input data and reduce potential bias in learnable
functions by enriching the available hypothesis space.
\\ ( https://arxiv.org/abs/2402.01502 ,  5847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01509 (*cross-listing*)
Date: Fri, 2 Feb 2024 15:43:51 GMT   (1326kb,D)

Title: Advancing Brain Tumor Inpainting with Generative Models
Authors: Ruizhi Zhu, Xinru Zhang, Haowen Pang, Chundan Xu, Chuyang Ye
Categories: eess.IV cs.CV cs.LG
\\
  Synthesizing healthy brain scans from diseased brain scans offers a potential
solution to address the limitations of general-purpose algorithms, such as
tissue segmentation and brain extraction algorithms, which may not effectively
handle diseased images. We consider this a 3D inpainting task and investigate
the adaptation of 2D inpainting methods to meet the requirements of 3D magnetic
resonance imaging(MRI) data. Our contributions encompass potential
modifications tailored to MRI-specific needs, and we conducted evaluations of
multiple inpainting techniques using the BraTS2023 Inpainting datasets to
assess their efficacy and limitations.
\\ ( https://arxiv.org/abs/2402.01509 ,  1326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01520 (*cross-listing*)
Date: Fri, 2 Feb 2024 16:06:24 GMT   (102kb)

Title: Low-Resource Cross-Domain Singing Voice Synthesis via Reduced
  Self-Supervised Speech Representations
Authors: Panos Kakoulidis, Nikolaos Ellinas, Georgios Vamvoukakis, Myrsini
  Christidou, Alexandra Vioni, Georgia Maniati, Junkwang Oh, Gunu Jho, Inchul
  Hwang, Pirros Tsiakoulis, Aimilios Chalamandaris
Categories: cs.SD cs.LG eess.AS
Comments: Accepted to IEEE ICASSP SASB 2024
\\
  In this paper, we propose a singing voice synthesis model, Karaoker-SSL, that
is trained only on text and speech data as a typical multi-speaker acoustic
model. It is a low-resource pipeline that does not utilize any singing data
end-to-end, since its vocoder is also trained on speech data. Karaoker-SSL is
conditioned by self-supervised speech representations in an unsupervised
manner. We preprocess these representations by selecting only a subset of their
task-correlated dimensions. The conditioning module is indirectly guided to
capture style information during training by multi-tasking. This is achieved
with a Conformer-based module, which predicts the pitch from the acoustic
model's output. Thus, Karaoker-SSL allows singing voice synthesis without
reliance on hand-crafted and domain-specific features. There are also no
requirements for text alignments or lyrics timestamps. To refine the voice
quality, we employ a U-Net discriminator that is conditioned on the target
speaker and follows a Diffusion GAN training scheme.
\\ ( https://arxiv.org/abs/2402.01520 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01524 (*cross-listing*)
Date: Fri, 2 Feb 2024 16:10:29 GMT   (9792kb,D)

Title: HyperPlanes: Hypernetwork Approach to Rapid NeRF Adaptation
Authors: Pawe{\l} Batorski, Dawid Malarz, Marcin Przewi\k{e}\'zlikowski, Marcin
  Mazur, S{\l}awomir Tadeja, Przemys{\l}aw Spurek
Categories: cs.CV cs.LG
\\
  Neural radiance fields (NeRFs) are a widely accepted standard for
synthesizing new 3D object views from a small number of base images. However,
NeRFs have limited generalization properties, which means that we need to use
significant computational resources to train individual architectures for each
item we want to represent. To address this issue, we propose a few-shot
learning approach based on the hypernetwork paradigm that does not require
gradient optimization during inference. The hypernetwork gathers information
from the training data and generates an update for universal weights. As a
result, we have developed an efficient method for generating a high-quality 3D
object representation from a small number of images in a single step. This has
been confirmed by direct comparison with the state-of-the-art solutions and a
comprehensive ablation study.
\\ ( https://arxiv.org/abs/2402.01524 ,  9792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01542 (*cross-listing*)
Date: Fri, 2 Feb 2024 16:35:02 GMT   (2291kb,D)

Title: Learning Collective Variables for Protein Folding with Labeled Data
  Augmentation through Geodesic Interpolation
Authors: Soojung Yang, Juno Nam, Johannes C. B. Dietschreit, Rafael
  G\'omez-Bombarelli
Categories: physics.chem-ph cs.LG q-bio.BM
\\
  In molecular dynamics (MD) simulations, rare events, such as protein folding,
are typically studied by means of enhanced sampling techniques, most of which
rely on the definition of a collective variable (CV) along which the
acceleration occurs. Obtaining an expressive CV is crucial, but often hindered
by the lack of information about the particular event, e.g., the transition
from unfolded to folded conformation. We propose a simulation-free data
augmentation strategy using physics-inspired metrics to generate geodesic
interpolations resembling protein folding transitions, thereby improving
sampling efficiency without true transition state samples. Leveraging
interpolation progress parameters, we introduce a regression-based learning
scheme for CV models, which outperforms classifier-based methods when
transition state data is limited and noisy
\\ ( https://arxiv.org/abs/2402.01542 ,  2291kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01571 (*cross-listing*)
Date: Fri, 2 Feb 2024 17:07:39 GMT   (3620kb,D)

Title: Spiking Music: Audio Compression with Event Based Auto-encoders
Authors: Martim Lisboa, Guillaume Bellec
Categories: cs.SD cs.LG cs.NE eess.AS
\\
  Neurons in the brain communicate information via punctual events called
spikes. The timing of spikes is thought to carry rich information, but it is
not clear how to leverage this in digital systems. We demonstrate that
event-based encoding is efficient for audio compression. To build this
event-based representation we use a deep binary auto-encoder, and under high
sparsity pressure, the model enters a regime where the binary event matrix is
stored more efficiently with sparse matrix storage algorithms. We test this on
the large MAESTRO dataset of piano recordings against vector quantized
auto-encoders. Not only does our "Spiking Music compression" algorithm achieve
a competitive compression/reconstruction trade-off, but selectivity and
synchrony between encoded events and piano key strikes emerge without
supervision in the sparse regime.
\\ ( https://arxiv.org/abs/2402.01571 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01598 (*cross-listing*)
Date: Fri, 2 Feb 2024 17:51:49 GMT   (609kb,D)

Title: Learning from Two Decades of Blood Pressure Data: Demography-Specific
  Patterns Across 75 Million Patient Encounters
Authors: Seyedeh Somayyeh Mousavi and Yuting Guo and Abeed Sarker and Reza
  Sameni
Categories: q-bio.PE cs.LG q-bio.QM stat.AP
\\
  Hypertension remains a global health concern with a rising prevalence,
necessitating effective monitoring and understanding of blood pressure (BP)
dynamics. This study delves into the wealth of information derived from BP
measurement, a crucial approach in informing our understanding of hypertensive
trends. Numerous studies have reported on the relationship between BP variation
and various factors. In this research, we leveraged an extensive dataset
comprising 75 million records spanning two decades, offering a unique
opportunity to explore and analyze BP variations across demographic features
such as age, race, and gender. Our findings revealed that gender-based BP
variation was not statistically significant, challenging conventional
assumptions. Interestingly, systolic blood pressure (SBP) consistently
increased with age, while diastolic blood pressure (DBP) displayed a
distinctive peak in the forties age group. Moreover, our analysis uncovered
intriguing similarities in the distribution of BP among some of the racial
groups. This comprehensive investigation contributes to the ongoing discourse
on hypertension and underscores the importance of considering diverse
demographic factors in understanding BP variations. Our results provide
valuable insights that may inform personalized healthcare approaches tailored
to specific demographic profiles.
\\ ( https://arxiv.org/abs/2402.01598 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01617 (*cross-listing*)
Date: Fri, 2 Feb 2024 18:27:21 GMT   (29434kb,D)

Title: A GP-based Robust Motion Planning Framework for Agile Autonomous Robot
  Navigation and Recovery in Unknown Environments
Authors: Nicholas Mohammad, Jacob Higgins, Nicola Bezzo
Categories: cs.RO cs.LG
Comments: To Appear in 2024 IEEE/RSJ International Conference on Robotics and
  Automation (ICRA), 2024
\\
  For autonomous mobile robots, uncertainties in the environment and system
model can lead to failure in the motion planning pipeline, resulting in
potential collisions. In order to achieve a high level of robust autonomy,
these robots should be able to proactively predict and recover from such
failures. To this end, we propose a Gaussian Process (GP) based model for
proactively detecting the risk of future motion planning failure. When this
risk exceeds a certain threshold, a recovery behavior is triggered that
leverages the same GP model to find a safe state from which the robot may
continue towards the goal. The proposed approach is trained in simulation only
and can generalize to real world environments on different robotic platforms.
Simulations and physical experiments demonstrate that our framework is capable
of both predicting planner failures and recovering the robot to states where
planner success is likely, all while producing agile motion.
\\ ( https://arxiv.org/abs/2402.01617 ,  29434kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01635 (*cross-listing*)
Date: Fri, 2 Feb 2024 18:54:18 GMT   (1112kb,D)

Title: kNN Algorithm for Conditional Mean and Variance Estimation with
  Automated Uncertainty Quantification and Variable Selection
Authors: Marcos Matabuena, Juan C. Vidal, Oscar Hernan Madrid Padilla,
  Jukka-Pekka Onnela
Categories: stat.ME cs.LG stat.CO stat.ML
\\
  In this paper, we introduce a kNN-based regression method that synergizes the
scalability and adaptability of traditional non-parametric kNN models with a
novel variable selection technique. This method focuses on accurately
estimating the conditional mean and variance of random response variables,
thereby effectively characterizing conditional distributions across diverse
scenarios.Our approach incorporates a robust uncertainty quantification
mechanism, leveraging our prior estimation work on conditional mean and
variance. The employment of kNN ensures scalable computational efficiency in
predicting intervals and statistical accuracy in line with optimal
non-parametric rates. Additionally, we introduce a new kNN semi-parametric
algorithm for estimating ROC curves, accounting for covariates. For selecting
the smoothing parameter k, we propose an algorithm with theoretical
guarantees.Incorporation of variable selection enhances the performance of the
method significantly over conventional kNN techniques in various modeling
tasks. We validate the approach through simulations in low, moderate, and
high-dimensional covariate spaces. The algorithm's effectiveness is
particularly notable in biomedical applications as demonstrated in two case
studies. Concluding with a theoretical analysis, we highlight the consistency
and convergence rate of our method over traditional kNN models, particularly
when the underlying regression model takes values in a low-dimensional space.
\\ ( https://arxiv.org/abs/2402.01635 ,  1112kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2206.06854
replaced with revised version Fri, 2 Feb 2024 08:59:26 GMT   (6528kb,D)

Title: On the explainable properties of 1-Lipschitz Neural Networks: An Optimal
  Transport Perspective
Authors: Mathieu Serrurier (IRIT-ADRIA, UT), Franck Mamalet (UT), Thomas Fel
  (UT), Louis B\'ethune (UT3, UT, IRIT-ADRIA), Thibaut Boissin (UT)
Categories: cs.AI cs.CR cs.CV cs.LG stat.ML
Journal-ref: Conference on Neural Information Processing Systems (NeurIPS),
  Neural Information Processing Systems Foundation, Dec 2023, New Orleans
  (Louisiana), United States
\\ ( https://arxiv.org/abs/2206.06854 ,  6528kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03731
replaced with revised version Thu, 1 Feb 2024 21:39:50 GMT   (815kb,D)

Title: Working Memory Capacity of ChatGPT: An Empirical Study
Authors: Dongyu Gong, Xingchen Wan, Dingmin Wang
Categories: cs.AI cs.CL q-bio.NC
Comments: Accepted at the 38th AAAI Conference on Artificial Intelligence
  (AAAI-24)
\\ ( https://arxiv.org/abs/2305.03731 ,  815kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07722
replaced with revised version Thu, 1 Feb 2024 23:05:51 GMT   (602kb,D)

Title: In Search of Verifiability: Explanations Rarely Enable Complementary
  Performance in AI-Advised Decision Making
Authors: Raymond Fok, Daniel S. Weld
Categories: cs.AI cs.HC
Comments: 10 pages, 6 figures, 1 table, working paper
\\ ( https://arxiv.org/abs/2305.07722 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01403
replaced with revised version Thu, 1 Feb 2024 23:12:41 GMT   (22347kb,D)

Title: Learning Multi-Agent Communication with Contrastive Learning
Authors: Yat Long Lo, Biswa Sengupta, Jakob Foerster, Michael Noukhovitch
Categories: cs.AI cs.LG
Comments: The 12th International Conference on Learning Representations (ICLR)
\\ ( https://arxiv.org/abs/2307.01403 ,  22347kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00300
replaced with revised version Fri, 2 Feb 2024 03:53:12 GMT   (2712kb,D)

Title: Towards the Identifiability and Explainability for Personalized Learner
  Modeling: An Inductive Paradigm
Authors: Jiatong Li, Qi Liu, Fei Wang, Jiayu Liu, Zhenya Huang, Fangzhou Yao,
  Linbo Zhu, Yu Su
Categories: cs.AI
Comments: Accepted by Proceedings of the ACM Web Conference 2024 (WWW '24)
\\ ( https://arxiv.org/abs/2309.00300 ,  2712kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10293
replaced with revised version Fri, 2 Feb 2024 08:07:40 GMT   (1371kb,D)

Title: QXAI: Explainable AI Framework for Quantitative Analysis in Patient
  Monitoring Systems
Authors: Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Juan D. Velasquez,
  Niall Higgins
Categories: cs.AI
Comments: This work has been submitted to the ELSEVIER for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible
\\ ( https://arxiv.org/abs/2309.10293 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15443
replaced with revised version Fri, 2 Feb 2024 08:57:16 GMT   (1096kb,D)

Title: DiffuserLite: Towards Real-time Diffusion Planning
Authors: Zibin Dong, Jianye Hao, Yifu Yuan, Fei Ni, Yitian Wang, Pengyi Li and
  Yan Zheng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.15443 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17527
replaced with revised version Fri, 2 Feb 2024 05:54:58 GMT   (850kb,D)

Title: Learning to Stop Cut Generation for Efficient Mixed-Integer Linear
  Programming
Authors: Haotian Ling, Zhihai Wang, Jie Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.17527 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00591
replaced with revised version Fri, 2 Feb 2024 08:58:41 GMT   (875kb,D)

Title: Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
Authors: Nicolas Lazzari, Stefano De Giorgis, Aldo Gangemi, Valentina Presutti
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.00591 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10923
replaced with revised version Fri, 2 Feb 2024 14:06:28 GMT   (8252kb,D)

Title: Language Models as Inductive Reasoners
Authors: Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong
  Liu, Jianfeng Gao, Furu Wei
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024 (main)
\\ ( https://arxiv.org/abs/2212.10923 ,  8252kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07303
replaced with revised version Fri, 2 Feb 2024 10:10:40 GMT   (391kb,D)

Title: Multi-Relational Hyperbolic Word Embeddings from Natural Language
  Definitions
Authors: Marco Valentino, Danilo S. Carvalho, Andr\'e Freitas
Categories: cs.CL cs.LG
Comments: Accepted at the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024), camera-ready
\\ ( https://arxiv.org/abs/2305.07303 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12641
replaced with revised version Fri, 2 Feb 2024 07:57:05 GMT   (513kb,D)

Title: A Comprehensive Survey of Sentence Representations: From the BERT Epoch
  to the ChatGPT Era and Beyond
Authors: Abhinav Ramesh Kashyap, Thanh-Tung Nguyen, Viktor Schlegel, Stefan
  Winkler, See-Kiong Ng, Soujanya Poria
Categories: cs.CL
Comments: Accepted to EACL'24
\\ ( https://arxiv.org/abs/2305.12641 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14279
replaced with revised version Fri, 2 Feb 2024 18:37:07 GMT   (8767kb,D)

Title: Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs
Authors: Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen
  Zhao, Samuel R. Bowman, Kyunghyun Cho
Categories: cs.CL
Comments: Accepted to TMLR: https://openreview.net/forum?id=5nBqY1y96B
\\ ( https://arxiv.org/abs/2305.14279 ,  8767kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17026
replaced with revised version Fri, 2 Feb 2024 18:04:58 GMT   (935kb,D)

Title: How Powerful are Decoder-Only Transformer Neural Models?
Authors: Jesse Roberts
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2305.17026 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09896
replaced with revised version Fri, 2 Feb 2024 18:31:34 GMT   (1255kb,D)

Title: Is Self-Repair a Silver Bullet for Code Generation?
Authors: Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao,
  Armando Solar-Lezama
Categories: cs.CL cs.AI cs.PL cs.SE
Comments: Accepted to ICLR 2024. Added additional Code Llama experiments and
  fixed a data processing error harming Code Llama's reported self-repair
  performance on HumanEval
\\ ( https://arxiv.org/abs/2306.09896 ,  1255kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16513
replaced with revised version Fri, 2 Feb 2024 12:16:12 GMT   (522kb)

Title: Deception Abilities Emerged in Large Language Models
Authors: Thilo Hagendorff
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2307.16513 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01606
replaced with revised version Fri, 2 Feb 2024 14:15:32 GMT   (8571kb,D)

Title: Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese
  Geographic Re-Ranking
Authors: Yong Cao, Ruixue Ding, Boli Chen, Xianzhi Li, Min Chen, Daniel
  Hershcovich, Pengjun Xie, and Fei Huang
Categories: cs.CL
Comments: 15 pages, 5 figures, EACL 2024 main
\\ ( https://arxiv.org/abs/2309.01606 ,  8571kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11085
replaced with revised version Fri, 2 Feb 2024 13:50:42 GMT   (277kb,D)

Title: Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained
  Language Models
Authors: Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.11085 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11244
replaced with revised version Thu, 1 Feb 2024 19:05:44 GMT   (518kb,D)

Title: Entity Matching using Large Language Models
Authors: Ralph Peeters, Christian Bizer
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2310.11244 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05112
replaced with revised version Fri, 2 Feb 2024 06:48:24 GMT   (417kb,D)

Title: A Survey of Large Language Models in Medicine: Principles, Applications,
  and Challenges
Authors: Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge
  Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng
  Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo,
  David A. Clifton
Categories: cs.CL cs.AI
Comments: Preprint. Version 3. 54 pages
\\ ( https://arxiv.org/abs/2311.05112 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11855
replaced with revised version Fri, 2 Feb 2024 08:28:01 GMT   (4315kb,D)

Title: Evil Geniuses: Delving into the Safety of LLM-based Agents
Authors: Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, Hang Su
Categories: cs.CL
Comments: 11 pages
\\ ( https://arxiv.org/abs/2311.11855 ,  4315kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02783
replaced with revised version Thu, 1 Feb 2024 22:51:24 GMT   (6984kb,D)

Title: Large Language Models on Graphs: A Comprehensive Survey
Authors: Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han
Categories: cs.CL cs.LG
Comments: 24 pages
\\ ( https://arxiv.org/abs/2312.02783 ,  6984kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09043
replaced with revised version Fri, 2 Feb 2024 17:40:52 GMT   (370kb,D)

Title: Topic Bias in Emotion Classification
Authors: Maximilian Wegge and Roman Klinger
Categories: cs.CL
Comments: accepted to W-NUT at EACL 2024
\\ ( https://arxiv.org/abs/2312.09043 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13382
replaced with revised version Fri, 2 Feb 2024 18:20:03 GMT   (52kb)

Title: DSPy Assertions: Computational Constraints for Self-Refining Language
  Model Pipelines
Authors: Arnav Singhvi, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik
  Sen, Matei Zaharia, Omar Khattab
Categories: cs.CL cs.AI cs.PL
Comments: Arnav*, Manish*, Shangyin* contributed equally to this work
\\ ( https://arxiv.org/abs/2312.13382 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16903
replaced with revised version Fri, 2 Feb 2024 10:37:53 GMT   (1129kb,D)

Title: Spike No More: Stabilizing the Pre-training of Large Language Models
Authors: Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2312.16903 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03462
replaced with revised version Fri, 2 Feb 2024 12:34:25 GMT   (162kb,D)

Title: Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
Authors: Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng
  Dou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.03462 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04592
replaced with revised version Fri, 2 Feb 2024 09:36:58 GMT   (104kb)

Title: An Assessment on Comprehending Mental Health through Large Language
  Models
Authors: Mihael Arcan, David-Paul Niland and Fionn Delahunty
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.04592 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08417
replaced with revised version Fri, 2 Feb 2024 09:10:11 GMT   (1308kb,D)

Title: Contrastive Preference Optimization: Pushing the Boundaries of LLM
  Performance in Machine Translation
Authors: Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen,
  Benjamin Van Durme, Kenton Murray, Young Jin Kim
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.08417 ,  1308kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09407
replaced with revised version Thu, 1 Feb 2024 22:35:29 GMT   (14125kb,D)

Title: Deciphering Textual Authenticity: A Generalized Strategy through the
  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated
  Text
Authors: Mazal Bethany, Brandon Wherry, Emet Bethany, Nishant Vishwamitra,
  Anthony Rios, Peyman Najafirad
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.09407 ,  14125kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10352
replaced with revised version Fri, 2 Feb 2024 12:35:15 GMT   (10858kb,D)

Title: Bridging Cultural Nuances in Dialogue Agents through Cultural Value
  Surveys
Authors: Yong Cao, Min Chen, Daniel Hershcovich
Categories: cs.CL
Comments: 17pages, 7 figures, EACL 2024 findings
\\ ( https://arxiv.org/abs/2401.10352 ,  10858kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11819
replaced with revised version Fri, 2 Feb 2024 02:35:13 GMT   (5773kb,D)

Title: SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in
  Chinese
Authors: Liang Xu, Hang Xue, Lei Zhu, Kangkang Zhao
Categories: cs.CL cs.AI
Comments: Dataset revised and finalized, results updated with new model; 8
  pages, 7 figures, 4 tables
\\ ( https://arxiv.org/abs/2401.11819 ,  5773kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15316
replaced with revised version Fri, 2 Feb 2024 14:06:31 GMT   (246kb,D)

Title: UNSEE: Unsupervised Non-contrastive Sentence Embeddings
Authors: \"Omer Veysel \c{C}a\u{g}atan
Categories: cs.CL
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2401.15316 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15378
replaced with revised version Thu, 1 Feb 2024 20:28:11 GMT   (991kb)

Title: A RAG-based Question Answering System Proposal for Understanding Islam:
  MufassirQAS LLM
Authors: Ahmet Yusuf Alan, Enis Karaarslan, \"Omer Aydin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.15378 ,  991kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16184
replaced with revised version Fri, 2 Feb 2024 04:14:26 GMT   (4344kb,D)

Title: On the Semantics of LM Latent Space: A Vocabulary-defined Approach
Authors: Jian Gu, Chunyang Chen, Aldeida Aleti
Categories: cs.CL cs.LG
Comments: under peer review
\\ ( https://arxiv.org/abs/2401.16184 ,  4344kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16405
replaced with revised version Fri, 2 Feb 2024 14:53:14 GMT   (185kb,D)

Title: Scaling Sparse Fine-Tuning to Large Language Models
Authors: Alan Ansell and Ivan Vuli\'c and Hannah Sterz and Anna Korhonen and
  Edoardo M. Ponti
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.16405 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16578
replaced with revised version Fri, 2 Feb 2024 17:28:22 GMT   (1758kb,D)

Title: Leveraging Professional Radiologists' Expertise to Enhance LLMs'
  Evaluation for Radiology Reports
Authors: Qingqing Zhu, Xiuying Chen, Qiao Jin, Benjamin Hou, Tejas Sudharshan
  Mathai, Pritam Mukherjee, Xin Gao, Ronald M Summers, Zhiyong Lu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.16578 ,  1758kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16587
replaced with revised version Fri, 2 Feb 2024 16:47:16 GMT   (7173kb,D)

Title: A Linguistic Comparison between Human and ChatGPT-Generated
  Conversations
Authors: Morgan Sandler, Hyesun Choung, Arun Ross, Prabu David
Categories: cs.CL cs.AI cs.CY
Comments: Preprint. Pending review and feedback from ICPRAI2024
\\ ( https://arxiv.org/abs/2401.16587 ,  7173kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16727
replaced with revised version Fri, 2 Feb 2024 04:07:25 GMT   (8359kb,D)

Title: Recent Advances in Hate Speech Moderation: Multimodality and the Role of
  Large Models
Authors: Ming Shan Hee, Shivam Sharma, Rui Cao, Palash Nandi, Tanmoy
  Chakraborty, Roy Ka-Wei Lee
Categories: cs.CL
Comments: Preprint; Under-Review
\\ ( https://arxiv.org/abs/2401.16727 ,  8359kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17498
replaced with revised version Thu, 1 Feb 2024 20:43:02 GMT   (721kb,D)

Title: Improving QA Model Performance with Cartographic Inoculation
Authors: Allen Chen (UT Austin), Okan Tanrikulu (UT Austin)
Categories: cs.CL
Comments: 9 pages, 6 figures
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2401.17498 ,  721kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00559
replaced with revised version Fri, 2 Feb 2024 07:32:41 GMT   (8386kb,D)

Title: A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for
  Verifiers of Reasoning Chains
Authors: Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or
  Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva
Categories: cs.CL
Comments: Dataset at https://huggingface.co/datasets/google/reveal
\\ ( https://arxiv.org/abs/2402.00559 ,  8386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00786
replaced with revised version Fri, 2 Feb 2024 17:43:41 GMT   (4612kb,D)

Title: CroissantLLM: A Truly Bilingual French-English Language Model
Authors: Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, Ant\'onio Loison,
  Duarte M. Alves, Caio Corro, Nicolas Boizard, Jo\~ao Alves, Ricardo Rei,
  Pedro H. Martins, Antoni Bigata Casademunt, Fran\c{c}ois Yvon, Andr\'e F.T.
  Martins, Gautier Viaud, C\'eline Hudelot, Pierre Colombo
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.00786 ,  4612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00856
replaced with revised version Fri, 2 Feb 2024 15:50:10 GMT   (4818kb,D)

Title: Towards Efficient and Exact Optimization of Language Model Alignment
Authors: Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie
  Tang, Minlie Huang
Categories: cs.CL
Comments: 24 pages, 9 figures
\\ ( https://arxiv.org/abs/2402.00856 ,  4818kb)
------------------------------------------------------------------------------
\\
arXiv:2110.03155
replaced with revised version Fri, 2 Feb 2024 18:31:23 GMT   (20098kb,D)

Title: The Benefits of Being Categorical Distributional: Uncertainty-aware
  Regularized Exploration in Reinforcement Learning
Authors: Ke Sun, Yingnan Zhao, Enze Shi, Yafei Wang, Xiaodong Yan, Bei Jiang,
  Linglong Kong
Categories: cs.LG
\\ ( https://arxiv.org/abs/2110.03155 ,  20098kb)
------------------------------------------------------------------------------
\\
arXiv:2202.00769
replaced with revised version Fri, 2 Feb 2024 17:59:50 GMT   (23002kb,D)

Title: Distributional Reinforcement Learning by Sinkhorn Divergence
Authors: Ke Sun, Yingnan Zhao, Wulong Liu, Bei Jiang, Linglong Kong
Categories: cs.LG stat.ML
Comments: arXiv admin note: text overlap with arXiv:2110.03155
\\ ( https://arxiv.org/abs/2202.00769 ,  23002kb)
------------------------------------------------------------------------------
\\
arXiv:2206.11004
replaced with revised version Fri, 2 Feb 2024 01:05:42 GMT   (2686kb,D)

Title: Auto-Encoding Adversarial Imitation Learning
Authors: Kaifeng Zhang, Rui Zhao, Ziming Zhang, Yang Gao
Categories: cs.LG
Comments: 13 pages
\\ ( https://arxiv.org/abs/2206.11004 ,  2686kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09404
replaced with revised version Thu, 1 Feb 2024 19:30:47 GMT   (3724kb,D)

Title: Measures of Information Reflect Memorization Patterns
Authors: Rachit Bansal, Danish Pruthi, Yonatan Belinkov
Categories: cs.LG cs.IT math.IT
Comments: 22 pages; NeurIPS 2022. Code and data at
  https://rachitbansal.github.io/information-measures
\\ ( https://arxiv.org/abs/2210.09404 ,  3724kb)
------------------------------------------------------------------------------
\\
arXiv:2210.13954
replaced with revised version Fri, 2 Feb 2024 13:56:21 GMT   (2326kb,D)

Title: I Prefer not to Say: Protecting User Consent in Models with Optional
  Personal Data
Authors: Tobias Leemann, Martin Pawelczyk, Christian Thomas Eberle, Gjergji
  Kasneci
Categories: cs.LG cs.AI cs.CY stat.ML
Comments: v5: AAAI-24 Camera-Ready Version Including Appendices. v1: NeurIPS
  2022 Workshop on Algorithmic Fairness through the Lens of Causality and
  Privacy (AFCP)
\\ ( https://arxiv.org/abs/2210.13954 ,  2326kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04411
replaced with revised version Thu, 1 Feb 2024 21:32:04 GMT   (1087kb,D)

Title: Motif-guided Time Series Counterfactual Explanations
Authors: Peiyu Li, Soukaina Filali Boubrahimi, Shah Muhammad Hamdi
Categories: cs.LG
Comments: 13 pages, accepted at 2-nd Workshop on Explainable and Ethical AI -
  ICPR 2022
\\ ( https://arxiv.org/abs/2211.04411 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00133
replaced with revised version Fri, 2 Feb 2024 00:00:02 GMT   (850kb,D)

Title: Generative Adversarial Learning of Sinkhorn Algorithm Initializations
Authors: Jonathan Geuter, Vaios Laschos
Categories: cs.LG math.OC stat.ML
Comments: 15 pages, 9 figures
MSC-class: 68T07 (Primary) 90C08 (Secondary)
ACM-class: I.2.6; G.3; G.4
\\ ( https://arxiv.org/abs/2212.00133 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10777
replaced with revised version Thu, 1 Feb 2024 23:34:04 GMT   (9753kb,D)

Title: Hierarchically branched diffusion models leverage dataset structure for
  class-conditional generation
Authors: Alex M. Tseng, Max Shen, Tommaso Biancalani, Gabriele Scalia
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2212.10777 ,  9753kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13734
replaced with revised version Fri, 2 Feb 2024 16:42:19 GMT   (3192kb,D)

Title: Improving Monte Carlo Evaluation with Offline Data
Authors: Shuze Liu, Shangtong Zhang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.13734 ,  3192kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05092
replaced with revised version Fri, 2 Feb 2024 16:18:10 GMT   (1575kb,D)

Title: Task Aware Dreamer for Task Generalization in Reinforcement Learning
Authors: Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Songming Liu,
  Dong Yan, Jun Zhu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.05092 ,  1575kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07988
replaced with revised version Fri, 2 Feb 2024 14:43:40 GMT   (20368kb,D)

Title: Unbalanced and Light Optimal Transport
Authors: Milena Gazdieva, Arip Asadulaev, Alexander Korotin, Evgeny Burnaev
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.07988 ,  20368kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03674
replaced with revised version Fri, 2 Feb 2024 18:04:02 GMT   (1450kb,D)

Title: Machine Learning with Requirements: a Manifesto
Authors: Eleonora Giunchiglia, Fergus Imrie, Mihaela van der Schaar, Thomas
  Lukasiewicz
Categories: cs.LG cs.AI cs.SE
\\ ( https://arxiv.org/abs/2304.03674 ,  1450kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15659
replaced with revised version Fri, 2 Feb 2024 16:38:35 GMT   (738kb,D)

Title: How to escape sharp minima with random perturbations
Authors: Kwangjun Ahn, Ali Jadbabaie, Suvrit Sra
Categories: cs.LG cs.AI math.OC
Comments: Comments would be appreciated!
\\ ( https://arxiv.org/abs/2305.15659 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15927
replaced with revised version Fri, 2 Feb 2024 13:53:56 GMT   (1787kb,D)

Title: Learning Directed Graphical Models with Optimal Transport
Authors: Vy Vo, Trung Le, Long-Tung Vuong, He Zhao, Edwin Bonilla, Dinh Phung
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2305.15927 ,  1787kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06534
replaced with revised version Fri, 2 Feb 2024 04:05:21 GMT   (5580kb,D)

Title: K-Tensors: Clustering Positive Semi-Definite Matrices
Authors: Hanchao Zhang, Baoyi Shi, Thaddeus Tarpey
Categories: cs.LG stat.ME
\\ ( https://arxiv.org/abs/2306.06534 ,  5580kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09912
replaced with revised version Fri, 2 Feb 2024 03:27:33 GMT   (1204kb)

Title: Towards Quantum Federated Learning
Authors: Chao Ren, Han Yu, Rudai Yan, Minrui Xu, Yuan Shen, Huihui Zhu, Dusit
  Niyato, Zhao Yang Dong, Leong Chuan Kwek
Categories: cs.LG quant-ph
Comments: Survey of quantum federated learning (QFL)
\\ ( https://arxiv.org/abs/2306.09912 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03813
replaced with revised version Fri, 2 Feb 2024 06:04:46 GMT   (271kb)

Title: Controlling Chaotic Maps using Next-Generation Reservoir Computing
Authors: Robert M. Kent and Wendson A. S. Barbosa and Daniel J. Gauthier
Categories: cs.LG cs.NE cs.SY eess.SY nlin.CD
Comments: 9 pages, 8 figures
Journal-ref: Chaos 34, 023102 (2024)
DOI: 10.1063/5.0165864
\\ ( https://arxiv.org/abs/2307.03813 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04870
replaced with revised version Fri, 2 Feb 2024 12:59:24 GMT   (55kb,D)

Title: RACH-Space: Reconstructing Adaptive Convex Hull Space with Applications
  in Weak Supervision
Authors: Woojoo Na, Abiy Tasissa
Categories: cs.LG math.OC
Comments: 12 pages
\\ ( https://arxiv.org/abs/2307.04870 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16304
replaced with revised version Fri, 2 Feb 2024 12:06:21 GMT   (250kb,D)

Title: You Shall Pass: Dealing with the Zero-Gradient Problem in Predict and
  Optimize for Convex Optimization
Authors: Grigorii Veviurko, Wendelin B\"ohmer, and Mathijs de Weerdt
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2307.16304 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08480
replaced with revised version Fri, 2 Feb 2024 13:57:48 GMT   (4048kb,D)

Title: Label Propagation Techniques for Artifact Detection in Imbalanced
  Classes using Photoplethysmogram Signals
Authors: Clara Macabiau, Thanh-Dung Le, Kevin Albert, Philippe Jouvet, Rita
  Noumeir
Categories: cs.LG eess.SP
Comments: Under preparation to submit to IEEE for possible publications
MSC-class: 68T02
\\ ( https://arxiv.org/abs/2308.08480 ,  4048kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10186
replaced with revised version Fri, 2 Feb 2024 07:53:34 GMT   (643kb,D)

Title: Graph-enabled Reinforcement Learning for Time Series Forecasting with
  Adaptive Intelligence
Authors: Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Jianming Yong, and
  Yuefeng Li
Categories: cs.LG cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2309.10186 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10283
replaced with revised version Fri, 2 Feb 2024 07:49:27 GMT   (4008kb,D)

Title: FRAMU: Attention-based Machine Unlearning using Federated Reinforcement
  Learning
Authors: Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, Taotao Cai, Xiaofeng
  Zhu, and Qing Li
Categories: cs.LG cs.AI cs.CR
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2309.10283 ,  4008kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02063
replaced with revised version Fri, 2 Feb 2024 11:52:22 GMT   (81kb,D)

Title: Lessons Learned from EXMOS User Studies: A Technical Report Summarizing
  Key Takeaways from User Studies Conducted to Evaluate The EXMOS Platform
Authors: Aditya Bhattacharya, Simone Stumpf, Lucija Gosak, Gregor Stiglic,
  Katrien Verbert
Categories: cs.LG cs.HC
Comments: It is a technical report only. The contents are not peer-reviewed.
  Please reach out to the main author for any questions
\\ ( https://arxiv.org/abs/2310.02063 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02258
replaced with revised version Fri, 2 Feb 2024 02:09:51 GMT   (1237kb,D)

Title: A Neural Scaling Law from Lottery Ticket Ensembling
Authors: Ziming Liu, Max Tegmark
Categories: cs.LG cs.AI physics.data-an stat.ML
Comments: 14 pages, 13 figures. Note from authors: the theory in this paper is
  questionable; we are trying our best to fix it. Empirical results still stand
\\ ( https://arxiv.org/abs/2310.02258 ,  1237kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05495
replaced with revised version Fri, 2 Feb 2024 15:04:51 GMT   (275kb,D)

Title: On the Convergence of Federated Averaging under Partial Participation
  for Over-parameterized Neural Networks
Authors: Xin Liu, Wei li, Dazhi Zhan, Yu Pan, Xin Ma, Yu Ding, Zhisong Pan
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.05495 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06380
replaced with revised version Fri, 2 Feb 2024 17:31:05 GMT   (939kb,D)

Title: CAST: Cluster-Aware Self-Training for Tabular Data
Authors: Minwook Kim, Juseong Kim, Ki Beom Kim, Giltae Song
Categories: cs.LG
Comments: 10 pages for main body, and 16 additional pages for reference and
  appendix
\\ ( https://arxiv.org/abs/2310.06380 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10107
replaced with revised version Thu, 1 Feb 2024 22:52:24 GMT   (37kb)

Title: Regret Analysis of the Posterior Sampling-based Learning Algorithm for
  Episodic POMDPs
Authors: Dengwang Tang, Rahul Jain, Ashutosh Nayyar, Pierluigi Nuzzo
Categories: cs.LG cs.AI cs.SY eess.SY stat.ML
Comments: 35 pages, 1 figure
MSC-class: 93E35
\\ ( https://arxiv.org/abs/2310.10107 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11531
replaced with revised version Thu, 1 Feb 2024 22:58:28 GMT   (31kb)

Title: Efficient Online Learning with Offline Datasets for Infinite Horizon
  MDPs: A Bayesian Approach
Authors: Dengwang Tang, Rahul Jain, Botao Hao, Zheng Wen
Categories: cs.LG cs.AI cs.SY eess.SY stat.ML
Comments: 22 pages
MSC-class: 93E35
\\ ( https://arxiv.org/abs/2310.11531 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11714
replaced with revised version Fri, 2 Feb 2024 05:42:13 GMT   (7396kb,D)

Title: On the Evaluation of Generative Models in Distributed Learning Tasks
Authors: Zixiao Wang, Farzan Farnia, Zhenghao Lin, Yunheng Shen, Bei Yu
Categories: cs.LG
Comments: 20 pages, 20 figures
\\ ( https://arxiv.org/abs/2310.11714 ,  7396kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13164
replaced with revised version Fri, 2 Feb 2024 16:43:57 GMT   (650kb,D)

Title: Almost Equivariance via Lie Algebra Convolutions
Authors: Daniel McNeela
Categories: cs.LG stat.ML
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2310.13164 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13836
replaced with revised version Fri, 2 Feb 2024 18:07:37 GMT   (13860kb,D)

Title: Foundation Model's Embedded Representations May Detect Distribution
  Shift
Authors: Max Vargas, Adam Tsou, Andrew Engel, Tony Chiang
Categories: cs.LG cs.CL
Comments: 17 pages, 8 figures, 5 tables
\\ ( https://arxiv.org/abs/2310.13836 ,  13860kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13886
replaced with revised version Fri, 2 Feb 2024 18:35:11 GMT   (9603kb,D)

Title: Nonlinear Filtering with Brenier Optimal Transport Maps
Authors: Mohammad Al-Jarrah, Niyizhen Jin, Bamdad Hosseini, Amirhossein
  Taghvaei
Categories: cs.LG math.OC
Comments: 25 pages, 16 figures, 1 Table
\\ ( https://arxiv.org/abs/2310.13886 ,  9603kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15351
replaced with revised version Fri, 2 Feb 2024 15:28:17 GMT   (398kb,D)

Title: Random Exploration in Bayesian Optimization: Order-Optimal Regret and
  Computational Efficiency
Authors: Sudeep Salgia, Sattar Vakili, Qing Zhao
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.15351 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00168
replaced with revised version Fri, 2 Feb 2024 03:41:50 GMT   (2871kb,D)

Title: The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from
  Human Feedback
Authors: Nathan Lambert and Roberto Calandra
Categories: cs.LG
Comments: 11 pages, 5 figures
\\ ( https://arxiv.org/abs/2311.00168 ,  2871kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02516
replaced with revised version Fri, 2 Feb 2024 09:46:20 GMT   (2679kb,D)

Title: Forward $\chi^2$ Divergence Based Variational Importance Sampling
Authors: Chengrui Li, Yule Wang, Weihan Li and Anqi Wu
Categories: cs.LG stat.CO stat.ML
\\ ( https://arxiv.org/abs/2311.02516 ,  2679kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02818
replaced with revised version Fri, 2 Feb 2024 00:45:38 GMT   (2344kb,D)

Title: Signal Processing Meets SGD: From Momentum to Filter
Authors: Zhipeng Yao, Yu Zhang, Dazhou Li
Categories: cs.LG eess.SP
Comments: arXiv admin note: text overlap with arXiv:2010.07468 by other authors
\\ ( https://arxiv.org/abs/2311.02818 ,  2344kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04843
replaced with revised version Thu, 1 Feb 2024 23:17:06 GMT   (4231kb,D)

Title: Bridging Dimensions: Confident Reachability for High-Dimensional
  Controllers
Authors: Yuang Geng, Souradeep Dutta, Ivan Ruchkin
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.04843 ,  4231kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06597
replaced with revised version Fri, 2 Feb 2024 14:03:32 GMT   (4646kb,D)

Title: Understanding Grokking Through A Robustness Viewpoint
Authors: Zhiquan Tan, Weiran Huang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.06597 ,  4646kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13870
replaced with revised version Fri, 2 Feb 2024 12:37:02 GMT   (733kb,D)

Title: Multi-intention Inverse Q-learning for Interpretable Behavior
  Representation
Authors: Hao Zhu, Brice De La Crompe, Gabriel Kalweit, Artur Schneider, Maria
  Kalweit, Ilka Diester, Joschka Boedecker
Categories: cs.LG q-bio.NC
\\ ( https://arxiv.org/abs/2311.13870 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17795
replaced with revised version Fri, 2 Feb 2024 08:06:51 GMT   (205kb,D)

Title: Marginal Laplacian Score
Authors: Guy Hay and Ohad Volk
Categories: cs.LG stat.ML
Comments: 10 pages
ACM-class: I.5.0
\\ ( https://arxiv.org/abs/2311.17795 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17929
replaced with revised version Fri, 2 Feb 2024 18:33:43 GMT   (18065kb,D)

Title: New Online Communities: Graph Deep Learning on Anonymous Voting Networks
  to Identify Sybils in Polycentric Governance
Authors: Quinn DuPont
Categories: cs.LG cs.SY eess.SY
MSC-class: 68T07
ACM-class: H.1.1; K.4.3; I.6.5
\\ ( https://arxiv.org/abs/2311.17929 ,  18065kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06182
replaced with revised version Fri, 2 Feb 2024 02:53:22 GMT   (364kb,D)

Title: Why "classic" Transformers are shallow and how to make them go deep
Authors: Yueyao Yu, Yin Zhang
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2312.06182 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07950
replaced with revised version Fri, 2 Feb 2024 06:55:52 GMT   (990kb,D)

Title: CBQ: Cross-Block Quantization for Large Language Models
Authors: Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting
  Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2312.07950 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09196
replaced with revised version Thu, 1 Feb 2024 20:40:44 GMT   (1428kb,D)

Title: DIRECT: Deep Active Learning under Imbalance and Label Noise
Authors: Shyam Nuggehalli, Jifan Zhang, Lalit Jain, Robert Nowak
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2312.09196 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16243
replaced with revised version Fri, 2 Feb 2024 04:45:45 GMT   (4146kb,D)

Title: Data Mixture in Training Un-assures Out-of-Distribution Generalization
Authors: Songming Zhang, Yuxiao Luo, Qizhou Wang, Haoang Chi, Weikai Li, Bo
  Han, Jinyan Li
Categories: cs.LG
Comments: 18 pages, 9 figures
\\ ( https://arxiv.org/abs/2312.16243 ,  4146kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02524
replaced with revised version Thu, 1 Feb 2024 22:06:51 GMT   (13615kb,D)

Title: Comprehensive Exploration of Synthetic Data Generation: A Survey
Authors: Andr\'e Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel
  Kounev, Mark Leznik, Kyle Chard, Ian Foster
Categories: cs.LG cs.AI cs.CV
Comments: Fixed bug in Figure 44
\\ ( https://arxiv.org/abs/2401.02524 ,  13615kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05015
replaced with revised version Fri, 2 Feb 2024 12:24:57 GMT   (118kb,D)

Title: An Information Theoretic Approach to Interaction-Grounded Learning
Authors: Xiaoyan Hu, Farzan Farnia, Ho-fung Leung
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.05015 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13239
replaced with revised version Fri, 2 Feb 2024 00:19:53 GMT   (211kb,D)

Title: Adaptive Crowdsourcing Via Self-Supervised Learning
Authors: Anmol Kagrecha, Henrik Marklund, Benjamin Van Roy, Hong Jun Jeon,
  Richard Zeckhauser
Categories: cs.LG cs.HC
Comments: 33 pages, 3 figures
\\ ( https://arxiv.org/abs/2401.13239 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16775
replaced with revised version Fri, 2 Feb 2024 09:36:04 GMT   (2741kb,D)

Title: Activity Detection for Massive Connectivity in Cell-free Networks with
  Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity
  Probability: A Bayesian Approach
Authors: Hao Zhang, Qingfeng Lin, Yang Li, Lei Cheng, Yik-Chung Wu
Categories: cs.LG eess.SP
Comments: 16 pages, 9 figures, accepted for publication in IEEE Transactions on
  Signal Processing
MSC-class: 68T01
DOI: 10.1109/TSP.2024.3361090
\\ ( https://arxiv.org/abs/2401.16775 ,  2741kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17435
replaced with revised version Thu, 1 Feb 2024 18:57:18 GMT   (601kb,D)

Title: Can Large Language Models Replace Economic Choice Prediction Labs?
Authors: Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz
Categories: cs.LG cs.AI cs.CL cs.GT cs.HC
\\ ( https://arxiv.org/abs/2401.17435 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17615
replaced with revised version Fri, 2 Feb 2024 05:04:21 GMT   (1764kb,D)

Title: Graph Multi-Similarity Learning for Molecular Property Prediction
Authors: Hao Xu, Zhengyang Zhou, Pengyu Hong
Categories: cs.LG cs.CE
\\ ( https://arxiv.org/abs/2401.17615 ,  1764kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17780
replaced with revised version Fri, 2 Feb 2024 13:55:51 GMT   (149kb,D)

Title: A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with
  Uniform PAC Guarantees
Authors: Toshinori Kitamura, Tadashi Kozuno, Masahiro Kato, Yuki Ichihara,
  Soichiro Nishimori, Akiyoshi Sannai, Sho Sonoda, Wataru Kumagai, Yutaka
  Matsuo
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.17780 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00306
replaced with revised version Fri, 2 Feb 2024 17:29:21 GMT   (1294kb,D)

Title: An Accurate and Low-Parameter Machine Learning Architecture for Next
  Location Prediction
Authors: Calvin Jary and Nafiseh Kahani
Categories: cs.LG cs.AI
Comments: Paper was accepted and presented in person at the 2023 IEEE Future
  Networks World Forum, in Baltimore, Maryland, USA
\\ ( https://arxiv.org/abs/2402.00306 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00318
replaced with revised version Fri, 2 Feb 2024 17:08:39 GMT   (63kb)

Title: Analog-digital Scheduling for Federated Learning: A
  Communication-Efficient Approach
Authors: Muhammad Faraz Ul Abrar and Nicol\`o Michelusi
Categories: cs.LG cs.IT eess.SP math.IT
Comments: Appeared at the 2023 Asilomar Conference on Signals, Systems, and
  Computers
\\ ( https://arxiv.org/abs/2402.00318 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00351
replaced with revised version Fri, 2 Feb 2024 03:27:08 GMT   (28130kb,D)

Title: Machine Unlearning for Image-to-Image Generative Models
Authors: Guihong Li, Hsiang Hsu, Chun-Fu Chen, Radu Marculescu
Categories: cs.LG cs.CV
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2402.00351 ,  28130kb)
------------------------------------------------------------------------------
\\
arXiv:2209.11812
replaced with revised version Fri, 2 Feb 2024 16:55:56 GMT   (1266kb,D)

Title: Explanations, Fairness, and Appropriate Reliance in Human-AI
  Decision-Making
Authors: Jakob Schoeffer, Maria De-Arteaga, Niklas Kuehl
Categories: cs.HC cs.AI
Comments: Accepted at ACM CHI Conference on Human Factors in Computing Systems
  (CHI '24)
\\ ( https://arxiv.org/abs/2209.11812 ,  1266kb)
------------------------------------------------------------------------------
\\
arXiv:2210.11049
replaced with revised version Fri, 2 Feb 2024 08:11:13 GMT   (2751kb,D)

Title: How Does a Deep Learning Model Architecture Impact Its Privacy? A
  Comprehensive Study of Privacy Attacks on CNNs and Transformers
Authors: Guangsheng Zhang, Bo Liu, Huan Tian, Tianqing Zhu, Ming Ding, Wanlei
  Zhou
Categories: cs.CR cs.AI cs.LG stat.ML
Comments: To appear in USENIX Security 2024
\\ ( https://arxiv.org/abs/2210.11049 ,  2751kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08062
replaced with revised version Fri, 2 Feb 2024 02:04:32 GMT   (4419kb)

Title: Fossil Image Identification using Deep Learning Ensembles of Data
  Augmented Multiviews
Authors: Chengbin Hou, Xinyu Lin, Hanhui Huang, Sheng Xu, Junxuan Fan, Yukun
  Shi, Hairong Lv
Categories: cs.CV cs.AI q-bio.PE
Comments: published in Methods in Ecology and Evolution
Journal-ref: Methods in Ecology and Evolution, 14, 3020-3034 (2023)
DOI: 10.1111/2041-210X.14229
\\ ( https://arxiv.org/abs/2302.08062 ,  4419kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16150
replaced with revised version Fri, 2 Feb 2024 05:40:41 GMT   (6460kb)

Title: Multimodal video and IMU kinematic dataset on daily life activities
  using affordable devices (VIDIMU)
Authors: Mario Mart\'inez-Zarzuela, Javier Gonz\'alez-Alonso, M\'iriam
  Ant\'on-Rodr\'iguez, Francisco J. D\'iaz-Pernas, Henning M\"uller, Cristina
  Sim\'on-Mart\'inez
Categories: cs.CV cs.AI cs.LG eess.IV
Journal-ref: Sci Data 10, 648 (2023)
DOI: 10.1038/s41597-023-02554-9
\\ ( https://arxiv.org/abs/2303.16150 ,  6460kb)
------------------------------------------------------------------------------
\\
arXiv:2303.18240
replaced with revised version Thu, 1 Feb 2024 19:42:05 GMT   (22410kb,D)

Title: Where are we in the search for an Artificial Visual Cortex for Embodied
  Intelligence?
Authors: Arjun Majumdar and Karmesh Yadav and Sergio Arnaud and Yecheng Jason
  Ma and Claire Chen and Sneha Silwal and Aryan Jain and Vincent-Pierre Berges
  and Pieter Abbeel and Jitendra Malik and Dhruv Batra and Yixin Lin and
  Oleksandr Maksymets and Aravind Rajeswaran and Franziska Meier
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: Project website: https://eai-vc.github.io
\\ ( https://arxiv.org/abs/2303.18240 ,  22410kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12241
replaced with revised version Fri, 2 Feb 2024 15:01:15 GMT   (2725kb,D)

Title: Positive AI: Key Challenges in Designing Artificial Intelligence for
  Wellbeing
Authors: Willem van der Maden, Derek Lomas, Malak Sadek, Paul Hekkert
Categories: cs.CY cs.AI
ACM-class: I.2
\\ ( https://arxiv.org/abs/2304.12241 ,  2725kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03610
replaced with revised version Fri, 2 Feb 2024 15:46:42 GMT   (13476kb,D)

Title: The Role of Data Curation in Image Captioning
Authors: Wenyan Li, Jonas F. Lotz, Chen Qiu, Desmond Elliott
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2305.03610 ,  13476kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19075
replaced with revised version Thu, 1 Feb 2024 20:41:40 GMT   (14195kb,D)

Title: Language-Conditioned Imitation Learning with Base Skill Priors under
  Unstructured Data
Authors: Hongkuan Zhou, Zhenshan Bing, Xiangtong Yao, Xiaojie Su, Chenguang
  Yang, Kai Huang, Alois Knoll
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2305.19075 ,  14195kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05817
replaced with revised version Fri, 2 Feb 2024 12:11:44 GMT   (1799kb,D)

Title: How Can Recommender Systems Benefit from Large Language Models: A Survey
Authors: Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang,
  Yong Liu, Chuhan Wu, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming
  Tang, Weinan Zhang
Categories: cs.IR cs.AI
Comments: New version released with 27-page main content; Look-up table in
  appendix
\\ ( https://arxiv.org/abs/2306.05817 ,  1799kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11842
replaced with revised version Fri, 2 Feb 2024 14:12:42 GMT   (1785kb,D)

Title: ${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative
  Multi-Agent Reinforcement Learning
Authors: Dingyang Chen, Qi Zhang
Categories: cs.MA cs.AI cs.LG
\\ ( https://arxiv.org/abs/2308.11842 ,  1785kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14168 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 20:33:02 GMT   (5451kb,D)

Title: Randomized Forward Mode of Automatic Differentiation For Optimization
  Algorithms
Authors: Khemraj Shukla and Yeonjong Shin
Categories: math.OC cs.AI cs.LG
Comments: 22 Pages, 7 Figures
MSC-class: 65K05, 65B99, 65Y20
\\ ( https://arxiv.org/abs/2310.14168 ,  5451kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06130 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 09:20:22 GMT   (7220kb,D)

Title: High-dimensional mixed-categorical Gaussian processes with application
  to multidisciplinary design optimization for a green aircraft
Authors: Paul Saves, Youssef Diouane, Nathalie Bartoli, Thierry Lefebvre,
  Joseph Morlier
Categories: math.OC cs.AI stat.ML
Comments: v2 - Structural and Multidisciplinary Optimization
\\ ( https://arxiv.org/abs/2311.06130 ,  7220kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09200 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 15:23:17 GMT   (883kb,D)

Title: Are Normalizing Flows the Key to Unlocking the Exponential Mechanism? A
  Path through the Accuracy-Privacy Ceiling Constraining Differentially Private
  ML
Authors: Robert A. Bridges, Vandy J. Tombs, Christopher B. Stanley
Categories: stat.ML cs.AI cs.CR cs.LG math.PR
\\ ( https://arxiv.org/abs/2311.09200 ,  883kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07586
replaced with revised version Fri, 2 Feb 2024 04:56:56 GMT   (32456kb,D)

Title: Characteristic Guidance: Non-linear Correction for Diffusion Model at
  Large Guidance Scale
Authors: Candi Zheng, Yuan Lan
Categories: cs.CV cs.AI cs.LG physics.data-an
Comments: 8 pages, 7 figures
\\ ( https://arxiv.org/abs/2312.07586 ,  32456kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07856
replaced with revised version Fri, 2 Feb 2024 08:25:16 GMT   (257kb,D)

Title: DTL: Disentangled Transfer Learning for Visual Recognition
Authors: Minghao Fu, Ke Zhu, Jianxin Wu
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.07856 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10109
replaced with revised version Fri, 2 Feb 2024 03:08:07 GMT   (0kb,I)

Title: Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image
  Enhancement
Authors: Xiaofeng Zhang, Zishan Xu, Hao Tang, Chaochen Gu, Wei Chen, Shanying
  Zhu, Xinping Guan
Categories: cs.CV cs.AI
Comments: It needs revised
\\ ( https://arxiv.org/abs/2312.10109 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15101
replaced with revised version Fri, 2 Feb 2024 17:26:07 GMT   (1496kb,D)

Title: Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model
  Conversions
Authors: Nikolaos Louloudakis, Perry Gibson, Jos\'e Cano, and Ajitha Rajan
Categories: cs.SE cs.AI cs.CV cs.LG
Comments: 12 pages, 3 figures, 4 tables, 1 algorithm
\\ ( https://arxiv.org/abs/2312.15101 ,  1496kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05975
replaced with revised version Fri, 2 Feb 2024 17:30:51 GMT   (6962kb,D)

Title: End-to-end Learnable Clustering for Intent Learning in Recommendation
Authors: Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Wenliang Zhong,
  Xinwang Liu, Guannan Zhang, Kejun Zhang
Categories: cs.IR cs.AI
Comments: 24 pages
\\ ( https://arxiv.org/abs/2401.05975 ,  6962kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06683
replaced with revised version Fri, 2 Feb 2024 09:54:18 GMT   (146kb,D)

Title: DQNC2S: DQN-based Cross-stream Crisis event Summarizer
Authors: Daniele Rege Cambrin, Luca Cagliero, Paolo Garza
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: accepted at ECIR 2024
\\ ( https://arxiv.org/abs/2401.06683 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07519
replaced with revised version Fri, 2 Feb 2024 16:15:22 GMT   (43794kb,D)

Title: InstantID: Zero-shot Identity-Preserving Generation in Seconds
Authors: Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li,
  Xu Tang, and Yao Hu
Categories: cs.CV cs.AI
Comments: Technical Report, project page available at
  https://instantid.github.io/
\\ ( https://arxiv.org/abs/2401.07519 ,  43794kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15963
replaced with revised version Fri, 2 Feb 2024 18:11:27 GMT   (1807kb,D)

Title: NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional
  Correctness
Authors: Manav Singhal, Tushar Aggarwal, Abhijeet Awasthi, Nagarajan Natarajan,
  Aditya Kanade
Categories: cs.SE cs.AI cs.CL cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2401.15963 ,  1807kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00045
replaced with revised version Fri, 2 Feb 2024 02:50:59 GMT   (34235kb,D)

Title: Detecting Multimedia Generated by Large AI Models: A Survey
Authors: Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding,
  Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu
Categories: cs.MM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.00045 ,  34235kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05356
replaced with revised version Fri, 2 Feb 2024 04:31:00 GMT   (1535kb,D)

Title: Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
Authors: Jian Gu, Chunyang Chen, Aldeida Aleti
Categories: cs.SE cs.CL cs.LG
Comments: 12 pages, 5 figures, 6 tables, under peer review
\\ ( https://arxiv.org/abs/2312.05356 ,  1535kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12425
replaced with revised version Fri, 2 Feb 2024 02:06:20 GMT   (36288kb,D)

Title: The Neglected Tails of Vision-Language Models
Authors: Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva
  Ramanan, James Caverlee, Shu Kong
Categories: cs.CV cs.CL cs.LG
Comments: Project Page:
  https://shubhamprshr27.github.io/neglected-tails-of-vlms/
\\ ( https://arxiv.org/abs/2401.12425 ,  36288kb)
------------------------------------------------------------------------------
\\
arXiv:2104.03158 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 07:58:40 GMT   (284kb,D)

Title: Simple Imputation Rules for Prediction with Missing Data: Contrasting
  Theoretical Guarantees with Empirical Performance
Authors: Dimitris Bertsimas, Arthur Delarue, Jean Pauphilet
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2104.03158 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2202.07365 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 09:42:13 GMT   (3465kb,D)

Title: A Statistical Learning View of Simple Kriging
Authors: Emilia Siviero, Emilie Chautru, Stephan Cl\'emen\c{c}on
Categories: stat.ML cs.LG
Comments: 41 pages
\\ ( https://arxiv.org/abs/2202.07365 ,  3465kb)
------------------------------------------------------------------------------
\\
arXiv:2206.14051
replaced with revised version Fri, 2 Feb 2024 13:56:17 GMT   (518kb,D)

Title: Enhancing Business Process Simulation Models with Extraneous Activity
  Delays
Authors: David Chapela-Campa and Marlon Dumas
Categories: cs.SE cs.LG
Comments: Extended version of the ICPM 2022 publication (see v1)
Journal-ref: Information Systems (2024), 102346
DOI: 10.1016/j.is.2024.102346
\\ ( https://arxiv.org/abs/2206.14051 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01672
replaced with revised version Thu, 1 Feb 2024 20:57:59 GMT   (28260kb,D)

Title: Bringing motion taxonomies to continuous domains via GPLVM on hyperbolic
  manifolds
Authors: No\'emie Jaquier, Leonel Rozo, Miguel Gonz\'alez-Duque, Viacheslav
  Borovitskiy, Tamim Asfour
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2210.01672 ,  28260kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10249 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 12:29:07 GMT   (7050kb,D)

Title: Learning efficient backprojections across cortical hierarchies in real
  time
Authors: Kevin Max, Laura Kriener, Garibaldi Pineda Garc\'ia, Thomas Nowotny,
  Ismael Jaras, Walter Senn, Mihai A. Petrovici
Categories: q-bio.NC cs.LG cs.NE
Comments: Updated with streamlined main part, CIFAR-10 simulations, including
  DFA and minor fixes
\\ ( https://arxiv.org/abs/2212.10249 ,  7050kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12565 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 13:27:53 GMT   (4227kb,D)

Title: Variational Linearized Laplace Approximation for Bayesian Deep Learning
Authors: Luis A. Ortega, Sim\'on Rodr\'iguez Santana, Daniel Hern\'andez-Lobato
Categories: stat.ML cs.LG
Comments: Pre-print, under revision
\\ ( https://arxiv.org/abs/2302.12565 ,  4227kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11435 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 18:52:51 GMT   (22128kb,D)

Title: Inversion by Direct Iteration: An Alternative to Denoising Diffusion for
  Image Restoration
Authors: Mauricio Delbracio and Peyman Milanfar
Categories: eess.IV cs.CV cs.LG
Journal-ref: Transactions on Machine Learning Research (TMLR), 2023
\\ ( https://arxiv.org/abs/2303.11435 ,  22128kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12534
replaced with revised version Thu, 1 Feb 2024 22:10:07 GMT   (193kb,D)

Title: BertRLFuzzer: A BERT and Reinforcement Learning Based Fuzzer
Authors: Piyush Jha, Joseph Scott, Jaya Sriram Ganeshna, Mudit Singh, Vijay
  Ganesh
Categories: cs.SE cs.CR cs.LG
\\ ( https://arxiv.org/abs/2305.12534 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15577 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 13:26:27 GMT   (5015kb,D)

Title: Minimizing $f$-Divergences by Interpolating Velocity Fields
Authors: Song Liu, Jiahao Yu, Jack Simons, Mingxuan Yi, Mark Beaumont
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.15577 ,  5015kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18453 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 09:56:45 GMT   (2526kb,D)

Title: Conditional Diffusion Models for Semantic 3D Brain MRI Synthesis
Authors: Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2305.18453 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05185 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 16:37:31 GMT   (1032kb)

Title: On the Identification and Optimization of Nonsmooth Superposition
  Operators in Semilinear Elliptic PDEs
Authors: Constantin Christof and Julia Kowalczyk
Categories: math.OC cs.LG
Comments: Minor revision; to appear in ESAIM COCV
MSC-class: 35J61, 49J50, 49J52, 49K20, 49M05, 68T07
\\ ( https://arxiv.org/abs/2306.05185 ,  1032kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05366
replaced with revised version Fri, 2 Feb 2024 09:25:41 GMT   (2640kb,D)

Title: Ordinal Potential-based Player Rating
Authors: Nelson Vadori and Rahul Savani
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2306.05366 ,  2640kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11313 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 17:36:32 GMT   (14645kb,D)

Title: Deep graph kernel point processes
Authors: Zheng Dong, Matthew Repasky, Xiuyuan Cheng, Yao Xie
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2306.11313 ,  14645kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06895
replaced with revised version Fri, 2 Feb 2024 16:55:00 GMT   (36824kb,D)

Title: MagiCapture: High-Resolution Multi-Concept Portrait Customization
Authors: Junha Hyung, Jaeyo Shin, and Jaegul Choo
Categories: cs.CV cs.GR cs.LG
Comments: 18 pages, 17 figures
\\ ( https://arxiv.org/abs/2309.06895 ,  36824kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07929
replaced with revised version Fri, 2 Feb 2024 08:02:35 GMT   (10724kb,D)

Title: Prompting Segmentation with Sound Is Generalizable Audio-Visual Source
  Localizer
Authors: Yaoting Wang, Weisong Liu, Guangyao Li, Jian Ding, Di Hu, Xi Li
Categories: cs.CV cs.LG cs.MM cs.SD eess.AS
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2309.07929 ,  10724kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15687
replaced with revised version Thu, 1 Feb 2024 19:05:15 GMT   (2305kb,D)

Title: Breaking On-Chip Communication Anonymity using Flow Correlation Attacks
Authors: Hansika Weerasena, and Prabhat Mishra
Categories: cs.CR cs.AR cs.LG
\\ ( https://arxiv.org/abs/2309.15687 ,  2305kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08204
replaced with revised version Fri, 2 Feb 2024 18:31:52 GMT   (5570kb,D)

Title: STELLA: Continual Audio-Video Pre-training with Spatio-Temporal
  Localized Alignment
Authors: Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, and Sung Ju Hwang
Categories: cs.CV cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2310.08204 ,  5570kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18449 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 16:14:01 GMT   (25333kb,D)

Title: Conditional Generative Representation for Black-Box Optimization with
  Implicit Constraints
Authors: Wenqian Xing, Jungho Lee, Chong Liu, Shixiang Zhu
Categories: stat.ML cs.CE cs.LG
\\ ( https://arxiv.org/abs/2310.18449 ,  25333kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10359
replaced with revised version Thu, 1 Feb 2024 19:36:15 GMT   (4609kb,D)

Title: FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel
  Identification
Authors: Wenqing Wu
Categories: cs.DC cs.LG
Comments: 21 pages, 21 figures. Added a timeline figure to demonstrate low
  priority tasks JCT stability. Updated all multi-tasking experiments with a
  newer NVIDIA driver version
\\ ( https://arxiv.org/abs/2311.10359 ,  4609kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12616 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 16:24:14 GMT   (2904kb,D)

Title: Online Variational Sequential Monte Carlo
Authors: Alessandro Mastrototaro and Jimmy Olsson
Categories: stat.ML cs.LG
Comments: In this version there are additional simulations in Section 5.1, some
  added references, and minor typos fixed
\\ ( https://arxiv.org/abs/2312.12616 ,  2904kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
