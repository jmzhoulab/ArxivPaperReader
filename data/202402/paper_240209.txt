Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200011 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月9日 16:39
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed  7 Feb 24 19:00:00 GMT  to  Thu  8 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.05121
Date: Sun, 4 Feb 2024 00:47:53 GMT   (110kb,D)

Title: Large Language Model for Table Processing: A Survey
Authors: Weizheng Lu and Jiaming Zhang and Jing Zhang and Yueguo Chen
Categories: cs.AI cs.CL
\\
  Tables, typically two-dimensional and structured to store large amounts of
data, are essential in daily activities like database queries, spreadsheet
calculations, and generating reports from web tables. Automating these
table-centric tasks with Large Language Models (LLMs) offers significant public
benefits, garnering interest from academia and industry. This survey provides
an extensive overview of table tasks, encompassing not only the traditional
areas like table question answering (Table QA) and fact verification, but also
newly emphasized aspects such as table manipulation and advanced table data
analysis. Additionally, it goes beyond the early strategies of pre-training and
fine-tuning small language models, to include recent paradigms in LLM usage.
The focus here is particularly on instruction-tuning, prompting, and
agent-based approaches within the realm of LLMs. Finally, we highlight several
challenges, ranging from private deployment and efficient inference to the
development of extensive benchmarks for table manipulation and advanced data
analysis.
\\ ( https://arxiv.org/abs/2402.05121 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05135
Date: Tue, 6 Feb 2024 11:29:44 GMT   (2758kb,D)

Title: CADReN: Contextual Anchor-Driven Relational Network for Controllable
  Cross-Graphs Node Importance Estimation
Authors: Zijie Zhong, Yunhui Zhang, Ziyi Chang, Zengchang Qin
Categories: cs.AI cs.CL cs.IR
Comments: 8 pages, 6 figures
MSC-class: 68T07
\\
  Node Importance Estimation (NIE) is crucial for integrating external
information into Large Language Models through Retriever-Augmented Generation.
Traditional methods, focusing on static, single-graph characteristics, lack
adaptability to new graphs and user-specific requirements. CADReN, our proposed
method, addresses these limitations by introducing a Contextual Anchor (CA)
mechanism. This approach enables the network to assess node importance relative
to the CA, considering both structural and semantic features within Knowledge
Graphs (KGs). Extensive experiments show that CADReN achieves better
performance in cross-graph NIE task, with zero-shot prediction ability. CADReN
is also proven to match the performance of previous models on single-graph NIE
task. Additionally, we introduce and opensource two new datasets, RIC200 and
WK1K, specifically designed for cross-graph NIE research, providing a valuable
resource for future developments in this domain.
\\ ( https://arxiv.org/abs/2402.05135 ,  2758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05138
Date: Tue, 6 Feb 2024 19:16:55 GMT   (456kb,D)

Title: SceMQA: A Scientific College Entrance Level Multimodal Question
  Answering Benchmark
Authors: Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu
  Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, Xiangliang Zhang
Categories: cs.AI cs.CL
Comments: Work in progress
\\
  The paper introduces SceMQA, a novel benchmark for scientific multimodal
question answering at the college entrance level. It addresses a critical
educational phase often overlooked in existing benchmarks, spanning high school
to pre-college levels. SceMQA focuses on core science subjects including
Mathematics, Physics, Chemistry, and Biology. It features a blend of
multiple-choice and free-response formats, ensuring a comprehensive evaluation
of AI models' abilities. Additionally, our benchmark provides specific
knowledge points for each problem and detailed explanations for each answer.
SceMQA also uniquely presents problems with identical contexts but varied
questions to facilitate a more thorough and accurate assessment of reasoning
capabilities. In the experiment, we evaluate both open-source and close-source
state-of-the-art Multimodal Large Language Models (MLLMs), across various
experimental settings. The results show that further research and development
are needed in developing more capable MLLM, as highlighted by only 50% to 60%
accuracy achieved by the strongest models. Our benchmark and analysis will be
available at https://scemqa.github.io/
\\ ( https://arxiv.org/abs/2402.05138 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05307
Date: Wed, 7 Feb 2024 23:00:24 GMT   (1487kb,D)

Title: Three Pathways to Neurosymbolic Reinforcement Learning with
  Interpretable Model and Policy Networks
Authors: Peter Graf and Patrick Emami
Categories: cs.AI cs.LG
\\
  Neurosymbolic AI combines the interpretability, parsimony, and explicit
reasoning of classical symbolic approaches with the statistical learning of
data-driven neural approaches. Models and policies that are simultaneously
differentiable and interpretable may be key enablers of this marriage. This
paper demonstrates three pathways to implementing such models and policies in a
real-world reinforcement learning setting. Specifically, we study a broad class
of neural networks that build interpretable semantics directly into their
architecture. We reveal and highlight both the potential and the essential
difficulties of combining logic, simulation, and learning. One lesson is that
learning benefits from continuity and differentiability, but classical logic is
discrete and non-differentiable. The relaxation to real-valued, differentiable
representations presents a trade-off; the more learnable, the less
interpretable. Another lesson is that using logic in the context of a numerical
simulation involves a non-trivial mapping from raw (e.g., real-valued time
series) simulation data to logical predicates. Some open questions this note
exposes include: What are the limits of rule-based controllers, and how
learnable are they? Do the differentiable interpretable approaches discussed
here scale to large, complex, uncertain systems? Can we truly achieve
interpretability? We highlight these and other themes across the three
approaches.
\\ ( https://arxiv.org/abs/2402.05307 ,  1487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05346
Date: Thu, 8 Feb 2024 01:41:28 GMT   (500kb,D)

Title: KIX: A Metacognitive Generalization Framework
Authors: Arun Kumar, Paul Schrater
Categories: cs.AI cs.LG cs.RO
\\
  Humans and other animals aptly exhibit general intelligence behaviors in
solving a variety of tasks with flexibility and ability to adapt to novel
situations by reusing and applying high level knowledge acquired over time. But
artificial agents are more of a specialist, lacking such generalist behaviors.
Artificial agents will require understanding and exploiting critical structured
knowledge representations. We present a metacognitive generalization framework,
Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects
leveraging type space facilitate the learning of transferable interaction
concepts and generalization. It is a natural way of integrating knowledge into
reinforcement learning and promising to act as an enabler for autonomous and
generalist behaviors in artificial intelligence systems.
\\ ( https://arxiv.org/abs/2402.05346 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05359
Date: Thu, 8 Feb 2024 02:37:30 GMT   (585kb,D)

Title: Guiding Large Language Models with Divide-and-Conquer Program for
  Discerning Problem Solving
Authors: Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
Categories: cs.AI cs.CL cs.LG
Comments: Preprint
\\
  Foundation models, such as Large language Models (LLMs), have attracted
significant amount of interest due to their large number of applications.
Existing works show that appropriate prompt design, such as Chain-of-Thoughts,
can unlock LLM's powerful capacity in diverse areas. However, when handling
tasks involving repetitive sub-tasks and/or deceptive contents, such as
arithmetic calculation and article-level fake news detection, existing
prompting strategies either suffers from insufficient expressive power or
intermediate errors triggered by hallucination. To make LLM more discerning to
such intermediate errors, we propose to guide LLM with a Divide-and-Conquer
program that simultaneously ensures superior expressive power and disentangles
task decomposition, sub-task resolution, and resolution assembly process.
Theoretic analysis reveals that our strategy can guide LLM to extend the
expressive power of fixed-depth Transformer. Experiments indicate that our
proposed method can achieve better performance than typical prompting
strategies in tasks bothered by intermediate errors and deceptive contents,
such as large integer multiplication, hallucination detection and
misinformation detection.
\\ ( https://arxiv.org/abs/2402.05359 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05391
Date: Thu, 8 Feb 2024 04:04:36 GMT   (9616kb,D)

Title: Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
Authors: Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang
  Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, Jiaqi Li, Xiaoze Liu,
  Jeff Z. Pan, Ningyu Zhang, Huajun Chen
Categories: cs.AI cs.CV cs.IR cs.LG
Comments: Ongoing work; 55 pages, 619 citations, 11 Tables, 13 Figures; Paper
  list is available at https://github.com/zjukg/KG-MM-Survey
\\
  Knowledge Graphs (KGs) play a pivotal role in advancing various AI
applications, with the semantic web community's exploration into multi-modal
dimensions unlocking new avenues for innovation. In this survey, we carefully
review over 300 articles, focusing on KG-aware research in two principal
aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal
tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into
the MMKG realm. We begin by defining KGs and MMKGs, then explore their
construction progress. Our review includes two primary task categories:
KG-aware multi-modal learning tasks, such as Image Classification and Visual
Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph
Completion and Entity Alignment, highlighting specific research trajectories.
For most of these tasks, we provide definitions, evaluation benchmarks, and
additionally outline essential insights for conducting relevant research.
Finally, we discuss current challenges and identify emerging trends, such as
progress in Large Language Modeling and Multi-modal Pre-training strategies.
This survey aims to serve as a comprehensive reference for researchers already
involved in or considering delving into KG and multi-modal learning research,
offering insights into the evolving landscape of MMKG research and supporting
future work.
\\ ( https://arxiv.org/abs/2402.05391 ,  9616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05467
Date: Thu, 8 Feb 2024 07:56:49 GMT   (39953kb,D)

Title: Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation
  and Echopraxia
Authors: Guangyu Shen, Siyuan Cheng, Kaiyuan Zhang, Guanhong Tao, Shengwei An,
  Lu Yan, Zhuo Zhang, Shiqing Ma, Xiangyu Zhang
Categories: cs.AI cs.CL cs.CR
\\
  Large Language Models (LLMs) have become prevalent across diverse sectors,
transforming human life with their extraordinary reasoning and comprehension
abilities. As they find increased use in sensitive tasks, safety concerns have
gained widespread attention. Extensive efforts have been dedicated to aligning
LLMs with human moral principles to ensure their safe deployment. Despite their
potential, recent research indicates aligned LLMs are prone to specialized
jailbreaking prompts that bypass safety measures to elicit violent and harmful
content. The intrinsic discrete nature and substantial scale of contemporary
LLMs pose significant challenges in automatically generating diverse,
efficient, and potent jailbreaking prompts, representing a continuous obstacle.
In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious
Exploitation and Echopraxia), a novel optimization-based method inspired by two
psychological concepts: subconsciousness and echopraxia, which describe the
processes of the mind that occur without conscious awareness and the
involuntary mimicry of actions, respectively. Evaluations across 6 open-source
LLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success
Rate of 91.5\%, outperforming five current methods by up to 47.0\% with an 8x
reduction in overhead. Furthermore, it displays significant transferability and
stealth, successfully evading established detection mechanisms. The code of our
work is available at
\url{https://github.com/SolidShen/RIPPLE_official/tree/official}
\\ ( https://arxiv.org/abs/2402.05467 ,  39953kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05605
Date: Thu, 8 Feb 2024 12:04:43 GMT   (2122kb,D)

Title: Optimizing Delegation in Collaborative Human-AI Hybrid Teams
Authors: Andrew Fuchs, Andrea Passarella, and Marco Conti
Categories: cs.AI cs.HC cs.LG
\\
  When humans and autonomous systems operate together as what we refer to as a
hybrid team, we of course wish to ensure the team operates successfully and
effectively. We refer to team members as agents. In our proposed framework, we
address the case of hybrid teams in which, at any time, only one team member
(the control agent) is authorized to act as control for the team. To determine
the best selection of a control agent, we propose the addition of an AI manager
(via Reinforcement Learning) which learns as an outside observer of the team.
The manager learns a model of behavior linking observations of agent
performance and the environment/world the team is operating in, and from these
observations makes the most desirable selection of a control agent. We restrict
the manager task by introducing a set of constraints. The manager constraints
indicate acceptable team operation, so a violation occurs if the team enters a
condition which is unacceptable and requires manager intervention. To ensure
minimal added complexity or potential inefficiency for the team, the manager
should attempt to minimize the number of times the team reaches a constraint
violation and requires subsequent manager intervention. Therefore our manager
is optimizing its selection of authorized agents to boost overall team
performance while minimizing the frequency of manager intervention. We
demonstrate our manager performance in a simulated driving scenario
representing the case of a hybrid team of agents composed of a human driver and
autonomous driving system. We perform experiments for our driving scenario with
interfering vehicles, indicating the need for collision avoidance and proper
speed control. Our results indicate a positive impact of our manager, with some
cases resulting in increased team performance up to ~187% that of the best solo
agent performance.
\\ ( https://arxiv.org/abs/2402.05605 ,  2122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05786
Date: Thu, 8 Feb 2024 16:24:40 GMT   (450kb)

Title: Prompting Fairness: Artificial Intelligence as Game Players
Authors: Jazmia Henry
Categories: cs.AI cs.GT
Comments: preprint
\\
  Utilitarian games such as dictator games to measure fairness have been
studied in the social sciences for decades. These games have given us insight
into not only how humans view fairness but also in what conditions the
frequency of fairness, altruism and greed increase or decrease. While these
games have traditionally been focused on humans, the rise of AI gives us the
ability to study how these models play these games. AI is becoming a constant
in human interaction and examining how these models portray fairness in game
play can give us some insight into how AI makes decisions. Over 101 rounds of
the dictator game, I conclude that AI has a strong sense of fairness that is
dependant of it it deems the person it is playing with as trustworthy, framing
has a strong effect on how much AI gives a recipient when designated the
trustee, and there may be evidence that AI experiences inequality aversion just
as humans.
\\ ( https://arxiv.org/abs/2402.05786 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05808
Date: Thu, 8 Feb 2024 16:46:26 GMT   (628kb,D)

Title: Training Large Language Models for Reasoning through Reverse Curriculum
  Reinforcement Learning
Authors: Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He,
  Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran
  Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi
  Zhang, Xuanjing Huang
Categories: cs.AI cs.CL cs.LG
Comments: Preprint
\\
  In this paper, we propose R$^3$: Learning Reasoning through Reverse
Curriculum Reinforcement Learning (RL), a novel method that employs only
outcome supervision to achieve the benefits of process supervision for large
language models. The core challenge in applying RL to complex reasoning is to
identify a sequence of actions that result in positive rewards and provide
appropriate supervision for optimization. Outcome supervision provides sparse
rewards for final results without identifying error locations, whereas process
supervision offers step-wise rewards but requires extensive manual annotation.
R$^3$ overcomes these limitations by learning from correct demonstrations.
Specifically, R$^3$ progressively slides the start state of reasoning from a
demonstration's end to its beginning, facilitating easier model exploration at
all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome
supervision to offer step-level signals and precisely pinpoint errors. Using
Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$
points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds
the baseline by $4.2$ points across three backbone models, and without any
extra data, Codellama-7B + R$^3$ performs comparable to larger models or
closed-source models.
\\ ( https://arxiv.org/abs/2402.05808 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05829
Date: Thu, 8 Feb 2024 17:08:08 GMT   (1400kb,D)

Title: Limitations of Agents Simulated by Predictive Models
Authors: Raymond Douglas, Jacek Karwowski, Chan Bae, Andis Draguns, Victoria
  Krakovna
Categories: cs.AI
\\
  There is increasing focus on adapting predictive models into agent-like
systems, most notably AI assistants based on language models. We outline two
structural reasons for why these models can fail when turned into agents.
First, we discuss auto-suggestive delusions. Prior work has shown theoretically
that models fail to imitate agents that generated the training data if the
agents relied on hidden observations: the hidden observations act as
confounding variables, and the models treat actions they generate as evidence
for nonexistent observations. Second, we introduce and formally study a
related, novel limitation: predictor-policy incoherence. When a model generates
a sequence of actions, the model's implicit prediction of the policy that
generated those actions can serve as a confounding variable. The result is that
models choose actions as if they expect future actions to be suboptimal,
causing them to be overly conservative. We show that both of those failures are
fixed by including a feedback loop from the environment, that is, re-training
the models on their own actions. We give simple demonstrations of both
limitations using Decision Transformers and confirm that empirical results
agree with our conceptual and formal analysis. Our treatment provides a
unifying view of those failure modes, and informs the question of why
fine-tuning offline learned policies with online learning makes them more
effective.
\\ ( https://arxiv.org/abs/2402.05829 ,  1400kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05863
Date: Thu, 8 Feb 2024 17:51:48 GMT   (2093kb,D)

Title: How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis
Authors: Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo
  Tagliabue, Dan Jurafsky, James Zou
Categories: cs.AI cs.CL
\\
  Negotiation is the basis of social interactions; humans negotiate everything
from the price of cars to how to share common resources. With rapidly growing
interest in using large language models (LLMs) to act as agents on behalf of
human users, such LLM agents would also need to be able to negotiate. In this
paper, we study how well LLMs can negotiate with each other. We develop
NegotiationArena: a flexible framework for evaluating and probing the
negotiation abilities of LLM agents. We implemented three types of scenarios in
NegotiationArena to assess LLM's behaviors in allocating shared resources
(ultimatum games), aggregate resources (trading games) and buy/sell goods
(price negotiations). Each scenario allows for multiple turns of flexible
dialogues between LLM agents to allow for more complex negotiations.
Interestingly, LLM agents can significantly boost their negotiation outcomes by
employing certain behavioral tactics. For example, by pretending to be desolate
and desperate, LLMs can improve their payoffs by 20\% when negotiating against
the standard GPT-4. We also quantify irrational negotiation behaviors exhibited
by the LLM agents, many of which also appear in humans. Together,
\NegotiationArena offers a new environment to investigate LLM interactions,
enabling new insights into LLM's theory of mind, irrationality, and reasoning
abilities.
\\ ( https://arxiv.org/abs/2402.05863 ,  2093kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05894
Date: Thu, 8 Feb 2024 18:33:21 GMT   (437kb,D)

Title: Large Language Model Meets Graph Neural Network in Knowledge
  Distillation
Authors: Shengxiang Hu, Guobing Zou, Song Yang, Bofeng Zhang, Yixin Chen
Categories: cs.AI cs.LG
Comments: 17 pages, 6 figures, 4 tables
MSC-class: 68T30, 68R10, 68T05
\\
  Despite recent community revelations about the advancements and potential of
Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the
deployment of LLMs for production is hindered by their high computational and
storage requirements, as well as long latencies during inference.
Simultaneously, although traditional Graph Neural Networks (GNNs) are light
weight and adept at learning structural features of graphs, their ability to
grasp the complex semantics in TAGs is somewhat constrained for real
applications. To address these limitations, we concentrate on the downstream
task of node classification in TAG and propose a novel graph knowledge
distillation framework, termed Linguistic Graph Knowledge Distillation
(LinguGKD), using LLMs as teacher models and GNNs as student models for
knowledge distillation. It involves TAG-oriented instruction tuning of LLM on
designed node classification prompts, followed by aligning the hierarchically
learned node features of the teacher LLM and the student GNN in latent space,
employing a layer-adaptive contrastive learning strategy. Through extensive
experiments on a variety of LLM and GNN models and multiple benchmark datasets,
the proposed LinguGKD significantly boosts the student GNN's predictive
accuracy and convergence rate, without the need of extra data or model
parameters. Compared to teacher LLM, distilled GNN achieves superior inference
speed equipped with much fewer computing and storage demands, when surpassing
the teacher LLM's classification performance on some of benchmark datasets.
\\ ( https://arxiv.org/abs/2402.05894 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05929
Date: Thu, 8 Feb 2024 18:58:02 GMT   (31731kb,D)

Title: An Interactive Agent Foundation Model
Authors: Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul
  Tang, Ehsan Adeli, Shrinidhi Kowshika Lakshmikanth, Kevin Schulman, Arnold
  Milstein, Demetri Terzopoulos, Ade Famoti, Noboru Kuno, Ashley Llorens, Hoi
  Vo, Katsu Ikeuchi, Li Fei-Fei, Jianfeng Gao, Naoki Wake, Qiuyuan Huang
Categories: cs.AI cs.LG cs.RO
\\
  The development of artificial intelligence systems is transitioning from
creating static, task-specific models to dynamic, agent-based systems capable
of performing well in a wide range of applications. We propose an Interactive
Agent Foundation Model that uses a novel multi-task agent training paradigm for
training AI agents across a wide range of domains, datasets, and tasks. Our
training paradigm unifies diverse pre-training strategies, including visual
masked auto-encoders, language modeling, and next-action prediction, enabling a
versatile and adaptable AI framework. We demonstrate the performance of our
framework across three separate domains -- Robotics, Gaming AI, and Healthcare.
Our model demonstrates its ability to generate meaningful and contextually
relevant outputs in each area. The strength of our approach lies in its
generality, leveraging a variety of data sources such as robotics sequences,
gameplay data, large-scale video datasets, and textual information for
effective multimodal and multi-task learning. Our approach provides a promising
avenue for developing generalist, action-taking, multimodal systems.
\\ ( https://arxiv.org/abs/2402.05929 ,  31731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05116
Date: Fri, 19 Jan 2024 17:14:46 GMT   (3859kb,D)

Title: Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and
  Google Bard Content in Relation to BioMedical Literature
Authors: Jakub Klimczak and Ahmed Abdeen Hamed
Categories: cs.CL cs.DL cs.IR
Comments: 15 pages, 10 figures, 4 tables; and 1 algorithm
\\
  Background: The emergence of generative AI tools, empowered by Large Language
Models (LLMs), has shown powerful capabilities in generating content. To date,
the assessment of the usefulness of such content, generated by what is known as
prompt engineering, has become an interesting research question. Objectives
Using the mean of prompt engineering, we assess the similarity and closeness of
such contents to real literature produced by scientists. Methods In this
exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to
generate clinical content to be compared with literature counterparts, (2) we
assess the similarities of the contents generated by comparing them with
counterparts from biomedical literature. Our approach is to use text-mining
approaches to compare documents and associated bigrams and to use network
analysis to assess the terms' centrality. Results The experiments demonstrated
that ChatGPT outperformed Google Bard in cosine document similarity (38% to
34%), Jaccard document similarity (23% to 19%), TF-IDF bigram similarity (47%
to 41%), and term network centrality (degree and closeness). We also found new
links that emerged in ChatGPT bigram networks that did not exist in literature
bigram networks. Conclusions: The obtained similarity results show that ChatGPT
outperformed Google Bard in document similarity, bigrams, and degree and
closeness centrality. We also observed that ChatGPT offers linkage to terms
that are connected in the literature. Such connections could inspire asking
interesting questions and generate new hypotheses.
\\ ( https://arxiv.org/abs/2402.05116 ,  3859kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05119
Date: Sat, 3 Feb 2024 04:45:25 GMT   (5575kb,D)

Title: A Closer Look at the Limitations of Instruction Tuning
Authors: Sreyan Ghosh and Chandra Kiran Reddy Evuru and Sonal Kumar and
  Ramaneswaran S and Deepali Aneja and Zeyu Jin and Ramani Duraiswami and
  Dinesh Manocha
Categories: cs.CL cs.AI
\\
  Instruction Tuning (IT), the process of training large language models (LLMs)
using instruction-response pairs, has emerged as the predominant method for
transforming base pre-trained LLMs into open-domain conversational agents.
While IT has achieved notable success and widespread adoption, its limitations
and shortcomings remain underexplored. In this paper, through rigorous
experiments and an in-depth analysis of the changes LLMs undergo through IT, we
reveal various limitations of IT. In particular, we show that (1) IT fails to
enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning
response initiation and style tokens, and full-parameter fine-tuning leads to
knowledge degradation. (2) Copying response patterns from IT datasets derived
from knowledgeable sources leads to a decline in response quality. (3)
Full-parameter fine-tuning increases hallucination by inaccurately borrowing
tokens from conceptually similar instances in the IT dataset for generating
responses. (4) Popular methods to improve IT do not lead to performance
improvements over a simple LoRA fine-tuned model. Our findings reveal that
responses generated solely from pre-trained knowledge consistently outperform
responses by models that learn any form of new knowledge from IT on open-source
datasets. We hope the insights and challenges revealed inspire future work.
\\ ( https://arxiv.org/abs/2402.05119 ,  5575kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05120
Date: Sat, 3 Feb 2024 05:55:24 GMT   (2521kb,D)

Title: More Agents Is All You Need
Authors: Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye
Categories: cs.CL cs.AI cs.LG
\\
  We find that, simply via a sampling-and-voting method, the performance of
large language models (LLMs) scales with the number of agents instantiated.
Also, this method is orthogonal to existing complicated methods to further
enhance LLMs, while the degree of enhancement is correlated to the task
difficulty. We conduct comprehensive experiments on a wide range of LLM
benchmarks to verify the presence of our finding, and to study the properties
that can facilitate its occurrence. Our code is publicly available at:
\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
\\ ( https://arxiv.org/abs/2402.05120 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05123
Date: Sun, 4 Feb 2024 13:32:01 GMT   (41kb)

Title: A Survey on Data Selection for LLM Instruction Tuning
Authors: Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, Dianhui Chu
Categories: cs.CL
\\
  Instruction tuning is a vital step of training large language models (LLM),
so how to enhance the effect of instruction tuning has received increased
attention. Existing works indicate that the quality of the dataset is more
crucial than the quantity during instruction tuning of LLM. Therefore, recently
a lot of studies focus on exploring the methods of selecting high-quality
subset from instruction datasets, aiming to reduce training costs and enhance
the instruction-following capabilities of LLMs. This paper presents a
comprehensive survey on data selection for LLM instruction tuning. Firstly, we
introduce the wildly used instruction datasets. Then, we propose a new taxonomy
of the data selection methods and provide a detailed introduction of recent
advances,and the evaluation strategies and results of data selection methods
are also elaborated in detail. Finally, we emphasize the open challenges and
present new frontiers of this task.
\\ ( https://arxiv.org/abs/2402.05123 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05125
Date: Mon, 5 Feb 2024 00:06:08 GMT   (1046kb,D)

Title: Zero-Shot Clinical Trial Patient Matching with LLMs
Authors: Michael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth W.
  Mahaffey, Nigam H. Shah
Categories: cs.CL cs.AI
\\
  Matching patients to clinical trials is a key unsolved challenge in bringing
new drugs to market. Today, identifying patients who meet a trial's eligibility
criteria is highly manual, taking up to 1 hour per patient. Automated screening
is challenging, however, as it requires understanding unstructured clinical
text. Large language models (LLMs) offer a promising solution. In this work, we
explore their application to trial matching. First, we design an LLM-based
system which, given a patient's medical history as unstructured clinical text,
evaluates whether that patient meets a set of inclusion criteria (also
specified as free text). Our zero-shot system achieves state-of-the-art scores
on the n2c2 2018 cohort selection benchmark. Second, we improve the data and
cost efficiency of our method by identifying a prompting strategy which matches
patients an order of magnitude faster and more cheaply than the status quo, and
develop a two-stage retrieval pipeline that reduces the number of tokens
processed by up to a third while retaining high performance. Third, we evaluate
the interpretability of our system by having clinicians evaluate the natural
language justifications generated by the LLM for each eligibility decision, and
show that it can output coherent explanations for 97% of its correct decisions
and 75% of its incorrect ones. Our results establish the feasibility of using
LLMs to accelerate clinical trial operations.
\\ ( https://arxiv.org/abs/2402.05125 ,  1046kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05126
Date: Mon, 5 Feb 2024 03:00:44 GMT   (328kb,D)

Title: Graph Neural Network and NER-Based Text Summarization
Authors: Imaad Zaffar Khan, Amaan Aijaz Sheikh, Utkarsh Sinha
Categories: cs.CL cs.LG
\\
  With the abundance of data and information in todays time, it is nearly
impossible for man, or, even machine, to go through all of the data line by
line. What one usually does is to try to skim through the lines and retain the
absolutely important information, that in a more formal term is called
summarization. Text summarization is an important task that aims to compress
lengthy documents or articles into shorter, coherent representations while
preserving the core information and meaning. This project introduces an
innovative approach to text summarization, leveraging the capabilities of Graph
Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with
their exceptional ability to capture and process the relational data inherent
in textual information, are adept at understanding the complex structures
within large documents. Meanwhile, NER systems contribute by identifying and
emphasizing key entities, ensuring that the summarization process maintains a
focus on the most critical aspects of the text. By integrating these two
technologies, our method aims to enhances the efficiency of summarization and
also tries to ensures a high degree relevance in the condensed content. This
project, therefore, offers a promising direction for handling the ever
increasing volume of textual data in an information-saturated world.
\\ ( https://arxiv.org/abs/2402.05126 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05127
Date: Mon, 5 Feb 2024 06:08:06 GMT   (1504kb)

Title: Illuminate: A novel approach for depression detection with explainable
  analysis and proactive therapy using prompt engineering
Authors: Aryan Agrawal
Categories: cs.CL cs.AI cs.LG
Comments: 10 pages, 9 figures, 9 tables
\\
  This paper introduces a novel paradigm for depression detection and treatment
using advanced Large Language Models (LLMs): Generative Pre-trained Transformer
4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized
prompts to diagnose, explain, and suggest therapeutic interventions for
depression. A unique few-shot prompting method enhances the models' ability to
analyze and explain depressive symptoms based on the DSM-5 criteria. In the
interaction phase, the models engage in empathetic dialogue management, drawing
from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,
fostering supportive interactions with individuals experiencing major
depressive disorders. Additionally, the research introduces the Illuminate
Database, enriched with various CBT modules, aiding in personalized therapy
recommendations. The study evaluates LLM performance using metrics such as F1
scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) across different test sets, demonstrating their
effectiveness. This comprehensive approach blends cutting-edge AI with
established psychological methods, offering new possibilities in mental health
care and showcasing the potential of LLMs in revolutionizing depression
diagnosis and treatment strategies.
\\ ( https://arxiv.org/abs/2402.05127 ,  1504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05128
Date: Mon, 5 Feb 2024 11:58:56 GMT   (1434kb,D)

Title: Enhancing Textbook Question Answering Task with Large Language Models
  and Retrieval Augmented Generation
Authors: Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali
  Alkhathlan, Amani Jamal
Categories: cs.CL cs.AI
\\
  Textbook question answering (TQA) is a challenging task in artificial
intelligence due to the complex nature of context and multimodal data. Although
previous research has significantly improved the task, there are still some
limitations including the models' weak reasoning and inability to capture
contextual information in the lengthy context. The introduction of large
language models (LLMs) has revolutionized the field of AI, however, directly
applying LLMs often leads to inaccurate answers. This paper proposes a
methodology that handle the out-of-domain scenario in TQA where concepts are
spread across different lessons by incorporating the retrieval augmented
generation (RAG) technique and utilize transfer learning to handle the long
context and enhance reasoning abilities. Through supervised fine-tuning of the
LLM model Llama-2 and the incorporation of RAG, our architecture outperforms
the baseline, achieving a 4.12% accuracy improvement on validation set and
9.84% on test set for non-diagram multiple-choice questions.
\\ ( https://arxiv.org/abs/2402.05128 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05129
Date: Mon, 5 Feb 2024 15:43:50 GMT   (349kb)

Title: Best Practices for Text Annotation with Large Language Models
Authors: Petter T\"ornberg
Categories: cs.CL
\\
  Large Language Models (LLMs) have ushered in a new era of text annotation, as
their ease-of-use, high accuracy, and relatively low costs have meant that
their use has exploded in recent months. However, the rapid growth of the field
has meant that LLM-based annotation has become something of an academic Wild
West: the lack of established practices and standards has led to concerns about
the quality and validity of research. Researchers have warned that the
ostensible simplicity of LLMs can be misleading, as they are prone to bias,
misunderstandings, and unreliable results. Recognizing the transformative
potential of LLMs, this paper proposes a comprehensive set of standards and
best practices for their reliable, reproducible, and ethical use. These
guidelines span critical areas such as model selection, prompt engineering,
structured prompting, prompt stability analysis, rigorous model validation, and
the consideration of ethical and legal implications. The paper emphasizes the
need for a structured, directed, and formalized approach to using LLMs, aiming
to ensure the integrity and robustness of text annotation practices, and
advocates for a nuanced and critical engagement with LLMs in social scientific
research.
\\ ( https://arxiv.org/abs/2402.05129 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05130
Date: Mon, 5 Feb 2024 16:47:17 GMT   (966kb,D)

Title: LB-KBQA: Large-language-model and BERT based Knowledge-Based Question
  and Answering System
Authors: Yan Zhao, Zhongyun Li, Jiaxing Wang
Categories: cs.CL cs.AI
\\
  Generative Artificial Intelligence (AI), because of its emergent abilities,
has empowered various fields, one typical of which is large language models
(LLMs). One of the typical application fields of Generative AI is large
language models (LLMs), and the natural language understanding capability of
LLM is dramatically improved when compared with conventional AI-based methods.
The natural language understanding capability has always been a barrier to the
intent recognition performance of the Knowledge-Based-Question-and-Answer
(KBQA) system, which arises from linguistic diversity and the newly appeared
intent. Conventional AI-based methods for intent recognition can be divided
into semantic parsing-based and model-based approaches. However, both of the
methods suffer from limited resources in intent recognition. To address this
issue, we propose a novel KBQA system based on a Large Language Model(LLM) and
BERT (LB-KBQA). With the help of generative AI, our proposed method could
detect newly appeared intent and acquire new knowledge. In experiments on
financial domain question answering, our model has demonstrated superior
effectiveness.
\\ ( https://arxiv.org/abs/2402.05130 ,  966kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05131
Date: Mon, 5 Feb 2024 22:35:42 GMT   (256kb)

Title: Financial Report Chunking for Effective Retrieval Augmented Generation
Authors: Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, and
  Leah Li
Categories: cs.CL
\\
  Chunking information is a key step in Retrieval Augmented Generation (RAG).
Current research primarily centers on paragraph-level chunking. This approach
treats all texts as equal and neglects the information contained in the
structure of documents. We propose an expanded approach to chunk documents by
moving beyond mere paragraph-level chunking to chunk primary by structural
element components of documents. Dissecting documents into these constituent
elements creates a new way to chunk documents that yields the best chunk size
without tuning. We introduce a novel framework that evaluates how chunking
based on element types annotated by document understanding models contributes
to the overall context and accuracy of the information retrieved. We also
demonstrate how this approach impacts RAG assisted Question & Answer task
performance. Our research includes a comprehensive analysis of various element
types, their role in effective information retrieval, and the impact they have
on the quality of RAG outputs. Findings support that element type based
chunking largely improve RAG results on financial reporting. Through this
research, we are also able to answer how to uncover highly accurate RAG.
\\ ( https://arxiv.org/abs/2402.05131 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05132
Date: Mon, 5 Feb 2024 22:48:28 GMT   (6690kb,D)

Title: TexShape: Information Theoretic Sentence Embedding for Language Models
Authors: H. Kaan Kale, Homa Esfahanizadeh, Noel Elias, Oguzhan Baser, Muriel
  Medard, Sriram Vishwanath
Categories: cs.CL cs.IT math.IT
Comments: Submitted to the 2024 IEEE International Symposium on Information
  Theory
\\
  With the exponential growth in data volume and the emergence of
data-intensive applications, particularly in the field of machine learning,
concerns related to resource utilization, privacy, and fairness have become
paramount. This paper focuses on the textual domain of data and addresses
challenges regarding encoding sentences to their optimized representations
through the lens of information-theory. In particular, we use empirical
estimates of mutual information, using the Donsker-Varadhan definition of
Kullback-Leibler divergence. Our approach leverages this estimation to train an
information-theoretic sentence embedding, called TexShape, for (task-based)
data compression or for filtering out sensitive information, enhancing privacy
and fairness. In this study, we employ a benchmark language model for initial
text representation, complemented by neural networks for information-theoretic
compression and mutual information estimations. Our experiments demonstrate
significant advancements in preserving maximal targeted information and minimal
sensitive information over adverse compression ratios, in terms of predictive
accuracy of downstream models that are trained using the compressed data.
\\ ( https://arxiv.org/abs/2402.05132 ,  6690kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05133
Date: Tue, 6 Feb 2024 04:18:58 GMT   (74kb,D)

Title: Personalized Language Modeling from Personalized Human Feedback
Authors: Xinyu Li, Zachary C. Lipton, Liu Leqi
Categories: cs.CL cs.AI cs.LG
\\
  Reinforcement Learning from Human Feedback (RLHF) is the current dominating
framework to fine-tune large language models to better align with human
preferences. However, the underlying premise of algorithms developed under this
framework can be problematic when user preferences encoded in human feedback
are diverse. In this work, we aim to address this problem by developing methods
for building personalized language models. We first formally introduce the task
of learning from personalized human feedback and explain why vanilla RLHF can
be problematic in this context. We then propose a general Personalized-RLHF
(P-RLHF) framework, which requires one to jointly learn a user model and a
language (or reward) model. The user model takes in user information and
outputs user representations. Its structure encodes our assumptions about user
preferences underlying the feedback data. We develop new learning objectives
for personalized reward modeling and personalized Direct Preference
Optimization. To demonstrate the efficacy of our method, we test it on
real-world text summarization data with annotated preferences and annotator
information. We fine-tune GPT-J 6B to obtain personalized language (and reward)
models, which outperform non-personalized models in terms of aligning with
individual preferences.
\\ ( https://arxiv.org/abs/2402.05133 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05136
Date: Tue, 6 Feb 2024 13:11:19 GMT   (2104kb,D)

Title: LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to
  256K
Authors: Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui
  Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan,
  Yu Wang
Categories: cs.CL
\\
  State-of-the-art large language models (LLMs) are now claiming remarkable
supported context lengths of 256k or even more. In contrast, the average
context lengths of mainstream benchmarks are insufficient (5k-21k), and they
suffer from potential knowledge leakage and inaccurate metrics, resulting in
biased evaluation. This paper introduces LV-Eval, a challenging long-context
benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up
to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA,
comprising 11 bilingual datasets. The design of LV-Eval has incorporated three
key techniques, namely confusing facts insertion, keyword and phrase
replacement, and keyword-recall-based metric design. The advantages of LV-Eval
include controllable evaluation across different context lengths, challenging
test instances with confusing facts, mitigated knowledge leakage, and more
objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation
studies on the techniques used in LV-Eval construction. The results reveal
that: (i) Commercial LLMs generally outperform open-source LLMs when evaluated
within length levels shorter than their claimed context length. However, their
overall performance is surpassed by open-source LLMs with longer context
lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k, exhibit a
relatively gentle degradation of performance, but their absolute performances
may not necessarily be higher than those of LLMs with shorter context lengths.
(iii) LLMs' performances can significantly degrade in the presence of confusing
information, especially in the pressure test of "needle in a haystack". (iv)
Issues related to knowledge leakage and inaccurate metrics introduce bias in
evaluation, and these concerns are alleviated in LV-Eval. All datasets and
evaluation codes are released at: https://github.com/infinigence/LVEval.
\\ ( https://arxiv.org/abs/2402.05136 ,  2104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05201
Date: Wed, 7 Feb 2024 19:11:23 GMT   (637kb,D)

Title: The Effect of Sampling Temperature on Problem Solving in Large Language
  Models
Authors: Matthew Renze and Erhan Guven
Categories: cs.CL cs.AI
\\
  In this research study, we empirically investigate the effect of sampling
temperature on the performance of Large Language Models (LLMs) on various
problem-solving tasks. We created a multiple-choice question-and-answer (MCQA)
exam by randomly sampling problems from standard LLM benchmarks. Then, we used
four popular LLMs with five prompt-engineering techniques to solve the MCQA
problems while increasing the sampling temperature from 0.0 to 1.0. Despite
anecdotal reports to the contrary, our empirical results indicate that changes
in temperature in the range 0.0 to 1.0 do not have a statistically significant
impact on LLM performance for problem-solving tasks. In addition, these results
appear to hold regardless of the LLM, the prompt-engineering technique, or the
problem domain. All code, data, and supplemental materials are available on
GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
\\ ( https://arxiv.org/abs/2402.05201 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05224
Date: Wed, 7 Feb 2024 20:02:09 GMT   (8812kb,D)

Title: VerAs: Verify then Assess STEM Lab Reports
Authors: Berk Atil, Mahsa Sheikhi Karizaki, Rebecca J. Passonneau
Categories: cs.CL cs.AI cs.LG
\\
  With an increasing focus in STEM education on critical thinking skills,
science writing plays an ever more important role in curricula that stress
inquiry skills. A recently published dataset of two sets of college level lab
reports from an inquiry-based physics curriculum relies on analytic assessment
rubrics that utilize multiple dimensions, specifying subject matter knowledge
and general components of good explanations. Each analytic dimension is
assessed on a 6-point scale, to provide detailed feedback to students that can
help them improve their science writing skills. Manual assessment can be slow,
and difficult to calibrate for consistency across all students in large
classes. While much work exists on automated assessment of open-ended questions
in STEM subjects, there has been far less work on long-form writing such as lab
reports. We present an end-to-end neural architecture that has separate
verifier and assessment modules, inspired by approaches to Open Domain Question
Answering (OpenQA). VerAs first verifies whether a report contains any content
relevant to a given rubric dimension, and if so, assesses the relevant
sentences. On the lab reports, VerAs outperforms multiple baselines based on
OpenQA systems or Automated Essay Scoring (AES). VerAs also performs well on an
analytic rubric for middle school physics essays.
\\ ( https://arxiv.org/abs/2402.05224 ,  8812kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05282
Date: Wed, 7 Feb 2024 21:54:53 GMT   (144kb,D)

Title: TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing
Authors: Ran Zmigrod, Zhiqiang Ma, Armineh Nourbakhsh, Sameena Shah
Categories: cs.CL
\\
  Visually Rich Form Understanding (VRFU) poses a complex research problem due
to the documents' highly structured nature and yet highly variable style and
content. Current annotation schemes decompose form understanding and omit key
hierarchical structure, making development and evaluation of end-to-end models
difficult. In this paper, we propose a novel F1 metric to evaluate form parsers
and describe a new content-agnostic, tree-based annotation scheme for VRFU:
TreeForm. We provide methods to convert previous annotation schemes into
TreeForm structures and evaluate TreeForm predictions using a modified version
of the normalized tree-edit distance. We present initial baselines for our
end-to-end performance metric and the TreeForm edit distance, averaged over the
FUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm
encourages deeper research in annotating, modeling, and evaluating the
complexities of form-like documents.
\\ ( https://arxiv.org/abs/2402.05282 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05376
Date: Thu, 8 Feb 2024 03:17:38 GMT   (112kb,D)

Title: Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms
  in Large Language Models
Authors: Feihu Jin, Yifan Liu, Ying Tan
Categories: cs.CL
Comments: 17 pages, 5 figures, 16 tables
\\
  Large Language Models (LLMs) have demonstrated remarkable performance across
diverse tasks and exhibited impressive reasoning abilities by applying
zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature
of sentence prefixes during the pre-training phase, existing zero-shot CoT
prompting methods that employ identical CoT prompting across all task instances
may not be optimal. In this paper, we introduce a novel zero-shot prompting
method that leverages evolutionary algorithms to generate diverse promptings
for LLMs dynamically. Our approach involves initializing two CoT promptings,
performing evolutionary operations based on LLMs to create a varied set, and
utilizing the LLMs to select a suitable CoT prompting for a given problem.
Additionally, a rewriting operation, guided by the selected CoT prompting,
enhances the understanding of the LLMs about the problem. Extensive experiments
conducted across ten reasoning datasets demonstrate the superior performance of
our proposed method compared to current zero-shot CoT prompting methods on
GPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore
the adaptability and effectiveness of our method in various reasoning tasks.
\\ ( https://arxiv.org/abs/2402.05376 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05403
Date: Thu, 8 Feb 2024 04:42:29 GMT   (2361kb)

Title: In-Context Principle Learning from Mistakes
Authors: Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra,
  Yiming Yang, Niket Tandon, Uri Alon
Categories: cs.CL cs.AI
\\
  In-context learning (ICL, also known as few-shot prompting) has been the
standard method of adapting LLMs to downstream tasks, by learning from a few
input-output examples. Nonetheless, all ICL-based approaches only learn from
correct input-output pairs. In this paper, we revisit this paradigm, by
learning more from the few given input-output examples. We introduce Learning
Principles (LEAP): First, we intentionally induce the model to make mistakes on
these few examples; then we reflect on these mistakes, and learn explicit
task-specific "principles" from them, which help solve similar problems and
avoid common mistakes; finally, we prompt the model to answer unseen test
questions using the original few-shot examples and these learned general
principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop
question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning,
and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the
strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and
Claude-2.1. For example, LEAP improves over the standard few-shot prompting
using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does
not require any more input or examples than the standard few-shot prompting
settings.
\\ ( https://arxiv.org/abs/2402.05403 ,  2361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05435
Date: Thu, 8 Feb 2024 06:20:01 GMT   (368kb,D)

Title: GPT-4 Generated Narratives of Life Events using a Structured Narrative
  Prompt: A Validation Study
Authors: Christopher J. Lynch, Erik Jensen, Madison H. Munro, Virginia Zamponi,
  Joseph Martinez, Kevin O'Brien, Brandon Feldhaus, Katherine Smith, Ann Marie
  Reinhold, and Ross Gore
Categories: cs.CL cs.AI cs.LG
Comments: 29 pages, 24 figures
ACM-class: I.2.7; I.6.4
\\
  Large Language Models (LLMs) play a pivotal role in generating vast arrays of
narratives, facilitating a systematic exploration of their effectiveness for
communicating life events in narrative form. In this study, we employ a
zero-shot structured narrative prompt to generate 24,000 narratives using
OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and
evaluate their validity in conveying birth, death, hiring, and firing events.
Remarkably, 87.43% of the narratives sufficiently convey the intention of the
structured prompt. To automate the identification of valid and invalid
narratives, we train and validate nine Machine Learning models on the
classified datasets. Leveraging these models, we extend our analysis to predict
the classifications of the remaining 21,120 narratives. All the ML models
excelled at classifying valid narratives as valid, but experienced challenges
at simultaneously classifying invalid narratives as invalid. Our findings not
only advance the study of LLM capabilities, limitations, and validity but also
offer practical insights for narrative generation and natural language
processing applications.
\\ ( https://arxiv.org/abs/2402.05435 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05440
Date: Thu, 8 Feb 2024 06:34:11 GMT   (171kb,D)

Title: Improving Agent Interactions in Virtual Environments with Language
  Models
Authors: Jack Zhang
Categories: cs.CL
\\
  Enhancing AI systems with efficient communication skills for effective human
assistance necessitates proactive initiatives from the system side to discern
specific circumstances and interact aptly. This research focuses on a
collective building assignment in the Minecraft dataset, employing language
modeling to enhance task understanding through state-of-the-art methods. These
models focus on grounding multi-modal understanding and task-oriented dialogue
comprehension tasks, providing insights into their interpretative and
responsive capabilities. Our experimental results showcase a substantial
improvement over existing methods, indicating a promising direction for future
research in this domain.
\\ ( https://arxiv.org/abs/2402.05440 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05455
Date: Thu, 8 Feb 2024 07:20:02 GMT   (601kb,D)

Title: Large Language Models for Psycholinguistic Plausibility Pretesting
Authors: Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant
Categories: cs.CL
\\
  In psycholinguistics, the creation of controlled materials is crucial to
ensure that research outcomes are solely attributed to the intended
manipulations and not influenced by extraneous factors. To achieve this,
psycholinguists typically pretest linguistic materials, where a common pretest
is to solicit plausibility judgments from human evaluators on specific
sentences. In this work, we investigate whether Language Models (LMs) can be
used to generate these plausibility judgements. We investigate a wide range of
LMs across multiple linguistic structures and evaluate whether their
plausibility judgements correlate with human judgements. We find that GPT-4
plausibility judgements highly correlate with human judgements across the
structures we examine, whereas other LMs correlate well with humans on commonly
used syntactic structures. We then test whether this correlation implies that
LMs can be used instead of humans for pretesting. We find that when
coarse-grained plausibility judgements are needed, this works well, but when
fine-grained judgements are necessary, even GPT-4 does not provide satisfactory
discriminative power.
\\ ( https://arxiv.org/abs/2402.05455 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05457
Date: Thu, 8 Feb 2024 07:21:45 GMT   (410kb,D)

Title: It's Never Too Late: Fusing Acoustic Information into Large Language
  Models for Automatic Speech Recognition
Authors: Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu
  Chen, Ensiong Chng, Chao-Han Huck Yang
Categories: cs.CL cs.AI cs.MM cs.SD eess.AS
Comments: Accepted to ICLR 2024, 17 pages. This work will be open sourced under
  MIT license
\\
  Recent studies have successfully shown that large language models (LLMs) can
be successfully used for generative error correction (GER) on top of the
automatic speech recognition (ASR) output. Specifically, an LLM is utilized to
carry out a direct mapping from the N-best hypotheses list generated by an ASR
system to the predicted output transcription. However, despite its
effectiveness, GER introduces extra data uncertainty since the LLM is trained
without taking into account acoustic information available in the speech
signal. In this work, we aim to overcome such a limitation by infusing acoustic
information before generating the predicted transcription through a novel late
fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a
multimodal fusion approach implemented into an auto-regressive decoding process
and works in two stages: (i) It first analyzes and calibrates the token-level
LLM decision, and (ii) it then dynamically assimilates the information from the
acoustic modality. Experimental evidence collected from various ASR tasks shows
that UADF surpasses existing fusion mechanisms in several ways. It yields
significant improvements in word error rate (WER) while mitigating data
uncertainty issues in LLM and addressing the poor generalization relied with
sole modality during fusion. We also demonstrate that UADF seamlessly adapts to
audio-visual speech recognition.
\\ ( https://arxiv.org/abs/2402.05457 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05512
Date: Thu, 8 Feb 2024 09:44:02 GMT   (5909kb,D)

Title: GPTs Are Multilingual Annotators for Sequence Generation Tasks
Authors: Juhwan Choi, Eunju Lee, Kyohoon Jin, YoungBin Kim
Categories: cs.CL cs.AI
Comments: EACL 2024 Findings: Camera-ready version
\\
  Data annotation is an essential step for constructing new datasets. However,
the conventional approach of data annotation through crowdsourcing is both
time-consuming and expensive. In addition, the complexity of this process
increases when dealing with low-resource languages owing to the difference in
the language pool of crowdworkers. To address these issues, this study proposes
an autonomous annotation method by utilizing large language models, which have
been recently demonstrated to exhibit remarkable performance. Through our
experiments, we demonstrate that the proposed method is not just cost-efficient
but also applicable for low-resource language annotation. Additionally, we
constructed an image captioning dataset using our approach and are committed to
open this dataset for future study. We have opened our source code for further
study and reproducibility.
\\ ( https://arxiv.org/abs/2402.05512 ,  5909kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05515
Date: Thu, 8 Feb 2024 09:48:02 GMT   (7657kb,D)

Title: NoisyICL: A Little Noise in Model Parameters Calibrates In-context
  Learning
Authors: Yufeng Zhao, Yoshihiro Sakai, Naoya Inoue
Categories: cs.CL cs.AI
Comments: 19 pages, 28 figures, 7 tables (5 pages, 4 figures, 1 table in main
  body)
\\
  In-Context Learning (ICL) is suffering from unsatisfactory performance and
under-calibration due to high prior bias and unfaithful confidence. Some
previous works fine-tuned language models for better ICL performance with
enormous datasets and computing costs. In this paper, we propose NoisyICL,
simply perturbing the model parameters by random noises to strive for better
performance and calibration. Our experiments on 2 models and 12 downstream
datasets show that NoisyICL can help ICL produce more accurate predictions. Our
further analysis indicates that NoisyICL enables the model to provide more fair
predictions, and also with less unfaithful confidence. Therefore, we believe
that NoisyICL is an effective calibration of ICL. Our experimental code is
uploaded to Github.
\\ ( https://arxiv.org/abs/2402.05515 ,  7657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05545
Date: Thu, 8 Feb 2024 10:29:11 GMT   (9kb)

Title: Named Entity Recognition for Address Extraction in Speech-to-Text
  Transcriptions Using Synthetic Data
Authors: Bibi\'ana Laj\v{c}inov\'a, Patrik Val\'abek and Michal Spi\v{s}iak
Categories: cs.CL
ACM-class: I.2.7
\\
  This paper introduces an approach for building a Named Entity Recognition
(NER) model built upon a Bidirectional Encoder Representations from
Transformers (BERT) architecture, specifically utilizing the SlovakBERT model.
This NER model extracts address parts from data acquired from speech-to-text
transcriptions. Due to scarcity of real data, a synthetic dataset using GPT API
was generated. The importance of mimicking spoken language variability in this
artificial data is emphasized. The performance of our NER model, trained solely
on synthetic data, is evaluated using small real test dataset.
\\ ( https://arxiv.org/abs/2402.05545 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05547
Date: Thu, 8 Feb 2024 10:32:06 GMT   (614kb,D)

Title: Benchmarking Large Language Models on Communicative Medical Coaching: a
  Novel System and Dataset
Authors: Hengguan Huang, Songtao Wang, Hongfu Liu, Hao Wang and Ye Wang
Categories: cs.CL cs.AI
Comments: NA
\\
  Traditional applications of natural language processing (NLP) in healthcare
have predominantly focused on patient-centered services, enhancing patient
interactions and care delivery, such as through medical dialogue systems.
However, the potential of NLP to benefit inexperienced doctors, particularly in
areas such as communicative medical coaching, remains largely unexplored. We
introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within
this framework, both a patient agent and a coaching agent collaboratively
support medical learners in practicing their medical communication skills
during consultations. Unlike traditional dialogue systems, ChatCoach provides a
simulated environment where a human doctor can engage in medical dialogue with
a patient agent. Simultaneously, a coaching agent provides real-time feedback
to the doctor. To construct the ChatCoach system, we developed a dataset and
integrated Large Language Models such as ChatGPT and Llama2, aiming to assess
their effectiveness in communicative medical coaching tasks. Our comparative
analysis demonstrates that instruction-tuned Llama2 significantly outperforms
ChatGPT's prompting-based approaches.
\\ ( https://arxiv.org/abs/2402.05547 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05571
Date: Thu, 8 Feb 2024 11:16:13 GMT   (383kb)

Title: Traditional Machine Learning Models and Bidirectional Encoder
  Representations From Transformer (BERT)-Based Automatic Classification of
  Tweets About Eating Disorders: Algorithm Development and Validation Study
Authors: Jos\'e Alberto Ben\'itez-Andrades, Jos\'e-Manuel Alija-P\'erez,
  Maria-Esther Vidal, Rafael Pastor-Vargas and Mar\'ia Teresa Garc\'ia-Ord\'as
Categories: cs.CL cs.LG
Journal-ref: JMIR Medical Informatics, Volume 10, Issue 2, 2022, ID e34492
DOI: 10.2196/34492
\\
  Background: Eating disorders are increasingly prevalent, and social networks
offer valuable information.
  Objective: Our goal was to identify efficient machine learning models for
categorizing tweets related to eating disorders.
  Methods: Over three months, we collected tweets about eating disorders. A
2,000-tweet subset was labeled for: (1) being written by individuals with
eating disorders, (2) promoting eating disorders, (3) informativeness, and (4)
scientific content. Both traditional machine learning and deep learning models
were employed for classification, assessing accuracy, F1 score, and
computational time.
  Results: From 1,058,957 collected tweets, transformer-based bidirectional
encoder representations achieved the highest F1 scores (71.1%-86.4%) across all
four categories.
  Conclusions: Transformer-based models outperform traditional techniques in
classifying eating disorder-related tweets, though they require more
computational resources.
\\ ( https://arxiv.org/abs/2402.05571 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05581
Date: Thu, 8 Feb 2024 11:31:23 GMT   (8510kb,D)

Title: Establishing degrees of closeness between audio recordings along
  different dimensions using large-scale cross-lingual models
Authors: Maxime Fily, Guillaume Wisniewski, Severine Guillaume, Gilles Adda,
  Alexis Michaud
Categories: cs.CL cs.SD eess.AS
Comments: Published in Findings of the EACL2024
\\
  In the highly constrained context of low-resource language studies, we
explore vector representations of speech from a pretrained model to determine
their level of abstraction with regard to the audio signal. We propose a new
unsupervised method using ABX tests on audio recordings with carefully curated
metadata to shed light on the type of information present in the
representations. ABX tests determine whether the representations computed by a
multilingual speech model encode a given characteristic. Three experiments are
devised: one on room acoustics aspects, one on linguistic genre, and one on
phonetic aspects. The results confirm that the representations extracted from
recordings with different linguistic/extra-linguistic characteristics differ
along the same lines. Embedding more audio signal in one vector better
discriminates extra-linguistic characteristics, whereas shorter snippets are
better to distinguish segmental information. The method is fully unsupervised,
potentially opening new research avenues for comparative work on
under-documented languages.
\\ ( https://arxiv.org/abs/2402.05581 ,  8510kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05584
Date: Thu, 8 Feb 2024 11:36:23 GMT   (37kb,D)

Title: AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods
  in Low-resource Regimes
Authors: Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song and Youngbin Kim
Categories: cs.CL cs.AI
Comments: EACL 2024 Student Research Workshop
\\
  Text data augmentation is a complex problem due to the discrete nature of
sentences. Although rule-based augmentation methods are widely adopted in
real-world applications because of their simplicity, they suffer from potential
semantic damage. Previous researchers have suggested easy data augmentation
with soft labels (softEDA), employing label smoothing to mitigate this problem.
However, finding the best factor for each model and dataset is challenging;
therefore, using softEDA in real-world applications is still difficult. In this
paper, we propose adapting AutoAugment to solve this problem. The experimental
results suggest that the proposed method can boost existing augmentation
methods and that rule-based methods can enhance cutting-edge pre-trained
language models. We offer the source code.
\\ ( https://arxiv.org/abs/2402.05584 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05591
Date: Thu, 8 Feb 2024 11:44:25 GMT   (40kb)

Title: SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels
Authors: Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song, Youngbin Kim
Categories: cs.CL cs.AI
Comments: ICLR 2023 Tiny Papers
\\
  Rule-based text data augmentation is widely used for NLP tasks due to its
simplicity. However, this method can potentially damage the original meaning of
the text, ultimately hurting the performance of the model. To overcome this
limitation, we propose a straightforward technique for applying soft labels to
augmented data. We conducted experiments across seven different classification
tasks and empirically demonstrated the effectiveness of our proposed approach.
We have publicly opened our source code for reproducibility.
\\ ( https://arxiv.org/abs/2402.05591 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05602
Date: Thu, 8 Feb 2024 12:01:24 GMT   (14457kb,D)

Title: AttnLRP: Attention-Aware Layer-wise Relevance Propagation for
  Transformers
Authors: Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer,
  Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek
Categories: cs.CL cs.AI cs.CV cs.LG
\\
  Large Language Models are prone to biased predictions and hallucinations,
underlining the paramount importance of understanding their model-internal
reasoning process. However, achieving faithful attributions for the entirety of
a black-box transformer model and maintaining computational efficiency is an
unsolved challenge. By extending the Layer-wise Relevance Propagation
attribution method to handle attention layers, we address these challenges
effectively. While partial solutions exist, our method is the first to
faithfully and holistically attribute not only input but also latent
representations of transformer models with the computational efficiency similar
to a singular backward pass. Through extensive evaluations against existing
methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we
demonstrate that our proposed approach surpasses alternative methods in terms
of faithfulness and enables the understanding of latent representations,
opening up the door for concept-based explanations. We provide an open-source
implementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.
\\ ( https://arxiv.org/abs/2402.05602 ,  14457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05616
Date: Thu, 8 Feb 2024 12:19:32 GMT   (189kb,D)

Title: Pretrained Generative Language Models as General Learning Frameworks for
  Sequence-Based Tasks
Authors: Ben Fauber
Categories: cs.CL cs.AI cs.LG
\\
  We propose that small pretrained foundational generative language models with
millions of parameters can be utilized as a general learning framework for
sequence-based tasks. Our proposal overcomes the computational resource, skill
set, and timeline challenges associated with training neural networks and
language models from scratch. Further, our approach focuses on creating small
and highly specialized models that can accurately execute a challenging task of
which the base model is incapable of performing. We demonstrate that 125M,
350M, and 1.3B parameter pretrained foundational language models can be
instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve
near state-of-the-art results on challenging cheminformatics tasks. We also
demonstrate the role of successive language model fine-tuning epochs on
improved outcomes, as well as the importance of both data formatting and
pretrained foundational language model selection for instruction fine-tuning
success.
\\ ( https://arxiv.org/abs/2402.05616 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05617
Date: Thu, 8 Feb 2024 12:20:28 GMT   (76kb,D)

Title: Deep Learning-based Computational Job Market Analysis: A Survey on Skill
  Extraction and Classification from Job Postings
Authors: Elena Senger, Mike Zhang, Rob van der Goot, Barbara Plank
Categories: cs.CL
Comments: Published at NLP4HR 2024 (EACL Workshop)
\\
  Recent years have brought significant advances to Natural Language Processing
(NLP), which enabled fast progress in the field of computational job market
analysis. Core tasks in this application domain are skill extraction and
classification from job postings. Because of its quick growth and its
interdisciplinary nature, there is no exhaustive assessment of this emerging
field. This survey aims to fill this gap by providing a comprehensive overview
of deep learning methodologies, datasets, and terminologies specific to
NLP-driven skill extraction and classification. Our comprehensive cataloging of
publicly available datasets addresses the lack of consolidated information on
dataset creation and characteristics. Finally, the focus on terminology
addresses the current lack of consistent definitions for important concepts,
such as hard and soft skills, and terms relating to skill extraction and
classification.
\\ ( https://arxiv.org/abs/2402.05617 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05624
Date: Thu, 8 Feb 2024 12:28:18 GMT   (762kb,D)

Title: Efficient Models for the Detection of Hate, Abuse and Profanity
Authors: Christoph Tillmann, Aashka Trivedi, Bishwaranjan Bhattacharjee
Categories: cs.CL cs.AI cs.HC
Comments: 8 pages, 7 figures
\\
  Large Language Models (LLMs) are the cornerstone for many Natural Language
Processing (NLP) tasks like sentiment analysis, document classification, named
entity recognition, question answering, summarization, etc. LLMs are often
trained on data which originates from the web. This data is prone to having
content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,
please refer to the Appendix. Due to the LLMs being exposed to HAP content
during training, the models learn it and may then generate hateful or profane
content. For example, when the open-source RoBERTa model (specifically, the
RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted
to replace the mask token in `I do not know that Persian people are that MASK`
it returns the word `stupid` with the highest score. This is unacceptable in
civil discourse.The detection of Hate, Abuse and Profanity in text is a vital
component of creating civil and unbiased LLMs, which is needed not only for
English, but for all languages. In this article, we briefly describe the
creation of HAP detectors and various ways of using them to make models civil
and acceptable in the output they generate.
\\ ( https://arxiv.org/abs/2402.05624 ,  762kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05629
Date: Thu, 8 Feb 2024 12:36:29 GMT   (1251kb,D)

Title: Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature
  of Aggregated Factual Claims in Long-Form Generations
Authors: Cheng-Han Chiang, Hung-yi Lee
Categories: cs.CL
\\
  Long-form generations from large language models (LLMs) contains a mix of
factual and non-factual claims, making evaluating factuality difficult. To
evaluate factual precision of long-form generations in a more fine-grained way,
prior works propose to decompose long-form generations into multiple verifiable
facts and verify those facts independently. The factuality of the generation is
the proportion of verifiable facts among all the facts. Such methods assume
that combining factual claims forms a factual paragraph. This paper shows that
the assumption can be violated due to entity ambiguity. We show that LLMs can
generate paragraphs that contain verifiable facts, but the facts are combined
to form a non-factual paragraph due to entity ambiguity. We further reveal that
existing factual precision metrics, including FActScore and citation recall,
cannot properly evaluate the factuality of these non-factual paragraphs. To
address this, we introduce an enhanced metric, D-FActScore, specifically
designed for content with ambiguous entities. We evaluate the D-FActScores of
people biographies generated with retrieval-augmented generation (RAG). We show
that D-FActScore can better assess the factuality of paragraphs with entity
ambiguity than FActScore. We also find that four widely used open-source LLMs
tend to mix information of distinct entities to form non-factual paragraphs.
\\ ( https://arxiv.org/abs/2402.05629 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05672
Date: Thu, 8 Feb 2024 13:47:50 GMT   (30kb,D)

Title: Multilingual E5 Text Embeddings: A Technical Report
Authors: Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder,
  Furu Wei
Categories: cs.CL cs.IR
Comments: 6 pages
\\
  This technical report presents the training methodology and evaluation
results of the open-source multilingual E5 text embedding models, released in
mid-2023. Three embedding models of different sizes (small / base / large) are
provided, offering a balance between the inference efficiency and embedding
quality. The training procedure adheres to the English E5 model recipe,
involving contrastive pre-training on 1 billion multilingual text pairs,
followed by fine-tuning on a combination of labeled datasets. Additionally, we
introduce a new instruction-tuned embedding model, whose performance is on par
with state-of-the-art, English-only models of similar sizes. Information
regarding the model release can be found at
https://github.com/microsoft/unilm/tree/master/e5 .
\\ ( https://arxiv.org/abs/2402.05672 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05699
Date: Thu, 8 Feb 2024 14:21:03 GMT   (1167kb,D)

Title: Self-Alignment of Large Language Models via Monopolylogue-based Social
  Scene Simulation
Authors: Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng
  Wang, Siheng Chen
Categories: cs.CL cs.AI cs.CY
Comments: 36 pages, 9 figures
\\
  Aligning large language models (LLMs) with human values is imperative to
mitigate potential adverse effects resulting from their misuse. Drawing from
the sociological insight that acknowledging all parties' concerns is a key
factor in shaping human values, this paper proposes a novel direction to align
LLMs by themselves: social scene simulation. To achieve this, we present
MATRIX, a novel social scene simulator that emulates realistic scenes around a
user's input query, enabling the LLM to take social consequences into account
before responding. MATRIX serves as a virtual rehearsal space, akin to a
Monopolylogue, where the LLM performs diverse roles related to the query and
practice by itself. To inject this alignment, we fine-tune the LLM with
MATRIX-simulated data, ensuring adherence to human values without compromising
inference speed. We theoretically show that the LLM with MATRIX outperforms
Constitutional AI under mild assumptions. Finally, extensive experiments
validate that our method outperforms over 10 baselines across 4 benchmarks. As
evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning
with human values. Code is available at https://github.com/pangxianghe/MATRIX.
\\ ( https://arxiv.org/abs/2402.05699 ,  1167kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05706
Date: Thu, 8 Feb 2024 14:35:09 GMT   (502kb,D)

Title: Unified Speech-Text Pretraining for Spoken Dialog Modeling
Authors: Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Jungwhan
  Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Sungroh Yoon, Kang Min Yoo
Categories: cs.CL cs.SD eess.AS
\\
  While recent work shows promising results in expanding the capabilities of
large language models (LLM) to directly understand and synthesize speech, an
LLM-based strategy for modeling spoken dialogs remains elusive and calls for
further investigation. This work proposes an extensive speech-text LLM
framework, named the Unified Spoken Dialog Model (USDM), to generate coherent
spoken responses with organic prosodic features relevant to the given input
speech without relying on automatic speech recognition (ASR) or text-to-speech
(TTS) solutions. Our approach employs a multi-step speech-text inference scheme
that leverages chain-of-reasoning capabilities exhibited by the underlying LLM.
We also propose a generalized speech-text pretraining scheme that helps with
capturing cross-modal semantics. Automatic and human evaluations show that the
proposed approach is effective in generating natural-sounding spoken responses,
outperforming both prior and cascaded baselines. Detailed comparative studies
reveal that, despite the cascaded approach being stronger in individual
components, the joint speech-text modeling improves robustness against
recognition errors and speech quality. Demo is available at
https://unifiedsdm.github.io.
\\ ( https://arxiv.org/abs/2402.05706 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05733
Date: Thu, 8 Feb 2024 15:08:57 GMT   (2531kb,D)

Title: TimeArena: Shaping Efficient Multitasking Language Agents in a
  Time-Aware Simulation
Authors: Yikai Zhang, Siyu Yuan, Caiyu Hu, Kyle Richardson, Yanghua Xiao,
  Jiangjie Chen
Categories: cs.CL
Comments: Work in progress
\\
  Despite remarkable advancements in emulating human-like behavior through
Large Language Models (LLMs), current textual simulations do not adequately
address the notion of time. To this end, we introduce TimeArena, a novel
textual simulated environment that incorporates complex temporal dynamics and
constraints that better reflect real-life planning scenarios. In TimeArena,
agents are asked to complete multiple tasks as soon as possible, allowing for
parallel processing to save time. We implement the dependency between actions,
the time duration for each action, and the occupancy of the agent and the
objects in the environment. TimeArena grounds to 30 real-world tasks in
cooking, household activities, and laboratory work. We conduct extensive
experiments with various state-of-the-art LLMs using TimeArena. Our findings
reveal that even the most powerful models, e.g., GPT-4, still lag behind humans
in effective multitasking, underscoring the need for enhanced temporal
awareness in the development of language agents.
\\ ( https://arxiv.org/abs/2402.05733 ,  2531kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05755
Date: Thu, 8 Feb 2024 15:39:32 GMT   (1709kb,D)

Title: SpiRit-LM: Interleaved Spoken and Written Language Model
Authors: Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha
  Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan
  Mavlyutov, Itai Gat, Gabriel Synnaeve, Juan Pino, Benoit Sagot, Emmanuel
  Dupoux
Categories: cs.CL cs.SD eess.AS
\\
  We introduce SPIRIT-LM, a foundation multimodal language model that freely
mixes text and speech. Our model is based on a pretrained text language model
that we extend to the speech modality by continuously training it on text and
speech units. Speech and text sequences are concatenated as a single set of
tokens, and trained with a word-level interleaving method using a small
automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two
versions: a BASE version that uses speech semantic units and an EXPRESSIVE
version that models expressivity using pitch and style units in addition to the
semantic units. For both versions, the text is encoded with subword BPE tokens.
The resulting model displays both the semantic abilities of text models and the
expressive abilities of speech models. Additionally, we demonstrate that
SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities
(i.e. ASR, TTS, Speech Classification).
\\ ( https://arxiv.org/abs/2402.05755 ,  1709kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05783
Date: Thu, 8 Feb 2024 16:17:24 GMT   (8139kb,D)

Title: Text-to-Code Generation with Modality-relative Pre-training
Authors: Fenia Christopoulou, Guchun Zhang, Gerasimos Lampouras
Categories: cs.CL
Comments: 15 pages, 5 figures, 6 tables. Accepted at EACL 2024
\\
  Large pre-trained language models have recently been expanded and applied to
programming language tasks with great success, often through further
pre-training of a strictly-natural language model--where training sequences
typically contain both natural and (linearised) programming language. Such
approaches effectively map both modalities of the sequence into the same
embedding space. However, programming language keywords (e.g. ``while'') often
have very strictly defined semantics. As such, transfer learning from their
natural language usage may not necessarily be beneficial to their code
application and vise versa. Assuming an already pre-trained language model, in
this work we investigate how sequence tokens can be adapted and represented
differently, depending on which modality they belong to, and to the ultimate
benefit of the downstream task. We experiment with separating embedding spaces
between modalities during further model pre-training with modality-relative
training objectives. We focus on text-to-code generation and observe consistent
improvements across two backbone models and two test sets, measuring pass@$k$
and a novel incremental variation.
\\ ( https://arxiv.org/abs/2402.05783 ,  8139kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05794
Date: Thu, 8 Feb 2024 16:36:11 GMT   (1035kb,D)

Title: Phonetically rich corpus construction for a low-resourced language
Authors: Marcellus Amadeus and William Alberto Cruz Casta\~neda and Wilmer
  Lobato and Niasche Aquino
Categories: cs.CL cs.AI
\\
  Speech technologies rely on capturing a speaker's voice variability while
obtaining comprehensive language information. Textual prompts and sentence
selection methods have been proposed in the literature to comprise such
adequate phonetic data, referred to as a phonetically rich \textit{corpus}.
However, they are still insufficient for acoustic modeling, especially critical
for languages with limited resources. Hence, this paper proposes a novel
approach and outlines the methodological aspects required to create a
\textit{corpus} with broad phonetic coverage for a low-resourced language,
Brazilian Portuguese. Our methodology includes text dataset collection up to a
sentence selection algorithm based on triphone distribution. Furthermore, we
propose a new phonemic classification according to acoustic-articulatory speech
features since the absolute number of distinct triphones, or low-probability
triphones, does not guarantee an adequate representation of every possible
combination. Using our algorithm, we achieve a 55.8\% higher percentage of
distinct triphones -- for samples of similar size -- while the currently
available phonetic-rich corpus, CETUC and TTS-Portuguese, 12.6\% and 12.3\% in
comparison to a non-phonetically rich dataset.
\\ ( https://arxiv.org/abs/2402.05794 ,  1035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05812
Date: Thu, 8 Feb 2024 16:49:41 GMT   (341kb,D)

Title: FAQ-Gen: An automated system to generate domain-specific FAQs to aid
  content comprehension
Authors: Sahil Kale, Gautam Khaire, Jay Patankar
Categories: cs.CL
Comments: 13 pages, 4 figures
\\
  Frequently Asked Questions (FAQs) refer to the most common inquiries about
specific content. They serve as content comprehension aids by simplifying
topics and enhancing understanding through succinct presentation of
information. In this paper, we address FAQ generation as a well-defined Natural
Language Processing (NLP) task through the development of an end-to-end system
leveraging text-to-text transformation models. We present a literature review
covering traditional question-answering systems, highlighting their limitations
when applied directly to the FAQ generation task. We propose our system capable
of building FAQs from textual content tailored to specific domains, enhancing
their accuracy and relevance. We utilise self-curated algorithms for obtaining
optimal representation of information to be provided as input and also for
ranking the question-answer pairs to maximise human comprehension. Qualitative
human evaluation showcases the generated FAQs to be well-constructed and
readable, while also utilising domain-specific constructs to highlight
domain-based nuances and jargon in the original content.
\\ ( https://arxiv.org/abs/2402.05812 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05813
Date: Thu, 8 Feb 2024 16:50:01 GMT   (1782kb,D)

Title: Selective Forgetting: Advancing Machine Unlearning Techniques and
  Evaluation in Language Models
Authors: Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong and Georg
  Gottlob
Categories: cs.CL cs.AI
\\
  The aim of this study is to investigate Machine Unlearning (MU), a burgeoning
field focused on addressing concerns related to neural models inadvertently
retaining personal or sensitive data. Here, a novel approach is introduced to
achieve precise and selective forgetting within language models. Unlike
previous methodologies that adopt completely opposing training objectives, this
approach aims to mitigate adverse effects on language model performance,
particularly in generation tasks. Furthermore, two innovative evaluation
metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and
Sensitive Information Memory Accuracy (S-MA), designed to gauge the
effectiveness of sensitive information elimination. To reinforce the forgetting
framework, an effective method for annotating sensitive scopes is presented,
involving both online and offline strategies. The online selection mechanism
leverages language probability scores to ensure computational efficiency, while
the offline annotation entails a robust two-stage process based on Large
Language Models (LLMs).
\\ ( https://arxiv.org/abs/2402.05813 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05827
Date: Thu, 8 Feb 2024 17:06:45 GMT   (3798kb,D)

Title: Is it Possible to Edit Large Language Models Robustly?
Authors: Xinbei Ma, Tianjie Ju, Jiyang Qiu, Zhuosheng Zhang, Hai Zhao, Lifeng
  Liu, Yulong Wang
Categories: cs.CL
Comments: Working in progress
\\
  Large language models (LLMs) have played a pivotal role in building
communicative AI to imitate human behaviors but face the challenge of efficient
customization. To tackle this challenge, recent studies have delved into the
realm of model editing, which manipulates specific memories of language models
and changes the related language generation. However, the robustness of model
editing remains an open question. This work seeks to understand the strengths
and limitations of editing methods, thus facilitating robust, realistic
applications of communicative AI. Concretely, we conduct extensive analysis to
address the three key research questions. Q1: Can edited LLMs behave
consistently resembling communicative AI in realistic situations? Q2: To what
extent does the rephrasing of prompts lead LLMs to deviate from the edited
knowledge memory? Q3: Which knowledge features are correlated with the
performance and robustness of editing? Our experimental results uncover a
substantial disparity between existing editing methods and the practical
application of LLMs. On rephrased prompts that are complex and flexible but
common in realistic applications, the performance of editing experiences a
significant decline. Further analysis shows that more popular knowledge is
memorized better, easier to recall, and more challenging to edit effectively.
\\ ( https://arxiv.org/abs/2402.05827 ,  3798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05864
Date: Thu, 8 Feb 2024 17:54:23 GMT   (143kb,D)

Title: Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
Authors: Xuandong Zhao, Lei Li, Yu-Xiang Wang
Categories: cs.CL cs.CR cs.LG
\\
  In this paper, we propose a new decoding method called Permute-and-Flip (PF)
decoder. It enjoys robustness properties similar to the standard sampling
decoder, but is provably up to 2x better in its quality-robustness tradeoff
than sampling and never worse than any other decoder. We also design a
cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but
naturally tailored for PF decoder. The watermarking scheme does not change the
distribution to sample, while allowing arbitrarily low false positive rate and
high recall whenever the generated text has high entropy. Our experiments show
that the PF decoder (and its watermarked counterpart) significantly
outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms
of perplexity, while retaining the same robustness (and detectability), hence
making it a promising new approach for LLM decoding. The code is available at
https://github.com/XuandongZhao/pf-decoding
\\ ( https://arxiv.org/abs/2402.05864 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05868
Date: Thu, 8 Feb 2024 17:57:11 GMT   (2503kb,D)

Title: PromptCrypt: Prompt Encryption for Secure Communication with Large
  Language Models
Authors: Guo Lin, Wenyue Hua, Yongfeng Zhang
Categories: cs.CL cs.AI cs.CR cs.IR cs.LG
Comments: 12 pages, 4 figures, 2 tables, comments and suggestions are welcome
\\
  Cloud-based large language models (LLMs) such as ChatGPT have increasingly
become integral to daily operations, serving as vital tools across various
applications. While these models offer substantial benefits in terms of
accessibility and functionality, they also introduce significant privacy
concerns: the transmission and storage of user data in cloud infrastructures
pose substantial risks of data breaches and unauthorized access to sensitive
information; even if the transmission and storage of data is encrypted, the LLM
service provider itself still knows the real contents of the data, preventing
individuals or entities from confidently using such LLM services. To address
these concerns, this paper proposes a simple yet effective mechanism
PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs
before sending them to LLM, effectively rendering them indecipherable to human
or LLM's examination while retaining the original intent of the prompt, thus
ensuring the model's performance remains unaffected. We conduct experiments on
three tasks, personalized recommendation, sentiment analysis, and tabular data
analysis. Experiment results reveal that PromptCrypt can encrypt personal
information within prompts in such a manner that not only prevents the
discernment of sensitive data by humans or LLM itself, but also maintains or
even improves the precision without further tuning, achieving comparable or
even better task accuracy than directly prompting the LLM without prompt
encryption. These results highlight the practicality of adopting encryption
measures that safeguard user privacy without compromising the functional
integrity and performance of LLMs. Code and dataset are available at
https://github.com/agiresearch/PromptCrypt.
\\ ( https://arxiv.org/abs/2402.05868 ,  2503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05880
Date: Thu, 8 Feb 2024 18:14:33 GMT   (1693kb,D)

Title: Generative Echo Chamber? Effects of LLM-Powered Search Systems on
  Diverse Information Seeking
Authors: Nikhil Sharma, Q. Vera Liao, Ziang Xiao
Categories: cs.CL cs.AI cs.HC
Comments: Accepted in CHI'24
\\
  Large language models (LLMs) powered conversational search systems have
already been used by hundreds of millions of people, and are believed to bring
many benefits over conventional search. However, while decades of research and
public discourse interrogated the risk of search systems in increasing
selective exposure and creating echo chambers -- limiting exposure to diverse
opinions and leading to opinion polarization, little is known about such a risk
of LLM-powered conversational search. We conduct two experiments to
investigate: 1) whether and how LLM-powered conversational search increases
selective exposure compared to conventional search; 2) whether and how LLMs
with opinion biases that either reinforce or challenge the user's view change
the effect. Overall, we found that participants engaged in more biased
information querying with LLM-powered conversational search, and an opinionated
LLM reinforcing their views exacerbated this bias. These results present
critical implications for the development of LLMs and conversational search
systems, and the policy governing these technologies.
\\ ( https://arxiv.org/abs/2402.05880 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05904
Date: Thu, 8 Feb 2024 18:43:05 GMT   (1416kb,D)

Title: FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs
Authors: Eun Cheol Choi, Emilio Ferrara
Categories: cs.CL cs.CY cs.HC cs.SI
\\
  Our society is facing rampant misinformation harming public health and trust.
To address the societal challenge, we introduce FACT-GPT, a system leveraging
Large Language Models (LLMs) to automate the claim matching stage of
fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social
media content that aligns with, contradicts, or is irrelevant to previously
debunked claims. Our evaluation shows that our specialized LLMs can match the
accuracy of larger models in identifying related claims, closely mirroring
human judgment. This research provides an automated solution for efficient
claim matching, demonstrates the potential of LLMs in supporting fact-checkers,
and offers valuable resources for further research in the field.
\\ ( https://arxiv.org/abs/2402.05904 ,  1416kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05913
Date: Thu, 8 Feb 2024 18:49:09 GMT   (822kb,D)

Title: Efficient Stagewise Pretraining via Progressive Subnetworks
Authors: Abhishek Panigrahi, Nikunj Saunshi, Kaifeng Lyu, Sobhan Miryoosefi,
  Sashank Reddi, Satyen Kale, Sanjiv Kumar
Categories: cs.CL cs.LG
\\
  Recent developments in large language models have sparked interest in
efficient pretraining methods. A recent effective paradigm is to perform
stage-wise training, where the size of the model is gradually increased over
the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the
resource and wall-time savings are appealing, it has limitations, particularly
the inability to evaluate the full model during earlier stages, and degradation
in model quality due to smaller model capacity in the initial stages. In this
work, we propose an alternative framework, progressive subnetwork training,
that maintains the full model throughout training, but only trains subnetworks
within the model in each step. We focus on a simple instantiation of this
framework, Random Path Training (RaPTr) that only trains a sub-path of layers
in each step, progressively increasing the path lengths in stages. RaPTr
achieves better pre-training loss for BERT and UL2 language models while
requiring 20-33% fewer FLOPs compared to standard training, and is competitive
or better than other efficient training methods. Furthermore, RaPTr shows
better downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5%
compared to standard training and stacking. Finally, we provide a theoretical
basis for RaPTr to justify (a) the increasing complexity of subnetworks in
stages, and (b) the stability in loss across stage transitions due to residual
connections and layer norm.
\\ ( https://arxiv.org/abs/2402.05913 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05930
Date: Thu, 8 Feb 2024 18:58:02 GMT   (5698kb,D)

Title: WebLINX: Real-World Website Navigation with Multi-Turn Dialogue
Authors: Xing Han L\`u, Zden\v{e}k Kasner, Siva Reddy
Categories: cs.CL cs.CV cs.LG
\\
  We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx
\\ ( https://arxiv.org/abs/2402.05930 ,  5698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05140
Date: Tue, 6 Feb 2024 20:11:54 GMT   (1320kb,D)

Title: Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
Authors: Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis,
  Nicolo Fusi
Categories: cs.LG cs.AI cs.CL
\\
  Large Language Models (LLMs) have demonstrated remarkable proficiency in
understanding and generating natural language. However, their capabilities wane
in highly specialized domains underrepresented in the pretraining corpus, such
as physical and biomedical sciences. This work explores how to repurpose
general LLMs into effective task solvers for specialized domains. We introduce
a novel, model-agnostic framework for learning custom input tags, which are
parameterized as continuous vectors appended to the LLM's embedding layer, to
condition the LLM. We design two types of input tags: domain tags are used to
delimit specialized representations (e.g., chemical formulas) and provide
domain-relevant context; function tags are used to represent specific functions
(e.g., predicting molecular properties) and compress function-solving
instructions. We develop a three-stage protocol to learn these tags using
auxiliary data and domain knowledge. By explicitly disentangling task domains
from task functions, our method enables zero-shot generalization to unseen
problems through diverse combinations of the input tags. It also boosts LLM's
performance in various specialized domains, such as predicting protein or
chemical properties and modeling drug-target interactions, outperforming expert
models tailored to these tasks.
\\ ( https://arxiv.org/abs/2402.05140 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05145
Date: Wed, 7 Feb 2024 08:15:30 GMT   (349kb,D)

Title: Online Learning Approach for Survival Analysis
Authors: Camila Fernandez (LPSM), Pierre Gaillard (Thoth), Joseph de Vilmarest,
  Olivier Wintenberger (LPSM (UMR\_8001))
Categories: cs.LG physics.data-an stat.ML
\\
  We introduce an online mathematical framework for survival analysis, allowing
real time adaptation to dynamic environments and censored data. This framework
enables the estimation of event time distributions through an optimal second
order online convex optimization algorithm-Online Newton Step (ONS). This
approach, previously unexplored, presents substantial advantages, including
explicit algorithms with non-asymptotic convergence guarantees. Moreover, we
analyze the selection of ONS hyperparameters, which depends on the
exp-concavity property and has a significant influence on the regret bound. We
propose a stochastic approach that guarantees logarithmic stochastic regret for
ONS. Additionally, we introduce an adaptive aggregation method that ensures
robustness in hyperparameter selection while maintaining fast regret bounds.
The findings of this paper can extend beyond the survival analysis field, and
are relevant for any case characterized by poor exp-concavity and unstable ONS.
Finally, these assertions are illustrated by simulation experiments.
\\ ( https://arxiv.org/abs/2402.05145 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05146
Date: Wed, 7 Feb 2024 09:00:30 GMT   (2521kb,D)

Title: Compressing Deep Reinforcement Learning Networks with a Dynamic
  Structured Pruning Method for Autonomous Driving
Authors: Wensheng Su, Zhenni Li, Minrui Xu, Jiawen Kang, Dusit Niyato, Shengli
  Xie
Categories: cs.LG cs.AI cs.RO
\\
  Deep reinforcement learning (DRL) has shown remarkable success in complex
autonomous driving scenarios. However, DRL models inevitably bring high memory
consumption and computation, which hinders their wide deployment in
resource-limited autonomous driving devices. Structured Pruning has been
recognized as a useful method to compress and accelerate DRL models, but it is
still challenging to estimate the contribution of a parameter (i.e., neuron) to
DRL models. In this paper, we introduce a novel dynamic structured pruning
approach that gradually removes a DRL model's unimportant neurons during the
training stage. Our method consists of two steps, i.e. training DRL models with
a group sparse regularizer and removing unimportant neurons with a dynamic
pruning threshold. To efficiently train the DRL model with a small number of
important neurons, we employ a neuron-importance group sparse regularizer. In
contrast to conventional regularizers, this regularizer imposes a penalty on
redundant groups of neurons that do not significantly influence the output of
the DRL model. Furthermore, we design a novel structured pruning strategy to
dynamically determine the pruning threshold and gradually remove unimportant
neurons with a binary mask. Therefore, our method can remove not only redundant
groups of neurons of the DRL model but also achieve high and robust
performance. Experimental results show that the proposed method is competitive
with existing DRL pruning methods on discrete control environments (i.e.,
CartPole-v1 and LunarLander-v2) and MuJoCo continuous environments (i.e.,
Hopper-v3 and Walker2D-v3). Specifically, our method effectively compresses
$93\%$ neurons and $96\%$ weights of the DRL model in four challenging DRL
environments with slight accuracy degradation.
\\ ( https://arxiv.org/abs/2402.05146 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05147
Date: Wed, 7 Feb 2024 09:36:54 GMT   (4435kb,D)

Title: ApiQ: Finetuning of 2-Bit Quantized Large Language Model
Authors: Baohao Liao, Christof Monz
Categories: cs.LG cs.CL
\\
  Memory-efficient finetuning of large language models (LLMs) has recently
attracted huge attention with the increasing size of LLMs, primarily due to the
constraints posed by GPU memory limitations and the comparable results of these
methods with full finetuning. Despite the advancements, current strategies for
memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance
across diverse bit-width quantizations and multifaceted tasks. This
inconsistency largely stems from the detrimental impact of the quantization
process on preserved knowledge, leading to catastrophic forgetting and
undermining the utilization of pretrained models for finetuning purposes. In
this work, we introduce a novel quantization framework named ApiQ, designed to
restore the lost information from quantization by concurrently initializing
LoRA components and quantizing the weights of LLMs. This approach ensures the
maintenance of the original LLM's activation precision while mitigating the
error propagation from shallower into deeper layers. Through comprehensive
evaluations conducted on a spectrum of language tasks with various models, ApiQ
demonstrably minimizes activation error during quantization. Consequently, it
consistently achieves superior finetuning outcomes across various bit-widths of
quantization.
\\ ( https://arxiv.org/abs/2402.05147 ,  4435kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05149
Date: Wed, 7 Feb 2024 11:11:46 GMT   (2783kb,D)

Title: FlowPG: Action-constrained Policy Gradient with Normalizing Flows
Authors: Janaka Chathuranga Brahmanage, Jiajing Ling, Akshat Kumar
Categories: cs.LG cs.AI
Journal-ref: Thirty-seventh Conference on Neural Information Processing
  Systems. 2023
\\
  Action-constrained reinforcement learning (ACRL) is a popular approach for
solving safety-critical and resource-allocation related decision making
problems. A major challenge in ACRL is to ensure agent taking a valid action
satisfying constraints in each RL step. Commonly used approach of using a
projection layer on top of the policy network requires solving an optimization
program which can result in longer training time, slow convergence, and zero
gradient problem. To address this, first we use a normalizing flow model to
learn an invertible, differentiable mapping between the feasible action space
and the support of a simple distribution on a latent variable, such as
Gaussian. Second, learning the flow model requires sampling from the feasible
action space, which is also challenging. We develop multiple methods, based on
Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such
action sampling for convex and non-convex constraints. Third, we integrate the
learned normalizing flow with the DDPG algorithm. By design, a well-trained
normalizing flow will transform policy output into a valid action without
requiring an optimization solver. Empirically, our approach results in
significantly fewer constraint violations (upto an order-of-magnitude for
several instances) and is multiple times faster on a variety of continuous
control tasks.
\\ ( https://arxiv.org/abs/2402.05149 ,  2783kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05150
Date: Wed, 7 Feb 2024 12:54:15 GMT   (387kb,D)

Title: Designing deep neural networks for driver intention recognition
Authors: Koen Vellenga, H. Joe Steinhauer, Alexander Karlsson, G\"oran Falkman,
  Asli Rhodin and Ashok Koppisetty
Categories: cs.LG cs.NE
\\
  Driver intention recognition studies increasingly rely on deep neural
networks. Deep neural networks have achieved top performance for many different
tasks, but it is not a common practice to explicitly analyse the complexity and
performance of the network's architecture. Therefore, this paper applies neural
architecture search to investigate the effects of the deep neural network
architecture on a real-world safety critical application with limited
computational capabilities. We explore a pre-defined search space for three
deep neural network layer types that are capable to handle sequential data (a
long-short term memory, temporal convolution, and a time-series transformer
layer), and the influence of different data fusion strategies on the driver
intention recognition performance. A set of eight search strategies are
evaluated for two driver intention recognition datasets. For the two datasets,
we observed that there is no search strategy clearly sampling better deep
neural network architectures. However, performing an architecture search does
improve the model performance compared to the original manually designed
networks. Furthermore, we observe no relation between increased model
complexity and higher driver intention recognition performance. The result
indicate that multiple architectures yield similar performance, regardless of
the deep neural network layer type or fusion strategy.
\\ ( https://arxiv.org/abs/2402.05150 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05151
Date: Wed, 7 Feb 2024 13:09:23 GMT   (181kb,D)

Title: CrashFormer: A Multimodal Architecture to Predict the Risk of Crash
Authors: Amin Karimi Monsefi, Pouya Shiri, Ahmad Mohammadshirazi, Nastaran
  Karimi Monsefi, Ron Davies, Sobhan Moosavi and Rajiv Ramnath
Categories: cs.LG cs.AI
Comments: The paper is accepted In 1st ACM SIGSPATIAL International Workshop on
  Advances in Urban-AI (UrbanAI 23), November 13, 2023, Hamburg, Germany
DOI: 10.1145/3615900.3628769
\\
  Reducing traffic accidents is a crucial global public safety concern.
Accident prediction is key to improving traffic safety, enabling proactive
measures to be taken before a crash occurs, and informing safety policies,
regulations, and targeted interventions. Despite numerous studies on accident
prediction over the past decades, many have limitations in terms of
generalizability, reproducibility, or feasibility for practical use due to
input data or problem formulation. To address existing shortcomings, we propose
CrashFormer, a multi-modal architecture that utilizes comprehensive (but
relatively easy to obtain) inputs such as the history of accidents, weather
information, map images, and demographic information. The model predicts the
future risk of accidents on a reasonably acceptable cadence (i.e., every six
hours) for a geographical location of 5.161 square kilometers. CrashFormer is
composed of five components: a sequential encoder to utilize historical
accidents and weather data, an image encoder to use map imagery data, a raw
data encoder to utilize demographic information, a feature fusion module for
aggregating the encoded features, and a classifier that accepts the aggregated
data and makes predictions accordingly. Results from extensive real-world
experiments in 10 major US cities show that CrashFormer outperforms
state-of-the-art sequential and non-sequential models by 1.8% in F1-score on
average when using ``sparse'' input data.
\\ ( https://arxiv.org/abs/2402.05151 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05153
Date: Wed, 7 Feb 2024 13:51:33 GMT   (2584kb,D)

Title: Estimating On-road Transportation Carbon Emissions from Open Data of
  Road Network and Origin-destination Flow Data
Authors: Jinwei Zeng and Yu Liu and Jingtao Ding and Jian Yuan and Yong Li
Categories: cs.LG
\\
  Accounting for over 20% of the total carbon emissions, the precise estimation
of on-road transportation carbon emissions is crucial for carbon emission
monitoring and efficient mitigation policy formulation. However, existing
estimation methods typically depend on hard-to-collect individual statistics of
vehicle miles traveled to calculate emissions, thereby suffering from high data
collection difficulty. To relieve this issue by utilizing the strong pattern
recognition of artificial intelligence, we incorporate two sources of open data
representative of the transportation demand and capacity factors, the
origin-destination (OD) flow data and the road network data, to build a
hierarchical heterogeneous graph learning method for on-road carbon emission
estimation (HENCE). Specifically, a hierarchical graph consisting of the road
network level, community level, and region level is constructed to model the
multi-scale road network-based connectivity and travel connection between
spatial areas. Heterogeneous graphs consisting of OD links and spatial links
are further built at both the community level and region level to capture the
intrinsic interactions between travel demand and road network accessibility.
Extensive experiments on two large-scale real-world datasets demonstrate
HENCE's effectiveness and superiority with R-squared exceeding 0.75 and
outperforming baselines by 9.60% on average, validating its success in
pioneering the use of artificial intelligence to empower carbon emission
management and sustainability development. The implementation codes are
available at this link: https://github.com/tsinghua-fib-lab/HENCE.
\\ ( https://arxiv.org/abs/2402.05153 ,  2584kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05162
Date: Wed, 7 Feb 2024 18:34:38 GMT   (649kb,D)

Title: Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank
  Modifications
Authors: Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi,
  Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson
Categories: cs.LG cs.AI cs.CL
Comments: 22 pages, 9 figures. Project page is available at
  https://boyiwei.com/alignment-attribution/
\\
  Large language models (LLMs) show inherent brittleness in their safety
mechanisms, as evidenced by their susceptibility to jailbreaking and even
non-malicious fine-tuning. This study explores this brittleness of safety
alignment by leveraging pruning and low-rank modifications. We develop methods
to identify critical regions that are vital for safety guardrails, and that are
disentangled from utility-relevant regions at both the neuron and rank levels.
Surprisingly, the isolated regions we find are sparse, comprising about $3\%$
at the parameter level and $2.5\%$ at the rank level. Removing these regions
compromises safety without significantly impacting utility, corroborating the
inherent brittleness of the model's safety mechanisms. Moreover, we show that
LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications
to the safety-critical regions are restricted. These findings underscore the
urgent need for more robust safety strategies in LLMs.
\\ ( https://arxiv.org/abs/2402.05162 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05164
Date: Wed, 7 Feb 2024 18:58:18 GMT   (4112kb,D)

Title: A Resource Model For Neural Scaling Law
Authors: Jinyeop Song, Ziming Liu, Max Tegmark, Jeff Gore
Categories: cs.LG cs.AI cs.NE
Comments: 10 pages, 8 figures, Under review as a workshop paper at ICLR 2024
\\
  Neural scaling laws characterize how model performance improves as the model
size scales up. Inspired by empirical observations, we introduce a resource
model of neural scaling. A task is usually composite hence can be decomposed
into many subtasks, which compete for resources (measured by the number of
neurons allocated to subtasks). On toy problems, we empirically find that: (1)
The loss of a subtask is inversely proportional to its allocated neurons. (2)
When multiple subtasks are present in a composite task, the resources acquired
by each subtask uniformly grow as models get larger, keeping the ratios of
acquired resources constants. We hypothesize these findings to be generally
true and build a model to predict neural scaling laws for general composite
tasks, which successfully replicates the neural scaling law of Chinchilla
models reported in arXiv:2203.15556. We believe that the notion of resource
used in this paper will be a useful tool for characterizing and diagnosing
neural networks.
\\ ( https://arxiv.org/abs/2402.05164 ,  4112kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05173
Date: Wed, 7 Feb 2024 19:00:01 GMT   (1020kb,D)

Title: Towards Understanding Inductive Bias in Transformers: A View From
  Infinity
Authors: Itay Lavie, Guy Gur-Ari and Zohar Ringel
Categories: cs.LG cond-mat.dis-nn stat.ML
\\
  We study inductive bias in Transformers in the infinitely over-parameterized
Gaussian process limit and argue transformers tend to be biased towards more
permutation symmetric functions in sequence space. We show that the
representation theory of the symmetric group can be used to give quantitative
analytical predictions when the dataset is symmetric to permutations between
tokens. We present a simplified transformer block and solve the model at the
limit, including accurate predictions for the learning curves and network
outputs. We show that in common setups, one can derive tight bounds in the form
of a scaling law for the learnability as a function of the context length.
Finally, we argue WikiText dataset, does indeed possess a degree of permutation
symmetry.
\\ ( https://arxiv.org/abs/2402.05173 ,  1020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05203
Date: Wed, 7 Feb 2024 19:15:33 GMT   (1504kb,D)

Title: Bellman Conformal Inference: Calibrating Prediction Intervals For Time
  Series
Authors: Zitong Yang, Emmanuel Cand\`es, Lihua Lei
Categories: cs.LG stat.ML
Comments: 17 pages, 4 figures
\\
  We introduce Bellman Conformal Inference (BCI), a framework that wraps around
any time series forecasting models and provides calibrated prediction
intervals. Unlike the existing methods, BCI is able to leverage multi-step
ahead forecasts and explicitly optimize the average interval lengths by solving
a one-dimensional stochastic control problem (SCP) at each time step. In
particular, we use the dynamic programming algorithm to find the optimal policy
for the SCP. We prove that BCI achieves long-term coverage under arbitrary
distribution shifts and temporal dependence, even with poor multi-step ahead
forecasts. We find empirically that BCI avoids uninformative intervals that
have infinite lengths and generates substantially shorter prediction intervals
on volatility forecasting problems when compared with existing methods.
\\ ( https://arxiv.org/abs/2402.05203 ,  1504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05232
Date: Wed, 7 Feb 2024 20:12:27 GMT   (512kb,D)

Title: Universal Neural Functionals
Authors: Allan Zhou, Chelsea Finn, James Harrison
Categories: cs.LG cs.AI
\\
  A challenging problem in many modern machine learning tasks is to process
weight-space features, i.e., to transform or extract information from the
weights and gradients of a neural network. Recent works have developed
promising weight-space models that are equivariant to the permutation
symmetries of simple feedforward networks. However, they are not applicable to
general architectures, since the permutation symmetries of a weight space can
be complicated by recurrence or residual connections. This work proposes an
algorithm that automatically constructs permutation equivariant models, which
we refer to as universal neural functionals (UNFs), for any weight space. Among
other applications, we demonstrate how UNFs can be substituted into existing
learned optimizer designs, and find promising improvements over prior methods
when optimizing small image classifiers and language models. Our results
suggest that learned optimizers can benefit from considering the (symmetry)
structure of the weight space they optimize. We open-source our library for
constructing UNFs at
https://github.com/AllanYangZhou/universal_neural_functional.
\\ ( https://arxiv.org/abs/2402.05232 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05234
Date: Wed, 7 Feb 2024 20:14:22 GMT   (46168kb,D)

Title: QGFN: Controllable Greediness with Action Values
Authors: Elaine Lau, Stephen Zhewen Lu, Ling Pan, Doina Precup, Emmanuel Bengio
Categories: cs.LG
Comments: Under review
\\
  Generative Flow Networks (GFlowNets; GFNs) are a family of
reward/energy-based generative methods for combinatorial objects, capable of
generating diverse and high-utility samples. However, biasing GFNs towards
producing high-utility samples is non-trivial. In this work, we leverage
connections between GFNs and reinforcement learning (RL) and propose to combine
the GFN policy with an action-value estimate, $Q$, to create greedier sampling
policies which can be controlled by a mixing parameter. We show that several
variants of the proposed method, QGFN, are able to improve on the number of
high-reward samples generated in a variety of tasks without sacrificing
diversity.
\\ ( https://arxiv.org/abs/2402.05234 ,  46168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05252
Date: Wed, 7 Feb 2024 20:53:53 GMT   (3554kb,D)

Title: Learning Fair Ranking Policies via Differentiable Optimization of
  Ordered Weighted Averages
Authors: My H. Dinh, James Kotary, Ferdinando Fioretto
Categories: cs.LG cs.AI cs.CY
\\
  Learning to Rank (LTR) is one of the most widely used machine learning
applications. It is a key component in platforms with profound societal
impacts, including job search, healthcare information retrieval, and social
media content feeds. Conventional LTR models have been shown to produce biases
results, stimulating a discourse on how to address the disparities introduced
by ranking systems that solely prioritize user relevance. However, while
several models of fair learning to rank have been proposed, they suffer from
deficiencies either in accuracy or efficiency, thus limiting their
applicability to real-world ranking platforms. This paper shows how
efficiently-solvable fair ranking models, based on the optimization of Ordered
Weighted Average (OWA) functions, can be integrated into the training loop of
an LTR model to achieve favorable balances between fairness, user utility, and
runtime efficiency. In particular, this paper is the first to show how to
backpropagate through constrained optimizations of OWA objectives, enabling
their use in integrated prediction and decision models.
\\ ( https://arxiv.org/abs/2402.05252 ,  3554kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05264
Date: Wed, 7 Feb 2024 21:19:05 GMT   (1202kb,D)

Title: AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size
Authors: Petr Ostroukhov, Aigerim Zhumabayeva, Chulu Xiang, Alexander Gasnikov,
  Martin Tak\'a\v{c}, Dmitry Kamzolov
Categories: cs.LG math.OC
\\
  This paper presents a novel adaptation of the Stochastic Gradient Descent
(SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive
step size with an adjustable batch size. An increase in batch size and a
decrease in step size are well-known techniques to tighten the area of
convergence of SGD and decrease its variance. A range of studies by R. Byrd and
J. Nocedal introduced various testing techniques to assess the quality of
mini-batch gradient approximations and choose the appropriate batch sizes at
every step. Methods that utilized exact tests were observed to converge within
$O(LR^2/\varepsilon)$ iterations. Conversely, inexact test implementations
sometimes resulted in non-convergence and erratic performance. To address these
challenges, AdaBatchGrad incorporates both adaptive batch and step sizes,
enhancing the method's robustness and stability. For exact tests, our approach
converges in $O(LR^2/\varepsilon)$ iterations, analogous to standard gradient
descent. For inexact tests, it achieves convergence in $O(\max\lbrace
LR^2/\varepsilon, \sigma^2 R^2/\varepsilon^2 \rbrace )$ iterations. This makes
AdaBatchGrad markedly more robust and computationally efficient relative to
prevailing methods. To substantiate the efficacy of our method, we
experimentally show, how the introduction of adaptive step size and adaptive
batch size gradually improves the performance of regular SGD. The results imply
that AdaBatchGrad surpasses alternative methods, especially when applied to
inexact tests.
\\ ( https://arxiv.org/abs/2402.05264 ,  1202kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05274
Date: Wed, 7 Feb 2024 21:43:57 GMT   (34kb)

Title: Convergence for Natural Policy Gradient on Infinite-State Average-Reward
  Markov Decision Processes
Authors: Isaac Grosof, Siva Theja Maguluri, R. Srikant
Categories: cs.LG
Comments: 26 pages
\\
  Infinite-state Markov Decision Processes (MDPs) are essential in modeling and
optimizing a wide variety of engineering problems. In the reinforcement
learning (RL) context, a variety of algorithms have been developed to learn and
optimize these MDPs. At the heart of many popular policy-gradient based
learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the
Natural Policy Gradient (NPG) algorithm. Convergence results for these RL
algorithms rest on convergence results for the NPG algorithm. However, all
existing results on the convergence of the NPG algorithm are limited to
finite-state settings.
  We prove the first convergence rate bound for the NPG algorithm for
infinite-state average-reward MDPs, proving a $O(1/\sqrt{T})$ convergence rate,
if the NPG algorithm is initialized with a good initial policy. Moreover, we
show that in the context of a large class of queueing MDPs, the MaxWeight
policy suffices to satisfy our initial-policy requirement and achieve a
$O(1/\sqrt{T})$ convergence rate. Key to our result are state-dependent bounds
on the relative value function achieved by the iterate policies of the NPG
algorithm.
\\ ( https://arxiv.org/abs/2402.05274 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05275
Date: Wed, 7 Feb 2024 21:46:26 GMT   (593kb)

Title: Exploring Hierarchical Classification Performance for Time Series Data:
  Dissimilarity Measures and Classifier Comparisons
Authors: Celal Alagoz
Categories: cs.LG
Comments: 9 pages, 2 figures, 5th International Mediterranean Congress 1,
  1367-1376
MSC-class: 62H30
ACM-class: I.5.2; I.5.3
\\
  The comparative performance of hierarchical classification (HC) and flat
classification (FC) methodologies in the realm of time series data analysis is
investigated in this study. Dissimilarity measures, including Jensen-Shannon
Distance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance
(CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF,
and SVM. A subset of datasets from the UCR archive, focusing on multi-class
cases comprising more than two classes, is employed for analysis. A significant
trend is observed wherein HC demonstrates significant superiority over FC when
paired with MINIROCKET utilizing TSD, diverging from conventional
understandings. Conversely, FC exhibits consistent dominance across all
configurations when employing alternative classifiers such as STSF and SVM.
Moreover, TSD is found to consistently outperform both CBD and JSD across
nearly all scenarios, except in instances involving the STSF classifier where
CBD showcases superior performance. This discrepancy underscores the nuanced
nature of dissimilarity measures and emphasizes the importance of their
tailored selection based on the dataset and classifier employed. Valuable
insights into the dynamic interplay between classification methodologies and
dissimilarity measures in the realm of time series data analysis are provided
by these findings. By elucidating the performance variations across different
configurations, a foundation is laid for refining classification methodologies
and dissimilarity measures to optimize performance in diverse analytical
scenarios. Furthermore, the need for continued research aimed at elucidating
the underlying mechanisms driving classification performance in time series
data analysis is underscored, with implications for enhancing predictive
modeling and decision-making in various domains.
\\ ( https://arxiv.org/abs/2402.05275 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05279
Date: Wed, 7 Feb 2024 21:49:51 GMT   (1561kb,D)

Title: Safety Filters for Black-Box Dynamical Systems by Learning
  Discriminating Hyperplanes
Authors: Will Lavanakul, Jason J. Choi, Koushil Sreenath, Claire J. Tomlin
Categories: cs.LG
Comments: * indicate co-first authors. This is an extended version of the paper
  submitted to L4DC 2024
\\
  Learning-based approaches are emerging as an effective approach for safety
filters for black-box dynamical systems. Existing methods have relied on
certificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi
(HJ) reachability value functions. The primary motivation for our work is the
recognition that ultimately, enforcing the safety constraint as a control input
constraint at each state is what matters. By focusing on this constraint, we
can eliminate dependence on any specific certificate function-based design. To
achieve this, we define a discriminating hyperplane that shapes the half-space
constraint on control input at each state, serving as a sufficient condition
for safety. This concept not only generalizes over traditional safety methods
but also simplifies safety filter design by eliminating dependence on specific
certificate functions. We present two strategies to learn the discriminating
hyperplane: (a) a supervised learning approach, using pre-verified control
invariant sets for labeling, and (b) a reinforcement learning (RL) approach,
which does not require such labels. The main advantage of our method, unlike
conventional safe RL approaches, is the separation of performance and safety.
This offers a reusable safety filter for learning new tasks, avoiding the need
to retrain from scratch. As such, we believe that the new notion of the
discriminating hyperplane offers a more generalizable direction towards
designing safety filters, encompassing and extending existing
certificate-function-based or safe RL methodologies.
\\ ( https://arxiv.org/abs/2402.05279 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05280
Date: Wed, 7 Feb 2024 21:53:01 GMT   (62kb)

Title: No Dimensional Sampling Coresets for Classification
Authors: Meysam Alishahi and Jeff M. Phillips
Categories: cs.LG cs.CG
Comments: 47 Pages
\\
  We refine and generalize what is known about coresets for classification
problems via the sensitivity sampling framework. Such coresets seek the
smallest possible subsets of input data, so one can optimize a loss function on
the coreset and ensure approximation guarantees with respect to the original
data. Our analysis provides the first no dimensional coresets, so the size does
not depend on the dimension. Moreover, our results are general, apply for
distributional input and can use iid samples, so provide sample complexity
bounds, and work for a variety of loss functions. A key tool we develop is a
Radamacher complexity version of the main sensitivity sampling approach, which
can be of independent interest.
\\ ( https://arxiv.org/abs/2402.05280 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05284
Date: Wed, 7 Feb 2024 21:58:40 GMT   (698kb,D)

Title: Analyzing Adversarial Inputs in Deep Reinforcement Learning
Authors: Davide Corsi, Guy Amir, Guy Katz, Alessandro Farinelli
Categories: cs.LG
\\
  In recent years, Deep Reinforcement Learning (DRL) has become a popular
paradigm in machine learning due to its successful applications to real-world
and complex systems. However, even the state-of-the-art DRL models have been
shown to suffer from reliability concerns -- for example, their susceptibility
to adversarial inputs, i.e., small and abundant input perturbations that can
fool the models into making unpredictable and potentially dangerous decisions.
This drawback limits the deployment of DRL systems in safety-critical contexts,
where even a small error cannot be tolerated. In this work, we present a
comprehensive analysis of the characterization of adversarial inputs, through
the lens of formal verification. Specifically, we introduce a novel metric, the
Adversarial Rate, to classify models based on their susceptibility to such
perturbations, and present a set of tools and algorithms for its computation.
Our analysis empirically demonstrates how adversarial inputs can affect the
safety of a given DRL system with respect to such perturbations. Moreover, we
analyze the behavior of these configurations to suggest several useful
practices and guidelines to help mitigate the vulnerability of trained DRL
networks.
\\ ( https://arxiv.org/abs/2402.05284 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05290
Date: Wed, 7 Feb 2024 22:09:46 GMT   (2194kb,D)

Title: Do Transformer World Models Give Better Policy Gradients?
Authors: Michel Ma, Tianwei Ni, Clement Gehring, Pierluca D'Oro, Pierre-Luc
  Bacon
Categories: cs.LG cs.AI
Comments: Michel Ma and Pierluca D'Oro contributed equally
\\
  A natural approach for reinforcement learning is to predict future rewards by
unrolling a neural network world model, and to backpropagate through the
resulting computational graph to learn a policy. However, this method often
becomes impractical for long horizons since typical world models induce
hard-to-optimize loss landscapes. Transformers are known to efficiently
propagate gradients overlong horizons: could they be the solution to this
problem? Surprisingly, we show that commonly-used transformer world models
produce circuitous gradient paths, which can be detrimental to long-range
policy gradients. To tackle this challenge, we propose a class of world models
called Actions World Models (AWMs), designed to provide more direct routes for
gradient propagation. We integrate such AWMs into a policy gradient framework
that underscores the relationship between network architectures and the policy
gradient updates they inherently represent. We demonstrate that AWMs can
generate optimization landscapes that are easier to navigate even when compared
to those from the simulator itself. This property allows transformer AWMs to
produce better policies than competitive baselines in realistic long-horizon
tasks.
\\ ( https://arxiv.org/abs/2402.05290 ,  2194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05291
Date: Wed, 7 Feb 2024 22:10:36 GMT   (31938kb,D)

Title: Graph Neural Networks as Fast and High-fidelity Emulators for
  Finite-Element Ice Sheet Modeling
Authors: Maryam Rahnemoonfar, Younghyun Koo
Categories: cs.LG cs.CE
Comments: 12 pages, 7 figures, 3 tables, Submitted to Nature Communications on
  Feb 7, 2024
\\
  Although the finite element approach of the Ice-sheet and Sea-level System
Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly
and accurately, such numerical modeling requires intensive computation on
central processing units (CPU). In this study, we develop graph neural networks
(GNN) as fast surrogate models to preserve the finite element structure of
ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG),
we train and test three GNNs: graph convolutional network (GCN), graph
attention network (GAT), and equivariant graph convolutional network (EGCN).
These GNNs reproduce ice thickness and velocity with better accuracy than the
classic convolutional neural network (CNN) and multi-layer perception (MLP). In
particular, GNNs successfully capture the ice mass loss and acceleration
induced by higher basal melting rates in the PIG. When our GNN emulators are
implemented on graphic processing units (GPUs), they show up to 50 times faster
computational time than the CPU-based ISSM simulation.
\\ ( https://arxiv.org/abs/2402.05291 ,  31938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05293
Date: Wed, 7 Feb 2024 22:14:14 GMT   (1909kb)

Title: A comparative study on feature selection for a risk prediction model for
  colorectal cancer
Authors: N. Cueto-L\'opez, M. T. Garc\'ia-Ord\'as, V. D\'avila-Batista, V.
  Moreno, N. Aragon\'es, and R. Alaiz-Rodr\'iguez
Categories: cs.LG cs.AI
\\
  Background and objective
  Risk prediction models aim at identifying people at higher risk of developing
a target disease. Feature selection is particularly important to improve the
prediction model performance avoiding overfitting and to identify the leading
cancer risk (and protective) factors. Assessing the stability of feature
selection/ranking algorithms becomes an important issue when the aim is to
analyze the features with more prediction power. Methods
  This work is focused on colorectal cancer, assessing several feature ranking
algorithms in terms of performance for a set of risk prediction models (Neural
Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest
Neighbors and Boosted Trees). Additionally, their robustness is evaluated
following a conventional approach with scalar stability metrics and a visual
approach proposed in this work to study both similarity among feature ranking
techniques as well as their individual stability. A comparative analysis is
carried out between the most relevant features found out in this study and
features provided by the experts according to the state-of-the-art knowledge.
Results
  The two best performance results in terms of Area Under the ROC Curve (AUC)
are achieved with a SVM classifier using the top-41 features selected by the
SVM wrapper approach (AUC=0.693) and Logistic Regression with the top-40
features selected by the Pearson (AUC=0.689). Experiments showed that
performing feature selection contributes to classification performance with a
3.9% and 1.9% improvement in AUC for the SVM and Logistic Regression
classifier, respectively, with respect to the results using the full feature
set. The visual approach proposed in this work allows to see that the Neural
Network-based wrapper ranking is the most unstable while the Random Forest is
the most stable.
\\ ( https://arxiv.org/abs/2402.05293 ,  1909kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05294
Date: Wed, 7 Feb 2024 22:16:53 GMT   (26785kb,D)

Title: Examining Modality Incongruity in Multimodal Federated Learning for
  Medical Vision and Language-based Disease Detection
Authors: Pramit Saha, Divyanshu Mishra, Felix Wagner, Konstantinos Kamnitsas,
  J. Alison Noble
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: 42 pages
\\
  Multimodal Federated Learning (MMFL) utilizes multiple modalities in each
client to build a more powerful Federated Learning (FL) model than its unimodal
counterpart. However, the impact of missing modality in different clients, also
called modality incongruity, has been greatly overlooked. This paper, for the
first time, analyses the impact of modality incongruity and reveals its
connection with data heterogeneity across participating clients. We
particularly inspect whether incongruent MMFL with unimodal and multimodal
clients is more beneficial than unimodal FL. Furthermore, we examine three
potential routes of addressing this issue. Firstly, we study the effectiveness
of various self-attention mechanisms towards incongruity-agnostic information
fusion in MMFL. Secondly, we introduce a modality imputation network (MIN)
pre-trained in a multimodal client for modality translation in unimodal clients
and investigate its potential towards mitigating the missing modality problem.
Thirdly, we assess the capability of client-level and server-level
regularization techniques towards mitigating modality incongruity effects.
Experiments are conducted under several MMFL settings on two publicly available
real-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology
reports.
\\ ( https://arxiv.org/abs/2402.05294 ,  26785kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05295
Date: Wed, 7 Feb 2024 22:17:37 GMT   (3864kb)

Title: An information theoretic approach to quantify the stability of feature
  selection and ranking algorithms
Authors: Alaiz-Rodriguez, R., and Parnell, A. C
Categories: cs.LG cs.AI
\\
  Feature selection is a key step when dealing with high dimensional data. In
particular, these techniques simplify the process of knowledge discovery from
the data by selecting the most relevant features out of the noisy, redundant
and irrelevant features. A problem that arises in many of these practical
applications is that the outcome of the feature selection algorithm is not
stable. Thus, small variations in the data may yield very different feature
rankings. Assessing the stability of these methods becomes an important issue
in the previously mentioned situations. We propose an information theoretic
approach based on the Jensen Shannon divergence to quantify this robustness.
Unlike other stability measures, this metric is suitable for different
algorithm outcomes: full ranked lists, feature subsets as well as the lesser
studied partial ranked lists. This generalized metric quantifies the difference
among a whole set of lists with the same size, following a probabilistic
approach and being able to give more importance to the disagreements that
appear at the top of the list. Moreover, it possesses desirable properties
including correction for change, upper lower bounds and conditions for a
deterministic selection. We illustrate the use of this stability metric with
data generated in a fully controlled way and compare it with popular metrics
including the Spearmans rank correlation and the Kunchevas index on feature
ranking and selection outcomes, respectively. Additionally, experimental
validation of the proposed approach is carried out on a real-world problem of
food quality assessment showing its potential to quantify stability from
different perspectives.
\\ ( https://arxiv.org/abs/2402.05295 ,  3864kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05296
Date: Wed, 7 Feb 2024 22:19:08 GMT   (2643kb)

Title: Classifying spam emails using agglomerative hierarchical clustering and
  a topic-based approach
Authors: F. Janez-Martino, R. Alaiz-Rodriguez, V. Gonzalez-Castro, E. Fidalgo,
  and E. Alegre
Categories: cs.LG cs.AI
\\
  Spam emails are unsolicited, annoying and sometimes harmful messages which
may contain malware, phishing or hoaxes. Unlike most studies that address the
design of efficient anti-spam filters, we approach the spam email problem from
a different and novel perspective. Focusing on the needs of cybersecurity
units, we follow a topic-based approach for addressing the classification of
spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S,
two novel datasets with approximately 15K emails each in English and Spanish,
respectively, and we label them using agglomerative hierarchical clustering
into 11 classes. We evaluate 16 pipelines, combining four text representation
techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words,
Word2Vec and BERT- and four classifiers: Support Vector Machine, N\"aive Bayes,
Random Forest and Logistic Regression. Experimental results show that the
highest performance is achieved with TF-IDF and LR for the English dataset,
with a F1 score of 0.953 and an accuracy of 94.6%, and while for the Spanish
dataset, TF-IDF with NB yields a F1 score of 0.945 and 98.5% accuracy.
Regarding the processing time, TF-IDF with LR leads to the fastest
classification, processing an English and Spanish spam email in and on average,
respectively.
\\ ( https://arxiv.org/abs/2402.05296 ,  2643kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05306
Date: Wed, 7 Feb 2024 22:53:54 GMT   (9294kb,D)

Title: Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making
Authors: Yuan Tian, Wenqi Zhou, Hao Dong, David S. Kammer, Olga Fink
Categories: cs.LG cs.AI
\\
  Symbolic regression holds great potential for uncovering underlying
mathematical and physical relationships from empirical data. While existing
transformer-based models have recently achieved significant success in this
domain, they face challenges in terms of generalizability and adaptability.
Typically, in cases where the output expressions do not adequately fit
experimental data, the models lack efficient mechanisms to adapt or modify the
expression. This inflexibility hinders their application in real-world
scenarios, particularly in discovering unknown physical or biological
relationships. Inspired by how human experts refine and adapt expressions, we
introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based
model that redefines symbolic regression as a sequential decision-making task.
Sym-Q leverages supervised demonstrations and refines expressions based on
reward signals indicating the quality of fitting precision. Its distinctive
ability to manage the complexity of expression trees and perform precise
step-wise updates significantly enhances flexibility and efficiency. Our
results demonstrate that Sym-Q excels not only in recovering underlying
mathematical structures but also uniquely learns to efficiently refine the
output expression based on reward signals, thereby discovering underlying
expressions. Sym-Q paves the way for more intuitive and impactful discoveries
in physical science, marking a substantial advancement in the field of symbolic
regression.
\\ ( https://arxiv.org/abs/2402.05306 ,  9294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05309
Date: Wed, 7 Feb 2024 23:02:53 GMT   (11151kb,D)

Title: Investigating Generalization Behaviours of Generative Flow Networks
Authors: Lazar Atanackovic, Emmanuel Bengio
Categories: cs.LG
\\
  Generative Flow Networks (GFlowNets, GFNs) are a generative framework for
learning unnormalized probability mass functions over discrete spaces. Since
their inception, GFlowNets have proven to be useful for learning generative
models in applications where the majority of the discrete space is unvisited
during training. This has inspired some to hypothesize that GFlowNets, when
paired with deep neural networks (DNNs), have favourable generalization
properties. In this work, we empirically verify some of the hypothesized
mechanisms of generalization of GFlowNets. In particular, we find that the
functions that GFlowNets learn to approximate have an implicit underlying
structure which facilitate generalization. We also find that GFlowNets are
sensitive to being trained offline and off-policy; however, the reward
implicitly learned by GFlowNets is robust to changes in the training
distribution.
\\ ( https://arxiv.org/abs/2402.05309 ,  11151kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05322
Date: Wed, 7 Feb 2024 23:50:00 GMT   (278kb,D)

Title: Learning on Multimodal Graphs: A Survey
Authors: Ciyuan Peng, Jiayuan He and Feng Xia
Categories: cs.LG cs.AI cs.GR cs.SI
Comments: 9 pages, 1 figure
\\
  Multimodal data pervades various domains, including healthcare, social media,
and transportation, where multimodal graphs play a pivotal role. Machine
learning on multimodal graphs, referred to as multimodal graph learning (MGL),
is essential for successful artificial intelligence (AI) applications. The
burgeoning research in this field encompasses diverse graph data types and
modalities, learning techniques, and application scenarios. This survey paper
conducts a comparative analysis of existing works in multimodal graph learning,
elucidating how multimodal learning is achieved across different graph types
and exploring the characteristics of prevalent learning techniques.
Additionally, we delineate significant applications of multimodal graph
learning and offer insights into future directions in this domain.
Consequently, this paper serves as a foundational resource for researchers
seeking to comprehend existing MGL techniques and their applicability across
diverse scenarios.
\\ ( https://arxiv.org/abs/2402.05322 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05353
Date: Thu, 8 Feb 2024 02:21:33 GMT   (13912kb,D)

Title: Revisiting Early-Learning Regularization When Federated Learning Meets
  Noisy Labels
Authors: Taehyeon Kim, Donggyu Kim, Se-Young Yun
Categories: cs.LG cs.DC
\\
  In the evolving landscape of federated learning (FL), addressing label noise
presents unique challenges due to the decentralized and diverse nature of data
collection across clients. Traditional centralized learning approaches to
mitigate label noise are constrained in FL by privacy concerns and the
heterogeneity of client data. This paper revisits early-learning
regularization, introducing an innovative strategy, Federated Label-mixture
Regularization (FLR). FLR adeptly adapts to FL's complexities by generating new
pseudo labels, blending local and global model predictions. This method not
only enhances the accuracy of the global model in both i.i.d. and non-i.i.d.
settings but also effectively counters the memorization of noisy labels.
Demonstrating compatibility with existing label noise and FL techniques, FLR
paves the way for improved generalization in FL environments fraught with label
inaccuracies.
\\ ( https://arxiv.org/abs/2402.05353 ,  13912kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05356
Date: Thu, 8 Feb 2024 02:29:33 GMT   (1217kb,D)

Title: Exploring Learning Complexity for Downstream Data Pruning
Authors: Wenyu Jiang, Zhenlong Liu, Zejian Xie, Songxin Zhang, Bingyi Jing,
  Hongxin Wei
Categories: cs.LG
\\
  The over-parameterized pre-trained models pose a great challenge to
fine-tuning with limited computation resources. An intuitive solution is to
prune the less informative samples from the fine-tuning dataset. A series of
training-based scoring functions are proposed to quantify the informativeness
of the data subset but the pruning cost becomes non-negligible due to the heavy
parameter updating. For efficient pruning, it is viable to adapt the similarity
scoring function of geometric-based methods from training-based to
training-free. However, we empirically show that such adaption distorts the
original pruning and results in inferior performance on the downstream tasks.
In this paper, we propose to treat the learning complexity (LC) as the scoring
function for classification and regression tasks. Specifically, the learning
complexity is defined as the average predicted confidence of subnets with
different capacities, which encapsulates data processing within a converged
model. Then we preserve the diverse and easy samples for fine-tuning. Extensive
experiments with vision datasets demonstrate the effectiveness and efficiency
of the proposed scoring function for classification tasks. For the instruction
fine-tuning of large language models, our method achieves state-of-the-art
performance with stable convergence, outperforming the full training with only
10\% of the instruction dataset.
\\ ( https://arxiv.org/abs/2402.05356 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05367
Date: Thu, 8 Feb 2024 02:57:47 GMT   (105kb)

Title: Principled Preferential Bayesian Optimization
Authors: Wenjie Xu, Wenbin Wang, Yuning Jiang, Bratislav Svetozarevic, Colin N.
  Jones
Categories: cs.LG
\\
  We study the problem of preferential Bayesian optimization (BO), where we aim
to optimize a black-box function with only preference feedback over a pair of
candidate solutions. Inspired by the likelihood ratio idea, we construct a
confidence set of the black-box function using only the preference feedback. An
optimistic algorithm with an efficient computational method is then developed
to solve the problem, which enjoys an information-theoretic bound on the
cumulative regret, a first-of-its-kind for preferential BO. This bound further
allows us to design a scheme to report an estimated best solution, with a
guaranteed convergence rate. Experimental results on sampled instances from
Gaussian processes, standard test functions, and a thermal comfort optimization
problem all show that our method stably achieves better or competitive
performance as compared to the existing state-of-the-art heuristics, which,
however, do not have theoretical guarantees on regret bounds or convergence.
\\ ( https://arxiv.org/abs/2402.05367 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05369
Date: Thu, 8 Feb 2024 02:58:47 GMT   (549kb,D)

Title: Noise Contrastive Alignment of Language Models with Explicit Rewards
Authors: Huayu Chen, Guande He, Hang Su, Jun Zhu
Categories: cs.LG cs.CL
\\
  User intentions are typically formalized as evaluation rewards to be
maximized when fine-tuning language models (LMs). Existing alignment methods,
such as Direct Preference Optimization (DPO), are mainly tailored for pairwise
preference data where rewards are implicitly defined rather than explicitly
given. In this paper, we introduce a general framework for LM alignment,
leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling
reward datasets explicitly annotated with scalar evaluations. Our framework
comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct
extraction of an LM policy from reward data as well as preference data.
Notably, we show that the DPO loss is a special case of our proposed InfoNCA
objective under pairwise preference settings, thereby integrating and extending
current alignment theories. By contrasting NCA and InfoNCA, we show that
InfoNCA and DPO adjust relative likelihood across different responses to a
single instruction, while NCA optimizes absolute likelihood for each response.
We apply our methods to align a 7B language model with a GPT-4 annotated reward
dataset. Experimental results suggest that InfoNCA surpasses the DPO baseline
in GPT-4 evaluations, while NCA enjoys better training stability with
competitive performance.
\\ ( https://arxiv.org/abs/2402.05369 ,  549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05370
Date: Thu, 8 Feb 2024 03:00:50 GMT   (1163kb,D)

Title: Attention as Robust Representation for Time Series Forecasting
Authors: PeiSong Niu, Tian Zhou, Xue Wang, Liang Sun, Rong Jin
Categories: cs.LG cs.AI
\\
  Time series forecasting is essential for many practical applications, with
the adoption of transformer-based models on the rise due to their impressive
performance in NLP and CV. Transformers' key feature, the attention mechanism,
dynamically fusing embeddings to enhance data representation, often relegating
attention weights to a byproduct role. Yet, time series data, characterized by
noise and non-stationarity, poses significant forecasting challenges. Our
approach elevates attention weights as the primary representation for time
series, capitalizing on the temporal relationships among data points to improve
forecasting accuracy. Our study shows that an attention map, structured using
global landmarks and local windows, acts as a robust kernel representation for
data points, withstanding noise and shifts in distribution. Our method
outperforms state-of-the-art models, reducing mean squared error (MSE) in
multivariate time series forecasting by a notable 3.6% without altering the
core neural network architecture. It serves as a versatile component that can
readily replace recent patching based embedding schemes in transformer-based
models, boosting their performance.
\\ ( https://arxiv.org/abs/2402.05370 ,  1163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05379
Date: Thu, 8 Feb 2024 03:29:10 GMT   (188kb,D)

Title: Tradeoffs of Diagonal Fisher Information Matrix Estimators
Authors: Alexander Soen and Ke Sun
Categories: cs.LG stat.ML
\\
  The Fisher information matrix characterizes the local geometry in the
parameter space of neural networks. It elucidates insightful theories and
useful tools to understand and optimize neural networks. Given its high
computational cost, practitioners often use random estimators and evaluate only
the diagonal entries. We examine two such estimators, whose accuracy and sample
complexity depend on their associated variances. We derive bounds of the
variances and instantiate them in regression and classification networks. We
navigate trade-offs of both estimators based on analytical and numerical
studies. We find that the variance quantities depend on the non-linearity with
respect to different parameter groups and should not be neglected when
estimating the Fisher information.
\\ ( https://arxiv.org/abs/2402.05379 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05396
Date: Thu, 8 Feb 2024 04:16:35 GMT   (1130kb,D)

Title: TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph
  Representation Learning
Authors: Gangda Deng, Hongkuan Zhou, Hanqing Zeng, Yinglong Xia, Christopher
  Leung, Jianbo Li, Rajgopal Kannan, Viktor Prasanna
Categories: cs.LG cs.AI
\\
  Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated
state-of-the-art performance in various high-impact applications, including
fraud detection and content recommendation. Despite the success of TGNNs, they
are prone to the prevalent noise found in real-world dynamic graphs like
time-deprecated links and skewed interaction distribution. The noise causes two
critical issues that significantly compromise the accuracy of TGNNs: (1) models
are supervised by inferior interactions, and (2) noisy input induces high
variance in the aggregated messages. However, current TGNN denoising techniques
do not consider the diverse and dynamic noise pattern of each node. In
addition, they also suffer from the excessive mini-batch generation overheads
caused by traversing more neighbors. We believe the remedy for fast and
accurate TGNNs lies in temporal adaptive sampling. In this work, we propose
TASER, the first adaptive sampling method for TGNNs optimized for accuracy,
efficiency, and scalability. TASER adapts its mini-batch selection based on
training dynamics and temporal neighbor selection based on the contextual,
structural, and temporal properties of past interactions. To alleviate the
bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal
neighbor finder and a dedicated GPU feature cache. We evaluate the performance
of TASER using two state-of-the-art backbone TGNNs. On five popular datasets,
TASER outperforms the corresponding baselines by an average of 2.3% in Mean
Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training
time.
\\ ( https://arxiv.org/abs/2402.05396 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05400
Date: Thu, 8 Feb 2024 04:31:21 GMT   (8026kb,D)

Title: Optimizing for ROC Curves on Class-Imbalanced Data by Training over a
  Family of Loss Functions
Authors: Kelsey Lieberman, Shuai Yuan, Swarna Kamlam Ravindran, Carlo Tomasi
Categories: cs.LG cs.CV
\\
  Although binary classification is a well-studied problem in computer vision,
training reliable classifiers under severe class imbalance remains a
challenging problem. Recent work has proposed techniques that mitigate the
effects of training under imbalance by modifying the loss functions or
optimization methods. While this work has led to significant improvements in
the overall accuracy in the multi-class case, we observe that slight changes in
hyperparameter values of these methods can result in highly variable
performance in terms of Receiver Operating Characteristic (ROC) curves on
binary problems with severe imbalance. To reduce the sensitivity to
hyperparameter choices and train more general models, we propose training over
a family of loss functions, instead of a single loss function. We develop a
method for applying Loss Conditional Training (LCT) to an imbalanced
classification problem. Extensive experiment results, on both CIFAR and Kaggle
competition datasets, show that our method improves model performance and is
more robust to hyperparameter choices. Code will be made available at:
https://github.com/klieberman/roc_lct.
\\ ( https://arxiv.org/abs/2402.05400 ,  8026kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05401
Date: Thu, 8 Feb 2024 04:35:09 GMT   (1499kb,D)

Title: Adaptive Activation Functions for Predictive Modeling with Sparse
  Experimental Data
Authors: Farhad Pourkamali-Anaraki, Tahamina Nasrin, Robert E. Jensen, Amy M.
  Peterson, Christopher J. Hansen
Categories: cs.LG cs.NE stat.ML
Comments: 7 figures
\\
  A pivotal aspect in the design of neural networks lies in selecting
activation functions, crucial for introducing nonlinear structures that capture
intricate input-output patterns. While the effectiveness of adaptive or
trainable activation functions has been studied in domains with ample data,
like image classification problems, significant gaps persist in understanding
their influence on classification accuracy and predictive uncertainty in
settings characterized by limited data availability. This research aims to
address these gaps by investigating the use of two types of adaptive activation
functions. These functions incorporate shared and individual trainable
parameters per hidden layer and are examined in three testbeds derived from
additive manufacturing problems containing fewer than one hundred training
instances. Our investigation reveals that adaptive activation functions, such
as Exponential Linear Unit (ELU) and Softplus, with individual trainable
parameters, result in accurate and confident prediction models that outperform
fixed-shape activation functions and the less flexible method of using
identical trainable activation functions in a hidden layer. Therefore, this
work presents an elegant way of facilitating the design of adaptive neural
networks in scientific and engineering problems.
\\ ( https://arxiv.org/abs/2402.05401 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05406
Date: Thu, 8 Feb 2024 04:48:26 GMT   (2072kb,D)

Title: Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
Authors: Lucio Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith,
  Graham Neubig, Ameet Talwalkar
Categories: cs.LG cs.CL
Comments: 15 pages, 4 fiigures, 15 tables
\\
  Given the generational gap in available hardware between lay practitioners
and the most endowed institutions, LLMs are becoming increasingly inaccessible
as they grow in size. Whilst many approaches have been proposed to compress
LLMs to make their resource consumption manageable, these methods themselves
tend to be resource intensive, putting them out of the reach of the very user
groups they target. In this work, we explore the problem of structured pruning
of LLMs using only forward passes. We seek to empower practitioners to prune
models so large that their available hardware has just enough memory to run
inference. We develop Bonsai, a gradient-free, perturbative pruning method
capable of delivering small, fast, and accurate pruned models.
  We observe that Bonsai outputs pruned models that (i) outperform those
generated by more expensive gradient-based structured pruning methods, and (ii)
are twice as fast (with comparable accuracy) as those generated by
semi-structured pruning methods requiring comparable resources as Bonsai. We
also leverage Bonsai to produce a new sub-2B model using a single A6000 that
yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM
leaderboard.
\\ ( https://arxiv.org/abs/2402.05406 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05407
Date: Thu, 8 Feb 2024 04:48:51 GMT   (818kb,D)

Title: Version age-based client scheduling policy for federated learning
Authors: Xinyi Hu, Nikolaos Pappas, Howard H. Yang
Categories: cs.LG cs.DC
Comments: 5 pages, 4 figures, ICASSP 2024
\\
  Federated Learning (FL) has emerged as a privacy-preserving machine learning
paradigm facilitating collaborative training across multiple clients without
sharing local data. Despite advancements in edge device capabilities,
communication bottlenecks present challenges in aggregating a large number of
clients; only a portion of the clients can update their parameters upon each
global aggregation. This phenomenon introduces the critical challenge of
stragglers in FL and the profound impact of client scheduling policies on
global model convergence and stability. Existing scheduling strategies address
staleness but predominantly focus on either timeliness or content. Motivated by
this, we introduce the novel concept of Version Age of Information (VAoI) to
FL. Unlike traditional Age of Information metrics, VAoI considers both
timeliness and content staleness. Each client's version age is updated
discretely, indicating the freshness of information. VAoI is incorporated into
the client scheduling policy to minimize the average VAoI, mitigating the
impact of outdated local updates and enhancing the stability of FL systems.
\\ ( https://arxiv.org/abs/2402.05407 ,  818kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05421
Date: Thu, 8 Feb 2024 05:26:40 GMT   (7566kb,D)

Title: DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement
  and Imitation Learning
Authors: Weikang Wan, Yufei Wang, Zackory Erickson, David Held
Categories: cs.LG cs.AI cs.RO
\\
  This paper introduces DiffTOP, which utilizes Differentiable Trajectory
OPtimization as the policy representation to generate actions for deep
reinforcement and imitation learning. Trajectory optimization is a powerful and
widely used algorithm in control, parameterized by a cost and a dynamics
function. The key to our approach is to leverage the recent progress in
differentiable trajectory optimization, which enables computing the gradients
of the loss with respect to the parameters of trajectory optimization. As a
result, the cost and dynamics functions of trajectory optimization can be
learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior
model-based RL algorithms, as the dynamics model in DiffTOP is learned to
directly maximize task performance by differentiating the policy gradient loss
through the trajectory optimization process. We further benchmark DiffTOP for
imitation learning on standard robotic manipulation task suites with
high-dimensional sensory observations and compare our method to feed-forward
policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15
model-based RL tasks and 13 imitation learning tasks with high-dimensional
image and point cloud inputs, DiffTOP outperforms prior state-of-the-art
methods in both domains.
\\ ( https://arxiv.org/abs/2402.05421 ,  7566kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05424
Date: Thu, 8 Feb 2024 05:42:13 GMT   (9118kb,D)

Title: Neural Circuit Diagrams: Robust Diagrams for the Communication,
  Implementation, and Analysis of Deep Learning Architectures
Authors: Vincent Abbott
Categories: cs.LG
Journal-ref: Transactions on Machine Learning Research (2024)
\\
  Diagrams matter. Unfortunately, the deep learning community has no standard
method for diagramming architectures. The current combination of linear algebra
notation and ad-hoc diagrams fails to offer the necessary precision to
understand architectures in all their detail. However, this detail is critical
for faithful implementation, mathematical analysis, further innovation, and
ethical assurances. I present neural circuit diagrams, a graphical language
tailored to the needs of communicating deep learning architectures. Neural
circuit diagrams naturally keep track of the changing arrangement of data,
precisely show how operations are broadcast over axes, and display the critical
parallel behavior of linear operations. A lingering issue with existing
diagramming methods is the inability to simultaneously express the detail of
axes and the free arrangement of data, which neural circuit diagrams solve.
Their compositional structure is analogous to code, creating a close
correspondence between diagrams and implementation.
  In this work, I introduce neural circuit diagrams for an audience of machine
learning researchers. After introducing neural circuit diagrams, I cover a host
of architectures to show their utility and breed familiarity. This includes the
transformer architecture, convolution (and its difficult-to-explain
extensions), residual networks, the U-Net, and the vision transformer. I
include a Jupyter notebook that provides evidence for the close correspondence
between diagrams and code. Finally, I examine backpropagation using neural
circuit diagrams. I show their utility in providing mathematical insight and
analyzing algorithms' time and space complexities.
\\ ( https://arxiv.org/abs/2402.05424 ,  9118kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05427
Date: Thu, 8 Feb 2024 05:52:45 GMT   (38797kb,D)

Title: A Sampling Theory Perspective on Activations for Implicit Neural
  Representations
Authors: Hemanth Saratchandran, Sameera Ramasinghe, Violetta Shevchenko,
  Alexander Long, Simon Lucey
Categories: cs.LG
\\
  Implicit Neural Representations (INRs) have gained popularity for encoding
signals as compact, differentiable entities. While commonly using techniques
like Fourier positional encodings or non-traditional activation functions
(e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content,
their properties lack exploration within a unified theoretical framework.
Addressing this gap, we conduct a comprehensive analysis of these activations
from a sampling theory perspective. Our investigation reveals that sinc
activations, previously unused in conjunction with INRs, are theoretically
optimal for signal encoding. Additionally, we establish a connection between
dynamical systems and INRs, leveraging sampling theory to bridge these two
paradigms.
\\ ( https://arxiv.org/abs/2402.05427 ,  38797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05428
Date: Thu, 8 Feb 2024 05:54:08 GMT   (5196kb,D)

Title: Mixture Density Networks for Classification with an Application to
  Product Bundling
Authors: Narendhar Gugulothu, Sanjay P. Bhat, Tejas Bodas
Categories: cs.LG cs.AI
\\
  While mixture density networks (MDNs) have been extensively used for
regression tasks, they have not been used much for classification tasks. One
reason for this is that the usability of MDNs for classification is not clear
and straightforward. In this paper, we propose two MDN-based models for
classification tasks. Both models fit mixtures of Gaussians to the the data and
use the fitted distributions to classify a given sample by evaluating the
learnt cumulative distribution function for the given input features. While the
proposed MDN-based models perform slightly better than, or on par with, five
baseline classification models on three publicly available datasets, the real
utility of our models comes out through a real-world product bundling
application. Specifically, we use our MDN-based models to learn the
willingness-to-pay (WTP) distributions for two products from synthetic sales
data of the individual products. The Gaussian mixture representation of the
learnt WTP distributions is then exploited to obtain the WTP distribution of
the bundle consisting of both the products. The proposed MDN-based models are
able to approximate the true WTP distributions of both products and the bundle
well.
\\ ( https://arxiv.org/abs/2402.05428 ,  5196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05439
Date: Thu, 8 Feb 2024 06:32:06 GMT   (9911kb,D)

Title: Learning Uncertainty-Aware Temporally-Extended Actions
Authors: Joongkyu Lee, Seung Joon Park, Yunhao Tang, Min-hwan Oh
Categories: cs.LG stat.ML
Comments: Accepted in AAAI 2024 (Main Technical Track)
\\
  In reinforcement learning, temporal abstraction in the action space,
exemplified by action repetition, is a technique to facilitate policy learning
through extended actions. However, a primary limitation in previous studies of
action repetition is its potential to degrade performance, particularly when
sub-optimal actions are repeated. This issue often negates the advantages of
action repetition. To address this, we propose a novel algorithm named
Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to
accurately measure uncertainty during action extension. This feature allows
policies to strategically choose between emphasizing exploration or adopting an
uncertainty-averse approach, tailored to their specific needs. We demonstrate
the effectiveness of UTE through experiments in Gridworld and Atari 2600
environments. Our findings show that UTE outperforms existing action repetition
algorithms, effectively mitigating their inherent limitations and significantly
enhancing policy learning efficiency.
\\ ( https://arxiv.org/abs/2402.05439 ,  9911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05443
Date: Thu, 8 Feb 2024 06:45:03 GMT   (36817kb,D)

Title: Scalable Wasserstein Gradient Flow for Generative Modeling through
  Unbalanced Optimal Transport
Authors: Jaemoo Choi, Jaewoong Choi, Myungjoo Kang
Categories: cs.LG cs.CV
Comments: 20 pages, 11 figures
\\
  Wasserstein Gradient Flow (WGF) describes the gradient dynamics of
probability density within the Wasserstein space. WGF provides a promising
approach for conducting optimization over the probability distributions.
Numerically approximating the continuous WGF requires the time discretization
method. The most well-known method for this is the JKO scheme. In this regard,
previous WGF models employ the JKO scheme and parametrize transport map for
each JKO step. However, this approach results in quadratic training complexity
$O(K^2)$ with the number of JKO step $K$. This severely limits the scalability
of WGF models. In this paper, we introduce a scalable WGF-based generative
model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form
of the JKO step, derived from the equivalence between the JKO step and the
Unbalanced Optimal Transport. Our approach reduces the training complexity to
$O(K)$. We demonstrate that our model significantly outperforms existing
WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.19
on CelebA-HQ-256, which are comparable to state-of-the-art image generative
models.
\\ ( https://arxiv.org/abs/2402.05443 ,  36817kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05445
Date: Thu, 8 Feb 2024 06:53:31 GMT   (1049kb,D)

Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention
Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda
  Liu, Jie Luo, Xianglong Liu and Michele Magno
Categories: cs.LG cs.CL
\\
  The LoRA-finetuning quantization of LLMs has been extensively studied to
obtain accurate yet compact LLMs for deployment on resource-constrained
hardware. However, existing methods cause the quantized LLM to severely degrade
and even fail to benefit from the finetuning of LoRA. This paper proposes a
novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate
through information retention. The proposed IR-QLoRA mainly relies on two
technologies derived from the perspective of unified information: (1)
statistics-based Information Calibration Quantization allows the quantized
parameters of LLM to retain original information accurately; (2)
finetuning-based Information Elastic Connection makes LoRA utilizes elastic
representation transformation with diverse information. Comprehensive
experiments show that IR-QLoRA can significantly improve accuracy across LLaMA
and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%
improvement on MMLU compared with the state-of-the-art methods. The significant
performance gain requires only a tiny 0.31% additional time consumption,
revealing the satisfactory efficiency of our IRQLoRA. We highlight that
IR-QLoRA enjoys excellent versatility, compatible with various frameworks
(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.
The code is available at https://github.com/htqin/ir-qlora.
\\ ( https://arxiv.org/abs/2402.05445 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05453
Date: Thu, 8 Feb 2024 07:14:17 GMT   (266kb,D)

Title: Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss
Authors: Zhenlong Liu, Lei Feng, Huiping Zhuang, Xiaofeng Cao, Hongxin Wei
Categories: cs.LG cs.CR
\\
  Machine learning models are susceptible to membership inference attacks
(MIAs), which aim to infer whether a sample is in the training set. Existing
work utilizes gradient ascent to enlarge the loss variance of training data,
alleviating the privacy risk. However, optimizing toward a reverse direction
may cause the model parameters to oscillate near local minima, leading to
instability and suboptimal performance. In this work, we propose a novel method
-- Convex-Concave Loss, which enables a high variance of training loss
distribution by gradient descent. Our method is motivated by the theoretical
analysis that convex losses tend to decrease the loss variance during training.
Thus, our key idea behind CCL is to reduce the convexity of loss functions with
a concave term. Trained with CCL, neural networks produce losses with high
variance for training data, reinforcing the defense against MIAs. Extensive
experiments demonstrate the superiority of CCL, achieving state-of-the-art
balance in the privacy-utility trade-off.
\\ ( https://arxiv.org/abs/2402.05453 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05468
Date: Thu, 8 Feb 2024 08:00:11 GMT   (5632kb,D)

Title: Implicit Diffusion: Efficient Optimization through Stochastic Sampling
Authors: Pierre Marion, Anna Korba, Peter Bartlett, Mathieu Blondel, Valentin
  De Bortoli, Arnaud Doucet, Felipe Llinares-L\'opez, Courtney Paquette,
  Quentin Berthet
Categories: cs.LG
Comments: 33 pages, 13 figures
\\
  We present a new algorithm to optimize distributions defined implicitly by
parameterized stochastic diffusions. Doing so allows us to modify the outcome
distribution of sampling processes by optimizing over their parameters. We
introduce a general framework for first-order optimization of these processes,
that performs jointly, in a single loop, optimization and sampling steps. This
approach is inspired by recent advances in bilevel optimization and automatic
implicit differentiation, leveraging the point of view of sampling as
optimization over the space of probability distributions. We provide
theoretical guarantees on the performance of our method, as well as
experimental results demonstrating its effectiveness in real-world settings.
\\ ( https://arxiv.org/abs/2402.05468 ,  5632kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05476
Date: Thu, 8 Feb 2024 08:08:23 GMT   (1173kb,D)

Title: Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy
  Optimization
Authors: Talha Bozkus and Urbashi Mitra
Categories: cs.LG eess.SP
\\
  Reinforcement learning (RL) is a classical tool to solve network control or
policy optimization problems in unknown environments. The original Q-learning
suffers from performance and complexity challenges across very large networks.
Herein, a novel model-free ensemble reinforcement learning algorithm which
adapts the classical Q-learning is proposed to handle these challenges for
networks which admit Markov decision process (MDP) models. Multiple Q-learning
algorithms are run on multiple, distinct, synthetically created and
structurally related Markovian environments in parallel; the outputs are fused
using an adaptive weighting mechanism based on the Jensen-Shannon divergence
(JSD) to obtain an approximately optimal policy with low complexity. The
theoretical justification of the algorithm, including the convergence of key
statistics and Q-functions are provided. Numerical results across several
network models show that the proposed algorithm can achieve up to 55% less
average policy error with up to 50% less runtime complexity than the
state-of-the-art Q-learning algorithms. Numerical results validate assumptions
made in the theoretical analysis.
\\ ( https://arxiv.org/abs/2402.05476 ,  1173kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05491
Date: Thu, 8 Feb 2024 08:55:34 GMT   (1101kb)

Title: Determining the severity of Parkinson's disease in patients using a
  multi task neural network
Authors: Mar\'ia Teresa Garc\'ia-Ord\'as, Jos\'e Alberto Ben\'itez-Andrades,
  Jose Aveleira-Mata, Jos\'e-Manuel Alija-P\'erez and Carmen Benavides
Categories: cs.LG cs.SD eess.AS
Journal-ref: Multimedia Tools and Applications, Volume 83, pages 6077-6092,
  2024
DOI: 10.1007/s11042-023-14932-x
\\
  Parkinson's disease is easy to diagnose when it is advanced, but it is very
difficult to diagnose in its early stages. Early diagnosis is essential to be
able to treat the symptoms. It impacts on daily activities and reduces the
quality of life of both the patients and their families and it is also the
second most prevalent neurodegenerative disorder after Alzheimer in people over
the age of 60. Most current studies on the prediction of Parkinson's severity
are carried out in advanced stages of the disease. In this work, the study
analyzes a set of variables that can be easily extracted from voice analysis,
making it a very non-intrusive technique. In this paper, a method based on
different deep learning techniques is proposed with two purposes. On the one
hand, to find out if a person has severe or non-severe Parkinson's disease, and
on the other hand, to determine by means of regression techniques the degree of
evolution of the disease in a given patient. The UPDRS (Unified Parkinson's
Disease Rating Scale) has been used by taking into account both the motor and
total labels, and the best results have been obtained using a mixed multi-layer
perceptron (MLP) that classifies and regresses at the same time and the most
important features of the data obtained are taken as input, using an
autoencoder. A success rate of 99.15% has been achieved in the problem of
predicting whether a person suffers from severe Parkinson's disease or
non-severe Parkinson's disease. In the degree of disease involvement prediction
problem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a
full deep learning pipeline for data preprocessing and classification has
proven to be very promising in the field Parkinson's outperforming the
state-of-the-art proposals.
\\ ( https://arxiv.org/abs/2402.05491 ,  1101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05495
Date: Thu, 8 Feb 2024 09:07:38 GMT   (1581kb)

Title: Heart disease risk prediction using deep learning techniques with
  feature augmentation
Authors: Mar\'ia Teresa Garc\'ia-Ord\'as, Mart\'in Bay\'on-Guti\'errez, Carmen
  Benavides, Jose Aveleira-Mata and Jos\'e Alberto Ben\'itez-Andrades
Categories: cs.LG
Journal-ref: Multimedia Tools and Applications, Volume 82, pp. 31759 - 31773,
  August 2024
DOI: 10.1007/s11042-023-14817-z
\\
  Cardiovascular diseases state as one of the greatest risks of death for the
general population. Late detection in heart diseases highly conditions the
chances of survival for patients. Age, sex, cholesterol level, sugar level,
heart rate, among other factors, are known to have an influence on
life-threatening heart problems, but, due to the high amount of variables, it
is often difficult for an expert to evaluate each patient taking this
information into account. In this manuscript, the authors propose using deep
learning methods, combined with feature augmentation techniques for evaluating
whether patients are at risk of suffering cardiovascular disease. The results
of the proposed methods outperform other state of the art methods by 4.4%,
leading to a precision of a 90%, which presents a significant improvement, even
more so when it comes to an affliction that affects a large population.
\\ ( https://arxiv.org/abs/2402.05495 ,  1581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05521
Date: Thu, 8 Feb 2024 10:01:29 GMT   (2607kb,D)

Title: Linearizing Models for Efficient yet Robust Private Inference
Authors: Sreetama Sarkar, Souvik Kundu, Peter A. Beerel
Categories: cs.LG cs.AI cs.CR
\\
  The growing concern about data privacy has led to the development of private
inference (PI) frameworks in client-server applications which protects both
data privacy and model IP. However, the cryptographic primitives required yield
significant latency overhead which limits its wide-spread application. At the
same time, changing environments demand the PI service to be robust against
various naturally occurring and gradient-based perturbations. Despite several
works focused on the development of latency-efficient models suitable for PI,
the impact of these models on robustness has remained unexplored. Towards this
goal, this paper presents RLNet, a class of robust linearized networks that can
yield latency improvement via reduction of high-latency ReLU operations while
improving the model performance on both clean and corrupted images. In
particular, RLNet models provide a "triple win ticket" of improved
classification accuracy on clean, naturally perturbed, and gradient-based
perturbed images using a shared-mask shared-weight architecture with over an
order of magnitude fewer ReLUs than baseline models. To demonstrate the
efficacy of RLNet, we perform extensive experiments with ResNet and WRN model
variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Our experimental
evaluations show that RLNet can yield models with up to 11.14x fewer ReLUs,
with accuracy close to the all-ReLU models, on clean, naturally perturbed, and
gradient-based perturbed images. Compared with the SoTA non-robust linearized
models at similar ReLU budgets, RLNet achieves an improvement in adversarial
accuracy of up to ~47%, naturally perturbed accuracy up to ~16.4%, while
improving clean image accuracy up to ~1.5%.
\\ ( https://arxiv.org/abs/2402.05521 ,  2607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05525
Date: Thu, 8 Feb 2024 10:05:11 GMT   (435kb,D)

Title: Differentially Private Model-Based Offline Reinforcement Learning
Authors: Alexandre Rio, Merwan Barlier, Igor Colin, Albert Thomas
Categories: cs.LG cs.AI cs.CR stat.ML
\\
  We address offline reinforcement learning with privacy guarantees, where the
goal is to train a policy that is differentially private with respect to
individual trajectories in the dataset. To achieve this, we introduce DP-MORL,
an MBRL algorithm coming with differential privacy guarantees. A private model
of the environment is first learned from offline data using DP-FedAvg, a
training method for neural networks that provides differential privacy
guarantees at the trajectory level. Then, we use model-based policy
optimization to derive a policy from the (penalized) private model, without any
further interaction with the system or access to the input data. We empirically
show that DP-MORL enables the training of private RL agents from offline data
and we furthermore outline the price of privacy in this setting.
\\ ( https://arxiv.org/abs/2402.05525 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05529
Date: Thu, 8 Feb 2024 10:07:30 GMT   (671kb,D)

Title: Asynchronous Diffusion Learning with Agent Subsampling and Local Updates
Authors: Elsa Rizk, Kun Yuan, Ali H. Sayed
Categories: cs.LG cs.MA
\\
  In this work, we examine a network of agents operating asynchronously, aiming
to discover an ideal global model that suits individual local datasets. Our
assumption is that each agent independently chooses when to participate
throughout the algorithm and the specific subset of its neighbourhood with
which it will cooperate at any given moment. When an agent chooses to take
part, it undergoes multiple local updates before conveying its outcomes to the
sub-sampled neighbourhood. Under this setup, we prove that the resulting
asynchronous diffusion strategy is stable in the mean-square error sense and
provide performance guarantees specifically for the federated learning setting.
We illustrate the findings with numerical simulations.
\\ ( https://arxiv.org/abs/2402.05529 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05536
Date: Thu, 8 Feb 2024 10:15:41 GMT   (1003kb)

Title: Empowering machine learning models with contextual knowledge for
  enhancing the detection of eating disorders in social media posts
Authors: Jos\'e Alberto Ben\'itez-Andrades, Mar\'ia Teresa Garc\'ia-Ord\'as,
  Mayra Russo, Ahmad Sakor, Luis Daniel Fernandes Rotger and Maria-Esther Vidal
Categories: cs.LG cs.CL
Journal-ref: Semantic Web, Volume 4, Issue 5, pp. 873-892, 2023
DOI: 10.3233/SW-223269
\\
  Social networks are vital for information sharing, especially in the health
sector for discussing diseases and treatments. These platforms, however, often
feature posts as brief texts, posing challenges for Artificial Intelligence
(AI) in understanding context. We introduce a novel hybrid approach combining
community-maintained knowledge graphs (like Wikidata) with deep learning to
enhance the categorization of social media posts. This method uses advanced
entity recognizers and linkers (like Falcon 2.0) to connect short post entities
to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word
embeddings (like BERT) are then employed to create rich, context-based
representations of these posts.
  Our focus is on the health domain, particularly in identifying posts related
to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in
early diagnosis. We tested our approach on a dataset of 2,000 tweets about
eating disorders, finding that merging word embeddings with knowledge graph
information enhances the predictive models' reliability. This methodology aims
to assist health experts in spotting patterns indicative of mental disorders,
thereby improving early detection and accurate diagnosis for personalized
medicine.
\\ ( https://arxiv.org/abs/2402.05536 ,  1003kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05541
Date: Thu, 8 Feb 2024 10:22:12 GMT   (977kb,D)

Title: Reinforcement Learning as a Catalyst for Robust and Fair Federated
  Learning: Deciphering the Dynamics of Client Contributions
Authors: Jialuo He, Wei Chen, Xiaojin Zhang
Categories: cs.LG cs.AI cs.DC
\\
  Recent advancements in federated learning (FL) have produced models that
retain user privacy by training across multiple decentralized devices or
systems holding local data samples. However, these strategies often neglect the
inherent challenges of statistical heterogeneity and vulnerability to
adversarial attacks, which can degrade model robustness and fairness.
Personalized FL strategies offer some respite by adjusting models to fit
individual client profiles, yet they tend to neglect server-side aggregation
vulnerabilities. To address these issues, we propose Reinforcement Federated
Learning (RFL), a novel framework that leverages deep reinforcement learning to
adaptively optimize client contribution during aggregation, thereby enhancing
both model robustness against malicious clients and fairness across
participants under non-identically distributed settings. To achieve this goal,
we propose a meticulous approach involving a Deep Deterministic Policy
Gradient-based algorithm for continuous control of aggregation weights, an
innovative client selection method based on model parameter distances, and a
reward mechanism guided by validation set performance. Empirically, extensive
experiments demonstrate that, in terms of robustness, RFL outperforms the
state-of-the-art methods, while maintaining comparable levels of fairness,
offering a promising solution to build resilient and fair federated systems.
\\ ( https://arxiv.org/abs/2402.05541 ,  977kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05546
Date: Thu, 8 Feb 2024 10:29:46 GMT   (17222kb,D)

Title: Offline Actor-Critic Reinforcement Learning Scales to Large Models
Authors: Jost Tobias Springenberg, Abbas Abdolmaleki, Jingwei Zhang, Oliver
  Groth, Michael Bloesch, Thomas Lampe, Philemon Brakel, Sarah Bechtle, Steven
  Kapturowski, Roland Hafner, Nicolas Heess, Martin Riedmiller
Categories: cs.LG cs.AI cs.RO
\\
  We show that offline actor-critic reinforcement learning can scale to large
models - such as transformers - and follows similar scaling laws as supervised
learning. We find that offline actor-critic algorithms can outperform strong,
supervised, behavioral cloning baselines for multi-task training on a large
dataset containing both sub-optimal and expert behavior on 132 continuous
control tasks. We introduce a Perceiver-based actor-critic model and elucidate
the key model features needed to make offline RL work with self- and
cross-attention modules. Overall, we find that: i) simple offline actor critic
algorithms are a natural choice for gradually moving away from the currently
predominant paradigm of behavioral cloning, and ii) via offline RL it is
possible to learn multi-task policies that master many domains simultaneously,
including real robotics tasks, from sub-optimal demonstrations or
self-generated data.
\\ ( https://arxiv.org/abs/2402.05546 ,  17222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05558
Date: Thu, 8 Feb 2024 10:52:37 GMT   (7068kb,D)

Title: Flashback: Understanding and Mitigating Forgetting in Federated Learning
Authors: Mohammed Aljahdali, Ahmed M. Abdelmoniem, Marco Canini, Samuel
  Horv\'ath
Categories: cs.LG cs.AI cs.CV cs.DC
\\
  In Federated Learning (FL), forgetting, or the loss of knowledge across
rounds, hampers algorithm convergence, particularly in the presence of severe
data heterogeneity among clients. This study explores the nuances of this
issue, emphasizing the critical role of forgetting in FL's inefficient learning
within heterogeneous data contexts. Knowledge loss occurs in both client-local
updates and server-side aggregation steps; addressing one without the other
fails to mitigate forgetting. We introduce a metric to measure forgetting
granularly, ensuring distinct recognition amid new knowledge acquisition.
Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic
distillation approach that is used to regularize the local models, and
effectively aggregate their knowledge. Across different benchmarks, Flashback
outperforms other methods, mitigates forgetting, and achieves faster
round-to-target-accuracy, by converging in 6 to 16 rounds.
\\ ( https://arxiv.org/abs/2402.05558 ,  7068kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05566
Date: Thu, 8 Feb 2024 11:04:11 GMT   (139kb,D)

Title: Succint Interaction-Aware Explanations
Authors: Sascha Xu, Joscha C\"uppers, Jilles Vreeken
Categories: cs.LG
\\
  SHAP is a popular approach to explain black-box models by revealing the
importance of individual features. As it ignores feature interactions, SHAP
explanations can be confusing up to misleading. NSHAP, on the other hand,
reports the additive importance for all subsets of features. While this does
include all interacting sets of features, it also leads to an exponentially
sized, difficult to interpret explanation. In this paper, we propose to combine
the best of these two worlds, by partitioning the features into parts that
significantly interact, and use these parts to compose a succinct,
interpretable, additive explanation. We derive a criterion by which to measure
the representativeness of such a partition for a models behavior, traded off
against the complexity of the resulting explanation. To efficiently find the
best partition out of super-exponentially many, we show how to prune
sub-optimal solutions using a statistical test, which not only improves runtime
but also helps to detect spurious interactions. Experiments on synthetic and
real world data show that our explanations are both more accurate resp. more
easily interpretable than those of SHAP and NSHAP.
\\ ( https://arxiv.org/abs/2402.05566 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05569
Date: Thu, 8 Feb 2024 11:10:39 GMT   (668kb,D)

Title: Hypergraph Node Classification With Graph Neural Networks
Authors: Bohan Tang, Zexi Liu, Keyue Jiang, Siheng Chen, Xiaowen Dong
Categories: cs.LG cs.AI eess.SP stat.ML
\\
  Hypergraphs, with hyperedges connecting more than two nodes, are key for
modelling higher-order interactions in real-world data. The success of graph
neural networks (GNNs) reveals the capability of neural networks to process
data with pairwise interactions. This inspires the usage of neural networks for
data with higher-order interactions, thereby leading to the development of
hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically
considered distinct since they are designed for data on different geometric
topologies. However, in this paper, we theoretically demonstrate that, in the
context of node classification, most HyperGNNs can be approximated using a GNN
with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a
simple and efficient framework comprising a GNN and a weighted clique expansion
(WCE), for hypergraph node classification. Experiments on nine real-world
hypergraph node classification benchmarks showcase that WCE-GNN demonstrates
not only higher classification accuracy compared to state-of-the-art HyperGNNs,
but also superior memory and runtime efficiency.
\\ ( https://arxiv.org/abs/2402.05569 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05575
Date: Thu, 8 Feb 2024 11:19:58 GMT   (1224kb,D)

Title: Simultaneously Achieving Group Exposure Fairness and Within-Group
  Meritocracy in Stochastic Bandits
Authors: Subham Pokhriyal, Shweta Jain, Ganesh Ghalme, Swapnil Dhamal and Sujit
  Gujar
Categories: cs.LG cs.AI cs.CY cs.MA
Comments: Accepted in AAMAS 2024
\\
  Existing approaches to fairness in stochastic multi-armed bandits (MAB)
primarily focus on exposure guarantee to individual arms. When arms are
naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which
considers two levels of fairness. At the first level, Bi-Level Fairness
guarantees a certain minimum exposure to each group. To address the unbalanced
allocation of pulls to individual arms within a group, we consider meritocratic
fairness at the second level, which ensures that each arm is pulled according
to its merit within the group. Our work shows that we can adapt a UCB-based
algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group
Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic
Fairness within each group. We first show that one can decompose regret bounds
into two components: (a) regret due to anytime group exposure fairness and (b)
regret due to meritocratic fairness within each group. Our proposed algorithm
BF-UCB balances these two regrets optimally to achieve the upper bound of
$O(\sqrt{T})$ on regret; $T$ being the stopping time. With the help of
simulated experiments, we further show that BF-UCB achieves sub-linear regret;
provides better group and individual exposure guarantees compared to existing
algorithms; and does not result in a significant drop in reward with respect to
UCB algorithm, which does not impose any fairness constraint.
\\ ( https://arxiv.org/abs/2402.05575 ,  1224kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05576
Date: Thu, 8 Feb 2024 11:23:11 GMT   (854kb,D)

Title: Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via
  Finite Geometry
Authors: Anastasis Kratsios, A. Martina Neuman, Gudmund Pammer
Categories: cs.LG
\\
  Many of the foundations of machine learning rely on the idealized premise
that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core
assumption is systematically violated in practice due to digital computing
limitations from finite machine precision, rounding, and limited RAM. In short,
digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting
these discrete structures, we show the curse of dimensionality in statistical
learning is systematically broken when models are implemented on real
computers. Consequentially, we obtain new generalization bounds with
dimension-free rates for kernel and deep ReLU MLP regressors, which are
implemented on real-world machines.
  Our results are derived using a new non-asymptotic concentration of measure
result between a probability measure over any finite metric space and its
empirical version associated with $N$ i.i.d. samples when measured in the
$1$-Wasserstein distance. Unlike standard concentration of measure results, the
concentration rates in our bounds do not hold uniformly for all sample sizes
$N$; instead, our rates can adapt to any given $N$. This yields significantly
tighter bounds for realistic sample sizes while achieving the optimal
worst-case rate of $\mathcal{O}(1/N^{1/2})$ for massive. Our results are built
on new techniques combining metric embedding theory with optimal transport
\\ ( https://arxiv.org/abs/2402.05576 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05626
Date: Thu, 8 Feb 2024 12:30:29 GMT   (3352kb,D)

Title: The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary
  Points, Saddle Escaping, and Network Embedding
Authors: Zhengqing Wu, Berfin Simsek, Francois Ged
Categories: cs.LG
\\
  In this paper, we investigate the loss landscape of one-hidden-layer neural
networks with ReLU-like activation functions trained with the empirical squared
loss. As the activation function is non-differentiable, it is so far unclear
how to completely characterize the stationary points. We propose the conditions
for stationarity that apply to both non-differentiable and differentiable
cases. Additionally, we show that, if a stationary point does not contain
"escape neurons", which are defined with first-order conditions, then it must
be a local minimum. Moreover, for the scalar-output case, the presence of an
escape neuron guarantees that the stationary point is not a local minimum. Our
results refine the description of the saddle-to-saddle training process
starting from infinitesimally small (vanishing) initialization for shallow
ReLU-like networks, linking saddle escaping directly with the parameter changes
of escape neurons. Moreover, we are also able to fully discuss how network
embedding, which is to instantiate a narrower network within a wider network,
reshapes the stationary points.
\\ ( https://arxiv.org/abs/2402.05626 ,  3352kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05627
Date: Thu, 8 Feb 2024 12:31:08 GMT   (112kb,D)

Title: Binding Dynamics in Rotating Features
Authors: Sindy L\"owe, Francesco Locatello, Max Welling
Categories: cs.LG cs.AI cs.CV q-bio.NC
\\
  In human cognition, the binding problem describes the open question of how
the brain flexibly integrates diverse information into cohesive object
representations. Analogously, in machine learning, there is a pursuit for
models capable of strong generalization and reasoning by learning
object-centric representations in an unsupervised manner. Drawing from
neuroscientific theories, Rotating Features learn such representations by
introducing vector-valued features that encapsulate object characteristics in
their magnitudes and object affiliation in their orientations. The
"$\chi$-binding" mechanism, embedded in every layer of the architecture, has
been shown to be crucial, but remains poorly understood. In this paper, we
propose an alternative "cosine binding" mechanism, which explicitly computes
the alignment between features and adjusts weights accordingly, and we show
that it achieves equivalent performance. This allows us to draw direct
connections to self-attention and biological neural processes, and to shed
light on the fundamental dynamics for object-centric representations to emerge
in Rotating Features.
\\ ( https://arxiv.org/abs/2402.05627 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05628
Date: Thu, 8 Feb 2024 12:35:41 GMT   (2262kb,D)

Title: RepQuant: Towards Accurate Post-Training Quantization of Large
  Transformer Models via Scale Reparameterization
Authors: Zhikai Li, Xuewen Liu, Jing Zhang, and Qingyi Gu
Categories: cs.LG
\\
  Large transformer models have demonstrated remarkable success. Post-training
quantization (PTQ), which requires only a small dataset for calibration and
avoids end-to-end retraining, is a promising solution for compressing these
large models. Regrettably, existing PTQ methods typically exhibit non-trivial
performance loss. We find that the performance bottleneck stems from
over-consideration of hardware compatibility in the quantization process,
compelling them to reluctantly employ simple quantizers, albeit at the expense
of accuracy. With the above insights, we propose RepQuant, a novel PTQ
framework with quantization-inference decoupling paradigm to address the above
issues. RepQuant employs complex quantizers in the quantization process and
simplified quantizers in the inference process, and performs mathematically
equivalent transformations between the two through quantization scale
reparameterization, thus ensuring both accurate quantization and efficient
inference. More specifically, we focus on two components with extreme
distributions: LayerNorm activations and Softmax activations. Initially, we
apply channel-wise quantization and log$\sqrt{2}$ quantization, respectively,
which are tailored to their distributions. In particular, for the former, we
introduce a learnable per-channel dual clipping scheme, which is designed to
efficiently identify outliers in the unbalanced activations with fine
granularity. Then, we reparameterize the scales to hardware-friendly layer-wise
quantization and log2 quantization for inference. Moreover, quantized weight
reconstruction is seamlessly integrated into the above procedure to further
push the performance limits. Extensive experiments are performed on different
large-scale transformer variants on multiple tasks, including vision, language,
and multi-modal transformers, and RepQuant encouragingly demonstrates
significant performance advantages.
\\ ( https://arxiv.org/abs/2402.05628 ,  2262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05643
Date: Thu, 8 Feb 2024 12:58:07 GMT   (2506kb,D)

Title: Improving Token-Based World Models with Parallel Observation Prediction
Authors: Lior Cohen, Kaixin Wang, Bingyi Kang, Shie Mannor
Categories: cs.LG cs.AI
\\
  Motivated by the success of Transformers when applied to sequences of
discrete symbols, token-based world models (TBWMs) were recently proposed as
sample-efficient methods. In TBWMs, the world model consumes agent experience
as a language-like sequence of tokens, where each observation constitutes a
sub-sequence. However, during imagination, the sequential token-by-token
generation of next observations results in a severe bottleneck, leading to long
training times, poor GPU utilization, and limited representations. To resolve
this bottleneck, we devise a novel Parallel Observation Prediction (POP)
mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode
tailored to our reinforcement learning setting. We incorporate POP in a novel
TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster
imagination compared to prior TBWMs. REM attains superhuman performance on 12
out of 26 games of the Atari 100K benchmark, while training in less than 12
hours. Our code is available at \url{https://github.com/leor-c/REM}.
\\ ( https://arxiv.org/abs/2402.05643 ,  2506kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05660
Date: Thu, 8 Feb 2024 13:24:57 GMT   (414kb,D)

Title: Rethinking Propagation for Unsupervised Graph Domain Adaptation
Authors: Meihan Liu, Zeyu Fang, Zhen Zhang, Ming Gu, Sheng Zhou, Xin Wang,
  Jiajun Bu
Categories: cs.LG cs.AI
Comments: Accepted by AAAI-24
\\
  Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a
labelled source graph to an unlabelled target graph in order to address the
distribution shifts between graph domains. Previous works have primarily
focused on aligning data from the source and target graph in the representation
space learned by graph neural networks (GNNs). However, the inherent
generalization capability of GNNs has been largely overlooked. Motivated by our
empirical analysis, we reevaluate the role of GNNs in graph domain adaptation
and uncover the pivotal role of the propagation process in GNNs for adapting to
different graph domains. We provide a comprehensive theoretical analysis of
UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN
Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter
by removing propagation layers in source graph and stacking multiple
propagation layers in target graph. Based on the empirical and theoretical
analysis mentioned above, we propose a simple yet effective approach called
A2GNN for graph domain adaptation. Through extensive experiments on real-world
datasets, we demonstrate the effectiveness of our proposed A2GNN framework.
\\ ( https://arxiv.org/abs/2402.05660 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05663
Date: Thu, 8 Feb 2024 13:27:10 GMT   (15849kb,D)

Title: Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave
  Prediction
Authors: Raphael Chekroun, Han Wang, Jonathan Lee, Marin Toromanoff, Sascha
  Hornauer, Fabien Moutarde, Maria Laura Delle Monache
Categories: cs.LG cs.AI cs.RO
\\
  Accurate real-time traffic state forecasting plays a pivotal role in traffic
control research. In particular, the CIRCLES consortium project necessitates
predictive techniques to mitigate the impact of data source delays. After the
success of the MegaVanderTest experiment, this paper aims at overcoming the
current system limitations and develop a more suited approach to improve the
real-time traffic state estimation for the next iterations of the experiment.
In this paper, we introduce the SA-LSTM, a deep forecasting method integrating
Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM)
yielding state-of-the-art results in real-time mesoscale traffic forecasting.
We extend this approach to multi-step forecasting with the n-step SA-LSTM,
which outperforms traditional multi-step forecasting methods in the trade-off
between short-term and long-term predictions, all while operating in real-time.
\\ ( https://arxiv.org/abs/2402.05663 ,  15849kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05667
Date: Thu, 8 Feb 2024 13:38:23 GMT   (4917kb,D)

Title: S$\Omega$I: Score-based O-INFORMATION Estimation
Authors: Mustapha Bounoua, Giulio Franzese, Pietro Michiardi
Categories: cs.LG cs.IT math.IT
\\
  The analysis of scientific data and complex multivariate systems requires
information quantities that capture relationships among multiple random
variables. Recently, new information-theoretic measures have been developed to
overcome the shortcomings of classical ones, such as mutual information, that
are restricted to considering pairwise interactions. Among them, the concept of
information synergy and redundancy is crucial for understanding the high-order
dependencies between variables. One of the most prominent and versatile
measures based on this concept is O-information, which provides a clear and
scalable way to quantify the synergy-redundancy balance in multivariate
systems. However, its practical application is limited to simplified cases. In
this work, we introduce S$\Omega$I, which allows for the first time to compute
O-information without restrictive assumptions about the system. Our experiments
validate our approach on synthetic data, and demonstrate the effectiveness of
S$\Omega$I in the context of a real-world use case.
\\ ( https://arxiv.org/abs/2402.05667 ,  4917kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05675
Date: Thu, 8 Feb 2024 13:53:11 GMT   (1566kb,D)

Title: Is Adversarial Training with Compressed Datasets Effective?
Authors: Tong Chen, Raghavendra Selvan
Categories: cs.LG
Comments: 20 pages, 14 figures, 3 tables
\\
  Dataset Condensation (DC) refers to the recent class of dataset compression
methods that generate a smaller, synthetic, dataset from a larger dataset. This
synthetic dataset retains the essential information of the original dataset,
enabling models trained on it to achieve performance levels comparable to those
trained on the full dataset. Most current DC methods have mainly concerned with
achieving high test performance with limited data budget, and have not directly
addressed the question of adversarial robustness. In this work, we investigate
the impact of adversarial robustness on models trained with compressed
datasets. We show that the compressed datasets obtained from DC methods are not
effective in transferring adversarial robustness to models. As a solution to
improve dataset compression efficiency and adversarial robustness
simultaneously, we propose a novel robustness-aware dataset compression method
based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed
method is (1) obtained by one-time computation and is applicable for any model,
(2) more effective than DC methods when applying adversarial training over MFC,
(3) provably robust by minimizing the generalized adversarial loss.
Additionally, empirical evaluation on three datasets shows that the proposed
method is able to achieve better robustness and performance trade-off compared
to DC methods such as distribution matching.
\\ ( https://arxiv.org/abs/2402.05675 ,  1566kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05680
Date: Thu, 8 Feb 2024 13:58:16 GMT   (26kb)

Title: Interpretable classifiers for tabular data via discretization and
  feature selection
Authors: Reijo Jaakkola, Tomi Janhunen, Antti Kuusisto, Masood Feyzbakhsh
  Rankooh, Miikka Vilander
Categories: cs.LG cs.AI cs.LO
ACM-class: I.2.6; F.4.1; I.2.4; E.2
\\
  We introduce a method for computing immediately human interpretable yet
accurate classifiers from tabular data. The classifiers obtained are short
DNF-formulas, computed via first discretizing the original data to Boolean form
and then using feature selection coupled with a very fast algorithm for
producing the best possible Boolean classifier for the setting. We demonstrate
the approach via 14 experiments, obtaining results with accuracies mainly
similar to ones obtained via random forests, XGBoost, and existing results for
the same datasets in the literature. In several cases, our approach in fact
outperforms the reference results in relation to accuracy, even though the main
objective of our study is the immediate interpretability of our classifiers. We
also prove a new result on the probability that the classifier we obtain from
real-life data corresponds to the ideally best classifier with respect to the
background distribution the data comes from.
\\ ( https://arxiv.org/abs/2402.05680 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05689
Date: Thu, 8 Feb 2024 14:07:20 GMT   (4081kb,D)

Title: Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of
  Average-Reward Restless Bandits
Authors: Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang
Categories: cs.LG math.OC math.PR
Comments: 41 pages, 3 figures
MSC-class: 90C40
ACM-class: G.3; I.6
\\
  We consider the infinite-horizon, average-reward restless bandit problem in
discrete time. We propose a new class of policies that are designed to drive a
progressively larger subset of arms toward the optimal distribution. We show
that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality
gap for an $N$-armed problem, provided that the single-armed relaxed problem is
unichain and aperiodic. Our approach departs from most existing work that
focuses on index or priority policies, which rely on the Uniform Global
Attractor Property (UGAP) to guarantee convergence to the optimum, or a
recently developed simulation-based policy, which requires a Synchronization
Assumption (SA).
\\ ( https://arxiv.org/abs/2402.05689 ,  4081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05713
Date: Thu, 8 Feb 2024 14:40:32 GMT   (10340kb,D)

Title: Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on
  Vulnerable Patient Populations
Authors: Pranav Kulkarni, Andrew Chan, Nithya Navarathna, Skylar Chan, Paul H.
  Yi, Vishwa S. Parekh
Categories: cs.LG cs.AI cs.CV
Comments: 26 pages, 20 figures, 1 table
\\
  The proliferation of artificial intelligence (AI) in radiology has shed light
on the risk of deep learning (DL) models exacerbating clinical biases towards
vulnerable patient populations. While prior literature has focused on
quantifying biases exhibited by trained DL models, demographically targeted
adversarial bias attacks on DL models and its implication in the clinical
environment remains an underexplored field of research in medical imaging. In
this work, we demonstrate that demographically targeted label poisoning attacks
can introduce adversarial underdiagnosis bias in DL models and degrade
performance on underrepresented groups without impacting overall model
performance. Moreover, our results across multiple performance metrics and
demographic groups like sex, age, and their intersectional subgroups indicate
that a group's vulnerability to undetectable adversarial bias attacks is
directly correlated with its representation in the model's training data.
\\ ( https://arxiv.org/abs/2402.05713 ,  10340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05723
Date: Thu, 8 Feb 2024 14:54:17 GMT   (1587kb,D)

Title: In-Context Learning Can Re-learn Forbidden Tasks
Authors: Sophie Xhonneux, David Dobre, Jian Tang, Gauthier Gidel, Dhanya
  Sridhar
Categories: cs.LG cs.CR
Comments: 19 pages, 7 figures
\\
  Despite significant investment into safety training, large language models
(LLMs) deployed in the real world still suffer from numerous vulnerabilities.
One perspective on LLM safety training is that it algorithmically forbids the
model from answering toxic or harmful queries. To assess the effectiveness of
safety training, in this work, we study forbidden tasks, i.e., tasks the model
is designed to refuse to answer. Specifically, we investigate whether
in-context learning (ICL) can be used to re-learn forbidden tasks despite the
explicit fine-tuning of the model to refuse them. We first examine a toy
example of refusing sentiment classification to demonstrate the problem. Then,
we use ICL on a model fine-tuned to refuse to summarise made-up news articles.
Finally, we investigate whether ICL can undo safety training, which could
represent a major security risk. For the safety task, we look at Vicuna-7B,
Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on
Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL
attack that uses the chat template tokens like a prompt injection attack to
achieve a better attack success rate on Vicuna-7B and Starling-7B.
  Trigger Warning: the appendix contains LLM-generated text with violence,
suicide, and misinformation.
\\ ( https://arxiv.org/abs/2402.05723 ,  1587kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05724
Date: Thu, 8 Feb 2024 14:54:47 GMT   (225kb,D)

Title: Model-Based RL for Mean-Field Games is not Statistically Harder than
  Single-Agent RL
Authors: Jiawei Huang, Niao He, Andreas Krause
Categories: cs.LG cs.AI cs.GT stat.ML
Comments: 49 Pages
\\
  We study the sample complexity of reinforcement learning (RL) in Mean-Field
Games (MFGs) with model-based function approximation that requires strategic
exploration to find a Nash Equilibrium policy. We introduce the Partial
Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize
the model class complexity. Notably, P-MBED measures the complexity of the
single-agent model class converted from the given mean-field model class, and
potentially, can be exponentially lower than the MBED proposed by
\citet{huang2023statistical}. We contribute a model elimination algorithm
featuring a novel exploration strategy and establish sample complexity results
polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic
realizability and Lipschitz continuity assumptions, \emph{learning Nash
Equilibrium in MFGs is no more statistically challenging than solving a
logarithmic number of single-agent RL problems}. We further extend our results
to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple
types of agents. This extension implies statistical tractability of a broader
class of Markov Games through the efficacy of mean-field approximation.
Finally, inspired by our theoretical algorithm, we present a heuristic approach
with improved computational efficiency and empirically demonstrate its
effectiveness.
\\ ( https://arxiv.org/abs/2402.05724 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05738
Date: Thu, 8 Feb 2024 15:15:09 GMT   (2652kb,D)

Title: Implicit Bias and Fast Convergence Rates for Self-attention
Authors: Bhavya Vasudeva, Puneesh Deora, Christos Thrampoulidis
Categories: cs.LG math.OC stat.ML
Comments: 41 pages, 7 figures
\\
  Self-attention, the core mechanism of transformers, distinguishes them from
traditional neural networks and drives their outstanding performance. Towards
developing the fundamental optimization principles of self-attention, we
investigate the implicit bias of gradient descent (GD) in training a
self-attention layer with fixed linear decoder in binary classification.
Drawing inspiration from the study of GD in linear logistic regression over
separable data, recent work demonstrates that as the number of iterations $t$
approaches infinity, the key-query matrix $W_t$ converges locally (with respect
to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our
work enhances this result in four aspects. Firstly, we identify non-trivial
data settings for which convergence is provably global, thus shedding light on
the optimization landscape. Secondly, we provide the first finite-time
convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of
sparsification in the attention map. Thirdly, through an analysis of normalized
GD and Polyak step-size, we demonstrate analytically that adaptive step-size
rules can accelerate the convergence of self-attention. Additionally, we remove
the restriction of prior work on a fixed linear decoder. Our results reinforce
the implicit-bias perspective of self-attention and strengthen its connections
to implicit-bias in linear logistic regression, despite the intricate
non-convex nature of the former.
\\ ( https://arxiv.org/abs/2402.05738 ,  2652kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05749
Date: Thu, 8 Feb 2024 15:33:09 GMT   (1028kb,D)

Title: Generalized Preference Optimization: A Unified Approach to Offline
  Alignment
Authors: Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello,
  R\'emi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo
  \'Avila Pires, Bilal Piot
Categories: cs.LG cs.AI
\\
  Offline preference optimization allows fine-tuning large models directly from
offline data, and has proved effective in recent alignment practices. We
propose generalized preference optimization (GPO), a family of offline losses
parameterized by a general class of convex functions. GPO enables a unified
view over preference optimization, encompassing existing algorithms such as
DPO, IPO and SLiC as special cases, while naturally introducing new variants.
The GPO framework also sheds light on how offline algorithms enforce
regularization, through the design of the convex function that defines the
loss. Our analysis and experiments reveal the connections and subtle
differences between the offline regularization and the KL divergence
regularization intended by the canonical RLHF formulation. In all, our results
present new algorithmic toolkits and empirical insights to alignment
practitioners.
\\ ( https://arxiv.org/abs/2402.05749 ,  1028kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05758
Date: Thu, 8 Feb 2024 15:41:48 GMT   (579kb,D)

Title: Latent variable model for high-dimensional point process with structured
  missingness
Authors: Maksim Sinelnikov, Manuel Haussmann and Harri L\"ahdesm\"aki
Categories: cs.LG stat.ML
\\
  Longitudinal data are important in numerous fields, such as healthcare,
sociology and seismology, but real-world datasets present notable challenges
for practitioners because they can be high-dimensional, contain structured
missingness patterns, and measurement time points can be governed by an unknown
stochastic process. While various solutions have been suggested, the majority
of them have been designed to account for only one of these challenges. In this
work, we propose a flexible and efficient latent-variable model that is capable
of addressing all these limitations. Our approach utilizes Gaussian processes
to capture temporal correlations between samples and their associated
missingness masks as well as to model the underlying point process. We
construct our model as a variational autoencoder together with deep neural
network parameterised encoder and decoder models, and develop a scalable
amortised variational inference approach for efficient model training. We
demonstrate competitive performance using both simulated and real datasets.
\\ ( https://arxiv.org/abs/2402.05758 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05766
Date: Thu, 8 Feb 2024 15:51:50 GMT   (141kb,D)

Title: Off-policy Distributional Q($\lambda$): Distributional RL without
  Importance Sampling
Authors: Yunhao Tang, Mark Rowland, R\'emi Munos, Bernardo \'Avila Pires, Will
  Dabney
Categories: cs.LG
\\
  We introduce off-policy distributional Q($\lambda$), a new addition to the
family of off-policy distributional evaluation algorithms. Off-policy
distributional Q($\lambda$) does not apply importance sampling for off-policy
learning, which introduces intriguing interactions with signed measures. Such
unique properties distributional Q($\lambda$) from other existing alternatives
such as distributional Retrace. We characterize the algorithmic properties of
distributional Q($\lambda$) and validate theoretical insights with tabular
experiments. We show how distributional Q($\lambda$)-C51, a combination of
Q($\lambda$) with the C51 agent, exhibits promising results on deep RL
benchmarks.
\\ ( https://arxiv.org/abs/2402.05766 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05774
Date: Thu, 8 Feb 2024 16:01:24 GMT   (31943kb,D)

Title: Stable Autonomous Flow Matching
Authors: Christopher Iliffe Sprague, Arne Elofsson, Hossein Azizpour
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: In submission
\\
  In contexts where data samples represent a physically stable state, it is
often assumed that the data points represent the local minima of an energy
landscape. In control theory, it is well-known that energy can serve as an
effective Lyapunov function. Despite this, connections between control theory
and generative models in the literature are sparse, even though there are
several machine learning applications with physically stable data points. In
this paper, we focus on such data and a recent class of deep generative models
called flow matching. We apply tools of stochastic stability for
time-independent systems to flow matching models. In doing so, we characterize
the space of flow matching models that are amenable to this treatment, as well
as draw connections to other control theory principles. We demonstrate our
theoretical results on two examples.
\\ ( https://arxiv.org/abs/2402.05774 ,  31943kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05782
Date: Thu, 8 Feb 2024 16:17:18 GMT   (3558kb,D)

Title: Analysing the Sample Complexity of Opponent Shaping
Authors: Kitty Fung, Qizhen Zhang, Chris Lu, Jia Wan, Timon Willi, Jakob
  Foerster
Categories: cs.LG cs.AI cs.GT cs.MA
Journal-ref: AAMAS 2024
\\
  Learning in general-sum games often yields collectively sub-optimal results.
Addressing this, opponent shaping (OS) methods actively guide the learning
processes of other agents, empirically leading to improved individual and group
performances in many settings. Early OS methods use higher-order derivatives to
shape the learning of co-players, making them unsuitable for shaping multiple
learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses
these by reframing the OS problem as a meta-game. In contrast to early OS
methods, there is little theoretical understanding of the M-FOS framework.
Providing theoretical guarantees for M-FOS is hard because A) there is little
literature on theoretical sample complexity bounds for meta-reinforcement
learning B) M-FOS operates in continuous state and action spaces, so
theoretical analysis is challenging. In this work, we present R-FOS, a tabular
version of M-FOS that is more suitable for theoretical analysis. R-FOS
discretises the continuous meta-game MDP into a tabular MDP. Within this
discretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to
derive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We
derive a sample complexity bound that is exponential in the cardinality of the
inner state and action space and the number of agents. Our bound guarantees
that, with high probability, the final policy learned by an R-FOS agent is
close to the optimal policy, apart from a constant factor. Finally, we
investigate how R-FOS's sample complexity scales in the size of state-action
space. Our theoretical results on scaling are supported empirically in the
Matching Pennies environment.
\\ ( https://arxiv.org/abs/2402.05782 ,  3558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05785
Date: Thu, 8 Feb 2024 16:23:29 GMT   (184kb,D)

Title: Limits of Transformer Language Models on Algorithmic Learning
Authors: Jonathan Thomm, Aleksandar Terzic, Geethan Karunaratne, Giacomo
  Camposampiero, Bernhard Sch\"olkopf, Abbas Rahimi
Categories: cs.LG cs.AI cs.CL
\\
  We analyze the capabilities of Transformer language models on learning
discrete algorithms. To this end, we introduce two new tasks demanding the
composition of several discrete sub-tasks. On both training LLaMA models from
scratch and prompting on GPT-4 and Gemini we measure learning compositions of
learned primitives. We observe that the compositional capabilities of
state-of-the-art Transformer language models are very limited and sample-wise
scale worse than relearning all sub-tasks for a new algorithmic composition. We
also present a theorem in complexity theory, showing that gradient descent on
memorizing feedforward models can be exponentially data inefficient.
\\ ( https://arxiv.org/abs/2402.05785 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05802
Date: Thu, 8 Feb 2024 16:41:03 GMT   (1176kb,D)

Title: Unsupervised Discovery of Clinical Disease Signatures Using
  Probabilistic Independence
Authors: Thomas A. Lasko, John M. Still, Thomas Z. Li, Marco Barbero Mota,
  William W. Stead, Eric V. Strobl, Bennett A. Landman, Fabien Maldonado
Categories: cs.LG stat.AP stat.ML
Comments: 29 Pages, 8 figures
ACM-class: I.2.6; I.2.1; J.3
\\
  Insufficiently precise diagnosis of clinical disease is likely responsible
for many treatment failures, even for common conditions and treatments. With a
large enough dataset, it may be possible to use unsupervised machine learning
to define clinical disease patterns more precisely. We present an approach to
learning these patterns by using probabilistic independence to disentangle the
imprint on the medical record of causal latent sources of disease. We inferred
a broad set of 2000 clinical signatures of latent sources from 9195 variables
in 269,099 Electronic Health Records. The learned signatures produced better
discrimination than the original variables in a lung cancer prediction task
unknown to the inference algorithm, predicting 3-year malignancy in patients
with no history of cancer before a solitary lung nodule was discovered. More
importantly, the signatures' greater explanatory power identified pre-nodule
signatures of apparently undiagnosed cancer in many of those patients.
\\ ( https://arxiv.org/abs/2402.05802 ,  1176kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05806
Date: Thu, 8 Feb 2024 16:45:12 GMT   (1647kb,D)

Title: On Calibration and Conformal Prediction of Deep Classifiers
Authors: Lahav Dabah, Tom Tirer
Categories: cs.LG stat.ML
\\
  In many classification applications, the prediction of a deep neural network
(DNN) based classifier needs to be accompanied with some confidence indication.
Two popular post-processing approaches for that aim are: 1) calibration:
modifying the classifier's softmax values such that their maximum (associated
with the prediction) better estimates the correctness probability; and 2)
conformal prediction (CP): devising a score (based on the softmax values) from
which a set of predictions with theoretically guaranteed marginal coverage of
the correct class is produced. While in practice both types of indications can
be desired, so far the interplay between them has not been investigated. Toward
filling this gap, in this paper we study the effect of temperature scaling,
arguably the most common calibration technique, on prominent CP methods. We
start with an extensive empirical study that among other insights shows that,
surprisingly, calibration has a detrimental effect on popular adaptive CP
methods: it frequently leads to larger prediction sets. Then, we turn to
theoretically analyze this behavior. We reveal several mathematical properties
of the procedure, according to which we provide a reasoning for the phenomenon.
Our study suggests that it may be worthwhile to utilize adaptive CP methods,
chosen for their enhanced conditional coverage, based on softmax values prior
to (or after canceling) temperature scaling calibration.
\\ ( https://arxiv.org/abs/2402.05806 ,  1647kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05821
Date: Thu, 8 Feb 2024 16:59:24 GMT   (3224kb,D)

Title: Guided Evolution with Binary Discriminators for ML Program Search
Authors: John D. Co-Reyes, Yingjie Miao, George Tucker, Aleksandra Faust,
  Esteban Real
Categories: cs.LG cs.NE
\\
  How to automatically design better machine learning programs is an open
problem within AutoML. While evolution has been a popular tool to search for
better ML programs, using learning itself to guide the search has been less
successful and less understood on harder problems but has the promise to
dramatically increase the speed and final performance of the optimization
process. We propose guiding evolution with a binary discriminator, trained
online to distinguish which program is better given a pair of programs. The
discriminator selects better programs without having to perform a costly
evaluation and thus speed up the convergence of evolution. Our method can
encode a wide variety of ML components including symbolic optimizers, neural
architectures, RL loss functions, and symbolic regression equations with the
same directed acyclic graph representation. By combining this representation
with modern GNNs and an adaptive mutation strategy, we demonstrate our method
can speed up evolution across a set of diverse problems including a 3.7x
speedup on the symbolic search for ML optimizers and a 4x speedup for RL loss
functions.
\\ ( https://arxiv.org/abs/2402.05821 ,  3224kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05823
Date: Thu, 8 Feb 2024 17:03:10 GMT   (21125kb,D)

Title: FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework
  for Robust Solar Power Forecasting
Authors: Ziqing Ma, Wenwei Wang, Tian Zhou, Chao Chen, Bingqing Peng, Liang
  Sun, Rong Jin
Categories: cs.LG cs.AI cs.CV
\\
  Accurate solar power forecasting is crucial to integrate photovoltaic plants
into the electric grid, schedule and secure the power grid safety. This problem
becomes more demanding for those newly installed solar plants which lack
sufficient data. Current research predominantly relies on historical solar
power data or numerical weather prediction in a single-modality format,
ignoring the complementary information provided in different modalities. In
this paper, we propose a multi-modality fusion framework to integrate
historical power data, numerical weather prediction, and satellite images,
significantly improving forecast performance. We introduce a vector quantized
framework that aligns modalities with varying information densities, striking a
balance between integrating sufficient information and averting model
overfitting. Our framework demonstrates strong zero-shot forecasting
capability, which is especially useful for those newly installed plants.
Moreover, we collect and release a multi-modal solar power (MMSP) dataset from
real-world plants to further promote the research of multi-modal solar
forecasting algorithms. Our extensive experiments show that our model not only
operates with robustness but also boosts accuracy in both zero-shot forecasting
and scenarios rich with training data, surpassing leading models. We have
incorporated it into our eForecaster platform and deployed it for more than 300
solar plants with a capacity of over 15GW.
\\ ( https://arxiv.org/abs/2402.05823 ,  21125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05828
Date: Thu, 8 Feb 2024 17:07:42 GMT   (4064kb,D)

Title: Discovering Temporally-Aware Reinforcement Learning Algorithms
Authors: Matthew Thomas Jackson, Chris Lu, Louis Kirsch, Robert Tjarko Lange,
  Shimon Whiteson, Jakob Nicolaus Foerster
Categories: cs.LG cs.AI
Comments: Published at ICLR 2024
\\
  Recent advancements in meta-learning have enabled the automatic discovery of
novel reinforcement learning algorithms parameterized by surrogate objective
functions. To improve upon manually designed algorithms, the parameterization
of this learned objective function must be expressive enough to represent novel
principles of learning (instead of merely recovering already established ones)
while still generalizing to a wide range of settings outside of its
meta-training distribution. However, existing methods focus on discovering
objective functions that, like many widely used objective functions in
reinforcement learning, do not take into account the total number of steps
allowed for training, or "training horizon". In contrast, humans use a plethora
of different learning objectives across the course of acquiring a new ability.
For instance, students may alter their studying techniques based on the
proximity to exam deadlines and their self-assessed capabilities. This paper
contends that ignoring the optimization time horizon significantly restricts
the expressive potential of discovered learning algorithms. We propose a simple
augmentation to two existing objective discovery approaches that allows the
discovered algorithm to dynamically update its objective function throughout
the agent's training procedure, resulting in expressive schedules and increased
generalization across different training horizons. In the process, we find that
commonly used meta-gradient approaches fail to discover such adaptive objective
functions while evolution strategies discover highly dynamic learning rules. We
demonstrate the effectiveness of our approach on a wide range of tasks and
analyze the resulting learned algorithms, which we find effectively balance
exploration and exploitation by modifying the structure of their learning rules
throughout the agent's lifetime.
\\ ( https://arxiv.org/abs/2402.05828 ,  4064kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05830
Date: Thu, 8 Feb 2024 17:09:12 GMT   (3352kb,D)

Title: Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization
  for Enhanced Time Series Forecasting
Authors: Yanjun Zhao, Tian Zhou, Chao Chen, Liang Sun, Yi Qian, Rong Jin
Categories: cs.LG cs.AI
\\
  Time series analysis is vital for numerous applications, and transformers
have become increasingly prominent in this domain. Leading methods customize
the transformer architecture from NLP and CV, utilizing a patching technique to
convert continuous signals into segments. Yet, time series data are uniquely
challenging due to significant distribution shifts and intrinsic noise levels.
To address these two challenges,we introduce the Sparse Vector Quantized
FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse
vector quantization technique coupled with Reverse Instance Normalization
(RevIN) to reduce noise impact and capture sufficient statistics for
forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the
transformer architecture. Our FFN-free approach trims the parameter count,
enhancing computational efficiency and reducing overfitting. Through
evaluations across ten benchmark datasets, including the newly introduced CAISO
dataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in
MAE for univariate and multivariate time series forecasting, respectively.
Moreover, it can be seamlessly integrated with existing transformer-based
models to elevate their performance.
\\ ( https://arxiv.org/abs/2402.05830 ,  3352kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05835
Date: Thu, 8 Feb 2024 17:12:49 GMT   (196kb,D)

Title: How Much is Unseen Depends Chiefly on Information About the Seen
Authors: Seongmin Lee and Marcel B\"ohme
Categories: cs.LG cs.NE stat.ML
Comments: 8 pages with 5 pages of appendix, 5 figures, 3 tables
\\
  It might seem counter-intuitive at first: We find that, in expectation, the
proportion of data points in an unknown population-that belong to classes that
do not appear in the training data-is almost entirely determined by the number
$f_k$ of classes that do appear in the training data the same number of times.
While in theory we show that the difference of the induced estimator decays
exponentially in the size of the sample, in practice the high variance prevents
us from using it directly for an estimator of the sample coverage. However, our
precise characterization of the dependency between $f_k$'s induces a large
search space of different representations of the expected value, which can be
deterministically instantiated as estimators. Hence, we turn to optimization
and develop a genetic algorithm that, given only the sample, searches for an
estimator with minimal mean-squared error (MSE). In our experiments, our
genetic algorithm discovers estimators that have a substantially smaller MSE
than the state-of-the-art Good-Turing estimator. This holds for over 96% of
runs when there are at least as many samples as classes. Our estimators' MSE is
roughly 80% of the Good-Turing estimator's.
\\ ( https://arxiv.org/abs/2402.05835 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05859
Date: Thu, 8 Feb 2024 17:43:22 GMT   (279kb,D)

Title: Learning to Route Among Specialized Experts for Zero-Shot Generalization
Authors: Mohammed Muqeeth, Haokun Liu, Yufan Liu, Colin Raffel
Categories: cs.LG
\\
  Recently, there has been a widespread proliferation of "expert" language
models that are specialized to a specific task or domain through
parameter-efficient fine-tuning. How can we recycle large collections of expert
language models to improve zero-shot generalization to unseen tasks? In this
work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of
Specialized Experts (PHATGOOSE), which learns to route among specialized
modules that were produced through parameter-efficient fine-tuning. Unlike past
methods that learn to route among specialized models, PHATGOOSE explores the
possibility that zero-shot generalization will be improved if different experts
can be adaptively chosen for each token and at each layer in the model.
Crucially, our method is post-hoc - it does not require simultaneous access to
the datasets used to create the specialized models and only requires a modest
amount of additional compute after each expert model is trained. In experiments
covering a range of specialized model collections and zero-shot generalization
benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc
routing and, in some cases, outperforms explicit multitask training (which
requires simultaneous data access). To better understand the routing strategy
learned by PHATGOOSE, we perform qualitative experiments to validate that
PHATGOOSE's performance stems from its ability to make adaptive per-token and
per-module expert choices. We release all of our code to support future work on
improving zero-shot generalization by recycling specialized experts.
\\ ( https://arxiv.org/abs/2402.05859 ,  279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05862
Date: Thu, 8 Feb 2024 17:51:44 GMT   (1234kb,D)

Title: Let Your Graph Do the Talking: Encoding Structured Data for LLMs
Authors: Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran
  Kazemi, Rami Al-Rfou, Jonathan Halcrow
Categories: cs.LG cs.AI cs.SI stat.ML
ACM-class: I.5.1; I.2.6; I.2.7
\\
  How can we best encode structured data into sequential form for use in large
language models (LLMs)? In this work, we introduce a parameter-efficient method
to explicitly represent structured data for LLMs. Our method, GraphToken,
learns an encoding function to extend prompts with explicit structured
information. Unlike other work which focuses on limited domains (e.g. knowledge
graph representation), our work is the first effort focused on the general
encoding of structured data to be used for various reasoning tasks. We show
that explicitly representing the graph structure allows significant
improvements to graph reasoning tasks. Specifically, we see across the board
improvements - up to 73% points - on node, edge and, graph-level tasks from the
GraphQA benchmark.
\\ ( https://arxiv.org/abs/2402.05862 ,  1234kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05876
Date: Thu, 8 Feb 2024 18:09:17 GMT   (1021kb,D)

Title: Federated Offline Reinforcement Learning: Collaborative Single-Policy
  Coverage Suffices
Authors: Jiin Woo, Laixi Shi, Gauri Joshi, Yuejie Chi
Categories: cs.LG cs.MA stat.ML
\\
  Offline reinforcement learning (RL), which seeks to learn an optimal policy
using offline data, has garnered significant interest due to its potential in
critical applications where online data collection is infeasible or expensive.
This work explores the benefit of federated learning for offline RL, aiming at
collaboratively leveraging offline datasets at multiple agents. Focusing on
finite-horizon episodic tabular Markov decision processes (MDPs), we design
FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for
federated offline RL. FedLCB-Q updates local Q-functions at agents with novel
learning rate schedules and aggregates them at a central server using
importance averaging and a carefully designed pessimistic penalty term. Our
sample complexity analysis reveals that, with appropriately chosen parameters
and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the
number of agents without requiring high-quality datasets at individual agents,
as long as the local datasets collectively cover the state-action space visited
by the optimal policy, highlighting the power of collaboration in the federated
setting. In fact, the sample complexity almost matches that of the single-agent
counterpart, as if all the data are stored at a central location, up to
polynomial factors of the horizon length. Furthermore, FedLCB-Q is
communication-efficient, where the number of communication rounds is only
linear with respect to the horizon length up to logarithmic factors.
\\ ( https://arxiv.org/abs/2402.05876 ,  1021kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05885
Date: Thu, 8 Feb 2024 18:23:05 GMT   (518kb,D)

Title: EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance
Authors: Aditya Bommakanti, Harshith Reddy Vonteri, Sayan Ranu, Panagiotis
  Karras
Categories: cs.LG
\\
  The need to identify graphs having small structural distance from a query
arises in biology, chemistry, recommender systems, and social network analysis.
Among several methods to measure inter graph distance, Graph Edit Distance
(GED) is preferred for its comprehensibility, yet hindered by the NP-hardness
of its computation. State-of-the-art GED approximations predominantly employ
neural methods, which, however, (i) lack an explanatory edit path corresponding
to the approximated GED; (ii) require the NP-hard generation of ground-truth
GEDs for training; and (iii) necessitate separate training on each dataset. In
this paper, we propose an efficient algebraic unsuper vised method, EUGENE,
that approximates GED and yields edit paths corresponding to the approx imated
cost, while eliminating the need for ground truth generation and data-specific
training. Extensive experimental evaluation demonstrates that the
aforementioned benefits of EUGENE do not come at the cost of efficacy.
Specifically, EUGENE consistently ranks among the most accurate methods across
all of the benchmark datasets and outperforms majority of the neural
approaches.
\\ ( https://arxiv.org/abs/2402.05885 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05906
Date: Thu, 8 Feb 2024 18:43:27 GMT   (614kb,D)

Title: Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative
  Markov Games
Authors: Hafez Ghaemi, Hamed Kebriaei, Alireza Ramezani Moghaddam, Majid Nili
  Ahamdabadi
Categories: cs.LG cs.AI cs.MA
ACM-class: I.2.6; I.2.11
\\
  Classical multi-agent reinforcement learning (MARL) assumes risk neutrality
and complete objectivity for agents. However, in settings where agents need to
consider or model human economic or social preferences, a notion of risk must
be incorporated into the RL optimization problem. This will be of greater
importance in MARL where other human or non-human agents are involved, possibly
with their own risk-sensitive policies. In this work, we consider
risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT),
a non-convex risk measure and a generalization of coherent measures of risk.
CPT is capable of explaining loss aversion in humans and their tendency to
overestimate/underestimate small/large probabilities. We propose a distributed
sampling-based actor-critic (AC) algorithm with CPT risk for network
aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.
Under a set of assumptions, we prove the convergence of the algorithm to a
subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental
results show that subjective CPT policies obtained by our algorithm can be
different from the risk-neutral ones, and agents with a higher loss aversion
are more inclined to socially isolate themselves in an NAMG.
\\ ( https://arxiv.org/abs/2402.05906 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05916
Date: Thu, 8 Feb 2024 18:51:55 GMT   (451kb,D)

Title: GenEFT: Understanding Statics and Dynamics of Model Generalization via
  Effective Theory
Authors: David D. Baek, Ziming Liu, Max Tegmark
Categories: cs.LG
Comments: 12 pages, 6 figures
\\
  We present GenEFT: an effective theory framework for shedding light on the
statics and dynamics of neural network generalization, and illustrate it with
graph learning examples. We first investigate the generalization phase
transition as data size increases, comparing experimental results with
information-theory-based approximations. We find generalization in a Goldilocks
zone where the decoder is neither too weak nor too powerful. We then introduce
an effective theory for the dynamics of representation learning, where
latent-space representations are modeled as interacting particles (repons), and
find that it explains our experimentally observed phase transition between
generalization and overfitting as encoder and decoder learning rates are
scanned. This highlights the power of physics-inspired effective theories for
bridging the gap between theoretical predictions and practice in machine
learning.
\\ ( https://arxiv.org/abs/2402.05916 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05926
Date: Thu, 8 Feb 2024 18:56:40 GMT   (2467kb,D)

Title: On the Convergence of Zeroth-Order Federated Tuning in Large Language
  Models
Authors: Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen
Categories: cs.LG cs.CL
\\
  The confluence of Federated Learning (FL) and Large Language Models (LLMs) is
ushering in a new era in privacy-preserving natural language processing.
However, the intensive memory requirements for fine-tuning LLMs pose
significant challenges, especially when deploying on edge devices with limited
computational resources. To circumvent this, we explore the novel integration
of Memory-efficient Zeroth-Order Optimization within a federated setting, a
synergy we denote as FedMeZO. Our study is the first to examine the theoretical
underpinnings of FedMeZO in the context of LLMs, tackling key questions
regarding the influence of large parameter spaces on optimization behavior, the
establishment of convergence properties, and the identification of critical
parameters for convergence to inform personalized federated strategies. Our
extensive empirical evidence supports the theory, showing that FedMeZO not only
converges faster than traditional first-order methods such as SGD but also
significantly reduces GPU memory usage during training to levels comparable to
those during inference. Moreover, the proposed personalized FL strategy that is
built upon the theoretical insights to customize the client-wise learning rate
can effectively accelerate loss reduction. We hope our work can help to bridge
theoretical and practical aspects of federated fine-tuning for LLMs and
facilitate further development and research.
\\ ( https://arxiv.org/abs/2402.05926 ,  2467kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05928
Date: Thu, 8 Feb 2024 18:57:42 GMT   (58kb)

Title: Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation
  for the Square Loss
Authors: Ingvar Ziemann, Stephen Tu, George J. Pappas, Nikolai Matni
Categories: cs.LG stat.ML
\\
  In this work, we study statistical learning with dependent ($\beta$-mixing)
data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$
where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p}
\|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the
search for a sharp noise interaction term, or variance proxy, in learning with
dependent data. Absent any realizability assumption, typical non-asymptotic
results exhibit variance proxies that are deflated \emph{multiplicatively} by
the mixing time of the underlying covariates process. We show that whenever the
topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class
$\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class:
$\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the
empirical risk minimizer achieves a rate that only depends on the complexity of
the class and second order statistics in its leading term. Our result holds
whether the problem is realizable or not and we refer to this as a \emph{near
mixing-free rate}, since direct dependence on mixing is relegated to an
additive higher order term. We arrive at our result by combining the above
notion of a weakly sub-Gaussian class with mixed tail generic chaining. This
combination allows us to compute sharp, instance-optimal rates for a wide range
of problems. %Our approach, reliant on mixed tail generic chaining, allows us
to obtain sharp, instance-optimal rates. Examples that satisfy our framework
include sub-Gaussian linear regression, more general smoothly parameterized
function classes, finite hypothesis classes, and bounded smoothness classes.
\\ ( https://arxiv.org/abs/2402.05928 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05933
Date: Thu, 8 Feb 2024 18:59:05 GMT   (10758kb,D)

Title: Time Series Diffusion in the Frequency Domain
Authors: Jonathan Crabb\'e, Nicolas Huynh, Jan Stanczuk, Mihaela van der Schaar
Categories: cs.LG cs.AI
Comments: 27 pages, 12 figures
\\
  Fourier analysis has been an instrumental tool in the development of signal
processing. This leads us to wonder whether this framework could similarly
benefit generative modelling. In this paper, we explore this question through
the scope of time series diffusion models. More specifically, we analyze
whether representing time series in the frequency domain is a useful inductive
bias for score-based diffusion models. By starting from the canonical SDE
formulation of diffusion in the time domain, we show that a dual diffusion
process occurs in the frequency domain with an important nuance: Brownian
motions are replaced by what we call mirrored Brownian motions, characterized
by mirror symmetries among their components. Building on this insight, we show
how to adapt the denoising score matching approach to implement diffusion
models in the frequency domain. This results in frequency diffusion models,
which we compare to canonical time diffusion models. Our empirical evaluation
on real-world datasets, covering various domains like healthcare and finance,
shows that frequency diffusion models better capture the training distribution
than time diffusion models. We explain this observation by showing that time
series from these datasets tend to be more localized in the frequency domain
than in the time domain, which makes them easier to model in the former case.
All our observations point towards impactful synergies between Fourier analysis
and diffusion models.
\\ ( https://arxiv.org/abs/2402.05933 ,  10758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05934
Date: Thu, 8 Feb 2024 18:59:30 GMT   (99kb,D)

Title: Classifying Nodes in Graphs without GNNs
Authors: Daniel Winter, Niv Cohen, Yedid Hoshen
Categories: cs.LG cs.SI
\\
  Graph neural networks (GNNs) are the dominant paradigm for classifying nodes
in a graph, but they have several undesirable attributes stemming from their
message passing architecture. Recently, distillation methods succeeded in
eliminating the use of GNNs at test time but they still require them during
training. We perform a careful analysis of the role that GNNs play in
distillation methods. This analysis leads us to propose a fully GNN-free
approach for node classification, not requiring them at train or test time. Our
method consists of three key components: smoothness constraints,
pseudo-labeling iterations and neighborhood-label histograms. Our final
approach can match the state-of-the-art accuracy on standard popular benchmarks
such as citation and co-purchase networks, without training a GNN.
\\ ( https://arxiv.org/abs/2402.05934 ,  99kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.02987 (*cross-listing*)
Date: Mon, 5 Feb 2024 13:18:42 GMT   (3473kb,D)

Title: Conversation Reconstruction Attack Against GPT Models
Authors: Junjie Chu and Zeyang Sha and Michael Backes and Yang Zhang
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: 17 pages, 11 figures
\\
  In recent times, significant advancements have been made in the field of
large language models (LLMs), represented by GPT series models. To optimize
task execution, users often engage in multi-round conversations with GPT models
hosted in cloud environments. These multi-round conversations, potentially
replete with private information, require transmission and storage within the
cloud. However, this operational paradigm introduces additional attack
surfaces. In this paper, we first introduce a specific Conversation
Reconstruction Attack targeting GPT models. Our introduced Conversation
Reconstruction Attack is composed of two steps: hijacking a session and
reconstructing the conversations. Subsequently, we offer an exhaustive
evaluation of the privacy risks inherent in conversations when GPT models are
subjected to the proposed attack. However, GPT-4 demonstrates certain
robustness to the proposed attacks. We then introduce two advanced attacks
aimed at better reconstructing previous conversations, specifically the UNR
attack and the PBU attack. Our experimental findings indicate that the PBU
attack yields substantial performance across all models, achieving semantic
similarity scores exceeding 0.60, while the UNR attack is effective solely on
GPT-3.5. Our results reveal the concern about privacy risks associated with
conversations involving GPT models and aim to draw the community's attention to
prevent the potential misuse of these models' remarkable capabilities. We will
responsibly disclose our findings to the suppliers of related large language
models.
\\ ( https://arxiv.org/abs/2402.02987 ,  3473kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05115 (*cross-listing*)
Date: Thu, 18 Jan 2024 09:03:33 GMT   (1030kb,D)

Title: Unsupervised Motion Retargeting for Human-Robot Imitation
Authors: Louis Annabi (Flowers, U2IS), Ziqi Ma (U2IS), Sao Mai Nguyen
  (Lab-STICC_RAMBO, U2IS, Flowers, IMT Atlantique - INFO)
Categories: cs.RO cs.AI cs.LG
Comments: Companion of the 2024 ACM/IEEE International Conference on
  Human-Robot Interactio, Mar 2024, Boulder (CO), United States
DOI: 10.1145/3568294.3580153
\\
  This early-stage research work aims to improve online human-robot imitation
by translating sequences of joint positions from the domain of human motions to
a domain of motions achievable by a given robot, thus constrained by its
embodiment. Leveraging the generalization capabilities of deep learning
methods, we address this problem by proposing an encoder-decoder neural network
model performing domain-to-domain translation. In order to train such a model,
one could use pairs of associated robot and human motions. Though, such paired
data is extremely rare in practice, and tedious to collect. Therefore, we turn
towards deep learning methods for unpaired domain-to-domain translation, that
we adapt in order to perform human-robot imitation.
\\ ( https://arxiv.org/abs/2402.05115 ,  1030kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05142 (*cross-listing*)
Date: Wed, 7 Feb 2024 01:45:14 GMT   (296kb)

Title: The Foundations of Computational Management: A Systematic Approach to
  Task Automation for the Integration of Artificial Intelligence into Existing
  Workflows
Authors: Tamen Jadad-Garcia, Alejandro R. Jadad
Categories: cs.SE cs.AI
Comments: 29 pages, 3 appendices
\\
  Driven by the rapid ascent of artificial intelligence (AI), organizations are
at the epicenter of a seismic shift, facing a crucial question: How can AI be
successfully integrated into existing operations? To help answer it, manage
expectations and mitigate frustration, this article introduces Computational
Management, a systematic approach to task automation for enhancing the ability
of organizations to harness AI's potential within existing workflows.
Computational Management acts as a bridge between the strategic insights of
management science with the analytical rigor of computational thinking. The
article offers three easy step-by-step procedures to begin the process of
implementing AI within a workflow. Such procedures focus on task
(re)formulation, on the assessment of the automation potential of tasks, on the
completion of task specification templates for AI selection and adaptation.
Included in the article there are manual and automated methods, with prompt
suggestions for publicly available LLMs, to complete these three procedures.
The first procedure, task (re)formulation, focuses on breaking down work
activities into basic units, so they can be completed by one agent, involve a
single well-defined action, and produce a distinct outcome. The second, allows
the assessment of the granular task and its suitability for automation, using
the Task Automation Index to rank tasks based on whether they have standardized
input, well-defined rules, repetitiveness, data dependency, and objective
outputs. The third, focuses on a task specification template which details
information on 16 critical components of tasks, and can be used as a checklist
to select or adapt the most suitable AI solution for integration into existing
workflows. Computational Management provides a roadmap and a toolkit for humans
and AI to thrive together, while enhancing organizational efficiency and
innovation.
\\ ( https://arxiv.org/abs/2402.05142 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05144 (*cross-listing*)
Date: Wed, 7 Feb 2024 08:01:45 GMT   (435kb,D)

Title: A Bandit Approach with Evolutionary Operators for Model Selection
Authors: Margaux Br\'eg\`ere (LPSM (UMR_8001), EDF R&D), Julie Keisler
  (CRIStAL, EDF R&D)
Categories: cs.NE cs.AI cs.LG math.OC
\\
  This paper formulates model selection as an infinite-armed bandit problem.
The models are arms, and picking an arm corresponds to a partial training of
the model (resource allocation). The reward is the accuracy of the selected
model after its partial training. In this best arm identification problem,
regret is the gap between the expected accuracy of the optimal model and that
of the model finally chosen. We first consider a straightforward generalization
of UCB-E to the stochastic infinite-armed bandit problem and show that, under
basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha
\in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla
algorithm, we introduce the algorithm Mutant-UCB that incorporates operators
from evolutionary algorithms. Tests carried out on three open source image
classification data sets attest to the relevance of this novel combining
approach, which outperforms the state-of-the-art for a fixed budget.
\\ ( https://arxiv.org/abs/2402.05144 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05148 (*cross-listing*)
Date: Wed, 7 Feb 2024 09:41:39 GMT   (1684kb,D)

Title: Cost Optimized Scheduling in Modular Electrolysis Plants
Authors: Vincent Henkel and Maximilian Kilthau and Felix Gehlhoff and Lukas
  Wagner and Alexander Fay
Categories: cs.SY cs.AI
\\
  In response to the global shift towards renewable energy resources, the
production of green hydrogen through electrolysis is emerging as a promising
solution. Modular electrolysis plants, designed for flexibility and
scalability, offer a dynamic response to the increasing demand for hydrogen
while accommodating the fluctuations inherent in renewable energy sources.
However, optimizing their operation is challenging, especially when a large
number of electrolysis modules needs to be coordinated, each with potentially
different characteristics.
  To address these challenges, this paper presents a decentralized scheduling
model to optimize the operation of modular electrolysis plants using the
Alternating Direction Method of Multipliers. The model aims to balance hydrogen
production with fluctuating demand, to minimize the marginal Levelized Cost of
Hydrogen (mLCOH), and to ensure adaptability to operational disturbances. A
case study validates the accuracy of the model in calculating mLCOH values
under nominal load conditions and demonstrates its responsiveness to dynamic
changes, such as electrolyzer module malfunctions and scale-up scenarios.
\\ ( https://arxiv.org/abs/2402.05148 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05154 (*cross-listing*)
Date: Wed, 7 Feb 2024 15:21:18 GMT   (3511kb,D)

Title: Adaptive Hypergraph Network for Trust Prediction
Authors: Rongwei Xu, Guanfeng Liu, Yan Wang, Xuyun Zhang, Kai Zheng, Xiaofang
  Zhou
Categories: cs.SI cs.AI
\\
  Trust plays an essential role in an individual's decision-making. Traditional
trust prediction models rely on pairwise correlations to infer potential
relationships between users. However, in the real world, interactions between
users are usually complicated rather than pairwise only. Hypergraphs offer a
flexible approach to modeling these complex high-order correlations (not just
pairwise connections), since hypergraphs can leverage hyperedeges to link more
than two nodes. However, most hypergraph-based methods are generic and cannot
be well applied to the trust prediction task. In this paper, we propose an
Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that
improves trust prediction accuracy by using higher-order correlations. AHNTP
utilizes Motif-based PageRank to capture high-order social influence
information. In addition, it constructs hypergroups from both node-level and
structure-level attributes to incorporate complex correlation information.
Furthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network
(GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user
embeddings, facilitating trust relationship prediction. To enhance model
generalization and robustness, we introduce a novel supervised contrastive
learning loss for optimization. Extensive experiments demonstrate the
superiority of our model over the state-of-the-art approaches in terms of trust
prediction accuracy. The source code of this work can be accessed via
https://github.com/Sherry-XU1995/AHNTP.
\\ ( https://arxiv.org/abs/2402.05154 ,  3511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05156 (*cross-listing*)
Date: Wed, 7 Feb 2024 16:31:58 GMT   (679kb,D)

Title: What About the Data? A Mapping Study on Data Engineering for AI Systems
Authors: Petra Heck
Categories: cs.DL cs.AI cs.DB
Comments: Preprint, accepted for CAIN24
DOI: 10.1145/3644815.3644954
\\
  AI systems cannot exist without data. Now that AI models (data science and
AI) have matured and are readily available to apply in practice, most
organizations struggle with the data infrastructure to do so. There is a
growing need for data engineers that know how to prepare data for AI systems or
that can setup enterprise-wide data architectures for analytical projects. But
until now, the data engineering part of AI engineering has not been getting
much attention, in favor of discussing the modeling part. In this paper we aim
to change this by perform a mapping study on data engineering for AI systems,
i.e., AI data engineering. We found 25 relevant papers between January 2019 and
June 2023, explaining AI data engineering activities. We identify which life
cycle phases are covered, which technical solutions or architectures are
proposed and which lessons learned are presented. We end by an overall
discussion of the papers with implications for practitioners and researchers.
This paper creates an overview of the body of knowledge on data engineering for
AI. This overview is useful for practitioners to identify solutions and best
practices as well as for researchers to identify gaps.
\\ ( https://arxiv.org/abs/2402.05156 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05158 (*cross-listing*)
Date: Wed, 7 Feb 2024 18:02:33 GMT   (12406kb,D)

Title: Enhancement of Bengali OCR by Specialized Models and Advanced Techniques
  for Diverse Document Types
Authors: AKM Shahariar Azad Rabby, Hasmot Ali, Md. Majedul Islam, Sheikh
  Abujar, Fuad Rahman
Categories: cs.CV cs.AI cs.LG
Comments: 8 pages, 7 figures, 4 table Link of the paper
  https://openaccess.thecvf.com/content/WACV2024W/WVLL/html/Rabby_Enhancement_of_Bengali_OCR_by_Specialized_Models_and_Advanced_Techniques_WACVW_2024_paper.html
Journal-ref: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) Workshops, 2024, pp. 1102-1109
\\
  This research paper presents a unique Bengali OCR system with some
capabilities. The system excels in reconstructing document layouts while
preserving structure, alignment, and images. It incorporates advanced image and
signature detection for accurate extraction. Specialized models for word
segmentation cater to diverse document types, including computer-composed,
letterpress, typewriter, and handwritten documents. The system handles static
and dynamic handwritten inputs, recognizing various writing styles.
Furthermore, it has the ability to recognize compound characters in Bengali.
Extensive data collection efforts provide a diverse corpus, while advanced
technical components optimize character and word recognition. Additional
contributions include image, logo, signature and table recognition, perspective
correction, layout reconstruction, and a queuing module for efficient and
scalable processing. The system demonstrates outstanding performance in
efficient and accurate text extraction and analysis.
\\ ( https://arxiv.org/abs/2402.05158 ,  12406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05160 (*cross-listing*)
Date: Wed, 7 Feb 2024 18:04:32 GMT   (3251kb,D)

Title: What's documented in AI? Systematic Analysis of 32K AI Model Cards
Authors: Weixin Liang, Nazneen Rajani, Xinyu Yang, Ezinwanne Ozoani, Eric Wu,
  Yiqun Chen, Daniel Scott Smith, James Zou
Categories: cs.SE cs.AI cs.LG
\\
  The rapid proliferation of AI models has underscored the importance of
thorough documentation, as it enables users to understand, trust, and
effectively utilize these models in various applications. Although developers
are encouraged to produce model cards, it's not clear how much information or
what information these cards contain. In this study, we conduct a comprehensive
analysis of 32,111 AI model documentations on Hugging Face, a leading platform
for distributing and deploying AI models. Our investigation sheds light on the
prevailing model card documentation practices. Most of the AI models with
substantial downloads provide model cards, though the cards have uneven
informativeness. We find that sections addressing environmental impact,
limitations, and evaluation exhibit the lowest filled-out rates, while the
training section is the most consistently filled-out. We analyze the content of
each section to characterize practitioners' priorities. Interestingly, there
are substantial discussions of data, sometimes with equal or even greater
emphasis than the model itself. To evaluate the impact of model cards, we
conducted an intervention study by adding detailed model cards to 42 popular
models which had no or sparse model cards previously. We find that adding model
cards is moderately correlated with an increase weekly download rates. Our
study opens up a new perspective for analyzing community norms and practices
for model documentation through large-scale data science and linguistics
analysis.
\\ ( https://arxiv.org/abs/2402.05160 ,  3251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05188 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:01:11 GMT   (23941kb,D)

Title: InCoRo: In-Context Learning for Robotics Control with Feedback Loops
Authors: Jiaqiang Ye Zhu, Carla Gomez Cano, David Vazquez Bermudez and Michal
  Drozdzal
Categories: cs.RO cs.AI cs.CL
\\
  One of the challenges in robotics is to enable robotic units with the
reasoning capability that would be robust enough to execute complex tasks in
dynamic environments. Recent advances in LLMs have positioned them as go-to
tools for simple reasoning tasks, motivating the pioneering work of Liang et
al. [35] that uses an LLM to translate natural language commands into low-level
static execution plans for robotic units. Using LLMs inside robotics systems
brings their generalization to a new level, enabling zero-shot generalization
to new tasks. This paper extends this prior work to dynamic environments. We
propose InCoRo, a system that uses a classical robotic feedback loop composed
of an LLM controller, a scene understanding unit, and a robot. Our system
continuously analyzes the state of the environment and provides adapted
execution commands, enabling the robot to adjust to changing environmental
conditions and correcting for controller errors. Our system does not require
any iterative optimization to learn to accomplish a task as it leverages
in-context learning with an off-the-shelf LLM model. Through an extensive
validation process involving two standardized industrial robotic units -- SCARA
and DELTA types -- we contribute knowledge about these robots, not popular in
the community, thereby enriching it. We highlight the generalization
capabilities of our system and show that (1) in-context learning in combination
with the current state-of-the-art LLMs is an effective way to implement a
robotic controller; (2) in static environments, InCoRo surpasses the prior art
in terms of the success rate; (3) in dynamic environments, we establish new
state-of-the-art for the SCARA and DELTA units, respectively. This research
paves the way towards building reliable, efficient, intelligent autonomous
systems that adapt to dynamic environments.
\\ ( https://arxiv.org/abs/2402.05188 ,  23941kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05200 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:10:36 GMT   (342kb,D)

Title: Are LLMs Ready for Real-World Materials Discovery?
Authors: Santiago Miret, N M Anoop Krishnan
Categories: cond-mat.mtrl-sci cs.AI cs.CL cs.LG
\\
  Large Language Models (LLMs) create exciting possibilities for powerful
language processing tools to accelerate research in materials science. While
LLMs have great potential to accelerate materials understanding and discovery,
they currently fall short in being practical materials science tools. In this
position paper, we show relevant failure cases of LLMs in materials science
that reveal current limitations of LLMs related to comprehending and reasoning
over complex, interconnected materials science knowledge. Given those
shortcomings, we outline a framework for developing Materials Science LLMs
(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis
generation followed by hypothesis testing. The path to attaining performant
MatSci-LLMs rests in large part on building high-quality, multi-modal datasets
sourced from scientific literature where various information extraction
challenges persist. As such, we describe key materials science information
extraction challenges which need to be overcome in order to build large-scale,
multi-modal datasets that capture valuable materials science knowledge.
Finally, we outline a roadmap for applying future MatSci-LLMs for real-world
materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated
In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials
Laboratories.
\\ ( https://arxiv.org/abs/2402.05200 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05271 (*cross-listing*)
Date: Wed, 7 Feb 2024 21:31:53 GMT   (956kb,D)

Title: Gradient descent induces alignment between weights and the empirical NTK
  for deep non-linear networks
Authors: Daniel Beaglehole, Ioannis Mitliagkas, Atish Agarwala
Categories: stat.ML cs.AI cs.LG
\\
  Understanding the mechanisms through which neural networks extract statistics
from input-label pairs is one of the most important unsolved problems in
supervised learning. Prior works have identified that the gram matrices of the
weights in trained neural networks of general architectures are proportional to
the average gradient outer product of the model, in a statement known as the
Neural Feature Ansatz (NFA). However, the reason these quantities become
correlated during training is poorly understood. In this work, we explain the
emergence of this correlation. We identify that the NFA is equivalent to
alignment between the left singular structure of the weight matrices and a
significant component of the empirical neural tangent kernels associated with
those weights. We establish that the NFA introduced in prior works is driven by
a centered NFA that isolates this alignment. We show that the speed of NFA
development can be predicted analytically at early training times in terms of
simple statistics of the inputs and labels. Finally, we introduce a simple
intervention to increase NFA correlation at any given layer, which dramatically
improves the quality of features learned.
\\ ( https://arxiv.org/abs/2402.05271 ,  956kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05301 (*cross-listing*)
Date: Wed, 7 Feb 2024 22:37:16 GMT   (979kb,D)

Title: BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and
  Parametric CAD Designs
Authors: Lyle Regenwetter, Yazan Abu Obaideh, Amin Heyrani Nobari, Faez Ahmed
Categories: cs.CV cs.AI cs.LG
\\
  This paper introduces a public dataset of 1.4 million procedurally-generated
bicycle designs represented parametrically, as JSON files, and as rasterized
images. The dataset is created through the use of a rendering engine which
harnesses the BikeCAD software to generate vector graphics from parametric
designs. This rendering engine is discussed in the paper and also released
publicly alongside the dataset. Though this dataset has numerous applications,
a principal motivation is the need to train cross-modal predictive models
between parametric and image-based design representations. For example, we
demonstrate that a predictive model can be trained to accurately estimate
Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric
representation directly. This allows similarity relations to be established
between parametric bicycle designs and text strings or reference images.
Trained predictive models are also made public. The dataset joins the BIKED
dataset family which includes thousands of mixed-representation human-designed
bicycle models and several datasets quantifying design performance. The code
and dataset can be found at:
https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main
\\ ( https://arxiv.org/abs/2402.05301 ,  979kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05355 (*cross-listing*)
Date: Thu, 8 Feb 2024 02:27:13 GMT   (226kb,D)

Title: A Survey on Safe Multi-Modal Learning System
Authors: Tianyi Zhao, Liangliang Zhang, Yao Ma and Lu Cheng
Categories: cs.CY cs.AI
\\
  With the wide deployment of multimodal learning systems (MMLS) in real-world
scenarios, safety concerns have become increasingly prominent. The absence of
systematic research into their safety is a significant barrier to progress in
this field. To bridge the gap, we present the first taxonomy for MMLS safety,
identifying four essential pillars of these concerns. Leveraging this taxonomy,
we conduct in-depth reviews for each pillar, highlighting key limitations based
on the current state of development. Finally, we pinpoint unique challenges in
MMLS safety and provide potential directions for future research.
\\ ( https://arxiv.org/abs/2402.05355 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05374 (*cross-listing*)
Date: Thu, 8 Feb 2024 03:12:25 GMT   (9813kb,D)

Title: CIC: A framework for Culturally-aware Image Captioning
Authors: Youngsik Yun and Jihie Kim
Categories: cs.CV cs.AI cs.CL
Comments: 14 pages, 10 figures
\\
  Image Captioning generates descriptive sentences from images using
Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved
greatly. However, current methods lack the generation of detailed descriptive
captions for the cultural elements depicted in the images, such as the
traditional clothing worn by people from Asian cultural groups. In this paper,
we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)},
that generates captions and describes cultural elements extracted from cultural
visual elements in images representing cultures. Inspired by methods combining
visual modality and Large Language Models (LLMs) through appropriate prompts,
our framework (1) generates questions based on cultural categories from images,
(2) extracts cultural visual elements from Visual Question Answering (VQA)
using generated questions, and (3) generates culturally-aware captions using
LLMs with the prompts. Our human evaluation conducted on 45 participants from 4
different cultural groups with a high understanding of the corresponding
culture shows that our proposed framework generates more culturally descriptive
captions when compared to the image captioning baseline based on VLPs. Our code
and dataset will be made publicly available upon acceptance.
\\ ( https://arxiv.org/abs/2402.05374 ,  9813kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05378 (*cross-listing*)
Date: Thu, 8 Feb 2024 03:22:12 GMT   (319kb,D)

Title: Graph Neural Networks for Physical-Layer Security in Multi-User
  Flexible-Duplex Networks
Authors: Tharaka Perera, Saman Atapattu, Yuting Fang, Jamie Evans
Categories: eess.SP cs.AI cs.CR cs.LG
\\
  This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD)
networks, considering scenarios involving eavesdroppers. Our investigation
revolves around the intricacies of the sum secrecy rate maximization problem,
particularly when faced with coordinated and distributed eavesdroppers
employing a Minimum Mean Square Error (MMSE) receiver. Our contributions
include an iterative classical optimization solution and an unsupervised
learning strategy based on Graph Neural Networks (GNNs). To the best of our
knowledge, this work marks the initial exploration of GNNs for PLS
applications. Additionally, we extend the GNN approach to address the absence
of eavesdroppers' channel knowledge. Extensive numerical simulations highlight
FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's
superiority over the classical method in both performance and time complexity.
\\ ( https://arxiv.org/abs/2402.05378 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05399 (*cross-listing*)
Date: Thu, 8 Feb 2024 04:27:14 GMT   (10718kb,D)

Title: CURE: Simulation-Augmented Auto-Tuning in Robotics
Authors: Md Abir Hossen, Sonam Kharade, Jason M. O'Kane, Bradley Schmerl, David
  Garlan, Pooyan Jamshidi
Categories: cs.RO cs.AI
Comments: Submitted in IEEE Transactions on Robotics (T-RO), 2024
\\
  Robotic systems are typically composed of various subsystems, such as
localization and navigation, each encompassing numerous configurable components
(e.g., selecting different planning algorithms). Once an algorithm has been
selected for a component, its associated configuration options must be set to
the appropriate values. Configuration options across the system stack interact
non-trivially. Finding optimal configurations for highly configurable robots to
achieve desired performance poses a significant challenge due to the
interactions between configuration options across software and hardware that
result in an exponentially large and complex configuration space. These
challenges are further compounded by the need for transferability between
different environments and robotic platforms. Data efficient optimization
algorithms (e.g., Bayesian optimization) have been increasingly employed to
automate the tuning of configurable parameters in cyber-physical systems.
However, such optimization algorithms converge at later stages, often after
exhausting the allocated budget (e.g., optimization steps, allotted time) and
lacking transferability. This paper proposes CURE -- a method that identifies
causally relevant configuration options, enabling the optimization process to
operate in a reduced search space, thereby enabling faster optimization of
robot performance. CURE abstracts the causal relationships between various
configuration options and robot performance objectives by learning a causal
model in the source (a low-cost environment such as the Gazebo simulator) and
applying the learned knowledge to perform optimization in the target (e.g.,
Turtlebot 3 physical robot). We demonstrate the effectiveness and
transferability of CURE by conducting experiments that involve varying degrees
of deployment changes in both physical robots and simulation.
\\ ( https://arxiv.org/abs/2402.05399 ,  10718kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05448 (*cross-listing*)
Date: Thu, 8 Feb 2024 07:01:00 GMT   (7283kb,D)

Title: Minecraft-ify: Minecraft Style Image Generation with Text-guided Image
  Editing for In-Game Application
Authors: Bumsoo Kim, Sanghyun Byun, Yonghoon Jung, Wonseop Shin, Sareer UI
  Amin, Sanghyun Seo
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
Comments: 2 pages, 2 figures. Accepted to NeurIPS 2023 Workshop on Machine
  Learning for Creativity and Design
\\
  In this paper, we first present the character texture generation system
\textit{Minecraft-ify}, specified to Minecraft video game toward in-game
application. Ours can generate face-focused image for texture mapping tailored
to 3D virtual character having cube manifold. While existing projects or works
only generate texture, proposed system can inverse the user-provided real
image, or generate average/random appearance from learned distribution.
Moreover, it can be manipulated with text-guidance using StyleGAN and
StyleCLIP. These features provide a more extended user experience with enlarged
freedom as a user-friendly AI-tool. Project page can be found at
https://gh-bumsookim.github.io/Minecraft-ify/
\\ ( https://arxiv.org/abs/2402.05448 ,  7283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05484 (*cross-listing*)
Date: Thu, 8 Feb 2024 08:25:41 GMT   (385kb)

Title: Leveraging AI for Enhanced Software Effort Estimation: A Comprehensive
  Study and Framework Proposal
Authors: Nhi Tran, Tan Tran, Nam Nguyen
Categories: cs.SE cs.AI
\\
  This paper presents an extensive study on the application of AI techniques
for software effort estimation in the past five years from 2017 to 2023. By
overcoming the limitations of traditional methods, the study aims to improve
accuracy and reliability. Through performance evaluation and comparison with
diverse Machine Learning models, including Artificial Neural Network (ANN),
Support Vector Machine (SVM), Linear Regression, Random Forest and other
techniques, the most effective method is identified. The proposed AI-based
framework holds the potential to enhance project planning and resource
allocation, contributing to the research area of software project effort
estimation.
\\ ( https://arxiv.org/abs/2402.05484 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05493 (*cross-listing*)
Date: Thu, 8 Feb 2024 09:03:17 GMT   (2198kb,D)

Title: Investigating White-Box Attacks for On-Device Models
Authors: Mingyi Zhou, Xiang Gao, Jing Wu, Kui Liu, Hailong Sun, Li Li
Categories: cs.SE cs.AI cs.CR
Comments: The International Conference on Software Engineering 2024 (ICSE'24)
DOI: 10.1145/3597503.3639144
\\
  Numerous mobile apps have leveraged deep learning capabilities. However,
on-device models are vulnerable to attacks as they can be easily extracted from
their corresponding mobile apps. Existing on-device attacking approaches only
generate black-box attacks, which are far less effective and efficient than
white-box strategies. This is because mobile deep learning frameworks like
TFLite do not support gradient computing, which is necessary for white-box
attacking algorithms. Thus, we argue that existing findings may underestimate
the harmfulness of on-device attacks. To this end, we conduct a study to answer
this research question: Can on-device models be directly attacked via white-box
strategies? We first systematically analyze the difficulties of transforming
the on-device model to its debuggable version, and propose a Reverse
Engineering framework for On-device Models (REOM), which automatically reverses
the compiled on-device TFLite model to the debuggable model. Specifically, REOM
first transforms compiled on-device models into Open Neural Network Exchange
format, then removes the non-debuggable parts, and converts them to the
debuggable DL models format that allows attackers to exploit in a white-box
setting. Our experimental results show that our approach is effective in
achieving automated transformation among 244 TFLite models. Compared with
previous attacks using surrogate models, REOM enables attackers to achieve
higher attack success rates with a hundred times smaller attack perturbations.
In addition, because the ONNX platform has plenty of tools for model format
exchanging, the proposed method based on the ONNX platform can be adapted to
other model formats. Our findings emphasize the need for developers to
carefully consider their model deployment strategies, and use white-box methods
to evaluate the vulnerability of on-device models.
\\ ( https://arxiv.org/abs/2402.05493 ,  2198kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05519 (*cross-listing*)
Date: Thu, 8 Feb 2024 10:00:40 GMT   (535kb)

Title: Can ChatGPT evaluate research quality?
Authors: Mike Thelwall
Categories: cs.DL cs.AI
\\
  Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research
evaluations on journal articles to automate this time-consuming task.
Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the
quality of journal articles using a case study of the published scoring
guidelines of the UK Research Excellence Framework (REF) 2021 to create a
research evaluation ChatGPT. This was applied to 51 of my own articles and
compared against my own quality judgements. Findings: ChatGPT-4 can produce
plausible document summaries and quality evaluation rationales that match the
REF criteria. Its overall scores have weak correlations with my self-evaluation
scores of the same documents (averaging r=0.281 over 15 iterations, with 8
being statistically significantly different from 0). In contrast, the average
scores from the 15 iterations produced a statistically significant positive
correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds
seems more effective than individual scores. The positive correlation may be
due to ChatGPT being able to extract the author's significance, rigour, and
originality claims from inside each paper. If my weakest articles are removed,
then the correlation with average scores (r=0.200) falls below statistical
significance, suggesting that ChatGPT struggles to make fine-grained
evaluations. Research limitations: The data is self-evaluations of a
convenience sample of articles from one academic in one field. Practical
implications: Overall, ChatGPT does not yet seem to be accurate enough to be
trusted for any formal or informal research quality evaluation tasks. Research
evaluators, including journal editors, should therefore take steps to control
its use. Originality/value: This is the first published attempt at
post-publication expert review accuracy testing for ChatGPT.
\\ ( https://arxiv.org/abs/2402.05519 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05563 (*cross-listing*)
Date: Thu, 8 Feb 2024 11:02:06 GMT   (17kb)

Title: Neural Multigrid Architectures
Authors: Vladimir Fanaskov
Categories: math.NA cs.AI cs.NA
DOI: 10.1109/IJCNN52387.2021.9533736
\\
  We propose a convenient matrix-free neural architecture for the multigrid
method. The architecture is simple enough to be implemented in less than fifty
lines of code, yet it encompasses a large number of distinct multigrid solvers.
We argue that a fixed neural network without dense layers can not realize an
efficient iterative method. Because of that, standard training protocols do not
lead to competitive solvers. To overcome this difficulty, we use parameter
sharing and serialization of layers. The resulting network can be trained on
linear problems with thousands of unknowns and retains its efficiency on
problems with millions of unknowns. From the point of view of numerical linear
algebra network's training corresponds to finding optimal smoothers for the
geometric multigrid method. We demonstrate our approach on a few second-order
elliptic equations. For tested linear systems, we obtain from two to five times
smaller spectral radius of the error propagation matrix compare to a basic
linear multigrid with Jacobi smoother.
\\ ( https://arxiv.org/abs/2402.05563 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05593 (*cross-listing*)
Date: Thu, 8 Feb 2024 11:46:26 GMT   (17001kb,D)

Title: A Concept for Reconstructing Stucco Statues from historic Sketches using
  synthetic Data only
Authors: Thomas P\"ollabauer, Julius K\"uhn
Categories: cs.CV cs.AI
Journal-ref: Eurographics Workshop on Graphics and Cultural Heritage 2022
\\
  In medieval times, stuccoworkers used a red color, called sinopia, to first
create a sketch of the to-be-made statue on the wall. Today, many of these
statues are destroyed, but using the original drawings, deriving from the red
color also called sinopia, we can reconstruct how the final statue might have
looked.We propose a fully-automated approach to reconstruct a point cloud and
show preliminary results by generating a color-image, a depth-map, as well as
surface normals requiring only a single sketch, and without requiring a
collection of other, similar samples. Our proposed solution allows real-time
reconstruction on-site, for instance, within an exhibition, or to generate a
useful starting point for an expert, trying to manually reconstruct the statue,
all while using only synthetic data for training.
\\ ( https://arxiv.org/abs/2402.05593 ,  17001kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05610 (*cross-listing*)
Date: Thu, 8 Feb 2024 12:08:52 GMT   (13357kb,D)

Title: Extending 6D Object Pose Estimators for Stereo Vision
Authors: Thomas P\"ollabauer, Jan Emrich, Volker Knauthe, Arjan Kuijper
Categories: cs.CV cs.AI
\\
  Estimating the 6D pose of objects accurately, quickly, and robustly remains a
difficult task. However, recent methods for directly regressing poses from RGB
images using dense features have achieved state-of-the-art results. Stereo
vision, which provides an additional perspective on the object, can help reduce
pose ambiguity and occlusion. Moreover, stereo can directly infer the distance
of an object, while mono-vision requires internalized knowledge of the object's
size. To extend the state-of-the-art in 6D object pose estimation to stereo, we
created a BOP compatible stereo version of the YCB-V dataset. Our method
outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo
vision and can easily be adopted for other dense feature-based algorithms.
\\ ( https://arxiv.org/abs/2402.05610 ,  13357kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05636 (*cross-listing*)
Date: Thu, 8 Feb 2024 12:47:57 GMT   (1279kb)

Title: The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on
  GitHub Copilot within Coporate Environment
Authors: Sayan Chatterjee, Ching Louis Liu, Gareth Rowland, Tim Hogarth
Categories: cs.SE cs.AI
Comments: 16 pages, 4 figures. in proceeding for 10th International Conference
  on Software Engineering (SEC 2024)
\\
  The increasing popularity of AI, particularly Large Language Models (LLMs),
has significantly impacted various domains, including Software Engineering.
This study explores the integration of AI tools in software engineering
practices within a large organization. We focus on ANZ Bank, which employs over
5000 engineers covering all aspects of the software development life cycle.
This paper details an experiment conducted using GitHub Copilot, a notable AI
tool, within a controlled environment to evaluate its effectiveness in
real-world engineering tasks. Additionally, this paper shares initial findings
on the productivity improvements observed after GitHub Copilot was adopted on a
large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment
with GitHub Copilot included two weeks of preparation and four weeks of active
testing. The study evaluated participant sentiment and the tool's impact on
productivity, code quality, and security. Initially, participants used GitHub
Copilot for proposed use-cases, with their feedback gathered through regular
surveys. In the second phase, they were divided into Control and Copilot
groups, each tackling the same Python challenges, and their experiences were
again surveyed. Results showed a notable boost in productivity and code quality
with GitHub Copilot, though its impact on code security remained inconclusive.
Participant responses were overall positive, confirming GitHub Copilot's
effectiveness in large-scale software engineering environments. Early data from
1000 engineers also indicated a significant increase in productivity and job
satisfaction.
\\ ( https://arxiv.org/abs/2402.05636 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05650 (*cross-listing*)
Date: Thu, 8 Feb 2024 13:07:31 GMT   (1239kb,D)

Title: Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation
  of LLM-Supported SE Tasks
Authors: Wei Wang, Huilong Ning, Gaowei Zhang, Libo Liu and Yi Wang
Categories: cs.SE cs.AI
Comments: The paper has been accepted by ACM International Conference on the
  Foundations of Software Engineering (FSE 2024)
\\
  Recently, large language models (LLM) based generative AI has been gaining
momentum for their impressive high-quality performances in multiple domains,
particularly after the release of the ChatGPT. Many believe that they have the
potential to perform general-purpose problem-solving in software development
and replace human software developers. Nevertheless, there are in a lack of
serious investigation into the capability of these LLM techniques in fulfilling
software development tasks. In a controlled 2 $\times$ 2 between-subject
experiment with 109 participants, we examined whether and to what degree
working with ChatGPT was helpful in the coding task and typical software
development task and how people work with ChatGPT. We found that while ChatGPT
performed well in solving simple coding problems, its performance in supporting
typical software development tasks was not that good. We also observed the
interactions between participants and ChatGPT and found the relations between
the interactions and the outcomes. Our study thus provides first-hand insights
into using ChatGPT to fulfill software engineering tasks with real-world
developers and motivates the need for novel interaction mechanisms that help
developers effectively work with large language models to achieve desired
outcomes.
\\ ( https://arxiv.org/abs/2402.05650 ,  1239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05668 (*cross-listing*)
Date: Thu, 8 Feb 2024 13:42:50 GMT   (2555kb,D)

Title: Comprehensive Assessment of Jailbreak Attacks Against LLMs
Authors: Junjie Chu and Yugeng Liu and Ziqing Yang and Xinyue Shen and Michael
  Backes and Yang Zhang
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: 18 pages, 12 figures
\\
  Misuse of the Large Language Models (LLMs) has raised widespread concern. To
address this issue, safeguards have been taken to ensure that LLMs align with
social ethics. However, recent findings have revealed an unsettling
vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By
applying techniques, such as employing role-playing scenarios, adversarial
examples, or subtle subversion of safety objectives as a prompt, LLMs can
produce an inappropriate or even harmful response. While researchers have
studied several categories of jailbreak attacks, they have done so in
isolation. To fill this gap, we present the first large-scale measurement of
various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak
methods from four categories, 160 questions from 16 violation categories, and
six popular LLMs. Our extensive experimental results demonstrate that the
optimized jailbreak prompts consistently achieve the highest attack success
rates, as well as exhibit robustness across different LLMs. Some jailbreak
prompt datasets, available from the Internet, can also achieve high attack
success rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the
claims from many organizations regarding the coverage of violation categories
in their policies, the attack success rates from these categories remain high,
indicating the challenges of effectively aligning LLM policies and the ability
to counter jailbreak attacks. We also discuss the trade-off between the attack
performance and efficiency, as well as show that the transferability of the
jailbreak prompts is still viable, becoming an option for black-box models.
Overall, our research highlights the necessity of evaluating different
jailbreak methods. We hope our study can provide insights for future research
on jailbreak attacks and serve as a benchmark tool for evaluating them for
practitioners.
\\ ( https://arxiv.org/abs/2402.05668 ,  2555kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05703 (*cross-listing*)
Date: Thu, 8 Feb 2024 14:27:34 GMT   (6262kb,D)

Title: Offline Risk-sensitive RL with Partial Observability to Enhance
  Performance in Human-Robot Teaming
Authors: Giorgio Angelotti, Caroline P. C. Chanel, Adam H. M. Pinto, Christophe
  Lounis, Corentin Chauffaut, Nicolas Drougard
Categories: cs.MA cs.AI cs.HC cs.LG cs.RO
Comments: Accepted as a full paper at AAMAS 2024
\\
  The integration of physiological computing into mixed-initiative human-robot
interaction systems offers valuable advantages in autonomous task allocation by
incorporating real-time features as human state observations into the
decision-making system. This approach may alleviate the cognitive load on human
operators by intelligently allocating mission tasks between agents.
Nevertheless, accommodating a diverse pool of human participants with varying
physiological and behavioral measurements presents a substantial challenge. To
address this, resorting to a probabilistic framework becomes necessary, given
the inherent uncertainty and partial observability on the human's state. Recent
research suggests to learn a Partially Observable Markov Decision Process
(POMDP) model from a data set of previously collected experiences that can be
solved using Offline Reinforcement Learning (ORL) methods. In the present work,
we not only highlight the potential of partially observable representations and
physiological measurements to improve human operator state estimation and
performance, but also enhance the overall mission effectiveness of a
human-robot team. Importantly, as the fixed data set may not contain enough
information to fully represent complex stochastic processes, we propose a
method to incorporate model uncertainty, thus enabling risk-sensitive
sequential decision-making. Experiments were conducted with a group of
twenty-six human participants within a simulated robot teleoperation
environment, yielding empirical evidence of the method's efficacy. The obtained
adaptive task allocation policy led to statistically significant higher scores
than the one that was used to collect the data set, allowing for generalization
across diverse participants also taking into account risk-sensitive metrics.
\\ ( https://arxiv.org/abs/2402.05703 ,  6262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05712 (*cross-listing*)
Date: Thu, 8 Feb 2024 14:39:16 GMT   (9938kb,D)

Title: DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion
  Transformer
Authors: Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, Zhen
  Lei
Categories: cs.CV cs.AI
Comments: 9 pages, 5 figures. Code is avalable at
  https://github.com/theEricMa/DiffSpeaker
\\
  Speech-driven 3D facial animation is important for many multimedia
applications. Recent work has shown promise in using either Diffusion models or
Transformer architectures for this task. However, their mere aggregation does
not lead to improved performance. We suspect this is due to a shortage of
paired audio-4D data, which is crucial for the Transformer to effectively
perform as a denoiser within the Diffusion framework. To tackle this issue, we
present DiffSpeaker, a Transformer-based network equipped with novel biased
conditional attention modules. These modules serve as substitutes for the
traditional self/cross-attention in standard Transformers, incorporating
thoughtfully designed biases that steer the attention mechanisms to concentrate
on both the relevant task-specific and diffusion-related conditions. We also
explore the trade-off between accurate lip synchronization and non-verbal
facial expressions within the Diffusion paradigm. Experiments show our model
not only achieves state-of-the-art performance on existing benchmarks, but also
fast inference speed owing to its ability to generate facial motions in
parallel.
\\ ( https://arxiv.org/abs/2402.05712 ,  9938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05741 (*cross-listing*)
Date: Thu, 8 Feb 2024 15:19:50 GMT   (2277kb,D)

Title: Real-World Robot Applications of Foundation Models: A Review
Authors: Kento Kawaharazuka, Tatsuya Matsushima, Andrew Gambardella, Jiaxian
  Guo, Chris Paxton, Andy Zeng
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  Recent developments in foundation models, like Large Language Models (LLMs)
and Vision-Language Models (VLMs), trained on extensive data, facilitate
flexible application across different tasks and modalities. Their impact spans
various fields, including healthcare, education, and robotics. This paper
provides an overview of the practical application of foundation models in
real-world robotics, with a primary emphasis on the replacement of specific
components within existing robot systems. The summary encompasses the
perspective of input-output relationships in foundation models, as well as
their role in perception, motion planning, and control within the field of
robotics. This paper concludes with a discussion of future challenges and
implications for practical robot applications.
\\ ( https://arxiv.org/abs/2402.05741 ,  2277kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05747 (*cross-listing*)
Date: Thu, 8 Feb 2024 15:32:22 GMT   (29340kb,D)

Title: Jacquard V2: Refining Datasets using the Human In the Loop Data
  Correction Method
Authors: Qiuhao Li and Shenghai Yuan
Categories: cs.CV cs.AI
\\
  In the context of rapid advancements in industrial automation, vision-based
robotic grasping plays an increasingly crucial role. In order to enhance visual
recognition accuracy, the utilization of large-scale datasets is imperative for
training models to acquire implicit knowledge related to the handling of
various objects. Creating datasets from scratch is a time and labor-intensive
process. Moreover, existing datasets often contain errors due to automated
annotations aimed at expediency, making the improvement of these datasets a
substantial research challenge. Consequently, several issues have been
identified in the annotation of grasp bounding boxes within the popular
Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance
dataset quality. This approach relies on backbone deep learning networks to
predict object positions and orientations for robotic grasping. Predictions
with Intersection over Union (IOU) values below 0.2 undergo an assessment by
human operators. After their evaluation, the data is categorized into False
Negatives(FN) and True Negatives(TN). FN are then subcategorized into either
missing annotations or catastrophic labeling errors. Images lacking labels are
augmented with valid grasp bounding box information, whereas images afflicted
by catastrophic labeling errors are completely removed. The open-source tool
Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading
to the removal of 2,884 images and the incorporation of ground truth
information for 30,292 images. The enhanced dataset, named the Jacquard V2
Grasping Dataset, served as the training data for a range of neural networks.
\\ ( https://arxiv.org/abs/2402.05747 ,  29340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05804 (*cross-listing*)
Date: Thu, 8 Feb 2024 16:41:41 GMT   (42732kb,D)

Title: InkSight: Offline-to-Online Handwriting Conversion by Learning to Read
  and Write
Authors: Blagoj Mitrevski, Arina Rak, Julian Schnitzler, Chengkun Li, Andrii
  Maksai, Jesse Berent, Claudiu Musat
Categories: cs.CV cs.AI
\\
  Digital note-taking is gaining popularity, offering a durable, editable, and
easily indexable way of storing notes in the vectorized form, known as digital
ink. However, a substantial gap remains between this way of note-taking and
traditional pen-and-paper note-taking, a practice still favored by a vast
majority. Our work, InkSight, aims to bridge the gap by empowering physical
note-takers to effortlessly convert their work (offline handwriting) to digital
ink (online handwriting), a process we refer to as Derendering. Prior research
on the topic has focused on the geometric properties of images, resulting in
limited generalization beyond their training domains. Our approach combines
reading and writing priors, allowing training a model in the absence of large
amounts of paired samples, which are difficult to obtain. To our knowledge,
this is the first work that effectively derenders handwritten text in arbitrary
photos with diverse visual characteristics and backgrounds. Furthermore, it
generalizes beyond its training domain into simple sketches. Our human
evaluation reveals that 87% of the samples produced by our model on the
challenging HierText dataset are considered as a valid tracing of the input
image and 67% look like a pen trajectory traced by a human.
\\ ( https://arxiv.org/abs/2402.05804 ,  42732kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05809 (*cross-listing*)
Date: Thu, 8 Feb 2024 16:47:43 GMT   (83922kb,D)

Title: You Only Need One Color Space: An Efficient Network for Low-light Image
  Enhancement
Authors: Yixu Feng, Cheng Zhang, Pei Wang, Peng Wu, Qingsen Yan, Yanning Zhang
Categories: cs.CV cs.AI
\\
  Low-Light Image Enhancement (LLIE) task tends to restore the details and
visual information from corrupted low-light images. Most existing methods learn
the mapping function between low/normal-light images by Deep Neural Networks
(DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves
amplifying image signals, and applying these color spaces to low-light images
with a low signal-to-noise ratio can introduce sensitivity and instability into
the enhancement process. Consequently, this results in the presence of color
artifacts and brightness artifacts in the enhanced images. To alleviate this
problem, we propose a novel trainable color space, named
Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color
from RGB channels to mitigate the instability during enhancement but also
adapts to low-light images in different illumination ranges due to the
trainable parameters. Further, we design a novel Color and Intensity Decoupling
Network (CIDNet) with two branches dedicated to processing the decoupled image
brightness and color in the HVI space. Within CIDNet, we introduce the
Lightweight Cross-Attention (LCA) module to facilitate interaction between
image structure and content information in both branches, while also
suppressing noise in low-light images. Finally, we conducted 22 quantitative
and qualitative experiments to show that the proposed CIDNet outperforms the
state-of-the-art methods on 11 datasets. The code will be available at
https://github.com/Fediory/HVI-CIDNet.
\\ ( https://arxiv.org/abs/2402.05809 ,  83922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05889 (*cross-listing*)
Date: Thu, 8 Feb 2024 18:27:22 GMT   (3462kb,D)

Title: CREMA: Multimodal Compositional Video Reasoning via Efficient Modular
  Adaptation and Fusion
Authors: Shoubin Yu, Jaehong Yoon, Mohit Bansal
Categories: cs.CV cs.AI cs.CL
Comments: project page: https://CREMA-VideoLLM.github.io/
\\
  Despite impressive advancements in multimodal compositional reasoning
approaches, they are still limited in their flexibility and efficiency by
processing fixed modality inputs while updating a lot of model parameters. This
paper tackles these critical challenges and proposes CREMA, an efficient and
modular modality-fusion framework for injecting any new modality into video
reasoning. We first augment multiple informative modalities (such as optical
flow, 3D point cloud, audio) from given videos without extra human annotation
by leveraging existing pre-trained models. Next, we introduce a query
transformer with multiple parameter-efficient modules associated with each
accessible modality. It projects diverse modality features to the LLM token
embedding space, allowing the model to integrate different data types for
response generation. Furthermore, we propose a fusion module designed to
compress multimodal queries, maintaining computational efficiency in the LLM
while combining additional modalities. We validate our method on video-3D,
video-audio, and video-language reasoning tasks and achieve better/equivalent
performance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and
SeViLA while using 96% fewer trainable parameters. We provide extensive
analyses of CREMA, including the impact of each modality on reasoning domains,
the design of the fusion module, and example visualizations.
\\ ( https://arxiv.org/abs/2402.05889 ,  3462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05902 (*cross-listing*)
Date: Thu, 8 Feb 2024 18:41:41 GMT   (1511kb)

Title: ClickSAM: Fine-tuning Segment Anything Model using click prompts for
  ultrasound image segmentation
Authors: Aimee Guo, Gace Fei, Hemanth Pasupuletic, and Jing Wang
Categories: cs.CV cs.AI physics.med-ph
Comments: 6 pages, 2 figures, SPIE Medical Imaging Conference 2024
\\
  The newly released Segment Anything Model (SAM) is a popular tool used in
image processing due to its superior segmentation accuracy, variety of input
prompts, training capabilities, and efficient model design. However, its
current model is trained on a diverse dataset not tailored to medical images,
particularly ultrasound images. Ultrasound images tend to have a lot of noise,
making it difficult to segment out important structures. In this project, we
developed ClickSAM, which fine-tunes the Segment Anything Model using click
prompts for ultrasound images. ClickSAM has two stages of training: the first
stage is trained on single-click prompts centered in the ground-truth contours,
and the second stage focuses on improving the model performance through
additional positive and negative click prompts. By comparing the first stage
predictions to the ground-truth masks, true positive, false positive, and false
negative segments are calculated. Positive clicks are generated using the true
positive and false negative segments, and negative clicks are generated using
the false positive segments. The Centroidal Voronoi Tessellation algorithm is
then employed to collect positive and negative click prompts in each segment
that are used to enhance the model performance during the second stage of
training. With click-train methods, ClickSAM exhibits superior performance
compared to other existing models for ultrasound image segmentation.
\\ ( https://arxiv.org/abs/2402.05902 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05932 (*cross-listing*)
Date: Thu, 8 Feb 2024 18:59:03 GMT   (6276kb,D)

Title: Driving Everywhere with Large Language Model Policy Adaptation
Authors: Boyi Li and Yue Wang and Jiageng Mao and Boris Ivanovic and Sushant
  Veer and Karen Leung and Marco Pavone
Categories: cs.RO cs.AI cs.CL
\\
  Adapting driving behavior to new environments, customs, and laws is a
long-standing problem in autonomous driving, precluding the widespread
deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a
simple yet powerful tool that enables human drivers and autonomous vehicles
alike to drive everywhere by adapting their tasks and motion plans to traffic
rules in new locations. LLaDA achieves this by leveraging the impressive
zero-shot generalizability of large language models (LLMs) in interpreting the
traffic rules in the local driver handbook. Through an extensive user study, we
show that LLaDA's instructions are useful in disambiguating in-the-wild
unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion
planning policies in real-world datasets; LLaDA outperforms baseline planning
approaches on all our metrics. Please check our website for more details:
https://boyiliee.github.io/llada.
\\ ( https://arxiv.org/abs/2402.05932 ,  6276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05935 (*cross-listing*)
Date: Thu, 8 Feb 2024 18:59:48 GMT   (12924kb,D)

Title: SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large
  Language Models
Authors: Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng
  Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi
  Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Code and models are released at
  https://github.com/Alpha-VLLM/LLaMA2-Accessory
\\
  We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
\\ ( https://arxiv.org/abs/2402.05935 ,  12924kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05122 (*cross-listing*)
Date: Sun, 4 Feb 2024 05:01:38 GMT   (324kb)

Title: History of generative Artificial Intelligence (AI) chatbots: past,
  present, and future development
Authors: Md. Al-Amin, Mohammad Shazed Ali, Abdus Salam, Arif Khan, Ashraf Ali,
  Ahsan Ullah, Md Nur Alam, Shamsul Kabir Chowdhury
Categories: cs.GL cs.CL cs.HC
\\
  This research provides an in-depth comprehensive review of the progress of
chatbot technology over time, from the initial basic systems relying on rules
to today's advanced conversational bots powered by artificial intelligence.
Spanning many decades, the paper explores the major milestones, innovations,
and paradigm shifts that have driven the evolution of chatbots. Looking back at
the very basic statistical model in 1906 via the early chatbots, such as ELIZA
and ALICE in the 1960s and 1970s, the study traces key innovations leading to
today's advanced conversational agents, such as ChatGPT and Google Bard. The
study synthesizes insights from academic literature and industry sources to
highlight crucial milestones, including the introduction of Turing tests,
influential projects such as CALO, and recent transformer-based models. Tracing
the path forward, the paper highlights how natural language processing and
machine learning have been integrated into modern chatbots for more
sophisticated capabilities. This chronological survey of the chatbot landscape
provides a holistic reference to understand the technological and historical
factors propelling conversational AI. By synthesizing learnings from this
historical analysis, the research offers important context about the
developmental trajectory of chatbots and their immense future potential across
various field of application which could be the potential take ways for the
respective research community and stakeholders.
\\ ( https://arxiv.org/abs/2402.05122 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05195 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:07:10 GMT   (25530kb,D)

Title: $\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion
  Models by Leveraging CLIP Latent Space
Authors: Maitreya Patel, Sangmin Jung, Chitta Baral, Yezhou Yang
Categories: cs.CV cs.CL
Comments: Project page: https://eclipse-t2i.github.io/Lambda-ECLIPSE/
\\
  Despite the recent advances in personalized text-to-image (P-T2I) generative
models, subject-driven T2I remains challenging. The primary bottlenecks include
1) Intensive training resource requirements, 2) Hyper-parameter sensitivity
leading to inconsistent outputs, and 3) Balancing the intricacies of novel
visual concept and composition alignment. We start by re-iterating the core
philosophy of T2I diffusion models to address the above limitations.
Predominantly, contemporary subject-driven T2I approaches hinge on Latent
Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention
layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the
latent space of these diffusion models significantly escalates resource
demands, leading to inconsistent results and necessitating numerous iterations
for a single desired image. Recently, ECLIPSE has demonstrated a more
resource-efficient pathway for training UnCLIP-based T2I models, circumventing
the need for diffusion text-to-image priors. Building on this, we introduce
$\lambda$-ECLIPSE. Our method illustrates that effective P-T2I does not
necessarily depend on the latent space of diffusion models. $\lambda$-ECLIPSE
achieves single, multi-subject, and edge-guided T2I personalization with just
34M parameters and is trained on a mere 74 GPU hours using 1.6M image-text
interleaved data. Through extensive experiments, we also establish that
$\lambda$-ECLIPSE surpasses existing baselines in composition alignment while
preserving concept alignment performance, even with significantly lower
resource utilization.
\\ ( https://arxiv.org/abs/2402.05195 ,  25530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05318 (*cross-listing*)
Date: Wed, 7 Feb 2024 23:39:40 GMT   (181kb,D)

Title: Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs
Authors: Dipankar Sarkar
Categories: cs.IR cs.CL cs.LG
\\
  Information retrieval is a rapidly evolving field of information retrieval,
which is characterized by a continuous refinement of techniques and
technologies, from basic hyperlink-based navigation to sophisticated
algorithm-driven search engines. This paper aims to provide a comprehensive
overview of the evolution of Information Retrieval Technology, with a
particular focus on the role of Large Language Models (LLMs) in bridging the
gap between traditional search methods and the emerging paradigm of answer
retrieval. The integration of LLMs in the realms of response retrieval and
indexing signifies a paradigm shift in how users interact with information
systems. This paradigm shift is driven by the integration of large language
models (LLMs) like GPT-4, which are capable of understanding and generating
human-like text, thus enabling them to provide more direct and contextually
relevant answers to user queries. Through this exploration, we seek to
illuminate the technological milestones that have shaped this journey and the
potential future directions in this rapidly changing field.
\\ ( https://arxiv.org/abs/2402.05318 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05779 (*cross-listing*)
Date: Thu, 8 Feb 2024 16:11:23 GMT   (5817kb,D)

Title: Examining Gender and Racial Bias in Large Vision-Language Models Using a
  Novel Dataset of Parallel Images
Authors: Kathleen C. Fraser and Svetlana Kiritchenko
Categories: cs.CY cs.CL cs.CV
Comments: To appear at EACL 2024
\\
  Following on recent advances in large language models (LLMs) and subsequent
chat models, a new wave of large vision-language models (LVLMs) has emerged.
Such models can incorporate images as input in addition to text, and perform
tasks such as visual question answering, image captioning, story generation,
etc. Here, we examine potential gender and racial biases in such systems, based
on the perceived characteristics of the people in the input images. To
accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday
Scenarios). The PAIRS dataset contains sets of AI-generated images of people,
such that the images are highly similar in terms of background and visual
content, but differ along the dimensions of gender (man, woman) and race
(Black, white). By querying the LVLMs with such images, we observe significant
differences in the responses according to the perceived gender or race of the
person depicted.
\\ ( https://arxiv.org/abs/2402.05779 ,  5817kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05819 (*cross-listing*)
Date: Thu, 8 Feb 2024 16:55:21 GMT   (778kb,D)

Title: Integrating Self-supervised Speech Model with Pseudo Word-level Targets
  from Visually-grounded Speech Model
Authors: Hung-Chieh Fang, Nai-Xuan Ye, Yi-Jen Shih, Puyuan Peng, Hsuan-Fu Wang,
  Layne Berry, Hung-yi Lee, David Harwath
Categories: eess.AS cs.CL cs.LG
Comments: Accepted to ICASSP 2024 workshop on Self-supervision in Audio,
  Speech, and Beyond (SASB)
\\
  Recent advances in self-supervised speech models have shown significant
improvement in many downstream tasks. However, these models predominantly
centered on frame-level training objectives, which can fall short in spoken
language understanding tasks that require semantic comprehension. Existing
works often rely on additional speech-text data as intermediate targets, which
is costly in the real-world setting. To address this challenge, we propose
Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level
targets into the training process, where the targets are derived from a
visually-ground speech model, notably eliminating the need for speech-text
paired data. Our experimental results on four spoken language understanding
(SLU) benchmarks suggest the superiority of our model in capturing semantic
information.
\\ ( https://arxiv.org/abs/2402.05819 ,  778kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05114 (*cross-listing*)
Date: Wed, 10 Jan 2024 10:22:25 GMT   (72kb)

Title: A Light-weight and Unsupervised Method for Near Real-time Behavioral
  Analysis using Operational Data Measurement
Authors: Tom Richard Vargis, Siavash Ghiasvand
Categories: cs.DC cs.LG
\\
  Monitoring the status of large computing systems is essential to identify
unexpected behavior and improve their performance and uptime. However, due to
the large-scale and distributed design of such computing systems as well as a
large number of monitoring parameters, automated monitoring methods should be
applied. Such automatic monitoring methods should also have the ability to
adapt themselves to the continuous changes in the computing system. In
addition, they should be able to identify behavioral anomalies in useful time,
to perform appropriate reactions. This work proposes a general lightweight and
unsupervised method for near real-time anomaly detection using operational data
measurement on large computing systems. The proposed model requires as little
as 4 hours of data and 50 epochs for each training process to accurately
resemble the behavioral pattern of computing systems.
\\ ( https://arxiv.org/abs/2402.05114 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05137 (*cross-listing*)
Date: Tue, 6 Feb 2024 19:00:00 GMT   (4863kb,D)

Title: LtU-ILI: An All-in-One Framework for Implicit Inference in Astrophysics
  and Cosmology
Authors: Matthew Ho, Deaglan J. Bartlett, Nicolas Chartier, Carolina
  Cuesta-Lazaro, Simon Ding, Axel Lapel, Pablo Lemos, Christopher C. Lovell, T.
  Lucas Makinen, Chirag Modi, Viraj Pandya, Shivam Pandey, Lucia A. Perez,
  Benjamin Wandelt, Greg L. Bryan
Categories: astro-ph.IM astro-ph.CO astro-ph.GA cs.LG
Comments: 20 pages, 10 figures, submitted to the Open Journal of Astrophysics.
  Code available at https://github.com/maho3/ltu-ili
\\
  This paper presents the Learning the Universe Implicit Likelihood Inference
(LtU-ILI) pipeline, a codebase for rapid, user-friendly, and cutting-edge
machine learning (ML) inference in astrophysics and cosmology. The pipeline
includes software for implementing various neural architectures, training
schema, priors, and density estimators in a manner easily adaptable to any
research workflow. It includes comprehensive validation metrics to assess
posterior estimate coverage, enhancing the reliability of inferred results.
Additionally, the pipeline is easily parallelizable, designed for efficient
exploration of modeling hyperparameters. To demonstrate its capabilities, we
present real applications across a range of astrophysics and cosmology
problems, such as: estimating galaxy cluster masses from X-ray photometry;
inferring cosmology from matter power spectra and halo point clouds;
characterising progenitors in gravitational wave signals; capturing physical
dust parameters from galaxy colors and luminosities; and establishing
properties of semi-analytic models of galaxy formation. We also include
exhaustive benchmarking and comparisons of all implemented methods as well as
discussions about the challenges and pitfalls of ML inference in astronomical
sciences. All code and examples are made publicly available at
https://github.com/maho3/ltu-ili.
\\ ( https://arxiv.org/abs/2402.05137 ,  4863kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05141 (*cross-listing*)
Date: Tue, 6 Feb 2024 21:44:07 GMT   (494kb,D)

Title: Tensor Completion via Integer Optimization
Authors: Xin Chen, Sukanya Kudva, Yongzheng Dai, Anil Aswani, Chen Chen
Categories: math.OC cs.LG
\\
  The main challenge with the tensor completion problem is a fundamental
tension between computation power and the information-theoretic sample
complexity rate. Past approaches either achieve the information-theoretic rate
but lack practical algorithms to compute the corresponding solution, or have
polynomial-time algorithms that require an exponentially-larger number of
samples for low estimation error. This paper develops a novel tensor completion
algorithm that resolves this tension by achieving both provable convergence (in
numerical tolerance) in a linear number of oracle steps and the
information-theoretic rate. Our approach formulates tensor completion as a
convex optimization problem constrained using a gauge-based tensor norm, which
is defined in a way that allows the use of integer linear optimization to solve
linear separation problems over the unit-ball in this new norm. Adaptations
based on this insight are incorporated into a Frank-Wolfe variant to build our
algorithm. We show our algorithm scales-well using numerical experiments on
tensors with up to ten million entries.
\\ ( https://arxiv.org/abs/2402.05141 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05155 (*cross-listing*)
Date: Wed, 7 Feb 2024 16:14:04 GMT   (97kb,D)

Title: Non-convergence to global minimizers for Adam and stochastic gradient
  descent optimization and constructions of local minimizers in the training of
  artificial neural networks
Authors: Arnulf Jentzen, Adrian Riekert
Categories: math.OC cs.LG
Comments: 36 pages
\\
  Stochastic gradient descent (SGD) optimization methods such as the plain
vanilla SGD method and the popular Adam optimizer are nowadays the method of
choice in the training of artificial neural networks (ANNs). Despite the
remarkable success of SGD methods in the ANN training in numerical simulations,
it remains in essentially all practical relevant scenarios an open problem to
rigorously explain why SGD methods seem to succeed to train ANNs. In
particular, in most practically relevant supervised learning problems, it seems
that SGD methods do with high probability not converge to global minimizers in
the optimization landscape of the ANN training problem. Nevertheless, it
remains an open problem of research to disprove the convergence of SGD methods
to global minimizers. In this work we solve this research problem in the
situation of shallow ANNs with the rectified linear unit (ReLU) and related
activations with the standard mean square error loss by disproving in the
training of such ANNs that SGD methods (such as the plain vanilla SGD, the
momentum SGD, the AdaGrad, the RMSprop, and the Adam optimizers) can find a
global minimizer with high probability. Even stronger, we reveal in the
training of such ANNs that SGD methods do with high probability fail to
converge to global minimizers in the optimization landscape. The findings of
this work do, however, not disprove that SGD methods succeed to train ANNs
since they do not exclude the possibility that SGD methods find good local
minimizers whose risk values are close to the risk values of the global
minimizers. In this context, another key contribution of this work is to
establish the existence of a hierarchical structure of local minimizers with
distinct risk values in the optimization landscape of ANN training problems
with ReLU and related activations.
\\ ( https://arxiv.org/abs/2402.05155 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05176 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:00:02 GMT   (7183kb,D)

Title: cecilia: A Machine Learning-Based Pipeline for Measuring Metal
  Abundances of Helium-rich Polluted White Dwarfs
Authors: M. Badenas-Agusti, J. Via\~na, A. Vanderburg, S. Blouin, P. Dufour, S.
  Xu, L. Sha
Categories: astro-ph.IM astro-ph.EP astro-ph.SR cs.LG
Comments: 28 pages, 16 figures, 5 tables. Accepted for publication in MNRAS
\\
  Over the past several decades, conventional spectral analysis techniques of
polluted white dwarfs have become powerful tools to learn about the geology and
chemistry of extrasolar bodies. Despite their proven capabilities and extensive
legacy of scientific discoveries, these techniques are however still limited by
their manual, time-intensive, and iterative nature. As a result, they are
susceptible to human errors and are difficult to scale up to population-wide
studies of metal pollution. This paper seeks to address this problem by
presenting cecilia, the first Machine Learning (ML)-powered spectral modeling
code designed to measure the metal abundances of intermediate-temperature
(10,000$\leq T_{\rm eff} \leq$20,000 K), Helium-rich polluted white dwarfs.
Trained with more than 22,000 randomly drawn atmosphere models and stellar
parameters, our pipeline aims to overcome the limitations of classical methods
by replacing the generation of synthetic spectra from computationally expensive
codes and uniformly spaced model grids, with a fast, automated, and efficient
neural-network-based interpolator. More specifically, cecilia combines
state-of-the-art atmosphere models, powerful artificial intelligence tools, and
robust statistical techniques to rapidly generate synthetic spectra of polluted
white dwarfs in high-dimensional space, and enable accurate ($\lesssim$0.1 dex)
and simultaneous measurements of 14 stellar parameters -- including 11
elemental abundances -- from real spectroscopic observations. As massively
multiplexed astronomical surveys begin scientific operations, cecilia's
performance has the potential to unlock large-scale studies of extrasolar
geochemistry and propel the field of white dwarf science into the era of Big
Data. In doing so, we aspire to uncover new statistical insights that were
previously impractical with traditional white dwarf characterisation
techniques.
\\ ( https://arxiv.org/abs/2402.05176 ,  7183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05187 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:01:06 GMT   (3641kb,D)

Title: Meta-learning the mirror map in policy mirror descent
Authors: Carlo Alfano, Sebastian Towers, Silvia Sapora, Chris Lu, Patrick
  Rebeschini
Categories: stat.ML cs.LG math.OC
\\
  Policy Mirror Descent (PMD) is a popular framework in reinforcement learning,
serving as a unifying perspective that encompasses numerous algorithms. These
algorithms are derived through the selection of a mirror map and enjoy
finite-time convergence guarantees. Despite its popularity, the exploration of
PMD's full potential is limited, with the majority of research focusing on a
particular mirror map -- namely, the negative entropy -- which gives rise to
the renowned Natural Policy Gradient (NPG) method. It remains uncertain from
existing theoretical studies whether the choice of mirror map significantly
influences PMD's efficacy. In our work, we conduct empirical investigations to
show that the conventional mirror map choice (NPG) often yields
less-than-optimal outcomes across several standard benchmark environments. By
applying a meta-learning approach, we identify more efficient mirror maps that
enhance performance, both on average and in terms of best performance achieved
along the training trajectory. We analyze the characteristics of these learned
mirror maps and reveal shared traits among certain settings. Our results
suggest that mirror maps have the potential to be adaptable across various
environments, raising questions about how to best match a mirror map to an
environment's structure and characteristics.
\\ ( https://arxiv.org/abs/2402.05187 ,  3641kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05193 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:05:27 GMT   (43665kb,D)

Title: JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible
  Two-phase Flows
Authors: Deniz A. Bezgin, Aaron B. Buhendwa, Nikolaus A. Adams
Categories: physics.flu-dyn cs.CE cs.LG
\\
  In our effort to facilitate machine learning-assisted computational fluid
dynamics (CFD), we introduce the second iteration of JAX-Fluids. JAX-Fluids is
a Python-based fully-differentiable CFD solver designed for compressible
single- and two-phase flows. In this work, the first version is extended to
incorporate high-performance computing (HPC) capabilities. We introduce a
parallelization strategy utilizing JAX primitive operations that scales
efficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024
TPU v3 cores) HPC systems. We further demonstrate the stable parallel
computation of automatic differentiation gradients across extended integration
trajectories. The new code version offers enhanced two-phase flow modeling
capabilities. In particular, a five-equation diffuse-interface model is
incorporated which complements the level-set sharp-interface model. Additional
algorithmic improvements include positivity-preserving limiters for increased
robustness, support for stretched Cartesian meshes, refactored I/O handling,
comprehensive post-processing routines, and an updated list of state-of-the-art
high-order numerical discretization schemes. We verify newly added numerical
models by showcasing simulation results for single- and two-phase flows,
including turbulent boundary layer and channel flows, air-helium shock bubble
interactions, and air-water shock drop interactions.
\\ ( https://arxiv.org/abs/2402.05193 ,  43665kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05210 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:35:09 GMT   (5962kb,D)

Title: Anatomically-Controllable Medical Image Generation with
  Segmentation-Guided Diffusion Models
Authors: Nicholas Konz, Yuwen Chen, Haoyu Dong, Maciej A. Mazurowski
Categories: eess.IV cs.CV cs.LG stat.ML
Comments: Code and synthetic dataset:
  https://github.com/mazurowski-lab/segmentation-guided-diffusion
\\
  Diffusion models have enabled remarkably high-quality medical image
generation, which can help mitigate the expenses of acquiring and annotating
new images by supplementing small or imbalanced datasets, along with other
applications. However, these are hampered by the challenge of enforcing global
anatomical realism in generated images. To this end, we propose a diffusion
model for anatomically-controlled medical image generation. Our model follows a
multi-class anatomical segmentation mask at each sampling step and incorporates
a \textit{random mask ablation} training algorithm, to enable conditioning on a
selected combination of anatomical constraints while allowing flexibility in
other anatomical areas. This also improves the network's learning of anatomical
realism for the completely unconditional (unconstrained generation) case.
Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets
demonstrates superior anatomical realism and input mask faithfulness over
state-of-the-art models. We also offer an accessible codebase and release a
dataset of generated paired breast MRIs. Our approach facilitates diverse
applications, including pre-registered image generation, counterfactual
scenarios, and others.
\\ ( https://arxiv.org/abs/2402.05210 ,  5962kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05218 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:51:13 GMT   (1783kb,D)

Title: Self-calibrated convolution towards glioma segmentation
Authors: Felipe C. R. Salvagnini and Gerson O. Barbosa and Alexandre X. Falcao
  and Cid A. N. Santos
Categories: eess.IV cs.CV cs.LG
DOI: 10.1109/SIPAIM56729.2023.10373517
\\
  Accurate brain tumor segmentation in the early stages of the disease is
crucial for the treatment's effectiveness, avoiding exhaustive visual
inspection of a qualified specialist on 3D MR brain images of multiple
protocols (e.g., T1, T2, T2-FLAIR, T1-Gd). Several networks exist for Glioma
segmentation, being nnU-Net one of the best. In this work, we evaluate
self-calibrated convolutions in different parts of the nnU-Net network to
demonstrate that self-calibrated modules in skip connections can significantly
improve the enhanced-tumor and tumor-core segmentation accuracy while
preserving the wholetumor segmentation accuracy.
\\ ( https://arxiv.org/abs/2402.05218 ,  1783kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05220 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:52:35 GMT   (513kb,D)

Title: On Parameter Estimation in Deviated Gaussian Mixture of Experts
Authors: Huy Nguyen and Khai Nguyen and Nhat Ho
Categories: stat.ML cs.LG
Comments: 34 pages, 3 figures
\\
  We consider the parameter estimation problem in the deviated Gaussian mixture
of experts in which the data are generated from $(1 - \lambda^{\ast}) g_0(Y|
X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast}
f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$, where $X, Y$ are
respectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a
known function, $\lambda^{\ast} \in [0, 1]$ is true but unknown mixing
proportion, and $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$
for $1 \leq i \leq k^{\ast}$ are unknown parameters of the Gaussian mixture of
experts. This problem arises from the goodness-of-fit test when we would like
to test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or
they are generated from the whole mixture (alternative hypothesis). Based on
the algebraic structure of the expert functions and the distinguishability
between $g_0$ and the mixture part, we construct novel Voronoi-based loss
functions to capture the convergence rates of maximum likelihood estimation
(MLE) for our models. We further demonstrate that our proposed loss functions
characterize the local convergence rates of parameter estimation more
accurately than the generalized Wasserstein, a loss function being commonly
used for estimating parameters in the Gaussian mixture of experts.
\\ ( https://arxiv.org/abs/2402.05220 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05330 (*cross-listing*)
Date: Thu, 8 Feb 2024 00:12:18 GMT   (7204kb,D)

Title: Classification under Nuisance Parameters and Generalized Label Shift in
  Likelihood-Free Inference
Authors: Luca Masserano, Alex Shen, Michele Doro, Tommaso Dorigo, Rafael
  Izbicki, Ann B. Lee
Categories: stat.ML cs.LG
Comments: 25 pages, 18 figures
\\
  An open scientific challenge is how to classify events with reliable measures
of uncertainty, when we have a mechanistic model of the data-generating process
but the distribution over both labels and latent nuisance parameters is
different between train and target data. We refer to this type of
distributional shift as generalized label shift (GLS). Direct classification
using observed data $\mathbf{X}$ as covariates leads to biased predictions and
invalid uncertainty estimates of labels $Y$. We overcome these biases by
proposing a new method for robust uncertainty quantification that casts
classification as a hypothesis testing problem under nuisance parameters. The
key idea is to estimate the classifier's receiver operating characteristic
(ROC) across the entire nuisance parameter space, which allows us to devise
cutoffs that are invariant under GLS. Our method effectively endows a
pre-trained classifier with domain adaptation capabilities and returns valid
prediction sets while maintaining high power. We demonstrate its performance on
two challenging scientific problems in biology and astroparticle physics with
data from realistic mechanistic models.
\\ ( https://arxiv.org/abs/2402.05330 ,  7204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05372 (*cross-listing*)
Date: Thu, 8 Feb 2024 03:02:59 GMT   (8840kb,D)

Title: Reduced-order modeling of unsteady fluid flow using neural network
  ensembles
Authors: Rakesh Halder, Mohammadmehdi Ataei, Hesam Salehipour, Krzysztof
  Fidkowski, Kevin Maki
Categories: physics.flu-dyn cs.LG
\\
  The use of deep learning has become increasingly popular in reduced-order
models (ROMs) to obtain low-dimensional representations of full-order models.
Convolutional autoencoders (CAEs) are often used to this end as they are adept
at handling data that are spatially distributed, including solutions to partial
differential equations. When applied to unsteady physics problems, ROMs also
require a model for time-series prediction of the low-dimensional latent
variables. Long short-term memory (LSTM) networks, a type of recurrent neural
network useful for modeling sequential data, are frequently employed in
data-driven ROMs for autoregressive time-series prediction. When making
predictions at unseen design points over long time horizons, error propagation
is a frequently encountered issue, where errors made early on can compound over
time and lead to large inaccuracies. In this work, we propose using bagging, a
commonly used ensemble learning technique, to develop a fully data-driven ROM
framework referred to as the CAE-eLSTM ROM that uses CAEs for spatial
reconstruction of the full-order model and LSTM ensembles for time-series
prediction. When applied to two unsteady fluid dynamics problems, our results
show that the presented framework effectively reduces error propagation and
leads to more accurate time-series prediction of latent variables at unseen
points.
\\ ( https://arxiv.org/abs/2402.05372 ,  8840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05382 (*cross-listing*)
Date: Thu, 8 Feb 2024 03:46:32 GMT   (6938kb,D)

Title: Task-customized Masked AutoEncoder via Mixture of Cluster-conditional
  Experts
Authors: Zhili Liu, Kai Chen, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li,
  James T. Kwok
Categories: cs.CV cs.LG
Comments: Accepted by ICLR 2023
\\
  Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that
achieves promising results in model pre-training. However, when the various
downstream tasks have data distributions different from the pre-training data,
the semantically irrelevant pre-training information might result in negative
transfer, impeding MAE's scalability. To address this issue, we propose a novel
MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE),
which can be trained once but provides customized pre-training models for
diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE
trains each expert only with semantically relevant images by using
cluster-conditional gates. Thus, each downstream task can be allocated to its
customized model pre-trained with data most similar to the downstream data.
Experiments on a collection of 11 downstream tasks show that MoCE outperforms
the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art
self-supervised learning results on detection and segmentation.
\\ ( https://arxiv.org/abs/2402.05382 ,  6938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05417 (*cross-listing*)
Date: Thu, 8 Feb 2024 05:18:11 GMT   (333kb,D)

Title: Segmentation-free Connectionist Temporal Classification loss based OCR
  Model for Text Captcha Classification
Authors: Vaibhav Khatavkar, Makarand Velankar and Sneha Petkar
Categories: cs.CV cs.CR cs.LG
Comments: 17 pages, 5 figures
\\
  Captcha are widely used to secure systems from automatic responses by
distinguishing computer responses from human responses. Text, audio, video,
picture picture-based Optical Character Recognition (OCR) are used for creating
captcha. Text-based OCR captcha are the most often used captcha which faces
issues namely, complex and distorted contents. There are attempts to build
captcha detection and classification-based systems using machine learning and
neural networks, which need to be tuned for accuracy. The existing systems face
challenges in the recognition of distorted characters, handling variable-length
captcha and finding sequential dependencies in captcha. In this work, we
propose a segmentation-free OCR model for text captcha classification based on
the connectionist temporal classification loss technique. The proposed model is
trained and tested on a publicly available captcha dataset. The proposed model
gives 99.80\% character level accuracy, while 95\% word level accuracy. The
accuracy of the proposed model is compared with the state-of-the-art models and
proves to be effective. The variable length complex captcha can be thus
processed with the segmentation-free connectionist temporal classification loss
technique with dependencies which will be massively used in securing the
software systems.
\\ ( https://arxiv.org/abs/2402.05417 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05482 (*cross-listing*)
Date: Thu, 8 Feb 2024 08:23:33 GMT   (1381kb,D)

Title: A Non-Intrusive Neural Quality Assessment Model for Surface
  Electromyography Signals
Authors: Cho-Yuan Lee, Kuan-Chen Wang, Kai-Chun Liu, Xugang Lu, Ping-Chen Yeh,
  and Yu Tsao
Categories: eess.SP cs.LG
Comments: 5 pages, 4 figures
\\
  In practical scenarios involving the measurement of surface electromyography
(sEMG) in muscles, particularly those areas near the heart, one of the primary
sources of contamination is the presence of electrocardiogram (ECG) signals. To
assess the quality of real-world sEMG data more effectively, this study
proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG
signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an
end-to-end training strategy. Our experimental framework utilizes real-world
sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive
Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database,
respectively. The experimental results demonstrate the superiority of QASE-net
over the previous assessment model, exhibiting significantly reduced prediction
errors and notably higher linear correlations with the ground truth. These
findings show the potential of QASE-net to substantially enhance the
reliability and precision of sEMG quality assessment in practical applications.
\\ ( https://arxiv.org/abs/2402.05482 ,  1381kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05501 (*cross-listing*)
Date: Thu, 8 Feb 2024 09:19:26 GMT   (562kb,D)

Title: Machine Learning Augmented Branch and Bound for Mixed Integer Linear
  Programming
Authors: Lara Scavuzzo and Karen Aardal and Andrea Lodi and Neil Yorke-Smith
Categories: math.OC cs.LG
\\
  Mixed Integer Linear Programming (MILP) is a pillar of mathematical
optimization that offers a powerful modeling language for a wide range of
applications. During the past decades, enormous algorithmic progress has been
made in solving MILPs, and many commercial and academic software packages
exist. Nevertheless, the availability of data, both from problem instances and
from solvers, and the desire to solve new problems and larger (real-life)
instances, trigger the need for continuing algorithmic development. MILP
solvers use branch and bound as their main component. In recent years, there
has been an explosive development in the use of machine learning algorithms for
enhancing all main tasks involved in the branch-and-bound algorithm, such as
primal heuristics, branching, cutting planes, node selection and solver
configuration decisions. This paper presents a survey of such approaches,
addressing the vision of integration of machine learning and mathematical
optimization as complementary technologies, and how this integration can
benefit MILP solving. In particular, we give detailed attention to machine
learning algorithms that automatically optimize some metric of branch-and-bound
efficiency. We also address how to represent MILPs in the context of applying
learning algorithms, MILP benchmarks and software.
\\ ( https://arxiv.org/abs/2402.05501 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05526 (*cross-listing*)
Date: Thu, 8 Feb 2024 10:05:28 GMT   (423kb,D)

Title: Buffer Overflow in Mixture of Experts
Authors: Jamie Hayes, Ilia Shumailov, Itay Yona
Categories: cs.CR cs.LG
\\
  Mixture of Experts (MoE) has become a key ingredient for scaling large
foundation models while keeping inference costs steady. We show that expert
routing strategies that have cross-batch dependencies are vulnerable to
attacks. Malicious queries can be sent to a model and can affect a model's
output on other benign queries if they are grouped in the same batch. We
demonstrate this via a proof-of-concept attack in a toy experimental setting.
\\ ( https://arxiv.org/abs/2402.05526 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05543 (*cross-listing*)
Date: Thu, 8 Feb 2024 10:22:45 GMT   (635kb,D)

Title: Machine learning applied to omics data
Authors: Aida Calvi\~no and Almudena Moreno-Ribera and Silvia Pineda
Categories: q-bio.GN cs.LG stat.AP
Comments: Part of the book "Statistical Methods at the Forefront of Biomedical
  Advances" published by Springer Cham
DOI: 10.1007/978-3-031-32729-2_2
\\
  In this chapter we illustrate the use of some Machine Learning techniques in
the context of omics data. More precisely, we review and evaluate the use of
Random Forest and Penalized Multinomial Logistic Regression for integrative
analysis of genomics and immunomics in pancreatic cancer. Furthermore, we
propose the use of association rules with predictive purposes to overcome the
low predictive power of the previously mentioned models. Finally, we apply the
reviewed methods to a real data set from TCGA made of 107 tumoral pancreatic
samples and 117,486 germline SNPs, showing the good performance of the proposed
methods to predict the immunological infiltration in pancreatic cancer.
\\ ( https://arxiv.org/abs/2402.05543 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05552 (*cross-listing*)
Date: Thu, 8 Feb 2024 10:42:47 GMT   (16kb)

Title: Learning quantum Hamiltonians at any temperature in polynomial time with
  Chebyshev and bit complexity
Authors: Ales Wodecki and Jakub Marecek
Categories: quant-ph cs.LG math.OC
Comments: 16 pages
\\
  We consider the problem of learning local quantum Hamiltonians given copies
of their Gibbs state at a known inverse temperature, following Haah et al.
[2108.04842] and Bakshi et al. [arXiv:2310.02243]. Our main technical
contribution is a new flat polynomial approximation of the exponential function
based on the Chebyshev expansion, which enables the formulation of learning
quantum Hamiltonians as a polynomial optimization problem. This, in turn, can
benefit from the use of moment/SOS relaxations, whose polynomial bit complexity
requires careful analysis [O'Donnell, ITCS 2017]. Finally, we show that
learning a $k$-local Hamiltonian, whose dual interaction graph is of bounded
degree, runs in polynomial time under mild assumptions.
\\ ( https://arxiv.org/abs/2402.05552 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05639 (*cross-listing*)
Date: Thu, 8 Feb 2024 12:50:38 GMT   (1407kb,D)

Title: Nonparametric Instrumental Variable Regression through Stochastic
  Approximate Gradients
Authors: Caio Peixoto, Yuri Saporito, Yuri Fonseca
Categories: stat.ML cs.LG
Comments: 22 pages, 5 figures
\\
  This paper proposes SAGD-IV, a novel framework for conducting nonparametric
instrumental variable (NPIV) regression by employing stochastic approximate
gradients to minimize the projected populational risk. Instrumental Variables
(IVs) are widely used in econometrics to address estimation problems in the
presence of unobservable confounders, and the Machine Learning community has
devoted significant effort to improving existing methods and devising new ones
in the NPIV setting, which is known to be an ill-posed linear inverse problem.
We provide theoretical support for our algorithm and further exemplify its
competitive performance through empirical experiments. Furthermore, we address,
with promising results, the case of binary outcomes, which has not received as
much attention from the community as its continuous counterpart.
\\ ( https://arxiv.org/abs/2402.05639 ,  1407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05645 (*cross-listing*)
Date: Thu, 8 Feb 2024 13:00:18 GMT   (154kb,D)

Title: Investigating Reproducibility in Deep Learning-Based Software Fault
  Prediction
Authors: Adil Mukhtar, Dietmar Jannach, Franz Wotawa
Categories: cs.SE cs.LG
Comments: 12 pages, 7 figures
\\
  Over the past few years, deep learning methods have been applied for a wide
range of Software Engineering (SE) tasks, including in particular for the
important task of automatically predicting and localizing faults in software.
With the rapid adoption of increasingly complex machine learning models, it
however becomes more and more difficult for scholars to reproduce the results
that are reported in the literature. This is in particular the case when the
applied deep learning models and the evaluation methodology are not properly
documented and when code and data are not shared. Given some recent -- and very
worrying -- findings regarding reproducibility and progress in other areas of
applied machine learning, the goal of this work is to analyze to what extent
the field of software engineering, in particular in the area of software fault
prediction, is plagued by similar problems. We have therefore conducted a
systematic review of the current literature and examined the level of
reproducibility of 56 research articles that were published between 2019 and
2022 in top-tier software engineering conferences. Our analysis revealed that
scholars are apparently largely aware of the reproducibility problem, and about
two thirds of the papers provide code for their proposed deep learning models.
However, it turned out that in the vast majority of cases, crucial elements for
reproducibility are missing, such as the code of the compared baselines, code
for data pre-processing or code for hyperparameter tuning. In these cases, it
therefore remains challenging to exactly reproduce the results in the current
research literature. Overall, our meta-analysis therefore calls for improved
research practices to ensure the reproducibility of machine-learning based
research.
\\ ( https://arxiv.org/abs/2402.05645 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05674 (*cross-listing*)
Date: Thu, 8 Feb 2024 13:52:35 GMT   (2131kb,D)

Title: A High Dimensional Model for Adversarial Training: Geometry and
  Trade-Offs
Authors: Kasimir Tanner, Matteo Vilucchio, Bruno Loureiro, Florent Krzakala
Categories: stat.ML cond-mat.dis-nn cs.LG
\\
  This work investigates adversarial training in the context of margin-based
linear classifiers in the high-dimensional regime where the dimension $d$ and
the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We
introduce a tractable mathematical model where the interplay between the data
and adversarial attacker geometries can be studied, while capturing the core
phenomenology observed in the adversarial robustness literature. Our main
theoretical contribution is an exact asymptotic description of the sufficient
statistics for the adversarial empirical risk minimiser, under generic convex
and non-increasing losses. Our result allow us to precisely characterise which
directions in the data are associated with a higher generalisation/robustness
trade-off, as defined by a robustness and a usefulness metric. In particular,
we unveil the existence of directions which can be defended without penalising
accuracy. Finally, we show the advantage of defending non-robust features
during training, identifying a uniform protection as an inherently effective
defence mechanism.
\\ ( https://arxiv.org/abs/2402.05674 ,  2131kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05696 (*cross-listing*)
Date: Thu, 8 Feb 2024 14:19:29 GMT   (695kb)

Title: Fixed width treelike neural networks capacity analysis -- generic
  activations
Authors: Mihailo Stojnic
Categories: stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR
\\
  We consider the capacity of \emph{treelike committee machines} (TCM) neural
networks. Relying on Random Duality Theory (RDT), \cite{Stojnictcmspnncaprdt23}
recently introduced a generic framework for their capacity analysis. An upgrade
based on the so-called \emph{partially lifted} RDT (pl RDT) was then presented
in \cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the
networks with the most typical, \emph{sign}, activations. Here, on the other
hand, we focus on networks with other, more general, types of activations and
show that the frameworks of
\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently
powerful to enable handling of such scenarios as well. In addition to the
standard \emph{linear} activations, we uncover that particularly convenient
results can be obtained for two very commonly used activations, namely, the
\emph{quadratic} and \emph{rectified linear unit (ReLU)} ones. In more concrete
terms, for each of these activations, we obtain both the RDT and pl RDT based
memory capacities upper bound characterization for \emph{any} given (even)
number of the hidden layer neurons, $d$. In the process, we also uncover the
following two, rather remarkable, facts: 1) contrary to the common wisdom, both
sets of results show that the bounding capacity decreases for large $d$ (the
width of the hidden layer) while converging to a constant value; and 2) the
maximum bounding capacity is achieved for the networks with precisely
\textbf{\emph{two}} hidden layer neurons! Moreover, the large $d$ converging
values are observed to be in excellent agrement with the statistical physics
replica theory based predictions.
\\ ( https://arxiv.org/abs/2402.05696 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05715 (*cross-listing*)
Date: Thu, 8 Feb 2024 14:43:56 GMT   (23602kb,D)

Title: Collaborative non-parametric two-sample testing
Authors: Alejandro de la Concha, Nicolas Vayatis, Argyris Kalogeratos
Categories: stat.ML cs.LG
\\
  This paper addresses the multiple two-sample test problem in a
graph-structured setting, which is a common scenario in fields such as Spatial
Statistics and Neuroscience. Each node $v$ in fixed graph deals with a
two-sample testing problem between two node-specific probability density
functions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null
hypothesis $p_v = q_v$ should be rejected, under the assumption that connected
nodes would yield similar test outcomes. We propose the non-parametric
collaborative two-sample testing (CTST) framework that efficiently leverages
the graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our
methodology integrates elements from f-divergence estimation, Kernel Methods,
and Multitask Learning. We use synthetic experiments and a real sensor network
detecting seismic activity to demonstrate that CTST outperforms
state-of-the-art non-parametric statistical tests that apply at each node
independently, hence disregard the geometry of the problem.
\\ ( https://arxiv.org/abs/2402.05715 ,  23602kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05718 (*cross-listing*)
Date: Thu, 8 Feb 2024 14:47:37 GMT   (6640kb,D)

Title: REMEDI: Corrective Transformations for Improved Neural Entropy
  Estimation
Authors: Viktor Nilsson, Anirban Samaddar, Sandeep Madireddy, Pierre Nyquist
Categories: stat.ML cs.LG
Comments: 27 pages, 17 figures
MSC-class: 94A17 (Primary) 68T01, 94A08 (Secondary)
\\
  Information theoretic quantities play a central role in machine learning. The
recent surge in the complexity of data and models has increased the demand for
accurate estimation of these quantities. However, as the dimension grows the
estimation presents significant challenges, with existing methods struggling
already in relatively low dimensions. To address this issue, in this work, we
introduce $\texttt{REMEDI}$ for efficient and accurate estimation of
differential entropy, a fundamental information theoretic quantity. The
approach combines the minimization of the cross-entropy for simple, adaptive
base models and the estimation of their deviation, in terms of the relative
entropy, from the data density. Our approach demonstrates improvement across a
broad spectrum of estimation tasks, encompassing entropy estimation on both
synthetic and natural data. Further, we extend important theoretical
consistency results to a more generalized setting required by our approach. We
illustrate how the framework can be naturally extended to information theoretic
supervised learning models, with a specific focus on the Information Bottleneck
approach. It is demonstrated that the method delivers better accuracy compared
to the existing methods in Information Bottleneck. In addition, we explore a
natural connection between $\texttt{REMEDI}$ and generative modeling using
rejection sampling and Langevin dynamics.
\\ ( https://arxiv.org/abs/2402.05718 ,  6640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05719 (*cross-listing*)
Date: Thu, 8 Feb 2024 14:50:07 GMT   (264kb)

Title: Exact capacity of the \emph{wide} hidden layer treelike neural networks
  with generic activations
Authors: Mihailo Stojnic
Categories: stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR
\\
  Recent progress in studying \emph{treelike committee machines} (TCM) neural
networks (NN) in
\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23}
showed that the Random Duality Theory (RDT) and its a \emph{partially
lifted}(pl RDT) variant are powerful tools that can be used for very precise
networks capacity analysis. Here, we consider \emph{wide} hidden layer networks
and uncover that certain aspects of numerical difficulties faced in
\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we
employ recently developed \emph{fully lifted} (fl) RDT to characterize the
\emph{wide} ($d\rightarrow \infty$) TCM nets capacity. We obtain explicit,
closed form, capacity characterizations for a very generic class of the hidden
layer activations. While the utilized approach significantly lowers the amount
of the needed numerical evaluations, the ultimate fl RDT usefulness and success
still require a solid portion of the residual numerical work. To get the
concrete capacity values, we take four very famous activations examples:
\emph{\textbf{ReLU}}, \textbf{\emph{quadratic}}, \textbf{\emph{erf}}, and
\textbf{\emph{tanh}}. After successfully conducting all the residual numerical
work for all of them, we uncover that the whole lifting mechanism exhibits a
remarkably rapid convergence with the relative improvements no better than
$\sim 0.1\%$ happening already on the 3-rd level of lifting. As a convenient
bonus, we also uncover that the capacity characterizations obtained on the
first and second level of lifting precisely match those obtained through the
statistical physics replica theory methods in \cite{ZavPeh21} for the generic
and in \cite{BalMalZech19} for the ReLU activations.
\\ ( https://arxiv.org/abs/2402.05719 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05787 (*cross-listing*)
Date: Thu, 8 Feb 2024 16:24:44 GMT   (450kb,D)

Title: How do Transformers perform In-Context Autoregressive Learning?
Authors: Michael E. Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, Gabriel
  Peyr\'e
Categories: stat.ML cs.LG
Comments: 24 pages
\\
  Transformers have achieved state-of-the-art performance in language modeling
tasks. However, the reasons behind their tremendous success are still unclear.
In this paper, towards a better understanding, we train a Transformer model on
a simple next token prediction task, where sequences are generated as a
first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained
Transformer predicts the next token by first learning $W$ in-context, then
applying a prediction mapping. We call the resulting procedure in-context
autoregressive learning. More precisely, focusing on commuting orthogonal
matrices $W$, we first show that a trained one-layer linear Transformer
implements one step of gradient descent for the minimization of an inner
objective function, when considering augmented tokens. When the tokens are not
augmented, we characterize the global minima of a one-layer diagonal linear
multi-head Transformer. Importantly, we exhibit orthogonality between heads and
show that positional encoding captures trigonometric relations in the data. On
the experimental side, we consider the general case of non-commuting orthogonal
matrices and generalize our theoretical findings.
\\ ( https://arxiv.org/abs/2402.05787 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05817 (*cross-listing*)
Date: Thu, 8 Feb 2024 16:54:20 GMT   (891kb)

Title: Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A
  Supervised Contrastive Learning
Authors: Pouria Yazdian Anari, Fiona Obiezu, Nathan Lay, Fatemeh Dehghani
  Firouzabadi, Aditi Chaurasia, Mahshid Golagha, Shiva Singh, Fatemeh
  Homayounieh, Aryan Zahergivar, Stephanie Harmon, Evrim Turkbey, Rabindra
  Gautam, Kevin Ma, Maria Merino, Elizabeth C. Jones, Mark W. Ball, W. Marston
  Linehan, Baris Turkbey, Ashkan A. Malayeri
Categories: eess.IV cs.CV cs.LG
\\
  Introduction This study explores the use of the latest You Only Look Once
(YOLO V7) object detection method to enhance kidney detection in medical
imaging by training and testing a modified YOLO V7 on medical image formats.
Methods Study includes 878 patients with various subtypes of renal cell
carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans
for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from
a retrospective maintained database, and bounding boxes were drawn around their
tumors. A primary model was trained on 80% of annotated cases, with 20% saved
for testing (primary test set). The best primary model was then used to
identify tumors in the remaining 861 patients and bounding box coordinates were
generated on their scans using the model. Ten benchmark training sets were
created with generated coordinates on not-segmented patients. The final model
used to predict the kidney in the primary test set. We reported the positive
predictive value (PPV), sensitivity, and mean average precision (mAP). Results
The primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of
0.87 +/- 0.04, and mAP of 0.91 +/- 0.02. The best primary model yielded a PPV
of 0.97, sensitivity of 0.92, and mAP of 0.95. The final model demonstrated an
average PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95
+/- 0.01. Conclusion Using a semi-supervised approach with a medical image
library, we developed a high-performing model for kidney detection. Further
external validation is required to assess the model's generalizability.
\\ ( https://arxiv.org/abs/2402.05817 ,  891kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05841 (*cross-listing*)
Date: Thu, 8 Feb 2024 17:18:01 GMT   (16325kb,D)

Title: Dirichlet Flow Matching with Applications to DNA Sequence Design
Authors: Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger,
  Regina Barzilay, Tommi Jaakkola
Categories: q-bio.BM cs.LG
\\
  Discrete diffusion or flow models could enable faster and more controllable
sequence generation than autoregressive models. We show that na\"ive linear
flow matching on the simplex is insufficient toward this goal since it suffers
from discontinuities in the training target and further pathologies. To
overcome this, we develop Dirichlet flow matching on the simplex based on
mixtures of Dirichlet distributions as probability paths. In this framework, we
derive a connection between the mixtures' scores and the flow's vector field
that allows for classifier and classifier-free guidance. Further, we provide
distilled Dirichlet flow matching, which enables one-step sequence generation
with minimal performance hits, resulting in $O(L)$ speedups compared to
autoregressive models. On complex DNA sequence generation tasks, we demonstrate
superior performance compared to all baselines in distributional metrics and in
achieving desired design targets for generated sequences. Finally, we show that
our classifier-free guidance approach improves unconditional generation and is
effective for generating DNA that satisfies design targets. Code is available
at https://github.com/HannesStark/dirichlet-flow-matching.
\\ ( https://arxiv.org/abs/2402.05841 ,  16325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05856 (*cross-listing*)
Date: Wed, 7 Feb 2024 09:32:35 GMT   (1354kb,D)

Title: Structure-Informed Protein Language Model
Authors: Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, Aur\'elie Lozano,
  Payel Das, Jian Tang
Categories: q-bio.BM cs.LG
\\
  Protein language models are a powerful tool for learning protein
representations through pre-training on vast protein sequence datasets.
However, traditional protein language models lack explicit structural
supervision, despite its relevance to protein function. To address this issue,
we introduce the integration of remote homology detection to distill structural
information into protein language models without requiring explicit protein
structures as input. We evaluate the impact of this structure-informed training
on downstream protein function prediction tasks. Experimental results reveal
consistent improvements in function annotation accuracy for EC number and GO
term prediction. Performance on mutant datasets, however, varies based on the
relationship between targeted properties and protein structures. This
underscores the importance of considering this relationship when applying
structure-aware training to protein function prediction tasks. Code and model
weights are available at https://github.com/DeepGraphLearning/esm-s.
\\ ( https://arxiv.org/abs/2402.05856 ,  1354kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05878 (*cross-listing*)
Date: Thu, 8 Feb 2024 18:13:26 GMT   (938kb,D)

Title: Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm
  Identification in Structured Bandits
Authors: Nicolas Nguyen, Imad Aouali, Andr\'as Gy\"orgy, Claire Vernade
Categories: stat.ML cs.LG
\\
  We study the problem of Bayesian fixed-budget best-arm identification (BAI)
in structured bandits. We propose an algorithm that uses fixed allocations
based on the prior information and the structure of the environment. We provide
theoretical bounds on its performance across diverse models, including the
first prior-dependent upper bounds for linear and hierarchical BAI. Our key
contribution is introducing new proof methods that result in tighter bounds for
multi-armed BAI compared to existing methods. We extensively compare our
approach to other fixed-budget BAI methods, demonstrating its consistent and
robust performance in various settings. Our work improves our understanding of
Bayesian fixed-budget BAI in structured bandits and highlights the
effectiveness of our approach in practical scenarios.
\\ ( https://arxiv.org/abs/2402.05878 ,  938kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2302.02104
replaced with revised version Thu, 8 Feb 2024 13:49:57 GMT   (8901kb,D)

Title: HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation
  and A Strong Structure-Hardness-Aware Baseline
Authors: Yang Li, Xinyan Chen, Wenxuan Guo, Xijun Li, Wanqian Luo, Junhua
  Huang, Hui-Ling Zhen, Mingxuan Yuan, Junchi Yan
Categories: cs.AI cs.LG
Comments: Published at SIGKDD 2023, see
  http://dl.acm.org/doi/10.1145/3580305.3599837
\\ ( https://arxiv.org/abs/2302.02104 ,  8901kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05893
replaced with revised version Thu, 8 Feb 2024 18:31:14 GMT   (2635kb)

Title: Learning Team-Based Navigation: A Review of Deep Reinforcement Learning
  Techniques for Multi-Agent Pathfinding
Authors: Jaehoon Chung, Jamil Fayyad, Younes Al Younes, and Homayoun Najjaran
Categories: cs.AI cs.LG cs.MA cs.RO cs.SY eess.SY
Comments: 36 pages, 10 figures, published in Artif Intell Rev 57, 41 (2024)
Journal-ref: Artif Intell Rev 57, 41 (2024)
DOI: 10.1007/s10462-023-10670-6
\\ ( https://arxiv.org/abs/2308.05893 ,  2635kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09323
replaced with revised version Thu, 8 Feb 2024 14:38:04 GMT   (1228kb,D)

Title: Answering Causal Queries at Layer 3 with DiscoSCMs-Embracing
  Heterogeneity
Authors: Heyang Gong
Categories: cs.AI cs.LG stat.ME
\\ ( https://arxiv.org/abs/2309.09323 ,  1228kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00217
replaced with revised version Thu, 8 Feb 2024 03:49:46 GMT   (1181kb)

Title: Can Large Language Models Capture Public Opinion about Global Warming?
  An Empirical Assessment of Algorithmic Fidelity and Bias
Authors: S. Lee, T. Q. Peng, M. H. Goldberg, S. A. Rosenthal, J. E. Kotcher, E.
  W. Maibach and A. Leiserowitz
Categories: cs.AI cs.CY
Comments: 34 pages, 6 figures, 1 table
\\ ( https://arxiv.org/abs/2311.00217 ,  1181kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07607
replaced with revised version Thu, 8 Feb 2024 09:32:44 GMT   (130kb,D)

Title: Modeling Choice via Self-Attention
Authors: Joohwan Ko, Andrew A. Li
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.07607 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08157
replaced with revised version Thu, 8 Feb 2024 13:27:28 GMT   (1767kb,D)

Title: CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal
  Feature Removal Problem
Authors: Qian Chen, Taolin Zhang, Dongyang Li, Xiaofeng He
Categories: cs.AI
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2312.08157 ,  1767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03678
replaced with revised version Thu, 8 Feb 2024 00:24:32 GMT   (1856kb,D)

Title: Logical Specifications-guided Dynamic Task Sampling for Reinforcement
  Learning Agents
Authors: Yash Shukla, Tanushree Burman, Abhishek Kulkarni, Robert Wright,
  Alvaro Velasquez, Jivko Sinapov
Categories: cs.AI cs.LG cs.RO
\\ ( https://arxiv.org/abs/2402.03678 ,  1856kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04154
replaced with revised version Thu, 8 Feb 2024 00:15:48 GMT   (1492kb,D)

Title: Read to Play (R2-Play): Decision Transformer with Multimodal Game
  Instruction
Authors: Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu
  Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He and Jie Fu
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.04154 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04578
replaced with revised version Thu, 8 Feb 2024 17:01:00 GMT   (33188kb,D)

Title: S-Agents: self-organizing agents in open-ended environment
Authors: Jiaqi Chen and Yuxian Jiang and Jiachen Lu and Li Zhang
Categories: cs.AI cs.MA
Comments: Preview, 23 pages, 12 figure
\\ ( https://arxiv.org/abs/2402.04578 ,  33188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04971
replaced with revised version Thu, 8 Feb 2024 02:10:26 GMT   (2539kb,D)

Title: Multi-Sender Persuasion -- A Computational Perspective
Authors: Safwan Hossain, Tonghan Wang, Tao Lin, Yiling Chen, David C. Parkes,
  Haifeng Xu
Categories: cs.AI cs.GT
\\ ( https://arxiv.org/abs/2402.04971 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10786
replaced with revised version Wed, 7 Feb 2024 18:54:27 GMT   (2467kb,D)

Title: Real-World Deployment and Evaluation of Kwame for Science, An AI
  Teaching Assistant for Science Education in West Africa
Authors: George Boateng, Samuel John, Samuel Boateng, Philemon Badu, Patrick
  Agyeman-Budu and Victor Kumbol
Categories: cs.CL cs.CY cs.HC cs.IR
Comments: 14 pages, under review at International Conference on Artificial
  Intelligence in Education (AIED 2024)
\\ ( https://arxiv.org/abs/2302.10786 ,  2467kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07303
replaced with revised version Thu, 8 Feb 2024 09:29:08 GMT   (390kb,D)

Title: Multi-Relational Hyperbolic Word Embeddings from Natural Language
  Definitions
Authors: Marco Valentino, Danilo S. Carvalho, Andr\'e Freitas
Categories: cs.CL cs.LG
Comments: Accepted at the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024), camera-ready
\\ ( https://arxiv.org/abs/2305.07303 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12392
replaced with revised version Thu, 8 Feb 2024 04:04:25 GMT   (315kb,D)

Title: PiVe: Prompting with Iterative Verification Improving Graph-based
  Generative Capability of LLMs
Authors: Jiuzhou Han, Nigel Collier, Wray Buntine, Ehsan Shareghi
Categories: cs.CL
Comments: Our code and data is at https://github.com/Jiuzhouh/PiVe (Added
  results for GPT-4, with new experiments on larger set of few-shot numbers and
  diverse prompting approaches.)
\\ ( https://arxiv.org/abs/2305.12392 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12815
replaced with revised version Thu, 8 Feb 2024 02:22:53 GMT   (3787kb,D)

Title: Investigating Agency of LLMs in Human-AI Collaboration Tasks
Authors: Ashish Sharma, Sudha Rao, Chris Brockett, Akanksha Malhotra, Nebojsa
  Jojic, Bill Dolan
Categories: cs.CL
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2305.12815 ,  3787kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01189
replaced with revised version Thu, 8 Feb 2024 16:19:14 GMT   (10223kb,D)

Title: Trainable Transformer in Transformer
Authors: Abhishek Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora
Categories: cs.CL cs.LG
Comments: Code base:
  https://github.com/abhishekpanigrahi1996/transformer_in_transformer
\\ ( https://arxiv.org/abs/2307.01189 ,  10223kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04804
replaced with revised version Thu, 8 Feb 2024 11:09:53 GMT   (2120kb,D)

Title: S2vNTM: Semi-supervised vMF Neural Topic Modeling
Authors: Weijie Xu, Jay Desai, Srinivasan Sengamedu, Xiaoyu Jiang, Francis
  Iannacci
Categories: cs.CL cs.AI
Comments: 17 pages, 9 figures, ICLR Workshop 2023. arXiv admin note: text
  overlap with arXiv:2307.01226
MSC-class: 68T50
ACM-class: I.2.7
Journal-ref: ICLR Workshop 2023
\\ ( https://arxiv.org/abs/2307.04804 ,  2120kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07034
replaced with revised version Thu, 8 Feb 2024 16:35:36 GMT   (7681kb,D)

Title: Sensitivity, Performance, Robustness: Deconstructing the Effect of
  Sociodemographic Prompting
Authors: Tilman Beck, Hendrik Schuff, Anne Lauscher, Iryna Gurevych
Categories: cs.CL cs.AI
Comments: EACL 2024 camera-ready
\\ ( https://arxiv.org/abs/2309.07034 ,  7681kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07311
replaced with revised version Wed, 7 Feb 2024 21:40:55 GMT   (6555kb,D)

Title: Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and
  Simplicity Bias in MLMs
Authors: Angelica Chen, Ravid Shwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt,
  Naomi Saphra
Categories: cs.CL
Comments: ICLR 2024 camera-ready
\\ ( https://arxiv.org/abs/2309.07311 ,  6555kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15516
replaced with revised version Thu, 8 Feb 2024 04:06:33 GMT   (8189kb,D)

Title: Teaching Text-to-Image Models to Communicate in Dialog
Authors: Xiaowen Sun, Jiazhan Feng, Yuxuan Wang, Yuxuan Lai, Xingyu Shen,
  Dongyan Zhao
Categories: cs.CL cs.AI cs.CV
Comments: Work in progress
\\ ( https://arxiv.org/abs/2309.15516 ,  8189kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00152
replaced with revised version Thu, 8 Feb 2024 18:23:33 GMT   (190kb,D)

Title: Learning to Rewrite Prompts for Personalized Text Generation
Authors: Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Michael Bendersky
Categories: cs.CL
Comments: In Proceedings of the ACM Web Conference 2024 (WWW '24)
DOI: 10.1145/3589334.3645408
\\ ( https://arxiv.org/abs/2310.00152 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00313
replaced with revised version Thu, 8 Feb 2024 00:39:07 GMT   (10617kb,D)

Title: Decoding In-Context Learning: Neuroscience-inspired Analysis of
  Representations in Large Language Models
Authors: Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Rapha\"el
  Milli\`ere, Ida Momennejad
Categories: cs.CL
Comments: Submitted to ICML 2024
\\ ( https://arxiv.org/abs/2310.00313 ,  10617kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03249
replaced with revised version Wed, 7 Feb 2024 20:18:54 GMT   (5349kb,D)

Title: Can Large Language Models be Good Path Planners? A Benchmark and
  Investigation on Spatial-temporal Reasoning
Authors: Mohamed Aghzal, Erion Plaku, Ziyu Yao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.03249 ,  5349kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14450
replaced with revised version Thu, 8 Feb 2024 15:17:15 GMT   (1130kb,D)

Title: TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings
Authors: Hans W. A. Hanley, Zakir Durumeric
Categories: cs.CL cs.CY cs.LG
Comments: Accepted to EMNLP 2023; Updated citations
\\ ( https://arxiv.org/abs/2310.14450 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03952
replaced with revised version Wed, 7 Feb 2024 22:10:35 GMT   (1136kb,D)

Title: An Analysis of Dialogue Repair in Voice Assistants
Authors: Matthew Galbraith
Categories: cs.CL cs.HC cs.RO
Comments: In WTF Workshop Proceedings (arXiv:2401.04108) held in conjunction
  with the ACM conference on Conversational User Interfaces (CUI), 19 - 21/07
  2023, in Eindhoven, The Netherlands
Report-no: WTFCUI/2023/04
\\ ( https://arxiv.org/abs/2311.03952 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14877
replaced with revised version Thu, 8 Feb 2024 17:29:54 GMT   (229kb,D)

Title: Robust Knowledge Extraction from Large Language Models using Social
  Choice Theory
Authors: Nico Potyka, Yuqicheng Zhu, Yunjie He, Evgeny Kharlamov, Steffen Staab
Categories: cs.CL cs.AI
Comments: Accepted by AAMAS 2024 as a full paper
\\ ( https://arxiv.org/abs/2312.14877 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04679
replaced with revised version Thu, 8 Feb 2024 15:43:44 GMT   (1124kb,D)

Title: RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation
Authors: Mahdi Nikdan, Soroush Tabesh, Elvir Crn\v{c}evi\'c, Dan Alistarh
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.04679 ,  1124kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10020
replaced with revised version Thu, 8 Feb 2024 10:19:53 GMT   (1048kb,D)

Title: Self-Rewarding Language Models
Authors: Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar
  Sukhbaatar, Jing Xu, Jason Weston
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.10020 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14016
replaced with revised version Thu, 8 Feb 2024 03:53:34 GMT   (1865kb,D)

Title: Towards Uncertainty-Aware Language Agent
Authors: Jiuzhou Han and Wray Buntine and Ehsan Shareghi
Categories: cs.CL
Comments: The code and data are at https://uala-agent.github.io. (Updated the
  design for multi-inference setup to be comparable with single-inference
  experiments.). arXiv admin note: text overlap with arXiv:2310.05915
\\ ( https://arxiv.org/abs/2401.14016 ,  1865kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00746
replaced with revised version Thu, 8 Feb 2024 17:47:19 GMT   (1009kb,D)

Title: Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
Authors: Mingyu Jin, Qinkai Yu, Chong Zhang, Dong Shu, Suiyuan Zhu, Mengnan Du,
  Yongfeng Zhang, Yanda Meng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00746 ,  1009kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00794
replaced with revised version Wed, 7 Feb 2024 21:01:16 GMT   (342kb,D)

Title: ReAGent: A Model-agnostic Feature Attribution Method for Generative
  Language Models
Authors: Zhixue Zhao, Boxuan Shan
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at AAAI24 workshop ReLM
\\ ( https://arxiv.org/abs/2402.00794 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01697
replaced with revised version Thu, 8 Feb 2024 06:22:06 GMT   (288kb,D)

Title: APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data
  Annotation
Authors: Yiming Zhu, Zhizhuo Yin, Gareth Tyson, Ehsan-Ul Haq, Lik-Hang Lee, Pan
  Hui
Categories: cs.CL
Comments: Just accepted by WWW 2024
\\ ( https://arxiv.org/abs/2402.01697 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03216
replaced with revised version Thu, 8 Feb 2024 17:32:52 GMT   (1238kb,D)

Title: BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge Distillation
Authors: Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\ ( https://arxiv.org/abs/2402.03216 ,  1238kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04614
replaced with revised version Thu, 8 Feb 2024 02:30:49 GMT   (791kb,D)

Title: Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations
  from Large Language Models
Authors: Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.04614 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05044
replaced with revised version Thu, 8 Feb 2024 02:50:22 GMT   (12088kb,D)

Title: SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large
  Language Models
Authors: Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin,
  Yu Qiao, Jing Shao
Categories: cs.CL cs.AI cs.CR cs.LG
\\ ( https://arxiv.org/abs/2402.05044 ,  12088kb)
------------------------------------------------------------------------------
\\
arXiv:2106.04096
replaced with revised version Thu, 8 Feb 2024 11:02:15 GMT   (75kb,D)

Title: Linear Convergence of Entropy-Regularized Natural Policy Gradient with
  Linear Function Approximation
Authors: Semih Cayci, Niao He, R. Srikant
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2106.04096 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2110.05286
replaced with revised version Wed, 7 Feb 2024 23:45:53 GMT   (14701kb,D)

Title: Learning from Ambiguous Demonstrations with Self-Explanation Guided
  Reinforcement Learning
Authors: Yantian Zha, Lin Guan, and Subbarao Kambhampati
Categories: cs.LG
\\ ( https://arxiv.org/abs/2110.05286 ,  14701kb)
------------------------------------------------------------------------------
\\
arXiv:2205.13147
replaced with revised version Thu, 8 Feb 2024 03:21:26 GMT   (7558kb,D)

Title: Matryoshka Representation Learning
Authors: Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford,
  Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham
  Kakade, Prateek Jain, Ali Farhadi
Categories: cs.LG cs.CV
Comments: Edited related work to include intrinsic dimensionality works
\\ ( https://arxiv.org/abs/2205.13147 ,  7558kb)
------------------------------------------------------------------------------
\\
arXiv:2207.08041
replaced with revised version Thu, 8 Feb 2024 17:35:56 GMT   (5596kb,D)

Title: Personalized PCA: Decoupling Shared and Unique Features
Authors: Naichen Shi and Raed Al Kontar
Categories: cs.LG math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2207.08041 ,  5596kb)
------------------------------------------------------------------------------
\\
arXiv:2208.04508
replaced with revised version Thu, 8 Feb 2024 00:26:31 GMT   (45kb)

Title: Training Overparametrized Neural Networks in Sublinear Time
Authors: Yichuan Deng, Hang Hu, Zhao Song, Omri Weinstein, Danyang Zhuo
Categories: cs.LG cs.DS stat.ML
\\ ( https://arxiv.org/abs/2208.04508 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2210.07996
replaced with revised version Thu, 8 Feb 2024 07:09:23 GMT   (104kb)

Title: Degeneracy is OK: Logarithmic Regret for Network Revenue Management with
  Indiscrete Distributions
Authors: Jiashuo Jiang, Will Ma and Jiawei Zhang
Categories: cs.LG math.PR
\\ ( https://arxiv.org/abs/2210.07996 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2301.09109
replaced with revised version Thu, 8 Feb 2024 04:41:49 GMT   (2520kb,D)

Title: Federated Recommendation with Additive Personalization
Authors: Zhiwei Li, Guodong Long, Tianyi Zhou
Categories: cs.LG
Comments: 9 pages, conference
\\ ( https://arxiv.org/abs/2301.09109 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2301.09734
replaced with revised version Thu, 8 Feb 2024 17:16:39 GMT   (10562kb,D)

Title: Topological Learning in Multi-Class Data Sets
Authors: Christopher Griffin and Trevor Karn and Benjamin Apple
Categories: cs.LG physics.data-an
Comments: 16 pages, 18 figures. This is a revision of v3 that fixes a typo on
  Page 2 that resulted in an incorrect equation that did not match the
  description around it
\\ ( https://arxiv.org/abs/2301.09734 ,  10562kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11270
replaced with revised version Thu, 8 Feb 2024 04:16:52 GMT   (122kb,D)

Title: Principled Reinforcement Learning with Human Feedback from Pairwise or
  $K$-wise Comparisons
Authors: Banghua Zhu, Jiantao Jiao, Michael I. Jordan
Categories: cs.LG cs.AI cs.HC math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2301.11270 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2303.01213
replaced with revised version Thu, 8 Feb 2024 08:26:47 GMT   (2113kb,D)

Title: DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural
  Network Worry-Free?
Authors: Victor Qu\'etu, Enzo Tartaglione
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.01213 ,  2113kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07896
replaced with revised version Thu, 8 Feb 2024 10:22:42 GMT   (101kb,D)

Title: Out-of-Variable Generalization for Discriminative Models
Authors: Siyuan Guo, Jonas Wildberger, Bernhard Sch\"olkopf
Categories: cs.LG cs.AI stat.ML
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2304.07896 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10557
replaced with revised version Thu, 8 Feb 2024 15:01:34 GMT   (372kb,D)

Title: An Introduction to Transformers
Authors: Richard E. Turner
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2304.10557 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13105
replaced with revised version Thu, 8 Feb 2024 12:24:55 GMT   (20649kb)

Title: Attention-Enhanced Deep Learning for Device-Free Through-the-Wall
  Presence Detection Using Indoor WiFi Systems
Authors: Li-Hsiang Shen, An-Hung Hsiao, Kuan-I Lu, and Kai-Ten Feng
Categories: cs.LG cs.AI
Comments: Accepted by IEEE Sensors Journal
\\ ( https://arxiv.org/abs/2304.13105 ,  20649kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05257
replaced with revised version Thu, 8 Feb 2024 16:09:20 GMT   (1123kb,D)

Title: Survey of Federated Learning Models for Spatial-Temporal Mobility
  Applications
Authors: Yacine Belal and Sonia Ben Mokhtar, Hamed Haddadi, Jaron Wang and Afra
  Mashhadi
Categories: cs.LG cs.AI cs.DC cs.IR cs.SI
ACM-class: A.1; D.4.6; H.4.3; H.5.6; I.2.6; I.5.3; I.5.8
\\ ( https://arxiv.org/abs/2305.05257 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06344
replaced with revised version Thu, 8 Feb 2024 17:31:40 GMT   (375kb,D)

Title: Orthogonal Transforms in Neural Networks Amount to Effective
  Regularization
Authors: Krzysztof Zaj\k{a}c and Wojciech Sopot and Pawe{\l} Wachel
Categories: cs.LG cs.NE cs.SY eess.SY
\\ ( https://arxiv.org/abs/2305.06344 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06927
replaced with revised version Thu, 8 Feb 2024 00:53:12 GMT   (440kb,D)

Title: Convergence of Alternating Gradient Descent for Matrix Factorization
Authors: Rachel Ward and Tamara G. Kolda
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2305.06927 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14405
replaced with revised version Thu, 8 Feb 2024 10:11:27 GMT   (498kb,D)

Title: NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix
  Operations for Efficient Inference
Authors: Ruiqi Sun, Siwei Ye, Jie Zhao, Xin He, Yiran Li, An Zou
Categories: cs.LG cs.AI cs.AR
Comments: 11 pages, 6figures, Submitted to 41st International Conference on
  Machine Learning
\\ ( https://arxiv.org/abs/2305.14405 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19510
replaced with revised version Thu, 8 Feb 2024 15:43:22 GMT   (1174kb,D)

Title: Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape
Authors: Kedar Karhadkar, Michael Murray, Hanna Tseran, Guido Mont\'ufar
Categories: cs.LG math.CO stat.ML
Comments: 40 pages
\\ ( https://arxiv.org/abs/2305.19510 ,  1174kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19838
replaced with revised version Thu, 8 Feb 2024 16:47:35 GMT   (4391kb,D)

Title: Relaxing the Additivity Constraints in Decentralized No-Regret
  High-Dimensional Bayesian Optimization
Authors: Anthony Bardou, Patrick Thiran and Thomas Begin
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.19838 ,  4391kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01482
replaced with revised version Thu, 8 Feb 2024 03:31:04 GMT   (41135kb,D)

Title: Contextualizing MLP-Mixers Spatiotemporally for Urban Data Forecast at
  Scale
Authors: Tong Nie, Guoyang Qin, Lijun Sun, Wei Ma, Yu Mei, Jian Sun
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.01482 ,  41135kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07084
replaced with revised version Thu, 8 Feb 2024 18:09:25 GMT   (16155kb,D)

Title: Safe Reinforcement Learning as Wasserstein Variational Inference: Formal
  Methods for Interpretability
Authors: Yanran Wang, David Boyle
Categories: cs.LG cs.AI cs.RO cs.SY eess.SY
Comments: 24 pages, 6 figures, containing Appendix
\\ ( https://arxiv.org/abs/2307.07084 ,  16155kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12971
replaced with revised version Wed, 7 Feb 2024 20:15:27 GMT   (790kb,D)

Title: Big Data - Supply Chain Management Framework for Forecasting: Data
  Preprocessing and Machine Learning Techniques
Authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed
  Ridoy, and M. F. Mridha
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2307.12971 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16704
replaced with revised version Thu, 8 Feb 2024 15:16:33 GMT   (413kb,D)

Title: Lookbehind-SAM: k steps back, 1 step forward
Authors: Gon\c{c}alo Mordido, Pranshu Malviya, Aristide Baratin, Sarath Chandar
Categories: cs.LG cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2307.16704 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00031
replaced with revised version Thu, 8 Feb 2024 12:48:23 GMT   (151kb,D)

Title: Reinforcement Learning for Generative AI: State of the Art,
  Opportunities and Open Research Challenges
Authors: Giorgio Franceschelli and Mirco Musolesi
Categories: cs.LG cs.AI
Comments: Published in JAIR at
  https://www.jair.org/index.php/jair/article/view/15278
Journal-ref: JAIR 79 (2024) 417-446
DOI: 10.1613/jair.1.15278
\\ ( https://arxiv.org/abs/2308.00031 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01084
replaced with revised version Thu, 8 Feb 2024 15:58:38 GMT   (15004kb,D)

Title: Data-Driven Identification of Quadratic Representations for Nonlinear
  Hamiltonian Systems using Weakly Symplectic Liftings
Authors: S\"uleyman Yildiz, Pawan Goyal, Thomas Bendokat and Peter Benner
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.01084 ,  15004kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14120
replaced with revised version Wed, 7 Feb 2024 20:07:50 GMT   (1340kb)

Title: Large Language Models Streamline Automated Machine Learning for Clinical
  Studies
Authors: Soroosh Tayebi Arasteh, Tianyu Han, Mahshad Lotfinia, Christiane Kuhl,
  Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung
Categories: cs.LG cs.AI cs.CL
Comments: Accepted for publication in Nature Communications. 2024
\\ ( https://arxiv.org/abs/2308.14120 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01945
replaced with revised version Thu, 8 Feb 2024 02:12:48 GMT   (4424kb,D)

Title: OHQ: On-chip Hardware-aware Quantization
Authors: Wei Huang, Haotong Qin, Yangdong Liu, Jingzhuo Liang, Yulun Zhang,
  Ying Li, Xianglong Liu
Categories: cs.LG cs.AI cs.AR
Comments: 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2309.01945 ,  4424kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13160
replaced with revised version Thu, 8 Feb 2024 17:37:56 GMT   (10935kb,D)

Title: How to train your VAE
Authors: Mariano Rivera
Categories: cs.LG cs.AI cs.CV
Comments: 5 pages, 3 figures
MSC-class: 68T07
ACM-class: I.2.4; I.4.5
\\ ( https://arxiv.org/abs/2309.13160 ,  10935kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04353
replaced with revised version Thu, 8 Feb 2024 00:32:00 GMT   (1082kb,D)

Title: An In-Context Learning Agent for Formal Theorem-Proving
Authors: Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, Swarat
  Chaudhuri
Categories: cs.LG cs.AI cs.LO cs.PL
\\ ( https://arxiv.org/abs/2310.04353 ,  1082kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04521
replaced with revised version Wed, 7 Feb 2024 19:33:39 GMT   (1640kb,D)

Title: Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie
  Algebras
Authors: Tzu-Yuan Lin, Minghan Zhu, Maani Ghaffari
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.04521 ,  1640kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06312
replaced with revised version Wed, 7 Feb 2024 20:59:41 GMT   (3324kb,D)

Title: Discovering Mixtures of Structural Causal Models from Time Series Data
Authors: Sumanth Varambally, Yi-An Ma, Rose Yu
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.06312 ,  3324kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08278
replaced with revised version Thu, 8 Feb 2024 05:49:04 GMT   (2062kb,D)

Title: Lag-Llama: Towards Foundation Models for Probabilistic Time Series
  Forecasting
Authors: Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Hena Ghonia,
  Rishika Bhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, George
  Adamopoulos, Roland Riachi, Nadhir Hassen, Marin Bilo\v{s}, Sahil Garg,
  Anderson Schneider, Nicolas Chapados, Alexandre Drouin, Valentina
  Zantedeschi, Yuriy Nevmyvaka, Irina Rish
Categories: cs.LG cs.AI
Comments: First two authors contributed equally. All data, models and code used
  are open-source. GitHub:
  https://github.com/time-series-foundation-models/lag-llama
\\ ( https://arxiv.org/abs/2310.08278 ,  2062kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13367
replaced with revised version Thu, 8 Feb 2024 08:24:53 GMT   (23728kb,D)

Title: VFedMH: Vertical Federated Learning for Training Multiple Heterogeneous
  Models
Authors: Shuo Wang and Keke Gai and Jing Yu and Liehuang Zhu and Kim-Kwang
  Raymond Choo and Bin Xiao
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2310.13367 ,  23728kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19789
replaced with revised version Thu, 8 Feb 2024 12:31:18 GMT   (2329kb,D)

Title: DiffEnc: Variational Diffusion with a Learned Encoder
Authors: Beatrix M. G. Nielsen, Anders Christensen, Andrea Dittadi, Ole Winther
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/2310.19789 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00875
replaced with revised version Thu, 8 Feb 2024 18:17:03 GMT   (2019kb,D)

Title: Learning Collective Behaviors from Observation
Authors: Jinchao Feng and Ming Zhong
Categories: cs.LG cs.MA math.DS
\\ ( https://arxiv.org/abs/2311.00875 ,  2019kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18208
replaced with revised version Thu, 8 Feb 2024 03:46:12 GMT   (5159kb,D)

Title: SMaRt: Improving GANs with Score Matching Regularity
Authors: Mengfei Xia, Yujun Shen, Ceyuan Yang, Ran Yi, Wenping Wang, Yong-jin
  Liu
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2311.18208 ,  5159kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18780
replaced with revised version Thu, 8 Feb 2024 15:40:35 GMT   (389kb,D)

Title: MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for
  General Time Series Forecasting
Authors: Linfeng Du, Ji Xin, Alex Labach, Saba Zuberi, Maksims Volkovs, Rahul
  G. Krishnan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.18780 ,  389kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06837
replaced with revised version Wed, 7 Feb 2024 19:56:43 GMT   (1831kb,D)

Title: Spectral State Space Models
Authors: Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.06837 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10308
replaced with revised version Thu, 8 Feb 2024 05:20:18 GMT   (6189kb,D)

Title: Event-Based Contrastive Learning for Medical Time Series
Authors: Hyewon Jeong, Nassim Oufattole, Matthew Mcdermott, Aparna Balagopalan,
  Bryan Jangeesingh, Marzyeh Ghassemi, Collin Stultz
Categories: cs.LG
Comments: Accepted at Unifying Representations in Neural Models Workshop in
  NeurIPS 2023
\\ ( https://arxiv.org/abs/2312.10308 ,  6189kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10396
replaced with revised version Thu, 8 Feb 2024 16:58:43 GMT   (51kb)

Title: How Far Can Fairness Constraints Help Recover From Biased Data?
Authors: Mohit Sharma, Amit Deshpande
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.10396 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11441
replaced with revised version Thu, 8 Feb 2024 15:52:29 GMT   (165kb,D)

Title: Social Learning: Towards Collaborative Learning with Large Language
  Models
Authors: Amirkeivan Mohtashami, Florian Hartmann, Sian Gooding, Lukas Zilka,
  Matt Sharifi, Blaise Aguera y Arcas
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2312.11441 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12112
replaced with revised version Wed, 7 Feb 2024 19:00:35 GMT   (3306kb,D)

Title: Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation
  in ultra low-data regimes
Authors: Nabeel Seedat, Nicolas Huynh, Boris van Breugel, Mihaela van der
  Schaar
Categories: cs.LG cs.AI
Comments: *Seedat & Huynh contributed equally
\\ ( https://arxiv.org/abs/2312.12112 ,  3306kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13327
replaced with revised version Thu, 8 Feb 2024 12:30:09 GMT   (2950kb,D)

Title: In-Context Reinforcement Learning for Variable Action Spaces
Authors: Viacheslav Sinii, Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman,
  Sergey Kolesnikov
Categories: cs.LG cs.AI
Comments: Preprint, Under Review; code:
  https://github.com/corl-team/headless-ad
\\ ( https://arxiv.org/abs/2312.13327 ,  2950kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01172
replaced with revised version Thu, 8 Feb 2024 17:14:54 GMT   (13630kb,D)

Title: Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing
  Bearing Faults
Authors: Mohammad Al-Sa'd, Tuomas Jalonen, Serkan Kiranyaz, and Moncef Gabbouj
Categories: cs.LG cs.AI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2401.01172 ,  13630kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02740
replaced with revised version Thu, 8 Feb 2024 03:42:47 GMT   (21kb)

Title: Fairness-Aware Job Scheduling for Multi-Job Federated Learning
Authors: Yuxin Shi, Han Yu
Categories: cs.LG cs.AI cs.DC
Comments: accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2401.02740 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03756
replaced with revised version Thu, 8 Feb 2024 17:41:43 GMT   (43kb,D)

Title: Adaptive Experimental Design for Policy Learning
Authors: Masahiro Kato and Kyohei Okumura and Takuya Ishihara and Toru Kitagawa
Categories: cs.LG cs.AI econ.EM stat.ME stat.ML
Comments: arXiv admin note: text overlap with arXiv:2302.02988
\\ ( https://arxiv.org/abs/2401.03756 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09071
replaced with revised version Thu, 8 Feb 2024 04:21:43 GMT   (412kb,D)

Title: Rethinking Spectral Graph Neural Networks with Spatially Adaptive
  Filtering
Authors: Jingwei Guo, Kaizhu Huang, Xinping Yi, Zixian Su, and Rui Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.09071 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10371
replaced with revised version Wed, 7 Feb 2024 19:08:28 GMT   (737kb,D)

Title: Langevin Unlearning: A New Perspective of Noisy Gradient Descent for
  Machine Unlearning
Authors: Eli Chien, Haoyu Wang, Ziang Chen, Pan Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.10371 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16661
replaced with revised version Thu, 8 Feb 2024 10:13:19 GMT   (50kb)

Title: Generalization of LiNGAM that allows confounding
Authors: Joe Suzuki and Tian-Le Yang
Categories: cs.LG cs.IT math.IT math.ST stat.TH
Comments: arXiv admin note: text overlap with arXiv:2007.11131 by other authors
\\ ( https://arxiv.org/abs/2401.16661 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01350
replaced with revised version Thu, 8 Feb 2024 16:22:43 GMT   (3127kb,D)

Title: pFedMoE: Data-Level Personalization with Mixture of Experts for
  Model-Heterogeneous Personalized Federated Learning
Authors: Liping Yi, Han Yu, Chao Ren, Heng Zhang, Gang Wang, Xiaoguang Liu,
  Xiaoxiao Li
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2402.01350 ,  3127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02275
replaced with revised version Thu, 8 Feb 2024 18:35:26 GMT   (31350kb,D)

Title: SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing
  Applications using a Generative Approach
Authors: Tianshi Wang, Jinyang Li, Ruijie Wang, Denizhan Kara, Shengzhong Liu,
  Davis Wertheimer, Antoni Viros-i-Martin, Raghu Ganti, Mudhakar Srivatsa,
  Tarek Abdelzaher
Categories: cs.LG
Comments: Published in ACM Conference on Embedded Networked Sensor Systems
  (SenSys 23), November, 2023, Istanbul, Turkiye. This is the author's version
  of the work. It is posted here for your personal use. Not for redistribution.
  Publication rights licensed to the Association for Computing Machinery
DOI: 10.1145/3625687.3625785
\\ ( https://arxiv.org/abs/2402.02275 ,  31350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02616
replaced with revised version Thu, 8 Feb 2024 16:50:16 GMT   (0kb,I)

Title: The Virtues of Pessimism in Inverse Reinforcement Learning
Authors: David Wu and Gokul Swamy and J. Andrew Bagnell and Zhiwei Steven Wu
  and Sanjiban Choudhury
Categories: cs.LG
Comments: This paper has been withdrawn by the authors pending edits from other
  authors
\\ ( https://arxiv.org/abs/2402.02616 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04163
replaced with revised version Thu, 8 Feb 2024 09:30:24 GMT   (13979kb)

Title: Tempered Calculus for ML: Application to Hyperbolic Model Embedding
Authors: Richard Nock and Ehsan Amid and Frank Nielsen and Alexander Soen and
  Manfred K. Warmuth
Categories: cs.LG
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2402.04163 ,  13979kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04924
replaced with revised version Thu, 8 Feb 2024 02:38:14 GMT   (10753kb,D)

Title: Two Trades is not Baffled: Condensing Graph via Crafting Rational
  Gradient Matching
Authors: Tianle Zhang and Yuchen Zhang and Kun Wang and Kai Wang and Beining
  Yang and Kaipeng Zhang and Wenqi Shao and Ping Liu and Joey Tianyi Zhou and
  Yang You
Categories: cs.LG
Comments: An effective method for graph condensation
\\ ( https://arxiv.org/abs/2402.04924 ,  10753kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05050
replaced with revised version Thu, 8 Feb 2024 10:29:25 GMT   (462kb,D)

Title: Federated Learning Can Find Friends That Are Beneficial
Authors: Nazarii Tupitsa and Samuel Horv\'ath and Martin Tak\'a\v{c} and Eduard
  Gorbunov
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2402.05050 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03637
replaced with revised version Thu, 8 Feb 2024 10:54:32 GMT   (140kb)

Title: Boolean Observation Games
Authors: Hans van Ditmarsch and Sunil Simon
Categories: cs.GT cs.AI
Journal-ref: Journal of Artificial Intelligence Research, volume 79, 2024,
  pages 307-357
DOI: 10.1613/jair.1.15248
\\ ( https://arxiv.org/abs/2202.03637 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2210.08659
replaced with revised version Thu, 8 Feb 2024 17:19:53 GMT   (903kb)

Title: Towards More Efficient Shared Autonomous Mobility: A Learning-Based
  Fleet Repositioning Approach
Authors: Monika Filipovska, Michael Hyland, Haimanti Bala
Categories: eess.SY cs.AI cs.CY cs.SY
\\ ( https://arxiv.org/abs/2210.08659 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2301.00712 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 07:49:07 GMT   (835kb)

Title: On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness
  Results and Improved Analysis
Authors: Lesi Chen, Jing Xu and Jingzhao Zhang
Categories: math.OC cs.AI cs.LG
Comments: Add new upper bounds of nonconvex-PL bilevel problems compared to
  version 1 in 2023.1
\\ ( https://arxiv.org/abs/2301.00712 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14263
replaced with revised version Thu, 8 Feb 2024 07:06:39 GMT   (3496kb,D)

Title: Revolutionizing Cyber Threat Detection with Large Language Models: A
  privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices
Authors: Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C.
  Cordeiro, Merouane Debbah, Thierry Lestable, Narinderjit Singh Thandi
Categories: cs.CR cs.AI
Comments: This paper has been accepted for publication in IEEE Access:
  http://dx.doi.org/10.1109/ACCESS.2024.3363469
\\ ( https://arxiv.org/abs/2306.14263 ,  3496kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02312
replaced with revised version Wed, 7 Feb 2024 22:28:28 GMT   (1626kb)

Title: Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of
  ChatGPT Answers to Stack Overflow Questions
Authors: Samia Kabir, David N. Udo-Imeh, Bonan Kou, Tianyi Zhang
Categories: cs.SE cs.AI
Comments: This paper has been conditionally accepted for the CHI Conference on
  Human Factors in Computing Systems (CHI'24). The new version of this paper
  has been modified with updated discussions to address more stakeholders
\\ ( https://arxiv.org/abs/2308.02312 ,  1626kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12944
replaced with revised version Thu, 8 Feb 2024 11:56:03 GMT   (6830kb,D)

Title: DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution
  Mechanism for 5G and Beyond Solar Small Cell Networks
Authors: Daksh Dave, Vinay Chamola, Sandeep Joshi, Sherali Zeadally
Categories: cs.NI cs.AI cs.LG cs.NE
\\ ( https://arxiv.org/abs/2311.12944 ,  6830kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14301
replaced with revised version Thu, 8 Feb 2024 02:09:01 GMT   (1422kb,D)

Title: Autoencoder Based Face Verification System
Authors: Enoch Solomon, Abraham Woubie and Eyael Solomon Emiru
Categories: cs.CV cs.AI cs.CY
\\ ( https://arxiv.org/abs/2312.14301 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06550
replaced with revised version Thu, 8 Feb 2024 06:23:42 GMT   (11501kb)

Title: Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery
  and Geographical Prior
Authors: Chuanji Shi, Yingying Zhang, Jiaotuan Wang, Xin Guo and Qiqi Zhu
Categories: cs.CV cs.AI
Comments: 9 pages, 9 figures
MSC-class: 68T99
ACM-class: I.4.9
\\ ( https://arxiv.org/abs/2401.06550 ,  11501kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08103
replaced with revised version Thu, 8 Feb 2024 02:12:19 GMT   (214kb,D)

Title: Resolving Ethics Trade-offs in Implementing Responsible AI
Authors: Conrad Sanderson, Emma Schleiger, David Douglas, Petra Kuhnert,
  Qinghua Lu
Categories: cs.CY cs.AI
ACM-class: K.4.1
\\ ( https://arxiv.org/abs/2401.08103 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01703
replaced with revised version Thu, 8 Feb 2024 16:28:26 GMT   (286kb)

Title: A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver
  Interaction in Los Angeles
Authors: Benjamin A.T. Grahama, Lauren Brown, Georgios Chochlakis, Morteza
  Dehghani, Raquel Delerme, Brittany Friedman, Ellie Graeden, Preni Golazizian,
  Rajat Hebbar, Parsa Hejabi, Aditya Kommineni, Mayag\"uez Salinas, Michael
  Sierra-Ar\'evalo, Jackson Trager, Nicholas Weller, and Shrikanth Narayan
Categories: cs.CY cs.AI cs.LG eess.AS
Comments: 13 pages
ACM-class: I.2.0; I.2.7
\\ ( https://arxiv.org/abs/2402.01703 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01804
replaced with revised version Wed, 7 Feb 2024 21:14:36 GMT   (1251kb,D)

Title: Analysis of Internet of Things implementation barriers in the cold
  supply chain: an integrated ISM-MICMAC and DEMATEL approach
Authors: Kazrin Ahmad, Md. Saiful Islam, Md Abrar Jahin, and M. F. Mridha
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2402.01804 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03227
replaced with revised version Thu, 8 Feb 2024 10:40:26 GMT   (3069kb,D)

Title: IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of
  brain MR images
Authors: Vincent Roca, Gr\'egory Kuchcinski, Jean-Pierre Pruvo, Dorian
  Manouvriez, Renaud Lopes
Categories: cs.CV cs.AI cs.LG
Comments: 23 pages, 8 figures; typos corrected
\\ ( https://arxiv.org/abs/2402.03227 ,  3069kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02772
replaced with revised version Thu, 8 Feb 2024 10:09:18 GMT   (299kb)

Title: Attention or Convolution: Transformer Encoders in Audio Language Models
  for Inference Efficiency
Authors: Sungho Jeon, Ching-Feng Yeh, Hakan Inan, Wei-Ning Hsu, Rashi Rungta,
  Yashar Mehdad, Daniel Bikel
Categories: cs.SD cs.CL eess.AS
Comments: 5 pages; accepted to Self-supervision in Audio, Speech and Beyond
  (SASB) workshop in ICASSP24
\\ ( https://arxiv.org/abs/2311.02772 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02632
replaced with revised version Thu, 8 Feb 2024 14:24:40 GMT   (524kb,D)

Title: GIRT-Model: Automated Generation of Issue Report Templates
Authors: Nafiseh Nikeghbal, Amir Hossein Kargaran, Abbas Heydarnoori
Categories: cs.SE cs.CL
Comments: Accepted to be published at the 21st IEEE/ACM International
  Conference on Mining Software Repositories (MSR 2024)
DOI: 10.1145/3643991.3644906
\\ ( https://arxiv.org/abs/2402.02632 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03916
replaced with revised version Thu, 8 Feb 2024 16:09:36 GMT   (358kb,D)

Title: Can Large Language Models Detect Rumors on Social Media?
Authors: Qiang Liu, Xiang Tao, Junfei Wu, Shu Wu, Liang Wang
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2402.03916 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2102.10019 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 18:58:55 GMT   (1209kb,D)

Title: The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative
  Information
Authors: Claire Lazar Reich
Categories: stat.ML cs.LG econ.TH
\\ ( https://arxiv.org/abs/2102.10019 ,  1209kb)
------------------------------------------------------------------------------
\\
arXiv:2205.10200 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 15:19:14 GMT   (4081kb,D)

Title: The Fairness of Credit Scoring Models
Authors: Christophe Hurlin, Christophe P\'erignon, and S\'ebastien Saurin
Categories: stat.ML cs.LG q-fin.RM
\\ ( https://arxiv.org/abs/2205.10200 ,  4081kb)
------------------------------------------------------------------------------
\\
arXiv:2207.04950
replaced with revised version Thu, 8 Feb 2024 15:12:39 GMT   (68kb)

Title: Neural and spectral operator surrogates: unified construction and
  expression rate bounds
Authors: Lukas Herrmann, Christoph Schwab, Jakob Zech
Categories: math.NA cs.LG cs.NA
\\ ( https://arxiv.org/abs/2207.04950 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2211.12829
replaced with revised version Thu, 8 Feb 2024 03:02:22 GMT   (4366kb,D)

Title: Unsupervised 3D Keypoint Discovery with Multi-View Geometry
Authors: Sina Honari, Chen Zhao, Mathieu Salzmann, Pascal Fua
Categories: cs.CV cs.LG
Comments: Accepted in "3DV 2024"
\\ ( https://arxiv.org/abs/2211.12829 ,  4366kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02809 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 13:51:42 GMT   (44339kb,D)

Title: Listen2Scene: Interactive material-aware binaural sound propagation for
  reconstructed 3D scenes
Authors: Anton Ratnarajah, Dinesh Manocha
Categories: eess.AS cs.CV cs.LG cs.MM cs.SD
Comments: Accepted to IEEE VR 2024. Project page:
  https://anton-jeran.github.io/Listen2Scene/
\\ ( https://arxiv.org/abs/2302.02809 ,  44339kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10555
replaced with revised version Wed, 7 Feb 2024 21:28:45 GMT   (7689kb,D)

Title: LiDAR Spoofing Meets the New-Gen: Capability Improvements, Broken
  Assumptions, and New Attack Strategies
Authors: Takami Sato, Yuki Hayakawa, Ryo Suzuki, Yohsuke Shiiki, Kentaro
  Yoshioka, Qi Alfred Chen
Categories: cs.CR cs.CV cs.LG
Comments: The first 3 authors are co-first
Journal-ref: Network and Distributed System Security (NDSS) Symposium 2024
DOI: 10.14722/ndss.2024.23350
\\ ( https://arxiv.org/abs/2303.10555 ,  7689kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04359
replaced with revised version Thu, 8 Feb 2024 17:00:13 GMT   (1818kb,D)

Title: ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate
  Nearest Neighbor Search Algorithms
Authors: Magdalen Dobson Manohar, Zheqi Shen, Guy E. Blelloch, Laxman
  Dhulipala, Yan Gu, Harsha Vardhan Simhadri, Yihan Sun
Categories: cs.IR cs.LG
\\ ( https://arxiv.org/abs/2305.04359 ,  1818kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01710 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 13:26:57 GMT   (26632kb,D)

Title: A Data-Driven Measure of Relative Uncertainty for Misclassification
  Detection
Authors: Eduardo Dadalto, Marco Romanelli, Georg Pichler, and Pablo Piantanida
Categories: stat.ML cs.LG
Comments: Accepted in ICLR2024
MSC-class: 68T01
\\ ( https://arxiv.org/abs/2306.01710 ,  26632kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02574
replaced with revised version Wed, 7 Feb 2024 21:52:21 GMT   (999kb,D)

Title: Bayesian Learning of Optimal Policies in Markov Decision Processes with
  Countably Infinite State-Space
Authors: Saghar Adler, Vijay Subramanian
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2306.02574 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05957
replaced with revised version Thu, 8 Feb 2024 14:54:53 GMT   (41167kb,D)

Title: DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic
  Latent Particles
Authors: Tal Daniel, Aviv Tamar
Categories: cs.CV cs.LG
Comments: TMLR 2024. Project site: https://taldatech.github.io/ddlp-web
\\ ( https://arxiv.org/abs/2306.05957 ,  41167kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13149
replaced with revised version Wed, 7 Feb 2024 23:08:29 GMT   (3001kb,D)

Title: "Filling the Blanks'': Identifying Micro-activities that Compose Complex
  Human Activities of Daily Living
Authors: Soumyajit Chatterjee, Bivas Mitra and Sandip Chakraborty
Categories: cs.HC cs.LG
Comments: 23 pages, 4 tables, 7 figures
\\ ( https://arxiv.org/abs/2306.13149 ,  3001kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03748 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 17:47:12 GMT   (72kb,D)

Title: Incentive-Theoretic Bayesian Inference for Collaborative Science
Authors: Stephen Bates, Michael I. Jordan, Michael Sklar, Jake A. Soloff
Categories: stat.ME cs.GT cs.LG stat.ML
\\ ( https://arxiv.org/abs/2307.03748 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04191 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 12:09:37 GMT   (19kb)

Title: On the sample complexity of parameter estimation in logistic regression
  with normal design
Authors: Daniel Hsu, Arya Mazumdar
Categories: math.ST cs.IT cs.LG math.IT stat.ML stat.TH
\\ ( https://arxiv.org/abs/2307.04191 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04725
replaced with revised version Thu, 8 Feb 2024 18:08:57 GMT   (23087kb,D)

Title: AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models
  without Specific Tuning
Authors: Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu
  Qiao, Maneesh Agrawala, Dahua Lin, Bo Dai
Categories: cs.CV cs.GR cs.LG
Comments: Codes and Supplementary Material:
  https://github.com/guoyww/AnimateDiff
\\ ( https://arxiv.org/abs/2307.04725 ,  23087kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07688 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 09:06:00 GMT   (1479kb)

Title: Enhancing Network Initialization for Medical AI Models Using
  Large-Scale, Unlabeled Natural Images
Authors: Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel
  Truhn, Sven Nebelung
Categories: eess.IV cs.CV cs.LG
Comments: Published in European Radiology Experimental
Journal-ref: Eur Radiol Exp 8, 10 (2024)
DOI: 10.1186/s41747-023-00411-3
\\ ( https://arxiv.org/abs/2308.07688 ,  1479kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05102 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 14:06:38 GMT   (11kb)

Title: Is Learning in Biological Neural Networks based on Stochastic Gradient
  Descent? An analysis using stochastic processes
Authors: S\"oren Christensen and Jan Kallsen
Categories: q-bio.NC cs.LG cs.NE math.PR
MSC-class: 92C20, 68T07
\\ ( https://arxiv.org/abs/2309.05102 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03945 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 20:06:32 GMT   (390kb,D)

Title: On Wasserstein distances for affine transformations of random vectors
Authors: Keaton Hamm, Andrzej Korzeniowski
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.03945 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01210 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 10:21:04 GMT   (157kb,D)

Title: When accurate prediction models yield harmful self-fulfilling prophecies
Authors: Wouter A.C. van Amsterdam, Nan van Geloven, Jesse H. Krijthe, Rajesh
  Ranganath, Giovanni Cin\'a
Categories: stat.ME cs.LG stat.ML
\\ ( https://arxiv.org/abs/2312.01210 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03406
replaced with revised version Thu, 8 Feb 2024 02:55:08 GMT   (12265kb,D)

Title: SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting
Authors: Chao Chen, Tian Zhou, Yanjun Zhao, Hui Liu, Liang Sun, Rong Jin
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.03406 ,  12265kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17293 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 09:36:58 GMT   (3680kb,D)

Title: $\mu$GUIDE: a framework for microstructure imaging via generalized
  uncertainty-driven inference using deep learning
Authors: Ma\"eliss Jallais and Marco Palombo
Categories: eess.IV cs.LG physics.med-ph
\\ ( https://arxiv.org/abs/2312.17293 ,  3680kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02889
replaced with revised version Wed, 7 Feb 2024 21:38:35 GMT   (27852kb,D)

Title: Energy-Preserving Reduced Operator Inference for Efficient Design and
  Control
Authors: Tomoki Koike, Elizabeth Qian
Categories: math.NA cs.LG cs.NA math.DS
Comments: 17 pages, AIAA SciTech Forum 2024
DOI: 10.2514/6.2024-1012
\\ ( https://arxiv.org/abs/2401.02889 ,  27852kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04855
replaced with revised version Thu, 8 Feb 2024 16:05:17 GMT   (34445kb,D)

Title: LPAC: Learnable Perception-Action-Communication Loops with Applications
  to Coverage Control
Authors: Saurav Agarwal, Ramya Muthukrishnan, Walker Gosrich, Vijay Kumar,
  Alejandro Ribeiro
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2401.04855 ,  34445kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00418
replaced with revised version Thu, 8 Feb 2024 12:49:23 GMT   (464kb)

Title: Benchmarking Transferable Adversarial Attacks
Authors: Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen
Categories: cs.CV cs.LG
Comments: Accepted by NDSS 2024 Workshop
\\ ( https://arxiv.org/abs/2402.00418 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01393
replaced with revised version Thu, 8 Feb 2024 08:09:17 GMT   (4019kb,D)

Title: ALERT-Transformer: Bridging Asynchronous and Synchronous Machine
  Learning for Real-Time Event-based Spatio-Temporal Data
Authors: Carmen Martin-Turrero, Maxence Bouvier, Manuel Breitenstein, Pietro
  Zanuttigh, Vincent Parret
Categories: cs.CV cs.LG cs.NE
Comments: Preprint version. 8 pages, 7 figures, under review
MSC-class: 68T05
ACM-class: I.2.6; I.2.10; I.4.8; I.4.10; D.2.2; D.1.4
\\ ( https://arxiv.org/abs/2402.01393 ,  4019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02152
replaced with revised version Thu, 8 Feb 2024 12:58:36 GMT   (46kb)

Title: Position Paper: Why the Shooting in the Dark Method Dominates
  Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
Authors: David Rohde
Categories: cs.IR cs.LG stat.ML
Comments: 11 pages
\\ ( https://arxiv.org/abs/2402.02152 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02242
replaced with revised version Thu, 8 Feb 2024 08:17:57 GMT   (99kb,D)

Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
Authors: Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing
  Li, Yuntao Du
Categories: cs.CV cs.LG
Comments: 9 pages, 3 figures, 2 tables
\\ ( https://arxiv.org/abs/2402.02242 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03019
replaced with revised version Thu, 8 Feb 2024 02:34:21 GMT   (34029kb,D)

Title: Taylor Videos for Action Recognition
Authors: Lei Wang and Xiuyuan Yuan and Tom Gedeon and Liang Zheng
Categories: cs.CV cs.LG
Comments: Research report
\\ ( https://arxiv.org/abs/2402.03019 ,  34029kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03149
replaced with revised version Wed, 7 Feb 2024 22:32:03 GMT   (2968kb,D)

Title: A Comparative Analysis of Microrings Based Incoherent Photonic GEMM
  Accelerators
Authors: Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, Oluwaseun
  Adewunmi Alo, and Ishan Thakkar
Categories: cs.AR cs.ET cs.LG cs.NE
Comments: To Appear at ISQED 2024
\\ ( https://arxiv.org/abs/2402.03149 ,  2968kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04022 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 07:56:25 GMT   (2249kb,D)

Title: A General Theory for Kernel Packets: from state space model to compactly
  supported basis
Authors: Liang Ding and Rui Tuo
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.04022 ,  2249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04436 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 17:00:30 GMT   (12kb,D)

Title: Continuous Multidimensional Scaling
Authors: Michael W. Trosset, Carey E. Priebe
Categories: stat.ML cs.LG
Comments: 15 pages. Modified a sentence in the Abstract for greater clarity
MSC-class: 62H99
\\ ( https://arxiv.org/abs/2402.04436 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04825
replaced with revised version Thu, 8 Feb 2024 06:26:54 GMT   (547kb,D)

Title: Fast Timing-Conditioned Latent Audio Diffusion
Authors: Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, Jordi Pons
Categories: cs.SD cs.LG eess.AS
Comments: Code: https://github.com/Stability-AI/stable-audio-tools. Metrics:
  https://github.com/Stability-AI/stable-audio-metrics. Demo:
  https://stability-ai.github.io/stable-audio-demo
\\ ( https://arxiv.org/abs/2402.04825 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05067 (*cross-listing*)
replaced with revised version Thu, 8 Feb 2024 07:37:50 GMT   (4803kb,D)

Title: Multiscale Modelling with Physics-informed Neural Network: from
  Large-scale Dynamics to Small-scale Predictions in Complex Systems
Authors: Jing Wang and Zheng Li and Pengyu Lai and Rui Wang and Di Yang and
  Dewu Yang and Hui Xu
Categories: physics.flu-dyn cs.LG physics.comp-ph
\\ ( https://arxiv.org/abs/2402.05067 ,  4803kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
