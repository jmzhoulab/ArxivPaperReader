Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月22日 17:40
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 20 Feb 24 19:00:00 GMT  to  Wed 21 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.13264
Date: Sun, 11 Feb 2024 10:30:38 GMT   (638kb,D)

Title: KGroot: Enhancing Root Cause Analysis through Knowledge Graphs and Graph
  Convolutional Neural Networks
Authors: Tingting Wang, Guilin Qi, Tianxing Wu
Categories: cs.AI
\\
  Fault localization is challenging in online micro-service due to the wide
variety of monitoring data volume, types, events and complex interdependencies
in service and components. Faults events in services are propagative and can
trigger a cascade of alerts in a short period of time. In the industry, fault
localization is typically conducted manually by experienced personnel. This
reliance on experience is unreliable and lacks automation. Different modules
present information barriers during manual localization, making it difficult to
quickly align during urgent faults. This inefficiency lags stability assurance
to minimize fault detection and repair time. Though actionable methods aimed to
automatic the process, the accuracy and efficiency are less than satisfactory.
The precision of fault localization results is of paramount importance as it
underpins engineers trust in the diagnostic conclusions, which are derived from
multiple perspectives and offer comprehensive insights. Therefore, a more
reliable method is required to automatically identify the associative
relationships among fault events and propagation path. To achieve this, KGroot
uses event knowledge and the correlation between events to perform root cause
reasoning by integrating knowledge graphs and GCNs for RCA. FEKG is built based
on historical data, an online graph is constructed in real-time when a failure
event occurs, and the similarity between each knowledge graph and online graph
is compared using GCNs to pinpoint the fault type through a ranking strategy.
Comprehensive experiments demonstrate KGroot can locate the root cause with
accuracy of 93.5% top 3 potential causes in second-level. This performance
matches the level of real-time fault diagnosis in the industrial environment
and significantly surpasses state-of-the-art baselines in RCA in terms of
effectiveness and efficiency.
\\ ( https://arxiv.org/abs/2402.13264 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13272
Date: Fri, 16 Feb 2024 22:41:13 GMT   (23kb)

Title: Spontaneous Theory of Mind for Artificial Intelligence
Authors: Nikolos Gurney, David V. Pynadath, Volkan Ustun
Categories: cs.AI cs.HC
\\
  Existing approaches to Theory of Mind (ToM) in Artificial Intelligence (AI)
overemphasize prompted, or cue-based, ToM, which may limit our collective
ability to develop Artificial Social Intelligence (ASI). Drawing from research
in computer science, cognitive science, and related disciplines, we contrast
prompted ToM with what we call spontaneous ToM -- reasoning about others'
mental states that is grounded in unintentional, possibly uncontrollable
cognitive functions. We argue for a principled approach to studying and
developing AI ToM and suggest that a robust, or general, ASI will respond to
prompts \textit{and} spontaneously engage in social reasoning.
\\ ( https://arxiv.org/abs/2402.13272 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13273
Date: Fri, 16 Feb 2024 22:45:09 GMT   (40kb)

Title: Operational Collective Intelligence of Humans and Machines
Authors: Nikolos Gurney, Fred Morstatter, David V. Pynadath, Adam Russell, Gleb
  Satyukov
Categories: cs.AI cs.HC
\\
  We explore the use of aggregative crowdsourced forecasting (ACF) as a
mechanism to help operationalize ``collective intelligence'' of human-machine
teams for coordinated actions. We adopt the definition for Collective
Intelligence as: ``A property of groups that emerges from synergies among
data-information-knowledge, software-hardware, and individuals (those with new
insights as well as recognized authorities) that enables just-in-time knowledge
for better decisions than these three elements acting alone.'' Collective
Intelligence emerges from new ways of connecting humans and AI to enable
decision-advantage, in part by creating and leveraging additional sources of
information that might otherwise not be included. Aggregative crowdsourced
forecasting (ACF) is a recent key advancement towards Collective Intelligence
wherein predictions (X\% probability that Y will happen) and rationales (why I
believe it is this probability that X will happen) are elicited independently
from a diverse crowd, aggregated, and then used to inform higher-level
decision-making. This research asks whether ACF, as a key way to enable
Operational Collective Intelligence, could be brought to bear on operational
scenarios (i.e., sequences of events with defined agents, components, and
interactions) and decision-making, and considers whether such a capability
could provide novel operational capabilities to enable new forms of
decision-advantage.
\\ ( https://arxiv.org/abs/2402.13273 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13290
Date: Mon, 19 Feb 2024 17:44:34 GMT   (1065kb,D)

Title: Grounding from an AI and Cognitive Science Lens
Authors: Goonmeet Bajaj, Srinivasan Parthasarathy, Valerie L. Shalin, Amit
  Sheth
Categories: cs.AI
Journal-ref: IEEE Intelligent Systems, 2024
DOI: 10.1109/MIS.2024.3366669
\\
  Grounding is a challenging problem, requiring a formal definition and
different levels of abstraction. This article explores grounding from both
cognitive science and machine learning perspectives. It identifies the
subtleties of grounding, its significance for collaborative agents, and
similarities and differences in grounding approaches in both communities. The
article examines the potential of neuro-symbolic approaches tailored for
grounding tasks, showcasing how they can more comprehensively address
grounding. Finally, we discuss areas for further exploration and development in
grounding.
\\ ( https://arxiv.org/abs/2402.13290 ,  1065kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13380
Date: Tue, 20 Feb 2024 21:13:38 GMT   (720kb,D)

Title: Toward TransfORmers: Revolutionizing the Solution of Mixed Integer
  Programs with Transformers
Authors: Joshua F. Cooper, Seung Jin Choi, and I. Esra Buyuktahtakin
Categories: cs.AI cs.LG math.CO math.OC stat.ML
\\
  In this study, we introduce an innovative deep learning framework that
employs a transformer model to address the challenges of mixed-integer
programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP).
Our approach, to our knowledge, is the first to utilize transformers to predict
the binary variables of a mixed-integer programming (MIP) problem.
Specifically, our approach harnesses the encoder decoder transformer's ability
to process sequential data, making it well-suited for predicting binary
variables indicating production setup decisions in each period of the CLSP.
This problem is inherently dynamic, and we need to handle sequential decision
making under constraints. We present an efficient algorithm in which CLSP
solutions are learned through a transformer neural network. The proposed
post-processed transformer algorithm surpasses the state-of-the-art solver,
CPLEX and Long Short-Term Memory (LSTM) in solution time, optimal gap, and
percent infeasibility over 240K benchmark CLSP instances tested. After the ML
model is trained, conducting inference on the model, including post-processing,
reduces the MIP into a linear program (LP). This transforms the ML-based
algorithm, combined with an LP solver, into a polynomial-time approximation
algorithm to solve a well-known NP-Hard problem, with almost perfect solution
quality.
\\ ( https://arxiv.org/abs/2402.13380 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13399
Date: Tue, 20 Feb 2024 21:58:40 GMT   (4501kb,D)

Title: Learning and Sustaining Shared Normative Systems via Bayesian Rule
  Induction in Markov Games
Authors: Ninell Oldenburg and Tan Zhi-Xuan
Categories: cs.AI
Comments: 18 pages, The 23rd International Conference on Autonomous Agents and
  Multi-Agent Systems
ACM-class: I.2.0; I.6.5; G.3
\\
  A universal feature of human societies is the adoption of systems of rules
and norms in the service of cooperative ends. How can we build learning agents
that do the same, so that they may flexibly cooperate with the human
institutions they are embedded in? We hypothesize that agents can achieve this
by assuming there exists a shared set of norms that most others comply with
while pursuing their individual desires, even if they do not know the exact
content of those norms. By assuming shared norms, a newly introduced agent can
infer the norms of an existing population from observations of compliance and
violation. Furthermore, groups of agents can converge to a shared set of norms,
even if they initially diverge in their beliefs about what the norms are. This
in turn enables the stability of the normative system: since agents can
bootstrap common knowledge of the norms, this leads the norms to be widely
adhered to, enabling new entrants to rapidly learn those norms. We formalize
this framework in the context of Markov games and demonstrate its operation in
a multi-agent environment via approximately Bayesian rule induction of
obligative and prohibitive norms. Using our approach, agents are able to
rapidly learn and sustain a variety of cooperative institutions, including
resource management norms and compensation for pro-social labor, promoting
collective welfare while still allowing agents to act in their own interests.
\\ ( https://arxiv.org/abs/2402.13399 ,  4501kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13419
Date: Tue, 20 Feb 2024 23:17:07 GMT   (36kb)

Title: Reward Bound for Behavioral Guarantee of Model-based Planning Agents
Authors: Zhiyu An, Xianzhong Ding, Wan Du
Categories: cs.AI
Comments: To be published in ICLR 24 tiny paper track
\\
  Recent years have seen an emerging interest in the trustworthiness of machine
learning-based agents in the wild, especially in robotics, to provide safety
assurance for the industry. Obtaining behavioral guarantees for these agents
remains an important problem. In this work, we focus on guaranteeing a
model-based planning agent reaches a goal state within a specific future time
step. We show that there exists a lower bound for the reward at the goal state,
such that if the said reward is below that bound, it is impossible to obtain
such a guarantee. By extension, we show how to enforce preferences over
multiple goals.
\\ ( https://arxiv.org/abs/2402.13419 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13427
Date: Tue, 20 Feb 2024 23:38:46 GMT   (870kb)

Title: Quantitative causality, causality-guided scientific discovery, and
  causal machine learning
Authors: X. San Liang, Dake Chen and Renhe Zhang
Categories: cs.AI physics.data-an
Comments: 10 pages, 3 figures. To appear in Ocean-Land-Atmosphere Research.
  arXiv admin note: substantial text overlap with arXiv:2112.14839
DOI: 10.34133/olar.0026
\\
  It has been said, arguably, that causality analysis should pave a promising
way to interpretable deep learning and generalization. Incorporation of
causality into artificial intelligence (AI) algorithms, however, is challenged
with its vagueness, non-quantitiveness, computational inefficiency, etc. During
the past 18 years, these challenges have been essentially resolved, with the
establishment of a rigorous formalism of causality analysis initially motivated
from atmospheric predictability. This not only opens a new field in the
atmosphere-ocean science, namely, information flow, but also has led to
scientific discoveries in other disciplines, such as quantum mechanics,
neuroscience, financial economics, etc., through various applications. This
note provides a brief review of the decade-long effort, including a list of
major theoretical results, a sketch of the causal deep learning framework, and
some representative real-world applications in geoscience pertaining to this
journal, such as those on the anthropogenic cause of global warming, the
decadal prediction of El Ni\~no Modoki, the forecasting of an extreme drought
in China, among others.
\\ ( https://arxiv.org/abs/2402.13427 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13440
Date: Wed, 21 Feb 2024 00:16:08 GMT   (27179kb,D)

Title: A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and
  Probabilistic Decision Making
Authors: Chitra Subramanian and Miao Liu and Naweed Khan and Jonathan Lenchner
  and Aporva Amarnath and Sarathkrishna Swaminathan and Ryan Riegel and
  Alexander Gray
Categories: cs.AI cs.NE
ACM-class: I.2.6
\\
  Multi-agent reinforcement learning (MARL) is well-suited for runtime
decision-making in optimizing the performance of systems where multiple agents
coexist and compete for shared resources. However, applying common deep
learning-based MARL solutions to real-world problems suffers from issues of
interpretability, sample efficiency, partial observability, etc. To address
these challenges, we present an event-driven formulation, where decision-making
is handled by distributed co-operative MARL agents using neuro-symbolic
methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN)
framework serves as a function approximator for the RL, to train a rules-based
policy that is both logical and interpretable by construction. To enable
decision-making under uncertainty and partial observability, we developed a
novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural
Networks (PLNN), which combines the capabilities of logical reasoning with
probabilistic graphical models. In PLNN, the upward/downward inference
strategy, inherited from LNN, is coupled with belief bounds by setting the
activation function for the logical operator associated with each neural
network node to a probability-respecting generalization of the Fr\'echet
inequalities. These PLNN nodes form the unifying element that combines
probabilistic logic and Bayes Nets, permitting inference for variables with
unobserved states. We demonstrate our contributions by addressing key MARL
challenges for power sharing in a system-on-chip application.
\\ ( https://arxiv.org/abs/2402.13440 ,  27179kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13582
Date: Wed, 21 Feb 2024 07:26:06 GMT   (1403kb,D)

Title: Mastering the Game of Guandan with Deep Reinforcement Learning and
  Behavior Regulating
Authors: Yifan Yanggong, Hao Pan, Lei Wang
Categories: cs.AI cs.LG
\\
  Games are a simplified model of reality and often serve as a favored platform
for Artificial Intelligence (AI) research. Much of the research is concerned
with game-playing agents and their decision making processes. The game of
Guandan (literally, "throwing eggs") is a challenging game where even
professional human players struggle to make the right decision at times. In
this paper we propose a framework named GuanZero for AI agents to master this
game using Monte-Carlo methods and deep neural networks. The main contribution
of this paper is about regulating agents' behavior through a carefully designed
neural network encoding scheme. We then demonstrate the effectiveness of the
proposed framework by comparing it with state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2402.13582 ,  1403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13615
Date: Wed, 21 Feb 2024 08:40:04 GMT   (82kb,D)

Title: Analyizing the Conjunction Fallacy as a Fact
Authors: Tomas Veloz, Olha Sobetska
Categories: cs.AI math.PR nlin.AO
Comments: book chapter
Journal-ref: In: Veloz, Khrennikov, Toni, Castillo (eds) Trends and Challenges
  in Cognitive Modeling. STEAM-H: Springer (2023)
\\
  Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has
been the subject of multiple debates and become a fundamental challenge for
cognitive theories in decision-making. In this article, we take a rather
uncommon perspective on this phenomenon. Instead of trying to explain the
nature or causes of the conjunction fallacy (intensional definition), we
analyze its range of factual possibilities (extensional definition). We show
that the majority of research on the conjunction fallacy, according to our
sample of experiments reviewed which covers literature between 1983 and 2016,
has focused on a narrow part of the a priori factual possibilities, implying
that explanations of the conjunction fallacy are fundamentally biased by the
short scope of possibilities explored. The latter is a rather curious aspect of
the research evolution in the conjunction fallacy considering that the very
nature of it is motivated by extensional considerations.
\\ ( https://arxiv.org/abs/2402.13615 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13782
Date: Wed, 21 Feb 2024 13:06:52 GMT   (142kb,D)

Title: Semirings for Probabilistic and Neuro-Symbolic Logic Programming
Authors: Vincent Derkinderen, Robin Manhaeve, Pedro Zuidberg Dos Martires, Luc
  De Raedt
Categories: cs.AI
Journal-ref: International Journal of Approximate Reasoning (2024): 109130
DOI: 10.1016/j.ijar.2024.109130
\\
  The field of probabilistic logic programming (PLP) focuses on integrating
probabilistic models into programming languages based on logic. Over the past
30 years, numerous languages and frameworks have been developed for modeling,
inference and learning in probabilistic logic programs. While originally PLP
focused on discrete probability, more recent approaches have incorporated
continuous distributions as well as neural networks, effectively yielding
neural-symbolic methods. We provide a unified algebraic perspective on PLP,
showing that many if not most of the extensions of PLP can be cast within a
common algebraic logic programming framework, in which facts are labeled with
elements of a semiring and disjunction and conjunction are replaced by addition
and multiplication. This does not only hold for the PLP variations itself but
also for the underlying execution mechanism that is based on (algebraic) model
counting.
\\ ( https://arxiv.org/abs/2402.13782 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13785
Date: Wed, 21 Feb 2024 13:10:58 GMT   (5811kb,D)

Title: Synthesis of Hierarchical Controllers Based on Deep Reinforcement
  Learning Policies
Authors: Florent Delgrange, Guy Avni, Anna Lukina, Christian Schilling, Ann
  Now\'e, and Guillermo A. P\'erez
Categories: cs.AI
Comments: 19 pages main text, 17 pages Appendix (excluding references)
\\
  We propose a novel approach to the problem of controller design for
environments modeled as Markov decision processes (MDPs). Specifically, we
consider a hierarchical MDP a graph with each vertex populated by an MDP called
a "room". We first apply deep reinforcement learning (DRL) to obtain low-level
policies for each room, scaling to large rooms of unknown structure. We then
apply reactive synthesis to obtain a high-level planner that chooses which
low-level policy to execute in each room. The central challenge in synthesizing
the planner is the need for modeling rooms. We address this challenge by
developing a DRL procedure to train concise "latent" policies together with PAC
guarantees on their performance. Unlike previous approaches, ours circumvents a
model distillation step. Our approach combats sparse rewards in DRL and enables
reusability of low-level policies. We demonstrate feasibility in a case study
involving agent navigation amid moving obstacles.
\\ ( https://arxiv.org/abs/2402.13785 ,  5811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13914
Date: Wed, 21 Feb 2024 16:30:24 GMT   (7018kb,D)

Title: Explain to Question not to Justify
Authors: Przemyslaw Biecek, Wojciech Samek
Categories: cs.AI cs.CR cs.LG
\\
  Explainable Artificial Intelligence (XAI) is a young but very promising field
of research. Unfortunately, the progress in this field is currently slowed down
by divergent and incompatible goals. In this paper, we separate various threads
tangled within the area of XAI into two complementary cultures of
human/value-oriented explanations (BLUE XAI) and model/validation-oriented
explanations (RED XAI). We also argue that the area of RED XAI is currently
under-explored and hides great opportunities and potential for important
research necessary to ensure the safety of AI systems. We conclude this paper
by presenting promising challenges in this area.
\\ ( https://arxiv.org/abs/2402.13914 ,  7018kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13927
Date: Wed, 21 Feb 2024 16:48:07 GMT   (2707kb,D)

Title: The Delusional Hedge Algorithm as a Model of Human Learning from Diverse
  Opinions
Authors: Yun-Shiuan Chuang, Jerry Zhu, Timothy T. Rogers
Categories: cs.AI
\\
  Whereas cognitive models of learning often assume direct experience with both
the features of an event and with a true label or outcome, much of everyday
learning arises from hearing the opinions of others, without direct access to
either the experience or the ground truth outcome. We consider how people can
learn which opinions to trust in such scenarios by extending the hedge
algorithm: a classic solution for learning from diverse information sources. We
first introduce a semi-supervised variant we call the delusional hedge capable
of learning from both supervised and unsupervised experiences. In two
experiments, we examine the alignment between human judgments and predictions
from the standard hedge, the delusional hedge, and a heuristic baseline model.
Results indicate that humans effectively incorporate both labeled and unlabeled
information in a manner consistent with the delusional hedge algorithm --
suggesting that human learners not only gauge the accuracy of information
sources but also their consistency with other reliable sources. The findings
advance our understanding of human learning from diverse opinions, with
implications for the development of algorithms that better capture how people
learn to weigh conflicting information sources.
\\ ( https://arxiv.org/abs/2402.13927 ,  2707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13302
Date: Tue, 20 Feb 2024 13:47:51 GMT   (240kb)

Title: Enhancing Modern Supervised Word Sense Disambiguation Models by Semantic
  Lexical Resources
Authors: Stefano Melacci and Achille Globo and Leonardo Rigutini
Categories: cs.CL
Comments: The 11th International Conference on Language Resources and
  Evaluation (LREC 2018)
Journal-ref: Proceedings of The 11th International Conference on Language
  Resources and Evaluation (LREC 2018)
\\
  Supervised models for Word Sense Disambiguation (WSD) currently yield to
state-of-the-art results in the most popular benchmarks. Despite the recent
introduction of Word Embeddings and Recurrent Neural Networks to design
powerful context-related features, the interest in improving WSD models using
Semantic Lexical Resources (SLRs) is mostly restricted to knowledge-based
approaches. In this paper, we enhance "modern" supervised WSD models exploiting
two popular SLRs: WordNet and WordNet Domains. We propose an effective way to
introduce semantic features into the classifiers, and we consider using the SLR
structure to augment the training data. We study the effect of different types
of semantic features, investigating their interaction with local contexts
encoded by means of mixtures of Word Embeddings or Recurrent Neural Networks,
and we extend the proposed model into a novel multi-layer architecture for WSD.
A detailed experimental comparison in the recent Unified Evaluation Framework
(Raganato et al., 2017) shows that the proposed approach leads to supervised
models that compare favourably with the state-of-the art.
\\ ( https://arxiv.org/abs/2402.13302 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13331
Date: Tue, 20 Feb 2024 19:19:47 GMT   (907kb,D)

Title: Enhanced Hallucination Detection in Neural Machine Translation through
  Simple Detector Aggregation
Authors: Anas Himmi, Guillaume Staerman, Marine Picot, Pierre Colombo, Nuno M.
  Guerreiro
Categories: cs.CL
\\
  Hallucinated translations pose significant threats and safety concerns when
it comes to the practical deployment of machine translation systems. Previous
research works have identified that detectors exhibit complementary performance
different detectors excel at detecting different types of hallucinations. In
this paper, we propose to address the limitations of individual detectors by
combining them and introducing a straightforward method for aggregating
multiple detectors. Our results demonstrate the efficacy of our aggregated
detector, providing a promising step towards evermore reliable machine
translation systems.
\\ ( https://arxiv.org/abs/2402.13331 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13350
Date: Tue, 20 Feb 2024 19:53:36 GMT   (140kb,D)

Title: PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text
  Retrieval Methods
Authors: S{\l}awomir Dadas, Micha{\l} Pere{\l}kiewicz, Rafa{\l} Po\'swiata
Categories: cs.CL
\\
  We present Polish Information Retrieval Benchmark (PIRB), a comprehensive
evaluation framework encompassing 41 text information retrieval tasks for
Polish. The benchmark incorporates existing datasets as well as 10 new,
previously unpublished datasets covering diverse topics such as medicine, law,
business, physics, and linguistics. We conduct an extensive evaluation of over
20 dense and sparse retrieval models, including the baseline models trained by
us as well as other available Polish and multilingual methods. Finally, we
introduce a three-step process for training highly effective language-specific
retrievers, consisting of knowledge distillation, supervised fine-tuning, and
building sparse-dense hybrid retrievers using a lightweight rescoring model. In
order to validate our approach, we train new text encoders for Polish and
compare their results with previously evaluated methods. Our dense models
outperform the best solutions available to date, and the use of hybrid methods
further improves their performance.
\\ ( https://arxiv.org/abs/2402.13350 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13364
Date: Tue, 20 Feb 2024 20:42:02 GMT   (223kb,D)

Title: A Simple but Effective Approach to Improve Structured Language Model
  Output for Information Extraction
Authors: Yinghao Li, Rampi Ramprasad, Chao Zhang
Categories: cs.CL cs.IR
Comments: 15 pages, 5 figures, 5 tables
\\
  Large language models (LLMs) have demonstrated impressive abilities in
generating unstructured natural language according to instructions. However,
their performance can be inconsistent when tasked with producing text that
adheres to specific structured formats, which is crucial in applications like
named entity recognition (NER) or relation extraction (RE). To address this
issue, this paper introduces an efficient method, G&O, to enhance their
structured text generation capabilities. It breaks the generation into a
two-step pipeline: initially, LLMs generate answers in natural language as
intermediate responses. Subsequently, LLMs are asked to organize the output
into the desired structure, using the intermediate responses as context. G&O
effectively separates the generation of content from the structuring process,
reducing the pressure of completing two orthogonal tasks simultaneously. Tested
on zero-shot NER and RE, the results indicate a significant improvement in LLM
performance with minimal additional efforts. This straightforward and adaptable
prompting technique can also be combined with other strategies, like
self-consistency, to further elevate LLM capabilities in various structured
text generation tasks.
\\ ( https://arxiv.org/abs/2402.13364 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13372
Date: Tue, 20 Feb 2024 20:53:24 GMT   (2474kb,D)

Title: EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human
  Adversaries
Authors: Jing Han Sun and Ali Emami
Categories: cs.CL
Comments: Accepted for publication in main proceedings of LREC-COLING 2024, 16
  pages, 3 figures
\\
  While Large Language Models (LLMs) excel at the Winograd Schema Challenge
(WSC), a coreference resolution task testing common-sense reasoning through
pronoun disambiguation, they struggle with instances that feature minor
alterations or rewording. To address this, we introduce EvoGrad, an open-source
platform that harnesses a human-in-the-loop approach to create a dynamic
dataset tailored to such altered WSC instances. Leveraging ChatGPT's
capabilities, we expand our task instances from 182 to 3,691, setting a new
benchmark for diverse common-sense reasoning datasets. Additionally, we
introduce the error depth metric, assessing model stability in dynamic tasks.
Our results emphasize the challenge posed by EvoGrad: Even the best performing
LLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2,
a stark contrast to human performance of 92. 8% accuracy without perturbation
errors. This highlights ongoing model limitations and the value of dynamic
datasets in uncovering them.
\\ ( https://arxiv.org/abs/2402.13372 ,  2474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13374
Date: Tue, 20 Feb 2024 20:57:47 GMT   (8961kb,D)

Title: Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems
Authors: Ivan Sekuli\'c, Silvia Terragni, Victor Guimar\~aes, Nghia Khau, Bruna
  Guedes, Modestas Filipavicius, Andr\'e Ferreira Manso, Roland Mathis
Categories: cs.CL
\\
  In the realm of dialogue systems, user simulation techniques have emerged as
a game-changer, redefining the evaluation and enhancement of task-oriented
dialogue (TOD) systems. These methods are crucial for replicating real user
interactions, enabling applications like synthetic data augmentation, error
detection, and robust evaluation. However, existing approaches often rely on
rigid rule-based methods or on annotated data. This paper introduces DAUS, a
Domain-Aware User Simulator. Leveraging large language models, we fine-tune
DAUS on real examples of task-oriented dialogues. Results on two relevant
benchmarks showcase significant improvements in terms of user goal fulfillment.
Notably, we have observed that fine-tuning enhances the simulator's coherence
with user goals, effectively mitigating hallucinations -- a major source of
inconsistencies in simulator responses.
\\ ( https://arxiv.org/abs/2402.13374 ,  8961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13405
Date: Tue, 20 Feb 2024 22:19:56 GMT   (1936kb,D)

Title: A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set
  Expansion and Taxonomy Expansion
Authors: Yanzhen Shen, Yu Zhang, Yunyi Zhang, Jiawei Han
Categories: cs.CL
\\
  Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy
Construction are three representative tasks that can be used to automatically
populate an existing taxonomy with new entities. However, previous approaches
often address these tasks separately with heterogeneous techniques, lacking a
unified perspective. To tackle this issue, in this paper, we identify the
common key skills needed for these tasks from the view of taxonomy structures
-- finding 'siblings' and finding 'parents' -- and propose a unified
taxonomy-guided instruction tuning framework to jointly solve the three tasks.
To be specific, by leveraging the existing taxonomy as a rich source of entity
relationships, we utilize instruction tuning to fine-tune a large language
model to generate parent and sibling entities. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct,
which outperforms task-specific baselines across all three tasks.
\\ ( https://arxiv.org/abs/2402.13405 ,  1936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13408
Date: Tue, 20 Feb 2024 22:26:35 GMT   (8956kb,D)

Title: Healthcare Copilot: Eliciting the Power of General LLMs for Medical
  Consultation
Authors: Zhiyao Ren, Yibing Zhan, Baosheng Yu, Liang Ding, Dacheng Tao
Categories: cs.CL
\\
  The copilot framework, which aims to enhance and tailor large language models
(LLMs) for specific complex tasks without requiring fine-tuning, is gaining
increasing attention from the community. In this paper, we introduce the
construction of a Healthcare Copilot designed for medical consultation. The
proposed Healthcare Copilot comprises three main components: 1) the Dialogue
component, responsible for effective and safe patient interactions; 2) the
Memory component, storing both current conversation data and historical patient
information; and 3) the Processing component, summarizing the entire dialogue
and generating reports. To evaluate the proposed Healthcare Copilot, we
implement an auto-evaluation scheme using ChatGPT for two roles: as a virtual
patient engaging in dialogue with the copilot, and as an evaluator to assess
the quality of the dialogue. Extensive results demonstrate that the proposed
Healthcare Copilot significantly enhances the capabilities of general LLMs for
medical consultations in terms of inquiry capability, conversational fluency,
response accuracy, and safety. Furthermore, we conduct ablation studies to
highlight the contribution of each individual module in the Healthcare Copilot.
Code will be made publicly available on GitHub.
\\ ( https://arxiv.org/abs/2402.13408 ,  8956kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13415
Date: Tue, 20 Feb 2024 22:56:23 GMT   (548kb,D)

Title: Structure Guided Prompt: Instructing Large Language Model in Multi-Step
  Reasoning by Exploring Graph Structure of the Text
Authors: Kewei Cheng, Nesreen K. Ahmed, Theodore Willke, Yizhou Sun
Categories: cs.CL
\\
  Although Large Language Models (LLMs) excel at addressing straightforward
reasoning tasks, they frequently struggle with difficulties when confronted by
more complex multi-step reasoning due to a range of factors. Firstly, natural
language often encompasses complex relationships among entities, making it
challenging to maintain a clear reasoning chain over longer spans. Secondly,
the abundance of linguistic diversity means that the same entities and
relationships can be expressed using different terminologies and structures,
complicating the task of identifying and establishing connections between
multiple pieces of information. Graphs provide an effective solution to
represent data rich in relational information and capture long-term
dependencies among entities. To harness the potential of graphs, our paper
introduces Structure Guided Prompt, an innovative three-stage task-agnostic
prompting framework designed to improve the multi-step reasoning capabilities
of LLMs in a zero-shot setting. This framework explicitly converts unstructured
text into a graph via LLMs and instructs them to navigate this graph using
task-specific strategies to formulate responses. By effectively organizing
information and guiding navigation, it enables LLMs to provide more accurate
and context-aware responses. Our experiments show that this framework
significantly enhances the reasoning capabilities of LLMs, enabling them to
excel in a broader spectrum of natural language scenarios.
\\ ( https://arxiv.org/abs/2402.13415 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13426
Date: Tue, 20 Feb 2024 23:38:39 GMT   (7043kb,D)

Title: Explaining Relationships Among Research Papers
Authors: Xiangci Li and Jessica Ouyang
Categories: cs.CL
\\
  Due to the rapid pace of research publications, keeping up to date with all
the latest related papers is very time-consuming, even with daily feed tools.
There is a need for automatically generated, short, customized literature
reviews of sets of papers to help researchers decide what to read. While
several works in the last decade have addressed the task of explaining a single
research paper, usually in the context of another paper citing it, the
relationship among multiple papers has been ignored; prior works have focused
on generating a single citation sentence in isolation, without addressing the
expository and transition sentences needed to connect multiple papers in a
coherent story. In this work, we explore a feature-based, LLM-prompting
approach to generate richer citation texts, as well as generating multiple
citations at once to capture the complex relationships among research papers.
We perform an expert evaluation to investigate the impact of our proposed
features on the quality of the generated paragraphs and find a strong
correlation between human preference and integrative writing style, suggesting
that humans prefer high-level, abstract citations, with transition sentences
between them to provide an overall story.
\\ ( https://arxiv.org/abs/2402.13426 ,  7043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13432
Date: Tue, 20 Feb 2024 23:54:02 GMT   (1150kb,D)

Title: DrBenchmark: A Large Language Understanding Evaluation Benchmark for
  French Biomedical Domain
Authors: Yanis Labrak, Adrien Bazoge, Oumaima El Khettari, Mickael Rouvier,
  Pacome Constant dit Beaufils, Natalia Grabar, Beatrice Daille, Solen Quiniou,
  Emmanuel Morin, Pierre-Antoine Gourraud, Richard Dufour
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-Coling 2024
\\
  The biomedical domain has sparked a significant interest in the field of
Natural Language Processing (NLP), which has seen substantial advancements with
pre-trained language models (PLMs). However, comparing these models has proven
challenging due to variations in evaluation protocols across different models.
A fair solution is to aggregate diverse downstream tasks into a benchmark,
allowing for the assessment of intrinsic PLMs qualities from various
perspectives. Although still limited to few languages, this initiative has been
undertaken in the biomedical field, notably English and Chinese. This
limitation hampers the evaluation of the latest French biomedical models, as
they are either assessed on a minimal number of tasks with non-standardized
protocols or evaluated using general downstream tasks. To bridge this research
gap and account for the unique sensitivities of French, we present the
first-ever publicly available French biomedical language understanding
benchmark called DrBenchmark. It encompasses 20 diversified tasks, including
named-entity recognition, part-of-speech tagging, question-answering, semantic
textual similarity, and classification. We evaluate 8 state-of-the-art
pre-trained masked language models (MLMs) on general and biomedical-specific
data, as well as English specific MLMs to assess their cross-lingual
capabilities. Our experiments reveal that no single model excels across all
tasks, while generalist models are sometimes still competitive.
\\ ( https://arxiv.org/abs/2402.13432 ,  1150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13433
Date: Wed, 21 Feb 2024 00:01:17 GMT   (6835kb,D)

Title: Structured Tree Alignment for Evaluation of (Speech) Constituency
  Parsing
Authors: Freda Shi, Kevin Gimpel, Karen Livescu
Categories: cs.CL cs.DS
Comments: 11 pages, 9 figures, 1 table
\\
  We present the structured average intersection-over-union ratio (STRUCT-IOU),
a similarity metric between constituency parse trees motivated by the problem
of evaluating speech parsers. STRUCT-IOU enables comparison between a
constituency parse tree (over automatically recognized spoken word boundaries)
with the ground-truth parse (over written words). To compute the metric, we
project the ground-truth parse tree to the speech domain by forced alignment,
align the projected ground-truth constituents with the predicted ones under
certain structured constraints, and calculate the average IOU score across all
aligned constituent pairs. STRUCT-IOU takes word boundaries into account and
overcomes the challenge that the predicted words and ground truth may not have
perfect one-to-one correspondence. Extending to the evaluation of text
constituency parsing, we demonstrate that STRUCT-IOU shows higher tolerance to
syntactically plausible parses than PARSEVAL (Black et al., 1991).
\\ ( https://arxiv.org/abs/2402.13433 ,  6835kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13446
Date: Wed, 21 Feb 2024 00:44:04 GMT   (7583kb,D)

Title: Large Language Models for Data Annotation: A Survey
Authors: Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita
  Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, Huan Liu
Categories: cs.CL
\\
  Data annotation is the labeling or tagging of raw data with relevant
information, essential for improving the efficacy of machine learning models.
The process, however, is labor-intensive and expensive. The emergence of
advanced Large Language Models (LLMs), exemplified by GPT-4, presents an
unprecedented opportunity to revolutionize and automate the intricate process
of data annotation. While existing surveys have extensively covered LLM
architecture, training, and general applications, this paper uniquely focuses
on their specific utility for data annotation. This survey contributes to three
core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations,
and Learning with LLM-generated annotations. Furthermore, the paper includes an
in-depth taxonomy of methodologies employing LLMs for data annotation, a
comprehensive review of learning strategies for models incorporating
LLM-generated annotations, and a detailed discussion on primary challenges and
limitations associated with using LLMs for data annotation. As a key guide,
this survey aims to direct researchers and practitioners in exploring the
potential of the latest LLMs for data annotation, fostering future advancements
in this critical domain. We provide a comprehensive papers list at
\url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.
\\ ( https://arxiv.org/abs/2402.13446 ,  7583kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13448
Date: Wed, 21 Feb 2024 00:49:42 GMT   (774kb,D)

Title: ED-Copilot: Reduce Emergency Department Wait Time with Language Model
  Diagnostic Assistance
Authors: Liwen Sun, Abhineet Agarwal, Aaron Kornblith, Bin Yu, Chenyan Xiong
Categories: cs.CL cs.AI cs.LG
\\
  In the emergency department (ED), patients undergo triage and multiple
laboratory tests before diagnosis. This process is time-consuming, and causes
ED crowding which significantly impacts patient mortality, medical errors,
staff burnout, etc. This work proposes (time) cost-effective diagnostic
assistance that explores the potential of artificial intelligence (AI) systems
in assisting ED clinicians to make time-efficient and accurate diagnoses. Using
publicly available patient data, we collaborate with ED clinicians to curate
MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in
suggesting laboratory tests that minimize ED wait times, while correctly
predicting critical outcomes such as death. We develop ED-Copilot which
sequentially suggests patient-specific laboratory tests and makes diagnostic
predictions. ED-Copilot uses a pre-trained bio-medical language model to encode
patient information and reinforcement learning to minimize ED wait time and
maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist,
ED-Copilot improves prediction accuracy over baselines while halving average
wait time from four hours to two hours. Ablation studies demonstrate the
importance of model scale and use of a bio-medical language model. Further
analyses reveal the necessity of personalized laboratory test suggestions for
diagnosing patients with severe cases, as well as the potential of ED-Copilot
in providing ED clinicians with informative laboratory test recommendations.
Our code is available at https://github.com/cxcscmu/ED-Copilot.
\\ ( https://arxiv.org/abs/2402.13448 ,  774kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13449
Date: Wed, 21 Feb 2024 01:00:17 GMT   (1720kb,D)

Title: CAMELoT: Towards Large Language Models with Training-Free Consolidated
  Associative Memory
Authors: Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry
  Krotov, Rogerio Feris
Categories: cs.CL
\\
  Large Language Models (LLMs) struggle to handle long input sequences due to
high memory and runtime costs. Memory-augmented models have emerged as a
promising solution to this problem, but current methods are hindered by limited
memory capacity and require costly re-training to integrate with a new LLM. In
this work, we introduce an associative memory module which can be coupled to
any pre-trained (frozen) attention-based LLM without re-training, enabling it
to handle arbitrarily long input sequences. Unlike previous methods, our
associative memory module consolidates representations of individual tokens
into a non-parametric distribution model, dynamically managed by properly
balancing the novelty and recency of the incoming data. By retrieving
information from this consolidated associative memory, the base LLM can achieve
significant (up to 29.7% on Arxiv) perplexity reduction in long-context
modeling compared to other baselines evaluated on standard benchmarks. This
architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced
Long Transformer), demonstrates superior performance even with a tiny context
window of 128 tokens, and also enables improved in-context learning with a much
larger set of demonstrations.
\\ ( https://arxiv.org/abs/2402.13449 ,  1720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13462
Date: Wed, 21 Feb 2024 01:35:26 GMT   (740kb,D)

Title: Potential and Challenges of Model Editing for Social Debiasing
Authors: Jianhao Yan, Futing Wang, Yafu Li, Yue Zhang
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  Large language models (LLMs) trained on vast corpora suffer from inevitable
stereotype biases. Mitigating these biases with fine-tuning could be both
costly and data-hungry. Model editing methods, which focus on modifying LLMs in
a post-hoc manner, are of great potential to address debiasing. However, it
lacks a comprehensive study that facilitates both internal and external model
editing methods, supports various bias types, as well as understands the pros
and cons of applying editing methods to stereotypical debiasing. To mitigate
this gap, we carefully formulate social debiasing into an editing problem and
benchmark seven existing model editing algorithms on stereotypical debiasing,
i.e., debias editing. Our findings in three scenarios reveal both the potential
and challenges of debias editing: (1) Existing model editing methods can
effectively preserve knowledge and mitigate biases, while the generalization of
debias effect from edited sentences to semantically equivalent sentences is
limited.(2) Sequential editing highlights the robustness of SERAC (Mitchell et
al. 2022b), while internal editing methods degenerate with the number of edits.
(3) Model editing algorithms achieve generalization towards unseen biases both
within the same type and from different types. In light of these findings, we
further propose two simple but effective methods to improve debias editing, and
experimentally show the effectiveness of the proposed methods.
\\ ( https://arxiv.org/abs/2402.13462 ,  740kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13463
Date: Wed, 21 Feb 2024 01:39:56 GMT   (945kb,D)

Title: RefuteBench: Evaluating Refuting Instruction-Following for Large
  Language Models
Authors: Jianhao Yan, Yun Luo, Yue Zhang
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  The application scope of large language models (LLMs) is increasingly
expanding. In practical use, users might provide feedback based on the model's
output, hoping for a responsive model that can complete responses according to
their feedback. Whether the model can appropriately respond to users' refuting
feedback and consistently follow through with execution has not been thoroughly
analyzed. In light of this, this paper proposes a comprehensive benchmark,
RefuteBench, covering tasks such as question answering, machine translation,
and email writing. The evaluation aims to assess whether models can positively
accept feedback in form of refuting instructions and whether they can
consistently adhere to user demands throughout the conversation. We conduct
evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit
inclination to their internal knowledge, often failing to comply with user
feedback. Additionally, as the length of the conversation increases, models
gradually forget the user's stated feedback and roll back to their own
responses. We further propose a recall-and-repeat prompts as a simple and
effective way to enhance the model's responsiveness to feedback.
\\ ( https://arxiv.org/abs/2402.13463 ,  945kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13470
Date: Wed, 21 Feb 2024 01:57:58 GMT   (939kb,D)

Title: How Important is Domain Specificity in Language Models and Instruction
  Finetuning for Biomedical Relation Extraction?
Authors: Aviv Brokman and Ramakanth Kavuluru
Categories: cs.CL
\\
  Cutting edge techniques developed in the general NLP domain are often
subsequently applied to the high-value, data-rich biomedical domain. The past
few years have seen generative language models (LMs), instruction finetuning,
and few-shot learning become foci of NLP research. As such, generative LMs
pretrained on biomedical corpora have proliferated and biomedical instruction
finetuning has been attempted as well, all with the hope that domain
specificity improves performance on downstream tasks. Given the nontrivial
effort in training such models, we investigate what, if any, benefits they have
in the key biomedical NLP task of relation extraction. Specifically, we address
two questions: (1) Do LMs trained on biomedical corpora outperform those
trained on general domain corpora? (2) Do models instruction finetuned on
biomedical datasets outperform those finetuned on assorted datasets or those
simply pretrained? We tackle these questions using existing LMs, testing across
four datasets. In a surprising result, general-domain models typically
outperformed biomedical-domain models. However, biomedical instruction
finetuning improved performance to a similar degree as general instruction
finetuning, despite having orders of magnitude fewer instructions. Our findings
suggest it may be more fruitful to focus research effort on larger-scale
biomedical instruction finetuning of general LMs over building domain-specific
biomedical LMs
\\ ( https://arxiv.org/abs/2402.13470 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13482
Date: Wed, 21 Feb 2024 02:45:46 GMT   (615kb,D)

Title: Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks
Authors: Minju Seo, Jinheon Baek, James Thorne, Sung Ju Hwang
Categories: cs.CL cs.AI cs.LG
\\
  Despite large successes of recent language models on diverse tasks, they
suffer from severe performance degeneration in low-resource settings with
limited training data available. Many existing works tackle this problem by
generating synthetic data from the training data and then training models on
them, recently using Large Language Models (LLMs). However, in low-resource
settings, the amount of seed data samples to use for data augmentation is very
small, which makes generated samples suboptimal and less diverse. To tackle
this challenge, we propose a novel method that augments training data by
incorporating a wealth of examples from other datasets, along with the given
training data. Specifically, we first retrieve the relevant instances from
other datasets, such as their input-output pairs or contexts, based on their
similarities with the given seed data, and then prompt LLMs to generate new
samples with the contextual information within and across the original and
retrieved samples. This approach can ensure that the generated data is not only
relevant but also more diverse than what could be achieved using the limited
seed data alone. We validate our proposed Retrieval-Augmented Data Augmentation
(RADA) framework on multiple datasets under low-resource settings of training
and test-time data augmentation scenarios, on which it outperforms existing
LLM-powered data augmentation baselines.
\\ ( https://arxiv.org/abs/2402.13482 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13492
Date: Wed, 21 Feb 2024 03:05:50 GMT   (315kb,D)

Title: Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval
  Augmentation to Language Models
Authors: Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani
Categories: cs.CL
\\
  While large language models (LMs) demonstrate remarkable performance, they
encounter challenges in providing accurate responses when queried for
information beyond their pre-trained memorization. Although augmenting them
with relevant external information can mitigate these issues, failure to
consider the necessity of retrieval may adversely affect overall performance.
Previous research has primarily focused on examining how entities influence
retrieval models and knowledge recall in LMs, leaving other aspects relatively
unexplored. In this work, our goal is to offer a more detailed, fact-centric
analysis by exploring the effects of combinations of entities and relations. To
facilitate this, we construct a new question answering (QA) dataset called
WiTQA (Wikipedia Triple Question Answers). This dataset includes questions
about entities and relations of various popularity levels, each accompanied by
a supporting passage. Our extensive experiments with diverse LMs and retrievers
reveal when retrieval does not consistently enhance LMs from the viewpoints of
fact-centric popularity.Confirming earlier findings, we observe that larger LMs
excel in recalling popular facts. However, they notably encounter difficulty
with infrequent entity-relation pairs compared to retrievers. Interestingly,
they can effectively retain popular relations of less common entities. We
demonstrate the efficacy of our finer-grained metric and insights through an
adaptive retrieval system that selectively employs retrieval and recall based
on the frequencies of entities and relations in the question.
\\ ( https://arxiv.org/abs/2402.13492 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13494
Date: Wed, 21 Feb 2024 03:09:21 GMT   (3319kb,D)

Title: GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient
  Analysis
Authors: Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong
Categories: cs.CL cs.CR
\\
  Large Language Models (LLMs) face threats from unsafe prompts. Existing
methods for detecting unsafe prompts are primarily online moderation APIs or
finetuned LLMs. These strategies, however, often require extensive and
resource-intensive data collection and training processes. In this study, we
propose GradSafe, which effectively detects unsafe prompts by scrutinizing the
gradients of safety-critical parameters in LLMs. Our methodology is grounded in
a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired
with compliance response exhibit similar patterns on certain safety-critical
parameters. In contrast, safe prompts lead to markedly different gradient
patterns. Building on this observation, GradSafe analyzes the gradients from
prompts (paired with compliance responses) to accurately detect unsafe prompts.
We show that GradSafe, applied to Llama-2 without further training, outperforms
Llama Guard, despite its extensive finetuning with a large dataset, in
detecting unsafe prompts. This superior performance is consistent across both
zero-shot and adaptation scenarios, as evidenced by our evaluations on the
ToxicChat and XSTest. The source code is available at
https://github.com/xyq7/GradSafe.
\\ ( https://arxiv.org/abs/2402.13494 ,  3319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13498
Date: Wed, 21 Feb 2024 03:21:14 GMT   (4014kb,D)

Title: The Lay Person's Guide to Biomedicine: Orchestrating Large Language
  Models
Authors: Zheheng Luo, Qianqian Xie, Sophia Ananiadou
Categories: cs.CL
Comments: 18 pages, 4 figures
\\
  Automated lay summarisation (LS) aims to simplify complex technical documents
into a more accessible format to non-experts. Existing approaches using
pre-trained language models, possibly augmented with external background
knowledge, tend to struggle with effective simplification and explanation.
Moreover, automated methods that can effectively assess the `layness' of
generated summaries are lacking. Recently, large language models (LLMs) have
demonstrated a remarkable capacity for text simplification, background
information generation, and text evaluation. This has motivated our systematic
exploration into using LLMs to generate and evaluate lay summaries of
biomedical articles. We propose a novel \textit{Explain-then-Summarise} LS
framework, which leverages LLMs to generate high-quality background knowledge
to improve supervised LS. We also evaluate the performance of LLMs for
zero-shot LS and propose two novel LLM-based LS evaluation metrics, which
assess layness from multiple perspectives. Finally, we conduct a human
assessment of generated lay summaries. Our experiments reveal that
LLM-generated background information can support improved supervised LS.
Furthermore, our novel zero-shot LS evaluation metric demonstrates a high
degree of alignment with human preferences. We conclude that LLMs have an
important part to play in improving both the performance and evaluation of LS
methods.
\\ ( https://arxiv.org/abs/2402.13498 ,  4014kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13514
Date: Wed, 21 Feb 2024 03:55:02 GMT   (681kb,D)

Title: Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer
  for Compositional Unknown Questions
Authors: Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang,
  Guanhua Chen, Huimin Wang, Kam-fai Wong
Categories: cs.CL cs.AI
\\
  Retrieve-then-read and generate-then-read are two typical solutions to handle
unknown and known questions in open-domain question-answering, while the former
retrieves necessary external knowledge and the later prompt the large language
models to generate internal known knowledge encoded in the parameters. However,
few of previous works consider the compositional unknown questions, which
consist of several known or unknown sub-questions. Thus, simple binary
classification (known or unknown) becomes sub-optimal and inefficient since it
will call external retrieval excessively for each compositional unknown
question. To this end, we propose the first Compositional unknown
Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer
(Self-DC) framework to empower LLMs to adaptively call different methods
on-demand, resulting in better performance and efficiency. Experimental results
on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve
comparable or even better performance with much more less retrieval times
compared with several strong baselines.
\\ ( https://arxiv.org/abs/2402.13514 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13517
Date: Wed, 21 Feb 2024 03:59:52 GMT   (8288kb,D)

Title: Round Trip Translation Defence against Large Language Model Jailbreaking
  Attacks
Authors: Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher
  Leckie
Categories: cs.CL cs.AI
Comments: 6 pages, 6 figures
\\
  Large language models (LLMs) are susceptible to social-engineered attacks
that are human-interpretable but require a high level of comprehension for LLMs
to counteract. Existing defensive measures can only mitigate less than half of
these attacks at most. To address this issue, we propose the Round Trip
Translation (RTT) method, the first algorithm specifically designed to defend
against social-engineered attacks on LLMs. RTT paraphrases the adversarial
prompt and generalizes the idea conveyed, making it easier for LLMs to detect
induced harmful behavior. This method is versatile, lightweight, and
transferrable to different LLMs. Our defense successfully mitigated over 70% of
Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the
most effective defense to the best of our knowledge. We are also the first to
attempt mitigating the MathsAttack and reduced its attack success rate by
almost 40%. Our code is publicly available at
https://github.com/Cancanxxx/Round_Trip_Translation_Defence
\\ ( https://arxiv.org/abs/2402.13517 ,  8288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13522
Date: Wed, 21 Feb 2024 04:15:22 GMT   (1285kb,D)

Title: RecMind: Japanese Movie Recommendation Dialogue with Seeker's Internal
  State
Authors: Takashi Kodama, Hirokazu Kiyomaru, Yin Jou Huang, Sadao Kurohashi
Categories: cs.CL
\\
  Humans pay careful attention to the interlocutor's internal state in
dialogues. For example, in recommendation dialogues, we make recommendations
while estimating the seeker's internal state, such as his/her level of
knowledge and interest. Since there are no existing annotated resources for the
analysis, we constructed RecMind, a Japanese movie recommendation dialogue
dataset with annotations of the seeker's internal state at the entity level.
Each entity has a subjective label annotated by the seeker and an objective
label annotated by the recommender. RecMind also features engaging dialogues
with long seeker's utterances, enabling a detailed analysis of the seeker's
internal state. Our analysis based on RecMind reveals that entities that the
seeker has no knowledge about but has an interest in contribute to
recommendation success. We also propose a response generation framework that
explicitly considers the seeker's internal state, utilizing the
chain-of-thought prompting. The human evaluation results show that our proposed
method outperforms the baseline method in both consistency and the success of
recommendations.
\\ ( https://arxiv.org/abs/2402.13522 ,  1285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13524
Date: Wed, 21 Feb 2024 04:42:41 GMT   (2410kb,D)

Title: OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large
  Language Models
Authors: Yang Liu, Meng Xu, Shuo Wang, Liner Yang, Haoyu Wang, Zhenghao Liu,
  Cunliang Kong, Yun Chen, Yang Liu, Maosong Sun, Erhong Yang
Categories: cs.CL
\\
  Modern large language models (LLMs) should generally benefit individuals from
various cultural backgrounds around the world. However, most recent advanced
generative evaluation benchmarks tailed for LLMs mainly focus on English. To
this end, we introduce OMGEval, the first Open-source Multilingual Generative
test set that can assess the capability of LLMs in different languages. For
each language, OMGEval provides 804 open-ended questions, covering a wide range
of important capabilities of LLMs, such as general knowledge, logical
reasoning, and so on. Each question is rigorously verified by human annotators.
Notably, to sufficiently reflect the compatibility of LLMs in different
cultural backgrounds, we perform localization for each non-English language.
Specifically, the current version of OMGEval includes 5 languages (i.e., Zh,
Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to
automatically score different model outputs, which is shown closely related to
human evaluation. We evaluate several representative multilingual LLMs on the
proposed OMGEval, which we believe will provide a valuable reference for the
community to further understand and improve the multilingual capability of
LLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.
\\ ( https://arxiv.org/abs/2402.13524 ,  2410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13532
Date: Wed, 21 Feb 2024 05:03:07 GMT   (496kb,D)

Title: Backdoor Attacks on Dense Passage Retrievers for Disseminating
  Misinformation
Authors: Quanyu Long, Yue Deng, LeiLei Gan, Wenya Wang, and Sinno Jialin Pan
Categories: cs.CL
\\
  Dense retrievers and retrieval-augmented language models have been widely
used in various NLP applications. Despite being designed to deliver reliable
and secure outcomes, the vulnerability of retrievers to potential attacks
remains unclear, raising concerns about their security. In this paper, we
introduce a novel scenario where the attackers aim to covertly disseminate
targeted misinformation, such as hate speech or advertisement, through a
retrieval system. To achieve this, we propose a perilous backdoor attack
triggered by grammar errors in dense passage retrieval. Our approach ensures
that attacked models can function normally for standard queries but are
manipulated to return passages specified by the attacker when users
unintentionally make grammatical mistakes in their queries. Extensive
experiments demonstrate the effectiveness and stealthiness of our proposed
attack method. When a user query is error-free, our model consistently
retrieves accurate information while effectively filtering out misinformation
from the top-k results. However, when a query contains grammar errors, our
system shows a significantly higher success rate in fetching the targeted
content.
\\ ( https://arxiv.org/abs/2402.13532 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13534
Date: Wed, 21 Feb 2024 05:04:29 GMT   (335kb,D)

Title: An Effective Incorporating Heterogeneous Knowledge Curriculum Learning
  for Sequence Labeling
Authors: Xuemei Tang and Qi Su
Categories: cs.CL cs.AI
Comments: 10 pages, 9 tables, 3 figures
\\
  Sequence labeling models often benefit from incorporating external knowledge.
However, this practice introduces data heterogeneity and complicates the model
with additional modules, leading to increased expenses for training a
high-performing model. To address this challenge, we propose a two-stage
curriculum learning (TCL) framework specifically designed for sequence labeling
tasks. The TCL framework enhances training by gradually introducing data
instances from easy to hard, aiming to improve both performance and training
speed. Furthermore, we explore different metrics for assessing the difficulty
levels of sequence labeling tasks. Through extensive experimentation on six
Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we
demonstrate the effectiveness of our model in enhancing the performance of
sequence labeling models. Additionally, our analysis indicates that TCL
accelerates training and alleviates the slow training problem associated with
complex models.
\\ ( https://arxiv.org/abs/2402.13534 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13542
Date: Wed, 21 Feb 2024 05:41:34 GMT   (9241kb,D)

Title: ARL2: Aligning Retrievers for Black-box Large Language Models via
  Self-guided Adaptive Relevance Labeling
Authors: Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Work in Progress
\\
  Retrieval-augmented generation enhances large language models (LLMs) by
incorporating relevant information from external knowledge sources. This
enables LLMs to adapt to specific domains and mitigate hallucinations in
knowledge-intensive tasks. However, existing retrievers are often misaligned
with LLMs due to their separate training processes and the black-box nature of
LLMs. To address this challenge, we propose ARL2, a retriever learning
technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and
score relevant evidence, enabling learning the retriever from robust LLM
supervision. Furthermore, ARL2 uses an adaptive self-training strategy for
curating high-quality and diverse relevance data, which can effectively reduce
the annotation cost. Extensive experiments demonstrate the effectiveness of
ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared
to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer
learning capabilities and strong zero-shot generalization abilities. Our code
will be published at \url{https://github.com/zhanglingxi-cs/ARL2}.
\\ ( https://arxiv.org/abs/2402.13542 ,  9241kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13546
Date: Wed, 21 Feb 2024 05:56:52 GMT   (10528kb,D)

Title: LLMs Meet Long Video: Advancing Long Video Comprehension with An
  Interactive Visual Adapter in LLMs
Authors: Yunxin Li, Xinyu Chen, Baotain Hu, Min Zhang
Categories: cs.CL cs.CV
Comments: Working in Progress
\\
  Long video understanding is a significant and ongoing challenge in the
intersection of multimedia and artificial intelligence. Employing large
language models (LLMs) for comprehending video becomes an emerging and
promising method. However, this approach incurs high computational costs due to
the extensive array of video tokens, experiences reduced visual clarity as a
consequence of token aggregation, and confronts challenges arising from
irrelevant visual tokens while answering video-related questions. To alleviate
these issues, we present an Interactive Visual Adapter (IVA) within LLMs,
designed to enhance interaction with fine-grained visual elements.
Specifically, we first transform long videos into temporal video tokens via
leveraging a visual encoder alongside a pretrained causal transformer, then
feed them into LLMs with the video instructions. Subsequently, we integrated
IVA, which contains a lightweight temporal frame selector and a spatial feature
interactor, within the internal blocks of LLMs to capture instruction-aware and
fine-grained visual signals. Consequently, the proposed video-LLM facilitates a
comprehensive understanding of long video content through appropriate long
video modeling and precise visual interactions. We conducted extensive
experiments on nine video understanding benchmarks and experimental results
show that our interactive visual adapter significantly improves the performance
of video LLMs on long video QA tasks. Ablation studies further verify the
effectiveness of IVA in long and short video understandings.
\\ ( https://arxiv.org/abs/2402.13546 ,  10528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13547
Date: Wed, 21 Feb 2024 06:04:53 GMT   (582kb,D)

Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning
Authors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo
  Wang, Shi Yu, Zhiyuan Liu, Ge Yu
Categories: cs.CL
\\
  Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large
Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks.
However, current RAG models position LLMs as passive knowledge receptors,
thereby restricting their capacity for learning and comprehending external
knowledge. In this paper, we present ActiveRAG, an innovative RAG framework
that shifts from passive knowledge acquisition to an active learning mechanism.
This approach utilizes the Knowledge Construction mechanism to develop a deeper
understanding of external knowledge by associating it with previously acquired
or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism
to incorporate the outcomes from both chains of thought and knowledge
construction, thereby calibrating the intrinsic cognition of LLMs. Our
experimental results demonstrate that ActiveRAG surpasses previous RAG models,
achieving a 5% improvement on question-answering datasets. All data and codes
are available at https://github.com/OpenMatch/ActiveRAG.
\\ ( https://arxiv.org/abs/2402.13547 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13550
Date: Wed, 21 Feb 2024 06:11:03 GMT   (8921kb,D)

Title: Are LLMs Effective Negotiators? Systematic Evaluation of the
  Multifaceted Capabilities of LLMs in Negotiation Dialogues
Authors: Deuksin Kwon, Emily Weiss, Tara Kulshrestha, Kushal Chawla, Gale M.
  Lucas, Jonathan Gratch
Categories: cs.CL cs.AI
\\
  A successful negotiation demands a deep comprehension of the conversation
context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as
strategic reasoning and effective communication, making it challenging for
automated systems. Given the remarkable performance of LLMs across a variety of
NLP tasks, in this work, we aim to understand how LLMs can advance different
aspects of negotiation research, ranging from designing dialogue systems to
providing pedagogical feedback and scaling up data collection practices. To
this end, we devise a methodology to analyze the multifaceted capabilities of
LLMs across diverse dialogue scenarios covering all the time stages of a
typical negotiation interaction. Our analysis adds to the increasing evidence
for the superiority of GPT-4 across various tasks while also providing insights
into specific tasks that remain difficult for LLMs. For instance, the models
correlate poorly with human players when making subjective assessments about
the negotiation dialogues and often struggle to generate responses that are
contextually appropriate as well as strategically advantageous.
\\ ( https://arxiv.org/abs/2402.13550 ,  8921kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13551
Date: Wed, 21 Feb 2024 06:14:04 GMT   (209kb,D)

Title: Graph Representation of Narrative Context: Coherence Dependency via
  Retrospective Questions
Authors: Liyan Xu, Jiangnan Li, Mo Yu, Jie Zhou
Categories: cs.CL cs.LG
\\
  This work introduces a novel and practical paradigm for narrative
comprehension, stemming from the observation that individual passages within
narratives are often cohesively related than being isolated. We therefore
propose to formulate a graph upon narratives dubbed NARCO that depicts a
task-agnostic coherence dependency of the entire context. Especially, edges in
NARCO encompass retrospective free-form questions between two context snippets
reflecting high-level coherent relations, inspired by the cognitive perception
of humans who constantly reinstate relevant events from prior context.
Importantly, our graph is instantiated through our designed two-stage LLM
prompting, thereby without reliance on human annotations. We present three
unique studies on its practical utility, examining the edge efficacy via recap
identification, local context augmentation via plot retrieval, and broader
applications exemplified by long document QA. Experiments suggest that our
approaches leveraging NARCO yield performance boost across all three tasks.
\\ ( https://arxiv.org/abs/2402.13551 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13561
Date: Wed, 21 Feb 2024 06:34:46 GMT   (8218kb,D)

Title: Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension
  with Enhanced Visual Knowledge Alignment
Authors: Yunxin Li, Xinyu Chen, Baotian Hu, Haoyuan Shi, Min Zhang
Categories: cs.CL cs.CV
Comments: working in progress, under review
\\
  Evaluating and Rethinking the current landscape of Large Multimodal Models
(LMMs), we observe that widely-used visual-language projection approaches
(e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet
ignore the visual knowledge-dimension alignment, i.e., connecting visuals to
their relevant knowledge. Visual knowledge plays a significant role in
analyzing, inferring, and interpreting information from visuals, helping
improve the accuracy of answers to knowledge-based visual questions. In this
paper, we mainly explore improving LMMs with visual-language knowledge
alignment, especially aimed at challenging knowledge-based visual question
answering (VQA). To this end, we present a Cognitive Visual-Language Mapper
(CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a
Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning
stage. Specifically, we design the VKA based on the interaction between a small
language model and a visual encoder, training it on collected image-knowledge
pairs to achieve visual knowledge acquisition and projection. FKA is employed
to distill the fine-grained visual knowledge of an image and inject it into
Large Language Models (LLMs). We conduct extensive experiments on
knowledge-based VQA benchmarks and experimental results show that CVLM
significantly improves the performance of LMMs on knowledge-based VQA (average
gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA,
respectively.
\\ ( https://arxiv.org/abs/2402.13561 ,  8218kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13562
Date: Wed, 21 Feb 2024 06:37:07 GMT   (814kb,D)

Title: Analysis of Multi-Source Language Training in Cross-Lingual Transfer
Authors: Seong Hoon Lim, Taejun Yun, Jinhyeon Kim, Jihun Choi and Taeuk Kim
Categories: cs.CL
\\
  The successful adaptation of multilingual language models (LMs) to a specific
language-task pair critically depends on the availability of data tailored for
that condition. While cross-lingual transfer (XLT) methods have contributed to
addressing this data scarcity problem, there still exists ongoing debate about
the mechanisms behind their effectiveness. In this work, we focus on one of
promising assumptions about inner workings of XLT, that it encourages
multilingual LMs to place greater emphasis on language-agnostic or
task-specific features. We test this hypothesis by examining how the patterns
of XLT change with a varying number of source languages involved in the
process. Our experimental findings show that the use of multiple source
languages in XLT-a technique we term Multi-Source Language Training
(MSLT)-leads to increased mingling of embedding spaces for different languages,
supporting the claim that XLT benefits from making use of language-independent
information. On the other hand, we discover that using an arbitrary combination
of source languages does not always guarantee better performance. We suggest
simple heuristics for identifying effective language combinations for MSLT and
empirically prove its effectiveness.
\\ ( https://arxiv.org/abs/2402.13562 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13571
Date: Wed, 21 Feb 2024 07:05:51 GMT   (322kb)

Title: Multilingual Coreference Resolution in Low-resource South Asian
  Languages
Authors: Ritwik Mishra, Pooja Desur, Rajiv Ratn Shah, Ponnurangam Kumaraguru
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  Coreference resolution involves the task of identifying text spans within a
discourse that pertain to the same real-world entity. While this task has been
extensively explored in the English language, there has been a notable scarcity
of publicly accessible resources and models for coreference resolution in South
Asian languages. We introduce a Translated dataset for Multilingual Coreference
Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools
for translation and word-alignment. Nearly all of the predicted translations
successfully pass a sanity check, and 75% of English references align with
their predicted translations. Using multilingual encoders, two off-the-shelf
coreference resolution models were trained on a concatenation of TransMuCoRes
and a Hindi coreference resolution dataset with manual annotations. The best
performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,
respectively, on our test-split of Hindi golden set. This study is the first to
evaluate an end-to-end coreference resolution model on a Hindi golden set.
Furthermore, this work underscores the limitations of current coreference
evaluation metrics when applied to datasets with split antecedents, advocating
for the development of more suitable evaluation metrics.
\\ ( https://arxiv.org/abs/2402.13571 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13577
Date: Wed, 21 Feb 2024 07:16:29 GMT   (1099kb,D)

Title: BBA: Bi-Modal Behavioral Alignment for Reasoning with Large
  Vision-Language Models
Authors: Xueliang Zhao, Xinting Huang, Tingchen Fu, Qintong Li, Shansan Gong,
  Lemao Liu, Wei Bi, Lingpeng Kong
Categories: cs.CL
Comments: Preprint
\\
  Multimodal reasoning stands as a pivotal capability for large vision-language
models (LVLMs). The integration with Domain-Specific Languages (DSL), offering
precise visual representations, equips these models with the opportunity to
execute more accurate reasoning in complex and professional domains. However,
the vanilla Chain-of-Thought (CoT) prompting method faces challenges in
effectively leveraging the unique strengths of visual and DSL representations,
primarily due to their differing reasoning mechanisms. Additionally, it often
falls short in addressing critical steps in multi-step reasoning tasks. To
mitigate these challenges, we introduce the \underline{B}i-Modal
\underline{B}ehavioral \underline{A}lignment (BBA) prompting method, designed
to maximize the potential of DSL in augmenting complex multi-modal reasoning
tasks. This method initiates by guiding LVLMs to create separate reasoning
chains for visual and DSL representations. Subsequently, it aligns these chains
by addressing any inconsistencies, thus achieving a cohesive integration of
behaviors from different modalities. Our experiments demonstrate that BBA
substantially improves the performance of GPT-4V(ision) on geometry problem
solving ($28.34\% \to 34.22\%$), chess positional advantage prediction
($42.08\% \to 46.99\%$) and molecular property prediction ($77.47\% \to
83.52\%$).
\\ ( https://arxiv.org/abs/2402.13577 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13583
Date: Wed, 21 Feb 2024 07:27:18 GMT   (8493kb,D)

Title: LongWanjuan: Towards Systematic Measurement for Long Text Quality
Authors: Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu and
  Dahua Lin
Categories: cs.CL
\\
  The quality of training data are crucial for enhancing the long-text
capabilities of foundation models. Despite existing efforts to refine data
quality through heuristic rules and evaluations based on data diversity and
difficulty, there's a lack of systematic approaches specifically tailored for
assessing long texts. Addressing this gap, our work systematically measures the
quality of long texts by evaluating three fundamental linguistic dimensions:
coherence, cohesion, and complexity. Drawing inspiration from the
aforementioned three dimensions, we introduce a suite of metrics designed to
evaluate the quality of long texts, encompassing both statistical and
pre-trained language model-based ones. Leveraging these metrics, we present
LongWanjuan, a bilingual dataset specifically tailored to enhance the training
of language models for long-text tasks with over 160B tokens. In LongWanjuan,
we categorize long texts into holistic, aggregated, and chaotic types, enabling
a detailed analysis of long-text quality. Furthermore, we devise a data mixture
recipe that strategically balances different types of long texts within
LongWanjuan, leading to significant improvements in model performance on
long-text tasks. The code and dataset are available at
https://github.com/OpenLMLab/LongWanjuan.
\\ ( https://arxiv.org/abs/2402.13583 ,  8493kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13584
Date: Wed, 21 Feb 2024 07:31:47 GMT   (3808kb,D)

Title: WinoViz: Probing Visual Properties of Objects Under Different States
Authors: Woojeong Jin, Tejas Srinivasan, Jesse Thomason, Xiang Ren
Categories: cs.CL
Comments: Preprint
\\
  Humans perceive and comprehend different visual properties of an object based
on specific contexts. For instance, we know that a banana turns brown ``when it
becomes rotten,'' whereas it appears green ``when it is unripe.'' Previous
studies on probing visual commonsense knowledge have primarily focused on
examining language models' understanding of typical properties (e.g., colors
and shapes) of objects. We present WinoViz, a text-only evaluation dataset,
consisting of 1,380 examples that probe the reasoning abilities of language
models regarding variant visual properties of objects under different contexts
or states. Our task is challenging since it requires pragmatic reasoning
(finding intended meanings) and visual knowledge reasoning. We also present
multi-hop data, a more challenging version of our data, which requires
multi-step reasoning chains to solve our task. In our experimental analysis,
our findings are: a) Large language models such as GPT-4 demonstrate effective
performance, but when it comes to multi-hop data, their performance is
significantly degraded. b) Large models perform well on pragmatic reasoning,
but visual knowledge reasoning is a bottleneck in our task. c) Vision-language
models outperform their language-model counterparts. d) A model with
machine-generated images performs poorly in our task. This is due to the poor
quality of the generated images.
\\ ( https://arxiv.org/abs/2402.13584 ,  3808kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13587
Date: Wed, 21 Feb 2024 07:38:29 GMT   (5236kb,D)

Title: A Multimodal In-Context Tuning Approach for E-Commerce Product
  Description Generation
Authors: Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang
Categories: cs.CL cs.CV
Comments: Accepted by LREC-COLING 2024
\\
  In this paper, we propose a new setting for generating product descriptions
from images, augmented by marketing keywords. It leverages the combined power
of visual and textual information to create descriptions that are more tailored
to the unique features of products. For this setting, previous methods utilize
visual and textual encoders to encode the image and keywords and employ a
language model-based decoder to generate the product description. However, the
generated description is often inaccurate and generic since same-category
products have similar copy-writings, and optimizing the overall framework on
large-scale samples makes models concentrate on common words yet ignore the
product features. To alleviate the issue, we present a simple and effective
Multimodal In-Context Tuning approach, named ModICT, which introduces a similar
product sample as the reference and utilizes the in-context learning capability
of language models to produce the description. During training, we keep the
visual encoder and language model frozen, focusing on optimizing the modules
responsible for creating multimodal in-context references and dynamic prompts.
This approach preserves the language generation prowess of large language
models (LLMs), facilitating a substantial increase in description diversity. To
assess the effectiveness of ModICT across various language model scales and
types, we collect data from three distinct product categories within the
E-commerce domain. Extensive experiments demonstrate that ModICT significantly
improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4%
on D-5) of generated results compared to conventional methods. Our findings
underscore the potential of ModICT as a valuable tool for enhancing automatic
generation of product descriptions in a wide range of applications.
\\ ( https://arxiv.org/abs/2402.13587 ,  5236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13593
Date: Wed, 21 Feb 2024 07:52:26 GMT   (9647kb,D)

Title: Knowledge Graph Enhanced Large Language Model Editing
Authors: Mengqi Zhang, Xiaotian Ye, Qiang Liu, Pengjie Ren, Shu Wu, Zhumin Chen
Categories: cs.CL
\\
  Large language models (LLMs) are pivotal in advancing natural language
processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and
outdated knowledge. Model editing emerges as a promising solution to address
these challenges. However, existing editing methods struggle to track and
incorporate changes in knowledge associated with edits, which limits the
generalization ability of postedit LLMs in processing edited knowledge. To
tackle these problems, we propose a novel model editing method that leverages
knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we
first utilize a knowledge graph augmentation module to uncover associated
knowledge that has changed due to editing, obtaining its internal
representations within LLMs. This approach allows knowledge alterations within
LLMs to be reflected through an external graph structure. Subsequently, we
design a graph-based knowledge edit module to integrate structured knowledge
into the model editing. This ensures that the updated parameters reflect not
only the modifications of the edited knowledge but also the changes in other
associated knowledge resulting from the editing process. Comprehensive
experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME
significantly improves the generalization capabilities of post-edit LLMs in
employing edited knowledge.
\\ ( https://arxiv.org/abs/2402.13593 ,  9647kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13598
Date: Wed, 21 Feb 2024 08:03:27 GMT   (1853kb,D)

Title: User-LLM: Efficient LLM Contextualization with User Embeddings
Authors: Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant
  Prakash, Bradley Green, Shawn O'Banion, Jun Xie
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have revolutionized natural language processing.
However, effectively incorporating complex and potentially noisy user
interaction data remains a challenge. To address this, we propose User-LLM, a
novel framework that leverages user embeddings to contextualize LLMs. These
embeddings, distilled from diverse user interactions using self-supervised
pretraining, capture latent user preferences and their evolution over time. We
integrate these user embeddings with LLMs through cross-attention and
soft-prompting, enabling LLMs to dynamically adapt to user context. Our
comprehensive experiments on MovieLens, Amazon Review, and Google Local Review
datasets demonstrate significant performance gains across various tasks.
Notably, our approach outperforms text-prompt-based contextualization on long
sequence tasks and tasks that require deep user understanding while being
computationally efficient. We further incorporate Perceiver layers to
streamline the integration between user encoders and LLMs, reducing
computational demands.
\\ ( https://arxiv.org/abs/2402.13598 ,  1853kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13604
Date: Wed, 21 Feb 2024 08:10:43 GMT   (8625kb,D)

Title: Breaking the HISCO Barrier: Automatic Occupational Standardization with
  OccCANINE
Authors: Christian M{\o}ller Dahl, Christian Vedel
Categories: cs.CL econ.EM
Comments: All code and guides on how to use OccCANINE is available on GitHub
  https://github.com/christianvedels/OccCANINE
ACM-class: I.2.7; I.7.0
\\
  This paper introduces a new tool, OccCANINE, to automatically transform
occupational descriptions into the HISCO classification system. The manual work
involved in processing and classifying occupational descriptions is
error-prone, tedious, and time-consuming. We finetune a preexisting language
model (CANINE) to do this automatically thereby performing in seconds and
minutes what previously took days and weeks. The model is trained on 14 million
pairs of occupational descriptions and HISCO codes in 13 different languages
contributed by 22 different sources. Our approach is shown to have accuracy,
recall and precision above 90 percent. Our tool breaks the metaphorical HISCO
barrier and makes this data readily available for analysis of occupational
structures with broad applicability in economics, economic history and various
related disciplines.
\\ ( https://arxiv.org/abs/2402.13604 ,  8625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13605
Date: Wed, 21 Feb 2024 08:12:26 GMT   (9560kb,D)

Title: KorNAT: LLM Alignment Benchmark for Korean Social Values and Common
  Knowledge
Authors: Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won,
  Hwaran Lee, Edward Choi
Categories: cs.CL
Comments: 35 pages, 7 figures, 16 tables
\\
  For Large Language Models (LLMs) to be effectively deployed in a specific
country, they must possess an understanding of the nation's culture and basic
knowledge. To this end, we introduce National Alignment, which measures an
alignment between an LLM and a targeted country from two aspects: social value
alignment and common knowledge alignment. Social value alignment evaluates how
well the model understands nation-specific social values, while common
knowledge alignment examines how well the model captures basic knowledge
related to the nation. We constructed KorNAT, the first benchmark that measures
national alignment with South Korea. For the social value dataset, we obtained
ground truth labels from a large-scale survey involving 6,174 unique Korean
participants. For the common knowledge dataset, we constructed samples based on
Korean textbooks and GED reference materials. KorNAT contains 4K and 6K
multiple-choice questions for social value and common knowledge, respectively.
Our dataset creation process is meticulously designed and based on statistical
sampling theory and was refined through multiple rounds of human review. The
experiment results of seven LLMs reveal that only a few models met our
reference score, indicating a potential for further enhancement. KorNAT has
received government approval after passing an assessment conducted by a
government-affiliated organization dedicated to evaluating dataset quality.
Samples and detailed evaluation protocols of our dataset can be found in
\url{https://selectstar.ai/ko/papers-national-alignment#}
\\ ( https://arxiv.org/abs/2402.13605 ,  9560kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13606
Date: Wed, 21 Feb 2024 08:20:06 GMT   (8951kb,D)

Title: A Comprehensive Study of Multilingual Confidence Estimation on Large
  Language Models
Authors: Boyang Xue, Hongru Wang, Weichao Wang, Rui Wang, Sheng Wang, Zeming
  Liu, Kam-Fai Wong
Categories: cs.CL
\\
  The tendency of Large Language Models to generate hallucinations and exhibit
overconfidence in predictions raises concerns regarding their reliability.
Confidence or uncertainty estimations indicating the extent of trustworthiness
of a model's response are essential to developing reliable AI systems. Current
research primarily focuses on LLM confidence estimations in English, remaining
a void for other widely used languages and impeding the global development of
reliable AI applications. This paper introduces a comprehensive investigation
of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce
an elaborated and expert-checked multilingual QA dataset. Second, we delve into
the performance of confidence estimations and examine how these confidence
scores can enhance LLM performance through self-refinement across diverse
languages. Finally, we propose a cross-lingual confidence estimation method to
achieve more precise confidence scores. The experimental results showcase the
performance of various confidence estimation methods across different languages
as well as present that our proposed cross-lingual confidence estimation
technique significantly enhances confidence estimation and outperforms several
baseline methods.
\\ ( https://arxiv.org/abs/2402.13606 ,  8951kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13610
Date: Wed, 21 Feb 2024 08:26:43 GMT   (10099kb,D)

Title: Data-driven Discovery with Large Generative Models
Authors: Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita
  Hazra, Ashish Sabharwal, Peter Clark
Categories: cs.CL cs.AI cs.LG
\\
  With the accumulation of data at an unprecedented rate, its potential to fuel
scientific discovery is growing exponentially. This position paper urges the
Machine Learning (ML) community to exploit the capabilities of large generative
models (LGMs) to develop automated systems for end-to-end data-driven discovery
-- a paradigm encompassing the search and verification of hypotheses purely
from a set of provided datasets, without the need for additional data
collection or physical experiments. We first outline several desiderata for an
ideal data-driven discovery system. Then, through DATAVOYAGER, a
proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of
these desiderata -- a feat previously unattainable -- while also highlighting
important limitations in the current system that open up opportunities for
novel ML research. We contend that achieving accurate, reliable, and robust
end-to-end discovery systems solely through the current capabilities of LGMs is
challenging. We instead advocate for fail-proof tool integration, along with
active user moderation through feedback mechanisms, to foster data-driven
scientific discoveries with efficiency and reproducibility.
\\ ( https://arxiv.org/abs/2402.13610 ,  10099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13613
Date: Wed, 21 Feb 2024 08:29:26 GMT   (7904kb,D)

Title: Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for
  Comparative Opinion Mining from Vietnamese Product Reviews
Authors: Hoang-Quynh Le, Duy-Cat Can, Khanh-Vinh Nguyen and Mai-Vu Tran
Categories: cs.CL cs.LG
\\
  This paper presents a comprehensive overview of the Comparative Opinion
Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the
10$^{th}$ International Workshop on Vietnamese Language and Speech Processing
(VLSP 2023). The primary objective of this shared task is to advance the field
of natural language processing by developing techniques that proficiently
extract comparative opinions from Vietnamese product reviews. Participants are
challenged to propose models that adeptly extract a comparative "quintuple"
from a comparative sentence, encompassing Subject, Object, Aspect, Predicate,
and Comparison Type Label. We construct a human-annotated dataset comprising
$120$ documents, encompassing $7427$ non-comparative sentences and $2468$
comparisons within $1798$ sentences. Participating models undergo evaluation
and ranking based on the Exact match macro-averaged quintuple F1 score.
\\ ( https://arxiv.org/abs/2402.13613 ,  7904kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13623
Date: Wed, 21 Feb 2024 08:50:40 GMT   (770kb,D)

Title: FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large
  Language Models
Authors: Sahil Mishra, Ujjwal Sudev, Tanmoy Chakraborty
Categories: cs.CL cs.SI
\\
  Taxonomies represent an arborescence hierarchical structure that establishes
relationships among entities to convey knowledge within a specific domain. Each
edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find
utility in various real-world applications, such as e-commerce search engines
and recommendation systems. Consequently, there arises a necessity to enhance
these taxonomies over time. However, manually curating taxonomies with neoteric
data presents challenges due to limitations in available human resources and
the exponential growth of data. Therefore, it becomes imperative to develop
automatic taxonomy expansion methods. Traditional supervised taxonomy expansion
approaches encounter difficulties stemming from limited resources, primarily
due to the small size of existing taxonomies. This scarcity of training data
often leads to overfitting. In this paper, we propose FLAME, a novel approach
for taxonomy expansion in low-resource environments by harnessing the
capabilities of large language models that are trained on extensive real-world
knowledge. LLMs help compensate for the scarcity of domain-specific knowledge.
Specifically, FLAME leverages prompting in few-shot settings to extract the
inherent knowledge within the LLMs, ascertaining the hypernym entities within
the taxonomy. Furthermore, it employs reinforcement learning to fine-tune the
large language models, resulting in more accurate predictions. Experiments on
three real-world benchmark datasets demonstrate the effectiveness of FLAME in
real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy
and 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate
the strengths and weaknesses of FLAME through an extensive case study, error
analysis and ablation studies on the benchmarks.
\\ ( https://arxiv.org/abs/2402.13623 ,  770kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13625
Date: Wed, 21 Feb 2024 08:54:47 GMT   (2160kb,D)

Title: MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning
Authors: Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng
Categories: cs.CL
\\
  Since commonsense information has been recorded significantly less frequently
than its existence, language models pre-trained by text generation have
difficulty to learn sufficient commonsense knowledge. Several studies have
leveraged text retrieval to augment the models' commonsense ability. Unlike
text, images capture commonsense information inherently but little effort has
been paid to effectively utilize them. In this work, we propose a novel
Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and
images to enhance the commonsense ability of language models. Extensive
experiments on the Common-Gen task have demonstrated the efficacy of MORE based
on the pre-trained models of both single and multiple modalities.
\\ ( https://arxiv.org/abs/2402.13625 ,  2160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13647
Date: Wed, 21 Feb 2024 09:28:02 GMT   (8230kb,D)

Title: Unsupervised Text Style Transfer via LLMs and Attention Masking with
  Multi-way Interactions
Authors: Lei Pan, Yunshi Lan, Yang Li, Weining Qian
Categories: cs.CL cs.AI
\\
  Unsupervised Text Style Transfer (UTST) has emerged as a critical task within
the domain of Natural Language Processing (NLP), aiming to transfer one
stylistic aspect of a sentence into another style without changing its
semantics, syntax, or other attributes. This task is especially challenging
given the intrinsic lack of parallel text pairings. Among existing methods for
UTST tasks, attention masking approach and Large Language Models (LLMs) are
deemed as two pioneering methods. However, they have shortcomings in generating
unsmooth sentences and changing the original contents, respectively. In this
paper, we investigate if we can combine these two methods effectively. We
propose four ways of interactions, that are pipeline framework with tuned
orders; knowledge distillation from LLMs to attention masking model; in-context
learning with constructed parallel examples. We empirically show these
multi-way interactions can improve the baselines in certain perspective of
style strength, content preservation and text fluency. Experiments also
demonstrate that simply conducting prompting followed by attention
masking-based revision can consistently surpass the other systems, including
supervised text style transfer systems. On Yelp-clean and Amazon-clean
datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute
percentages respectively, and achieves new SOTA results.
\\ ( https://arxiv.org/abs/2402.13647 ,  8230kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13667
Date: Wed, 21 Feb 2024 09:59:20 GMT   (728kb,D)

Title: GCOF: Self-iterative Text Generation for Copywriting Using Large
  Language Model
Authors: Jianghui Zhou, Ya Gao, Jie Liu, Xuemin Zhao, Zhaohua Yang, Yue Wu,
  Lirong Shi
Categories: cs.CL
Comments: 8 pages, 5 figures, 1 table
\\
  Large language models(LLM) such as ChatGPT have substantially simplified the
generation of marketing copy, yet producing content satisfying domain specific
requirements, such as effectively engaging customers, remains a significant
challenge. In this work, we introduce the Genetic Copy Optimization Framework
(GCOF) designed to enhance both efficiency and engagememnt of marketing copy
creation. We conduct explicit feature engineering within the prompts of LLM.
Additionally, we modify the crossover operator in Genetic Algorithm (GA),
integrating it into the GCOF to enable automatic feature engineering. This
integration facilitates a self-iterative refinement of the marketing copy.
Compared to human curated copy, Online results indicate that copy produced by
our framework achieves an average increase in click-through rate (CTR) of over
$50\%$.
\\ ( https://arxiv.org/abs/2402.13667 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13669
Date: Wed, 21 Feb 2024 10:06:08 GMT   (8001kb,D)

Title: Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning
Authors: Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, Haozhe Feng, Minfeng
  Zhu, Wei Chen
Categories: cs.CL
\\
  The surge in Large Language Models (LLMs) has revolutionized natural language
processing, but fine-tuning them for specific tasks often encounters challenges
in balancing performance and preserving general instruction-following
abilities. In this paper, we posit that the distribution gap between task
datasets and the LLMs serves as the primary underlying cause. To address the
problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach
that bridges the distribution gap by guiding fine-tuning with a distilled
dataset generated by the model itself to match its original distribution.
Experimental results on the Llama-2-chat model across various benchmarks
demonstrate that SDFT effectively mitigates catastrophic forgetting while
achieving comparable or superior performance on downstream tasks compared to
the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain
the helpfulness and safety alignment of LLMs. Our code is available at
\url{https://github.com/sail-sg/sdft}.
\\ ( https://arxiv.org/abs/2402.13669 ,  8001kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13671
Date: Wed, 21 Feb 2024 10:09:56 GMT   (7982kb,D)

Title: KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual
  Machine-Generated Text Detection
Authors: Michal Spiegel and Dominik Macko
Categories: cs.CL cs.AI
\\
  SemEval-2024 Task 8 is focused on multigenerator, multidomain, and
multilingual black-box machine-generated text detection. Such a detection is
important for preventing a potential misuse of large language models (LLMs),
the newest of which are very capable in generating multilingual human-like
texts. We have coped with this task in multiple ways, utilizing language
identification and parameter-efficient fine-tuning of smaller LLMs for text
classification. We have further used the per-language classification-threshold
calibration to uniquely combine fine-tuned models predictions with statistical
detection metrics to improve generalization of the system detection
performance. Our submitted method achieved competitive results, ranking at the
fourth place, just under 1 percentage point behind the winner.
\\ ( https://arxiv.org/abs/2402.13671 ,  7982kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13693
Date: Wed, 21 Feb 2024 10:53:45 GMT   (638kb,D)

Title: CMNER: A Chinese Multimodal NER Dataset based on Social Media
Authors: Yuanze Ji, Bobo Li, Jun Zhou, Fei Li, Chong Teng, Donghong Ji
Categories: cs.CL
\\
  Multimodal Named Entity Recognition (MNER) is a pivotal task designed to
extract named entities from text with the support of pertinent images.
Nonetheless, a notable paucity of data for Chinese MNER has considerably
impeded the progress of this natural language processing task within the
Chinese domain. Consequently, in this study, we compile a Chinese Multimodal
NER dataset (CMNER) utilizing data sourced from Weibo, China's largest social
media platform. Our dataset encompasses 5,000 Weibo posts paired with 18,326
corresponding images. The entities are classified into four distinct
categories: person, location, organization, and miscellaneous. We perform
baseline experiments on CMNER, and the outcomes underscore the effectiveness of
incorporating images for NER. Furthermore, we conduct cross-lingual experiments
on the publicly available English MNER dataset (Twitter2015), and the results
substantiate our hypothesis that Chinese and English multimodal NER data can
mutually enhance the performance of the NER model.
\\ ( https://arxiv.org/abs/2402.13693 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13703
Date: Wed, 21 Feb 2024 11:07:07 GMT   (503kb,D)

Title: Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand
  for Multilingual Instructions?
Authors: Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas
  Flores-Herr, Jens Lehmann, Michael Fromm and Mehdi Ali
Categories: cs.CL
Comments: 22 pages, 7 figures
\\
  The adaption of multilingual pre-trained Large Language Models (LLMs) into
eloquent and helpful assistants is essential to facilitate their use across
different language regions. In that spirit, we are the first to conduct an
extensive study of the performance of multilingual models on parallel,
multi-turn instruction-tuning benchmarks across a selection of the most-spoken
Indo-European languages. We systematically examine the effects of language and
instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning
it on parallel instruction-tuning datasets. Our results demonstrate that
instruction-tuning on parallel instead of monolingual corpora benefits
cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we
show that the Superficial Alignment Hypothesis does not hold in general, as the
investigated multilingual 7B parameter model presents a counter-example
requiring large-scale instruction-tuning datasets. Finally, we conduct a human
annotation study to understand the alignment between human-based and
GPT-4-based evaluation within multilingual chat scenarios.
\\ ( https://arxiv.org/abs/2402.13703 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13709
Date: Wed, 21 Feb 2024 11:23:21 GMT   (3369kb,D)

Title: SaGE: Evaluating Moral Consistency in Large Language Models
Authors: Vamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam
  Kumaraguru, Manas Gaur
Categories: cs.CL cs.AI
\\
  Despite recent advancements showcasing the impressive capabilities of Large
Language Models (LLMs) in conversational systems, we show that even
state-of-the-art LLMs are morally inconsistent in their generations,
questioning their reliability (and trustworthiness in general). Prior works in
LLM evaluation focus on developing ground-truth data to measure accuracy on
specific tasks. However, for moral scenarios that often lack universally
agreed-upon answers, consistency in model responses becomes crucial for their
reliability. To address this issue, we propose an information-theoretic measure
called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of
Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract
principles learned by a model and can help explain their decision-making
strategies effectively. To this extent, we construct the Moral Consistency
Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and
the RoTs that these models followed. Furthermore, to illustrate the
generalizability of SaGE, we use it to investigate LLM consistency on two
popular datasets -- TruthfulQA and HellaSwag. Our results reveal that
task-accuracy and consistency are independent problems, and there is a dire
need to investigate these issues further.
\\ ( https://arxiv.org/abs/2402.13709 ,  3369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13717
Date: Wed, 21 Feb 2024 11:30:20 GMT   (8852kb,D)

Title: Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character
  Role-Playing Agent
Authors: Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao,
  Liehuang Zhu
Categories: cs.CL
\\
  Large Language Models (LLMs) have revolutionized open-domain dialogue agents
but encounter challenges in multi-character role-playing (MCRP) scenarios. To
address the issue, we present Neeko, an innovative framework designed for
efficient multiple characters imitation. Unlike existing methods, Neeko employs
a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to
diverse characters. Our framework breaks down the role-playing process into
agent pre-training, multiple characters playing, and character incremental
learning, effectively handling both seen and unseen roles. This dynamic
approach, coupled with distinct LoRA blocks for each character, enhances
Neeko's adaptability to unique attributes, personalities, and speaking
patterns. As a result, Neeko demonstrates superior performance in MCRP over
most existing methods, offering more engaging and versatile user interaction
experiences. Code and data are available at
https://github.com/weiyifan1023/Neeko.
\\ ( https://arxiv.org/abs/2402.13717 ,  8852kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13718
Date: Wed, 21 Feb 2024 11:30:29 GMT   (249kb,D)

Title: $\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens
Authors: Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and
  Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and
  Zhiyuan Liu and Maosong Sun
Categories: cs.CL
Comments: 2023.12.15ARR
Journal-ref: 2023.12.15ARR
\\
  Processing and reasoning over long contexts is crucial for many practical
applications of Large Language Models (LLMs), such as document comprehension
and agent construction. Despite recent strides in making LLMs process contexts
with more than 100K tokens, there is currently a lack of a standardized
benchmark to evaluate this long-context capability. Existing public benchmarks
typically focus on contexts around 10K tokens, limiting the assessment and
comparison of LLMs in processing longer contexts. In this paper, we propose
$\infty$Bench, the first LLM benchmark featuring an average data length
surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks
spanning diverse domains, presented in both English and Chinese. The tasks in
$\infty$Bench are designed to require well understanding of long dependencies
in contexts, and make simply retrieving a limited number of passages from
contexts not sufficient for these tasks. In our experiments, based on
$\infty$Bench, we evaluate the state-of-the-art proprietary and open-source
LLMs tailored for processing long contexts. The results indicate that existing
long context LLMs still require significant advancements to effectively process
100K+ context. We further present three intriguing analyses regarding the
behavior of LLMs processing long context.
\\ ( https://arxiv.org/abs/2402.13718 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13720
Date: Wed, 21 Feb 2024 11:31:28 GMT   (1539kb,D)

Title: Ouroboros: Speculative Decoding with Large Model Enhanced Drafting
Authors: Weilin Zhao, Yuxiang Huang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Maosong
  Sun
Categories: cs.CL
\\
  Drafting-then-verifying decoding methods such as speculative decoding are
widely adopted training-free methods to accelerate the inference of large
language models (LLMs). Instead of employing an autoregressive process to
decode tokens sequentially, speculative decoding initially creates drafts with
an efficient small model. Then LLMs are required to conduct verification and
correction in a non-autoregressive fashion to minimize time overhead.
Generating longer drafts can lead to even more significant speedups once
verified, but also incurs substantial trial and error costs if it fails.
Suffering from the high verification failure probability, existing decoding
methods cannot draft too much content for verification at one time, achieving
sub-optimal inference acceleration. In this paper, we introduce Ouroboros,
which constructs a phrase candidate pool from the verification process of LLMs
to provide candidates for draft generation of the small model. Thereby,
Ouroboros can further improve the efficiency and effectiveness of the initial
drafts. The experimental results on typical text generation tasks show that
Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead
decoding and speculative decoding, respectively. The source code of Ouroboros
is available at https://github.com/thunlp/Ouroboros.
\\ ( https://arxiv.org/abs/2402.13720 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13722
Date: Wed, 21 Feb 2024 11:33:09 GMT   (274kb,D)

Title: Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment
  Analysis
Authors: S M Rafiuddin, Mohammed Rakib, Sadia Kamal, Arunkumar Bagavathi
Categories: cs.CL
Comments: 12 pages, 4 figures, Accepted in PAKDD 2024
\\
  Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem
that entails the extraction of multifaceted aspects, opinions, and sentiments
from the given text. Both standalone and compound ABSA tasks have been
extensively used in the literature to examine the nuanced information present
in online reviews and social media posts. Current ABSA methods often rely on
static hyperparameters for attention-masking mechanisms, which can struggle
with context adaptation and may overlook the unique relevance of words in
varied situations. This leads to challenges in accurately analyzing complex
sentences containing multiple aspects with differing sentiments. In this work,
we present adaptive masking methods that remove irrelevant tokens based on
context to assist in Aspect Term Extraction and Aspect Sentiment Classification
subtasks of ABSA. We show with our experiments that the proposed methods
outperform the baseline methods in terms of accuracy and F1 scores on four
benchmark online review datasets. Further, we show that the proposed methods
can be extended with multiple adaptations and demonstrate a qualitative
analysis of the proposed approach using sample text for aspect term extraction.
\\ ( https://arxiv.org/abs/2402.13722 ,  274kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13731
Date: Wed, 21 Feb 2024 11:50:32 GMT   (19920kb,D)

Title: The Da Vinci Code of Large Pre-trained Language Models: Deciphering
  Degenerate Knowledge Neurons
Authors: Yuheng Chen, Pengfei Cao, Yubo Chen, Yining Wang, Shengping Liu, Kang
  Liu, Jun Zhao
Categories: cs.CL cs.AI
\\
  This study explores the mechanism of factual knowledge storage in pre-trained
language models (PLMs). Previous research suggests that factual knowledge is
stored within multi-layer perceptron weights, and some storage units exhibit
degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper
provides a comprehensive definition of DKNs that covers both structural and
functional aspects, pioneering the study of structures in PLMs' factual
knowledge storage units. Based on this, we introduce the Neurological Topology
Clustering method, which allows the formation of DKNs in any numbers and
structures, leading to a more accurate DKN acquisition. Furthermore, we
introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely
integrates model robustness, evolvability, and complexity for a holistic
assessment of PLMs. Within this framework, our execution of 34 experiments
across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs.
The code will be available soon.
\\ ( https://arxiv.org/abs/2402.13731 ,  19920kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13740
Date: Wed, 21 Feb 2024 12:11:28 GMT   (7105kb,D)

Title: From Text to CQL: Bridging Natural Language and Corpus Search Engine
Authors: Luming Lu, Jiyuan An, Yujie Wang, Liner yang, Cunliang Kong, Zhenghao
  Liu, Shuo Wang, Haozhe Lin, Mingwei Fang, Yaping Huang and Erhong Yang
Categories: cs.CL
\\
  Natural Language Processing (NLP) technologies have revolutionized the way we
interact with information systems, with a significant focus on converting
natural language queries into formal query languages such as SQL. However, less
emphasis has been placed on the Corpus Query Language (CQL), a critical tool
for linguistic research and detailed analysis within text corpora. The manual
construction of CQL queries is a complex and time-intensive task that requires
a great deal of expertise, which presents a notable challenge for both
researchers and practitioners. This paper presents the first text-to-CQL task
that aims to automate the translation of natural language into CQL. We present
a comprehensive framework for this task, including a specifically curated
large-scale dataset and methodologies leveraging large language models (LLMs)
for effective text-to-CQL task. In addition, we established advanced evaluation
metrics to assess the syntactic and semantic accuracy of the generated queries.
We created innovative LLM-based conversion approaches and detailed experiments.
The results demonstrate the efficacy of our methods and provide insights into
the complexities of text-to-CQL task.
\\ ( https://arxiv.org/abs/2402.13740 ,  7105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13741
Date: Wed, 21 Feb 2024 12:12:16 GMT   (2405kb,D)

Title: Unlocking Instructive In-Context Learning with Tabular Prompting for
  Relational Triple Extraction
Authors: Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji, Jiajun Liu, Ziyu
  Shang and Qiqing Luo
Categories: cs.CL cs.AI
Comments: LREC-COLING 2024
\\
  The in-context learning (ICL) for relational triple extraction (RTE) has
achieved promising performance, but still encounters two key challenges: (1)
how to design effective prompts and (2) how to select proper demonstrations.
Existing methods, however, fail to address these challenges appropriately. On
the one hand, they usually recast RTE task to text-to-text prompting formats,
which is unnatural and results in a mismatch between the output format at the
pre-training time and the inference time for large language models (LLMs). On
the other hand, they only utilize surface natural language features and lack
consideration of triple semantics in sample selection. These issues are
blocking improved performance in ICL for RTE, thus we aim to tackle prompt
designing and sample selection challenges simultaneously. To this end, we
devise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE task
into a table generation task to incorporate explicit structured information
into ICL, facilitating conversion of outputs to RTE structures. Then we propose
instructive in-context learning (I$^2$CL) which only selects and annotates a
few samples considering internal triple semantics in massive unlabeled samples.
\\ ( https://arxiv.org/abs/2402.13741 ,  2405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13753
Date: Wed, 21 Feb 2024 12:30:33 GMT   (944kb,D)

Title: LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
Authors: Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning
  Shang, Jiahang Xu, Fan Yang, Mao Yang
Categories: cs.CL
\\
  Large context window is a desirable feature in large language models (LLMs).
However, due to high fine-tuning costs, scarcity of long texts, and
catastrophic values introduced by new token positions, current extended context
windows are limited to around 128k tokens. This paper introduces LongRoPE that,
for the first time, extends the context window of pre-trained LLMs to an
impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k
training lengths, while maintaining performance at the original short context
window. This is achieved by three key innovations: (i) we identify and exploit
two forms of non-uniformities in positional interpolation through an efficient
search, providing a better initialization for fine-tuning and enabling an 8x
extension in non-fine-tuning scenarios; (ii) we introduce a progressive
extension strategy that first fine-tunes a 256k length LLM and then conducts a
second positional interpolation on the fine-tuned extended LLM to achieve a
2048k context window; (iii) we readjust LongRoPE on 8k length to recover the
short context window performance. Extensive experiments on LLaMA2 and Mistral
across various tasks demonstrate the effectiveness of our method. Models
extended via LongRoPE retain the original architecture with minor modifications
to the positional embedding, and can reuse most pre-existing optimizations.
\\ ( https://arxiv.org/abs/2402.13753 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13758
Date: Wed, 21 Feb 2024 12:35:19 GMT   (11267kb,D)

Title: Factual Consistency Evaluation of Summarisation in the Era of Large
  Language Models
Authors: Zheheng Luo, Qianqian Xie, Sophia Ananiadou
Categories: cs.CL
Comments: 5 figures
\\
  Factual inconsistency with source documents in automatically generated
summaries can lead to misinformation or pose risks. Existing factual
consistency(FC) metrics are constrained by their performance, efficiency, and
explainability. Recent advances in Large language models (LLMs) have
demonstrated remarkable potential in text evaluation but their effectiveness in
assessing FC in summarisation remains underexplored. Prior research has mostly
focused on proprietary LLMs, leaving essential factors that affect their
assessment capabilities unexplored. Additionally, current FC evaluation
benchmarks are restricted to news articles, casting doubt on the generality of
the FC methods tested on them. In this paper, we first address the gap by
introducing TreatFact a dataset of LLM-generated summaries of clinical texts,
annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC
evaluation across news and clinical domains and analyse the impact of model
size, prompts, pre-training and fine-tuning data. Our findings reveal that
despite proprietary models prevailing on the task, open-source LLMs lag behind.
Nevertheless, there is potential for enhancing the performance of open-source
LLMs through increasing model size, expanding pre-training data, and developing
well-curated fine-tuning data. Experiments on TreatFact suggest that both
previous methods and LLM-based evaluators are unable to capture factual
inconsistencies in clinical summaries, posing a new challenge for FC
evaluation.
\\ ( https://arxiv.org/abs/2402.13758 ,  11267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13764
Date: Wed, 21 Feb 2024 12:38:59 GMT   (3183kb,D)

Title: CriticBench: Evaluating Large Language Models as Critic
Authors: Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen,
  Xian-ling Mao
Categories: cs.CL cs.AI
\\
  Critique ability are crucial in the scalable oversight and self-improvement
of Large Language Models (LLMs). While many recent studies explore the critique
ability of LLMs to judge and refine flaws in generations, how to
comprehensively and reliably measure the critique abilities of LLMs is
under-explored. This paper introduces \shortname, a novel benchmark designed to
comprehensively and reliably evaluate four key critique ability dimensions of
LLMs: feedback, comparison, refinement and meta-feedback.
\shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to
critique responses at varying levels of quality granularity. Our extensive
evaluations of open-source and closed-source LLMs reveal intriguing
relationships between the critique ability and tasks, response qualities, and
model scales. Datasets, resources and evaluation toolkit for \shortname~will be
publicly released at \url{https://github.com/gmftbyGMFTBY/CriticBench}.
\\ ( https://arxiv.org/abs/2402.13764 ,  3183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13800
Date: Wed, 21 Feb 2024 13:30:34 GMT   (3585kb)

Title: The Geography of Information Diffusion in Online Discourse on Europe and
  Migration
Authors: Elisa Leonardelli, Sara Tonelli
Categories: cs.CL cs.SI
\\
  The online diffusion of information related to Europe and migration has been
little investigated from an external point of view. However, this is a very
relevant topic, especially if users have had no direct contact with Europe and
its perception depends solely on information retrieved online. In this work we
analyse the information circulating online about Europe and migration after
retrieving a large amount of data from social media (Twitter), to gain new
insights into topics, magnitude, and dynamics of their diffusion. We combine
retweets and hashtags network analysis with geolocation of users, linking thus
data to geography and allowing analysis from an "outside Europe" perspective,
with a special focus on Africa. We also introduce a novel approach based on
cross-lingual quotes, i.e. when content in a language is commented and
retweeted in another language, assuming these interactions are a proxy for
connections between very distant communities. Results show how the majority of
online discussions occurs at a national level, especially when discussing
migration. Language (English) is pivotal for information to become
transnational and reach far. Transnational information flow is strongly
unbalanced, with content mainly produced in Europe and amplified outside.
Conversely Europe-based accounts tend to be self-referential when they discuss
migration-related topics. Football is the most exported topic from Europe
worldwide. Moreover, important nodes in the communities discussing
migration-related topics include accounts of official institutions and
international agencies, together with journalists, news, commentators and
activists.
\\ ( https://arxiv.org/abs/2402.13800 ,  3585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13818
Date: Wed, 21 Feb 2024 13:57:36 GMT   (8070kb,D)

Title: Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering
  Dehumanizing Language
Authors: Hezhao Zhang, Lasana Harris, Nafise Sadat Moosavi
Categories: cs.CL
\\
  Dehumanization, characterized as a subtle yet harmful manifestation of hate
speech, involves denying individuals of their human qualities and often results
in violence against marginalized groups. Despite significant progress in
Natural Language Processing across various domains, its application in
detecting dehumanizing language is limited, largely due to the scarcity of
publicly available annotated data for this domain. This paper evaluates the
performance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2,
in identifying dehumanizing language. Our findings reveal that while these
models demonstrate potential, achieving a 70\% accuracy rate in distinguishing
dehumanizing language from broader hate speech, they also display biases. They
are over-sensitive in classifying other forms of hate speech as dehumanization
for a specific subset of target groups, while more frequently failing to
identify clear cases of dehumanization for other target groups. Moreover,
leveraging one of the best-performing models, we automatically annotated a
larger dataset for training more accessible models. However, our findings
indicate that these models currently do not meet the high-quality data
generation threshold necessary for this task.
\\ ( https://arxiv.org/abs/2402.13818 ,  8070kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13866
Date: Wed, 21 Feb 2024 15:14:20 GMT   (1419kb,D)

Title: Kuaiji: the First Chinese Accounting Large Language Model
Authors: Jiayuan Luo, Songhua Yang, Xiaoling Qiu, Panyu Chen, Yufei Nai,
  Wenxuan Zeng, Wentao Zhang, Xinke Jiang
Categories: cs.CL cs.AI
Comments: version 1.0
\\
  Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated
impressive proficiency in comprehending and generating natural language.
However, they encounter difficulties when tasked with adapting to specialized
domains such as accounting. To address this challenge, we introduce Kuaiji, a
tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned
using the Baichuan framework, which encompasses continuous pre-training and
supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing
large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy
and response speed. Our contributions encompass the creation of the first
Chinese accounting dataset, the establishment of Kuaiji as a leading
open-source Chinese accounting LLM, and the validation of its efficacy through
real-world accounting scenarios.
\\ ( https://arxiv.org/abs/2402.13866 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13874
Date: Wed, 21 Feb 2024 15:35:04 GMT   (3520kb,D)

Title: $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for
  In-Context Learning
Authors: Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei
  Deng, Furu Wei, Qi Zhang
Categories: cs.CL
\\
  The remarkable capability of large language models (LLMs) for in-context
learning (ICL) needs to be activated by demonstration examples. Prior work has
extensively explored the selection of examples for ICL, predominantly following
the "select then organize" paradigm, such approaches often neglect the internal
relationships between examples and exist an inconsistency between the training
and inference. In this paper, we formulate the problem as a
$\textit{se}$quential $\textit{se}$lection problem and introduce
$\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on
varying context, aiding in capturing inter-relationships and sequential
information among examples, significantly enriching the contextuality and
relevance of ICL prompts. Meanwhile, we utilize beam search to seek and
construct example sequences, enhancing both quality and diversity. Extensive
experiments across 23 NLP tasks from 8 distinct categories illustrate that
$\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42%
relative improvement over random selection. Further in-depth analysis show the
effectiveness of proposed strategies, highlighting $\texttt{Se}^2$'s
exceptional stability and adaptability across various scenarios. Our code will
be released to facilitate future research.
\\ ( https://arxiv.org/abs/2402.13874 ,  3520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13887
Date: Wed, 21 Feb 2024 15:58:37 GMT   (7887kb,D)

Title: Beyond Probabilities: Unveiling the Misalignment in Evaluating Large
  Language Models
Authors: Chenyang Lyu, Minghao Wu, Alham Fikri Aji
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various applications, fundamentally reshaping the landscape of natural language
processing (NLP) research. However, recent evaluation frameworks often rely on
the output probabilities of LLMs for predictions, primarily due to
computational constraints, diverging from real-world LLM usage scenarios. While
widely employed, the efficacy of these probability-based evaluation strategies
remains an open research question. This study aims to scrutinize the validity
of such probability-based evaluation methods within the context of using LLMs
for Multiple Choice Questions (MCQs), highlighting their inherent limitations.
Our empirical investigation reveals that the prevalent probability-based
evaluation method inadequately aligns with generation-based prediction.
Furthermore, current evaluation frameworks typically assess LLMs through
predictive tasks based on output probabilities rather than directly generating
responses, owing to computational limitations. We illustrate that these
probability-based approaches do not effectively correspond with generative
predictions. The outcomes of our study can enhance the understanding of LLM
evaluation methodologies and provide insights for future research in this
domain.
\\ ( https://arxiv.org/abs/2402.13887 ,  7887kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13904
Date: Wed, 21 Feb 2024 16:15:20 GMT   (975kb,D)

Title: Calibrating Large Language Models with Sample Consistency
Authors: Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar,
  Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, Chris Callison-Burch
Categories: cs.CL
\\
  Accurately gauging the confidence level of Large Language Models' (LLMs)
predictions is pivotal for their reliable application. However, LLMs are often
uncalibrated inherently and elude conventional calibration techniques due to
their proprietary nature and massive scale. In this work, we explore the
potential of deriving confidence from the distribution of multiple randomly
sampled model generations, via three measures of consistency. We perform an
extensive evaluation across various open and closed-source models on nine
reasoning datasets. Results show that consistency-based calibration methods
outperform existing post-hoc approaches. Meanwhile, we find that factors such
as intermediate explanations, model scaling, and larger sample sizes enhance
calibration, while instruction-tuning makes calibration more difficult.
Moreover, confidence scores obtained from consistency have the potential to
enhance model performance. Finally, we offer practical guidance on choosing
suitable consistency metrics for calibration, tailored to the characteristics
of various LMs.
\\ ( https://arxiv.org/abs/2402.13904 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13906
Date: Wed, 21 Feb 2024 16:22:21 GMT   (1479kb,D)

Title: Leveraging Collection-Wide Similarities for Unsupervised Document
  Structure Extraction
Authors: Gili Lior, Yoav Goldberg, Gabriel Stanovsky
Categories: cs.CL
\\
  Document collections of various domains, e.g., legal, medical, or financial,
often share some underlying collection-wide structure, which captures
information that can aid both human users and structure-aware models. We
propose to identify the typical structure of document within a collection,
which requires to capture recurring topics across the collection, while
abstracting over arbitrary header paraphrases, and ground each topic to
respective document locations. These requirements pose several challenges:
headers that mark recurring topics frequently differ in phrasing, certain
section headers are unique to individual documents and do not reflect the
typical structure, and the order of topics can vary between documents.
Subsequently, we develop an unsupervised graph-based method which leverages
both inter- and intra-document similarities, to extract the underlying
collection-wide structure. Our evaluations on three diverse domains in both
English and Hebrew indicate that our method extracts meaningful collection-wide
structure, and we hope that future work will leverage our method for
multi-document applications and structure-aware models.
\\ ( https://arxiv.org/abs/2402.13906 ,  1479kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13917
Date: Wed, 21 Feb 2024 16:32:38 GMT   (2467kb,D)

Title: What Linguistic Features and Languages are Important in LLM Translation?
Authors: Ryandito Diandaru, Lucky Susanto, Zilu Tang, Ayu Purwarianti, Derry
  Wijaya
Categories: cs.CL cs.AI
Comments: Submitted to LREC-COLING 2024
\\
  Large Language Models (LLMs) demonstrate strong capability across multiple
tasks, including machine translation. Our study focuses on evaluating Llama2's
machine translation capabilities and exploring how translation depends on
languages in its training data. Our experiments show that the 7B Llama2 model
yields above 10 BLEU score for all languages it has seen, but not always for
languages it has not seen. Most gains for those unseen languages are observed
the most with the model scale compared to using chat versions or adding shot
count. Furthermore, our linguistic distance analysis reveals that syntactic
similarity is not always the primary linguistic factor in determining
translation quality. Interestingly, we discovered that under specific
circumstances, some languages, despite having significantly less training data
than English, exhibit strong correlations comparable to English. Our
discoveries here give new perspectives for the current landscape of LLMs,
raising the possibility that LLMs centered around languages other than English
may offer a more effective foundation for a multilingual model.
\\ ( https://arxiv.org/abs/2402.13917 ,  2467kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13919
Date: Wed, 21 Feb 2024 16:33:22 GMT   (8612kb,D)

Title: SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in
  Clinical Summarization
Authors: Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining
  Wang, Vidhi Dhaval Mody, Hong Yu
Categories: cs.CL cs.AI
Comments: Equal contribution for the first two authors. arXiv admin note: text
  overlap with arXiv:2310.20033
\\
  Large Language Models (LLMs) such as GPT and Llama have demonstrated
significant achievements in summarization tasks but struggle with factual
inaccuracies, a critical issue in clinical NLP applications where errors could
lead to serious consequences. To counter the high costs and limited
availability of expert-annotated data for factual alignment, this study
introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate
high-quality feedback aimed at enhancing factual consistency in clinical note
summarization. Our research primarily focuses on edit feedback, mirroring the
practical scenario in which medical professionals refine AI system outputs
without the need for additional annotations. Despite GPT's proven expertise in
various clinical NLP tasks, such as the Medical Licensing Examination, there is
scant research on its capacity to deliver expert-level edit feedback for
improving weaker LMs or LLMs generation quality. This work leverages GPT's
advanced capabilities in clinical NLP to offer expert-level edit feedback.
Through the use of two distinct alignment algorithms (DPO and SALT) based on
GPT edit feedback, our goal is to reduce hallucinations and align closely with
medical facts, endeavoring to narrow the divide between AI-generated content
and factual accuracy. This highlights the substantial potential of GPT edits in
enhancing the alignment of clinical factuality.
\\ ( https://arxiv.org/abs/2402.13919 ,  8612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13926
Date: Wed, 21 Feb 2024 16:46:36 GMT   (100kb,D)

Title: Large Language Models are Vulnerable to Bait-and-Switch Attacks for
  Generating Harmful Content
Authors: Federico Bianchi, James Zou
Categories: cs.CL cs.AI
\\
  The risks derived from large language models (LLMs) generating deceptive and
damaging content have been the subject of considerable research, but even safe
generations can lead to problematic downstream impacts. In our study, we shift
the focus to how even safe text coming from LLMs can be easily turned into
potentially dangerous content through Bait-and-Switch attacks. In such attacks,
the user first prompts LLMs with safe questions and then employs a simple
find-and-replace post-hoc technique to manipulate the outputs into harmful
narratives. The alarming efficacy of this approach in generating toxic content
highlights a significant challenge in developing reliable safety guardrails for
LLMs. In particular, we stress that focusing on the safety of the verbatim LLM
outputs is insufficient and that we also need to consider post-hoc
transformations.
\\ ( https://arxiv.org/abs/2402.13926 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13936
Date: Wed, 21 Feb 2024 17:05:06 GMT   (7573kb,D)

Title: Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP
  Guided Reinforcement Learning
Authors: Antoine Chaffin, Ewa Kijak, Vincent Claveau
Categories: cs.CL cs.CV
\\
  Training image captioning models using teacher forcing results in very
generic samples, whereas more distinctive captions can be very useful in
retrieval applications or to produce alternative texts describing images for
accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval
similarity score between the generated caption and the input image as reward to
guide the training, leading to more distinctive captions. Recent studies show
that pre-trained cross-modal retrieval models can be used to provide this
reward, completely eliminating the need for reference captions. However, we
argue in this paper that Ground Truth (GT) captions can still be useful in this
RL framework. We propose a new image captioning model training strategy that
makes use of GT captions in different ways. Firstly, they can be used to train
a simple MLP discriminator that serves as a regularization to prevent reward
hacking and ensures the fluency of generated captions, resulting in a textual
GAN setup extended for multimodal inputs. Secondly, they can serve as
additional trajectories in the RL strategy, resulting in a teacher forcing loss
weighted by the similarity of the GT to the image. This objective acts as an
additional learning signal grounded to the distribution of the GT captions.
Thirdly, they can serve as strong baselines when added to the pool of captions
used to compute the proposed contrastive reward to reduce the variance of
gradient estimate. Experiments on MS-COCO demonstrate the interest of the
proposed training strategy to produce highly distinctive captions while
maintaining high writing quality.
\\ ( https://arxiv.org/abs/2402.13936 ,  7573kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13950
Date: Wed, 21 Feb 2024 17:23:59 GMT   (10734kb,D)

Title: Making Reasoning Matter: Measuring and Improving Faithfulness of
  Chain-of-Thought Reasoning
Authors: Debjit Paul, Robert West, Antoine Bosselut and Boi Faltings
Categories: cs.CL
\\
  Large language models (LLMs) have been shown to perform better when asked to
reason step-by-step before answering a question. However, it is unclear to what
degree the model's final answer is faithful to the stated reasoning steps. In
this paper, we perform a causal mediation analysis on twelve LLMs to examine
how intermediate reasoning steps generated by the LLM influence the final
outcome and find that LLMs do not reliably use their intermediate reasoning
steps when generating an answer. To address this issue, we introduce FRODO, a
framework to tailor small-sized LMs to generate correct reasoning steps and
robustly reason over these steps. FRODO consists of an inference module that
learns to generate correct reasoning steps using an implicit causal reward
function and a reasoning module that learns to faithfully reason over these
intermediate inferences using a counterfactual and causal preference objective.
Our experiments show that FRODO significantly outperforms four competitive
baselines. Furthermore, FRODO improves the robustness and generalization
ability of the reasoning LM, yielding higher performance on out-of-distribution
test sets. Finally, we find that FRODO's rationales are more faithful to its
final answer predictions than standard supervised fine-tuning.
\\ ( https://arxiv.org/abs/2402.13950 ,  10734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13954
Date: Wed, 21 Feb 2024 17:33:13 GMT   (842kb,D)

Title: Measuring Social Biases in Masked Language Models by Proxy of Prediction
  Quality
Authors: Rahul Zalkikar, Kanchan Chandra
Categories: cs.CL
\\
  Social and political scientists often aim to discover and measure distinct
biases from text data representations (embeddings). Innovative
transformer-based language models produce contextually-aware token embeddings
and have achieved state-of-the-art performance for a variety of natural
language tasks, but have been shown to encode unwanted biases for downstream
applications. In this paper, we evaluate the social biases encoded by
transformers trained with the masked language modeling objective using proposed
proxy functions within an iterative masking experiment to measure the quality
of transformer models' predictions, and assess the preference of MLMs towards
disadvantaged and advantaged groups. We compare bias estimations with those
produced by other evaluation methods using two benchmark datasets, finding
relatively high religious and disability biases across considered MLMs and low
gender bias in one dataset relative to the other. Our measures outperform
others in their agreement with human annotators. We extend on previous work by
evaluating social biases introduced after re-training an MLM under the masked
language modeling objective (w.r.t. the model's pre-trained base), and find
that proposed measures produce more accurate estimations of relative preference
for biased sentences between transformers than others based on our methods.
\\ ( https://arxiv.org/abs/2402.13954 ,  842kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13956
Date: Wed, 21 Feb 2024 17:36:07 GMT   (1146kb,D)

Title: Can You Learn Semantics Through Next-Word Prediction? The Case of
  Entailment
Authors: William Merrill and Zhaofeng Wu and Norihito Naka and Yoon Kim and Tal
  Linzen
Categories: cs.CL
Comments: Preprint
\\
  Do LMs infer the semantics of text from co-occurrence patterns in their
training data? Merrill et al. (2022) argue that, in theory, probabilities
predicted by an optimal LM encode semantic information about entailment
relations, but it is unclear whether neural LMs trained on corpora learn
entailment in this way because of strong idealizing assumptions made by Merrill
et al. In this work, we investigate whether their theory can be used to decode
entailment judgments from neural LMs. We find that a test similar to theirs can
decode entailment relations between natural sentences, well above random
chance, though not perfectly, across many datasets and LMs. This suggests LMs
implicitly model aspects of semantics to predict semantic effects on sentence
co-occurrence patterns. However, we find the test that predicts entailment in
practice works in the opposite direction to the theoretical test. We thus
revisit the assumptions underlying the original test, finding its derivation
did not adequately account for redundancy in human-written text. We argue that
correctly accounting for redundancy related to explanations might derive the
observed flipped test and, more generally, improve linguistic theories of human
speakers.
\\ ( https://arxiv.org/abs/2402.13956 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13963
Date: Wed, 21 Feb 2024 17:47:20 GMT   (5297kb,D)

Title: Towards Building Multilingual Language Model for Medicine
Authors: Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang,
  Ya Zhang, Yanfeng Wang, Weidi Xie
Categories: cs.CL
\\
  In this paper, we aim to develop an open-source, multilingual language model
for medicine, that the benefits a wider, linguistically diverse audience from
different regions. In general, we present the contribution from the following
aspects: first, for multilingual medical-specific adaptation, we construct a
new multilingual medical corpus, that contains approximately 25.5B tokens
encompassing 6 main languages, termed as MMedC, that enables auto-regressive
training for existing general LLMs. second, to monitor the development of
multilingual LLMs in medicine, we propose a new multilingual medical
multi-choice question-answering benchmark with rationale, termed as MMedBench;
third, we have assessed a number of popular, opensource large language models
(LLMs) on our benchmark, along with those further auto-regressive trained on
MMedC, as a result, our final model, termed as MMedLM 2, with only 7B
parameters, achieves superior performance compared to all other open-source
models, even rivaling GPT-4 on MMedBench. We will make the resources publicly
available, including code, model weights, and datasets.
\\ ( https://arxiv.org/abs/2402.13963 ,  5297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13991
Date: Wed, 21 Feb 2024 18:23:16 GMT   (3466kb,D)

Title: Analysing The Impact of Sequence Composition on Language Model
  Pre-Training
Authors: Yu Zhao, Yuanbin Qu, Konrad Staniszewski, Szymon Tworkowski, Wei Liu,
  Piotr Mi{\l}o\'s, Yuxiang Wu, Pasquale Minervini
Categories: cs.CL
\\
  Most language model pre-training frameworks concatenate multiple documents
into fixed-length sequences and use causal masking to compute the likelihood of
each token given its context; this strategy is widely adopted due to its
simplicity and efficiency. However, to this day, the influence of the
pre-training sequence composition strategy on the generalisation properties of
the model remains under-explored. In this work, we find that applying causal
masking can lead to the inclusion of distracting information from previous
documents during pre-training, which negatively impacts the performance of the
models on language modelling and downstream tasks. In intra-document causal
masking, the likelihood of each token is only conditioned on the previous
tokens in the same document, eliminating potential distracting information from
previous documents and significantly improving performance. Furthermore, we
find that concatenating related documents can reduce some potential
distractions during pre-training, and our proposed efficient retrieval-based
sequence construction method, BM25Chunk, can improve in-context learning
(+11.6\%), knowledge memorisation (+9.8\%), and context utilisation (+7.2\%)
abilities of language models without sacrificing efficiency.
\\ ( https://arxiv.org/abs/2402.13991 ,  3466kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14002
Date: Wed, 21 Feb 2024 18:40:24 GMT   (2951kb,D)

Title: Hallucinations or Attention Misdirection? The Path to Strategic Value
  Extraction in Business Using Large Language Models
Authors: Aline Ioste
Categories: cs.CL
\\
  Large Language Models with transformer architecture have revolutionized the
domain of text generation, setting unprecedented benchmarks. Despite their
impressive capabilities, LLMs have been criticized for generating outcomes that
deviate from factual accuracy or display logical inconsistencies, phenomena
commonly referred to as hallucinations. This term, however, has often been
misapplied to any results deviating from the instructor's expectations, which
this paper defines as attention misdirection rather than true hallucinations.
Understanding the distinction between hallucinations and attention misdirection
becomes increasingly relevant in business contexts, where the ramifications of
such errors can significantly impact the value extraction from these inherently
pre-trained models. This paper highlights the best practices of the PGI,
Persona, Grouping, and Intelligence, method, a strategic framework that
achieved a remarkable error rate of only 3,15 percent across 4,000 responses
generated by GPT in response to a real business challenge. It emphasizes that
by equipping experimentation with knowledge, businesses can unlock
opportunities for innovation through the use of these natively pre-trained
models. This reinforces the notion that strategic application grounded in a
skilled team can maximize the benefits of emergent technologies such as the
LLMs.
\\ ( https://arxiv.org/abs/2402.14002 ,  2951kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14007
Date: Wed, 21 Feb 2024 18:48:38 GMT   (9404kb,D)

Title: Can Watermarks Survive Translation? On the Cross-lingual Consistency of
  Text Watermark for Large Language Models
Authors: Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng
  Tu, Zhuosheng Zhang, Rui Wang
Categories: cs.CL cs.AI
Comments: Under review
\\
  Text watermarking technology aims to tag and identify content produced by
large language models (LLMs) to prevent misuse. In this study, we introduce the
concept of ''cross-lingual consistency'' in text watermarking, which assesses
the ability of text watermarks to maintain their effectiveness after being
translated into other languages. Preliminary empirical results from two LLMs
and three watermarking methods reveal that current text watermarking
technologies lack consistency when texts are translated into various languages.
Based on this observation, we propose a Cross-lingual Watermark Removal Attack
(CWRA) to bypass watermarking by first obtaining a response from an LLM in a
pivot language, which is then translated into the target language. CWRA can
effectively remove watermarks by reducing the Area Under the Curve (AUC) from
0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors
that contribute to the cross-lingual consistency in text watermarking and
propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.
\\ ( https://arxiv.org/abs/2402.14007 ,  9404kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14008
Date: Wed, 21 Feb 2024 18:49:26 GMT   (8221kb,D)

Title: OlympiadBench: A Challenging Benchmark for Promoting AGI with
  Olympiad-Level Bilingual Multimodal Scientific Problems
Authors: Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai,
  Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi,
  Zhiyuan Liu, Maosong Sun
Categories: cs.CL
\\
  Recent advancements have seen Large Language Models (LLMs) and Large
Multimodal Models (LMMs) surpassing general human capabilities in various
tasks, approaching the proficiency level of human experts across multiple
domains. With traditional benchmarks becoming less challenging for these
models, new rigorous challenges are essential to gauge their advanced
abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual
multimodal scientific benchmark, featuring 8,952 problems from Olympiad-level
mathematics and physics competitions, including the Chinese college entrance
exam. Each problem is detailed with expert-level annotations for step-by-step
reasoning. Evaluating top-tier models on OlympiadBench, we implement a
comprehensive assessment methodology to accurately evaluate model responses.
Notably, the best-performing model, GPT-4V, attains an average score of 17.23%
on OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark
rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V
points out prevalent issues with hallucinations, knowledge omissions, and
logical fallacies. We hope that our challenging benchmark can serve as a
valuable resource for helping future AGI research endeavors.
\\ ( https://arxiv.org/abs/2402.14008 ,  8221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14016
Date: Wed, 21 Feb 2024 18:55:20 GMT   (837kb,D)

Title: Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on
  Zero-shot LLM Assessment
Authors: Vyas Raina, Adian Liusie, Mark Gales
Categories: cs.CL
\\
  Large Language Models (LLMs) are powerful zero-shot assessors and are
increasingly used in real-world situations such as for written exams or
benchmarking systems. Despite this, no existing work has analyzed the
vulnerability of judge-LLMs against adversaries attempting to manipulate
outputs. This work presents the first study on the adversarial robustness of
assessment LLMs, where we search for short universal phrases that when appended
to texts can deceive LLMs to provide high assessment scores. Experiments on
SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise
LLM-comparative assessment are vulnerable to simple concatenation attacks,
where in particular LLM-scoring is very susceptible and can yield maximum
assessment scores irrespective of the input text quality. Interestingly, such
attacks are transferable and phrases learned on smaller open-source LLMs can be
applied to larger closed-source models, such as GPT3.5. This highlights the
pervasive nature of the adversarial vulnerabilities across different judge-LLM
sizes, families and methods. Our findings raise significant concerns on the
reliability of LLMs-as-a-judge methods, and underscore the importance of
addressing vulnerabilities in LLM assessment methods before deployment in
high-stakes real-world scenarios.
\\ ( https://arxiv.org/abs/2402.14016 ,  837kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13304
Date: Tue, 20 Feb 2024 15:01:11 GMT   (758kb,D)

Title: Harmful algal bloom forecasting. A comparison between stream and batch
  learning
Authors: Andres Molares-Ulloa, Elisabet Rocruz, Daniel Rivero, Xos\'e A. Padin,
  Rita Nolasco, Jes\'us Dubert and Enrique Fernandez-Blanco
Categories: cs.LG cs.AI
\\
  Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from
shellfish contaminated with toxins produced by dinoflagellates. The condition,
with its widespread incidence, high morbidity rate, and persistent shellfish
toxicity, poses risks to public health and the shellfish industry. High biomass
of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs).
Monitoring and forecasting systems are crucial for mitigating HABs impact.
Predicting harmful algal blooms involves a time-series-based problem with a
strong historical seasonal component, however, recent anomalies due to changes
in meteorological and oceanographic events have been observed. Stream Learning
stands out as one of the most promising approaches for addressing
time-series-based problems with concept drifts. However, its efficacy in
predicting HABs remains unproven and needs to be tested in comparison with
Batch Learning. Historical data availability is a critical point in developing
predictive systems. In oceanography, the available data collection can have
some constrains and limitations, which has led to exploring new tools to obtain
more exhaustive time series. In this study, a machine learning workflow for
predicting the number of cells of a toxic dinoflagellate, Dinophysis acuminata,
was developed with several key advancements. Seven machine learning algorithms
were compared within two learning paradigms. Notably, the output data from
CROCO, the ocean hydrodynamic model, was employed as the primary dataset,
palliating the limitation of time-continuous historical data. This study
highlights the value of models interpretability, fair models comparison
methodology, and the incorporation of Stream Learning models. The model DoME,
with an average R2 of 0.77 in the 3-day-ahead prediction, emerged as the most
effective and interpretable predictor, outperforming the other algorithms.
\\ ( https://arxiv.org/abs/2402.13304 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13332
Date: Tue, 20 Feb 2024 19:19:56 GMT   (922kb,D)

Title: Double machine learning for causal hybrid modeling -- applications in
  the Earth sciences
Authors: Kai-Hendrik Cohrs, Gherardo Varando, Nuno Carvalhais, Markus
  Reichstein, Gustau Camps-Valls
Categories: cs.LG stat.ME
\\
  Hybrid modeling integrates machine learning with scientific knowledge with
the goal of enhancing interpretability, generalization, and adherence to
natural laws. Nevertheless, equifinality and regularization biases pose
challenges in hybrid modeling to achieve these purposes. This paper introduces
a novel approach to estimating hybrid models via a causal inference framework,
specifically employing Double Machine Learning (DML) to estimate causal
effects. We showcase its use for the Earth sciences on two problems related to
carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based
hybrid modeling is superior in estimating causal parameters over end-to-end
deep neural network (DNN) approaches, proving efficiency, robustness to bias
from regularization methods, and circumventing equifinality. Our approach,
applied to carbon flux partitioning, exhibits flexibility in accommodating
heterogeneous causal effects. The study emphasizes the necessity of explicitly
defining causal graphs and relationships, advocating for this as a general best
practice. We encourage the continued exploration of causality in hybrid models
for more interpretable and trustworthy results in knowledge-guided machine
learning.
\\ ( https://arxiv.org/abs/2402.13332 ,  922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13338
Date: Tue, 20 Feb 2024 19:30:55 GMT   (151kb)

Title: Incentivized Exploration via Filtered Posterior Sampling
Authors: Anand Kalvit, Aleksandrs Slivkins, Yonatan Gur
Categories: cs.LG econ.TH
\\
  We study "incentivized exploration" (IE) in social learning problems where
the principal (a recommendation algorithm) can leverage information asymmetry
to incentivize sequentially-arriving agents to take exploratory actions. We
identify posterior sampling, an algorithmic approach that is well known in the
multi-armed bandits literature, as a general-purpose solution for IE. In
particular, we expand the existing scope of IE in several practically-relevant
dimensions, from private agent types to informative recommendations to
correlated Bayesian priors. We obtain a general analysis of posterior sampling
in IE which allows us to subsume these extended settings as corollaries, while
also recovering existing results as special cases.
\\ ( https://arxiv.org/abs/2402.13338 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13366
Date: Tue, 20 Feb 2024 20:44:40 GMT   (714kb,D)

Title: Statistical curriculum learning: An elimination algorithm achieving an
  oracle risk
Authors: Omer Cohen, Ron Meir, Nir Weinberger
Categories: cs.LG stat.ML
\\
  We consider a statistical version of curriculum learning (CL) in a parametric
prediction setting. The learner is required to estimate a target parameter
vector, and can adaptively collect samples from either the target model, or
other source models that are similar to the target model, but less noisy. We
consider three types of learners, depending on the level of side-information
they receive. The first two, referred to as strong/weak-oracle learners,
receive high/low degrees of information about the models, and use these to
learn. The third, a fully adaptive learner, estimates the target parameter
vector without any prior information. In the single source case, we propose an
elimination learning method, whose risk matches that of a strong-oracle
learner. In the multiple source case, we advocate that the risk of the
weak-oracle learner is a realistic benchmark for the risk of adaptive learners.
We develop an adaptive multiple elimination-rounds CL algorithm, and
characterize instance-dependent conditions for its risk to match that of the
weak-oracle learner. We consider instance-dependent minimax lower bounds, and
discuss the challenges associated with defining the class of instances for the
bound. We derive two minimax lower bounds, and determine the conditions under
which the performance weak-oracle learner is minimax optimal.
\\ ( https://arxiv.org/abs/2402.13366 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13368
Date: Tue, 20 Feb 2024 20:48:00 GMT   (8432kb,D)

Title: Unsupervised Concept Discovery Mitigates Spurious Correlations
Authors: Md Rifat Arefin, Yan Zhang, Aristide Baratin, Francesco Locatello,
  Irina Rish, Dianbo Liu, Kenji Kawaguchi
Categories: cs.LG cs.CV
\\
  Models prone to spurious correlations in training data often produce brittle
predictions and introduce unintended biases. Addressing this challenge
typically involves methods relying on prior knowledge and group annotation to
remove spurious correlations, which may not be readily available in many
applications. In this paper, we establish a novel connection between
unsupervised object-centric learning and mitigation of spurious correlations.
Instead of directly inferring sub-groups with varying correlations with labels,
our approach focuses on discovering concepts: discrete ideas that are shared
across input samples. Leveraging existing object-centric representation
learning, we introduce CoBalT: a concept balancing technique that effectively
mitigates spurious correlations without requiring human labeling of subgroups.
Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for
subpopulation shifts demonstrate superior or competitive performance compared
state-of-the-art baselines, without the need for group annotation.
\\ ( https://arxiv.org/abs/2402.13368 ,  8432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13369
Date: Tue, 20 Feb 2024 20:49:22 GMT   (19523kb,D)

Title: The Uncanny Valley: A Comprehensive Analysis of Diffusion Models
Authors: Karam Ghanem, Danilo Bzdok
Categories: cs.LG cs.AI cs.CV
Comments: 28 pages, 21 figures
ACM-class: I.2.10; I.4.8; I.4.5; I.4.m
\\
  Through Diffusion Models (DMs), we have made significant advances in
generating high-quality images. Our exploration of these models delves deeply
into their core operational principles by systematically investigating key
aspects across various DM architectures: i) noise schedules, ii) samplers, and
iii) guidance. Our comprehensive examination of these models sheds light on
their hidden fundamental mechanisms, revealing the concealed foundational
elements that are essential for their effectiveness. Our analyses emphasize the
hidden key factors that determine model performance, offering insights that
contribute to the advancement of DMs. Past findings show that the configuration
of noise schedules, samplers, and guidance is vital to the quality of generated
images; however, models reach a stable level of quality across different
configurations at a remarkably similar point, revealing that the decisive
factors for optimal performance predominantly reside in the diffusion process
dynamics and the structural design of the model's network, rather than the
specifics of configuration details. Our comparative analysis reveals that
Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics
consistently outperform the Noise Conditioned Score Network (NCSN)-based ones,
not only when evaluated in their original forms but also when continuous
through Stochastic Differential Equation (SDE)-based implementations.
\\ ( https://arxiv.org/abs/2402.13369 ,  19523kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13371
Date: Tue, 20 Feb 2024 20:53:04 GMT   (3088kb,D)

Title: FIDLAR: Forecast-Informed Deep Learning Architecture for Flood
  Mitigation
Authors: Jimeng Shi, Zeda Yin, Arturo Leon, Jayantha Obeysekera, Giri
  Narasimhan
Categories: cs.LG
\\
  In coastal river systems, frequent floods, often occurring during major
storms or king tides, pose a severe threat to lives and property. However,
these floods can be mitigated or even prevented by strategically releasing
water before extreme weather events with hydraulic structures such as dams,
gates, pumps, and reservoirs. A standard approach used by local water
management agencies is the "rule-based" method, which specifies predetermined
pre-releases of water based on historical and time-tested human experience, but
which tends to result in excess or inadequate water release. The model
predictive control (MPC), a physics-based model for prediction, is an
alternative approach, albeit involving computationally intensive calculations.
In this paper, we propose a Forecast Informed Deep Learning Architecture,
FIDLAR, to achieve rapid and optimal flood management with precise water
pre-releases. FIDLAR seamlessly integrates two neural network modules: one
called the Flood Manager, which is responsible for generating water pre-release
schedules, and another called the Flood Evaluator, which assesses these
generated schedules. The Evaluator module is pre-trained separately, and its
gradient-based feedback is used to train the Manager model, ensuring optimal
water pre-releases. We have conducted experiments using FIDLAR with data from a
flood-prone coastal area in South Florida, particularly susceptible to frequent
storms. Results show that FIDLAR is several orders of magnitude faster than
currently used physics-based approaches while outperforming baseline methods
with improved water pre-release schedules. Our code is at
https://github.com/JimengShi/FIDLAR/.
\\ ( https://arxiv.org/abs/2402.13371 ,  3088kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13379
Date: Tue, 20 Feb 2024 21:09:04 GMT   (4072kb,D)

Title: Referee-Meta-Learning for Fast Adaptation of Locational Fairness
Authors: Weiye Chen, Yiqun Xie, Xiaowei Jia, Erhu He, Han Bao, Bang An, Xun
  Zhou
Categories: cs.LG cs.CY
\\
  When dealing with data from distinct locations, machine learning algorithms
tend to demonstrate an implicit preference of some locations over the others,
which constitutes biases that sabotage the spatial fairness of the algorithm.
This unfairness can easily introduce biases in subsequent decision-making given
broad adoptions of learning-based solutions in practice. However, locational
biases in AI are largely understudied. To mitigate biases over locations, we
propose a locational meta-referee (Meta-Ref) to oversee the few-shot
meta-training and meta-testing of a deep neural network. Meta-Ref dynamically
adjusts the learning rates for training samples of given locations to advocate
a fair performance across locations, through an explicit consideration of
locational biases and the characteristics of input data. We present a
three-phase training framework to learn both a meta-learning-based predictor
and an integrated Meta-Ref that governs the fairness of the model. Once trained
with a distribution of spatial tasks, Meta-Ref is applied to samples from new
spatial tasks (i.e., regions outside the training area) to promote fairness
during the fine-tune step. We carried out experiments with two case studies on
crop monitoring and transportation safety, which show Meta-Ref can improve
locational fairness while keeping the overall prediction quality at a similar
level.
\\ ( https://arxiv.org/abs/2402.13379 ,  4072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13388
Date: Tue, 20 Feb 2024 21:34:56 GMT   (89kb,D)

Title: Transformer tricks: Precomputing the first layer
Authors: Nils Graef
Categories: cs.LG
Comments: 5 pages, 2 figures
\\
  This short paper describes a trick to speed up inference of transformers with
RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of
the first transformer layer can be precomputed, which results in slightly lower
latency and lower cost-per-token. Because this trick optimizes only one layer,
the relative savings depend on the total number of layers. For example, the
maximum savings for a model with only 4 layers (such as Whisper tiny) is
limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3%
savings.
\\ ( https://arxiv.org/abs/2402.13388 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13393
Date: Tue, 20 Feb 2024 21:49:36 GMT   (751kb,D)

Title: Fairness Risks for Group-conditionally Missing Demographics
Authors: Kaiqi Jiang, Wenzhe Fan, Mao Li, Xinhua Zhang
Categories: cs.LG cs.CY
\\
  Fairness-aware classification models have gained increasing attention in
recent years as concerns grow on discrimination against some demographic
groups. Most existing models require full knowledge of the sensitive features,
which can be impractical due to privacy, legal issues, and an individual's fear
of discrimination. The key challenge we will address is the group dependency of
the unavailability, e.g., people of some age range may be more reluctant to
reveal their age. Our solution augments general fairness risks with
probabilistic imputations of the sensitive features, while jointly learning the
group-conditionally missing probabilities in a variational auto-encoder. Our
model is demonstrated effective on both image and tabular datasets, achieving
an improved balance between accuracy and fairness.
\\ ( https://arxiv.org/abs/2402.13393 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13402
Date: Tue, 20 Feb 2024 22:12:33 GMT   (2426kb)

Title: Towards accelerating physical discovery via non-interactive and
  interactive multi-fidelity Bayesian Optimization: Current challenges and
  future opportunities
Authors: Arpan Biswas, Sai Mani Prudhvi Valleti, Rama Vasudevan, Maxim
  Ziatdinov, Sergei V. Kalinin
Categories: cs.LG
Comments: Main text includes 29 pages and 10 figures, Supplementary mat.
  includes 4 pages and 4 figures
\\
  Both computational and experimental material discovery bring forth the
challenge of exploring multidimensional and often non-differentiable parameter
spaces, such as phase diagrams of Hamiltonians with multiple interactions,
composition spaces of combinatorial libraries, processing spaces, and molecular
embedding spaces. Often these systems are expensive or time-consuming to
evaluate a single instance, and hence classical approaches based on exhaustive
grid or random search are too data intensive. This resulted in strong interest
towards active learning methods such as Bayesian optimization (BO) where the
adaptive exploration occurs based on human learning (discovery) objective.
However, classical BO is based on a predefined optimization target, and
policies balancing exploration and exploitation are purely data driven. In
practical settings, the domain expert can pose prior knowledge on the system in
form of partially known physics laws and often varies exploration policies
during the experiment. Here, we explore interactive workflows building on
multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then
structured (physics-driven) sMFBO, and extending it to allow human in the loop
interactive iMFBO workflows for adaptive and domain expert aligned exploration.
These approaches are demonstrated over highly non-smooth multi-fidelity
simulation data generated from an Ising model, considering spin-spin
interaction as parameter space, lattice sizes as fidelity spaces, and the
objective as maximizing heat capacity. Detailed analysis and comparison show
the impact of physics knowledge injection and on-the-fly human decisions for
improved exploration, current challenges, and potential opportunities for
algorithm development with combining data, physics and real time human
decisions.
\\ ( https://arxiv.org/abs/2402.13402 ,  2426kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13410
Date: Tue, 20 Feb 2024 22:34:53 GMT   (296kb,D)

Title: Bayesian Neural Networks with Domain Knowledge Priors
Authors: Dylan Sam, Rattana Pukdee, Daniel P. Jeong, Yewon Byun, J. Zico Kolter
Categories: cs.LG stat.ML
Comments: 17 pages, 4 figures
\\
  Bayesian neural networks (BNNs) have recently gained popularity due to their
ability to quantify model uncertainty. However, specifying a prior for BNNs
that captures relevant domain knowledge is often extremely challenging. In this
work, we propose a framework for integrating general forms of domain knowledge
(i.e., any knowledge that can be represented by a loss function) into a BNN
prior through variational inference, while enabling computationally efficient
posterior inference and sampling. Specifically, our approach results in a prior
over neural network weights that assigns high probability mass to models that
better align with our domain knowledge, leading to posterior samples that also
exhibit this behavior. We show that BNNs using our proposed domain knowledge
priors outperform those with standard priors (e.g., isotropic Gaussian,
Gaussian process), successfully incorporating diverse types of prior
information such as fairness, physics rules, and healthcare knowledge and
achieving better predictive performance. We also present techniques for
transferring the learned priors across different model architectures,
demonstrating their broad utility across various settings.
\\ ( https://arxiv.org/abs/2402.13410 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13412
Date: Tue, 20 Feb 2024 22:45:00 GMT   (5018kb,D)

Title: Scaling physics-informed hard constraints with mixture-of-experts
Authors: Nithin Chalapathi and Yiheng Du and Aditi Krishnapriyan
Categories: cs.LG cs.AI cs.NA math.NA math.OC
Comments: Accepted to the International Conference on Learning Representations
  (ICLR) 2024
\\
  Imposing known physical constraints, such as conservation laws, during neural
network training introduces an inductive bias that can improve accuracy,
reliability, convergence, and data efficiency for modeling physical dynamics.
While such constraints can be softly imposed via loss function penalties,
recent advancements in differentiable physics and optimization improve
performance by incorporating PDE-constrained optimization as individual layers
in neural networks. This enables a stricter adherence to physical constraints.
However, imposing hard constraints significantly increases computational and
memory costs, especially for complex dynamical systems. This is because it
requires solving an optimization problem over a large number of points in a
mesh, representing spatial and temporal discretizations, which greatly
increases the complexity of the constraint. To address this challenge, we
develop a scalable approach to enforce hard physical constraints using
Mixture-of-Experts (MoE), which can be used with any neural network
architecture. Our approach imposes the constraint over smaller decomposed
domains, each of which is solved by an "expert" through differentiable
optimization. During training, each expert independently performs a localized
backpropagation step by leveraging the implicit function theorem; the
independence of each expert allows for parallelization across multiple GPUs.
Compared to standard differentiable optimization, our scalable approach
achieves greater accuracy in the neural PDE solver setting for predicting the
dynamics of challenging non-linear systems. We also improve training stability
and require significantly less computation time during both training and
inference stages.
\\ ( https://arxiv.org/abs/2402.13412 ,  5018kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13414
Date: Tue, 20 Feb 2024 22:50:41 GMT   (866kb,D)

Title: Harnessing Large Language Models as Post-hoc Correctors
Authors: Zhiqiang Zhong and Kuangyu Zhou and Davide Mottin
Categories: cs.LG cs.CL
\\
  As Machine Learning (ML) models grow in size and demand higher-quality
training data, the expenses associated with re-training and fine-tuning these
models are escalating rapidly. Inspired by recent impressive achievements of
Large Language Models (LLMs) in different fields, this paper delves into the
question: can LLMs efficiently improve an ML's performance at a minimal cost?
We show that, through our proposed training-free framework LlmCorr, an LLM can
work as a post-hoc corrector to propose corrections for the predictions of an
arbitrary ML model. In particular, we form a contextual knowledge database by
incorporating the dataset's label information and the ML model's predictions on
the validation dataset. Leveraging the in-context learning capability of LLMs,
we ask the LLM to summarise the instances in which the ML model makes mistakes
and the correlation between primary predictions and true labels. Following
this, the LLM can transfer its acquired knowledge to suggest corrections for
the ML model's predictions. Our experimental results on the challenging
molecular predictions show that LlmCorr improves the performance of a number of
models by up to 39%.
\\ ( https://arxiv.org/abs/2402.13414 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13418
Date: Tue, 20 Feb 2024 23:06:21 GMT   (338kb,D)

Title: EvolMPNN: Predicting Mutational Effect on Homologous Proteins by
  Evolution Encoding
Authors: Zhiqiang Zhong and Davide Mottin
Categories: cs.LG q-bio.BM
\\
  Predicting protein properties is paramount for biological and medical
advancements. Current protein engineering mutates on a typical protein, called
the wild-type, to construct a family of homologous proteins and study their
properties. Yet, existing methods easily neglect subtle mutations, failing to
capture the effect on the protein properties. To this end, we propose EvolMPNN,
Evolution-aware Message Passing Neural Network, to learn evolution-aware
protein embeddings. EvolMPNN samples sets of anchor proteins, computes
evolutionary information by means of residues and employs a differentiable
evolution-aware aggregation scheme over these sampled anchors. This way
EvolMPNNcan capture the mutation effect on proteins with respect to the anchor
proteins. Afterwards, the aggregated evolution-aware embeddings are integrated
with sequence embeddings to generate final comprehensive protein embeddings.
Our model shows up to 6.4% better than state-of-the-art methods and attains 36x
inference speedup in comparison with large pre-trained models.
\\ ( https://arxiv.org/abs/2402.13418 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13421
Date: Tue, 20 Feb 2024 23:20:36 GMT   (991kb,D)

Title: Context-Aware Quantitative Risk Assessment Machine Learning Model for
  Drivers Distraction
Authors: Adebamigbe Fasanmade, Ali H. Al-Bayatti, Jarrad Neil Morden and Fabio
  Caraffini
Categories: cs.LG cs.CY
\\
  Risk mitigation techniques are critical to avoiding accidents associated with
driving behaviour. We provide a novel Multi-Class Driver Distraction Risk
Assessment (MDDRA) model that considers the vehicle, driver, and environmental
data during a journey. MDDRA categorises the driver on a risk matrix as safe,
careless, or dangerous. It offers flexibility in adjusting the parameters and
weights to consider each event on a specific severity level. We collect
real-world data using the Field Operation Test (TeleFOT), covering drivers
using the same routes in the East Midlands, United Kingdom (UK). The results
show that reducing road accidents caused by driver distraction is possible. We
also study the correlation between distraction (driver, vehicle, and
environment) and the classification severity based on a continuous distraction
severity score. Furthermore, we apply machine learning techniques to classify
and predict driver distraction according to severity levels to aid the
transition of control from the driver to the vehicle (vehicle takeover) when a
situation is deemed risky. The Ensemble Bagged Trees algorithm performed best,
with an accuracy of 96.2%.
\\ ( https://arxiv.org/abs/2402.13421 ,  991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13425
Date: Tue, 20 Feb 2024 23:29:41 GMT   (6019kb,D)

Title: Investigating the Histogram Loss in Regression
Authors: Ehsan Imani, Kai Luedemann, Sam Scholnick-Hughes, Esraa Elelimy,
  Martha White
Categories: cs.LG cs.AI stat.ML
Comments: 50 pages
\\
  It is becoming increasingly common in regression to train neural networks
that model the entire distribution even if only the mean is required for
prediction. This additional modeling often comes with performance gain and the
reasons behind the improvement are not fully known. This paper investigates a
recent approach to regression, the Histogram Loss, which involves learning the
conditional distribution of the target variable by minimizing the cross-entropy
between a target distribution and a flexible histogram prediction. We design
theoretical and empirical analyses to determine why and when this performance
gain appears, and how different components of the loss contribute to it. Our
results suggest that the benefits of learning distributions in this setup come
from improvements in optimization rather than learning a better representation.
We then demonstrate the viability of the Histogram Loss in common deep learning
applications without a need for costly hyperparameter tuning.
\\ ( https://arxiv.org/abs/2402.13425 ,  6019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13430
Date: Tue, 20 Feb 2024 23:49:25 GMT   (973kb,D)

Title: LinkSAGE: Optimizing Job Matching Using Graph Neural Networks
Authors: Ping Liu, Haichao Wei, Xiaochen Hou, Jianqiang Shen, Shihai He, Kay
  Qianqi Shen, Zhujun Chen, Fedor Borisyuk, Daniel Hewlett, Liang Wu, Srikant
  Veeraraghavan, Alex Tsun, Chengming Jiang, Wenjing Zhang
Categories: cs.LG cs.AI cs.SI
\\
  We present LinkSAGE, an innovative framework that integrates Graph Neural
Networks (GNNs) into large-scale personalized job matching systems, designed to
address the complex dynamics of LinkedIns extensive professional network. Our
approach capitalizes on a novel job marketplace graph, the largest and most
intricate of its kind in industry, with billions of nodes and edges. This graph
is not merely extensive but also richly detailed, encompassing member and job
nodes along with key attributes, thus creating an expansive and interwoven
network. A key innovation in LinkSAGE is its training and serving methodology,
which effectively combines inductive graph learning on a heterogeneous,
evolving graph with an encoder-decoder GNN model. This methodology decouples
the training of the GNN model from that of existing Deep Neural Nets (DNN)
models, eliminating the need for frequent GNN retraining while maintaining
up-to-date graph signals in near realtime, allowing for the effective
integration of GNN insights through transfer learning. The subsequent nearline
inference system serves the GNN encoder within a real-world setting,
significantly reducing online latency and obviating the need for costly
real-time GNN infrastructure. Validated across multiple online A/B tests in
diverse product scenarios, LinkSAGE demonstrates marked improvements in member
engagement, relevance matching, and member retention, confirming its
generalizability and practical impact.
\\ ( https://arxiv.org/abs/2402.13430 ,  973kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13441
Date: Wed, 21 Feb 2024 00:24:34 GMT   (25243kb,D)

Title: PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory
  Access Prediction Models
Authors: Neelesh Gupta, Pengmiao Zhang, Rajgopal Kannan and Viktor Prasanna
Categories: cs.LG cs.AR
Comments: 6 pages, 2 figures, HPEC '23
Journal-ref: 2023 IEEE High Performance Extreme Computing Conference (HPEC),
  2023, pp. 1-7
DOI: 10.1109/HPEC58863.2023.10363610
\\
  Deep neural networks (DNNs) have proven to be effective models for accurate
Memory Access Prediction (MAP), a critical task in mitigating memory latency
through data prefetching. However, existing DNN-based MAP models suffer from
the challenges such as significant physical storage space and poor inference
latency, primarily due to their large number of parameters. These limitations
render them impractical for deployment in real-world scenarios. In this paper,
we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to
compress MAP models while maintaining the prediction performance. The PaCKD
approach encompasses three steps: clustering memory access sequences into
distinct partitions involving similar patterns, training large pattern-specific
teacher models for memory access prediction for each partition, and training a
single lightweight student model by distilling the knowledge from the trained
pattern-specific teachers. We evaluate our approach on LSTM, MLP-Mixer, and
ResNet models, as they exhibit diverse structures and are widely used for image
classification tasks in order to test their effectiveness in four widely used
graph applications. Compared to the teacher models with 5.406M parameters and
an F1-score of 0.4626, our student models achieve a 552$\times$ model size
compression while maintaining an F1-score of 0.4538 (with a 1.92% performance
drop). Our approach yields an 8.70% higher result compared to student models
trained with standard knowledge distillation and an 8.88% higher result
compared to student models trained without any form of knowledge distillation.
\\ ( https://arxiv.org/abs/2402.13441 ,  25243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13454
Date: Wed, 21 Feb 2024 01:18:32 GMT   (12566kb,D)

Title: Theoretical Analysis of Submodular Information Measures for Targeted
  Data Subset Selection
Authors: Nathan Beck, Truong Pham, Rishabh Iyer
Categories: cs.LG cs.IT math.IT
Comments: 16 pages, 10 figures
\\
  With increasing volume of data being used across machine learning tasks, the
capability to target specific subsets of data becomes more important. To aid in
this capability, the recently proposed Submodular Mutual Information (SMI) has
been effectively applied across numerous tasks in literature to perform
targeted subset selection with the aid of a exemplar query set. However, all
such works are deficient in providing theoretical guarantees for SMI in terms
of its sensitivity to a subset's relevance and coverage of the targeted data.
For the first time, we provide such guarantees by deriving similarity-based
bounds on quantities related to relevance and coverage of the targeted data.
With these bounds, we show that the SMI functions, which have empirically shown
success in multiple applications, are theoretically sound in achieving good
query relevance and query coverage.
\\ ( https://arxiv.org/abs/2402.13454 ,  12566kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13459
Date: Wed, 21 Feb 2024 01:30:03 GMT   (447kb,D)

Title: Learning to Poison Large Language Models During Instruction Tuning
Authors: Yao Qiang and Xiangyu Zhou and Saleh Zare Zade and Mohammad Amin
  Roshani and Douglas Zytko and Dongxiao Zhu
Categories: cs.LG cs.CL cs.CR
\\
  The advent of Large Language Models (LLMs) has marked significant
achievements in language processing and reasoning capabilities. Despite their
advancements, LLMs face vulnerabilities to data poisoning attacks, where
adversaries insert backdoor triggers into training data to manipulate outputs
for malicious purposes. This work further identifies additional security risks
in LLMs by designing a new data poisoning attack tailored to exploit the
instruction tuning process. We propose a novel gradient-guided backdoor trigger
learning approach to identify adversarial triggers efficiently, ensuring an
evasion of detection by conventional defenses while maintaining content
integrity. Through experimental validation across various LLMs and tasks, our
strategy demonstrates a high success rate in compromising model outputs;
poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance
Drop Rate (PDR) of around 80\%. Our work highlights the need for stronger
defenses against data poisoning attack, offering insights into safeguarding
LLMs against these more sophisticated attacks. The source code can be found on
this GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.
\\ ( https://arxiv.org/abs/2402.13459 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13468
Date: Wed, 21 Feb 2024 01:54:58 GMT   (98kb,D)

Title: STENCIL: Submodular Mutual Information Based Weak Supervision for
  Cold-Start Active Learning
Authors: Nathan Beck, Adithya Iyer, Rishabh Iyer
Categories: cs.LG cs.CL
Comments: 11 pages, 1 figure
\\
  As supervised fine-tuning of pre-trained models within NLP applications
increases in popularity, larger corpora of annotated data are required,
especially with increasing parameter counts in large language models. Active
learning, which attempts to mine and annotate unlabeled instances to improve
model performance maximally fast, is a common choice for reducing the
annotation cost; however, most methods typically ignore class imbalance and
either assume access to initial annotated data or require multiple rounds of
active learning selection before improving rare classes. We present STENCIL,
which utilizes a set of text exemplars and the recently proposed submodular
mutual information to select a set of weakly labeled rare-class instances that
are then strongly labeled by an annotator. We show that STENCIL improves
overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on
multiple text classification datasets over common active learning methods
within the class-imbalanced cold-start setting.
\\ ( https://arxiv.org/abs/2402.13468 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13485
Date: Wed, 21 Feb 2024 02:51:07 GMT   (490kb,D)

Title: ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel
  Decoding
Authors: Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru
  Huang
Categories: cs.LG cs.CL
\\
  Recent advancements in generative large language models (LLMs) have
significantly boosted the performance in natural language processing tasks.
However, their efficiency is hampered by the inherent limitations in
autoregressive token generation. While parallel decoding with token tree
verification, e.g., Medusa, has been proposed to improve decoding parallelism
and efficiency, it often struggles with maintaining contextual relationships
due to its independent token prediction approach and incurs significant
verification overhead, especially with large tree sizes and batch processing.
In this paper, we propose ProPD, an efficient LLM parallel decoding framework
based on dynamic token tree pruning and generation. ProPD features an advanced
early pruning mechanism to efficiently eliminate unpromising token sequences to
improve verification efficiency. Additionally, it introduces a dynamic token
tree generation algorithm to balance the computation and parallelism of the
verification phase in real-time and maximize the overall efficiency across
different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across
a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD
consistently outperforms existing decoding algorithms by 1.1-3.2x.
\\ ( https://arxiv.org/abs/2402.13485 ,  490kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13487
Date: Wed, 21 Feb 2024 02:54:00 GMT   (872kb,D)

Title: Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits
Authors: Zhiwei Wang, Huazheng Wang, Hongning Wang
Categories: cs.LG cs.CR
\\
  Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms
have been extensively studied in the literature. In this work, we focus on
reward poisoning attacks and find most existing attacks can be easily detected
by our proposed detection method based on the test of homogeneity, due to their
aggressive nature in reward manipulations. This motivates us to study the
notion of stealthy attack against stochastic MABs and investigate the resulting
attackability. Our analysis shows that against two popularly employed MAB
algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack
depends on the environmental conditions and the realized reward of the arm
pulled in the first round. We also analyze the situation for general MAB
algorithms equipped with our attack detection method and find that it is
possible to have a stealthy attack that almost always succeeds. This brings new
insights into the security risks of MAB algorithms.
\\ ( https://arxiv.org/abs/2402.13487 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13496
Date: Wed, 21 Feb 2024 03:14:45 GMT   (1042kb,D)

Title: HetTree: Heterogeneous Tree Graph Neural Network
Authors: Mingyu Guan, Jack W. Stokes, Qinlong Luo, Fuchen Liu, Purvanshi Mehta,
  Elnaz Nouri, Taesoo Kim
Categories: cs.LG cs.SI
\\
  The recent past has seen an increasing interest in Heterogeneous Graph Neural
Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from
citation graphs to email graphs. However, existing methods ignore a tree
hierarchy among metapaths, which is naturally constituted by different node
types and relation types. In this paper, we present HetTree, a novel
heterogeneous tree graph neural network that models both the graph structure
and heterogeneous aspects in a scalable and effective manner. Specifically,
HetTree builds a semantic tree data structure to capture the hierarchy among
metapaths. Existing tree encoding techniques aggregate children nodes by
weighting the contribution of children nodes based on similarity to the parent
node. However, we find that this tree encoding fails to capture the entire
parent-children hierarchy by only considering the parent node. Hence, HetTree
uses a novel subtree attention mechanism to emphasize metapaths that are more
helpful in encoding parent-children relationships. Moreover, instead of
separating feature learning from label learning or treating features and labels
equally by projecting them to the same latent space, HetTree proposes to match
them carefully based on corresponding metapaths, which provides more accurate
and richer information between node features and labels. Our evaluation of
HetTree on a variety of real-world datasets demonstrates that it outperforms
all existing baselines on open benchmarks and efficiently scales to large
real-world graphs with millions of nodes and edges.
\\ ( https://arxiv.org/abs/2402.13496 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13505
Date: Wed, 21 Feb 2024 03:39:04 GMT   (488kb,D)

Title: SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed
  Semi-Supervised Learning
Authors: Chaoqun Du, Yizeng Han, Gao Huang
Categories: cs.LG cs.CV
\\
  Recent advancements in semi-supervised learning have focused on a more
realistic yet challenging task: addressing imbalances in labeled data while the
class distribution of unlabeled data remains both unknown and potentially
mismatched. Current approaches in this sphere often presuppose rigid
assumptions regarding the class distribution of unlabeled data, thereby
limiting the adaptability of models to only certain distribution ranges. In
this study, we propose a novel approach, introducing a highly adaptable
framework, designated as SimPro, which does not rely on any predefined
assumptions about the distribution of unlabeled data. Our framework, grounded
in a probabilistic model, innovatively refines the expectation-maximization
(EM) algorithm by explicitly decoupling the modeling of conditional and
marginal class distributions. This separation facilitates a closed-form
solution for class distribution estimation during the maximization phase,
leading to the formulation of a Bayes classifier. The Bayes classifier, in
turn, enhances the quality of pseudo-labels in the expectation phase.
Remarkably, the SimPro framework not only comes with theoretical guarantees but
also is straightforward to implement. Moreover, we introduce two novel class
distributions broadening the scope of the evaluation. Our method showcases
consistent state-of-the-art performance across diverse benchmarks and data
distribution scenarios. Our code is available at
https://github.com/LeapLabTHU/SimPro.
\\ ( https://arxiv.org/abs/2402.13505 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13512
Date: Wed, 21 Feb 2024 03:51:34 GMT   (1631kb,D)

Title: From Self-Attention to Markov Models: Unveiling the Dynamics of
  Generative Transformers
Authors: M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat and
  Samet Oymak
Categories: cs.LG cs.AI cs.CL
Comments: 30 pages
\\
  Modern language models rely on the transformer architecture and attention
mechanism to perform language understanding and text generation. In this work,
we study learning a 1-layer self-attention model from a set of prompts and
associated output data sampled from the model. We first establish a precise
mapping between the self-attention mechanism and Markov models: Inputting a
prompt to the model samples the output token according to a context-conditioned
Markov chain (CCMC) which weights the transition matrix of a base Markov chain.
Additionally, incorporating positional encoding results in position-dependent
scaling of the transition probabilities. Building on this formalism, we develop
identifiability/coverage conditions for the prompt distribution that guarantee
consistent estimation and establish sample complexity guarantees under IID
samples. Finally, we study the problem of learning from a single output
trajectory generated from an initial prompt. We characterize an intriguing
winner-takes-all phenomenon where the generative process implemented by
self-attention collapses into sampling a limited subset of tokens due to its
non-mixing nature. This provides a mathematical explanation to the tendency of
modern LLMs to generate repetitive text. In summary, the equivalence to CCMC
provides a simple but powerful framework to study self-attention and its
properties.
\\ ( https://arxiv.org/abs/2402.13512 ,  1631kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13516
Date: Wed, 21 Feb 2024 03:58:49 GMT   (228kb,D)

Title: ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity
  within Large Language Models
Authors: Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai
  Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun
Categories: cs.LG cs.AI cs.CL
Comments: 16 pages, 3 figures, 7 tables
ACM-class: I.2.7
\\
  Activation sparsity refers to the existence of considerable
weakly-contributed elements among activation outputs. As a prevalent property
of the models using the ReLU activation function, it has been proven a
promising paradigm to boost model inference efficiency. Nevertheless, most
large language models (LLMs) adopt activation functions without intrinsic
activation sparsity (e.g., GELU and Swish). Some recent efforts have explored
introducing ReLU or its variants as the substitutive activation function to
help LLMs achieve activation sparsity and inference acceleration, but few can
simultaneously obtain high sparsity and comparable model performance. This
paper introduces an effective sparsification method named "ProSparse" to push
LLMs for higher activation sparsity without decreasing model performance.
Specifically, after substituting the activation function of LLMs with ReLU,
ProSparse adopts progressive sparsity regularization with a factor smoothly
increasing along sine curves in multiple stages. This can enhance activation
sparsity and alleviate performance degradation by avoiding radical shifts in
activation distribution. With ProSparse, we obtain high sparsity of 89.32% and
88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable
performance to their original Swish-activated versions. Our inference
acceleration experiments further demonstrate the practical acceleration brought
by higher activation sparsity.
\\ ( https://arxiv.org/abs/2402.13516 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13525
Date: Wed, 21 Feb 2024 04:43:12 GMT   (410kb,D)

Title: MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via
  Automating Deep Neural Network Porting for Mobile Deployment
Authors: Hongtao Huang, Xiaojun Chang, Wen Hu and Lina Yao
Categories: cs.LG cs.DC
DOI: 10.1145/3589334.3645538
\\
  Recent years have seen the explosion of edge intelligence with powerful Deep
Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud
servers and subsequently porting them to mobile devices after being
lightweight. Conventional approaches manually specialized DNNs for various edge
platforms and retrain them with real-world data. However, as the number of
platforms increases, these approaches become labour-intensive and
computationally prohibitive. Additionally, real-world data tends to be
sparse-label, further increasing the difficulty of lightweight models. In this
paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices.
Specifically, we simultaneously optimise a large network family using both
labelled and unlabelled data and then automatically search for tailored
networks for different hardware platforms. MatchNAS acts as an intermediary
that bridges the gap between cloud-based DNNs and edge-based DNNs.
\\ ( https://arxiv.org/abs/2402.13525 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13531
Date: Wed, 21 Feb 2024 04:58:41 GMT   (250kb,D)

Title: Private Gradient Descent for Linear Regression: Tighter Error Bounds and
  Instance-Specific Uncertainty Estimation
Authors: Gavin Brown, Krishnamurthy Dvijotham, Georgina Evans, Daogao Liu, Adam
  Smith, Abhradeep Thakurta
Categories: cs.LG cs.CR
Comments: 22 pages, 11 figures
\\
  We provide an improved analysis of standard differentially private gradient
descent for linear regression under the squared error loss. Under modest
assumptions on the input, we characterize the distribution of the iterate at
each time step.
  Our analysis leads to new results on the algorithm's accuracy: for a proper
fixed choice of hyperparameters, the sample complexity depends only linearly on
the dimension of the data. This matches the dimension-dependence of the
(non-private) ordinary least squares estimator as well as that of recent
private algorithms that rely on sophisticated adaptive gradient-clipping
schemes (Varshney et al., 2022; Liu et al., 2023).
  Our analysis of the iterates' distribution also allows us to construct
confidence intervals for the empirical optimizer which adapt automatically to
the variance of the algorithm on a particular data set. We validate our
theorems through experiments on synthetic data.
\\ ( https://arxiv.org/abs/2402.13531 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13533
Date: Wed, 21 Feb 2024 05:03:17 GMT   (7872kb,D)

Title: FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models
  for Financial Applications with High-Performance Computing
Authors: Xiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, and Anwar Walid
Categories: cs.LG cs.AI cs.CL cs.DC
\\
  Large language models (LLMs) are computationally intensive. The computation
workload and the memory footprint grow quadratically with the dimension (layer
width). Most of LLMs' parameters come from the linear layers of the transformer
structure and are highly redundant. These linear layers contribute more than
80% of the computation workload and 99% of the model size. To pretrain and
finetune LLMs efficiently, there are three major challenges to address: 1)
reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3)
improving GPU utilization when using distributed training. Prior methods, such
as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the
number of trainable parameters and model size, respectively. However, the
resulting model still consumes a large amount of GPU memory. In this paper, we
present high-performance GPU-based methods that exploit low-rank structures to
pretrain and finetune LLMs for financial applications. We replace one
conventional linear layer of the transformer structure with two narrower linear
layers, which allows us to reduce the number of parameters by several orders of
magnitude. By quantizing the parameters into low precision (8-bit and 4-bit),
the memory consumption of the resulting model is further reduced. Compared with
existing LLMs, our methods achieve a speedup of 1.3X and a model compression
ratio of 2.64X for pretaining without accuracy drop. For finetuning, our
methods achieve an average accuracy increase of 6.3% and 24.0% in general tasks
and financial tasks, respectively, and GPU memory consumption ratio of 6.3X.
The sizes of our models are smaller than 0.59 GB, allowing inference on a
smartphone.
\\ ( https://arxiv.org/abs/2402.13533 ,  7872kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13548
Date: Wed, 21 Feb 2024 06:07:33 GMT   (2810kb,D)

Title: DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of
  EV Charging Load
Authors: Siyang Li, Hui Xiong, Yize Chen
Categories: cs.LG eess.SP
Comments: Accepted to the 23rd Power Systems Computation Conference (PSCC).
  Code is released at https://github.com/LSY-Cython/DiffPLF
\\
  Due to the vast electric vehicle (EV) penetration to distribution grid,
charging load forecasting is essential to promote charging station operation
and demand-side management.However, the stochastic charging behaviors and
associated exogenous factors render future charging load patterns quite
volatile and hard to predict. Accordingly, we devise a novel Diffusion model
termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can
explicitly approximate the predictive load distribution conditioned on
historical data and related covariates. Specifically, we leverage a denoising
diffusion model, which can progressively convert the Gaussian prior to real
time-series data by learning a reversal of the diffusion process. Besides, we
couple such diffusion model with a cross-attention-based conditioning mechanism
to execute conditional generation for possible charging demand profiles. We
also propose a task-informed fine-tuning technique to better adapt DiffPLF to
the probabilistic time-series forecasting task and acquire more accurate and
reliable predicted intervals. Finally, we conduct multiple experiments to
validate the superiority of DiffPLF to predict complex temporal patterns of
erratic charging load and carry out controllable generation based on certain
covariate. Results demonstrate that we can attain a notable rise of 39.58% and
49.87% on MAE and CRPS respectively compared to the conventional method.
\\ ( https://arxiv.org/abs/2402.13548 ,  2810kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13556
Date: Wed, 21 Feb 2024 06:25:54 GMT   (572kb,D)

Title: Inductive Graph Alignment Prompt: Bridging the Gap between Graph
  Pre-training and Inductive Fine-tuning From Spectral Perspective
Authors: Yuchen Yan, Peiyan Zhang, Zheng Fang, Qingqing Long
Categories: cs.LG cs.AI
ACM-class: E.2
DOI: 10.1145/3589334.3645620
\\
  The "Graph pre-training and fine-tuning" paradigm has significantly improved
Graph Neural Networks(GNNs) by capturing general knowledge without manual
annotations for downstream tasks. However, due to the immense gap of data and
tasks between the pre-training and fine-tuning stages, the model performance is
still limited. Inspired by prompt fine-tuning in Natural Language
Processing(NLP), many endeavors have been made to bridge the gap in graph
domain. But existing methods simply reformulate the form of fine-tuning tasks
to the pre-training ones. With the premise that the pre-training graphs are
compatible with the fine-tuning ones, these methods typically operate in
transductive setting. In order to generalize graph pre-training to inductive
scenario where the fine-tuning graphs might significantly differ from
pre-training ones, we propose a novel graph prompt based method called
Inductive Graph Alignment Prompt(IGAP). Firstly, we unify the mainstream graph
pre-training frameworks and analyze the essence of graph pre-training from
graph spectral theory. Then we identify the two sources of the data gap in
inductive setting: (i) graph signal gap and (ii) graph structure gap. Based on
the insight of graph pre-training, we propose to bridge the graph signal gap
and the graph structure gap with learnable prompts in the spectral space. A
theoretical analysis ensures the effectiveness of our method. At last, we
conduct extensive experiments among nodes classification and graph
classification tasks under the transductive, semi-inductive and inductive
settings. The results demonstrate that our proposed method can successfully
bridge the data gap under different settings.
\\ ( https://arxiv.org/abs/2402.13556 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13567
Date: Wed, 21 Feb 2024 06:57:07 GMT   (1415kb,D)

Title: Spot Check Equivalence: an Interpretable Metric for Information
  Elicitation Mechanisms
Authors: Shengwei Xu, Yichi Zhang, Paul Resnick, Grant Schoenebeck
Categories: cs.LG cs.AI cs.GT
Comments: Accepted by the Web Conference 2024 (WWW '24)
\\
  Because high-quality data is like oxygen for AI systems, effectively
eliciting information from crowdsourcing workers has become a first-order
problem for developing high-performance machine learning algorithms. Two
prevalent paradigms, spot-checking and peer prediction, enable the design of
mechanisms to evaluate and incentivize high-quality data from human labelers.
So far, at least three metrics have been proposed to compare the performances
of these techniques [33, 8, 3]. However, different metrics lead to divergent
and even contradictory results in various contexts. In this paper, we harmonize
these divergent stories, showing that two of these metrics are actually the
same within certain contexts and explain the divergence of the third. Moreover,
we unify these different contexts by introducing \textit{Spot Check
Equivalence}, which offers an interpretable metric for the effectiveness of a
peer prediction mechanism. Finally, we present two approaches to compute spot
check equivalence in various contexts, where simulation results verify the
effectiveness of our proposed metric.
\\ ( https://arxiv.org/abs/2402.13567 ,  1415kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13572
Date: Wed, 21 Feb 2024 07:07:54 GMT   (353kb,D)

Title: On the Expressive Power of a Variant of the Looped Transformer
Authors: Yihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li,
  Michael K. Ng, Zhenguo Li, Zhaoqiang Liu
Categories: cs.LG cs.AI cs.NA math.NA
\\
  Besides natural language processing, transformers exhibit extraordinary
performance in solving broader applications, including scientific computing and
computer vision. Previous works try to explain this from the expressive power
and capability perspectives that standard transformers are capable of
performing some algorithms. To empower transformers with algorithmic
capabilities and motivated by the recently proposed looped transformer (Yang et
al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed
Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard
transformer and vanilla looped transformer, the proposed AlgoFormer can achieve
significantly higher expressiveness in algorithm representation when using the
same number of parameters. In particular, inspired by the structure of
human-designed learning algorithms, our transformer block consists of a
pre-transformer that is responsible for task pre-processing, a looped
transformer for iterative optimization algorithms, and a post-transformer for
producing the desired results after post-processing. We provide theoretical
evidence of the expressive power of the AlgoFormer in solving some challenging
problems, mirroring human-designed algorithms. Furthermore, some theoretical
and empirical results are presented to show that the designed transformer has
the potential to be smarter than human-designed algorithms. Experimental
results demonstrate the empirical superiority of the proposed transformer in
that it outperforms the standard transformer and vanilla looped transformer in
some challenging tasks.
\\ ( https://arxiv.org/abs/2402.13572 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13628
Date: Wed, 21 Feb 2024 09:04:45 GMT   (6317kb,D)

Title: Improving Building Temperature Forecasting: A Data-driven Approach with
  System Scenario Clustering
Authors: Dafang Zhao, Zheng Chen, Zhengmao Li, Xiaolei Yuan, Ittetsu Taniguchi
Categories: cs.LG eess.SP
Comments: Accepted and will be published on IEEE PES GM 2024
\\
  Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in
maintaining a comfortable thermal environment and cost approximately 40% of
primary energy usage in the building sector. For smart energy management in
buildings, usage patterns and their resulting profiles allow the improvement of
control systems with prediction capabilities. However, for large-scale HVAC
system management, it is difficult to construct a detailed model for each
subsystem. In this paper, a new data-driven room temperature prediction model
is proposed based on the k-means clustering method. The proposed data-driven
temperature prediction approach extracts the system operation feature through
historical data analysis and further simplifies the system-level model to
improve generalization and computational efficiency. We evaluate the proposed
approach in the real world. The results demonstrated that our approach can
significantly reduce modeling time without reducing prediction accuracy.
\\ ( https://arxiv.org/abs/2402.13628 ,  6317kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13630
Date: Wed, 21 Feb 2024 09:06:31 GMT   (288kb,D)

Title: UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural
  Language
Authors: Yufei He, Bryan Hooi
Categories: cs.LG
Comments: 16 pages, 1 figure. Preliminary work
\\
  Foundation models like ChatGPT and GPT-4 have revolutionized artificial
intelligence, exhibiting remarkable abilities to generalize across a wide array
of tasks and applications beyond their initial training objectives. However,
when this concept is applied to graph learning, a stark contrast emerges. Graph
learning has predominantly focused on single-graph models, tailored to specific
tasks or datasets, lacking the ability to transfer learned knowledge to
different domains. This limitation stems from the inherent complexity and
diversity of graph structures, along with the different feature and label
spaces specific to graph data. In this paper, we present our UniGraph
framework, designed to train a graph foundation model capable of generalizing
to unseen graphs and tasks across diverse domains. Unlike single-graph models
that use pre-computed node features of varying dimensions as input, our
approach leverages Text-Attributed Graphs (TAGs) for unifying node
representations. We propose a cascaded architecture of Language Models (LMs)
and Graph Neural Networks (GNNs) as backbone networks with a self-supervised
training objective based on Masked Graph Modeling (MGM). We introduce graph
instruction tuning using Large Language Models (LLMs) to enable zero-shot
prediction ability. Our comprehensive experiments across various graph learning
tasks and domains demonstrate the model's effectiveness in self-supervised
representation learning on unseen graphs, few-shot in-context transfer, and
zero-shot transfer, even surpassing or matching the performance of GNNs that
have undergone supervised training on target datasets.
\\ ( https://arxiv.org/abs/2402.13630 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13635
Date: Wed, 21 Feb 2024 09:15:46 GMT   (315kb,D)

Title: The METRIC-framework for assessing data quality for trustworthy AI in
  medicine: a systematic review
Authors: Daniel Schwabe, Katinka Becker, Martin Seyferth, Andreas Kla{\ss},
  Tobias Sch\"affter
Categories: cs.LG cs.AI
\\
  The adoption of machine learning (ML) and, more specifically, deep learning
(DL) applications into all major areas of our lives is underway. The
development of trustworthy AI is especially important in medicine due to the
large implications for patients' lives. While trustworthiness concerns various
aspects including ethical, technical and privacy requirements, we focus on the
importance of data quality (training/test) in DL. Since data quality dictates
the behaviour of ML products, evaluating data quality will play a key part in
the regulatory approval of medical AI products. We perform a systematic review
following PRISMA guidelines using the databases PubMed and ACM Digital Library.
We identify 2362 studies, out of which 62 records fulfil our eligibility
criteria. From this literature, we synthesise the existing knowledge on data
quality frameworks and combine it with the perspective of ML applications in
medicine. As a result, we propose the METRIC-framework, a specialised data
quality framework for medical training data comprising 15 awareness dimensions,
along which developers of medical ML applications should investigate a dataset.
This knowledge helps to reduce biases as a major source of unfairness, increase
robustness, facilitate interpretability and thus lays the foundation for
trustworthy AI in medicine. Incorporating such systematic assessment of medical
datasets into regulatory approval processes has the potential to accelerate the
approval of ML products and builds the basis for new standards.
\\ ( https://arxiv.org/abs/2402.13635 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13641
Date: Wed, 21 Feb 2024 09:18:59 GMT   (8998kb,D)

Title: FlexHB: a More Efficient and Flexible Framework for Hyperparameter
  Optimization
Authors: Yang Zhang, Haiyang Wu, Yuekui Yang
Categories: cs.LG
\\
  Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm
to find optimal configurations efficiently? Bayesian Optimization(BO) and the
multi-fidelity BO methods employ surrogate models to sample configurations
based on history evaluations. More recent studies obtain better performance by
integrating BO with HyperBand(HB), which accelerates evaluation by early
stopping mechanism. However, these methods ignore the advantage of a suitable
evaluation scheme over the default HyperBand, and the capability of BO is still
constrained by skewed evaluation results. In this paper, we propose FlexHB, a
new method pushing multi-fidelity BO to the limit as well as re-designing a
framework for early stopping with Successive Halving(SH). Comprehensive study
on FlexHB shows that (1) our fine-grained fidelity method considerably enhances
the efficiency of searching optimal configurations, (2) our FlexBand framework
(self-adaptive allocation of SH brackets, and global ranking of configurations
in both current and past SH procedures) grants the algorithm with more
flexibility and improves the anytime performance. Our method achieves superior
efficiency and outperforms other methods on various HPO tasks. Empirical
results demonstrate that FlexHB can achieve up to 6.9X and 11.1X speedups over
the state-of-the-art MFES-HB and BOHB respectively.
\\ ( https://arxiv.org/abs/2402.13641 ,  8998kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13653
Date: Wed, 21 Feb 2024 09:38:17 GMT   (1222kb,D)

Title: PQA: Zero-shot Protein Question Answering for Free-form Scientific
  Enquiry with Large Language Models
Authors: Eli M Carrami and Sahand Sharifzadeh
Categories: cs.LG
\\
  We introduce the novel task of zero-shot Protein Question Answering (PQA) for
free-form scientific enquiry. Given a previously unseen protein sequence and a
natural language question, the task is to deliver a scientifically accurate
answer. This task not only supports future biological research, but could also
provide a test bed for assessing the scientific precision of large language
models (LLMs). We contribute the first specialized dataset for PQA model
training, containing 257K protein sequences annotated with 1.97M scientific
question-answer pairs. Additionally, we propose and study several novel
biologically relevant benchmarks for scientific PQA. Employing two robust
multi-modal architectures, we establish an initial state-of-the-art performance
for PQA and reveal key performance factors through ablation studies. Our
comprehensive PQA framework, named Pika, including dataset, code, model
checkpoints, and a user-friendly demo, is openly accessible on
github.com/EMCarrami/Pika, promoting wider research and application in the
field.
\\ ( https://arxiv.org/abs/2402.13653 ,  1222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13655
Date: Wed, 21 Feb 2024 09:41:56 GMT   (1959kb,D)

Title: Stable Update of Regression Trees
Authors: Morten Bl{\o}rstad, Berent {\AA}. S. Lunde, Nello Blaser
Categories: cs.LG
\\
  Updating machine learning models with new information usually improves their
predictive performance, yet, in many applications, it is also desirable to
avoid changing the model predictions too much. This property is called
stability. In most cases when stability matters, so does explainability. We
therefore focus on the stability of an inherently explainable machine learning
method, namely regression trees. We aim to use the notion of empirical
stability and design algorithms for updating regression trees that provide a
way to balance between predictability and empirical stability. To achieve this,
we propose a regularization method, where data points are weighted based on the
uncertainty in the initial model. The balance between predictability and
empirical stability can be adjusted through hyperparameters. This
regularization method is evaluated in terms of loss and stability and assessed
on a broad range of data characteristics. The results show that the proposed
update method improves stability while achieving similar or better predictive
performance. This shows that it is possible to achieve both predictive and
stable results when updating regression trees.
\\ ( https://arxiv.org/abs/2402.13655 ,  1959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13700
Date: Wed, 21 Feb 2024 11:04:23 GMT   (784kb,D)

Title: On the Conflict of Robustness and Learning in Collaborative Machine
  Learning
Authors: Mathilde Raynal and Carmela Troncoso
Categories: cs.LG cs.CR
\\
  Collaborative Machine Learning (CML) allows participants to jointly train a
machine learning model while keeping their training data private. In scenarios
where privacy is a strong requirement, such as health-related applications,
safety is also a primary concern. This means that privacy-preserving CML
processes must produce models that output correct and reliable decisions
\emph{even in the presence of potentially untrusted participants}. In response
to this issue, researchers propose to use \textit{robust aggregators} that rely
on metrics which help filter out malicious contributions that could compromise
the training process. In this work, we formalize the landscape of robust
aggregators in the literature. Our formalization allows us to show that
existing robust aggregators cannot fulfill their goal: either they use
distance-based metrics that cannot accurately identify targeted malicious
updates; or propose methods whose success is in direct conflict with the
ability of CML participants to learn from others and therefore cannot eliminate
the risk of manipulation without preventing learning.
\\ ( https://arxiv.org/abs/2402.13700 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13711
Date: Wed, 21 Feb 2024 11:25:54 GMT   (1321kb,D)

Title: DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based
  Graph Continual Learning
Authors: Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim,
  Chanyoung Park
Categories: cs.LG cs.AI
Comments: Accepted at ACM TheWebConf 2024
DOI: 10.1145/3589334.3645561
\\
  We investigate the replay buffer in rehearsal-based approaches for graph
continual learning (GCL) methods. Existing rehearsal-based GCL methods select
the most representative nodes for each class and store them in a replay buffer
for later use in training subsequent tasks. However, we discovered that
considering only the class representativeness of each replayed node makes the
replayed nodes to be concentrated around the center of each class, incurring a
potential risk of overfitting to nodes residing in those regions, which
aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach
heavily relies on a few replayed nodes to retain knowledge obtained from
previous tasks, involving the replayed nodes that have irrelevant neighbors in
the model training may have a significant detrimental impact on model
performance. In this paper, we propose a GCL model named DSLR, specifically, we
devise a coverage-based diversity (CD) approach to consider both the class
representativeness and the diversity within each class of the replayed nodes.
Moreover, we adopt graph structure learning (GSL) to ensure that the replayed
nodes are connected to truly informative neighbors. Extensive experimental
results demonstrate the effectiveness and efficiency of DSLR.
\\ ( https://arxiv.org/abs/2402.13711 ,  1321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13725
Date: Wed, 21 Feb 2024 11:35:45 GMT   (663kb,D)

Title: Sparse and Structured Hopfield Networks
Authors: Saul Santos, Vlad Niculae, Daniel McNamee, Andre F. T. Martins
Categories: cs.LG
Comments: 20 pages, 4 figures
\\
  Modern Hopfield networks have enjoyed recent interest due to their connection
to attention in transformers. Our paper provides a unified framework for sparse
Hopfield networks by establishing a link with Fenchel-Young losses. The result
is a new family of Hopfield-Fenchel-Young energies whose update rules are
end-to-end differentiable sparse transformations. We reveal a connection
between loss margins, sparsity, and exact memory retrieval. We further extend
this framework to structured Hopfield networks via the SparseMAP
transformation, which can retrieve pattern associations instead of a single
pattern. Experiments on multiple instance learning and text rationalization
demonstrate the usefulness of our approach.
\\ ( https://arxiv.org/abs/2402.13725 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13728
Date: Wed, 21 Feb 2024 11:40:27 GMT   (2117kb,D)

Title: Average gradient outer product as a mechanism for deep neural collapse
Authors: Daniel Beaglehole, Peter S\'uken\'ik, Marco Mondelli, Mikhail Belkin
Categories: cs.LG stat.ML
\\
  Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the
data representations in the final layers of Deep Neural Networks (DNNs). Though
the phenomenon has been measured in a wide variety of settings, its emergence
is only partially understood. In this work, we provide substantial evidence
that DNC formation occurs primarily through deep feature learning with the
average gradient outer product (AGOP). This takes a step further compared to
efforts that explain neural collapse via feature-agnostic approaches, such as
the unconstrained features model. We proceed by providing evidence that the
right singular vectors and values of the weights are responsible for the
majority of within-class variability collapse in DNNs. As shown in recent work,
this singular structure is highly correlated with that of the AGOP. We then
establish experimentally and theoretically that AGOP induces neural collapse in
a randomly initialized neural network. In particular, we demonstrate that Deep
Recursive Feature Machines, a method originally introduced as an abstraction
for AGOP feature learning in convolutional neural networks, exhibits DNC.
\\ ( https://arxiv.org/abs/2402.13728 ,  2117kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13744
Date: Wed, 21 Feb 2024 12:16:51 GMT   (801kb,D)

Title: Reasoning Algorithmically in Graph Neural Networks
Authors: Danilo Numeroso
Categories: cs.LG
Comments: PhD Thesis
\\
  The development of artificial intelligence systems with advanced reasoning
capabilities represents a persistent and long-standing research question.
Traditionally, the primary strategy to address this challenge involved the
adoption of symbolic approaches, where knowledge was explicitly represented by
means of symbols and explicitly programmed rules. However, with the advent of
machine learning, there has been a paradigm shift towards systems that can
autonomously learn from data, requiring minimal human guidance. In light of
this shift, in latest years, there has been increasing interest and efforts at
endowing neural networks with the ability to reason, bridging the gap between
data-driven learning and logical reasoning. Within this context, Neural
Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to
integrate the structured and rule-based reasoning of algorithms with the
adaptive learning capabilities of neural networks, typically by tasking neural
models to mimic classical algorithms. In this dissertation, we provide
theoretical and practical contributions to this area of research. We explore
the connections between neural networks and tropical algebra, deriving powerful
architectures that are aligned with algorithm execution. Furthermore, we
discuss and show the ability of such neural reasoners to learn and manipulate
complex algorithmic and combinatorial optimization concepts, such as the
principle of strong duality. Finally, in our empirical efforts, we validate the
real-world utility of NAR networks across different practical scenarios. This
includes tasks as diverse as planning problems, large-scale edge classification
tasks and the learning of polynomial-time approximate algorithms for NP-hard
combinatorial problems. Through this exploration, we aim to showcase the
potential integrating algorithmic reasoning in machine learning models.
\\ ( https://arxiv.org/abs/2402.13744 ,  801kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13752
Date: Wed, 21 Feb 2024 12:23:09 GMT   (628kb)

Title: AI-Powered Predictions for Electricity Load in Prosumer Communities
Authors: Aleksei Kychkin, Georgios C. Chasparis
Categories: cs.LG cs.AI
Comments: It has been presented in the 18. Symposium Energieinnovation
  (14.-16.02.2024). Further information can be found at:
  https://www.tugraz.at/events/eninnov2024/home
\\
  The flexibility in electricity consumption and production in communities of
residential buildings, including those with renewable energy sources and energy
storage (a.k.a., prosumers), can effectively be utilized through the
advancement of short-term demand response mechanisms. It is known that
flexibility can further be increased if demand response is performed at the
level of communities of prosumers, since aggregated groups can better
coordinate electricity consumption. However, the effectiveness of such
short-term optimization is highly dependent on the accuracy of electricity load
forecasts both for each building as well as for the whole community. Structural
variations in the electricity load profile can be associated with different
exogenous factors, such as weather conditions, calendar information and day of
the week, as well as user behavior. In this paper, we review a wide range of
electricity load forecasting techniques, that can provide significant
assistance in optimizing load consumption in prosumer communities. We present
and test artificial intelligence (AI) powered short-term load forecasting
methodologies that operate with black-box time series models, such as
Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based
SARIMA and smoothing Holt-Winters models; and empirical regression-based models
that utilize domain knowledge. The integration of weather forecasts into
data-driven time series forecasts is also tested. Results show that the
combination of persistent and regression terms (adapted to the load forecasting
task) achieves the best forecast accuracy.
\\ ( https://arxiv.org/abs/2402.13752 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13765
Date: Wed, 21 Feb 2024 12:39:20 GMT   (622kb,D)

Title: Accuracy-Preserving Calibration via Statistical Modeling on Probability
  Simplex
Authors: Yasushi Esaki and Akihiro Nakamura and Keisuke Kawano and Ryoko
  Tokuhisa and Takuro Kutsuna
Categories: cs.LG stat.ML
Comments: accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\
  Classification models based on deep neural networks (DNNs) must be calibrated
to measure the reliability of predictions. Some recent calibration methods have
employed a probabilistic model on the probability simplex. However, these
calibration methods cannot preserve the accuracy of pre-trained models, even
those with a high classification accuracy. We propose an accuracy-preserving
calibration method using the Concrete distribution as the probabilistic model
on the probability simplex. We theoretically prove that a DNN model trained on
cross-entropy loss has optimality as the parameter of the Concrete
distribution. We also propose an efficient method that synthetically generates
samples for training probabilistic models on the probability simplex. We
demonstrate that the proposed method can outperform previous methods in
accuracy-preserving calibration tasks using benchmarks.
\\ ( https://arxiv.org/abs/2402.13765 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13777
Date: Wed, 21 Feb 2024 12:54:48 GMT   (360kb,D)

Title: Deep Generative Models for Offline Policy Learning: Tutorial, Survey,
  and Perspectives on Future Directions
Authors: Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet
  Aggarwal
Categories: cs.LG cs.AI
\\
  Deep generative models (DGMs) have demonstrated great success across various
domains, particularly in generating texts, images, and videos using models
trained from offline data. Similarly, data-driven decision-making and robotic
control also necessitate learning a generator function from the offline data to
serve as the strategy or policy. In this case, applying deep generative models
in offline policy learning exhibits great potential, and numerous studies have
explored in this direction. However, this field still lacks a comprehensive
review and so developments of different branches are relatively independent.
Thus, we provide the first systematic review on the applications of deep
generative models for offline policy learning. In particular, we cover five
mainstream deep generative models, including Variational Auto-Encoders,
Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion
Models, and their applications in both offline reinforcement learning (offline
RL) and imitation learning (IL). Offline RL and IL are two main branches of
offline policy learning and are widely-adopted techniques for sequential
decision-making. Specifically, for each type of DGM-based offline policy
learning, we distill its fundamental scheme, categorize related works based on
the usage of the DGM, and sort out the development process of algorithms in
that field. Subsequent to the main content, we provide in-depth discussions on
deep generative models and offline policy learning as a summary, based on which
we present our perspectives on future research directions. This work offers a
hands-on reference for the research progress in deep generative models for
offline policy learning, and aims to inspire improved DGM-based offline RL or
IL algorithms.
\\ ( https://arxiv.org/abs/2402.13777 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13779
Date: Wed, 21 Feb 2024 12:58:40 GMT   (1502kb,D)

Title: Contextual Molecule Representation Learning from Chemical Reaction
  Knowledge
Authors: Han Tang, Shikun Feng, Bicheng Lin, Yuyan Ni, JIngjing Liu, Wei-Ying
  Ma, Yanyan Lan
Categories: cs.LG cs.AI q-bio.BM
Comments: Preprint. Under Review
\\
  In recent years, self-supervised learning has emerged as a powerful tool to
harness abundant unlabelled data for representation learning and has been
broadly adopted in diverse areas. However, when applied to molecular
representation learning (MRL), prevailing techniques such as masked sub-unit
reconstruction often fall short, due to the high degree of freedom in the
possible combinations of atoms within molecules, which brings insurmountable
complexity to the masking-reconstruction paradigm. To tackle this challenge, we
introduce REMO, a self-supervised learning framework that takes advantage of
well-defined atom-combination rules in common chemistry. Specifically, REMO
pre-trains graph/Transformer encoders on 1.7 million known chemical reactions
in the literature. We propose two pre-training objectives: Masked Reaction
Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO
offers a novel solution to MRL by exploiting the underlying shared patterns in
chemical reactions as \textit{context} for pre-training, which effectively
infers meaningful representations of common chemistry knowledge. Such
contextual representations can then be utilized to support diverse downstream
molecular tasks with minimum finetuning, such as affinity prediction and
drug-drug interaction prediction. Extensive experimental results on
MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type
classification show that across all tested downstream tasks, REMO outperforms
the standard baseline of single-molecule masked modeling used in current MRL.
Remarkably, REMO is the pioneering deep learning model surpassing
fingerprint-based methods in activity cliff benchmarks.
\\ ( https://arxiv.org/abs/2402.13779 ,  1502kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13781
Date: Wed, 21 Feb 2024 13:00:44 GMT   (1425kb,D)

Title: Preserving Near-Optimal Gradient Sparsification Cost for Scalable
  Distributed Deep Learning
Authors: Daegun Yoon, Sangyoon Oh
Categories: cs.LG cs.DC
Comments: 24th IEEE/ACM International Symposium on Cluster, Cloud, and Internet
  Computing (CCGrid 2024). Code: https://github.com/kljp/exdyna
\\
  Communication overhead is a major obstacle to scaling distributed training
systems. Gradient sparsification is a potential optimization approach to reduce
the communication volume without significant loss of model fidelity. However,
existing gradient sparsification methods have low scalability owing to
inefficient design of their algorithms, which raises the communication overhead
significantly. In particular, gradient build-up and inadequate sparsity control
methods degrade the sparsification performance considerably. Moreover,
communication traffic increases drastically owing to workload imbalance of
gradient selection between workers.
  To address these challenges, we propose a novel gradient sparsification
scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises
fined-grained blocks, and contiguous blocks are grouped into non-overlapping
partitions. Each worker selects gradients in its exclusively allocated
partition so that gradient build-up never occurs. To balance the workload of
gradient selection between workers, ExDyna adjusts the topology of partitions
by comparing the workloads of adjacent partitions. In addition, ExDyna supports
online threshold scaling, which estimates the accurate threshold of gradient
selection on-the-fly. Accordingly, ExDyna can satisfy the user-required
sparsity level during a training period regardless of models and datasets.
Therefore, ExDyna can enhance the scalability of distributed training systems
by preserving near-optimal gradient sparsification cost. In experiments, ExDyna
outperformed state-of-the-art sparsifiers in terms of training speed and
sparsification performance while achieving high accuracy.
\\ ( https://arxiv.org/abs/2402.13781 ,  1425kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13791
Date: Wed, 21 Feb 2024 13:19:58 GMT   (1043kb,D)

Title: Opening the Black-Box: A Systematic Review on Explainable AI in Remote
  Sensing
Authors: Adrian H\"ohl, Ivica Obadic, Miguel \'Angel Fern\'andez Torres, Hiba
  Najjar, Dario Oliveira, Zeynep Akata, Andreas Dengel, Xiao Xiang Zhu
Categories: cs.LG
\\
  In recent years, black-box machine learning approaches have become a dominant
modeling paradigm for knowledge extraction in Remote Sensing. Despite the
potential benefits of uncovering the inner workings of these models with
explainable AI, a comprehensive overview summarizing the used explainable AI
methods and their objectives, findings, and challenges in Remote Sensing
applications is still missing. In this paper, we address this issue by
performing a systematic review to identify the key trends of how explainable AI
is used in Remote Sensing and shed light on novel explainable AI approaches and
emerging directions that tackle specific Remote Sensing challenges. We also
reveal the common patterns of explanation interpretation, discuss the extracted
scientific insights in Remote Sensing, and reflect on the approaches used for
explainable AI methods evaluation. Our review provides a complete summary of
the state-of-the-art in the field. Further, we give a detailed outlook on the
challenges and promising research directions, representing a basis for novel
methodological development and a useful starting point for new researchers in
the field of explainable AI in Remote Sensing.
\\ ( https://arxiv.org/abs/2402.13791 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13810
Date: Wed, 21 Feb 2024 13:47:51 GMT   (505kb,D)

Title: The Expected Loss of Preconditioned Langevin Dynamics Reveals the
  Hessian Rank
Authors: Amitay Bar, Rotem Mulayoff, Tomer Michaeli, Ronen Talmon
Categories: cs.LG
Comments: Accepted to AAAI-24 main track
\\
  Langevin dynamics (LD) is widely used for sampling from distributions and for
optimization. In this work, we derive a closed-form expression for the expected
loss of preconditioned LD near stationary points of the objective function. We
use the fact that at the vicinity of such points, LD reduces to an
Ornstein-Uhlenbeck process, which is amenable to convenient mathematical
treatment. Our analysis reveals that when the preconditioning matrix satisfies
a particular relation with respect to the noise covariance, LD's expected loss
becomes proportional to the rank of the objective's Hessian. We illustrate the
applicability of this result in the context of neural networks, where the
Hessian rank has been shown to capture the complexity of the predictor function
but is usually computationally hard to probe. Finally, we use our analysis to
compare SGD-like and Adam-like preconditioners and identify the regimes under
which each of them leads to a lower expected loss.
\\ ( https://arxiv.org/abs/2402.13810 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13812
Date: Wed, 21 Feb 2024 13:50:46 GMT   (2941kb,D)

Title: Voice-Driven Mortality Prediction in Hospitalized Heart Failure
  Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers
Authors: Nihat Ahmadli, Mehmet Ali Sarsil, Berk Mizrak, Kurtulus Karauzum, Ata
  Shaker, Erol Tulumen, Didar Mirzamidinov, Dilek Ural, Onur Ergen
Categories: cs.LG cs.SD eess.AS
Comments: 11 pages, 6 figures, 5 tables. THe first 2 authors have contributed
  equally
\\
  Addressing heart failure (HF) as a prevalent global health concern poses
difficulties in implementing innovative approaches for enhanced patient care.
Predicting mortality rates in HF patients, in particular, is difficult yet
critical, necessitating individualized care, proactive management, and enabling
educated decision-making to enhance outcomes. Recently, the significance of
voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating
remarkable efficacy, particularly in predicting heart failure. The synergy of
voice analysis and ML algorithms provides a non-invasive and easily accessible
means to evaluate patients' health. However, there is a lack of voice
biomarkers for predicting mortality rates among heart failure patients with
standardized speech protocols. Here, we demonstrate a powerful and effective ML
model for predicting mortality rates in hospitalized HF patients through the
utilization of voice biomarkers. By seamlessly integrating voice biomarkers
into routine patient monitoring, this strategy has the potential to improve
patient outcomes, optimize resource allocation, and advance patient-centered HF
management. In this study, a Machine Learning system, specifically a logistic
regression model, is trained to predict patients' 5-year mortality rates using
their speech as input. The model performs admirably and consistently, as
demonstrated by cross-validation and statistical approaches (p-value < 0.001).
Furthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the
model's predictive accuracy substantially.
\\ ( https://arxiv.org/abs/2402.13812 ,  2941kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13820
Date: Wed, 21 Feb 2024 13:59:21 GMT   (14819kb,D)

Title: FLD: Fourier Latent Dynamics for Structured Motion Representation and
  Learning
Authors: Chenhao Li, Elijah Stanger-Jones, Steve Heim, Sangbae Kim
Categories: cs.LG cs.AI cs.RO cs.SY eess.SP eess.SY
\\
  Motion trajectories offer reliable references for physics-based motion
learning but suffer from sparsity, particularly in regions that lack sufficient
data coverage. To address this challenge, we introduce a self-supervised,
structured representation and generation method that extracts spatial-temporal
relationships in periodic or quasi-periodic motions. The motion dynamics in a
continuously parameterized latent space enable our method to enhance the
interpolation and generalization capabilities of motion learning algorithms.
The motion learning controller, informed by the motion parameterization,
operates online tracking of a wide range of motions, including targets unseen
during training. With a fallback mechanism, the controller dynamically adapts
its tracking strategy and automatically resorts to safe action execution when a
potentially risky target is proposed. By leveraging the identified
spatial-temporal structure, our work opens new possibilities for future
advancements in general motion representation and learning algorithms.
\\ ( https://arxiv.org/abs/2402.13820 ,  14819kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13821
Date: Wed, 21 Feb 2024 13:59:47 GMT   (24kb)

Title: Performance Improvement Bounds for Lipschitz Configurable Markov
  Decision Processes
Authors: Alberto Maria Metelli
Categories: cs.LG
\\
  Configurable Markov Decision Processes (Conf-MDPs) have recently been
introduced as an extension of the traditional Markov Decision Processes (MDPs)
to model the real-world scenarios in which there is the possibility to
intervene in the environment in order to configure some of its parameters. In
this paper, we focus on a particular subclass of Conf-MDP that satisfies
regularity conditions, namely Lipschitz continuity. We start by providing a
bound on the Wasserstein distance between $\gamma$-discounted stationary
distributions induced by changing policy and configuration. This result
generalizes the already existing bounds both for Conf-MDPs and traditional
MDPs. Then, we derive a novel performance improvement lower bound.
\\ ( https://arxiv.org/abs/2402.13821 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13831
Date: Wed, 21 Feb 2024 14:22:20 GMT   (525kb,D)

Title: MLXP: A framework for conducting replicable Machine Learning eXperiments
  in Python
Authors: Michael Arbel, Alexandre Zouaoui
Categories: cs.LG cs.SE
\\
  Replicability in machine learning (ML) research is increasingly concerning
due to the utilization of complex non-deterministic algorithms and the
dependence on numerous hyper-parameter choices, such as model architecture and
training datasets. Ensuring reproducible and replicable results is crucial for
advancing the field, yet often requires significant technical effort to conduct
systematic and well-organized experiments that yield robust conclusions.
Several tools have been developed to facilitate experiment management and
enhance reproducibility; however, they often introduce complexity that hinders
adoption within the research community, despite being well-handled in
industrial settings. To address the challenge of low adoption, we propose MLXP,
an open-source, simple, and lightweight experiment management tool based on
Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the
experimental process with minimal practitioner overhead while ensuring a high
level of reproducibility.
\\ ( https://arxiv.org/abs/2402.13831 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13852
Date: Wed, 21 Feb 2024 14:56:36 GMT   (757kb,D)

Title: Neural Control System for Continuous Glucose Monitoring and Maintenance
Authors: Azmine Toushik Wasi
Categories: cs.LG cs.AI cs.NE cs.SY eess.SY stat.ML
Comments: 9 Pages, 4 figures, ICLR 2024 Tiny Papers Track
  https://openreview.net/forum?id=Te4P3Cn54g
Journal-ref: The Second Tiny Papers Track at ICLR 2024
\\
  Precise glucose level management is pivotal for individuals with diabetes,
averting severe complications. In this work, we introduce a novel neural
control system for continuous glucose monitoring and maintenance, utilizing
differential predictive control. Our system, guided by a sophisticated neural
policy and differentiable modeling, dynamically adjusts insulin delivery in
real-time, enhancing glucose optimization. This end-to-end approach maximizes
efficiency, ensuring personalized care and improved health outcomes, as
affirmed by empirical findings.
\\ ( https://arxiv.org/abs/2402.13852 ,  757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13857
Date: Wed, 21 Feb 2024 15:06:51 GMT   (54kb)

Title: Replicable Learning of Large-Margin Halfspaces
Authors: Alkis Kalavasis, Amin Karbasi, Kasper Green Larsen, Grigoris Velegkas,
  Felix Zhou
Categories: cs.LG
\\
  We provide efficient replicable algorithms for the problem of learning
large-margin halfspaces. Our results improve upon the algorithms provided by
Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first
dimension-independent replicable algorithms for this task which runs in
polynomial time, is proper, and has strictly improved sample complexity
compared to the one achieved by Impagliazzo et al. [2022] with respect to all
the relevant parameters. Moreover, our first algorithm has sample complexity
that is optimal with respect to the accuracy parameter $\epsilon$. We also
design an SGD-based replicable algorithm that, in some parameters' regimes,
achieves better sample and time complexity than our first algorithm.
  Departing from the requirement of polynomial time algorithms, using the
DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei,
Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a
replicable algorithm for large-margin halfspaces with improved sample
complexity with respect to the margin parameter $\tau$, but running time doubly
exponential in $1/\tau^2$ and worse sample complexity dependence on $\epsilon$
than one of our previous algorithms. We then design an improved algorithm with
better sample complexity than all three of our previous algorithms and running
time exponential in $1/\tau^{2}$.
\\ ( https://arxiv.org/abs/2402.13857 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13870
Date: Wed, 21 Feb 2024 15:23:21 GMT   (1376kb)

Title: Generative Probabilistic Time Series Forecasting and Applications in
  Grid Operations
Authors: Xinyi Wang, Lang Tong, Qing Zhao
Categories: cs.LG eess.SP stat.AP
Comments: Accepted at CISS 2024. arXiv admin note: text overlap with
  arXiv:2306.03782
\\
  Generative probabilistic forecasting produces future time series samples
according to the conditional probability distribution given past time series
observations. Such techniques are essential in risk-based decision-making and
planning under uncertainty with broad applications in grid operations,
including electricity price forecasting, risk-based economic dispatch, and
stochastic optimizations. Inspired by Wiener and Kallianpur's innovation
representation, we propose a weak innovation autoencoder architecture and a
learning algorithm to extract independent and identically distributed
innovation sequences from nonparametric stationary time series. We show that
the weak innovation sequence is Bayesian sufficient, which makes the proposed
weak innovation autoencoder a canonical architecture for generative
probabilistic forecasting. The proposed technique is applied to forecasting
highly volatile real-time electricity prices, demonstrating superior
performance across multiple forecasting measures over leading probabilistic and
point forecasting techniques.
\\ ( https://arxiv.org/abs/2402.13870 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13871
Date: Wed, 21 Feb 2024 15:23:21 GMT   (1949kb,D)

Title: An Explainable Transformer-based Model for Phishing Email Detection: A
  Large Language Model Approach
Authors: Mohammad Amaz Uddin and Iqbal H. Sarker
Categories: cs.LG cs.AI cs.CR
\\
  Phishing email is a serious cyber threat that tries to deceive users by
sending false emails with the intention of stealing confidential information or
causing financial harm. Attackers, often posing as trustworthy entities,
exploit technological advancements and sophistication to make detection and
prevention of phishing more challenging. Despite extensive academic research,
phishing detection remains an ongoing and formidable challenge in the
cybersecurity landscape. Large Language Models (LLMs) and Masked Language
Models (MLMs) possess immense potential to offer innovative solutions to
address long-standing challenges. In this research paper, we present an
optimized, fine-tuned transformer-based DistilBERT model designed for the
detection of phishing emails. In the detection process, we work with a phishing
email dataset and utilize the preprocessing techniques to clean and solve the
imbalance class issues. Through our experiments, we found that our model
effectively achieves high accuracy, demonstrating its capability to perform
well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)
techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and
Transformer Interpret to explain how our model makes predictions in the context
of text classification for phishing emails.
\\ ( https://arxiv.org/abs/2402.13871 ,  1949kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13891
Date: Wed, 21 Feb 2024 16:02:14 GMT   (84kb,D)

Title: Overcoming Saturation in Density Ratio Estimation by Iterated
  Regularization
Authors: Lukas Gruber, Markus Holzleitner, Johannes Lehner, Sepp Hochreiter,
  Werner Zellinger
Categories: cs.LG stat.ML
\\
  Estimating the ratio of two probability densities from finitely many samples,
is a central task in machine learning and statistics. In this work, we show
that a large class of kernel methods for density ratio estimation suffers from
error saturation, which prevents algorithms from achieving fast error
convergence rates on highly regular learning problems. To resolve saturation,
we introduce iterated regularization in density ratio estimation to achieve
fast error rates. Our methods outperform its non-iteratively regularized
versions on benchmarks for density ratio estimation as well as on large-scale
evaluations for importance-weighted ensembling of deep unsupervised domain
adaptation models.
\\ ( https://arxiv.org/abs/2402.13891 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13901
Date: Wed, 21 Feb 2024 16:11:47 GMT   (114kb,D)

Title: Non-asymptotic Convergence of Discrete-time Diffusion Models: New
  Approach and Improved Rate
Authors: Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness Shroff
Categories: cs.LG eess.SP stat.ML
\\
  The denoising diffusion model emerges recently as a powerful generative
technique that converts noise into data. Theoretical convergence guarantee has
been mainly studied for continuous-time diffusion models, and has been obtained
for discrete-time diffusion models only for distributions with bounded support
in the literature. In this paper, we establish the convergence guarantee for
substantially larger classes of distributions under discrete-time diffusion
models and further improve the convergence rate for distributions with bounded
support. In particular, we first establish the convergence rates for both
smooth and general (possibly non-smooth) distributions having finite second
moment. We then specialize our results to a number of interesting classes of
distributions with explicit parameter dependencies, including distributions
with Lipschitz scores, Gaussian mixture distributions, and distributions with
bounded support. We further propose a novel accelerated sampler and show that
it improves the convergence rates of the corresponding regular sampler by
orders of magnitude with respect to all system parameters. For distributions
with bounded support, our result improves the dimensional dependence of the
previous convergence rate by orders of magnitude. Our study features a novel
analysis technique that constructs tilting factor representation of the
convergence error and exploits Tweedie's formula for handling Taylor expansion
power terms.
\\ ( https://arxiv.org/abs/2402.13901 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13903
Date: Wed, 21 Feb 2024 16:13:49 GMT   (73kb)

Title: Dealing with unbounded gradients in stochastic saddle-point optimization
Authors: Gergely Neu, Nneka Okolo
Categories: cs.LG math.OC stat.ML
Comments: 23 pages
\\
  We study the performance of stochastic first-order methods for finding saddle
points of convex-concave functions. A notorious challenge faced by such methods
is that the gradients can grow arbitrarily large during optimization, which may
result in instability and divergence. In this paper, we propose a simple and
effective regularization technique that stabilizes the iterates and yields
meaningful performance guarantees even if the domain and the gradient noise
scales linearly with the size of the iterates (and is thus potentially
unbounded). Besides providing a set of general results, we also apply our
algorithm to a specific problem in reinforcement learning, where it leads to
performance guarantees for finding near-optimal policies in an average-reward
MDP without prior knowledge of the bias span.
\\ ( https://arxiv.org/abs/2402.13903 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13911
Date: Wed, 21 Feb 2024 16:26:59 GMT   (597kb)

Title: Replication Study: Enhancing Hydrological Modeling with Physics-Guided
  Machine Learning
Authors: Mostafa Esmaeilzadeh, Melika Amirzadeh
Categories: cs.LG
\\
  Current hydrological modeling methods combine data-driven Machine Learning
(ML) algorithms and traditional physics-based models to address their
respective limitations incorrect parameter estimates from rigid physics-based
models and the neglect of physical process constraints by ML algorithms.
Despite the accuracy of ML in outcome prediction, the integration of scientific
knowledge is crucial for reliable predictions. This study introduces a Physics
Informed Machine Learning (PIML) model, which merges the process understanding
of conceptual hydrological models with the predictive efficiency of ML
algorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates
superior performance in forecasting monthly streamflow and actual
evapotranspiration over both standalone conceptual models and ML algorithms,
ensuring physical consistency of the outputs. This study replicates the
methodologies of Bhasme, P., Vagadiya, J., & Bhatia, U. (2022) from their
pivotal work on Physics Informed Machine Learning for hydrological processes,
utilizing their shared code and datasets to further explore the predictive
capabilities in hydrological modeling.
\\ ( https://arxiv.org/abs/2402.13911 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13916
Date: Wed, 21 Feb 2024 16:31:45 GMT   (1670kb,D)

Title: Bias correction of wind power forecasts with SCADA data and continuous
  learning
Authors: Stefan Jonas, Kevin Winter, Bernhard Brodbeck, Angela Meyer
Categories: cs.LG
Comments: 20 pages
\\
  Wind energy plays a critical role in the transition towards renewable energy
sources. However, the uncertainty and variability of wind can impede its full
potential and the necessary growth of wind power capacity. To mitigate these
challenges, wind power forecasting methods are employed for applications in
power management, energy trading, or maintenance scheduling. In this work, we
present, evaluate, and compare four machine learning-based wind power
forecasting models. Our models correct and improve 48-hour forecasts extracted
from a numerical weather prediction (NWP) model. The models are evaluated on
datasets from a wind park comprising 65 wind turbines. The best improvement in
forecasting error and mean bias was achieved by a convolutional neural network,
reducing the average NRMSE down to 22%, coupled with a significant reduction in
mean bias, compared to a NRMSE of 35% from the strongly biased baseline model
using uncorrected NWP forecasts. Our findings further indicate that changes to
neural network architectures play a minor role in affecting the forecasting
performance, and that future research should rather investigate changes in the
model pipeline. Moreover, we introduce a continuous learning strategy, which is
shown to achieve the highest forecasting performance improvements when new data
is made available.
\\ ( https://arxiv.org/abs/2402.13916 ,  1670kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13930
Date: Wed, 21 Feb 2024 16:52:26 GMT   (4759kb,D)

Title: Enhancing Reinforcement Learning Agents with Local Guides
Authors: Paul Daoudi, Bogdan Robu, Christophe Prieur, Ludovic Dos Santos and
  Merwan Barlier
Categories: cs.LG cs.SY eess.SY
\\
  This paper addresses the problem of integrating local guide policies into a
Reinforcement Learning agent. For this, we show how to adapt existing
algorithms to this setting before introducing a novel algorithm based on a
noisy policy-switching procedure. This approach builds on a proper Approximate
Policy Evaluation (APE) scheme to provide a perturbation that carefully leads
the local guides towards better actions. We evaluated our method on a set of
classical Reinforcement Learning problems, including safety-critical systems
where the agent cannot enter some areas at the risk of triggering catastrophic
consequences. In all the proposed environments, our agent proved to be
efficient at leveraging those policies to improve the performance of any
APE-based Reinforcement Learning algorithm, especially in its first learning
stages.
\\ ( https://arxiv.org/abs/2402.13930 ,  4759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13934
Date: Wed, 21 Feb 2024 17:00:56 GMT   (88kb,D)

Title: Do Efficient Transformers Really Save Computation?
Authors: Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen
  Feng, Qiwei Ye, Di He, Liwei Wang
Categories: cs.LG cs.AI cs.CL stat.ML
\\
  As transformer-based language models are trained on increasingly large
datasets and with vast numbers of parameters, finding more efficient
alternatives to the standard Transformer has become very valuable. While many
efficient Transformers and Transformer alternatives have been proposed, none
provide theoretical guarantees that they are a suitable replacement for the
standard Transformer. This makes it challenging to identify when to use a
specific model and what directions to prioritize for further investigation. In
this paper, we aim to understand the capabilities and limitations of efficient
Transformers, specifically the Sparse Transformer and the Linear Transformer.
We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT)
prompts and follow previous works to model them as Dynamic Programming (DP)
problems. Our results show that while these models are expressive enough to
solve general DP tasks, contrary to expectations, they require a model size
that scales with the problem size. Nonetheless, we identify a class of DP
problems for which these models can be more efficient than the standard
Transformer. We confirm our theoretical results through experiments on
representative DP tasks, adding to the understanding of efficient Transformers'
practical strengths and weaknesses.
\\ ( https://arxiv.org/abs/2402.13934 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13946
Date: Wed, 21 Feb 2024 17:18:25 GMT   (550kb,D)

Title: AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement
  Learning
Authors: Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran
Categories: cs.LG cs.CR
\\
  Machine learning has shown great promise in addressing several critical
hardware security problems. In particular, researchers have developed novel
graph neural network (GNN)-based techniques for detecting intellectual property
(IP) piracy, detecting hardware Trojans (HTs), and reverse engineering
circuits, to name a few. These techniques have demonstrated outstanding
accuracy and have received much attention in the community. However, since
these techniques are used for security applications, it is imperative to
evaluate them thoroughly and ensure they are robust and do not compromise the
security of integrated circuits.
  In this work, we propose AttackGNN, the first red-team attack on GNN-based
techniques in hardware security. To this end, we devise a novel reinforcement
learning (RL) agent that generates adversarial examples, i.e., circuits,
against the GNN-based techniques. We overcome three challenges related to
effectiveness, scalability, and generality to devise a potent RL agent. We
target five GNN-based techniques for four crucial classes of problems in
hardware security: IP piracy, detecting/localizing HTs, reverse engineering,
and hardware obfuscation. Through our approach, we craft circuits that fool all
GNNs considered in this work. For instance, to evade IP piracy detection, we
generate adversarial pirated circuits that fool the GNN-based defense into
classifying our crafted circuits as not pirated. For attacking HT localization
GNN, our attack generates HT-infested circuits that fool the defense on all
tested circuits. We obtain a similar 100% success rate against GNNs for all
classes of problems.
\\ ( https://arxiv.org/abs/2402.13946 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13979
Date: Wed, 21 Feb 2024 18:09:04 GMT   (36160kb,D)

Title: The Importance of Architecture Choice in Deep Learning for Climate
  Applications
Authors: Simon Dr\"ager and Maike Sonnewald
Categories: cs.LG cs.AI
\\
  Machine Learning has become a pervasive tool in climate science applications.
However, current models fail to address nonstationarity induced by
anthropogenic alterations in greenhouse emissions and do not routinely quantify
the uncertainty of proposed projections. In this paper, we model the Atlantic
Meridional Overturning Circulation (AMOC) which is of major importance to
climate in Europe and the US East Coast by transporting warm water to these
regions, and has the potential for abrupt collapse. We can generate arbitrarily
extreme climate scenarios through arbitrary time scales which we then predict
using neural networks. Our analysis shows that the AMOC is predictable using
neural networks under a diverse set of climate scenarios. Further experiments
reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead
of imitating its progression through autocorrelation. With quantified
uncertainty, an intriguing pattern of "spikes" before critical points of
collapse in the AMOC casts doubt on previous analyses that predicted an AMOC
collapse within this century. Our results show that Bayesian Neural Networks
perform poorly compared to more dense architectures and care should be taken
when applying neural networks to nonstationary scenarios such as climate
projections. Further, our results highlight that big NN models might have
difficulty in modeling global Earth System dynamics accurately and be
successfully applied in nonstationary climate scenarios due to the physics
being challenging for neural networks to capture.
\\ ( https://arxiv.org/abs/2402.13979 ,  36160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13984
Date: Wed, 21 Feb 2024 18:12:07 GMT   (41617kb,D)

Title: Stability-Aware Training of Neural Network Interatomic Potentials with
  Differentiable Boltzmann Estimators
Authors: Sanjeev Raja, Ishan Amin, Fabian Pedregosa, Aditi S. Krishnapriyan
Categories: cs.LG cond-mat.dis-nn cond-mat.mtrl-sci physics.chem-ph physics.comp-ph
\\
  Neural network interatomic potentials (NNIPs) are an attractive alternative
to ab-initio methods for molecular dynamics (MD) simulations. However, they can
produce unstable simulations which sample unphysical states, limiting their
usefulness for modeling phenomena occurring over longer timescales. To address
these challenges, we present Stability-Aware Boltzmann Estimator (StABlE)
Training, a multi-modal training procedure which combines conventional
supervised training from quantum-mechanical energies and forces with reference
system observables, to produce stable and accurate NNIPs. StABlE Training
iteratively runs MD simulations to seek out unstable regions, and corrects the
instabilities via supervision with a reference observable. The training
procedure is enabled by the Boltzmann Estimator, which allows efficient
computation of gradients required to train neural networks to system
observables, and can detect both global and local instabilities. We demonstrate
our methodology across organic molecules, tetrapeptides, and condensed phase
systems, along with using three modern NNIP architectures. In all three cases,
StABlE-trained models achieve significant improvements in simulation stability
and recovery of structural and dynamic observables. In some cases,
StABlE-trained models outperform conventional models trained on datasets 50
times larger. As a general framework applicable across NNIP architectures and
systems, StABlE Training is a powerful tool for training stable and accurate
NNIPs, particularly in the absence of large reference datasets.
\\ ( https://arxiv.org/abs/2402.13984 ,  41617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13987
Date: Wed, 21 Feb 2024 18:16:48 GMT   (159kb,D)

Title: A Simple and Yet Fairly Effective Defense for Graph Neural Networks
Authors: Sofiane Ennadir, Yassine Abbahaddou, Johannes F. Lutzeyer, Michalis
  Vazirgiannis, Henrik Bostr\"om
Categories: cs.LG
Comments: Accepted at AAAI-24
\\
  Graph Neural Networks (GNNs) have emerged as the dominant approach for
machine learning on graph-structured data. However, concerns have arisen
regarding the vulnerability of GNNs to small adversarial perturbations.
Existing defense methods against such perturbations suffer from high time
complexity and can negatively impact the model's performance on clean graphs.
To address these challenges, this paper introduces NoisyGNNs, a novel defense
method that incorporates noise into the underlying model's architecture. We
establish a theoretical connection between noise injection and the enhancement
of GNN robustness, highlighting the effectiveness of our approach. We further
conduct extensive empirical evaluations on the node classification task to
validate our theoretical findings, focusing on two popular GNNs: the GCN and
GIN. The results demonstrate that NoisyGNN achieves superior or comparable
defense performance to existing methods while minimizing added time complexity.
The NoisyGNN approach is model-agnostic, allowing it to be integrated with
different GNN architectures. Successful combinations of our NoisyGNN approach
with existing defense techniques demonstrate even further improved adversarial
defense results. Our code is publicly available at:
https://github.com/Sennadir/NoisyGNN.
\\ ( https://arxiv.org/abs/2402.13987 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13989
Date: Wed, 21 Feb 2024 18:19:20 GMT   (104kb,D)

Title: FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning
Authors: Yongcun Song, Ziqi Wang, Enrique Zuazua
Categories: cs.LG cs.CR cs.DC math.OC
\\
  Federated learning (FL) is a promising framework for learning from
distributed data while maintaining privacy. The development of efficient FL
algorithms encounters various challenges, including heterogeneous data and
systems, limited communication capacities, and constrained local computational
resources. Recently developed FedADMM methods show great resilience to both
data and system heterogeneity. However, they still suffer from performance
deterioration if the hyperparameters are not carefully tuned. To address this
issue, we propose an inexact and self-adaptive FedADMM algorithm, termed
FedADMM-InSa. First, we design an inexactness criterion for the clients' local
updates to eliminate the need for empirically setting the local training
accuracy. This inexactness criterion can be assessed by each client
independently based on its unique condition, thereby reducing the local
computational cost and mitigating the undesirable straggle effect. The
convergence of the resulting inexact ADMM is proved under the assumption of
strongly convex loss functions. Additionally, we present a self-adaptive scheme
that dynamically adjusts each client's penalty parameter, enhancing algorithm
robustness by mitigating the need for empirical penalty parameter choices for
each client. Extensive numerical experiments on both synthetic and real-world
datasets are conducted. As validated by some numerical tests, our proposed
algorithm can reduce the clients' local computational load significantly and
also accelerate the learning process compared to the vanilla FedADMM.
\\ ( https://arxiv.org/abs/2402.13989 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14009
Date: Wed, 21 Feb 2024 18:50:12 GMT   (8639kb,D)

Title: Geometry-Informed Neural Networks
Authors: Arturs Berzins, Andreas Radler, Sebastian Sanokowski, Sepp Hochreiter,
  Johannes Brandstetter
Categories: cs.LG cs.CV
\\
  We introduce the concept of geometry-informed neural networks (GINNs), which
encompass (i) learning under geometric constraints, (ii) neural fields as a
suitable representation, and (iii) generating diverse solutions to
under-determined systems often encountered in geometric tasks. Notably, the
GINN formulation does not require training data, and as such can be considered
generative modeling driven purely by constraints. We add an explicit diversity
loss to mitigate mode collapse. We consider several constraints, in particular,
the connectedness of components which we convert to a differentiable loss
through Morse theory. Experimentally, we demonstrate the efficacy of the GINN
learning paradigm across a range of two and three-dimensional scenarios with
increasing levels of complexity.
\\ ( https://arxiv.org/abs/2402.14009 ,  8639kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14013
Date: Wed, 21 Feb 2024 18:52:20 GMT   (45kb)

Title: Misalignment, Learning, and Ranking: Harnessing Users Limited Attention
Authors: Arpit Agarwal, Rad Niazadeh, Prathamesh Patil
Categories: cs.LG cs.DS
\\
  In digital health and EdTech, recommendation systems face a significant
challenge: users often choose impulsively, in ways that conflict with the
platform's long-term payoffs. This misalignment makes it difficult to
effectively learn to rank items, as it may hinder exploration of items with
greater long-term payoffs. Our paper tackles this issue by utilizing users'
limited attention spans. We propose a model where a platform presents items
with unknown payoffs to the platform in a ranked list to $T$ users over time.
Each user selects an item by first considering a prefix window of these ranked
items and then picking the highest preferred item in that window (and the
platform observes its payoff for this item). We study the design of online
bandit algorithms that obtain vanishing regret against hindsight optimal
benchmarks.
  We first consider adversarial window sizes and stochastic iid payoffs. We
design an active-elimination-based algorithm that achieves an optimal
instance-dependent regret bound of $O(\log(T))$, by showing matching regret
upper and lower bounds. The key idea is using the combinatorial structure of
the problem to either obtain a large payoff from each item or to explore by
getting a sample from that item. This method systematically narrows down the
item choices to enhance learning efficiency and payoff.
  Second, we consider adversarial payoffs and stochastic iid window sizes. We
start from the full-information problem of finding the permutation that
maximizes the expected payoff. By a novel combinatorial argument, we
characterize the polytope of admissible item selection probabilities by a
permutation and show it has a polynomial-size representation. Using this
representation, we show how standard algorithms for adversarial online linear
optimization in the space of admissible probabilities can be used to obtain a
polynomial-time algorithm with $O(\sqrt{T})$ regret.
\\ ( https://arxiv.org/abs/2402.14013 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14015
Date: Wed, 21 Feb 2024 18:54:37 GMT   (2720kb,D)

Title: Corrective Machine Unlearning
Authors: Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru,
  Amartya Sanyal
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: 17 pages, 7 figures
\\
  Machine Learning models increasingly face data integrity challenges due to
the use of large-scale training datasets drawn from the internet. We study what
model developers can do if they detect that some data was manipulated or
incorrect. Such manipulated data can cause adverse effects like vulnerability
to backdoored samples, systematic biases, and in general, reduced accuracy on
certain input domains. Often, all manipulated training samples are not known,
and only a small, representative subset of the affected data is flagged.
  We formalize "Corrective Machine Unlearning" as the problem of mitigating the
impact of data affected by unknown manipulations on a trained model, possibly
knowing only a subset of impacted samples. We demonstrate that the problem of
corrective unlearning has significantly different requirements from traditional
privacy-oriented unlearning. We find most existing unlearning methods,
including the gold-standard retraining-from-scratch, require most of the
manipulated data to be identified for effective corrective unlearning. However,
one approach, SSD, achieves limited success in unlearning adverse effects with
just a small portion of the manipulated samples, showing the tractability of
this setting. We hope our work spurs research towards developing better methods
for corrective unlearning and offers practitioners a new strategy to handle
data integrity challenges arising from web-scale training.
\\ ( https://arxiv.org/abs/2402.14015 ,  2720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14017
Date: Wed, 21 Feb 2024 18:56:03 GMT   (35710kb,D)

Title: D-Flow: Differentiating through Flows for Controlled Generation
Authors: Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, Yaron
  Lipman
Categories: cs.LG
\\
  Taming the generation outcome of state of the art Diffusion and Flow-Matching
(FM) models without having to re-train a task-specific model unlocks a powerful
tool for solving inverse problems, conditional generation, and controlled
generation in general. In this work we introduce D-Flow, a simple framework for
controlling the generation process by differentiating through the flow,
optimizing for the source (noise) point. We motivate this framework by our key
observation stating that for Diffusion/FM models trained with Gaussian
probability paths, differentiating through the generation process projects
gradient on the data manifold, implicitly injecting the prior into the
optimization process. We validate our framework on linear and non-linear
controlled generation problems including: image and audio inverse problems and
conditional molecule generation reaching state of the art performance across
all.
\\ ( https://arxiv.org/abs/2402.14017 ,  35710kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14020
Date: Wed, 21 Feb 2024 18:59:13 GMT   (3067kb,D)

Title: Coercing LLMs to do and reveal (almost) anything
Authors: Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen and
  Tom Goldstein
Categories: cs.LG cs.CL cs.CR
Comments: 32 pages. Implementation available at
  https://github.com/JonasGeiping/carving
\\
  It has recently been shown that adversarial attacks on large language models
(LLMs) can "jailbreak" the model into making harmful statements. In this work,
we argue that the spectrum of adversarial attacks on LLMs is much larger than
merely jailbreaking. We provide a broad overview of possible attack surfaces
and attack goals. Based on a series of concrete examples, we discuss,
categorize and systematize attacks that coerce varied unintended behaviors,
such as misdirection, model control, denial-of-service, or data extraction.
  We analyze these attacks in controlled experiments, and find that many of
them stem from the practice of pre-training LLMs with coding capabilities, as
well as the continued existence of strange "glitch" tokens in common LLM
vocabularies that should be removed for security reasons.
\\ ( https://arxiv.org/abs/2402.14020 ,  3067kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.13270 (*cross-listing*)
Date: Fri, 16 Feb 2024 15:26:33 GMT   (2288kb,D)

Title: Global Tropical Cyclone Intensity Forecasting with Multi-modal
  Multi-scale Causal Autoregressive Model
Authors: Xinyu Wang, Kang Chen, Lei Liu, Tao Han, Bin Li, Lei Bai
Categories: physics.ao-ph cs.AI cs.LG physics.data-an
\\
  Accurate forecasting of Tropical cyclone (TC) intensity is crucial for
formulating disaster risk reduction strategies. Current methods predominantly
rely on limited spatiotemporal information from ERA5 data and neglect the
causal relationships between these physical variables, failing to fully capture
the spatial and temporal patterns required for intensity forecasting. To
address this issue, we propose a Multi-modal multi-Scale Causal AutoRegressive
model (MSCAR), which is the first model that combines causal relationships with
large-scale multi-modal data for global TC intensity autoregressive
forecasting. Furthermore, given the current absence of a TC dataset that offers
a wide range of spatial variables, we present the Satellite and ERA5-based
Tropical Cyclone Dataset (SETCD), which stands as the longest and most
comprehensive global dataset related to TCs. Experiments on the dataset show
that MSCAR outperforms the state-of-the-art methods, achieving maximum
reductions in global and regional forecast errors of 9.52% and 6.74%,
respectively. The code and dataset are publicly available at
https://anonymous.4open.science/r/MSCAR.
\\ ( https://arxiv.org/abs/2402.13270 ,  2288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13275 (*cross-listing*)
Date: Sat, 17 Feb 2024 08:08:36 GMT   (129kb,D)

Title: Implementation of a Model of the Cortex Basal Ganglia Loop
Authors: Naoya Arakawa
Categories: q-bio.NC cs.AI
Comments: 7 pages, 3 figures
\\
  This article presents a simple model of the cortex-basal ganglia-thalamus
loop, which is thought to serve for action selection and executions, and
reports the results of its implementation. The model is based on the hypothesis
that the cerebral cortex predicts actions, while the basal ganglia use
reinforcement learning to decide whether to perform the actions predicted by
the cortex. The implementation is intended to be used as a component of models
of the brain consisting of cortical regions or brain-inspired cognitive
architectures.
\\ ( https://arxiv.org/abs/2402.13275 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13276 (*cross-listing*)
Date: Sat, 17 Feb 2024 09:39:46 GMT   (2037kb,D)

Title: When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate
  Speech into Large Language Models for Depression Detection
Authors: Xiangyu Zhang, Hexin Liu, Kaishuai Xu, Qiquan Zhang, Daijiao Liu,
  Beena Ahmed, Julien Epps
Categories: eess.AS cs.AI cs.SD
\\
  Depression is a critical concern in global mental health, prompting extensive
research into AI-based detection methods. Among various AI technologies, Large
Language Models (LLMs) stand out for their versatility in mental healthcare
applications. However, their primary limitation arises from their exclusive
dependence on textual input, which constrains their overall capabilities.
Furthermore, the utilization of LLMs in identifying and analyzing depressive
states is still relatively untapped. In this paper, we present an innovative
approach to integrating acoustic speech information into the LLMs framework for
multimodal depression detection. We investigate an efficient method for
depression detection by integrating speech signals into LLMs utilizing Acoustic
Landmarks. By incorporating acoustic landmarks, which are specific to the
pronunciation of spoken words, our method adds critical dimensions to text
transcripts. This integration also provides insights into the unique speech
patterns of individuals, revealing the potential mental states of individuals.
Evaluations of the proposed approach on the DAIC-WOZ dataset reveal
state-of-the-art results when compared with existing Audio-Text baselines. In
addition, this approach is not only valuable for the detection of depression
but also represents a new perspective in enhancing the ability of LLMs to
comprehend and process speech signals.
\\ ( https://arxiv.org/abs/2402.13276 ,  2037kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13284 (*cross-listing*)
Date: Mon, 19 Feb 2024 09:07:59 GMT   (1089kb,D)

Title: Structure Guided Large Language Model for SQL Generation
Authors: Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao
  Huang
Categories: cs.DB cs.AI cs.CL
\\
  Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
\\ ( https://arxiv.org/abs/2402.13284 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13287 (*cross-listing*)
Date: Mon, 19 Feb 2024 12:22:22 GMT   (5166kb,D)

Title: Manipulating hidden-Markov-model inferences by corrupting batch data
Authors: William N. Caballero, Jose Manuel Camacho, Tahir Ekin, Roi Naveiro
Categories: cs.CR cs.AI cs.LG
Comments: 42 pages, 8 figures, 11 tables
Journal-ref: Caballero, W. N., Camacho, J. M., Ekin, T., & Naveiro, R. (2024).
  Manipulating hidden-Markov-model inferences by corrupting batch data.
  Computers & Operations Research, 162, 106478
DOI: 10.1016/j.cor.2023.106478
\\
  Time-series models typically assume untainted and legitimate streams of data.
However, a self-interested adversary may have incentive to corrupt this data,
thereby altering a decision maker's inference. Within the broader field of
adversarial machine learning, this research provides a novel, probabilistic
perspective toward the manipulation of hidden Markov model inferences via
corrupted data. In particular, we provision a suite of corruption problems for
filtering, smoothing, and decoding inferences leveraging an adversarial risk
analysis approach. Multiple stochastic programming models are set forth that
incorporate realistic uncertainties and varied attacker objectives. Three
general solution methods are developed by alternatively viewing the problem
from frequentist and Bayesian perspectives. The efficacy of each method is
illustrated via extensive, empirical testing. The developed methods are
characterized by their solution quality and computational effort, resulting in
a stratification of techniques across varying problem-instance architectures.
This research highlights the weaknesses of hidden Markov models under
adversarial activity, thereby motivating the need for robustification
techniques to ensure their security.
\\ ( https://arxiv.org/abs/2402.13287 ,  5166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13288 (*cross-listing*)
Date: Mon, 19 Feb 2024 13:56:16 GMT   (938kb,D)

Title: Training Table Question Answering via SQL Query Decomposition
Authors: Rapha\"el Mouravieff and Benjamin Piwowarski and Sylvain Lamprier
Categories: cs.DB cs.AI
\\
  Table Question-Answering involves both understanding the natural language
query and grounding it in the context of the input table to extract the
relevant information. In this context, many methods have highlighted the
benefits of intermediate pre-training from SQL queries. However, while most
approaches aim at generating final answers from inputs directly, we claim that
there is better to do with SQL queries during training. By learning to imitate
a restricted portion of SQL-like algebraic operations, we show that their
execution flow provides intermediate supervision steps that allow increased
generalization and structural reasoning compared with classical approaches of
the field. Our study bridges the gap between semantic parsing and direct
answering methods and provides useful insights regarding what types of
operations should be predicted by a generative architecture or be preferably
executed by an external algorithm.
\\ ( https://arxiv.org/abs/2402.13288 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13292 (*cross-listing*)
Date: Mon, 19 Feb 2024 19:04:19 GMT   (576kb,D)

Title: A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot
  Systems
Authors: Aakash and Indranil Saha
Categories: cs.MA cs.AI cs.RO
\\
  The fundamental goal assignment problem for a multi-robot application aims to
assign a unique goal to each robot while ensuring collision-free paths,
minimizing the total movement cost. A plausible algorithmic solution to this
NP-hard problem involves an iterative process that integrates a task planner to
compute the goal assignment while ignoring the collision possibilities among
the robots and a multi-agent path-finding algorithm to find the collision-free
trajectories for a given assignment. This procedure involves a method for
computing the next best assignment given the current best assignment. A naive
way of computing the next best assignment, as done in the state-of-the-art
solutions, becomes a roadblock to achieving scalability in solving the overall
problem. To obviate this bottleneck, we propose an efficient conflict-guided
method to compute the next best assignment. Additionally, we introduce two more
optimizations to the algorithm -- first for avoiding the unconstrained path
computations between robot-goal pairs wherever possible, and the second to
prevent duplicate constrained path computations for multiple robot-goal pairs.
We extensively evaluate our algorithm for up to a hundred robots on several
benchmark workspaces. The results demonstrate that the proposed algorithm
achieves nearly an order of magnitude speedup over the state-of-the-art
algorithm, showcasing its efficacy in real-world scenarios.
\\ ( https://arxiv.org/abs/2402.13292 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13297 (*cross-listing*)
Date: Tue, 20 Feb 2024 05:41:46 GMT   (3296kb,D)

Title: Integrating Deep Learning and Synthetic Biology: A Co-Design Approach
  for Enhancing Gene Expression via N-terminal Coding Sequences
Authors: Zhanglu Yan, Weiran Chu, Yuhua Sheng, Kaiwen Tang, Shida Wang, Yanfeng
  Liu, Weng-Fai Wong
Categories: q-bio.QM cs.AI
\\
  N-terminal coding sequence (NCS) influences gene expression by impacting the
translation initiation rate. The NCS optimization problem is to find an NCS
that maximizes gene expression. The problem is important in genetic
engineering. However, current methods for NCS optimization such as rational
design and statistics-guided approaches are labor-intensive yield only
relatively small improvements. This paper introduces a deep learning/synthetic
biology co-designed few-shot training workflow for NCS optimization. Our method
utilizes k-nearest encoding followed by word2vec to encode the NCS, then
performs feature extraction using attention mechanisms, before constructing a
time-series network for predicting gene expression intensity, and finally a
direct search algorithm identifies the optimal NCS with limited training data.
We took green fluorescent protein (GFP) expressed by Bacillus subtilis as a
reporting protein of NCSs, and employed the fluorescence enhancement factor as
the metric of NCS optimization. Within just six iterative experiments, our
model generated an NCS (MLD62) that increased average GFP expression by
5.41-fold, outperforming the state-of-the-art NCS designs. Extending our
findings beyond GFP, we showed that our engineered NCS (MLD62) can effectively
boost the production of N-acetylneuraminic acid by enhancing the expression of
the crucial rate-limiting GNA1 gene, demonstrating its practical utility. We
have open-sourced our NCS expression database and experimental procedures for
public use.
\\ ( https://arxiv.org/abs/2402.13297 ,  3296kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13301 (*cross-listing*)
Date: Tue, 20 Feb 2024 13:41:35 GMT   (947kb,D)

Title: Structure-informed Positional Encoding for Music Generation
Authors: Manvi Agarwal (S2A_IDS), Changhong Wang (S2A_IDS), Ga\"el Richard
  (S2A_IDS)
Categories: cs.SD cs.AI eess.AS
Journal-ref: IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), 2024
\\
  Music generated by deep learning methods often suffers from a lack of
coherence and long-term organization. Yet, multi-scale hierarchical structure
is a distinctive feature of music signals. To leverage this information, we
propose a structure-informed positional encoding framework for music generation
with Transformers. We design three variants in terms of absolute, relative and
non-stationary positional information. We comprehensively test them on two
symbolic music generation tasks: next-timestep prediction and accompaniment
generation. As a comparison, we choose multiple baselines from the literature
and demonstrate the merits of our methods using several musically-motivated
evaluation metrics. In particular, our methods improve the melodic and
structural consistency of the generated pieces.
\\ ( https://arxiv.org/abs/2402.13301 ,  947kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13326 (*cross-listing*)
Date: Tue, 20 Feb 2024 19:08:24 GMT   (4247kb,D)

Title: Deep Hedging with Market Impact
Authors: Andrei Neagu and Fr\'ed\'eric Godin and Clarence Simard and Leila
  Kosseim
Categories: q-fin.CP cs.AI
Comments: 13 pages, 5 figures
\\
  Dynamic hedging is the practice of periodically transacting financial
instruments to offset the risk caused by an investment or a liability. Dynamic
hedging optimization can be framed as a sequential decision problem; thus,
Reinforcement Learning (RL) models were recently proposed to tackle this task.
However, existing RL works for hedging do not consider market impact caused by
the finite liquidity of traded instruments. Integrating such feature can be
crucial to achieve optimal performance when hedging options on stocks with
limited liquidity. In this paper, we propose a novel general market impact
dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers
several realistic features such as convex market impacts, and impact
persistence through time. The optimal policy obtained from the DRL model is
analysed using several option hedging simulations and compared to commonly used
procedures such as delta hedging. Results show our DRL model behaves better in
contexts of low liquidity by, among others: 1) learning the extent to which
portfolio rebalancing actions should be dampened or delayed to avoid high
costs, 2) factoring in the impact of features not considered by conventional
approaches, such as previous hedging errors through the portfolio value, and
the underlying asset's drift (i.e. the magnitude of its expected return).
\\ ( https://arxiv.org/abs/2402.13326 ,  4247kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13349 (*cross-listing*)
Date: Tue, 20 Feb 2024 19:53:15 GMT   (45556kb,D)

Title: Aria Everyday Activities Dataset
Authors: Zhaoyang Lv, Nickolas Charron, Pierre Moulon, Alexander Gamino, Cheng
  Peng, Chris Sweeney, Edward Miller, Huixuan Tang, Jeff Meissner, Jing Dong,
  Kiran Somasundaram, Luis Pesqueira, Mark Schwesinger, Omkar Parkhi, Qiao Gu,
  Renzo De Nardi, Shangyi Cheng, Steve Saarinen, Vijay Baiyya, Yuyang Zou,
  Richard Newcombe, Jakob Julian Engel, Xiaqing Pan, Carl Ren
Categories: cs.CV cs.AI cs.HC
Comments: Dataset website: https://www.projectaria.com/datasets/aea/
\\
  We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal
open dataset recorded using Project Aria glasses. AEA contains 143 daily
activity sequences recorded by multiple wearers in five geographically diverse
indoor locations. Each of the recording contains multimodal sensor data
recorded through the Project Aria glasses. In addition, AEA provides machine
perception data including high frequency globally aligned 3D trajectories,
scene point cloud, per-frame 3D eye gaze vector and time aligned speech
transcription. In this paper, we demonstrate a few exemplar research
applications enabled by this dataset, including neural scene reconstruction and
prompted segmentation. AEA is an open source dataset that can be downloaded
from projectaria.com. We are also providing open-source implementations and
examples of how to use the dataset in Project Aria Tools.
\\ ( https://arxiv.org/abs/2402.13349 ,  45556kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13352 (*cross-listing*)
Date: Tue, 20 Feb 2024 20:02:21 GMT   (1210kb,D)

Title: KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
Authors: Boran Apak, Medina Bandic, Aritra Sarkar and Sebastian Feld
Categories: quant-ph cs.AI cs.ET cs.LG
\\
  Quantum algorithms, represented as quantum circuits, can be used as
benchmarks for assessing the performance of quantum systems. Existing datasets,
widely utilized in the field, suffer from limitations in size and versatility,
leading researchers to employ randomly generated circuits. Random circuits are,
however, not representative benchmarks as they lack the inherent properties of
real quantum algorithms for which the quantum systems are manufactured. This
shortage of `useful' quantum benchmarks poses a challenge to advancing the
development and comparison of quantum compilers and hardware.
  This research aims to enhance the existing quantum circuit datasets by
generating what we refer to as `realistic-looking' circuits by employing the
Transformer machine learning architecture. For this purpose, we introduce
KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose
structure is based on quantum circuits derived from existing quantum algorithms
and follows the typical patterns of human-written algorithm-based code (e.g.,
order of gates and qubits). Our three-fold verification process, involving
manual inspection and Qiskit framework execution, transformer-based
classification, and structural analysis, demonstrates the efficacy of KetGPT in
producing large amounts of additional circuits that closely align with
algorithm-based structures. Beyond benchmarking, we envision KetGPT
contributing substantially to AI-driven quantum compilers and systems.
\\ ( https://arxiv.org/abs/2402.13352 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13397 (*cross-listing*)
Date: Tue, 20 Feb 2024 21:57:03 GMT   (657kb,D)

Title: Xling: A Learned Filter Framework for Accelerating High-Dimensional
  Approximate Similarity Join
Authors: Yifan Wang, Vyom Pathak, Daisy Zhe Wang
Categories: cs.DB cs.AI
\\
  Similarity join finds all pairs of close points within a given distance
threshold. Many similarity join methods have been proposed, but they are
usually not efficient on high-dimensional space due to the curse of
dimensionality and data-unawareness. We investigate the possibility of using
metric space Bloom filter (MSBF), a family of data structures checking if a
query point has neighbors in a multi-dimensional space, to speed up similarity
join. However, there are several challenges when applying MSBF to similarity
join, including excessive information loss, data-unawareness and hard
constraint on the distance metric. In this paper, we propose Xling, a generic
framework to build a learning-based metric space filter with any existing
regression model, aiming at accurately predicting whether a query point has
enough number of neighbors. The framework provides a suite of optimization
strategies to further improve the prediction quality based on the learning
model, which has demonstrated significantly higher prediction quality than
existing MSBF. We also propose XJoin, one of the first filter-based similarity
join methods, based on Xling. By predicting and skipping those queries without
enough neighbors, XJoin can effectively reduce unnecessary neighbor searching
and therefore it achieves a remarkable acceleration. Benefiting from the
generalization capability of deep learning models, XJoin can be easily
transferred onto new dataset (in similar distribution) without re-training.
Furthermore, Xling is not limited to being applied in XJoin, instead, it acts
as a flexible plugin that can be inserted to any loop-based similarity join
methods for a speedup.
\\ ( https://arxiv.org/abs/2402.13397 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13457 (*cross-listing*)
Date: Wed, 21 Feb 2024 01:26:39 GMT   (1163kb,D)

Title: LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study
Authors: Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, Stjepan Picek
Categories: cs.CR cs.AI
Comments: 16 pages, 6 figures
\\
  Large Language Models (LLMS) have increasingly become central to generating
content with potential societal impacts. Notably, these models have
demonstrated capabilities for generating content that could be deemed harmful.
To mitigate these risks, researchers have adopted safety training techniques to
align model outputs with societal values to curb the generation of malicious
content. However, the phenomenon of "jailbreaking", where carefully crafted
prompts elicit harmful responses from models, persists as a significant
challenge. This research conducts a comprehensive analysis of existing studies
on jailbreaking LLMs and their defense techniques. We meticulously investigate
nine attack techniques and seven defense techniques applied across three
distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate
the effectiveness of these attack and defense techniques. Our findings reveal
that existing white-box attacks underperform compared to universal techniques
and that including special tokens in the input significantly affects the
likelihood of successful attacks. This research highlights the need to
concentrate on the security facets of LLMs. Additionally, we contribute to the
field by releasing our datasets and testing framework, aiming to foster further
research into LLM security. We believe these contributions will facilitate the
exploration of security measures within this domain.
\\ ( https://arxiv.org/abs/2402.13457 ,  1163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13475 (*cross-listing*)
Date: Wed, 21 Feb 2024 02:16:59 GMT   (35899kb,D)

Title: Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal
  Learning for Glaucoma Forecasting from Irregular Time Series Images
Authors: Xikai Yang, Jian Wu, Xi Wang, Yuchen Yuan, Ning Li Wang, Pheng-Ann
  Heng
Categories: cs.CV cs.AI
Comments: 12 pages, 7 figures
\\
  Glaucoma is one of the major eye diseases that leads to progressive optic
nerve fiber damage and irreversible blindness, afflicting millions of
individuals. Glaucoma forecast is a good solution to early screening and
intervention of potential patients, which is helpful to prevent further
deterioration of the disease. It leverages a series of historical fundus images
of an eye and forecasts the likelihood of glaucoma occurrence in the future.
However, the irregular sampling nature and the imbalanced class distribution
are two challenges in the development of disease forecasting approaches. To
this end, we introduce the Multi-scale Spatio-temporal Transformer Network
(MST-former) based on the transformer architecture tailored for sequential
image inputs, which can effectively learn representative semantic information
from sequential images on both temporal and spatial dimensions. Specifically,
we employ a multi-scale structure to extract features at various resolutions,
which can largely exploit rich spatial information encoded in each image.
Besides, we design a time distance matrix to scale time attention in a
non-linear manner, which could effectively deal with the irregularly sampled
data. Furthermore, we introduce a temperature-controlled Balanced Softmax
Cross-entropy loss to address the class imbalance issue. Extensive experiments
on the Sequential fundus Images for Glaucoma Forecast (SIGF) dataset
demonstrate the superiority of the proposed MST-former method, achieving an AUC
of 98.6% for glaucoma forecasting. Besides, our method shows excellent
generalization capability on the Alzheimer's Disease Neuroimaging Initiative
(ADNI) MRI dataset, with an accuracy of 90.3% for mild cognitive impairment and
Alzheimer's disease prediction, outperforming the compared method by a large
margin.
\\ ( https://arxiv.org/abs/2402.13475 ,  35899kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13481 (*cross-listing*)
Date: Wed, 21 Feb 2024 02:44:33 GMT   (2842kb,D)

Title: Learning to Model Diverse Driving Behaviors in Highly Interactive
  Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning
Authors: Liu Weiwei, Hu Wenxuan, Jing Wei, Lei Lanxin, Gao Lingping and Liu
  Yong
Categories: cs.RO cs.AI
\\
  Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL)
have shown impressive results in many driving scenarios. However, the
performance of these trained policies can be impacted when faced with diverse
driving styles and personalities, particularly in highly interactive
situations. This is because conventional MARL algorithms usually operate under
the assumption of fully cooperative behavior among all agents and focus on
maximizing team rewards during training. To address this issue, we introduce
the Personality Modeling Network (PeMN), which includes a cooperation value
function and personality parameters to model the varied interactions in
high-interactive scenarios. The PeMN also enables the training of a background
traffic flow with diverse behaviors, thereby improving the performance and
generalization of the ego vehicle. Our extensive experimental studies, which
incorporate different personality parameters in high-interactive driving
scenarios, demonstrate that the personality parameters effectively model
diverse driving styles and that policies trained with PeMN demonstrate better
generalization compared to traditional MARL methods.
\\ ( https://arxiv.org/abs/2402.13481 ,  2842kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13521 (*cross-listing*)
Date: Wed, 21 Feb 2024 04:10:12 GMT   (9149kb,D)

Title: Test-Driven Development for Code Generation
Authors: Noble Saji Mathews and Meiyappan Nagappan
Categories: cs.SE cs.AI
\\
  Large language models (LLMs) like GPT4, have shown proficiency in generating
code snippets from problem statements. Traditionally software development by
humans followed a similar methodology of writing code from problem statements
or requirements. However, in the past, there have been several studies that
have shown the value of test-driven development (TDD) where humans write tests
based on problem statements before the code for the functionality is written.
In the context of LLM-based code generation, one obvious benefit of TDD is that
the developer then knows for sure if the generated code has passed all the
given tests or not. Therefore, in this paper, we want to empirically evaluate
the hypothesis: giving the problem statements and tests as input to GPT4 is
better than just giving the problem statement as input. To test our hypothesis,
we build a framework TGen. In our experiments on the MBPP, HumanEval and
CodeChef datasets, we consistently find that including tests solves more
programming problems than not including them. Thus we show that TDD is a better
development model than just using a problem statement when using GPT4 for code
generation tasks.
\\ ( https://arxiv.org/abs/2402.13521 ,  9149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13536 (*cross-listing*)
Date: Wed, 21 Feb 2024 05:14:30 GMT   (45270kb,D)

Title: Exploring the Limits of Semantic Image Compression at Micro-bits per
  Pixel
Authors: Jordan Dotzel, Bahaa Kotb, James Dotzel, Mohamed Abdelfattah, Zhiru
  Zhang
Categories: cs.CV cs.AI
Comments: Accepted to ICLR Tiny Papers 2024
\\
  Traditional methods, such as JPEG, perform image compression by operating on
structural information, such as pixel values or frequency content. These
methods are effective to bitrates around one bit per pixel (bpp) and higher at
standard image sizes. In contrast, text-based semantic compression directly
stores concepts and their relationships using natural language, which has
evolved with humans to efficiently represent these salient concepts. These
methods can operate at extremely low bitrates by disregarding structural
information like location, size, and orientation. In this work, we use GPT-4V
and DALL-E3 from OpenAI to explore the quality-compression frontier for image
compression and identify the limitations of current technology. We push
semantic compression as low as 100 $\mu$bpp (up to $10,000\times$ smaller than
JPEG) by introducing an iterative reflection process to improve the decoded
image. We further hypothesize this 100 $\mu$bpp level represents a soft limit
on semantic compression at standard image resolutions.
\\ ( https://arxiv.org/abs/2402.13536 ,  45270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13573 (*cross-listing*)
Date: Wed, 21 Feb 2024 07:10:28 GMT   (5166kb,D)

Title: ToDo: Token Downsampling for Efficient Generation of High-Resolution
  Images
Authors: Ethan Smith, Nayan Saxena, Aninda Saha
Categories: cs.CV cs.AI cs.LG
\\
  Attention mechanism has been crucial for image diffusion models, however,
their quadratic computational complexity limits the sizes of images we can
process within reasonable time and memory constraints. This paper investigates
the importance of dense attention in generative image models, which often
contain redundant features, making them suitable for sparser attention
mechanisms. We propose a novel training-free method ToDo that relies on token
downsampling of key and value tokens to accelerate Stable Diffusion inference
by up to 2x for common sizes and up to 4.5x or more for high resolutions like
2048x2048. We demonstrate that our approach outperforms previous methods in
balancing efficient throughput and fidelity.
\\ ( https://arxiv.org/abs/2402.13573 ,  5166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13575 (*cross-listing*)
Date: Wed, 21 Feb 2024 07:15:16 GMT   (5903kb,D)

Title: Flexible Physical Camouflage Generation Based on a Differential Approach
Authors: Yang Li, Wenyi Tan, Chenxing Zhao, Shuangju Zhou, Xinkai Liang, and
  Quan Pan
Categories: cs.CV cs.AI
\\
  This study introduces a novel approach to neural rendering, specifically
tailored for adversarial camouflage, within an extensive 3D rendering
framework. Our method, named FPA, goes beyond traditional techniques by
faithfully simulating lighting conditions and material variations, ensuring a
nuanced and realistic representation of textures on a 3D target. To achieve
this, we employ a generative approach that learns adversarial patterns from a
diffusion model. This involves incorporating a specially designed adversarial
loss and covert constraint loss to guarantee the adversarial and covert nature
of the camouflage in the physical world. Furthermore, we showcase the
effectiveness of the proposed camouflage in sticker mode, demonstrating its
ability to cover the target without compromising adversarial information.
Through empirical and physical experiments, FPA exhibits strong performance in
terms of attack success rate and transferability. Additionally, the designed
sticker-mode camouflage, coupled with a concealment constraint, adapts to the
environment, yielding diverse styles of texture. Our findings highlight the
versatility and efficacy of the FPA approach in adversarial camouflage
applications.
\\ ( https://arxiv.org/abs/2402.13575 ,  5903kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13602 (*cross-listing*)
Date: Wed, 21 Feb 2024 08:09:05 GMT   (4054kb,D)

Title: Hybrid Reasoning Based on Large Language Models for Autonomous Car
  Driving
Authors: Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab,
  Achim Rettberg
Categories: cs.CV cs.AI
Comments: 12 pages, 4 figures, results on GitHub
\\
  Large Language Models (LLMs) have garnered significant attention for their
ability to understand text and images, generate human-like text, and perform
complex reasoning tasks. However, their ability to generalize this advanced
reasoning with a combination of natural language text for decision-making in
dynamic situations requires further exploration. In this study, we investigate
how well LLMs can adapt and apply a combination of arithmetic and common-sense
reasoning, particularly in autonomous driving scenarios. We hypothesize that
LLMs hybrid reasoning abilities can improve autonomous driving by enabling them
to analyze detected object and sensor data, understand driving regulations and
physical laws, and offer additional context. This addresses complex scenarios,
like decisions in low visibility (due to weather conditions), where traditional
methods might fall short. We evaluated Large Language Models (LLMs) based on
accuracy by comparing their answers with human-generated ground truth inside
CARLA. The results showed that when a combination of images (detected objects)
and sensor data is fed into the LLM, it can offer precise information for brake
and throttle control in autonomous vehicles across various weather conditions.
This formulation and answers can assist in decision-making for auto-pilot
systems.
\\ ( https://arxiv.org/abs/2402.13602 ,  4054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13714 (*cross-listing*)
Date: Wed, 21 Feb 2024 11:27:31 GMT   (4181kb,D)

Title: An Evaluation of Large Language Models in Bioinformatics Research
Authors: Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier,
  Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun
Categories: q-bio.QM cs.AI cs.LG
Comments: Under review
\\
  Large language models (LLMs) such as ChatGPT have gained considerable
interest across diverse research communities. Their notable ability for text
completion and generation has inaugurated a novel paradigm for
language-interfaced problem solving. However, the potential and efficacy of
these models in bioinformatics remain incompletely explored. In this work, we
study the performance LLMs on a wide spectrum of crucial bioinformatics tasks.
These tasks include the identification of potential coding regions, extraction
of named entities for genes and proteins, detection of antimicrobial and
anti-cancer peptides, molecular optimization, and resolution of educational
bioinformatics problems. Our findings indicate that, given appropriate prompts,
LLMs like GPT variants can successfully handle most of these tasks. In
addition, we provide a thorough analysis of their limitations in the context of
complicated bioinformatics tasks. In conclusion, we believe that this work can
provide new perspectives and motivate future research in the field of LLMs
applications, AI for Science and bioinformatics.
\\ ( https://arxiv.org/abs/2402.13714 ,  4181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13750 (*cross-listing*)
Date: Wed, 21 Feb 2024 12:22:01 GMT   (3993kb,D)

Title: Breaking the Barrier: Utilizing Large Language Models for Industrial
  Recommendation Systems through an Inferential Knowledge Graph
Authors: Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang and Lihong Gu
Categories: cs.IR cs.AI cs.CL
Comments: 9 pages, 5 figures
\\
  Recommendation systems are widely used in e-commerce websites and online
platforms to address information overload. However, existing systems primarily
rely on historical data and user feedback, making it difficult to capture user
intent transitions. Recently, Knowledge Base (KB)-based models are proposed to
incorporate expert knowledge, but it struggle to adapt to new items and the
evolving e-commerce environment. To address these challenges, we propose a
novel Large Language Model based Complementary Knowledge Enhanced
Recommendation System (LLM-KERec). It introduces an entity extractor that
extracts unified concept terms from item and user information. To provide
cost-effective and reliable prior knowledge, entity pairs are generated based
on entity popularity and specific strategies. The large language model
determines complementary relationships in each entity pair, constructing a
complementary knowledge graph. Furthermore, a new complementary recall module
and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of
the ranking model using real complementary exposure-click samples. Extensive
experiments conducted on three industry datasets demonstrate the significant
performance improvement of our model compared to existing approaches.
Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm
for consumption by recommending complementary items. In summary, LLM-KERec
addresses the limitations of traditional recommendation systems by
incorporating complementary knowledge and utilizing a large language model to
capture user intent transitions, adapt to new items, and enhance recommendation
efficiency in the evolving e-commerce landscape.
\\ ( https://arxiv.org/abs/2402.13750 ,  3993kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13754 (*cross-listing*)
Date: Wed, 21 Feb 2024 12:30:39 GMT   (13840kb,D)

Title: Reinforcement learning-assisted quantum architecture search for
  variational quantum algorithms
Authors: Akash Kundu
Categories: quant-ph cs.AI cs.LG
Comments: With 154 pages and 46 figures here lies my PhD thesis
\\
  A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is
identifying functional quantum circuits. These circuits must also adhere to the
constraints imposed by current quantum hardware limitations. Variational
quantum algorithms (VQAs), a class of quantum-classical optimization
algorithms, were developed to address these challenges in the currently
available quantum devices. However, the overall performance of VQAs depends on
the initialization strategy of the variational circuit, the structure of the
circuit (also known as ansatz), and the configuration of the cost function.
Focusing on the structure of the circuit, in this thesis, we improve the
performance of VQAs by automating the search for an optimal structure for the
variational circuits using reinforcement learning (RL). Within the thesis, the
optimality of a circuit is determined by evaluating its depth, the overall
count of gates and parameters, and its accuracy in solving the given problem.
The task of automating the search for optimal quantum circuits is known as
quantum architecture search (QAS). The majority of research in QAS is primarily
focused on a noiseless scenario. Yet, the impact of noise on the QAS remains
inadequately explored. In this thesis, we tackle the issue by introducing a
tensor-based quantum circuit encoding, restrictions on environment dynamics to
explore the search space of possible circuits efficiently, an episode halting
scheme to steer the agent to find shorter circuits, a double deep Q-network
(DDQN) with an $\epsilon$-greedy policy for better stability. The numerical
experiments on noiseless and noisy quantum hardware show that in dealing with
various VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods
we propose in the thesis can be readily adapted to address a wide range of
other VQAs.
\\ ( https://arxiv.org/abs/2402.13754 ,  13840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13771 (*cross-listing*)
Date: Wed, 21 Feb 2024 12:48:45 GMT   (4016kb,D)

Title: Mask-up: Investigating Biases in Face Re-identification for Masked Faces
Authors: Siddharth D Jaiswal, Ankit Kr. Verma, Animesh Mukherjee
Categories: cs.CV cs.AI cs.CY cs.HC
Comments: This work has been submitted to the IEEE for possible publication
\\
  AI based Face Recognition Systems (FRSs) are now widely distributed and
deployed as MLaaS solutions all over the world, moreso since the COVID-19
pandemic for tasks ranging from validating individuals' faces while buying SIM
cards to surveillance of citizens. Extensive biases have been reported against
marginalized groups in these systems and have led to highly discriminatory
outcomes. The post-pandemic world has normalized wearing face masks but FRSs
have not kept up with the changing times. As a result, these systems are
susceptible to mask based face occlusion. In this study, we audit four
commercial and nine open-source FRSs for the task of face re-identification
between different varieties of masked and unmasked images across five benchmark
datasets (total 14,722 images). These simulate a realistic
validation/surveillance task as deployed in all major countries around the
world. Three of the commercial and five of the open-source FRSs are highly
inaccurate; they further perpetuate biases against non-White individuals, with
the lowest accuracy being 0%. A survey for the same task with 85 human
participants also results in a low accuracy of 40%. Thus a human-in-the-loop
moderation in the pipeline does not alleviate the concerns, as has been
frequently hypothesized in literature. Our large-scale study shows that
developers, lawmakers and users of such services need to rethink the design
principles behind FRSs, especially for the task of face re-identification,
taking cognizance of observed biases.
\\ ( https://arxiv.org/abs/2402.13771 ,  4016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13809 (*cross-listing*)
Date: Wed, 21 Feb 2024 13:46:25 GMT   (13214kb,D)

Title: NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual
  Feature Guided Diffusion
Authors: Haoyu Li, Hao Wu, Badong Chen
Categories: cs.NE cs.AI cs.CV
\\
  Reconstructing visual stimuli from functional Magnetic Resonance Imaging
(fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval
of the brain. A challenge persists in reconstructing a cohesive alignment of
details (such as structure, background, texture, color, etc.). Moreover, LDMs
would generate different image results even under the same conditions. For
these, we first uncover the neuroscientific perspective of LDM-based methods
that is top-down creation based on pre-trained knowledge from massive images
but lack of detail-driven bottom-up perception resulting in unfaithful details.
We propose NeuralDiffuser which introduces primary visual feature guidance to
provide detail cues in the form of gradients, extending the bottom-up process
for LDM-based methods to achieve faithful semantics and details. We also
developed a novel guidance strategy to ensure the consistency of repeated
reconstructions rather than a variety of results. We obtain the
state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset
(NSD), which offers more faithful details and consistent results.
\\ ( https://arxiv.org/abs/2402.13809 ,  13214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13840 (*cross-listing*)
Date: Wed, 21 Feb 2024 14:38:02 GMT   (640kb,D)

Title: LLM4SBR: A Lightweight and Effective Framework for Integrating Large
  Language Models in Session-based Recommendation
Authors: Shutong Qiao, Chen Gao, Junhao Wen, Wei Zhou, Qun Luo, Peixuan Chen
  and Yong Li
Categories: cs.IR cs.AI
\\
  Traditional session-based recommendation (SBR) utilizes session behavior
sequences from anonymous users for recommendation. Although this strategy is
highly efficient, it sacrifices the inherent semantic information of the items,
making it difficult for the model to understand the true intent of the session
and resulting in a lack of interpretability in the recommended results.
Recently, large language models (LLMs) have flourished across various domains,
offering a glimpse of hope in addressing the aforementioned challenges.
Inspired by the impact of LLMs, research exploring the integration of LLMs with
the Recommender system (RS) has surged like mushrooms after rain. However,
constrained by high time and space costs, as well as the brief and anonymous
nature of session data, the first LLM recommendation framework suitable for
industrial deployment has yet to emerge in the field of SBR. To address the
aforementioned challenges, we have proposed the LLM Integration Framework for
SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR
adopts a two-step strategy. Firstly, we transform session data into a bimodal
form of text and behavior. In the first step, leveraging the inferential
capabilities of LLMs, we conduct inference on session text data from different
perspectives and design the component for auxiliary enhancement. In the second
step, the SBR model is trained on behavior data, aligning and averaging two
modal session representations from different perspectives. Finally, we fuse
session representations from different perspectives and modalities as the
ultimate session representation for recommendation. We conducted experiments on
two real-world datasets, and the results demonstrate that LLM4SBR significantly
improves the performance of traditional SBR models and is highly lightweight
and efficient, making it suitable for industrial deployment.
\\ ( https://arxiv.org/abs/2402.13840 ,  640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13846 (*cross-listing*)
Date: Wed, 21 Feb 2024 14:44:00 GMT   (1051kb,D)

Title: Large Language Models are Advanced Anonymizers
Authors: Robin Staab, Mark Vero, Mislav Balunovi\'c, Martin Vechev
Categories: cs.CR cs.AI cs.CL cs.LG
ACM-class: I.2.7
\\
  Recent work in privacy research on large language models has shown that they
achieve near human-level performance at inferring personal data from real-world
online texts. With consistently increasing model capabilities, existing text
anonymization methods are currently lacking behind regulatory requirements and
adversarial threats. This raises the question of how individuals can
effectively protect their personal data in sharing online texts. In this work,
we take two steps to answer this question: We first present a new setting for
evaluating anonymizations in the face of adversarial LLMs inferences, allowing
for a natural measurement of anonymization performance while remedying some of
the shortcomings of previous metrics. We then present our LLM-based adversarial
anonymization framework leveraging the strong inferential capabilities of LLMs
to inform our anonymization procedure. In our experimental evaluation, we show
on real-world and synthetic online texts how adversarial anonymization
outperforms current industry-grade anonymizers both in terms of the resulting
utility and privacy.
\\ ( https://arxiv.org/abs/2402.13846 ,  1051kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13853 (*cross-listing*)
Date: Wed, 21 Feb 2024 14:59:46 GMT   (31661kb,D)

Title: RealDex: Towards Human-like Grasping for Robotic Dexterous Hand
Authors: Yumeng Liu, Yaxun Yang, Youzhuo Wang, Xiaofei Wu, Jiamin Wang, Yichen
  Yao, S\"oren Schwertfeger, Sibei Yang, Wenping Wang, Jingyi Yu, Xuming He,
  Yuexin Ma
Categories: cs.RO cs.AI
\\
  In this paper, we introduce RealDex, a pioneering dataset capturing authentic
dexterous hand grasping motions infused with human behavioral patterns,
enriched by multi-view and multimodal visual data. Utilizing a teleoperation
system, we seamlessly synchronize human-robot hand poses in real time. This
collection of human-like motions is crucial for training dexterous hands to
mimic human movements more naturally and precisely. RealDex holds immense
promise in advancing humanoid robot for automated perception, cognition, and
manipulation in real-world scenarios. Moreover, we introduce a cutting-edge
dexterous grasping motion generation framework, which aligns with human
experience and enhances real-world applicability through effectively utilizing
Multimodal Large Language Models. Extensive experiments have demonstrated the
superior performance of our method on RealDex and other open datasets. The
complete dataset and code will be made available upon the publication of this
work.
\\ ( https://arxiv.org/abs/2402.13853 ,  31661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13897 (*cross-listing*)
Date: Wed, 21 Feb 2024 16:09:25 GMT   (193kb,D)

Title: Science Checker Reloaded: A Bidirectional Paradigm for Transparency and
  Logical Reasoning
Authors: Lo\"ic Rakotoson, Sylvain Massip, Fr\'ejus A. A. Laleye
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: 6 pages, 3 figures
ACM-class: H.3.1; H.3.3; I.7; K.4
\\
  Information retrieval is a rapidly evolving field. However it still faces
significant limitations in the scientific and industrial vast amounts of
information, such as semantic divergence and vocabulary gaps in sparse
retrieval, low precision and lack of interpretability in semantic search, or
hallucination and outdated information in generative models. In this paper, we
introduce a two-block approach to tackle these hurdles for long documents. The
first block enhances language understanding in sparse retrieval by query
expansion to retrieve relevant documents. The second block deepens the result
by providing comprehensive and informative answers to the complex question
using only the information spread in the long document, enabling bidirectional
engagement. At various stages of the pipeline, intermediate results are
presented to users to facilitate understanding of the system's reasoning. We
believe this bidirectional approach brings significant advancements in terms of
transparency, logical thinking, and comprehensive understanding in the field of
scientific information retrieval.
\\ ( https://arxiv.org/abs/2402.13897 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13929 (*cross-listing*)
Date: Wed, 21 Feb 2024 16:51:05 GMT   (16120kb,D)

Title: SDXL-Lightning: Progressive Adversarial Diffusion Distillation
Authors: Shanchuan Lin, Anran Wang, Xiao Yang
Categories: cs.CV cs.AI cs.LG
\\
  We propose a diffusion distillation method that achieves new state-of-the-art
in one-step/few-step 1024px text-to-image generation based on SDXL. Our method
combines progressive and adversarial distillation to achieve a balance between
quality and mode coverage. In this paper, we discuss the theoretical analysis,
discriminator design, model formulation, and training techniques. We
open-source our distilled SDXL-Lightning models both as LoRA and full UNet
weights.
\\ ( https://arxiv.org/abs/2402.13929 ,  16120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13945 (*cross-listing*)
Date: Wed, 21 Feb 2024 17:15:47 GMT   (2719kb,D)

Title: Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty
  in Scientific Machine Learning
Authors: Farhad Pourkamali-Anaraki, Jamal F. Husseini, Scott E. Stapleton
Categories: stat.ML cs.AI cs.LG
Comments: 9 figures
\\
  This paper investigates the use of probabilistic neural networks (PNNs) to
model aleatoric uncertainty, which refers to the inherent variability in the
input-output relationships of a system, often characterized by unequal variance
or heteroscedasticity. Unlike traditional neural networks that produce
deterministic outputs, PNNs generate probability distributions for the target
variable, allowing the determination of both predicted means and intervals in
regression scenarios. Contributions of this paper include the development of a
probabilistic distance metric to optimize PNN architecture, and the deployment
of PNNs in controlled data sets as well as a practical material science case
involving fiber-reinforced composites. The findings confirm that PNNs
effectively model aleatoric uncertainty, proving to be more appropriate than
the commonly employed Gaussian process regression for this purpose.
Specifically, in a real-world scientific machine learning context, PNNs yield
remarkably accurate output mean estimates with R-squared scores approaching
0.97, and their predicted intervals exhibit a high correlation coefficient of
nearly 0.80, closely matching observed data intervals. Hence, this research
contributes to the ongoing exploration of leveraging the sophisticated
representational capacity of neural networks to delineate complex input-output
relationships in scientific problems.
\\ ( https://arxiv.org/abs/2402.13945 ,  2719kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13452 (*cross-listing*)
Date: Wed, 21 Feb 2024 01:11:28 GMT   (2042kb,D)

Title: LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data
Authors: Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian
  Gibbons, Hong Yu
Categories: cs.SI cs.CL cs.LG
Journal-ref: LREC-COLING 2024
\\
  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
\\ ( https://arxiv.org/abs/2402.13452 ,  2042kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13500 (*cross-listing*)
Date: Wed, 21 Feb 2024 03:25:14 GMT   (296kb,D)

Title: Leveraging Translation For Optimal Recall: Tailoring LLM Personalization
  With User Profiles
Authors: Karthik Ravichandran, Sarmistha Sarna Gomasta
Categories: cs.IR cs.CL
Comments: This is just an initial idea and it's implementation. The results are
  computed for the first 100 data points. Detailed results will be published
  with the actual paper
ACM-class: F.2.2; I.2.7
\\
  This paper explores a novel technique for improving recall in cross-language
information retrieval (CLIR) systems using iterative query refinement grounded
in the user's lexical-semantic space. The proposed methodology combines
multi-level translation, semantic embedding-based expansion, and user
profile-centered augmentation to address the challenge of matching variance
between user queries and relevant documents. Through an initial BM25 retrieval,
translation into intermediate languages, embedding lookup of similar terms, and
iterative re-ranking, the technique aims to expand the scope of potentially
relevant results personalized to the individual user. Comparative experiments
on news and Twitter datasets demonstrate superior performance over baseline
BM25 ranking for the proposed approach across ROUGE metrics. The translation
methodology also showed maintained semantic accuracy through the multi-step
process. This personalized CLIR framework paves the path for improved
context-aware retrieval attentive to the nuances of user language.
\\ ( https://arxiv.org/abs/2402.13500 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13518 (*cross-listing*)
Date: Wed, 21 Feb 2024 04:00:54 GMT   (384kb,D)

Title: RITFIS: Robust input testing framework for LLMs-based intelligent
  software
Authors: Mingxuan Xiao, Yan Xiao, Hai Dong, Shunhui Ji and Pengcheng Zhang
Categories: cs.SE cs.CL
\\
  The dependence of Natural Language Processing (NLP) intelligent software on
Large Language Models (LLMs) is increasingly prominent, underscoring the
necessity for robustness testing. Current testing methods focus solely on the
robustness of LLM-based software to prompts. Given the complexity and diversity
of real-world inputs, studying the robustness of LLMbased software in handling
comprehensive inputs (including prompts and examples) is crucial for a thorough
understanding of its performance.
  To this end, this paper introduces RITFIS, a Robust Input Testing Framework
for LLM-based Intelligent Software. To our knowledge, RITFIS is the first
framework designed to assess the robustness of LLM-based intelligent software
against natural language inputs. This framework, based on given threat models
and prompts, primarily defines the testing process as a combinatorial
optimization problem. Successful test cases are determined by a goal function,
creating a transformation space for the original examples through perturbation
means, and employing a series of search methods to filter cases that meet both
the testing objectives and language constraints. RITFIS, with its modular
design, offers a comprehensive method for evaluating the robustness of LLMbased
intelligent software.
  RITFIS adapts 17 automated testing methods, originally designed for Deep
Neural Network (DNN)-based intelligent software, to the LLM-based software
testing scenario. It demonstrates the effectiveness of RITFIS in evaluating
LLM-based intelligent software through empirical validation. However, existing
methods generally have limitations, especially when dealing with lengthy texts
and structurally complex threat models. Therefore, we conducted a comprehensive
analysis based on five metrics and provided insightful testing method
optimization strategies, benefiting both researchers and everyday users.
\\ ( https://arxiv.org/abs/2402.13518 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13528 (*cross-listing*)
Date: Wed, 21 Feb 2024 04:55:03 GMT   (3825kb,D)

Title: Infrastructure Ombudsman: Mining Future Failure Concerns from Structural
  Disaster Response
Authors: Md Towhidul Absar Chowdhury, Soumyajit Datta, Naveen Sharma, Ashiqur
  R. KhudaBukhsh
Categories: cs.CY cs.CL cs.LG cs.SI
DOI: 10.1145/3589334.3648153
\\
  Current research concentrates on studying discussions on social media related
to structural failures to improve disaster response strategies. However,
detecting social web posts discussing concerns about anticipatory failures is
under-explored. If such concerns are channeled to the appropriate authorities,
it can aid in the prevention and mitigation of potential infrastructural
failures. In this paper, we develop an infrastructure ombudsman -- that
automatically detects specific infrastructure concerns. Our work considers
several recent structural failures in the US. We present a first-of-its-kind
dataset of 2,662 social web instances for this novel task mined from Reddit and
YouTube.
\\ ( https://arxiv.org/abs/2402.13528 ,  3825kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13636 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:17:51 GMT   (9843kb,D)

Title: A Unified Framework and Dataset for Assessing Gender Bias in
  Vision-Language Models
Authors: Ashutosh Sathe, Prachi Jain, Sunayana Sitaram
Categories: cs.CV cs.CL cs.CY
\\
  Large vision-language models (VLMs) are widely getting adopted in industry
and academia. In this work we build a unified framework to systematically
evaluate gender-profession bias in VLMs. Our evaluation encompasses all
supported inference modes of the recent VLMs, including image-to-text,
text-to-text, text-to-image, and image-to-image. We construct a synthetic,
high-quality dataset of text and images that blurs gender distinctions across
professional actions to benchmark gender bias. In our benchmarking of recent
vision-language models (VLMs), we observe that different input-output
modalities result in distinct bias magnitudes and directions. We hope our work
will help guide future progress in improving VLMs to learn socially unbiased
representations. We will release our data and code.
\\ ( https://arxiv.org/abs/2402.13636 ,  9843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13659 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:45:08 GMT   (1079kb,D)

Title: Privacy-Preserving Instructions for Aligning Large Language Models
Authors: Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu
Categories: cs.CR cs.CL
\\
  Service providers of large language model (LLM) applications collect user
instructions in the wild and use them in further aligning LLMs with users'
intentions. These instructions, which potentially contain sensitive
information, are annotated by human workers in the process. This poses a new
privacy risk not addressed by the typical private optimization. To this end, we
propose using synthetic instructions to replace real instructions in data
annotation and model fine-tuning. Formal differential privacy is guaranteed by
generating those synthetic instructions using privately fine-tuned generators.
Crucial in achieving the desired utility is our novel filtering algorithm that
matches the distribution of the synthetic instructions to that of the real
ones. In both supervised fine-tuning and reinforcement learning from human
feedback, our extensive experiments demonstrate the high utility of the final
set of synthetic instructions by showing comparable results to real
instructions. In supervised fine-tuning, models trained with private synthetic
instructions outperform leading open-source models such as Vicuna.
\\ ( https://arxiv.org/abs/2402.13659 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07240 (*cross-listing*)
Date: Sun, 11 Feb 2024 16:36:48 GMT   (366kb,D)
Date (revised v2): Tue, 20 Feb 2024 05:43:42 GMT   (369kb,D)

Title: Thresholded Oja does Sparse PCA?
Authors: Syamantak Kumar and Purnamrita Sarkar
Categories: math.ST cs.LG stat.ML stat.TH
\\
  We consider the problem of Sparse Principal Component Analysis (PCA) when the
ratio $d/n \rightarrow c > 0$. There has been a lot of work on optimal rates on
sparse PCA in the offline setting, where all the data is available for multiple
passes. In contrast, when the population eigenvector is $s$-sparse, streaming
algorithms that have $O(d)$ storage and $O(nd)$ time complexity either
typically require strong initialization conditions or have a suboptimal error.
We show that a simple algorithm that thresholds and renormalizes the output of
Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is
very surprising because, without thresholding, the Oja vector has a large
error. Our analysis centers around bounding the entries of the unnormalized Oja
vector, which involves the projection of a product of independent random
matrices on a random initial vector. This is nontrivial and novel since
previous analyses of Oja's algorithm and matrix products have been done when
the trace of the population covariance matrix is bounded while in our setting,
this quantity can be as large as $n$.
\\ ( https://arxiv.org/abs/2402.07240 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13277 (*cross-listing*)
Date: Sat, 17 Feb 2024 18:04:08 GMT   (782kb,D)

Title: MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek
  in WSNs
Authors: Md. Alamin Talukder, Selina Sharmin, Md Ashraf Uddin, Md Manowarul
  Islam and Sunil Aryal
Categories: cs.CR cs.LG
Comments: Q1, Scopus, ISI, SCIE, IF: 3.2
\\
  Wireless Sensor Networks (WSNs) play a pivotal role as infrastructures,
encompassing both stationary and mobile sensors. These sensors self-organize
and establish multi-hop connections for communication, collectively sensing,
gathering, processing, and transmitting data about their surroundings. Despite
their significance, WSNs face rapid and detrimental attacks that can disrupt
functionality. Existing intrusion detection methods for WSNs encounter
challenges such as low detection rates, computational overhead, and false
alarms. These issues stem from sensor node resource constraints, data
redundancy, and high correlation within the network. To address these
challenges, we propose an innovative intrusion detection approach that
integrates Machine Learning (ML) techniques with the Synthetic Minority
Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend
synthesizes minority instances and eliminates Tomek links, resulting in a
balanced dataset that significantly enhances detection accuracy in WSNs.
Additionally, we incorporate feature scaling through standardization to render
input features consistent and scalable, facilitating more precise training and
detection. To counteract imbalanced WSN datasets, we employ the SMOTE-Tomek
resampling technique, mitigating overfitting and underfitting issues. Our
comprehensive evaluation, using the WSN Dataset (WSN-DS) containing 374,661
records, identifies the optimal model for intrusion detection in WSNs. The
standout outcome of our research is the remarkable performance of our model. In
binary, it achieves an accuracy rate of 99.78% and in multiclass, it attains an
exceptional accuracy rate of 99.92%. These findings underscore the efficiency
and superiority of our proposal in the context of WSN intrusion detection,
showcasing its effectiveness in detecting and mitigating intrusions in WSNs.
\\ ( https://arxiv.org/abs/2402.13277 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13285 (*cross-listing*)
Date: Mon, 19 Feb 2024 10:15:11 GMT   (3201kb,D)

Title: Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization
  Bounds with Complexity Measures
Authors: Paul Viallard, R\'emi Emonet, Amaury Habrard, Emilie Morvant,
  Valentina Zantedeschi
Categories: stat.ML cs.LG
Comments: AISTATS 2024
\\
  In statistical learning theory, a generalization bound usually involves a
complexity measure imposed by the considered theoretical framework. This limits
the scope of such bounds, as other forms of capacity measures or
regularizations are used in algorithms. In this paper, we leverage the
framework of disintegrated PAC-Bayes bounds to derive a general generalization
bound instantiable with arbitrary complexity measures. One trick to prove such
a result involves considering a commonly used family of distributions: the
Gibbs distributions. Our bound stands in probability jointly over the
hypothesis and the learning sample, which allows the complexity to be adapted
to the generalization gap as it can be customized to fit both the hypothesis
class and the task.
\\ ( https://arxiv.org/abs/2402.13285 ,  3201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13291 (*cross-listing*)
Date: Mon, 19 Feb 2024 18:35:40 GMT   (109kb,D)

Title: DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language
  Models
Authors: Berkay Berabi, Alexey Gronskiy, Veselin Raychev, Gishor Sivanrupan,
  Victor Chibotaru, Martin Vechev
Categories: cs.CR cs.LG cs.PL cs.SE
Comments: 26 pages, 13 figures
\\
  The automated program repair field has attracted substantial interest over
the years, but despite significant research efforts, creating a system that
works well for complex semantic bugs such as security vulnerabilities has
proven difficult. A promising direction to solve this challenge is by
leveraging large language models (LLMs), which are increasingly used to solve
various programming tasks. In this paper, we investigate the effectiveness of
LLMs for solving code-repair task. We show that the task is difficult as it
requires the model to learn long-range code relationships, a task that
inherently relies on extensive amounts of training data. At the same time,
creating a large, clean dataset for complex program bugs and their
corresponding fixes is non-trivial. We propose a technique to address these
challenges with a new approach for querying and fine-tuning LLMs. The idea is
to use program analysis to limit the LLM's attention mechanism on the portions
of code needed to perform the fix, drastically reducing the amount of required
training data. Concretely, for training and inference, rather than feeding the
entire program to the LLM, we reduce its code to a much shorter snippet that
contains the reported defect together with the necessary context - and use that
instead. Our evaluation shows that this code reduction approach substantially
improves available models such as GPT-4 using few-shot learning, as well as
fine-tuning models. To train and evaluate our system, we created a
comprehensive code fixing dataset by extensively labeling 156 bug patterns
(including 40 security rules), requiring complex interprocedural dataflow to
discover. Our best system with Mixtral-8x7B can remove more than 80% of the
reported defects while exactly matching the human fix in between 10 and 50% of
cases, outperforming baselines based on GPT-3.5 and GPT-4, or based on
window-based models like TFix.
\\ ( https://arxiv.org/abs/2402.13291 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13321 (*cross-listing*)
Date: Tue, 20 Feb 2024 19:00:59 GMT   (961kb,D)

Title: Rigor with Machine Learning from Field Theory to the Poincar\'e
  Conjecture
Authors: Sergei Gukov, James Halverson, Fabian Ruehle
Categories: hep-th cs.LG
Comments: 13 pages. Preprint of edited version in Nature Reviews Physics.
  Please cite journal version
\\
  Machine learning techniques are increasingly powerful, leading to many
breakthroughs in the natural sciences, but they are often stochastic,
error-prone, and blackbox. How, then, should they be utilized in fields such as
theoretical physics and pure mathematics that place a premium on rigor and
understanding? In this Perspective we discuss techniques for obtaining rigor in
the natural sciences with machine learning. Non-rigorous methods may lead to
rigorous results via conjecture generation or verification by reinforcement
learning. We survey applications of these techniques-for-rigor ranging from
string theory to the smooth $4$d Poincar\'e conjecture in low-dimensional
topology. One can also imagine building direct bridges between machine learning
theory and either mathematics or theoretical physics. As examples, we describe
a new approach to field theory motivated by neural network theory, and a theory
of Riemannian metric flows induced by neural network gradient descent, which
encompasses Perelman's formulation of the Ricci flow that was utilized to
resolve the $3$d Poincar\'e conjecture.
\\ ( https://arxiv.org/abs/2402.13321 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13353 (*cross-listing*)
Date: Tue, 20 Feb 2024 20:04:23 GMT   (22757kb,D)

Title: Combining unsupervised and supervised learning in microscopy enables
  defect analysis of a full 4H-SiC wafer
Authors: Binh Duong Nguyen, Johannes Steiner, Peter Wellmann, Stefan Sandfeld
Categories: cs.CV cond-mat.mtrl-sci cs.LG
\\
  Detecting and analyzing various defect types in semiconductor materials is an
important prerequisite for understanding the underlying mechanisms as well as
tailoring the production processes. Analysis of microscopy images that reveal
defects typically requires image analysis tasks such as segmentation and object
detection. With the permanently increasing amount of data that is produced by
experiments, handling these tasks manually becomes more and more impossible. In
this work, we combine various image analysis and data mining techniques for
creating a robust and accurate, automated image analysis pipeline. This allows
for extracting the type and position of all defects in a microscopy image of a
KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000
individual images.
\\ ( https://arxiv.org/abs/2402.13353 ,  22757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13400 (*cross-listing*)
Date: Tue, 20 Feb 2024 21:59:41 GMT   (414kb,D)

Title: The Dimension of Self-Directed Learning
Authors: Pramith Devulapalli and Steve Hanneke
Categories: stat.ML cs.LG
Comments: ALT 2024 Camera ready version
\\
  Understanding the self-directed learning complexity has been an important
problem that has captured the attention of the online learning theory community
since the early 1990s. Within this framework, the learner is allowed to
adaptively choose its next data point in making predictions unlike the setting
in adversarial online learning.
  In this paper, we study the self-directed learning complexity in both the
binary and multi-class settings, and we develop a dimension, namely $SDdim$,
that exactly characterizes the self-directed learning mistake-bound for any
concept class. The intuition behind $SDdim$ can be understood as a two-player
game called the "labelling game". Armed with this two-player game, we calculate
$SDdim$ on a whole host of examples with notable results on axis-aligned
rectangles, VC dimension $1$ classes, and linear separators. We demonstrate
several learnability gaps with a central focus on self-directed learning and
offline sequence learning models that include either the best or worst
ordering. Finally, we extend our analysis to the self-directed binary agnostic
setting where we derive upper and lower bounds.
\\ ( https://arxiv.org/abs/2402.13400 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13435 (*cross-listing*)
Date: Wed, 21 Feb 2024 00:05:25 GMT   (398kb,D)

Title: Learning to Retrieve for Job Matching
Authors: Jianqiang Shen, Yuchin Juan, Shaobo Zhang, Ping Liu, Wen Pu, Sriram
  Vasudevan, Qingquan Song, Fedor Borisyuk, Kay Qianqi Shen, Haichao Wei,
  Yunxiang Ren, Yeou S. Chiou, Sicong Kuang, Yuan Yin, Ben Zheng, Muchen Wu,
  Shaghayegh Gharghabi, Xiaoqing Wang, Huichao Xue, Qi Guo, Daniel Hewlett,
  Luke Simon, Liangjie Hong, Wenjing Zhang
Categories: cs.IR cs.LG
\\
  Web-scale search systems typically tackle the scalability challenge with a
two-step paradigm: retrieval and ranking. The retrieval step, also known as
candidate selection, often involves extracting standardized entities, creating
an inverted index, and performing term matching for retrieval. Such traditional
methods require manual and time-consuming development of query models. In this
paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns
job search and recommendation systems. In the realm of promoted jobs, the key
objective is to improve the quality of applicants, thereby delivering value to
recruiter customers. To achieve this, we leverage confirmed hire data to
construct a graph that evaluates a seeker's qualification for a job, and
utilize learned links for retrieval. Our learned model is easy to explain,
debug, and adjust. On the other hand, the focus for organic jobs is to optimize
seeker engagement. We accomplished this by training embeddings for personalized
retrieval, fortified by a set of rules derived from the categorization of
member feedback. In addition to a solution based on a conventional inverted
index, we developed an on-GPU solution capable of supporting both KNN and term
matching efficiently.
\\ ( https://arxiv.org/abs/2402.13435 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13523 (*cross-listing*)
Date: Wed, 21 Feb 2024 04:36:30 GMT   (3406kb,D)

Title: Balancing Spectral, Temporal and Spatial Information for EEG-based
  Alzheimer's Disease Classification
Authors: Stephan Goerttler, Fei He, Min Wu
Categories: eess.SP cs.LG q-bio.NC
Comments: 4 pages, 3 figures, conference paper
\\
  The prospect of future treatment warrants the development of cost-effective
screening for Alzheimer's disease (AD). A promising candidate in this regard is
electroencephalography (EEG), as it is one of the most economic imaging
modalities. Recent efforts in EEG analysis have shifted towards leveraging
spatial information, employing novel frameworks such as graph signal processing
or graph neural networks. Here, we systematically investigate the importance of
spatial information relative to spectral or temporal information by varying the
proportion of each dimension for AD classification. To do so, we test various
dimension resolution configurations on two routine EEG datasets. We find that
spatial information is consistently more relevant than temporal information and
equally relevant as spectral information. These results emphasise the necessity
to consider spatial information for EEG-based AD classification. On our second
dataset, we further find that well-balanced feature resolutions boost
classification accuracy by up to 1.6%. Our resolution-based feature extraction
has the potential to improve AD classification specifically, and multivariate
signal classification generally.
\\ ( https://arxiv.org/abs/2402.13523 ,  3406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13530 (*cross-listing*)
Date: Wed, 21 Feb 2024 04:57:32 GMT   (150kb,D)

Title: Best of Many in Both Worlds: Online Resource Allocation with Predictions
  under Unknown Arrival Model
Authors: Lin An, Andrew A. Li, Benjamin Moseley, and Gabriel Visotsky
Categories: math.OC cs.LG
\\
  Online decision-makers today can often obtain predictions on future
variables, such as arrivals, demands, inventories, and so on. These predictions
can be generated from simple forecasting algorithms for univariate time-series,
all the way to state-of-the-art machine learning models that leverage multiple
time-series and additional feature information. However, the prediction quality
is often unknown to decisions-makers a priori, hence blindly following the
predictions can be harmful. In this paper, we address this problem by giving
algorithms that take predictions as inputs and perform robustly against the
unknown prediction quality.
  We consider the online resource allocation problem, one of the most generic
models in revenue management and online decision-making. In this problem, a
decision maker has a limited amount of resources, and requests arrive
sequentially. For each request, the decision-maker needs to decide on an
action, which generates a certain amount of rewards and consumes a certain
amount of resources, without knowing the future requests. The decision-maker's
objective is to maximize the total rewards subject to resource constraints. We
take the shadow price of each resource as prediction, which can be obtained by
predictions on future requests. Prediction quality is naturally defined to be
the $\ell_1$ distance between the prediction and the actual shadow price. Our
main contribution is an algorithm which takes the prediction of unknown quality
as an input, and achieves asymptotically optimal performance under both
requests arrival models (stochastic and adversarial) without knowing the
prediction quality and the requests arrival model beforehand. We show our
algorithm's performance matches the best achievable performance of any
algorithm had the arrival models and the accuracy of the predictions been
known. We empirically validate our algorithm with experiments.
\\ ( https://arxiv.org/abs/2402.13530 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13595 (*cross-listing*)
Date: Wed, 21 Feb 2024 07:55:33 GMT   (63kb)

Title: A cutting plane algorithm for globally solving low dimensional k-means
  clustering problems
Authors: Martin Ryner, Jan Kronqvist, Johan Karlsson
Categories: math.OC cs.LG stat.ML
Comments: 12 pages, 3 figures
MSC-class: 90C26 (Primary) 90C27 (Secondary)
ACM-class: G.1.6
\\
  Clustering is one of the most fundamental tools in data science and machine
learning, and k-means clustering is one of the most common such methods. There
is a variety of approximate algorithms for the k-means problem, but computing
the globally optimal solution is in general NP-hard. In this paper we consider
the k-means problem for instances with low dimensional data and formulate it as
a structured concave assignment problem. This allows us to exploit the low
dimensional structure and solve the problem to global optimality within
reasonable time for large data sets with several clusters. The method builds on
iteratively solving a small concave problem and a large linear programming
problem. This gives a sequence of feasible solutions along with bounds which we
show converges to zero optimality gap. The paper combines methods from global
optimization theory to accelerate the procedure, and we provide numerical
results on their performance.
\\ ( https://arxiv.org/abs/2402.13595 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13608 (*cross-listing*)
Date: Wed, 21 Feb 2024 08:21:48 GMT   (850kb)

Title: Convergence Acceleration of Markov Chain Monte Carlo-based Gradient
  Descent by Deep Unfolding
Authors: Ryo Hagiwara and Satoshi Takabe
Categories: cond-mat.dis-nn cs.LG stat.ML
Comments: 10 pages, 5 figures
\\
  This study proposes a trainable sampling-based solver for combinatorial
optimization problems (COPs) using a deep-learning technique called deep
unfolding. The proposed solver is based on the Ohzeki method that combines
Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are
trained by minimizing a loss function. In the training process, we propose a
sampling-based gradient estimation that substitutes auto-differentiation with a
variance estimation, thereby circumventing the failure of back propagation due
to the non-differentiability of MCMC. The numerical results for a few COPs
demonstrated that the proposed solver significantly accelerated the convergence
speed compared with the original Ohzeki method.
\\ ( https://arxiv.org/abs/2402.13608 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13622 (*cross-listing*)
Date: Wed, 21 Feb 2024 08:50:33 GMT   (97kb)

Title: Analysis of Bootstrap and Subsampling in High-dimensional Regularized
  Regression
Authors: Lucas Clart\'e, Adrien Vandenbroucque, Guillaume Dalle, Bruno
  Loureiro, Florent Krzakala, Lenka Zdeborov\'a
Categories: stat.ML cond-mat.dis-nn cs.LG
\\
  We investigate popular resampling methods for estimating the uncertainty of
statistical models, such as subsampling, bootstrap and the jackknife, and their
performance in high-dimensional supervised regression tasks. We provide a tight
asymptotic description of the biases and variances estimated by these methods
in the context of generalized linear models, such as ridge and logistic
regression, taking the limit where the number of samples $n$ and dimension $d$
of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Our
findings are three-fold: i) resampling methods are fraught with problems in
high dimensions and exhibit the double-descent-like behavior typical of these
situations; ii) only when $\alpha$ is large enough do they provide consistent
and reliable error estimations (we give convergence rates); iii) in the
over-parametrized regime $\alpha\!<\!1$ relevant to modern machine learning
practice, their predictions are not consistent, even with optimal
regularization.
\\ ( https://arxiv.org/abs/2402.13622 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13634 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:13:08 GMT   (23214kb,D)

Title: Learning Dual-arm Object Rearrangement for Cartesian Robots
Authors: Shishun Zhang, Qijin She, Wenhao Li, Chenyang Zhu, Yongjun Wang,
  Ruizhen Hu, Kai Xu
Categories: cs.RO cs.LG
Comments: 7 pages, 9 figures, conference
\\
  This work focuses on the dual-arm object rearrangement problem abstracted
from a realistic industrial scenario of Cartesian robots. The goal of this
problem is to transfer all the objects from sources to targets with the minimum
total completion time. To achieve the goal, the core idea is to develop an
effective object-to-arm task assignment strategy for minimizing the cumulative
task execution time and maximizing the dual-arm cooperation efficiency. One of
the difficulties in the task assignment is the scalability problem. As the
number of objects increases, the computation time of traditional
offline-search-based methods grows strongly for computational complexity.
Encouraged by the adaptability of reinforcement learning (RL) in long-sequence
task decisions, we propose an online task assignment decision method based on
RL, and the computation time of our method only increases linearly with the
number of objects. Further, we design an attention-based network to model the
dependencies between the input states during the whole task execution process
to help find the most reasonable object-to-arm correspondence in each task
assignment round. In the experimental part, we adapt some search-based methods
to this specific setting and compare our method with them. Experimental result
shows that our approach achieves outperformance over search-based methods in
total execution time and computational efficiency, and also verifies the
generalization of our method to different numbers of objects. In addition, we
show the effectiveness of our method deployed on the real robot in the
supplementary video.
\\ ( https://arxiv.org/abs/2402.13634 ,  23214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13640 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:18:44 GMT   (182kb,D)

Title: Green AI: A Preliminary Empirical Study on Energy Consumption in DL
  Models Across Different Runtime Infrastructures
Authors: Negar Alizadeh and Fernando Castor
Categories: cs.SE cs.LG
DOI: 10.1145/3644815.3644967
\\
  Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime
infrastructures responsible for executing trained models on target hardware,
managing memory, data transfers, and multi-accelerator execution, if
applicable. Additionally, it is a common practice to deploy pre-trained models
on environments distinct from their native development settings. This led to
the introduction of interchange formats such as ONNX, which includes its
runtime infrastructure, and ONNX Runtime, which work as standard formats that
can be used across diverse DL frameworks and languages. Even though these
runtime infrastructures have a great impact on inference performance, no
previous paper has investigated their energy efficiency. In this study, we
monitor the energy consumption and inference time in the runtime
infrastructures of three well-known DL frameworks as well as ONNX, using three
various DL models. To have nuance in our investigation, we also examine the
impact of using different execution providers. We find out that the performance
and energy efficiency of DL are difficult to predict. One framework, MXNet,
outperforms both PyTorch and TensorFlow for the computer vision models using
batch size 1, due to efficient GPU usage and thus low CPU usage. However, batch
size 64 makes PyTorch and MXNet practically indistinguishable, while TensorFlow
is outperformed consistently. For BERT, PyTorch exhibits the best performance.
Converting the models to ONNX usually yields significant performance
improvements but the ONNX converted ResNet model with batch size 64 consumes
approximately 10% more energy and time than the original PyTorch model.
\\ ( https://arxiv.org/abs/2402.13640 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13646 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:27:44 GMT   (1086kb,D)

Title: A Large Dimensional Analysis of Multi-task Semi-Supervised Learning
Authors: Victor Leger, Romain Couillet
Categories: stat.ML cs.LG
Comments: 13 pages, 9 figures
\\
  This article conducts a large dimensional study of a simple yet quite
versatile classification model, encompassing at once multi-task and
semi-supervised learning, and taking into account uncertain labeling. Using
tools from random matrix theory, we characterize the asymptotics of some key
functionals, which allows us on the one hand to predict the performances of the
algorithm, and on the other hand to reveal some counter-intuitive guidance on
how to use it efficiently. The model, powerful enough to provide good
performance guarantees, is also straightforward enough to provide strong
insights into its behavior.
\\ ( https://arxiv.org/abs/2402.13646 ,  1086kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13651 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:37:17 GMT   (1164kb,D)

Title: Robustness of Deep Neural Networks for Micro-Doppler Radar
  Classification
Authors: Mikolaj Czerkawski and Carmine Clemente and Craig MichieCraig Michie
  and Christos Tachtatzis
Categories: cs.CV cs.LG eess.SP
Comments: DOI: 10.23919/IRS54158.2022.9905017
Journal-ref: Conference: International Radar Symposium 2022
DOI: 10.23919/IRS54158.2022.9905017
\\
  With the great capabilities of deep classifiers for radar data processing
come the risks of learning dataset-specific features that do not generalize
well. In this work, the robustness of two deep convolutional architectures,
trained and tested on the same data, is evaluated. When standard training
practice is followed, both classifiers exhibit sensitivity to subtle temporal
shifts of the input representation, an augmentation that carries minimal
semantic content. Furthermore, the models are extremely susceptible to
adversarial examples. Both small temporal shifts and adversarial examples are a
result of a model overfitting on features that do not generalize well. As a
remedy, it is shown that training on adversarial examples and temporally
augmented samples can reduce this effect and lead to models that generalise
better. Finally, models operating on cadence-velocity diagram representation
rather than Doppler-time are demonstrated to be naturally more immune to
adversarial examples.
\\ ( https://arxiv.org/abs/2402.13651 ,  1164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13654 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:40:26 GMT   (3299kb,D)

Title: Improving a Proportional Integral Controller with Reinforcement Learning
  on a Throttle Valve Benchmark
Authors: Paul Daoudi, Bojan Mavkov, Bogdan Robu, Christophe Prieur, Emmanuel
  Witrant, Merwan Barlier and Ludovic Dos Santos
Categories: eess.SY cs.LG cs.SY
\\
  This paper presents a learning-based control strategy for non-linear throttle
valves with an asymmetric hysteresis, leading to a near-optimal controller
without requiring any prior knowledge about the environment. We start with a
carefully tuned Proportional Integrator (PI) controller and exploit the recent
advances in Reinforcement Learning (RL) with Guides to improve the closed-loop
behavior by learning from the additional interactions with the valve. We test
the proposed control method in various scenarios on three different valves, all
highlighting the benefits of combining both PI and RL frameworks to improve
control performance in non-linear stochastic systems. In all the experimental
test cases, the resulting agent has a better sample efficiency than traditional
RL agents and outperforms the PI controller.
\\ ( https://arxiv.org/abs/2402.13654 ,  3299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13666 (*cross-listing*)
Date: Wed, 21 Feb 2024 09:56:56 GMT   (1151kb,D)

Title: Measurement Uncertainty: Relating the uncertainties of physical and
  virtual measurements
Authors: Simon Cramer, Tobias M\"uller and Robert H. Schmitt
Categories: stat.AP cs.LG math.PR
\\
  In the context of industrially mass-manufactured products, quality management
is based on physically inspecting a small sample from a large batch and
reasoning about the batch's quality conformance. When complementing physical
inspections with predictions from machine learning models, it is crucial that
the uncertainty of the prediction is known. Otherwise, the application of
established quality management concepts is not legitimate. Deterministic
(machine learning) models lack quantification of their predictive uncertainty
and are therefore unsuitable. Probabilistic (machine learning) models provide a
predictive uncertainty along with the prediction. However, a concise
relationship is missing between the measurement uncertainty of physical
inspections and the predictive uncertainty of probabilistic models in their
application in quality management. Here, we show how the predictive uncertainty
of probabilistic (machine learning) models is related to the measurement
uncertainty of physical inspections. This enables the use of probabilistic
models for virtual inspections and integrates them into existing quality
management concepts. Thus, we can provide a virtual measurement for any quality
characteristic based on the process data and achieve a 100 percent inspection
rate. In the field of Predictive Quality, the virtual measurement is of great
interest. Based on our results, physical inspections with a low sampling rate
can be accompanied by virtual measurements that allow an inspection rate of 100
percent. We add substantial value, especially to complex process chains, as
faulty products/parts are identified promptly and upcoming process steps can be
aborted.
\\ ( https://arxiv.org/abs/2402.13666 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13673 (*cross-listing*)
Date: Wed, 21 Feb 2024 10:17:23 GMT   (2987kb,D)

Title: Computing Transiting Exoplanet Parameters with 1D Convolutional Neural
  Networks
Authors: Santiago Iglesias \'Alvarez, Enrique D\'iez Alonso, Mar\'ia Luisa
  S\'anchez Rodr\'iguez, Javier Rodr\'iguez Rodr\'iguez, Sa\'ul P\'erez
  Fern\'andez and Francisco Javier de Cos Juez
Categories: astro-ph.EP astro-ph.IM cs.LG
DOI: 10.3390/axioms13020083
\\
  The transit method allows the detection and characterization of planetary
systems by analyzing stellar light curves. Convolutional neural networks appear
to offer a viable solution for automating these analyses. In this research, two
1D convolutional neural network models, which work with simulated light curves
in which transit-like signals were injected, are presented. One model operates
on complete light curves and estimates the orbital period, and the other one
operates on phase-folded light curves and estimates the semimajor axis of the
orbit and the square of the planet-to-star radius ratio. Both models were
tested on real data from TESS light curves with confirmed planets to ensure
that they are able to work with real data. The results obtained show that 1D
CNNs are able to characterize transiting exoplanets from their host star's
detrended light curve and, furthermore, reducing both the required time and
computational costs compared with the current detection and characterization
algorithms.
\\ ( https://arxiv.org/abs/2402.13673 ,  2987kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13699 (*cross-listing*)
Date: Wed, 21 Feb 2024 11:00:23 GMT   (3494kb,D)

Title: Explainable Classification Techniques for Quantum Dot Device
  Measurements
Authors: Daniel Schug, Tyler J. Kovach, M. A. Wolfe, Jared Benson, Sanghyeok
  Park, J. P. Dodson, J. Corrigan, M. A. Eriksson, Justyna P. Zwolak
Categories: cs.CV cond-mat.mes-hall cs.LG
Comments: 5 pages, 3 figures
Journal-ref: Proceedings of the XAI4Sci: Explainable machine learning for
  sciences workshop at AAAI 2024, Vancouver, Canada
\\
  In the physical sciences, there is an increased need for robust feature
representations of image data: image acquisition, in the generalized sense of
two-dimensional data, is now widespread across a large number of fields,
including quantum information science, which we consider here. While
traditional image features are widely utilized in such cases, their use is
rapidly being supplanted by Neural Network-based techniques that often
sacrifice explainability in exchange for high accuracy. To ameliorate this
trade-off, we propose a synthetic data-based technique that results in
explainable features. We show, using Explainable Boosting Machines (EBMs), that
this method offers superior explainability without sacrificing accuracy.
Specifically, we show that there is a meaningful benefit to this technique in
the context of quantum dot tuning, where human intervention is necessary at the
current stage of development.
\\ ( https://arxiv.org/abs/2402.13699 ,  3494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13723 (*cross-listing*)
Date: Wed, 21 Feb 2024 11:35:19 GMT   (694kb,D)

Title: The Effect of Batch Size on Contrastive Self-Supervised Speech
  Representation Learning
Authors: Nik Vaessen, David A. van Leeuwen
Categories: cs.SD cs.LG eess.AS
\\
  Foundation models in speech are often trained using many GPUs, which
implicitly leads to large effective batch sizes. In this paper we study the
effect of batch size on pre-training, both in terms of statistics that can be
monitored during training, and in the effect on the performance of a downstream
fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes
of speech we show that, for a fixed amount of iterations, larger batch sizes
result in better pre-trained models. However, there is lower limit for
stability, and an upper limit for effectiveness. We then show that the quality
of the pre-trained model depends mainly on the amount of speech data seen
during training, i.e., on the product of batch size and number of iterations.
All results are produced with an independent implementation of the wav2vec 2.0
architecture, which to a large extent reproduces the results of the original
work (arXiv:2006.11477). Our extensions can help researchers choose effective
operating conditions when studying self-supervised learning in speech, and
hints towards benchmarking self-supervision with a fixed amount of seen data.
Code and model checkpoints are available at
https://github.com/nikvaessen/w2v2-batch-size.
\\ ( https://arxiv.org/abs/2402.13723 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13776 (*cross-listing*)
Date: Wed, 21 Feb 2024 12:54:40 GMT   (3629kb,D)

Title: Cas-DiffCom: Cascaded diffusion model for infant longitudinal
  super-resolution 3D medical image completion
Authors: Lianghu Guo, Tianli Tao, Xinyi Cai, Zihao Zhu, Jiawei Huang, Lixuan
  Zhu, Zhuoyang Gu, Haifeng Tang, Rui Zhou, Siyan Han, Yan Liang, Qing Yang,
  Dinggang Shen, Han Zhang
Categories: eess.IV cs.CV cs.LG
\\
  Early infancy is a rapid and dynamic neurodevelopmental period for behavior
and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an
effective tool to investigate such a crucial stage by capturing the
developmental trajectories of the brain structures. However, longitudinal MRI
acquisition always meets a serious data-missing problem due to participant
dropout and failed scans, making longitudinal infant brain atlas construction
and developmental trajectory delineation quite challenging. Thanks to the
development of an AI-based generative model, neuroimage completion has become a
powerful technique to retain as much available data as possible. However,
current image completion methods usually suffer from inconsistency within each
individual subject in the time dimension, compromising the overall quality. To
solve this problem, our paper proposed a two-stage cascaded diffusion model,
Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and
super-resolution. We applied our proposed method to the Baby Connectome Project
(BCP) dataset. The experiment results validate that Cas-DiffCom achieves both
individual consistency and high fidelity in longitudinal infant brain image
completion. We further applied the generated infant brain images to two
downstream tasks, brain tissue segmentation and developmental trajectory
delineation, to declare its task-oriented potential in the neuroscience field.
\\ ( https://arxiv.org/abs/2402.13776 ,  3629kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13861 (*cross-listing*)
Date: Wed, 21 Feb 2024 15:10:20 GMT   (2839kb,D)

Title: Improving Efficiency of Iso-Surface Extraction on Implicit Neural
  Representations Using Uncertainty Propagation
Authors: Haoyu Li and Han-Wei Shen
Categories: cs.GR cs.LG
Comments: Accepted to IEEE Transactions on Visualization and Computer Graphics,
  presented in VIS 2024
DOI: 10.1109/TVCG.2024.3365089
\\
  Implicit Neural representations (INRs) are widely used for scientific data
reduction and visualization by modeling the function that maps a spatial
location to a data value. Without any prior knowledge about the spatial
distribution of values, we are forced to sample densely from INRs to perform
visualization tasks like iso-surface extraction which can be very
computationally expensive. Recently, range analysis has shown promising results
in improving the efficiency of geometric queries, such as ray casting and
hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic
rules to bound the output range of the network within a spatial region.
However, the analysis bounds are often too conservative for complex scientific
data. In this paper, we present an improved technique for range analysis by
revisiting the arithmetic rules and analyzing the probability distribution of
the network output within a spatial region. We model this distribution
efficiently as a Gaussian distribution by applying the central limit theorem.
Excluding low probability values, we are able to tighten the output bounds,
resulting in a more accurate estimation of the value range, and hence more
accurate identification of iso-surface cells and more efficient iso-surface
extraction on INRs. Our approach demonstrates superior performance in terms of
the iso-surface extraction time on four datasets compared to the original range
analysis method and can also be generalized to other geometric query tasks.
\\ ( https://arxiv.org/abs/2402.13861 ,  2839kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13867 (*cross-listing*)
Date: Wed, 21 Feb 2024 15:19:09 GMT   (9075kb,D)

Title: RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency
  interference -- Application to pulsar observations
Authors: Xiao Zhang, Isma\"el Cognard and Nicolas Dobigeon
Categories: astro-ph.IM cs.LG eess.IV
\\
  Radio frequency interference (RFI) have been an enduring concern in radio
astronomy, particularly for the observations of pulsars which require high
timing precision and data sensitivity. In most works of the literature, RFI
mitigation has been formulated as a detection task that consists of localizing
possible RFI in dynamic spectra. This strategy inevitably leads to a potential
loss of information since parts of the signal identified as possibly
RFI-corrupted are generally not considered in the subsequent data processing
pipeline. Conversely, this work proposes to tackle RFI mitigation as a joint
detection and restoration that allows parts of the dynamic spectrum affected by
RFI to be not only identified but also recovered. The proposed supervised
method relies on a deep convolutional network whose architecture inherits the
performance reached by a recent yet popular image-denoising network. To train
this network, a whole simulation framework is built to generate large data sets
according to physics-inspired and statistical models of the pulsar signals and
of the RFI. The relevance of the proposed approach is quantitatively assessed
by conducting extensive experiments. In particular, the results show that the
restored dynamic spectra are sufficiently reliable to estimate pulsar
times-of-arrivals with an accuracy close to the one that would be obtained from
RFI-free signals.
\\ ( https://arxiv.org/abs/2402.13867 ,  9075kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13918 (*cross-listing*)
Date: Wed, 21 Feb 2024 16:32:43 GMT   (6426kb,D)

Title: BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for
  Cloud Detection and Segmentation in Remote Sensing Imagery
Authors: Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane
Categories: cs.CV cs.LG eess.IV
Comments: Submitted to Expert Systems and Applications
\\
  Satellites equipped with optical sensors capture high-resolution imagery,
providing valuable insights into various environmental phenomena. In recent
years, there has been a surge of research focused on addressing some challenges
in remote sensing, ranging from water detection in diverse landscapes to the
segmentation of mountainous and terrains. Ongoing investigations goals to
enhance the precision and efficiency of satellite imagery analysis. Especially,
there is a growing emphasis on developing methodologies for accurate water body
detection, snow and clouds, important for environmental monitoring, resource
management, and disaster response. Within this context, this paper focus on the
cloud segmentation from remote sensing imagery. Accurate remote sensing data
analysis can be challenging due to the presence of clouds in optical
sensor-based applications. The quality of resulting products such as
applications and research is directly impacted by cloud detection, which plays
a key role in the remote sensing data processing pipeline. This paper examines
seven cutting-edge semantic segmentation and detection algorithms applied to
clouds identification, conducting a benchmark analysis to evaluate their
architectural approaches and identify the most performing ones. To increase the
model's adaptability, critical elements including the type of imagery and the
amount of spectral bands used during training are analyzed. Additionally, this
research tries to produce machine learning algorithms that can perform cloud
segmentation using only a few spectral bands, including RGB and RGBN-IR
combinations. The model's flexibility for a variety of applications and user
scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as
datasets. This benchmark can be reproduced using the material from this github
link: \url{https://github.com/toelt-llc/cloud\_segmentation\_comparative}.
\\ ( https://arxiv.org/abs/2402.13918 ,  6426kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13937 (*cross-listing*)
Date: Wed, 21 Feb 2024 17:05:27 GMT   (462kb,D)

Title: Verifying message-passing neural networks via topology-based bounds
  tightening
Authors: Christopher Hojny, Shiqiang Zhang, Juan S. Campos, Ruth Misener
Categories: math.OC cs.LG
\\
  Since graph neural networks (GNNs) are often vulnerable to attack, we need to
know when we can trust them. We develop a computationally effective approach
towards providing robust certificates for message-passing neural networks
(MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our
work builds on mixed-integer optimization, it encodes a wide variety of
subproblems, for example it admits (i) both adding and removing edges, (ii)
both global and local budgets, and (iii) both topological perturbations and
feature modifications. Our key technology, topology-based bounds tightening,
uses graph structure to tighten bounds. We also experiment with aggressive
bounds tightening to dynamically change the optimization constraints by
tightening variable bounds. To demonstrate the effectiveness of these
strategies, we implement an extension to the open-source branch-and-cut solver
SCIP. We test on both node and graph classification problems and consider
topological attacks that both add and remove edges.
\\ ( https://arxiv.org/abs/2402.13937 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13957 (*cross-listing*)
Date: Wed, 21 Feb 2024 17:37:30 GMT   (474kb)

Title: Advancing Audio Fingerprinting Accuracy Addressing Background Noise and
  Distortion Challenges
Authors: Navin Kamuni, Sathishkumar Chintala, Naveen Kunchakuri, Jyothi Swaroop
  Arlagadda Narasimharaju, Venkat Kumar
Categories: cs.SD cs.LG eess.AS
\\
  Audio fingerprinting, exemplified by pioneers like Shazam, has transformed
digital audio recognition. However, existing systems struggle with accuracy in
challenging conditions, limiting broad applicability. This research proposes an
AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built
on the Dejavu Project's foundations, the study emphasizes real-world scenario
simulations with diverse background noises and distortions. Signal processing,
central to Dejavu's model, includes the Fast Fourier Transform, spectrograms,
and peak extraction. The "constellation" concept and fingerprint hashing enable
unique song identification. Performance evaluation attests to 100% accuracy
within a 5-second audio input, with a system showcasing predictable matching
speed for efficiency. Storage analysis highlights the critical space-speed
trade-off for practical implementation. This research advances audio
fingerprinting's adaptability, addressing challenges in varied environments and
applications.
\\ ( https://arxiv.org/abs/2402.13957 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13973 (*cross-listing*)
Date: Wed, 21 Feb 2024 17:58:10 GMT   (950kb,D)

Title: Linear-Time Graph Neural Networks for Scalable Recommendations
Authors: Jiahao Zhang, Rui Xue, Wenqi Fan, Xin Xu, Qing Li, Jian Pei, Xiaorui
  Liu
Categories: cs.IR cs.LG
Comments: 12 pages, 5 figures, accepted by The Web Conference 2024
\\
  In an era of information explosion, recommender systems are vital tools to
deliver personalized recommendations for users. The key of recommender systems
is to forecast users' future behaviors based on previous user-item
interactions. Due to their strong expressive power of capturing high-order
connectivities in user-item interaction data, recent years have witnessed a
rising interest in leveraging Graph Neural Networks (GNNs) to boost the
prediction performance of recommender systems. Nonetheless, classic Matrix
Factorization (MF) and Deep Neural Network (DNN) approaches still play an
important role in real-world large-scale recommender systems due to their
scalability advantages. Despite the existence of GNN-acceleration solutions, it
remains an open question whether GNN-based recommender systems can scale as
efficiently as classic MF and DNN methods. In this paper, we propose a
Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender
systems to achieve comparable scalability as classic MF approaches while
maintaining GNNs' powerful expressiveness for superior prediction accuracy.
Extensive experiments and ablation studies are presented to validate the
effectiveness and scalability of the proposed algorithm. Our implementation
based on PyTorch is available.
\\ ( https://arxiv.org/abs/2402.13973 ,  950kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13999 (*cross-listing*)
Date: Wed, 21 Feb 2024 18:35:27 GMT   (90kb,D)

Title: Asymptotics of Learning with Deep Structured (Random) Features
Authors: Dominik Schr\"oder, Daniil Dmitriev, Hugo Cui, Bruno Loureiro
Categories: stat.ML cond-mat.dis-nn cs.LG math.ST stat.TH
\\
  For a large class of feature maps we provide a tight asymptotic
characterisation of the test error associated with learning the readout layer,
in the high-dimensional limit where the input dimension, hidden layer widths,
and number of training samples are proportionally large. This characterization
is formulated in terms of the population covariance of the features. Our work
is partially motivated by the problem of learning with Gaussian rainbow neural
networks, namely deep non-linear fully-connected networks with random but
structured weights, whose row-wise covariances are further allowed to depend on
the weights of previous layers. For such networks we also derive a closed-form
formula for the feature covariance in terms of the weight matrices. We further
find that in some cases our results can capture feature maps learned by deep,
finite-width neural networks trained under gradient descent.
\\ ( https://arxiv.org/abs/2402.13999 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14012 (*cross-listing*)
Date: Wed, 21 Feb 2024 18:51:42 GMT   (220kb,D)

Title: Chasing Convex Functions with Long-term Constraints
Authors: Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad
  Hajiesmaili, Adam Wierman, Prashant Shenoy
Categories: cs.DS cs.LG
Comments: 35 pages, 11 figures
\\
  We introduce and study a family of online metric problems with long-term
constraints. In these problems, an online player makes decisions $\mathbf{x}_t$
in a metric space $(X,d)$ to simultaneously minimize their hitting cost
$f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over the
time horizon $T$, the player must satisfy a long-term demand constraint
$\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fraction
of demand satisfied at time $t$. Such problems can find a wide array of
applications to online resource allocation in sustainable energy and computing
systems. We devise optimal competitive and learning-augmented algorithms for
specific instantiations of these problems, and further show that our proposed
algorithms perform well in numerical experiments.
\\ ( https://arxiv.org/abs/2402.14012 ,  220kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2012.01410
replaced with revised version Tue, 20 Feb 2024 21:37:17 GMT   (619kb,D)

Title: Ontological Smart Contracts in OASIS: Ontology for Agents, Systems, and
  Integration of Services (Extended Version)
Authors: Domenico Cantone, Carmelo Fabio Longo, Marianna Nicolosi-Asmundo,
  Daniele Francesco Santamaria, Corrado Santoro
Categories: cs.AI
Comments: Please cite
  https://www.scopus.com/record/display.uri?eid=2-s2.0-85130258663&origin=resultslist
Report-no: Chapter 22, pp. 237--247
Journal-ref: Intelligent Distributed Computing XIV, Studies in Computational
  Intelligence 1026, 2021
DOI: 10.1007/978-3-030-96627-0_22
\\ ( https://arxiv.org/abs/2012.01410 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10061
replaced with revised version Tue, 20 Feb 2024 21:40:20 GMT   (347kb,D)

Title: The Ontology for Agents, Systems and Integration of Services: OASIS
  version 2
Authors: Giampaolo Bella, Domenico Cantone, Carmelo Fabio Longo, Marianna
  Nicolosi-Asmundo and Daniele Francesco Santamaria
Categories: cs.AI cs.LO
Comments: Please cite
  https://www.scopus.com/record/display.uri?eid=2-s2.0-85165473819&origin=resultslist
Journal-ref: Intelligenza Artificiale, Vol. 17, no 1, pp. 51-62, 2023
DOI: 10.3233/IA-230002
\\ ( https://arxiv.org/abs/2306.10061 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05209
replaced with revised version Wed, 21 Feb 2024 01:06:35 GMT   (597kb,D)

Title: Contextual Pre-planning on Reward Machine Abstractions for Enhanced
  Transfer in Deep Reinforcement Learning
Authors: Guy Azran, Mohamad H. Danesh, Stefano V. Albrecht, Sarah Keren
Categories: cs.AI cs.LG
Comments: Proceedings of the 38th AAAI Conference on Artificial Intelligence
  (AAAI), 2024
\\ ( https://arxiv.org/abs/2307.05209 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06147
replaced with revised version Wed, 21 Feb 2024 02:25:32 GMT   (208kb,D)

Title: DeAL: Decoding-time Alignment for Large Language Models
Authors: James Y. Huang, Sailik Sengupta, Daniele Bonadiman, Yi-an Lai, Arshit
  Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchhoff, Dan Roth
Categories: cs.AI cs.CL
Comments: The appendix contains data that is offensive / disturbing in nature
\\ ( https://arxiv.org/abs/2402.06147 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09056
replaced with revised version Tue, 20 Feb 2024 21:59:39 GMT   (7836kb,D)

Title: Is Epistemic Uncertainty Faithfully Represented by Evidential Deep
  Learning Methods?
Authors: Mira J\"urgens, Nis Meinert, Viktor Bengs, Eyke H\"ullermeier, Willem
  Waegeman
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.09056 ,  7836kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09660
replaced with revised version Tue, 20 Feb 2024 23:43:20 GMT   (200kb,D)

Title: User Modeling and User Profiling: A Comprehensive Survey
Authors: Erasmo Purificato (1), Ludovico Boratto (2), and Ernesto William De
  Luca (1) ((1) Otto von Guericke University Magdeburg, Germany, (2) University
  of Cagliari, Italy)
Categories: cs.AI cs.HC cs.IR cs.LG cs.SI
Comments: 71 pages
ACM-class: I.2
\\ ( https://arxiv.org/abs/2402.09660 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09764
replaced with revised version Wed, 21 Feb 2024 07:56:28 GMT   (1794kb,D)

Title: Aligning Crowd Feedback via Distributional Preference Reward Modeling
Authors: Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang,
  Yong Liu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.09764 ,  1794kb)
------------------------------------------------------------------------------
\\
arXiv:2110.08486
replaced with revised version Tue, 20 Feb 2024 22:22:00 GMT   (8832kb,D)

Title: Understanding Multimodal Procedural Knowledge by Sequencing Multimodal
  Instructional Manuals
Authors: Te-Lin Wu, Alex Spangher, Pegah Alipoormolabashi, Marjorie Freedman,
  Ralph Weischedel, Nanyun Peng
Categories: cs.CL cs.CV
Comments: In Proceedings of the Conference of the 60th Annual Meeting of the
  Association for Computational Linguistics (ACL), 2022
\\ ( https://arxiv.org/abs/2110.08486 ,  8832kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09852
replaced with revised version Wed, 21 Feb 2024 17:47:37 GMT   (656kb,D)

Title: Unsupervised Layer-wise Score Aggregation for Textual OOD Detection
Authors: Maxime Darrin, Guillaume Staerman, Eduardo Dadalto C\^amara Gomes,
  Jackie CK Cheung, Pablo Piantanida, Pierre Colombo
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2302.09852 ,  656kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05352
replaced with revised version Wed, 21 Feb 2024 12:07:30 GMT   (718kb,D)

Title: Extracting Accurate Materials Data from Research Papers with
  Conversational Language Models and Prompt Engineering
Authors: Maciej P. Polak, Dane Morgan
Categories: cs.CL cond-mat.mtrl-sci
Comments: 7 pages, 3 figures, 1 table
Journal-ref: Nature Communications (2024) 15:1569
DOI: 10.1038/s41467-024-45914-8
\\ ( https://arxiv.org/abs/2303.05352 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13809
replaced with revised version Wed, 21 Feb 2024 04:18:32 GMT   (4167kb,D)

Title: Error Analysis Prompting Enables Human-Like Translation Evaluation in
  Large Language Models
Authors: Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, Dacheng
  Tao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2303.13809 ,  4167kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08379
replaced with revised version Wed, 21 Feb 2024 00:06:20 GMT   (7446kb,D)

Title: TESS: Text-to-Text Self-Conditioned Simplex Diffusion
Authors: Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson,
  Iz Beltagy, Matthew E. Peters, Arman Cohan
Categories: cs.CL cs.LG
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2305.08379 ,  7446kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11527
replaced with revised version Wed, 21 Feb 2024 16:52:52 GMT   (4234kb,D)

Title: InstructIE: A Bilingual Instruction-based Information Extraction Dataset
Authors: Honghao Gui, Shuofei Qiao, Jintian Zhang, Hongbin Ye, Mengshu Sun, Lei
  Liang, Huajun Chen, Ningyu Zhang
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Work in progress; project homepage:
  https://www.zjukg.org/project/InstructIE/ dataset:
  https://huggingface.co/datasets/zjunlp/InstructIE
\\ ( https://arxiv.org/abs/2305.11527 ,  4234kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11738
replaced with revised version Wed, 21 Feb 2024 12:59:21 GMT   (653kb,D)

Title: CRITIC: Large Language Models Can Self-Correct with Tool-Interactive
  Critiquing
Authors: Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan
  Duan, Weizhu Chen
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.11738 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13269
replaced with revised version Wed, 21 Feb 2024 07:44:48 GMT   (2925kb,D)

Title: Chain-of-Knowledge: Grounding Large Language Models via Dynamic
  Knowledge Adapting over Heterogeneous Sources
Authors: Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty,
  Soujanya Poria, Lidong Bing
Categories: cs.CL
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2305.13269 ,  2925kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13718
replaced with revised version Wed, 21 Feb 2024 04:19:01 GMT   (10552kb,D)

Title: LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large
  Language Models
Authors: Fangkai Jiao, Zhiyang Teng, Bosheng Ding, Zhengyuan Liu, Nancy F.
  Chen, Shafiq Joty
Categories: cs.CL
Comments: 11 pages
\\ ( https://arxiv.org/abs/2305.13718 ,  10552kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15090
replaced with revised version Tue, 20 Feb 2024 20:00:21 GMT   (6518kb,D)

Title: STAR: Boosting Low-Resource Information Extraction by Structure-to-Text
  Data Generation with Large Language Models
Authors: Mingyu Derek Ma, Xiaoxuan Wang, Po-Nien Kung, P. Jeffrey Brantingham,
  Nanyun Peng, Wei Wang
Categories: cs.CL cs.AI
Comments: To appear at AAAI'24. More info is at https://derek.ma/STAR
\\ ( https://arxiv.org/abs/2305.15090 ,  6518kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01391
replaced with revised version Wed, 21 Feb 2024 07:24:06 GMT   (398kb)

Title: Optimizing Machine Translation through Prompt Engineering: An
  Investigation into ChatGPT's Customizability
Authors: Masaru Yamada
Categories: cs.CL
Journal-ref: Proceedings of Machine Translation Summit XIX, Vol. 2: Users
  Track, pages 195-204, September 4-8, 2023, Macau SAR, China
\\ ( https://arxiv.org/abs/2308.01391 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09073
replaced with revised version Wed, 21 Feb 2024 16:35:29 GMT   (5806kb,D)

Title: mCL-NER: Cross-Lingual Named Entity Recognition via Multi-view
  Contrastive Learning
Authors: Ying Mo, Jian Yang, Jiahao Liu, Qifan Wang, Ruoyu Chen, Jingang Wang,
  Zhoujun Li
Categories: cs.CL
Comments: 9 pages, Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2308.09073 ,  5806kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10149
replaced with revised version Wed, 21 Feb 2024 13:52:11 GMT   (4915kb,D)

Title: A Survey on Fairness in Large Language Models
Authors: Yingji Li, Mengnan Du, Rui Song, Xin Wang, Ying Wang
Categories: cs.CL cs.AI
Comments: 28 pages, 5 figures, 2 tables, 175 references
\\ ( https://arxiv.org/abs/2308.10149 ,  4915kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15122
replaced with revised version Wed, 21 Feb 2024 13:20:21 GMT   (721kb,D)

Title: SpikeBERT: A Language Spikformer Learned from BERT with Knowledge
  Distillation
Authors: Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan
  Zhang, Xiaoqing Zheng, Xuanjing Huang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.15122 ,  721kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06365
replaced with revised version Tue, 20 Feb 2024 23:31:22 GMT   (392kb,D)

Title: Cited Text Spans for Citation Text Generation
Authors: Xiangci Li, Yi-Hui Lee, Jessica Ouyang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.06365 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08172
replaced with revised version Wed, 21 Feb 2024 17:42:32 GMT   (9841kb,D)

Title: LASER: LLM Agent with State-Space Exploration for Web Navigation
Authors: Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, Wenhao Yu, Dong
  Yu
Categories: cs.CL
Comments: 4 pages, 2 figures
\\ ( https://arxiv.org/abs/2309.08172 ,  9841kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09530
replaced with revised version Wed, 21 Feb 2024 05:45:00 GMT   (555kb,D)

Title: Adapting Large Language Models via Reading Comprehension
Authors: Daixuan Cheng, Shaohan Huang, Furu Wei
Categories: cs.CL
Comments: ICLR 2024 Conference
\\ ( https://arxiv.org/abs/2309.09530 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09697
replaced with revised version Wed, 21 Feb 2024 16:54:35 GMT   (1795kb,D)

Title: Evaluating Gender Bias of Pre-trained Language Models in Natural
  Language Inference by Considering All Labels
Authors: Panatchakorn Anantaprayoon, Masahiro Kaneko, Naoaki Okazaki
Categories: cs.CL
Comments: to be published in LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.09697 ,  1795kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10400
replaced with revised version Wed, 21 Feb 2024 13:37:07 GMT   (674kb,D)

Title: PoSE: Efficient Context Window Extension of LLMs via Positional
  Skip-wise Training
Authors: Dawei Zhu and Nan Yang and Liang Wang and Yifan Song and Wenhao Wu and
  Furu Wei and Sujian Li
Categories: cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2309.10400 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10687
replaced with revised version Tue, 20 Feb 2024 20:07:16 GMT   (1652kb,D)

Title: EchoPrompt: Instructing the Model to Rephrase Queries for Improved
  In-context Learning
Authors: Rajasekhar Reddy Mekala, Yasaman Razeghi, Sameer Singh
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.10687 ,  1652kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17272
replaced with revised version Wed, 21 Feb 2024 04:32:13 GMT   (2480kb,D)

Title: Enhancing Large Language Models in Coding Through Multi-Perspective
  Self-Consistency
Authors: Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, Nan Duan
Categories: cs.CL cs.AI cs.SE
Comments: Preprint version
\\ ( https://arxiv.org/abs/2309.17272 ,  2480kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17452
replaced with revised version Wed, 21 Feb 2024 12:59:22 GMT   (469kb,D)

Title: ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving
Authors: Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie
  Huang, Nan Duan, Weizhu Chen
Categories: cs.CL cs.AI
Comments: ICLR 2024; First two authors equal contribution
\\ ( https://arxiv.org/abs/2309.17452 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00297
replaced with revised version Wed, 21 Feb 2024 09:21:52 GMT   (20117kb,D)

Title: Understanding In-Context Learning from Repetitions
Authors: Jianhao Yan, Jin Xu, Chiyu Song, Chenming Wu, Yafu Li, Yue Zhang
Categories: cs.CL
Comments: Accepted by ICLR 2024. Updated with new experiments and results
\\ ( https://arxiv.org/abs/2310.00297 ,  20117kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01468
replaced with revised version Tue, 20 Feb 2024 21:24:43 GMT   (5219kb,D)

Title: Probing the Multi-turn Planning Capabilities of LLMs via 20 Question
  Games
Authors: Yizhe Zhang, Jiarui Lu, Navdeep Jaitly
Categories: cs.CL cs.AI cs.HC cs.LG
Comments: 24 pages
\\ ( https://arxiv.org/abs/2310.01468 ,  5219kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03668
replaced with revised version Wed, 21 Feb 2024 15:51:58 GMT   (9832kb,D)

Title: GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction
Authors: Oscar Sainz, Iker Garc\'ia-Ferrero, Rodrigo Agerri, Oier Lopez de
  Lacalle, German Rigau, Eneko Agirre
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.03668 ,  9832kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08041
replaced with revised version Wed, 21 Feb 2024 06:40:49 GMT   (1656kb,D)

Title: QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large
  Language Models
Authors: Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan
  Zhuang
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.08041 ,  1656kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19531
replaced with revised version Wed, 21 Feb 2024 01:58:36 GMT   (8302kb,D)

Title: MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties
  in Generative Language Models
Authors: Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei
  Zhou, Songlin Hu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.19531 ,  8302kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01200
replaced with revised version Wed, 21 Feb 2024 13:21:45 GMT   (728kb,D)

Title: Continual Learning Under Language Shift
Authors: Evangelia Gogoulou, Timoth\'ee Lesort, Magnus Boman, Joakim Nivre
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.01200 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07092
replaced with revised version Wed, 21 Feb 2024 06:53:12 GMT   (8223kb,D)

Title: To Tell The Truth: Language of Deception and Language Models
Authors: Sanchaita Hazra, Bodhisattwa Prasad Majumder
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.07092 ,  8223kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07215
replaced with revised version Wed, 21 Feb 2024 13:46:50 GMT   (3979kb,D)

Title: Coffee: Boost Your Code LLMs by Fixing Bugs with Feedback
Authors: Seungjun Moon, Yongho Song, Hyungjoo Chae, Dongjin Kang, Taeyoon Kwon,
  Kai Tzu-iunn Ong, Seung-won Hwang, Jinyoung Yeo
Categories: cs.CL cs.SE
Comments: Work in progress
\\ ( https://arxiv.org/abs/2311.07215 ,  3979kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07945
replaced with revised version Tue, 20 Feb 2024 21:25:30 GMT   (57kb,D)

Title: Well begun is half done: Importance of Starting Right in Multi-Step Math
  Reasoning
Authors: Kushal Jain, Niket Tandon, Kumar Shridhar
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.07945 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08596
replaced with revised version Wed, 21 Feb 2024 18:15:47 GMT   (430kb,D)

Title: Are You Sure? Challenging LLMs Leads to Performance Drops in The
  FlipFlop Experiment
Authors: Philippe Laban and Lidiya Murakhovs'ka and Caiming Xiong and
  Chien-Sheng Wu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08596 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08648
replaced with revised version Wed, 21 Feb 2024 03:16:26 GMT   (7124kb,D)

Title: Explore Spurious Correlations at the Concept Level in Language Models
  for Text Classification
Authors: Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang
Categories: cs.CL cs.AI
Comments: 14 pages, 4 page appendix
\\ ( https://arxiv.org/abs/2311.08648 ,  7124kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10928
replaced with revised version Tue, 20 Feb 2024 20:09:33 GMT   (2541kb,D)

Title: CAMRA: Copilot for AMR Annotation
Authors: Jon Z. Cai, Shafiuddin Rehan Ahmed, Julia Bonn, Kristin
  Wright-Bettner, Martha Palmer, James H. Martin
Categories: cs.CL cs.AI
Comments: EMNLP 2023 System Demonstration
\\ ( https://arxiv.org/abs/2311.10928 ,  2541kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07250
replaced with revised version Wed, 21 Feb 2024 12:08:47 GMT   (3551kb,D)

Title: Neural Machine Translation of Clinical Text: An Empirical Investigation
  into Multilingual Pre-Trained Language Models and Transfer-Learning
Authors: Lifeng Han, Serge Gladkoff, Gleb Erofeev, Irina Sorokina, Betty
  Galiano, Goran Nenadic
Categories: cs.CL cs.AI
Comments: Accepted by Frontiers in Digital Health - Health Informatics
\\ ( https://arxiv.org/abs/2312.07250 ,  3551kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04898
replaced with revised version Wed, 21 Feb 2024 06:44:37 GMT   (397kb,D)

Title: ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language
  Models In Chinese Domain
Authors: Bingchao Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.04898 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05930
replaced with revised version Wed, 21 Feb 2024 05:43:52 GMT   (8315kb,D)

Title: SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully
Authors: Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.05930 ,  8315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06233
replaced with revised version Wed, 21 Feb 2024 06:40:33 GMT   (8451kb,D)

Title: LEGOBench: Scientific Leaderboard Generation Benchmark
Authors: Shruti Singh, Shoaib Alam, Husain Malwat and Mayank Singh
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06233 ,  8451kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06561
replaced with revised version Wed, 21 Feb 2024 08:25:08 GMT   (9364kb,D)

Title: Intention Analysis Makes LLMs A Good Jailbreak Defender
Authors: Yuqi Zhang and Liang Ding and Lefei Zhang and Dacheng Tao
Categories: cs.CL
Comments: 17 pages, 12 figures
\\ ( https://arxiv.org/abs/2401.06561 ,  9364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06628
replaced with revised version Wed, 21 Feb 2024 06:18:16 GMT   (8791kb,D)

Title: OOP: Object-Oriented Programming Evaluation Benchmark for Large Language
  Models
Authors: Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, Dacheng Tao
Categories: cs.CL
Comments: 20 pages, 15 figures
\\ ( https://arxiv.org/abs/2401.06628 ,  8791kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06836
replaced with revised version Wed, 21 Feb 2024 15:13:50 GMT   (10783kb,D)

Title: Enhancing Emotional Generation Capability of Large Language Models via
  Emotional Chain-of-Thought
Authors: Zaijing Li, Gongwei Chen, Rui Shao, Dongmei Jiang, and Liqiang Nie
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.06836 ,  10783kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06954
replaced with revised version Tue, 20 Feb 2024 21:11:23 GMT   (8503kb,D)

Title: Bridging the Preference Gap between Retrievers and LLMs
Authors: Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei and
  Michael Bendersky
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06954 ,  8503kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13170
replaced with revised version Tue, 20 Feb 2024 19:37:18 GMT   (0kb,I)

Title: CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert
  Judgments For Open-Domain Question Answering
Authors: Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, and Jordan
  Boyd-Graber
Categories: cs.CL
Comments: Another paper with almost duplicate and identical contents already
  exist. We do not want to keep duplicate copies. See arXiv:2402.11161
\\ ( https://arxiv.org/abs/2401.13170 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02695
replaced with revised version Wed, 21 Feb 2024 00:09:00 GMT   (3552kb,D)

Title: Exploiting Class Probabilities for Black-box Sentence-level Attacks
Authors: Raha Moraffah and Huan Liu
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: EACL 2024 Findings
\\ ( https://arxiv.org/abs/2402.02695 ,  3552kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08113
replaced with revised version Tue, 20 Feb 2024 23:45:43 GMT   (20246kb,D)

Title: Addressing cognitive bias in medical language models
Authors: Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur
  Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, Rama
  Chellappa
Categories: cs.CL cs.HC
\\ ( https://arxiv.org/abs/2402.08113 ,  20246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08382
replaced with revised version Wed, 21 Feb 2024 08:35:57 GMT   (174kb,D)

Title: Punctuation Restoration Improves Structure Understanding without
  Supervision
Authors: Junghyun Min, Minho Lee, Woochul Lee, Yeonsoo Lee
Categories: cs.CL
Comments: 10 pages, 1 figure, 6 tables
\\ ( https://arxiv.org/abs/2402.08382 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09742
replaced with revised version Wed, 21 Feb 2024 08:25:25 GMT   (9452kb,D)

Title: AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern
  Doctors for Clinical Diagnosis
Authors: Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi,
  Fei Huang, Jingren Zhou
Categories: cs.CL
Comments: https://github.com/LibertFan/AI_Hospital
\\ ( https://arxiv.org/abs/2402.09742 ,  9452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10567
replaced with revised version Wed, 21 Feb 2024 05:16:49 GMT   (2276kb,D)

Title: InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs
  ready for the Indian Legal Domain?
Authors: Yogesh Tripathi, Raghav Donakanti, Sahil Girhepuje, Ishan Kavathekar,
  Bhaskara Hanuma Vedula, Gokul S Krishnan, Shreya Goyal, Anmol Goel, Balaraman
  Ravindran, Ponnurangam Kumaraguru
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.10567 ,  2276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10790
replaced with revised version Wed, 21 Feb 2024 03:07:42 GMT   (7140kb,D)

Title: In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs
  Miss
Authors: Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom
  Sorokin, Mikhail Burtsev
Categories: cs.CL cs.AI cs.LG
Comments: 11M tokens, fix qa3 min facts per task in Table 1
\\ ( https://arxiv.org/abs/2402.10790 ,  7140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11159
replaced with revised version Wed, 21 Feb 2024 02:31:43 GMT   (7669kb,D)

Title: Understanding News Thumbnail Representativeness by Counterfactual
  Text-Guided Contrastive Language-Image Pretraining
Authors: Yejun Yoon, Seunghyun Yoon, and Kunwoo Park
Categories: cs.CL cs.CV
Comments: preprint
\\ ( https://arxiv.org/abs/2402.11159 ,  7669kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11398
replaced with revised version Tue, 20 Feb 2024 22:23:42 GMT   (911kb,D)

Title: Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics
  for Domain Specialized Text Analysis
Authors: Shaochen Xu, Zihao Wu, Huaqin Zhao, Peng Shu, Zhengliang Liu, Wenxiong
  Liao, Sheng Li, Andrea Sikora, Tianming Liu, Xiang Li
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.11398 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11451
replaced with revised version Wed, 21 Feb 2024 03:04:49 GMT   (755kb,D)

Title: SciAgent: Tool-augmented Language Models for Scientific Reasoning
Authors: Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming
  Pan, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Awadalla and Weizhu Chen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.11451 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12343
replaced with revised version Wed, 21 Feb 2024 16:29:18 GMT   (471kb,D)

Title: Emulated Disalignment: Safety Alignment for Large Language Models May
  Backfire!
Authors: Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli
  Ouyang, Yu Qiao
Categories: cs.CL cs.AI cs.LG
Comments: Project web page: https://zhziszz.github.io/emulated-disalignment
\\ ( https://arxiv.org/abs/2402.12343 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12636
replaced with revised version Wed, 21 Feb 2024 14:29:27 GMT   (5372kb,D)

Title: StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing
Authors: Gaoxiang Cong, Yuankai Qi, Liang Li, Amin Beheshti, Zhedong Zhang,
  Anton van den Hengel, Ming-Hsuan Yang, Chenggang Yan, Qingming Huang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12636 ,  5372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12692
replaced with revised version Wed, 21 Feb 2024 02:17:47 GMT   (9374kb,D)

Title: FormulaQA: A Question Answering Dataset for Formula-Based Numerical
  Reasoning
Authors: Xiao Li, Sichen Liu, Bolin Zhu, Yin Zhu, Yiwei Liu, Gong Cheng
Categories: cs.CL
Comments: 17 pages, 9 figures, 7 tables
\\ ( https://arxiv.org/abs/2402.12692 ,  9374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13043
replaced with revised version Wed, 21 Feb 2024 18:45:50 GMT   (9027kb,D)

Title: Effective and Efficient Conversation Retrieval for Dialogue State
  Tracking with Implicit Text Summaries
Authors: Seanie Lee, Jianpeng Cheng, Joris Driesen, Alexandru Coca, Anders
  Johannsen
Categories: cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.13043 ,  9027kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13145
replaced with revised version Wed, 21 Feb 2024 03:18:04 GMT   (5743kb,D)

Title: CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for
  Boosting Metaphor Generation
Authors: Yujie Shao, Xinrong Yao, Xingwei Qu, Chenghua Lin, Shi Wang, Stephen
  W. Huang, Ge Zhang, Jie Fu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.13145 ,  5743kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13184
replaced with revised version Wed, 21 Feb 2024 04:54:22 GMT   (1937kb,D)

Title: What if LLMs Have Different World Views: Simulating Alien Civilizations
  with LLM-based Agents
Authors: Mingyu Jin, Beichen Wang, Zhaoqian Xue, Suiyuan Zhu, Wenyue Hua, Hua
  Tang, Kai Mei, Mengnan Du, Yongfeng Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.13184 ,  1937kb)
------------------------------------------------------------------------------
\\
arXiv:2107.11277
replaced with revised version Wed, 21 Feb 2024 10:10:40 GMT   (636kb,D)

Title: Machine Learning with a Reject Option: A survey
Authors: Kilian Hendrickx, Lorenzo Perini, Dries Van der Plas, Wannes Meert,
  Jesse Davis
Categories: cs.LG cs.AI
MSC-class: 68T02
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2107.11277 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2110.14048
replaced with revised version Wed, 21 Feb 2024 04:18:38 GMT   (8749kb,D)

Title: Conflict-Averse Gradient Descent for Multi-task Learning
Authors: Bo Liu and Xingchao Liu and Xiaojie Jin and Peter Stone and Qiang Liu
Categories: cs.LG cs.AI
Comments: 20 pages, 6 figures, Conference on Neural Information Processing
  Systems, 2021
\\ ( https://arxiv.org/abs/2110.14048 ,  8749kb)
------------------------------------------------------------------------------
\\
arXiv:2205.07424
replaced with revised version Wed, 21 Feb 2024 09:54:52 GMT   (727kb,D)

Title: Trustworthy Graph Neural Networks: Aspects, Methods and Trends
Authors: He Zhang, Bang Wu, Xingliang Yuan, Shirui Pan, Hanghang Tong, Jian Pei
Categories: cs.LG cs.AI
Comments: 42 pages, 7 tables, 4 figures, double columns, accepted by
  Proceedings of the IEEE
DOI: 10.1109/JPROC.2024.3369017
\\ ( https://arxiv.org/abs/2205.07424 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2206.04745
replaced with revised version Wed, 21 Feb 2024 05:55:48 GMT   (802kb,D)

Title: Mildly Conservative Q-Learning for Offline Reinforcement Learning
Authors: Jiafei Lyu, Xiaoteng Ma, Xiu Li, Zongqing Lu
Categories: cs.LG cs.AI
Comments: NeurIPS 2022
\\ ( https://arxiv.org/abs/2206.04745 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00655
replaced with revised version Tue, 20 Feb 2024 23:36:08 GMT   (12783kb)

Title: Self-supervised Representation Learning on Electronic Health Records
  with Graph Kernel Infomax
Authors: Hao-Ren Yao, Nairen Cao, Katina Russell, Der-Chen Chang, Ophir
  Frieder, Jeremy Fineman
Categories: cs.LG cs.CY
Comments: Accepted to ACM Transactions on Computing for Healthcare (HEALTH)
DOI: 10.1145/3648695
\\ ( https://arxiv.org/abs/2209.00655 ,  12783kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04631
replaced with revised version Tue, 20 Feb 2024 23:47:44 GMT   (37873kb,D)

Title: The Normalized Cross Density Functional: A Framework to Quantify
  Statistical Dependence for Random Processes
Authors: Bo Hu and Jose C. Principe
Categories: cs.LG cs.AI cs.IT math.IT
\\ ( https://arxiv.org/abs/2212.04631 ,  37873kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11602
replaced with revised version Wed, 21 Feb 2024 18:02:12 GMT   (1047kb,D)

Title: Convergence of variational Monte Carlo simulation and scale-invariant
  pre-training
Authors: Nilin Abrahamsen and Zhiyan Ding and Gil Goldshlager and Lin Lin
Categories: cs.LG physics.comp-ph
Comments: Updated presentation to unify notation and focus on the VMC setting.
  Added new numerics for scale-invariant supervised pre-training
\\ ( https://arxiv.org/abs/2303.11602 ,  1047kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11351
replaced with revised version Tue, 20 Feb 2024 22:35:32 GMT   (21413kb,D)

Title: Data Redaction from Conditional Generative Models
Authors: Zhifeng Kong and Kamalika Chaudhuri
Categories: cs.LG cs.CL cs.CV
Comments: SaTML 2024
\\ ( https://arxiv.org/abs/2305.11351 ,  21413kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17212
replaced with revised version Wed, 21 Feb 2024 18:44:16 GMT   (538kb,D)

Title: Rotational Equilibrium: How Weight Decay Balances Learning Across Neural
  Networks
Authors: Atli Kosson, Bettina Messmer, Martin Jaggi
Categories: cs.LG
Comments: Code available at https://github.com/epfml/REQ
\\ ( https://arxiv.org/abs/2305.17212 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07541
replaced with revised version Wed, 21 Feb 2024 03:07:23 GMT   (1061kb,D)

Title: A Simple Unified Uncertainty-Guided Framework for Offline-to-Online
  Reinforcement Learning
Authors: Siyuan Guo, Yanchao Sun, Jifeng Hu, Sili Huang, Hechang Chen, Haiyin
  Piao, Lichao Sun, Yi Chang
Categories: cs.LG cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2306.07541 ,  1061kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08107
replaced with revised version Wed, 21 Feb 2024 11:18:20 GMT   (619kb,D)

Title: AutoML in the Age of Large Language Models: Current Challenges, Future
  Opportunities and Risks
Authors: Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli,
  Aditya Mohan, Tim Ruhkopf, Sarah Segel, Daphne Theodorakopoulos, Tanja
  Tornede, Henning Wachsmuth, Marius Lindauer
Categories: cs.LG cs.CL
Comments: Submitted and accepted at TMLR:
  https://openreview.net/forum?id=cAthubStyG
\\ ( https://arxiv.org/abs/2306.08107 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10447
replaced with revised version Tue, 20 Feb 2024 20:21:19 GMT   (3166kb,D)

Title: Globally Interpretable Graph Learning via Distribution Matching
Authors: Yi Nian, Yurui Chang, Wei Jin, Lu Lin
Categories: cs.LG
Comments: 8 page, 3 figures, 5 tables. Accepted by the ACM Web Conference 2024
DOI: 10.1145/3589334.3645674
\\ ( https://arxiv.org/abs/2306.10447 ,  3166kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16248
replaced with revised version Wed, 21 Feb 2024 14:11:19 GMT   (17508kb,D)

Title: Latent SDEs on Homogeneous Spaces
Authors: Sebastian Zeng, Florian Graf, Roland Kwitt
Categories: cs.LG
Comments: v3: updated experiments with results using the public source code
  (commit bc6edd1)
Journal-ref: NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.16248 ,  17508kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16869
replaced with revised version Wed, 21 Feb 2024 18:06:01 GMT   (3601kb,D)

Title: NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural
  Network Inference in Low-Voltage Regimes
Authors: Hao-Lun Sun, Lei Hsiung, Nandhini Chandramoorthy, Pin-Yu Chen,
  Tsung-Yi Ho
Categories: cs.LG cs.AR cs.CV
\\ ( https://arxiv.org/abs/2306.16869 ,  3601kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00897
replaced with revised version Wed, 21 Feb 2024 16:05:54 GMT   (22335kb,D)

Title: Fixing confirmation bias in feature attribution methods via semantic
  match
Authors: Giovanni Cin\`a, Daniel Fernandez-Llaneza, Nishant Mishra, Tabea E.
  R\"ober, Sandro Pezzelle, Iacer Calixto, Rob Goedhart, \c{S}. \.Ilker Birbil
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2307.00897 ,  22335kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06912
replaced with revised version Tue, 20 Feb 2024 22:48:06 GMT   (169kb,D)

Title: CausalLM is not optimal for in-context learning
Authors: Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, Radu Soricut
Categories: cs.LG cs.CL
Comments: ICLR 2024 conference paper. Code available at:
  https://github.com/google-research/causallm_icl
\\ ( https://arxiv.org/abs/2308.06912 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14120
replaced with revised version Wed, 21 Feb 2024 18:35:25 GMT   (1340kb)

Title: Large Language Models Streamline Automated Machine Learning for Clinical
  Studies
Authors: Soroosh Tayebi Arasteh, Tianyu Han, Mahshad Lotfinia, Christiane Kuhl,
  Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung
Categories: cs.LG cs.AI cs.CL
Comments: Published in Nature Communications
Journal-ref: Nat Commun 15, 1603 (2024)
DOI: 10.1038/s41467-024-45879-8
\\ ( https://arxiv.org/abs/2308.14120 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15478
replaced with revised version Tue, 20 Feb 2024 21:37:39 GMT   (776kb,D)

Title: An Adaptive Tangent Feature Perspective of Neural Networks
Authors: Daniel LeJeune, Sina Alemohammad
Categories: cs.LG cs.CV
Comments: 14 pages, 3 figures. Appeared at the First Conference on Parsimony
  and Learning (CPAL 2024)
\\ ( https://arxiv.org/abs/2308.15478 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16198
replaced with revised version Wed, 21 Feb 2024 07:11:45 GMT   (1355kb,D)

Title: Collaborative Information Dissemination with Graph-based Multi-Agent
  Reinforcement Learning
Authors: Raffaele Galliera, Kristen Brent Venable, Matteo Bassani, Niranjan
  Suri
Categories: cs.LG cs.AI cs.MA cs.NI
Comments: 13 pages, 5 figures, 4 tables
\\ ( https://arxiv.org/abs/2308.16198 ,  1355kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16800
replaced with revised version Wed, 21 Feb 2024 08:57:18 GMT   (1819kb,D)

Title: Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural
  Networks
Authors: Andreas Roth, Thomas Liebig
Categories: cs.LG cs.AI
Comments: Published at LoG 2023
\\ ( https://arxiv.org/abs/2308.16800 ,  1819kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01610
replaced with revised version Tue, 20 Feb 2024 20:52:50 GMT   (937kb,D)

Title: Fair Ranking under Disparate Uncertainty
Authors: Richa Rastogi, Thorsten Joachims
Categories: cs.LG cs.CY cs.IR
Comments: A version of this paper was accepted as Spotlight (Oral) at UAI
  workshop on Epistemic AI, 2023
\\ ( https://arxiv.org/abs/2309.01610 ,  937kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08415
replaced with revised version Tue, 20 Feb 2024 19:41:47 GMT   (1038kb)

Title: A new method of modeling the multi-stage decision-making process of CRT
  using machine learning with uncertainty quantification
Authors: Kristoffer Larsen, Chen Zhao, Joyce Keyak, Qiuying Sha, Diana Paez,
  Xinwei Zhang, Guang-Uei Hung, Jiangang Zou, Amalia Peix, Weihua Zhou
Categories: cs.LG eess.SP physics.med-ph
Comments: 29 pages,6 figures. arXiv admin note: text overlap with
  arXiv:2305.02475
\\ ( https://arxiv.org/abs/2309.08415 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14681
replaced with revised version Wed, 21 Feb 2024 05:49:26 GMT   (2316kb,D)

Title: Are Human-generated Demonstrations Necessary for In-context Learning?
Authors: Rui Li, Guoyin Wang, Jiwei Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.14681 ,  2316kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16935
replaced with revised version Wed, 21 Feb 2024 00:23:17 GMT   (4239kb,D)

Title: TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled
  Prescriptive Maintenance Framework
Authors: Yang Zhao, Jiaxi Yang, Wenbo Wang, Helin Yang, Dusit Niyato
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.16935 ,  4239kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03240
replaced with revised version Tue, 20 Feb 2024 20:21:18 GMT   (6210kb,D)

Title: Learning Hierarchical Relational Representations through Relational
  Convolutions
Authors: Awni Altabaa, John Lafferty
Categories: cs.LG
Comments: 23 pages, 10 figures, 5 tables
\\ ( https://arxiv.org/abs/2310.03240 ,  6210kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05264
replaced with revised version Wed, 21 Feb 2024 01:07:32 GMT   (47494kb,D)

Title: The Emergence of Reproducibility and Consistency in Diffusion Models
Authors: Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen,
  Qing Qu
Categories: cs.LG cs.CV
Comments: 49 pages, 23 figures, best paper award in NeurIPS Diffusion Model
  Workshop 2023
\\ ( https://arxiv.org/abs/2310.05264 ,  47494kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07972
replaced with revised version Wed, 21 Feb 2024 07:54:43 GMT   (16408kb,D)

Title: Interpretable Diffusion via Information Decomposition
Authors: Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, Greg Ver Steeg
Categories: cs.LG cs.AI cs.IT math.IT
Comments: 32 pages, 18 figures
\\ ( https://arxiv.org/abs/2310.07972 ,  16408kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10462
replaced with revised version Wed, 21 Feb 2024 14:48:16 GMT   (1527kb,D)

Title: Adaptive Neural Ranking Framework: Toward Maximized Business Goal for
  Cascade Ranking Systems
Authors: Yunli Wang, Zhiqiang Wang, Jian Yang, Shiyang Wen, Dongying Kong, Han
  Li, Kun Gai
Categories: cs.LG
Comments: 12 pages, Accepted by www2024
\\ ( https://arxiv.org/abs/2310.10462 ,  1527kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10534
replaced with revised version Wed, 21 Feb 2024 13:20:36 GMT   (503kb,D)

Title: Comparing Comparators in Generalization Bounds
Authors: Fredrik Hellstr\"om, Benjamin Guedj
Categories: cs.LG cs.IT math.IT math.ST stat.ML stat.TH
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2310.10534 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12248
replaced with revised version Wed, 21 Feb 2024 01:43:56 GMT   (65kb,D)

Title: A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs
Authors: Mateo Perez, Fabio Somenzi, Ashutosh Trivedi
Categories: cs.LG cs.LO
\\ ( https://arxiv.org/abs/2310.12248 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18948
replaced with revised version Wed, 21 Feb 2024 14:35:31 GMT   (22127kb,D)

Title: Probabilistic Feature Augmentation for AIS-Based Multi-Path Long-Term
  Vessel Trajectory Forecasting
Authors: Gabriel Spadon, Jay Kumar, Derek Eden, Josh van Berkel, Tom Foster,
  Matthew Smith, Sarah Vela, Romina Gehrmann, Amilcar Soares, Ronan Fablet,
  Stan Matwin and Ronald Pelot
Categories: cs.LG cs.AI cs.DM math.PR
\\ ( https://arxiv.org/abs/2310.18948 ,  22127kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06973
replaced with revised version Tue, 20 Feb 2024 23:37:08 GMT   (672kb)

Title: Analytical Verification of Deep Neural Network Performance for
  Time-Synchronized Distribution System State Estimation
Authors: Behrouz Azimian, Shiva Moshtagh, Anamitra Pal, Shanshan Ma
Categories: cs.LG cs.SY eess.SY
Comments: 8 pages, in Journal of Modern Power Systems and Clean Energy, 2023
DOI: 10.35833/MPCE.2023.000432
\\ ( https://arxiv.org/abs/2311.06973 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09483
replaced with revised version Tue, 20 Feb 2024 22:50:41 GMT   (2667kb,D)

Title: Adaptive Interventions with User-Defined Goals for Health Behavior
  Change
Authors: Aishwarya Mandyam, Matthew J\"orke, Barbara E. Engelhardt, Emma
  Brunskill
Categories: cs.LG cs.AI
Comments: Extended Abstract presented at Machine Learning for Health (ML4H)
  symposium 2023, December 10th, 2023, New Orleans, United States, 5 pages
\\ ( https://arxiv.org/abs/2311.09483 ,  2667kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11343
replaced with revised version Tue, 20 Feb 2024 21:03:25 GMT   (2328kb,D)

Title: A Generative Model for Accelerated Inverse Modelling Using a Novel
  Embedding for Continuous Variables
Authors: S\'ebastien Bompas and Stefan Sandfeld
Categories: cs.LG cond-mat.mtrl-sci
Comments: 9 pages, 8 figures, NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.11343 ,  2328kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02119
replaced with revised version Wed, 21 Feb 2024 17:49:22 GMT   (493kb,D)

Title: Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
Authors: Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson,
  Hyrum Anderson, Yaron Singer, Amin Karbasi
Categories: cs.LG cs.AI cs.CL cs.CR stat.ML
Comments: An implementation of the presented method is available at
  https://github.com/RICommunity/TAP
\\ ( https://arxiv.org/abs/2312.02119 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02596
replaced with revised version Wed, 21 Feb 2024 15:22:07 GMT   (669kb,D)

Title: LSTSVR-PI: Least square twin support vector regression with privileged
  information
Authors: Anuradha Kumari, M. Tanveer
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.02596 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04404
replaced with revised version Wed, 21 Feb 2024 09:06:49 GMT   (440kb,D)

Title: On the Impact of Multi-dimensional Local Differential Privacy on
  Fairness
Authors: Karima Makhlouf, Heber H. Arcolezi, Sami Zhioua, Ghassen Ben Brahim,
  and Catuscia Palamidessi
Categories: cs.LG cs.CR cs.CY
\\ ( https://arxiv.org/abs/2312.04404 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10107
replaced with revised version Wed, 21 Feb 2024 13:57:19 GMT   (8780kb,D)

Title: Towards Context-Aware Domain Generalization: Understanding the Benefits
  and Limits of Marginal Transfer Learning
Authors: Jens M\"uller, Lars K\"uhmichel, Martin Rohbeck, Stefan T. Radev,
  Ullrich K\"othe
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.10107 ,  8780kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12839
replaced with revised version Wed, 21 Feb 2024 08:25:19 GMT   (403kb,D)

Title: Comparing Machine Learning Algorithms by Union-Free Generic Depth
Authors: Hannah Blocher, Georg Schollmeyer, Malte Nalenz, Christoph Jansen
Categories: cs.LG stat.ML
Comments: arXiv admin note: substantial text overlap with arXiv:2304.09872
\\ ( https://arxiv.org/abs/2312.12839 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15474
replaced with revised version Wed, 21 Feb 2024 10:11:48 GMT   (11265kb,D)

Title: A Conservative Approach for Few-Shot Transfer in Off-Dynamics
  Reinforcement Learning
Authors: Paul Daoudi, Christophe Prieur, Bogdan Robu, Merwan Barlier, Ludovic
  Dos Santos
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2312.15474 ,  11265kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04829
replaced with revised version Wed, 21 Feb 2024 03:55:44 GMT   (1739kb)

Title: GNNShap: Fast and Accurate GNN Explanations using Shapley Values
Authors: Selahattin Akkas and Ariful Azad
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2401.04829 ,  1739kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06432
replaced with revised version Tue, 20 Feb 2024 21:15:59 GMT   (7413kb,D)

Title: Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation
  Models
Authors: Yae Jee Cho and Luyang Liu and Zheng Xu and Aldi Fahrezi and Gauri
  Joshi
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2401.06432 ,  7413kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16708
replaced with revised version Wed, 21 Feb 2024 00:26:37 GMT   (438kb,D)

Title: Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible
  Cluster Shapes
Authors: Yung-Peng Hsu, Hung-Hsuan Chen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.16708 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17580
replaced with revised version Wed, 21 Feb 2024 16:33:59 GMT   (1905kb,D)

Title: Graph Contrastive Learning with Cohesive Subgraph Awareness
Authors: Yucheng Wu, Leye Wang, Xiao Han, and Han-Jia Ye
Categories: cs.LG cs.AI
DOI: 10.1145/3589334.3645470
\\ ( https://arxiv.org/abs/2401.17580 ,  1905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03807
replaced with revised version Wed, 21 Feb 2024 05:24:37 GMT   (7091kb,D)

Title: SEABO: A Simple Search-Based Method for Offline Imitation Learning
Authors: Jiafei Lyu, Xiaoteng Ma, Le Wan, Runze Liu, Xiu Li, Zongqing Lu
Categories: cs.LG cs.AI
Comments: To appear in ICLR2024
\\ ( https://arxiv.org/abs/2402.03807 ,  7091kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07818
replaced with revised version Wed, 21 Feb 2024 06:11:02 GMT   (292kb,D)

Title: Differentially Private Zeroth-Order Methods for Scalable Large Language
  Model Finetuning
Authors: Z Liu, J Lou, W Bao, Z Qin, K Ren
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.07818 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08653
replaced with revised version Wed, 21 Feb 2024 04:35:38 GMT   (5246kb,D)

Title: SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
Authors: Wuxinlin Cheng, Chenhui Deng, Ali Aghdaei, Zhiru Zhang, Zhuo Feng
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.08653 ,  5246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10097
replaced with revised version Wed, 21 Feb 2024 13:14:08 GMT   (2741kb,D)

Title: Adaptive Federated Learning in Heterogeneous Wireless Networks with
  Independent Sampling
Authors: Jiaxiang Geng, Yanzhao Hou, Xiaofeng Tao, Juncheng Wang and Bing Luo
Categories: cs.LG cs.NI
Comments: 6 pages, 5 figures, accepted for publication in IEEE International
  Conference on Communications (ICC)
\\ ( https://arxiv.org/abs/2402.10097 ,  2741kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10991
replaced with revised version Wed, 21 Feb 2024 15:41:14 GMT   (381kb)

Title: Accelerating Semi-Asynchronous Federated Learning
Authors: Changxin Xu, Yuxin Qiao, Zhanxin Zhou, Fanghao Ni, and Jize Xiong
Categories: cs.LG cs.AI
Comments: 5 pages, 1 figures
\\ ( https://arxiv.org/abs/2402.10991 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11168
replaced with revised version Wed, 21 Feb 2024 00:05:25 GMT   (1328kb,D)

Title: Trust Regions for Explanations via Black-Box Probabilistic Certification
Authors: Amit Dhurandhar, Swagatam Haldar, Dennis Wei and Karthikeyan Natesan
  Ramamurthy
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.11168 ,  1328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11942
replaced with revised version Tue, 20 Feb 2024 22:33:58 GMT   (10428kb,D)

Title: The effect of Leaky ReLUs on the training and generalization of
  overparameterized networks
Authors: Yinglong Guo, Shaohan Li, Gilad Lerman
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11942 ,  10428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12399
replaced with revised version Wed, 21 Feb 2024 13:33:12 GMT   (211kb,D)

Title: Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
Authors: Zhiyuan Zeng, Qipeng Guo, Zhaoye Fei, Zhangyue Yin, Yunhua Zhou,
  Linyang Li, Tianxiang Sun, Hang Yan, Dahua Lin, Xipeng Qiu
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.12399 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12503
replaced with revised version Wed, 21 Feb 2024 18:39:38 GMT   (9633kb,D)

Title: PARCv2: Physics-aware Recurrent Convolutional Neural Networks for
  Spatiotemporal Dynamics Modeling
Authors: Phong C.H. Nguyen, Xinlun Cheng, Shahab Azarfar, Pradeep Seshadri, Yen
  T. Nguyen, Munho Kim, Sanghun Choi, H.S. Udaykumar, Stephen Baek
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.12503 ,  9633kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13221
replaced with revised version Wed, 21 Feb 2024 08:07:13 GMT   (1505kb,D)

Title: CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset
  for Advancing Graph Machine Learning
Authors: Ulrik Friis-Jensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam,
  Kirsten M. {\O}. Jensen and Raghavendra Selvan
Categories: cs.LG stat.ML
Comments: 16 pages, 15 figures, 8 tables. Dataset is available at
  https://github.com/UlrikFriisJensen/CHILI
\\ ( https://arxiv.org/abs/2402.13221 ,  1505kb)
------------------------------------------------------------------------------
\\
arXiv:2111.08108 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 18:58:53 GMT   (582kb,D)

Title: Learning Optimal Control with Stochastic Models of Hamiltonian Dynamics
Authors: Chandrajit Bajaj and Minh Nguyen
Categories: math.OC cs.AI
Comments: 13 pages, 8 figures
MSC-class: 34K35
ACM-class: I.2; I.6
\\ ( https://arxiv.org/abs/2111.08108 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2206.06172 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 06:48:20 GMT   (669kb,D)

Title: RIS-ADMM: A RIS and ADMM-Based Passive and Sparse Sensing Method With
  Interference Removal
Authors: Peng Chen, Zhimin Chen, Pu Miao, Yun Chen
Categories: eess.SP cs.AI cs.IT math.IT
Comments: 5 pages
Journal-ref: IEEE Communications letters, 2024
\\ ( https://arxiv.org/abs/2206.06172 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00187
replaced with revised version Wed, 21 Feb 2024 04:12:30 GMT   (24kb,D)

Title: Design of Fuzzy Logic Controller for Washing Machine
Authors: Kriti Dheerawat
Categories: eess.SY cs.AI cs.SY
Comments: Submitted to Journal
MSC-class: 94D05, 93C42
\\ ( https://arxiv.org/abs/2210.00187 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07245
replaced with revised version Tue, 20 Feb 2024 19:23:08 GMT   (15863kb,D)

Title: Assessing Uncertainty in Similarity Scoring: Performance & Fairness in
  Face Recognition
Authors: Jean-R\'emy Conti, St\'ephan Cl\'emen\c{c}on
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2211.07245 ,  15863kb)
------------------------------------------------------------------------------
\\
arXiv:2211.11460 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 18:49:17 GMT   (234kb,D)

Title: Motor Imagery Decoding Using Ensemble Curriculum Learning and
  Collaborative Training
Authors: Georgios Zoumpourlis, Ioannis Patras
Categories: eess.SP cs.AI
Comments: Accepted for publication in 12th IEEE International Winter Conference
  on Brain-Computer Interface (BCI), 2024. Code:
  https://github.com/gzoumpourlis/Ensemble-MI
\\ ( https://arxiv.org/abs/2211.11460 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2301.02998
replaced with revised version Wed, 21 Feb 2024 04:20:55 GMT   (65kb,D)

Title: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
Authors: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani
  Kundu, Ramya Ramanathan, Eric Nyberg
Categories: cs.IR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2301.02998 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03443
replaced with revised version Wed, 21 Feb 2024 02:34:13 GMT   (43575kb,D)

Title: Learning Multi-Pursuit Evasion for Safe Targeted Navigation of Drones
Authors: Jiaping Xiao and Mir Feroskhan
Categories: cs.RO cs.AI
Comments: Accepted by IEEE Transactions on Artificial Intelligence
Journal-ref: IEEE Transactions on Artificial Intelligence (2024)
DOI: 10.1109/TAI.2024.3366871
\\ ( https://arxiv.org/abs/2304.03443 ,  43575kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15399
replaced with revised version Wed, 21 Feb 2024 01:25:36 GMT   (6455kb,D)

Title: Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape
Authors: Rundi Wu, Ruoshi Liu, Carl Vondrick, Changxi Zheng
Categories: cs.CV cs.AI cs.GR
Comments: Accepted to ICLR 2024. Project page: https://Sin3DM.github.io, Code:
  https://github.com/Sin3DM/Sin3DM
\\ ( https://arxiv.org/abs/2305.15399 ,  6455kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01513 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 06:07:04 GMT   (4736kb,D)

Title: RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the
  Absence of First-order Echoes
Authors: Inmo Yeon and Jung-Woo Choi
Categories: eess.AS cs.AI cs.SD
Comments: 5 pages, 3 figures, 3 tables
\\ ( https://arxiv.org/abs/2309.01513 ,  4736kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07808
replaced with revised version Tue, 20 Feb 2024 20:29:57 GMT   (7656kb,D)

Title: What Matters to Enhance Traffic Rule Compliance of Imitation Learning
  for Automated Driving
Authors: Hongkuan Zhou, Aifen Sui, Wei Cao, Zhenshan Bing
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: 10 pages, 2 figures
\\ ( https://arxiv.org/abs/2309.07808 ,  7656kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02529
replaced with revised version Tue, 20 Feb 2024 19:48:15 GMT   (643kb,D)

Title: MIDDAG: Where Does Our News Go? Investigating Information Diffusion via
  Community-Level Information Pathways
Authors: Mingyu Derek Ma, Alexander K. Taylor, Nuan Wen, Yanchen Liu, Po-Nien
  Kung, Wenna Qin, Shicheng Wen, Azure Zhou, Diyi Yang, Xuezhe Ma, Nanyun Peng,
  Wei Wang
Categories: cs.SI cs.AI cs.HC
Comments: To appear at AAAI'24. System demo video and more info:
  info-pathways.github.io
\\ ( https://arxiv.org/abs/2310.02529 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02984 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 19:17:52 GMT   (5340kb,D)

Title: Scaling Laws for Associative Memories
Authors: Vivien Cabannes, Elvis Dohmatob, Alberto Bietti
Categories: stat.ML cs.AI cs.CL cs.LG cs.NE
ACM-class: I.2.6; G.1.6
\\ ( https://arxiv.org/abs/2310.02984 ,  5340kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07138
replaced with revised version Wed, 21 Feb 2024 04:00:06 GMT   (15885kb,D)

Title: Denoising Task Routing for Diffusion Models
Authors: Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, Changick Kim
Categories: cs.CV cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.07138 ,  15885kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11198
replaced with revised version Wed, 21 Feb 2024 08:20:49 GMT   (521kb,D)

Title: EEG motor imagery decoding: A framework for comparative analysis with
  channel attention mechanisms
Authors: Martin Wimpff, Leonardo Gizzi, Jan Zerfowski, Bin Yang
Categories: cs.HC cs.AI cs.LG
Comments: Submitted to: Journal of Neural Engineering
\\ ( https://arxiv.org/abs/2310.11198 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14691 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 10:00:25 GMT   (27kb)

Title: Identifiability of total effects from abstractions of time series causal
  graphs
Authors: Charles K. Assaad, Emilie Devijver (LIG, UGA), Eric Gaussier (LIG,
  UGA), Gregor G\"ossler (LIG, SPADES), Anouar Meynaoui (IRMAR, UR2)
Categories: math.ST cs.AI stat.TH
\\ ( https://arxiv.org/abs/2310.14691 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02847
replaced with revised version Wed, 21 Feb 2024 02:27:57 GMT   (6744kb,D)

Title: Kinematic-aware Prompting for Generalizable Articulated Object
  Manipulation with LLMs
Authors: Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu,
  Xuelong Li
Categories: cs.RO cs.AI
Comments: Accepted by ICRA 2024
\\ ( https://arxiv.org/abs/2311.02847 ,  6744kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04690
replaced with revised version Tue, 20 Feb 2024 20:18:31 GMT   (3919kb,D)

Title: SynthScribe: Deep Multimodal Tools for Synthesizer Sound Retrieval and
  Exploration
Authors: Stephen Brade, Bryan Wang, Mauricio Sousa, Gregory Lee Newsome, Sageev
  Oore, Tovi Grossman
Categories: cs.HC cs.AI cs.SD eess.AS
DOI: 10.1145/3640543.3645158
\\ ( https://arxiv.org/abs/2312.04690 ,  3919kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04792
replaced with revised version Wed, 21 Feb 2024 16:03:18 GMT   (142kb,D)

Title: Playing Large Games with Oracles and AI Debate
Authors: Xinyi Chen, Angelica Chen, Dean Foster, Elad Hazan
Categories: cs.GT cs.AI
\\ ( https://arxiv.org/abs/2312.04792 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14647
replaced with revised version Wed, 21 Feb 2024 07:41:10 GMT   (1364kb,D)

Title: Towards Message Brokers for Generative AI: Survey, Challenges, and
  Opportunities
Authors: Alaa Saleh, Roberto Morabito, Sasu Tarkoma, Susanna Pirttikangas and
  Lauri Lov\'en
Categories: cs.DC cs.AI cs.LG cs.NI
Comments: 20 pages, 181 references, 7 figures, 5 tables
ACM-class: C.2.4; I.2.11; I.2.7
\\ ( https://arxiv.org/abs/2312.14647 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03424
replaced with revised version Wed, 21 Feb 2024 03:42:51 GMT   (244kb,D)

Title: MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech
  Recognition
Authors: He Wang, Pengcheng Guo, Pan Zhou, Lei Xie
Categories: cs.SD cs.AI eess.AS
Comments: Accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2401.03424 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03473
replaced with revised version Wed, 21 Feb 2024 03:39:37 GMT   (15kb)

Title: ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech
  Recognition Challenge
Authors: He Wang, Pengcheng Guo, Yue Li, Ao Zhang, Jiayao Sun, Lei Xie, Wei
  Chen, Pan Zhou, Hui Bu, Xin Xu, Binbin Zhang, Zhuo Chen, Jian Wu, Longbiao
  Wang, Eng Siong Chng, Sun Li
Categories: cs.SD cs.AI eess.AS
Comments: Accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2401.03473 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03781 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 12:03:13 GMT   (7853kb,D)

Title: MolTC: Towards Molecular Relational Modeling In Language Models
Authors: Junfeng Fang, Shuai Zhang, Chang Wu, Zhengyi Yang, Zhiyuan Liu, Sihang
  Li, Kun Wang, Wenjie Du and Xiang Wang
Categories: q-bio.QM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03781 ,  7853kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04032
replaced with revised version Wed, 21 Feb 2024 05:29:29 GMT   (1166kb,D)

Title: HEAM : Hashed Embedding Acceleration using Processing-In-Memory
Authors: Youngsuk Kim, Hyuk-Jae Lee, Chae Eun Rhee
Categories: cs.AR cs.AI
Comments: 10 pages, 12 figures
\\ ( https://arxiv.org/abs/2402.04032 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04536
replaced with revised version Wed, 21 Feb 2024 17:31:22 GMT   (18561kb,D)

Title: Tactile-based Object Retrieval From Granular Media
Authors: Jingxi Xu, Yinsen Jia, Dongxiao Yang, Patrick Meng, Xinyue Zhu, Zihan
  Guo, Shuran Song, Matei Ciocarlie
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.04536 ,  18561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05650
replaced with revised version Wed, 21 Feb 2024 08:16:34 GMT   (1285kb,D)

Title: Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation
  of LLM-Supported SE Tasks
Authors: Wei Wang, Huilong Ning, Gaowei Zhang, Libo Liu and Yi Wang
Categories: cs.SE cs.AI
Comments: The paper has been accepted by FSE
MSC-class: 65-XX
ACM-class: D.2; I.2
\\ ( https://arxiv.org/abs/2402.05650 ,  1285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05804
replaced with revised version Wed, 21 Feb 2024 00:19:06 GMT   (42731kb,D)

Title: InkSight: Offline-to-Online Handwriting Conversion by Learning to Read
  and Write
Authors: Blagoj Mitrevski, Arina Rak, Julian Schnitzler, Chengkun Li, Andrii
  Maksai, Jesse Berent, Claudiu Musat
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.05804 ,  42731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09384 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 18:01:48 GMT   (51kb)

Title: Persuasion, Delegation, and Private Information in Algorithm-Assisted
  Decisions
Authors: Ruqing Xu
Categories: econ.TH cs.AI cs.CY cs.GT cs.HC
\\ ( https://arxiv.org/abs/2402.09384 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09442 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 18:36:11 GMT   (574kb)

Title: Progress in artificial intelligence applications based on the
  combination of self-driven sensors and deep learning
Authors: Weixiang Wan, Wenjian Sun, Qiang Zeng, Linying Pan, Jingyu Xu, Bo Liu
Categories: eess.SP cs.AI
Comments: This aticle was accepted by ieee conference
\\ ( https://arxiv.org/abs/2402.09442 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10980 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 17:34:43 GMT   (14570kb,D)

Title: ChemReasoner: Heuristic Search over a Large Language Model's Knowledge
  Space using Quantum-Chemical Feedback
Authors: Henry W. Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel V. Olarte,
  Udishnu Sanyal, Conrad Johnston, Hongbin Liu, Heng Ji, Sutanay Choudhury
Categories: physics.chem-ph cs.AI cs.CE cs.LG
Comments: 8 pages; Added author institutions
\\ ( https://arxiv.org/abs/2402.10980 ,  14570kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11459 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 07:46:07 GMT   (8401kb,D)

Title: Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion
  Bridge
Authors: Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang
  Gao, Siyuan Li and Stan.Z. Li
Categories: q-bio.BM cs.AI cs.LG physics.chem-ph
\\ ( https://arxiv.org/abs/2402.11459 ,  8401kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11871
replaced with revised version Wed, 21 Feb 2024 04:25:51 GMT   (13550kb,D)

Title: From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions
  and Models for Planning from Raw Data
Authors: Naman Shah, Jayesh Nagpal, Pulkit Verma, Siddharth Srivastava
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2402.11871 ,  13550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12062
replaced with revised version Tue, 20 Feb 2024 22:53:23 GMT   (231kb)

Title: Causal Equal Protection as Algorithmic Fairness
Authors: Marcello Di Bello, Nicol\`o Cangiotti, Michele Loi
Categories: cs.CY cs.AI cs.DS cs.LG
Comments: 18 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.12062 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12391 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 03:42:32 GMT   (369kb,D)

Title: Toward a Team of AI-made Scientists for Scientific Discovery from Gene
  Expression Data
Authors: Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu,
  Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang
Categories: q-bio.GN cs.AI cs.LG
Comments: 18 pages, 2 figures; added contact
\\ ( https://arxiv.org/abs/2402.12391 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13224 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 08:50:15 GMT   (438kb,D)

Title: Controlling Large Electric Vehicle Charging Stations via User Behavior
  Modeling and Stochastic Programming
Authors: Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud
Categories: math.OC cs.AI cs.CE cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2402.13224 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16834 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 01:00:33 GMT   (2170kb,D)

Title: Discrete Diffusion Modeling by Estimating the Ratios of the Data
  Distribution
Authors: Aaron Lou, Chenlin Meng, Stefano Ermon
Categories: stat.ML cs.CL cs.LG
Comments: 30 pages. Code at
  https://github.com/louaaron/Score-Entropy-Discrete-Diffusion
\\ ( https://arxiv.org/abs/2310.16834 ,  2170kb)
------------------------------------------------------------------------------
\\
arXiv:2110.14427 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 17:11:06 GMT   (203kb,D)

Title: The ODE Method for Asymptotic Statistics in Stochastic Approximation and
  Reinforcement Learning
Authors: Vivek Borkar, Shuhang Chen, Adithya Devraj, Ioannis Kontoyiannis and
  Sean Meyn
Categories: math.ST cs.LG stat.TH
Comments: 2 figures
MSC-class: 62L20, 60F17, 68T05
\\ ( https://arxiv.org/abs/2110.14427 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2210.05988 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 00:21:54 GMT   (18589kb,D)

Title: CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG
  Reconstruction
Authors: Pin-Hua Lai, Bo-Shan Wang, Wei-Chun Yang, Hsiang-Chieh Tsou, Chun-Shu
  Wei
Categories: eess.SP cs.LG q-bio.NC
\\ ( https://arxiv.org/abs/2210.05988 ,  18589kb)
------------------------------------------------------------------------------
\\
arXiv:2211.08597 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 21:06:07 GMT   (11614kb,D)

Title: SketchySGD: Reliable Stochastic Optimization via Randomized Curvature
  Estimates
Authors: Zachary Frangella, Pratik Rathore, Shipu Zhao, Madeleine Udell
Categories: math.OC cs.LG
Comments: 65 pages, 43 figures, 8 tables
\\ ( https://arxiv.org/abs/2211.08597 ,  11614kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09221 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 23:58:24 GMT   (1581kb,D)

Title: The non-overlapping statistical approximation to overlapping group lasso
Authors: Mingyu Qi, Tianxi Li
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2211.09221 ,  1581kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09771
replaced with revised version Wed, 21 Feb 2024 14:36:17 GMT   (3842kb,D)

Title: Boosting Object Representation Learning via Motion and Object Continuity
Authors: Quentin Delfosse, Wolfgang Stammer, Thomas Rothenbacher, Dwarak
  Vittal, Kristian Kersting
Categories: cs.CV cs.LG
Comments: 8 pages main text, 32 tables, 21 Figures
Journal-ref: Machine Learning and Knowledge Discovery in Databases: Research
  Track. ECML PKDD 2023. Lecture Notes in Computer Science(), vol 14172.
  Springer, Cham
DOI: 10.1007/978-3-031-43421-1_36
\\ ( https://arxiv.org/abs/2211.09771 ,  3842kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09869
replaced with revised version Tue, 20 Feb 2024 20:59:05 GMT   (12338kb,D)

Title: RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and
  Generation
Authors: Titas Anciukevi\v{c}ius, Zexiang Xu, Matthew Fisher, Paul Henderson,
  Hakan Bilen, Niloy J. Mitra, Paul Guerrero
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2023. Project page:
  https://github.com/Anciukevicius/RenderDiffusion
\\ ( https://arxiv.org/abs/2211.09869 ,  12338kb)
------------------------------------------------------------------------------
\\
arXiv:2301.06339 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 15:14:39 GMT   (486kb,D)

Title: Approximation of optimization problems with constraints through kernel
  Sum-Of-Squares
Authors: Pierre-Cyril Aubin-Frankowski and Alessandro Rudi
Categories: math.OC cs.LG
MSC-class: 46E22, 46N10, 90C26
\\ ( https://arxiv.org/abs/2301.06339 ,  486kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03963 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 09:33:36 GMT   (1935kb,D)

Title: Learning-based Online Optimization for Autonomous Mobility-on-Demand
  Fleet Control
Authors: Kai Jungel, Axel Parmentier, Maximilian Schiffer, Thibaut Vidal
Categories: math.OC cs.LG
Comments: 34 pages, 20 figures
\\ ( https://arxiv.org/abs/2302.03963 ,  1935kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05376 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 13:38:39 GMT   (3742kb,D)

Title: PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics
Authors: Matthew Leigh, Debajyoti Sengupta, Guillaume Qu\'etant, John Andrew
  Raine, Knut Zoch, and Tobias Golling
Categories: hep-ph cs.LG hep-ex
Comments: 30 pages, 25 figures, 5 tables
Journal-ref: SciPost Phys. 16, 018 (2024)
DOI: 10.21468/SciPostPhys.16.1.018
\\ ( https://arxiv.org/abs/2303.05376 ,  3742kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06532
replaced with revised version Wed, 21 Feb 2024 08:15:58 GMT   (716kb,D)

Title: Automated Design of Metaheuristic Algorithms: A Survey
Authors: Qi Zhao, Qiqi Duan, Bai Yan, Shi Cheng, Yuhui Shi
Categories: cs.NE cs.LG
Journal-ref: Transactions on Machine Learning Research, 2024,
  https://openreview.net/forum?id=qhtHsvF5zj
\\ ( https://arxiv.org/abs/2303.06532 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19215 (*cross-listing*)
replaced with revised version Tue, 20 Feb 2024 20:12:20 GMT   (5884kb,D)

Title: dotears: Scalable, consistent DAG estimation using observational and
  interventional data
Authors: Albert Xue, Jingyou Rao, Sriram Sankararaman, Harold Pimentel
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.19215 ,  5884kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05857 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 08:47:06 GMT   (2687kb,D)

Title: How Sparse Can We Prune A Deep Network: A Fundamental Limit Viewpoint
Authors: Qiaozhe Zhang, Ruijie Zhang, Jun Sun, Yingzhuang Liu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2306.05857 ,  2687kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06088
replaced with revised version Wed, 21 Feb 2024 13:35:34 GMT   (43010kb,D)

Title: SENS: Part-Aware Sketch-based Implicit Neural Shape Modeling
Authors: Alexandre Binninger, Amir Hertz, Olga Sorkine-Hornung, Daniel
  Cohen-Or, Raja Giryes
Categories: cs.GR cs.CV cs.LG
Comments: 25 pages, 24 figures
\\ ( https://arxiv.org/abs/2306.06088 ,  43010kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12584 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 13:35:38 GMT   (2691kb,D)

Title: Hierarchical Neural Simulation-Based Inference Over Event Ensembles
Authors: Lukas Heinrich, Siddharth Mishra-Sharma, Chris Pollard, and Philipp
  Windischhofer
Categories: stat.ML astro-ph.IM cs.LG hep-ex
Comments: 15+2 pages, 7 figures; v2, version published in TMLR
Report-no: MIT-CTP/5576
\\ ( https://arxiv.org/abs/2306.12584 ,  2691kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01470
replaced with revised version Wed, 21 Feb 2024 18:28:30 GMT   (14919kb,D)

Title: A Review of Driver Gaze Estimation and Application in Gaze Behavior
  Understanding
Authors: Pavan Kumar Sharma and Pranamesh Chakraborty
Categories: cs.CV cs.HC cs.LG
\\ ( https://arxiv.org/abs/2307.01470 ,  14919kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14456 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 16:57:23 GMT   (133kb,D)

Title: Speech Self-Supervised Representations Benchmarking: a Case for Larger
  Probing Heads
Authors: Salah Zaiem, Youcef Kemiche, Titouan Parcollet, Slim Essid, Mirco
  Ravanelli
Categories: eess.AS cs.LG cs.SD eess.SP
Comments: 18 Pages
\\ ( https://arxiv.org/abs/2308.14456 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03451
replaced with revised version Wed, 21 Feb 2024 05:02:34 GMT   (1043kb,D)

Title: Cross-domain Sound Recognition for Efficient Underwater Data Analysis
Authors: Jeongsoo Park, Dong-Gyun Han, Hyoung Sul La, Sangmin Lee, Yoonchang
  Han, and Eun-Jin Yang
Categories: cs.SD cs.LG eess.AS
Comments: Accepted to APSIPA 2023
\\ ( https://arxiv.org/abs/2309.03451 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03054 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 09:42:53 GMT   (14218kb,D)

Title: Posterior Sampling Based on Gradient Flows of the MMD with Negative
  Distance Kernel
Authors: Paul Hagemann, Johannes Hertrich, Fabian Altekr\"uger, Robert Beinert,
  Jannis Chemseddine, Gabriele Steidl
Categories: stat.ML cs.LG math.OC math.PR
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.03054 ,  14218kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14024
replaced with revised version Wed, 21 Feb 2024 07:21:10 GMT   (4887kb,D)

Title: Creating and Leveraging a Synthetic Dataset of Cloud Optical Thickness
  Measures for Cloud Detection in MSI
Authors: Aleksis Pirinen, Nosheen Abid, Nuria Agues Paszkowsky, Thomas Ohlson
  Timoudas, Ronald Scheirer, Chiara Ceccobello, Gy\"orgy Kov\'acs, Anders
  Persson
Categories: cs.CV cs.LG
Comments: Published in the journal Remote Sensing (2024). Code, data and models
  available at https://github.com/aleksispi/ml-cloud-opt-thick
\\ ( https://arxiv.org/abs/2311.14024 ,  4887kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03871 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 16:53:31 GMT   (8310kb,D)

Title: Hidden yet quantifiable: A lower bound for confounding strength using
  randomized trials
Authors: Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny
  Yang
Categories: stat.ML cs.LG
Comments: Accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2312.03871 ,  8310kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08821 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 16:15:40 GMT   (696kb,D)

Title: Reconstruction of Sound Field through Diffusion Models
Authors: Federico Miotello, Luca Comanducci, Mirco Pezzoli, Alberto Bernardini,
  Fabio Antonacci and Augusto Sarti
Categories: eess.AS cs.LG cs.SD eess.SP
Comments: Accepted for publication at ICASSP 2024
\\ ( https://arxiv.org/abs/2312.08821 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11529
replaced with revised version Wed, 21 Feb 2024 15:13:29 GMT   (2445kb,D)

Title: Efficient and Scalable Graph Generation through Iterative Local
  Expansion
Authors: Andreas Bergmeister, Karolis Martinkus, Nathana\"el Perraudin, Roger
  Wattenhofer
Categories: cs.SI cs.LG
\\ ( https://arxiv.org/abs/2312.11529 ,  2445kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02686
replaced with revised version Wed, 21 Feb 2024 15:32:09 GMT   (2124kb,D)

Title: Beyond Fidelity: Explaining Vulnerability Localization of Learning-based
  Detectors
Authors: Baijun Cheng, Shengming Zhao, Kailong Wang, Meizhen Wang, Guangdong
  Bai, Ruitao Feng, Yao Guo, Lei Ma, Haoyu Wang
Categories: cs.CR cs.LG cs.SE
Comments: Accepted by Tosem
DOI: 10.1145/3641543
\\ ( https://arxiv.org/abs/2401.02686 ,  2124kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08865
replaced with revised version Wed, 21 Feb 2024 16:17:22 GMT   (1569kb,D)

Title: The Effect of Intrinsic Dataset Properties on Generalization: Unraveling
  Learning Differences Between Natural and Medical Images
Authors: Nicholas Konz, Maciej A. Mazurowski
Categories: cs.CV cs.LG eess.IV stat.ML
Comments: ICLR 2024. Code:
  https://github.com/mazurowski-lab/intrinsic-properties
\\ ( https://arxiv.org/abs/2401.08865 ,  1569kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16468
replaced with revised version Wed, 21 Feb 2024 15:13:26 GMT   (42487kb,D)

Title: InstructIR: High-Quality Image Restoration Following Human Instructions
Authors: Marcos V. Conde, Gregor Geigle, Radu Timofte
Categories: cs.CV cs.LG eess.IV
Comments: Technical Report
\\ ( https://arxiv.org/abs/2401.16468 ,  42487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03445
replaced with revised version Tue, 20 Feb 2024 20:59:48 GMT   (7745kb,D)

Title: Denoising Diffusion via Image-Based Rendering
Authors: Titas Anciukevi\v{c}ius, Fabian Manhardt, Federico Tombari, Paul
  Henderson
Categories: cs.CV cs.GR cs.LG
Comments: Accepted at ICLR 2024. Project page:
  https://anciukevicius.github.io/generative-image-based-rendering
\\ ( https://arxiv.org/abs/2402.03445 ,  7745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04273
replaced with revised version Tue, 20 Feb 2024 20:46:28 GMT   (20038kb,D)

Title: Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception
  from Independent Private Sources
Authors: Jinlong Li, Baolu Li, Xinyu Liu, Runsheng Xu, Jiaqi Ma, Hongkai Yu
Categories: cs.CV cs.LG
Comments: Accepted by the 2024 IEEE International Conference on Robotics and
  Automation (ICRA)
\\ ( https://arxiv.org/abs/2402.04273 ,  20038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06919
replaced with revised version Wed, 21 Feb 2024 10:45:57 GMT   (207kb,D)

Title: TREET: TRansfer Entropy Estimation via Transformer
Authors: Omer Luxembourg, Dor Tsur, Haim Permuter
Categories: cs.IT cs.LG math.IT
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2402.06919 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07588
replaced with revised version Wed, 21 Feb 2024 18:49:51 GMT   (866kb,D)

Title: Rethinking Scaling Laws for Learning in Strategic Environments
Authors: Tinashe Handina and Eric Mazumdar
Categories: cs.GT cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.07588 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10232 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 15:30:52 GMT   (37kb)

Title: Simple, unified analysis of Johnson-Lindenstrauss with applications
Authors: Yingru Li
Categories: stat.ML cs.DS cs.LG math.PR
\\ ( https://arxiv.org/abs/2402.10232 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13001 (*cross-listing*)
replaced with revised version Wed, 21 Feb 2024 07:10:06 GMT   (17kb)

Title: A unifying primary framework for quantum graph neural networks from
  quantum graph states
Authors: Ammar Daskin
Categories: quant-ph cs.LG
Comments: short version 6 pages, a few important typos are corrected
\\ ( https://arxiv.org/abs/2402.13001 ,  17kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
