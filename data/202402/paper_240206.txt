Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月6日 17:40
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri  2 Feb 24 19:00:00 GMT  to  Mon  5 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.01677
Date: Sat, 20 Jan 2024 08:44:34 GMT   (299kb,D)

Title: Embedding Ontologies via Incoprorating Extensional and Intensional
  Knowledge
Authors: Keyu Wang, Guilin Qi, Jiaoyan Chen, Tianxing Wu
Categories: cs.AI cs.CL
Comments: Submitting to IJCAI2024; 9 pages and 3 figures
\\
  Ontologies contain rich knowledge within domain, which can be divided into
two categories, namely extensional knowledge and intensional knowledge.
Extensional knowledge provides information about the concrete instances that
belong to specific concepts in the ontology, while intensional knowledge
details inherent properties, characteristics, and semantic associations among
concepts. However, existing ontology embedding approaches fail to take both
extensional knowledge and intensional knowledge into fine consideration
simultaneously. In this paper, we propose a novel ontology embedding approach
named EIKE (Extensional and Intensional Knowledge Embedding) by representing
ontologies in two spaces, called extensional space and intensional space. EIKE
presents a unified framework for embedding instances, concepts and their
relations in an ontology, applying a geometry-based method to model extensional
knowledge and a pretrained language model to model intensional knowledge, which
can capture both structure information and textual information. Experimental
results show that EIKE significantly outperforms state-of-the-art methods in
three datasets for both triple classification and link prediction, indicating
that EIKE provides a more comprehensive and representative perspective of the
domain.
\\ ( https://arxiv.org/abs/2402.01677 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01786
Date: Thu, 1 Feb 2024 21:51:09 GMT   (13730kb,D)

Title: COA-GPT: Generative Pre-trained Transformers for Accelerated Course of
  Action Development in Military Operations
Authors: Vinicius G. Goecks, Nicholas Waytowich
Categories: cs.AI cs.CL cs.HC cs.LG
Comments: Submitted to the NATO Science and Technology Organization Symposium
  (ICMCIS) organized by the Information Systems Technology (IST) Panel,
  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
ACM-class: I.2.6; I.2.7; J.7
\\
  The development of Courses of Action (COAs) in military operations is
traditionally a time-consuming and intricate process. Addressing this
challenge, this study introduces COA-GPT, a novel algorithm employing Large
Language Models (LLMs) for rapid and efficient generation of valid COAs.
COA-GPT incorporates military doctrine and domain expertise to LLMs through
in-context learning, allowing commanders to input mission information - in both
text and image formats - and receive strategically aligned COAs for review and
approval. Uniquely, COA-GPT not only accelerates COA development, producing
initial COAs within seconds, but also facilitates real-time refinement based on
commander feedback. This work evaluates COA-GPT in a military-relevant scenario
within a militarized version of the StarCraft II game, comparing its
performance against state-of-the-art reinforcement learning algorithms. Our
results demonstrate COA-GPT's superiority in generating strategically sound
COAs more swiftly, with added benefits of enhanced adaptability and alignment
with commander intentions. COA-GPT's capability to rapidly adapt and update
COAs during missions presents a transformative potential for military planning,
particularly in addressing planning discrepancies and capitalizing on emergent
windows of opportunities.
\\ ( https://arxiv.org/abs/2402.01786 ,  13730kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01817
Date: Fri, 2 Feb 2024 14:43:18 GMT   (4551kb,D)

Title: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
Authors: Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit
  Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy
Categories: cs.AI cs.LG
\\
  There is considerable confusion about the role of Large Language Models
(LLMs) in planning and reasoning tasks. On one side are over-optimistic claims
that LLMs can indeed do these tasks with just the right prompting or
self-verification strategies. On the other side are perhaps over-pessimistic
claims that all that LLMs are good for in planning/reasoning tasks are as mere
translators of the problem specification from one syntactic format to another,
and ship the problem off to external symbolic solvers. In this position paper,
we take the view that both these extremes are misguided. We argue that
auto-regressive LLMs cannot, by themselves, do planning or self-verification
(which is after all a form of reasoning), and shed some light on the reasons
for misunderstandings in the literature. We will also argue that LLMs should be
viewed as universal approximate knowledge sources that have much more
meaningful roles to play in planning/reasoning tasks beyond simple
front-end/back-end format translators. We present a vision of {\bf LLM-Modulo
Frameworks} that combine the strengths of LLMs with external model-based
verifiers in a tighter bi-directional interaction regime. We will show how the
models driving the external verifiers themselves can be acquired with the help
of LLMs. We will also argue that rather than simply pipelining LLMs and
symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic
approach that offers tighter integration between LLMs and symbolic components,
and allows extending the scope of model-based planning/reasoning regimes
towards more flexible knowledge, problem and preference specifications.
\\ ( https://arxiv.org/abs/2402.01817 ,  4551kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01889
Date: Fri, 2 Feb 2024 20:33:14 GMT   (2265kb,D)

Title: The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning
Authors: Daniel Cunnington, Mark Law, Jorge Lobo, Alessandra Russo
Categories: cs.AI cs.LG
Comments: Pre-print
\\
  Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI
systems, as interpretable symbolic techniques provide formal behaviour
guarantees. The challenge is how to effectively integrate neural and symbolic
computation, to enable learning and reasoning from raw data. Existing pipelines
that train the neural and symbolic components sequentially require extensive
labelling, whereas end-to-end approaches are limited in terms of scalability,
due to the combinatorial explosion in the symbol grounding problem. In this
paper, we leverage the implicit knowledge within foundation models to enhance
the performance in NeSy tasks, whilst reducing the amount of data labelling and
manual engineering. We introduce a new architecture, called NeSyGPT, which
fine-tunes a vision-language foundation model to extract symbolic features from
raw data, before learning a highly expressive answer set program to solve a
downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has
superior accuracy over various baselines, and can scale to complex NeSy tasks.
Finally, we highlight the effective use of a large language model to generate
the programmatic interface between the neural and symbolic components,
significantly reducing the amount of manual engineering required.
\\ ( https://arxiv.org/abs/2402.01889 ,  2265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02033
Date: Sat, 3 Feb 2024 05:14:03 GMT   (13kb)

Title: Benchmark for CEC 2024 Competition on Multiparty Multiobjective
  Optimization
Authors: Wenjian Luo, Peilan Xu, Shengxiang Yang, Yuhui Shi
Categories: cs.AI cs.NE
Comments: 15 pages, 0 figures
\\
  The competition focuses on Multiparty Multiobjective Optimization Problems
(MPMOPs), where multiple decision makers have conflicting objectives, as seen
in applications like UAV path planning. Despite their importance, MPMOPs remain
understudied in comparison to conventional multiobjective optimization. The
competition aims to address this gap by encouraging researchers to explore
tailored modeling approaches. The test suite comprises two parts: problems with
common Pareto optimal solutions and Biparty Multiobjective UAV Path Planning
(BPMO-UAVPP) problems with unknown solutions. Optimization algorithms for the
first part are evaluated using Multiparty Inverted Generational Distance
(MPIGD), and the second part is evaluated using Multiparty Hypervolume (MPHV)
metrics. The average algorithm ranking across all problems serves as a
performance benchmark.
\\ ( https://arxiv.org/abs/2402.02033 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02053
Date: Sat, 3 Feb 2024 06:16:28 GMT   (3783kb,D)

Title: Affordable Generative Agents
Authors: Yangbin Yu, Qin Zhang, Junyou Li, Qiang Fu, Deheng Ye
Categories: cs.AI cs.HC
\\
  The emergence of large language models (LLMs) has significantly advanced the
simulation of believable interactive agents. However, the substantial cost on
maintaining the prolonged agent interactions poses challenge over the
deployment of believable LLM-based agents. Therefore, in this paper, we develop
Affordable Generative Agents (AGA), a framework for enabling the generation of
believable and low-cost interactions on both agent-environment and inter-agents
levels. Specifically, for agent-environment interactions, we substitute
repetitive LLM inferences with learned policies; while for inter-agent
interactions, we model the social relationships between agents and compress
auxiliary dialogue information. Extensive experiments on multiple environments
show the effectiveness and efficiency of our proposed framework. Also, we delve
into the mechanisms of emergent believable behaviors lying in LLM agents,
demonstrating that agents can only generate finite behaviors in fixed
environments, based upon which, we understand ways to facilitate emergent
interaction behaviors. Our code is publicly available at:
\url{https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents}.
\\ ( https://arxiv.org/abs/2402.02053 ,  3783kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02146
Date: Sat, 3 Feb 2024 13:28:35 GMT   (256kb,D)

Title: Emergency Computing: An Adaptive Collaborative Inference Method Based on
  Hierarchical Reinforcement Learning
Authors: Weiqi Fu, Lianming Xu, Xin Wu, Li Wang, Aiguo Fei
Categories: cs.AI cs.NI
\\
  In achieving effective emergency response, the timely acquisition of
environmental information, seamless command data transmission, and prompt
decision-making are crucial. This necessitates the establishment of a resilient
emergency communication dedicated network, capable of providing communication
and sensing services even in the absence of basic infrastructure. In this
paper, we propose an Emergency Network with Sensing, Communication,
Computation, Caching, and Intelligence (E-SC3I). The framework incorporates
mechanisms for emergency computing, caching, integrated communication and
sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large
user base, reliable data transmission over unstable links, and dynamic network
deployment in a changing environment. However, these advantages come at the
cost of significant computation overhead. Therefore, we specifically
concentrate on emergency computing and propose an adaptive collaborative
inference method (ACIM) based on hierarchical reinforcement learning.
Experimental results demonstrate our method's ability to achieve rapid
inference of AI models with constrained computational and communication
resources.
\\ ( https://arxiv.org/abs/2402.02146 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02164
Date: Sat, 3 Feb 2024 14:24:21 GMT   (433kb)

Title: TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular
  Representation
Authors: Juan-Ni Wu, Tong Wang, Li-Juan Tang, Hai-Long Wu, Ru-Qin Yu
Categories: cs.AI q-bio.BM
Comments: 7 pages, 3 figures, 2 tables
\\
  String-based molecular representations, such as SMILES, are a de facto
standard for linearly representing molecular information. However, the must be
paired symbols and the parsing algorithm result in long grammatical
dependencies, making it difficult for even state-of-the-art deep learning
models to accurately comprehend the syntax and semantics. Although DeepSMILES
and SELFIES have addressed certain limitations, they still struggle with
advanced grammar, which makes some strings difficult to read. This study
introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES
family. Comparative experiments between TSIS and another fragment-based linear
solution, SAFE, indicate that SAFE presents challenges in managing long-term
dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as
its foundational data structure, which sets it apart from the SAFE model. The
performance of TSIS models surpasses that of SAFE models, indicating that the
tree structure of the t-SMILES family provides certain advantages.
\\ ( https://arxiv.org/abs/2402.02164 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02330
Date: Sun, 4 Feb 2024 03:47:10 GMT   (4551kb,D)

Title: Enhance Reasoning for Large Language Models in the Game Werewolf
Authors: Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, Haobo
  Fu
Categories: cs.AI
\\
  This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.
\\ ( https://arxiv.org/abs/2402.02330 ,  4551kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02392
Date: Sun, 4 Feb 2024 08:11:45 GMT   (3950kb,D)

Title: DeLLMa: A Framework for Decision Making Under Uncertainty with Large
  Language Models
Authors: Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger
Categories: cs.AI cs.CL cs.LG
Comments: 23 pages, 17 figures
\\
  Large language models (LLMs) are increasingly used across society, including
in domains like business, engineering, and medicine. These fields often grapple
with decision-making under uncertainty, a critical yet challenging task. In
this paper, we show that directly prompting LLMs on these types of
decision-making problems yields poor results, especially as the problem
complexity increases. To overcome this limitation, we propose DeLLMa
(Decision-making Large Language Model assistant), a framework designed to
enhance decision-making accuracy in uncertain environments. DeLLMa involves a
multi-step scaffolding procedure, drawing upon principles from decision theory
and utility theory, to provide an optimal and human-auditable decision-making
process. We validate our framework on decision-making environments involving
real agriculture and finance data. Our results show that DeLLMa can
significantly improve LLM decision-making performance, achieving up to a 40%
increase in accuracy over competing methods.
\\ ( https://arxiv.org/abs/2402.02392 ,  3950kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02468
Date: Sun, 4 Feb 2024 13:02:27 GMT   (2073kb,D)

Title: Fast Peer Adaptation with Context-aware Exploration
Authors: Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou Wang
Categories: cs.AI cs.LG cs.MA
\\
  Fast adapting to unknown peers (partners or opponents) with different
strategies is a key challenge in multi-agent games. To do so, it is crucial for
the agent to efficiently probe and identify the peer's strategy, as this is the
prerequisite for carrying out the best response in adaptation. However, it is
difficult to explore the strategies of unknown peers, especially when the games
are partially observable and have a long horizon. In this paper, we propose a
peer identification reward, which rewards the learning agent based on how well
it can identify the behavior pattern of the peer over the historical context,
such as the observation over multiple episodes. This reward motivates the agent
to learn a context-aware policy for effective exploration and fast adaptation,
i.e., to actively seek and collect informative feedback from peers when
uncertain about their policies and to exploit the context to perform the best
response when confident. We evaluate our method on diverse testbeds that
involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed
(Predator-Prey-W) games with peer agents. We demonstrate that our method
induces more active exploration behavior, achieving faster adaptation and
better outcomes than existing methods.
\\ ( https://arxiv.org/abs/2402.02468 ,  2073kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02547
Date: Sun, 4 Feb 2024 15:50:42 GMT   (1499kb)

Title: Integration of cognitive tasks into artificial general intelligence test
  for large models
Authors: Youzhi Qu, Chen Wei, Penghui Du, Wenxin Che, Chi Zhang, Wanli Ouyang,
  Yatao Bian, Feiyang Xu, Bin Hu, Kai Du, Haiyan Wu, Jia Liu, Quanying Liu
Categories: cs.AI
\\
  During the evolution of large models, performance evaluation is necessarily
performed on the intermediate models to assess their capabilities, and on the
well-trained model to ensure safety before practical application. However,
current model evaluations mainly rely on specific tasks and datasets, lacking a
united framework for assessing the multidimensional intelligence of large
models. In this perspective, we advocate for a comprehensive framework of
artificial general intelligence (AGI) test, aimed at fulfilling the testing
needs of large language models and multi-modal large models with enhanced
capabilities. The AGI test framework bridges cognitive science and natural
language processing to encompass the full spectrum of intelligence facets,
including crystallized intelligence, a reflection of amassed knowledge and
experience; fluid intelligence, characterized by problem-solving and adaptive
reasoning; social intelligence, signifying comprehension and adaptation within
multifaceted social scenarios; and embodied intelligence, denoting the ability
to interact with its physical environment. To assess the multidimensional
intelligence of large models, the AGI test consists of a battery of
well-designed cognitive tests adopted from human intelligence tests, and then
naturally encapsulates into an immersive virtual community. We propose that the
complexity of AGI testing tasks should increase commensurate with the
advancements in large models. We underscore the necessity for the
interpretation of test results to avoid false negatives and false positives. We
believe that cognitive science-inspired AGI tests will effectively guide the
targeted improvement of large models in specific dimensions of intelligence and
accelerate the integration of large models into human society.
\\ ( https://arxiv.org/abs/2402.02547 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02611
Date: Sun, 4 Feb 2024 20:56:09 GMT   (7072kb,D)

Title: PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial
  Reasoning Problems?
Authors: Chinmay Mittal, Krishna Kartik, Mausam, Parag Singla
Categories: cs.AI cs.CL cs.LG
\\
  Recent works have explored the use of LLMs for reasoning tasks focussing on
relatively simple problems, such as logical question answering. In our work, we
wish to tackle more complicated problems, significantly expanding the
capabilities of these models. Particularly, we explore whether LLMs can solve
challenging first-order combinatorial reasoning problems, an example being the
popular puzzle Sudoku. These problems have an underlying first-order structure
described by a general description in natural language and can be instantiated
to instances of varying sizes. Moreover these problems are computationally
intensive requiring several reasoning steps to reach the solution. We present
PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even
when aided by symbolic solvers perform rather poorly on our benchmark. In
response we propose a new approach, Puzzle-LM which combines LLMs with both
symbolic solvers and program interpreters enabling them to reason about such
challenging problems. We also show how feedback from smaller solved instances
can help improve this reasoning ability.
\\ ( https://arxiv.org/abs/2402.02611 ,  7072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02658
Date: Mon, 5 Feb 2024 00:57:51 GMT   (777kb,D)

Title: Multi-step Problem Solving Through a Verifier: An Empirical Analysis on
  Model-induced Process Supervision
Authors: Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu,
  Jingbo Shang
Categories: cs.AI cs.CL cs.LG
\\
  Process supervision, using a trained verifier to evaluate the intermediate
steps generated by reasoner, has demonstrated significant improvements in
multi-step problem solving. In this paper, to avoid expensive human annotation
effort on the verifier training data, we introduce Model-induced Process
Supervision (MiPS), a novel method for automating data curation. MiPS annotates
an intermediate step by sampling completions of this solution through the
reasoning model, and obtaining an accuracy defined as the proportion of correct
completions. Errors in the reasoner would cause MiPS to underestimate the
accuracy of intermediate steps, therefore, we suggest and empirically show that
verification focusing on high predicted scores of the verifier shall be
preferred over that of low predicted scores, contrary to prior work. Our
approach significantly improves the performance of PaLM 2 on math and coding
tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with
an output supervision trained verifier). Additionally, our study demonstrates
that the verifier exhibits strong generalization ability across different
reasoning models.
\\ ( https://arxiv.org/abs/2402.02658 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02716
Date: Mon, 5 Feb 2024 04:25:24 GMT   (324kb,D)

Title: Understanding the planning of LLM agents: A survey
Authors: Xu Huang and Weiwen Liu and Xiaolong Chen and Xingmei Wang and Hao
  Wang and Defu Lian and Yasheng Wang and Ruiming Tang and Enhong Chen
Categories: cs.AI cs.LG
Comments: 9 pages, 2 tables, 2 figures
\\
  As Large Language Models (LLMs) have shown significant intelligence, the
progress to leverage LLMs as planning modules of autonomous agents has
attracted more attention. This survey provides the first systematic view of
LLM-based agents planning, covering recent works aiming to improve planning
ability. We provide a taxonomy of existing works on LLM-Agent planning, which
can be categorized into Task Decomposition, Plan Selection, External Module,
Reflection and Memory. Comprehensive analyses are conducted for each direction,
and further challenges for the field of research are discussed.
\\ ( https://arxiv.org/abs/2402.02716 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02805
Date: Mon, 5 Feb 2024 08:26:33 GMT   (2886kb,D)

Title: Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
Authors: Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang,
  Anthony Cohn, Janet B. Pierrehumbert
Categories: cs.AI cs.CL cs.LG
\\
  Reasoning about asynchronous plans is challenging since it requires
sequential and parallel planning to optimize time costs. Can large language
models (LLMs) succeed at this task? Here, we present the first large-scale
study investigating this question. We find that a representative set of closed
and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not
supplied with illustrations about the task-solving process in our benchmark
AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that
combines graphs with natural language prompts and achieves state-of-the-art
results. We show that although PLaG can boost model performance, LLMs still
suffer from drastic degradation when task complexity increases, highlighting
the limits of utilizing LLMs for simulating digital devices. We see our study
as an exciting step towards using LLMs as efficient autonomous agents.
\\ ( https://arxiv.org/abs/2402.02805 ,  2886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02885
Date: Mon, 5 Feb 2024 10:54:14 GMT   (681kb,D)

Title: A Review on Building Blocks of Decentralized Artificial Intelligence
Authors: Vid Kersic, Muhamed Turkanovic
Categories: cs.AI cs.CR cs.DC
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Artificial intelligence is transforming our lives, and technological progress
and transfer from the academic and theoretical sphere to the real world are
accelerating yearly. But during that progress and transition, several open
problems and questions need to be addressed for the field to develop ethically,
such as digital privacy, ownership, and control. These are some of the reasons
why the currently most popular approaches of artificial intelligence, i.e.,
centralized AI (CEAI), are questionable, with other directions also being
widely explored, such as decentralized artificial intelligence (DEAI), to solve
some of the most reaching problems. This paper provides a systematic literature
review (SLR) of existing work in the field of DEAI, presenting the findings of
71 identified studies. The paper's primary focus is identifying the building
blocks of DEAI solutions and networks, tackling the DEAI analysis from a
bottom-up approach. In the end, future directions of research and open problems
are proposed.
\\ ( https://arxiv.org/abs/2402.02885 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02978
Date: Mon, 5 Feb 2024 13:06:35 GMT   (744kb,D)

Title: Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL
Authors: Haya Majid Qureshi, Wolfgang Faber
Categories: cs.AI cs.LO
Comments: Under consideration in Theory and Practice of Logic Programming
  (TPLP)
\\
  Metamodeling is a general approach to expressing knowledge about classes and
properties in an ontology. It is a desirable modeling feature in multiple
applications that simplifies the extension and reuse of ontologies.
Nevertheless, allowing metamodeling without restrictions is problematic for
several reasons, mainly due to undecidability issues. Practical languages,
therefore, forbid classes to occur as instances of other classes or treat such
occurrences as semantically different objects. Specifically, meta-querying in
SPARQL under the Direct Semantic Entailment Regime (DSER) uses the latter
approach, thereby effectively not supporting meta-queries. However, several
extensions enabling different metamodeling features have been proposed over the
last decade. This paper deals with the Metamodeling Semantics (MS) over OWL 2
QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in
Lenzerini et al. (2015) and Lenzerini et al. (2020); Cima et al. (2017). A
reduction from OWL 2 QL to Datalog for meta-querying was proposed in Cima et
al. (2017). In this paper, we experiment with various logic programming tools
that support Datalog querying to determine their suitability as back-ends to
MSER query answering. These tools stem from different logic programming
paradigms (Prolog, pure Datalog, Answer Set Programming, Hybrid Knowledge
Bases). Our work shows that the Datalog approach to MSER querying is practical
also for sizeable ontologies with limited resources (time and memory). This
paper significantly extends Qureshi & Faber (2021) by a more detailed
experimental analysis and more background. Under consideration in Theory and
Practice of Logic Programming (TPLP).
\\ ( https://arxiv.org/abs/2402.02978 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03136
Date: Mon, 5 Feb 2024 16:03:44 GMT   (5099kb,D)

Title: Mastering Zero-Shot Interactions in Cooperative and Competitive
  Simultaneous Games
Authors: Yannik Mahlau, Frederik Schubert, Bodo Rosenhahn
Categories: cs.AI cs.GT
\\
  The combination of self-play and planning has achieved great successes in
sequential games, for instance in Chess and Go. However, adapting algorithms
such as AlphaZero to simultaneous games poses a new challenge. In these games,
missing information about concurrent actions of other agents is a limiting
factor as they may select different Nash equilibria or do not play optimally at
all. Thus, it is vital to model the behavior of the other agents when
interacting with them in simultaneous games. To this end, we propose Albatross:
AlphaZero for Learning Bounded-rational Agents and Temperature-based Response
Optimization using Simulated Self-play. Albatross learns to play the novel
equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which
enables cooperation and competition with agents of any playing strength. We
perform an extensive evaluation of Albatross on a set of cooperative and
competitive simultaneous perfect-information games. In contrast to AlphaZero,
Albatross is able to exploit weak agents in the competitive game of
Battlesnake. Additionally, it yields an improvement of 37.6% compared to
previous state of the art in the cooperative Overcooked benchmark.
\\ ( https://arxiv.org/abs/2402.03136 ,  5099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03164
Date: Mon, 5 Feb 2024 16:32:12 GMT   (78kb)

Title: Decidable Reasoning About Time in Finite-Domain Situation Calculus
  Theories
Authors: Till Hofmann, Stefan Schupp, Gerhard Lakemeyer
Categories: cs.AI
\\
  Representing time is crucial for cyber-physical systems and has been studied
extensively in the Situation Calculus. The most commonly used approach
represents time by adding a real-valued fluent $\mathit{time}(a)$ that attaches
a time point to each action and consequently to each situation. We show that in
this approach, checking whether there is a reachable situation that satisfies a
given formula is undecidable, even if the domain of discourse is restricted to
a finite set of objects. We present an alternative approach based on
well-established results from timed automata theory by introducing clocks as
real-valued fluents with restricted successor state axioms and comparison
operators. %that only allow comparisons against fixed rationals. With this
restriction, we can show that the reachability problem for finite-domain basic
action theories is decidable. Finally, we apply our results on Golog program
realization by presenting a decidable procedure for determining an action
sequence that is a successful execution of a given program.
\\ ( https://arxiv.org/abs/2402.03164 ,  78kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03181
Date: Mon, 5 Feb 2024 16:46:16 GMT   (14509kb,D)

Title: C-RAG: Certified Generation Risks for Retrieval-Augmented Language
  Models
Authors: Mintong Kang, Nezihe Merve G\"urel, Ning Yu, Dawn Song, Bo Li
Categories: cs.AI
\\
  Despite the impressive capabilities of large language models (LLMs) across
diverse applications, they still suffer from trustworthiness issues, such as
hallucinations and misalignments. Retrieval-augmented language models (RAG)
have been proposed to enhance the credibility of generations by grounding
external knowledge, but the theoretical understandings of their generation
risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed
lead to low generation risks, 2) how to provide provable guarantees on the
generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions
enable RAG models to reduce generation risks. We propose C-RAG, the first
framework to certify generation risks for RAG models. Specifically, we provide
conformal risk analysis for RAG models and certify an upper confidence bound of
generation risks, which we refer to as conformal generation risk. We also
provide theoretical guarantees on conformal generation risks for general
bounded risk functions under test distribution shifts. We prove that RAG
achieves a lower conformal generation risk than that of a single LLM when the
quality of the retrieval model and transformer is non-trivial. Our intensive
empirical results demonstrate the soundness and tightness of our conformal
generation risk guarantees across four widely-used NLP datasets on four
state-of-the-art retrieval models.
\\ ( https://arxiv.org/abs/2402.03181 ,  14509kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03310
Date: Mon, 5 Feb 2024 18:59:36 GMT   (34142kb,D)

Title: V-IRL: Grounding Virtual Intelligence in Real Life
Authors: Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, Saining Xie
Categories: cs.AI cs.CV
Comments: Project page: https://virl-platform.github.io
\\
  There is a sensory gulf between the Earth that humans inhabit and the digital
realms in which modern AI agents are created. To develop AI agents that can
sense, think, and act as flexibly as humans in real-world settings, it is
imperative to bridge the realism gap between the digital and physical worlds.
How can we embody agents in an environment as rich and diverse as the one we
inhabit, without the constraints imposed by real hardware and control? Towards
this end, we introduce V-IRL: a platform that enables agents to scalably
interact with the real world in a virtual yet realistic environment. Our
platform serves as a playground for developing agents that can accomplish
various practical tasks and as a vast testbed for measuring progress in
capabilities spanning perception, decision-making, and interaction with
real-world data across the entire globe.
\\ ( https://arxiv.org/abs/2402.03310 ,  34142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01641
Date: Thu, 28 Dec 2023 20:44:26 GMT   (722kb,D)

Title: Universal Syntactic Structures: Modeling Syntax for Various Natural
  Languages
Authors: Min K. Kim, Hafu Takero, Sara Fedovik
Categories: cs.CL
Comments: 22 pages, 9 figures
\\
  We aim to provide an explanation for how the human brain might connect words
for sentence formation. A novel approach to modeling syntactic representation
is introduced, potentially showing the existence of universal syntactic
structures for all natural languages. As the discovery of DNA's double helix
structure shed light on the inner workings of genetics, we wish to introduce a
basic understanding of how language might work in the human brain. It could be
the brain's way of encoding and decoding knowledge. It also brings some insight
into theories in linguistics, psychology, and cognitive science. After looking
into the logic behind universal syntactic structures and the methodology of the
modeling technique, we attempt to analyze corpora that showcase universality in
the language process of different natural languages such as English and Korean.
Lastly, we discuss the critical period hypothesis, universal grammar, and a few
other assertions on language for the purpose of advancing our understanding of
the human brain.
\\ ( https://arxiv.org/abs/2402.01641 ,  722kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01642
Date: Tue, 2 Jan 2024 01:44:15 GMT   (873kb)

Title: Detection of Machine-Generated Text: Literature Survey
Authors: Dmytro Valiaiev
Categories: cs.CL cs.LG
\\
  Since language models produce fake text quickly and easily, there is an
oversupply of such content in the public domain. The degree of sophistication
and writing style has reached a point where differentiating between human
authored and machine-generated content is nearly impossible. As a result, works
generated by language models rather than human authors have gained significant
media attention and stirred controversy.Concerns regarding the possible
influence of advanced language models on society have also arisen, needing a
fuller knowledge of these processes. Natural language generation (NLG) and
generative pre-trained transformer (GPT) models have revolutionized a variety
of sectors: the scope not only permeated throughout journalism and customer
service but also reached academia. To mitigate the hazardous implications that
may arise from the use of these models, preventative measures must be
implemented, such as providing human agents with the capacity to distinguish
between artificially made and human composed texts utilizing automated systems
and possibly reverse-engineered language models. Furthermore, to ensure a
balanced and responsible approach, it is critical to have a full grasp of the
socio-technological ramifications of these breakthroughs. This literature
survey aims to compile and synthesize accomplishments and developments in the
aforementioned work, while also identifying future prospects. It also gives an
overview of machine-generated text trends and explores the larger societal
implications. Ultimately, this survey intends to contribute to the development
of robust and effective approaches for resolving the issues connected with the
usage and detection of machine-generated text by exploring the interplay
between the capabilities of language models and their possible implications.
\\ ( https://arxiv.org/abs/2402.01642 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01643
Date: Thu, 21 Dec 2023 01:47:49 GMT   (212kb,D)

Title: L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
Authors: Md. Kowsher, Md. Shohanur Islam Sobuj, Asif Mahmud, Nusrat Jahan
  Prottasha and Prakash Bhat
Categories: cs.CL cs.AI cs.LG
\\
  Efficiently fine-tuning Large Language Models (LLMs) for specific tasks
presents a considerable challenge in natural language processing. Traditional
methods, like prompt or prefix tuning, typically rely on arbitrary tokens for
training, leading to prolonged training times and generalized token use across
various class labels. To address these issues, this paper introduces L-Tuning,
an efficient fine-tuning approach designed for classification tasks within the
Natural Language Inference (NLI) framework. Diverging from conventional
methods, L-Tuning focuses on the fine-tuning of label tokens processed through
a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This
technique not only improves the fine-tuning accuracy and efficiency but also
facilitates the generation of distinct label embeddings for each class,
enhancing the model's training nuance. Our experimental results indicate a
significant improvement in training efficiency and classification accuracy with
L-Tuning compared to traditional approaches, marking a promising advancement in
fine-tuning LLMs for complex language tasks. \\ Code is available at:
\textcolor{red}{\href{https://github.com/Kowsher/L-Tuning}{\texttt{https://github.com/Kowsher/L-Tuning}}}.
\\ ( https://arxiv.org/abs/2402.01643 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01661
Date: Sat, 13 Jan 2024 18:42:27 GMT   (822kb)

Title: Tracing the Genealogies of Ideas with Large Language Model Embeddings
Authors: Lucian Li
Categories: cs.CL cs.SI
\\
  In this paper, I present a novel method to detect intellectual influence
across a large corpus. Taking advantage of the unique affordances of large
language models in encoding semantic and structural meaning while remaining
robust to paraphrasing, we can search for substantively similar ideas and hints
of intellectual influence in a computationally efficient manner. Such a method
allows us to operationalize different levels of confidence: we can allow for
direct quotation, paraphrase, or speculative similarity while remaining open
about the limitations of each threshold. I apply an ensemble method combining
General Text Embeddings, a state-of-the-art sentence embedding method optimized
to capture semantic content and an Abstract Meaning Representation graph
representation designed to capture structural similarities in argumentation
style and the use of metaphor. I apply this method to vectorize sentences from
a corpus of roughly 400,000 nonfiction books and academic publications from the
19th century for instances of ideas and arguments appearing in Darwin's
publications. This functions as an initial evaluation and proof of concept; the
method is not limited to detecting Darwinian ideas but is capable of detecting
similarities on a large scale in a wide range of corpora and contexts.
\\ ( https://arxiv.org/abs/2402.01661 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01676
Date: Fri, 19 Jan 2024 19:36:54 GMT   (74kb,D)

Title: Language models align with human judgments on key grammatical
  constructions
Authors: Jennifer Hu, Kyle Mahowald, Gary Lupyan, Anna Ivanova, Roger Levy
Categories: cs.CL cs.AI
Comments: Response to Dentella et al. (2023)
\\
  Do Large Language Models (LLMs) make human-like linguistic generalizations?
Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence
grammatically correct in English?") to elicit grammaticality judgments of 80
English sentences, concluding that LLMs demonstrate a "yes-response bias" and a
"failure to distinguish grammatical from ungrammatical sentences". We
re-evaluate LLM performance using well-established practices and find that
DGL's data in fact provide evidence for just how well LLMs capture human
behaviors. Models not only achieve high accuracy overall, but also capture
fine-grained variation in human linguistic judgments.
\\ ( https://arxiv.org/abs/2402.01676 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01679
Date: Sat, 20 Jan 2024 13:44:21 GMT   (9133kb,D)

Title: StickerConv: Generating Multimodal Empathetic Responses from Scratch
Authors: Yiqun Zhang, Fanheng Kong, Peidong Wang, Shuang Sun, Lingshuai Wang,
  Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song
Categories: cs.CL cs.AI
Comments: On going work
\\
  Stickers, while widely recognized for enhancing empathetic communication in
online interactions, remain underexplored in current empathetic dialogue
research. In this paper, we introduce the Agent for StickerConv (Agent4SC),
which uses collaborative agent interactions to realistically simulate human
behavior with sticker usage, thereby enhancing multimodal empathetic
communication. Building on this foundation, we develop a multimodal empathetic
dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K
unique stickers, and 2K diverse conversational scenarios, specifically designs
to augment the generation of empathetic responses in a multimodal context. To
leverage the richness of this dataset, we propose PErceive and Generate
Stickers (PEGS), a multimodal empathetic response generation model,
complemented by a comprehensive set of empathy evaluation metrics based on LLM.
Our experiments demonstrate PEGS's effectiveness in generating contextually
relevant and emotionally resonant multimodal empathetic responses, contributing
to the advancement of more nuanced and engaging empathetic dialogue systems.
Our project page is available at https://neu-datamining.github.io/StickerConv .
\\ ( https://arxiv.org/abs/2402.01679 ,  9133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01680
Date: Sun, 21 Jan 2024 23:36:14 GMT   (5000kb,D)

Title: Large Language Model based Multi-Agents: A Survey of Progress and
  Challenges
Authors: Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei,
  Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang
Categories: cs.CL cs.AI cs.MA
Comments: This work is ongoing and we welcome your contribution!
\\
  Large Language Models (LLMs) have achieved remarkable success across a wide
array of tasks. Due to the impressive planning and reasoning abilities of LLMs,
they have been used as autonomous agents to do many tasks automatically.
Recently, based on the development of using one LLM as a single planning or
decision-making agent, LLM-based multi-agent systems have achieved considerable
progress in complex problem-solving and world simulation. To provide the
community with an overview of this dynamic field, we present this survey to
offer an in-depth discussion on the essential aspects of multi-agent systems
based on LLMs, as well as the challenges. Our goal is for readers to gain
substantial insights on the following questions: What domains and environments
do LLM-based multi-agents simulate? How are these agents profiled and how do
they communicate? What mechanisms contribute to the growth of agents'
capacities? For those interested in delving into this field of study, we also
summarize the commonly used datasets or benchmarks for them to have convenient
access. To keep researchers updated on the latest studies, we maintain an
open-source GitHub repository, dedicated to outlining the research on LLM-based
multi-agent systems.
\\ ( https://arxiv.org/abs/2402.01680 ,  5000kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01681
Date: Mon, 22 Jan 2024 06:02:39 GMT   (18715kb,D)

Title: Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social
  Media Communications
Authors: Yuhang Zhou, Paiheng Xu, Xiyao Wang, Xuan Lu, Ge Gao, Wei Ai
Categories: cs.CL cs.AI
Comments: 12 pages, 2 page appendix
\\
  Emojis, which encapsulate semantics beyond mere words or phrases, have become
prevalent in social network communications. This has spurred increasing
scholarly interest in exploring their attributes and functionalities. However,
emoji-related research and application face two primary challenges. First,
researchers typically rely on crowd-sourcing to annotate emojis in order to
understand their sentiments, usage intentions, and semantic meanings. Second,
subjective interpretations by users can often lead to misunderstandings of
emojis and cause the communication barrier. Large Language Models (LLMs) have
achieved significant success in various annotation tasks, with ChatGPT
demonstrating expertise across multiple domains. In our study, we assess
ChatGPT's effectiveness in handling previously annotated and downstream tasks.
Our objective is to validate the hypothesis that ChatGPT can serve as a viable
alternative to human annotators in emoji research and that its ability to
explain emoji meanings can enhance clarity and transparency in online
communications. Our findings indicate that ChatGPT has extensive knowledge of
emojis. It is adept at elucidating the meaning of emojis across various
application scenarios and demonstrates the potential to replace human
annotators in a range of tasks.
\\ ( https://arxiv.org/abs/2402.01681 ,  18715kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01684
Date: Mon, 22 Jan 2024 07:58:31 GMT   (581kb,D)

Title: A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs
  Using the CGC-LORA Algorithm
Authors: Chao Song and Zhihao Ye and Qiqiang Lin and Qiuying Peng and Jun Wang
Categories: cs.CL cs.AI cs.LG
\\
  With the productive evolution of large language models (LLMs) in the field of
natural language processing (NLP), tons of effort has been made to effectively
fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or
multiple specific domain. In practice, there are two prevailing ways, in which
the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained
LLMs are fine-tuned a few times independently using the corresponding training
samples from each task. (ii) An Integrated Model: Samples from all tasks are
employed to fine-tune a pre-trianed LLM unitedly. To address the high computing
cost and seesawing issue simultaneously, we propose a unified framework that
implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel
Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work
aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme.
For a given cluster of tasks, we design an innovative layer that contains two
types of experts as additional trainable parameters to make LoRA be compatible
with MTL. To comprehensively evaluate the proposed framework, we conduct
well-designed experiments on two public datasets. The experimental results
demonstrate that the unified framework with CGC-LoRA modules achieves higher
evaluation scores than all benchmarks on both two datasets.
\\ ( https://arxiv.org/abs/2402.01684 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01685
Date: Mon, 22 Jan 2024 08:47:50 GMT   (5625kb,D)

Title: SMUTF: Schema Matching Using Generative Tags and Hybrid Features
Authors: Yu Zhang, Mei Di, Haozheng Luo, Chenwei Xu, Richard Tzong-Han Tsai
Categories: cs.CL cs.AI cs.DB
\\
  We introduce SMUTF, a unique approach for large-scale tabular data schema
matching (SM), which assumes that supervised learning does not affect
performance in open-domain tasks, thereby enabling effective cross-domain
matching. This system uniquely combines rule-based feature engineering,
pre-trained language models, and generative large language models. In an
innovative adaptation inspired by the Humanitarian Exchange Language, we deploy
'generative tags' for each data column, enhancing the effectiveness of SM.
SMUTF exhibits extensive versatility, working seamlessly with any pre-existing
pre-trained embeddings, classification methods, and generative models.
  Recognizing the lack of extensive, publicly available datasets for SM, we
have created and open-sourced the HDXSM dataset from the public humanitarian
data. We believe this to be the most exhaustive SM dataset currently available.
In evaluations across various public datasets and the novel HDXSM dataset,
SMUTF demonstrated exceptional performance, surpassing existing
state-of-the-art models in terms of accuracy and efficiency, and} improving the
F1 score by 11.84% and the AUC of ROC by 5.08%.
\\ ( https://arxiv.org/abs/2402.01685 ,  5625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01690
Date: Tue, 23 Jan 2024 16:30:22 GMT   (2784kb,D)

Title: Linguistic-Based Mild Cognitive Impairment Detection Using Informative
  Loss
Authors: Ali Pourramezan Fard, Mohammad H. Mahoor, Muath Alsuhaibani and Hiroko
  H. Dodgec
Categories: cs.CL cs.LG
\\
  This paper presents a deep learning method using Natural Language Processing
(NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and
Normal Cognitive (NC) conditions in older adults. We propose a framework that
analyzes transcripts generated from video interviews collected within the
I-CONECT study project, a randomized controlled trial aimed at improving
cognitive functions through video chats. Our proposed NLP framework consists of
two Transformer-based modules, namely Sentence Embedding (SE) and Sentence
Cross Attention (SCA). First, the SE module captures contextual relationships
between words within each sentence. Subsequently, the SCA module extracts
temporal features from a sequence of sentences. This feature is then used by a
Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC.
To build a robust model, we propose a novel loss function, called InfoLoss,
that considers the reduction in entropy by observing each sequence of sentences
to ultimately enhance the classification accuracy. The results of our
comprehensive model evaluation using the I-CONECT dataset show that our
framework can distinguish between MCI and NC with an average area under the
curve of 84.75%.
\\ ( https://arxiv.org/abs/2402.01690 ,  2784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01692
Date: Tue, 23 Jan 2024 21:55:34 GMT   (1590kb,D)

Title: Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by
  Self-Supervised Representation Mixing and Embedding Initialization
Authors: Wei-Ping Huang, Sung-Feng Huang, Hung-yi Lee
Categories: cs.CL cs.LG
Comments: Accepted by ASRU 2023
\\
  This paper presents an effective transfer learning framework for language
adaptation in text-to-speech systems, with a focus on achieving language
adaptation using minimal labeled and unlabeled data. While many works focus on
reducing the usage of labeled data, very few consider minimizing the usage of
unlabeled data. By utilizing self-supervised features in the pretraining stage,
replacing the noisy portion of pseudo labels with these features during
fine-tuning, and incorporating an embedding initialization trick, our method
leverages more information from unlabeled data compared to conventional
approaches. Experimental results show that our framework is able to synthesize
intelligible speech in unseen languages with only 4 utterances of labeled data
and 15 minutes of unlabeled data. Our methodology continues to surpass
conventional techniques, even when a greater volume of data is accessible.
These findings highlight the potential of our data-efficient language
adaptation framework.
\\ ( https://arxiv.org/abs/2402.01692 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01693
Date: Tue, 23 Jan 2024 22:03:51 GMT   (2164kb)

Title: Quality of Answers of Generative Large Language Models vs Peer Patients
  for Interpreting Lab Test Results for Lay Patients: Evaluation Study
Authors: Zhe He, Balu Bhasuran, Qiao Jin, Shubo Tian, Karim Hanna, Cindy
  Shavor, Lisbeth Garcia Arguello, Patrick Murray, Zhiyong Lu
Categories: cs.CL cs.AI
\\
  Lab results are often confusing and hard to understand. Large language models
(LLMs) such as ChatGPT have opened a promising avenue for patients to get their
questions answered. We aim to assess the feasibility of using LLMs to generate
relevant, accurate, helpful, and unharmful responses to lab test-related
questions asked by patients and to identify potential issues that can be
mitigated with augmentation approaches. We first collected lab test results
related question and answer data from Yahoo! Answers and selected 53 QA pairs
for this study. Using the LangChain framework and ChatGPT web portal, we
generated responses to the 53 questions from four LLMs including GPT-4, Meta
LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their
answers using standard QA similarity-based evaluation metrics including ROUGE,
BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge
whether a target model has higher quality in terms of relevance, correctness,
helpfulness, and safety than the baseline model. Finally, we performed a manual
evaluation with medical experts for all the responses to seven selected
questions on the same four aspects. The results of Win Rate and medical expert
evaluation both showed that GPT-4's responses achieved better scores than all
the other LLM responses and human responses on all four aspects (relevance,
correctness, helpfulness, and safety). However, LLM responses occasionally also
suffer from a lack of interpretation in one's medical context, incorrect
statements, and lack of references. We find that compared to other three LLMs
and human answer from the Q&A website, GPT-4's responses are more accurate,
helpful, relevant, and safer. However, there are cases which GPT-4 responses
are inaccurate and not individualized. We identified a number of ways to
improve the quality of LLM responses.
\\ ( https://arxiv.org/abs/2402.01693 ,  2164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01694
Date: Tue, 23 Jan 2024 23:42:41 GMT   (2345kb,D)

Title: ARGS: Alignment as Reward-Guided Search
Authors: Maxim Khanov, Jirayu Burapacheep, Yixuan Li
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024
\\
  Aligning large language models with human objectives is paramount, yet common
approaches including RLHF suffer from unstable and resource-intensive training.
In response to this challenge, we introduce ARGS, Alignment as Reward-Guided
Search, a novel framework that integrates alignment into the decoding process,
eliminating the need for expensive RL training. By adjusting the model's
probabilistic predictions using a reward signal, ARGS generates texts with
semantic diversity while being aligned with human preferences, offering a
promising and flexible solution for aligning language models. Notably, ARGS
demonstrates consistent enhancements in average reward compared to baselines
across diverse alignment tasks and various model dimensions. For example, under
the same greedy-based decoding strategy, our method improves the average reward
by 19.56% relative to the baseline and secures a preference or tie score of
64.33% in GPT-4 evaluation. We believe that our framework, emphasizing
decoding-time alignment, paves the way for more responsive language models in
the future. Code is publicly available at:
\url{https://github.com/deeplearning-wisc/args}.
\\ ( https://arxiv.org/abs/2402.01694 ,  2345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01695
Date: Wed, 24 Jan 2024 03:11:36 GMT   (1569kb,D)

Title: Language-Guided World Models: A Model-Based Approach to AI Control
Authors: Alex Zhang, Khanh Nguyen, Jens Tuyls, Albert Lin, Karthik Narasimhan
Categories: cs.CL cs.AI cs.LG
\\
  Installing probabilistic world models into artificial agents opens an
efficient channel for humans to communicate with and control these agents. In
addition to updating agent policies, humans can modify their internal world
models in order to influence their decisions. The challenge, however, is that
currently existing world models are difficult for humans to adapt because they
lack a natural communication interface. Aimed at addressing this shortcoming,
we develop Language-Guided World Models (LWMs), which can capture environment
dynamics by reading language descriptions. These models enhance agent
communication efficiency, allowing humans to simultaneously alter their
behavior on multiple tasks with concise language feedback. They also enable
agents to self-learn from texts originally written to instruct humans. To
facilitate the development of LWMs, we design a challenging benchmark based on
the game of MESSENGER (Hanjie et al., 2021), requiring compositional
generalization to new language descriptions and environment dynamics. Our
experiments reveal that the current state-of-the-art Transformer architecture
performs poorly on this benchmark, motivating us to design a more robust
architecture. To showcase the practicality of our proposed LWMs, we simulate a
scenario where these models augment the interpretability and safety of an agent
by enabling it to generate and discuss plans with a human before execution. By
effectively incorporating language feedback on the plan, the models boost the
agent performance in the real environment by up to three times without
collecting any interactive experiences in this environment.
\\ ( https://arxiv.org/abs/2402.01695 ,  1569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01696
Date: Wed, 24 Jan 2024 04:44:42 GMT   (2241kb,D)

Title: HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text
  Classification
Authors: Vidit Jain, Mukund Rungta, Yuchen Zhuang, Yue Yu, Zeyu Wang, Mu Gao,
  Jeffrey Skolnick, Chao Zhang
Categories: cs.CL cs.LG
\\
  Hierarchical text classification (HTC) is a complex subtask under multi-label
text classification, characterized by a hierarchical label taxonomy and data
imbalance. The best-performing models aim to learn a static representation by
combining document and hierarchical label information. However, the relevance
of document sections can vary based on the hierarchy level, necessitating a
dynamic document representation. To address this, we propose HiGen, a
text-generation-based framework utilizing language models to encode dynamic
text representations. We introduce a level-guided loss function to capture the
relationship between text and label name semantics. Our approach incorporates a
task-specific pretraining strategy, adapting the language model to in-domain
knowledge and significantly enhancing performance for classes with limited
examples. Furthermore, we present a new and valuable dataset called ENZYME,
designed for HTC, which comprises articles from PubMed with the goal of
predicting Enzyme Commission (EC) numbers. Through extensive experiments on the
ENZYME dataset and the widely recognized WOS and NYT datasets, our methodology
demonstrates superior performance, surpassing existing approaches while
efficiently handling data and mitigating class imbalance. The data and code
will be released publicly.
\\ ( https://arxiv.org/abs/2402.01696 ,  2241kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01697
Date: Wed, 24 Jan 2024 10:09:11 GMT   (361kb,D)

Title: APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data
  Annotation
Authors: Yiming Zhu, Zhizhuo Yin, Ehsan-Ul Haq, Lik-Hang Lee, Gareth Tyson, Pan
  Hui
Categories: cs.CL
\\
  Recent research has highlighted the potential of LLM applications, like
ChatGPT, for performing label annotation on social computing text. However, it
is already well known that performance hinges on the quality of the input
prompts. To address this, there has been a flurry of research into prompt
tuning -- techniques and guidelines that attempt to improve the quality of
prompts. Yet these largely rely on manual effort and prior knowledge of the
dataset being annotated. To address this limitation, we propose APT-Pipe, an
automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts
to enhance ChatGPT's text classification performance on any given dataset. We
implement APT-Pipe and test it across twelve distinct text classification
datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher
weighted F1-score on nine out of twelve experimented datasets, with an
improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as
a framework by showing how it can be extended to support additional tuning
mechanisms.
\\ ( https://arxiv.org/abs/2402.01697 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01698
Date: Wed, 24 Jan 2024 10:50:01 GMT   (1991kb,D)

Title: Large language model empowered participatory urban planning
Authors: Zhilun Zhou, Yuming Lin, Yong Li
Categories: cs.CL cs.AI
Comments: 26 pages, 7 figures, 2 tables
\\
  Participatory urban planning is the mainstream of modern urban planning and
involves the active engagement of different stakeholders. However, the
traditional participatory paradigm encounters challenges in time and manpower,
while the generative planning tools fail to provide adjustable and inclusive
solutions. This research introduces an innovative urban planning approach
integrating Large Language Models (LLMs) within the participatory process. The
framework, based on the crafted LLM agent, consists of role-play, collaborative
generation, and feedback iteration, solving a community-level land-use task
catering to 1000 distinct interests. Empirical experiments in diverse urban
communities exhibit LLM's adaptability and effectiveness across varied planning
scenarios. The results were evaluated on four metrics, surpassing human experts
in satisfaction and inclusion, and rivaling state-of-the-art reinforcement
learning methods in service and ecology. Further analysis shows the advantage
of LLM agents in providing adjustable and inclusive solutions with natural
language reasoning and strong scalability. While implementing the recent
advancements in emulating human behavior for planning, this work envisions both
planners and citizens benefiting from low-cost, efficient LLM agents, which is
crucial for enhancing participation and realizing participatory urban planning.
\\ ( https://arxiv.org/abs/2402.01698 ,  1991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01700
Date: Wed, 24 Jan 2024 13:47:39 GMT   (772kb)

Title: Question answering systems for health professionals at the point of care
  -- a systematic review
Authors: Gregory Kell, Angus Roberts, Serge Umansky, Linglong Qian, Davide
  Ferrari, Frank Soboczenski, Byron Wallace, Nikhil Patel, Iain J Marshall
Categories: cs.CL cs.AI
Comments: Accepted to the Journal of the American Medical Informatics
  Association (JAMIA)
\\
  Objective: Question answering (QA) systems have the potential to improve the
quality of clinical care by providing health professionals with the latest and
most relevant evidence. However, QA systems have not been widely adopted. This
systematic review aims to characterize current medical QA systems, assess their
suitability for healthcare, and identify areas of improvement.
  Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library,
ACL Anthology and forward and backward citations on 7th February 2023. We
included peer-reviewed journal and conference papers describing the design and
evaluation of biomedical QA systems. Two reviewers screened titles, abstracts,
and full-text articles. We conducted a narrative synthesis and risk of bias
assessment for each study. We assessed the utility of biomedical QA systems.
  Results: We included 79 studies and identified themes, including question
realism, answer reliability, answer utility, clinical specialism, systems,
usability, and evaluation methods. Clinicians' questions used to train and
evaluate QA systems were restricted to certain sources, types and complexity
levels. No system communicated confidence levels in the answers or sources.
Many studies suffered from high risks of bias and applicability concerns. Only
8 studies completely satisfied any criterion for clinical utility, and only 7
reported user evaluations. Most systems were built with limited input from
clinicians.
  Discussion: While machine learning methods have led to increased accuracy,
most studies imperfectly reflected real-world healthcare information needs. Key
research priorities include developing more realistic healthcare QA datasets
and considering the reliability of answer sources, rather than merely focusing
on accuracy.
\\ ( https://arxiv.org/abs/2402.01700 ,  772kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01702
Date: Wed, 24 Jan 2024 17:57:12 GMT   (939kb,D)

Title: Fluent dreaming for language models
Authors: T. Ben Thompson (1), Zygimantas Straznickas (1), Michael Sklar (1)
  ((1) Confirm Labs)
Categories: cs.CL cs.AI
Comments: 11 pages, 6 figures, 4 tables
\\
  Feature visualization, also known as "dreaming", offers insights into vision
models by optimizing the inputs to maximize a neuron's activation or other
internal component. However, dreaming has not been successfully applied to
language models because the input space is discrete. We extend Greedy
Coordinate Gradient, a method from the language model adversarial attack
literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO
optimizes the input prompt to simultaneously maximize the Pareto frontier
between a chosen internal feature and prompt fluency, enabling fluent dreaming
for language models. We demonstrate dreaming with neurons, output logits and
arbitrary directions in activation space. We measure the fluency of the
resulting prompts and compare language model dreaming with max-activating
dataset examples. Critically, fluent dreaming allows automatically exploring
the behavior of model internals in reaction to mildly out-of-distribution
prompts. Code for running EPO is available at
https://github.com/Confirm-Solutions/dreamy. A companion page demonstrating
code usage is at https://confirmlabs.org/posts/dreamy.html
\\ ( https://arxiv.org/abs/2402.01702 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01704
Date: Wed, 24 Jan 2024 22:22:00 GMT   (3411kb,D)

Title: States as Strings as Strategies: Steering Language Models with
  Game-Theoretic Solvers
Authors: Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi,
  Luke Marris, Georgios Piliouras, Karl Tuyls
Categories: cs.CL cs.AI cs.GT
Comments: 32 pages, 8 figures, code available @
  https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py
\\
  Game theory is the study of mathematical models of strategic interactions
among rational agents. Language is a key medium of interaction for humans,
though it has historically proven difficult to model dialogue and its strategic
motivations mathematically. A suitable model of the players, strategies, and
payoffs associated with linguistic interactions (i.e., a binding to the
conventional symbolic logic of game theory) would enable existing
game-theoretic algorithms to provide strategic solutions in the space of
language. In other words, a binding could provide a route to computing stable,
rational conversational strategies in dialogue. Large language models (LLMs)
have arguably reached a point where their generative capabilities can enable
realistic, human-like simulations of natural dialogue. By prompting them in
various ways, we can steer their responses towards different output utterances.
Leveraging the expressivity of natural language, LLMs can also help us quickly
generate new dialogue scenarios, which are grounded in real world applications.
In this work, we present one possible binding from dialogue to game theory as
well as generalizations of existing equilibrium finding algorithms to this
setting. In addition, by exploiting LLMs generation capabilities along with our
proposed binding, we can synthesize a large repository of formally-defined
games in which one can study and test game-theoretic solution concepts. We also
demonstrate how one can combine LLM-driven game generation, game-theoretic
solvers, and imitation learning to construct a process for improving the
strategic capabilities of LLMs.
\\ ( https://arxiv.org/abs/2402.01704 ,  3411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01706
Date: Thu, 25 Jan 2024 02:57:40 GMT   (6295kb,D)

Title: MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse
  Worlds
Authors: Xiaolong Jin, Zhuo Zhang, Xiangyu Zhang
Categories: cs.CL cs.LG
\\
  Large Language Model (LLM) alignment aims to ensure that LLM outputs match
with human values. Researchers have demonstrated the severity of alignment
problems with a large spectrum of jailbreak techniques that can induce LLMs to
produce malicious content during conversations. Finding the corresponding
jailbreaking prompts usually requires substantial human intelligence or
computation resources. In this paper, we report that LLMs have different levels
of alignment in various contexts. As such, by systematically constructing many
contexts, called worlds, leveraging a Domain Specific Language describing
possible worlds (e.g., time, location, characters, actions and languages) and
the corresponding compiler, we can cost-effectively expose latent alignment
issues. Given the low cost of our method, we are able to conduct a large scale
study regarding LLM alignment issues in different worlds. Our results show that
our method outperforms the-state-of-the-art jailbreaking techniques on both
effectiveness and efficiency. In addition, our results indicate that existing
LLMs are extremely vulnerable to nesting worlds and programming language
worlds. They imply that existing alignment training focuses on the real-world
and is lacking in various (virtual) worlds where LLMs can be exploited.
\\ ( https://arxiv.org/abs/2402.01706 ,  6295kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01708
Date: Thu, 25 Jan 2024 11:47:06 GMT   (268kb,D)

Title: Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech
  Generators
Authors: Wiebke Hutiri, Oresiti Papakyriakopoulos, Alice Xiang
Categories: cs.CL cs.AI cs.CY eess.AS
Comments: 24 pages, 4 tables, 4 figures
\\
  The rapid and wide-scale adoption of AI to generate human speech poses a
range of significant ethical and safety risks to society that need to be
addressed. For example, a growing number of speech generation incidents are
associated with swatting attacks in the United States, where anonymous
perpetrators create synthetic voices that call police officers to close down
schools and hospitals, or to violently gain access to innocent citizens' homes.
Incidents like this demonstrate that multimodal generative AI risks and harms
do not exist in isolation, but arise from the interactions of multiple
stakeholders and technical AI systems. In this paper we analyse speech
generation incidents to study how patterns of specific harms arise. We find
that specific harms can be categorised according to the exposure of affected
individuals, that is to say whether they are a subject of, interact with,
suffer due to, or are excluded from speech generation systems. Similarly,
specific harms are also a consequence of the motives of the creators and
deployers of the systems. Based on these insights we propose a conceptual
framework for modelling pathways to ethical and safety harms of AI, which we
use to develop a taxonomy of harms of speech generators. Our relational
approach captures the complexity of risks and harms in sociotechnical AI
systems, and yields an extensible taxonomy that can support appropriate policy
interventions and decision making for responsible multimodal model development
and release of speech generators.
\\ ( https://arxiv.org/abs/2402.01708 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01712
Date: Thu, 25 Jan 2024 18:25:05 GMT   (3821kb,D)

Title: Socially Aware Synthetic Data Generation for Suicidal Ideation Detection
  Using Large Language Models
Authors: Hamideh Ghanadian, Isar Nejadgholi, Hussein Al Osman
Categories: cs.CL cs.AI cs.LG
Journal-ref: IEEE Access
DOI: 10.1109/ACCESS.2024.3358206
\\
  Suicidal ideation detection is a vital research area that holds great
potential for improving mental health support systems. However, the sensitivity
surrounding suicide-related data poses challenges in accessing large-scale,
annotated datasets necessary for training effective machine learning models. To
address this limitation, we introduce an innovative strategy that leverages the
capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to
create synthetic data for suicidal ideation detection. Our data generation
approach is grounded in social factors extracted from psychology literature and
aims to ensure coverage of essential information related to suicidal ideation.
In our study, we benchmarked against state-of-the-art NLP classification
models, specifically, those centered around the BERT family structures. When
trained on the real-world dataset, UMD, these conventional models tend to yield
F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed
by social factors, offers consistent F1-scores of 0.82 for both models,
suggesting that the richness of topics in synthetic data can bridge the
performance gap across different model complexities. Most impressively, when we
combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a
substantial increase in performance, achieving an F1-score of 0.88 on the UMD
test set. Such results underscore the cost-effectiveness and potential of our
approach in confronting major challenges in the field, such as data scarcity
and the quest for diversity in data representation.
\\ ( https://arxiv.org/abs/2402.01712 ,  3821kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01713
Date: Thu, 25 Jan 2024 20:14:50 GMT   (2366kb,D)

Title: Prompting Large Language Models for Zero-Shot Clinical Prediction with
  Structured Longitudinal Electronic Health Record Data
Authors: Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin
  Liao, Ewen M. Harrison, Liantao Ma, Chengwei Pan
Categories: cs.CL cs.AI cs.LG
\\
  The inherent complexity of structured longitudinal Electronic Health Records
(EHR) data poses a significant challenge when integrated with Large Language
Models (LLMs), which are traditionally tailored for natural language
processing. Motivated by the urgent need for swift decision-making during new
disease outbreaks, where traditional predictive models often fail due to a lack
of historical data, this research investigates the adaptability of LLMs, like
GPT-4, to EHR data. We particularly focus on their zero-shot capabilities,
which enable them to make predictions in scenarios in which they haven't been
explicitly trained. In response to the longitudinal, sparse, and
knowledge-infused nature of EHR data, our prompting approach involves taking
into account specific EHR characteristics such as units and reference ranges,
and employing an in-context learning strategy that aligns with clinical
contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets
demonstrate that with our elaborately designed prompting framework, LLMs can
improve prediction performance in key tasks such as mortality, length-of-stay,
and 30-day readmission by about 35\%, surpassing ML models in few-shot
settings. Our research underscores the potential of LLMs in enhancing clinical
decision-making, especially in urgent healthcare situations like the outbreak
of emerging diseases with no labeled data. The code is publicly available at
https://github.com/yhzhu99/llm4healthcare for reproducibility.
\\ ( https://arxiv.org/abs/2402.01713 ,  2366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01714
Date: Thu, 25 Jan 2024 20:17:06 GMT   (4583kb,D)

Title: TrICy: Trigger-guided Data-to-text Generation with Intent aware
  Attention-Copy
Authors: Vibhav Agarwal, Sourav Ghosh, Harichandana BSS, Himanshu Arora, Barath
  Raj Kandur Raja
Categories: cs.CL cs.AI cs.LG
Comments: Published in the IEEE/ACM Transactions on Audio, Speech, and Language
  Processing. (Sourav Ghosh and Vibhav Agarwal contributed equally to this
  work.)
DOI: 10.1109/TASLP.2024.3353574
\\
  Data-to-text (D2T) generation is a crucial task in many natural language
understanding (NLU) applications and forms the foundation of task-oriented
dialog systems. In the context of conversational AI solutions that can work
directly with local data on the user's device, architectures utilizing large
pre-trained language models (PLMs) are impractical for on-device deployment due
to a high memory footprint. To this end, we propose TrICy, a novel lightweight
framework for an enhanced D2T task that generates text sequences based on the
intent in context and may further be guided by user-provided triggers. We
leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words
accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L:
70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom
dataset related to text messaging applications, showcase our architecture's
effectiveness. Moreover, we show that by leveraging an optional trigger input,
data-to-text generation quality increases significantly and achieves the new
SOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that
TrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively
over LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some
scenarios, performance improvement due to triggers is observed even when they
are absent in training.
\\ ( https://arxiv.org/abs/2402.01714 ,  4583kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01715
Date: Thu, 25 Jan 2024 23:15:45 GMT   (387kb,D)

Title: ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis
Authors: Alessio Buscemi and Daniele Proverbio
Categories: cs.CL cs.AI
\\
  Automated sentiment analysis using Large Language Model (LLM)-based models
like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic
research and in industrial applications. However, assessment and validation of
their performance in case of ambiguous or ironic text is still poor. In this
study, we constructed nuanced and ambiguous scenarios, we translated them in 10
languages, and we predicted their associated sentiment using popular LLMs. The
results are validated against post-hoc human responses. Ambiguous scenarios are
often well-coped by ChatGPT and Gemini, but we recognise significant biases and
inconsistent performance across models and evaluated human languages. This work
provides a standardised methodology for automated sentiment analysis evaluation
and makes a call for action to further improve the algorithms and their
underlying data, to improve their performance, interpretability and
applicability.
\\ ( https://arxiv.org/abs/2402.01715 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01717
Date: Fri, 26 Jan 2024 08:23:29 GMT   (432kb,D)

Title: From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical
  Regulatory Compliance Process
Authors: Jaewoong Kim (Sungkyunkwan University), Moohong Min (Sungkyunkwan
  University)
Categories: cs.CL cs.AI
Comments: Total number of pages: 9. Total number of figures: 2. For the source
  code and experimental results of this paper, see
  https://github.com/jwoongkim11/QA-RAG. For the dataset used in training and
  evaluating the model, see https://huggingface.co/datasets/Jaymax/FDA
  Pharmaceuticals FAQ
ACM-class: I.2.7; I.2.1; J.3
\\
  Regulatory compliance in the pharmaceutical industry entails navigating
through complex and voluminous guidelines, often requiring significant human
resources. To address these challenges, our study introduces a chatbot model
that utilizes generative AI and the Retrieval Augmented Generation (RAG)
method. This chatbot is designed to search for guideline documents relevant to
the user inquiries and provide answers based on the retrieved guidelines.
Recognizing the inherent need for high reliability in this domain, we propose
the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In
comparative experiments, the QA-RAG model demonstrated a significant
improvement in accuracy, outperforming all other baselines including
conventional RAG methods. This paper details QA-RAG's structure and performance
evaluation, emphasizing its potential for the regulatory compliance domain in
the pharmaceutical industry and beyond. We have made our work publicly
available for further research and development.
\\ ( https://arxiv.org/abs/2402.01717 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01719
Date: Fri, 26 Jan 2024 18:05:47 GMT   (275kb,D)

Title: Measuring Moral Inconsistencies in Large Language Models
Authors: Vamshi Krishna Bonagiri, Sreeram Vennam, Manas Gaur, Ponnurangam
  Kumaraguru
Categories: cs.CL cs.LG
Comments: BBNLP 2023
\\
  A Large Language Model~(LLM) is considered consistent if semantically
equivalent prompts produce semantically equivalent responses. Despite recent
advancements showcasing the impressive capabilities of LLMs in conversational
systems, we show that even state-of-the-art LLMs are highly inconsistent in
their generations, questioning their reliability. Prior research has tried to
measure this with task-specific accuracies. However, this approach is
unsuitable for moral scenarios, such as the trolley problem, with no
``correct'' answer. To address this issue, we propose a novel
information-theoretic measure called Semantic Graph Entropy~(SGE) to measure
the consistency of an LLM in moral scenarios. We leverage ``Rules of
Thumb''~(RoTs) to explain a model's decision-making strategies and further
enhance our metric. Compared to existing consistency metrics, SGE correlates
better with human judgments across five LLMs. In the future, we aim to
investigate the root causes of LLM inconsistencies and propose improvements.
\\ ( https://arxiv.org/abs/2402.01719 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01722
Date: Sat, 27 Jan 2024 00:18:07 GMT   (1214kb,D)

Title: Enhancing Large Language Model Performance To Answer Questions and
  Extract Information More Accurately
Authors: Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid,
  Natan Vidra, Tommy Clifford
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) generate responses to questions; however, their
effectiveness is often hindered by sub-optimal quality of answers and
occasional failures to provide accurate responses to questions. To address
these challenges, a fine-tuning process is employed, involving feedback and
examples to refine models. The objective is to enhance AI models through
continuous feedback loops, utilizing metrics such as cosine similarity, LLM
evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like
GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on
financial datasets, including the FinanceBench and RAG Instruct Benchmark
Tester Dataset, illustrating the necessity of fine-tuning. The results showcase
the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,
providing superior question and answering capabilities. Notably, the
combination of fine-tuning the LLM with a process known as Retrieval Augmented
Generation (RAG) proves to generate responses with improved accuracy.
\\ ( https://arxiv.org/abs/2402.01722 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01723
Date: Sat, 27 Jan 2024 03:37:55 GMT   (812kb,D)

Title: An Empirical Study on Large Language Models in Accuracy and Robustness
  under Chinese Industrial Scenarios
Authors: Zongjie Li, Wenying Qiu, Pingchuan Ma, Yichen Li, You Li, Sijia He,
  Baozheng Jiang, Shuai Wang, Weixi Gu
Categories: cs.CL cs.AI
\\
  Recent years have witnessed the rapid development of large language models
(LLMs) in various domains. To better serve the large number of Chinese users,
many commercial vendors in China have adopted localization strategies, training
and providing local LLMs specifically customized for Chinese users.
Furthermore, looking ahead, one of the key future applications of LLMs will be
practical deployment in industrial production by enterprises and users in those
sectors. However, the accuracy and robustness of LLMs in industrial scenarios
have not been well studied. In this paper, we present a comprehensive empirical
study on the accuracy and robustness of LLMs in the context of the Chinese
industrial production area. We manually collected 1,200 domain-specific
problems from 8 different industrial sectors to evaluate LLM accuracy.
Furthermore, we designed a metamorphic testing framework containing four
industrial-specific stability categories with eight abilities, totaling 13,631
questions with variants to evaluate LLM robustness. In total, we evaluated 9
different LLMs developed by Chinese vendors, as well as four different LLMs
developed by global vendors. Our major findings include: (1) Current LLMs
exhibit low accuracy in Chinese industrial contexts, with all LLMs scoring less
than 0.6. (2) The robustness scores vary across industrial sectors, and local
LLMs overall perform worse than global ones. (3) LLM robustness differs
significantly across abilities. Global LLMs are more robust under
logical-related variants, while advanced local LLMs perform better on problems
related to understanding Chinese industrial terminology. Our study results
provide valuable guidance for understanding and promoting the industrial domain
capabilities of LLMs from both development and industrial enterprise
perspectives. The results further motivate possible research directions and
tooling support.
\\ ( https://arxiv.org/abs/2402.01723 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01724
Date: Sat, 27 Jan 2024 06:40:08 GMT   (376kb,D)

Title: CERM: Context-aware Literature-based Discovery via Sentiment Analysis
Authors: Julio Christian Young and Uchenna Akujuobi
Categories: cs.CL cs.LG
DOI: 10.3233/FAIA230604
\\
  Driven by the abundance of biomedical publications, we introduce a sentiment
analysis task to understand food-health relationship. Prior attempts to
incorporate health into recipe recommendation and analysis systems have
primarily focused on ingredient nutritional components or utilized basic
computational models trained on curated labeled data. Enhanced models that
capture the inherent relationship between food ingredients and biomedical
concepts can be more beneficial for food-related research, given the wealth of
information in biomedical texts. Considering the costly data labeling process,
these models should effectively utilize both labeled and unlabeled data. This
paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that
captures the sentiment of a text based on an entity pair. ERSA extends the
widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our
study concentrates on the ERSA task applied to biomedical texts, focusing on
(entity-entity) pairs of biomedical and food concepts. ERSA poses a significant
challenge compared to traditional sentiment analysis tasks, as sentence
sentiment may not align with entity relationship sentiment. Additionally, we
propose CERM, a semi-supervised architecture that combines different word
embeddings to enhance the encoding of the ERSA task. Experimental results
showcase the model's efficiency across diverse learning scenarios.
\\ ( https://arxiv.org/abs/2402.01724 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01725
Date: Sat, 27 Jan 2024 08:09:33 GMT   (1153kb,D)

Title: Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing
  Security in Large Language Models
Authors: Yunhong He, Jianling Qiu, Wei Zhang, Zhengqing Yuan
Categories: cs.CL cs.AI
\\
  Recent advancements in large language models (LLMs) have significantly
enhanced capabilities in natural language processing and artificial
intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized
text generation, translation, and question-answering tasks due to the
transformative Transformer model. Despite their widespread use, LLMs present
challenges such as ethical dilemmas when models are compelled to respond
inappropriately, susceptibility to phishing attacks, and privacy violations.
This paper addresses these challenges by introducing a multi-pronged approach
that includes: 1) filtering sensitive vocabulary from user input to prevent
unethical responses; 2) detecting role-playing to halt interactions that could
lead to 'prison break' scenarios; 3) implementing custom rule engines to
restrict the generation of prohibited content; and 4) extending these
methodologies to various LLM derivatives like Multi-Model Large Language Models
(MLLMs). Our approach not only fortifies models against unethical manipulations
and privacy breaches but also maintains their high performance across tasks. We
demonstrate state-of-the-art performance under various attack prompts, without
compromising the model's core functionalities. Furthermore, the introduction of
differentiated security levels empowers users to control their personal data
disclosure. Our methods contribute to reducing social risks and conflicts
arising from technological abuse, enhance data protection, and promote social
equity. Collectively, this research provides a framework for balancing the
efficiency of question-answering systems with user privacy and ethical
standards, ensuring a safer user experience and fostering trust in AI
technology.
\\ ( https://arxiv.org/abs/2402.01725 ,  1153kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01726
Date: Sat, 27 Jan 2024 14:32:12 GMT   (4308kb,D)

Title: Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of
  Text Messages
Authors: N'yoma Diamond
Categories: cs.CL cs.AI cs.HC
\\
  For many people, anxiety, depression, and other social and mental factors can
make composing text messages an active challenge. To remedy this problem, large
language models (LLMs) may yet prove to be the perfect tool to assist users
that would otherwise find texting difficult or stressful. However, despite
rapid uptake in LLM usage, considerations for their assistive usage in text
message composition have not been explored. A primary concern regarding LLM
usage is that poor public sentiment regarding AI introduces the possibility
that its usage may harm perceptions of AI-assisted text messages, making usage
counter-productive. To (in)validate this possibility, we explore how the belief
that a text message did or did not receive AI assistance in composition alters
its perceived tone, clarity, and ability to convey intent. In this study, we
survey the perceptions of 26 participants on 18 randomly labeled pre-composed
text messages. In analyzing the participants' ratings of message tone, clarity,
and ability to convey intent, we find that there is no statistically
significant evidence that the belief that AI is utilized alters recipient
perceptions. This provides hopeful evidence that LLM-based text message
composition assistance can be implemented without the risk of
counter-productive outcomes.
\\ ( https://arxiv.org/abs/2402.01726 ,  4308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01728
Date: Sat, 27 Jan 2024 22:49:43 GMT   (848kb,D)

Title: Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain
  Specific Knowledge
Authors: Weimin Fu, Shijie Li, Yifang Zhao, Haocheng Ma, Raj Dutta, Xuan Zhang,
  Kaichen Yang, Yier Jin, Xiaolong Guo
Categories: cs.CL cs.AI
Comments: 6 pages, 6 figures
Journal-ref: 29th IEEE/ACM Asia and South Pacific Design Automation Conference
  (ASP-DAC); 2024 January; Incheon Songdo Convensia, South Korea
\\
  In the rapidly evolving semiconductor industry, where research, design,
verification, and manufacturing are intricately linked, the potential of Large
Language Models to revolutionize hardware design and security verification is
immense. The primary challenge, however, lies in the complexity of hardware
specific issues that are not adequately addressed by the natural language or
software code knowledge typically acquired during the pretraining stage.
Additionally, the scarcity of datasets specific to the hardware domain poses a
significant hurdle in developing a foundational model. Addressing these
challenges, this paper introduces Hardware Phi 1.5B, an innovative large
language model specifically tailored for the hardware domain of the
semiconductor industry. We have developed a specialized, tiered dataset
comprising small, medium, and large subsets and focused our efforts on
pretraining using the medium dataset. This approach harnesses the compact yet
efficient architecture of the Phi 1.5B model. The creation of this first
pretrained, hardware domain specific large language model marks a significant
advancement, offering improved performance in hardware design and verification
tasks and illustrating a promising path forward for AI applications in the
semiconductor sector.
\\ ( https://arxiv.org/abs/2402.01728 ,  848kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01729
Date: Sun, 28 Jan 2024 08:56:49 GMT   (8052kb,D)

Title: Contextualization Distillation from Large Language Model for Knowledge
  Graph Completion
Authors: Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024 findings
\\
  While textual information significantly enhances the performance of
pre-trained language models (PLMs) in knowledge graph completion (KGC), the
static and noisy nature of existing corpora collected from Wikipedia articles
or synsets definitions often limits the potential of PLM-based KGC models. To
surmount these challenges, we introduce the Contextualization Distillation
strategy, a versatile plug-in-and-play approach compatible with both
discriminative and generative KGC frameworks. Our method begins by instructing
large language models (LLMs) to transform compact, structural triplets into
context-rich segments. Subsequently, we introduce two tailored auxiliary tasks,
reconstruction and contextualization, allowing smaller KGC models to assimilate
insights from these enriched triplets. Comprehensive evaluations across diverse
datasets and KGC techniques highlight the efficacy and adaptability of our
approach, revealing consistent performance enhancements irrespective of
underlying pipelines or architectures. Moreover, our analysis makes our method
more explainable and provides insight into generating path selection, as well
as the choosing of suitable distillation tasks. All the code and data in this
work will be released at
https://github.com/David-Li0406/Contextulization-Distillation
\\ ( https://arxiv.org/abs/2402.01729 ,  8052kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01730
Date: Sun, 28 Jan 2024 09:25:12 GMT   (9608kb,D)

Title: Evaluating LLM - Generated Multimodal Diagnosis from Medical Images and
  Symptom Analysis
Authors: Dimitrios P. Panagoulias, Maria Virvou and George A. Tsihrintzis
Categories: cs.CL cs.AI
Comments: Department of Informatics, University of Piraeus, Greece
\\
  Large language models (LLMs) constitute a breakthrough state-of-the-art
Artificial Intelligence technology which is rapidly evolving and promises to
aid in medical diagnosis. However, the correctness and the accuracy of their
returns has not yet been properly evaluated. In this work, we propose an LLM
evaluation paradigm that incorporates two independent steps of a novel
methodology, namely (1) multimodal LLM evaluation via structured interactions
and (2) follow-up, domain-specific analysis based on data extracted via the
previous interactions. Using this paradigm, (1) we evaluate the correctness and
accuracy of LLM-generated medical diagnosis with publicly available multimodal
multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a
systemic and comprehensive analysis of extracted results. We used
GPT-4-Vision-Preview as the LLM to respond to complex, medical questions
consisting of both images and text, and we explored a wide range of diseases,
conditions, chemical compounds, and related entity types that are included in
the vast knowledge domain of Pathology. GPT-4-Vision-Preview performed quite
well, scoring approximately 84\% of correct diagnoses. Next, we further
analyzed the findings of our work, following an analytical approach which
included Image Metadata Analysis, Named Entity Recognition and Knowledge
Graphs. Weaknesses of GPT-4-Vision-Preview were revealed on specific knowledge
paths, leading to a further understanding of its shortcomings in specific
areas. Our methodology and findings are not limited to the use of
GPT-4-Vision-Preview, but a similar approach can be followed to evaluate the
usefulness and accuracy of other LLMs and, thus, improve their use with further
optimization.
\\ ( https://arxiv.org/abs/2402.01730 ,  9608kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01733
Date: Mon, 29 Jan 2024 06:49:53 GMT   (606kb)

Title: Development and Testing of Retrieval Augmented Generation in Large
  Language Models -- A Case Study Report
Authors: YuHe Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan
  Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat
  Ling Ong, Daniel Shu Wei Ting
Categories: cs.CL cs.AI
Comments: NA
\\
  Purpose: Large Language Models (LLMs) hold significant promise for medical
applications. Retrieval Augmented Generation (RAG) emerges as a promising
approach for customizing domain knowledge in LLMs. This case study presents the
development and evaluation of an LLM-RAG pipeline tailored for healthcare,
focusing specifically on preoperative medicine.
  Methods: We developed an LLM-RAG model using 35 preoperative guidelines and
tested it against human-generated responses, with a total of 1260 responses
evaluated. The RAG process involved converting clinical documents into text
using Python-based frameworks like LangChain and Llamaindex, and processing
these texts into chunks for embedding and retrieval. Vector storage techniques
and selected embedding models to optimize data retrieval, using Pinecone for
vector storage with a dimensionality of 1536 and cosine similarity for loss
metrics. Human-generated answers, provided by junior doctors, were used as a
comparison.
  Results: The LLM-RAG model generated answers within an average of 15-20
seconds, significantly faster than the 10 minutes typically required by humans.
Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This
accuracy was further increased to 91.4% when the model was enhanced with RAG.
Compared to the human-generated instructions, which had an accuracy of 86.3%,
the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610).
  Conclusions: In this case study, we demonstrated a LLM-RAG model for
healthcare implementation. The pipeline shows the advantages of grounded
knowledge, upgradability, and scalability as important aspects of healthcare
LLM deployment.
\\ ( https://arxiv.org/abs/2402.01733 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01734
Date: Mon, 29 Jan 2024 08:07:41 GMT   (1281kb,D)

Title: CFTM: Continuous time fractional topic model
Authors: Kei Nakagawa, Kohei Hayashi, Yugo Fujimoto
Categories: cs.CL cs.LG q-fin.CP stat.AP
\\
  In this paper, we propose the Continuous Time Fractional Topic Model (cFTM),
a new method for dynamic topic modeling. This approach incorporates fractional
Brownian motion~(fBm) to effectively identify positive or negative correlations
in topic and word distribution over time, revealing long-term dependency or
roughness. Our theoretical analysis shows that the cFTM can capture these
long-term dependency or roughness in both topic and word distributions,
mirroring the main characteristics of fBm. Moreover, we prove that the
parameter estimation process for the cFTM is on par with that of LDA,
traditional topic models. To demonstrate the cFTM's property, we conduct
empirical study using economic news articles. The results from these tests
support the model's ability to identify and track long-term dependency or
roughness in topics over time.
\\ ( https://arxiv.org/abs/2402.01734 ,  1281kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01735
Date: Mon, 29 Jan 2024 08:28:32 GMT   (10078kb,D)

Title: VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large
  Models
Authors: Yi Zhao, Yilin Zhang, Rong Xiang, Jing Li, Hillming Li
Categories: cs.CL cs.AI
Comments: under review
\\
  Visually Impaired Assistance (VIA) aims to automatically help visually
impaired (VI) handle daily activities. The advancement of VIA primarily depends
on developments in Computer Vision (CV) and Natural Language Processing (NLP),
both of which exhibit cutting-edge paradigms with large models (LMs).
Furthermore, LMs have shown exceptional multimodal abilities to tackle
challenging physically-grounded tasks such as embodied robots. To investigate
the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in
VIA applications, we present an extensive study for the task of VIA with LMs
(\textbf{VIALM}). In this task, given an \textit{image} illustrating the
physical environments and a \textit{linguistic request} from a VI user, VIALM
aims to output step-by-step \textit{guidance} to assist the VI user in
fulfilling the request grounded in the environment. The study consists of a
survey reviewing recent LM research and benchmark experiments examining
selected LMs' capabilities in VIA. The results indicate that while LMs can
augment VIA, their output cannot be well \textit{grounded} in reality (i.e.,
25.7\% GPT-4's responses) and lacks \textit{fine-grained} guidance (i.e.,
32.1\% GPT-4's responses).
\\ ( https://arxiv.org/abs/2402.01735 ,  10078kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01736
Date: Mon, 29 Jan 2024 08:54:21 GMT   (9672kb,D)

Title: SADAS: A Dialogue Assistant System Towards Remediating Norm Violations
  in Bilingual Socio-Cultural Conversations
Authors: Yuncheng Hua, Zhuang Li, Linhao Luo, Kadek Ananta Satriadi, Tao Feng,
  Haolan Zhan, Lizhen Qu, Suraj Sharma, Ingrid Zukerman, Zhaleh Semnani-Azad
  and Gholamreza Haffari
Categories: cs.CL cs.AI
Comments: 8 pages, 2 figures
ACM-class: I.2.7
\\
  In today's globalized world, bridging the cultural divide is more critical
than ever for forging meaningful connections. The Socially-Aware Dialogue
Assistant System (SADAS) is our answer to this global challenge, and it's
designed to ensure that conversations between individuals from diverse cultural
backgrounds unfold with respect and understanding. Our system's novel
architecture includes: (1) identifying the categories of norms present in the
dialogue, (2) detecting potential norm violations, (3) evaluating the severity
of these violations, (4) implementing targeted remedies to rectify the
breaches, and (5) articulates the rationale behind these corrective actions. We
employ a series of State-Of-The-Art (SOTA) techniques to build different
modules, and conduct numerous experiments to select the most suitable backbone
model for each of the modules. We also design a human preference experiment to
validate the overall performance of the system. We will open-source our system
(including source code, tools and applications), hoping to advance future
research. A demo video of our system can be found
at:~\url{https://youtu.be/JqetWkfsejk}. We have released our code and software
at:~\url{https://github.com/AnonymousEACLDemo/SADAS}.
\\ ( https://arxiv.org/abs/2402.01736 ,  9672kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01737
Date: Mon, 29 Jan 2024 09:07:40 GMT   (5409kb,D)

Title: Assistive Large Language Model Agents for Socially-Aware Negotiation
  Dialogues
Authors: Yuncheng Hua, Lizhen Qu, Gholamreza Haffari
Categories: cs.CL cs.AI
Comments: 18 pages, 1 figure, 11 tables; Under review in IJCAI 2024
ACM-class: I.2.7
\\
  In this work, we aim to develop LLM agents to mitigate social norm violations
in negotiations in a multi-agent setting. We simulate real-world negotiations
by letting two large Language Models (LLMs) play the roles of two negotiators
in each conversation. A third LLM acts as a remediation agent to rewrite
utterances violating norms for improving negotiation outcomes. As it is a novel
task, no manually constructed data is available. To address this limitation, we
introduce a value impact based In-Context Learning (ICL) method to identify
high-quality ICL examples for the LLM-based remediation agents, where the value
impact function measures the quality of negotiation outcomes. We show the
connection of this method to policy learning and provide rich empirical
evidence to demonstrate its effectiveness in negotiations across three
different topics: product sale, housing price, and salary negotiation. The
source code and the generated dataset will be publicly available upon
acceptance.
\\ ( https://arxiv.org/abs/2402.01737 ,  5409kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01738
Date: Mon, 29 Jan 2024 09:44:45 GMT   (1010kb,D)

Title: C4Q: A Chatbot for Quantum
Authors: Yaiza Aragon\'es-Soria and Manuel Oriol
Categories: cs.CL quant-ph
Comments: Paper accepted in the 5th International Workshop on Quantum Software
  Engineering (Q-SE 2024)
\\
  Quantum computing is a growing field that promises many real-world
applications such as quantum cryptography or quantum finance. The number of
people able to use quantum computing is however still very small. This
limitation comes from the difficulty to understand the concepts and to know how
to start coding. Therefore, there is a need for tools that can assist
non-expert in overcoming this complexity. One possibility would be to use
existing conversational agents. Unfortunately ChatGPT and other Large-Language
Models produce inaccurate results. This article presents C4Q, a chatbot that
answers accurately basic questions and guides users when trying to code quantum
programs. Contrary to other approaches C4Q uses a pre-trained large language
model only to discover and classify user requests. It then generates an
accurate answer using an own engine. Thanks to this architectural design, C4Q's
answers are always correct, and thus C4Q can become a support tool that makes
quantum computing more available to non-experts.
\\ ( https://arxiv.org/abs/2402.01738 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01739
Date: Mon, 29 Jan 2024 12:05:02 GMT   (678kb,D)

Title: OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
Authors: Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu
  Zhou, Yang You
Categories: cs.CL cs.AI cs.DC cs.LG
\\
  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
\\ ( https://arxiv.org/abs/2402.01739 ,  678kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01740
Date: Mon, 29 Jan 2024 15:43:23 GMT   (9063kb,D)

Title: Compensatory Biases Under Cognitive Load: Reducing Selection Bias in
  Large Language Models
Authors: J. E. Eicher and R. F. Irgoli\v{c}
Categories: cs.CL cs.AI
Comments: 27 pages, 23 figures
ACM-class: I.2.0
\\
  Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have
become instrumental in interpreting and executing semantic-based tasks.
Unfortunately, these models' inherent biases, akin to human cognitive biases,
adversely affect their performance. Particularly affected is object selection
from lists; a fundamental operation in digital navigation and decision-making.
This research critically examines these biases and quantifies the effects on a
representative list selection task. To explore these biases, we conducted a
series of controlled experiments, manipulating temperature, list length, object
identity, object type, prompt complexity, and model. This enabled us to isolate
and measure the influence of the biases on selection behavior. Our findings
show that bias structure is strongly dependent on the model, with object type
modulating the magnitude of the effect. With a strong primacy effect, causing
the first objects in a list to be disproprotionately represented in outputs.
Furthermore the usage of guard rails, a prompt engineering method of ensuring a
response structure, can increase bias and decrease instruction adherence when
combined with a selection task. The bias is ablated when the guard rail step is
separated from the list sampling step, lowering the complexity of each
individual task. The implications of this research are two-fold, practically
providing a guide for designing unbiased LLM applications and theoretically
suggesting that LLMs experience a form of cognitive load compensated for by
increasing bias.
\\ ( https://arxiv.org/abs/2402.01740 ,  9063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01741
Date: Mon, 29 Jan 2024 16:03:29 GMT   (1025kb)

Title: Development and Testing of a Novel Large Language Model-Based Clinical
  Decision Support Systems for Medication Safety in 12 Clinical Specialties
Authors: Jasmine Chiat Ling Ong, Liyuan Jin, Kabilan Elangovan, Gilbert Yong
  San Lim, Daniel Yan Zheng Lim, Gerald Gui Ren Sng, Yuhe Ke, Joshua Yi Min
  Tung, Ryan Jian Zhong, Christopher Ming Yao Koh, Keane Zhi Hao Lee, Xiang
  Chen, Jack Kian Chng, Aung Than, Ken Junyang Goh, Daniel Shu Wei Ting
Categories: cs.CL cs.AI
Comments: 13 pages, 6 Figures
\\
  Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large
Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe
medication prescription. This model addresses the limitations of traditional
rule-based CDSS by providing relevant prescribing error alerts tailored to
patient context and institutional guidelines.
  Objective: The study evaluates the efficacy of an LLM-based CDSS in
identifying medication errors across various medical and surgical case
vignettes, compared to a human expert panel. It also examines clinician
preferences among different CDSS integration modalities: junior pharmacist,
LLM-based CDSS alone, and a combination of both.
  Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the
study involved 61 prescribing error scenarios within 23 clinical vignettes
across 12 specialties. An expert panel assessed these cases using the PCNE
classification and NCC MERP index. Three junior pharmacists independently
reviewed each vignette under simulated conditions.
  Main Outcomes and Measures: The study assesses the LLM-based CDSS's accuracy,
precision, recall, and F1 scores in identifying Drug-Related Problems (DRPs),
compared to junior pharmacists alone or in an assistive mode with the CDSS.
  Results: The co-pilot mode of RAG-LLM significantly improved DRP
identification accuracy by 22% over solo pharmacists. It showed higher recall
and F1 scores, indicating better detection of severe DRPs, despite a slight
decrease in precision. Accuracy varied across categories when pharmacists had
access to RAG-LLM responses.
  Conclusions: The RAG-LLM based CDSS enhances medication error identification
accuracy when used with junior pharmacists, especially in detecting severe
DRPs.
\\ ( https://arxiv.org/abs/2402.01741 ,  1025kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01742
Date: Mon, 29 Jan 2024 16:36:31 GMT   (7367kb,D)

Title: Towards Optimizing the Costs of LLM Usage
Authors: Shivanshu Shekhar, Tanishq Dubey, Koyel Mukherjee, Apoorv Saxena,
  Atharv Tyagi, Nishanth Kotla
Categories: cs.CL cs.AI cs.LG
Comments: 8 pages + Appendix, Total 12 pages
\\
  Generative AI and LLMs in particular are heavily used nowadays for various
document processing tasks such as question answering and summarization.
However, different LLMs come with different capabilities for different tasks as
well as with different costs, tokenization, and latency. In fact, enterprises
are already incurring huge costs of operating or using LLMs for their
respective use cases.
  In this work, we propose optimizing the usage costs of LLMs by estimating
their output quality (without actually invoking the LLMs), and then solving an
optimization routine for the LLM selection to either keep costs under a budget,
or minimize the costs, in a quality and latency aware manner. We propose a
model to predict the output quality of LLMs on document processing tasks like
summarization, followed by an LP rounding algorithm to optimize the selection
of LLMs. We study optimization problems trading off the quality and costs, both
theoretically and empirically. We further propose a sentence simplification
model for reducing the number of tokens in a controlled manner. Additionally,
we propose several deterministic heuristics for reducing tokens in a quality
aware manner, and study the related optimization problem of applying the
heuristics optimizing the quality and cost trade-off. We perform extensive
empirical validation of our methods on not only enterprise datasets but also on
open-source datasets, annotated by us, and show that we perform much better
compared to closest baselines. Our methods reduce costs by 40%- 90% while
improving quality by 4%-7%. We will release the annotated open source datasets
to the community for further research and exploration.
\\ ( https://arxiv.org/abs/2402.01742 ,  7367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01750
Date: Tue, 30 Jan 2024 06:55:17 GMT   (7604kb,D)

Title: PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using
  Large Language Models
Authors: Jiaxuan Li and Minxi Yang and Dahua Gao and Wenlong Xu and Guangming
  Shi
Categories: cs.CL cs.AI
Comments: 11 pages,11 figures, submitted to IJCAI 2024
\\
  Current communication technologies face limitations in terms of theoretical
capacity, spectrum availability, and power resources. Pragmatic communication,
leveraging terminal intelligence for selective data transmission, offers
resource conservation. Existing research lacks universal intention resolution
tools, limiting applicability to specific tasks. This paper proposes an image
pragmatic communication framework based on a Pragmatic Agent for Communication
Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE
sequentially performs semantic perception, intention resolution, and
intention-oriented coding. To ensure the effective utilization of LLM in
communication, a knowledge base is designed to supplement the necessary
knowledge, dedicated prompts are introduced to facilitate understanding of
pragmatic communication scenarios and task requirements, and a chain of thought
is designed to assist in making reasonable trade-offs between transmission
efficiency and cost. For experimental validation, this paper constructs an
image pragmatic communication dataset along with corresponding evaluation
standards. Simulation results indicate that the proposed method outperforms
traditional and non-LLM-based pragmatic communication in terms of transmission
efficiency.
\\ ( https://arxiv.org/abs/2402.01750 ,  7604kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01751
Date: Tue, 30 Jan 2024 07:55:43 GMT   (1285kb)

Title: Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's
  Dementia
Authors: Balamurali B T, Jer-Ming Chen
Categories: cs.CL cs.AI
Comments: 22 pages
\\
  Large language models (LLMs) find increasing applications in many fields.
Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in
their current form, as publicly available - for their ability to recognize
Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual
input derived from spontaneous speech recordings. Zero-shot learning approach
is used at two levels of independent queries, with the second query
(chain-of-thought prompting) eliciting more detailed than the first. Each LLM
chatbot's performance is evaluated on the prediction generated in terms of
accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots
generated three-class outcome ("AD", "CN", or "Unsure"). When positively
identifying AD, Bard produced highest true-positives (89% recall) and highest
F1 score (71%), but tended to misidentify CN as AD, with high confidence (low
"Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest
true-negatives at 56% and highest F1 score (62%), adopting a diplomatic stance
(moderate "Unsure" rates). Overall, three LLM chatbots identify AD vs CN
surpassing chance-levels but do not currently satisfy clinical application.
\\ ( https://arxiv.org/abs/2402.01751 ,  1285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01759
Date: Tue, 30 Jan 2024 16:21:47 GMT   (394kb,D)

Title: Systematic Literature Review: Computational Approaches for Humour Style
  Classification
Authors: Mary Ogbuka Kenneth, Foaad Khosmood and Abbas Edalat
Categories: cs.CL cs.AI cs.LG
\\
  Understanding various humour styles is essential for comprehending the
multifaceted nature of humour and its impact on fields such as psychology and
artificial intelligence. This understanding has revealed that humour, depending
on the style employed, can either have therapeutic or detrimental effects on an
individual's health and relationships. Although studies dedicated exclusively
to computational-based humour style analysis remain somewhat rare, an expansive
body of research thrives within related task, particularly binary humour and
sarcasm recognition. In this systematic literature review (SLR), we survey the
landscape of computational techniques applied to these related tasks and also
uncover their fundamental relevance to humour style analysis. Through this
study, we unveil common approaches, illuminate various datasets and evaluation
metrics, and effectively navigate the complex terrain of humour research. Our
efforts determine potential research gaps and outlined promising directions.
Furthermore, the SLR identifies a range of features and computational models
that can seamlessly transition from related tasks like binary humour and
sarcasm detection to invigorate humour style classification. These features
encompass incongruity, sentiment and polarity analysis, ambiguity detection,
acoustic nuances, visual cues, contextual insights, and more. The computational
models that emerge contain traditional machine learning paradigms, neural
network architectures, transformer-based models, and specialised models attuned
to the nuances of humour. Finally, the SLR provides access to existing datasets
related to humour and sarcasm, facilitating the work of future researchers.
\\ ( https://arxiv.org/abs/2402.01759 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01761
Date: Tue, 30 Jan 2024 17:38:54 GMT   (128kb,D)

Title: Rethinking Interpretability in the Era of Large Language Models
Authors: Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana,
  Jianfeng Gao
Categories: cs.CL cs.AI cs.LG
Comments: 7 pages
\\
  Interpretable machine learning has exploded as an area of interest over the
last decade, sparked by the rise of increasingly large datasets and deep neural
networks. Simultaneously, large language models (LLMs) have demonstrated
remarkable capabilities across a wide array of tasks, offering a chance to
rethink opportunities in interpretable machine learning. Notably, the
capability to explain in natural language allows LLMs to expand the scale and
complexity of patterns that can be given to a human. However, these new
capabilities raise new challenges, such as hallucinated explanations and
immense computational costs.
  In this position paper, we start by reviewing existing methods to evaluate
the emerging field of LLM interpretation (both interpreting LLMs and using LLMs
for explanation). We contend that, despite their limitations, LLMs hold the
opportunity to redefine interpretability with a more ambitious scope across
many applications, including in auditing LLMs themselves. We highlight two
emerging research priorities for LLM interpretation: using LLMs to directly
analyze new datasets and to generate interactive explanations.
\\ ( https://arxiv.org/abs/2402.01761 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01765
Date: Wed, 31 Jan 2024 13:45:25 GMT   (253kb,D)

Title: LLMs Simulate Big Five Personality Traits: Further Evidence
Authors: Aleksandra Sorokovikova, Natalia Fedorova, Sharwin Rezagholi, Ivan P.
  Yamshchikov
Categories: cs.CL cs.AI
ACM-class: I.2.7; J.4; I.2.1
\\
  An empirical investigation into the simulation of the Big Five personality
traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is
presented. We analyze the personality traits simulated by these models and
their stability. This contributes to the broader understanding of the
capabilities of LLMs to simulate personality traits and the respective
implications for personalized human-computer interaction.
\\ ( https://arxiv.org/abs/2402.01765 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01766
Date: Wed, 31 Jan 2024 14:52:02 GMT   (5985kb,D)

Title: LLM Voting: Human Choices and AI Collective Decision Making
Authors: Joshua C. Yang, Marcin Korecki, Damian Dailisan, Carina I. Hausladen,
  and Dirk Helbing
Categories: cs.CL cs.AI cs.CY cs.LG econ.GN q-fin.EC
Comments: Submitted to ICML2024
MSC-class: 68T05, 91B14, 91C20
ACM-class: I.2.7; J.4; K.4.1
\\
  This paper investigates the voting behaviors of Large Language Models (LLMs),
particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting
patterns. Our approach included a human voting experiment to establish a
baseline for human preferences and a parallel experiment with LLM agents. The
study focused on both collective outcomes and individual preferences, revealing
differences in decision-making and inherent biases between humans and LLMs. We
observed a trade-off between preference diversity and alignment in LLMs, with a
tendency towards more uniform choices as compared to the diverse preferences of
human voters. This finding indicates that LLMs could lead to more homogenized
collective outcomes when used in voting assistance, underscoring the need for
cautious integration of LLMs into democratic processes.
\\ ( https://arxiv.org/abs/2402.01766 ,  5985kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01767
Date: Thu, 1 Feb 2024 02:24:15 GMT   (9884kb,D)

Title: HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents
  QA
Authors: Xinyue Chen, Pengyu Gao, Jiangjiang Song, Xiaoyang Tan
Categories: cs.CL cs.AI cs.LG
\\
  As language model agents leveraging external tools rapidly evolve,
significant progress has been made in question-answering(QA) methodologies
utilizing supplementary documents and the Retrieval-Augmented Generation (RAG)
approach. This advancement has improved the response quality of language models
and alleviates the appearance of hallucination. However, these methods exhibit
limited retrieval accuracy when faced with massive indistinguishable documents,
presenting notable challenges in their practical application. In response to
these emerging challenges, we present HiQA, an advanced framework for
multi-document question-answering (MDQA) that integrates cascading metadata
into content as well as a multi-route retrieval mechanism. We also release a
benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA
demonstrates the state-of-the-art performance in multi-document environments.
\\ ( https://arxiv.org/abs/2402.01767 ,  9884kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01769
Date: Thu, 1 Feb 2024 03:01:11 GMT   (2880kb,D)

Title: Redefining "Hallucination" in LLMs: Towards a psychology-informed
  framework for mitigating misinformation
Authors: Elijah Berberette, Jack Hutchins, Amir Sadovnik
Categories: cs.CL cs.AI
\\
  In recent years, large language models (LLMs) have become incredibly popular,
with ChatGPT for example being used by over a billion users. While these models
exhibit remarkable language understanding and logical prowess, a notable
challenge surfaces in the form of "hallucinations." This phenomenon results in
LLMs outputting misinformation in a confident manner, which can lead to
devastating consequences with such a large user base. However, we question the
appropriateness of the term "hallucination" in LLMs, proposing a psychological
taxonomy based on cognitive biases and other psychological phenomena. Our
approach offers a more fine-grained understanding of this phenomenon, allowing
for targeted solutions. By leveraging insights from how humans internally
resolve similar challenges, we aim to develop strategies to mitigate LLM
hallucinations. This interdisciplinary approach seeks to move beyond
conventional terminology, providing a nuanced understanding and actionable
pathways for improvement in LLM reliability.
\\ ( https://arxiv.org/abs/2402.01769 ,  2880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01771
Date: Thu, 1 Feb 2024 07:15:58 GMT   (2639kb,D)

Title: BlackMamba: Mixture of Experts for State-Space Models
Authors: Quentin Anthony, Yury Tokpanov, Paolo Glorioso, Beren Millidge
Categories: cs.CL cs.AI cs.DC cs.LG
\\
  State-space models (SSMs) have recently demonstrated competitive performance
to transformers at large-scale language modeling benchmarks while achieving
linear time and memory complexity as a function of sequence length. Mamba, a
recently released SSM model, shows impressive performance in both language
modeling and long sequence processing tasks. Simultaneously, mixture-of-expert
(MoE) models have shown remarkable performance while significantly reducing the
compute and latency costs of inference at the expense of a larger memory
footprint. In this paper, we present BlackMamba, a novel architecture that
combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate
that BlackMamba performs competitively against both Mamba and transformer
baselines, and outperforms in inference and training FLOPs. We fully train and
open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a
custom dataset. We show that BlackMamba inherits and combines both of the
benefits of SSM and MoE architectures, combining linear-complexity generation
from SSM with cheap and fast inference from MoE. We release all weights,
checkpoints, and inference code open-source. Inference code at:
https://github.com/Zyphra/BlackMamba
\\ ( https://arxiv.org/abs/2402.01771 ,  2639kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01772
Date: Thu, 1 Feb 2024 10:55:03 GMT   (2766kb,D)

Title: Disentangling the Roles of Target-Side Transfer and Regularization in
  Multilingual Machine Translation
Authors: Yan Meng and Christof Monz
Categories: cs.CL cs.AI cs.LG
\\
  Multilingual Machine Translation (MMT) benefits from knowledge transfer
across different language pairs. However, improvements in one-to-many
translation compared to many-to-one translation are only marginal and sometimes
even negligible. This performance discrepancy raises the question of to what
extent positive transfer plays a role on the target-side for one-to-many MT. In
this paper, we conduct a large-scale study that varies the auxiliary target
side languages along two dimensions, i.e., linguistic similarity and corpus
size, to show the dynamic impact of knowledge transfer on the main language
pairs. We show that linguistically similar auxiliary target languages exhibit
strong ability to transfer positive knowledge. With an increasing size of
similar target languages, the positive transfer is further enhanced to benefit
the main language pairs. Meanwhile, we find distant auxiliary target languages
can also unexpectedly benefit main language pairs, even with minimal positive
transfer ability. Apart from transfer, we show distant auxiliary target
languages can act as a regularizer to benefit translation performance by
enhancing the generalization and model inference calibration.
\\ ( https://arxiv.org/abs/2402.01772 ,  2766kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01777
Date: Thu, 1 Feb 2024 15:58:13 GMT   (548kb)

Title: On the Psychology of GPT-4: Moderately anxious, slightly masculine,
  honest, and humble
Authors: Adrita Barua, Gary Brase, Ke Dong, Pascal Hitzler, Eugene Vasserman
Categories: cs.CL cs.AI cs.HC
Comments: 16 pages, 8 tables, 1 code repository
\\
  We subject GPT-4 to a number of rigorous psychometric tests and analyze the
results. We find that, compared to the average human, GPT-4 tends to show more
honesty and humility, and less machiavellianism and narcissism. It sometimes
exhibits ambivalent sexism, leans slightly toward masculinity, is moderately
anxious but mostly not depressive (but not always). It shows human-average
numerical literacy and has cognitive reflection abilities that are above human
average for verbal tasks.
\\ ( https://arxiv.org/abs/2402.01777 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01781
Date: Thu, 1 Feb 2024 19:12:25 GMT   (2723kb,D)

Title: When Benchmarks are Targets: Revealing the Sensitivity of Large Language
  Model Leaderboards
Authors: Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan
  Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi,
  Nora Altwairesh, Areeb Alowisheq, M Saiful Bari, Haidar Khan
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Model (LLM) leaderboards based on benchmark rankings are
regularly used to guide practitioners in model selection. Often, the published
leaderboard rankings are taken at face value - we show this is a (potentially
costly) mistake. Under existing leaderboards, the relative performance of LLMs
is highly sensitive to (often minute) details. We show that for popular
multiple choice question benchmarks (e.g. MMLU) minor perturbations to the
benchmark, such as changing the order of choices or the method of answer
selection, result in changes in rankings up to 8 positions. We explain this
phenomenon by conducting systematic experiments over three broad categories of
benchmark perturbations and identifying the sources of this behavior. Our
analysis results in several best-practice recommendations, including the
advantage of a hybrid scoring method for answer selection. Our study highlights
the dangers of relying on simple benchmark evaluations and charts the path for
more robust evaluation schemes on the existing benchmarks.
\\ ( https://arxiv.org/abs/2402.01781 ,  2723kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01783
Date: Thu, 1 Feb 2024 20:56:07 GMT   (153kb,D)

Title: Hierarchical Multi-Label Classification of Online Vaccine Concerns
Authors: Chloe Qinyu Zhu, Rickard Stureborg, Bhuwan Dhingra
Categories: cs.CL cs.AI cs.LG
Comments: Published in AAAI 2024 Health Intelligence workshop
\\
  Vaccine concerns are an ever-evolving target, and can shift quickly as seen
during the COVID-19 pandemic. Identifying longitudinal trends in vaccine
concerns and misinformation might inform the healthcare space by helping public
health efforts strategically allocate resources or information campaigns. We
explore the task of detecting vaccine concerns in online discourse using large
language models (LLMs) in a zero-shot setting without the need for expensive
training datasets. Since real-time monitoring of online sources requires
large-scale inference, we explore cost-accuracy trade-offs of different
prompting strategies and offer concrete takeaways that may inform choices in
system designs for current applications. An analysis of different prompting
strategies reveals that classifying the concerns over multiple passes through
the LLM, each consisting a boolean question whether the text mentions a vaccine
concern or not, works the best. Our results indicate that GPT-4 can strongly
outperform crowdworker accuracy when compared to ground truth annotations
provided by experts on the recently introduced VaxConcerns dataset, achieving
an overall F1 score of 78.7%.
\\ ( https://arxiv.org/abs/2402.01783 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01788
Date: Fri, 2 Feb 2024 02:41:28 GMT   (7262kb,D)

Title: LitLLM: A Toolkit for Scientific Literature Review
Authors: Shubham Agarwal, Issam H. Laradji, Laurent Charlin, Christopher Pal
Categories: cs.CL cs.AI
\\
  Conducting literature reviews for scientific papers is essential for
understanding research, its limitations, and building on existing work. It is a
tedious task which makes an automatic literature review generator appealing.
Unfortunately, many existing works that generate such reviews using Large
Language Models (LLMs) have significant limitations. They tend to
hallucinate-generate non-actual information-and ignore the latest research they
have not been trained on. To address these limitations, we propose a toolkit
that operates on Retrieval Augmented Generation (RAG) principles, specialized
prompting and instructing techniques with the help of LLMs. Our system first
initiates a web search to retrieve relevant papers by summarizing user-provided
abstracts into keywords using an off-the-shelf LLM. Authors can enhance the
search by supplementing it with relevant papers or keywords, contributing to a
tailored retrieval process. Second, the system re-ranks the retrieved papers
based on the user-provided abstract. Finally, the related work section is
generated based on the re-ranked results and the abstract. There is a
substantial reduction in time and effort for literature review compared to
traditional methods, establishing our toolkit as an efficient alternative. Our
open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM
and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM)
with the video demo at https://youtu.be/E2ggOZBAFw0.
\\ ( https://arxiv.org/abs/2402.01788 ,  7262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01805
Date: Fri, 2 Feb 2024 09:45:33 GMT   (1267kb,D)

Title: Exploring the Limitations of Graph Reasoning in Large Language Models
Authors: Palaash Agrawal, Shavak Vasania and Cheston Tan
Categories: cs.CL cs.AI
\\
  Pretrained Large Language Models have demonstrated various types of reasoning
capabilities through language-based prompts alone. However, in this paper, we
test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5,
Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In
particular, we design 10 distinct problems of graph traversal, each
representing increasing levels of complexity. Further, we analyze the
performance of models across various settings such as varying sizes of graphs
as well as different forms of k-shot prompting. We highlight various
limitations, biases, and properties of LLMs through this benchmarking process,
such as an inverse relation to the average degrees of freedom of traversal per
node in graphs, the overall negative impact of k-shot prompting on graph
reasoning tasks, and a positive response bias which prevents LLMs from
identifying the absence of a valid solution. Finally, we propose a new
prompting technique specially designed for graph traversal tasks, known as
PathCompare, which shows a notable increase in the performance of LLMs in
comparison to standard prompting and CoT.
\\ ( https://arxiv.org/abs/2402.01805 ,  1267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01806
Date: Fri, 2 Feb 2024 10:06:43 GMT   (253kb,D)

Title: HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack
  on Text
Authors: Han Liu, Zhi Xu, Xiaotong Zhang, Feng Zhang, Fenglong Ma, Hongyang
  Chen, Hong Yu and Xianchao Zhang
Categories: cs.CL cs.AI
\\
  Black-box hard-label adversarial attack on text is a practical and
challenging task, as the text data space is inherently discrete and
non-differentiable, and only the predicted label is accessible. Research on
this problem is still in the embryonic stage and only a few methods are
available. Nevertheless, existing methods rely on the complex heuristic
algorithm or unreliable gradient estimation strategy, which probably fall into
the local optimum and inevitably consume numerous queries, thus are difficult
to craft satisfactory adversarial examples with high semantic similarity and
low perturbation rate in a limited query budget. To alleviate above issues, we
propose a simple yet effective framework to generate high quality textual
adversarial examples under the black-box hard-label attack scenarios, named
HQA-Attack. Specifically, after initializing an adversarial example randomly,
HQA-attack first constantly substitutes original words back as many as
possible, thus shrinking the perturbation rate. Then it leverages the synonym
set of the remaining changed words to further optimize the adversarial example
with the direction which can improve the semantic similarity and satisfy the
adversarial condition simultaneously. In addition, during the optimizing
procedure, it searches a transition synonym word for each changed word, thus
avoiding traversing the whole synonym set and reducing the query number to some
extent. Extensive experimental results on five text classification datasets,
three natural language inference datasets and two real-world APIs have shown
that the proposed HQA-Attack method outperforms other strong baselines
significantly.
\\ ( https://arxiv.org/abs/2402.01806 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01812
Date: Fri, 2 Feb 2024 13:23:15 GMT   (87kb,D)

Title: Distilling LLMs' Decomposition Abilities into Compact Language Models
Authors: Denis Tarasov, Kumar Shridhar
Categories: cs.CL cs.AI cs.LG
Comments: https://github.com/DT6A/GSM8K-AI-SubQ
\\
  Large Language Models (LLMs) have demonstrated proficiency in their reasoning
abilities, yet their large size presents scalability challenges and limits any
further customization. In contrast, compact models offer customized training
but often fall short in solving complex reasoning tasks. This study focuses on
distilling the LLMs' decomposition skills into compact models using offline
reinforcement learning. We leverage the advancements in the LLM`s capabilities
to provide feedback and generate a specialized task-specific dataset for
training compact models. The development of an AI-generated dataset and the
establishment of baselines constitute the primary contributions of our work,
underscoring the potential of compact models in replicating complex
problem-solving skills.
\\ ( https://arxiv.org/abs/2402.01812 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01822
Date: Fri, 2 Feb 2024 16:35:00 GMT   (7682kb,D)

Title: Building Guardrails for Large Language Models
Authors: Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie
  Meng, Wenjie Ruan, Xiaowei Huang
Categories: cs.CL cs.AI
Comments: Under Review
\\
  As Large Language Models (LLMs) become more integrated into our daily lives,
it is crucial to identify and mitigate their risks, especially when the risks
can have profound impacts on human users and societies. Guardrails, which
filter the inputs or outputs of LLMs, have emerged as a core safeguarding
technology. This position paper takes a deep look at current open-source
solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the
challenges and the road towards building more complete solutions. Drawing on
robust evidence from previous research, we advocate for a systematic approach
to construct guardrails for LLMs, based on comprehensive consideration of
diverse contexts across various LLMs applications. We propose employing
socio-technical methods through collaboration with a multi-disciplinary team to
pinpoint precise technical requirements, exploring advanced neural-symbolic
implementations to embrace the complexity of the requirements, and developing
verification and testing to ensure the utmost quality of the final product.
\\ ( https://arxiv.org/abs/2402.01822 ,  7682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01825
Date: Fri, 2 Feb 2024 17:09:33 GMT   (1186kb,D)

Title: Fractal Patterns May Unravel the Intelligence in Next-Token Prediction
Authors: Ibrahim Alabdulmohsin, Vinh Q. Tran, Mostafa Dehghani
Categories: cs.CL cs.AI
Comments: 15 pages, 10 tables, 6 figures
\\
  We study the fractal structure of language, aiming to provide a precise
formalism for quantifying properties that may have been previously suspected
but not formally shown. We establish that language is: (1) self-similar,
exhibiting complexities at all levels of granularity, with no particular
characteristic context length, and (2) long-range dependent (LRD), with a Hurst
parameter of approximately H=0.70. Based on these findings, we argue that
short-term patterns/dependencies in language, such as in paragraphs, mirror the
patterns/dependencies over larger scopes, like entire documents. This may shed
some light on how next-token prediction can lead to a comprehension of the
structure of text at multiple levels of granularity, from words and clauses to
broader contexts and intents. We also demonstrate that fractal parameters
improve upon perplexity-based bits-per-byte (BPB) in predicting downstream
performance. We hope these findings offer a fresh perspective on language and
the mechanisms underlying the success of LLMs.
\\ ( https://arxiv.org/abs/2402.01825 ,  1186kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01826
Date: Fri, 2 Feb 2024 18:15:51 GMT   (291kb,D)

Title: Leveraging Large Language Models for Analyzing Blood Pressure Variations
  Across Biological Sex from Scientific Literature
Authors: Yuting Guo, Seyedeh Somayyeh Mousavi, Reza Sameni, Abeed Sarker
Categories: cs.CL cs.AI
\\
  Hypertension, defined as blood pressure (BP) that is above normal, holds
paramount significance in the realm of public health, as it serves as a
critical precursor to various cardiovascular diseases (CVDs) and significantly
contributes to elevated mortality rates worldwide. However, many existing BP
measurement technologies and standards might be biased because they do not
consider clinical outcomes, comorbidities, or demographic factors, making them
inconclusive for diagnostic purposes. There is limited data-driven research
focused on studying the variance in BP measurements across these variables. In
this work, we employed GPT-35-turbo, a large language model (LLM), to
automatically extract the mean and standard deviation values of BP for both
males and females from a dataset comprising 25 million abstracts sourced from
PubMed. 993 article abstracts met our predefined inclusion criteria (i.e.,
presence of references to blood pressure, units of blood pressure such as mmHg,
and mention of biological sex). Based on the automatically-extracted
information from these articles, we conducted an analysis of the variations of
BP values across biological sex. Our results showed the viability of utilizing
LLMs to study the BP variations across different demographic factors.
\\ ( https://arxiv.org/abs/2402.01826 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01828
Date: Fri, 2 Feb 2024 18:23:09 GMT   (457kb,D)

Title: Retrieval Augmented End-to-End Spoken Dialog Models
Authors: Mingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han, Yuan Cao, Dian Yu,
  Laurent El Shafey
Categories: cs.CL cs.AI
Journal-ref: Proc. ICASSP 2024
\\
  We recently developed SLM, a joint speech and language model, which fuses a
pretrained foundational speech model and a large language model (LLM), while
preserving the in-context learning capability intrinsic to the pretrained LLM.
In this paper, we apply SLM to speech dialog applications where the dialog
states are inferred directly from the audio signal.
  Task-oriented dialogs often contain domain-specific entities, i.e.,
restaurants, hotels, train stations, and city names, which are difficult to
recognize, however, critical for the downstream applications. Inspired by the
RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented
SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to
retrieve text entities mentioned in the audio. The retrieved entities are then
added as text inputs to the underlying SLM to bias model predictions. We
evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that
this retrieval augmentation boosts model performance, achieving joint goal
accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error
rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach
is broadly applicable to other speech tasks requiring contextual information or
domain-specific entities, such as contextual ASR with biasing capability.
\\ ( https://arxiv.org/abs/2402.01828 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01830
Date: Fri, 2 Feb 2024 18:49:26 GMT   (2411kb,D)

Title: Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in
  Open-environment
Authors: Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui Liu, Yu
  Wang, Ming Pang, Li Yuan
Categories: cs.CL cs.AI cs.LG
\\
  Existing large language models (LLMs) evaluation methods typically focus on
testing the performance on some closed-environment and domain-specific
benchmarks with human annotations. In this paper, we explore a novel
unsupervised evaluation direction, utilizing peer-review mechanisms to measure
LLMs automatically. In this setting, both open-source and closed-source LLMs
lie in the same environment, capable of answering unlabeled questions and
evaluating each other, where each LLM's response score is jointly determined by
other anonymous ones. To obtain the ability hierarchy among these models, we
assign each LLM a learnable capability parameter to adjust the final ranking.
We formalize it as a constrained optimization problem, intending to maximize
the consistency of each LLM's capabilities and scores. The key assumption
behind is that high-level LLM can evaluate others' answers more accurately than
low-level ones, while higher-level LLM can also achieve higher response scores.
Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap
in aligning human rankings. We perform experiments on multiple datasets with
these metrics, validating the effectiveness of the proposed approach.
\\ ( https://arxiv.org/abs/2402.01830 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01874
Date: Fri, 2 Feb 2024 20:01:15 GMT   (102kb,D)

Title: The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
  Learning and Large Language Models
Authors: Moschoula Pternea, Prerna Singh, Abir Chakraborty, Yagna Oruganti,
  Mirco Milletari, Sayli Bapat, Kebei Jiang
Categories: cs.CL cs.AI cs.LG cs.RO
Comments: 30 pages (including bibliography), 1 figure, 7 tables
\\
  In this work, we review research studies that combine Reinforcement Learning
(RL) and Large Language Models (LLMs), two areas that owe their momentum to the
development of deep neural networks. We propose a novel taxonomy of three main
classes based on the way that the two model types interact with each other. The
first class, RL4LLM, includes studies where RL is leveraged to improve the
performance of LLMs on tasks related to Natural Language Processing. L4LLM is
divided into two sub-categories depending on whether RL is used to directly
fine-tune an existing LLM or to improve the prompt of the LLM. In the second
class, LLM4RL, an LLM assists the training of an RL model that performs a task
that is not inherently related to natural language. We further break down
LLM4RL based on the component of the RL training framework that the LLM assists
or replaces, namely reward shaping, goal generation, and policy function.
Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a
common planning framework without either of them contributing to training or
fine-tuning of the other. We further branch this class to distinguish between
studies with and without natural language feedback. We use this taxonomy to
explore the motivations behind the synergy of LLMs and RL and explain the
reasons for its success, while pinpointing potential shortcomings and areas
where further research is needed, as well as alternative methodologies that
serve the same goal.
\\ ( https://arxiv.org/abs/2402.01874 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01878
Date: Fri, 2 Feb 2024 20:08:10 GMT   (1294kb,D)

Title: LiPO: Listwise Preference Optimization through Learning-to-Rank
Authors: Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh
  Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, Peter J. Liu,
  Xuanhui Wang
Categories: cs.CL cs.LG
\\
  Aligning language models (LMs) with curated human feedback is critical to
control their behaviors in real-world applications. Several recent policy
optimization methods, such as DPO and SLiC, serve as promising alternatives to
the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In
practice, human feedback often comes in a format of a ranked list over multiple
responses to amortize the cost of reading prompt. Multiple responses can also
be ranked by reward models or AI feedback. There lacks such a study on directly
fitting upon a list of responses. In this work, we formulate the LM alignment
as a listwise ranking problem and describe the Listwise Preference Optimization
(LiPO) framework, where the policy can potentially learn more effectively from
a ranked list of plausible responses given the prompt. This view draws an
explicit connection to Learning-to-Rank (LTR), where most existing preference
optimization work can be mapped to existing ranking objectives, especially
pairwise ones. Following this connection, we provide an examination of ranking
objectives that are not well studied for LM alignment withDPO and SLiC as
special cases when list size is two. In particular, we highlight a specific
method, LiPO-{\lambda}, which leverages a state-of-the-art listwise ranking
objective and weights each preference pair in a more advanced manner. We show
that LiPO-{\lambda} can outperform DPO and SLiC by a clear margin on two
preference alignment tasks.
\\ ( https://arxiv.org/abs/2402.01878 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01917
Date: Fri, 2 Feb 2024 21:38:12 GMT   (35kb)

Title: Whispering in Norwegian: Navigating Orthographic and Dialectic
  Challenges
Authors: Per E Kummervold, Javier de la Rosa, Freddy Wetjen, Rolv-Arild
  Braaten, Per Erik Solberg
Categories: cs.CL
\\
  This article introduces NB-Whisper, an adaptation of OpenAI's Whisper,
specifically fine-tuned for Norwegian language Automatic Speech Recognition
(ASR). We highlight its key contributions and summarise the results achieved in
converting spoken Norwegian into written forms and translating other languages
into Norwegian. We show that we are able to improve the Norwegian Bokm{\aa}l
transcription by OpenAI Whisper Large-v3 from a WER of 10.4 to 6.6 on the
Fleurs Dataset and from 6.8 to 2.2 on the NST dataset.
\\ ( https://arxiv.org/abs/2402.01917 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01935
Date: Fri, 2 Feb 2024 22:19:15 GMT   (8232kb,D)

Title: Code Representation Learning At Scale
Authors: Dejiao Zhang, Wasi Ahmad, Ming Tan, Hantian Ding, Ramesh Nallapati,
  Dan Roth, Xiaofei Ma, Bing Xiang
Categories: cs.CL
Comments: 10 pages
Journal-ref: ICLR 2024
\\
  Recent studies have shown that code language models at scale demonstrate
significant performance gains on downstream tasks, i.e., code generation.
However, most of the existing works on code representation learning train
models at a hundred million parameter scale using very limited pretraining
corpora. In this work, we fuel code representation learning with a vast amount
of code data via a two-stage pretraining scheme. We first train the encoders
via a mix that leverages both randomness in masking language modeling and the
structure aspect of programming language. We then enhance the representations
via contrastive learning with hard negative and hard positive constructed in an
unsupervised manner. We establish an off-the-shelf encoder model that
persistently outperforms the existing models on a wide variety of downstream
tasks by large margins. To comprehend the factors contributing to successful
code representation learning, we conduct detailed ablations and share our
findings on (i) a customized and effective token-level denoising scheme for
source code; (ii) the importance of hard negatives and hard positives; (iii)
how the proposed bimodal contrastive learning boost the cross-lingual semantic
search performance; and (iv) how the pretraining schemes decide the downstream
task performance scales with the model size.
\\ ( https://arxiv.org/abs/2402.01935 ,  8232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01939
Date: Fri, 2 Feb 2024 22:25:44 GMT   (7700kb,D)

Title: A Morphologically-Aware Dictionary-based Data Augmentation Technique for
  Machine Translation of Under-Represented Languages
Authors: Md Mahfuz Ibn Alam, Sina Ahmadi and Antonios Anastasopoulos
Categories: cs.CL
\\
  The availability of parallel texts is crucial to the performance of machine
translation models. However, most of the world's languages face the predominant
challenge of data scarcity. In this paper, we propose strategies to synthesize
parallel data relying on morpho-syntactic information and using bilingual
lexicons along with a small amount of seed parallel data. Our methodology
adheres to a realistic scenario backed by the small parallel seed data. It is
linguistically informed, as it aims to create augmented data that is more
likely to be grammatically correct. We analyze how our synthetic data can be
combined with raw parallel data and demonstrate a consistent improvement in
performance in our experiments on 14 languages (28 English <-> X pairs) ranging
from well- to very low-resource ones. Our method leads to improvements even
when using only five seed sentences and a bilingual lexicon.
\\ ( https://arxiv.org/abs/2402.01939 ,  7700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01945
Date: Fri, 2 Feb 2024 22:42:33 GMT   (7720kb,D)

Title: A Case Study on Filtering for End-to-End Speech Translation
Authors: Md Mahfuz Ibn Alam and Antonios Anastasopoulos
Categories: cs.CL
\\
  It is relatively easy to mine a large parallel corpus for any machine
learning task, such as speech-to-text or speech-to-speech translation. Although
these mined corpora are large in volume, their quality is questionable. This
work shows that the simplest filtering technique can trim down these big, noisy
datasets to a more manageable, clean dataset. We also show that using this
clean dataset can improve the model's performance, as in the case of the
multilingual-to-English Speech Translation (ST) model, where, on average, we
obtain a 4.65 BLEU score improvement.
\\ ( https://arxiv.org/abs/2402.01945 ,  7720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01967
Date: Sat, 3 Feb 2024 00:23:36 GMT   (7181kb,D)

Title: MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate
  Speech and Target Detection Using Transformer Ensembles
Authors: Amrita Ganguly, Al Nahian Bin Emran, Sadiya Sayara Chowdhury Puspo, Md
  Nishat Raihan, Dhiman Goswami, Marcos Zampieri
Categories: cs.CL
\\
  The automatic identification of offensive language such as hate speech is
important to keep discussions civil in online communities. Identifying hate
speech in multimodal content is a particularly challenging task because
offensiveness can be manifested in either words or images or a juxtaposition of
the two. This paper presents the MasonPerplexity submission for the Shared Task
on Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task
is divided into two sub-tasks: sub-task A focuses on the identification of hate
speech and sub-task B focuses on the identification of targets in text-embedded
images during political events. We use an XLM-roBERTa-large model for sub-task
A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and
BERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A
and 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.
\\ ( https://arxiv.org/abs/2402.01967 ,  7181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01976
Date: Sat, 3 Feb 2024 01:06:33 GMT   (6840kb,D)

Title: MasonPerplexity at ClimateActivism 2024: Integrating Advanced Ensemble
  Techniques and Data Augmentation for Climate Activism Stance and Hate Event
  Identification
Authors: Al Nahian Bin Emran, Amrita Ganguly, Sadiya Sayara Chowdhury Puspo,
  Dhiman Goswami, Md Nishat Raihan
Categories: cs.CL
\\
  The task of identifying public opinions on social media, particularly
regarding climate activism and the detection of hate events, has emerged as a
critical area of research in our rapidly changing world. With a growing number
of people voicing either to support or oppose to climate-related issues -
understanding these diverse viewpoints has become increasingly vital. Our team,
MasonPerplexity, participates in a significant research initiative focused on
this subject. We extensively test various models and methods, discovering that
our most effective results are achieved through ensemble modeling, enhanced by
data augmentation techniques like back-translation. In the specific components
of this research task, our team achieved notable positions, ranking 5th, 1st,
and 6th in the respective sub-tasks, thereby illustrating the effectiveness of
our approach in this important field of study.
\\ ( https://arxiv.org/abs/2402.01976 ,  6840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01980
Date: Sat, 3 Feb 2024 01:33:16 GMT   (7698kb,D)

Title: SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks
Authors: Gourab Dey, Adithya V Ganesan, Yash Kumar Lal, Manal Shah, Shreyashee
  Sinha, Matthew Matero, Salvatore Giorgi, Vivek Kulkarni, H. Andrew Schwartz
Categories: cs.CL
Comments: Short paper accepted to EACL 2024. 4 pgs, 2 tables
\\
  Social science NLP tasks, such as emotion or humor detection, are required to
capture the semantics along with the implicit pragmatics from text, often with
limited amounts of training data. Instruction tuning has been shown to improve
the many capabilities of large language models (LLMs) such as commonsense
reasoning, reading comprehension, and computer programming. However, little is
known about the effectiveness of instruction tuning on the social domain where
implicit pragmatic cues are often needed to be captured. We explore the use of
instruction tuning for social science NLP tasks and introduce Socialite-Llama
-- an open-source, instruction-tuned Llama. On a suite of 20 social science
tasks, Socialite-Llama improves upon the performance of Llama as well as
matches or improves upon the performance of a state-of-the-art, multi-task
finetuned model on a majority of them. Further, Socialite-Llama also leads to
improvement on 5 out of 6 related social tasks as compared to Llama, suggesting
instruction tuning can lead to generalized social understanding. All resources
including our code, model and dataset can be found through
bit.ly/socialitellama.
\\ ( https://arxiv.org/abs/2402.01980 ,  7698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01981
Date: Sat, 3 Feb 2024 01:40:11 GMT   (610kb,D)

Title: Self-Debiasing Large Language Models: Zero-Shot Recognition and
  Reduction of Stereotypes
Authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Tong
  Yu, Hanieh Deilamsalehy, Ruiyi Zhang, Sungchul Kim, Franck Dernoncourt
Categories: cs.CL cs.AI cs.CY cs.LG
\\
  Large language models (LLMs) have shown remarkable advances in language
generation and understanding but are also prone to exhibiting harmful social
biases. While recognition of these behaviors has generated an abundance of bias
mitigation techniques, most require modifications to the training data, model
parameters, or decoding strategy, which may be infeasible without access to a
trainable model. In this work, we leverage the zero-shot capabilities of LLMs
to reduce stereotyping in a technique we introduce as zero-shot self-debiasing.
With two approaches, self-debiasing via explanation and self-debiasing via
reprompting, we show that self-debiasing can significantly reduce the degree of
stereotyping across nine different social groups while relying only on the LLM
itself and a simple prompt, with explanations correctly identifying invalid
assumptions and reprompting delivering the greatest reductions in bias. We hope
this work opens inquiry into other zero-shot techniques for bias mitigation.
\\ ( https://arxiv.org/abs/2402.01981 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02008
Date: Sat, 3 Feb 2024 03:44:57 GMT   (5987kb,D)

Title: How well do LLMs cite relevant medical references? An evaluation
  framework and analyses
Authors: Kevin Wu, Eric Wu, Ally Cassasola, Angela Zhang, Kevin Wei, Teresa
  Nguyen, Sith Riantawan, Patricia Shi Riantawan, Daniel E. Ho, James Zou
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) are currently being used to answer medical
questions across a variety of clinical domains. Recent top-performing
commercial LLMs, in particular, are also capable of citing sources to support
their responses. In this paper, we ask: do the sources that LLMs generate
actually support the claims that they make? To answer this, we propose three
contributions. First, as expert medical annotations are an expensive and
time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is
highly accurate in validating source relevance, agreeing 88% of the time with a
panel of medical doctors. Second, we develop an end-to-end, automated pipeline
called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs
on a dataset of 1200 generated questions, totaling over 40K pairs of statements
and sources. Interestingly, we find that between ~50% to 90% of LLM responses
are not fully supported by the sources they provide. We also evaluate GPT-4
with retrieval augmented generation (RAG) and find that, even still, around
30\% of individual statements are unsupported, while nearly half of its
responses are not fully supported. Third, we open-source our curated dataset of
medical questions and expert annotations for future evaluations. Given the
rapid pace of LLM development and the potential harms of incorrect or outdated
medical information, it is crucial to also understand and quantify their
capability to produce relevant, trustworthy medical references.
\\ ( https://arxiv.org/abs/2402.02008 ,  5987kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02030
Date: Sat, 3 Feb 2024 05:01:04 GMT   (4468kb,D)

Title: Panacea: Pareto Alignment via Preference Adaptation for LLMs
Authors: Yifan Zhong, Chengdong Ma, Xiaoyuan Zhang, Ziran Yang, Qingfu Zhang,
  Siyuan Qi, Yaodong Yang
Categories: cs.CL
\\
  Current methods for large language model alignment typically use scalar human
preference labels. However, this convention tends to oversimplify the
multi-dimensional and heterogeneous nature of human preferences, leading to
reduced expressivity and even misalignment. This paper presents Panacea, an
innovative approach that reframes alignment as a multi-dimensional preference
optimization problem. Panacea trains a single model capable of adapting online
and Pareto-optimally to diverse sets of preferences without the need for
further tuning. A major challenge here is using a low-dimensional preference
vector to guide the model's behavior, despite it being governed by an
overwhelmingly large number of parameters. To address this, Panacea is designed
to use singular value decomposition (SVD)-based low-rank adaptation, which
allows the preference vector to be simply injected online as singular values.
Theoretically, we prove that Panacea recovers the entire Pareto front with
common loss aggregation methods under mild conditions. Moreover, our
experiments demonstrate, for the first time, the feasibility of aligning a
single LLM to represent a spectrum of human preferences through various
optimization methods. Our work marks a step forward in effectively and
efficiently aligning models to diverse and intricate human preferences in a
controllable and Pareto-optimal manner.
\\ ( https://arxiv.org/abs/2402.02030 ,  4468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02056
Date: Sat, 3 Feb 2024 06:36:11 GMT   (887kb,D)

Title: AnthroScore: A Computational Linguistic Measure of Anthropomorphism
Authors: Myra Cheng, Kristina Gligoric, Tiziano Piccardi, Dan Jurafsky
Categories: cs.CL cs.AI cs.CY
Comments: EACL 2024 Main Conference
\\
  Anthropomorphism, or the attribution of human-like characteristics to
non-human entities, has shaped conversations about the impacts and
possibilities of technology. We present AnthroScore, an automatic metric of
implicit anthropomorphism in language. We use a masked language model to
quantify how non-human entities are implicitly framed as human by the
surrounding context. We show that AnthroScore corresponds with human judgments
of anthropomorphism and dimensions of anthropomorphism described in social
science literature. Motivated by concerns of misleading anthropomorphism in
computer science discourse, we use AnthroScore to analyze 15 years of research
papers and downstream news articles. In research papers, we find that
anthropomorphism has steadily increased over time, and that papers related to
language models have the most anthropomorphism. Within ACL papers, temporal
increases in anthropomorphism are correlated with key neural advancements.
Building upon concerns of scientific misinformation in mass media, we identify
higher levels of anthropomorphism in news headlines compared to the research
papers they cite. Since AnthroScore is lexicon-free, it can be directly applied
to a wide range of text sources.
\\ ( https://arxiv.org/abs/2402.02056 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02077
Date: Sat, 3 Feb 2024 08:16:39 GMT   (606kb,D)

Title: Investigating Content Planning for Navigating Trade-offs in
  Knowledge-Grounded Dialogue
Authors: Kushal Chawla, Hannah Rashkin, Gaurav Singh Tomar, David Reitter
Categories: cs.CL
Comments: Accepted at EACL 2024 Main Conference (Long)
\\
  Knowledge-grounded dialogue generation is a challenging task because it
requires satisfying two fundamental yet often competing constraints: being
responsive in a manner that is specific to what the conversation partner has
said while also being attributable to an underlying source document. In this
work, we bring this trade-off between these two objectives (specificity and
attribution) to light and ask the question: Can explicit content planning
before the response generation help the model to address this challenge? To
answer this question, we design a framework called PLEDGE, which allows us to
experiment with various plan variables explored in prior work, supporting both
metric-agnostic and metric-aware approaches. While content planning shows
promise, our results on whether it can actually help to navigate this trade-off
are mixed -- planning mechanisms that are metric-aware (use automatic metrics
during training) are better at automatic evaluations but underperform in human
judgment compared to metric-agnostic mechanisms. We discuss how this may be
caused by over-fitting to automatic metrics and the need for future work to
better calibrate these metrics towards human judgment. We hope the observations
from our analysis will inform future work that aims to apply content planning
in this context.
\\ ( https://arxiv.org/abs/2402.02077 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02078
Date: Sat, 3 Feb 2024 08:16:43 GMT   (183kb,D)

Title: Exploring the Robustness of Task-oriented Dialogue Systems for
  Colloquial German Varieties
Authors: Ekaterina Artemova and Verena Blaschke and Barbara Plank
Categories: cs.CL
Comments: To appear in EACL 2024 (main)
\\
  Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the
transfer learning paradigm by training a joint model for intent recognition and
slot-filling in English and applying it, zero-shot, to other languages. We
address a gap in prior research, which often overlooked the transfer to
lower-resource colloquial varieties due to limited test data. Inspired by prior
work on English varieties, we craft and manually evaluate perturbation rules
that transform German sentences into colloquial forms and use them to
synthesize test sets in four ToD datasets. Our perturbation rules cover 18
distinct language phenomena, enabling us to explore the impact of each
perturbation on slot and intent performance. Using these new datasets, we
conduct an experimental evaluation across six different transformers. Here, we
demonstrate that when applied to colloquial varieties, ToD systems maintain
their intent recognition performance, losing 6% (4.62 percentage points) in
accuracy on average. However, they exhibit a significant drop in slot
detection, with a decrease of 31% (21 percentage points) in slot F1 score. Our
findings are further supported by a transfer experiment from Standard American
English to synthetic Urban African American Vernacular English.
\\ ( https://arxiv.org/abs/2402.02078 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02080
Date: Sat, 3 Feb 2024 08:22:51 GMT   (699kb,D)

Title: Translation Errors Significantly Impact Low-Resource Languages in
  Cross-Lingual Learning
Authors: Ashish Sunil Agrawal, Barah Fazili, Preethi Jyothi
Categories: cs.CL
Comments: Accepted to main proceedings of "The 18th Conference of the European
  Chapter of the Association for Computational Linguistics"
\\
  Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language
understanding consist of parallel versions of English evaluation sets in
multiple target languages created with the help of professional translators.
When creating such parallel data, it is critical to ensure high-quality
translations for all target languages for an accurate characterization of
cross-lingual transfer. In this work, we find that translation inconsistencies
do exist and interestingly they disproportionally impact low-resource languages
in XNLI. To identify such inconsistencies, we propose measuring the gap in
performance between zero-shot evaluations on the human-translated and
machine-translated target text across multiple target languages; relatively
large gaps are indicative of translation errors. We also corroborate that
translation errors exist for two target languages, namely Hindi and Urdu, by
doing a manual reannotation of human-translated test instances in these two
languages and finding poor agreement with the original English labels these
instances were supposed to inherit.
\\ ( https://arxiv.org/abs/2402.02080 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02082
Date: Sat, 3 Feb 2024 08:44:11 GMT   (6576kb,D)

Title: GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative
  Decoding
Authors: Cunxiao Du, Jing Jiang, Xu Yuanchen, Jiawei Wu, Sicheng Yu, Yongqi Li,
  Shenggui Li, Kai Xu, Liqiang Nie, Zhaopeng Tu, Yang You
Categories: cs.CL
\\
  Speculative decoding is a relatively new decoding framework that leverages
small and efficient draft models to reduce the latency of LLMs. In this study,
we introduce GliDe and CaPE, two low-hassle modifications to vanilla
speculative decoding to further improve the decoding speed of a frozen LLM.
Specifically, GliDe is a modified draft model architecture that reuses the
cached keys and values from the target LLM, while CaPE is a proposal expansion
method that uses the draft model's confidence scores to help select additional
candidate tokens for verification. Extensive experiments on different
benchmarks demonstrate that our proposed GliDe draft model significantly
reduces the expected decoding latency. Additional evaluation using walltime
reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend
the improvement to 2.61x with CaPE. We will release our code, data, and the
trained draft models.
\\ ( https://arxiv.org/abs/2402.02082 ,  6576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02084
Date: Sat, 3 Feb 2024 08:50:50 GMT   (6849kb,D)

Title: Revisiting the Markov Property for Machine Translation
Authors: Cunxiao Du, Hao Zhou, Zhaopeng Tu, Jing Jiang
Categories: cs.CL
Comments: EACL (Findings)
\\
  In this paper, we re-examine the Markov property in the context of neural
machine translation. We design a Markov Autoregressive Transformer~(MAT) and
undertake a comprehensive assessment of its performance across four WMT
benchmarks. Our findings indicate that MAT with an order larger than 4 can
generate translations with quality on par with that of conventional
autoregressive transformers. In addition, counter-intuitively, we also find
that the advantages of utilizing a higher-order MAT do not specifically
contribute to the translation of longer sentences.
\\ ( https://arxiv.org/abs/2402.02084 ,  6849kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02099
Date: Sat, 3 Feb 2024 09:41:52 GMT   (5633kb,D)

Title: Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in
  Multilingual Language Models
Authors: Sara Rajaee and Christof Monz
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to EACL 2024
\\
  Recent advances in training multilingual language models on large datasets
seem to have shown promising results in knowledge transfer across languages and
achieve high performance on downstream tasks. However, we question to what
extent the current evaluation benchmarks and setups accurately measure
zero-shot cross-lingual knowledge transfer. In this work, we challenge the
assumption that high zero-shot performance on target tasks reflects high
cross-lingual ability by introducing more challenging setups involving
instances with multiple languages. Through extensive experiments and analysis,
we show that the observed high performance of multilingual models can be
largely attributed to factors not requiring the transfer of actual linguistic
knowledge, such as task- and surface-level knowledge. More specifically, we
observe what has been transferred across languages is mostly data artifacts and
biases, especially for low-resource languages. Our findings highlight the
overlooked drawbacks of existing cross-lingual test data and evaluation setups,
calling for a more nuanced understanding of the cross-lingual capabilities of
multilingual models.
\\ ( https://arxiv.org/abs/2402.02099 ,  5633kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02101
Date: Sat, 3 Feb 2024 09:48:54 GMT   (8327kb,D)

Title: Are Large Language Models Good Prompt Optimizers?
Authors: Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi
  Zhang, Xuanjing Huang
Categories: cs.CL cs.AI
\\
  LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as
Prompt Optimizers to self-reflect and refine prompts, has shown promising
performance in recent studies. Despite the success, the underlying mechanism of
this approach remains unexplored, and the true effectiveness of LLMs as Prompt
Optimizers requires further validation. In this work, we conducted a
comprehensive study to uncover the actual mechanism of LLM-based Prompt
Optimization. Our findings reveal that the LLM optimizers struggle to identify
the true causes of errors during reflection, tending to be biased by their own
prior knowledge rather than genuinely reflecting on the errors. Furthermore,
even when the reflection is semantically valid, the LLM optimizers often fail
to generate appropriate prompts for the target models with a single prompt
refinement step, partly due to the unpredictable behaviors of the target
models. Based on the observations, we introduce a new "Automatic Behavior
Optimization" paradigm, which directly optimizes the target model's behavior in
a more controllable manner. We hope our study can inspire new directions for
automatic prompt optimization development.
\\ ( https://arxiv.org/abs/2402.02101 ,  8327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02113
Date: Sat, 3 Feb 2024 10:41:05 GMT   (7650kb,D)

Title: Zero-shot Sentiment Analysis in Low-Resource Languages Using a
  Multilingual Sentiment Lexicon
Authors: Fajri Koto and Tilman Beck and Zeerak Talat and Iryna Gurevych and
  Timothy Baldwin
Categories: cs.CL
Comments: Accepted at EACL 2024
\\
  Improving multilingual language models capabilities in low-resource languages
is generally difficult due to the scarcity of large-scale data in those
languages. In this paper, we relax the reliance on texts in low-resource
languages by using multilingual lexicons in pretraining to enhance multilingual
capabilities. Specifically, we focus on zero-shot sentiment analysis tasks
across 34 languages, including 6 high/medium-resource languages, 25
low-resource languages, and 3 code-switching datasets. We demonstrate that
pretraining using multilingual lexicons, without using any sentence-level
sentiment data, achieves superior zero-shot performance compared to models
fine-tuned on English sentiment datasets, and large language models like
GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen
low-resource languages to code-mixed scenarios involving high-resource
languages.
\\ ( https://arxiv.org/abs/2402.02113 ,  7650kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02130
Date: Sat, 3 Feb 2024 12:19:47 GMT   (12901kb,D)

Title: Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
Authors: Yanbin Wei, Shuai Fu, Weisen Jiang, James T. Kwok, Yu Zhang
Categories: cs.CL
\\
  Large Language Models (LLMs) are increasingly used for various tasks with
graph structures, such as robotic planning, knowledge graph completion, and
common-sense reasoning. Though LLMs can comprehend graph information in a
textual format, they overlook the rich visual modality, which is an intuitive
way for humans to comprehend structural information and conduct graph
reasoning. The potential benefits and capabilities of representing graph
structures as visual images (i.e., visual graph) is still unexplored. In this
paper, we take the first step in incorporating visual information into graph
reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple
(graph, image, textual description). We conduct extensive experiments on the
GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph
reasoning tasks show that combining textual and visual information together
performs better than using one modality alone. Moreover, the LLaVA-7B/13B
models finetuned on the training set achieve higher accuracy than the
closed-source model GPT-4(V). We also study the effects of augmentations in
graph reasoning.
\\ ( https://arxiv.org/abs/2402.02130 ,  12901kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02135
Date: Sat, 3 Feb 2024 12:52:36 GMT   (9854kb,D)

Title: Do Moral Judgment and Reasoning Capability of LLMs Change with Language?
  A Study using the Multilingual Defining Issues Test
Authors: Aditi Khandelwal, Utkarsh Agarwal, Kumar Tanmay, Monojit Choudhury
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2024 (main)
\\
  This paper explores the moral judgment and moral reasoning abilities
exhibited by Large Language Models (LLMs) across languages through the Defining
Issues Test. It is a well known fact that moral judgment depends on the
language in which the question is asked. We extend the work of beyond English,
to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe
three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial
multilingual text processing and generation abilities. Our study shows that the
moral reasoning ability for all models, as indicated by the post-conventional
score, is substantially inferior for Hindi and Swahili, compared to Spanish,
Russian, Chinese and English, while there is no clear trend for the performance
of the latter four languages. The moral judgments too vary considerably by the
language.
\\ ( https://arxiv.org/abs/2402.02135 ,  9854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02144
Date: Sat, 3 Feb 2024 13:23:51 GMT   (266kb,D)

Title: Probing Critical Learning Dynamics of PLMs for Hate Speech Detection
Authors: Sarah Masud, Mohammad Aflah Khan, Vikram Goyal, Md Shad Akhtar, Tanmoy
  Chakraborty
Categories: cs.CL
Comments: 20 pages, 9 figures, 14 tables. Accepted at EACL'24
\\
  Despite the widespread adoption, there is a lack of research into how various
critical aspects of pretrained language models (PLMs) affect their performance
in hate speech detection. Through five research questions, our findings and
recommendations lay the groundwork for empirically investigating different
aspects of PLMs' use in hate speech detection. We deep dive into comparing
different pretrained models, evaluating their seed robustness, finetuning
settings, and the impact of pretraining data collection time. Our analysis
reveals early peaks for downstream tasks during pretraining, the limited
benefit of employing a more recent pretraining corpus, and the significance of
specific layers during finetuning. We further call into question the use of
domain-specific models and highlight the need for dynamic datasets for
benchmarking hate speech detection.
\\ ( https://arxiv.org/abs/2402.02144 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02145
Date: Sat, 3 Feb 2024 13:27:32 GMT   (8240kb,D)

Title: Analyzing Sentiment Polarity Reduction in News Presentation through
  Contextual Perturbation and Large Language Models
Authors: Alapan Kuila, Somnath Jena, Sudeshna Sarkar, Partha Pratim Chakrabarti
Categories: cs.CL
Comments: Accepted in ICON 2023
\\
  In today's media landscape, where news outlets play a pivotal role in shaping
public opinion, it is imperative to address the issue of sentiment manipulation
within news text. News writers often inject their own biases and emotional
language, which can distort the objectivity of reporting. This paper introduces
a novel approach to tackle this problem by reducing the polarity of latent
sentiments in news content. Drawing inspiration from adversarial attack-based
sentence perturbation techniques and a prompt based method using ChatGPT, we
employ transformation constraints to modify sentences while preserving their
core semantics. Using three perturbation methods: replacement, insertion, and
deletion coupled with a context-aware masked language model, we aim to maximize
the desired sentiment score for targeted news aspects through a beam search
algorithm. Our experiments and human evaluations demonstrate the effectiveness
of these two models in achieving reduced sentiment polarity with minimal
modifications while maintaining textual similarity, fluency, and grammatical
correctness. Comparative analysis confirms the competitive performance of the
adversarial attack based perturbation methods and prompt-based methods,
offering a promising solution to foster more objective news reporting and
combat emotional language bias in the media.
\\ ( https://arxiv.org/abs/2402.02145 ,  8240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02175
Date: Sat, 3 Feb 2024 14:54:13 GMT   (565kb,D)

Title: Enhancing Complex Question Answering over Knowledge Graphs through
  Evidence Pattern Retrieval
Authors: Wentao Ding, Jinmao Li, Liangchuan Luo, Yuzhong Qu
Categories: cs.CL cs.IR
Comments: Accepted to TheWebConf'24 (WWW 2024). This is a preprint version; the
  CR version will include more details. Github:
  https://github.com/nju-websoft/EPR-KGQA
\\
  Information retrieval (IR) methods for KGQA consist of two stages: subgraph
extraction and answer reasoning. We argue current subgraph extraction methods
underestimate the importance of structural dependencies among evidence facts.
We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural
dependencies during subgraph extraction. We implement EPR by indexing the
atomic adjacency pattern of resource pairs. Given a question, we perform dense
retrieval to obtain atomic patterns formed by resource pairs. We then enumerate
their combinations to construct candidate evidence patterns. These evidence
patterns are scored using a neural model, and the best one is selected to
extract a subgraph for downstream answer reasoning. Experimental results
demonstrate that the EPR-based approach has significantly improved the F1
scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves
competitive performance on WebQuestionsSP.
\\ ( https://arxiv.org/abs/2402.02175 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02212
Date: Sat, 3 Feb 2024 17:13:03 GMT   (70kb,D)

Title: A Data Generation Perspective to the Mechanism of In-Context Learning
Authors: Haitao Mao, Guangliang Liu, Yao Ma, Rongrong Wang, Jiliang Tang
Categories: cs.CL
Comments: 11 pages, 1 figure
\\
  In-Context Learning (ICL) empowers Large Language Models (LLMs) with the
capacity to learn in context, achieving downstream generalization without
gradient updates but with a few in-context examples. Despite the encouraging
empirical success, the underlying mechanism of ICL remains unclear, and
existing research offers various viewpoints of understanding. These studies
propose intuition-driven and ad-hoc technical solutions for interpreting ICL,
illustrating an ambiguous road map. In this paper, we leverage a data
generation perspective to reinterpret recent efforts and demonstrate the
potential broader usage of popular technical solutions, approaching a
systematic angle. For a conceptual definition, we rigorously adopt the terms of
skill learning and skill recognition. The difference between them is skill
learning can learn new data generation functions from in-context data. We also
provide a comprehensive study on the merits and weaknesses of different
solutions, and highlight the uniformity among them given the perspective of
data generation, establishing a technical foundation for future research to
incorporate the strengths of different lines of research.
\\ ( https://arxiv.org/abs/2402.02212 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02243
Date: Sat, 3 Feb 2024 19:19:34 GMT   (675kb)

Title: Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding
Authors: Stevan Harnad
Categories: cs.CL q-bio.NC
Comments: 48 pages, 25 references
\\
  Apart from what (little) OpenAI may be concealing from us, we all know
(roughly) how ChatGPT works (its huge text database, its statistics, its vector
representations, and their huge number of parameters, its next-word training,
and so on). But none of us can say (hand on heart) that we are not surprised by
what ChatGPT has proved to be able to do with these resources. This has even
driven some of us to conclude that ChatGPT actually understands. It is not true
that it understands. But it is also not true that we understand how it can do
what it can do. I will suggest some hunches about benign biases: convergent
constraints that emerge at LLM scale that may be helping ChatGPT do so much
better than we would have expected. These biases are inherent in the nature of
language itself, at LLM scale, and they are closely linked to what it is that
ChatGPT lacks, which is direct sensorimotor grounding to connect its words to
their referents and its propositions to their meanings. These convergent biases
are related to (1) the parasitism of indirect verbal grounding on direct
sensorimotor grounding, (2) the circularity of verbal definition, (3) the
mirroring of language production and comprehension, (4) iconicity in
propositions at LLM scale, (5) computational counterparts of human categorical
perception in category learning by neural nets, and perhaps also (6) a
conjecture by Chomsky about the laws of thought. The exposition will be in the
form of a dialogue with ChatGPT-4.
\\ ( https://arxiv.org/abs/2402.02243 ,  675kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02244
Date: Sat, 3 Feb 2024 19:20:02 GMT   (232kb,D)

Title: Beyond the Limits: A Survey of Techniques to Extend the Context Length
  in Large Language Models
Authors: Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi
  Rezagholizadeh, Armaghan Eshaghi
Categories: cs.CL cs.LG
\\
  Recently, large language models (LLMs) have shown remarkable capabilities
including understanding context, engaging in logical reasoning, and generating
responses. However, this is achieved at the expense of stringent computational
and memory requirements, hindering their ability to effectively support long
input sequences. This survey provides an inclusive review of the recent
techniques and methods devised to extend the sequence length in LLMs, thereby
enhancing their capacity for long-context understanding. In particular, we
review and categorize a wide range of techniques including architectural
modifications, such as modified positional encoding and altered attention
mechanisms, which are designed to enhance the processing of longer sequences
while avoiding a proportional increase in computational requirements. The
diverse methodologies investigated in this study can be leveraged across
different phases of LLMs, i.e., training, fine-tuning and inference. This
enables LLMs to efficiently process extended sequences. The limitations of the
current methodologies is discussed in the last section along with the
suggestions for future research directions, underscoring the importance of
sequence length in the continued advancement of LLMs.
\\ ( https://arxiv.org/abs/2402.02244 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02255
Date: Sat, 3 Feb 2024 20:22:54 GMT   (8608kb,D)

Title: Frequency Explains the Inverse Correlation of Large Language Models'
  Size, Training Data Amount, and Surprisal's Fit to Reading Times
Authors: Byung-Doh Oh, Shisen Yue, William Schuler
Categories: cs.CL cs.LG
Comments: EACL 2024
\\
  Recent studies have shown that as Transformer-based language models become
larger and are trained on very large amounts of data, the fit of their
surprisal estimates to naturalistic human reading times degrades. The current
work presents a series of analyses showing that word frequency is a key
explanatory factor underlying these two trends. First, residual errors from
four language model families on four corpora show that the inverse correlation
between model size and fit to reading times is the strongest on the subset of
least frequent words, which is driven by excessively accurate predictions of
larger model variants. Additionally, training dynamics reveal that during later
training steps, all model variants learn to predict rare words and that larger
model variants do so more accurately, which explains the detrimental effect of
both training data amount and model size on fit to reading times. Finally, a
feature attribution analysis demonstrates that larger model variants are able
to accurately predict rare words based on both an effectively longer context
window size as well as stronger local associations compared to smaller model
variants. Taken together, these results indicate that Transformer-based
language models' surprisal estimates diverge from human-like expectations due
to the superhumanly complex associations they learn for predicting rare words.
\\ ( https://arxiv.org/abs/2402.02255 ,  8608kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02262
Date: Sat, 3 Feb 2024 20:58:09 GMT   (265kb,D)

Title: Data Quality Matters: Suicide Intention Detection on Social Media Posts
  Using a RoBERTa-CNN Model
Authors: Emily Lin, Jian Sun, Hsingyu Chen, and Mohammad H. Mahoor
Categories: cs.CL cs.AI
Comments: 4 pages, 1 figure, 4 tables
\\
  Suicide remains a global health concern for the field of health, which
urgently needs innovative approaches for early detection and intervention. In
this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit
posts and present a novel approach to suicide detection using the cutting-edge
RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach).
RoBERTa is used for various Natural Language Processing (NLP) tasks, including
text classification and sentiment analysis. The effectiveness of the RoBERTa
lies in its ability to capture textual information and form semantic
relationships within texts. By adding the Convolution Neural Network (CNN)
layer to the original model, the RoBERTa enhances its ability to capture
important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we
experimented on the Suicide and Depression Detection dataset and obtained solid
results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard
deviation (STD) of 0.0009. It also reaches over 97.5% mean AUC value with an
STD of 0.0013. In the meanwhile, RoBERTa-CNN outperforms competitive methods,
demonstrating the robustness and ability to capture nuanced linguistic patterns
for suicidal intentions. Therefore, RoBERTa-CNN can detect suicide intention on
text data very well.
\\ ( https://arxiv.org/abs/2402.02262 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02285
Date: Sat, 3 Feb 2024 22:49:00 GMT   (4155kb,D)

Title: SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State
  Tracking
Authors: Atharva Kulkarni, Bo-Hsiang Tseng, Joel Ruben Antony Moniz, Dhivya
  Piraviperumal, Hong Yu, Shruti Bhargava
Categories: cs.CL cs.AI cs.LG
Comments: 9 pages. 4 figures, EACL 2024 main conference
\\
  In-context learning with Large Language Models (LLMs) has emerged as a
promising avenue of research in Dialog State Tracking (DST). However, the
best-performing in-context learning methods involve retrieving and adding
similar examples to the prompt, requiring access to labeled training data.
Procuring such training data for a wide range of domains and applications is
time-consuming, expensive, and, at times, infeasible. While zero-shot learning
requires no training data, it significantly lags behind the few-shot setup.
Thus, `\textit{Can we efficiently generate synthetic data for any dialogue
schema to enable few-shot prompting?}' Addressing this question, we propose
\method, a data generation framework tailored for DST, utilizing LLMs. Our
approach only requires the dialogue schema and a few hand-crafted dialogue
templates to synthesize natural, coherent, and free-flowing dialogues with DST
annotations. Few-shot learning using data from {\method} results in $4-5%$
improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1
and 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of
the performance compared to the few-shot setup using human-annotated training
data. Our synthetic data and code can be accessed at
https://github.com/apple/ml-synthdst
\\ ( https://arxiv.org/abs/2402.02285 ,  4155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02289
Date: Sat, 3 Feb 2024 23:03:51 GMT   (368kb,D)

Title: SemPool: Simple, robust, and interpretable KG pooling for enhancing
  language models
Authors: Costas Mavromatis, Petros Karypis, George Karypis
Categories: cs.CL
\\
  Knowledge Graph (KG) powered question answering (QA) performs complex
reasoning over language semantics as well as knowledge facts. Graph Neural
Networks (GNNs) learn to aggregate information from the underlying KG, which is
combined with Language Models (LMs) for effective reasoning with the given
question. However, GNN-based methods for QA rely on the graph information of
the candidate answer nodes, which limits their effectiveness in more
challenging settings where critical answer information is not included in the
KG. We propose a simple graph pooling approach that learns useful semantics of
the KG that can aid the LM's reasoning and that its effectiveness is robust
under graph perturbations. Our method, termed SemPool, represents KG facts with
pre-trained LMs, learns to aggregate their semantic information, and fuses it
at different layers of the LM. Our experimental results show that SemPool
outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on
average when answer information is missing from the KG. In addition, SemPool
offers interpretability on what type of graph information is fused at different
LM layers.
\\ ( https://arxiv.org/abs/2402.02289 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02315
Date: Sun, 4 Feb 2024 02:06:57 GMT   (1444kb,D)

Title: A Survey of Large Language Models in Finance (FinLLMs)
Authors: Jean Lee, Nicholas Stevens, Soyeon Caren Han, Minseok Song
Categories: cs.CL q-fin.GN
Comments: More information on https://github.com/adlnlp/FinLLMs
\\
  Large Language Models (LLMs) have shown remarkable capabilities across a wide
variety of Natural Language Processing (NLP) tasks and have attracted attention
from multiple domains, including financial services. Despite the extensive
research into general-domain LLMs, and their immense potential in finance,
Financial LLM (FinLLM) research remains limited. This survey provides a
comprehensive overview of FinLLMs, including their history, techniques,
performance, and opportunities and challenges. Firstly, we present a
chronological overview of general-domain Pre-trained Language Models (PLMs)
through to current FinLLMs, including the GPT-series, selected open-source
LLMs, and financial LMs. Secondly, we compare five techniques used across
financial PLMs and FinLLMs, including training methods, training data, and
fine-tuning methods. Thirdly, we summarize the performance evaluations of six
benchmark tasks and datasets. In addition, we provide eight advanced financial
NLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we
discuss the opportunities and the challenges facing FinLLMs, such as
hallucination, privacy, and efficiency. To support AI research in finance, we
compile a collection of accessible datasets and evaluation benchmarks on
GitHub.
\\ ( https://arxiv.org/abs/2402.02315 ,  1444kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02379
Date: Sun, 4 Feb 2024 07:33:45 GMT   (7771kb,D)

Title: Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an
  Entity-Centric Perspective
Authors: Chong Zhang, Yixi Zhao, Chenshu Yuan, Yi Tu, Ya Guo, Qi Zhang
Categories: cs.CL
\\
  Recently developed pre-trained text-and-layout models (PTLMs) have shown
remarkable success in multiple information extraction tasks on visually-rich
documents. However, the prevailing evaluation pipeline may not be sufficiently
robust for assessing the information extraction ability of PTLMs, due to
inadequate annotations within the benchmarks. Therefore, we claim the necessary
standards for an ideal benchmark to evaluate the information extraction ability
of PTLMs. We then introduce EC-FUNSD, an entity-centric benckmark designed for
the evaluation of semantic entity recognition and entity linking on
visually-rich documents. This dataset contains diverse formats of document
layouts and annotations of semantic-driven entities and their relations.
Moreover, this dataset disentangles the falsely coupled annotation of segment
and entity that arises from the block-level annotation of FUNSD. Experiment
results demonstrate that state-of-the-art PTLMs exhibit overfitting tendencies
on the prevailing benchmarks, as their performance sharply decrease when the
dataset bias is removed.
\\ ( https://arxiv.org/abs/2402.02379 ,  7771kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02380
Date: Sun, 4 Feb 2024 07:39:06 GMT   (1048kb)

Title: Evaluating Large Language Models in Analysing Classroom Dialogue
Authors: Yun Long, Haifeng Luo, Yu Zhang
Categories: cs.CL cs.AI cs.HC
\\
  This study explores the application of Large Language Models (LLMs),
specifically GPT-4, in the analysis of classroom dialogue, a crucial research
task for both teaching diagnosis and quality improvement. Recognizing the
knowledge-intensive and labor-intensive nature of traditional qualitative
methods in educational research, this study investigates the potential of LLM
to streamline and enhance the analysis process. The study involves datasets
from a middle school, encompassing classroom dialogues across mathematics and
Chinese classes. These dialogues were manually coded by educational experts and
then analyzed using a customised GPT-4 model. This study focuses on comparing
manual annotations with the outputs of GPT-4 to evaluate its efficacy in
analyzing educational dialogues. Time efficiency, inter-coder agreement, and
inter-coder reliability between human coders and GPT-4 are evaluated. Results
indicate substantial time savings with GPT-4, and a high degree of consistency
in coding between the model and human coders, with some discrepancies in
specific codes. These findings highlight the strong potential of LLM in
teaching evaluation and facilitation.
\\ ( https://arxiv.org/abs/2402.02380 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02388
Date: Sun, 4 Feb 2024 07:59:06 GMT   (341kb,D)

Title: Solution-oriented Agent-based Models Generation with Verifier-assisted
  Iterative In-context Learning
Authors: Tong Niu, Weihao Zhang, Rong Zhao
Categories: cs.CL cs.AI cs.LG cs.SE
\\
  Agent-based models (ABMs) stand as an essential paradigm for proposing and
validating hypothetical solutions or policies aimed at addressing challenges
posed by complex systems and achieving various objectives. This process demands
labor-intensive endeavors and multidisciplinary expertise. Large language
models (LLMs) encapsulating cross-domain knowledge and programming proficiency
could potentially alleviate the difficulty of this process. However, LLMs excel
in handling sequential information, making it challenging for analyzing the
intricate interactions and nonlinear dynamics inherent in ABMs. Additionally,
due to the lack of self-evaluation capability of LLMs, relying solely on LLMs
is insufficient to effectively accomplish this process. In this paper, we
present SAGE, a general solution-oriented ABM generation framework designed for
automatic modeling and generating solutions for targeted problems. Unlike
approaches reliant on expert handcrafting or resource-intensive neural network
training, SAGE establishes a verifier-assisted iterative in-context learning
process employing large language models (LLMs) to leverages their inherent
cross-domain knowledge for tackling intricate demands from diverse domain
scenarios. In SAGE, we introduce an semi-structured conceptual representation
expliciting the intricate structures of ABMs and an objective representation to
guide LLMs in modeling scenarios and proposing hypothetical solutions through
in-context learning. To ensure the model executability and solution
feasibility, SAGE devises a two-level verifier with chain-of-thought prompting
tailored to the complex interactions and non-linear dynamics of ABMs, driving
the iterative generation optimization. Moreover, we construct an evaluation
dataset of solution-oriented ABMs from open sources.It contains practical
models across various domains.
\\ ( https://arxiv.org/abs/2402.02388 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02389
Date: Sun, 4 Feb 2024 08:01:07 GMT   (1374kb,D)

Title: KICGPT: Large Language Model with Knowledge in Context for Knowledge
  Graph Completion
Authors: Yanbin Wei, Qiushi Huang, James T. Kwok, Yu Zhang
Categories: cs.CL
Comments: Accepted to EMNLP 2023 Findings
DOI: 10.18653/v1/2023.findings-emnlp.580
\\
  Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph
incompleteness and supporting downstream applications. Many models have been
proposed for KGC. They can be categorized into two main classes: triple-based
and text-based approaches. Triple-based methods struggle with long-tail
entities due to limited structural information and imbalanced entity
distributions. Text-based methods alleviate this issue but require costly
training for language models and specific finetuning for knowledge graphs,
which limits their efficiency. To alleviate these limitations, in this paper,
we propose KICGPT, a framework that integrates a large language model (LLM) and
a triple-based KGC retriever. It alleviates the long-tail problem without
incurring additional training overhead. KICGPT uses an in-context learning
strategy called Knowledge Prompt, which encodes structural knowledge into
demonstrations to guide the LLM. Empirical results on benchmark datasets
demonstrate the effectiveness of KICGPT with smaller training overhead and no
finetuning.
\\ ( https://arxiv.org/abs/2402.02389 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02408
Date: Sun, 4 Feb 2024 08:57:54 GMT   (7712kb,D)

Title: GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large
  Language Model
Authors: Xuanchang Zhang, Zhuosheng Zhang, Hai Zhao
Categories: cs.CL cs.LG
\\
  Despite the rapid progress of large language models (LLMs), their task
performance remains sensitive to prompt design. Recent studies have explored
leveraging the LLM itself as an optimizer to identify optimal prompts that
maximize task accuracy. However, when evaluating prompts, such approaches
heavily rely on elusive manually annotated gold labels to calculate task
accuracy for each candidate prompt, which hinders the widespread implementation
and generality. To overcome the limitation, this work proposes a gold
label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold
labels. Motivated by the observed correlation between self-consistency and the
accuracy of the answer, we adopt self-consistency as the initial evaluation
score. Subsequently, we refine the scores of prompts producing identical
answers to be mutually consistent. Experimental results show that GLaPE
provides reliable evaluations uniform with accuracy, even in the absence of
gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt
optimization yields effective prompts comparable to accuracy-based ones. The
code is publicly available at https://github.com/thunderous77/GLaPE.
\\ ( https://arxiv.org/abs/2402.02408 ,  7712kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02416
Date: Sun, 4 Feb 2024 09:24:51 GMT   (2207kb,D)

Title: Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
Authors: Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang,
  Xuehai Pan, Juntao Dai, Yaodong Yang
Categories: cs.CL cs.AI cs.LG
Comments: 35 pages
\\
  Efforts to align Large Language Models (LLMs) are mainly conducted via
Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF
encounters major challenges including training reward models, actor-critic
engineering, and importantly, it requires access to LLM parameters. Here we
introduce Aligner, a new efficient alignment paradigm that bypasses the whole
RLHF process by learning the correctional residuals between the aligned and the
unaligned answers. Our Aligner offers several key advantages. Firstly, it is an
autoregressive seq2seq model that is trained on the query-answer-correction
dataset via supervised learning; this offers a parameter-efficient alignment
solution with minimal resources. Secondly, the Aligner facilitates
weak-to-strong generalization; finetuning large pretrained models by Aligner's
supervisory signals demonstrates strong performance boost. Thirdly, Aligner
functions as a model-agnostic plug-and-play module, allowing for its direct
application on different open-source and API-based models. Remarkably,
Aligner-7B improves 11 different LLMs by 18% in helpfulness and 23% in
harmlessness on average (GPT-4 by 26.9% and 17.5%). When finetuning (strong)
Llama2-70B with (weak) Aligner-7B's supervision, we can improve Llama2 by 8.2%
in helpfulness and 61.6% in harmlessness. See our dataset and code at
\url{https://aligner2024.github.io}.
\\ ( https://arxiv.org/abs/2402.02416 ,  2207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02420
Date: Sun, 4 Feb 2024 09:36:31 GMT   (124kb,D)

Title: Factuality of Large Language Models in the Year 2024
Authors: Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Georgi Georgiev,
  Rocktim Jyoti Das, Preslav Nakov
Categories: cs.CL
Comments: 9 pages, 1 figure and 2 tables
\\
  Large language models (LLMs), especially when instruction-tuned for chat,
have become part of our daily lives, freeing people from the process of
searching, extracting, and integrating information from multiple sources by
offering a straightforward answer to a variety of questions in a single place.
Unfortunately, in many cases, LLM responses are factually incorrect, which
limits their applicability in real-world scenarios. As a result, research on
evaluating and improving the factuality of LLMs has attracted a lot of research
attention recently. In this survey, we critically analyze existing work with
the aim to identify the major challenges and their associated causes, pointing
out to potential solutions for improving the factuality of LLMs, and analyzing
the obstacles to automated factuality evaluation for open-ended text
generation. We further offer an outlook on where future research should go.
\\ ( https://arxiv.org/abs/2402.02420 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02449
Date: Sun, 4 Feb 2024 11:38:12 GMT   (1556kb,D)

Title: Surfing the modeling of PoS taggers in low-resource scenarios
Authors: Manuel Vilares Ferro, V\'ictor M. Darriba Bilbao, Francisco J.
  Ribadas-Pena, Jorge Gra\~na Gil
Categories: cs.CL cs.LG
Comments: 17 papes, 5 figures
MSC-class: 68, 68T50
Journal-ref: Mathematics 2022, 10(19), 3526
DOI: 10.3390/math10193526
\\
  The recent trend towards the application of deep structured techniques has
revealed the limits of huge models in natural language processing. This has
reawakened the interest in traditional machine learning algorithms, which have
proved still to be competitive in certain contexts, in particular low-resource
settings. In parallel, model selection has become an essential task to boost
performance at reasonable cost, even more so when we talk about processes
involving domains where the training and/or computational resources are scarce.
Against this backdrop, we evaluate the early estimation of learning curves as a
practical mechanism for selecting the most appropriate model in scenarios
characterized by the use of non-deep learners in resource-lean settings. On the
basis of a formal approximation model previously evaluated under conditions of
wide availability of training and validation resources, we study the
reliability of such an approach in a different and much more demanding
operationalenvironment. Using as case study the generation of PoS taggers for
Galician, a language belonging to the Western Ibero-Romance group, the
experimental results are consistent with our expectations.
\\ ( https://arxiv.org/abs/2402.02449 ,  1556kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02515
Date: Sun, 4 Feb 2024 15:00:52 GMT   (397kb)

Title: Modeling of learning curves with applications to pos tagging
Authors: Manuel Vilares Ferro, Victor M. Darriba Bilbao, Francisco J. Ribadas
  Pena
Categories: cs.CL cs.AI cs.LG
Comments: 30 pages, 11 figures
Journal-ref: Manuel Vilares Ferro, Victor M. Darriba Bilbao, Francisco J.
  Ribadas Pena. Modeling of learning curves with applications to pos tagging.
  Computer Speech & Language, 41, pp 1-28 (2017). ISSN 0885-2308. Elsevier
DOI: 10.1016/j.csl.2016.06.001
\\
  An algorithm to estimate the evolution of learning curves on the whole of a
training data base, based on the results obtained from a portion and using a
functional strategy, is introduced. We approximate iteratively the sought value
at the desired time, independently of the learning technique used and once a
point in the process, called prediction level, has been passed. The proposal
proves to be formally correct with respect to our working hypotheses and
includes a reliable proximity condition. This allows the user to fix a
convergence threshold with respect to the accuracy finally achievable, which
extends the concept of stopping criterion and seems to be effective even in the
presence of distorting observations.
  Our aim is to evaluate the training effort, supporting decision making in
order to reduce the need for both human and computational resources during the
learning process. The proposal is of interest in at least three operational
procedures. The first is the anticipation of accuracy gain, with the purpose of
measuring how much work is needed to achieve a certain degree of performance.
The second relates the comparison of efficiency between systems at training
time, with the objective of completing this task only for the one that best
suits our requirements. The prediction of accuracy is also a valuable item of
information for customizing systems, since we can estimate in advance the
impact of settings on both the performance and the development costs. Using the
generation of part-of-speech taggers as an example application, the
experimental results are consistent with our expectations.
\\ ( https://arxiv.org/abs/2402.02515 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02516
Date: Sun, 4 Feb 2024 15:02:17 GMT   (590kb)

Title: Adaptive scheduling for adaptive sampling in POS taggers construction
Authors: Manuel Vilares Ferro, Victor M. Darriba Bilbao, Jes\'us Vilares Ferro
Categories: cs.CL cs.AI cs.LG
Comments: 23 pager, 10 figures
Journal-ref: Manuel Vilares Ferro, Victor M. Darriba Bilbao, Jesus Vilares
  Ferro. Adaptive scheduling for adaptive sampling in POS taggers construction.
  Computer Speech & Language, 60, 101020 (2020), pp 1-18. ISSN 0885-2308.
  Elsevier
DOI: 10.1016/j.csl.2019.101020
\\
  We introduce an adaptive scheduling for adaptive sampling as a novel way of
machine learning in the construction of part-of-speech taggers. The goal is to
speed up the training on large data sets, without significant loss of
performance with regard to an optimal configuration. In contrast to previous
methods using a random, fixed or regularly rising spacing between the
instances, ours analyzes the shape of the learning curve geometrically in
conjunction with a functional model to increase or decrease it at any time. The
algorithm proves to be formally correct regarding our working hypotheses.
Namely, given a case, the following one is the nearest ensuring a net gain of
learning ability from the former, it being possible to modulate the level of
requirement for this condition. We also improve the robustness of sampling by
paying greater attention to those regions of the training data base subject to
a temporary inflation in performance, thus preventing the learning from
stopping prematurely.
  The proposal has been evaluated on the basis of its reliability to identify
the convergence of models, corroborating our expectations. While a concrete
halting condition is used for testing, users can choose any condition
whatsoever to suit their own specific needs.
\\ ( https://arxiv.org/abs/2402.02516 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02522
Date: Sun, 4 Feb 2024 15:10:34 GMT   (688kb)

Title: Absolute convergence and error thresholds in non-active adaptive
  sampling
Authors: Manuel Vilares Ferro, Victor M. Darriba Bilbao, Jes\'us Vilares Ferro
Categories: cs.CL cs.AI cs.LG
Comments: 27 pages, 10 figures
Journal-ref: Manuel Vilares Ferro a, Victor M. Darriba Bilbao, Jesus Vilares
  Ferro. Absolute convergence and error thresholds in non-active adaptive
  sampling. Journal of Computer and System Sciences, 129 (2020) , pp 39-61.
  ISSN 1090-2724. Elsevier
DOI: 10.1016/j.jcss.2022.05.002
\\
  Non-active adaptive sampling is a way of building machine learning models
from a training data base which are supposed to dynamically and automatically
derive guaranteed sample size. In this context and regardless of the strategy
used in both scheduling and generating of weak predictors, a proposal for
calculating absolute convergence and error thresholds is described. We not only
make it possible to establish when the quality of the model no longer
increases, but also supplies a proximity condition to estimate in absolute
terms how close it is to achieving such a goal, thus supporting decision making
for fine-tuning learning parameters in model selection. The technique proves
its correctness and completeness with respect to our working hypotheses, in
addition to strengthening the robustness of the sampling scheme. Tests meet our
expectations and illustrate the proposal in the domain of natural language
processing, taking the generation of part-of-speech taggers as case study.
\\ ( https://arxiv.org/abs/2402.02522 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02541
Date: Sun, 4 Feb 2024 15:41:35 GMT   (1052kb,D)

Title: Knowledge Generation for Zero-shot Knowledge-based VQA
Authors: Rui Cao and Jing Jiang
Categories: cs.CL cs.CV
Comments: accepted as Findings in EACL 2023;
\\
  Previous solutions to knowledge-based visual question answering~(K-VQA)
retrieve knowledge from external knowledge bases and use supervised learning to
train the K-VQA model. Recently pre-trained LLMs have been used as both a
knowledge source and a zero-shot QA model for K-VQA and demonstrated promising
results. However, these recent methods do not explicitly show the knowledge
needed to answer the questions and thus lack interpretability. Inspired by
recent work on knowledge generation from LLMs for text-based QA, in this work
we propose and test a similar knowledge-generation-based K-VQA method, which
first generates knowledge from an LLM and then incorporates the generated
knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA
benchmarks and found that our method performs better than previous zero-shot
K-VQA methods and our generated knowledge is generally relevant and helpful.
\\ ( https://arxiv.org/abs/2402.02541 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02548
Date: Sun, 4 Feb 2024 15:52:46 GMT   (5862kb,D)

Title: "What's my model inside of?": Exploring the role of environments for
  grounded natural language understanding
Authors: Ronen Tamari
Categories: cs.CL cs.AI cs.SI
Comments: PhD Thesis
\\
  In contrast to classical cognitive science which studied brains in isolation,
ecological approaches focused on the role of the body and environment in
shaping cognition. Similarly, in this thesis we adopt an ecological approach to
grounded natural language understanding (NLU) research. Grounded language
understanding studies language understanding systems situated in the context of
events, actions and precepts in naturalistic/simulated virtual environments.
Where classic research tends to focus on designing new models and optimization
methods while treating environments as given, we explore the potential of
environment design for improving data collection and model development. We
developed novel training and annotation approaches for procedural text
understanding based on text-based game environments. We also drew upon embodied
cognitive linguistics literature to propose a roadmap for grounded NLP
research, and to inform the development of a new benchmark for measuring the
progress of large language models on challenging commonsense reasoning tasks.
We leveraged the richer supervision provided by text-based game environments to
develop Breakpoint Transformers, a novel approach to modeling intermediate
semantic information in long narrative or procedural texts. Finally, we
integrated theories on the role of environments in collective human
intelligence to propose a design for AI-augmented "social thinking
environments" for knowledge workers like scientists.
\\ ( https://arxiv.org/abs/2402.02548 ,  5862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02549
Date: Sun, 4 Feb 2024 15:52:59 GMT   (6387kb,D)

Title: Are Large Language Models Table-based Fact-Checkers?
Authors: Hangwen Zhang, Qingyi Si, Peng Fu, Zheng Lin, Weiping Wang
Categories: cs.CL cs.LG
Comments: CSCWD 2024
\\
  Table-based Fact Verification (TFV) aims to extract the entailment relation
between statements and structured tables. Existing TFV methods based on
small-scaled models suffer from insufficient labeled data and weak zero-shot
ability. Recently, the appearance of Large Language Models (LLMs) has gained
lots of attraction in research fields. They have shown powerful zero-shot and
in-context learning abilities on several NLP tasks, but their potential on TFV
is still unknown. In this work, we implement a preliminary study about whether
LLMs are table-based fact-checkers. In detail, we design diverse prompts to
explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and
few-shot TFV capability. Besides, we carefully design and construct TFV
instructions to study the performance gain brought by the instruction tuning of
LLMs. Experimental results demonstrate that LLMs can achieve acceptable results
on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning
can stimulate the TFV capability significantly. We also make some valuable
findings about the format of zero-shot prompts and the number of in-context
examples. Finally, we analyze some possible directions to promote the accuracy
of TFV via LLMs, which is beneficial to further research of table reasoning.
\\ ( https://arxiv.org/abs/2402.02549 ,  6387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02558
Date: Sun, 4 Feb 2024 16:18:01 GMT   (248kb,D)

Title: Enhancing Robustness in Biomedical NLI Models: A Probing Approach for
  Clinical Trials
Authors: Ata Mustafa
Categories: cs.CL cs.LG
\\
  Large Language Models have revolutionized various fields and industries, such
as Conversational AI, Content Generation, Information Retrieval, Business
Intelligence, and Medical, to name a few. One major application in the field of
medical is to analyze and investigate clinical trials for entailment
tasks.However, It has been observed that Large Language Models are susceptible
to shortcut learning, factual inconsistency, and performance degradation with
little variation in context. Adversarial and robust testing is performed to
ensure the integrity of models output. But, ambiguity still persists. In order
to ensure the integrity of the reasoning performed and investigate the model
has correct syntactic and semantic understanding probing is used. Here, I used
mnestic probing to investigate the Sci-five model, trained on clinical trial. I
investigated the model for feature learnt with respect to natural logic. To
achieve the target, I trained task specific probes. Used these probes to
investigate the final layers of trained model. Then, fine tuned the trained
model using iterative null projection. The results shows that model accuracy
improved. During experimentation, I observed that size of the probe has affect
on the fine tuning process.
\\ ( https://arxiv.org/abs/2402.02558 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02559
Date: Sun, 4 Feb 2024 16:23:16 GMT   (44615kb,D)

Title: NavHint: Vision and Language Navigation Agent with a Hint Generator
Authors: Yue Zhang, Quan Guo, Parisa Kordjamshidi
Categories: cs.CL
\\
  Existing work on vision and language navigation mainly relies on
navigation-related losses to establish the connection between vision and
language modalities, neglecting aspects of helping the navigation agent build a
deep understanding of the visual environment. In our work, we provide indirect
supervision to the navigation agent through a hint generator that provides
detailed visual descriptions. The hint generator assists the navigation agent
in developing a global understanding of the visual environment. It directs the
agent's attention toward related navigation details, including the relevant
sub-instruction, potential challenges in recognition and ambiguities in
grounding, and the targeted viewpoint description. To train the hint generator,
we construct a synthetic dataset based on landmarks in the instructions and
visible and distinctive objects in the visual environment. We evaluate our
method on the R2R and R4R datasets and achieve state-of-the-art on several
metrics. The experimental results demonstrate that generating hints not only
enhances the navigation performance but also helps improve the interpretability
of the agent's actions.
\\ ( https://arxiv.org/abs/2402.02559 ,  44615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02563
Date: Sun, 4 Feb 2024 16:45:01 GMT   (460kb,D)

Title: DefInt: A Default-interventionist Framework for Efficient Reasoning with
  Hybrid Large Language Models
Authors: Yu Shang, Yu Li, Fengli Xu, Yong Li
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages, 10 figures, 14 tables
\\
  Large language models (LLMs) have shown impressive emergent abilities in a
wide range of tasks, but still face challenges in handling complex reasoning
problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT)
have predominately focused on enhancing accuracy, but overlook the rapidly
increasing token cost, which could be particularly problematic for open-ended
real-world tasks with huge solution spaces. Motivated by the dual process
theory of human cognition, we propose a Default-Interventionist framework
(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,
DefInt uses smaller-scale language models to generate low-cost reasoning
thoughts, which resembles the fast intuitions produced by System 1. If the
intuitions are considered with low confidence, DefInt will invoke the
reflective reasoning of scaled-up language models as the intervention of System
2, which can override the default thoughts and rectify the reasoning process.
Experiments on five representative reasoning tasks show that DefInt
consistently achieves state-of-the-art reasoning accuracy and solution
diversity. More importantly, it substantially reduces the token cost by 49%-79%
compared to the second accurate baselines. Specifically, the open-ended tasks
have an average 75% token cost reduction. Code repo with all prompts will be
released upon publication.
\\ ( https://arxiv.org/abs/2402.02563 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02564
Date: Sun, 4 Feb 2024 16:56:08 GMT   (440kb,D)

Title: A Truly Joint Neural Architecture for Segmentation and Parsing
Authors: Danit Yshaayahu Levi and Reut Tsarfaty
Categories: cs.CL
\\
  Contemporary multilingual dependency parsers can parse a diverse set of
languages, but for Morphologically Rich Languages (MRLs), performance is
attested to be lower than other languages. The key challenge is that, due to
high morphological complexity and ambiguity of the space-delimited input
tokens, the linguistic units that act as nodes in the tree are not known in
advance. Pre-neural dependency parsers for MRLs subscribed to the joint
morpho-syntactic hypothesis, stating that morphological segmentation and
syntactic parsing should be solved jointly, rather than as a pipeline where
segmentation precedes parsing. However, neural state-of-the-art parsers to date
use a strict pipeline. In this paper we introduce a joint neural architecture
where a lattice-based representation preserving all morphological ambiguity of
the input is provided to an arc-factored model, which then solves the
morphological segmentation and syntactic parsing tasks at once. Our experiments
on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art
performance on parsing, tagging and segmentation of the Hebrew section of UD,
using a single model. This proposed architecture is LLM-based and language
agnostic, providing a solid foundation for MRLs to obtain further performance
improvements and bridge the gap with other languages.
\\ ( https://arxiv.org/abs/2402.02564 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02572
Date: Sun, 4 Feb 2024 17:32:52 GMT   (3227kb,D)

Title: A Quantitative Discourse Analysis of Asian Workers in the US Historical
  Newspapers
Authors: Jaihyun Park, Ryan Cordell
Categories: cs.CL
Comments: 3rd International Conference on Natural Language Processing for
  Digital Humanities (NLP4DH)
\\
  Warning: This paper contains examples of offensive language targetting
marginalized population. The digitization of historical texts invites
researchers to explore the large-scale corpus of historical texts with
computational methods. In this study, we present computational text analysis on
a relatively understudied topic of how Asian workers are represented in
historical newspapers in the United States. We found that the word "coolie" was
semantically different in some States (e.g., Massachusetts, Rhode Island,
Wyoming, Oklahoma, and Arkansas) with the different discourses around coolie.
We also found that then-Confederate newspapers and then-Union newspapers formed
distinctive discourses by measuring over-represented words. Newspapers from
then-Confederate States associated coolie with slavery-related words. In
addition, we found Asians were perceived to be inferior to European immigrants
and subjected to the target of racism. This study contributes to supplementing
the qualitative analysis of racism in the United States with quantitative
discourse analysis.
\\ ( https://arxiv.org/abs/2402.02572 ,  3227kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02591
Date: Sun, 4 Feb 2024 19:54:44 GMT   (30kb)

Title: On the performance of phonetic algorithms in microtext normalization
Authors: Yerai Doval, Manuel Vilares, Jes\'us Vilares
Categories: cs.CL
Comments: Accepted for publication in journal Expert Systems with Applications
Journal-ref: Expert Systems with Applications, Volume 113, 2018, Pages 213-222
DOI: 10.1016/j.eswa.2018.07.016
\\
  User-generated content published on microblogging social networks constitutes
a priceless source of information. However, microtexts usually deviate from the
standard lexical and grammatical rules of the language, thus making its
processing by traditional intelligent systems very difficult. As an answer,
microtext normalization consists in transforming those non-standard microtexts
into standard well-written texts as a preprocessing step, allowing traditional
approaches to continue with their usual processing. Given the importance of
phonetic phenomena in non-standard text formation, an essential element of the
knowledge base of a normalizer would be the phonetic rules that encode these
phenomena, which can be found in the so-called phonetic algorithms.
  In this work we experiment with a wide range of phonetic algorithms for the
English language. The aim of this study is to determine the best phonetic
algorithms within the context of candidate generation for microtext
normalization. In other words, we intend to find those algorithms that taking
as input non-standard terms to be normalized allow us to obtain as output the
smallest possible sets of normalization candidates which still contain the
corresponding target standard words. As it will be stated, the choice of the
phonetic algorithm will depend heavily on the capabilities of the candidate
selection mechanism which we usually find at the end of a microtext
normalization pipeline. The faster it can make the right choices among big
enough sets of candidates, the more we can sacrifice on the precision of the
phonetic algorithms in favour of coverage in order to increase the overall
performance of the normalization system.
  KEYWORDS: microtext normalization; phonetic algorithm; fuzzy matching;
Twitter; texting
\\ ( https://arxiv.org/abs/2402.02591 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02617
Date: Sun, 4 Feb 2024 21:24:54 GMT   (302kb,D)

Title: Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study
  on Speech Emotion Recognition
Authors: Alexandra Saliba, Yuanchao Li, Ramon Sanabria, Catherine Lai
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to ICASSP2024 Self-supervision in Audio, Speech and Beyond
  (SASB) workshop. First two authors contributed equally
\\
  The efficacy of self-supervised speech models has been validated, yet the
optimal utilization of their representations remains challenging across diverse
tasks. In this study, we delve into Acoustic Word Embeddings (AWEs), a
fixed-length feature derived from continuous representations, to explore their
advantages in specific tasks. AWEs have previously shown utility in capturing
acoustic discriminability. In light of this, we propose measuring layer-wise
similarity between AWEs and word embeddings, aiming to further investigate the
inherent context within AWEs. Moreover, we evaluate the contribution of AWEs,
in comparison to other types of speech features, in the context of Speech
Emotion Recognition (SER). Through a comparative experiment and a layer-wise
accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore
differences between AWEs and raw self-supervised representations, as well as
the proper utilization of AWEs alone and in combination with word embeddings.
Our findings underscore the acoustic context conveyed by AWEs and showcase the
highly competitive SER accuracies by appropriately employing AWEs.
\\ ( https://arxiv.org/abs/2402.02617 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02622
Date: Sun, 4 Feb 2024 21:44:09 GMT   (1154kb,D)

Title: DenseFormer: Enhancing Information Flow in Transformers via Depth
  Weighted Averaging
Authors: Matteo Pagliardini, Amirkeivan Mohtashami, Francois Fleuret, Martin
  Jaggi
Categories: cs.CL cs.LG
\\
  The transformer architecture from Vaswani et al. (2017) is now ubiquitous
across application domains, from natural language processing to speech
processing and image understanding. We propose DenseFormer, a simple
modification to the standard architecture that improves the perplexity of the
model without increasing its size -- adding a few thousand parameters for
large-scale models in the 100B parameters range. Our approach relies on an
additional averaging step after each transformer block, which computes a
weighted average of current and past representations -- we refer to this
operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit
coherent patterns of information flow, revealing the strong and structured
reuse of activations from distant layers. Experiments demonstrate that
DenseFormer is more data efficient, reaching the same perplexity of much deeper
transformer models, and that for the same perplexity, these new models
outperform transformer baselines in terms of memory efficiency and inference
time.
\\ ( https://arxiv.org/abs/2402.02622 ,  1154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02633
Date: Sun, 4 Feb 2024 22:56:56 GMT   (9163kb,D)

Title: Predicting Machine Translation Performance on Low-Resource Languages:
  The Role of Domain Similarity
Authors: Eric Khiu, Hasti Toossi, David Anugraha, Jinyu Liu, Jiaxu Li, Juan
  Armando Parra Flores, Leandro Acros Roman, A. Seza Do\u{g}ru\"oz, En-Shiun
  Annie Lee
Categories: cs.CL cs.LG
Comments: 13 pages, 5 figures, accepted to EACL 2024, findings
\\
  Fine-tuning and testing a multilingual large language model is expensive and
challenging for low-resource languages (LRLs). While previous studies have
predicted the performance of natural language processing (NLP) tasks using
machine learning methods, they primarily focus on high-resource languages,
overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate
three factors: the size of the fine-tuning corpus, the domain similarity
between fine-tuning and testing corpora, and the language similarity between
source and target languages. We employ classical regression models to assess
how these factors impact the model's performance. Our results indicate that
domain similarity has the most critical impact on predicting the performance of
Machine Translation models.
\\ ( https://arxiv.org/abs/2402.02633 ,  9163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02636
Date: Sun, 4 Feb 2024 23:04:02 GMT   (5101kb,D)

Title: Can Large Language Models Learn Independent Causal Mechanisms?
Authors: Ga\"el Gendron, Bao Trung Nguyen, Alex Yuxuan Peng, Michael Witbrock,
  Gillian Dobbie
Categories: cs.CL cs.IT cs.LG math.IT
Comments: 17 pages, 8 pages for the main paper and 9 pages for references and
  appendices, 12 figures
ACM-class: I.2.3; I.2.6; I.2.7; G.3
\\
  Despite impressive performance on language modelling and complex reasoning
tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon
settings or with distribution shifts, exhibiting some lack of generalisation
ability. This issue has usually been alleviated by feeding more training data
into the LLM. However, this method is brittle, as the scope of tasks may not be
readily predictable or may evolve, and updating the model with new data
generally requires extensive additional training. By contrast, systems, such as
causal models, that learn abstract variables and causal relationships can
demonstrate increased robustness against changes in the distribution. One
reason for this success is the existence and use of Independent Causal
Mechanisms (ICMs) representing high-level concepts that only sparsely interact.
In this work, we apply two concepts from causality to learn ICMs within LLMs.
We develop a new LLM architecture composed of multiple sparsely interacting
language modelling modules. We introduce a routing scheme to induce
specialisation of the network into domain-specific modules. We also present a
Mutual Information minimisation objective that trains a separate module to
learn abstraction and domain-invariant mechanisms. We show that such causal
constraints can improve out-of-distribution performance on abstract and causal
reasoning tasks.
\\ ( https://arxiv.org/abs/2402.02636 ,  5101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02639
Date: Sun, 4 Feb 2024 23:23:51 GMT   (8040kb,D)

Title: It's how you do things that matters": Attending to Process to Better
  Serve Indigenous Communities with Language Technologies
Authors: Ned Cooper, Courtney Heldreth, Ben Hutchinson
Categories: cs.CL
Journal-ref: Proceedings of the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024)
\\
  Indigenous languages are historically under-served by Natural Language
Processing (NLP) technologies, but this is changing for some languages with the
recent scaling of large multilingual models and an increased focus by the NLP
community on endangered languages. This position paper explores ethical
considerations in building NLP technologies for Indigenous languages, based on
the premise that such projects should primarily serve Indigenous communities.
We report on interviews with 17 researchers working in or with Aboriginal
and/or Torres Strait Islander communities on language technology projects in
Australia. Drawing on insights from the interviews, we recommend practices for
NLP researchers to increase attention to the process of engagements with
Indigenous communities, rather than focusing only on decontextualised
artefacts.
\\ ( https://arxiv.org/abs/2402.02639 ,  8040kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02648
Date: Mon, 5 Feb 2024 00:44:28 GMT   (253kb,D)

Title: Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses
Authors: Jinwoo Ahn
Categories: cs.CL cs.AI
Comments: Still Ongoing Work
\\
  Large Language Models (LLMs) frequently suffer from knowledge-intensive
questions, often being inconsistent by providing different outputs despite
given the same input. The response quality worsens when the user expresses a
firm opposing stance which causes the LLMs to adjust its response despite the
correct initial one. These behaviors decrease the reliability and validity of
the responses provided by these models. In this paper, we attempt to 1) raise
awareness of the inherent risks that follow from overly relying on AI agents
like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate
more from the actual answer and 2) suggest a novel prompting method, Recursive
Chain of Feedback (R-CoF), that we are conducting further study. The CoF system
takes in an open-ended multi-step question. Then, we repetitively provide
meaningless feedback requesting another attempt. Our preliminary experiments
show that such feedback only decreases the quality of the response. On the
other hand, to mitigate the effects of the aforementioned inconsistencies, we
present a novel method of recursively revising the initial incorrect reasoning
provided by the LLM by repetitively breaking down each incorrect step into
smaller individual problems.
\\ ( https://arxiv.org/abs/2402.02648 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02655
Date: Mon, 5 Feb 2024 00:54:40 GMT   (8403kb,D)

Title: VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based
  Machine Reading Comprehension
Authors: Thinh Phuoc Ngo, Khoa Tran Anh Dang, Son T. Luu, Kiet Van Nguyen, Ngan
  Luu-Thuy Nguyen
Categories: cs.CL
Comments: Accepted as main conference paper at EACL 2024
\\
  This paper presents the development process of a Vietnamese spoken language
corpus for machine reading comprehension (MRC) tasks and provides insights into
the challenges and opportunities associated with using real-world data for
machine reading comprehension tasks. The existing MRC corpora in Vietnamese
mainly focus on formal written documents such as Wikipedia articles, online
newspapers, or textbooks. In contrast, the VlogQA consists of 10,076
question-answer pairs based on 1,230 transcript documents sourced from YouTube
-- an extensive source of user-uploaded content, covering the topics of food
and travel. By capturing the spoken language of native Vietnamese speakers in
natural settings, an obscure corner overlooked in Vietnamese research, the
corpus provides a valuable resource for future research in reading
comprehension tasks for the Vietnamese language. Regarding performance
evaluation, our deep-learning models achieved the highest F1 score of 75.34% on
the test set, indicating significant progress in machine reading comprehension
for Vietnamese spoken language data. In terms of EM, the highest score we
accomplished is 53.97%, which reflects the challenge in processing spoken-based
content and highlights the need for further improvement.
\\ ( https://arxiv.org/abs/2402.02655 ,  8403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02656
Date: Mon, 5 Feb 2024 00:56:30 GMT   (1094kb,D)

Title: RACER: An LLM-powered Methodology for Scalable Analysis of
  Semi-structured Mental Health Interviews
Authors: Satpreet Harcharan Singh, Kevin Jiang, Kanchan Bhasin, Ashutosh
  Sabharwal, Nidal Moukaddam, Ankit B Patel
Categories: cs.CL q-bio.QM
\\
  Semi-structured interviews (SSIs) are a commonly employed data-collection
method in healthcare research, offering in-depth qualitative insights into
subject experiences. Despite their value, the manual analysis of SSIs is
notoriously time-consuming and labor-intensive, in part due to the difficulty
of extracting and categorizing emotional responses, and challenges in scaling
human evaluation for large populations. In this study, we develop RACER, a
Large Language Model (LLM) based expert-guided automated pipeline that
efficiently converts raw interview transcripts into insightful domain-relevant
themes and sub-themes. We used RACER to analyze SSIs conducted with 93
healthcare professionals and trainees to assess the broad personal and
professional mental health impacts of the COVID-19 crisis. RACER achieves
moderately high agreement with two human evaluators (72%), which approaches the
human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with
similar content involving nuanced emotional, ambivalent/dialectical, and
psychological statements. Our study highlights the opportunities and challenges
in using LLMs to improve research efficiency and opens new avenues for scalable
analysis of SSIs in healthcare research.
\\ ( https://arxiv.org/abs/2402.02656 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02680
Date: Mon, 5 Feb 2024 02:32:09 GMT   (5628kb,D)

Title: Large Language Models are Geographically Biased
Authors: Rohin Manvi, Samar Khanna, Marshall Burke, David Lobell, Stefano Ermon
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) inherently carry the biases contained in their
training corpora, which can lead to the perpetuation of societal harm. As the
impact of these foundation models grows, understanding and evaluating their
biases becomes crucial to achieving fairness and accuracy. We propose to study
what LLMs know about the world we live in through the lens of geography. This
approach is particularly powerful as there is ground truth for the numerous
aspects of human life that are meaningfully projected onto geographic space
such as culture, race, language, politics, and religion. We show various
problematic geographic biases, which we define as systemic errors in geospatial
predictions. Initially, we demonstrate that LLMs are capable of making accurate
zero-shot geospatial predictions in the form of ratings that show strong
monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We
then show that LLMs exhibit common biases across a range of objective and
subjective topics. In particular, LLMs are clearly biased against locations
with lower socioeconomic conditions (e.g. most of Africa) on a variety of
sensitive subjective topics such as attractiveness, morality, and intelligence
(Spearman's $\rho$ of up to 0.70). Finally, we introduce a bias score to
quantify this and find that there is significant variation in the magnitude of
bias across existing LLMs.
\\ ( https://arxiv.org/abs/2402.02680 ,  5628kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02695
Date: Mon, 5 Feb 2024 03:15:26 GMT   (3538kb,D)

Title: Exploiting Class Probabilities for Black-box Sentence-level Attacks
Authors: Raha Moraffah and Huan Liu
Categories: cs.CL cs.AI cs.LG
Comments: EACL 2024 Findings
\\
  Sentence-level attacks craft adversarial sentences that are synonymous with
correctly-classified sentences but are misclassified by the text classifiers.
Under the black-box setting, classifiers are only accessible through their
feedback to queried inputs, which is predominately available in the form of
class probabilities. Even though utilizing class probabilities results in
stronger attacks, due to the challenges of using them for sentence-level
attacks, existing attacks use either no feedback or only the class labels.
Overcoming the challenges, we develop a novel algorithm that uses class
probabilities for black-box sentence-level attacks, investigate the
effectiveness of using class probabilities on the attack's success, and examine
the question if it is worthy or practical to use class probabilities by
black-box sentence-level attacks. We conduct extensive evaluations of the
proposed attack comparing with the baselines across various classifiers and
benchmark datasets.
\\ ( https://arxiv.org/abs/2402.02695 ,  3538kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02750
Date: Mon, 5 Feb 2024 06:06:47 GMT   (3109kb,D)

Title: KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
Authors: Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu,
  Vladimir Braverman, Beidi Chen, Xia Hu
Categories: cs.CL cs.LG cs.PF
DOI: 10.13140/RG.2.2.28167.37282
\\
  Efficiently serving large language models (LLMs) requires batching many
requests together to reduce the cost per request. Yet, the key-value (KV)
cache, which stores attention keys and values to avoid re-computations,
significantly increases memory demands and becomes the new bottleneck in speed
and memory usage. This memory demand increases with larger batch sizes and
longer context lengths. Additionally, the inference speed is limited by the
size of KV cache, as the GPU's SRAM must load the entire KV cache from the main
GPU memory for each token generated, causing the computational core to be idle
during this process. A straightforward and effective solution to reduce KV
cache size is quantization, which decreases the total bytes taken by KV cache.
However, there is a lack of in-depth studies that explore the element
distribution of KV cache to understand the hardness and limitation of KV cache
quantization. To fill the gap, we conducted a comprehensive study on the
element distribution in KV cache of popular LLMs. Our findings indicate that
the key cache should be quantized per-channel, i.e., group elements along the
channel dimension and quantize them together. In contrast, the value cache
should be quantized per-token. From this analysis, we developed a tuning-free
2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly
implementation, KIVI can enable Llama (Llama-2), Falcon, and Mistral models to
maintain almost the same quality while using $\mathbf{2.6\times}$ less peak
memory usage (including the model weight). This reduction in memory usage
enables up to $\mathbf{4\times}$ larger batch size, bringing
$\mathbf{2.35\times \sim 3.47\times}$ throughput on real LLM inference
workload. The source code is available at https://github.com/jy-yuan/KIVI.
\\ ( https://arxiv.org/abs/2402.02750 ,  3109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02782
Date: Mon, 5 Feb 2024 07:33:25 GMT   (851kb,D)

Title: From Partial to Strictly Incremental Constituent Parsing
Authors: Ana Ezquerro, Carlos G\'omez-Rodr\'iguez, David Vilares
Categories: cs.CL
Comments: Accepted at EACL 2024
\\
  We study incremental constituent parsers to assess their capacity to output
trees based on prefix representations alone. Guided by strictly left-to-right
generative language models and tree-decoding modules, we build parsers that
adhere to a strong definition of incrementality across languages. This builds
upon work that asserted incrementality, but that mostly only enforced it on
either the encoder or the decoder. Finally, we conduct an analysis against
non-incremental and partially incremental models.
\\ ( https://arxiv.org/abs/2402.02782 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02791
Date: Mon, 5 Feb 2024 07:59:38 GMT   (350kb,D)

Title: Rethinking Optimization and Architecture for Tiny Language Models
Authors: Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai,
  Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe Wang
Categories: cs.CL cs.AI cs.LG
\\
  The power of large language models (LLMs) has been demonstrated through
numerous data and computing resources. However, the application of language
models on mobile devices is facing huge challenge on the computation and memory
costs, that is, tiny language models with high performance are urgently
required. Limited by the highly complex training process, there are many
details for optimizing language models that are seldom studied carefully. In
this study, based on a tiny language model with 1B parameters, we carefully
design a series of empirical study to analyze the effect of each component.
Three perspectives are mainly discussed, i.e., neural architecture, parameter
initialization, and optimization strategy. Several design formulas are
empirically proved especially effective for tiny language models, including
tokenizer compression, architecture tweaking, parameter inheritance and
multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B
Pro on 1.6T multilingual corpora, following the established formulas.
Experimental results demonstrate the improved optimization and architecture
yield a notable average improvement of 8.87 on benchmark evaluation sets for
PanGu-$\pi$-1B Pro. Besides, PanGu-$\pi$-1.5B Pro surpasses a range of SOTA
models with larger model sizes, validating its superior performance. The code
will be released soon (https://github.com/YuchuanTian/RethinkTinyLM).
\\ ( https://arxiv.org/abs/2402.02791 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02801
Date: Mon, 5 Feb 2024 08:19:56 GMT   (1844kb,D)

Title: KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language
  Models
Authors: Fei Yuan, Chang Ma, Shuai Yuan, Qiushi Sun, Lei Li
Categories: cs.CL cs.AI cs.LG
\\
  The lottery ticket hypothesis posits the existence of ``winning tickets''
within a randomly initialized neural network. Do winning tickets exist for LLMs
in fine-tuning scenarios? How can we find such winning tickets? In this paper,
we propose KS-Lottery, a method to identify a small subset of LLM parameters
highly effective in multilingual fine-tuning. Our key idea is to use
Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before
and after fine-tuning. We further theoretically prove that KS-Lottery can find
the certified winning tickets in the embedding layer, fine-tuning on the found
parameters is guaranteed to perform as well as full fine-tuning. Comparing
KS-Lottery with other parameter-efficient tuning algorithms on translation
tasks, the experimental results show that KS-Lottery finds a much smaller set
of parameters for fine-tuning while achieving the comparable performance as
full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens'
embedding of LLaMA suffices to reach the fine-tuning translation performance.
Code and model will be released to the public.
\\ ( https://arxiv.org/abs/2402.02801 ,  1844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02807
Date: Mon, 5 Feb 2024 08:35:33 GMT   (125kb,D)

Title: Are Sounds Sound for Phylogenetic Reconstruction?
Authors: Luise H\"auser and Gerhard J\"ager and Taraka Rama and Johann-Mattis
  List and Alexandros Stamatakis
Categories: cs.CL
Comments: Paper accepted for SIGTYP (2024): H\"auser, Luise; J\"ager, Gerhard;
  List, Johann-Mattis; Rama, Taraka; and Stamatakis, Alexandros (2024): Are
  sounds sound for phylogenetic reconstruction? In: Proceedings of the 6th
  Workshop on Research in Computational Linguistic Typology and Multilingual
  NLP (SIGTYP 2024)
\\
  In traditional studies on language evolution, scholars often emphasize the
importance of sound laws and sound correspondences for phylogenetic inference
of language family trees. However, to date, computational approaches have
typically not taken this potential into account. Most computational studies
still rely on lexical cognates as major data source for phylogenetic
reconstruction in linguistics, although there do exist a few studies in which
authors praise the benefits of comparing words at the level of sound sequences.
Building on (a) ten diverse datasets from different language families, and (b)
state-of-the-art methods for automated cognate and sound correspondence
detection, we test, for the first time, the performance of sound-based versus
cognate-based approaches to phylogenetic reconstruction. Our results show that
phylogenies reconstructed from lexical cognates are topologically closer, by
approximately one third with respect to the generalized quartet distance on
average, to the gold standard phylogenies than phylogenies reconstructed from
sound correspondences.
\\ ( https://arxiv.org/abs/2402.02807 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02837
Date: Mon, 5 Feb 2024 09:48:07 GMT   (124kb,D)

Title: With a Little Help from my (Linguistic) Friends: Topic Segmentation of
  Multi-party Casual Conversations
Authors: Amandine Decker (LORIA, UL, CNRS, SEMAGRAMME, GU), Maxime Amblard
  (SEMAGRAMME, LORIA)
Categories: cs.CL
Journal-ref: CODI 2024 - 5th workshop on Computational Approaches to Discourse,
  Mar 2024, Malta, Malta
\\
  Topics play an important role in the global organisation of a conversation as
what is currently discussed constrains the possible contributions of the
participant. Understanding the way topics are organised in interaction would
provide insight on the structure of dialogue beyond the sequence of utterances.
However, studying this high-level structure is a complex task that we try to
approach by first segmenting dialogues into smaller topically coherent sets of
utterances. Understanding the interactions between these segments would then
enable us to propose a model of topic organisation at a dialogue level. In this
paper we work with open-domain conversations and try to reach a comparable
level of accuracy as recent machine learning based topic segmentation models
but with a formal approach. The features we identify as meaningful for this
task help us understand better the topical structure of a conversation.
\\ ( https://arxiv.org/abs/2402.02837 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02844
Date: Mon, 5 Feb 2024 09:57:15 GMT   (7896kb,D)

Title: Comparing Knowledge Sources for Open-Domain Scientific Claim
  Verification
Authors: Juraj Vladika, Florian Matthes
Categories: cs.CL cs.AI cs.IR
Comments: Accepted to EACL 2024
\\
  The increasing rate at which scientific knowledge is discovered and health
claims shared online has highlighted the importance of developing efficient
fact-checking systems for scientific claims. The usual setting for this task in
the literature assumes that the documents containing the evidence for claims
are already provided and annotated or contained in a limited corpus. This
renders the systems unrealistic for real-world settings where knowledge sources
with potentially millions of documents need to be queried to find relevant
evidence. In this paper, we perform an array of experiments to test the
performance of open-domain claim verification systems. We test the final
verdict prediction of systems on four datasets of biomedical and health claims
in different settings. While keeping the pipeline's evidence selection and
verdict prediction parts constant, document retrieval is performed over three
common knowledge sources (PubMed, Wikipedia, Google) and using two different
information retrieval techniques. We show that PubMed works better with
specialized biomedical claims, while Wikipedia is more suited for everyday
health concerns. Likewise, BM25 excels in retrieval precision, while semantic
search in recall of relevant evidence. We discuss the results, outline frequent
retrieval patterns and challenges, and provide promising future directions.
\\ ( https://arxiv.org/abs/2402.02844 ,  7896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02864
Date: Mon, 5 Feb 2024 10:24:40 GMT   (1573kb,D)

Title: EEVEE: An Easy Annotation Tool for Natural Language Processing
Authors: Axel Sorensen, Siyao Peng, Barbara Plank, Rob van der Goot
Categories: cs.CL
Comments: 6 pages; accepted to The Linguistic Annotation Workshop (LAW) at EACL
  2024
\\
  Annotation tools are the starting point for creating Natural Language
Processing (NLP) datasets. There is a wide variety of tools available; setting
up these tools is however a hindrance. We propose EEVEE, an annotation tool
focused on simplicity, efficiency, and ease of use. It can run directly in the
browser (no setup required) and uses tab-separated files (as opposed to
character offsets or task-specific formats) for annotation. It allows for
annotation of multiple tasks on a single dataset and supports four task-types:
sequence labeling, span labeling, text classification and seq2seq.
\\ ( https://arxiv.org/abs/2402.02864 ,  1573kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02872
Date: Mon, 5 Feb 2024 10:39:32 GMT   (8788kb,D)

Title: How do Large Language Models Learn In-Context? Query and Key Matrices of
  In-Context Heads are Two Towers for Metric Learning
Authors: Zeping Yu, Sophia Ananiadou
Categories: cs.CL cs.LG
Comments: preprint
\\
  We explore the mechanism of in-context learning and propose a hypothesis
using locate-and-project method. In shallow layers, the features of
demonstrations are merged into their corresponding labels, and the features of
the input text are aggregated into the last token. In deep layers, in-context
heads make great contributions. In each in-context head, the value-output
matrix extracts the labels' features. Query and key matrices compute the
attention weights between the input text and each demonstration. The larger the
attention weight is, the more label information is transferred into the last
token for predicting the next word. Query and key matrices can be regarded as
two towers for learning the similarity metric between the input text and each
demonstration. Based on this hypothesis, we explain why imbalanced labels and
demonstration order affect predictions. We conduct experiments on GPT2 large,
Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study
provides a new method and a reasonable hypothesis for understanding the
mechanism of in-context learning. Our code will be released on github.
\\ ( https://arxiv.org/abs/2402.02872 ,  8788kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02883
Date: Mon, 5 Feb 2024 10:49:05 GMT   (209kb,D)

Title: Approximate Attributions for Off-the-Shelf Siamese Transformers
Authors: Lucas M\"oller and Dmitry Nikolaev and Sebastian Pad\'o
Categories: cs.CL cs.LG
Comments: Accepted for EACL 2024, St. Julian's, Malta
\\
  Siamese encoders such as sentence transformers are among the least understood
deep models. Established attribution methods cannot tackle this model class
since it compares two inputs rather than processing a single one. To address
this gap, we have recently proposed an attribution method specifically for
Siamese encoders (M\"oller et al., 2023). However, it requires models to be
adjusted and fine-tuned and therefore cannot be directly applied to
off-the-shelf models. In this work, we reassess these restrictions and propose
(i) a model with exact attribution ability that retains the original model's
predictive performance and (ii) a way to compute approximate attributions for
off-the-shelf models. We extensively compare approximate and exact attributions
and use them to analyze the models' attendance to different linguistic aspects.
We gain insights into which syntactic roles Siamese transformers attend to,
confirm that they mostly ignore negation, explore how they judge semantically
opposite adjectives, and find that they exhibit lexical bias.
\\ ( https://arxiv.org/abs/2402.02883 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02896
Date: Mon, 5 Feb 2024 11:05:20 GMT   (344kb,D)

Title: LLM Agents in Interaction: Measuring Personality Consistency and
  Linguistic Alignment in Interacting Populations of Large Language Models
Authors: Ivar Frisch, Mario Giulianelli
Categories: cs.CL cs.AI cs.CY cs.MA
Comments: To appear in Proceedings of the 1st Personalization of Generative AI
  Workshop, EACL 2024
\\
  While both agent interaction and personalisation are vibrant topics in
research on large language models (LLMs), there has been limited focus on the
effect of language interaction on the behaviour of persona-conditioned LLM
agents. Such an endeavour is important to ensure that agents remain consistent
to their assigned traits yet are able to engage in open, naturalistic
dialogues. In our experiments, we condition GPT-3.5 on personality profiles
through prompting and create a two-group population of LLM agents using a
simple variability-inducing sampling algorithm. We then administer personality
tests and submit the agents to a collaborative writing task, finding that
different profiles exhibit different degrees of personality consistency and
linguistic alignment to their conversational partners. Our study seeks to lay
the groundwork for better understanding of dialogue-based interaction between
LLMs and highlights the need for new approaches to crafting robust, more
human-like LLM personas for interactive environments.
\\ ( https://arxiv.org/abs/2402.02896 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02915
Date: Mon, 5 Feb 2024 11:32:13 GMT   (186kb,D)

Title: A Computational Model for the Assessment of Mutual Intelligibility Among
  Closely Related Languages
Authors: Jessica Nieder and Johann-Mattis List
Categories: cs.CL
Comments: To appear in: Proceedings of the 6th Workshop on Research in
  Computational Linguistic Typology and Multilingual NLP (SIGTYP 2024)
\\
  Closely related languages show linguistic similarities that allow speakers of
one language to understand speakers of another language without having actively
learned it. Mutual intelligibility varies in degree and is typically tested in
psycholinguistic experiments. To study mutual intelligibility computationally,
we propose a computer-assisted method using the Linear Discriminative Learner,
a computational model developed to approximate the cognitive processes by which
humans learn languages, which we expand with multilingual semantic vectors and
multilingual sound classes. We test the model on cognate data from German,
Dutch, and English, three closely related Germanic languages. We find that our
model's comprehension accuracy depends on 1) the automatic trimming of
inflections and 2) the language pair for which comprehension is tested. Our
multilingual modelling approach does not only offer new methodological findings
for automatic testing of mutual intelligibility across languages but also
extends the use of Linear Discriminative Learning to multilingual settings.
\\ ( https://arxiv.org/abs/2402.02915 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02926
Date: Mon, 5 Feb 2024 11:47:36 GMT   (276kb,D)

Title: Automated Cognate Detection as a Supervised Link Prediction Task with
  Cognate Transformer
Authors: V.S.D.S.Mahesh Akavarapu and Arnab Bhattacharya
Categories: cs.CL cs.LG
Comments: Accepted to EACL-2024 main conference
ACM-class: I.2.7
\\
  Identification of cognates across related languages is one of the primary
problems in historical linguistics. Automated cognate identification is helpful
for several downstream tasks including identifying sound correspondences,
proto-language reconstruction, phylogenetic classification, etc. Previous
state-of-the-art methods for cognate identification are mostly based on
distributions of phonemes computed across multilingual wordlists and make
little use of the cognacy labels that define links among cognate clusters. In
this paper, we present a transformer-based architecture inspired by
computational biology for the task of automated cognate detection. Beyond a
certain amount of supervision, this method performs better than the existing
methods, and shows steady improvement with further increase in supervision,
thereby proving the efficacy of utilizing the labeled information. We also
demonstrate that accepting multiple sequence alignments as input and having an
end-to-end architecture with link prediction head saves much computation time
while simultaneously yielding superior performance.
\\ ( https://arxiv.org/abs/2402.02926 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02975
Date: Mon, 5 Feb 2024 12:56:22 GMT   (9201kb,D)

Title: Putting Context in Context: the Impact of Discussion Structure on Text
  Classification
Authors: Nicol\`o Penzo, Antonio Longa, Bruno Lepri, Sara Tonelli, Marco
  Guerini
Categories: cs.CL
Comments: Accepted to EACL 2024 main conference
\\
  Current text classification approaches usually focus on the content to be
classified. Contextual aspects (both linguistic and extra-linguistic) are
usually neglected, even in tasks based on online discussions. Still in many
cases the multi-party and multi-turn nature of the context from which these
elements are selected can be fruitfully exploited. In this work, we propose a
series of experiments on a large dataset for stance detection in English, in
which we evaluate the contribution of different types of contextual
information, i.e. linguistic, structural and temporal, by feeding them as
natural language input into a transformer-based model. We also experiment with
different amounts of training data and analyse the topology of local discussion
networks in a privacy-compliant way. Results show that structural information
can be highly beneficial to text classification but only under certain
circumstances (e.g. depending on the amount of training data and on discussion
chain complexity). Indeed, we show that contextual information on smaller
datasets from other classification tasks does not yield significant
improvements. Our framework, based on local discussion networks, allows the
integration of structural information, while minimising user profiling, thus
preserving their privacy.
\\ ( https://arxiv.org/abs/2402.02975 ,  9201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03009
Date: Mon, 5 Feb 2024 13:47:53 GMT   (14393kb,D)

Title: UniMem: Towards a Unified View of Long-Context Large Language Models
Authors: Junjie Fang, Likai Tang, Hongzhe Bi, Yujia Qin, Si Sun, Zhenyu Li,
  Haolun Li, Yongjian Li, Xin Cong, Yukun Yan, Xiaodong Shi, Sen Song, Yankai
  Lin, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
\\
  Long-context processing is a critical ability that constrains the
applicability of large language models. Although there exist various methods
devoted to enhancing the long-context processing ability of large language
models (LLMs), they are developed in an isolated manner and lack systematic
analysis and integration of their strengths, hindering further developments. In
this paper, we introduce UniMem, a unified framework that reformulates existing
long-context methods from the view of memory augmentation of LLMs. UniMem is
characterized by four key dimensions: Memory Management, Memory Writing, Memory
Reading, and Memory Injection, providing a systematic theory for understanding
various long-context methods. We reformulate 16 existing methods based on
UniMem and analyze four representative methods: Transformer-XL, Memorizing
Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their
design principles and strengths. Based on these analyses, we propose UniMix, an
innovative approach that integrates the strengths of these algorithms.
Experimental results show that UniMix achieves superior performance in handling
long contexts with significantly lower perplexity than baselines.
\\ ( https://arxiv.org/abs/2402.03009 ,  14393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03043
Date: Mon, 5 Feb 2024 14:29:54 GMT   (1286kb,D)

Title: SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach
Authors: Mohammad N.S. Jahromi, Satya. M. Muddamsetty, Asta Sofie Stage
  Jarlner, Anna Murphy H{\o}genhaug, Thomas Gammeltoft-Hansen, Thomas B.
  Moeslund
Categories: cs.CL cs.LG
Comments: Preprint submitted to Elsevier on Jan 5th, 2024
\\
  Explainable AI (XAI) aids in deciphering 'black-box' models. While several
methods have been proposed and evaluated primarily in the image domain, the
exploration of explainability in the text domain remains a growing research
area. In this paper, we delve into the applicability of XAI methods for the
text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU)
XAI method, recognized for its superior capability in localizing entire salient
regions in image-based classification is extended to textual data. The extended
method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to
generate heatmaps at a granular, word-based level, thereby providing
explanations that highlight contextually significant textual elements crucial
for model predictions. Given the absence of a unified standard for assessing
XAI methods, this study applies a holistic three-tiered comprehensive
evaluation framework: Functionally-Grounded, Human-Grounded and
Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT
across various experiments. We find that, in sentiment analysis task of a movie
review dataset, SIDU-TXT excels in both functionally and human-grounded
evaluations, demonstrating superior performance through quantitative and
qualitative analyses compared to benchmarks like Grad-CAM and LIME. In the
application-grounded evaluation within the sensitive and complex legal domain
of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable
performances, each with its own set of strengths and weaknesses. However, both
methods fall short of entirely fulfilling the sophisticated criteria of expert
expectations, highlighting the imperative need for additional research in XAI
methods suitable for such domains.
\\ ( https://arxiv.org/abs/2402.03043 ,  1286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03049
Date: Mon, 5 Feb 2024 14:33:56 GMT   (2686kb,D)

Title: EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models
Authors: Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Zhen Bi,
  Huajun Chen
Categories: cs.CL cs.AI cs.HC cs.IR cs.LG
Comments: Ongoing work; the project website is at
  https://zjunlp.github.io/project/EasyInstruct, code is at
  https://github.com/zjunlp/EasyInstruct, demo is at
  https://huggingface.co/spaces/zjunlp/EasyInstruct
\\
  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with a running demo App at
https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for
broader research centered on instruction data.
\\ ( https://arxiv.org/abs/2402.03049 ,  2686kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03053
Date: Mon, 5 Feb 2024 14:36:51 GMT   (288kb,D)

Title: Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for
  Semantic Representations
Authors: Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan
Categories: cs.CL cs.LG
\\
  In this work, we present a comprehensive exploration of finetuning Malaysian
language models, specifically Llama2 and Mistral, on embedding tasks involving
negative and positive pairs. We release two distinct models tailored for
Semantic Similarity and Retrieval-Augmented Generation (RAG).
  For Semantic Similarity, our 600 million parameter Llama2 model outperforms
OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my,
c.cari.com.my, Malay news, and Malaysian Twitter test sets.
  In the realm of RAG models, our approach proves competitive with OpenAI
text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion
parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu"
keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10
for the lom.agc.gov.my dataset.
  These findings underscore the effectiveness of our finetuning strategy and
highlight the performance gains in both Semantic Similarity and RAG tasks.
  All models released at
https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99
\\ ( https://arxiv.org/abs/2402.03053 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03067
Date: Mon, 5 Feb 2024 14:59:29 GMT   (374kb)

Title: Multilingual transformer and BERTopic for short text topic modeling: The
  case of Serbian
Authors: Darija Medvecki, Bojana Ba\v{s}aragin, Adela Ljaji\'c, Nikola
  Milo\v{s}evi\'c
Categories: cs.CL cs.AI
Journal-ref: Trajanovic, M., Filipovic, N., Zdravkovic, M. (eds) Disruptive
  Information Technologies for a Smart Society. ICIST 2023. Lecture Notes in
  Networks and Systems, vol 872. Springer, Cham
DOI: 10.1007/978-3-031-50755-7_16
\\
  This paper presents the results of the first application of BERTopic, a
state-of-the-art topic modeling technique, to short text written in a
morphologi-cally rich language. We applied BERTopic with three multilingual
embed-ding models on two levels of text preprocessing (partial and full) to
evalu-ate its performance on partially preprocessed short text in Serbian. We
also compared it to LDA and NMF on fully preprocessed text. The experiments
were conducted on a dataset of tweets expressing hesitancy toward COVID-19
vaccination. Our results show that with adequate parameter setting, BERTopic
can yield informative topics even when applied to partially pre-processed short
text. When the same parameters are applied in both prepro-cessing scenarios,
the performance drop on partially preprocessed text is minimal. Compared to LDA
and NMF, judging by the keywords, BERTopic offers more informative topics and
gives novel insights when the number of topics is not limited. The findings of
this paper can be significant for re-searchers working with other
morphologically rich low-resource languages and short text.
\\ ( https://arxiv.org/abs/2402.03067 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03099
Date: Mon, 5 Feb 2024 15:28:43 GMT   (4231kb,D)

Title: Intent-based Prompt Calibration: Enhancing prompt optimization with
  synthetic boundary cases
Authors: Elad Levi, Eli Brosh, Matan Friedmann
Categories: cs.CL cs.AI cs.LG
\\
  Prompt engineering is a challenging and important task due to the high
sensitivity of Large Language Models (LLMs) to the given prompt and the
inherent ambiguity of a textual task instruction. Automatic prompt engineering
is essential to achieve optimized performance from LLMs. Recent studies have
demonstrated the capabilities of LLMs to automatically conduct prompt
engineering by employing a meta-prompt that incorporates the outcomes of the
last trials and proposes an improved prompt. However, this requires a
high-quality benchmark to compare different prompts, which is difficult and
expensive to acquire in many real-world use cases. In this work, we introduce a
new method for automatic prompt engineering, using a calibration process that
iteratively refines the prompt to the user intent. During the optimization
process, the system jointly generates synthetic data of boundary use cases and
optimizes the prompt according to the generated dataset. We demonstrate the
effectiveness of our method with respect to strong proprietary models on
real-world tasks such as moderation and generation. Our method outperforms
state-of-the-art methods with a limited number of annotated samples.
Furthermore, we validate the advantages of each one of the system's key
components. Our system is built in a modular way, facilitating easy adaptation
to other tasks. The code is available
$\href{https://github.com/Eladlev/AutoPrompt}{here}$.
\\ ( https://arxiv.org/abs/2402.03099 ,  4231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03131
Date: Mon, 5 Feb 2024 15:57:32 GMT   (511kb,D)

Title: Constrained Decoding for Cross-lingual Label Projection
Authors: Duong Minh Le, Yang Chen, Alan Ritter, Wei Xu
Categories: cs.CL cs.LG
Comments: Accepted at ICLR 2024
\\
  Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a
popular learning paradigm for low-resource languages with no labeled training
data. However, for NLP tasks that involve fine-grained predictions on words and
phrases, the performance of zero-shot cross-lingual transfer learning lags far
behind supervised fine-tuning methods. Therefore, it is common to exploit
translation and label projection to further improve the performance by (1)
translating training data that is available in a high-resource language (e.g.,
English) together with the gold labels into low-resource languages, and/or (2)
translating test data in low-resource languages to a high-source language to
run inference on, then projecting the predicted span-level labels back onto the
original test data. However, state-of-the-art marker-based label projection
methods suffer from translation quality degradation due to the extra label
markers injected in the input to the translation model. In this work, we
explore a new direction that leverages constrained decoding for label
projection to overcome the aforementioned issues. Our new method not only can
preserve the quality of translated texts but also has the versatility of being
applicable to both translating training and translating test data strategies.
This versatility is crucial as our experiments reveal that translating test
data can lead to a considerable boost in performance compared to translating
only training data. We evaluate on two cross-lingual transfer tasks, namely
Named Entity Recognition and Event Argument Extraction, spanning 20 languages.
The results demonstrate that our approach outperforms the state-of-the-art
marker-based method by a large margin and also shows better performance than
other label projection methods that rely on external word alignment.
\\ ( https://arxiv.org/abs/2402.03131 ,  511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03137
Date: Mon, 5 Feb 2024 16:05:32 GMT   (336kb,D)

Title: Sociolinguistically Informed Interpretability: A Case Study on Hinglish
  Emotion Classification
Authors: Kushal Tatariya, Heather Lent, Johannes Bjerva, Miryam de Lhoneux
Categories: cs.CL cs.LG
Comments: 5 pages, Accepted to SIGTYP 2024 @ EACL
\\
  Emotion classification is a challenging task in NLP due to the inherent
idiosyncratic and subjective nature of linguistic expression, especially with
code-mixed data. Pre-trained language models (PLMs) have achieved high
performance for many tasks and languages, but it remains to be seen whether
these models learn and are robust to the differences in emotional expression
across languages. Sociolinguistic studies have shown that Hinglish speakers
switch to Hindi when expressing negative emotions and to English when
expressing positive emotions. To understand if language models can learn these
associations, we study the effect of language on emotion prediction across 3
PLMs on a Hinglish emotion classification dataset. Using LIME and token level
language ID, we find that models do learn these associations between language
choice and emotional expression. Moreover, having code-mixed data present in
the pre-training can augment that learning when task-specific data is scarce.
We also conclude from the misclassifications that the models may overgeneralise
this heuristic to other infrequent examples where this sociolinguistic
phenomenon does not apply.
\\ ( https://arxiv.org/abs/2402.03137 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03163
Date: Mon, 5 Feb 2024 16:31:03 GMT   (275kb,D)

Title: Linguistic features for sentence difficulty prediction in ABSA
Authors: Adrian-Gabriel Chifu and S\'ebastien Fournier
Categories: cs.CL cs.IR
\\
  One of the challenges of natural language understanding is to deal with the
subjectivity of sentences, which may express opinions and emotions that add
layers of complexity and nuance. Sentiment analysis is a field that aims to
extract and analyze these subjective elements from text, and it can be applied
at different levels of granularity, such as document, paragraph, sentence, or
aspect. Aspect-based sentiment analysis is a well-studied topic with many
available data sets and models. However, there is no clear definition of what
makes a sentence difficult for aspect-based sentiment analysis. In this paper,
we explore this question by conducting an experiment with three data sets:
"Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent Sentiment
Classification), and a merged version of these three datasets. We study the
impact of domain diversity and syntactic diversity on difficulty. We use a
combination of classifiers to identify the most difficult sentences and analyze
their characteristics. We employ two ways of defining sentence difficulty. The
first one is binary and labels a sentence as difficult if the classifiers fail
to correctly predict the sentiment polarity. The second one is a six-level
scale based on how many of the top five best-performing classifiers can
correctly predict the sentiment polarity. We also define 9 linguistic features
that, combined, aim at estimating the difficulty at sentence level.
\\ ( https://arxiv.org/abs/2402.03163 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03171
Date: Mon, 5 Feb 2024 16:39:15 GMT   (64kb,D)

Title: Homograph Attacks on Maghreb Sentiment Analyzers
Authors: Fatima Zahra Qachfar, Rakesh M. Verma
Categories: cs.CL cs.CR cs.LG
Comments: NAML, North Africans in Machine Leaning, NeurIPS, Neural Information
  Processing Systems
\\
  We examine the impact of homograph attacks on the Sentiment Analysis (SA)
task of different Arabic dialects from the Maghreb North-African countries.
Homograph attacks result in a 65.3% decrease in transformer classification from
an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this
study is to highlight LLMs weaknesses' and to prioritize ethical and
responsible Machine Learning.
\\ ( https://arxiv.org/abs/2402.03171 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03172
Date: Mon, 5 Feb 2024 16:40:23 GMT   (568kb,D)

Title: Accurate and Well-Calibrated ICD Code Assignment Through Attention Over
  Diverse Label Embeddings
Authors: Gon\c{c}alo Gomes, Isabel Coutinho, Bruno Martins
Categories: cs.CL cs.AI
Comments: Accepted to EACL2024
\\
  Although the International Classification of Diseases (ICD) has been adopted
worldwide, manually assigning ICD codes to clinical text is time-consuming,
error-prone, and expensive, motivating the development of automated approaches.
This paper describes a novel approach for automated ICD coding, combining
several ideas from previous related work. We specifically employ a strong
Transformer-based model as a text encoder and, to handle lengthy clinical
narratives, we explored either (a) adapting the base encoder model into a
Longformer, or (b) dividing the text into chunks and processing each chunk
independently. The representations produced by the encoder are combined with a
label embedding mechanism that explores diverse ICD code synonyms. Experiments
with different splits of the MIMIC-III dataset show that the proposed approach
outperforms the current state-of-the-art models in ICD coding, with the label
embeddings significantly contributing to the good performance. Our approach
also leads to properly calibrated classification results, which can effectively
inform downstream tasks such as quantification.
\\ ( https://arxiv.org/abs/2402.03172 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03173
Date: Mon, 5 Feb 2024 16:41:02 GMT   (10211kb,D)

Title: Multi: Multimodal Understanding Leaderboard with Text and Images
Authors: Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun,
  Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Situo Zhang, Zihan Zhao,
  Liangtai Sun, Kai Yu
Categories: cs.CL cs.AI cs.CV
Comments: Details and access are available at:
  https://OpenDFM.github.io/MULTI-Benchmark/
\\
  Rapid progress in multimodal large language models (MLLMs) highlights the
need to introduce challenging yet realistic benchmarks to the academic
community. Existing benchmarks primarily focus on simple natural image
understanding, but Multi emerges as a cutting-edge benchmark for MLLMs,
offering a comprehensive dataset for evaluating MLLMs against understanding
complex figures and tables, and scientific questions. This benchmark,
reflecting current realistic examination styles, provides multimodal inputs and
requires responses that are either precise or open-ended, similar to real-life
school tests. It challenges MLLMs with a variety of tasks, ranging from formula
derivation to image detail analysis, and cross-modality reasoning. Multi
includes over 18,000 questions, with a focus on science-based QA in diverse
formats. We also introduce Multi-Elite, a 500-question subset for testing the
extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning
research with more than 4,500 knowledge pieces. Our evaluation indicates
significant potential for MLLM advancement, with GPT-4V achieving a 63.7%
accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and
53.7%. Multi serves not only as a robust evaluation platform but also paves the
way for the development of expert-level AI.
\\ ( https://arxiv.org/abs/2402.03173 ,  10211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03177
Date: Mon, 5 Feb 2024 16:44:17 GMT   (2676kb,D)

Title: CIDAR: Culturally Relevant Instruction Dataset For Arabic
Authors: Zaid Alyafeai, Khalid Almubarak, Ahmed Ashraf, Deema Alnuhait, Saied
  Alshahrani, Gubran A. Q. Abdulrahman, Gamil Ahmed, Qais Gawah, Zead Saleh,
  Mustafa Ghaleb, Yousef Ali, Maged S. Al-Shaibani
Categories: cs.CL cs.LG
\\
  Instruction tuning has emerged as a prominent methodology for teaching Large
Language Models (LLMs) to follow instructions. However, current instruction
datasets predominantly cater to English or are derived from English-dominated
LLMs, resulting in inherent biases toward Western culture. This bias
significantly impacts the linguistic structures of non-English languages such
as Arabic, which has a distinct grammar reflective of the diverse cultures
across the Arab region. This paper addresses this limitation by introducing
CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic
instruction-tuning dataset culturally-aligned by human reviewers. CIDAR
contains 10,000 instruction and output pairs that represent the Arab region. We
discuss the cultural relevance of CIDAR via the analysis and comparison to
other models fine-tuned on other datasets. Our experiments show that CIDAR can
help enrich research efforts in aligning LLMs with the Arabic culture. All the
code is available at https://github.com/ARBML/CIDAR.
\\ ( https://arxiv.org/abs/2402.03177 ,  2676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03190
Date: Mon, 5 Feb 2024 16:56:11 GMT   (8789kb,D)

Title: Unified Hallucination Detection for Multimodal Large Language Models
Authors: Xiang Chen and Chenxi Wang and Yida Xue and Ningyu Zhang and Xiaoyan
  Yang and Qiang Li and Yue Shen and Jinjie Gu and Huajun Chen
Categories: cs.CL cs.AI cs.IR cs.LG cs.MM
Comments: Work in progress
\\
  Despite significant strides in multimodal tasks, Multimodal Large Language
Models (MLLMs) are plagued by the critical issue of hallucination. The reliable
detection of such hallucinations in MLLMs has, therefore, become a vital aspect
of model evaluation and the safeguarding of practical application deployment.
Prior research in this domain has been constrained by a narrow focus on
singular tasks, an inadequate range of hallucination categories addressed, and
a lack of detailed granularity. In response to these challenges, our work
expands the investigative horizons of hallucination detection. We present a
novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate
the evaluation of advancements in hallucination detection methods.
Additionally, we unveil a novel unified multimodal hallucination detection
framework, UNIHD, which leverages a suite of auxiliary tools to validate the
occurrence of hallucinations robustly. We demonstrate the effectiveness of
UNIHD through meticulous evaluation and comprehensive analysis. We also provide
strategic insights on the application of specific tools for addressing various
categories of hallucinations.
\\ ( https://arxiv.org/abs/2402.03190 ,  8789kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03216
Date: Mon, 5 Feb 2024 17:26:49 GMT   (1238kb,D)

Title: BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge Distillation
Authors: Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\
  In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.
\\ ( https://arxiv.org/abs/2402.03216 ,  1238kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03221
Date: Mon, 5 Feb 2024 17:33:22 GMT   (9004kb,D)

Title: "Define Your Terms" : Enhancing Efficient Offensive Speech
  Classification with Definition
Authors: Huy Nghiem, Umang Gupta, Fred Morstatter
Categories: cs.CL
Comments: Accepted to Main Conference, EACL 2024
\\
  The propagation of offensive content through social media channels has
garnered attention of the research community. Multiple works have proposed
various semantically related yet subtle distinct categories of offensive
speech. In this work, we explore meta-earning approaches to leverage the
diversity of offensive speech corpora to enhance their reliable and efficient
detection. We propose a joint embedding architecture that incorporates the
input's label and definition for classification via Prototypical Network. Our
model achieves at least 75% of the maximal F1-score while using less than 10%
of the available training data across 4 datasets. Our experimental findings
also provide a case study of training strategies valuable to combat resource
scarcity.
\\ ( https://arxiv.org/abs/2402.03221 ,  9004kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03223
Date: Mon, 5 Feb 2024 17:36:19 GMT   (220kb,D)

Title: English Prompts are Better for NLI-based Zero-Shot Emotion
  Classification than Target-Language Prompts
Authors: Patrick Barrei{\ss} and Roman Klinger and Jeremy Barnes
Categories: cs.CL
Comments: submitted to the PromptEng workshop at The Web Conf
\\
  Emotion classification in text is a challenging and subjective task, due to
the involved cognitive inference processes that are required to interpret a
textual stimulus. In addition, the set of emotion categories is highly
domain-specific. For instance, literature analysis might require the use of
aesthetic emotions (e.g., finding something beautiful), and social media
analysis could benefit from fine-grained sets (e.g., separating anger from
annoyance) in contrast to basic emotion categories. This renders the task an
interesting field for zero-shot classifications, in which the label set is not
known at model development time. Unfortunately, most resources for emotion
analysis are English, and therefore, most studies on emotion analysis have been
performed in English, including those that involve prompting language models
for text labels. This leaves us with a research gap that we address in this
paper: In which language should we prompt for emotion labels on non-English
texts? This is particularly of interest when we have access to a multilingual
large language model, because we could request labels with English prompts even
for non-English data. Our experiments with natural language inference-based
language models show that it is consistently better to use English prompts even
if the data is in a different language.
\\ ( https://arxiv.org/abs/2402.03223 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03242
Date: Mon, 5 Feb 2024 17:57:26 GMT   (8624kb,D)

Title: JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance
  Skill Matching
Authors: Antoine Magron, Anna Dai, Mike Zhang, Syrielle Montariol, Antoine
  Bosselut
Categories: cs.CL
Comments: Published at NLP4HR 2024 (EACL Workshop)
\\
  Recent approaches in skill matching, employing synthetic training data for
classification or similarity model training, have shown promising results,
reducing the need for time-consuming and expensive annotations. However,
previous synthetic datasets have limitations, such as featuring only one skill
per sentence and generally comprising short sentences. In this paper, we
introduce JobSkape, a framework to generate synthetic data that tackles these
limitations, specifically designed to enhance skill-to-taxonomy matching.
Within this framework, we create SkillSkape, a comprehensive open-source
synthetic dataset of job postings tailored for skill-matching tasks. We
introduce several offline metrics that show that our dataset resembles
real-world data. Additionally, we present a multi-step pipeline for skill
extraction and matching tasks using large language models (LLMs), benchmarking
against known supervised methodologies. We outline that the downstream
evaluation results on real-world data can beat baselines, underscoring its
efficacy and adaptability.
\\ ( https://arxiv.org/abs/2402.03242 ,  8624kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03271
Date: Mon, 5 Feb 2024 18:28:44 GMT   (669kb,D)

Title: Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information
  Seeking in Large Language Models
Authors: Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh
  Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi
Categories: cs.CL cs.AI cs.LG
Comments: Under review
\\
  In the face of uncertainty, the ability to seek information is of fundamental
importance. In many practical applications, such as medical diagnosis and
troubleshooting, the information needed to solve the task is not initially
given, and has to be actively sought by asking follow-up questions (for
example, a doctor asking a patient for more details about their symptoms). In
this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment
large language models with the ability to actively seek information by asking
effective questions. UoT combines 1) an uncertainty-aware simulation approach
which enables the model to simulate possible future scenarios and how likely
they are to occur, 2) uncertainty-based rewards motivated by information gain
which incentivizes the model to seek information, and 3) a reward propagation
scheme to select the optimal question to ask in a way that maximizes the
expected reward. In experiments on medical diagnosis, troubleshooting and the
'20 Questions' game, UoT achieves an average performance improvement of 57.8%
in the rate of successful task completion across multiple LLMs compared with
direct prompting, and also improves efficiency (i.e., the number of questions
needed to complete the task).
\\ ( https://arxiv.org/abs/2402.03271 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03284
Date: Mon, 5 Feb 2024 18:39:47 GMT   (1877kb,D)

Title: Deal, or no deal (or who knows)? Forecasting Uncertainty in
  Conversations using Large Language Models
Authors: Anthony Sicilia, Hyunwoo Kim, Khyathi Raghavi Chandu, Malihe Alikhani,
  Jack Hessel
Categories: cs.CL cs.AI cs.LG
Comments: 2 Figures; 7 Tables; 27 pages
\\
  Effective interlocutors account for the uncertain goals, beliefs, and
emotions of others. But even the best human conversationalist cannot perfectly
anticipate the trajectory of a dialogue. How well can language models represent
inherent uncertainty in conversations? We propose FortUne Dial, an expansion of
the long-standing "conversation forecasting" task: instead of just accuracy,
evaluation is conducted with uncertainty-aware metrics, effectively enabling
abstention on individual instances. We study two ways in which language models
potentially represent outcome uncertainty (internally, using scores and
directly, using tokens) and propose fine-tuning strategies to improve
calibration of both representations. Experiments on eight difficult negotiation
corpora demonstrate that our proposed fine-tuning strategies (a traditional
supervision strategy and an off-policy reinforcement learning strategy) can
calibrate smaller open-source models to compete with pre-trained models 10x
their size.
\\ ( https://arxiv.org/abs/2402.03284 ,  1877kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03300
Date: Mon, 5 Feb 2024 18:55:32 GMT   (3417kb,D)

Title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
  Language Models
Authors: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song,
  Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo
Categories: cs.CL cs.AI cs.LG
\\
  Mathematical reasoning poses a significant challenge for language models due
to its complex and structured nature. In this paper, we introduce DeepSeekMath
7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B
math-related tokens sourced from Common Crawl, together with natural language
and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the
competition-level MATH benchmark without relying on external toolkits and
voting techniques, approaching the performance level of Gemini-Ultra and GPT-4.
Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.
The mathematical reasoning capability of DeepSeekMath is attributed to two key
factors: First, we harness the significant potential of publicly available web
data through a meticulously engineered data selection pipeline. Second, we
introduce Group Relative Policy Optimization (GRPO), a variant of Proximal
Policy Optimization (PPO), that enhances mathematical reasoning abilities while
concurrently optimizing the memory usage of PPO.
\\ ( https://arxiv.org/abs/2402.03300 ,  3417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03303
Date: Mon, 5 Feb 2024 18:58:19 GMT   (3985kb,D)

Title: Nevermind: Instruction Override and Moderation in Large Language Models
Authors: Edward Kim
Categories: cs.CL cs.AI cs.LG
Comments: 11 pages
\\
  Given the impressive capabilities of recent Large Language Models (LLMs), we
investigate and benchmark the most popular proprietary and different sized open
source models on the task of explicit instruction following in conflicting
situations, e.g. overrides. These include the ability of the model to override
the knowledge within the weights of the model, the ability to override (or
moderate) extracted knowledge in the prompt, and lastly the ability to perform
a full jailbreak. Experimentation performed suggest several key findings to
improve instruction following - larger models perform the best in following
instructions that override internal and contextual instructions, and are
obedient, even to a fault. When scaling to longer contexts via rope scaling, a
significant buffer needs to be maintained from the edge of the perplexity cliff
in order to maintain instruction following capabilities. Finally, we observe
improving instruction following, and subsequently instruction
overrides/jailbreaks, is fundamentally at odds with the ability of a language
model to follow given safety filters or guidelines. Thus, we postulate the most
effective approach for safe, trustworthy AI should be dealt external to the LLM
itself.
\\ ( https://arxiv.org/abs/2402.03303 ,  3985kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01768
Date: Thu, 1 Feb 2024 02:57:07 GMT   (1809kb)

Title: Enriched Physics-informed Neural Networks for Dynamic
  Poisson-Nernst-Planck Systems
Authors: Xujia Huang, Fajie Wang, Benrong Zhang and Hanqing Liu
Categories: cs.LG physics.comp-ph
Comments: 24 pages, 16 figures, 6 tables
MSC-class: 65M99, 35M33, 68T07
\\
  This paper proposes a meshless deep learning algorithm, enriched
physics-informed neural networks (EPINNs), to solve dynamic
Poisson-Nernst-Planck (PNP) equations with strong coupling and nonlinear
characteristics. The EPINNs takes the traditional physics-informed neural
networks as the foundation framework, and adds the adaptive loss weight to
balance the loss functions, which automatically assigns the weights of losses
by updating the parameters in each iteration based on the maximum likelihood
estimate. The resampling strategy is employed in the EPINNs to accelerate the
convergence of loss function. Meanwhile, the GPU parallel computing technique
is adopted to accelerate the solving process. Four examples are provided to
demonstrate the validity and effectiveness of the proposed method. Numerical
results indicate that the new method has better applicability than traditional
numerical methods in solving such coupled nonlinear systems. More importantly,
the EPINNs is more accurate, stable, and fast than the traditional
physics-informed neural networks. This work provides a simple and
high-performance numerical tool for addressing PNPs with arbitrary boundary
shapes and boundary conditions.
\\ ( https://arxiv.org/abs/2402.01768 ,  1809kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01785
Date: Thu, 1 Feb 2024 21:34:34 GMT   (261kb,D)

Title: DoubleMLDeep: Estimation of Causal Effects with Multimodal Data
Authors: Sven Klaassen, Jan Teichert-Kluge, Philipp Bach, Victor Chernozhukov,
  Martin Spindler, Suhas Vijaykumar
Categories: cs.LG cs.AI econ.EM stat.ME stat.ML
MSC-class: 62, 91
ACM-class: I.2.0
\\
  This paper explores the use of unstructured, multimodal data, namely text and
images, in causal inference and treatment effect estimation. We propose a
neural network architecture that is adapted to the double machine learning
(DML) framework, specifically the partially linear model. An additional
contribution of our paper is a new method to generate a semi-synthetic dataset
which can be used to evaluate the performance of causal effect estimation in
the presence of text and images as confounders. The proposed methods and
architectures are evaluated on the semi-synthetic dataset and compared to
standard approaches, highlighting the potential benefit of using text and
images directly in causal studies. Our findings have implications for
researchers and practitioners in economics, marketing, finance, medicine and
data science in general who are interested in estimating causal quantities
using non-traditional data.
\\ ( https://arxiv.org/abs/2402.01785 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01790
Date: Fri, 2 Feb 2024 02:56:01 GMT   (8146kb,D)

Title: An introduction to graphical tensor notation for mechanistic
  interpretability
Authors: Jordan K. Taylor
Categories: cs.LG cs.AI
Comments: 30 pages, 75 figures
\\
  Graphical tensor notation is a simple way of denoting linear operations on
tensors, originating from physics. Modern deep learning consists almost
entirely of operations on or between tensors, so easily understanding tensor
operations is quite important for understanding these systems. This is
especially true when attempting to reverse-engineer the algorithms learned by a
neural network in order to understand its behavior: a field known as
mechanistic interpretability. It's often easy to get confused about which
operations are happening between tensors and lose sight of the overall
structure, but graphical tensor notation makes it easier to parse things at a
glance and see interesting equivalences. The first half of this document
introduces the notation and applies it to some decompositions (SVD, CP, Tucker,
and tensor network decompositions), while the second half applies it to some
existing some foundational approaches for mechanistically understanding
language models, loosely following ``A Mathematical Framework for Transformer
Circuits'', then constructing an example ``induction head'' circuit in
graphical tensor notation.
\\ ( https://arxiv.org/abs/2402.01790 ,  8146kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01797
Date: Fri, 2 Feb 2024 05:42:50 GMT   (455kb,D)

Title: Robust support vector machines via conic optimization
Authors: Valentina Cepeda, Andr\'es G\'omez, Shaoning Han
Categories: cs.LG math.OC stat.CO
\\
  We consider the problem of learning support vector machines robust to
uncertainty. It has been established in the literature that typical loss
functions, including the hinge loss, are sensible to data perturbations and
outliers, thus performing poorly in the setting considered. In contrast, using
the 0-1 loss or a suitable non-convex approximation results in robust
estimators, at the expense of large computational costs. In this paper we use
mixed-integer optimization techniques to derive a new loss function that better
approximates the 0-1 loss compared with existing alternatives, while preserving
the convexity of the learning problem. In our computational results, we show
that the proposed estimator is competitive with the standard SVMs with the
hinge loss in outlier-free regimes and better in the presence of outliers.
\\ ( https://arxiv.org/abs/2402.01797 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01798
Date: Fri, 2 Feb 2024 06:14:31 GMT   (298kb,D)

Title: Improved Quantization Strategies for Managing Heavy-tailed Gradients in
  Distributed Learning
Authors: Guangfeng Yan, Tan Li, Yuanzhang Xiao, Hanxu Hou and Linqi Song
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2402.01160
\\
  Gradient compression has surfaced as a key technique to address the challenge
of communication efficiency in distributed learning. In distributed deep
learning, however, it is observed that gradient distributions are heavy-tailed,
with outliers significantly influencing the design of compression strategies.
Existing parameter quantization methods experience performance degradation when
this heavy-tailed feature is ignored. In this paper, we introduce a novel
compression scheme specifically engineered for heavy-tailed gradients, which
effectively combines gradient truncation with quantization. This scheme is
adeptly implemented within a communication-limited distributed Stochastic
Gradient Descent (SGD) framework. We consider a general family of heavy-tail
gradients that follow a power-law distribution, we aim to minimize the error
resulting from quantization, thereby determining optimal values for two
critical parameters: the truncation threshold and the quantization density. We
provide a theoretical analysis on the convergence error bound under both
uniform and non-uniform quantization scenarios. Comparative experiments with
other benchmarks demonstrate the effectiveness of our proposed method in
managing the heavy-tailed gradients in a distributed learning environment.
\\ ( https://arxiv.org/abs/2402.01798 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01799
Date: Fri, 2 Feb 2024 06:29:34 GMT   (49kb)

Title: Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
Authors: Arnav Chavan, Raghav Magazine, Shubham Kushwaha, M\'erouane Debbah and
  Deepak Gupta
Categories: cs.LG cs.AI cs.CL
Comments: Brief Survey, Under review at IJCAI '24
\\
  Despite the impressive performance of LLMs, their widespread adoption faces
challenges due to substantial computational and memory requirements during
inference. Recent advancements in model compression and system-level
optimization methods aim to enhance LLM inference. This survey offers an
overview of these methods, emphasizing recent developments. Through experiments
on LLaMA(/2)-7B, we evaluate various compression techniques, providing
practical insights for efficient LLM deployment in a unified setting. The
empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these
methods. Drawing from survey insights, we identify current limitations and
discuss potential future directions to improve LLM inference efficiency. We
release the codebase to reproduce the results presented in this paper at
https://github.com/nyunAI/Faster-LLM-Survey
\\ ( https://arxiv.org/abs/2402.01799 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01801
Date: Fri, 2 Feb 2024 07:24:35 GMT   (194kb,D)

Title: Large Language Models for Time Series: A Survey
Authors: Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, Jingbo Shang
Categories: cs.LG cs.AI cs.CL
Comments: GitHub repository:
  https://github.com/xiyuanzh/awesome-llm-time-series
\\
  Large Language Models (LLMs) have seen significant use in domains such as
natural language processing and computer vision. Going beyond text, image and
graphics, LLMs present a significant potential for analysis of time series
data, benefiting domains such as climate, IoT, healthcare, traffic, audio and
finance. This survey paper provides an in-depth exploration and a detailed
taxonomy of the various methodologies employed to harness the power of LLMs for
time series analysis. We address the inherent challenge of bridging the gap
between LLMs' original text data training and the numerical nature of time
series data, and explore strategies for transferring and distilling knowledge
from LLMs to numerical time series analysis. We detail various methodologies,
including (1) direct prompting of LLMs, (2) time series quantization, (3)
alignment techniques, (4) utilization of the vision modality as a bridging
mechanism, and (5) the combination of LLMs with tools. Additionally, this
survey offers a comprehensive overview of the existing multimodal time series
and text datasets and delves into the challenges and future opportunities of
this emerging field. We maintain an up-to-date Github repository which includes
all the papers and datasets discussed in the survey.
\\ ( https://arxiv.org/abs/2402.01801 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01802
Date: Fri, 2 Feb 2024 07:25:53 GMT   (1518kb,D)

Title: An Auction-based Marketplace for Model Trading in Federated Learning
Authors: Yue Cui, Liuyi Yao, Yaliang Li, Ziqian Chen, Bolin Ding, Xiaofang Zhou
Categories: cs.LG cs.AI cs.GT
\\
  Federated learning (FL) is increasingly recognized for its efficacy in
training models using locally distributed data. However, the proper valuation
of shared data in this collaborative process remains insufficiently addressed.
In this work, we frame FL as a marketplace of models, where clients act as both
buyers and sellers, engaging in model trading. This FL market allows clients to
gain monetary reward by selling their own models and improve local model
performance through the purchase of others' models. We propose an auction-based
solution to ensure proper pricing based on performance gain. Incentive
mechanisms are designed to encourage clients to truthfully reveal their model
valuations. Furthermore, we introduce a reinforcement learning (RL) framework
for marketing operations, aiming to achieve maximum trading volumes under the
dynamic and evolving market status. Experimental results on four datasets
demonstrate that the proposed FL market can achieve high trading revenue and
fair downstream task accuracy.
\\ ( https://arxiv.org/abs/2402.01802 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01811
Date: Fri, 2 Feb 2024 11:43:59 GMT   (1484kb,D)

Title: A Distributionally Robust Optimisation Approach to Fair Credit Scoring
Authors: Pablo Casas, Christophe Mues, Huan Yu
Categories: cs.LG cs.CY
\\
  Credit scoring has been catalogued by the European Commission and the
Executive Office of the US President as a high-risk classification task, a key
concern being the potential harms of making loan approval decisions based on
models that would be biased against certain groups. To address this concern,
recent credit scoring research has considered a range of fairness-enhancing
techniques put forward by the machine learning community to reduce bias and
unfair treatment in classification systems. While the definition of fairness or
the approach they follow to impose it may vary, most of these techniques,
however, disregard the robustness of the results. This can create situations
where unfair treatment is effectively corrected in the training set, but when
producing out-of-sample classifications, unfair treatment is incurred again.
Instead, in this paper, we will investigate how to apply Distributionally
Robust Optimisation (DRO) methods to credit scoring, thereby empirically
evaluating how they perform in terms of fairness, ability to classify
correctly, and the robustness of the solution against changes in the marginal
proportions. In so doing, we find DRO methods to provide a substantial
improvement in terms of fairness, with almost no loss in performance. These
results thus indicate that DRO can improve fairness in credit scoring, provided
that further advances are made in efficiently implementing these systems. In
addition, our analysis suggests that many of the commonly used fairness metrics
are unsuitable for a credit scoring setting, as they depend on the choice of
classification threshold.
\\ ( https://arxiv.org/abs/2402.01811 ,  1484kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01821
Date: Fri, 2 Feb 2024 16:32:04 GMT   (484kb,D)

Title: Ecologically rational meta-learned inference explains human category
  learning
Authors: Akshay K. Jagadish, Julian Coda-Forno, Mirko Thalmann, Eric Schulz,
  and Marcel Binz
Categories: cs.LG cs.AI
Comments: 22 pages (8 pages of main text, 4 pages of references, and 10 pages
  of appendix), 7 figures, and 4 Tables
\\
  Ecological rationality refers to the notion that humans are rational agents
adapted to their environment. However, testing this theory remains challenging
due to two reasons: the difficulty in defining what tasks are ecologically
valid and building rational models for these tasks. In this work, we
demonstrate that large language models can generate cognitive tasks,
specifically category learning tasks, that match the statistics of real-world
tasks, thereby addressing the first challenge. We tackle the second challenge
by deriving rational agents adapted to these tasks using the framework of
meta-learning, leading to a class of models called ecologically rational
meta-learned inference (ERMI). ERMI quantitatively explains human data better
than seven other cognitive models in two different experiments. It additionally
matches human behavior on a qualitative level: (1) it finds the same tasks
difficult that humans find difficult, (2) it becomes more reliant on an
exemplar-based strategy for assigning categories with learning, and (3) it
generalizes to unseen stimuli in a human-like way. Furthermore, we show that
ERMI's ecologically valid priors allow it to achieve state-of-the-art
performance on the OpenML-CC18 classification benchmark.
\\ ( https://arxiv.org/abs/2402.01821 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01845
Date: Fri, 2 Feb 2024 19:02:47 GMT   (855kb,D)

Title: Multi-Armed Bandits with Interference
Authors: Su Jia, Peter Frazier, Nathan Kallus
Categories: cs.LG stat.ML
\\
  Experimentation with interference poses a significant challenge in
contemporary online platforms. Prior research on experimentation with
interference has concentrated on the final output of a policy. The cumulative
performance, while equally crucial, is less well understood. To address this
gap, we introduce the problem of {\em Multi-armed Bandits with Interference}
(MABI), where the learner assigns an arm to each of $N$ experimental units over
a time horizon of $T$ rounds. The reward of each unit in each round depends on
the treatments of {\em all} units, where the influence of a unit decays in the
spatial distance between units. Furthermore, we employ a general setup wherein
the reward functions are chosen by an adversary and may vary arbitrarily across
rounds and units. We first show that switchback policies achieve an optimal
{\em expected} regret $\tilde O(\sqrt T)$ against the best fixed-arm policy.
Nonetheless, the regret (as a random variable) for any switchback policy
suffers a high variance, as it does not account for $N$. We propose a cluster
randomization policy whose regret (i) is optimal in {\em expectation} and (ii)
admits a high probability bound that vanishes in $N$.
\\ ( https://arxiv.org/abs/2402.01845 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01849
Date: Fri, 2 Feb 2024 19:04:53 GMT   (308kb,D)

Title: Capturing waste collection planning expert knowledge in a fitness
  function through preference learning
Authors: Laura Fern\'andez D\'iaz, Miriam Fern\'andez D\'iaz, Jos\'e Ram\'on
  Quevedo, Elena Monta\~n\'es
Categories: cs.LG
Journal-ref: Engineering Applications of Artificial Intelligence 2021 Volume 99
  104113
DOI: 10.1016/j.engappai.2020.104113
\\
  This paper copes with the COGERSA waste collection process. Up to now,
experts have been manually designed the process using a trial and error
mechanism. This process is not globally optimized, since it has been
progressively and locally built as council demands appear. Planning
optimization algorithms usually solve it, but they need a fitness function to
evaluate a route planning quality. The drawback is that even experts are not
able to propose one in a straightforward way due to the complexity of the
process. Hence, the goal of this paper is to build a fitness function though a
preference framework, taking advantage of the available expert knowledge and
expertise. Several key performance indicators together with preference
judgments are carefully established according to the experts for learning a
promising fitness function. Particularly, the additivity property of them makes
the task be much more affordable, since it allows to work with routes rather
than with route plannings. Besides, a feature selection analysis is performed
over such indicators, since the experts suspect of a potential existing (but
unknown) redundancy among them. The experiment results confirm this hypothesis,
since the best $C-$index ($98\%$ against around $94\%$) is reached when 6 or 8
out of 21 indicators are taken. Particularly, truck load seems to be a highly
promising key performance indicator, together to the travelled distance along
non-main roads. A comparison with other existing approaches shows that the
proposed method clearly outperforms them, since the $C-$index goes from $72\%$
or $90\%$ to $98\%$.
\\ ( https://arxiv.org/abs/2402.01849 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01857
Date: Fri, 2 Feb 2024 19:26:00 GMT   (822kb,D)

Title: Position Paper: Assessing Robustness, Privacy, and Fairness in Federated
  Learning Integrated with Foundation Models
Authors: Xi Li, Jiaqi Wang
Categories: cs.LG cs.CR
Comments: Under review
\\
  Federated Learning (FL), while a breakthrough in decentralized machine
learning, contends with significant challenges such as limited data
availability and the variability of computational resources, which can stifle
the performance and scalability of the models. The integration of Foundation
Models (FMs) into FL presents a compelling solution to these issues, with the
potential to enhance data richness and reduce computational demands through
pre-training and data augmentation. However, this incorporation introduces
novel issues in terms of robustness, privacy, and fairness, which have not been
sufficiently addressed in the existing research. We make a preliminary
investigation into this field by systematically evaluating the implications of
FM-FL integration across these dimensions. We analyze the trade-offs involved,
uncover the threats and issues introduced by this integration, and propose a
set of criteria and strategies for navigating these challenges. Furthermore, we
identify potential research directions for advancing this field, laying a
foundation for future development in creating reliable, secure, and equitable
FL systems.
\\ ( https://arxiv.org/abs/2402.01857 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01858
Date: Fri, 2 Feb 2024 19:28:33 GMT   (27688kb,D)

Title: Explaining latent representations of generative models with large
  multimodal models
Authors: Mengdan Zhu, Zhenke Liu, Bo Pan, Abhinav Angirekula, Liang Zhao
Categories: cs.LG cs.AI cs.CL cs.CV
\\
  Learning interpretable representations of data generative latent factors is
an important topic for the development of artificial intelligence. With the
rise of the large multimodal model, it can align images with text to generate
answers. In this work, we propose a framework to comprehensively explain each
latent factor in the generative models using a large multimodal model. We
further measure the uncertainty of our generated explanations, quantitatively
evaluate the performance of explanation generation among multiple large
multimodal models, and qualitatively visualize the variations of each latent
factor to learn the disentanglement effects of different generative models on
explanations. Finally, we discuss the explanatory capabilities and limitations
of state-of-the-art large multimodal models.
\\ ( https://arxiv.org/abs/2402.01858 ,  27688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01862
Date: Fri, 2 Feb 2024 19:34:46 GMT   (1967kb,D)

Title: Parametric Feature Transfer: One-shot Federated Learning with Foundation
  Models
Authors: Mahdi Beitollahi, Alex Bie, Sobhan Hemati, Leo Maxime Brunswic, Xu Li,
  Xi Chen, Guojun Zhang
Categories: cs.LG cs.AI
Comments: 20 pages, 12 figures
\\
  In one-shot federated learning (FL), clients collaboratively train a global
model in a single round of communication. Existing approaches for one-shot FL
enhance communication efficiency at the expense of diminished accuracy. This
paper introduces FedPFT (Federated Learning with Parametric Feature Transfer),
a methodology that harnesses the transferability of foundation models to
enhance both accuracy and communication efficiency in one-shot FL. The approach
involves transferring per-client parametric models (specifically, Gaussian
mixtures) of features extracted from foundation models. Subsequently, each
parametric model is employed to generate synthetic features for training a
classifier head. Experimental results on eight datasets demonstrate that FedPFT
enhances the communication-accuracy frontier in both centralized and
decentralized FL scenarios, as well as across diverse data-heterogeneity
settings such as covariate shift and task shift, with improvements of up to
20.6%. Additionally, FedPFT adheres to the data minimization principle of FL,
as clients do not send real features. We demonstrate that sending real features
is vulnerable to potent reconstruction attacks. Moreover, we show that FedPFT
is amenable to formal privacy guarantees via differential privacy,
demonstrating favourable privacy-accuracy tradeoffs.
\\ ( https://arxiv.org/abs/2402.01862 ,  1967kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01863
Date: Fri, 2 Feb 2024 19:35:05 GMT   (4564kb,D)

Title: DFML: Decentralized Federated Mutual Learning
Authors: Yasser H. Khalil, Amir H. Estiri, Mahdi Beitollahi, Nader Asadi,
  Sobhan Hemati, Xu Li, Guojun Zhang, Xi Chen
Categories: cs.LG cs.AI
\\
  In the realm of real-world devices, centralized servers in Federated Learning
(FL) present challenges including communication bottlenecks and susceptibility
to a single point of failure. Additionally, contemporary devices inherently
exhibit model and data heterogeneity. Existing work lacks a Decentralized FL
(DFL) framework capable of accommodating such heterogeneity without imposing
architectural restrictions or assuming the availability of public data. To
address these issues, we propose a Decentralized Federated Mutual Learning
(DFML) framework that is serverless, supports nonrestrictive heterogeneous
models, and avoids reliance on public data. DFML effectively handles model and
data heterogeneity through mutual learning, which distills knowledge between
clients, and cyclically varying the amount of supervision and distillation
signals. Extensive experimental results demonstrate consistent effectiveness of
DFML in both convergence speed and global accuracy, outperforming prevalent
baselines under various conditions. For example, with the CIFAR-100 dataset and
50 clients, DFML achieves a substantial increase of +17.20% and +19.95% in
global accuracy under Independent and Identically Distributed (IID) and non-IID
data shifts, respectively.
\\ ( https://arxiv.org/abs/2402.01863 ,  4564kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01865
Date: Fri, 2 Feb 2024 19:43:15 GMT   (391kb,D)

Title: What Will My Model Forget? Forecasting Forgotten Examples in Language
  Model Refinement
Authors: Xisen Jin, Xiang Ren
Categories: cs.LG cs.CL stat.ML
\\
  Language models deployed in the wild make errors. However, simply updating
the model with the corrected error instances causes catastrophic forgetting --
the updated model makes errors on instances learned during the instruction
tuning or upstream training phase. Randomly replaying upstream data yields
unsatisfactory performance and often comes with high variance and poor
controllability. To this end, we try to forecast upstream examples that will be
forgotten due to a model update for improved controllability of the replay
process and interpretability. We train forecasting models given a collection of
online learned examples and corresponding forgotten upstream pre-training
examples. We propose a partially interpretable forecasting model based on the
observation that changes in pre-softmax logit scores of pretraining examples
resemble that of online learned examples, which performs decently on BART but
fails on T5 models. We further show a black-box classifier based on inner
products of example representations achieves better forecasting performance
over a series of setups. Finally, we show that we reduce forgetting of upstream
pretraining examples by replaying examples that are forecasted to be forgotten,
demonstrating the practical utility of forecasting example forgetting.
\\ ( https://arxiv.org/abs/2402.01865 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01867
Date: Fri, 2 Feb 2024 19:45:39 GMT   (247kb,D)

Title: Leveraging Large Language Models for Structure Learning in Prompted Weak
  Supervision
Authors: Jinyan Su, Peilin Yu, Jieyu Zhang, Stephen H. Bach
Categories: cs.LG cs.CL
Comments: Accepted to IEEE International Conference on Big Data 2023
\\
  Prompted weak supervision (PromptedWS) applies pre-trained large language
models (LLMs) as the basis for labeling functions (LFs) in a weak supervision
framework to obtain large labeled datasets. We further extend the use of LLMs
in the loop to address one of the key challenges in weak supervision: learning
the statistical dependency structure among supervision sources. In this work,
we ask the LLM how similar are these prompted LFs. We propose a Structure
Refining Module, a simple yet effective first approach based on the
similarities of the prompts by taking advantage of the intrinsic structure in
the embedding space. At the core of Structure Refining Module are Labeling
Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared
to previous methods that learn the dependencies from weak labels, our method
finds the dependencies which are intrinsic to the LFs and less dependent on the
data. We show that our Structure Refining Module improves the PromptedWS
pipeline by up to 12.7 points on the benchmark tasks. We also explore the
trade-offs between efficiency and performance with comprehensive ablation
experiments and analysis. Code for this project can be found in
https://github.com/BatsResearch/su-bigdata23-code.
\\ ( https://arxiv.org/abs/2402.01867 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01868
Date: Fri, 2 Feb 2024 19:46:43 GMT   (2333kb,D)

Title: Challenges in Training PINNs: A Loss Landscape Perspective
Authors: Pratik Rathore, Weimu Lei, Zachary Frangella, Lu Lu, Madeleine Udell
Categories: cs.LG math.OC stat.ML
Comments: 30 pages (including appendices), 8 figures, 2 tables
\\
  This paper explores challenges in training Physics-Informed Neural Networks
(PINNs), emphasizing the role of the loss landscape in the training process. We
examine difficulties in minimizing the PINN loss function, particularly due to
ill-conditioning caused by differential operators in the residual term. We
compare gradient-based optimizers Adam, L-BFGS, and their combination
Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel
second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN
performance. Theoretically, our work elucidates the connection between
ill-conditioned differential operators and ill-conditioning in the PINN loss
and shows the benefits of combining first- and second-order optimization
methods. Our work presents valuable insights and more powerful optimization
strategies for training PINNs, which could improve the utility of PINNs for
solving difficult partial differential equations.
\\ ( https://arxiv.org/abs/2402.01868 ,  2333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01869
Date: Fri, 2 Feb 2024 19:47:57 GMT   (913kb,D)

Title: APIServe: Efficient API Support for Large-Language Model Inferencing
Authors: Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao Zhang, Yiying Zhang
Categories: cs.LG cs.DC
\\
  Large language models are increasingly integrated with external tools and
APIs like ChatGPT plugins to extend their capability beyond language-centric
tasks. However, today's LLM inference systems are designed for standalone LLMs.
They treat API calls as new requests, causing unnecessary recomputation of
already computed contexts, which accounts for 37-40% of total model forwarding
time. This paper presents APIServe, the first LLM inference framework targeting
API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API
calls and dedicates saved memory for serving more requests. APISERVE improves
the overall serving throughput by 1.6x and completes 2x more requests per
second compared to the state-of-the-art LLM inference systems.
\\ ( https://arxiv.org/abs/2402.01869 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01879
Date: Fri, 2 Feb 2024 20:08:11 GMT   (9712kb,D)

Title: $\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial
  Examples
Authors: Antonio Emanuele Cin\`a, Francesco Villani, Maura Pintor, Lea
  Sch\"onherr, Battista Biggio, and Marcello Pelillo
Categories: cs.LG cs.CV
Comments: Code available at
  https://github.com/Cinofix/sigma-zero-adversarial-attack
\\
  Evaluating the adversarial robustness of deep networks to gradient-based
attacks is challenging. While most attacks consider $\ell_2$- and
$\ell_\infty$-norm constraints to craft input perturbations, only a few
investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular,
$\ell_0$-norm attacks remain the least studied due to the inherent complexity
of optimizing over a non-convex and non-differentiable constraint. However,
evaluating adversarial robustness under these attacks could reveal weaknesses
otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm
attacks. In this work, we propose a novel $\ell_0$-norm attack, called
$\sigma$-zero, which leverages an ad hoc differentiable approximation of the
$\ell_0$ norm to facilitate gradient-based optimization, and an adaptive
projection operator to dynamically adjust the trade-off between loss
minimization and perturbation sparsity. Extensive evaluations using MNIST,
CIFAR10, and ImageNet datasets, involving robust and non-robust models, show
that $\sigma$-zero finds minimum $\ell_0$-norm adversarial examples without
requiring any time-consuming hyperparameter tuning, and that it outperforms all
competing sparse attacks in terms of success rate, perturbation size, and
scalability.
\\ ( https://arxiv.org/abs/2402.01879 ,  9712kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01881
Date: Fri, 2 Feb 2024 20:12:05 GMT   (2099kb,D)

Title: Large Language Model Agent for Hyper-Parameter Optimization
Authors: Siyi Liu, Chen Gao, Yong Li
Categories: cs.LG cs.AI
\\
  Hyperparameter optimization is critical in modern machine learning, requiring
expert knowledge, numerous trials, and high computational and human resources.
Despite the advancements in Automated Machine Learning (AutoML), challenges in
terms of trial efficiency, setup complexity, and interoperability still
persist. To address these issues, we introduce a novel paradigm leveraging
Large Language Models (LLMs) to automate hyperparameter optimization across
diverse machine learning tasks, which is named AgentHPO (short for LLM
Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the
task information autonomously, conducts experiments with specific
hyperparameters (HPs), and iteratively optimizes them based on historical
trials. This human-like optimization process largely reduces the number of
required trials, simplifies the setup process, and enhances interpretability
and user trust, compared to traditional AutoML methods. Extensive empirical
experiments conducted on 12 representative machine-learning tasks indicate that
AgentHPO not only matches but also often surpasses the best human trials in
terms of performance while simultaneously providing explainable results.
Further analysis sheds light on the strategies employed by the LLM in
optimizing these tasks, highlighting its effectiveness and adaptability in
various scenarios.
\\ ( https://arxiv.org/abs/2402.01881 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01886
Date: Fri, 2 Feb 2024 20:21:09 GMT   (984kb,D)

Title: Inverse Reinforcement Learning by Estimating Expertise of Demonstrators
Authors: Mark Beliaev, Ramtin Pedarsani
Categories: cs.LG cs.AI
Comments: 12 pages, 3 figures, preprint
\\
  In Imitation Learning (IL), utilizing suboptimal and heterogeneous
demonstrations presents a substantial challenge due to the varied nature of
real-world data. However, standard IL algorithms consider these datasets as
homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators.
Previous approaches to this issue typically rely on impractical assumptions
like high-quality data subsets, confidence rankings, or explicit environmental
knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by
Estimating Expertise of Demonstrators, a novel framework that overcomes these
hurdles without prior knowledge of demonstrator expertise. IRLEED enhances
existing Inverse Reinforcement Learning (IRL) algorithms by combining a general
model for demonstrator suboptimality to address reward bias and action
variance, with a Maximum Entropy IRL framework to efficiently derive the
optimal policy from diverse, suboptimal demonstrations. Experiments in both
online and offline IL settings, with simulated and human-generated data,
demonstrate IRLEED's adaptability and effectiveness, making it a versatile
solution for learning from suboptimal demonstrations.
\\ ( https://arxiv.org/abs/2402.01886 ,  984kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01902
Date: Fri, 2 Feb 2024 21:05:56 GMT   (11258kb,D)

Title: EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting
  of Honeybee Time Series
Authors: Mst. Shamima Hossain, Christos Faloutsos, Boris Baer, Hyoseung Kim,
  Vassilis J. Tsotras
Categories: cs.LG
Comments: 9 pages, 7 figure, Accepted at 2024 SIAM International Conference on
  Data Mining (SDM'24)
\\
  Honeybees are vital for pollination and food production. Among many factors,
extreme temperature (e.g., due to climate change) is particularly dangerous for
bee health. Anticipating such extremities would allow beekeepers to take early
preventive action. Thus, given sensor (temperature) time series data from
beehives, how can we find patterns and do forecasting? Forecasting is crucial
as it helps spot unexpected behavior and thus issue warnings to the beekeepers.
In that case, what are the right models for forecasting? ARIMA, RNNs, or
something else?
  We propose the EBV (Electronic Bee-Veterinarian) method, which has the
following desirable properties: (i) principled: it is based on a) diffusion
equations from physics and b) control theory for feedback-loop controllers;
(ii) effective: it works well on multiple, real-world time sequences, (iii)
explainable: it needs only a handful of parameters (e.g., bee strength) that
beekeepers can easily understand and trust, and (iv) scalable: it performs
linearly in time. We applied our method to multiple real-world time sequences,
and found that it yields accurate forecasting (up to 49% improvement in RMSE
compared to baselines), and segmentation. Specifically, discontinuities
detected by EBV mostly coincide with domain expert's opinions, showcasing our
approach's potential and practical feasibility. Moreover, EBV is scalable and
fast, taking about 20 minutes on a stock laptop for reconstructing two months
of sensor data.
\\ ( https://arxiv.org/abs/2402.01902 ,  11258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01909
Date: Fri, 2 Feb 2024 21:21:55 GMT   (380kb,D)

Title: On Catastrophic Inheritance of Large Foundation Models
Authors: Hao Chen, Bhiksha Raj, Xing Xie, Jindong Wang
Categories: cs.LG cs.AI
\\
  Large foundation models (LFMs) are claiming incredible performances. Yet
great concerns have been raised about their mythic and uninterpreted potentials
not only in machine learning, but also in various other disciplines. In this
position paper, we propose to identify a neglected issue deeply rooted in LFMs:
Catastrophic Inheritance, describing the weaknesses and limitations inherited
from biased large-scale pre-training data to behaviors of LFMs on the
downstream tasks, including samples that are corrupted, long-tailed, noisy,
out-of-distributed, to name a few. Such inheritance can potentially cause
catastrophes to downstream applications, such as bias, lack of generalization,
deteriorated performance, security vulnerability, privacy leakage, and value
misalignment. We discuss the challenges behind this issue and propose UIM, a
framework to Understand the catastrophic inheritance of LFMs from both
pre-training and downstream adaptation, Interpret the implications of
catastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims
to unite both the machine learning and social sciences communities for more
responsible and promising AI development and deployment.
\\ ( https://arxiv.org/abs/2402.01909 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01911
Date: Fri, 2 Feb 2024 21:25:46 GMT   (1506kb,D)

Title: From PEFT to DEFT: Parameter Efficient Finetuning for Reducing
  Activation Density in Transformers
Authors: Bharat Runwal, Tejaswini Pedapati, Pin-Yu Chen
Categories: cs.LG
Comments: Preprint
\\
  Pretrained Language Models (PLMs) have become the de facto starting point for
fine-tuning on downstream tasks. However, as model sizes continue to increase,
traditional fine-tuning of all parameters becomes challenging. To address this,
parameter-efficient fine-tuning (PEFT) methods have gained popularity as a
means to adapt PLMs effectively. In parallel, recent studies have revealed the
presence of activation sparsity within the intermediate outputs of the
multilayer perception (MLP) blocks in transformers. Low activation density
enables efficient model inference on sparsity-aware hardware. Building upon
this insight, in this work, we propose a novel density loss that encourages
higher activation sparsity (equivalently, lower activation density) in the
pre-trained models. We demonstrate the effectiveness of our approach by
utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter,
Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse
downstream tasks. Experiments show that our proposed method DEFT,
Density-Efficient Fine-Tuning, can reduce the activation density consistently
and up to $\boldsymbol{50.72\%}$ on RoBERTa$_\mathrm{Large}$, and $\boldsymbol
{53.19\%}$ (encoder density) and $\boldsymbol{90.60\%}$ (decoder density) on
Flan-T5$_\mathrm{XXL}$ ($\boldsymbol{11B}$) compared to PEFT using GLUE and QA
(SQuAD) benchmarks respectively while maintaining competitive performance on
downstream tasks. We also showcase that DEFT works complementary with quantized
and pruned models
\\ ( https://arxiv.org/abs/2402.01911 ,  1506kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01920
Date: Fri, 2 Feb 2024 21:45:24 GMT   (13027kb,D)

Title: Preference Poisoning Attacks on Reward Model Learning
Authors: Junlin Wu, Jiongxiao Wang, Chaowei Xiao, Chenguang Wang, Ning Zhang,
  Yevgeniy Vorobeychik
Categories: cs.LG cs.AI cs.CL
\\
  Learning utility, or reward, models from pairwise comparisons is a
fundamental component in a number of application domains. These approaches
inherently entail collecting preference information from people, with feedback
often provided anonymously. Since preferences are subjective, there is no gold
standard to compare against; yet, reliance of high-impact systems on preference
learning creates a strong motivation for malicious actors to skew data
collected in this fashion to their ends. We investigate the nature and extent
of this vulnerability systematically by considering a threat model in which an
attacker can flip a small subset of preference comparisons with the goal of
either promoting or demoting a target outcome. First, we propose two classes of
algorithmic approaches for these attacks: a principled gradient-based
framework, and several variants of rank-by-distance methods. Next, we
demonstrate the efficacy of best attacks in both these classes in successfully
achieving malicious goals on datasets from three diverse domains: autonomous
control, recommendation system, and textual prompt-response preference
learning. We find that the best attacks are often highly successful, achieving
in the most extreme case 100% success rate with only 0.3% of the data poisoned.
However, which attack is best can vary significantly across domains,
demonstrating the value of our comprehensive vulnerability analysis that
involves several classes of attack algorithms. In addition, we observe that the
simpler and more scalable rank-by-distance approaches are often competitive
with the best, and on occasion significantly outperform gradient-based methods.
Finally, we show that several state-of-the-art defenses against other classes
of poisoning attacks exhibit, at best, limited efficacy in our setting.
\\ ( https://arxiv.org/abs/2402.01920 ,  13027kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01922
Date: Fri, 2 Feb 2024 21:48:50 GMT   (2114kb,D)

Title: A General Framework for Learning from Weak Supervision
Authors: Hao Chen, Jindong Wang, Lei Feng, Xiang Li, Yidong Wang, Xing Xie,
  Masashi Sugiyama, Rita Singh, Bhiksha Raj
Categories: cs.LG cs.AI
\\
  Weakly supervised learning generally faces challenges in applicability to
various scenarios with diverse weak supervision and in scalability due to the
complexity of existing algorithms, thereby hindering the practical deployment.
This paper introduces a general framework for learning from weak supervision
(GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization
(EM) formulation, adeptly accommodating various weak supervision sources,
including instance partial labels, aggregate statistics, pairwise observations,
and unlabeled data. We further present an advanced algorithm that significantly
simplifies the EM computational demands using a Non-deterministic Finite
Automaton (NFA) along with a forward-backward algorithm, which effectively
reduces time complexity from quadratic or factorial often required in existing
solutions to linear scale. The problem of learning from arbitrary weak
supervision is therefore converted to the NFA modeling of them. GLWS not only
enhances the scalability of machine learning models but also demonstrates
superior performance and versatility across 11 weak supervision scenarios. We
hope our work paves the way for further advancements and practical deployment
in this field.
\\ ( https://arxiv.org/abs/2402.01922 ,  2114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01928
Date: Fri, 2 Feb 2024 21:56:58 GMT   (44kb)

Title: Robust Counterfactual Explanations in Machine Learning: A Survey
Authors: Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni
Categories: cs.LG cs.AI
\\
  Counterfactual explanations (CEs) are advocated as being ideally suited to
providing algorithmic recourse for subjects affected by the predictions of
machine learning models. While CEs can be beneficial to affected individuals,
recent work has exposed severe issues related to the robustness of
state-of-the-art methods for obtaining CEs. Since a lack of robustness may
compromise the validity of CEs, techniques to mitigate this risk are in order.
In this survey, we review works in the rapidly growing area of robust CEs and
perform an in-depth analysis of the forms of robustness they consider. We also
discuss existing solutions and their limitations, providing a solid foundation
for future developments.
\\ ( https://arxiv.org/abs/2402.01928 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01929
Date: Fri, 2 Feb 2024 21:57:58 GMT   (2411kb,D)

Title: Sample, estimate, aggregate: A recipe for causal discovery foundation
  models
Authors: Menghua Wu, Yujia Bao, Regina Barzilay, Tommi Jaakkola
Categories: cs.LG
Comments: Preprint. Under review
\\
  Causal discovery, the task of inferring causal structure from data, promises
to accelerate scientific research, inform policy making, and more. However, the
per-dataset nature of existing causal discovery algorithms renders them slow,
data hungry, and brittle. Inspired by foundation models, we propose a causal
discovery framework where a deep learning model is pretrained to resolve
predictions from classical discovery algorithms run over smaller subsets of
variables. This method is enabled by the observations that the outputs from
classical algorithms are fast to compute for small problems, informative of
(marginal) data structure, and their structure outputs as objects remain
comparable across datasets. Our method achieves state-of-the-art performance on
synthetic and realistic datasets, generalizes to data generating mechanisms not
seen during training, and offers inference speeds that are orders of magnitude
faster than existing models.
\\ ( https://arxiv.org/abs/2402.01929 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01931
Date: Fri, 2 Feb 2024 22:01:27 GMT   (84kb,D)

Title: Digits micro-model for accurate and secure transactions
Authors: Chirag Chhablani, Nikhita Sharma, Jordan Hosier, and Vijay K. Gurbani
Categories: cs.LG cs.CL cs.SD
Comments: 7 pages, 1 figure, 5 tables
\\
  Automatic Speech Recognition (ASR) systems are used in the financial domain
to enhance the caller experience by enabling natural language understanding and
facilitating efficient and intuitive interactions. Increasing use of ASR
systems requires that such systems exhibit very low error rates. The
predominant ASR models to collect numeric data are large, general-purpose
commercial models -- Google Speech-to-text (STT), or Amazon Transcribe -- or
open source (OpenAI's Whisper). Such ASR models are trained on hundreds of
thousands of hours of audio data and require considerable resources to run.
Despite recent progress large speech recognition models, we highlight the
potential of smaller, specialized "micro" models. Such light models can be
trained perform well on number recognition specific tasks, competing with
general models like Whisper or Google STT while using less than 80 minutes of
training time and occupying at least an order of less memory resources. Also,
unlike larger speech recognition models, micro-models are trained on carefully
selected and curated datasets, which makes them highly accurate, agile, and
easy to retrain, while using low compute resources. We present our work on
creating micro models for multi-digit number recognition that handle diverse
speaking styles reflecting real-world pronunciation patterns. Our work
contributes to domain-specific ASR models, improving digit recognition
accuracy, and privacy of data. An added advantage, their low resource
consumption allows them to be hosted on-premise, keeping private data local
instead uploading to an external cloud. Our results indicate that our
micro-model makes less errors than the best-of-breed commercial or open-source
ASRs in recognizing digits (1.8% error rate of our best micro-model versus 5.8%
error rate of Whisper), and has a low memory footprint (0.66 GB VRAM for our
model versus 11 GB VRAM for Whisper).
\\ ( https://arxiv.org/abs/2402.01931 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01943
Date: Fri, 2 Feb 2024 22:39:50 GMT   (1847kb,D)

Title: Precedence-Constrained Winter Value for Effective Graph Data Valuation
Authors: Hongliang Chi, Jin Wei, Charu Aggarwal, Yao Ma
Categories: cs.LG
Comments: 17 pages in total
\\
  Data valuation is essential for quantifying data's worth, aiding in assessing
data quality and determining fair compensation. While existing data valuation
methods have proven effective in evaluating the value of Euclidean data, they
face limitations when applied to the increasingly popular graph-structured
data. Particularly, graph data valuation introduces unique challenges,
primarily stemming from the intricate dependencies among nodes and the
exponential growth in value estimation costs. To address the challenging
problem of graph data valuation, we put forth an innovative solution,
Precedence-Constrained Winter (PC-Winter) Value, to account for the complex
graph structure. Furthermore, we develop a variety of strategies to address the
computational challenges and enable efficient approximation of PC-Winter.
Extensive experiments demonstrate the effectiveness of PC-Winter across diverse
datasets and tasks.
\\ ( https://arxiv.org/abs/2402.01943 ,  1847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01955
Date: Fri, 2 Feb 2024 23:26:09 GMT   (1970kb,D)

Title: OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival
  Analysis
Authors: Lilian W. Bialokozowicz and Hoang M. Le and Tristan Sylvain, Peter A.
  I. Forsyth, Vineel Nagisetty, Greg Mori
Categories: cs.LG cs.AI math.FA
MSC-class: 68W25 (Primary), 65Z05 (Secondary)
ACM-class: I.2.0; J.3
\\
  This paper introduces the Orthogonal Polynomials Quadrature Algorithm for
Survival Analysis (OPSurv), a new method providing time-continuous functional
outputs for both single and competing risks scenarios in survival analysis.
OPSurv utilizes the initial zero condition of the Cumulative Incidence function
and a unique decomposition of probability densities using orthogonal
polynomials, allowing it to learn functional approximation coefficients for
each risk event and construct Cumulative Incidence Function estimates via
Gauss--Legendre quadrature. This approach effectively counters overfitting,
particularly in competing risks scenarios, enhancing model expressiveness and
control. The paper further details empirical validations and theoretical
justifications of OPSurv, highlighting its robust performance as an advancement
in survival analysis with competing risks.
\\ ( https://arxiv.org/abs/2402.01955 ,  1970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01960
Date: Fri, 2 Feb 2024 23:43:28 GMT   (2751kb,D)

Title: Calibrated Uncertainty Quantification for Operator Learning via
  Conformal Prediction
Authors: Ziqi Ma, Kamyar Azizzadenesheli, Anima Anandkumar
Categories: cs.LG
Comments: 14 pages, 7 figures
\\
  Operator learning has been increasingly adopted in scientific and engineering
applications, many of which require calibrated uncertainty quantification.
Since the output of operator learning is a continuous function, quantifying
uncertainty simultaneously at all points in the domain is challenging. Current
methods consider calibration at a single point or over one scalar function or
make strong assumptions such as Gaussianity. We propose a risk-controlling
quantile neural operator, a distribution-free, finite-sample functional
calibration conformal prediction method. We provide a theoretical calibration
guarantee on the coverage rate, defined as the expected percentage of points on
the function domain whose true value lies within the predicted uncertainty
ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure
prediction tasks validate our theoretical results, demonstrating calibrated
coverage and efficient uncertainty bands outperforming baseline methods. In
particular, on the 3D problem, our method is the only one that meets the target
calibration percentage (percentage of test samples for which the uncertainty
estimates are calibrated) of 98\%.
\\ ( https://arxiv.org/abs/2402.01960 ,  2751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01963
Date: Sat, 3 Feb 2024 00:11:29 GMT   (1406kb,D)

Title: Improving Large-Scale k-Nearest Neighbor Text Categorization with Label
  Autoencoders
Authors: Francisco J. Ribadas-Pena, Shuyuan Cao, V\'ictor M. Darriba Bilbao
Categories: cs.LG cs.CL cs.IR
Comments: 22 pages, 4 figures
MSC-class: 68T50, 68T07
ACM-class: I.2.6; I.2.7; H.3.1
Journal-ref: Mathematics 2022, 10(16), 2867
DOI: 10.3390/math10162867
\\
  In this paper, we introduce a multi-label lazy learning approach to deal with
automatic semantic indexing in large document collections in the presence of
complex and structured label vocabularies with high inter-label correlation.
The proposed method is an evolution of the traditional k-Nearest Neighbors
algorithm which uses a large autoencoder trained to map the large label space
to a reduced size latent space and to regenerate the predicted labels from this
latent space. We have evaluated our proposal in a large portion of the MEDLINE
biomedical document collection which uses the Medical Subject Headings (MeSH)
thesaurus as a controlled vocabulary. In our experiments we propose and
evaluate several document representation approaches and different label
autoencoder configurations.
\\ ( https://arxiv.org/abs/2402.01963 ,  1406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01964
Date: Sat, 3 Feb 2024 00:12:36 GMT   (1510kb,D)

Title: No Need to Look Back: An Efficient and Scalable Approach for Temporal
  Network Representation Learning
Authors: Yuhong Luo and Pan Li
Categories: cs.LG
\\
  Temporal graph representation learning (TGRL) is crucial for modeling
complex, dynamic systems in real-world networks. Traditional TGRL methods,
though effective, suffer from high computational demands and inference latency.
This is mainly induced by their inefficient sampling of temporal neighbors by
backtracking the interaction history of each node when making model inference.
This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB).
NLB employs a "forward recent sampling" strategy, which bypasses the need for
backtracking historical interactions. This strategy is implemented using a
GPU-executable size-constrained hash table for each node, recording
down-sampled recent interactions, which enables rapid response to queries with
minimal inference latency. The maintenance of this hash table is highly
efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing,
maximizing programmability, parallelism, and power efficiency. Empirical
evaluations demonstrate that NLB matches or surpasses state-of-the-art methods
in accuracy for link prediction and node classification across six real-world
datasets. Significantly, it is 1.32-4.40 $\times$ faster in training, 1.2-7.94
$\times$ more energy efficient, and 1.97-5.02 $\times$ more effective in
reducing inference latency compared to the most competitive baselines. The link
to the code: https://github.com/Graph-COM/NLB.
\\ ( https://arxiv.org/abs/2402.01964 ,  1510kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01965
Date: Sat, 3 Feb 2024 00:20:25 GMT   (1429kb,D)

Title: Analyzing Neural Network-Based Generative Diffusion Models through
  Convex Optimization
Authors: Fangzhao Zhang, Mert Pilanci
Categories: cs.LG math.OC
\\
  Diffusion models are becoming widely used in state-of-the-art image, video
and audio generation. Score-based diffusion models stand out among these
methods, necessitating the estimation of score function of the input data
distribution. In this study, we present a theoretical framework to analyze
two-layer neural network-based diffusion models by reframing score matching and
denoising score matching as convex optimization. Though existing diffusion
theory is mainly asymptotic, we characterize the exact predicted score function
and establish the convergence result for neural network-based diffusion models
with finite data. This work contributes to understanding what neural
network-based diffusion model learns in non-asymptotic settings.
\\ ( https://arxiv.org/abs/2402.01965 ,  1429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01969
Date: Sat, 3 Feb 2024 00:38:08 GMT   (4647kb,D)

Title: Simulation-Enhanced Data Augmentation for Machine Learning Pathloss
  Prediction
Authors: Ahmed P. Mohamed, Byunghyun Lee, Yaguang Zhang, Max Hollingsworth, C.
  Robert Anderson, James V. Krogmeier, David J. Love
Categories: cs.LG eess.SP
Comments: 6 pages, 5 figures, Accepted at ICC 2024
\\
  Machine learning (ML) offers a promising solution to pathloss prediction.
However, its effectiveness can be degraded by the limited availability of data.
To alleviate these challenges, this paper introduces a novel
simulation-enhanced data augmentation method for ML pathloss prediction. Our
method integrates synthetic data generated from a cellular coverage simulator
and independently collected real-world datasets. These datasets were collected
through an extensive measurement campaign in different environments, including
farms, hilly terrains, and residential areas. This comprehensive data
collection provides vital ground truth for model training. A set of channel
features was engineered, including geographical attributes derived from LiDAR
datasets. These features were then used to train our prediction model,
incorporating the highly efficient and robust gradient boosting ML algorithm,
CatBoost. The integration of synthetic data, as demonstrated in our study,
significantly improves the generalizability of the model in different
environments, achieving a remarkable improvement of approximately 12dB in terms
of mean absolute error for the best-case scenario. Moreover, our analysis
reveals that even a small fraction of measurements added to the simulation
training set, with proper data balance, can significantly enhance the model's
performance.
\\ ( https://arxiv.org/abs/2402.01969 ,  4647kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01975
Date: Sat, 3 Feb 2024 00:58:41 GMT   (2948kb,D)

Title: Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks
Authors: Duy M. H. Nguyen, Nina Lukashina, Tai Nguyen, An T. Le, TrungTin
  Nguyen, Nhat Ho, Jan Peters, Daniel Sonntag, Viktor Zaverkin, Mathias Niepert
Categories: cs.LG
Comments: First version
\\
  A molecule's 2D representation consists of its atoms, their attributes, and
the molecule's covalent bonds. A 3D (geometric) representation of a molecule is
called a conformer and consists of its atom types and Cartesian coordinates.
Every conformer has a potential energy, and the lower this energy, the more
likely it occurs in nature. Most existing machine learning methods for
molecular property prediction consider either 2D molecular graphs or 3D
conformer structure representations in isolation. Inspired by recent work on
using ensembles of conformers in conjunction with 2D graph representations, we
propose E(3)-invariant molecular conformer aggregation networks. The method
integrates a molecule's 2D representation with that of multiple of its
conformers. Contrary to prior work, we propose a novel 2D--3D aggregation
mechanism based on a differentiable solver for the \emph{Fused
Gromov-Wasserstein Barycenter} problem and the use of an efficient online
conformer generation method based on distance geometry. We show that the
proposed aggregation mechanism is E(3) invariant and provides an efficient GPU
implementation. Moreover, we demonstrate that the aggregation mechanism helps
to outperform state-of-the-art property prediction methods on established
datasets significantly.
\\ ( https://arxiv.org/abs/2402.01975 ,  2948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01987
Date: Sat, 3 Feb 2024 02:13:08 GMT   (551kb,D)

Title: Online Transfer Learning for RSV Case Detection
Authors: Yiming Sun, Yuhe Gao, Runxue Bao, Gregory F. Cooper, Jessi Espino,
  Harry Hochheiser, Marian G. Michaels, John M. Aronis, Ye Ye
Categories: cs.LG cs.AI
Comments: 10 pages, 4 figures
\\
  Transfer learning has become a pivotal technique in machine learning,
renowned for its effectiveness in various real-world applications. However, a
significant challenge arises when applying this approach to sequential
epidemiological data, often characterized by a scarcity of labeled information.
To address this challenge, we introduce Predictive Volume-Adaptive Weighting
(PVAW), a novel online multi-source transfer learning method. PVAW innovatively
implements a dynamic weighting mechanism within an ensemble model, allowing for
the automatic adjustment of weights based on the relevance and contribution of
each source and target model. We demonstrate the effectiveness of PVAW through
its application in analyzing Respiratory Syncytial Virus (RSV) data, collected
over multiple seasons at the University of Pittsburgh Medical Center. Our
method showcases significant improvements in model performance over existing
baselines, highlighting the potential of online transfer learning in handling
complex, sequential data. This study not only underscores the adaptability and
sophistication of transfer learning in healthcare but also sets a new direction
for future research in creating advanced predictive models.
\\ ( https://arxiv.org/abs/2402.01987 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01995
Date: Sat, 3 Feb 2024 02:36:59 GMT   (5767kb,D)

Title: Online Uniform Risk Times Sampling: First Approximation Algorithms,
  Learning Augmentation with Full Confidence Interval Integration
Authors: Xueqing Liu, Kyra Gan, Esmaeil Keyvanshokooh, Susan Murphy
Categories: cs.LG math.OC
\\
  In digital health, the strategy of allocating a limited treatment budget
across available risk times is crucial to reduce user fatigue. This strategy,
however, encounters a significant obstacle due to the unknown actual number of
risk times, a factor not adequately addressed by existing methods lacking
theoretical guarantees. This paper introduces, for the first time, the online
uniform risk times sampling problem within the approximation algorithm
framework. We propose two online approximation algorithms for this problem, one
with and one without learning augmentation, and provide rigorous theoretical
performance guarantees for them using competitive ratio analysis. We assess the
performance of our algorithms using both synthetic experiments and a real-world
case study on HeartSteps mobile applications.
\\ ( https://arxiv.org/abs/2402.01995 ,  5767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01999
Date: Sat, 3 Feb 2024 02:42:53 GMT   (11605kb,D)

Title: A Novel Hyperdimensional Computing Framework for Online Time Series
  Forecasting on the Edge
Authors: Mohamed Mejri, Chandramouli Amarnath, Abhijit Chatterjee
Categories: cs.LG cs.AI
\\
  In recent years, both online and offline deep learning models have been
developed for time series forecasting. However, offline deep forecasting models
fail to adapt effectively to changes in time-series data, while online deep
forecasting models are often expensive and have complex training procedures. In
this paper, we reframe the online nonlinear time-series forecasting problem as
one of linear hyperdimensional time-series forecasting. Nonlinear
low-dimensional time-series data is mapped to high-dimensional
(hyperdimensional) spaces for linear hyperdimensional prediction, allowing
fast, efficient and lightweight online time-series forecasting. Our framework,
TSF-HD, adapts to time-series distribution shifts using a novel co-training
framework for its hyperdimensional mapping and its linear hyperdimensional
predictor. TSF-HD is shown to outperform the state of the art, while having
reduced inference latency, for both short-term and long-term time series
forecasting. Our code is publicly available at
http://github.com/tsfhd2024/tsf-hd.git
\\ ( https://arxiv.org/abs/2402.01999 ,  11605kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02000
Date: Sat, 3 Feb 2024 02:50:51 GMT   (626kb,D)

Title: A Survey on Graph Condensation
Authors: Hongjia Xu, Liangliang Zhang, Yao Ma, Sheng Zhou, Zhuonan Zheng, Bu
  Jiajun
Categories: cs.LG
\\
  Analytics on large-scale graphs have posed significant challenges to
computational efficiency and resource requirements. Recently, Graph
condensation (GC) has emerged as a solution to address challenges arising from
the escalating volume of graph data. The motivation of GC is to reduce the
scale of large graphs to smaller ones while preserving essential information
for downstream tasks. For a better understanding of GC and to distinguish it
from other related topics, we present a formal definition of GC and establish a
taxonomy that systematically categorizes existing methods into three types
based on its objective, and classify the formulations to generate the condensed
graphs into two categories as modifying the original graphs or synthetic
completely new ones. Moreover, our survey includes a comprehensive analysis of
datasets and evaluation metrics in this field. Finally, we conclude by
addressing challenges and limitations, outlining future directions, and
offering concise guidelines to inspire future research in this field.
\\ ( https://arxiv.org/abs/2402.02000 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02005
Date: Sat, 3 Feb 2024 03:17:44 GMT   (128kb,D)

Title: Topology-Informed Graph Transformer
Authors: Yun Young Choi, Sun Woo Park, Minho Lee, Youngho Woo
Categories: cs.LG
\\
  Transformers have revolutionized performance in Natural Language Processing
and Vision, paving the way for their integration with Graph Neural Networks
(GNNs). One key challenge in enhancing graph transformers is strengthening the
discriminative power of distinguishing isomorphisms of graphs, which plays a
crucial role in boosting their predictive performances. To address this
challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel
transformer enhancing both discriminative power in detecting graph isomorphisms
and the overall performance of Graph Transformers. TIGT consists of four
components: A topological positional embedding layer using non-isomorphic
universal covers based on cyclic subgraphs of graphs to ensure unique graph
representation: A dual-path message-passing layer to explicitly encode
topological characteristics throughout the encoder layers: A global attention
mechanism: And a graph information layer to recalibrate channel-wise graph
features for better feature representation. TIGT outperforms previous Graph
Transformers in classifying synthetic dataset aimed at distinguishing
isomorphism classes of graphs. Additionally, mathematical analysis and
empirical evaluations highlight our model's competitive edge over
state-of-the-art Graph Transformers across various benchmark datasets.
\\ ( https://arxiv.org/abs/2402.02005 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02006
Date: Sat, 3 Feb 2024 03:23:08 GMT   (1745kb,D)

Title: PresAIse, An Enterprises Prescriptive AI Solution
Authors: Wei Sun, Scott McFaddin, Linh Ha Tran, Shivaram Subramanian, Kristjan
  Greenewald, Yeshi Tenzin, Zack Xue, Youssef Drissi, Markus Ettl
Categories: cs.LG
Comments: 14 pages
\\
  Prescriptive AI represents a transformative shift in decision-making,
offering causal insights and actionable recommendations. Despite its huge
potential, enterprise adoption often faces several challenges. The first
challenge is caused by the limitations of observational data for accurate
causal inference which is typically a prerequisite for good decision-making.
The second pertains to the interpretability of recommendations, which is
crucial for enterprise decision-making settings. The third challenge is the
silos between data scientists and business users, hindering effective
collaboration. This paper outlines an initiative from IBM Research, aiming to
address some of these challenges by offering a suite of prescriptive AI
solutions. Leveraging insights from various research papers, the solution suite
includes scalable causal inference methods, interpretable decision-making
approaches, and the integration of large language models (LLMs) to bridge
communication gaps via a conversation agent. A proof-of-concept, PresAIse,
demonstrates the solutions' potential by enabling non-ML experts to interact
with prescriptive AI models via a natural language interface, democratizing
advanced analytics for strategic decision-making.
\\ ( https://arxiv.org/abs/2402.02006 ,  1745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02007
Date: Sat, 3 Feb 2024 03:43:04 GMT   (19486kb,D)

Title: Understanding Time Series Anomaly State Detection through One-Class
  Classification
Authors: Hanxu Zhou, Yuan Zhang, Guangjie Leng, Ruofan Wang, Zhi-Qin John Xu
Categories: cs.LG
\\
  For a long time, research on time series anomaly detection has mainly focused
on finding outliers within a given time series. Admittedly, this is consistent
with some practical problems, but in other practical application scenarios,
people are concerned about: assuming a standard time series is given, how to
judge whether another test time series deviates from the standard time series,
which is more similar to the problem discussed in one-class classification
(OCC). Therefore, in this article, we try to re-understand and define the time
series anomaly detection problem through OCC, which we call 'time series
anomaly state detection problem'. We first use stochastic processes and
hypothesis testing to strictly define the 'time series anomaly state detection
problem', and its corresponding anomalies. Then, we use the time series
classification dataset to construct an artificial dataset corresponding to the
problem. We compile 38 anomaly detection algorithms and correct some of the
algorithms to adapt to handle this problem. Finally, through a large number of
experiments, we fairly compare the actual performance of various time series
anomaly detection algorithms, providing insights and directions for future
research by researchers.
\\ ( https://arxiv.org/abs/2402.02007 ,  19486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02009
Date: Sat, 3 Feb 2024 03:46:14 GMT   (249kb,D)

Title: Robust Multi-Task Learning with Excess Risks
Authors: Yifei He, Shiji Zhou, Guojun Zhang, Hyokun Yun, Yi Xu, Belinda Zeng,
  Trishul Chilimbi, Han Zhao
Categories: cs.LG
\\
  Multi-task learning (MTL) considers learning a joint model for multiple tasks
by optimizing a convex combination of all task losses. To solve the
optimization problem, existing methods use an adaptive weight updating scheme,
where task weights are dynamically adjusted based on their respective losses to
prioritize difficult tasks. However, these algorithms face a great challenge
whenever label noise is present, in which case excessive weights tend to be
assigned to noisy tasks that have relatively large Bayes optimal errors,
thereby overshadowing other tasks and causing performance to drop across the
board. To overcome this limitation, we propose Multi-Task Learning with Excess
Risks (ExcessMTL), an excess risk-based task balancing method that updates the
task weights by their distances to convergence instead. Intuitively, ExcessMTL
assigns higher weights to worse-trained tasks that are further from
convergence. To estimate the excess risks, we develop an efficient and accurate
method with Taylor approximation. Theoretically, we show that our proposed
algorithm achieves convergence guarantees and Pareto stationarity. Empirically,
we evaluate our algorithm on various MTL benchmarks and demonstrate its
superior performance over existing methods in the presence of label noise.
\\ ( https://arxiv.org/abs/2402.02009 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02010
Date: Sat, 3 Feb 2024 03:50:18 GMT   (1750kb,D)

Title: GenFormer: A Deep-Learning-Based Approach for Generating Multivariate
  Stochastic Processes
Authors: Haoran Zhao, Wayne Isaac Tan Uy
Categories: cs.LG stat.ML
\\
  Stochastic generators are essential to produce synthetic realizations that
preserve target statistical properties. We propose GenFormer, a stochastic
generator for spatio-temporal multivariate stochastic processes. It is
constructed using a Transformer-based deep learning model that learns a mapping
between a Markov state sequence and time series values. The synthetic data
generated by the GenFormer model preserves the target marginal distributions
and approximately captures other desired statistical properties even in
challenging applications involving a large number of spatial locations and a
long simulation horizon. The GenFormer model is applied to simulate synthetic
wind speed data at various stations in Florida to calculate exceedance
probabilities for risk management.
\\ ( https://arxiv.org/abs/2402.02010 ,  1750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02017
Date: Sat, 3 Feb 2024 04:17:09 GMT   (2271kb,D)

Title: Value-Aided Conditional Supervised Learning for Offline RL
Authors: Jeonghye Kim, Suyoung Lee, Woojun Kim, Youngchul Sung
Categories: cs.LG
\\
  Offline reinforcement learning (RL) has seen notable advancements through
return-conditioned supervised learning (RCSL) and value-based methods, yet each
approach comes with its own set of practical challenges. Addressing these, we
propose Value-Aided Conditional Supervised Learning (VCS), a method that
effectively synergizes the stability of RCSL with the stitching ability of
value-based methods. Based on the Neural Tangent Kernel analysis to discern
instances where value function may not lead to stable stitching, VCS injects
the value aid into the RCSL's loss function dynamically according to the
trajectory return. Our empirical studies reveal that VCS not only significantly
outperforms both RCSL and value-based methods but also consistently achieves,
or often surpasses, the highest trajectory returns across diverse offline RL
benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the
limits of what can be achieved and fostering further innovations.
\\ ( https://arxiv.org/abs/2402.02017 ,  2271kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02018
Date: Sat, 3 Feb 2024 04:21:07 GMT   (371kb,D)

Title: Position Paper: The Landscape and Challenges of HPC Research and LLMs
Authors: Le Chen, Nesreen K. Ahmed, Akash Dutta, Arijit Bhattacharjee, Sixing
  Yu, Quazi Ishtiaque Mahmud, Waqwoya Abebe, Hung Phan, Aishwarya Sarkar,
  Branden Butler, Niranjan Hasabnis, Gal Oren, Vy A. Vo, Juan Pablo Munoz,
  Theodore L. Willke, Tim Mattson, Ali Jannesari
Categories: cs.LG
\\
  Recently, language models (LMs), especially large language models (LLMs),
have revolutionized the field of deep learning. Both encoder-decoder models and
prompt-based techniques have shown immense potential for natural language
processing and code-based tasks. Over the past several years, many research
labs and institutions have invested heavily in high-performance computing,
approaching or breaching exascale performance levels. In this paper, we posit
that adapting and utilizing such language model-based techniques for tasks in
high-performance computing (HPC) would be very beneficial. This study presents
our reasoning behind the aforementioned position and highlights how existing
ideas can be improved and adapted for HPC tasks.
\\ ( https://arxiv.org/abs/2402.02018 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02021
Date: Sat, 3 Feb 2024 04:27:26 GMT   (2923kb,D)

Title: Transfer Learning in ECG Diagnosis: Is It Effective?
Authors: Cuong V. Nguyen and Cuong D.Do
Categories: cs.LG cs.CV
\\
  The adoption of deep learning in ECG diagnosis is often hindered by the
scarcity of large, well-labeled datasets in real-world scenarios, leading to
the use of transfer learning to leverage features learned from larger datasets.
Yet the prevailing assumption that transfer learning consistently outperforms
training from scratch has never been systematically validated. In this study,
we conduct the first extensive empirical study on the effectiveness of transfer
learning in multi-label ECG classification, by investigating comparing the
fine-tuning performance with that of training from scratch, covering a variety
of ECG datasets and deep neural networks. We confirm that fine-tuning is the
preferable choice for small downstream datasets; however, when the dataset is
sufficiently large, training from scratch can achieve comparable performance,
albeit requiring a longer training time to catch up. Furthermore, we find that
transfer learning exhibits better compatibility with convolutional neural
networks than with recurrent neural networks, which are the two most prevalent
architectures for time-series ECG applications. Our results underscore the
importance of transfer learning in ECG diagnosis, yet depending on the amount
of available data, researchers may opt not to use it, considering the
non-negligible cost associated with pre-training.
\\ ( https://arxiv.org/abs/2402.02021 ,  2923kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02023
Date: Sat, 3 Feb 2024 04:32:34 GMT   (8393kb,D)

Title: Self-Supervised Contrastive Forecasting
Authors: Junwoo Park, Daehoon Gwak, Jaegul Choo, Edward Choi
Categories: cs.LG cs.AI
Comments: Accepted at International Conference on Learning Representations
  (ICLR) 2024
\\
  Long-term forecasting presents unique challenges due to the time and memory
complexity of handling long sequences. Existing methods, which rely on sliding
windows to process long sequences, struggle to effectively capture long-term
variations that are partially caught within the short window (i.e.,
outer-window variations). In this paper, we introduce a novel approach that
overcomes this limitation by employing contrastive learning and enhanced
decomposition architecture, specifically designed to focus on long-term
variations. To this end, our contrastive loss incorporates global
autocorrelation held in the whole time series, which facilitates the
construction of positive and negative pairs in a self-supervised manner. When
combined with our decomposition networks, our contrastive learning
significantly improves long-term forecasting performance. Extensive experiments
demonstrate that our approach outperforms 14 baseline models in multiple
experiments over nine long-term benchmarks, especially in challenging scenarios
that require a significantly long output for forecasting. Source code is
available at
https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.
\\ ( https://arxiv.org/abs/2402.02023 ,  8393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02025
Date: Sat, 3 Feb 2024 04:40:31 GMT   (34kb)

Title: A Survey of Constraint Formulations in Safe Reinforcement Learning
Authors: Akifumi Wachi, Xun Shen, Yanan Sui
Categories: cs.LG cs.AI
\\
  Ensuring safety is critical when applying reinforcement learning (RL) to
real-world problems. Consequently, safe RL emerges as a fundamental and
powerful paradigm for safely optimizing an agent's policy from experimental
data. A popular safe RL approach is based on a constrained criterion, which
solves the problem of maximizing expected cumulative reward under safety
constraints. Though there has been recently a surge of such attempts to achieve
safety in RL, a systematic understanding of the field is difficult due to 1)
the diversity of constraint representations and 2) little discussion of their
interrelations. To address this knowledge gap, we provide a comprehensive
review of representative constraint formulations, along with a curated
selection of algorithms specifically designed for each formulation.
Furthermore, we elucidate the theoretical underpinnings that reveal the
mathematical mutual relations among common problem formulations. We conclude
with a discussion of the current state and future directions of safe
reinforcement learning research.
\\ ( https://arxiv.org/abs/2402.02025 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02028
Date: Sat, 3 Feb 2024 04:48:47 GMT   (148kb,D)

Title: Unlearnable Examples For Time Series
Authors: Yujing Jiang, Xingjun Ma, Sarah Monazam Erfani, James Bailey
Categories: cs.LG
\\
  Unlearnable examples (UEs) refer to training samples modified to be
unlearnable to Deep Neural Networks (DNNs). These examples are usually
generated by adding error-minimizing noises that can fool a DNN model into
believing that there is nothing (no error) to learn from the data. The concept
of UE has been proposed as a countermeasure against unauthorized data
exploitation on personal data. While UE has been extensively studied on images,
it is unclear how to craft effective UEs for time series data. In this work, we
introduce the first UE generation method to protect time series data from
unauthorized training by deep learning models. To this end, we propose a new
form of error-minimizing noise that can be \emph{selectively} applied to
specific segments of time series, rendering them unlearnable to DNN models
while remaining imperceptible to human observers. Through extensive experiments
on a wide range of time series datasets, we demonstrate that the proposed UE
generation method is effective in both classification and generation tasks. It
can protect time series data against unauthorized exploitation, while
preserving their utility for legitimate usage, thereby contributing to the
development of secure and trustworthy machine learning systems.
\\ ( https://arxiv.org/abs/2402.02028 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02031
Date: Sat, 3 Feb 2024 05:05:26 GMT   (38935kb,D)

Title: Multi-fidelity physics constrained neural networks for dynamical systems
Authors: Hao Zhou, Sibo Cheng, Rossella Arcucci
Categories: cs.LG physics.flu-dyn
Journal-ref: Zhou H, Cheng S, Arcucci R. Multi-fidelity physics constrained
  neural networks for dynamical systems. Computer Methods in Applied Mechanics
  and Engineering. 2024 Feb 15;420:116758
DOI: 10.1016/j.cma.2024.116758
\\
  Physics-constrained neural networks are commonly employed to enhance
prediction robustness compared to purely data-driven models, achieved through
the inclusion of physical constraint losses during the model training process.
However, one of the major challenges of physics-constrained neural networks
consists of the training complexity especially for high-dimensional systems. In
fact, conventional physics-constrained models rely on singular-fidelity data
necessitating the assessment of physical constraints within high-dimensional
fields, which introduces computational difficulties. Furthermore, due to the
fixed input size of the neural networks, employing multi-fidelity training data
can also be cumbersome. In this paper, we propose the Multi-Scale
Physics-Constrained Neural Network (MSPCNN), which offers a novel methodology
for incorporating data with different levels of fidelity into a unified latent
space through a customised multi-fidelity autoencoder. Additionally, multiple
decoders are concurrently trained to map latent representations of inputs into
various fidelity physical spaces. As a result, during the training of
predictive models, physical constraints can be evaluated within low-fidelity
spaces, yielding a trade-off between training efficiency and accuracy. In
addition, unlike conventional methods, MSPCNN also manages to employ
multi-fidelity data to train the predictive model. We assess the performance of
MSPCNN in two fluid dynamics problems, namely a two-dimensional Burgers' system
and a shallow water system. Numerical results clearly demonstrate the
enhancement of prediction accuracy and noise robustness when introducing
physical constraints in low-fidelity fields. On the other hand, as expected,
the training complexity can be significantly reduced by computing physical
constraint loss in the low-fidelity field rather than the high-fidelity one.
\\ ( https://arxiv.org/abs/2402.02031 ,  38935kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02032
Date: Sat, 3 Feb 2024 05:13:09 GMT   (259kb,D)

Title: RobustTSF: Towards Theory and Design of Robust Time Series Forecasting
  with Anomalies
Authors: Hao Cheng, Qingsong Wen, Yang Liu, Liang Sun
Categories: cs.LG
Comments: Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)
\\
  Time series forecasting is an important and forefront task in many real-world
applications. However, most of time series forecasting techniques assume that
the training data is clean without anomalies. This assumption is unrealistic
since the collected time series data can be contaminated in practice. The
forecasting model will be inferior if it is directly trained by time series
with anomalies. Thus it is essential to develop methods to automatically learn
a robust forecasting model from the contaminated data. In this paper, we first
statistically define three types of anomalies, then theoretically and
experimentally analyze the loss robustness and sample robustness when these
anomalies exist. Based on our analyses, we propose a simple and efficient
algorithm to learn a robust forecasting model. Extensive experiments show that
our method is highly robust and outperforms all existing approaches. The code
is available at https://github.com/haochenglouis/RobustTSF.
\\ ( https://arxiv.org/abs/2402.02032 ,  259kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02036
Date: Sat, 3 Feb 2024 05:19:02 GMT   (6683kb,D)

Title: Interpreting Graph Neural Networks with In-Distributed Proxies
Authors: Zhuomin Chen, Jiaxing Zhang, Jingchao Ni, Xiaoting Li, Yuchen Bian, Md
  Mezbahul Islam, Ananda Mohan Mondal, Hua Wei, Dongsheng Luo
Categories: cs.LG
Comments: 18 Pages, 13 figures
\\
  Graph Neural Networks (GNNs) have become a building block in graph data
processing, with wide applications in critical domains. The growing needs to
deploy GNNs in high-stakes applications necessitate explainability for users in
the decision-making processes. A popular paradigm for the explainability of
GNNs is to identify explainable subgraphs by comparing their labels with the
ones of original graphs. This task is challenging due to the substantial
distributional shift from the original graphs in the training set to the set of
explainable subgraphs, which prevents accurate prediction of labels with the
subgraphs. To address it, in this paper, we propose a novel method that
generates proxy graphs for explainable subgraphs that are in the distribution
of training data. We introduce a parametric method that employs graph
generators to produce proxy graphs. A new training objective based on
information theory is designed to ensure that proxy graphs not only adhere to
the distribution of training data but also preserve essential explanatory
factors. Such generated proxy graphs can be reliably used for approximating the
predictions of the true labels of explainable subgraphs. Empirical evaluations
across various datasets demonstrate our method achieves more accurate
explanations for GNNs.
\\ ( https://arxiv.org/abs/2402.02036 ,  6683kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02042
Date: Sat, 3 Feb 2024 05:35:58 GMT   (72kb)

Title: Learning General Parameterized Policies for Infinite Horizon Average
  Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
Authors: Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal
Categories: cs.LG cs.AI
Comments: arXiv admin note: text overlap with arXiv:2309.01922
\\
  This paper explores the realm of infinite horizon average reward Constrained
Markov Decision Processes (CMDP). To the best of our knowledge, this work is
the first to delve into the regret and constraint violation analysis of average
reward CMDPs with a general policy parametrization. To address this challenge,
we propose a primal dual based policy gradient algorithm that adeptly manages
the constraints while ensuring a low regret guarantee toward achieving a global
optimal policy. In particular, we demonstrate that our proposed algorithm
achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and
$\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
\\ ( https://arxiv.org/abs/2402.02042 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02043
Date: Sat, 3 Feb 2024 05:41:39 GMT   (35299kb,D)

Title: A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data
  Transmission
Authors: Wenjun Huang, Arghavan Rezvani, Hanning Chen, Yang Ni, Sanggeon Yun,
  Sungheon Jeong, and Mohsen Imani
Categories: cs.LG cs.AI
Comments: 14 pages, 6 figures
\\
  Applications in the Internet of Things (IoT) utilize machine learning to
analyze sensor-generated data. However, a major challenge lies in the lack of
targeted intelligence in current sensing systems, leading to vast data
generation and increased computational and communication costs. To address this
challenge, we propose a novel sensing module to equip sensing frameworks with
intelligent data transmission capabilities by integrating a highly efficient
machine learning model placed near the sensor. This model provides prompt
feedback for the sensing system to transmit only valuable data while discarding
irrelevant information by regulating the frequency of data transmission. The
near-sensor model is quantized and optimized for real-time sensor control. To
enhance the framework's performance, the training process is customized and a
"lazy" sensor deactivation strategy utilizing temporal information is
introduced. The suggested method is orthogonal to other IoT frameworks and can
be considered as a plugin for selective data transmission. The framework is
implemented, encompassing both software and hardware components. The
experiments demonstrate that the framework utilizing the suggested module
achieves over 85% system efficiency in terms of energy consumption and storage,
with negligible impact on performance. This methodology has the potential to
significantly reduce data output from sensors, benefiting a wide range of IoT
applications.
\\ ( https://arxiv.org/abs/2402.02043 ,  35299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02044
Date: Sat, 3 Feb 2024 05:43:39 GMT   (1851kb,D)

Title: Locally-Adaptive Quantization for Streaming Vector Search
Authors: Cecilia Aguerrebere and Mark Hildebrand and Ishwar Singh Bhati and
  Theodore Willke and Mariano Tepper
Categories: cs.LG cs.IR
\\
  Retrieving the most similar vector embeddings to a given query among a
massive collection of vectors has long been a key component of countless
real-world applications. The recently introduced Retrieval-Augmented Generation
is one of the most prominent examples. For many of these applications, the
database evolves over time by inserting new data and removing outdated data. In
these cases, the retrieval problem is known as streaming similarity search.
While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector
compression method, yields state-of-the-art search performance for non-evolving
databases, its usefulness in the streaming setting has not been yet
established. In this work, we study LVQ in streaming similarity search. In
support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and
multi-means LVQ that boost its search performance by up to 28% and 27%,
respectively. Our studies show that LVQ and its new variants enable blazing
fast vector search, outperforming its closest competitor by up to 9.4x for
identically distributed data and by up to 8.8x under the challenging scenario
of data distribution shifts (i.e., where the statistical distribution of the
data changes over time). We release our contributions as part of Scalable
Vector Search, an open-source library for high-performance similarity search.
\\ ( https://arxiv.org/abs/2402.02044 ,  1851kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02051
Date: Sat, 3 Feb 2024 06:01:21 GMT   (7548kb,D)

Title: Nonlinear subspace clustering by functional link neural networks
Authors: Long Shi, Lei Cao, Zhongpu Chen, Badong Chen, Yu Zhao
Categories: cs.LG
\\
  Nonlinear subspace clustering based on a feed-forward neural network has been
demonstrated to provide better clustering accuracy than some advanced subspace
clustering algorithms. While this approach demonstrates impressive outcomes, it
involves a balance between effectiveness and computational cost. In this study,
we employ a functional link neural network to transform data samples into a
nonlinear domain. Subsequently, we acquire a self-representation matrix through
a learning mechanism that builds upon the mapped samples. As the functional
link neural network is a single-layer neural network, our proposed method
achieves high computational efficiency while ensuring desirable clustering
performance. By incorporating the local similarity regularization to enhance
the grouping effect, our proposed method further improves the quality of the
clustering results. Additionally, we introduce a convex combination subspace
clustering scheme, which combining a linear subspace clustering method with the
functional link neural network subspace clustering approach. This combination
approach allows for a dynamic balance between linear and nonlinear
representations. Extensive experiments confirm the advancement of our methods.
The source code will be released on https://lshi91.github.io/ soon.
\\ ( https://arxiv.org/abs/2402.02051 ,  7548kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02052
Date: Sat, 3 Feb 2024 06:04:49 GMT   (1071kb)

Title: Feature Selection using the concept of Peafowl Mating in IDS
Authors: Partha Ghosh, Joy Sharma and Nilesh Pandey
Categories: cs.LG
Journal-ref: International Journal of Computer Networks & Communications
  (IJCNC) Vol.16, No.1, January 2024
DOI: 10.5121/ijcnc.2024.16104
\\
  Cloud computing has high applicability as an Internet based service that
relies on sharing computing resources. Cloud computing provides services that
are Infrastructure based, Platform based and Software based. The popularity of
this technology is due to its superb performance, high level of computing
ability, low cost of services, scalability, availability and flexibility. The
obtainability and openness of data in cloud environment make it vulnerable to
the world of cyber-attacks. To detect the attacks Intrusion Detection System is
used, that can identify the attacks and ensure information security. Such a
coherent and proficient Intrusion Detection System is proposed in this paper to
achieve higher certainty levels regarding safety in cloud environment. In this
paper, the mating behavior of peafowl is incorporated into an optimization
algorithm which in turn is used as a feature selection algorithm. The algorithm
is used to reduce the huge size of cloud data so that the IDS can work
efficiently on the cloud to detect intrusions. The proposed model has been
experimented with NSL-KDD dataset as well as Kyoto dataset and have proved to
be a better as well as an efficient IDS.
\\ ( https://arxiv.org/abs/2402.02052 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02054
Date: Sat, 3 Feb 2024 06:17:21 GMT   (1205kb,D)

Title: Neural Scaling Laws on Graphs
Authors: Jingzhe Liu, Haitao Mao, Zhikai Chen, Tong Zhao, Neil Shah, Jiliang
  Tang
Categories: cs.LG cs.AI
\\
  Deep graph models (e.g., graph neural networks and graph transformers) have
become important techniques for leveraging knowledge across various types of
graphs. Yet, the scaling properties of deep graph models have not been
systematically investigated, casting doubt on the feasibility of achieving
large graph models through enlarging the model and dataset sizes. In this work,
we delve into neural scaling laws on graphs from both model and data
perspectives. We first verify the validity of such laws on graphs, establishing
formulations to describe the scaling behaviors. For model scaling, we
investigate the phenomenon of scaling law collapse and identify overfitting as
the potential reason. Moreover, we reveal that the model depth of deep graph
models can impact the model scaling behaviors, which differ from observations
in other domains such as CV and NLP. For data scaling, we suggest that the
number of graphs can not effectively metric the graph data volume in scaling
law since the sizes of different graphs are highly irregular. Instead, we
reform the data scaling law with the number of edges as the metric to address
the irregular graph sizes. We further demonstrate the reformed law offers a
unified view of the data scaling behaviors for various fundamental graph tasks
including node classification, link prediction, and graph classification. This
work provides valuable insights into neural scaling laws on graphs, which can
serve as an essential step toward large graph models.
\\ ( https://arxiv.org/abs/2402.02054 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02055
Date: Sat, 3 Feb 2024 06:29:04 GMT   (1240kb,D)

Title: Variance Alignment Score: A Simple But Tough-to-Beat Data Selection
  Method for Multimodal Contrastive Learning
Authors: Yiping Wang, Yifang Chen, Wendan Yan, Kevin Jamieson, Simon Shaolei Du
Categories: cs.LG cs.AI
Comments: 17 pages, 4 figures
\\
  In recent years, data selection has emerged as a core issue for large-scale
visual-language model pretraining, especially on noisy web-curated datasets.
One widely adopted strategy assigns quality scores such as CLIP similarity for
each sample and retains the data pairs with the highest scores. However, these
approaches are agnostic of data distribution and always fail to select the most
informative samples. To solve this problem, we propose a simple yet
theoretically principled metric named Variance Alignment Score (VAS), which has
the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here,
$\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim
to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the
tensor product of single or multi-modal representations for the $i$-th sample.
We further design a new data selection method that maximizes the total VAS. We
provide theoretical analysis in a simplified setting to demonstrate the
theoretical advantage of VAS over random or other existing data selection.
Experimentally, applying VAS and CLIP scores together can outperform baselines
by a margin of $1.3\%$ average on 38 evaluation sets for noisy dataset DataComp
and $2.5\%$ on VTAB for high-quality dataset CC12M. Additionally, our ablation
study also shows visual features are better than text for calculating VAS, and
the related classical experimental design methods may fail under this context.
\\ ( https://arxiv.org/abs/2402.02055 ,  1240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02057
Date: Sat, 3 Feb 2024 06:37:50 GMT   (463kb,D)

Title: Break the Sequential Dependency of LLM Inference Using Lookahead
  Decoding
Authors: Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang
Categories: cs.LG cs.CL
\\
  Autoregressive decoding of large language models (LLMs) is memory bandwidth
bounded, resulting in high latency and significant wastes of the parallel
processing power of modern accelerators. Existing methods for accelerating LLM
decoding often require a draft model (e.g., speculative decoding), which is
nontrivial to obtain and unable to generalize. In this paper, we introduce
Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM
decoding without needing auxiliary models or data stores. It allows trading
per-step log(FLOPs) to reduce the number of total decoding steps, is more
parallelizable on single or multiple modern accelerators, and is compatible
with concurrent memory-efficient attention (e.g., FlashAttention). Our
implementation of Lookahead decoding can speed up autoregressive decoding by up
to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code
completion tasks. Our code is avialable at
https://github.com/hao-ai-lab/LookaheadDecoding
\\ ( https://arxiv.org/abs/2402.02057 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02065
Date: Sat, 3 Feb 2024 07:10:12 GMT   (1098kb,D)

Title: Training Implicit Networks for Image Deblurring using Jacobian-Free
  Backpropagation
Authors: Linghai Liu, Shuaicheng Tong, Lisa Zhao
Categories: cs.LG
\\
  Recent efforts in applying implicit networks to solve inverse problems in
imaging have achieved competitive or even superior results when compared to
feedforward networks. These implicit networks only require constant memory
during backpropagation, regardless of the number of layers. However, they are
not necessarily easy to train. Gradient calculations are computationally
expensive because they require backpropagating through a fixed point. In
particular, this process requires solving a large linear system whose size is
determined by the number of features in the fixed point iteration. This paper
explores a recently proposed method, Jacobian-free Backpropagation (JFB), a
backpropagation scheme that circumvents such calculation, in the context of
image deblurring problems. Our results show that JFB is comparable against
fine-tuned optimization schemes, state-of-the-art (SOTA) feedforward networks,
and existing implicit networks at a reduced computational cost.
\\ ( https://arxiv.org/abs/2402.02065 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02081
Date: Sat, 3 Feb 2024 08:41:51 GMT   (1006kb,D)

Title: Risk-Sensitive Diffusion: Learning the Underlying Distribution from
  Noisy Samples
Authors: Yangming Li, Max Ruiz Luyten, Mihaela van der Schaar
Categories: cs.LG
Comments: Under review paper
\\
  While achieving remarkable performances, we show that diffusion models are
fragile to the presence of noisy samples, limiting their potential in the vast
amount of settings where, unlike image synthesis, we are not blessed with clean
data. Motivated by our finding that such fragility originates from the
distribution gaps between noisy and clean samples along the diffusion process,
we introduce risk-sensitive SDE, a stochastic differential equation that is
parameterized by the risk (i.e., data "dirtiness") to adjust the distributions
of noisy samples, reducing misguidance while benefiting from their contained
information. The optimal expression for risk-sensitive SDE depends on the
specific noise distribution, and we derive its parameterizations that minimize
the misguidance of noisy samples for both Gaussian and general non-Gaussian
perturbations. We conduct extensive experiments on both synthetic and
real-world datasets (e.g., medical time series), showing that our model
effectively recovers the clean data distribution from noisy samples,
significantly outperforming conditional generation baselines.
\\ ( https://arxiv.org/abs/2402.02081 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02095
Date: Sat, 3 Feb 2024 09:22:07 GMT   (1196kb,D)

Title: Seeing is not always believing: The Space of Harmless Perturbations
Authors: Lu Chen, Shaofeng Li, Benhao Huang, Fan Yang, Zheng Li, Jie Li, Yuan
  Luo
Categories: cs.LG
\\
  In the context of deep neural networks, we expose the existence of a harmless
perturbation space, where perturbations leave the network output entirely
unaltered. Perturbations within this harmless perturbation space, regardless of
their magnitude when applied to images, exhibit no impact on the network's
outputs of the original images. Specifically, given any linear layer within the
network, where the input dimension $n$ exceeds the output dimension $m$, we
demonstrate the existence of a continuous harmless perturbation subspace with a
dimension of $(n-m)$. Inspired by this, we solve for a family of general
perturbations that consistently influence the network output, irrespective of
their magnitudes. With these theoretical findings, we explore the application
of harmless perturbations for privacy-preserving data usage. Our work reveals
the difference between DNNs and human perception that the significant
perturbations captured by humans may not affect the recognition of DNNs. As a
result, we utilize this gap to design a type of harmless perturbation that is
meaningless for humans while maintaining its recognizable features for DNNs.
\\ ( https://arxiv.org/abs/2402.02095 ,  1196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02104
Date: Sat, 3 Feb 2024 09:56:37 GMT   (304kb,D)

Title: Learning Structure-Aware Representations of Dependent Types
Authors: Konstantinos Kogkalidis, Orestis Melkonian, Jean-Philippe Bernardy
Categories: cs.LG cs.PL
Comments: 15 pages, submitted to ICML2024
\\
  Agda is a dependently-typed programming language and a proof assistant,
pivotal in proof formalization and programming language theory. This paper
extends the Agda ecosystem into machine learning territory, and, vice versa,
makes Agda-related resources available to machine learning practitioners. We
introduce and release a novel dataset of Agda program-proofs that is elaborate
and extensive enough to support various machine learning applications -- the
first of its kind. Leveraging the dataset's ultra-high resolution, detailing
proof states at the sub-type level, we propose a novel neural architecture
targeted at faithfully representing dependently-typed programs on the basis of
structural rather than nominal principles. We instantiate and evaluate our
architecture in a premise selection setup, where it achieves strong initial
results.
\\ ( https://arxiv.org/abs/2402.02104 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02110
Date: Sat, 3 Feb 2024 10:22:18 GMT   (1211kb,D)

Title: Composite Active Learning: Towards Multi-Domain Active Learning with
  Theoretical Guarantees
Authors: Guang-Yuan Hao, Hengguan Huang, Haotian Wang, Jie Gao, Hao Wang
Categories: cs.LG cs.AI
Journal-ref: AAAI 2024
\\
  Active learning (AL) aims to improve model performance within a fixed
labeling budget by choosing the most informative data points to label. Existing
AL focuses on the single-domain setting, where all data come from the same
domain (e.g., the same dataset). However, many real-world tasks often involve
multiple domains. For example, in visual recognition, it is often desirable to
train an image classifier that works across different environments (e.g.,
different backgrounds), where images from each environment constitute one
domain. Such a multi-domain AL setting is challenging for prior methods because
they (1) ignore the similarity among different domains when assigning labeling
budget and (2) fail to handle distribution shift of data across different
domains. In this paper, we propose the first general method, dubbed composite
active learning (CAL), for multi-domain AL. Our approach explicitly considers
the domain-level and instance-level information in the problem; CAL first
assigns domain-level budgets according to domain-level importance, which is
estimated by optimizing an upper error bound that we develop; with the
domain-level budgets, CAL then leverages a certain instance-level query
strategy to select samples to label from each domain. Our theoretical analysis
shows that our method achieves a better error bound compared to current AL
methods. Our empirical results demonstrate that our approach significantly
outperforms the state-of-the-art AL methods on both synthetic and real-world
multi-domain datasets. Code is available at
https://github.com/Wang-ML-Lab/multi-domain-active-learning.
\\ ( https://arxiv.org/abs/2402.02110 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02114
Date: Sat, 3 Feb 2024 10:43:22 GMT   (842kb,D)

Title: Handling Delayed Feedback in Distributed Online Optimization : A
  Projection-Free Approach
Authors: Tuan-Anh Nguyen, Nguyen Kim Thang, Denis Trystram
Categories: cs.LG cs.DS
\\
  Learning at the edges has become increasingly important as large quantities
of data are continually generated locally. Among others, this paradigm requires
algorithms that are simple (so that they can be executed by local devices),
robust (again uncertainty as data are continually generated), and reliable in a
distributed manner under network issues, especially delays. In this study, we
investigate the problem of online convex optimization under adversarial delayed
feedback. We propose two projection-free algorithms for centralised and
distributed settings in which they are carefully designed to achieve a regret
bound of O(\sqrt{B}) where B is the sum of delay, which is optimal for the OCO
problem in the delay setting while still being projection-free. We provide an
extensive theoretical study and experimentally validate the performance of our
algorithms by comparing them with existing ones on real-world problems.
\\ ( https://arxiv.org/abs/2402.02114 ,  842kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02124
Date: Sat, 3 Feb 2024 11:29:14 GMT   (207kb,D)

Title: Grammar-based evolutionary approach for automated workflow composition
  with domain-specific operators and ensemble diversity
Authors: Rafael Barbudo and Aurora Ram\'irez and Jos\'e Ra\'ul Romero
Categories: cs.LG
Comments: 32 pages, 7 figures, 6 tables, journal paper
MSC-class: 68T05
ACM-class: I.2.6
Journal-ref: R. Barbudo, A. Ram\'irez, & J.R. Romero. "Grammar-based
  evolutionary approach for automated workflow composition with domain-specific
  operators and ensemble diversity". Applied Soft Computing, 111292. 2024
DOI: 10.1016/j.asoc.2024.111292
\\
  The process of extracting valuable and novel insights from raw data involves
a series of complex steps. In the realm of Automated Machine Learning (AutoML),
a significant research focus is on automating aspects of this process,
specifically tasks like selecting algorithms and optimising their
hyper-parameters. A particularly challenging task in AutoML is automatic
workflow composition (AWC). AWC aims to identify the most effective sequence of
data preprocessing and ML algorithms, coupled with their best hyper-parameters,
for a specific dataset. However, existing AWC methods are limited in how many
and in what ways they can combine algorithms within a workflow.
  Addressing this gap, this paper introduces EvoFlow, a grammar-based
evolutionary approach for AWC. EvoFlow enhances the flexibility in designing
workflow structures, empowering practitioners to select algorithms that best
fit their specific requirements. EvoFlow stands out by integrating two
innovative features. First, it employs a suite of genetic operators, designed
specifically for AWC, to optimise both the structure of workflows and their
hyper-parameters. Second, it implements a novel updating mechanism that
enriches the variety of predictions made by different workflows. Promoting this
diversity helps prevent the algorithm from overfitting. With this aim, EvoFlow
builds an ensemble whose workflows differ in their misclassified instances.
  To evaluate EvoFlow's effectiveness, we carried out empirical validation
using a set of classification benchmarks. We begin with an ablation study to
demonstrate the enhanced performance attributable to EvoFlow's unique
components. Then, we compare EvoFlow with other AWC approaches, encompassing
both evolutionary and non-evolutionary techniques. Our findings show that
EvoFlow's specialised genetic operators and updating mechanism substantially
outperform current leading methods[..]
\\ ( https://arxiv.org/abs/2402.02124 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02139
Date: Sat, 3 Feb 2024 13:01:39 GMT   (8006kb,D)

Title: Using Deep Ensemble Forest for High Resolution Mapping of PM2.5 from
  MODIS MAIAC AOD in Tehran, Iran
Authors: Hossein Bagheri
Categories: cs.LG
Journal-ref: Environ Monit Assess 195, 377 (2023)
DOI: 10.1007/s10661-023-10951-1
\\
  High resolution mapping of PM2.5 concentration over Tehran city is
challenging because of the complicated behavior of numerous sources of
pollution and the insufficient number of ground air quality monitoring
stations. Alternatively, high resolution satellite Aerosol Optical Depth (AOD)
data can be employed for high resolution mapping of PM2.5. For this purpose,
different data-driven methods have been used in the literature. Recently, deep
learning methods have demonstrated their ability to estimate PM2.5 from AOD
data. However, these methods have several weaknesses in solving the problem of
estimating PM2.5 from satellite AOD data. In this paper, the potential of the
deep ensemble forest method for estimating the PM2.5 concentration from AOD
data was evaluated. The results showed that the deep ensemble forest method
with R2 = 0.74 gives a higher accuracy of PM2.5 estimation than deep learning
methods (R2 = 0.67) as well as classic data-driven methods such as random
forest (R2 = 0.68). Additionally, the estimated values of PM2.5 using the deep
ensemble forest algorithm were used along with ground data to generate a high
resolution map of PM2.5. Evaluation of the produced PM2.5 map revealed the good
performance of the deep ensemble forest for modeling the variation of PM2.5 in
the city of Tehran.
\\ ( https://arxiv.org/abs/2402.02139 ,  8006kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02165
Date: Sat, 3 Feb 2024 14:25:33 GMT   (2198kb,D)

Title: Towards Optimal Adversarial Robust Q-learning with Bellman
  Infinity-error
Authors: Haoran Li, Zicheng Zhang, Wang Luo, Congying Han, Yudong Hu, Tiande
  Guo, Shichen Liao
Categories: cs.LG
\\
  Establishing robust policies is essential to counter attacks or disturbances
affecting deep reinforcement learning (DRL) agents. Recent studies explore
state-adversarial robustness and suggest the potential lack of an optimal
robust policy (ORP), posing challenges in setting strict robustness
constraints. This work further investigates ORP: At first, we introduce a
consistency assumption of policy (CAP) stating that optimal actions in the
Markov decision process remain consistent with minor perturbations, supported
by empirical and theoretical evidence. Building upon CAP, we crucially prove
the existence of a deterministic and stationary ORP that aligns with the
Bellman optimal policy. Furthermore, we illustrate the necessity of
$L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding
clarifies the vulnerability of prior DRL algorithms that target the Bellman
optimal policy with $L^{1}$-norm and motivates us to train a Consistent
Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of
Bellman Infinity-error. The top-tier performance of CAR-DQN across various
benchmarks validates its practical effectiveness and reinforces the soundness
of our theoretical analysis.
\\ ( https://arxiv.org/abs/2402.02165 ,  2198kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02168
Date: Sat, 3 Feb 2024 14:29:01 GMT   (857kb,D)

Title: One Graph Model for Cross-domain Dynamic Link Prediction
Authors: Xuanwen Huang, Wei Chow, Yang Wang, Ziwei Chai, Chunping Wang, Lei
  Chen, Yang Yang
Categories: cs.LG cs.AI
Comments: Under review
\\
  This work proposes DyExpert, a dynamic graph model for cross-domain link
prediction. It can explicitly model historical evolving processes to learn the
evolution pattern of a specific downstream graph and subsequently make
pattern-specific link predictions. DyExpert adopts a decode-only transformer
and is capable of efficiently parallel training and inference by
\textit{conditioned link generation} that integrates both evolution modeling
and link prediction. DyExpert is trained by extensive dynamic graphs across
diverse domains, comprising 6M dynamic edges. Extensive experiments on eight
untrained graphs demonstrate that DyExpert achieves state-of-the-art
performance in cross-domain link prediction. Compared to the advanced baseline
under the same setting, DyExpert achieves an average of 11.40% improvement
Average Precision across eight graphs. More impressive, it surpasses the fully
supervised performance of 8 advanced baselines on 6 untrained graphs.
\\ ( https://arxiv.org/abs/2402.02168 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02186
Date: Sat, 3 Feb 2024 15:28:53 GMT   (5346kb,D)

Title: Evolution Guided Generative Flow Networks
Authors: Zarif Ikram, Ling Pan, Dianbo Liu
Categories: cs.LG cs.AI
Comments: 16 pages, 16 figues
\\
  Generative Flow Networks (GFlowNets) are a family of probabilistic generative
models that learn to sample compositional objects proportional to their
rewards. One big challenge of GFlowNets is training them effectively when
dealing with long time horizons and sparse rewards. To address this, we propose
Evolution guided generative flow networks (EGFN), a simple but powerful
augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our
method can work on top of any GFlowNets training objective, by training a set
of agent parameters using EA, storing the resulting trajectories in the
prioritized replay buffer, and training the GFlowNets agent using the stored
trajectories. We present a thorough investigation over a wide range of toy and
real-world benchmark tasks showing the effectiveness of our method in handling
long trajectories and sparse rewards.
\\ ( https://arxiv.org/abs/2402.02186 ,  5346kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02207
Date: Sat, 3 Feb 2024 16:43:42 GMT   (7418kb,D)

Title: Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large
  Language Models
Authors: Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy
  Hospedales
Categories: cs.LG
\\
  Current vision large language models (VLLMs) exhibit remarkable capabilities
yet are prone to generate harmful content and are vulnerable to even the
simplest jailbreaking attacks. Our initial analysis finds that this is due to
the presence of harmful data during vision-language instruction fine-tuning,
and that VLLM fine-tuning can cause forgetting of safety alignment previously
learned by the underpinning LLM. To address this issue, we first curate a
vision-language safe instruction-following dataset VLGuard covering various
harmful categories. Our experiments demonstrate that integrating this dataset
into standard vision-language fine-tuning or utilizing it for post-hoc
fine-tuning effectively safety aligns VLLMs. This alignment is achieved with
minimal impact on, or even enhancement of, the models' helpfulness. The
versatility of our safety fine-tuning dataset makes it a valuable resource for
safety-testing existing VLLMs, training new models or safeguarding pre-trained
VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject
unsafe instructions and substantially reduce the success rates of several
black-box adversarial attacks, which approach zero in many cases. The code and
dataset are available at https://github.com/ys-zong/VLGuard.
\\ ( https://arxiv.org/abs/2402.02207 ,  7418kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02211
Date: Sat, 3 Feb 2024 17:05:01 GMT   (1500kb,D)

Title: Query-decision Regression between Shortest Path and Minimum Steiner Tree
Authors: Guangmo Tong, Peng Zhao, Mina Samizadeh
Categories: cs.LG cs.DS
Comments: PAKDD 2024
\\
  Considering a graph with unknown weights, can we find the shortest path for a
pair of nodes if we know the minimal Steiner trees associated with some subset
of nodes? That is, with respect to a fixed latent decision-making system (e.g.,
a weighted graph), we seek to solve one optimization problem (e.g., the
shortest path problem) by leveraging information associated with another
optimization problem (e.g., the minimal Steiner tree problem). In this paper,
we study such a prototype problem called \textit{query-decision regression with
task shifts}, focusing on the shortest path problem and the minimum Steiner
tree problem. We provide theoretical insights regarding the design of
realizable hypothesis spaces for building scoring models, and present two
principled learning frameworks. Our experimental studies show that such
problems can be solved to a decent extent with statistical significance.
\\ ( https://arxiv.org/abs/2402.02211 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02216
Date: Sat, 3 Feb 2024 17:24:36 GMT   (144kb,D)

Title: Graph Foundation Models
Authors: Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao,
  Neil Shah, Michael Galkin, Jiliang Tang
Categories: cs.LG
Comments: 18 pages, 2 figures
\\
  Graph Foundation Model (GFM) is a new trending research topic in the graph
domain, aiming to develop a graph model capable of generalizing across
different graphs and tasks. However, a versatile GFM has not yet been achieved.
The key challenge in building GFM is how to enable positive transfer across
graphs with diverse structural patterns. Inspired by the existing foundation
models in the CV and NLP domains, we propose a novel perspective for the GFM
development by advocating for a ``graph vocabulary'', in which the basic
transferable units underlying graphs encode the invariance on graphs. We ground
the graph vocabulary construction from essential aspects including network
analysis, theoretical foundations, and stability. Such a vocabulary perspective
can potentially advance the future GFM design following the neural scaling
laws.
\\ ( https://arxiv.org/abs/2402.02216 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02225
Date: Sat, 3 Feb 2024 17:58:43 GMT   (15161kb,D)

Title: Rethinking the Starting Point: Enhancing Performance and Fairness of
  Federated Learning via Collaborative Pre-Training
Authors: Yun-Wei Chu, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G.
  Brinton
Categories: cs.LG
\\
  Most existing federated learning (FL) methodologies have assumed training
begins from a randomly initialized model. Recently, several studies have
empirically demonstrated that leveraging a pre-trained model can offer
advantageous initializations for FL. In this paper, we propose a collaborative
pre-training approach, CoPreFL, which strategically designs a pre-trained model
to serve as a good initialization for any downstream FL task. The key idea of
our pre-training algorithm is a meta-learning procedure which mimics downstream
distributed scenarios, enabling it to adapt to any unforeseen FL task.
CoPreFL's pre-training optimization procedure also strikes a balance between
average performance and fairness, with the aim of addressing these competing
challenges in downstream FL tasks through intelligent initializations.
Extensive experimental results validate that our pre-training method provides a
robust initialization for any unseen downstream FL task, resulting in enhanced
average performance and more equitable predictions.
\\ ( https://arxiv.org/abs/2402.02225 ,  15161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02229
Date: Sat, 3 Feb 2024 18:19:46 GMT   (7705kb,D)

Title: Vanilla Bayesian Optimization Performs Great in High Dimension
Authors: Carl Hvarfner and Erik Orm Hellsten and Luigi Nardi
Categories: cs.LG
\\
  High-dimensional problems have long been considered the Achilles' heel of
Bayesian optimization algorithms. Spurred by the curse of dimensionality, a
large collection of algorithms aim to make it more performant in this setting,
commonly by imposing various simplifying assumptions on the objective. In this
paper, we identify the degeneracies that make vanilla Bayesian optimization
poorly suited to high-dimensional tasks, and further show how existing
algorithms address these degeneracies through the lens of lowering the model
complexity. Moreover, we propose an enhancement to the prior assumptions that
are typical to vanilla Bayesian optimization algorithms, which reduces the
complexity to manageable levels without imposing structural restrictions on the
objective. Our modification - a simple scaling of the Gaussian process
lengthscale prior with the dimensionality - reveals that standard Bayesian
optimization works drastically better than previously thought in high
dimensions, clearly outperforming existing state-of-the-art algorithms on
multiple commonly considered real-world high-dimensional tasks.
\\ ( https://arxiv.org/abs/2402.02229 ,  7705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02230
Date: Sat, 3 Feb 2024 18:21:38 GMT   (113kb,D)

Title: Federated Learning with Differential Privacy
Authors: Adrien Banse, Jan Kreischer, Xavier Oliva i J\"urgens
Categories: cs.LG cs.AI cs.DC
Comments: Machine Learning (ML) & Federated Learning (FL); 4 pages, 3 figures
ACM-class: I.2.11
\\
  Federated learning (FL), as a type of distributed machine learning, is
capable of significantly preserving client's private data from being shared
among different parties. Nevertheless, private information can still be
divulged by analyzing uploaded parameter weights from clients. In this report,
we showcase our empirical benchmark of the effect of the number of clients and
the addition of differential privacy (DP) mechanisms on the performance of the
model on different types of data. Our results show that non-i.i.d and small
datasets have the highest decrease in performance in a distributed and
differentially private setting.
\\ ( https://arxiv.org/abs/2402.02230 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02239
Date: Sat, 3 Feb 2024 19:00:19 GMT   (474kb,D)

Title: Distributional Reduction: Unifying Dimensionality Reduction and
  Clustering with Gromov-Wasserstein Projection
Authors: Hugues Van Assel, C\'edric Vincent-Cuaz, Nicolas Courty, R\'emi
  Flamary, Pascal Frossard, Titouan Vayer
Categories: cs.LG stat.ML
\\
  Unsupervised learning aims to capture the underlying structure of potentially
large and high-dimensional datasets. Traditionally, this involves using
dimensionality reduction methods to project data onto interpretable spaces or
organizing points into meaningful clusters. In practice, these methods are used
sequentially, without guaranteeing that the clustering aligns well with the
conducted dimensionality reduction. In this work, we offer a fresh perspective:
that of distributions. Leveraging tools from optimal transport, particularly
the Gromov-Wasserstein distance, we unify clustering and dimensionality
reduction into a single framework called distributional reduction. This allows
us to jointly address clustering and dimensionality reduction with a single
optimization problem. Through comprehensive experiments, we highlight the
versatility and interpretability of our method and show that it outperforms
existing approaches across a variety of image and genomics datasets.
\\ ( https://arxiv.org/abs/2402.02239 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02249
Date: Sat, 3 Feb 2024 19:40:41 GMT   (286kb,D)

Title: Don't Label Twice: Quantity Beats Quality when Comparing Binary
  Classifiers on a Budget
Authors: Florian E. Dorner and Moritz Hardt
Categories: cs.LG
Comments: 34 pages, 3 Figures
\\
  We study how to best spend a budget of noisy labels to compare the accuracy
of two binary classifiers. It's common practice to collect and aggregate
multiple noisy labels for a given data point into a less noisy label via a
majority vote. We prove a theorem that runs counter to conventional wisdom. If
the goal is to identify the better of two classifiers, we show it's best to
spend the budget on collecting a single label for more samples. Our result
follows from a non-trivial application of Cram\'er's theorem, a staple in the
theory of large deviations. We discuss the implications of our work for the
design of machine learning benchmarks, where they overturn some time-honored
recommendations. In addition, our results provide sample size bounds superior
to what follows from Hoeffding's bound.
\\ ( https://arxiv.org/abs/2402.02249 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02254
Date: Sat, 3 Feb 2024 20:22:41 GMT   (1222kb)

Title: Teacher-Student Learning based Low Complexity Relay Selection in
  Wireless Powered Communications
Authors: Aysun Gurur Onalan, Berkay Kopru, Sinem Coleri
Categories: cs.LG cs.NI
\\
  Radio Frequency Energy Harvesting (RF-EH) networks are key enablers of
massive Internet-of-things by providing controllable and long-distance energy
transfer to energy-limited devices. Relays, helping either energy or
information transfer, have been demonstrated to significantly improve the
performance of these networks. This paper studies the joint relay selection,
scheduling, and power control problem in multiple-source-multiple-relay RF-EH
networks under nonlinear EH conditions. We first obtain the optimal solution to
the scheduling and power control problem for the given relay selection. Then,
the relay selection problem is formulated as a classification problem, for
which two convolutional neural network (CNN) based architectures are proposed.
While the first architecture employs conventional 2D convolution blocks and
benefits from skip connections between layers; the second architecture replaces
them with inception blocks, to decrease trainable parameter size without
sacrificing accuracy for memory-constrained applications. To decrease the
runtime complexity further, teacher-student learning is employed such that the
teacher network is larger, and the student is a smaller size CNN-based
architecture distilling the teacher's knowledge. A novel dichotomous
search-based algorithm is employed to determine the best architecture for the
student network. Our simulation results demonstrate that the proposed solutions
provide lower complexity than the state-of-art iterative approaches without
compromising optimality.
\\ ( https://arxiv.org/abs/2402.02254 ,  1222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02258
Date: Sat, 3 Feb 2024 20:33:39 GMT   (2612kb,D)

Title: XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event
  Prediction
Authors: Tingsong Xiao, Zelin Xu, Wenchong He, Jim Su, Yupu Zhang, Raymond
  Opoku, Ronald Ison, Jason Petho, Jiang Bian, Patrick Tighe, Parisa Rashidi,
  Zhe Jiang
Categories: cs.LG cs.AI
\\
  Event prediction aims to forecast the time and type of a future event based
on a historical event sequence. Despite its significance, several challenges
exist, including the irregularity of time intervals between consecutive events,
the existence of cycles, periodicity, and multi-scale event interactions, as
well as the high computational costs for long event sequences. Existing neural
temporal point processes (TPPs) methods do not capture the multi-scale nature
of event interactions, which is common in many real-world applications such as
clinical event data. To address these issues, we propose the
cross-temporal-scale transformer (XTSFormer), designed specifically for
irregularly timed event data. Our model comprises two vital components: a novel
Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures
the cyclical nature of time, and a hierarchical multi-scale temporal attention
mechanism. These scales are determined by a bottom-up clustering algorithm.
Extensive experiments on several real-world datasets show that our XTSFormer
outperforms several baseline methods in prediction performance.
\\ ( https://arxiv.org/abs/2402.02258 ,  2612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02263
Date: Sat, 3 Feb 2024 21:12:36 GMT   (540kb,D)

Title: MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly
  Mixed Classifiers
Authors: Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi
Categories: cs.LG cs.AI cs.CV
MSC-class: 68T07
\\
  Adversarial robustness often comes at the cost of degraded accuracy, impeding
the real-life application of robust classification models. Training-based
solutions for better trade-offs are limited by incompatibilities with
already-trained high-performance large models, necessitating the exploration of
training-free ensemble approaches. Observing that robust models are more
confident in correct predictions than in incorrect ones on clean and
adversarial data alike, we speculate amplifying this "benign confidence
property" can reconcile accuracy and robustness in an ensemble setting. To
achieve so, we propose "MixedNUTS", a training-free method where the output
logits of a robust classifier and a standard non-robust classifier are
processed by nonlinear transformations with only three parameters, which are
optimized through an efficient algorithm. MixedNUTS then converts the
transformed logits into probabilities and mixes them as the overall output. On
CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom
strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and
near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points,
sacrificing merely 0.87 points in robust accuracy.
\\ ( https://arxiv.org/abs/2402.02263 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02268
Date: Sat, 3 Feb 2024 21:29:31 GMT   (1410kb,D)

Title: Federated Learning with New Knowledge: Fundamentals, Advances, and
  Futures
Authors: Lixu Wang, Yang Zhao, Jiahua Dong, Ating Yin, Qinbin Li, Xiao Wang,
  Dusit Niyato, Qi Zhu
Categories: cs.LG cs.AI
Comments: 10 pages
\\
  Federated Learning (FL) is a privacy-preserving distributed learning approach
that is rapidly developing in an era where privacy protection is increasingly
valued. It is this rapid development trend, along with the continuous emergence
of new demands for FL in the real world, that prompts us to focus on a very
important problem: Federated Learning with New Knowledge. The primary challenge
here is to effectively incorporate various new knowledge into existing FL
systems and evolve these systems to reduce costs, extend their lifespan, and
facilitate sustainable development. In this paper, we systematically define the
main sources of new knowledge in FL, including new features, tasks, models, and
algorithms. For each source, we thoroughly analyze and discuss how to
incorporate new knowledge into existing FL systems and examine the impact of
the form and timing of new knowledge arrival on the incorporation process.
Furthermore, we comprehensively discuss the potential future directions for FL
with new knowledge, considering a variety of factors such as scenario setups,
efficiency, and security. There is also a continuously updating repository for
this topic: https://github.com/conditionWang/FLNK.
\\ ( https://arxiv.org/abs/2402.02268 ,  1410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02275
Date: Sat, 3 Feb 2024 22:08:11 GMT   (31349kb,D)

Title: SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing
  Applications using a Generative Approach
Authors: Tianshi Wang, Jinyang Li, Ruijie Wang, Denizhan Kara, Shengzhong Liu,
  Davis Wertheimer, Antoni Viros-i-Martin, Raghu Ganti, Mudhakar Srivatsa,
  Tarek Abdelzaher
Categories: cs.LG
Comments: Published in ACM Conference on Embedded Networked Sensor Systems
  (SenSys 23), November, 2023, Istanbul, Turkiye
\\
  This paper introduces SudokuSens, a generative framework for automated
generation of training data in machine-learning-based Internet-of-Things (IoT)
applications, such that the generated synthetic data mimic experimental
configurations not encountered during actual sensor data collection. The
framework improves the robustness of resulting deep learning models, and is
intended for IoT applications where data collection is expensive. The work is
motivated by the fact that IoT time-series data entangle the signatures of
observed objects with the confounding intrinsic properties of the surrounding
environment and the dynamic environmental disturbances experienced. To
incorporate sufficient diversity into the IoT training data, one therefore
needs to consider a combinatorial explosion of training cases that are
multiplicative in the number of objects considered and the possible
environmental conditions in which such objects may be encountered. Our
framework substantially reduces these multiplicative training needs. To
decouple object signatures from environmental conditions, we employ a
Conditional Variational Autoencoder (CVAE) that allows us to reduce data
collection needs from multiplicative to (nearly) linear, while synthetically
generating (data for) the missing conditions. To obtain robustness with respect
to dynamic disturbances, a session-aware temporal contrastive learning approach
is taken. Integrating the aforementioned two approaches, SudokuSens
significantly improves the robustness of deep learning for IoT applications. We
explore the degree to which SudokuSens benefits downstream inference tasks in
different data sets and discuss conditions under which the approach is
particularly effective.
\\ ( https://arxiv.org/abs/2402.02275 ,  31349kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02277
Date: Sat, 3 Feb 2024 22:14:54 GMT   (1746kb,D)

Title: Causal Bayesian Optimization via Exogenous Distribution Learning
Authors: Shaogang Ren, Xiaoning Qian
Categories: cs.LG stat.ML
\\
  Maximizing a target variable as an operational objective in a structured
causal model is an important problem. Existing Causal Bayesian Optimization
(CBO) methods either rely on hard interventions that alter the causal structure
to maximize the reward; or introduce action nodes to endogenous variables so
that the data generation mechanisms are adjusted to achieve the objective. In
this paper, a novel method is introduced to learn the distribution of exogenous
variables, which is typically ignored or marginalized through expectation by
existing methods.
  Exogenous distribution learning improves the approximation accuracy of
structured causal models in a surrogate model that is usually trained with
limited observational data. Moreover, the learned exogenous distribution
extends existing CBO to general causal schemes beyond Additive Noise Models
(ANM). The recovery of exogenous variables allows us to use a more flexible
prior for noise or unobserved hidden variables. A new CBO method is developed
by leveraging the learned exogenous distribution. Experiments on different
datasets and applications show the benefits of our proposed method.
\\ ( https://arxiv.org/abs/2402.02277 ,  1746kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02287
Date: Sat, 3 Feb 2024 22:55:31 GMT   (71kb,D)

Title: Future Directions in Foundations of Graph Machine Learning
Authors: Christopher Morris, Nadav Dym, Haggai Maron, \.Ismail \.Ilkan Ceylan,
  Fabrizio Frasca, Ron Levie, Derek Lim, Michael Bronstein, Martin Grohe, and
  Stefanie Jegelka
Categories: cs.LG cs.AI cs.DM cs.NE stat.ML
\\
  Machine learning on graphs, especially using graph neural networks (GNNs),
has seen a surge in interest due to the wide availability of graph data across
a broad spectrum of disciplines, from life to social and engineering sciences.
Despite their practical success, our theoretical understanding of the
properties of GNNs remains highly incomplete. Recent theoretical advancements
primarily focus on elucidating the coarse-grained expressive power of GNNs,
predominantly employing combinatorial techniques. However, these studies do not
perfectly align with practice, particularly in understanding the generalization
behavior of GNNs when trained with stochastic first-order optimization
techniques. In this position paper, we argue that the graph machine learning
community needs to shift its attention to developing a more balanced theory of
graph machine learning, focusing on a more thorough understanding of the
interplay of expressive power, generalization, and optimization.
\\ ( https://arxiv.org/abs/2402.02287 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02309
Date: Sun, 4 Feb 2024 01:29:24 GMT   (10247kb,D)

Title: Jailbreaking Attack against Multimodal Large Language Model
Authors: Zhenxing Niu and Haodong Ren and Xinbo Gao and Gang Hua and Rong Jin
Categories: cs.LG
\\
  This paper focuses on jailbreaking attacks against multi-modal large language
models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to
harmful user queries. A maximum likelihood-based algorithm is proposed to find
an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs
across multiple unseen prompts and images (i.e., data-universal property). Our
approach exhibits strong model-transferability, as the generated imgJP can be
transferred to jailbreak various models, including MiniGPT-v2, LLaVA,
InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a
connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we
introduce a construction-based method to harness our approach for
LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art
methods. The code is available here. \textbf{Warning: some content generated by
language models may be offensive to some readers.}
\\ ( https://arxiv.org/abs/2402.02309 ,  10247kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02314
Date: Sun, 4 Feb 2024 01:55:00 GMT   (9998kb,D)

Title: Selecting Large Language Model to Fine-tune via Rectified Scaling Law
Authors: Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian
  Li, Jianzhu Ma, Xiaojun Wan, James Zou, Yitao Liang
Categories: cs.LG cs.AI cs.CL
\\
  The ever-growing ecosystem of LLMs has posed a challenge in selecting the
most appropriate pre-trained model to fine-tune amidst a sea of options. Given
constrained resources, fine-tuning all models and making selections afterward
is unrealistic. In this work, we formulate this resource-constrained selection
task into predicting fine-tuning performance and illustrate its natural
connection with scaling laws. Unlike pre-training, We find that the fine-tuning
scaling curve includes not just the well-known "power phase" but also the
previously unobserved "pre-power phase". We also explain why existing scaling
laws fail to capture this phase transition phenomenon both theoretically and
empirically. To address this, we introduce the concept of "pre-learned data
size" into our rectified scaling law, which overcomes theoretical limitations
and fits experimental results much better. By leveraging our law, we propose a
novel LLM selection algorithm that selects the near-optimal model with hundreds
of times less resource consumption, while other methods may provide negatively
correlated selection.
\\ ( https://arxiv.org/abs/2402.02314 ,  9998kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02316
Date: Sun, 4 Feb 2024 02:09:18 GMT   (262kb,D)

Title: Your Diffusion Model is Secretly a Certifiably Robust Classifier
Authors: Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang,
  Hang Su, Jun Zhu
Categories: cs.LG cs.CV
\\
  Diffusion models are recently employed as generative classifiers for robust
classification. However, a comprehensive theoretical understanding of the
robustness of diffusion classifiers is still lacking, leading us to question
whether they will be vulnerable to future stronger attacks. In this study, we
propose a new family of diffusion classifiers, named Noised Diffusion
Classifiers~(NDCs), that possess state-of-the-art certified robustness.
Specifically, we generalize the diffusion classifiers to classify
Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these
distributions, approximating the likelihood using the ELBO, and calculating
classification probabilities via Bayes' theorem. We integrate these generalized
diffusion classifiers with randomized smoothing to construct smoothed
classifiers possessing non-constant Lipschitzness. Experimental results
demonstrate the superior certified robustness of our proposed NDCs. Notably, we
are the first to achieve 80\%+ and 70\%+ certified robustness on CIFAR-10 under
adversarial perturbations with $\ell_2$ norm less than 0.25 and 0.5,
respectively, using a single off-the-shelf diffusion model without any
additional data.
\\ ( https://arxiv.org/abs/2402.02316 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02317
Date: Sun, 4 Feb 2024 02:09:30 GMT   (9748kb,D)

Title: INViT: A Generalizable Routing Problem Solver with Invariant Nested View
  Transformer
Authors: Han Fang, Zhihao Song, Paul Weng, Yutong Ban
Categories: cs.LG
\\
  Recently, deep reinforcement learning has shown promising results for
learning fast heuristics to solve routing problems. Meanwhile, most of the
solvers suffer from generalizing to an unseen distribution or distributions
with different scales. To address this issue, we propose a novel architecture,
called Invariant Nested View Transformer (INViT), which is designed to enforce
a nested design together with invariant views inside the encoders to promote
the generalizability of the learned solver. It applies a modified policy
gradient algorithm enhanced with data augmentations. We demonstrate that the
proposed INViT achieves a dominant generalization performance on both TSP and
CVRP problems with various distributions and different problem scales.
\\ ( https://arxiv.org/abs/2402.02317 ,  9748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02318
Date: Sun, 4 Feb 2024 02:09:43 GMT   (284kb,D)

Title: Diversity Measurement and Subset Selection for Instruction Tuning
  Datasets
Authors: Peiqi Wang, Yikang Shen, Zhen Guo, Matthew Stallone, Yoon Kim, Polina
  Golland, Rameswar Panda
Categories: cs.LG
\\
  We aim to select data subsets for the fine-tuning of large language models to
more effectively follow instructions. Prior work has emphasized the importance
of diversity in dataset curation but relied on heuristics such as the number of
tasks. In this paper, we use determinantal point processes to capture the
diversity and quality of instruction tuning datasets for subset selection. We
propose to measure dataset diversity with log determinant distance that is the
distance between the dataset of interest and a maximally diverse reference
dataset. Our experiments demonstrate that the proposed diversity measure in the
normalized weight gradient space is correlated with downstream
instruction-following performance. Consequently, it can be used to inform when
data selection is the most helpful and to analyze dataset curation strategies.
We demonstrate the utility of our approach on various instruction tuning
datasets.
\\ ( https://arxiv.org/abs/2402.02318 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02321
Date: Sun, 4 Feb 2024 02:23:45 GMT   (345kb,D)

Title: Active Learning for Graphs with Noisy Structures
Authors: Hongliang Chi, Cong Qi, Suhang Wang, Yao Ma
Categories: cs.LG
\\
  Graph Neural Networks (GNNs) have seen significant success in tasks such as
node classification, largely contingent upon the availability of sufficient
labeled nodes. Yet, the excessive cost of labeling large-scale graphs led to a
focus on active learning on graphs, which aims for effective data selection to
maximize downstream model performance. Notably, most existing methods assume
reliable graph topology, while real-world scenarios often present noisy graphs.
Given this, designing a successful active learning framework for noisy graphs
is highly needed but challenging, as selecting data for labeling and obtaining
a clean graph are two tasks naturally interdependent: selecting high-quality
data requires clean graph structure while cleaning noisy graph structure
requires sufficient labeled data. Considering the complexity mentioned above,
we propose an active learning framework, GALClean, which has been specifically
designed to adopt an iterative approach for conducting both data selection and
graph purification simultaneously with best information learned from the prior
iteration. Importantly, we summarize GALClean as an instance of the
Expectation-Maximization algorithm, which provides a theoretical understanding
of its design and mechanisms. This theory naturally leads to an enhanced
version, GALClean+. Extensive experiments have demonstrated the effectiveness
and robustness of our proposed method across various types and levels of noisy
graphs.
\\ ( https://arxiv.org/abs/2402.02321 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02322
Date: Sun, 4 Feb 2024 02:26:40 GMT   (1359kb,D)

Title: Dynamic Incremental Optimization for Best Subset Selection
Authors: Shaogang Ren, Xiaoning Qian
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2207.02058
\\
  Best subset selection is considered the `gold standard' for many sparse
learning problems. A variety of optimization techniques have been proposed to
attack this non-smooth non-convex problem. In this paper, we investigate the
dual forms of a family of $\ell_0$-regularized problems. An efficient
primal-dual algorithm is developed based on the primal and dual problem
structures. By leveraging the dual range estimation along with the incremental
strategy, our algorithm potentially reduces redundant computation and improves
the solutions of best subset selection. Theoretical analysis and experiments on
synthetic and real-world datasets validate the efficiency and statistical
properties of the proposed solutions.
\\ ( https://arxiv.org/abs/2402.02322 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02325
Date: Sun, 4 Feb 2024 02:48:28 GMT   (1325kb,D)

Title: Role of Momentum in Smoothing Objective Function in Implicit Graduated
  Optimization
Authors: Naoki Sato and Hideaki Iiduka
Categories: cs.LG
\\
  While stochastic gradient descent (SGD) with momentum has fast convergence
and excellent generalizability, a theoretical explanation for this is lacking.
In this paper, we show that SGD with momentum smooths the objective function,
the degree of which is determined by the learning rate, the batch size, the
momentum factor, the variance of the stochastic gradient, and the upper bound
of the gradient norm. This theoretical finding reveals why momentum improves
generalizability and provides new insights into the role of the
hyperparameters, including momentum factor. We also present an implicit
graduated optimization algorithm that exploits the smoothing properties of SGD
with momentum and provide experimental results supporting our assertion that
SGD with momentum smooths the objective function.
\\ ( https://arxiv.org/abs/2402.02325 ,  1325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02328
Date: Sun, 4 Feb 2024 03:03:27 GMT   (44kb,D)

Title: Data-driven algorithm design using neural networks with applications to
  branch-and-cut
Authors: Hongyu Cheng, Sammy Khalife, Barbara Fiedorowicz, Amitabh Basu
Categories: cs.LG math.OC
\\
  Data-driven algorithm design is a paradigm that uses statistical and machine
learning techniques to select from a class of algorithms for a computational
problem an algorithm that has the best expected performance with respect to
some (unknown) distribution on the instances of the problem. We build upon
recent work in this line of research by introducing the idea where, instead of
selecting a single algorithm that has the best performance, we allow the
possibility of selecting an algorithm based on the instance to be solved. In
particular, given a representative sample of instances, we learn a neural
network that maps an instance of the problem to the most appropriate algorithm
{\em for that instance}. We formalize this idea and derive rigorous sample
complexity bounds for this learning problem, in the spirit of recent work in
data-driven algorithm design. We then apply this approach to the problem of
making good decisions in the branch-and-cut framework for mixed-integer
optimization (e.g., which cut to add?). In other words, the neural network will
take as input a mixed-integer optimization instance and output a decision that
will result in a small branch-and-cut tree for that instance. Our computational
results provide evidence that our particular way of using neural networks for
cut selection can make a significant impact in reducing branch-and-cut tree
sizes, compared to previous data-driven approaches.
\\ ( https://arxiv.org/abs/2402.02328 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02332
Date: Sun, 4 Feb 2024 03:54:31 GMT   (2093kb,D)

Title: Minusformer: Improving Time Series Forecasting by Progressively Learning
  Residuals
Authors: Daojun Liang, Haixia Zhang, Dongfeng Yuan, Bingzheng Zhang and Minggao
  Zhang
Categories: cs.LG
\\
  In this paper, we find that ubiquitous time series (TS) forecasting models
are prone to severe overfitting. To cope with this problem, we embrace a
de-redundancy approach to progressively reinstate the intrinsic values of TS
for future intervals. Specifically, we renovate the vanilla Transformer by
reorienting the information aggregation mechanism from addition to subtraction.
Then, we incorporate an auxiliary output branch into each block of the original
model to construct a highway leading to the ultimate prediction. The output of
subsequent modules in this branch will subtract the previously learned results,
enabling the model to learn the residuals of the supervision signal, layer by
layer. This designing facilitates the learning-driven implicit progressive
decomposition of the input and output streams, empowering the model with
heightened versatility, interpretability, and resilience against overfitting.
Since all aggregations in the model are minus signs, which is called
Minusformer. Extensive experiments demonstrate the proposed method outperform
existing state-of-the-art methods, yielding an average performance improvement
of 11.9% across various datasets.
\\ ( https://arxiv.org/abs/2402.02332 ,  2093kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02334
Date: Sun, 4 Feb 2024 04:07:39 GMT   (194kb,D)

Title: Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning
Authors: Yi Cheng, Renjun Hu, Haochao Ying, Xing Shi, Jian Wu, Wei Lin
Categories: cs.LG cs.AI
Comments: 11 pages, 8 figures, to be published to AAAI2024
ACM-class: I.2.4
\\
  Until recently, the question of the effective inductive bias of deep models
on tabular data has remained unanswered. This paper investigates the hypothesis
that arithmetic feature interaction is necessary for deep tabular learning. To
test this point, we create a synthetic tabular dataset with a mild feature
interaction assumption and examine a modified transformer architecture enabling
arithmetical feature interactions, referred to as AMFormer. Results show that
AMFormer outperforms strong counterparts in fine-grained tabular data modeling,
data efficiency in training, and generalization. This is attributed to its
parallel additive and multiplicative attention operators and prompt-based
optimization, which facilitate the separation of tabular samples in an extended
space with arithmetically-engineered features. Our extensive experiments on
real-world data also validate the consistent effectiveness, efficiency, and
rationale of AMFormer, suggesting it has established a strong inductive bias
for deep learning on tabular data. Code is available at
https://github.com/aigc-apps/AMFormer.
\\ ( https://arxiv.org/abs/2402.02334 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02342
Date: Sun, 4 Feb 2024 04:55:54 GMT   (10948kb,D)

Title: MetaOptimize: A Framework for Optimizing Step Sizes and Other
  Meta-parameters
Authors: Arsalan Sharifnassab, Saber Salehkaleybar, Richard Sutton
Categories: cs.LG cs.AI
\\
  This paper addresses the challenge of optimizing meta-parameters (i.e.,
hyperparameters) in machine learning algorithms, a critical factor influencing
training efficiency and model performance. Moving away from the computationally
expensive traditional meta-parameter search methods, we introduce MetaOptimize
framework that dynamically adjusts meta-parameters, particularly step sizes
(also known as learning rates), during training. More specifically,
MetaOptimize can wrap around any first-order optimization algorithm, tuning
step sizes on the fly to minimize a specific form of regret that accounts for
long-term effect of step sizes on training, through a discounted sum of future
losses. We also introduce low complexity variants of MetaOptimize that, in
conjunction with its adaptability to multiple optimization algorithms,
demonstrate performance competitive to those of best hand-crafted learning rate
schedules across various machine learning applications.
\\ ( https://arxiv.org/abs/2402.02342 ,  10948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02345
Date: Sun, 4 Feb 2024 05:03:06 GMT   (41506kb)

Title: Stereographic Spherical Sliced Wasserstein Distances
Authors: Huy Tran, Yikun Bai, Abihith Kothapalli, Ashkan Shahbazi, Xinran Liu,
  Rocio Diaz Martin, Soheil Kolouri
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  Comparing spherical probability distributions is of great interest in various
fields, including geology, medical domains, computer vision, and deep
representation learning. The utility of optimal transport-based distances, such
as the Wasserstein distance, for comparing probability measures has spurred
active research in developing computationally efficient variations of these
distances for spherical probability measures. This paper introduces a
high-speed and highly parallelizable distance for comparing spherical measures
using the stereographic projection and the generalized Radon transform, which
we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance.
We carefully address the distance distortion caused by the stereographic
projection and provide an extensive theoretical analysis of our proposed metric
and its rotationally invariant variation. Finally, we evaluate the performance
of the proposed metrics and compare them with recent baselines in terms of both
speed and accuracy through a wide range of numerical studies, including
gradient flows and self-supervised learning.
\\ ( https://arxiv.org/abs/2402.02345 ,  41506kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02347
Date: Sun, 4 Feb 2024 05:05:43 GMT   (32558kb,D)

Title: Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
Authors: Fangzhao Zhang, Mert Pilanci
Categories: cs.LG math.OC
\\
  In this work we study the enhancement of Low Rank Adaptation (LoRA)
fine-tuning procedure by introducing a Riemannian preconditioner in its
optimization step. Specifically, we introduce an $r\times r$ preconditioner in
each gradient step where $r$ is the LoRA rank. This preconditioner requires a
small change to existing optimizer code and creates virtually minuscule storage
and runtime overhead. Our experimental results with both large language models
and text-to-image diffusion models show that with our preconditioner, the
convergence and reliability of SGD and AdamW can be significantly enhanced.
Moreover, the training process becomes much more robust to hyperparameter
choices such as learning rate. Theoretically, we show that fine-tuning a
two-layer ReLU network in the convex paramaterization with our preconditioner
has convergence rate independent of condition number of the data matrix. This
new Riemannian preconditioner, previously explored in classic low-rank matrix
recovery, is introduced to deep learning tasks for the first time in our work.
We release our code at
https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.
\\ ( https://arxiv.org/abs/2402.02347 ,  32558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02354
Date: Sun, 4 Feb 2024 05:37:37 GMT   (66kb)

Title: A Paradigm for Potential Model Performance Improvement in Classification
  and Regression Problems. A Proof of Concept
Authors: Francisco Javier Lobo-Cabrera
Categories: cs.LG
\\
  A methodology that seeks to enhance model prediction performance is
presented. The method involves generating multiple auxiliary models that
capture relationships between attributes as a function of each other. Such
information serves to generate additional informative columns in the dataset
that can potentially enhance target prediction. A proof of case and related
code is provided.
\\ ( https://arxiv.org/abs/2402.02354 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02355
Date: Sun, 4 Feb 2024 05:41:27 GMT   (29637kb,D)

Title: Symbol: Generating Flexible Black-Box Optimizers through Symbolic
  Equation Learning
Authors: Jiacheng Chen, Zeyuan Ma, Hongshu Guo, Yining Ma, Jie Zhang, Yue-jiao
  Gong
Categories: cs.LG cs.NE
Comments: Accpted as a conference paper at ICLR 2024
\\
  Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness
neural networks to meta-learn configurations of traditional black-box
optimizers. Despite their success, they are inevitably restricted by the
limitations of predefined hand-crafted optimizers. In this paper, we present
\textsc{Symbol}, a novel framework that promotes the automated discovery of
black-box optimizers through symbolic equation learning. Specifically, we
propose a Symbolic Equation Generator (SEG) that allows closed-form
optimization rules to be dynamically generated for specific tasks and
optimization steps. Within \textsc{Symbol}, we then develop three distinct
strategies based on reinforcement learning, so as to meta-learn the SEG
efficiently. Extensive experiments reveal that the optimizers generated by
\textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO
baselines, but also exhibit exceptional zero-shot generalization abilities
across entirely unseen tasks with different problem dimensions, population
sizes, and optimization horizons. Furthermore, we conduct in-depth analyses of
our \textsc{Symbol} framework and the optimization rules that it generates,
underscoring its desirable flexibility and interpretability.
\\ ( https://arxiv.org/abs/2402.02355 ,  29637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02357
Date: Sun, 4 Feb 2024 05:50:38 GMT   (2642kb,D)

Title: Multi-modal Causal Structure Learning and Root Cause Analysis
Authors: Lecheng Zheng, Zhengzhang Chen, Jingrui He, Haifeng Chen
Categories: cs.LG
Comments: Accepted by the Web Conference 2024
\\
  Effective root cause analysis (RCA) is vital for swiftly restoring services,
minimizing losses, and ensuring the smooth operation and management of complex
systems. Previous data-driven RCA methods, particularly those employing causal
discovery techniques, have primarily focused on constructing dependency or
causal graphs for backtracking the root causes. However, these methods often
fall short as they rely solely on data from a single modality, thereby
resulting in suboptimal solutions. In this work, we propose Mulan, a unified
multi-modal causal structure learning method for root cause localization. We
leverage a log-tailored language model to facilitate log representation
learning, converting log sequences into time-series data. To explore intricate
relationships across different modalities, we propose a contrastive
learning-based approach to extract modality-invariant and modality-specific
representations within a shared latent space. Additionally, we introduce a
novel key performance indicator-aware attention mechanism for assessing
modality reliability and co-learning a final causal graph. Finally, we employ
random walk with restart to simulate system fault propagation and identify
potential root causes. Extensive experiments on three real-world datasets
validate the effectiveness of our proposed framework.
\\ ( https://arxiv.org/abs/2402.02357 ,  2642kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02361
Date: Sun, 4 Feb 2024 06:11:12 GMT   (582kb,D)

Title: Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness
Authors: Liang Qiao, Jun Shi, Xiaoyu Hao, Xi Fang, Minfan Zhao, Ziqi Zhu,
  Junshi Chen, Hong An, Bing Li, Honghui Yuan and Xinyang Wang
Categories: cs.LG
\\
  Tensor program optimization on Deep Learning Accelerators (DLAs) is critical
for efficient model deployment. Although search-based Deep Learning Compilers
(DLCs) have achieved significant performance gains compared to manual methods,
they still suffer from the persistent challenges of low search efficiency and
poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$,
following hardware/software co-design principles to hierarchically boost tensor
program optimization. Pruner comprises two primary components: a Parameterized
Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model
($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic
performance analysis tool, guiding the pruning of the search space, while the
latter enables the performance prediction of tensor programs according to the
critical data-flow patterns. Furthermore, to ensure effective cross-platform
adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy
using a Siamese network, which establishes a bidirectional feedback mechanism
to improve the robustness of the pre-trained cost model. The extensive
experimental results demonstrate the effectiveness and advancement of the
proposed Pruner in various tensor program tuning tasks across both online and
offline scenarios, with low resource overhead. The code is available at
https://github.com/qiaolian9/Pruner.
\\ ( https://arxiv.org/abs/2402.02361 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02362
Date: Sun, 4 Feb 2024 06:11:54 GMT   (336kb,D)

Title: Unification of Symmetries Inside Neural Networks: Transformer,
  Feedforward and Neural ODE
Authors: Koji Hashimoto, Yuji Hirono, Akiyoshi Sannai
Categories: cs.LG cs.AI hep-th physics.comp-ph
Comments: 11 pages, 3 figures
Report-no: KUNS-2992
\\
  Understanding the inner workings of neural networks, including transformers,
remains one of the most challenging puzzles in machine learning. This study
introduces a novel approach by applying the principles of gauge symmetries, a
key concept in physics, to neural network architectures. By regarding model
functions as physical observables, we find that parametric redundancies of
various machine learning models can be interpreted as gauge symmetries. We
mathematically formulate the parametric redundancies in neural ODEs, and find
that their gauge symmetries are given by spacetime diffeomorphisms, which play
a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a
continuum version of feedforward neural networks, we show that the parametric
redundancies in feedforward neural networks are indeed lifted to
diffeomorphisms in neural ODEs. We further extend our analysis to transformer
models, finding natural correspondences with neural ODEs and their gauge
symmetries. The concept of gauge symmetries sheds light on the complex behavior
of deep learning models through physics and provides us with a unifying
perspective for analyzing various machine learning architectures.
\\ ( https://arxiv.org/abs/2402.02362 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02364
Date: Sun, 4 Feb 2024 06:23:05 GMT   (8038kb,D)

Title: The Developmental Landscape of In-Context Learning
Authors: Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll,
  Susan Wei, Daniel Murfet
Categories: cs.LG cs.AI
\\
  We show that in-context learning emerges in transformers in discrete
developmental stages, when they are trained on either language modeling or
linear regression tasks. We introduce two methods for detecting the milestones
that separate these stages, by probing the geometry of the population loss in
both parameter space and function space. We study the stages revealed by these
new methods using a range of behavioral and structural metrics to establish
their validity.
\\ ( https://arxiv.org/abs/2402.02364 ,  8038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02366
Date: Sun, 4 Feb 2024 06:37:38 GMT   (14104kb,D)

Title: Transolver: A Fast Transformer Solver for PDEs on General Geometries
Authors: Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, Mingsheng Long
Categories: cs.LG
\\
  Transformers have empowered many milestones across various fields and have
recently been applied to solve partial differential equations (PDEs). However,
since PDEs are typically discretized into large-scale meshes with complex
geometries, it is challenging for Transformers to capture intricate physical
correlations directly from massive individual points. Going beyond superficial
and unwieldy meshes, we present Transolver based on a more foundational idea,
which is learning intrinsic physical states hidden behind discretized
geometries. Specifically, we propose a new Physics-Attention to adaptively
split the discretized domain into a series of learnable slices of flexible
shapes, where mesh points under similar physical states will be ascribed to the
same slice. By calculating attention to physics-aware tokens encoded from
slices, Transovler can effectively capture intricate physical correlations
under complex geometrics, which also empowers the solver with endogenetic
geometry-general modeling capacity and can be efficiently computed in linear
complexity. Transolver achieves consistent state-of-the-art with 22\% relative
gain across six standard benchmarks and also excels in large-scale industrial
simulations, including car and airfoil designs.
\\ ( https://arxiv.org/abs/2402.02366 ,  14104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02368
Date: Sun, 4 Feb 2024 06:55:55 GMT   (4529kb,D)

Title: Timer: Transformers for Time Series Analysis at Scale
Authors: Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang,
  Mingsheng Long
Categories: cs.LG
\\
  Deep learning has contributed remarkably to the advancement of time series
analysis. Still, deep models can encounter performance bottlenecks in
real-world small-sample scenarios, which can be concealed due to the
performance saturation with small models on current benchmarks. Meanwhile,
large models have demonstrated great powers in these scenarios through
large-scale pre-training. Continuous progresses have been achieved as the
emergence of large language models, exhibiting unprecedented ability in
few-shot generalization, scalability, and task generality, which is however
absent in time series models. To change the current practices of training small
models on specific datasets from scratch, this paper aims at an early
development of large time series models (LTSM). During pre-training, we curate
large-scale datasets with up to 1 billion time points, unify heterogeneous time
series into single-series sequence (S3) format, and develop the GPT-style
architecture toward LTSMs. To meet diverse application needs, we convert
forecasting, imputation, and anomaly detection of time series into a unified
generative task. The outcome of this study is a Time Series Transformer
(Timer), that is pre-trained by autoregressive next token prediction on large
multi-domain datasets, and is fine-tuned to downstream scenarios with promising
abilities as an LTSM.
\\ ( https://arxiv.org/abs/2402.02368 ,  4529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02370
Date: Sun, 4 Feb 2024 06:59:21 GMT   (1589kb,D)

Title: AutoTimes: Autoregressive Time Series Forecasters via Large Language
  Models
Authors: Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long
Categories: cs.LG
\\
  Foundation models of time series have not been fully developed due to the
limited availability of large-scale time series and the underexploration of
scalable pre-training. Based on the similar sequential structure of time series
and natural language, increasing research demonstrates the feasibility of
leveraging large language models (LLM) for time series. Nevertheless, prior
methods may overlook the consistency in aligning time series and natural
language, resulting in insufficient utilization of the LLM potentials. To fully
exploit the general-purpose token transitions learned from language modeling,
we propose AutoTimes to repurpose LLMs as Autoregressive Time series
forecasters, which is consistent with the acquisition and utilization of LLMs
without updating the parameters. The consequent forecasters can handle flexible
series lengths and achieve competitive performance as prevalent models.
Further, we present token-wise prompting that utilizes corresponding timestamps
to make our method applicable to multimodal scenarios. Analysis demonstrates
our forecasters inherit zero-shot and in-context learning capabilities of LLMs.
Empirically, AutoTimes exhibits notable method generality and achieves enhanced
performance by basing on larger LLMs, additional texts, or time series as
instructions.
\\ ( https://arxiv.org/abs/2402.02370 ,  1589kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02399
Date: Sun, 4 Feb 2024 08:23:41 GMT   (5263kb,D)

Title: FreDF: Learning to Forecast in Frequency Domain
Authors: Hao Wang, Licheng Pan, Zhichao Chen, Degui Yang, Sen Zhang, Yifei
  Yang, Xinggao Liu, Haoxuan Li, Dacheng Tao
Categories: cs.LG cs.AI stat.AP stat.ML
\\
  Time series modeling is uniquely challenged by the presence of
autocorrelation in both historical and label sequences. Current research
predominantly focuses on handling autocorrelation within the historical
sequence but often neglects its presence in the label sequence. Specifically,
emerging forecast models mainly conform to the direct forecast (DF) paradigm,
generating multi-step forecasts under the assumption of conditional
independence within the label sequence. This assumption disregards the inherent
autocorrelation in the label sequence, thereby limiting the performance of
DF-based models. In response to this gap, we introduce the Frequency-enhanced
Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation
by learning to forecast in the frequency domain. Our experiments demonstrate
that FreDF substantially outperforms existing state-of-the-art methods
including iTransformer and is compatible with a variety of forecast models.
\\ ( https://arxiv.org/abs/2402.02399 ,  5263kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02407
Date: Sun, 4 Feb 2024 08:57:42 GMT   (22500kb,D)

Title: Defining Neural Network Architecture through Polytope Structures of
  Dataset
Authors: Sangmin Lee, Abbas Mammadov, Jong Chul Ye
Categories: cs.LG
\\
  Current theoretical and empirical research in neural networks suggests that
complex datasets require large network architectures for thorough
classification, yet the precise nature of this relationship remains unclear.
This paper tackles this issue by defining upper and lower bounds for neural
network widths, which are informed by the polytope structure of the dataset in
question. We also delve into the application of these principles to simplicial
complexes and specific manifold shapes, explaining how the requirement for
network width varies in accordance with the geometric complexity of the
dataset. Moreover, we develop an algorithm to investigate a converse situation
where the polytope structure of a dataset can be inferred from its
corresponding trained neural networks. Through our algorithm, it is established
that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be
efficiently encapsulated using no more than two polytopes with a small number
of faces.
\\ ( https://arxiv.org/abs/2402.02407 ,  22500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02423
Date: Sun, 4 Feb 2024 09:40:22 GMT   (4487kb,D)

Title: Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement
  Learning with Diverse Human Feedback
Authors: Yifu Yuan, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu,
  Zhixin Feng, Kai Zhao, Yan Zheng
Categories: cs.LG cs.AI cs.RO
Comments: Published as a conference paper at ICLR 2024. The website is
  available at https://uni-rlhf.github.io/
\\
  Reinforcement Learning with Human Feedback (RLHF) has received significant
attention for performing tasks without the need for costly manual reward design
by aligning human preferences. It is crucial to consider diverse human feedback
types and various learning methods in different environments. However,
quantifying progress in RLHF with diverse feedback is challenging due to the
lack of standardized annotation platforms and widely used unified benchmarks.
To bridge this gap, we introduce Uni-RLHF, a comprehensive system
implementation tailored for RLHF. It aims to provide a complete workflow from
real human feedback, fostering progress in the development of practical
problems. Uni-RLHF contains three packages: 1) a universal multi-feedback
annotation platform, 2) large-scale crowdsourced feedback datasets, and 3)
modular offline RLHF baseline implementations. Uni-RLHF develops a
user-friendly annotation interface tailored to various feedback types,
compatible with a wide range of mainstream RL environments. We then establish a
systematic pipeline of crowdsourced annotations, resulting in large-scale
annotated datasets comprising more than 15 million steps across 30+ popular
tasks. Through extensive experiments, the results in the collected datasets
demonstrate competitive performance compared to those from well-designed manual
rewards. We evaluate various design choices and offer insights into their
strengths and potential areas of improvement. We wish to build valuable
open-source platforms, datasets, and baselines to facilitate the development of
more robust and reliable RLHF solutions based on realistic human feedback. The
website is available at https://uni-rlhf.github.io/.
\\ ( https://arxiv.org/abs/2402.02423 ,  4487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02425
Date: Sun, 4 Feb 2024 09:45:35 GMT   (4457kb,D)

Title: EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics
Authors: Qilong Ma, Haixu Wu, Lanxiang Xing, Jianmin Wang, Mingsheng Long
Categories: cs.LG physics.flu-dyn
\\
  Accurately predicting the future fluid is important to extensive areas, such
as meteorology, oceanology and aerodynamics. However, since the fluid is
usually observed from an Eulerian perspective, its active and intricate
dynamics are seriously obscured and confounded in static grids, bringing horny
challenges to the prediction. This paper introduces a new Lagrangian-guided
paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting
the future based on Eulerian observations, we propose the Eulerian-Lagrangian
Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by
tracking movements of adaptively sampled key particles on multiple scales and
integrating dynamics information over time. Concretely, a EuLag Block is
presented to communicate the learned Eulerian and Lagrangian features at each
moment and scale, where the motion of tracked particles is inferred from
Eulerian observations and their accumulated dynamics information is
incorporated into Eulerian fields to guide future prediction. Tracking key
particles not only provides a clear and interpretable clue for fluid dynamics
but also makes our model free from modeling complex correlations among massive
grids for better efficiency. Experimentally, EuLagNet excels in three
challenging fluid prediction tasks, covering both 2D and 3D, simulated and
real-world fluids.
\\ ( https://arxiv.org/abs/2402.02425 ,  4457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02429
Date: Sun, 4 Feb 2024 09:58:42 GMT   (5765kb,D)

Title: Towards an Information Theoretic Framework of Context-Based Offline
  Meta-Reinforcement Learning
Authors: Lanqing Li, Hai Zhang, Xinyu Zhang, Shatong Zhu, Junqiao Zhao,
  Pheng-Ann Heng
Categories: cs.LG
Comments: 20 pages, 8 figures, 5 tables. TLDR: We propose a novel information
  theoretic framework of the context-based offline meta-RL paradigm, which
  unifies several mainstream methods and leads to a general and
  state-of-the-art algorithm called UNICORN
\\
  As a marriage between offline RL and meta-RL, the advent of offline
meta-reinforcement learning (OMRL) has shown great promise in enabling RL
agents to multi-task and quickly adapt while acquiring knowledge safely. Among
which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a
universal policy conditioned on effective task representations. In this work,
by examining several key milestones in the field of COMRL, we propose to
integrate these seemingly independent methodologies into a unified information
theoretic framework. Most importantly, we show that the pre-existing COMRL
algorithms are essentially optimizing the same mutual information objective
between the task variable $\boldsymbol{M}$ and its latent representation
$\boldsymbol{Z}$ by implementing various approximate bounds. Based on the
theoretical insight and the information bottleneck principle, we arrive at a
novel algorithm dubbed UNICORN, which exhibits remarkable generalization across
a broad spectrum of RL benchmarks, context shift scenarios, data qualities and
deep learning architectures, attaining the new state-of-the-art. We believe
that our framework could open up avenues for new optimality bounds and COMRL
algorithms.
\\ ( https://arxiv.org/abs/2402.02429 ,  5765kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02438
Date: Sun, 4 Feb 2024 10:27:42 GMT   (65kb)

Title: Fast and interpretable Support Vector Classification based on the
  truncated ANOVA decomposition
Authors: Kseniya Akhalaya, Franziska Nestler, Daniel Potts
Categories: cs.LG cs.NA math.NA
\\
  Support Vector Machines (SVMs) are an important tool for performing
classification on scattered data, where one usually has to deal with many data
points in high-dimensional spaces. We propose solving SVMs in primal form using
feature maps based on trigonometric functions or wavelets. In small dimensional
settings the Fast Fourier Transform (FFT) and related methods are a powerful
tool in order to deal with the considered basis functions. For growing
dimensions the classical FFT-based methods become inefficient due to the curse
of dimensionality. Therefore, we restrict ourselves to multivariate basis
functions, each one of them depends only on a small number of dimensions. This
is motivated by the well-known sparsity of effects and recent results regarding
the reconstruction of functions from scattered data in terms of truncated
analysis of variance (ANOVA) decomposition, which makes the resulting model
even interpretable in terms of importance of the features as well as their
couplings. The usage of small superposition dimensions has the consequence that
the computational effort no longer grows exponentially but only polynomially
with respect to the dimension. In order to enforce sparsity regarding the basis
coefficients, we use the frequently applied $\ell_2$-norm and, in addition,
$\ell_1$-norm regularization. The found classifying function, which is the
linear combination of basis functions, and its variance can then be analyzed in
terms of the classical ANOVA decomposition of functions. Based on numerical
examples we show that we are able to recover the signum of a function that
perfectly fits our model assumptions. We obtain better results with
$\ell_1$-norm regularization, both in terms of accuracy and clarity of
interpretability.
\\ ( https://arxiv.org/abs/2402.02438 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02439
Date: Sun, 4 Feb 2024 10:30:23 GMT   (4549kb,D)

Title: DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based
  Trajectory Stitching
Authors: Guanghe Li, Yixiang Shan, Zhengbang Zhu, Ting Long, Weinan Zhang
Categories: cs.LG cs.AI
\\
  In offline reinforcement learning (RL), the performance of the learned policy
highly depends on the quality of offline datasets. However, in many cases, the
offline dataset contains very limited optimal trajectories, which poses a
challenge for offline RL algorithms as agents must acquire the ability to
transit to high-reward regions. To address this issue, we introduce
Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data
augmentation pipeline that systematically generates stitching transitions
between trajectories. DiffStitch effectively connects low-reward trajectories
with high-reward trajectories, forming globally optimal trajectories to address
the challenges faced by offline RL algorithms. Empirical experiments conducted
on D4RL datasets demonstrate the effectiveness of DiffStitch across RL
methodologies. Notably, DiffStitch demonstrates substantial enhancements in the
performance of one-step methods (IQL), imitation learning methods (TD3+BC), and
trajectory optimization methods (DT).
\\ ( https://arxiv.org/abs/2402.02439 ,  4549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02441
Date: Sun, 4 Feb 2024 10:41:40 GMT   (33kb,D)

Title: TopoX: A Suite of Python Packages for Machine Learning on Topological
  Domains
Authors: Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Agerberg,
  Ibrahem AlJabea, Ruben Ballester, Claudio Battiloro, Guillermo Bern\'ardez,
  Tolga Birdal, Aiden Brent, Peter Chin, Sergio Escalera, Odin Hoff Gardaa,
  Gurusankar Gopalakrishnan, Devendra Govil, Josef Hoppe, Maneel Reddy Karri,
  Jude Khouja, Manuel Lecha, Neal Livesay, Jan Mei{\ss}ner, Soham Mukherjee,
  Alexander Nikitin, Theodore Papamarkou, Jaro Pr'{i}lepok, Karthikeyan Natesan
  Ramamurthy, Paul Rosen, Aldo Guzm'{a}n-S'{a}enz, Alessandro Salatiello,
  Shreyas N. Samaga, Michael T. Schaub, Luca Scofano, Indro Spinelli, Lev
  Telyatnikov, Quang Truong, Robin Walters, Maosheng Yang, Olga Zaghen, Ghada
  Zamzmi, Ali Zia, Nina Miolane
Categories: cs.LG cs.AI stat.CO
\\
  We introduce topox, a Python software suite that provides reliable and
user-friendly building blocks for computing and machine learning on topological
domains that extend graphs: hypergraphs, simplicial, cellular, path and
combinatorial complexes. topox consists of three packages: toponetx facilitates
constructing and computing on these domains, including working with nodes,
edges and higher-order cells; topoembedx provides methods to embed topological
domains into vector spaces, akin to popular graph-based embedding algorithms
such as node2vec; topomodelx is built on top of PyTorch and offers a
comprehensive toolbox of higher-order message passing functions for neural
networks on topological domains. The extensively documented and unit-tested
source code of topox is available under MIT license at
https://github.com/pyt-team.
\\ ( https://arxiv.org/abs/2402.02441 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02442
Date: Sun, 4 Feb 2024 10:43:35 GMT   (1755kb,D)

Title: A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix
  Decomposition
Authors: Qingsong Wang, Chunfeng Cui, Deren Han
Categories: cs.LG eess.IV
Comments: 5 pages, 7 figures
\\
  Recently, there has been a growing interest in the exploration of Nonlinear
Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims
to find a low-rank matrix from a sparse nonnegative matrix with a per-element
nonlinear function. A typical choice is the Rectified Linear Unit (ReLU)
activation function. To address over-fitting in the existing ReLU-based NMD
model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to
as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for
handling the ReLU-NMD-T model. A distinctive feature, setting our work apart
from most existing studies, is the incorporation of both positive and negative
momentum parameters in our algorithm. Our numerical experiments on real-world
datasets show the effectiveness of the proposed model and algorithm. Moreover,
the code is available at https://github.com/nothing2wang/NMD-TM.
\\ ( https://arxiv.org/abs/2402.02442 ,  1755kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02446
Date: Sun, 4 Feb 2024 10:59:52 GMT   (403kb,D)

Title: LQER: Low-Rank Quantization Error Reconstruction for LLMs
Authors: Cheng Zhang, Jianyi Cheng, George A. Constantinides, and Yiren Zhao
Categories: cs.LG
\\
  Post-training quantization of Large Language Models (LLMs) is challenging. In
this work, we introduce Low-rank Quantization Error Reduction (LQER), which
combines quantization and low-rank approximation to recover the model
capability. LQER leverages an activation-induced scale matrix to drive the
singular value distribution of quantization error towards a desirable
distribution, which enables nearly-lossless W4A8 quantization on various LLMs
and downstream tasks without the need for knowledge distillation, grid search,
or gradient-base iterative optimization. Unlike existing methods, the
computation pattern of LQER eliminates the need for specialized Scatter and
Gather processes to collect high-precision weights from irregular memory
locations. Our W4A8 LLMs achieve near-lossless performance on six popular
downstream tasks, while using 1.36$\times$ fewer hardware resources than the
leading state-of-the-art method. We will open-source our framework once the
paper is accepted.
\\ ( https://arxiv.org/abs/2402.02446 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02447
Date: Sun, 4 Feb 2024 11:12:17 GMT   (1842kb,D)

Title: Breaking MLPerf Training: A Case Study on Optimizing BERT
Authors: Yongdeok Kim, Jaehyung Ahn, Myeongwoo Kim, Changin Choi, Heejae Kim,
  Narankhuu Tuvshinjargal, Seungwon Lee, Yanzi Zhang, Yuan Pei, Xiongzhan
  Linghu, Jingkun Ma, Lin Chen, Yuehua Dai, Sungjoo Yoo
Categories: cs.LG cs.CL
Comments: Total 15 pages (Appendix 3 pages)
\\
  Speeding up the large-scale distributed training is challenging in that it
requires improving various components of training including load balancing,
communication, optimizers, etc. We present novel approaches for fast
large-scale training of BERT model which individually ameliorates each
component thereby leading to a new level of BERT training performance. Load
balancing is imperative in distributed BERT training since its training
datasets are characterized by samples with various lengths. Communication cost,
which is proportional to the scale of distributed training, needs to be hidden
by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc.,
need to be carefully re-evaluated in the context of large-scale distributed
training. We propose two new ideas, (1) local presorting based on dataset
stratification for load balancing and (2) bucket-wise gradient clipping before
allreduce which allows us to benefit from the overlap of gradient computation
and synchronization as well as the fast training of gradient clipping before
allreduce. We also re-evaluate existing optimizers via hyperparameter
optimization and utilize ADAM, which also contributes to fast training via
larger batches than existing methods. Our proposed methods, all combined, give
the fastest MLPerf BERT training of 25.1 (22.3) seconds on 1,024 NVIDIA A100
GPUs, which is 1.33x (1.13x) and 1.57x faster than the other top two (one)
submissions to MLPerf v1.1 (v2.0). Our implementation and evaluation results
are available at MLPerf v1.1~v2.1.
\\ ( https://arxiv.org/abs/2402.02447 ,  1842kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02454
Date: Sun, 4 Feb 2024 11:54:07 GMT   (970kb,D)

Title: On the Role of Initialization on the Implicit Bias in Deep Linear
  Networks
Authors: Oria Gruber, Haim Avron
Categories: cs.LG cs.NA math.NA
\\
  Despite Deep Learning's (DL) empirical success, our theoretical understanding
of its efficacy remains limited. One notable paradox is that while conventional
wisdom discourages perfect data fitting, deep neural networks are designed to
do just that, yet they generalize effectively. This study focuses on exploring
this phenomenon attributed to the implicit bias at play. Various sources of
implicit bias have been identified, such as step size, weight initialization,
optimization algorithm, and number of parameters. In this work, we focus on
investigating the implicit bias originating from weight initialization. To this
end, we examine the problem of solving underdetermined linear systems in
various contexts, scrutinizing the impact of initialization on the implicit
regularization when using deep networks to solve such systems. Our findings
elucidate the role of initialization in the optimization and generalization
paradoxes, contributing to a more comprehensive understanding of DL's
performance characteristics.
\\ ( https://arxiv.org/abs/2402.02454 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02456
Date: Sun, 4 Feb 2024 12:06:13 GMT   (1663kb,D)

Title: Discovering More Effective Tensor Network Structure Search Algorithms
  via Large Language Models (LLMs)
Authors: Junhua Zeng, Guoxu Zhou, Chao Li, Zhun Sun, Qibin Zhao
Categories: cs.LG
\\
  Tensor network structure search (TN-SS), aiming at searching for suitable
tensor network (TN) structures in representing high-dimensional problems,
largely promotes the efficacy of TN in various machine learning applications.
Nonetheless, finding a satisfactory TN structure using existing algorithms
remains challenging. To develop more effective algorithms and avoid the human
labor-intensive development process, we explore the knowledge embedded in large
language models (LLMs) for the automatic design of TN-SS algorithms. Our
approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting
system that operates in an evolutionary-like manner. The experimental results,
derived from real-world data, demonstrate that GPTN-SS can effectively leverage
the insights gained from existing methods to develop novel TN-SS algorithms
that achieve a better balance between exploration and exploitation. These
algorithms exhibit superior performance in searching the high-quality TN
structures for natural image compression and model parameters compression while
also demonstrating generalizability in their performance.
\\ ( https://arxiv.org/abs/2402.02456 ,  1663kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02460
Date: Sun, 4 Feb 2024 12:21:38 GMT   (369kb,D)

Title: Review of multimodal machine learning approaches in healthcare
Authors: Felix Krones, Umar Marikkar, Guy Parsons, Adam Szmul, Adam Mahdi
Categories: cs.LG
Comments: 5 figures, 5 tables
\\
  Machine learning methods in healthcare have traditionally focused on using
data from a single modality, limiting their ability to effectively replicate
the clinical practice of integrating multiple sources of information for
improved decision making. Clinicians typically rely on a variety of data
sources including patients' demographic information, laboratory data, vital
signs and various imaging data modalities to make informed decisions and
contextualise their findings. Recent advances in machine learning have
facilitated the more efficient incorporation of multimodal data, resulting in
applications that better represent the clinician's approach. Here, we provide a
review of multimodal machine learning approaches in healthcare, offering a
comprehensive overview of recent literature. We discuss the various data
modalities used in clinical diagnosis, with a particular emphasis on imaging
data. We evaluate fusion techniques, explore existing multimodal datasets and
examine common training strategies.
\\ ( https://arxiv.org/abs/2402.02460 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02463
Date: Sun, 4 Feb 2024 12:24:52 GMT   (238kb,D)

Title: A Fast Method for Lasso and Logistic Lasso
Authors: Siu-Wing Cheng, Man Ting Wong
Categories: cs.LG stat.ML
\\
  We propose a fast method for solving compressed sensing, Lasso regression,
and Logistic Lasso regression problems that iteratively runs an appropriate
solver using an active set approach. We design a strategy to update the active
set that achieves a large speedup over a single call of several solvers,
including gradient projection for sparse reconstruction (GPSR), lassoglm of
Matlab, and glmnet. For compressed sensing, the hybrid of our method and GPSR
is 31.41 times faster than GPSR on average for Gaussian ensembles and 25.64
faster on average for binary ensembles. For Lasso regression, the hybrid of our
method and GPSR achieves a 30.67-fold average speedup in our experiments. In
our experiments on Logistic Lasso regression, the hybrid of our method and
lassoglm gives an 11.95-fold average speedup, and the hybrid of our method and
glmnet gives a 1.40-fold average speedup.
\\ ( https://arxiv.org/abs/2402.02463 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02464
Date: Sun, 4 Feb 2024 12:29:40 GMT   (14051kb,D)

Title: A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
Authors: Zhangyang Gao, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z. Li
Categories: cs.LG cs.AI
\\
  Can we model non-Euclidean graphs as pure language or even Euclidean vectors
while retaining their inherent information? The non-Euclidean property have
posed a long term challenge in graph modeling. Despite recent GNN and
Graphformer efforts encoding graphs as Euclidean vectors, recovering original
graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a
Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph
words in a Euclidean space, along with a GraphGPT decoder that reconstructs the
original graph from graph words to ensure information equivalence. We pretrain
GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained
Graph2Seq excels in graph representation learning, achieving state-of-the-art
results on 8/9 graph classification and regression tasks. (2) Pretrained
GraphGPT serves as a strong graph generator, demonstrated by its ability to
perform both unconditional and conditional graph generation. (3)
Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space,
overcoming previously known non-Euclidean challenge. (4) Our proposed novel
edge-centric GPT pretraining task is effective in graph fields, underscoring
its success in both representation and generation.
\\ ( https://arxiv.org/abs/2402.02464 ,  14051kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02475
Date: Sun, 4 Feb 2024 13:10:51 GMT   (9482kb,D)

Title: TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling
Authors: Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin
  Wang, Mingsheng Long
Categories: cs.LG
\\
  Time series pre-training has recently garnered wide attention for its
potential to reduce labeling expenses and benefit various downstream tasks.
Prior methods are mainly based on pre-training techniques well-acknowledged in
vision or language, such as masked modeling and contrastive learning. However,
randomly masking time series or calculating series-wise similarity will distort
or neglect inherent temporal correlations crucial in time series data. To
emphasize temporal correlation modeling, this paper proposes TimeSiam as a
simple but effective self-supervised pre-training framework for Time series
based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to
capture intrinsic temporal correlations between randomly sampled past and
current subseries. With a simple data augmentation method (e.g.~masking),
TimeSiam can benefit from diverse augmented subseries and learn internal
time-dependent representations through a past-to-current reconstruction.
Moreover, learnable lineage embeddings are also introduced to distinguish
temporal distance between sampled series and further foster the learning of
diverse temporal correlations. TimeSiam consistently outperforms extensive
advanced pre-training baselines, demonstrating superior forecasting and
classification capabilities across 13 standard benchmarks in both intra- and
cross-domain scenarios.
\\ ( https://arxiv.org/abs/2402.02475 ,  9482kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02478
Date: Sun, 4 Feb 2024 13:15:59 GMT   (4503kb,D)

Title: Why are hyperbolic neural networks effective? A study on hierarchical
  representation capability
Authors: Shicheng Tan, Huanjing Zhao, Shu Zhao, Yanping Zhang
Categories: cs.LG cs.AI
\\
  Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been
widely applied in recent years, motivated by the existence of an optimal
embedding in hyperbolic space that can preserve data hierarchical relationships
(termed Hierarchical Representation Capability, HRC) more accurately than
Euclidean space. However, there is no evidence to suggest that HNNs can achieve
this theoretical optimal embedding, leading to much research being built on
flawed motivations. In this paper, we propose a benchmark for evaluating HRC
and conduct a comprehensive analysis of why HNNs are effective through
large-scale experiments. Inspired by the analysis results, we propose several
pre-training strategies to enhance HRC and improve the performance of
downstream tasks, further validating the reliability of the analysis.
Experiments show that HNNs cannot achieve the theoretical optimal embedding.
The HRC is significantly affected by the optimization objectives and
hierarchical structures, and enhancing HRC through pre-training strategies can
significantly improve the performance of HNNs.
\\ ( https://arxiv.org/abs/2402.02478 ,  4503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02479
Date: Sun, 4 Feb 2024 13:16:29 GMT   (1757kb)

Title: BRAIn: Bayesian Reward-conditioned Amortized Inference for natural
  language generation from feedback
Authors: Gaurav Pandey, Yatin Nandwani, Tahira Naseem, Mayank Mishra, Guangxuan
  Xu, Dinesh Raghu, Sachindra Joshi, Asim Munawar, Ram\'on Fernandez Astudillo
Categories: cs.LG cs.AI
Comments: Under review
\\
  Following the success of Proximal Policy Optimization (PPO) for Reinforcement
Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood
Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that
are offline in nature and use rewards in an indirect manner. These techniques,
in particular DPO, have recently become the tools of choice for LLM alignment
due to their scalability and performance. However, they leave behind important
features of the PPO approach. Methods such as SLiC or RRHF make use of the
Reward Model (RM) only for ranking/preference, losing fine-grained information
and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce),
while methods such as DPO do not use even a separate reward model. In this
work, we propose a novel approach, named BRAIn, that re-introduces the RM as
part of a distribution matching approach.BRAIn considers the LLM distribution
conditioned on the assumption of output goodness and applies Bayes theorem to
derive an intractable posterior distribution where the RM is explicitly
represented. BRAIn then distills this posterior into an amortized inference
network through self-normalized importance sampling, leading to a scalable
offline algorithm that significantly outperforms prior art in summarization and
AntropicHH tasks. BRAIn also has interesting connections to PPO and DPO for
specific RM choices.
\\ ( https://arxiv.org/abs/2402.02479 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02484
Date: Sun, 4 Feb 2024 13:25:18 GMT   (363kb,D)

Title: Weisfeiler Leman for Euclidean Equivariant Machine Learning
Authors: Snir Hordan, Tal Amir, Nadav Dym
Categories: cs.LG
\\
  The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common
method for assessing the expressive power of graph neural networks (GNNs).
Recently, the $2$-WL test was proven to be complete on weighted graphs which
encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive
power is equivalent to the $2$-WL test are provably universal on point clouds.
Yet, this result is limited to invariant continuous functions on point clouds.
  In this paper we extend this result in three ways: Firstly, we show that
$2$-WL tests can be extended to point clouds which include both positions and
velocity, a scenario often encountered in applications. Secondly, we show that
PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds
with low complexity. Finally, we show that a simple modification of this PPGN
architecture can be used to obtain a universal equivariant architecture that
can approximate all continuous equivariant functions uniformly.
  Building on our results, we develop our WeLNet architecture, which can
process position-velocity pairs, compute functions fully equivariant to
permutations and rigid motions, and is provably complete and universal.
Remarkably, WeLNet is provably complete precisely in the setting in which it is
implemented in practice. Our theoretical results are complemented by
experiments showing WeLNet sets new state-of-the-art results on the N-Body
dynamics task and the GEOM-QM9 molecular conformation generation task.
\\ ( https://arxiv.org/abs/2402.02484 ,  363kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02513
Date: Sun, 4 Feb 2024 14:57:20 GMT   (765kb)

Title: Early stopping by correlating online indicators in neural networks
Authors: Manuel Vilares Ferro, Yerai Doval Mosquera, Francisco J. Ribadas Pena,
  Victor M. Darriba Bilbao
Categories: cs.LG cs.AI cs.CL cs.NE
Comments: 26 pages, 6 figures
Journal-ref: Manuel Vilares Ferro, Yerai Doval Mosquera, Francisco J. Ribadas
  Pena, Victor M. Darriba Bilbao. Early stopping by correlating online
  indicators in neural networks. Neural Networks, 159 (2023), pp 109-124. ISSN
  1879-2782. Elsevier
DOI: 10.1016/j.jcss.2022.05.002
\\
  In order to minimize the generalization error in neural networks, a novel
technique to identify overfitting phenomena when training the learner is
formally introduced. This enables support of a reliable and trustworthy early
stopping condition, thus improving the predictive power of that type of
modeling. Our proposal exploits the correlation over time in a collection of
online indicators, namely characteristic functions for indicating if a set of
hypotheses are met, associated with a range of independent stopping conditions
built from a canary judgment to evaluate the presence of overfitting. That way,
we provide a formal basis for decision making in terms of interrupting the
learning process.
  As opposed to previous approaches focused on a single criterion, we take
advantage of subsidiarities between independent assessments, thus seeking both
a wider operating range and greater diagnostic reliability. With a view to
illustrating the effectiveness of the halting condition described, we choose to
work in the sphere of natural language processing, an operational continuum
increasingly based on machine learning. As a case study, we focus on parser
generation, one of the most demanding and complex tasks in the domain. The
selection of cross-validation as a canary function enables an actual comparison
with the most representative early stopping conditions based on overfitting
identification, pointing to a promising start toward an optimal bias and
variance control.
\\ ( https://arxiv.org/abs/2402.02513 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02518
Date: Sun, 4 Feb 2024 15:03:47 GMT   (97kb)

Title: Latent Graph Diffusion: A Unified Framework for Generation and
  Prediction on Graphs
Authors: Zhou Cai, Xiyuan Wang, Muhan Zhang
Categories: cs.LG
\\
  In this paper, we propose the first framework that enables solving graph
learning tasks of all levels (node, edge and graph) and all types (generation,
regression and classification) with one model. We first propose Latent Graph
Diffusion (LGD), a generative model that can generate node, edge, and
graph-level features of all categories simultaneously. We achieve this goal by
embedding the graph structures and features into a latent space leveraging a
powerful encoder which can also be decoded, then training a diffusion model in
the latent space. LGD is also capable of conditional generation through a
specifically designed cross-attention mechanism. Then we formulate prediction
tasks including regression and classification as (conditional) generation,
which enables our LGD to solve tasks of all levels and all types with provable
guarantees. We verify the effectiveness of our framework with extensive
experiments, where our models achieve state-of-the-art or highly competitive
results across generation and regression tasks.
\\ ( https://arxiv.org/abs/2402.02518 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02526
Date: Sun, 4 Feb 2024 15:17:09 GMT   (1110kb,D)

Title: CompeteSMoE - Effective Training of Sparse Mixture of Experts via
  Competition
Authors: Quang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina
  Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, Nhat Ho
Categories: cs.LG
\\
  Sparse mixture of experts (SMoE) offers an appealing solution to scale up the
model complexity beyond the mean of increasing the network's depth or width.
However, effective training of SMoE has proven to be challenging due to the
representation collapse issue, which causes parameter redundancy and limited
representation potentials. In this work, we propose a competition mechanism to
address this fundamental challenge of representation collapse. By routing
inputs only to experts with the highest neural response, we show that, under
mild assumptions, competition enjoys the same convergence rate as the optimal
estimator. We further propose CompeteSMoE, an effective and efficient algorithm
to train large language models by deploying a simple router that predicts the
competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains
from the competition routing policy while having low computation overheads. Our
extensive empirical evaluations on two transformer architectures and a wide
range of tasks demonstrate the efficacy, robustness, and scalability of
CompeteSMoE compared to state-of-the-art SMoE strategies.
\\ ( https://arxiv.org/abs/2402.02526 ,  1110kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02561
Date: Sun, 4 Feb 2024 16:27:37 GMT   (267kb,D)

Title: Foundation Model Makes Clustering a Better Initialization for Active
  Learning
Authors: Han Yuan and Chuan Hong
Categories: cs.LG
\\
  Active learning selects the most informative samples from the unlabeled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model initialization. Most of the
previous studies resort to random sampling or naive clustering. However, random
sampling is prone to fluctuation, and naive clustering suffers from convergence
speed, particularly when dealing with high-dimensional data such as imaging
data. In this work, we propose to integrate foundation models with clustering
methods to select samples for active learning initialization. Foundation models
refer to those trained on massive datasets by the self-supervised paradigm and
capable of generating informative and compacted embeddings for various
downstream tasks. Leveraging these embeddings to replace raw features such as
pixel values, clustering quickly converges and identifies better initial
samples. For a comprehensive comparison, we included a classic
ImageNet-supervised model to acquire embeddings. Experiments on two clinical
tasks of image classification and segmentation demonstrated that foundation
model-based clustering efficiently pinpointed informative initial samples,
leading to models showcasing enhanced performance than the baseline methods. We
envisage that this study provides an effective paradigm for future active
learning.
\\ ( https://arxiv.org/abs/2402.02561 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02586
Date: Sun, 4 Feb 2024 19:04:37 GMT   (4065kb,D)

Title: ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars
  for Write Noise Mitigation
Authors: Abhiroop Bhattacharjee, Abhishek Moitra, and Priyadarshini Panda
Categories: cs.LG cs.ET
Comments: 9 pages, 10 figures, 3 tables, 1 appendix
\\
  Transformers have revolutionized various real-world applications from natural
language processing to computer vision. However, traditional von-Neumann
computing paradigm faces memory and bandwidth limitations in accelerating
transformers owing to their massive model sizes. To this end, In-memory
Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their
ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs)
with high energy-efficiencies, have emerged as a promising solution for
accelerating transformers. However, analog MVM operations in crossbars
introduce non-idealities, such as stochastic read & write noise, which affect
the inference accuracy of the deployed transformers. Specifically, we find
pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the
impact of write noise on the dynamically-generated Key (K) and Value (V)
matrices in the attention layers, an effect not accounted for in prior studies.
We, thus, propose ClipFormer, a transformation on the K and V matrices during
inference, to boost the non-ideal accuracies of pre-trained ViT models.
ClipFormer requires no additional hardware and training overhead and is
amenable to transformers deployed on any memristive crossbar platform. Our
experiments on Imagenet-1k dataset using pre-trained DeiT-S transformers,
subjected to standard training and variation-aware-training, show >10-40%
higher non-ideal accuracies at the high write noise regime by applying
ClipFormer.
\\ ( https://arxiv.org/abs/2402.02586 ,  4065kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02592
Date: Sun, 4 Feb 2024 20:00:45 GMT   (877kb,D)

Title: Unified Training of Universal Time Series Forecasting Transformers
Authors: Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio
  Savarese, Doyen Sahoo
Categories: cs.LG cs.AI
\\
  Deep learning for time series forecasting has traditionally operated within a
one-model-per-dataset framework, limiting its potential to leverage the
game-changing impact of large pre-trained models. The concept of universal
forecasting, emerging from pre-training on a vast collection of time series
datasets, envisions a single Large Time Series Model capable of addressing
diverse downstream forecasting tasks. However, constructing such a model poses
unique challenges specific to time series data: i) cross-frequency learning,
ii) accommodating an arbitrary number of variates for multivariate time series,
and iii) addressing the varying distributional properties inherent in
large-scale data. To address these challenges, we present novel enhancements to
the conventional time series Transformer architecture, resulting in our
proposed Masked Encoder-based Universal Time Series Forecasting Transformer
(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive
(LOTSA) featuring over 27B observations across nine domains, Moirai achieves
competitive or superior performance as a zero-shot forecaster when compared to
full-shot models. Code, model weights, and data will be released.
\\ ( https://arxiv.org/abs/2402.02592 ,  877kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02593
Date: Sun, 4 Feb 2024 20:01:22 GMT   (1898kb,D)

Title: Leveraging Continuously Differentiable Activation Functions for Learning
  in Quantized Noisy Environments
Authors: Vivswan Shah and Nathan Youngblood
Categories: cs.LG
\\
  Real-world analog systems intrinsically suffer from noise that can impede
model convergence and accuracy on a variety of deep learning models. We
demonstrate that differentiable activations like GELU and SiLU enable robust
propagation of gradients which help to mitigate analog quantization error that
is ubiquitous to all analog systems. We perform analysis and training of
convolutional, linear, and transformer networks in the presence of quantized
noise. Here, we are able to demonstrate that continuously differentiable
activation functions are significantly more noise resilient over conventional
rectified activations. As in the case of ReLU, the error in gradients are 100x
higher than those in GELU near zero. Our findings provide guidance for
selecting appropriate activations to realize performant and reliable hardware
implementations across several machine learning domains such as computer
vision, signal processing, and beyond.
\\ ( https://arxiv.org/abs/2402.02593 ,  1898kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02596
Date: Sun, 4 Feb 2024 20:06:20 GMT   (1196kb,D)

Title: Dual Interior-Point Optimization Learning
Authors: Michael Klamkin, Mathieu Tanneau, Pascal Van Hentenryck
Categories: cs.LG math.OC
\\
  This paper introduces Dual Interior Point Learning (DIPL) and Dual
Supergradient Learning (DSL) to learn dual feasible solutions to parametric
linear programs with bounded variables, which are pervasive across many
industries. DIPL mimics a novel dual interior point algorithm while DSL mimics
classical dual supergradient ascent. DIPL and DSL ensure dual feasibility by
predicting dual variables associated with the constraints then exploiting the
flexibility of the duals of the bound constraints. DIPL and DSL complement
existing primal learning methods by providing a certificate of quality. They
are shown to produce high-fidelity dual-feasible solutions to large-scale
optimal power flow problems providing valid dual bounds under 0.5% optimality
gap.
\\ ( https://arxiv.org/abs/2402.02596 ,  1196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02608
Date: Sun, 4 Feb 2024 20:49:53 GMT   (10004kb,D)

Title: Accelerating Inverse Reinforcement Learning with Expert Bootstrapping
Authors: David Wu and Sanjiban Choudhury
Categories: cs.LG
\\
  Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL)
search over candidate reward functions and solve a reinforcement learning
problem in the inner loop. This creates a rather strange inversion where a
harder problem, reinforcement learning, is in the inner loop of a presumably
easier problem, imitation learning. In this work, we show that better
utilization of expert demonstrations can reduce the need for hard exploration
in the inner RL loop, hence accelerating learning. Specifically, we propose two
simple recipes: (1) placing expert transitions into the replay buffer of the
inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner
about high reward states instead of forcing the learner to discover them
through extensive exploration, and (2) using expert actions in Q value
bootstrapping in order to improve the target Q value estimates and more
accurately describe high value expert states. Our methods show significant
gains over a MaxEntIRL baseline on the benchmark MuJoCo suite of tasks,
speeding up recovery to 70\% of deterministic expert performance by 2.13x on
HalfCheetah-v2, 2.6x on Ant-v2, 18x on Hopper-v2, and 3.36x on Walker2d-v2.
\\ ( https://arxiv.org/abs/2402.02608 ,  10004kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02616
Date: Sun, 4 Feb 2024 21:22:29 GMT   (10979kb,D)

Title: The Virtues of Pessimism in Inverse Reinforcement Learning
Authors: David Wu and Gokul Swamy and J. Andrew Bagnell and Zhiwei Steven Wu
  and Sanjiban Choudhury
Categories: cs.LG
\\
  Inverse Reinforcement Learning (IRL) is a powerful framework for learning
complex behaviors from expert demonstrations. However, it traditionally
requires repeatedly solving a computationally expensive reinforcement learning
(RL) problem in its inner loop. It is desirable to reduce the exploration
burden by leveraging expert demonstrations in the inner-loop RL. As an example,
recent work resets the learner to expert states in order to inform the learner
of high-reward expert states. However, such an approach is infeasible in the
real world. In this work, we consider an alternative approach to speeding up
the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's
data distribution, instantiated via the use of offline RL algorithms. We
formalize a connection between offline RL and IRL, enabling us to use an
arbitrary offline RL algorithm to improve the sample efficiency of IRL. We
validate our theory experimentally by demonstrating a strong correlation
between the efficacy of an offline RL algorithm and how well it works as part
of an IRL procedure. By using a strong offline RL algorithm as part of an IRL
procedure, we are able to find policies that match expert performance
significantly more efficiently than the prior art.
\\ ( https://arxiv.org/abs/2402.02616 ,  10979kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02619
Date: Sun, 4 Feb 2024 21:33:18 GMT   (842kb,D)

Title: Increasing Trust in Language Models through the Reuse of Verified
  Circuits
Authors: Philip Quirke, Clement Neo, Fazl Barez
Categories: cs.LG
Comments: 8 pages, 10 figures
\\
  Language Models (LMs) are increasingly used for a wide range of prediction
tasks, but their training can often neglect rare edge cases, reducing their
reliability. Here, we define a stringent standard of trustworthiness whereby
the task algorithm and circuit implementation must be verified, accounting for
edge cases, with no known failure modes. We show that a transformer model can
be trained to meet this standard if built using mathematically and logically
specified frameworks. In this paper, we fully verify a model for n-digit
integer addition. To exhibit the reusability of verified modules, we insert the
trained integer addition model into an untrained model and train the combined
model to perform both addition and subtraction. We find extensive reuse of the
addition circuits for both tasks, easing verification of the more complex
subtractor model. We discuss how inserting verified task modules into LMs can
leverage model reuse to improve verifiability and trustworthiness of language
models built using them. The reuse of verified circuits reduces the effort to
verify more complex composite models which we believe to be a significant step
towards safety of language models.
\\ ( https://arxiv.org/abs/2402.02619 ,  842kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02625
Date: Sun, 4 Feb 2024 22:12:29 GMT   (984kb,D)

Title: Enhancing Transformer RNNs with Multiple Temporal Perspectives
Authors: Razvan-Gabriel Dumitru, Darius Peteleaza, Mihai Surdeanu
Categories: cs.LG cs.AI cs.CL
Comments: 11 pages, 8 figures, 4 tables, in review for ICML 2024
ACM-class: I.2.0; I.2.7
\\
  We introduce the concept of multiple temporal perspectives, a novel approach
applicable to Recurrent Neural Network (RNN) architectures for enhancing their
understanding of sequential data. This method involves maintaining diverse
temporal views of previously encountered text, significantly enriching the
language models' capacity to interpret context. To show the efficacy of this
approach, we incorporate it into the Receptance Weighted Key Value (RWKV)
architecture, addressing its inherent challenge of retaining all historical
information within a single hidden state. Notably, this improvement is achieved
with a minimal increase in the number of parameters --even as little as
$0.04\%$ of the original number of parameters. Further, the additional
parameters necessary for the multiple temporal perspectives are fine-tuned with
minimal computational overhead, avoiding the need for a full pre-training. The
resulting model maintains linear computational complexity during prompt
inference, ensuring consistent efficiency across various sequence lengths. The
empirical results and ablation studies included in our research validate the
effectiveness of our approach, showcasing improved performance across multiple
benchmarks. The code, model weights and datasets are open-sourced at:
https://github.com/RazvanDu/TemporalRNNs.
\\ ( https://arxiv.org/abs/2402.02625 ,  984kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02627
Date: Sun, 4 Feb 2024 22:16:45 GMT   (1757kb,D)

Title: Stability Analysis of Various Symbolic Rule Extraction Methods from
  Recurrent Neural Network
Authors: Neisarg Dave, Daniel Kifer, C. Lee Giles, Ankur Mali
Categories: cs.LG
\\
  This paper analyzes two competing rule extraction methodologies: quantization
and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA
with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence
query($L^{*}$) methods across $10$ initialization seeds. We sampled the
datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN
cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments
establish the superior performance of O2RNN and quantization-based rule
extraction over others. $L^{*}$, primarily proposed for regular grammars,
performs similarly to quantization methods for Tomita languages when neural
networks are perfectly trained. However, for partially trained RNNs, $L^{*}$
shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita
6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization
methods result in rules with number of states very close to ground truth DFA.
Among RNN cells, O2RNN produces stable DFA consistently compared to other
cells. For Dyck Languages, we observe that although GRU outperforms other RNNs
in network performance, the DFA extracted by O2RNN has higher performance and
better stability. The stability is computed as the standard deviation of
accuracy on test sets on networks trained across $10$ seeds. On Dyck Languages,
quantization methods outperformed $L^{*}$ with better stability in accuracy and
the number of states. $L^{*}$ often showed instability in accuracy in the order
of $16\% - 22\%$ for GRU and MIRNN while deviation for quantization methods
varied in $5\% - 15\%$. In many instances with LSTM and GRU, DFA's extracted by
$L^{*}$ even failed to beat chance accuracy ($50\%$), while those extracted by
quantization method had standard deviation in the $7\%-17\%$ range. For O2RNN,
both rule extraction methods had deviation in the $0.5\% - 3\%$ range.
\\ ( https://arxiv.org/abs/2402.02627 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02629
Date: Sun, 4 Feb 2024 22:45:20 GMT   (2546kb,D)

Title: PROSAC: Provably Safe Certification for Machine Learning Models under
  Adversarial Attacks
Authors: Ziquan Liu, Zhuo Zhi, Ilija Bogunovic, Carsten Gerner-Beuerle, Miguel
  Rodrigues
Categories: cs.LG
\\
  It is widely known that state-of-the-art machine learning models, including
vision and language models, can be seriously compromised by adversarial
perturbations. It is therefore increasingly relevant to develop capabilities to
certify their performance in the presence of the most effective adversarial
attacks. Our paper offers a new approach to certify the performance of machine
learning models in the presence of adversarial attacks with population level
risk guarantees. In particular, we introduce the notion of $(\alpha,\zeta)$
machine learning model safety. We propose a hypothesis testing procedure, based
on the availability of a calibration set, to derive statistical guarantees
providing that the probability of declaring that the adversarial (population)
risk of a machine learning model is less than $\alpha$ (i.e. the model is
safe), while the model is in fact unsafe (i.e. the model adversarial population
risk is higher than $\alpha$), is less than $\zeta$. We also propose Bayesian
optimization algorithms to determine efficiently whether a machine learning
model is $(\alpha,\zeta)$-safe in the presence of an adversarial attack, along
with statistical guarantees. We apply our framework to a range of machine
learning models including various sizes of vision Transformer (ViT) and ResNet
models impaired by a variety of adversarial attacks, such as AutoAttack,
SquareAttack and natural evolution strategy attack, to illustrate the operation
of our approach. Importantly, we show that ViT's are generally more robust to
adversarial attacks than ResNets, and ViT-large is more robust than smaller
models. Our approach goes beyond existing empirical adversarial risk-based
certification guarantees. It formulates rigorous (and provable) performance
guarantees that can be used to satisfy regulatory requirements mandating the
use of state-of-the-art technical tools.
\\ ( https://arxiv.org/abs/2402.02629 ,  2546kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02631
Date: Sun, 4 Feb 2024 22:47:34 GMT   (1861kb,D)

Title: Learning to Understand: Identifying Interactions via the Mobius
  Transform
Authors: Justin S. Kang, Yigit E. Erginbas, Landon Butler, Ramtin Pedarsani,
  Kannan Ramchandran
Categories: cs.LG
Comments: 29 pages, 12 figures
\\
  One of the most fundamental problems in machine learning is finding
interpretable representations of the functions we learn. The Mobius transform
is a useful tool for this because its coefficients correspond to unique
importance scores on sets of input variables. The Mobius Transform is strongly
related (and in some cases equivalent) to the concept of Shapley value, which
is a widely used game-theoretic notion of importance. This work focuses on the
(typical) regime where the fraction of non-zero Mobius coefficients (and thus
interactions between inputs) is small compared to the set of all $2^n$ possible
interactions between $n$ inputs. When there are $K = O(2^{n \delta})$ with
$\delta \leq \frac{1}{3}$ non-zero coefficients chosen uniformly at random, our
algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and
$O(Kn^2)$ time with vanishing error as $K \rightarrow \infty$, the first
non-adaptive algorithm to do so. We also uncover a surprising connection
between group testing and the Mobius transform. In the case where all
interactions are between at most $t = \Theta(n^{\alpha})$ inputs, for $\alpha <
0.409$, we are able to leverage results from group testing to provide the first
algorithm that computes the Mobius transform in $O(Kt\log n)$ sample complexity
and $O(K\mathrm{poly}(n))$ time with vanishing error as $K \rightarrow \infty$.
Finally, we present a robust version of this algorithm that achieves the same
sample and time complexity under some assumptions, but with a factor depending
on noise variance. Our work is deeply interdisciplinary, drawing from tools
spanning across signal processing, algebra, information theory, learning theory
and group testing to address this important problem at the forefront of machine
learning.
\\ ( https://arxiv.org/abs/2402.02631 ,  1861kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02637
Date: Sun, 4 Feb 2024 23:11:19 GMT   (580kb,D)

Title: $C^*$-Algebraic Machine Learning: Moving in a New Direction
Authors: Yuka Hashimoto, Masahiro Ikeda, and Hachem Kadri
Categories: cs.LG math.OA stat.ML
Comments: position paper
\\
  Machine learning has a long collaborative tradition with several fields of
mathematics, such as statistics, probability and linear algebra. We propose a
new direction for machine learning research: $C^*$-algebraic ML $-$ a
cross-fertilization between $C^*$-algebra and machine learning. The
mathematical concept of $C^*$-algebra is a natural generalization of the space
of complex numbers. It enables us to unify existing learning strategies, and
construct a new framework for more diverse and information-rich data models. We
explain why and how to use $C^*$-algebras in machine learning, and provide
technical considerations that go into the design of $C^*$-algebraic learning
models in the contexts of kernel methods and neural networks. Furthermore, we
discuss open questions and challenges in $C^*$-algebraic ML and give our
thoughts for future development and applications.
\\ ( https://arxiv.org/abs/2402.02637 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02644
Date: Sun, 4 Feb 2024 23:51:04 GMT   (141kb,D)

Title: Variational DAG Estimation via State Augmentation With Stochastic
  Permutations
Authors: Edwin V. Bonilla, Pantelis Elinas, He Zhao, Maurizio Filippone,
  Vassili Kitsios, Terry O'Kane
Categories: cs.LG stat.ML
\\
  Estimating the structure of a Bayesian network, in the form of a directed
acyclic graph (DAG), from observational data is a statistically and
computationally hard problem with essential applications in areas such as
causal discovery. Bayesian approaches are a promising direction for solving
this task, as they allow for uncertainty quantification and deal with
well-known identifiability issues. From a probabilistic inference perspective,
the main challenges are (i) representing distributions over graphs that satisfy
the DAG constraint and (ii) estimating a posterior over the underlying
combinatorial space. We propose an approach that addresses these challenges by
formulating a joint distribution on an augmented space of DAGs and
permutations. We carry out posterior estimation via variational inference,
where we exploit continuous relaxations of discrete distributions. We show that
our approach can outperform competitive Bayesian and non-Bayesian benchmarks on
a range of synthetic and real datasets.
\\ ( https://arxiv.org/abs/2402.02644 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02651
Date: Mon, 5 Feb 2024 00:48:56 GMT   (7149kb,D)

Title: Vision-Language Models Provide Promptable Representations for
  Reinforcement Learning
Authors: William Chen and Oier Mees and Aviral Kumar and Sergey Levine
Categories: cs.LG cs.AI
\\
  Humans can quickly learn new behaviors by leveraging background world
knowledge. In contrast, agents trained with reinforcement learning (RL)
typically learn behaviors from scratch. We thus propose a novel approach that
uses the vast amounts of general and indexable world knowledge encoded in
vision-language models (VLMs) pre-trained on Internet-scale data for embodied
RL. We initialize policies with VLMs by using them as promptable
representations: embeddings that are grounded in visual observations and encode
semantic features based on the VLM's internal knowledge, as elicited through
prompts that provide task context and auxiliary information. We evaluate our
approach on visually-complex, long horizon RL tasks in Minecraft and robot
navigation in Habitat. We find that our policies trained on embeddings
extracted from general-purpose VLMs outperform equivalent policies trained on
generic, non-promptable image embeddings. We also find our approach outperforms
instruction-following methods and performs comparably to domain-specific
embeddings.
\\ ( https://arxiv.org/abs/2402.02651 ,  7149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02653
Date: Mon, 5 Feb 2024 00:52:50 GMT   (4610kb,D)

Title: Learning with Mixture of Prototypes for Out-of-Distribution Detection
Authors: Haodong Lu, Dong Gong, Shuo Wang, Jason Xue, Lina Yao, Kristen Moore
Categories: cs.LG cs.CV
Comments: Accepted at ICLR 2024
\\
  Out-of-distribution (OOD) detection aims to detect testing samples far away
from the in-distribution (ID) training data, which is crucial for the safe
deployment of machine learning models in the real world. Distance-based OOD
detection methods have emerged with enhanced deep representation learning. They
identify unseen OOD samples by measuring their distances from ID class
centroids or prototypes. However, existing approaches learn the representation
relying on oversimplified data assumptions, e.g, modeling ID data of each class
with one centroid class prototype or using loss functions not designed for OOD
detection, which overlook the natural diversities within the data. Naively
enforcing data samples of each class to be compact around only one prototype
leads to inadequate modeling of realistic data and limited performance. To
tackle these issues, we propose PrototypicAl Learning with a Mixture of
prototypes (PALM) which models each class with multiple prototypes to capture
the sample diversities, and learns more faithful and compact samples embeddings
to enhance OOD detection. Our method automatically identifies and dynamically
updates prototypes, assigning each sample to a subset of prototypes via
reciprocal neighbor soft assignment weights. PALM optimizes a maximum
likelihood estimation (MLE) loss to encourage the sample embeddings to be
compact around the associated prototypes, as well as a contrastive loss on all
prototypes to enhance intra-class compactness and inter-class discrimination at
the prototype level. Moreover, the automatic estimation of prototypes enables
our approach to be extended to the challenging OOD detection task with
unlabelled ID data. Extensive experiments demonstrate the superiority of PALM,
achieving state-of-the-art average AUROC performance of 93.82 on the
challenging CIFAR-100 benchmark. Code is available at
https://github.com/jeff024/PALM.
\\ ( https://arxiv.org/abs/2402.02653 ,  4610kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02663
Date: Mon, 5 Feb 2024 01:20:59 GMT   (56kb,D)

Title: Counterfactual Fairness Is Not Demographic Parity, and Other
  Observations
Authors: Ricardo Silva
Categories: cs.LG stat.ML
Comments: 17 pages, 2 figures
\\
  Blanket statements of equivalence between causal concepts and purely
probabilistic concepts should be approached with care. In this short note, I
examine a recent claim that counterfactual fairness is equivalent to
demographic parity. The claim fails to hold up upon closer examination. I will
take the opportunity to address some broader misunderstandings about
counterfactual fairness.
\\ ( https://arxiv.org/abs/2402.02663 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02665
Date: Mon, 5 Feb 2024 01:42:28 GMT   (64kb)

Title: Utility-Based Reinforcement Learning: Unifying Single-objective and
  Multi-objective Reinforcement Learning
Authors: Peter Vamplew, Cameron Foale, Conor F. Hayes, Patrick Mannion, Enda
  Howley, Richard Dazeley, Scott Johnson, Johan K\"allstr\"om, Gabriel Ramos,
  Roxana R\u{a}dulescu, Willem R\"opke, Diederik M. Roijers
Categories: cs.LG
Comments: Accepted for the Blue Sky Track at AAMAS'24
\\
  Research in multi-objective reinforcement learning (MORL) has introduced the
utility-based paradigm, which makes use of both environmental rewards and a
function that defines the utility derived by the user from those rewards. In
this paper we extend this paradigm to the context of single-objective
reinforcement learning (RL), and outline multiple potential benefits including
the ability to perform multi-policy learning across tasks relating to uncertain
objectives, risk-aware RL, discounting, and safe RL. We also examine the
algorithmic implications of adopting a utility-based approach.
\\ ( https://arxiv.org/abs/2402.02665 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02675
Date: Mon, 5 Feb 2024 02:21:11 GMT   (1208kb,D)

Title: Verifiable evaluations of machine learning models using zkSNARKs
Authors: Tobin South, Alexander Camuto, Shrey Jain, Shayla Nguyen, Robert
  Mahari, Christian Paquin, Jason Morton, Alex 'Sandy' Pentland
Categories: cs.LG cs.AI cs.CR
MSC-class: 68T01
\\
  In a world of increasing closed-source commercial machine learning models,
model evaluations from developers must be taken at face value. These benchmark
results, whether over task accuracy, bias evaluations, or safety checks, are
traditionally impossible to verify by a model end-user without the costly or
impossible process of re-performing the benchmark on black-box model outputs.
This work presents a method of verifiable model evaluation using model
inference through zkSNARKs. The resulting zero-knowledge computational proofs
of model outputs over datasets can be packaged into verifiable evaluation
attestations showing that models with fixed private weights achieve stated
performance or fairness metrics over public inputs. These verifiable
attestations can be performed on any standard neural network model with varying
compute requirements. For the first time, we demonstrate this across a sample
of real-world models and highlight key challenges and design solutions. This
presents a new transparency paradigm in the verifiable evaluation of private
models.
\\ ( https://arxiv.org/abs/2402.02675 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02678
Date: Mon, 5 Feb 2024 02:26:24 GMT   (1096kb,D)

Title: Counterfactual Explanations of Black-box Machine Learning Models using
  Causal Discovery with Applications to Credit Rating
Authors: Daisuke Takahashi, Shohei Shimizu and Takuma Tanaka
Categories: cs.LG
\\
  Explainable artificial intelligence (XAI) has helped elucidate the internal
mechanisms of machine learning algorithms, bolstering their reliability by
demonstrating the basis of their predictions. Several XAI models consider
causal relationships to explain models by examining the input-output
relationships of prediction models and the dependencies between features. The
majority of these models have been based their explanations on counterfactual
probabilities, assuming that the causal graph is known. However, this
assumption complicates the application of such models to real data, given that
the causal relationships between features are unknown in most cases. Thus, this
study proposed a novel XAI framework that relaxed the constraint that the
causal graph is known. This framework leveraged counterfactual probabilities
and additional prior information on causal structure, facilitating the
integration of a causal graph estimated through causal discovery methods and a
black-box classification model. Furthermore, explanatory scores were estimated
based on counterfactual probabilities. Numerical experiments conducted
employing artificial data confirmed the possibility of estimating the
explanatory score more accurately than in the absence of a causal graph.
Finally, as an application to real data, we constructed a classification model
of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan. We
demonstrated the effectiveness of the proposed method in cases where the causal
graph is unknown.
\\ ( https://arxiv.org/abs/2402.02678 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02681
Date: Mon, 5 Feb 2024 02:35:11 GMT   (3219kb,D)

Title: Equivariant Symmetry Breaking Sets
Authors: YuQing Xie, Tess Smidt
Categories: cs.LG cs.AI
Comments: 28 pages, 30 figures Submitted to ICML 2024
\\
  Equivariant neural networks (ENNs) have been shown to be extremely effective
in applications involving underlying symmetries. By construction ENNs cannot
produce lower symmetry outputs given a higher symmetry input. However,
spontaneous symmetry breaking occurs in many physical systems and we may obtain
a less symmetric stable state from an initial highly symmetric one. Hence, it
is imperative that we understand how to systematically break symmetry in ENNs.
In this work, we propose a novel symmetry breaking framework that is fully
equivariant. We emphasize that our approach is general and applicable to
equivariance under any group. To achieve this, we introduce the idea of
symmetry breaking sets (SBS). Rather than redesign existing networks, we design
sets of symmetry breaking objects which we feed into our network based on the
symmetry of our inputs and outputs. We show there is a natural way to define
equivariance on these sets, which gives an additional constraint. Minimizing
the size of these sets equates to data efficiency. We prove that minimizing
these sets translates to a well studied group theory problem, and tabulate
solutions to this problem for the point groups. Finally, we provide some
examples of symmetry breaking to demonstrate how our approach works in
practice.
\\ ( https://arxiv.org/abs/2402.02681 ,  3219kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02687
Date: Mon, 5 Feb 2024 02:54:50 GMT   (1525kb,D)

Title: Poisson Process for Bayesian Optimization
Authors: Xiaoxing Wang, Jiaxing Li, Chao Xue, Wei Liu, Weifeng Liu, Xiaokang
  Yang, Junchi Yan, Dacheng Tao
Categories: cs.LG cs.AI
\\
  BayesianOptimization(BO) is a sample-efficient black-box optimizer, and
extensive methods have been proposed to build the absolute function response of
the black-box function through a probabilistic surrogate model, including
Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian
process (GP). However, few methods have been explored to estimate the relative
rankings of candidates, which can be more robust to noise and have better
practicality than absolute function responses, especially when the function
responses are intractable but preferences can be acquired. To this end, we
propose a novel ranking-based surrogate model based on the Poisson process and
introduce an efficient BO framework, namely Poisson Process Bayesian
Optimization (PoPBO). Two tailored acquisition functions are further derived
from classic LCB and EI to accommodate it. Compared to the classic GP-BO
method, our PoPBO has lower computation costs and better robustness to noise,
which is verified by abundant experiments. The results on both simulated and
real-world benchmarks, including hyperparameter optimization (HPO) and neural
architecture search (NAS), show the effectiveness of PoPBO.
\\ ( https://arxiv.org/abs/2402.02687 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02692
Date: Mon, 5 Feb 2024 03:03:00 GMT   (537kb,D)

Title: Statistical Guarantees for Link Prediction using Graph Neural Networks
Authors: Alan Chung, Amin Saberi, Morgane Austern
Categories: cs.LG math.ST stat.ML stat.TH
\\
  This paper derives statistical guarantees for the performance of Graph Neural
Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We
propose a linear GNN architecture (LG-GNN) that produces consistent estimators
for the underlying edge probabilities. We establish a bound on the mean squared
error and give guarantees on the ability of LG-GNN to detect high-probability
edges. Our guarantees hold for both sparse and dense graphs. Finally, we
demonstrate some of the shortcomings of the classical GCN architecture, as well
as verify our results on real and synthetic datasets.
\\ ( https://arxiv.org/abs/2402.02692 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02696
Date: Mon, 5 Feb 2024 03:20:28 GMT   (1076kb,D)

Title: Causal Feature Selection for Responsible Machine Learning
Authors: Raha Moraffah, Paras Sheth, Saketh Vishnubhatla, and Huan Liu
Categories: cs.LG cs.AI
\\
  Machine Learning (ML) has become an integral aspect of many real-world
applications. As a result, the need for responsible machine learning has
emerged, focusing on aligning ML models to ethical and social values, while
enhancing their reliability and trustworthiness. Responsible ML involves many
issues. This survey addresses four main issues: interpretability, fairness,
adversarial robustness, and domain generalization. Feature selection plays a
pivotal role in the responsible ML tasks. However, building upon statistical
correlations between variables can lead to spurious patterns with biases and
compromised performance. This survey focuses on the current study of causal
feature selection: what it is and how it can reinforce the four aspects of
responsible ML. By identifying features with causal impacts on outcomes and
distinguishing causality from correlation, causal feature selection is posited
as a unique approach to ensuring ML models to be ethically and socially
responsible in high-stakes applications.
\\ ( https://arxiv.org/abs/2402.02696 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02697
Date: Mon, 5 Feb 2024 03:20:33 GMT   (91kb)

Title: Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit
  Models for High-dimensional Gaussian Mixtures
Authors: Zenan Ling, Longbo Li, Zhanbo Feng, Yixuan Zhang, Feng Zhou, Robert C.
  Qiu, Zhenyu Liao
Categories: cs.LG stat.ML
\\
  Deep equilibrium models (DEQs), as a typical implicit neural network, have
demonstrated remarkable success on various tasks. There is, however, a lack of
theoretical understanding of the connections and differences between implicit
DEQs and explicit neural network models. In this paper, leveraging recent
advances in random matrix theory (RMT), we perform an in-depth analysis on the
eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK)
matrices for implicit DEQs, when the input data are drawn from a
high-dimensional Gaussian mixture. We prove, in this setting, that the spectral
behavior of these Implicit-CKs and NTKs depend on the DEQ activation function
and initial weight variances, but only via a system of four nonlinear
equations. As a direct consequence of this theoretical result, we demonstrate
that a shallow explicit network can be carefully designed to produce the same
CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data,
empirical results show the proposed theory and design principle also apply to
popular real-world datasets.
\\ ( https://arxiv.org/abs/2402.02697 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02698
Date: Mon, 5 Feb 2024 03:21:23 GMT   (1116kb,D)

Title: Beyond Expectations: Learning with Stochastic Dominance Made Practical
Authors: Shicong Cen, Jincheng Mei, Hanjun Dai, Dale Schuurmans, Yuejie Chi, Bo
  Dai
Categories: cs.LG cs.AI math.OC
\\
  Stochastic dominance models risk-averse preferences for decision making with
uncertain outcomes, which naturally captures the intrinsic structure of the
underlying uncertainty, in contrast to simply resorting to the expectations.
Despite theoretically appealing, the application of stochastic dominance in
machine learning has been scarce, due to the following challenges:
$\textbf{i)}$, the original concept of stochastic dominance only provides a
$\textit{partial order}$, therefore, is not amenable to serve as an optimality
criterion; and $\textbf{ii)}$, an efficient computational recipe remains
lacking due to the continuum nature of evaluating stochastic dominance.%, which
barriers its application for machine learning.
  In this work, we make the first attempt towards establishing a general
framework of learning with stochastic dominance. We first generalize the
stochastic dominance concept to enable feasible comparisons between any
arbitrary pair of random variables. We next develop a simple and
computationally efficient approach for finding the optimal solution in terms of
stochastic dominance, which can be seamlessly plugged into many learning tasks.
Numerical experiments demonstrate that the proposed method achieves comparable
performance as standard risk-neutral strategies and obtains better trade-offs
against risk across a variety of applications including supervised learning,
reinforcement learning, and portfolio optimization.
\\ ( https://arxiv.org/abs/2402.02698 ,  1116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02700
Date: Mon, 5 Feb 2024 03:25:04 GMT   (64kb)

Title: Sample Complexity Characterization for Linear Contextual MDPs
Authors: Junze Deng, Yuan Cheng, Shaofeng Zou and Yingbin Liang
Categories: cs.LG stat.ML
Comments: accepted to AIstats2024
\\
  Contextual Markov decision processes (CMDPs) describe a class of
reinforcement learning problems in which the transition kernels and reward
functions can change over time with different MDPs indexed by a context
variable. While CMDPs serve as an important framework to model many real-world
applications with time-varying environments, they are largely unexplored from
theoretical perspective. In this paper, we study CMDPs under two linear
function approximation models: Model I with context-varying representations and
common linear weights for all contexts; and Model II with common
representations for all contexts and context-varying linear weights. For both
models, we propose novel model-based algorithms and show that they enjoy
guaranteed $\epsilon$-suboptimality gap with desired polynomial sample
complexity. In particular, instantiating our result for the first model to the
tabular CMDP improves the existing result by removing the reachability
assumption. Our result for the second model is the first-known result for such
a type of function approximation models. Comparison between our results for the
two models further indicates that having context-varying features leads to much
better sample efficiency than having common representations for all contexts
under linear CMDPs.
\\ ( https://arxiv.org/abs/2402.02700 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02701
Date: Mon, 5 Feb 2024 03:27:52 GMT   (9147kb,D)

Title: Understanding What Affects Generalization Gap in Visual Reinforcement
  Learning: Theory and Empirical Evidence
Authors: Jiafei Lyu, Le Wan, Xiu Li, Zongqing Lu
Categories: cs.LG cs.AI stat.ML
Comments: Part of this work is accepted as AAMAS 2024 extended abstract
\\
  Recently, there are many efforts attempting to learn useful policies for
continuous control in visual reinforcement learning (RL). In this scenario, it
is important to learn a generalizable policy, as the testing environment may
differ from the training environment, e.g., there exist distractors during
deployment. Many practical algorithms are proposed to handle this problem.
However, to the best of our knowledge, none of them provide a theoretical
understanding of what affects the generalization gap and why their proposed
methods work. In this paper, we bridge this issue by theoretically answering
the key factors that contribute to the generalization gap when the testing
environment has distractors. Our theories indicate that minimizing the
representation distance between training and testing environments, which aligns
with human intuition, is the most critical for the benefit of reducing the
generalization gap. Our theoretical results are supported by the empirical
evidence in the DMControl Generalization Benchmark (DMC-GB).
\\ ( https://arxiv.org/abs/2402.02701 ,  9147kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02705
Date: Mon, 5 Feb 2024 03:39:39 GMT   (4587kb,D)

Title: Representation Surgery for Multi-Task Model Merging
Authors: Enneng Yang and Li Shen and Zhenyi Wang and Guibing Guo and Xiaojun
  Chen and Xingwei Wang and Dacheng Tao
Categories: cs.LG cs.AI cs.CV
\\
  Multi-task learning (MTL) compresses the information from multiple tasks into
a unified backbone to improve computational efficiency and generalization.
Recent work directly merges multiple independently trained models to perform
MTL instead of collecting their raw data for joint training, greatly expanding
the application scenarios of MTL. However, by visualizing the representation
distribution of existing model merging schemes, we find that the merged model
often suffers from the dilemma of representation bias. That is, there is a
significant discrepancy in the representation distribution between the merged
and individual models, resulting in poor performance of merged MTL. In this
paper, we propose a representation surgery solution called "Surgery" to reduce
representation bias in the merged model. Specifically, Surgery is a lightweight
task-specific module that takes the representation of the merged model as input
and attempts to output the biases contained in the representation from the
merged model. We then designed an unsupervised optimization objective that
updates the Surgery module by minimizing the distance between the merged
model's representation and the individual model's representation. Extensive
experiments demonstrate significant MTL performance improvements when our
Surgery module is applied to state-of-the-art (SOTA) model merging schemes.
\\ ( https://arxiv.org/abs/2402.02705 ,  4587kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02711
Date: Mon, 5 Feb 2024 04:15:31 GMT   (3215kb,D)

Title: Architectural Strategies for the optimization of Physics-Informed Neural
  Networks
Authors: Hemanth Saratchandran, Shin-Fang Chng, Simon Lucey
Categories: cs.LG
\\
  Physics-informed neural networks (PINNs) offer a promising avenue for
tackling both forward and inverse problems in partial differential equations
(PDEs) by incorporating deep learning with fundamental physics principles.
Despite their remarkable empirical success, PINNs have garnered a reputation
for their notorious training challenges across a spectrum of PDEs. In this
work, we delve into the intricacies of PINN optimization from a neural
architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study
reveals that Gaussian activations surpass several alternate activations when it
comes to effectively training PINNs. Building on insights from numerical linear
algebra, we introduce a preconditioned neural architecture, showcasing how such
tailored architectures enhance the optimization process. Our theoretical
findings are substantiated through rigorous validation against established PDEs
within the scientific literature.
\\ ( https://arxiv.org/abs/2402.02711 ,  3215kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02713
Date: Mon, 5 Feb 2024 04:17:49 GMT   (878kb,D)

Title: Position Paper: What Can Large Language Models Tell Us about Time Series
  Analysis
Authors: Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang,
  Jindong Wang, Shirui Pan, Qingsong Wen
Categories: cs.LG cs.AI
Comments: 16 pages, 8 figures, 1 table
\\
  Time series analysis is essential for comprehending the complexities inherent
in various real-world systems and applications. Although large language models
(LLMs) have recently made significant strides, the development of artificial
general intelligence (AGI) equipped with time series analysis capabilities
remains in its nascent phase. Most existing time series models heavily rely on
domain knowledge and extensive model tuning, predominantly focusing on
prediction tasks. In this paper, we argue that current LLMs have the potential
to revolutionize time series analysis, thereby promoting efficient
decision-making and advancing towards a more universal form of time series
analytical intelligence. Such advancement could unlock a wide range of
possibilities, including modality switching and time series question answering.
We encourage researchers and practitioners to recognize the potential of LLMs
in advancing time series analysis and emphasize the need for trust in these
related efforts. Furthermore, we detail the seamless integration of time series
analysis with existing LLM technologies and outline promising avenues for
future research.
\\ ( https://arxiv.org/abs/2402.02713 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02720
Date: Mon, 5 Feb 2024 04:29:39 GMT   (509kb,D)

Title: Discounted Adaptive Online Prediction
Authors: Zhiyu Zhang, David Bombara, Heng Yang
Categories: cs.LG stat.ML
\\
  Online learning is not always about memorizing everything. Since the future
can be statistically very different from the past, a critical challenge is to
gracefully forget the history while new data comes in. To formalize this
intuition, we revisit the classical notion of discounted regret using recently
developed techniques in adaptive online learning. Our main result is a new
algorithm that adapts to the complexity of both the loss sequence and the
comparator, improving the widespread non-adaptive algorithm - gradient descent
with a constant learning rate. In particular, our theoretical guarantee does
not require any structural assumption beyond convexity, and the algorithm is
provably robust to suboptimal hyperparameter tuning. We further demonstrate
such benefits through online conformal prediction, a downstream online learning
task with set-membership decisions.
\\ ( https://arxiv.org/abs/2402.02720 ,  509kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02725
Date: Mon, 5 Feb 2024 04:49:59 GMT   (581kb)

Title: Innovative Cybersickness Detection: Exploring Head Movement Patterns in
  Virtual Reality
Authors: Masoud Salehi, Nikoo Javadpour, Brietta Beisner, Mohammadamin Sanaei,
  Stephen B. Gilbert
Categories: cs.LG eess.SP
Comments: 18 pages, 3 Figures, 3 Tables
\\
  Despite the widespread adoption of Virtual Reality (VR) technology,
cybersickness remains a barrier for some users. This research investigates head
movement patterns as a novel physiological marker for cybersickness detection.
Unlike traditional markers, head movements provide a continuous, non-invasive
measure that can be easily captured through the sensors embedded in all
commercial VR headsets. We used a publicly available dataset from a VR
experiment involving 75 participants and analyzed head movements across six
axes. An extensive feature extraction process was then performed on the head
movement dataset and its derivatives, including velocity, acceleration, and
jerk. Three categories of features were extracted, encompassing statistical,
temporal, and spectral features. Subsequently, we employed the Recursive
Feature Elimination method to select the most important and effective features.
In a series of experiments, we trained a variety of machine learning
algorithms. The results demonstrate a 76% accuracy and 83% precision in
predicting cybersickness in the subjects based on the head movements. This
study contribution to the cybersickness literature lies in offering a
preliminary analysis of a new source of data and providing insight into the
relationship of head movements and cybersickness.
\\ ( https://arxiv.org/abs/2402.02725 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02732
Date: Mon, 5 Feb 2024 05:22:58 GMT   (2587kb,D)

Title: A Generative Approach to Surrogate-based Black-box Attacks
Authors: Raha Moraffah, Huan Liu
Categories: cs.LG cs.AI
\\
  Surrogate-based black-box attacks have exposed the heightened vulnerability
of DNNs. These attacks are designed to craft adversarial examples for any
samples with black-box target feedback for only a given set of samples.
State-of-the-art surrogate-based attacks involve training a discriminative
surrogate that mimics the target's outputs. The goal is to learn the decision
boundaries of the target. The surrogate is then attacked by white-box attacks
to craft adversarial examples similar to the original samples but belong to
other classes. With limited samples, the discriminative surrogate fails to
accurately learn the target's decision boundaries, and these surrogate-based
attacks suffer from low success rates. Different from the discriminative
approach, we propose a generative surrogate that learns the distribution of
samples residing on or close to the target's decision boundaries. The
distribution learned by the generative surrogate can be used to craft
adversarial examples that have imperceptible differences from the original
samples but belong to other classes. The proposed generative approach results
in attacks with remarkably high attack success rates on various targets and
datasets.
\\ ( https://arxiv.org/abs/2402.02732 ,  2587kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02741
Date: Mon, 5 Feb 2024 05:48:03 GMT   (1985kb,D)

Title: Glocal Hypergradient Estimation with Koopman Operator
Authors: Ryuichiro Hataya and Yoshinobu Kawahara
Categories: cs.LG stat.ML
\\
  Gradient-based hyperparameter optimization methods update hyperparameters
using hypergradients, gradients of a meta criterion with respect to
hyperparameters. Previous research used two distinct update strategies:
optimizing hyperparameters using global hypergradients obtained after
completing model training or local hypergradients derived after every few model
updates. While global hypergradients offer reliability, their computational
cost is significant; conversely, local hypergradients provide speed but are
often suboptimal. In this paper, we propose glocal hypergradient estimation,
blending "global" quality with "local" efficiency. To this end, we use the
Koopman operator theory to linearize the dynamics of hypergradients so that the
global hypergradients can be efficiently approximated only by using a
trajectory of local hypergradients. Consequently, we can optimize
hyperparameters greedily using estimated global hypergradients, achieving both
reliability and efficiency simultaneously. Through numerical experiments of
hyperparameter optimization, including optimization of optimizers, we
demonstrate the effectiveness of the glocal hypergradient estimation.
\\ ( https://arxiv.org/abs/2402.02741 ,  1985kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02746
Date: Mon, 5 Feb 2024 06:00:54 GMT   (4583kb,D)

Title: Standard Gaussian Process is All You Need for High-Dimensional Bayesian
  Optimization
Authors: Zhitong Xu, Shandian Zhe
Categories: cs.LG
\\
  There has been a long-standing and widespread belief that Bayesian
Optimization (BO) with standard Gaussian process (GP), referred to as standard
BO, is ineffective in high-dimensional optimization problems. This perception
may partly stem from the intuition that GPs struggle with high-dimensional
inputs for covariance modeling and function estimation. While these concerns
seem reasonable, empirical evidence supporting this belief is lacking. In this
paper, we systematically investigated BO with standard GP regression across a
variety of synthetic and real-world benchmark problems for high-dimensional
optimization. Surprisingly, the performance with standard GP consistently ranks
among the best, often outperforming existing BO methods specifically designed
for high-dimensional optimization by a large margin. Contrary to the
stereotype, we found that standard GP can serve as a capable surrogate for
learning high-dimensional target functions. Without strong structural
assumptions, BO with standard GP not only excels in high-dimensional
optimization but also proves robust in accommodating various structures within
the target functions. Furthermore, with standard GP, achieving promising
optimization performance is possible by only using maximum likelihood
estimation, eliminating the need for expensive Markov-Chain Monte Carlo (MCMC)
sampling that might be required by more complex surrogate models. We thus
advocate for a re-evaluation and in-depth study of the potential of standard BO
in addressing high-dimensional problems.
\\ ( https://arxiv.org/abs/2402.02746 ,  4583kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02769
Date: Mon, 5 Feb 2024 07:05:17 GMT   (782kb,D)

Title: Learning from Teaching Regularization: Generalizable Correlations Should
  be Easy to Imitate
Authors: Can Jin, Tong Che, Hongwu Peng, Yiyuan Li, Marco Pavone
Categories: cs.LG cs.AI
\\
  Generalization remains a central challenge in machine learning. In this work,
we propose Learning from Teaching (LoT), a novel regularization technique for
deep neural networks to enhance generalization. Inspired by the human ability
to capture concise and abstract patterns, we hypothesize that generalizable
correlations are expected to be easier to teach. LoT operationalizes this
concept to improve the generalization of the main model with auxiliary student
learners. The student learners are trained by the main model and improve the
main model to capture more generalizable and teachable correlations by
providing feedback. Our experimental results across several domains, including
Computer Vision, Natural Language Processing, and Reinforcement Learning,
demonstrate that the introduction of LoT brings significant benefits compared
to merely training models on the original training data. It suggests the
effectiveness of LoT in identifying generalizable information without falling
into the swamp of complex patterns in data, making LoT a valuable addition to
the current machine learning frameworks.
\\ ( https://arxiv.org/abs/2402.02769 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02772
Date: Mon, 5 Feb 2024 07:12:02 GMT   (15134kb,D)

Title: Contrastive Diffuser: Planning Towards High Return States via
  Contrastive Learning
Authors: Yixiang Shan, Zhengbang Zhu, Ting Long, Qifan Liang, Yi Chang, Weinan
  Zhang, Liang Yin
Categories: cs.LG
Comments: 13 pages with appendix and references, 10 figures, 3 tables
\\
  Applying diffusion models in reinforcement learning for long-term planning
has gained much attention recently. Several diffusion-based methods have
successfully leveraged the modeling capabilities of diffusion for arbitrary
distributions. These methods generate subsequent trajectories for planning and
have demonstrated significant improvement. However, these methods are limited
by their plain base distributions and their overlooking of the diversity of
samples, in which different states have different returns. They simply leverage
diffusion to learn the distribution of offline dataset, generate the
trajectories whose states share the same distribution with the offline dataset.
As a result, the probability of these models reaching the high-return states is
largely dependent on the dataset distribution. Even equipped with the guidance
model, the performance is still suppressed. To address these limitations, in
this paper, we propose a novel method called CDiffuser, which devises a return
contrast mechanism to pull the states in generated trajectories towards
high-return states while pushing them away from low-return states to improve
the base distribution. Experiments on 14 commonly used D4RL benchmarks
demonstrate the effectiveness of our proposed method. Our code is publicly
available at https://anonymous.4open.science/r/ContrastiveDiffuser.
\\ ( https://arxiv.org/abs/2402.02772 ,  15134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02790
Date: Mon, 5 Feb 2024 07:56:02 GMT   (6088kb,D)

Title: Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear
  Unit (TeLU)
Authors: Alfredo Fernandez and Ankur Mali
Categories: cs.LG cs.NE
\\
  In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit
(TeLU), a novel neural network activation function, represented as $f(x) =
x{\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of
conventional activation functions like ReLU, GELU, and Mish by addressing the
vanishing and, to an extent, the exploding gradient problems. Our theoretical
analysis and empirical assessments reveal that TeLU outperforms existing
activation functions in stability and robustness, effectively adjusting
activation outputs' mean towards zero for enhanced training stability and
convergence. Extensive evaluations against popular activation functions (ReLU,
GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including
Resnet-50, demonstrate TeLU's lower variance and superior performance, even
under hyperparameter conditions optimized for other functions. In large-scale
tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet,
encompassing 860 scenarios, TeLU consistently showcased its effectiveness,
positioning itself as a potential new standard for neural network activation
functions, boosting stability and performance in diverse deep learning
applications.
\\ ( https://arxiv.org/abs/2402.02790 ,  6088kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02812
Date: Mon, 5 Feb 2024 08:42:39 GMT   (4722kb,D)

Title: State estimation of urban air pollution with statistical, physical, and
  super-learning graph models
Authors: Matthieu Dolbeault, Olga Mula and Agust\'in Somacal
Categories: cs.LG cs.NA math.NA physics.soc-ph
\\
  We consider the problem of real-time reconstruction of urban air pollution
maps. The task is challenging due to the heterogeneous sources of available
data, the scarcity of direct measurements, the presence of noise, and the large
surfaces that need to be considered. In this work, we introduce different
reconstruction methods based on posing the problem on city graphs. Our
strategies can be classified as fully data-driven, physics-driven, or hybrid,
and we combine them with super-learning models. The performance of the methods
is tested in the case of the inner city of Paris, France.
\\ ( https://arxiv.org/abs/2402.02812 ,  4722kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02820
Date: Mon, 5 Feb 2024 09:06:57 GMT   (3025kb,D)

Title: Revisiting VAE for Unsupervised Time Series Anomaly Detection: A
  Frequency Perspective
Authors: Zexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei,
  Saravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui Li,
  Gaogang Xie
Categories: cs.LG
Comments: WWW 2024
\\
  Time series Anomaly Detection (AD) plays a crucial role for web systems.
Various web systems rely on time series data to monitor and identify anomalies
in real time, as well as to initiate diagnosis and remediation procedures.
Variational Autoencoders (VAEs) have gained popularity in recent decades due to
their superior de-noising capabilities, which are useful for anomaly detection.
However, our study reveals that VAE-based methods face challenges in capturing
long-periodic heterogeneous patterns and detailed short-periodic trends
simultaneously. To address these challenges, we propose Frequency-enhanced
Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for
univariate time series. To ensure an accurate AD, FCVAE exploits an innovative
approach to concurrently integrate both the global and local frequency features
into the condition of Conditional Variational Autoencoder (CVAE) to
significantly increase the accuracy of reconstructing the normal data. Together
with a carefully designed "target attention" mechanism, our approach allows the
model to pick the most useful information from the frequency domain for better
short-periodic trend construction. Our FCVAE has been evaluated on public
datasets and a large-scale cloud system, and the results demonstrate that it
outperforms state-of-the-art methods. This confirms the practical applicability
of our approach in addressing the limitations of current VAE-based anomaly
detection models.
\\ ( https://arxiv.org/abs/2402.02820 ,  3025kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02823
Date: Mon, 5 Feb 2024 09:10:32 GMT   (1414kb,D)

Title: Evading Data Contamination Detection for Language Models is (too) Easy
Authors: Jasper Dekoninck, Mark Niklas M\"uller, Maximilian Baader, Marc
  Fischer, Martin Vechev
Categories: cs.LG cs.AI
\\
  Large language models are widespread, with their performance on benchmarks
frequently guiding user preferences for one model over another. However, the
vast amount of data these models are trained on can inadvertently lead to
contamination with public benchmarks, thus compromising performance
measurements. While recently developed contamination detection methods try to
address this issue, they overlook the possibility of deliberate contamination
by malicious model providers aiming to evade detection. We argue that this
setting is of crucial importance as it casts doubt on the reliability of public
benchmarks. To more rigorously study this issue, we propose a categorization of
both model providers and contamination detection methods. This reveals
vulnerabilities in existing methods that we exploit with EAL, a simple yet
effective contamination technique that significantly inflates benchmark
performance while completely evading current detection methods.
\\ ( https://arxiv.org/abs/2402.02823 ,  1414kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02827
Date: Mon, 5 Feb 2024 09:24:52 GMT   (1113kb,D)

Title: PowerGraph: A power grid benchmark dataset for graph neural networks
Authors: Anna Varbella, Kenza Amara, Blazhe Gjorgiev, Giovanni Sansavini
Categories: cs.LG
Comments: 30 pages, 4 figures, conference paper
\\
  Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of
GNN and enhance GNN applicability to diverse disciplines. The community
currently lacks public datasets of electrical power grids for GNN applications.
Indeed, GNNs can potentially capture complex power grid phenomena over
alternative machine learning techniques. Power grids are complex engineered
networks that are naturally amenable to graph representations. Therefore, GNN
have the potential for capturing the behavior of power grids over alternative
machine learning techniques. To this aim, we develop a graph dataset for
cascading failure events, which are the major cause of blackouts in electric
power grids. Historical blackout datasets are scarce and incomplete. The
assessment of vulnerability and the identification of critical components are
usually conducted via computationally expensive offline simulations of
cascading failures. Instead, we propose using machine learning models for the
online detection of cascading failures leveraging the knowledge of the system
state at the onset of the cascade. We develop PowerGraph, a graph dataset
modeling cascading failures in power grids, designed for two purposes, namely,
i) training GNN models for different graph-level tasks including multi-class
classification, binary classification, and regression, and ii) explaining GNN
models. The dataset generated via a physics-based cascading failure model
ensures the generality of the operating and environmental conditions by
spanning diverse failure scenarios. In addition, we foster the use of the
dataset to benchmark GNN explainability methods by assigning ground-truth
edge-level explanations. PowerGraph helps the development of better GNN models
for graph-level tasks and explainability, critical in many domains ranging from
chemistry to biology, where the systems and processes can be described as
graphs.
\\ ( https://arxiv.org/abs/2402.02827 ,  1113kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02834
Date: Mon, 5 Feb 2024 09:44:49 GMT   (1144kb,D)

Title: Shortened LLaMA: A Simple Depth Pruning for Large Language Models
Authors: Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook
  Choi, Junho Shin, Hyoung-Kyu Song
Categories: cs.LG
\\
  Structured pruning of modern large language models (LLMs) has emerged as a
way of decreasing their high computational needs. Width pruning reduces the
size of projection weight matrices (e.g., by removing attention heads) while
maintaining the number of layers. Depth pruning, in contrast, removes entire
layers or blocks, while keeping the size of the remaining weights unchanged.
Most current research focuses on either width-only or a blend of width and
depth pruning, with little comparative analysis between the two units (width
vs. depth) concerning their impact on LLM inference efficiency. In this work,
we show that a simple depth pruning approach can compete with recent width
pruning methods in terms of zero-shot task performance. Our pruning method
boosts inference speeds, especially under memory-constrained conditions that
require limited batch sizes for running LLMs, where width pruning is
ineffective. We hope this work can help deploy LLMs on local and edge devices.
\\ ( https://arxiv.org/abs/2402.02834 ,  1144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02858
Date: Mon, 5 Feb 2024 10:18:15 GMT   (2803kb,D)

Title: Deep autoregressive density nets vs neural ensembles for model-based
  offline reinforcement learning
Authors: Abdelhakim Benechehab, Albert Thomas and Bal\'azs K\'egl
Categories: cs.LG stat.ML
\\
  We consider the problem of offline reinforcement learning where only a set of
system transitions is made available for policy optimization. Following recent
advances in the field, we consider a model-based reinforcement learning
algorithm that infers the system dynamics from the available data and performs
policy optimization on imaginary model rollouts. This approach is vulnerable to
exploiting model errors which can lead to catastrophic failures on the real
system. The standard solution is to rely on ensembles for uncertainty
heuristics and to avoid exploiting the model where it is too uncertain. We
challenge the popular belief that we must resort to ensembles by showing that
better performance can be obtained with a single well-calibrated autoregressive
model on the D4RL benchmark. We also analyze static metrics of model-learning
and conclude on the important model properties for the final performance of the
agent.
\\ ( https://arxiv.org/abs/2402.02858 ,  2803kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02868
Date: Mon, 5 Feb 2024 10:30:47 GMT   (39768kb,D)

Title: Fine-tuning Reinforcement Learning Models is Secretly a Forgetting
  Mitigation Problem
Authors: Maciej Wo{\l}czyk, Bart{\l}omiej Cupia{\l}, Mateusz Ostaszewski,
  Micha{\l} Bortkiewicz, Micha{\l} Zaj\k{a}c, Razvan Pascanu, {\L}ukasz
  Kuci\'nski, Piotr Mi{\l}o\'s
Categories: cs.LG
\\
  Fine-tuning is a widespread technique that allows practitioners to transfer
pre-trained capabilities, as recently showcased by the successful applications
of foundation models. However, fine-tuning reinforcement learning (RL) models
remains a challenge. This work conceptualizes one specific cause of poor
transfer, accentuated in the RL setting by the interplay between actions and
observations: forgetting of pre-trained capabilities. Namely, a model
deteriorates on the state subspace of the downstream task not visited in the
initial phase of fine-tuning, on which the model behaved well due to
pre-training. This way, we lose the anticipated transfer benefits. We identify
conditions when this problem occurs, showing that it is common and, in many
cases, catastrophic. Through a detailed empirical analysis of the challenging
NetHack and Montezuma's Revenge environments, we show that standard knowledge
retention techniques mitigate the problem and thus allow us to take full
advantage of the pre-trained capabilities. In particular, in NetHack, we
achieve a new state-of-the-art for neural models, improving the previous best
score from $5$K to over $10$K points in the Human Monk scenario.
\\ ( https://arxiv.org/abs/2402.02868 ,  39768kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02870
Date: Mon, 5 Feb 2024 10:36:48 GMT   (43kb)

Title: Statistics without Interpretation: A Sober Look at Explainable Machine
  Learning
Authors: Sebastian Bordt, Ulrike von Luxburg
Categories: cs.LG
\\
  In the rapidly growing literature on explanation algorithms, it often remains
unclear what precisely these algorithms are for and how they should be used. We
argue that this is because explanation algorithms are often mathematically
complex but don't admit a clear interpretation. Unfortunately, complex
statistical methods that don't have a clear interpretation are bound to lead to
errors in interpretation, a fact that has become increasingly apparent in the
literature. In order to move forward, papers on explanation algorithms should
make clear how precisely the output of the algorithms should be interpreted.
They should also clarify what questions about the function can and cannot be
answered given the explanations. Our argument is based on the distinction
between statistics and their interpretation. It also relies on parallels
between explainable machine learning and applied statistics.
\\ ( https://arxiv.org/abs/2402.02870 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02890
Date: Mon, 5 Feb 2024 10:59:12 GMT   (16909kb,D)

Title: Black-Box Approximation and Optimization with Hierarchical Tucker
  Decomposition
Authors: Gleb Ryzhakov, Andrei Chertkov, Artem Basharin, Ivan Oseledets
Categories: cs.LG math.OC
\\
  We develop a new method HTBB for the multidimensional black-box approximation
and gradient-free optimization, which is based on the low-rank hierarchical
Tucker decomposition with the use of the MaxVol indices selection procedure.
Numerical experiments for 14 complex model problems demonstrate the robustness
of the proposed method for dimensions up to 1000, while it shows significantly
more accurate results than classical gradient-free optimization methods, as
well as approximation and optimization methods based on the popular tensor
train decomposition, which represents a simpler case of a tensor network.
\\ ( https://arxiv.org/abs/2402.02890 ,  16909kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02910
Date: Mon, 5 Feb 2024 11:25:45 GMT   (10189kb,D)

Title: DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage
  Temporal Convolutional Network
Authors: Meng Shang, Lenore Dedeyne, Jolan Dupont, Laura Vercauteren, Nadjia
  Amini, Laurence Lapauw, Evelien Gielen, Sabine Verschueren, Carolina Varon,
  Walter De Raedt, Bart Vanrumste
Categories: cs.LG cs.AI eess.SP
\\
  The Otago Exercise Program (OEP) represents a crucial rehabilitation
initiative tailored for older adults, aimed at enhancing balance and strength.
Despite previous efforts utilizing wearable sensors for OEP recognition,
existing studies have exhibited limitations in terms of accuracy and
robustness. This study addresses these limitations by employing a single
waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among
community-dwelling older adults in their daily lives. A cohort of 36 older
adults participated in laboratory settings, supplemented by an additional 7
older adults recruited for at-home assessments. The study proposes a Dual-Scale
Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level
sequence-to-sequence classification, incorporating them in one loss function.
In the first stage, the model focuses on recognizing each repetition of the
exercises (micro labels). Subsequent stages extend the recognition to encompass
the complete range of exercises (macro labels). The DS-MS-TCN model surpasses
existing state-of-the-art deep learning models, achieving f1-scores exceeding
80% and Intersection over Union (IoU) f1-scores surpassing 60% for all four
exercises evaluated. Notably, the model outperforms the prior study utilizing
the sliding window technique, eliminating the need for post-processing stages
and window size tuning. To our knowledge, we are the first to present a novel
perspective on enhancing Human Activity Recognition (HAR) systems through the
recognition of each repetition of activities.
\\ ( https://arxiv.org/abs/2402.02910 ,  10189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02933
Date: Mon, 5 Feb 2024 11:55:50 GMT   (1792kb,D)

Title: InterpretCC: Conditional Computation for Inherently Interpretable Neural
  Networks
Authors: Vinitra Swamy, Julian Blackwell, Jibril Frej, Martin Jaggi, Tanja
  K\"aser
Categories: cs.LG cs.CY cs.HC
\\
  Real-world interpretability for neural networks is a tradeoff between three
concerns: 1) it requires humans to trust the explanation approximation (e.g.
post-hoc approaches), 2) it compromises the understandability of the
explanation (e.g. automatically identified feature masks), and 3) it
compromises the model performance (e.g. decision trees). These shortcomings are
unacceptable for human-facing domains, like education, healthcare, or natural
language, which require trustworthy explanations, actionable interpretations,
and accurate predictions. In this work, we present InterpretCC (interpretable
conditional computation), a family of interpretable-by-design neural networks
that guarantee human-centric interpretability while maintaining comparable
performance to state-of-the-art models by adaptively and sparsely activating
features before prediction. We extend this idea into an interpretable
mixture-of-experts model, that allows humans to specify topics of interest,
discretely separates the feature space for each data point into topical
subnetworks, and adaptively and sparsely activates these topical subnetworks.
We demonstrate variations of the InterpretCC architecture for text and tabular
data across several real-world benchmarks: six online education courses, news
classification, breast cancer diagnosis, and review sentiment.
\\ ( https://arxiv.org/abs/2402.02933 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02949
Date: Mon, 5 Feb 2024 12:21:16 GMT   (5862kb,D)

Title: Kernel PCA for Out-of-Distribution Detection
Authors: Kun Fang, Qinghua Tao, Kexin Lv, Mingzhen He, Xiaolin Huang and Jie
  Yang
Categories: cs.LG
\\
  Out-of-Distribution (OoD) detection is vital for the reliability of Deep
Neural Networks (DNNs). Existing works have shown the insufficiency of
Principal Component Analysis (PCA) straightforwardly applied on the features of
DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA
suggests that the network features residing in OoD and InD are not well
separated by simply proceeding in a linear subspace, which instead can be
resolved through proper nonlinear mappings. In this work, we leverage the
framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD
and InD features are allocated with significantly different patterns. We devise
two feature mappings that induce non-linear kernels in KPCA to advocate the
separability between InD and OoD data in the subspace spanned by the principal
components. Given any test sample, the reconstruction error in such subspace is
then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time
complexity in inference. Extensive empirical results on multiple OoD data sets
and network structures verify the superiority of our KPCA-based detector in
efficiency and efficacy with state-of-the-art OoD detection performances.
\\ ( https://arxiv.org/abs/2402.02949 ,  5862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02951
Date: Mon, 5 Feb 2024 12:26:01 GMT   (367kb,D)

Title: Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine
  Workers
Authors: Ron Dorfman, Naseem Yehya, Kfir Y. Levy
Categories: cs.LG stat.ML
\\
  Byzantine-robust learning has emerged as a prominent fault-tolerant
distributed machine learning framework. However, most techniques consider the
static setting, wherein the identity of Byzantine machines remains fixed during
the learning process. This assumption does not capture real-world dynamic
Byzantine behaviors, which may include transient malfunctions or targeted
temporal attacks. Addressing this limitation, we propose $\textsf{DynaBRO}$ --
a new method capable of withstanding $\mathcal{O}(\sqrt{T})$ rounds of
Byzantine identity alterations (where $T$ is the total number of training
rounds), while matching the asymptotic convergence rate of the static setting.
Our method combines a multi-level Monte Carlo (MLMC) gradient estimation
technique with robust aggregation of worker updates and incorporates a
fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally,
by leveraging an adaptive learning rate, our approach eliminates the need for
knowing the percentage of Byzantine workers.
\\ ( https://arxiv.org/abs/2402.02951 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02964
Date: Mon, 5 Feb 2024 12:42:21 GMT   (301kb,D)

Title: Mixed Noise and Posterior Estimation with Conditional DeepGEM
Authors: Paul Hagemann, Johannes Hertrich, Maren Casfor, Sebastian Heidenreich,
  Gabriele Steidl
Categories: cs.LG physics.data-an
\\
  Motivated by indirect measurements and applications from nanometrology with a
mixed noise model, we develop a novel algorithm for jointly estimating the
posterior and the noise parameters in Bayesian inverse problems. We propose to
solve the problem by an expectation maximization (EM) algorithm. Based on the
current noise parameters, we learn in the E-step a conditional normalizing flow
that approximates the posterior. In the M-step, we propose to find the noise
parameter updates again by an EM algorithm, which has analytical formulas. We
compare the training of the conditional normalizing flow with the forward and
reverse KL, and show that our model is able to incorporate information from
many measurements, unlike previous approaches.
\\ ( https://arxiv.org/abs/2402.02964 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02976
Date: Mon, 5 Feb 2024 12:58:03 GMT   (41kb)

Title: Boosting, Voting Classifiers and Randomized Sample Compression Schemes
Authors: Arthur da Cunha, Kasper Green Larsen, Martin Ritzert
Categories: cs.LG stat.ML
\\
  In boosting, we aim to leverage multiple weak learners to produce a strong
learner. At the center of this paradigm lies the concept of building the strong
learner as a voting classifier, which outputs a weighted majority vote of the
weak learners. While many successful boosting algorithms, such as the iconic
AdaBoost, produce voting classifiers, their theoretical performance has long
remained sub-optimal: the best known bounds on the number of training examples
necessary for a voting classifier to obtain a given accuracy has so far always
contained at least two logarithmic factors above what is known to be achievable
by general weak-to-strong learners. In this work, we break this barrier by
proposing a randomized boosting algorithm that outputs voting classifiers whose
generalization error contains a single logarithmic dependency on the sample
size. We obtain this result by building a general framework that extends sample
compression methods to support randomized learning algorithms based on
sub-sampling.
\\ ( https://arxiv.org/abs/2402.02976 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02977
Date: Mon, 5 Feb 2024 12:58:29 GMT   (49830kb,D)

Title: Variational Flow Models: Flowing in Your Style
Authors: Kien Do, Duc Kieu, Toan Nguyen, Dang Nguyen, Hung Le, Dung Nguyen,
  Thin Nguyen
Categories: cs.LG cs.AI
\\
  We introduce a variational inference interpretation for models of "posterior
flows" - generalizations of "probability flows" to a broader class of
stochastic processes not necessarily diffusion processes. We coin the resulting
models as "Variational Flow Models". Additionally, we propose a systematic
training-free method to transform the posterior flow of a "linear" stochastic
process characterized by the equation Xt = at * X0 + st * X1 into a straight
constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation
facilitates fast sampling along the original posterior flow without training a
new model of the SC flow. The flexibility of our approach allows us to extend
our transformation to inter-convert two posterior flows from distinct "linear"
stochastic processes. Moreover, we can easily integrate high-order numerical
solvers into the transformed SC flow, further enhancing sampling accuracy and
efficiency. Rigorous theoretical analysis and extensive experimental results
substantiate the advantages of our framework.
\\ ( https://arxiv.org/abs/2402.02977 ,  49830kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02986
Date: Mon, 5 Feb 2024 13:16:38 GMT   (4534kb,D)

Title: A Safety-Adapted Loss for Pedestrian Detection in Automated Driving
Authors: Maria Lyssenko, Piyush Pimplikar, Maarten Bieshaar, Farzad Nozarian,
  Rudolph Triebel
Categories: cs.LG cs.CV
\\
  In safety-critical domains like automated driving (AD), errors by the object
detector may endanger pedestrians and other vulnerable road users (VRU). As
common evaluation metrics are not an adequate safety indicator, recent works
employ approaches to identify safety-critical VRU and back-annotate the risk to
the object detector. However, those approaches do not consider the safety
factor in the deep neural network (DNN) training process. Thus,
state-of-the-art DNN penalizes all misdetections equally irrespective of their
criticality. Subsequently, to mitigate the occurrence of critical failure
cases, i.e., false negatives, a safety-aware training strategy might be
required to enhance the detection performance for critical pedestrians. In this
paper, we propose a novel safety-aware loss variation that leverages the
estimated per-pedestrian criticality scores during training. We exploit the
reachability set-based time-to-collision (TTC-RSB) metric from the motion
domain along with distance information to account for the worst-case threat
quantifying the criticality. Our evaluation results using RetinaNet and FCOS on
the nuScenes dataset demonstrate that training the models with our safety-aware
loss function mitigates the misdetection of critical pedestrians without
sacrificing performance for the general case, i.e., pedestrians outside the
safety-critical zone.
\\ ( https://arxiv.org/abs/2402.02986 ,  4534kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02992
Date: Mon, 5 Feb 2024 13:31:28 GMT   (1978kb,D)

Title: Decoding-time Realignment of Language Models
Authors: Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello,
  Quentin Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal
  Valko, Mathieu Blondel
Categories: cs.LG cs.AI cs.CL
\\
  Aligning language models with human preferences is crucial for reducing
errors and biases in these models. Alignment techniques, such as reinforcement
learning from human feedback (RLHF), are typically cast as optimizing a
tradeoff between human preference rewards and a proximity regularization term
that encourages staying close to the unaligned model. Selecting an appropriate
level of regularization is critical: insufficient regularization can lead to
reduced model capabilities due to reward hacking, whereas excessive
regularization hinders alignment. Traditional methods for finding the optimal
regularization level require retraining multiple models with varying
regularization strengths. This process, however, is resource-intensive,
especially for large models. To address this challenge, we propose
decoding-time realignment (DeRa), a simple method to explore and evaluate
different regularization strengths in aligned models without retraining. DeRa
enables control over the degree of alignment, allowing users to smoothly
transition between unaligned and aligned models. It also enhances the
efficiency of hyperparameter tuning by enabling the identification of effective
regularization strengths using a validation dataset.
\\ ( https://arxiv.org/abs/2402.02992 ,  1978kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02996
Date: Mon, 5 Feb 2024 13:34:21 GMT   (8664kb,D)

Title: Text-Guided Image Clustering
Authors: Andreas Stephan, Lukas Miklautz, Kevin Sidak, Jan Philip Wahle, Bela
  Gipp, Claudia Plant, Benjamin Roth
Categories: cs.LG cs.CV
\\
  Image clustering divides a collection of images into meaningful groups,
typically interpreted post-hoc via human-given annotations. Those are usually
in the form of text, begging the question of using text as an abstraction for
image clustering. Current image clustering methods, however, neglect the use of
generated textual descriptions. We, therefore, propose Text-Guided Image
Clustering, i.e., generating text using image captioning and visual
question-answering (VQA) models and subsequently clustering the generated text.
Further, we introduce a novel approach to inject task- or domain knowledge for
clustering by prompting VQA models. Across eight diverse image clustering
datasets, our results show that the obtained text representations often
outperform image features. Additionally, we propose a counting-based cluster
explainability method. Our evaluations show that the derived keyword-based
explanations describe clusters better than the respective cluster accuracy
suggests. Overall, this research challenges traditional approaches and paves
the way for a paradigm shift in image clustering, using generated text.
\\ ( https://arxiv.org/abs/2402.02996 ,  8664kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02998
Date: Mon, 5 Feb 2024 13:37:00 GMT   (1356kb,D)

Title: Careful with that Scalpel: Improving Gradient Surgery with an EMA
Authors: Yu-Guan Hsieh, James Thornton, Eugene Ndiaye, Michal Klein, Marco
  Cuturi, Pierre Ablin
Categories: cs.LG stat.ML
\\
  Beyond minimizing a single training loss, many deep learning estimation
pipelines rely on an auxiliary objective to quantify and encourage desirable
properties of the model (e.g. performance on another dataset, robustness,
agreement with a prior). Although the simplest approach to incorporating an
auxiliary loss is to sum it with the training loss as a regularizer, recent
works have shown that one can improve performance by blending the gradients
beyond a simple sum; this is known as gradient surgery. We cast the problem as
a constrained minimization problem where the auxiliary objective is minimized
among the set of minimizers of the training loss. To solve this bilevel
problem, we follow a parameter update direction that combines the training loss
gradient and the orthogonal projection of the auxiliary gradient to the
training gradient. In a setting where gradients come from mini-batches, we
explain how, using a moving average of the training loss gradients, we can
carefully maintain this critical orthogonality property. We demonstrate that
our method, Bloop, can lead to much better performances on NLP and vision
experiments than other gradient surgery methods without EMA.
\\ ( https://arxiv.org/abs/2402.02998 ,  1356kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03006
Date: Mon, 5 Feb 2024 13:46:04 GMT   (2816kb)

Title: On the development of a practical Bayesian optimisation algorithm for
  expensive experiments and simulations with changing environmental conditions
Authors: Mike Diessner, Kevin J. Wilson, Richard D. Whalley
Categories: cs.LG stat.ML
Comments: 23 pages, 10 figures
\\
  Experiments in engineering are typically conducted in controlled environments
where parameters can be set to any desired value. This assumes that the same
applies in a real-world setting -- an assumption that is often incorrect as
many experiments are influenced by uncontrollable environmental conditions such
as temperature, humidity and wind speed. When optimising such experiments, the
focus should lie on finding optimal values conditionally on these
uncontrollable variables. This article extends Bayesian optimisation to the
optimisation of systems in changing environments that include controllable and
uncontrollable parameters. The extension fits a global surrogate model over all
controllable and environmental variables but optimises only the controllable
parameters conditional on measurements of the uncontrollable variables. The
method is validated on two synthetic test functions and the effects of the
noise level, the number of the environmental parameters, the parameter
fluctuation, the variability of the uncontrollable parameters, and the
effective domain size are investigated. ENVBO, the proposed algorithm resulting
from this investigation, is applied to a wind farm simulator with eight
controllable and one environmental parameter. ENVBO finds solutions for the
full domain of the environmental variable that outperforms results from
optimisation algorithms that only focus on a fixed environmental value in all
but one case while using a fraction of their evaluation budget. This makes the
proposed approach very sample-efficient and cost-effective. An off-the-shelf
open-source version of ENVBO is available via the NUBO Python package.
\\ ( https://arxiv.org/abs/2402.03006 ,  2816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03011
Date: Mon, 5 Feb 2024 13:50:08 GMT   (1401kb,D)

Title: On the Impact of Output Perturbation on Fairness in Binary Linear
  Classification
Authors: Vitalii Emelianov, Micha\"el Perrot
Categories: cs.LG
\\
  We theoretically study how differential privacy interacts with both
individual and group fairness in binary linear classification. More precisely,
we focus on the output perturbation mechanism, a classic approach in
privacy-preserving machine learning. We derive high-probability bounds on the
level of individual and group fairness that the perturbed models can achieve
compared to the original model. Hence, for individual fairness, we prove that
the impact of output perturbation on the level of fairness is bounded but grows
with the dimension of the model. For group fairness, we show that this impact
is determined by the distribution of so-called angular margins, that is signed
margins of the non-private model re-scaled by the norm of each example.
\\ ( https://arxiv.org/abs/2402.03011 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03014
Date: Mon, 5 Feb 2024 13:52:56 GMT   (1405kb,D)

Title: Whom to Trust? Elective Learning for Distributed Gaussian Process
  Regression
Authors: Zewen Yang, Xiaobing Dai, Akshat Dubey, Sandra Hirche, Georges Hattab
Categories: cs.LG cs.AI
Comments: 9 pages, conference preprint
\\
  This paper introduces an innovative approach to enhance distributed
cooperative learning using Gaussian process (GP) regression in multi-agent
systems (MASs). The key contribution of this work is the development of an
elective learning algorithm, namely prior-aware elective distributed GP
(Pri-GP), which empowers agents with the capability to selectively request
predictions from neighboring agents based on their trustworthiness. The
proposed Pri-GP effectively improves individual prediction accuracy, especially
in cases where the prior knowledge of an agent is incorrect. Moreover, it
eliminates the need for computationally intensive variance calculations for
determining aggregation weights in distributed GP. Furthermore, we establish a
prediction error bound within the Pri-GP framework, ensuring the reliability of
predictions, which is regarded as a crucial property in safety-critical MAS
applications.
\\ ( https://arxiv.org/abs/2402.03014 ,  1405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03017
Date: Mon, 5 Feb 2024 13:55:54 GMT   (20728kb,D)

Title: Toward Green and Human-Like Artificial Intelligence: A Complete Survey
  on Contemporary Few-Shot Learning Approaches
Authors: Georgios Tsoumplekas, Vladislav Li, Vasileios Argyriou, Anastasios
  Lytos, Eleftherios Fountoukidis, Sotirios K. Goudos, Ioannis D. Moscholios,
  Panagiotis Sarigiannidis
Categories: cs.LG cs.AI
Comments: 35 pages, 9 figures. Submitted to ACM Computing Surveys
\\
  Despite deep learning's widespread success, its data-hungry and
computationally expensive nature makes it impractical for many data-constrained
real-world applications. Few-Shot Learning (FSL) aims to address these
limitations by enabling rapid adaptation to novel learning tasks, seeing
significant growth in recent years. This survey provides a comprehensive
overview of the field's latest advancements. Initially, FSL is formally
defined, and its relationship with different learning fields is presented. A
novel taxonomy is introduced, extending previously proposed ones, and
real-world applications in classic and novel fields are described. Finally,
recent trends shaping the field, outstanding challenges, and promising future
research directions are discussed.
\\ ( https://arxiv.org/abs/2402.03017 ,  20728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03021
Date: Mon, 5 Feb 2024 14:00:53 GMT   (1395kb,D)

Title: Data-induced multiscale losses and efficient multirate gradient descent
  schemes
Authors: Juncai He, Liangchen Liu, and Yen-Hsi (Richard) Tsai
Categories: cs.LG cs.NA math.NA
Comments: 28 pages, 4 figures, submitted under review
MSC-class: 65F10, 65F45, 68T07
ACM-class: G.1.6; I.2.6
\\
  This paper investigates the impact of multiscale data on machine learning
algorithms, particularly in the context of deep learning. A dataset is
multiscale if its distribution shows large variations in scale across different
directions. This paper reveals multiscale structures in the loss landscape,
including its gradients and Hessians inherited from the data. Correspondingly,
it introduces a novel gradient descent approach, drawing inspiration from
multiscale algorithms used in scientific computing. This approach seeks to
transcend empirical learning rate selection, offering a more systematic,
data-informed strategy to enhance training efficiency, especially in the later
stages.
\\ ( https://arxiv.org/abs/2402.03021 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03038
Date: Mon, 5 Feb 2024 14:23:43 GMT   (4868kb,D)

Title: Automatic Combination of Sample Selection Strategies for Few-Shot
  Learning
Authors: Branislav Pecher, Ivan Srba, Maria Bielikova, Joaquin Vanschoren
Categories: cs.LG cs.AI cs.CL
\\
  In few-shot learning, such as meta-learning, few-shot fine-tuning or
in-context learning, the limited number of samples used to train a model have a
significant impact on the overall success. Although a large number of sample
selection strategies exist, their impact on the performance of few-shot
learning is not extensively known, as most of them have been so far evaluated
in typical supervised settings only. In this paper, we thoroughly investigate
the impact of 20 sample selection strategies on the performance of 5 few-shot
learning approaches over 8 image and 6 text datasets. In addition, we propose a
new method for automatic combination of sample selection strategies (ACSESS)
that leverages the strengths and complementary information of the individual
strategies. The experimental results show that our method consistently
outperforms the individual selection strategies, as well as the recently
proposed method for selecting support examples for in-context learning. We also
show a strong modality, dataset and approach dependence for the majority of
strategies as well as their dependence on the number of shots - demonstrating
that the sample selection strategies play a significant role for lower number
of shots, but regresses to random selection at higher number of shots.
\\ ( https://arxiv.org/abs/2402.03038 ,  4868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03046
Date: Mon, 5 Feb 2024 14:32:00 GMT   (35651kb,D)

Title: Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement
  Learning
Authors: Shengyi Huang and Quentin Gallou\'edec and Florian Felten and Antonin
  Raffin and Rousslan Fernand Julien Dossa and Yanxiao Zhao and Ryan Sullivan
  and Viktor Makoviychuk and Denys Makoviichuk and Mohamad H. Danesh and Cyril
  Roum\'egous and Jiayi Weng and Chufan Chen and Md Masudur Rahman and Jo\~ao
  G. M. Ara\'ujo and Guorui Quan and Daniel Tan and Timo Klein and Rujikorn
  Charakorn and Mark Towers and Yann Berthelot and Kinal Mehta and Dipam
  Chakraborty and Arjun KG and Valentin Charraut and Chang Ye and Zichen Liu
  and Lucas N. Alegre and Alexander Nikulin and Xiao Hu and Tianlin Liu and
  Jongwook Choi and Brent Yi
Categories: cs.LG
Comments: Under review
\\
  In many Reinforcement Learning (RL) papers, learning curves are useful
indicators to measure the effectiveness of RL algorithms. However, the complete
raw data of the learning curves are rarely available. As a result, it is
usually necessary to reproduce the experiments from scratch, which can be
time-consuming and error-prone. We present Open RL Benchmark, a set of fully
tracked RL experiments, including not only the usual data such as episodic
return, but also all algorithm-specific and system metrics. Open RL Benchmark
is community-driven: anyone can download, use, and contribute to the data. At
the time of writing, more than 25,000 runs have been tracked, for a cumulative
duration of more than 8 years. Open RL Benchmark covers a wide range of RL
libraries and reference implementations. Special care is taken to ensure that
each experiment is precisely reproducible by providing not only the full
parameters, but also the versions of the dependencies used to generate it. In
addition, Open RL Benchmark comes with a command-line interface (CLI) for easy
fetching and generating figures to present the results. In this document, we
include two case studies to demonstrate the usefulness of Open RL Benchmark in
practice. To the best of our knowledge, Open RL Benchmark is the first RL
benchmark of its kind, and the authors hope that it will improve and facilitate
the work of researchers in the field.
\\ ( https://arxiv.org/abs/2402.03046 ,  35651kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03055
Date: Mon, 5 Feb 2024 14:42:45 GMT   (155kb,D)

Title: Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes
  Uncertainty
Authors: Bahareh Tasdighi, Nicklas Werge, Yi-Shan Wu, Melih Kandemir
Categories: cs.LG
Comments: 18 pages, 4 figures, 7 tables
\\
  We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning
algorithm with improved continuous control performance thanks to its ability to
mitigate the exploration-exploitation trade-off. PAC achieves this by
seamlessly integrating stochastic policies and critics, creating a dynamic
synergy between the estimation of critic uncertainty and actor training. The
key contribution of our PAC algorithm is that it explicitly models and infers
epistemic uncertainty in the critic through Probably Approximately
Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty
enables PAC to adapt its exploration strategy as it learns, guiding the actor's
decision-making process. PAC compares favorably against fixed or pre-scheduled
exploration schemes of the prior art. The synergy between stochastic policies
and critics, guided by PAC-Bayes analysis, represents a fundamental step
towards a more adaptive and effective exploration strategy in deep
reinforcement learning. We report empirical evaluations demonstrating PAC's
enhanced stability and improved performance over the state of the art in
diverse continuous control problems.
\\ ( https://arxiv.org/abs/2402.03055 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03110
Date: Mon, 5 Feb 2024 15:38:01 GMT   (5698kb,D)

Title: Non-Stationary Latent Auto-Regressive Bandits
Authors: Anna L. Trella, Walter Dempsey, Finale Doshi-Velez, and Susan A.
  Murphy
Categories: cs.LG cs.AI
\\
  We consider the stochastic multi-armed bandit problem with non-stationary
rewards. We present a novel formulation of non-stationarity in the environment
where changes in the mean reward of the arms over time are due to some unknown,
latent, auto-regressive (AR) state of order $k$. We call this new environment
the latent AR bandit. Different forms of the latent AR bandit appear in many
real-world settings, especially in emerging scientific fields such as
behavioral health or education where there are few mechanistic models of the
environment. If the AR order $k$ is known, we propose an algorithm that
achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our
algorithm outperforms standard UCB across multiple non-stationary environments,
even if $k$ is mis-specified.
\\ ( https://arxiv.org/abs/2402.03110 ,  5698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03112
Date: Mon, 5 Feb 2024 15:44:43 GMT   (9075kb,D)

Title: Infrared Spectra Prediction for Diazo Groups Utilizing a Machine
  Learning Approach with Structural Attention Mechanism
Authors: Chengchun Liu and Fanyang Mo
Categories: cs.LG cs.AI physics.chem-ph
Comments: 21 pages, 5 figures
\\
  Infrared (IR) spectroscopy is a pivotal technique in chemical research for
elucidating molecular structures and dynamics through vibrational and
rotational transitions. However, the intricate molecular fingerprints
characterized by unique vibrational and rotational patterns present substantial
analytical challenges. Here, we present a machine learning approach employing a
Structural Attention Mechanism tailored to enhance the prediction and
interpretation of infrared spectra, particularly for diazo compounds. Our model
distinguishes itself by honing in on chemical information proximal to
functional groups, thereby significantly bolstering the accuracy, robustness,
and interpretability of spectral predictions. This method not only demystifies
the correlations between infrared spectral features and molecular structures
but also offers a scalable and efficient paradigm for dissecting complex
molecular interactions.
\\ ( https://arxiv.org/abs/2402.03112 ,  9075kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03115
Date: Mon, 5 Feb 2024 15:45:55 GMT   (6043kb,D)

Title: Discovering interpretable models of scientific image data with deep
  learning
Authors: Christopher J. Soelistyo and Alan R. Lowe
Categories: cs.LG
Comments: 33 pages (including appendices), 27 figures
\\
  How can we find interpretable, domain-appropriate models of natural phenomena
given some complex, raw data such as images? Can we use such models to derive
scientific insight from the data? In this paper, we propose some methods for
achieving this. In particular, we implement disentangled representation
learning, sparse deep neural network training and symbolic regression, and
assess their usefulness in forming interpretable models of complex image data.
We demonstrate their relevance to the field of bioimaging using a well-studied
test problem of classifying cell states in microscopy data. We find that such
methods can produce highly parsimonious models that achieve $\sim98\%$ of the
accuracy of black-box benchmark models, with a tiny fraction of the complexity.
We explore the utility of such interpretable models in producing scientific
explanations of the underlying biological phenomenon.
\\ ( https://arxiv.org/abs/2402.03115 ,  6043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03126
Date: Mon, 5 Feb 2024 15:51:49 GMT   (29kb,D)

Title: How Free is Parameter-Free Stochastic Optimization?
Authors: Amit Attia, Tomer Koren
Categories: cs.LG math.OC stat.ML
Comments: 27 pages
\\
  We study the problem of parameter-free stochastic optimization, inquiring
whether, and under what conditions, do fully parameter-free methods exist:
these are methods that achieve convergence rates competitive with optimally
tuned methods, without requiring significant knowledge of the true problem
parameters. Existing parameter-free methods can only be considered
``partially'' parameter-free, as they require some non-trivial knowledge of the
true problem parameters, such as a bound on the stochastic gradient norms, a
bound on the distance to a minimizer, etc. In the non-convex setting, we
demonstrate that a simple hyperparameter search technique results in a fully
parameter-free method that outperforms more sophisticated state-of-the-art
algorithms. We also provide a similar result in the convex setting with access
to noisy function values under mild noise assumptions. Finally, assuming only
access to stochastic gradients, we establish a lower bound that renders fully
parameter-free stochastic convex optimization infeasible, and provide a method
which is (partially) parameter-free up to the limit indicated by our lower
bound.
\\ ( https://arxiv.org/abs/2402.03126 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03138
Date: Mon, 5 Feb 2024 16:08:58 GMT   (1784kb,D)

Title: Just Cluster It: An Approach for Exploration in High-Dimensions using
  Clustering and Pre-Trained Representations
Authors: Stefan Sylvius Wagner and Stefan Harmeling
Categories: cs.LG cs.AI
\\
  In this paper we adopt a representation-centric perspective on exploration in
reinforcement learning, viewing exploration fundamentally as a density
estimation problem. We investigate the effectiveness of clustering
representations for exploration in 3-D environments, based on the observation
that the importance of pixel changes between transitions is less pronounced in
3-D environments compared to 2-D environments, where pixel changes between
transitions are typically distinct and significant. We propose a method that
performs episodic and global clustering on random representations and on
pre-trained DINO representations to count states, i.e, estimate pseudo-counts.
Surprisingly, even random features can be clustered effectively to count states
in 3-D environments, however when these become visually more complex,
pre-trained DINO representations are more effective thanks to the pre-trained
inductive biases in the representations. Overall, this presents a pathway for
integrating pre-trained biases into exploration. We evaluate our approach on
the VizDoom and Habitat environments, demonstrating that our method surpasses
other well-known exploration methods in these settings.
\\ ( https://arxiv.org/abs/2402.03138 ,  1784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03139
Date: Mon, 5 Feb 2024 16:09:35 GMT   (4876kb,D)

Title: Enhancing Neural Subset Selection: Integrating Background Information
  into Set Representations
Authors: Binghui Xie, Yatao Bian, Kaiwen zhou, Yongqiang Chen, Peilin Zhao, Bo
  Han, Wei Meng, James Cheng
Categories: cs.LG
\\
  Learning neural subset selection tasks, such as compound selection in
AI-aided drug discovery, have become increasingly pivotal across diverse
applications. The existing methodologies in the field primarily concentrate on
constructing models that capture the relationship between utility function
values and subsets within their respective supersets. However, these approaches
tend to overlook the valuable information contained within the superset when
utilizing neural networks to model set functions. In this work, we address this
oversight by adopting a probabilistic perspective. Our theoretical findings
demonstrate that when the target value is conditioned on both the input set and
subset, it is essential to incorporate an \textit{invariant sufficient
statistic} of the superset into the subset of interest for effective learning.
This ensures that the output value remains invariant to permutations of the
subset and its corresponding superset, enabling identification of the specific
superset from which the subset originated. Motivated by these insights, we
propose a simple yet effective information aggregation module designed to merge
the representations of subsets and supersets from a permutation invariance
perspective. Comprehensive empirical evaluations across diverse tasks and
datasets validate the enhanced efficacy of our approach over conventional
methods, underscoring the practicality and potency of our proposed strategies
in real-world contexts.
\\ ( https://arxiv.org/abs/2402.03139 ,  4876kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03141
Date: Mon, 5 Feb 2024 16:11:03 GMT   (3943kb,D)

Title: Boosting Long-Delayed Reinforcement Learning with Auxiliary
  Short-Delayed Task
Authors: Qingyuan Wu, Simon Sinong Zhan, Yixuan Wang, Chung-Wei Lin, Chen Lv,
  Qi Zhu, Chao Huang
Categories: cs.LG cs.AI cs.SY eess.SY
\\
  Reinforcement learning is challenging in delayed scenarios, a common
real-world situation where observations and interactions occur with delays.
State-of-the-art (SOTA) state-augmentation techniques either suffer from the
state-space explosion along with the delayed steps, or performance degeneration
in stochastic environments. To address these challenges, our novel
Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary
short-delayed task to accelerate the learning on a long-delayed task without
compromising the performance in stochastic environments. Specifically, AD-RL
learns the value function in the short-delayed task and then employs it with
the bootstrapping and policy improvement techniques in the long-delayed task.
We theoretically show that this can greatly reduce the sample complexity
compared to directly learning on the original long-delayed task. On
deterministic and stochastic benchmarks, our method remarkably outperforms the
SOTAs in both sample efficiency and policy performance.
\\ ( https://arxiv.org/abs/2402.03141 ,  3943kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03142
Date: Mon, 5 Feb 2024 16:11:43 GMT   (27113kb,D)

Title: Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for
  Large Language Models
Authors: Michele Mastromattei, Fabio Massimo Zanzotto
Categories: cs.LG
\\
  Neural network pruning has become increasingly crucial due to the complexity
of neural network models and their widespread use in various fields. Existing
pruning algorithms often suffer from limitations such as architecture
specificity, excessive complexity and reliance on complex calculations,
rendering them impractical for real-world applications. In this paper, we
propose KEN: a straightforward, universal and unstructured pruning algorithm
based on Kernel Density Estimation (KDE). KEN aims to construct optimized
transformer models by selectively preserving the most significant parameters
while restoring others to their pre-training state. This approach maintains
model performance while allowing storage of only the optimized subnetwork,
leading to significant memory savings. Extensive evaluations on seven
transformer models demonstrate that KEN achieves equal or better performance
than the original models with a minimum parameter reduction of 25%. In-depth
comparisons against other pruning and PEFT algorithms confirm KEN
effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that
visualizes the optimized model composition and the subnetwork selected by KEN.
\\ ( https://arxiv.org/abs/2402.03142 ,  27113kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03146
Date: Mon, 5 Feb 2024 16:13:00 GMT   (4367kb,D)

Title: A Multi-step Loss Function for Robust Learning of the Dynamics in
  Model-based Reinforcement Learning
Authors: Abdelhakim Benechehab, Albert Thomas, Giuseppe Paolo, Maurizio
  Filippone and Bal\'azs K\'egl
Categories: cs.LG stat.ML
\\
  In model-based reinforcement learning, most algorithms rely on simulating
trajectories from one-step models of the dynamics learned on data. A critical
challenge of this approach is the compounding of one-step prediction errors as
the length of the trajectory grows. In this paper we tackle this issue by using
a multi-step objective to train one-step models. Our objective is a weighted
sum of the mean squared error (MSE) loss at various future horizons. We find
that this new loss is particularly useful when the data is noisy (additive
Gaussian noise in the observations), which is often the case in real-life
environments. To support the multi-step loss, first we study its properties in
two tractable cases: i) uni-dimensional linear system, and ii) two-parameter
non-linear system. Second, we show in a variety of tasks (environments or
datasets) that the models learned with this loss achieve a significant
improvement in terms of the averaged R2-score on future prediction horizons.
Finally, in the pure batch reinforcement learning setting, we demonstrate that
one-step models serve as strong baselines when dynamics are deterministic,
while multi-step models would be more advantageous in the presence of noise,
highlighting the potential of our approach in real-world applications.
\\ ( https://arxiv.org/abs/2402.03146 ,  4367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03158
Date: Mon, 5 Feb 2024 16:27:59 GMT   (9473kb,D)

Title: Optimal and Near-Optimal Adaptive Vector Quantization
Authors: Ran Ben-Basat, Yaniv Ben-Itzhak, Michael Mitzenmacher, Shay Vargaftik
Categories: cs.LG cs.DS cs.IT cs.NI math.IT
\\
  Quantization is a fundamental optimization for many machine-learning use
cases, including compressing gradients, model weights and activations, and
datasets. The most accurate form of quantization is \emph{adaptive}, where the
error is minimized with respect to a given input, rather than optimizing for
the worst case. However, optimal adaptive quantization methods are considered
infeasible in terms of both their runtime and memory requirements.
  We revisit the Adaptive Vector Quantization (AVQ) problem and present
algorithms that find optimal solutions with asymptotically improved time and
space complexity. We also present an even faster near-optimal algorithm for
large inputs. Our experiments show our algorithms may open the door to using
AVQ more extensively in a variety of machine learning applications.
\\ ( https://arxiv.org/abs/2402.03158 ,  9473kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03170
Date: Mon, 5 Feb 2024 16:39:12 GMT   (1279kb,D)

Title: Is Mamba Capable of In-Context Learning?
Authors: Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank
  Hutter
Categories: cs.LG
\\
  This work provides empirical evidence that Mamba, a newly proposed selective
structured state space model, has similar in-context learning (ICL)
capabilities as transformers. We evaluated Mamba on tasks involving simple
function approximation as well as more complex natural language processing
problems. Our results demonstrate that across both categories of tasks, Mamba
matches the performance of transformer models for ICL. Further analysis reveals
that like transformers, Mamba appears to solve ICL problems by incrementally
optimizing its internal representations. Overall, our work suggests that Mamba
can be an efficient alternative to transformers for ICL tasks involving longer
input sequences.
\\ ( https://arxiv.org/abs/2402.03170 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03175
Date: Mon, 5 Feb 2024 16:42:10 GMT   (305kb,D)

Title: The Matrix: A Bayesian learning model for LLMs
Authors: Siddhartha Dalal and Vishal Misra
Categories: cs.LG cs.AI
Comments: 12 pages, 6 figures
ACM-class: I.2.7
\\
  In this paper, we introduce a Bayesian learning model to understand the
behavior of Large Language Models (LLMs). We explore the optimization metric of
LLMs, which is based on predicting the next token, and develop a novel model
grounded in this principle. Our approach involves constructing an ideal
generative text model represented by a multinomial transition probability
matrix with a prior, and we examine how LLMs approximate this matrix. We
discuss the continuity of the mapping between embeddings and multinomial
distributions, and present the Dirichlet approximation theorem to approximate
any prior. Additionally, we demonstrate how text generation by LLMs aligns with
Bayesian learning principles and delve into the implications for in-context
learning, specifically explaining why in-context learning emerges in larger
models where prompts are considered as samples to be updated. Our findings
indicate that the behavior of LLMs is consistent with Bayesian Learning,
offering new insights into their functioning and potential applications.
\\ ( https://arxiv.org/abs/2402.03175 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03182
Date: Mon, 5 Feb 2024 16:46:35 GMT   (163kb,D)

Title: Empowering Time Series Analysis with Large Language Models: A Survey
Authors: Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider,
  Yuriy Nevmyvaka, Dongjin Song
Categories: cs.LG
\\
  Recently, remarkable progress has been made over large language models
(LLMs), demonstrating their unprecedented capability in varieties of natural
language tasks. However, completely training a large general-purpose model from
the scratch is challenging for time series analysis, due to the large volumes
and varieties of time series data, as well as the non-stationarity that leads
to concept drift impeding continuous model adaptation and re-training. Recent
advances have shown that pre-trained LLMs can be exploited to capture complex
dependencies in time series data and facilitate various applications. In this
survey, we provide a systematic overview of existing methods that leverage LLMs
for time series analysis. Specifically, we first state the challenges and
motivations of applying language models in the context of time series as well
as brief preliminaries of LLMs. Next, we summarize the general pipeline for
LLM-based time series analysis, categorize existing methods into different
groups (i.e., direct query, tokenization, prompt design, fine-tune, and model
integration), and highlight the key ideas within each group. We also discuss
the applications of LLMs for both general and spatial-temporal time series
data, tailored to specific domains. Finally, we thoroughly discuss future
research opportunities to empower time series analysis with LLMs.
\\ ( https://arxiv.org/abs/2402.03182 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03187
Date: Mon, 5 Feb 2024 16:51:59 GMT   (1939kb,D)

Title: How Good is a Single Basin?
Authors: Kai Lion, Lorenzo Noci, Thomas Hofmann, Gregor Bachmann
Categories: cs.LG
\\
  The multi-modal nature of neural loss landscapes is often considered to be
the main driver behind the empirical success of deep ensembles. In this work,
we probe this belief by constructing various "connected" ensembles which are
restricted to lie in the same basin. Through our experiments, we demonstrate
that increased connectivity indeed negatively impacts performance. However,
when incorporating the knowledge from other basins implicitly through
distillation, we show that the gap in performance can be mitigated by
re-discovering (multi-basin) deep ensembles within a single basin. Thus, we
conjecture that while the extra-basin knowledge is at least partially present
in any given basin, it cannot be easily harnessed without learning it from
other basins.
\\ ( https://arxiv.org/abs/2402.03187 ,  1939kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03191
Date: Mon, 5 Feb 2024 16:57:24 GMT   (7973kb,D)

Title: Isotropy, Clusters, and Classifiers
Authors: Timothee Mickus, Stig-Arne Gr\"onroos, Joseph Attieh
Categories: cs.LG cs.CL
\\
  Whether embedding spaces use all their dimensions equally, i.e., whether they
are isotropic, has been a recent subject of discussion. Evidence has been
accrued both for and against enforcing isotropy in embedding spaces. In the
present paper, we stress that isotropy imposes requirements on the embedding
space that are not compatible with the presence of clusters -- which also
negatively impacts linear classification objectives. We demonstrate this fact
empirically and use it to shed light on previous results from the literature.
\\ ( https://arxiv.org/abs/2402.03191 ,  7973kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03201
Date: Mon, 5 Feb 2024 17:12:21 GMT   (38706kb,D)

Title: Guidance with Spherical Gaussian Constraint for Conditional Diffusion
Authors: Lingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, Ye Shi
Categories: cs.LG
\\
  Recent advances in diffusion models attempt to handle conditional generative
tasks by utilizing a differentiable loss function for guidance without the need
for additional training. While these methods achieved certain success, they
often compromise on sample quality and require small guidance step sizes,
leading to longer sampling processes. This paper reveals that the fundamental
issue lies in the manifold deviation during the sampling process when loss
guidance is employed. We theoretically show the existence of manifold deviation
by establishing a certain lower bound for the estimation error of the loss
guidance. To mitigate this problem, we propose Diffusion with Spherical
Gaussian constraint (DSG), drawing inspiration from the concentration
phenomenon in high-dimensional Gaussian distributions. DSG effectively
constrains the guidance step within the intermediate data manifold through
optimization and enables the use of larger guidance steps. Furthermore, we
present a closed-form solution for DSG denoising with the Spherical Gaussian
constraint. Notably, DSG can seamlessly integrate as a plugin module within
existing training-free conditional diffusion methods. Implementing DSG merely
involves a few lines of additional code with almost no extra computational
overhead, yet it leads to significant performance improvements. Comprehensive
experimental results in various conditional generation tasks validate the
superiority and adaptability of DSG in terms of both sample quality and time
efficiency.
\\ ( https://arxiv.org/abs/2402.03201 ,  38706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03207
Date: Mon, 5 Feb 2024 17:17:57 GMT   (29432kb,D)

Title: Light and Optimal Schr\"odinger Bridge Matching
Authors: Nikita Gushchin and Sergei Kholkin and Evgeny Burnaev and Alexander
  Korotin
Categories: cs.LG
\\
  Schr\"odinger Bridges (SB) have recently gained the attention of the ML
community as a promising extension of classic diffusion models which is also
interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB
exploit the pervasive bridge matching procedures. Such procedures aim to
recover a stochastic process transporting the mass between distributions given
only a transport plan between them. In particular, given the EOT plan, these
procedures can be adapted to solve SB. This fact is heavily exploited by recent
works giving rives to matching-based SB solvers. The cornerstone here is
recovering the EOT plan: recent works either use heuristical approximations
(e.g., the minibatch OT) or establish iterative matching procedures which by
the design accumulate the error during the training. We address these
limitations and propose a novel procedure to learn SB which we call the
\textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal
parameterization of the diffusion process and provably recovers the SB process
\textbf{(a)} with a single bridge matching step and \textbf{(b)} with arbitrary
transport plan as the input. Furthermore, we show that the optimal bridge
matching objective coincides with the recently discovered energy-based modeling
(EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a
light solver (which we call LightSB-M) to implement optimal matching in
practice using the Gaussian mixture parameterization of the Schr\"odinger
potential. We experimentally showcase the performance of our solver in a range
of practical tasks. The code for the LightSB-M solver can be found at
\url{https://github.com/SKholkin/LightSB-Matching}.
\\ ( https://arxiv.org/abs/2402.03207 ,  29432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03226
Date: Mon, 5 Feb 2024 17:37:46 GMT   (1036kb,D)

Title: FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion
Authors: Xing Han, Huy Nguyen, Carl Harris, Nhat Ho, Suchi Saria
Categories: cs.LG
Comments: 35 pages, 8 tables, 5 figures
\\
  As machine learning models in critical fields increasingly grapple with
multimodal data, they face the dual challenges of handling a wide array of
modalities, often incomplete due to missing elements, and the temporal
irregularity and sparsity of collected samples. Successfully leveraging this
complex data, while overcoming the scarcity of high-quality training samples,
is key to improving these models' predictive performance. We introduce
``FuseMoE'', a mixture-of-experts framework incorporated with an innovative
gating function. Designed to integrate a diverse number of modalities, FuseMoE
is effective in managing scenarios with missing modalities and irregularly
sampled data trajectories. Theoretically, our unique gating function
contributes to enhanced convergence rates, leading to better performance in
multiple downstream tasks. The practical utility of FuseMoE in real world is
validated by a challenging set of clinical risk prediction tasks.
\\ ( https://arxiv.org/abs/2402.03226 ,  1036kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03232
Date: Mon, 5 Feb 2024 17:45:12 GMT   (13308kb,D)

Title: Smart Flow Matching: On The Theory of Flow Matching Algorithms with
  Applications
Authors: Gleb Ryzhakov, Svetlana Pavlova, Egor Sevriugov, Ivan Oseledets
Categories: cs.LG
\\
  The paper presents the exact formula for the vector field that minimizes the
loss for the standard flow. This formula depends analytically on a given
distribution \rho_0 and an unknown one \rho_1. Based on the presented formula,
a new loss and algorithm for training a vector field model in the style of
Conditional Flow Matching are provided. Our loss, in comparison to the standard
Conditional Flow Matching approach, exhibits smaller variance when evaluated
through Monte Carlo sampling methods. Numerical experiments on synthetic models
and models on tabular data of large dimensions demonstrate better learning
results with the use of the presented algorithm.
\\ ( https://arxiv.org/abs/2402.03232 ,  13308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03243
Date: Mon, 5 Feb 2024 17:58:17 GMT   (562kb,D)

Title: PINN-BO: A Black-box Optimization Algorithm using Physics-Informed
  Neural Networks
Authors: Dat Phan-Trong, Hung The Tran, Alistair Shilton, Sunil Gupta
Categories: cs.LG
\\
  Black-box optimization is a powerful approach for discovering global optima
in noisy and expensive black-box functions, a problem widely encountered in
real-world scenarios. Recently, there has been a growing interest in leveraging
domain knowledge to enhance the efficacy of machine learning methods. Partial
Differential Equations (PDEs) often provide an effective means for elucidating
the fundamental principles governing the black-box functions. In this paper, we
propose PINN-BO, a black-box optimization algorithm employing Physics-Informed
Neural Networks that integrates the knowledge from Partial Differential
Equations (PDEs) to improve the sample efficiency of the optimization. We
analyze the theoretical behavior of our algorithm in terms of regret bound
using advances in NTK theory and prove that the use of the PDE alongside the
black-box function evaluations, PINN-BO leads to a tighter regret bound. We
perform several experiments on a variety of optimization tasks and show that
our algorithm is more sample-efficient compared to existing methods.
\\ ( https://arxiv.org/abs/2402.03243 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03244
Date: Mon, 5 Feb 2024 17:59:00 GMT   (360kb,D)

Title: Skill Set Optimization: Reinforcing Language Model Behavior via
  Transferable Skills
Authors: Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra,
  Sameer Singh, Peter Clark, Roy Fox
Categories: cs.LG cs.CL
Comments: 8 pages, preprint
\\
  Large language models (LLMs) have recently been used for sequential decision
making in interactive environments. However, leveraging environment reward
signals for continual LLM actor improvement is not straightforward. We propose
Skill Set Optimization (SSO) for improving LLM actor performance through
constructing and refining sets of transferable skills. SSO constructs skills by
extracting common subtrajectories with high rewards and generating subgoals and
instructions to represent each skill. These skills are provided to the LLM
actor in-context to reinforce behaviors with high rewards. Then, SSO further
refines the skill set by pruning skills that do not continue to result in high
rewards. We evaluate our method in the classic videogame NetHack and the text
environment ScienceWorld to demonstrate SSO's ability to optimize a set of
skills and perform in-context policy improvement. SSO outperforms baselines by
40% in our custom NetHack task and outperforms the previous state-of-the-art in
ScienceWorld by 35%.
\\ ( https://arxiv.org/abs/2402.03244 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03252
Date: Mon, 5 Feb 2024 18:09:48 GMT   (8948kb,D)

Title: Fair Active Ranking from Pairwise Preferences
Authors: Sruthi Gorantla and Sara Ahmadian
Categories: cs.LG
Comments: 39 pages, 3.1 MB
\\
  We investigate the problem of probably approximately correct and fair (PACF)
ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$
items that belong to disjoint groups, our goal is to find an $(\epsilon,
\delta)$-PACF-Ranking according to a fair objective function that we propose.
We assume access to an oracle, wherein, for each query, the learner can choose
a pair of items and receive stochastic winner feedback from the oracle. Our
proposed objective function asks to minimize the $\ell_q$ norm of the error of
the groups, where the error of a group is the $\ell_p$ norm of the error of all
the items within that group, for $p, q \geq 1$. This generalizes the objective
function of $\epsilon$-Best-Ranking, proposed by Saha & Gopalan (2019).
  By adopting our objective function, we gain the flexibility to explore
fundamental fairness concepts like equal or proportionate errors within a
unified framework. Adjusting parameters $p$ and $q$ allows tailoring to
specific fairness preferences. We present both group-blind and group-aware
algorithms and analyze their sample complexity. We provide matching lower
bounds up to certain logarithmic factors for group-blind algorithms. For a
restricted class of group-aware algorithms, we show that we can get reasonable
lower bounds. We conduct comprehensive experiments on both real-world and
synthetic datasets to complement our theoretical findings.
\\ ( https://arxiv.org/abs/2402.03252 ,  8948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03256
Date: Mon, 5 Feb 2024 18:14:28 GMT   (90kb)

Title: Learning Best-in-Class Policies for the Predict-then-Optimize Framework
Authors: Michael Huang, Vishal Gupta
Categories: cs.LG math.OC stat.ML
\\
  We propose a novel family of decision-aware surrogate losses, called
Perturbation Gradient (PG) losses, for the predict-then-optimize framework.
These losses directly approximate the downstream decision loss and can be
optimized using off-the-shelf gradient-based methods. Importantly, unlike
existing surrogate losses, the approximation error of our PG losses vanishes as
the number of samples grows. This implies that optimizing our surrogate loss
yields a best-in-class policy asymptotically, even in misspecified settings.
This is the first such result in misspecified settings and we provide numerical
evidence confirming our PG losses substantively outperform existing proposals
when the underlying model is misspecified and the noise is not centrally
symmetric. Insofar as misspecification is commonplace in practice -- especially
when we might prefer a simpler, more interpretable model -- PG losses offer a
novel, theoretically justified, method for computationally tractable
decision-aware learning.
\\ ( https://arxiv.org/abs/2402.03256 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03264
Date: Mon, 5 Feb 2024 18:22:21 GMT   (5365kb,D)

Title: MobilityGPT: Enhanced Human Mobility Modeling with a GPT model
Authors: Ammar Haydari, Dongjie Chen, Zhengfeng Lai, Chen-Nee Chuah
Categories: cs.LG
\\
  Generative models have shown promising results in capturing human mobility
characteristics and generating synthetic trajectories. However, it remains
challenging to ensure that the generated geospatial mobility data is
semantically realistic, including consistent location sequences, and reflects
real-world characteristics, such as constraining on geospatial limits. To
address these issues, we reformat human mobility modeling as an autoregressive
generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure
its controllable generation to alleviate the above challenges, we propose a
geospatially-aware generative model, MobilityGPT. We propose a gravity-based
sampling method to train a transformer for semantic sequence similarity. Then,
we constrained the training process via a road connectivity matrix that
provides the connectivity of sequences in trajectory generation, thereby
keeping generated trajectories in geospatial limits. Lastly, we constructed a
Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel
distance between training and the synthetically generated trajectories. Our
experiments on real-world datasets demonstrate that MobilityGPT outperforms
state-of-the-art methods in generating high-quality mobility trajectories that
are closest to real data in terms of origin-destination similarity, trip
length, travel radius, link, and gravity distributions.
\\ ( https://arxiv.org/abs/2402.03264 ,  5365kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03268
Date: Mon, 5 Feb 2024 18:25:51 GMT   (13522kb,D)

Title: Understanding the Reasoning Ability of Language Models From the
  Perspective of Reasoning Paths Aggregation
Authors: Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen,
  William Yang Wang
Categories: cs.LG cs.AI cs.CL
\\
  Pre-trained language models (LMs) are able to perform complex reasoning
without explicit fine-tuning. To understand how pre-training with a next-token
prediction objective contributes to the emergence of such reasoning capability,
we propose that we can view an LM as deriving new conclusions by aggregating
indirect reasoning paths seen at pre-training time. We found this perspective
effective in two important cases of reasoning: logic reasoning with knowledge
graphs (KGs) and math reasoning with math word problems (MWPs). More
specifically, we formalize the reasoning paths as random walk paths on the
knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a
weighted sum of relevant random walk path probabilities is a reasonable way to
explain how LMs reason. Experiments and analysis on multiple KG and MWP
datasets reveal the effect of training on random walk paths and suggest that
augmenting unlabeled random walk reasoning paths can improve real-world
multi-step reasoning performance.
\\ ( https://arxiv.org/abs/2402.03268 ,  13522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03270
Date: Mon, 5 Feb 2024 18:27:46 GMT   (2097kb)

Title: Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT
  Protocol
Authors: Hector Alaiz-Moreton (1), Jose Aveleira-Mata (2), Jorge Ondicol-Garcia
  (2), Angel Luis Mu\~noz-Casta\~neda (2), Isa\'ias Garc\'ia (1) and Carmen
  Benavides (1) ((1) Escuela de Ingenier\'ias, Universidad de Le\'on, (2)
  Research Institute of Applied Sciences in Cybersecurity, Universidad de
  Le\'on)
Categories: cs.LG cs.CR
ACM-class: I.2.0
Journal-ref: Complexity (New York, N.Y.), 2019, Vol.2019, p.1-11
DOI: 10.1155/2019/6516253
\\
  The large number of sensors and actuators that make up the Internet of Things
obliges these systems to use diverse technologies and protocols. This means
that IoT networks are more heterogeneous than traditional networks. This gives
rise to new challenges in cybersecurity to protect these systems and devices
which are characterized by being connected continuously to the Internet.
Intrusion detection systems (IDS) are used to protect IoT systems from the
various anomalies and attacks at the network level. Intrusion Detection Systems
(IDS) can be improved through machine learning techniques. Our work focuses on
creating classification models that can feed an IDS using a dataset containing
frames under attacks of an IoT system that uses the MQTT protocol. We have
addressed two types of method for classifying the attacks, ensemble methods and
deep learning models, more specifically recurrent networks with very
satisfactory results.
\\ ( https://arxiv.org/abs/2402.03270 ,  2097kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03282
Date: Mon, 5 Feb 2024 18:38:55 GMT   (64kb)

Title: A Framework for Partially Observed Reward-States in RLHF
Authors: Chinmaya Kausik, Mirco Mutti, Aldo Pacchiano, Ambuj Tewari
Categories: cs.LG cs.AI stat.ML
Comments: 47 pages. 13 pages for the main paper, 34 pages for the references
  and appendix
\\
  The study of reinforcement learning from human feedback (RLHF) has gained
prominence in recent years due to its role in the development of LLMs.
Neuroscience research shows that human responses to stimuli are known to depend
on partially-observed "internal states." Unfortunately current models of RLHF
do not take take this into consideration. Moreover most RLHF models do not
account for intermediate feedback, which is gaining importance in empirical
work and can help improve both sample complexity and alignment. To address
these limitations, we model RLHF as reinforcement learning with partially
observed reward-states (PORRL). We show reductions from the the two dominant
forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For
cardinal feedback, we develop generic statistically efficient algorithms and
instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we
show that a naive reduction to cardinal feedback fails to achieve sublinear
dueling regret. We then present the first explicit reduction that converts
guarantees for cardinal regret to dueling regret. We show that our models and
guarantees in both settings generalize and extend existing ones. Finally, we
identify a recursive structure on our model that could improve the statistical
and computational tractability of PORRL, giving examples from past work on RLHF
as well as learning perfect reward machines, which PORRL subsumes.
\\ ( https://arxiv.org/abs/2402.03282 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03287
Date: Mon, 5 Feb 2024 18:43:05 GMT   (45052kb,D)

Title: A Lennard-Jones Layer for Distribution Normalization
Authors: Mulun Na and Jonathan Klein and Biao Zhang and Wojtek Pa{\l}ubicki and
  S\"oren Pirk and Dominik L. Michels
Categories: cs.LG physics.comp-ph
Comments: Upon request, we are happy to share the source code to generate the
  results presented in this paper. Please contact the first or the last author
  of this manuscript
MSC-class: 68T07
ACM-class: I.2; I.3.5
\\
  We introduce the Lennard-Jones layer (LJL) for the equalization of the
density of 2D and 3D point clouds through systematically rearranging points
without destroying their overall structure (distribution normalization). LJL
simulates a dissipative process of repulsive and weakly attractive interactions
between individual points by considering the nearest neighbor of each point at
a given moment in time. This pushes the particles into a potential valley,
reaching a well-defined stable configuration that approximates an equidistant
sampling after the stabilization process. We apply LJLs to redistribute
randomly generated point clouds into a randomized uniform distribution.
Moreover, LJLs are embedded in the generation process of point cloud networks
by adding them at later stages of the inference process. The improvements in 3D
point cloud generation utilizing LJLs are evaluated qualitatively and
quantitatively. Finally, we apply LJLs to improve the point distribution of a
score-based 3D point cloud denoising network. In general, we demonstrate that
LJLs are effective for distribution normalization which can be applied at
negligible cost without retraining the given neural network.
\\ ( https://arxiv.org/abs/2402.03287 ,  45052kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03289
Date: Mon, 5 Feb 2024 18:47:04 GMT   (388kb,D)

Title: Make Every Move Count: LLM-based High-Quality RTL Code Generation Using
  MCTS
Authors: Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja
  Thakur, Ramesh Karri, Siddharth Garg, Jeyavijayan Rajendran
Categories: cs.LG cs.AI cs.AR
\\
  Existing large language models (LLMs) for register transfer level code
generation face challenges like compilation failures and suboptimal power,
performance, and area (PPA) efficiency. This is due to the lack of PPA
awareness in conventional transformer decoding algorithms. In response, we
present an automated transformer decoding algorithm that integrates Monte Carlo
tree-search for lookahead, guiding the transformer to produce compilable,
functionally correct, and PPA-optimized code. Empirical evaluation with a
fine-tuned language model on RTL codesets shows that our proposed technique
consistently generates functionally correct code compared to prompting-only
methods and effectively addresses the PPA-unawareness drawback of naive large
language models. For the largest design generated by the state-of-the-art LLM
(16-bit adder), our technique can achieve a 31.8% improvement in the area-delay
product.
\\ ( https://arxiv.org/abs/2402.03289 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03292
Date: Mon, 5 Feb 2024 18:50:27 GMT   (38762kb,D)

Title: Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
Authors: Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian
  Q. Weinberger, Dung D. Le
Categories: cs.LG cs.CV
\\
  Machine learning algorithms are increasingly provided as black-box cloud
services or pre-trained models, without access to their training data. This
motivates the problem of zero-shot out-of-distribution (OOD) detection.
Concretely, we aim to detect OOD objects that do not belong to the classifier's
label set but are erroneously classified as in-distribution (ID) objects. Our
approach, RONIN, uses an off-the-shelf diffusion model to replace detected
objects with inpainting. RONIN conditions the inpainting process with the
predicted ID label, drawing the input object closer to the in-distribution
domain. As a result, the reconstructed object is very close to the original in
the ID cases and far in the OOD cases, allowing RONIN to effectively
distinguish ID and OOD samples. Throughout extensive experiments, we
demonstrate that RONIN achieves competitive results compared to previous
approaches across several datasets, both in zero-shot and non-zero-shot
settings.
\\ ( https://arxiv.org/abs/2402.03292 ,  38762kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03293
Date: Mon, 5 Feb 2024 18:50:39 GMT   (217kb,D)

Title: Flora: Low-Rank Adapters Are Secretly Gradient Compressors
Authors: Yongchang Hao, Yanshuai Cao, Lili Mou
Categories: cs.LG cs.AI stat.ML
\\
  Despite large neural networks demonstrating remarkable abilities to complete
different tasks, they require excessive memory usage to store the optimization
states for training. To alleviate this, the low-rank adaptation (LoRA) is
proposed to reduce the optimization states by training fewer parameters.
However, LoRA restricts overall weight update matrices to be low-rank, limiting
the model performance. In this work, we investigate the dynamics of LoRA and
identify that it can be approximated by a random projection. Based on this
observation, we propose Flora, which is able to achieve high-rank updates by
resampling the projection matrices while enjoying the sublinear space
complexity of optimization states. We conduct experiments across different
tasks and model architectures to verify the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2402.03293 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03295
Date: Mon, 5 Feb 2024 18:51:17 GMT   (28kb,D)

Title: Ginger: An Efficient Curvature Approximation with Linear Complexity for
  General Neural Networks
Authors: Yongchang Hao, Yanshuai Cao, Lili Mou
Categories: cs.LG cs.AI math.OC stat.ML
\\
  Second-order optimization approaches like the generalized Gauss-Newton method
are considered more powerful as they utilize the curvature information of the
objective function with preconditioning matrices. Albeit offering tempting
theoretical benefits, they are not easily applicable to modern deep learning.
The major reason is due to the quadratic memory and cubic time complexity to
compute the inverse of the matrix. These requirements are infeasible even with
state-of-the-art hardware. In this work, we propose Ginger, an
eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our
method enjoys efficient linear memory and time complexity for each iteration.
Instead of approximating the conditioning matrix, we directly maintain its
inverse to make the approximation more accurate. We provide the convergence
result of Ginger for non-convex objectives. Our experiments on different tasks
with different model architectures verify the effectiveness of our method. Our
code is publicly available.
\\ ( https://arxiv.org/abs/2402.03295 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03299
Date: Mon, 5 Feb 2024 18:54:43 GMT   (5101kb,D)

Title: GUARD: Role-playing to Generate Natural-language Jailbreakings to Test
  Guideline Adherence of Large Language Models
Authors: Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang
Categories: cs.LG cs.CL cs.CV
Comments: 22 papges
\\
  The discovery of "jailbreaks" to bypass safety filters of Large Language
Models (LLMs) and harmful responses have encouraged the community to implement
safety measures. One major safety measure is to proactively test the LLMs with
jailbreaks prior to the release. Therefore, such testing will require a method
that can generate jailbreaks massively and efficiently. In this paper, we
follow a novel yet intuitive strategy to generate jailbreaks in the style of
the human generation. We propose a role-playing system that assigns four
different roles to the user LLMs to collaborate on new jailbreaks. Furthermore,
we collect existing jailbreaks and split them into different independent
characteristics using clustering frequency and semantic patterns sentence by
sentence. We organize these characteristics into a knowledge graph, making them
more accessible and easier to retrieve. Our system of different roles will
leverage this knowledge graph to generate new jailbreaks, which have proved
effective in inducing LLMs to generate unethical or guideline-violating
responses. In addition, we also pioneer a setting in our system that will
automatically follow the government-issued guidelines to generate jailbreaks to
test whether LLMs follow the guidelines accordingly. We refer to our system as
GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have
empirically validated the effectiveness of GUARD on three cutting-edge
open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a
widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the
realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing
GUARD's versatility and contributing valuable insights for the development of
safer, more reliable LLM-based applications across diverse modalities.
\\ ( https://arxiv.org/abs/2402.03299 ,  5101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03305
Date: Mon, 5 Feb 2024 18:58:38 GMT   (1981kb,D)

Title: Do Diffusion Models Learn Semantically Meaningful and Efficient
  Representations?
Authors: Qiyao Liang, Ziming Liu, Ila Fiete
Categories: cs.LG cs.AI cs.CV
Comments: 13 pages, 9 figures
\\
  Diffusion models are capable of impressive feats of image generation with
uncommon juxtapositions such as astronauts riding horses on the moon with
properly placed shadows. These outputs indicate the ability to perform
compositional generalization, but how do the models do so? We perform
controlled experiments on conditional DDPMs learning to generate 2D spherical
Gaussian bumps centered at specified $x$- and $y$-positions. Our results show
that the emergence of semantically meaningful latent representations is key to
achieving high performance. En route to successful performance over learning,
the model traverses three distinct phases of latent representations: (phase A)
no latent structure, (phase B) a 2D manifold of disordered states, and (phase
C) a 2D ordered manifold. Corresponding to each of these phases, we identify
qualitatively different generation behaviors: 1) multiple bumps are generated,
2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is
generated at the correct $x$ and y location. Furthermore, we show that even
under imbalanced datasets where features ($x$- versus $y$-positions) are
represented with skewed frequencies, the learning process for $x$ and $y$ is
coupled rather than factorized, demonstrating that simple vanilla-flavored
diffusion models cannot learn efficient representations in which localization
in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest
the need for future work to find inductive biases that will push generative
models to discover and exploit factorizable independent structures in their
inputs, which will be required to vault these models into more data-efficient
regimes.
\\ ( https://arxiv.org/abs/2402.03305 ,  1981kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2304.05057 (*cross-listing*)
Date: Tue, 11 Apr 2023 08:28:56 GMT   (3193kb,D)

Title: SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge
  Distillation in Magnetic Resonance Image Reconstruction
Authors: Matcha Naga Gayathri, Sriprabha Ramanarayanan, Mohammad Al Fahim,
  Rahul G S, Keerthi Ram, Mohanasankar Sivaprakasam
Categories: eess.IV cs.AI cs.CV
Comments: 18 pages, 8 figures. Accepted for publication at MIDL 2023. Code for
  our proposed method is available at
  https://github.com/GayathriMatcha/SFT-KD-Recon
\\
  Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration
have shown remarkable success in providing high-quality reconstruction.
However, as the number of cascades increases, the improvements in
reconstruction tend to become marginal, indicating possible excess model
capacity. Knowledge distillation (KD) is an emerging technique to compress
these models, in which a trained deep teacher network is used to distill
knowledge to a smaller student network such that the student learns to mimic
the behavior of the teacher. Most KD methods focus on effectively training the
student with a pre-trained teacher unaware of the student model. We propose
SFT-KD-Recon, a student-friendly teacher training approach along with the
student as a prior step to KD to make the teacher aware of the structure and
capacity of the student and enable aligning the representations of the teacher
with the student. In SFT, the teacher is jointly trained with the unfolded
branch configurations of the student blocks using three loss terms -
teacher-reconstruction loss, student-reconstruction loss, and teacher-student
imitation loss, followed by KD of the student. We perform extensive experiments
for MRI acceleration in 4x and 5x under-sampling on the brain and cardiac
datasets on five KD methods using the proposed approach as a prior step. We
consider the DC-CNN architecture and setup teacher as D5C5 (141765 parameters),
and student as D3C5 (49285 parameters), denoting a compression of 2.87:1.
Results show that (i) our approach consistently improves the KD methods with
improved reconstruction performance and image quality, and (ii) the student
distilled using our approach is competitive with the teacher, with the
performance gap reduced from 0.53 dB to 0.03 dB.
\\ ( https://arxiv.org/abs/2304.05057 ,  3193kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15018 (*cross-listing*)
Date: Fri, 26 Jan 2024 17:19:59 GMT   (1038kb)

Title: Enhancement of a Text-Independent Speaker Verification System by using
  Feature Combination and Parallel-Structure Classifiers
Authors: Kerlos Atia Abdalmalak and Ascensi\'on Gallardo-Antol'in
Categories: eess.AS cs.AI cs.LG cs.SD
Journal-ref: Neural Computing and Applications 29 (2018) 637-651
DOI: 10.1007/s00521-016-2470-x
\\
  Speaker Verification (SV) systems involve mainly two individual stages:
feature extraction and classification. In this paper, we explore these two
modules with the aim of improving the performance of a speaker verification
system under noisy conditions. On the one hand, the choice of the most
appropriate acoustic features is a crucial factor for performing robust speaker
verification. The acoustic parameters used in the proposed system are: Mel
Frequency Cepstral Coefficients (MFCC), their first and second derivatives
(Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC),
Perceptual Linear Predictive (PLP), and Relative Spectral Transform -
Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison
of different combinations of the previous features is discussed. On the other
hand, the major weakness of a conventional Support Vector Machine (SVM)
classifier is the use of generic traditional kernel functions to compute the
distances among data points. However, the kernel function of an SVM has great
influence on its performance. In this work, we propose the combination of two
SVM-based classifiers with different kernel functions: Linear kernel and
Gaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR)
classifier. The combination is carried out by means of a parallel structure
approach, in which different voting rules to take the final decision are
considered. Results show that significant improvement in the performance of the
SV system is achieved by using the combined features with the combined
classifiers either with clean speech or in the presence of noise. Finally, to
enhance the system more in noisy environments, the inclusion of the multiband
noise removal technique as a preprocessing stage is proposed.
\\ ( https://arxiv.org/abs/2401.15018 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01647 (*cross-listing*)
Date: Sat, 6 Jan 2024 08:03:08 GMT   (11534kb,D)

Title: Build Your Own Robot Friend: An Open-Source Learning Module for
  Accessible and Engaging AI Education
Authors: Zhonghao Shi, Allison O'Connell, Zongjian Li, Siqi Liu, Jennifer
  Ayissi, Guy Hoffman, Mohammad Soleymani, Maja J. Matari\'c
Categories: cs.CY cs.AI cs.HC cs.LG cs.RO
Comments: Accepted to the Proceedings of the AAAI Conference on Artificial
  Intelligence (2024)
\\
  As artificial intelligence (AI) is playing an increasingly important role in
our society and global economy, AI education and literacy have become necessary
components in college and K-12 education to prepare students for an AI-powered
society. However, current AI curricula have not yet been made accessible and
engaging enough for students and schools from all socio-economic backgrounds
with different educational goals. In this work, we developed an open-source
learning module for college and high school students, which allows students to
build their own robot companion from the ground up. This open platform can be
used to provide hands-on experience and introductory knowledge about various
aspects of AI, including robotics, machine learning (ML), software engineering,
and mechanical engineering. Because of the social and personal nature of a
socially assistive robot companion, this module also puts a special emphasis on
human-centered AI, enabling students to develop a better understanding of
human-AI interaction and AI ethics through hands-on learning activities. With
open-source documentation, assembling manuals and affordable materials,
students from different socio-economic backgrounds can personalize their
learning experience based on their individual educational goals. To evaluate
the student-perceived quality of our module, we conducted a usability testing
workshop with 15 college students recruited from a minority-serving
institution. Our results indicate that our AI module is effective,
easy-to-follow, and engaging, and it increases student interest in studying
AI/ML and robotics in the future. We hope that this work will contribute toward
accessible and engaging AI education in human-AI interaction for college and
high school students.
\\ ( https://arxiv.org/abs/2402.01647 ,  11534kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01651 (*cross-listing*)
Date: Tue, 9 Jan 2024 14:57:30 GMT   (1220kb,D)

Title: Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM
  Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative
  Values
Authors: Jon Chun and Katherine Elkins
Categories: cs.CY cs.AI
Comments: 23 pages, 6 figures (3 as tables), 1 table (in LaTeX)
MSC-class: 68T27, 68T30, 68T37, 91F20, 93B52
ACM-class: I.2.7; K.4.1; I.2.11; I.2.0; K.6.5
\\
  With the rise of individual and collaborative networks of autonomous agents,
AI is deployed in more key reasoning and decision-making roles. For this
reason, ethics-based audits play a pivotal role in the rapidly growing fields
of AI safety and regulation. This paper undertakes an ethics-based audit to
probe the 8 leading commercial and open-source Large Language Models including
GPT-4. We assess explicability and trustworthiness by a) establishing how well
different models engage in moral reasoning and b) comparing normative values
underlying models as ethical frameworks. We employ an experimental,
evidence-based approach that challenges the models with ethical dilemmas in
order to probe human-AI alignment. The ethical scenarios are designed to
require a decision in which the particulars of the situation may or may not
necessitate deviating from normative ethical principles. A sophisticated
ethical framework was consistently elicited in one model, GPT-4. Nonetheless,
troubling findings include underlying normative frameworks with clear bias
towards particular cultural norms. Many models also exhibit disturbing
authoritarian tendencies. Code is available at
https://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.
\\ ( https://arxiv.org/abs/2402.01651 ,  1220kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01652 (*cross-listing*)
Date: Tue, 9 Jan 2024 17:22:04 GMT   (285kb,D)

Title: User-Centric AI Analytics for Chronic Health Conditions Management
Authors: Aladdin Ayesh
Categories: cs.CY cs.AI cs.HC cs.LG
Comments: Keynote talk at IEEE Conference on Intelligent Methods, Systems, and
  Applications (IMSA), Cairo, Egypt, July 2023
MSC-class: 68T09, 68T99
ACM-class: I.2; I.2.1; J.3
\\
  The use of AI analytics in health informatics has seen a rapid growth in
recent years. In this talk, we look at AI analytics use in managing chronic
health conditions such as diabetes, obesity, etc. We focus on the challenges in
managing these conditions especially with drug-free approaches due to the
variations in individual circumstances. These variations directed the research
into user-centric approach leading to variety of research questions. In this
short paper, we give examples from recent and current research work and
conclude with what, in our opinion, to be the next steps and some remaining
open research questions.
\\ ( https://arxiv.org/abs/2402.01652 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01654 (*cross-listing*)
Date: Wed, 10 Jan 2024 09:59:12 GMT   (537kb)

Title: A Scoping Review of Energy Load Disaggregation
Authors: Bal\'azs Andr\'as Tolnai and Zheng Ma and Bo N{\o}rregaard
  J{\o}rgensen
Categories: cs.CY cs.AI
Journal-ref: Progress in Artificial Intelligence. EPIA 2023. Lecture Notes in
  Computer Science, vol 14116
DOI: 10.1007/978-3-031-49011-8_17
\\
  Energy load disaggregation can contribute to balancing power grids by
enhancing the effectiveness of demand-side management and promoting
electricity-saving behavior through increased consumer awareness. However, the
field currently lacks a comprehensive overview. To address this gap, this paper
con-ducts a scoping review of load disaggregation domains, data types, and
methods, by assessing 72 full-text journal articles. The findings reveal that
domestic electricity consumption is the most researched area, while others,
such as industrial load disaggregation, are rarely discussed. The majority of
research uses relatively low-frequency data, sampled between 1 and 60 seconds.
A wide variety of methods are used, and artificial neural networks are the most
common, followed by optimization strategies, Hidden Markov Models, and Graph
Signal Processing approaches.
\\ ( https://arxiv.org/abs/2402.01654 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01655 (*cross-listing*)
Date: Wed, 10 Jan 2024 19:13:19 GMT   (407kb,D)

Title: A Deep Learning Approach Towards Student Performance Prediction in
  Online Courses: Challenges Based on a Global Perspective
Authors: Abdallah Moubayed, MohammadNoor Injadat, Nouh Alhindawi, Ghassan
  Samara, Sara Abuasal, Raed Alazaidah
Categories: cs.CY cs.AI cs.LG
Comments: Accepted and presented in 24th International Arab Conference on
  Information Technology (ACIT'2023)
\\
  Analyzing and evaluating students' progress in any learning environment is
stressful and time consuming if done using traditional analysis methods. This
is further exasperated by the increasing number of students due to the shift of
focus toward integrating the Internet technologies in education and the focus
of academic institutions on moving toward e-Learning, blended, or online
learning models. As a result, the topic of student performance prediction has
become a vibrant research area in recent years. To address this, machine
learning and data mining techniques have emerged as a viable solution. To that
end, this work proposes the use of deep learning techniques (CNN and RNN-LSTM)
to predict the students' performance at the midpoint stage of the online course
delivery using three distinct datasets collected from three different regions
of the world. Experimental results show that deep learning models have
promising performance as they outperform other optimized traditional ML models
in two of the three considered datasets while also having comparable
performance for the third dataset.
\\ ( https://arxiv.org/abs/2402.01655 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01656 (*cross-listing*)
Date: Wed, 10 Jan 2024 19:50:37 GMT   (738kb,D)

Title: Promises and pitfalls of artificial intelligence for legal applications
Authors: Sayash Kapoor, Peter Henderson, Arvind Narayanan
Categories: cs.CY cs.AI
\\
  Is AI set to redefine the legal profession? We argue that this claim is not
supported by the current evidence. We dive into AI's increasingly prevalent
roles in three types of legal tasks: information processing; tasks involving
creativity, reasoning, or judgment; and predictions about the future. We find
that the ease of evaluating legal applications varies greatly across legal
tasks, based on the ease of identifying correct answers and the observability
of information relevant to the task at hand. Tasks that would lead to the most
significant changes to the legal profession are also the ones most prone to
overoptimism about AI capabilities, as they are harder to evaluate. We make
recommendations for better evaluation and deployment of AI in legal contexts.
\\ ( https://arxiv.org/abs/2402.01656 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01659 (*cross-listing*)
Date: Fri, 12 Jan 2024 14:58:13 GMT   (597kb)

Title: Generative Artificial Intelligence in Higher Education: Evidence from an
  Analysis of Institutional Policies and Guidelines
Authors: Nora McDonald, Aditya Johri, Areej Ali, Aayushi Hingle
Categories: cs.CY cs.AI
\\
  The release of ChatGPT in November 2022 prompted a massive uptake of
generative artificial intelligence (GenAI) across higher education institutions
(HEIs). HEIs scrambled to respond to its use, especially by students, looking
first to regulate it and then arguing for its productive integration within
teaching and learning. In the year since the release, HEIs have increasingly
provided policies and guidelines to direct GenAI. In this paper we examined
documents produced by 116 US universities categorized as high research activity
or R1 institutions to comprehensively understand GenAI related advice and
guidance given to institutional stakeholders. Through an extensive analysis, we
found the majority of universities (N=73, 63%) encourage the use of GenAI and
many provide detailed guidance for its use in the classroom (N=48, 41%). More
than half of all institutions provided sample syllabi (N=65, 56%) and half
(N=58, 50%) provided sample GenAI curriculum and activities that would help
instructors integrate and leverage GenAI in their classroom. Notably, most
guidance for activities focused on writing, whereas code and STEM-related
activities were mentioned half the time and vaguely even when they were (N=58,
50%). Finally, more than one half of institutions talked about the ethics of
GenAI on a range of topics broadly, including Diversity, Equity and Inclusion
(DEI) (N=60, 52%). Overall, based on our findings we caution that guidance for
faculty can become burdensome as extensive revision of pedagogical approaches
is often recommended in the policies.
\\ ( https://arxiv.org/abs/2402.01659 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01662 (*cross-listing*)
Date: Sun, 14 Jan 2024 08:57:45 GMT   (603kb)

Title: Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives
Authors: Meredith Ringel Morris and Jed R. Brubaker
Categories: cs.CY cs.AI
\\
  As AI systems quickly improve in both breadth and depth of performance, they
lend themselves to creating increasingly powerful and realistic agents,
including the possibility of agents modeled on specific people. We anticipate
that within our lifetimes it may become common practice for people to create a
custom AI agent to interact with loved ones and/or the broader world after
death. We call these generative ghosts, since such agents will be capable of
generating novel content rather than merely parroting content produced by their
creator while living. In this paper, we first discuss the design space of
potential implementations of generative ghosts. We then discuss the practical
and ethical implications of generative ghosts, including potential positive and
negative impacts on individuals and society. Based on these considerations, we
lay out a research agenda for the AI and HCI research communities to empower
people to create and interact with AI afterlives in a safe and beneficial
manner.
\\ ( https://arxiv.org/abs/2402.01662 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01668 (*cross-listing*)
Date: Mon, 15 Jan 2024 20:26:09 GMT   (1184kb,D)

Title: Determining the Difficulties of Students With Dyslexia via Virtual
  Reality and Artificial Intelligence: An Exploratory Analysis
Authors: Enrique Yeguas-Bol\'ivar, Jos\'e M. Alcalde-Llergo, Pilar
  Aparicio-Mart\'inez, Juri Taborri, Andrea Zingoni and Sara Pinzi
Categories: cs.CY cs.AI cs.CV cs.GR cs.HC
Comments: 7 pages, 5 figures, 3 tables, MetroXRAINE 2022 Conference, VRAILEXIA
  european project
Journal-ref: 2022 IEEE International Conference on Metrology for Extended
  Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE), Rome,
  Italy, 2022, pp. 585-590
DOI: 10.1109/MetroXRAINE54828.2022.9967589
\\
  Learning disorders are neurological conditions that affect the brain's
ability to interconnect communication areas. Dyslexic students experience
problems with reading, memorizing, and exposing concepts; however the magnitude
of these can be mitigated through both therapies and the creation of
compensatory mechanisms. Several efforts have been made to mitigate these
issues, leading to the creation of digital resources for students with specific
learning disorders attending primary and secondary education levels.
Conversely, a standard approach is still missed in higher education. The
VRAIlexia project has been created to tackle this issue by proposing two
different tools: a mobile application integrating virtual reality (VR) to
collect data quickly and easily, and an artificial intelligencebased software
(AI) to analyze the collected data for customizing the supporting methodology
for each student. The first one has been created and is being distributed among
dyslexic students in Higher Education Institutions, for the conduction of
specific psychological and psychometric tests. The second tool applies specific
artificial intelligence algorithms to the data gathered via the application and
other surveys. These AI techniques have allowed us to identify the most
relevant difficulties faced by the students' cohort. Our different models have
obtained around 90\% mean accuracy for predicting the support tools and
learning strategies.
\\ ( https://arxiv.org/abs/2402.01668 ,  1184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01669 (*cross-listing*)
Date: Tue, 16 Jan 2024 13:41:00 GMT   (7819kb,D)

Title: Improved Performances and Motivation in Intelligent Tutoring Systems:
  Combining Machine Learning and Learner Choice
Authors: Benjamin Cl\'ement (1 adn 3), H\'el\`ene Sauz\'eon (1 and 2), Didier
  Roy (1), Pierre-Yves Oudeyer (1) ((1) Inria FLOWERS team Talence France, (2)
  Universit\'e de Bordeaux BPH lab Bordeaux France, (3) EvidenceB Paris France)
Categories: cs.CY cs.AI cs.LG
Comments: 29 pages, 37 figures
ACM-class: I.2.1; I.2.6
\\
  Large class sizes pose challenges to personalized learning in schools, which
educational technologies, especially intelligent tutoring systems (ITS), aim to
address. In this context, the ZPDES algorithm, based on the Learning Progress
Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences
exercises that maximize learning progress (LP). This algorithm was previously
shown in field studies to boost learning performances for a wider diversity of
students compared to a hand-designed curriculum. However, its motivational
impact was not assessed. Also, ZPDES did not allow students to express choices.
This limitation in agency is at odds with the LPH theory concerned with
modeling curiosity-driven learning. We here study how the introduction of such
choice possibilities impact both learning efficiency and motivation. The given
choice concerns dimensions that are orthogonal to exercise difficulty, acting
as a playful feature.
  In an extensive field study (265 7-8 years old children, RCT design), we
compare systems based either on ZPDES or a hand-designed curriculum, both with
and without self-choice. We first show that ZPDES improves learning performance
and produces a positive and motivating learning experience. We then show that
the addition of choice triggers intrinsic motivation and reinforces the
learning effectiveness of the LP-based personalization. In doing so, it
strengthens the links between intrinsic motivation and performance progress
during the serious game. Conversely, deleterious effects of the playful feature
are observed for hand-designed linear paths. Thus, the intrinsic motivation
elicited by a playful feature is beneficial only if the curriculum
personalization is effective for the learner. Such a result deserves great
attention due to increased use of playful features in non adaptive educational
technologies.
\\ ( https://arxiv.org/abs/2402.01669 ,  7819kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01672 (*cross-listing*)
Date: Thu, 18 Jan 2024 09:01:49 GMT   (1169kb,D)

Title: Prerequisite Structure Discovery in Intelligent Tutoring Systems
Authors: Louis Annabi (Flowers, U2IS), Sao Mai Nguyen
Categories: cs.CY cs.AI cs.LG
Journal-ref: 2023 IEEE International Conference on Development and Learning
  (ICDL), Nov 2023, Macau, China. pp.176-181
DOI: 10.1109/icdl55364.2023.10364416
\\
  This paper addresses the importance of Knowledge Structure (KS) and Knowledge
Tracing (KT) in improving the recommendation of educational content in
intelligent tutoring systems. The KS represents the relations between different
Knowledge Components (KCs), while KT predicts a learner's success based on her
past history. The contribution of this research includes proposing a KT model
that incorporates the KS as a learnable parameter, enabling the discovery of
the underlying KS from learner trajectories. The quality of the uncovered KS is
assessed by using it to recommend content and evaluating the recommendation
algorithm with simulated students.
\\ ( https://arxiv.org/abs/2402.01672 ,  1169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01673 (*cross-listing*)
Date: Thu, 18 Jan 2024 09:12:48 GMT   (864kb)

Title: Legal and ethical implications of applications based on agreement
  technologies: the case of auction-based road intersections
Authors: Jos\'e-Antonio Santos, Alberto Fern\'andez, Mar Moreno-Rebato, Holger
  Billhardt, Jos\'e-A. Rodr\'iguez-Garc\'ia, Sascha Ossowski
Categories: cs.CY cs.AI
ACM-class: I.2.1
Journal-ref: Artif. Intell. Law 28(4): 385-414 (2020)
DOI: 10.1007/s10506-019-09259-8
\\
  Agreement Technologies refer to a novel paradigm for the construction of
distributed intelligent systems, where autonomous software agents negotiate to
reach agreements on behalf of their human users. Smart Cities are a key
application domain for Agreement Technologies. While several proofs of concept
and prototypes exist, such systems are still far from ready for being deployed
in the real-world. In this paper we focus on a novel method for managing
elements of smart road infrastructures of the future, namely the case of
auction-based road intersections. We show that, even though the key
technological elements for such methods are already available, there are
multiple non-technical issues that need to be tackled before they can be
applied in practice. For this purpose, we analyse legal and ethical
implications of auction-based road intersections in the context of
international regulations and from the standpoint of the Spanish legislation.
From this exercise, we extract a set of required modifications, of both
technical and legal nature, which need to be addressed so as to pave the way
for the potential real-world deployment of such systems in a future that may
not be too far away.
\\ ( https://arxiv.org/abs/2402.01673 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01686 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:01:37 GMT   (173kb)

Title: A Systematic Mapping Study of Digital Twins for Diagnosis in
  Transportation
Authors: Liliana Marie Prikler, Franz Wotawa (Graz University of Technology,
  Institute for Software Technology)
Categories: cs.CY cs.AI cs.HC
Journal-ref: 2023 10th International Conference on Dependable Systems and Their
  Applications (DSA), Tokyo, Japan, 2023, pp. 431-442
DOI: 10.1109/DSA59317.2023.00058
\\
  In recent years, digital twins have been proposed and implemented in various
fields with potential applications ranging from prototyping to maintenance.
Going forward, they are to enable numerous efficient and sustainable
technologies, among them autonomous cars. However, despite a large body of
research in many fields, academics have yet to agree on what exactly a digital
twin is -- and as a result, what its capabilities and limitations might be. To
further our understanding, we explore the capabilities of digital twins
concerning diagnosis in the field of transportation. We conduct a systematic
mapping study including digital twins of vehicles and their components, as well
as transportation infrastructure. We discovered that few papers on digital
twins describe any diagnostic process. Furthermore, most existing approaches
appear limited to system monitoring or fault detection. These findings suggest
that we need more research for diagnostic reasoning utilizing digital twins.
\\ ( https://arxiv.org/abs/2402.01686 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01688 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:29:54 GMT   (580kb,D)

Title: An Online Hierarchical Energy Management System for Energy Communities,
  Complying with the Current Technical Legislation Framework
Authors: Antonino Capillo, Enrico De Santis, Fabio Massimo Frattale Mascioli,
  Antonello Rizzi
Categories: cs.CY cs.AI
Comments: 26 pages, 18 figures
\\
  Efforts in the fight against Climate Change are increasingly oriented towards
new energy efficiency strategies in Smart Grids (SGs). In 2018, with proper
legislation, the European Union (EU) defined the Renewable Energy Community
(REC) as a local electrical grid whose participants share their self-produced
renewable energy, aiming at reducing bill costs by taking advantage of proper
incentives. That action aspires to accelerate the spread of local renewable
energy exploitation, whose costs could not be within everyone's reach. Since a
REC is technically an SG, the strategies above can be applied, and
specifically, practical Energy Management Systems (EMSs) are required.
Therefore, in this work, an online Hierarchical EMS (HEMS) is synthesized for
REC cost minimization to evaluate its superiority over a local self-consumption
approach. EU technical indications (as inherited from Italy) are diligently
followed, aiming for results that are as realistic as possible. Power flows
between REC nodes, or Microgrids (MGs) are optimized by taking Energy Storage
Systems (ESSs) and PV plant costs, energy purchase costs, and REC incentives. A
hybrid Fuzzy Inference System - Genetic Algorithm (FIS-GA) model is implemented
with the GA encoding the FIS parameters. Power generation and consumption,
which are the overall system input, are predicted by a LSTM trained on
historical data. The proposed hierarchical model achieves good precision in
short computation times and outperforms the self-consumption approach, leading
to about 20% savings compared to the latter. In addition, the Explainable AI
(XAI), which characterizes the model through the FIS, makes results more
reliable thanks to an excellent human interpretation level. To finish, the HEMS
is parametrized so that it is straightforward to switch to another Country's
technical legislation framework.
\\ ( https://arxiv.org/abs/2402.01688 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01691 (*cross-listing*)
Date: Tue, 23 Jan 2024 20:53:53 GMT   (448kb)

Title: Investigating Algorithm Review Boards for Organizational Responsible
  Artificial Intelligence Governance
Authors: Emily Hadley, Alan Blatecky, and Megan Comfort
Categories: cs.CY cs.AI
\\
  Organizations including companies, nonprofits, governments, and academic
institutions are increasingly developing, deploying, and utilizing artificial
intelligence (AI) tools. Responsible AI (RAI) governance approaches at
organizations have emerged as important mechanisms to address potential AI
risks and harms. In this work, we interviewed 17 technical contributors across
organization types (Academic, Government, Industry, Nonprofit) and sectors
(Finance, Health, Tech, Other) about their experiences with internal RAI
governance. Our findings illuminated the variety of organizational definitions
of RAI and accompanying internal governance approaches. We summarized the first
detailed findings on algorithm review boards (ARBs) and similar review
committees in practice, including their membership, scope, and measures of
success. We confirmed known robust model governance in finance sectors and
revealed extensive algorithm and AI governance with ARB-like review boards in
health sectors. Our findings contradict the idea that Institutional Review
Boards alone are sufficient for algorithm governance and posit that ARBs are
among the more impactful internal RAI governance approaches. Our results
suggest that integration with existing internal regulatory approaches and
leadership buy-in are among the most important attributes for success and that
financial tensions are the greatest challenge to effective organizational RAI.
We make a variety of suggestions for how organizational partners can learn from
these findings when building their own internal RAI frameworks. We outline
future directions for developing and measuring effectiveness of ARBs and other
internal RAI governance approaches.
\\ ( https://arxiv.org/abs/2402.01691 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01703 (*cross-listing*)
Date: Wed, 24 Jan 2024 19:56:20 GMT   (286kb)

Title: A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver
  Interaction in Los Angeles
Authors: Benjamin A.T. Grahama, Lauren Brown, Georgios Chochlakis, Morteza
  Dehghani, Raquel Delerme, Brittany Friedman, Ellie Graeden, Preni Golazizian,
  Rajat Hebbar, Parsa Hejabi, Aditya Kommeneni, Mayag\"uez Salinas, Michael
  Sierra-Ar\'evalo, Jackson Trager, Nicholas Weller, and Shrikanth Narayan
Categories: cs.CY cs.AI cs.LG eess.AS
Comments: 13 pages
ACM-class: I.2.0; I.2.7
\\
  Interactions between the government officials and civilians affect public
wellbeing and the state legitimacy that is necessary for the functioning of
democratic society. Police officers, the most visible and contacted agents of
the state, interact with the public more than 20 million times a year during
traffic stops. Today, these interactions are regularly recorded by body-worn
cameras (BWCs), which are lauded as a means to enhance police accountability
and improve police-public interactions. However, the timely analysis of these
recordings is hampered by a lack of reliable automated tools that can enable
the analysis of these complex and contested police-public interactions. This
article proposes an approach to developing new multi-perspective, multimodal
machine learning (ML) tools to analyze the audio, video, and transcript
information from this BWC footage. Our approach begins by identifying the
aspects of communication most salient to different stakeholders, including both
community members and police officers. We move away from modeling approaches
built around the existence of a single ground truth and instead utilize new
advances in soft labeling to incorporate variation in how different observers
perceive the same interactions. We argue that this inclusive approach to the
conceptualization and design of new ML tools is broadly applicable to the study
of communication and development of analytic tools across domains of human
interaction, including education, medicine, and the workplace.
\\ ( https://arxiv.org/abs/2402.01703 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01705 (*cross-listing*)
Date: Thu, 25 Jan 2024 00:54:10 GMT   (151kb)

Title: Beyond Behaviorist Representational Harms: A Plan for Measurement and
  Mitigation
Authors: Jennifer Chien and David Danks
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 23 pages, 7 figures
\\
  Algorithmic harms are commonly categorized as either allocative or
representational. This study specifically addresses the latter, focusing on an
examination of current definitions of representational harms to discern what is
included and what is not. This analysis motivates our expansion beyond
behavioral definitions to encompass harms to cognitive and affective states.
The paper outlines high-level requirements for measurement: identifying the
necessary expertise to implement this approach and illustrating it through a
case study. Our work highlights the unique vulnerabilities of large language
models to perpetrating representational harms, particularly when these harms go
unmeasured and unmitigated. The work concludes by presenting proposed
mitigations and delineating when to employ them. The overarching aim of this
research is to establish a framework for broadening the definition of
representational harms and to translate insights from fairness research into
practical measurement and mitigation praxis.
\\ ( https://arxiv.org/abs/2402.01705 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01711 (*cross-listing*)
Date: Thu, 25 Jan 2024 17:45:34 GMT   (5925kb,D)

Title: LLM on FHIR -- Demystifying Health Records
Authors: Paul Schmiedmayer, Adrit Rao, Philipp Zagar, Vishnu Ravi, Aydin
  Zahedivash, Arash Fereydooni, Oliver Aalami
Categories: cs.CY cs.AI
Comments: Pre-print of the paper submitted to the Call for Papers for the
  Special Focus Issue on ChatGPT and Large Language Models (LLMs) in
  Biomedicine and Health at the Journal of the American Medical Informatics
  Association:
  https://academic.oup.com/jamia/pages/call-for-papers-for-special-focus-issue
\\
  Objective: To enhance health literacy and accessibility of health information
for a diverse patient population by developing a patient-centered artificial
intelligence (AI) solution using large language models (LLMs) and Fast
Healthcare Interoperability Resources (FHIR) application programming interfaces
(APIs). Materials and Methods: The research involved developing LLM on FHIR, an
open-source mobile application allowing users to interact with their health
records using LLMs. The app is built on Stanford's Spezi ecosystem and uses
OpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient
dataset and evaluated by medical experts to assess the app's effectiveness in
increasing health literacy. The evaluation focused on the accuracy, relevance,
and understandability of the LLM's responses to common patient questions.
Results: LLM on FHIR demonstrated varying but generally high degrees of
accuracy and relevance in providing understandable health information to
patients. The app effectively translated medical data into patient-friendly
language and was able to adapt its responses to different patient profiles.
However, challenges included variability in LLM responses and the need for
precise filtering of health data. Discussion and Conclusion: LLMs offer
significant potential in improving health literacy and making health records
more accessible. LLM on FHIR, as a pioneering application in this field,
demonstrates the feasibility and challenges of integrating LLMs into patient
care. While promising, the implementation and pilot also highlight risks such
as inconsistent responses and the importance of replicable output. Future
directions include better resource identification mechanisms and executing LLMs
on-device to enhance privacy and reduce costs.
\\ ( https://arxiv.org/abs/2402.01711 ,  5925kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01720 (*cross-listing*)
Date: Fri, 26 Jan 2024 18:37:21 GMT   (1172kb)

Title: Deep Learning Based Amharic Chatbot for FAQs in Universities
Authors: Goitom Ybrah Hailu, Shishay Welay
Categories: cs.CY cs.AI cs.CL cs.LG
\\
  University students often spend a considerable amount of time seeking answers
to common questions from administrators or teachers. This can become tedious
for both parties, leading to a need for a solution. In response, this paper
proposes a chatbot model that utilizes natural language processing and deep
learning techniques to answer frequently asked questions (FAQs) in the Amharic
language. Chatbots are computer programs that simulate human conversation
through the use of artificial intelligence (AI), acting as a virtual assistant
to handle questions and other tasks. The proposed chatbot program employs
tokenization, normalization, stop word removal, and stemming to analyze and
categorize Amharic input sentences. Three machine learning model algorithms
were used to classify tokens and retrieve appropriate responses: Support Vector
Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented
through TensorFlow, Keras, and NLTK. The deep learning model achieved the best
results with 91.55% accuracy and a validation loss of 0.3548 using an Adam
optimizer and SoftMax activation function. The chatbot model was integrated
with Facebook Messenger and deployed on a Heroku server for 24-hour
accessibility. The experimental results demonstrate that the chatbot framework
achieved its objectives and effectively addressed challenges such as Amharic
Fidel variation, morphological variation, and lexical gaps. Future research
could explore the integration of Amharic WordNet to narrow the lexical gap and
support more complex questions.
\\ ( https://arxiv.org/abs/2402.01720 ,  1172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01721 (*cross-listing*)
Date: Fri, 26 Jan 2024 21:51:49 GMT   (2490kb,D)

Title: Attitudes Towards and Knowledge of Non-Consensual Synthetic Intimate
  Imagery in 10 Countries
Authors: Rebecca Umbach, Nicola Henry, Gemma Beard, Colleen Berryessa
Categories: cs.CY cs.AI cs.HC
\\
  Deepfake technology tools have become ubiquitous, "democratizing" the ability
to manipulate images and videos. One popular use of such technology is the
creation of sexually explicit content, which can then be posted and shared
widely on the internet. This article examines attitudes and behaviors related
to non-consensual synthetic intimate imagery (NSII) across over 16,000
respondents in 10 countries. Despite nascent societal awareness of NSII, NSII
behaviors were considered harmful. In regards to prevalence, 2.2% of all
respondents indicated personal victimization, and 1.8% all of respondents
indicated perpetration behaviors. Respondents from countries with relevant
legislation also reported perpetration and victimization experiences,
suggesting legislative action alone is not a sufficient solution to deter
perpetration. Technical considerations to reduce harms may include suggestions
for how individuals can better monitor their presence online, as well as
enforced platform policies which ban, or allow for removal of, NSII content.
\\ ( https://arxiv.org/abs/2402.01721 ,  2490kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01727 (*cross-listing*)
Date: Sat, 27 Jan 2024 21:02:50 GMT   (1417kb)

Title: Prompting Diverse Ideas: Increasing AI Idea Variance
Authors: Lennart Meincke, Ethan R. Mollick, Christian Terwiesch
Categories: cs.CY cs.AI
\\
  Unlike routine tasks where consistency is prized, in creativity and
innovation the goal is to create a diverse set of ideas. This paper delves into
the burgeoning interest in employing Artificial Intelligence (AI) to enhance
the productivity and quality of the idea generation process. While previous
studies have found that the average quality of AI ideas is quite high, prior
research also has pointed to the inability of AI-based brainstorming to create
sufficient dispersion of ideas, which limits novelty and the quality of the
overall best idea. Our research investigates methods to increase the dispersion
in AI-generated ideas. Using GPT-4, we explore the effect of different
prompting methods on Cosine Similarity, the number of unique ideas, and the
speed with which the idea space gets exhausted. We do this in the domain of
developing a new product development for college students, priced under $50. In
this context, we find that (1) pools of ideas generated by GPT-4 with various
plausible prompts are less diverse than ideas generated by groups of human
subjects (2) the diversity of AI generated ideas can be substantially improved
using prompt engineering (3) Chain-of-Thought (CoT) prompting leads to the
highest diversity of ideas of all prompts we evaluated and was able to come
close to what is achieved by groups of human subjects. It also was capable of
generating the highest number of unique ideas of any prompt we studied.
\\ ( https://arxiv.org/abs/2402.01727 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01732 (*cross-listing*)
Date: Sun, 28 Jan 2024 17:04:59 GMT   (1004kb,D)

Title: Identifying and Improving Disability Bias in GAI-Based Resume Screening
Authors: Kate Glazko, Yusuf Mohammed, Ben Kosa, Venkatesh Potluri, Jennifer
  Mankoff
Categories: cs.CY cs.AI
\\
  As Generative AI rises in adoption, its use has expanded to include domains
such as hiring and recruiting. However, without examining the potential of
bias, this may negatively impact marginalized populations, including people
with disabilities. To address this important concern, we present a resume audit
study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against
the same resume enhanced with an additional leadership award, scholarship,
panel presentation, and membership that are disability related. We find that
GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this
prejudice can be quantifiably reduced by training a custom GPTs on principles
of DEI and disability justice. Our study also includes a unique qualitative
analysis of the types of direct and indirect ableism GPT-4 uses to justify its
biased decisions and suggest directions for additional bias mitigation work.
Additionally, since these justifications are presumably drawn from training
data containing real-world biased statements made by humans, our analysis
suggests additional avenues for understanding and addressing human bias.
\\ ( https://arxiv.org/abs/2402.01732 ,  1004kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01743 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:16:57 GMT   (1087kb)

Title: The Reasoning Under Uncertainty Trap: A Structural AI Risk
Authors: Toby D. Pilditch
Categories: cs.CY cs.AI
Comments: 51 pages (excluding references), 7 chapters, 9 figures
\\
  This report examines a novel risk associated with current (and projected) AI
tools. Making effective decisions about future actions requires us to reason
under uncertainty (RUU), and doing so is essential to many critical real world
problems. Overfaced by this challenge, there is growing demand for AI tools
like LLMs to assist decision-makers. Having evidenced this demand and the
incentives behind it, we expose a growing risk: we 1) do not currently
sufficiently understand LLM capabilities in this regard, and 2) have no
guarantees of performance given fundamental computational explosiveness and
deep uncertainty constraints on accuracy. This report provides an exposition of
what makes RUU so challenging for both humans and machines, and relates these
difficulties to prospective AI timelines and capabilities. Having established
this current potential misuse risk, we go on to expose how this seemingly
additive risk (more misuse additively contributed to potential harm) in fact
has multiplicative properties. Specifically, we detail how this misuse risk
connects to a wider network of underlying structural risks (e.g., shifting
incentives, limited transparency, and feedback loops) to produce non-linear
harms. We go on to provide a solutions roadmap that targets multiple leverage
points in the structure of the problem. This includes recommendations for all
involved actors (prospective users, developers, and policy-makers) and enfolds
insights from areas including Decision-making Under Deep Uncertainty and
complex systems theory. We argue this report serves not only to raise awareness
(and subsequently mitigate/correct) of a current, novel AI risk, but also
awareness of the underlying class of structural risks by illustrating how their
interconnected nature poses twin-dangers of camouflaging their presence, whilst
amplifying their potential effects.
\\ ( https://arxiv.org/abs/2402.01743 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01744 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:23:25 GMT   (2064kb,D)

Title: Unveiling Molecular Moieties through Hierarchical Graph Explainability
Authors: Paolo Sortino, Salvatore Contino, Ugo Perricone and Roberto Pirrone
Categories: q-bio.QM cs.AI cs.LG q-bio.MN
\\
  Background: Graph Neural Networks (GNN) have emerged in very recent years as
a powerful tool for supporting in silico Virtual Screening. In this work we
present a GNN which uses Graph Convolutional architectures to achieve very
accurate multi-target screening. We also devised a hierarchical Explainable
Artificial Intelligence (XAI) technique to catch information directly at atom,
ring, and whole molecule level by leveraging the message passing mechanism. In
this way, we find the most relevant moieties involved in bioactivity
prediction. Results: We report a state-of-the-art GNN classifier on twenty
Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms
previous SOTA approaches proposed by the authors. Moreover, a CDK1-only
high-sensitivity version of the GNN has been designed to use our explainer in
order to avoid the inherent bias of multi-class models. The hierarchical
explainer has been validated by an expert chemist on 19 approved drugs on CDK1.
Our explainer provided information in accordance to the docking analysis for 17
out of the 19 test drugs. Conclusion: Our approach is a valid support for
shortening both the screening and the hit-to-lead phase. Detailed knowledge
about the molecular substructures that play a role in the inhibitory action,
can help the computational chemist to gain insights into the pharmacophoric
function of the molecule also for repurposing purposes.
\\ ( https://arxiv.org/abs/2402.01744 ,  2064kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01746 (*cross-listing*)
Date: Mon, 29 Jan 2024 22:34:01 GMT   (1208kb,D)

Title: 3DG: A Framework for Using Generative AI for Handling Sparse Learner
  Performance Data From Intelligent Tutoring Systems
Authors: Liang Zhang, Jionghao Lin, Conrad Borchers, Meng Cao, Xiangen Hu
Categories: cs.CY cs.AI cs.LG
Journal-ref: LAK 2024: International Workshop on Generative AI for Learning
  Analytics (GenAI-LA)
\\
  Learning performance data (e.g., quiz scores and attempts) is significant for
understanding learner engagement and knowledge mastery level. However, the
learning performance data collected from Intelligent Tutoring Systems (ITSs)
often suffers from sparsity, impacting the accuracy of learner modeling and
knowledge assessments. To address this, we introduce the 3DG framework
(3-Dimensional tensor for Densification and Generation), a novel approach
combining tensor factorization with advanced generative models, including
Generative Adversarial Network (GAN) and Generative Pre-trained Transformer
(GPT), for enhanced data imputation and augmentation. The framework operates by
first representing the data as a three-dimensional tensor, capturing dimensions
of learners, questions, and attempts. It then densifies the data through tensor
factorization and augments it using Generative AI models, tailored to
individual learning patterns identified via clustering. Applied to data from an
AutoTutor lesson by the Center for the Study of Adult Literacy (CSAL), the 3DG
framework effectively generated scalable, personalized simulations of learning
performance. Comparative analysis revealed GAN's superior reliability over
GPT-4 in this context, underscoring its potential in addressing data sparsity
challenges in ITSs and contributing to the advancement of personalized
educational technology.
\\ ( https://arxiv.org/abs/2402.01746 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01748 (*cross-listing*)
Date: Tue, 30 Jan 2024 00:21:41 GMT   (1498kb,D)

Title: Large Multi-Modal Models (LMMs) as Universal Foundation Models for
  AI-Native Wireless Systems
Authors: Shengzhe Xu, Christo Kurisummoottil Thomas, Omar Hashash, Nikhil
  Muralidhar, Walid Saad, Naren Ramakrishnan
Categories: cs.NI cs.AI cs.CL cs.IT cs.LG math.IT
\\
  Large language models (LLMs) and foundation models have been recently touted
as a game-changer for 6G systems. However, recent efforts on LLMs for wireless
networks are limited to a direct application of existing language mod- els that
were designed for natural language processing (NLP) applications. To address
this challenge and create wireless-centric foundation models, this paper
presents a comprehensive vision on how to design universal foundation models
that are tailored towards the deployment of artificial intelligence (AI)-native
networks. Diverging from NLP-based foundation models, the proposed framework
promotes the design of large multi-modal models (LMMs) fostered by three key
capabilities: 1) processing of multi-modal sensing data, 2) grounding of
physical symbol representations in real-world wireless systems using causal
reasoning and retrieval-augmented generation (RAG), and 3) enabling
instructibility from the wireless environment feedback to facilitate dynamic
network adaptation thanks to logical and mathematical reasoning facilitated by
neuro-symbolic AI. In essence, these properties enable the pro- posed LMM
framework to build universal capabilities that cater to various cross-layer
networking tasks and alignment of intents across different domains. Preliminary
results from experimental evaluation demonstrate the efficacy of grounding
using RAG in LMMs, and showcase the alignment of LMMs with wireless system
designs. Furthermore, the enhanced rationale exhibited in the responses to
mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the
logical and mathematical reasoning capabilities inherent in LMMs. Building on
those results, we present a sequel of open questions and challenges for LMMs.
We then conclude with a set of recommendations that ignite the path towards
LMM-empowered AI-native systems.
\\ ( https://arxiv.org/abs/2402.01748 ,  1498kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01749 (*cross-listing*)
Date: Tue, 30 Jan 2024 04:48:16 GMT   (7252kb,D)

Title: Towards Urban General Intelligence: A Review and Outlook of Urban
  Foundation Models
Authors: Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, Hui Xiong
Categories: cs.CY cs.AI cs.LG
\\
  Machine learning techniques are now integral to the advancement of
intelligent urban services, playing a crucial role in elevating the efficiency,
sustainability, and livability of urban environments. The recent emergence of
foundation models such as ChatGPT marks a revolutionary shift in the fields of
machine learning and artificial intelligence. Their unparalleled capabilities
in contextual understanding, problem solving, and adaptability across a wide
range of tasks suggest that integrating these models into urban domains could
have a transformative impact on the development of smart cities. Despite
growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces
challenges such as a lack of clear definitions, systematic reviews, and
universalizable solutions. To this end, this paper first introduces the concept
of UFM and discusses the unique challenges involved in building them. We then
propose a data-centric taxonomy that categorizes current UFM-related works,
based on urban data modalities and types. Furthermore, to foster advancement in
this field, we present a promising framework aimed at the prospective
realization of UFMs, designed to overcome the identified challenges.
Additionally, we explore the application landscape of UFMs, detailing their
potential impact in various urban contexts. Relevant papers and open-source
resources have been collated and are continuously updated at
https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.
\\ ( https://arxiv.org/abs/2402.01749 ,  7252kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01752 (*cross-listing*)
Date: Tue, 30 Jan 2024 08:08:34 GMT   (325kb)

Title: Identifying False Content and Hate Speech in Sinhala YouTube Videos by
  Analyzing the Audio
Authors: W. A. K. M. Wickramaarachchi, Sameeri Sathsara Subasinghe, K. K.
  Rashani Tharushika Wijerathna, A. Sahashra Udani Athukorala, Lakmini
  Abeywardhana, A. Karunasena
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
\\
  YouTube faces a global crisis with the dissemination of false information and
hate speech. To counter these issues, YouTube has implemented strict rules
against uploading content that includes false information or promotes hate
speech. While numerous studies have been conducted to reduce offensive
English-language content, there's a significant lack of research on Sinhala
content. This study aims to address the aforementioned gap by proposing a
solution to minimize the spread of violence and misinformation in Sinhala
YouTube videos. The approach involves developing a rating system that assesses
whether a video contains false information by comparing the title and
description with the audio content and evaluating whether the video includes
hate speech. The methodology encompasses several steps, including audio
extraction using the Pytube library, audio transcription via the fine-tuned
Whisper model, hate speech detection employing the distilroberta-base model and
a text classification LSTM model, and text summarization through the fine-tuned
BART-Large- XSUM model. Notably, the Whisper model achieved a 48.99\% word
error rate, while the distilroberta-base model demonstrated an F1 score of
0.856 and a recall value of 0.861 in comparison to the LSTM model, which
exhibited signs of overfitting.
\\ ( https://arxiv.org/abs/2402.01752 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01758 (*cross-listing*)
Date: Tue, 30 Jan 2024 12:39:58 GMT   (179kb,D)

Title: Aalap: AI Assistant for Legal & Paralegal Functions in India
Authors: Aman Tiwari, Prathamesh Kalamkar, Atreyo Banerjee, Saurabh Karn, Varun
  Hemachandran and Smita Gupta
Categories: cs.CY cs.AI cs.CL
\\
  Using proprietary Large Language Models on legal tasks poses challenges due
to data privacy issues, domain data heterogeneity, domain knowledge
sophistication, and domain objectives uniqueness. We created Aalalp, a
fine-tuned Mistral 7B model on instructions data related to specific Indian
legal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\% of
our test data and obtains an equivalent score in 34\% of the test data as
evaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning
rather than legal recall. Aalap is definitely helpful for the day-to-day
activities of lawyers, judges, or anyone working in legal systems.
\\ ( https://arxiv.org/abs/2402.01758 ,  179kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01760 (*cross-listing*)
Date: Tue, 30 Jan 2024 16:33:21 GMT   (6017kb,D)

Title: Trust and ethical considerations in a multi-modal, explainable AI-driven
  chatbot tutoring system: The case of collaboratively solving Rubik's Cube
Authors: Kausik Lakkaraju, Vedant Khandelwal, Biplav Srivastava, Forest
  Agostinelli, Hengtao Tang, Prathamjeet Singh, Dezhi Wu, Matt Irvin, Ashish
  Kundu
Categories: cs.CY cs.AI
Comments: Accepted at 'Neural Conversational AI Workshop - What's left to TEACH
  (Trustworthy, Enhanced, Adaptable, Capable, and Human-centric) chatbots?' at
  ICML 2023
\\
  Artificial intelligence (AI) has the potential to transform education with
its power of uncovering insights from massive data about student learning
patterns. However, ethical and trustworthy concerns of AI have been raised but
are unsolved. Prominent ethical issues in high school AI education include data
privacy, information leakage, abusive language, and fairness. This paper
describes technological components that were built to address ethical and
trustworthy concerns in a multi-modal collaborative platform (called ALLURE
chatbot) for high school students to collaborate with AI to solve the Rubik's
cube. In data privacy, we want to ensure that the informed consent of children,
parents, and teachers, is at the center of any data that is managed. Since
children are involved, language, whether textual, audio, or visual, is
acceptable both from users and AI and the system can steer interaction away
from dangerous situations. In information management, we also want to ensure
that the system, while learning to improve over time, does not leak information
about users from one group to another.
\\ ( https://arxiv.org/abs/2402.01760 ,  6017kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01762 (*cross-listing*)
Date: Tue, 30 Jan 2024 18:09:45 GMT   (402kb)

Title: Commercial AI, Conflict, and Moral Responsibility: A theoretical
  analysis and practical approach to the moral responsibilities associated with
  dual-use AI technology
Authors: Daniel Trusilo and David Danks
Categories: cs.CY cs.AI
Comments: 9 pages
ACM-class: K.4.1; K.5.2
\\
  This paper presents a theoretical analysis and practical approach to the
moral responsibilities when developing AI systems for non-military applications
that may nonetheless be used for conflict applications. We argue that AI
represents a form of crossover technology that is different from previous
historical examples of dual- or multi-use technology as it has a multiplicative
effect across other technologies. As a result, existing analyses of ethical
responsibilities around dual-use technologies do not necessarily work for AI
systems. We instead argue that stakeholders involved in the AI system lifecycle
are morally responsible for uses of their systems that are reasonably
foreseeable. The core idea is that an agent's moral responsibility for some
action is not necessarily determined by their intentions alone; we must also
consider what the agent could reasonably have foreseen to be potential outcomes
of their action, such as the potential use of a system in conflict even when it
is not designed for that. In particular, we contend that it is reasonably
foreseeable that: (1) civilian AI systems will be applied to active conflict,
including conflict support activities, (2) the use of civilian AI systems in
conflict will impact applications of the law of armed conflict, and (3)
crossover AI technology will be applied to conflicts that fall short of armed
conflict. Given these reasonably foreseeably outcomes, we present three
technically feasible actions that developers of civilian AIs can take to
potentially mitigate their moral responsibility: (a) establishing systematic
approaches to multi-perspective capability testing, (b) integrating digital
watermarking in model weight matrices, and (c) utilizing monitoring and
reporting mechanisms for conflict-related AI applications.
\\ ( https://arxiv.org/abs/2402.01762 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01763 (*cross-listing*)
Date: Tue, 30 Jan 2024 23:35:28 GMT   (663kb,D)

Title: When Large Language Models Meet Vector Databases: A Survey
Authors: Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Chunjiang Liu, Haiyun Xu,
  Kehai Chen
Categories: cs.DB cs.AI cs.CL cs.LG
\\
  The recent burst in Large Language Models has opened new frontiers in
human-like text processing and generation. However, alongside their remarkable
growth, Large Language Models have encountered critical challenges including
issues of hallucination, bias, real-time knowledge updates, and the high costs
of implementation and maintenance in commercial settings. Vector Databases,
another increasingly popular tool, offer potential solutions to these
challenges. These databases are adept at handling high-dimensional data and are
crucial for tasks such as efficient information retrieval and semantic search.
By integrating with Large Language Models, they significantly enhance AI
systems' ability to manage and utilize diverse data more effectively. This
survey paper provides an in-depth and unique analysis of the intersection
between Large Language Models and Vector Databases.
\\ ( https://arxiv.org/abs/2402.01763 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01770 (*cross-listing*)
Date: Thu, 1 Feb 2024 05:49:54 GMT   (204kb,D)

Title: Extending Interactive Science Exhibits into the Classroom using
  Anthropomorphized Chatbots and Bloom's Taxonomy
Authors: Yousuf Golding
Categories: cs.CY cs.AI
\\
  This study explores the use of Generative AI chatbots for transforming public
science exhibits into virtual experiences that can extend the engagement of
exhibits into the classroom. The broader goal is to increase accessibility of
science exhibits, especially for those marginalized in STEM due to various
factors, including cultural barriers. We hypothesize that turning exhibits into
first-person anthropomorphized chatbots with a personality, like quirky-talking
asteroids or comets, can increase engagement and learning. The paper mainly
explores if such techniques are possible using Generative AI (e.g. GPT) via
prompt engineering alone. The research includes an investigation into the
possibility of integrating interactive assessment via question-generation using
Bloom's Taxonomy. Initial results indicate that it is possible to combine these
techniques. As such, it lays a foundation for future classroom evaluations of
such chatbots to gauge their overall efficacy in extending the reach of science
exhibitions. The paper concludes by discussing extensions of the research to
fully evaluate effectiveness in virtual field-trips. We also include a brief
examination of additional ways to enhance student motivation towards learning
via chatbots.
\\ ( https://arxiv.org/abs/2402.01770 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01782 (*cross-listing*)
Date: Thu, 1 Feb 2024 19:57:08 GMT   (368kb,D)

Title: Benchmarking Spiking Neural Network Learning Methods with Varying
  Locality
Authors: Jiaqi Lin, Sen Lu, Malyaban Bal, Abhronil Sengupta
Categories: cs.NE cs.AI cs.CV cs.LG
\\
  Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics,
have shown to achieve performance comparable to Artificial Neural Networks
(ANNs) in several machine learning tasks. Information is processed as spikes
within SNNs in an event-based mechanism that significantly reduces energy
consumption. However, training SNNs is challenging due to the
non-differentiable nature of the spiking mechanism. Traditional approaches,
such as Backpropagation Through Time (BPTT), have shown effectiveness but comes
with additional computational and memory costs and are biologically
implausible. In contrast, recent works propose alternative learning methods
with varying degrees of locality, demonstrating success in classification
tasks. In this work, we show that these methods share similarities during the
training process, while they present a trade-off between biological
plausibility and performance. Further, this research examines the implicitly
recurrent nature of SNNs and investigates the influence of addition of explicit
recurrence to SNNs. We experimentally prove that the addition of explicit
recurrent weights enhances the robustness of SNNs. We also investigate the
performance of local learning methods under gradient and non-gradient based
adversarial attacks.
\\ ( https://arxiv.org/abs/2402.01782 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01787 (*cross-listing*)
Date: Thu, 1 Feb 2024 23:12:57 GMT   (2018kb,D)

Title: Harm Amplification in Text-to-Image Models
Authors: Susan Hao, Renee Shelby, Yuchi Liu, Hansa Srinivasan, Mukul Bhutani,
  Burcu Karagol Ayan, Shivani Poddar, Sarah Laszlo
Categories: cs.CY cs.AI cs.LG
\\
  Text-to-image (T2I) models have emerged as a significant advancement in
generative AI; however, there exist safety concerns regarding their potential
to produce harmful image outputs even when users input seemingly safe prompts.
This phenomenon, where T2I models generate harmful representations that were
not explicit in the input, poses a potentially greater risk than adversarial
prompts, leaving users unintentionally exposed to harms. Our paper addresses
this issue by first introducing a formal definition for this phenomenon, termed
harm amplification. We further contribute to the field by developing
methodologies to quantify harm amplification in which we consider the harm of
the model output in the context of user input. We then empirically examine how
to apply these different methodologies to simulate real-world deployment
scenarios including a quantification of disparate impacts across genders
resulting from harm amplification. Together, our work aims to offer researchers
tools to comprehensively address safety challenges in T2I systems and
contribute to the responsible deployment of generative AI models.
\\ ( https://arxiv.org/abs/2402.01787 ,  2018kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01789 (*cross-listing*)
Date: Fri, 2 Feb 2024 02:43:10 GMT   (2098kb)

Title: The Political Preferences of LLMs
Authors: David Rozado
Categories: cs.CY cs.AI cs.CL
\\
  We report here a comprehensive analysis about the political preferences
embedded in Large Language Models (LLMs). Namely, we administer 11 political
orientation tests, designed to identify the political preferences of the test
taker, to 24 state-of-the-art conversational LLMs, both close and open source.
The results indicate that when probed with questions/statements with political
connotations most conversational LLMs tend to generate responses that are
diagnosed by most political test instruments as manifesting preferences for
left-of-center viewpoints. We note that this is not the case for base (i.e.
foundation) models upon which LLMs optimized for conversation with humans are
built. However, base models' suboptimal performance at coherently answering
questions suggests caution when interpreting their classification by political
orientation tests. Though not conclusive, our results provide preliminary
evidence for the intriguing hypothesis that the embedding of political
preferences into LLMs might be happening mostly post-pretraining. Namely,
during the supervised fine-tuning (SFT) and/or Reinforcement Learning (RL)
stages of the conversational LLMs training pipeline. We provide further support
for this hypothesis by showing that LLMs are easily steerable into target
locations of the political spectrum via SFT requiring only modest compute and
custom data, illustrating the ability of SFT to imprint political preferences
onto LLMs. As LLMs have started to displace more traditional information
sources such as search engines or Wikipedia, the implications of political
biases embedded in LLMs has important societal ramifications.
\\ ( https://arxiv.org/abs/2402.01789 ,  2098kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01791 (*cross-listing*)
Date: Fri, 2 Feb 2024 03:59:35 GMT   (620kb,D)

Title: Variational Quantum Circuits Enhanced Generative Adversarial Network
Authors: Runqiu Shu, Xusheng Xu, Man-Hong Yung, Wei Cui
Categories: quant-ph cs.AI cs.ET cs.LG
\\
  Generative adversarial network (GAN) is one of the widely-adopted
machine-learning frameworks for a wide range of applications such as generating
high-quality images, video, and audio contents. However, training a GAN could
become computationally expensive for large neural networks. In this work, we
propose a hybrid quantum-classical architecture for improving GAN (denoted as
QC-GAN). The performance was examed numerically by benchmarking with a
classical GAN using MindSpore Quantum on the task of hand-written image
generation. The generator of the QC-GAN consists of a quantum variational
circuit together with a one-layer neural network, and the discriminator
consists of a traditional neural network. Leveraging the entangling and
expressive power of quantum circuits, our hybrid architecture achieved better
performance (Frechet Inception Distance) than the classical GAN, with much
fewer training parameters and number of iterations for convergence. We have
also demonstrated the superiority of QC-GAN over an alternative quantum GAN,
namely pathGAN, which could hardly generate 16$\times$16 or larger images. This
work demonstrates the value of combining ideas from quantum computing with
machine learning for both areas of Quantum-for-AI and AI-for-Quantum.
\\ ( https://arxiv.org/abs/2402.01791 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01804 (*cross-listing*)
Date: Fri, 2 Feb 2024 08:37:06 GMT   (1251kb,D)

Title: Analysis of Internet of Things implementation barriers in the cold
  supply chain: an integrated ISM-MICMAC and DEMATEL approach
Authors: Kazrin Ahmad, Md. Saiful Islam, Md Abrar Jahin, and M. F. Mridha
Categories: cs.CY cs.AI
\\
  Integrating Internet of Things (IoT) technology inside the cold supply chain
can enhance transparency, efficiency, and quality, optimizing operating
procedures and increasing productivity. The integration of IoT in this
complicated setting is hindered by specific barriers that need a thorough
examination. Prominent barriers to IoT implementation in the cold supply chain
are identified using a two-stage model. After reviewing the available
literature on the topic of IoT implementation, a total of 13 barriers were
found. The survey data was cross-validated for quality, and Cronbach's alpha
test was employed to ensure validity. This research applies the interpretative
structural modeling technique in the first phase to identify the main barriers.
Among those barriers, "regularity compliance" and "cold chain networks" are key
drivers for IoT adoption strategies. MICMAC's driving and dependence power
element categorization helps evaluate the barrier interactions. In the second
phase of this research, a decision-making trial and evaluation laboratory
methodology was employed to identify causal relationships between barriers and
evaluate them according to their relative importance. Each cause is a potential
drive, and if its efficiency can be enhanced, the system as a whole benefits.
The research findings provide industry stakeholders, governments, and
organizations with significant drivers of IoT adoption to overcome these
barriers and optimize the utilization of IoT technology to improve the
effectiveness and reliability of the cold supply chain.
\\ ( https://arxiv.org/abs/2402.01804 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01832 (*cross-listing*)
Date: Fri, 2 Feb 2024 18:59:58 GMT   (1256kb,D)

Title: SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?
Authors: Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr,
  Adel Bibi, Bernard Ghanem
Categories: cs.CV cs.AI cs.LG
Comments: Under review
\\
  We present SynthCLIP, a novel framework for training CLIP models with
entirely synthetic text-image pairs, significantly departing from previous
methods relying on real data. Leveraging recent text-to-image (TTI) generative
networks and large language models (LLM), we are able to generate synthetic
datasets of images and corresponding captions at any scale, with no human
intervention. With training at scale, SynthCLIP achieves performance comparable
to CLIP models trained on real datasets. We also introduce SynthCI-30M, a
purely synthetic dataset comprising 30 million captioned images. Our code,
trained models, and generated data are released at
https://github.com/hammoudhasan/SynthCLIP
\\ ( https://arxiv.org/abs/2402.01832 ,  1256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01841 (*cross-listing*)
Date: Fri, 2 Feb 2024 19:01:52 GMT   (2993kb,D)

Title: COMET: Generating Commit Messages using Delta Graph Context
  Representation
Authors: Abhinav Reddy Mandli, Saurabhsingh Rajput, and Tushar Sharma
Categories: cs.SE cs.AI cs.CL
Comments: 22 Pages, 7 Figures
\\
  Commit messages explain code changes in a commit and facilitate collaboration
among developers. Several commit message generation approaches have been
proposed; however, they exhibit limited success in capturing the context of
code changes. We propose Comet (Context-Aware Commit Message Generation), a
novel approach that captures context of code changes using a graph-based
representation and leverages a transformer-based model to generate high-quality
commit messages. Our proposed method utilizes delta graph that we developed to
effectively represent code differences. We also introduce a customizable
quality assurance module to identify optimal messages, mitigating subjectivity
in commit messages. Experiments show that Comet outperforms state-of-the-art
techniques in terms of bleu-norm and meteor metrics while being comparable in
terms of rogue-l. Additionally, we compare the proposed approach with the
popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable GPT
model, over zero-shot, one-shot, and multi-shot settings. We found Comet
outperforming the GPT models, on five and four metrics respectively and provide
competitive results with the two other metrics. The study has implications for
researchers, tool developers, and software developers. Software developers may
utilize Comet to generate context-aware commit messages. Researchers and tool
developers can apply the proposed delta graph technique in similar contexts,
like code review summarization.
\\ ( https://arxiv.org/abs/2402.01841 ,  2993kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01864 (*cross-listing*)
Date: Fri, 2 Feb 2024 19:35:34 GMT   (1393kb,D)

Title: (A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible
  LLM Policies for Legal Advice
Authors: Inyoung Cheong, King Xia, K.J. Kevin Feng, Quan Ze Chen, Amy X. Zhang
Categories: cs.CY cs.AI
Comments: 14 pages
\\
  The rapid proliferation of large language models (LLMs) as general purpose
chatbots available to the public raises hopes around expanding access to
professional guidance in law, medicine, and finance, while triggering concerns
about public reliance on LLMs for high-stakes circumstances. Prior research has
speculated on high-level ethical considerations but lacks concrete criteria
determining when and why LLM chatbots should or should not provide professional
assistance. Through examining the legal domain, we contribute a structured
expert analysis to uncover nuanced policy considerations around using LLMs for
professional advice, using methods inspired by case-based reasoning. We
convened workshops with 20 legal experts and elicited dimensions on appropriate
AI assistance for sample user queries (``cases''). We categorized our expert
dimensions into: (1) user attributes, (2) query characteristics, (3) AI
capabilities, and (4) impacts. Beyond known issues like hallucinations, experts
revealed novel legal problems, including that users' conversations with LLMs
are not protected by attorney-client confidentiality or bound to professional
ethics that guard against conflicted counsel or poor quality advice. This
accountability deficit led participants to advocate for AI systems to help
users polish their legal questions and relevant facts, rather than recommend
specific actions. More generally, we highlight the potential of case-based
expert deliberation as a method of responsibly translating professional
integrity and domain knowledge into design requirements to inform appropriate
AI behavior when generating advice in professional domains.
\\ ( https://arxiv.org/abs/2402.01864 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01877 (*cross-listing*)
Date: Fri, 2 Feb 2024 20:05:45 GMT   (5172kb,D)

Title: Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models
Authors: Justin Blalock, David Munechika, Harsha Karanth, Alec Helbling,
  Pratham Mehta, Seongmin Lee, Duen Horng Chau
Categories: cs.HC cs.AI cs.LG
Comments: 7 pages, 3 figures
\\
  The growing digital landscape of fashion e-commerce calls for interactive and
user-friendly interfaces for virtually trying on clothes. Traditional try-on
methods grapple with challenges in adapting to diverse backgrounds, poses, and
subjects. While newer methods, utilizing the recent advances of diffusion
models, have achieved higher-quality image generation, the human-centered
dimensions of mobile interface delivery and privacy concerns remain largely
unexplored. We present Mobile Fitting Room, the first on-device diffusion-based
virtual try-on system. To address multiple inter-related technical challenges
such as high-quality garment placement and model compression for mobile
devices, we present a novel technical pipeline and an interface design that
enables privacy preservation and user customization. A usage scenario
highlights how our tool can provide a seamless, interactive virtual try-on
experience for customers and provide a valuable service for fashion e-commerce
businesses.
\\ ( https://arxiv.org/abs/2402.01877 ,  5172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01968 (*cross-listing*)
Date: Sat, 3 Feb 2024 00:27:22 GMT   (178kb,D)

Title: A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges
  and Future Directions
Authors: Hung Du, Srikanth Thudumu, Rajesh Vasa and Kon Mouzakis
Categories: cs.MA cs.AI cs.LG
Comments: 11 pages, 1 figure
\\
  Research interest in autonomous agents is on the rise as an emerging topic.
The notable achievements of Large Language Models (LLMs) have demonstrated the
considerable potential to attain human-like intelligence in autonomous agents.
However, the challenge lies in enabling these agents to learn, reason, and
navigate uncertainties in dynamic environments. Context awareness emerges as a
pivotal element in fortifying multi-agent systems when dealing with dynamic
situations. Despite existing research focusing on both context-aware systems
and multi-agent systems, there is a lack of comprehensive surveys outlining
techniques for integrating context-aware systems with multi-agent systems. To
address this gap, this survey provides a comprehensive overview of
state-of-the-art context-aware multi-agent systems. First, we outline the
properties of both context-aware systems and multi-agent systems that
facilitate integration between these systems. Subsequently, we propose a
general process for context-aware systems, with each phase of the process
encompassing diverse approaches drawn from various application domains such as
collision avoidance in autonomous driving, disaster relief management, utility
management, supply chain management, human-AI interaction, and others. Finally,
we discuss the existing challenges of context-aware multi-agent systems and
provide future research directions in this field.
\\ ( https://arxiv.org/abs/2402.01968 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01994 (*cross-listing*)
Date: Sat, 3 Feb 2024 02:32:45 GMT   (25kb)

Title: Human-Centered Privacy Research in the Age of Large Language Models
Authors: Tianshi Li, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao,
  Zhiping Zhang
Categories: cs.HC cs.AI cs.CR
Comments: 4 pages, CHI EA'24
\\
  The emergence of large language models (LLMs), and their increased use in
user-facing systems, has led to substantial privacy concerns. To date, research
on these privacy concerns has been model-centered: exploring how LLMs lead to
privacy risks like memorization, or can be used to infer personal
characteristics about people from their content. We argue that there is a need
for more research focusing on the human aspect of these privacy issues: e.g.,
research on how design paradigms for LLMs affect users' disclosure behaviors,
users' mental models and preferences for privacy controls, and the design of
tools, systems, and artifacts that empower end-users to reclaim ownership over
their personal data. To build usable, efficient, and privacy-friendly systems
powered by these models with imperfect privacy properties, our goal is to
initiate discussions to outline an agenda for conducting human-centered
research on privacy issues in LLM-powered systems. This Special Interest Group
(SIG) aims to bring together researchers with backgrounds in usable security
and privacy, human-AI collaboration, NLP, or any other related domains to share
their perspectives and experiences on this problem, to help our community
establish a collective understanding of the challenges, research opportunities,
research methods, and strategies to collaborate with researchers outside of
HCI.
\\ ( https://arxiv.org/abs/2402.01994 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02026 (*cross-listing*)
Date: Sat, 3 Feb 2024 04:47:03 GMT   (7843kb,D)

Title: Multimodal-Enhanced Objectness Learner for Corner Case Detection in
  Autonomous Driving
Authors: Lixing Xiao, Ruixiao Shi, Xiaoyang Tang, Yi Zhou
Categories: cs.CV cs.AI
Comments: 7 pages,6 figures
\\
  Previous works on object detection have achieved high accuracy in closed-set
scenarios, but their performance in open-world scenarios is not satisfactory.
One of the challenging open-world problems is corner case detection in
autonomous driving. Existing detectors struggle with these cases, relying
heavily on visual appearance and exhibiting poor generalization ability. In
this paper, we propose a solution by reducing the discrepancy between known and
unknown classes and introduce a multimodal-enhanced objectness notion learner.
Leveraging both vision-centric and image-text modalities, our semi-supervised
learning framework imparts objectness knowledge to the student model, enabling
class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner
(MENOL) for Corner Case Detection, significantly improves recall for novel
classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8%
mAR-agnostic on the CODA-val dataset with just 5100 labeled training images,
MENOL outperforms the baseline ORE by 71.3% and 60.6%, respectively. The code
will be available at https://github.com/tryhiseyyysum/MENOL.
\\ ( https://arxiv.org/abs/2402.02026 ,  7843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02029 (*cross-listing*)
Date: Sat, 3 Feb 2024 04:55:22 GMT   (5274kb,D)

Title: ScribFormer: Transformer Makes CNN Work Better for Scribble-based
  Medical Image Segmentation
Authors: Zihan Li, Yuan Zheng, Dandan Shan, Shuzhou Yang, Qingde Li, Beizhan
  Wang, Yuanting Zhang, Qingqi Hong, Dinggang Shen
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by IEEE Transactions on Medical Imaging (TMI)
\\
  Most recent scribble-supervised segmentation methods commonly adopt a CNN
framework with an encoder-decoder architecture. Despite its multiple benefits,
this framework generally can only capture small-range feature dependency for
the convolutional layer with the local receptive field, which makes it
difficult to learn global shape information from the limited information
provided by scribble annotations. To address this issue, this paper proposes a
new CNN-Transformer hybrid solution for scribble-supervised medical image
segmentation called ScribFormer. The proposed ScribFormer model has a
triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer
branch, and an attention-guided class activation map (ACAM) branch.
Specifically, the CNN branch collaborates with the Transformer branch to fuse
the local features learned from CNN with the global representations obtained
from Transformer, which can effectively overcome limitations of existing
scribble-supervised segmentation methods. Furthermore, the ACAM branch assists
in unifying the shallow convolution features and the deep convolution features
to improve model's performance further. Extensive experiments on two public
datasets and one private dataset show that our ScribFormer has superior
performance over the state-of-the-art scribble-supervised segmentation methods,
and achieves even better results than the fully-supervised segmentation
methods. The code is released at https://github.com/HUANGLIZI/ScribFormer.
\\ ( https://arxiv.org/abs/2402.02029 ,  5274kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02066 (*cross-listing*)
Date: Sat, 3 Feb 2024 07:14:33 GMT   (405kb,D)

Title: Trustworthiness of $\mathbb{X}$ Users: A One-Class Classification
  Approach
Authors: Tanveer Khan, Fahad Sohrab, Antonis Michalas, Moncef Gabbouj
Categories: cs.SI cs.AI
\\
  $\mathbb{X}$ (formerly Twitter) is a prominent online social media platform
that plays an important role in sharing information making the content
generated on this platform a valuable source of information. Ensuring trust on
$\mathbb{X}$ is essential to determine the user credibility and prevents issues
across various domains. While assigning credibility to $\mathbb{X}$ users and
classifying them as trusted or untrusted is commonly carried out using
traditional machine learning models, there is limited exploration about the use
of One-Class Classification (OCC) models for this purpose. In this study, we
use various OCC models for $\mathbb{X}$ user classification. Additionally, we
propose using a subspace-learning-based approach that simultaneously optimizes
both the subspace and data description for OCC. We also introduce a novel
regularization term for Subspace Support Vector Data Description (SSVDD),
expressing data concentration in a lower-dimensional subspace that captures
diverse graph structures. Experimental results show superior performance of the
introduced regularization term for SSVDD compared to baseline models and
state-of-the-art techniques for $\mathbb{X}$ user classification.
\\ ( https://arxiv.org/abs/2402.02066 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02079 (*cross-listing*)
Date: Sat, 3 Feb 2024 08:19:26 GMT   (359kb,D)

Title: Prototypical Contrastive Learning through Alignment and Uniformity for
  Recommendation
Authors: Yangxun Ou, Lei Chen, Fenglin Pan, Yupeng Wu
Categories: cs.IR cs.AI
\\
  Graph Collaborative Filtering (GCF), one of the most widely adopted
recommendation system methods, effectively captures intricate relationships
between user and item interactions. Graph Contrastive Learning (GCL) based GCF
has gained significant attention as it leverages self-supervised techniques to
extract valuable signals from real-world scenarios. However, many methods
usually learn the instances of discrimination tasks that involve the
construction of contrastive pairs through random sampling. GCL approaches
suffer from sampling bias issues, where the negatives might have a semantic
structure similar to that of the positives, thus leading to a loss of effective
feature representation. To address these problems, we present the
\underline{Proto}typical contrastive learning through \underline{A}lignment and
\underline{U}niformity for recommendation, which is called \textbf{ProtoAU}.
Specifically, we first propose prototypes (cluster centroids) as a latent space
to ensure consistency across different augmentations from the origin graph,
aiming to eliminate the need for random sampling of contrastive pairs.
Furthermore, the absence of explicit negatives means that directly optimizing
the consistency loss between instance and prototype could easily result in
dimensional collapse issues. Therefore, we propose aligning and maintaining
uniformity in the prototypes of users and items as optimization objectives to
prevent falling into trivial solutions. Finally, we conduct extensive
experiments on four datasets and evaluate their performance on the task of link
prediction. Experimental results demonstrate that the proposed ProtoAU
outperforms other representative methods. The source codes of our proposed
ProtoAU are available at \url{https://github.com/oceanlvr/ProtoAU}.
\\ ( https://arxiv.org/abs/2402.02079 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02085 (*cross-listing*)
Date: Sat, 3 Feb 2024 08:52:06 GMT   (46587kb,D)

Title: DeCoF: Generated Video Detection via Frame Consistency
Authors: Long Ma, Jiajia Zhang, Hongping Deng, Ningyu Zhang, Yong Liao, Haiyang
  Yu
Categories: cs.CV cs.AI
\\
  The escalating quality of video generated by advanced video generation
methods leads to new security challenges in society, which makes generated
video detection an urgent research priority.To foster collaborative research in
this area, we construct the first open-source dataset explicitly for generated
video detection, providing a valuable resource for the community to benchmark
and improve detection methodologies. Through a series of carefully designed
probe experiments, our study explores the significance of temporal and spatial
artifacts in developing general and robust detectors for generated video. Based
on the principle of video frame consistency, we introduce a simple yet
effective detection model (DeCoF) that eliminates the impact of spatial
artifacts during generalizing feature learning. Our extensive experiments
demonstrate the efficacy of DeCoF in detecting videos produced by unseen video
generation models and confirm its powerful generalization capabilities across
several commercial proprietary models.
\\ ( https://arxiv.org/abs/2402.02085 ,  46587kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02094 (*cross-listing*)
Date: Sat, 3 Feb 2024 09:18:49 GMT   (9770kb,D)

Title: Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene
  Classification
Authors: Wenjia Xu, Jiuniu Wang, Zhiwei Wei, Mugen Peng, Yirong Wu
Categories: cs.CV cs.AI
Comments: Published in ISPRS P&RS. The code is available at
  https://github.com/wenjiaXu/RS_Scene_ZSL
Journal-ref: Wenjia Xu, Jiuniu Wang, Zhiwei Wei, Mugen Peng, Yirong Wu, Deep
  Semantic-Visual Alignment for zero-shot remote sensing image scene
  classification, ISPRS Journal of Photogrammetry and Remote Sensing, Volume
  198, 2023, Pages 140-152
DOI: 10.1016/j.isprsjprs.2023.02.012
\\
  Deep neural networks have achieved promising progress in remote sensing (RS)
image classification, for which the training process requires abundant samples
for each class. However, it is time-consuming and unrealistic to annotate
labels for each RS category, given the fact that the RS target database is
increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel
classes that are not seen during training, which provides a promising solution
for the aforementioned problem. However, previous ZSL models mainly depend on
manually-labeled attributes or word embeddings extracted from language models
to transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL
models use convolutional neural networks pre-trained on ImageNet, which focus
on the main objects appearing in each image, neglecting the background context
that also matters in RS scene classification. To address the above problems, we
propose to collect visually detectable attributes automatically. We predict
attributes for each class by depicting the semantic-visual similarity between
attributes and images. In this way, the attribute annotation process is
accomplished by machine instead of human as in other methods. Moreover, we
propose a Deep Semantic-Visual Alignment (DSVA) that take advantage of the
self-attention mechanism in the transformer to associate local image regions
together, integrating the background context information for prediction. The
DSVA model further utilizes the attribute attention maps to focus on the
informative image regions that are essential for knowledge transfer in ZSL, and
maps the visual images into attribute space to perform ZSL classification. With
extensive experiments, we show that our model outperforms other
state-of-the-art models by a large margin on a challenging large-scale RS scene
classification benchmark.
\\ ( https://arxiv.org/abs/2402.02094 ,  9770kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02097 (*cross-listing*)
Date: Sat, 3 Feb 2024 09:35:25 GMT   (411kb,D)

Title: Settling Decentralized Multi-Agent Coordinated Exploration by Novelty
  Sharing
Authors: Haobin Jiang, Ziluo Ding, Zongqing Lu
Categories: cs.MA cs.AI cs.LG
Comments: 17 pages, 15 figures
\\
  Exploration in decentralized cooperative multi-agent reinforcement learning
faces two challenges. One is that the novelty of global states is unavailable,
while the novelty of local observations is biased. The other is how agents can
explore in a coordinated way. To address these challenges, we propose MACE, a
simple yet effective multi-agent coordinated exploration method. By
communicating only local novelty, agents can take into account other agents'
local novelty to approximate the global novelty. Further, we newly introduce
weighted mutual information to measure the influence of one agent's action on
other agents' accumulated novelty. We convert it as an intrinsic reward in
hindsight to encourage agents to exert more influence on other agents'
exploration and boost coordinated exploration. Empirically, we show that MACE
achieves superior performance in three multi-agent environments with sparse
rewards.
\\ ( https://arxiv.org/abs/2402.02097 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02150 (*cross-listing*)
Date: Sat, 3 Feb 2024 13:39:22 GMT   (17325kb,D)

Title: Data-Driven Prediction of Seismic Intensity Distributions Featuring
  Hybrid Classification-Regression Models
Authors: Koyu Mizutani, Haruki Mitarai, Kakeru Miyazaki, Soichiro Kumano,
  Toshihiko Yamasaki
Categories: cs.CV cs.AI
\\
  Earthquakes are among the most immediate and deadly natural disasters that
humans face. Accurately forecasting the extent of earthquake damage and
assessing potential risks can be instrumental in saving numerous lives. In this
study, we developed linear regression models capable of predicting seismic
intensity distributions based on earthquake parameters: location, depth, and
magnitude. Because it is completely data-driven, it can predict intensity
distributions without geographical information. The dataset comprises seismic
intensity data from earthquakes that occurred in the vicinity of Japan between
1997 and 2020, specifically containing 1,857 instances of earthquakes with a
magnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We
trained both regression and classification models and combined them to take
advantage of both to create a hybrid model. The proposed model outperformed
commonly used Ground Motion Prediction Equations (GMPEs) in terms of the
correlation coefficient, F1 score, and MCC. Furthermore, the proposed model can
predict even abnormal seismic intensity distributions, a task at conventional
GMPEs often struggle.
\\ ( https://arxiv.org/abs/2402.02150 ,  17325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02167 (*cross-listing*)
Date: Sat, 3 Feb 2024 14:28:55 GMT   (7222kb,D)

Title: Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting
  Generative AI-based Visualizations
Authors: Luca Podo, Muhammad Ishmal, Marco Angelini
Categories: cs.HC cs.AI cs.LG
\\
  The automatic generation of visualizations is an old task that, through the
years, has shown more and more interest from the research and practitioner
communities. Recently, large language models (LLM) have become an interesting
option for supporting generative tasks related to visualization, demonstrating
initial promising results. At the same time, several pitfalls, like the
multiple ways of instructing an LLM to generate the desired result, the
different perspectives leading the generation (code-based, image-based,
grammar-based), and the presence of hallucinations even for the visualization
generation task, make their usage less affordable than expected. Following
similar initiatives for benchmarking LLMs, this paper copes with the problem of
modeling the evaluation of a generated visualization through an LLM. We propose
a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort
in its atomic components, characterizes their nature, and provides an overview
of how to implement and interpret them. We also designed and implemented an
evaluation platform that provides a benchmarking resource for the visualization
generation task. The platform supports automatic and manual scoring conducted
by multiple assessors to support a fine-grained and semantic evaluation based
on the EvaLLM stack. Two case studies on GPT3.5-turbo with Code Interpreter and
Llama2-70-b models show the benefits of EvaLLM and illustrate interesting
results on the current state-of-the-art LLM-generated visualizations.
\\ ( https://arxiv.org/abs/2402.02167 ,  7222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02181 (*cross-listing*)
Date: Sat, 3 Feb 2024 15:11:19 GMT   (1108kb)

Title: An Ontology-Based multi-domain model in Social Network Analysis:
  Experimental validation and case study
Authors: Jos\'e Alberto Ben\'itez-Andrades, Isa\'ias Garc\'ia-Rodr\'iguez,
  Carmen Benavides, H\'ector Al\'aiz-Moret\'on and Jos\'e Emilio Labra Gayo
Categories: cs.SI cs.AI
Journal-ref: Information Sciences, Volume 540, November 2020, Pages 390-413
DOI: 10.1016/j.ins.2020.06.008
\\
  The use of social network theory and methods of analysis have been applied to
different domains in recent years, including public health. The complete
procedure for carrying out a social network analysis (SNA) is a time-consuming
task that entails a series of steps in which the expert in social network
analysis could make mistakes. This research presents a multi-domain knowledge
model capable of automatically gathering data and carrying out different social
network analyses in different domains, without errors and obtaining the same
conclusions that an expert in SNA would obtain. The model is represented in an
ontology called OntoSNAQA, which is made up of classes, properties and rules
representing the domains of People, Questionnaires and Social Network Analysis.
Besides the ontology itself, different rules are represented by SWRL and SPARQL
queries. A Knowledge Based System was created using OntoSNAQA and applied to a
real case study in order to show the advantages of the approach. Finally, the
results of an SNA analysis obtained through the model were compared to those
obtained from some of the most widely used SNA applications: UCINET, Pajek,
Cytoscape and Gephi, to test and confirm the validity of the model.
\\ ( https://arxiv.org/abs/2402.02181 ,  1108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02184 (*cross-listing*)
Date: Sat, 3 Feb 2024 15:26:28 GMT   (4449kb)

Title: Sentiment analysis in non-fixed length audios using a Fully
  Convolutional Neural Network
Authors: Mar\'ia Teresa Garc\'ia-Ord\'as, H\'ector Alaiz-Moret\'on, Jos\'e
  Alberto Ben\'itez-Andrades, Isa\'ias Garc\'ia-Rodr\'iguez, Oscar
  Garc\'ia-Olalla and Carmen Benavides
Categories: cs.SD cs.AI
Journal-ref: Biomedical Signal Processing and Control, Volume 69, August 2021,
  ID 102946
DOI: 10.1016/j.bspc.2021.102946
\\
  In this work, a sentiment analysis method that is capable of accepting audio
of any length, without being fixed a priori, is proposed. Mel spectrogram and
Mel Frequency Cepstral Coefficients are used as audio description methods and a
Fully Convolutional Neural Network architecture is proposed as a classifier.
The results have been validated using three well known datasets: EMODB,
RAVDESS, and TESS. The results obtained were promising, outperforming the
state-of-the-art methods. Also, thanks to the fact that the proposed method
admits audios of any size, it allows a sentiment analysis to be made in near
real time, which is very interesting for a wide range of fields such as call
centers, medical consultations, or financial brokers.
\\ ( https://arxiv.org/abs/2402.02184 ,  4449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02188 (*cross-listing*)
Date: Sat, 3 Feb 2024 15:30:20 GMT   (2493kb)

Title: Diabetes detection using deep learning techniques with oversampling and
  feature augmentation
Authors: Mar\'ia Teresa Garc\'ia-Ord\'as, Carmen Benavides, Jos\'e Alberto
  Ben\'itez-Andrades, H\'ector Alaiz-Moret\'on and Isa\'ias
  Garc\'ia-Rodr\'iguez
Categories: cs.CV cs.AI
Journal-ref: Computer Methods and Programs in Biomedicine, Volume 202, April
  2021, ID 105968
DOI: 10.1016/j.cmpb.2021.105968
\\
  Background and objective: Diabetes is a chronic pathology which is affecting
more and more people over the years. It gives rise to a large number of deaths
each year. Furthermore, many people living with the disease do not realize the
seriousness of their health status early enough. Late diagnosis brings about
numerous health problems and a large number of deaths each year so the
development of methods for the early diagnosis of this pathology is essential.
  Methods: In this paper, a pipeline based on deep learning techniques is
proposed to predict diabetic people. It includes data augmentation using a
variational autoencoder (VAE), feature augmentation using an sparse autoencoder
(SAE) and a convolutional neural network for classification. Pima Indians
Diabetes Database, which takes into account information on the patients such as
the number of pregnancies, glucose or insulin level, blood pressure or age, has
been evaluated.
  Results: A 92.31% of accuracy was obtained when CNN classifier is trained
jointly the SAE for featuring augmentation over a well balanced dataset. This
means an increment of 3.17% of accuracy with respect the state-of-the-art.
  Conclusions: Using a full deep learning pipeline for data preprocessing and
classification has demonstrate to be very promising in the diabetes detection
field outperforming the state-of-the-art proposals.
\\ ( https://arxiv.org/abs/2402.02188 ,  2493kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02218 (*cross-listing*)
Date: Sat, 3 Feb 2024 17:27:14 GMT   (784kb,D)

Title: Machine Intelligence in Africa: a survey
Authors: Allahsera Auguste Tapo and Ali Traore and Sidy Danioko and Hamidou
  Tembine
Categories: cs.CY cs.AI
Comments: Accepted and to be presented at DSAI 2024
\\
  In the last 5 years, the availability of large audio datasets in African
countries has opened unlimited opportunities to build machine intelligence (MI)
technologies that are closer to the people and speak, learn, understand, and do
businesses in local languages, including for those who cannot read and write.
Unfortunately, these audio datasets are not fully exploited by current MI
tools, leaving several Africans out of MI business opportunities. Additionally,
many state-of-the-art MI models are not culture-aware, and the ethics of their
adoption indexes are questionable. The lack thereof is a major drawback in many
applications in Africa. This paper summarizes recent developments in machine
intelligence in Africa from a multi-layer multiscale and culture-aware ethics
perspective, showcasing MI use cases in 54 African countries through 400
articles on MI research, industry, government actions, as well as uses in art,
music, the informal economy, and small businesses in Africa. The survey also
opens discussions on the reliability of MI rankings and indexes in the African
continent as well as algorithmic definitions of unclear terms used in MI.
\\ ( https://arxiv.org/abs/2402.02218 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02246 (*cross-listing*)
Date: Sat, 3 Feb 2024 19:24:45 GMT   (1010kb,D)

Title: ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from
  Invoice Images
Authors: Adem Akdo\u{g}an and Murat Kurt
Categories: cs.CV cs.AI cs.IR cs.LG cs.NE
Comments: 6 pages, 4 figures, 3 tables
\\
  In this work, product tables in invoices are obtained autonomously via a deep
learning model, which is named as ExTTNet. Firstly, text is obtained from
invoice images using Optical Character Recognition (OCR) techniques. Tesseract
OCR engine [37] is used for this process. Afterwards, the number of existing
features is increased by using feature extraction methods to increase the
accuracy. Labeling process is done according to whether each text obtained as a
result of OCR is a table element or not. In this study, a multilayer artificial
neural network model is used. The training has been carried out with an Nvidia
RTX 3090 graphics card and taken $162$ minutes. As a result of the training,
the F1 score is $0.92$.
\\ ( https://arxiv.org/abs/2402.02246 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02274 (*cross-listing*)
Date: Sat, 3 Feb 2024 22:04:22 GMT   (1578kb)

Title: InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention
  for medical image Classification
Authors: Elham Sadeghnezhad, Sajjad Salem
Categories: cs.CV cs.AI cs.LG
\\
  Initial weighting is significant in deep neural networks because the random
selection of weights produces different outputs and increases the probability
of overfitting and underfitting. On the other hand, vector-based approaches to
extract vector features need rich vectors for more accurate classification. The
InceptionCapsule approach is presented to alleviate these two problems. This
approach uses transfer learning and the Inception-ResNet model to avoid random
selection of weights, which takes initial weights from ImageNet. It also uses
the output of Inception middle layers to generate rich vectors. Extracted
vectors are given to a capsule network for learning, which is equipped with an
attention technique. Kvasir data and BUSI with the GT dataset were used to
evaluate this approach. This model was able to achieve 97.62 accuracies in
5-class classification and also achieved 94.30 accuracies in 8-class
classification on Kvasir. In the BUSI with GT dataset, the proposed approach
achieved accuracy=98.88, Precision=95.34, and F1-score=93.74, which are
acceptable results compared to other approaches in the literature.
\\ ( https://arxiv.org/abs/2402.02274 ,  1578kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02286 (*cross-listing*)
Date: Sat, 3 Feb 2024 22:51:17 GMT   (2148kb,D)

Title: Multi-Level Feature Aggregation and Recursive Alignment Network for
  Real-Time Semantic Segmentation
Authors: Yanhua Zhang, Ke Zhang, Jingyu Wang, Yulin Wu, Wuwei Wang
Categories: cs.CV cs.AI cs.LG
Comments: 14 pages, 9 figures. Manuscript completed on April 30, 2022, and then
  submitted to Transactions on Image Processing
\\
  Real-time semantic segmentation is a crucial research for real-world
applications. However, many methods lay particular emphasis on reducing the
computational complexity and model size, while largely sacrificing the
accuracy. In some scenarios, such as autonomous navigation and driver
assistance system, accuracy and speed are equally important. To tackle this
problem, we propose a novel Multi-level Feature Aggregation and Recursive
Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at
real-time inference speed. We employ ResNet-18 as the backbone to ensure
efficiency, and propose three core components to compensate for the reduced
model capacity due to the shallow backbone. Specifically, we first design
Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical
features in the encoder to each scale to benefit subsequent spatial alignment
and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by
combining the flow-based alignment module with recursive upsampling
architecture for accurate and efficient spatial alignment between multi-scale
score maps. Finally, the Adaptive Scores Fusion Module (ASFM) is proposed to
adaptively fuse multi-scale scores so that the final prediction can favor
objects of multiple scales. Comprehensive experiments on three benchmark
datasets including Cityscapes, CamVid and PASCAL-Context show the effectiveness
and efficiency of our method. In particular, we achieve a better balance
between speed and accuracy than state-of-the-art real-time methods on
Cityscapes and CamVid datasets. Code is available at:
https://github.com/Yanhua-Zhang/MFARANet.
\\ ( https://arxiv.org/abs/2402.02286 ,  2148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02339 (*cross-listing*)
Date: Sun, 4 Feb 2024 04:28:02 GMT   (4569kb,D)

Title: Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation
Authors: Ti Wang, Mengyuan Liu, Hong Liu, Bin Ren, Yingxuan You, Wenhao Li,
  Nicu Sebe, Xia Li
Categories: cs.CV cs.AI cs.LG
\\
  Although data-driven methods have achieved success in 3D human pose
estimation, they often suffer from domain gaps and exhibit limited
generalization. In contrast, optimization-based methods excel in fine-tuning
for specific cases but are generally inferior to data-driven methods in overall
performance. We observe that previous optimization-based methods commonly rely
on projection constraint, which only ensures alignment in 2D space, potentially
leading to the overfitting problem. To address this, we propose an
Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the
prior information of pre-trained model and alleviates the overfitting problem
using the uncertainty of joints. Specifically, during the training phase, we
design an effective 2D-to-3D network for estimating the corresponding 3D pose
while quantifying the uncertainty of each 3D joint. For optimization during
testing, the proposed optimization framework freezes the pre-trained model and
optimizes only a latent state. Projection loss is then employed to ensure the
generated poses are well aligned in 2D space for high-quality optimization.
Furthermore, we utilize the uncertainty of each joint to determine how much
each joint is allowed for optimization. The effectiveness and superiority of
the proposed framework are validated through extensive experiments on two
challenging datasets: Human3.6M and MPI-INF-3DHP. Notably, our approach
outperforms the previous best result by a large margin of 4.5% on Human3.6M.
Our source code will be open-sourced.
\\ ( https://arxiv.org/abs/2402.02339 ,  4569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02367 (*cross-listing*)
Date: Sun, 4 Feb 2024 06:39:01 GMT   (8103kb,D)

Title: Exploring Intrinsic Properties of Medical Images for Self-Supervised
  Binary Semantic Segmentation
Authors: Pranav Singh and Jacopo Cirrone
Categories: cs.CV cs.AI
Comments: 22 pages, 12 figures, and 10 tables
\\
  Recent advancements in self-supervised learning have unlocked the potential
to harness unlabeled data for auxiliary tasks, facilitating the learning of
beneficial priors. This has been particularly advantageous in fields like
medical image analysis, where labeled data are scarce. Although effective for
classification tasks, this methodology has shown limitations in more complex
applications, such as medical image segmentation. In this paper, we introduce
Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation
(MedSASS), a dedicated self-supervised framework tailored for medical image
segmentation. We evaluate MedSASS against existing state- of-the-art methods
across four diverse medical datasets, showcasing its superiority. MedSASS
outperforms existing CNN-based self-supervised methods by 3.83% and matches the
performance of ViT-based methods. Furthermore, when MedSASS is trained
end-to-end, covering both encoder and decoder, it demonstrates significant
improvements of 14.4% for CNNs and 6% for ViT-based architectures compared to
existing state-of-the-art self-supervised strategies.
\\ ( https://arxiv.org/abs/2402.02367 ,  8103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02381 (*cross-listing*)
Date: Sun, 4 Feb 2024 07:44:06 GMT   (5340kb,D)

Title: Empowering Computing and Networks Convergence System with Distributed
  Cooperative Routing
Authors: Yujiao Hu, Qingmin Jia, Meng Shen, Renchao Xie, Tao Huang, F.Richard
  Yu
Categories: cs.NI cs.AI
Comments: Submit to IEEE Network
\\
  The emergence of intelligent applications and recent advances in the fields
of computing and networks are driving the development of computing and networks
convergence (CNC) system. However, existing researches failed to achieve
comprehensive scheduling optimization of computing and network resources. This
shortfall results in some requirements of computing requests unable to be
guaranteed in an end-to-end service pattern, negatively impacting the
development of CNC systems. In this article, we propose a distributed
cooperative routing framework for the CNC system to ensure the deadline
requirements and minimize the computation cost of requests. The framework
includes trading plane, management plane, control plane and forwarding plane.
The cross-plane cooperative end-to-end routing schemes consider both
computation efficiency of heterogeneous servers and the network congestion
degrees while making routing plan, thereby determining where to execute
requests and corresponding routing paths. Simulations results substantiates the
performance of our routing schemes in scheduling computing requests in the CNC
system.
\\ ( https://arxiv.org/abs/2402.02381 ,  5340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02498 (*cross-listing*)
Date: Sun, 4 Feb 2024 14:12:51 GMT   (373kb,D)

Title: Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to
  CT Image Fusion
Authors: Minheng Chen, Zhirun Zhang, Shuheng Gu, Zhangyang Ge and Youyong Kong
Categories: eess.IV cs.AI cs.CV
Comments: ISBI 2024
\\
  Image-based rigid 2D/3D registration is a critical technique for fluoroscopic
guided surgical interventions. In recent years, some learning-based fully
differentiable methods have produced beneficial outcomes while the process of
feature extraction and gradient flow transmission still lack controllability
and interpretability. To alleviate these problems, in this work, we propose a
novel fully differentiable correlation-driven network using a dual-branch
CNN-transformer encoder which enables the network to extract and separate
low-frequency global features from high-frequency local features. A
correlation-driven loss is further proposed for low-frequency feature and
high-frequency feature decomposition based on embedded information. Besides, a
training strategy that learns to approximate a convex-shape similarity function
is applied in our work. We test our approach on a in-house datasetand show that
it outperforms both existing fully differentiable learning-based registration
approaches and the conventional optimization-based baseline.
\\ ( https://arxiv.org/abs/2402.02498 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02500 (*cross-listing*)
Date: Sun, 4 Feb 2024 14:18:45 GMT   (6635kb,D)

Title: Point Cloud Matters: Rethinking the Impact of Different Observation
  Spaces on Robot Learning
Authors: Haoyi Zhu and Yating Wang and Di Huang and Weicai Ye and Wanli Ouyang
  and Tong He
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  In this study, we explore the influence of different observation spaces on
robot learning, focusing on three predominant modalities: RGB, RGB-D, and point
cloud. Through extensive experimentation on over 17 varied contact-rich
manipulation tasks, conducted across two benchmarks and simulators, we have
observed a notable trend: point cloud-based methods, even those with the
simplest designs, frequently surpass their RGB and RGB-D counterparts in
performance. This remains consistent in both scenarios: training from scratch
and utilizing pretraining. Furthermore, our findings indicate that point cloud
observations lead to improved policy zero-shot generalization in relation to
various geometry and visual clues, including camera viewpoints, lighting
conditions, noise levels and background appearance. The outcomes suggest that
3D point cloud is a valuable observation modality for intricate robotic tasks.
We will open-source all our codes and checkpoints, hoping that our insights can
help design more generalizable and robust robotic models.
\\ ( https://arxiv.org/abs/2402.02500 ,  6635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02519 (*cross-listing*)
Date: Sun, 4 Feb 2024 15:07:49 GMT   (2231kb,D)

Title: SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for
  Autonomous Driving
Authors: Lu Zhang, Peiliang Li, Sikang Liu, Shaojie Shen
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Code is available at https://github.com/HKUST-Aerial-Robotics/SIMPL
\\
  This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL)
for autonomous vehicles. Unlike conventional agent-centric methods with high
accuracy but repetitive computations and scene-centric methods with compromised
accuracy and generalizability, SIMPL delivers real-time, accurate motion
predictions for all relevant traffic participants. To achieve improvements in
both accuracy and inference speed, we propose a compact and efficient global
feature fusion module that performs directed message passing in a symmetric
manner, enabling the network to forecast future motion for all road users in a
single feed-forward pass and mitigating accuracy loss caused by viewpoint
shifting. Additionally, we investigate the continuous trajectory
parameterization using Bernstein basis polynomials in trajectory decoding,
allowing evaluations of states and their higher-order derivatives at any
desired time point, which is valuable for downstream planning tasks. As a
strong baseline, SIMPL exhibits highly competitive performance on Argoverse 1 &
2 motion forecasting benchmarks compared with other state-of-the-art methods.
Furthermore, its lightweight design and low inference latency make SIMPL highly
extensible and promising for real-world onboard deployment. We open-source the
code at https://github.com/HKUST-Aerial-Robotics/SIMPL.
\\ ( https://arxiv.org/abs/2402.02519 ,  2231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02544 (*cross-listing*)
Date: Sun, 4 Feb 2024 15:46:43 GMT   (18432kb,D)

Title: LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal
  Language Model
Authors: Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao
Categories: cs.CV cs.AI cs.LG
Comments: 32 pages, 8 figures. Github https://github.com/NJU-LHRS/LHRS-Bot
\\
  The revolutionary capabilities of large language models (LLMs) have paved the
way for multimodal large language models (MLLMs) and fostered diverse
applications across various specialized domains. In the remote sensing (RS)
field, however, the diverse geographical landscapes and varied objects in RS
imagery are not adequately considered in recent MLLM endeavors. To bridge this
gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an
informative RS-specific instruction dataset, LHRS-Instruct, leveraging the
extensive volunteered geographic information (VGI) and globally available RS
images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored
for RS image understanding through a novel multi-level vision-language
alignment strategy and a curriculum learning method. Comprehensive experiments
demonstrate that LHRS-Bot exhibits a profound understanding of RS images and
the ability to perform nuanced reasoning within the RS domain.
\\ ( https://arxiv.org/abs/2402.02544 ,  18432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02552 (*cross-listing*)
Date: Sun, 4 Feb 2024 15:54:37 GMT   (657kb,D)

Title: Neur2BiLO: Neural Bilevel Optimization
Authors: Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias B. Khalil
Categories: math.OC cs.AI cs.LG
\\
  Bilevel optimization deals with nested problems in which a leader takes the
first decision to minimize their objective function while accounting for a
follower's best-response reaction. Constrained bilevel problems with integer
variables are particularly notorious for their hardness. While exact solvers
have been proposed for mixed-integer linear bilevel optimization, they tend to
scale poorly with problem size and are hard to generalize to the non-linear
case. On the other hand, problem-specific algorithms (exact and heuristic) are
limited in scope. Under a data-driven setting in which similar instances of a
bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds
a neural network approximation of the leader's or follower's value function,
trained via supervised regression, into an easy-to-solve mixed-integer program.
Neur2BiLO serves as a heuristic that produces high-quality solutions extremely
fast for the bilevel knapsack interdiction problem, the "critical node game"
from network security, a donor-recipient healthcare problem, and discrete
network design from transportation planning. These problems are diverse in that
they have linear or non-linear objectives/constraints and integer or
mixed-integer variables, making Neur2BiLO unique in its versatility.
\\ ( https://arxiv.org/abs/2402.02552 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02600 (*cross-listing*)
Date: Sun, 4 Feb 2024 20:23:15 GMT   (1421kb,D)

Title: Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep
  Reinforcement Learning Approach
Authors: Brian Etter, James Lee Hu, Mohammedreza Ebrahimi, Weifeng Li, Xin Li,
  Hsinchun Chen
Categories: cs.CR cs.AI cs.LG
\\
  Adversarial Malware Generation (AMG), the gen- eration of adversarial malware
variants to strengthen Deep Learning (DL)-based malware detectors has emerged
as a crucial tool in the development of proactive cyberdefense. However, the
majority of extant works offer subtle perturbations or additions to executable
files and do not explore full-file obfuscation. In this study, we show that an
open-source encryption tool coupled with a Reinforcement Learning (RL)
framework can successfully obfuscate malware to evade state-of-the-art malware
detection engines and outperform techniques that use advanced modification
methods. Our results show that the proposed method improves the evasion rate
from 27%-49% compared to widely- used state-of-the-art reinforcement
learning-based methods.
\\ ( https://arxiv.org/abs/2402.02600 ,  1421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02623 (*cross-listing*)
Date: Sun, 4 Feb 2024 21:54:25 GMT   (397kb)

Title: Efficient Market Dynamics: Unraveling Informational Efficiency in UK
  Horse Racing Betting Markets Through Betfair's Time Series Analysis
Authors: Narayan Tondapu
Categories: cs.CE cs.AI
\\
  Using Betfair's time series data, an analysis of the United Kingdom (UK)
horse racing market reveals an interesting paradox: a market with short tails,
rapidly decaying autocorrelations, and no long-term memory. There seems to be a
remarkably high level of informational efficiency in betting exchange returns,
in contrast to financial assets that are characterized by heavy tails and
volatility clustering. The generalized Gaussian unconditional distribution with
a light tail point to a market where knowledge is quickly assimilated and
reflected in prices. This is further supported by the extremely quick fading of
autocorrelations and the absence of gain-loss asymmetry. Therefore, in addition
to measuring long-range memory, the Hurst exponent also shows mean reversion, a
sign that markets respond quickly to fresh information.
\\ ( https://arxiv.org/abs/2402.02623 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02624 (*cross-listing*)
Date: Sun, 4 Feb 2024 22:09:28 GMT   (2299kb,D)

Title: A Safe Reinforcement Learning driven Weights-varying Model Predictive
  Control for Autonomous Vehicle Motion Control
Authors: Baha Zarrouki, Marios Spanakakis and Johannes Betz
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
\\
  Determining the optimal cost function parameters of Model Predictive Control
(MPC) to optimize multiple control objectives is a challenging and
time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve
this problem by determining a Pareto optimal parameter set for an MPC with
static weights. However, a single parameter set may not deliver the most
optimal closed-loop control performance when the context of the MPC operating
conditions changes during its operation, urging the need to adapt the cost
function weights at runtime. Deep Reinforcement Learning (RL) algorithms can
automatically learn context-dependent optimal parameter sets and dynamically
adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights
from scratch in a continuous action space may lead to unsafe operating states.
To solve this, we propose a novel approach limiting the RL actions within a
safe learning space representing a catalog of pre-optimized BO Pareto-optimal
weight sets. We conceive a RL agent not to learn in a continuous space but to
proactively anticipate upcoming control tasks and to choose the most optimal
discrete actions, each corresponding to a single set of Pareto optimal weights,
context-dependent. Hence, even an untrained RL agent guarantees a safe and
optimal performance. Experimental results demonstrate that an untrained RL-WMPC
shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps
exhibit a performance beyond the Pareto-front.
\\ ( https://arxiv.org/abs/2402.02624 ,  2299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02643 (*cross-listing*)
Date: Sun, 4 Feb 2024 23:42:02 GMT   (24558kb,D)

Title: LLM-Enhanced Data Management
Authors: Xuanhe Zhou, Xinyang Zhao, Guoliang Li
Categories: cs.DB cs.AI cs.CL cs.LG
\\
  Machine learning (ML) techniques for optimizing data management problems have
been extensively studied and widely deployed in recent five years. However
traditional ML methods have limitations on generalizability (adapting to
different scenarios) and inference ability (understanding the context).
Fortunately, large language models (LLMs) have shown high generalizability and
human-competitive abilities in understanding context, which are promising for
data management tasks (e.g., database diagnosis, database tuning). However,
existing LLMs have several limitations: hallucination, high cost, and low
accuracy for complicated tasks. To address these challenges, we design LLMDB,
an LLM-enhanced data management paradigm which has generalizability and high
inference ability while avoiding hallucination, reducing LLM cost, and
achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid
hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high
cost of LLMs by vector databases which provide semantic search and caching
abilities. LLMDB improves the task accuracy by LLM agent which provides
multiple-round inference and pipeline executions. We showcase three real-world
scenarios that LLMDB can well support, including query rewrite, database
diagnosis and data analytics. We also summarize the open research challenges of
LLMDB.
\\ ( https://arxiv.org/abs/2402.02643 ,  24558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02718 (*cross-listing*)
Date: Mon, 5 Feb 2024 04:28:08 GMT   (2188kb,D)

Title: Denoising Time Cycle Modeling for Recommendation
Authors: Sicong Xie, Qunwei Li, Weidi Xu, Kaiming Shen, Shaohu Chen, Wenliang
  Zhong
Categories: cs.IR cs.AI
\\
  Recently, modeling temporal patterns of user-item interactions have attracted
much attention in recommender systems. We argue that existing methods ignore
the variety of temporal patterns of user behaviors. We define the subset of
user behaviors that are irrelevant to the target item as noises, which limits
the performance of target-related time cycle modeling and affect the
recommendation performance. In this paper, we propose Denoising Time Cycle
Modeling (DiCycle), a novel approach to denoise user behaviors and select the
subset of user behaviors that are highly related to the target item. DiCycle is
able to explicitly model diverse time cycle patterns for recommendation.
Extensive experiments are conducted on both public benchmarks and a real-world
dataset, demonstrating the superior performance of DiCycle over the
state-of-the-art recommendation methods.
\\ ( https://arxiv.org/abs/2402.02718 ,  2188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02733 (*cross-listing*)
Date: Mon, 5 Feb 2024 05:25:33 GMT   (12463kb,D)

Title: ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
Authors: Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
Comments: 8 pages, 9 figures, 1 table
\\
  Face re-aging is a prominent field in computer vision and graphics, with
significant applications in photorealistic domains such as movies, advertising,
and live streaming. Recently, the need to apply face re-aging to
non-photorealistic images, like comics, illustrations, and animations, has
emerged as an extension in various entertainment sectors. However, the absence
of a network capable of seamlessly editing the apparent age on NPR images means
that these tasks have been confined to a naive approach, applying each task
sequentially. This often results in unpleasant artifacts and a loss of facial
attributes due to domain discrepancies. In this paper, we introduce a novel
one-stage method for face re-aging combined with portrait style transfer,
executed in a single generative step. We leverage existing face re-aging and
style transfer networks, both trained within the same PR domain. Our method
uniquely fuses distinct latent vectors, each responsible for managing
aging-related attributes and NPR appearance. Adopting an exemplar-based
approach, our method offers greater flexibility than domain-level fine-tuning
approaches, which typically require separate training or fine-tuning for each
domain. This effectively addresses the limitation of requiring paired datasets
for re-aging and domain-level, data-driven approaches for stylization. Our
experiments show that our model can effortlessly generate re-aged images while
simultaneously transferring the style of examples, maintaining both natural
appearance and controllability.
\\ ( https://arxiv.org/abs/2402.02733 ,  12463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02764 (*cross-listing*)
Date: Mon, 5 Feb 2024 06:52:53 GMT   (4316kb,D)

Title: List-aware Reranking-Truncation Joint Model for Search and
  Retrieval-augmented Generation
Authors: Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, Xueqi Cheng
Categories: cs.IR cs.AI cs.CL
Comments: Accepted by WWW 2024
\\
  The results of information retrieval (IR) are usually presented in the form
of a ranked list of candidate documents, such as web search for humans and
retrieval-augmented generation for large language models (LLMs). List-aware
retrieval aims to capture the list-level contextual features to return a better
list, mainly including reranking and truncation. Reranking finely re-scores the
documents in the list. Truncation dynamically determines the cut-off point of
the ranked list to achieve the trade-off between overall relevance and avoiding
misinformation from irrelevant documents. Previous studies treat them as two
separate tasks and model them separately. However, the separation is not
optimal. First, it is hard to share the contextual information of the ranking
list between the two tasks. Second, the separate pipeline usually meets the
error accumulation problem, where the small error from the reranking stage can
largely affect the truncation stage. To solve these problems, we propose a
Reranking-Truncation joint model (GenRT) that can perform the two tasks
concurrently. GenRT integrates reranking and truncation via generative paradigm
based on encoder-decoder architecture. We also design the novel loss functions
for joint optimization to make the model learn both tasks. Sharing parameters
by the joint model is conducive to making full use of the common modeling
information of the two tasks. Besides, the two tasks are performed concurrently
and co-optimized to solve the error accumulation problem between separate
stages. Experiments on public learning-to-rank benchmarks and open-domain Q\&A
tasks show that our method achieves SOTA performance on both reranking and
truncation tasks for web search and retrieval-augmented LLMs.
\\ ( https://arxiv.org/abs/2402.02764 ,  4316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02768 (*cross-listing*)
Date: Mon, 5 Feb 2024 07:02:43 GMT   (1065kb)

Title: Intent Profiling and Translation Through Emergent Communication
Authors: Salwa Mostafa, Mohammed S. Elbamby, Mohamed K. Abdel-Aziz, and Mehdi
  Bennis
Categories: cs.IT cs.AI cs.LG math.IT
Journal-ref: IEEE International Conference on Communications (ICC2024)
\\
  To effectively express and satisfy network application requirements,
intent-based network management has emerged as a promising solution. In
intent-based methods, users and applications express their intent in a
high-level abstract language to the network. Although this abstraction
simplifies network operation, it induces many challenges to efficiently express
applications' intents and map them to different network capabilities.
Therefore, in this work, we propose an AI-based framework for intent profiling
and translation. We consider a scenario where applications interacting with the
network express their needs for network services in their domain language. The
machine-to-machine communication (i.e., between applications and the network)
is complex since it requires networks to learn how to understand the domain
languages of each application, which is neither practical nor scalable.
Instead, a framework based on emergent communication is proposed for intent
profiling, in which applications express their abstract quality-of-experience
(QoE) intents to the network through emergent communication messages.
Subsequently, the network learns how to interpret these communication messages
and map them to network capabilities (i.e., slices) to guarantee the requested
Quality-of-Service (QoS). Simulation results show that the proposed method
outperforms self-learning slicing and other baselines, and achieves a
performance close to the perfect knowledge baseline.
\\ ( https://arxiv.org/abs/2402.02768 ,  1065kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02781 (*cross-listing*)
Date: Mon, 5 Feb 2024 07:30:32 GMT   (174kb,D)

Title: Dual Knowledge Distillation for Efficient Sound Event Detection
Authors: Yang Xiao, Rohan Kumar Das
Categories: cs.SD cs.AI cs.CL cs.LG
Comments: Accepted to ICASSP 2024 (Deep Neural Network Model Compression
  Workshop)
\\
  Sound event detection (SED) is essential for recognizing specific sounds and
their temporal locations within acoustic signals. This becomes challenging
particularly for on-device applications, where computational resources are
limited. To address this issue, we introduce a novel framework referred to as
dual knowledge distillation for developing efficient SED systems in this work.
Our proposed dual knowledge distillation commences with temporal-averaging
knowledge distillation (TAKD), utilizing a mean student model derived from the
temporal averaging of the student model's parameters. This allows the student
model to indirectly learn from a pre-trained teacher model, ensuring a stable
knowledge distillation. Subsequently, we introduce embedding-enhanced feature
distillation (EEFD), which involves incorporating an embedding distillation
layer within the student model to bolster contextual learning. On DCASE 2023
Task 4A public evaluation dataset, our proposed SED system with dual knowledge
distillation having merely one-third of the baseline model's parameters,
demonstrates superior performance in terms of PSDS1 and PSDS2. This highlights
the importance of proposed dual knowledge distillation for compact SED systems,
which can be ideal for edge devices.
\\ ( https://arxiv.org/abs/2402.02781 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02803 (*cross-listing*)
Date: Mon, 5 Feb 2024 08:25:22 GMT   (954kb,D)

Title: Large Language Model Distilling Medication Recommendation Model
Authors: Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Zijian Zhang, Feng
  Tian and Yefeng Zheng
Categories: cs.IR cs.AI cs.LG
\\
  The recommendation of medication is a vital aspect of intelligent healthcare
systems, as it involves prescribing the most suitable drugs based on a
patient's specific health needs. Unfortunately, many sophisticated models
currently in use tend to overlook the nuanced semantics of medical data, while
only relying heavily on identities. Furthermore, these models face significant
challenges in handling cases involving patients who are visiting the hospital
for the first time, as they lack prior prescription histories to draw upon. To
tackle these issues, we harness the powerful semantic comprehension and
input-agnostic characteristics of Large Language Models (LLMs). Our research
aims to transform existing medication recommendation methodologies using LLMs.
In this paper, we introduce a novel approach called Large Language Model
Distilling Medication Recommendation (LEADER). We begin by creating appropriate
prompt templates that enable LLMs to suggest medications effectively. However,
the straightforward integration of LLMs into recommender systems leads to an
out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a
novel output layer and a refined tuning loss function. Although LLM-based
models exhibit remarkable capabilities, they are plagued by high computational
costs during inference, which is impractical for the healthcare sector. To
mitigate this, we have developed a feature-level knowledge distillation
technique, which transfers the LLM's proficiency to a more compact model.
Extensive experiments conducted on two real-world datasets, MIMIC-III and
MIMIC-IV, demonstrate that our proposed model not only delivers effective
results but also is efficient. To ease the reproducibility of our experiments,
we release the implementation code online.
\\ ( https://arxiv.org/abs/2402.02803 ,  954kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02826 (*cross-listing*)
Date: Mon, 5 Feb 2024 09:18:49 GMT   (585kb)

Title: SynthVision - Harnessing Minimal Input for Maximal Output in Computer
  Vision Models using Synthetic Image data
Authors: Yudara Kularathne, Prathapa Janitha, Sithira Ambepitiya, Thanveer
  Ahamed, Dinuka Wijesundara, Prarththanan Sothyrajah
Categories: cs.CV cs.AI cs.LG
Comments: 12 pages 5 figures 1 table
\\
  Rapid development of disease detection computer vision models is vital in
response to urgent medical crises like epidemics or events of bioterrorism.
However, traditional data gathering methods are too slow for these scenarios
necessitating innovative approaches to generate reliable models quickly from
minimal data. We demonstrate our new approach by building a comprehensive
computer vision model for detecting Human Papilloma Virus Genital warts using
only synthetic data. In our study, we employed a two phase experimental design
using diffusion models. In the first phase diffusion models were utilized to
generate a large number of diverse synthetic images from 10 HPV guide images
explicitly focusing on accurately depicting genital warts. The second phase
involved the training and testing vision model using this synthetic dataset.
This method aimed to assess the effectiveness of diffusion models in rapidly
generating high quality training data and the subsequent impact on the vision
model performance in medical image recognition. The study findings revealed
significant insights into the performance of the vision model trained on
synthetic images generated through diffusion models. The vision model showed
exceptional performance in accurately identifying cases of genital warts. It
achieved an accuracy rate of 96% underscoring its effectiveness in medical
image classification. For HPV cases the model demonstrated a high precision of
99% and a recall of 94%. In normal cases the precision was 95% with an
impressive recall of 99%. These metrics indicate the model capability to
correctly identify true positive cases and minimize false positives. The model
achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1
Score across both categories highlights the balanced nature of the model
precision and recall ensuring reliability and robustness in its predictions.
\\ ( https://arxiv.org/abs/2402.02826 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02904 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:16:32 GMT   (824kb,D)

Title: Replication of Impedance Identification Experiments on a
  Reinforcement-Learning-Controlled Digital Twin of Human Elbows
Authors: Hao Yu, Zebin Huang, Qingbo Liu, Ignacio Carlucho, and Mustafa Suphi
  Erden
Categories: cs.RO cs.AI cs.LG
Comments: 8 pages, 5 figures; Submitted to WCCI-2024
\\
  This study presents a pioneering effort to replicate human neuromechanical
experiments within a virtual environment utilising a digital human model. By
employing MyoSuite, a state-of-the-art human motion simulation platform
enhanced by Reinforcement Learning (RL), multiple types of impedance
identification experiments of human elbow were replicated on a musculoskeletal
model. We compared the elbow movement controlled by an RL agent with the motion
of an actual human elbow in terms of the impedance identified in
torque-perturbation experiments. The findings reveal that the RL agent exhibits
higher elbow impedance to stabilise the target elbow motion under perturbation
than a human does, likely due to its shorter reaction time and superior sensory
capabilities. This study serves as a preliminary exploration into the potential
of virtual environment simulations for neuromechanical research, offering an
initial yet promising alternative to conventional experimental approaches. An
RL-controlled digital twin with complete musculoskeletal models of the human
body is expected to be useful in designing experiments and validating
rehabilitation theory before experiments on real human subjects.
\\ ( https://arxiv.org/abs/2402.02904 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02921 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:41:37 GMT   (2019kb,D)

Title: Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation
Authors: Mehdi Acheli, Daniela Grigori, Matthias Weidlich
Categories: cs.DB cs.AI cs.LG
\\
  Process mining provides methods to analyse event logs generated by
information systems during the execution of processes. It thereby supports the
design, validation, and execution of processes in domains ranging from
healthcare, through manufacturing, to e-commerce. To explore the regularities
of flexible processes that show a large behavioral variability, it was
suggested to mine recurrent behavioral patterns that jointly describe the
underlying process. Existing approaches to behavioral pattern mining, however,
suffer from two limitations. First, they show limited scalability as
incremental computation is incorporated only in the generation of pattern
candidates, but not in the evaluation of their quality. Second, process
analysis based on mined patterns shows limited effectiveness due to an
overwhelmingly large number of patterns obtained in practical application
scenarios, many of which are redundant. In this paper, we address these
limitations to facilitate the analysis of complex, flexible processes based on
behavioral patterns. Specifically, we improve COBPAM, our initial behavioral
pattern mining algorithm, by an incremental procedure to evaluate the quality
of pattern candidates, optimizing thereby its efficiency. Targeting a more
effective use of the resulting patterns, we further propose pruning strategies
for redundant patterns and show how relations between the remaining patterns
are extracted and visualized to provide process insights. Our experiments with
diverse real-world datasets indicate a considerable reduction of the runtime
needed for pattern mining, while a qualitative assessment highlights how
relations between patterns guide the analysis of the underlying process.
\\ ( https://arxiv.org/abs/2402.02921 ,  2019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03040 (*cross-listing*)
Date: Mon, 5 Feb 2024 14:24:46 GMT   (12483kb,D)

Title: InteractiveVideo: User-Centric Controllable Video Generation with
  Synergistic Multimodal Instructions
Authors: Yiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao,
  Xiangyu Yue
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: Code, models, and demo are available at
  https://github.com/invictus717/InteractiveVideo
\\
  We introduce $\textit{InteractiveVideo}$, a user-centric framework for video
generation. Different from traditional generative approaches that operate based
on user-provided images or text, our framework is designed for dynamic
interaction, allowing users to instruct the generative model through various
intuitive mechanisms during the whole generation process, e.g. text and image
prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal
Instruction mechanism, designed to seamlessly integrate users' multimodal
instructions into generative models, thus facilitating a cooperative and
responsive interaction between user inputs and the generative process. This
approach enables iterative and fine-grained refinement of the generation result
through precise and effective user instructions. With
$\textit{InteractiveVideo}$, users are given the flexibility to meticulously
tailor key aspects of a video. They can paint the reference image, edit
semantics, and adjust video motions until their requirements are fully met.
Code, models, and demo are available at
https://github.com/invictus717/InteractiveVideo
\\ ( https://arxiv.org/abs/2402.03040 ,  12483kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03072 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:02:35 GMT   (1103kb,D)

Title: Learning to Abstract Visuomotor Mappings using Meta-Reinforcement
  Learning
Authors: Carlos A. Velazquez-Vargas, Isaac Ray Christian, Jordan A. Taylor and
  Sreejan Kumar
Categories: q-bio.NC cs.AI cs.LG
\\
  We investigated the human capacity to acquire multiple visuomotor mappings
for de novo skills. Using a grid navigation paradigm, we tested whether
contextual cues implemented as different "grid worlds", allow participants to
learn two distinct key-mappings more efficiently. Our results indicate that
when contextual information is provided, task performance is significantly
better. The same held true for meta-reinforcement learning agents that differed
in whether or not they receive contextual information when performing the task.
We evaluated their accuracy in predicting human performance in the task and
analyzed their internal representations. The results indicate that contextual
cues allow the formation of separate representations in space and time when
using different visuomotor mappings, whereas the absence of them favors sharing
one representation. While both strategies can allow learning of multiple
visuomotor mappings, we showed contextual cues provide a computational
advantage in terms of how many mappings can be learned.
\\ ( https://arxiv.org/abs/2402.03072 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03081 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:12:15 GMT   (2148kb,D)

Title: Preference-Conditioned Language-Guided Abstraction
Authors: Andi Peng, Andreea Bobu, Belinda Z. Li, Theodore R. Sumers, Ilia
  Sucholutsky, Nishanth Kumar, Thomas L. Griffiths, Julie A. Shah
Categories: cs.RO cs.AI cs.LG
Comments: HRI 2024
\\
  Learning from demonstrations is a common way for users to teach robots, but
it is prone to spurious feature correlations. Recent work constructs state
abstractions, i.e. visual representations containing task-relevant features,
from language as a way to perform more generalizable learning. However, these
abstractions also depend on a user's preference for what matters in a task,
which may be hard to describe or infeasible to exhaustively specify using
language alone. How do we construct abstractions to capture these latent
preferences? We observe that how humans behave reveals how they see the world.
Our key insight is that changes in human behavior inform us that there are
differences in preferences for how humans see the world, i.e. their state
abstractions. In this work, we propose using language models (LMs) to query for
those preferences directly given knowledge that a change in behavior has
occurred. In our framework, we use the LM in two ways: first, given a text
description of the task and knowledge of behavioral change between states, we
query the LM for possible hidden preferences; second, given the most likely
preference, we query the LM to construct the state abstraction. In this
framework, the LM is also able to ask the human directly when uncertain about
its own estimate. We demonstrate our framework's ability to construct effective
preference-conditioned abstractions in simulated experiments, a user study, as
well as on a real Spot robot performing mobile manipulation tasks.
\\ ( https://arxiv.org/abs/2402.03081 ,  2148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03119 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:47:54 GMT   (19546kb,D)

Title: Good Teachers Explain: Explanation-Enhanced Knowledge Distillation
Authors: Amin Parchami-Araghi, Moritz B\"ohle, Sukrut Rao, Bernt Schiele
Categories: cs.CV cs.AI cs.LG
Comments: 21 pages, 12 figures
\\
  Knowledge Distillation (KD) has proven effective for compressing large
teacher models into smaller student models. While it is well known that student
models can achieve similar accuracies as the teachers, it has also been shown
that they nonetheless often do not learn the same function. It is, however,
often highly desirable that the student's and teacher's functions share similar
properties such as basing the prediction on the same input features, as this
ensures that students learn the 'right features' from the teachers. In this
work, we explore whether this can be achieved by not only optimizing the
classic KD loss but also the similarity of the explanations generated by the
teacher and the student. Despite the idea being simple and intuitive, we find
that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides
large gains in terms of accuracy and student-teacher agreement, (2) ensures
that the student learns from the teacher to be right for the right reasons and
to give similar explanations, and (3) is robust with respect to the model
architectures, the amount of training data, and even works with 'approximate',
pre-computed explanations.
\\ ( https://arxiv.org/abs/2402.03119 ,  19546kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03130 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:56:19 GMT   (557kb)

Title: User-Centric Evaluation of ChatGPT Capability of Generating R Program
  Code
Authors: Tanha Miah and Hong Zhu
Categories: cs.SE cs.AI
Comments: The paper has been submitted to the journal Electronics for
  consideration of publication. It is in the review process
\\
  This paper reports an evaluation of ChatGPT's capability of generating R
programming language code from natural language input. A dataset specially
designed for generating R program code was constructed with metadata to support
scenario-based testing and evaluation of code generation capabilities in
various usage scenarios of different levels of difficulty and different types
of programs. The evaluation takes a multiple attempt process in which the
tester tries to complete the code generation task through a number of attempts
until a satisfactory solution is obtained or gives up after a fixed number of
maximal attempts. In each attempt the tester formulates a natural language
input to ChatGPT based on the previous results and the task to be completed. In
addition to the metrics of average numbers of attempts and average amount of
time taken to complete the tasks, the final generated solutions are then
assessed on a number of quality attributes, including accuracy, completeness,
conciseness, readability, well structuredness, logic clarity, depth of
ex-planation, and coverage of parameters. Our experiments demonstrated that
ChatGPT is in general highly capable of generating high quality R program code
as well as textual explanations although it may fail on hard programming tasks.
The experiment data also shows that human developers can hardly learn from
experiences naturally to improve the skill of using ChatGPT to generate code.
\\ ( https://arxiv.org/abs/2402.03130 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03176 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:43:53 GMT   (637kb)

Title: Comparison of Topic Modelling Approaches in the Banking Context
Authors: Bayode Ogunleye, Tonderai Maswera, Laurence Hirsch, Jotham Gaudoin,
  and Teresa Brunsdon
Categories: cs.IR cs.AI cs.LG stat.CO
Comments: 14 pages, Journal of Applied Science
ACM-class: H.3.3
Journal-ref: Applied Sciences (2023), 13(2), 797
DOI: 10.3390/app13020797
\\
  Topic modelling is a prominent task for automatic topic extraction in many
applications such as sentiment analysis and recommendation systems. The
approach is vital for service industries to monitor their customer discussions.
The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for
topic discovery has shown great performances, however, they are not consistent
in their results as these approaches suffer from data sparseness and inability
to model the word order in a document. Thus, this study presents the use of
Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the
BERTopic architecture. We have prepared a new dataset using tweets from
customers of Nigerian banks and we use this to compare the topic modelling
approaches. Our findings showed KernelPCA and K-means in the BERTopic
architecture-produced coherent topics with a coherence score of 0.8463.
\\ ( https://arxiv.org/abs/2402.03176 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03183 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:47:13 GMT   (1799kb,D)

Title: Predicting Configuration Performance in Multiple Environments with
  Sequential Meta-learning
Authors: Jingzhi Gong, Tao Chen
Categories: cs.SE cs.AI cs.LG cs.PF
Comments: This paper has been accepted by FSE'24
\\
  Learning and predicting the performance of given software configurations are
of high importance to many software engineering activities. While configurable
software systems will almost certainly face diverse running environments (e.g.,
version, hardware, and workload), current work often either builds performance
models under a single environment or fails to properly handle data from diverse
settings, hence restricting their accuracy for new environments. In this paper,
we target configuration performance learning under multiple environments. We do
so by designing SeMPL - a meta-learning framework that learns the common
understanding from configurations measured in distinct (meta) environments and
generalizes them to the unforeseen, target environment. What makes it unique is
that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train
the meta environments in parallel, we train them sequentially, one at a time.
The order of training naturally allows discriminating the contributions among
meta environments in the meta-model built, which fits better with the
characteristic of configuration data that is known to dramatically differ
between different environments. Through comparing with 15 state-of-the-art
models under nine systems, our extensive experimental results demonstrate that
SeMPL performs considerably better on 89% of the systems with up to 99%
accuracy improvement, while being data-efficient, leading to a maximum of 3.86x
speedup. All code and data can be found at our repository:
https://github.com/ideas-labo/SeMPL.
\\ ( https://arxiv.org/abs/2402.03183 ,  1799kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03204 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:15:00 GMT   (608kb,D)

Title: Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell
  Massive MIMO Systems
Authors: Tianzhang Cai, Qichen Wang, Shuai Zhang, \"Ozlem Tu\u{g}fe Demir,
  Cicek Cavdar
Categories: cs.IT cs.AI cs.LG math.IT
\\
  We develop a multi-agent reinforcement learning (MARL) algorithm to minimize
the total energy consumption of multiple massive MIMO (multiple-input
multiple-output) base stations (BSs) in a multi-cell network while preserving
the overall quality-of-service (QoS) by making decisions on the multi-level
advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is
modeled as a decentralized partially observable Markov decision process
(DEC-POMDP) to enable collaboration between individual BSs, which is necessary
to tackle inter-cell interference. A multi-agent proximal policy optimization
(MAPPO) algorithm is designed to learn a collaborative BS control policy. To
enhance its scalability, a modified version called MAPPO-neighbor policy is
further proposed. Simulation results demonstrate that the trained MAPPO agent
achieves better performance compared to baseline policies. Specifically,
compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the
MAPPO-neighbor policy reduces power consumption by approximately 8.7% during
low-traffic hours and improves energy efficiency by approximately 19% during
high-traffic hours, respectively.
\\ ( https://arxiv.org/abs/2402.03204 ,  608kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03214 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:25:04 GMT   (20133kb,D)

Title: Organic or Diffused: Can We Distinguish Human Art from AI-generated
  Images?
Authors: Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan,
  Reid Southen, Haitao Zheng, Ben Y. Zhao
Categories: cs.CV cs.AI cs.LG
\\
  The advent of generative AI images has completely disrupted the art world.
Identifying AI generated images from human art is a challenging problem whose
impact is growing over time. The failure to address this problem allows bad
actors to defraud individuals paying a premium for human art, and companies
whose stated policies forbid AI imagery. This is also critical for AI model
trainers, who need to filter training data to avoid potential model collapse.
There are several different approaches to distinguishing human art from AI
images, including classifiers trained by supervised learning, research tools
targeting diffusion models, and identification by professional artists using
their knowledge of artistic techniques. In this paper, we seek to understand
how well these approaches can perform against today's modern generative models
in both benign and adversarial settings. We curate real human art across 7
styles, generate matching images from 5 generative models, and apply 8
detectors (5 automated detectors and 3 different human groups including 180
crowdworkers, 4000+ professional artists, and 13 expert artists experienced at
detecting AI). Both Hive and expert artists do very well, but make mistakes in
different ways (Hive is weaker against adversarial perturbations while Expert
artists produce higher false positives). We believe these weaknesses will
remain as models continue to evolve, and use our data to demonstrate why a
combined team of human and automated detectors provides the best combination of
accuracy and robustness.
\\ ( https://arxiv.org/abs/2402.03214 ,  20133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03227 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:38:49 GMT   (3069kb,D)

Title: IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of
  brain MR images
Authors: Vincent Roca, Gr\'egory Kuchcinski, Jean-Pierre Pruvo, Dorian
  Manouvriez, Renaud Lopes
Categories: cs.CV cs.AI cs.LG
Comments: 23 pages, 8 figures
\\
  In MRI studies, the aggregation of imaging data from multiple acquisition
sites enhances sample size but may introduce site-related variabilities that
hinder consistency in subsequent analyses. Deep learning methods for image
translation have emerged as a solution for harmonizing MR images across sites.
In this study, we introduce IGUANe (Image Generation with Unified Adversarial
Networks), an original 3D model that leverages the strengths of domain
translation and straightforward application of style transfer methods for
multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture
by integrating an arbitrary number of domains for training through a
many-to-one strategy. During inference, the model can be applied to any image,
even from an unknown acquisition site, making it a universal generator for
harmonization. Trained on a dataset comprising T1-weighted images from 11
different scanners, IGUANe was evaluated on data from unseen sites. The
assessments included the transformation of MR images with traveling subjects,
the preservation of pairwise distances between MR images within domains, the
evolution of volumetric patterns related to age and Alzheimer$^\prime$s disease
(AD), and the performance in age regression and patient classification tasks.
Comparisons with other harmonization and normalization methods suggest that
IGUANe better preserves individual information in MR images and is more
suitable for maintaining and reinforcing variabilities related to age and AD.
Future studies may further assess IGUANe in other multicenter contexts, either
using the same model or retraining it for applications to different image
modalities.
\\ ( https://arxiv.org/abs/2402.03227 ,  3069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03246 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:03:53 GMT   (28221kb,D)

Title: SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
Authors: Mingrui Li, Shuhong Liu, Heng Zhou
Categories: cs.CV cs.AI cs.RO
\\
  Semantic understanding plays a crucial role in Dense Simultaneous
Localization and Mapping (SLAM), facilitating comprehensive scene
interpretation. Recent advancements that integrate Gaus- sian Splatting into
SLAM systems have demonstrated its effectiveness in generating high-quality
renderings through the use of explicit 3D Gaussian representations. Building on
this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system
grounded in 3D Gaussians, which provides precise 3D semantic segmentation
alongside high-fidelity reconstructions. Specifically, we propose to employ
multi-channel optimization during the mapping process, integrating appearance,
geometric, and semantic constraints with key-frame optimization to enhance
reconstruction quality. Extensive experiments demonstrate that SGS-SLAM
delivers state-of-the-art performance in camera pose estimation, map
reconstruction, and semantic segmentation, outperforming existing methods
meanwhile preserving real-time rendering ability.
\\ ( https://arxiv.org/abs/2402.03246 ,  28221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03247 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:05:34 GMT   (18981kb,D)

Title: HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible
  Dataflows for Energy-Efficient CNN Inference
Authors: Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, and Ishan Thakkar
Categories: cs.AR cs.AI cs.ET
Comments: The paper is under review at ACM TODAES
\\
  Several photonic microring resonators (MRRs) based analog accelerators have
been proposed to accelerate the inference of integer-quantized CNNs with
remarkably higher throughput and energy efficiency compared to their electronic
counterparts. However, the existing analog photonic accelerators suffer from
three shortcomings: (i) severe hampering of wavelength parallelism due to
various crosstalk effects, (ii) inflexibility of supporting various dataflows
other than the weight-stationary dataflow, and (iii) failure in fully
leveraging the ability of photodetectors to perform in-situ accumulations.
These shortcomings collectively hamper the performance and energy efficiency of
prior accelerators. To tackle these shortcomings, we present a novel Hybrid
timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid
time-amplitude analog optical multipliers (TAOMs) that increase the flexibility
of HEANA to support multiple dataflows. A spectrally hitless arrangement of
TAOMs significantly reduces the crosstalk effects, thereby increasing the
wavelength parallelism in HEANA. Moreover, HEANA employs our invented balanced
photo-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporal
accumulations to eliminate the need to use reduction networks in HEANA,
relieving it from related latency and energy overheads. Our evaluation for the
inference of four modern CNNs indicates that HEANA provides improvements of
atleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency),
respectively, for equal-area comparisons, on gmean over two MRR-based analog
CNN accelerators from prior work.
\\ ( https://arxiv.org/abs/2402.03247 ,  18981kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03251 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:09:33 GMT   (2369kb,D)

Title: CLIP Can Understand Depth
Authors: Dunam Kim, Seokju Lee
Categories: cs.CV cs.AI cs.LG
\\
  Recent studies on generalizing CLIP for monocular depth estimation reveal
that CLIP pre-trained on web-crawled data is inefficient for deriving proper
similarities between image patches and depth-related prompts. In this paper, we
adapt CLIP for meaningful quality of monocular depth estimation with dense
prediction, without fine-tuning its original vision-language alignment. By
jointly training a compact deconvolutional decoder with a tiny learnable
embedding matrix named mirror, as a static prompt for its text encoder, CLIP is
enabled to understand depth. With this approach, our model exhibits impressive
performance matching several previous state-of-the-art vision-only models on
the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth
estimation model with a large margin. Experiments on temporal depth consistency
and spatial continuity demonstrate that the prior knowledge of CLIP can be
effectively refined by our proposed framework. Furthermore, an ablation study
on mirror proves that the resulting model estimates depth utilizing knowledge
not only from the image encoder but also text encoder despite not being given
any prompt written in a human way. This research demonstrates that through
minimal adjustments, the prior knowledge of vision-language foundation models,
such as CLIP, can be generalized even to domains where learning during
pretraining is challenging. We facilitate future works focused on methods to
adjust suboptimal prior knowledge of vision-language models using non-human
language prompts, achieving performance on par with task-specific
state-of-the-art methodologies.
\\ ( https://arxiv.org/abs/2402.03251 ,  2369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03286 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:42:34 GMT   (25618kb,D)

Title: Training-Free Consistent Text-to-Image Generation
Authors: Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal
  Chechik, Yuval Atzmon
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project page is in https://consistory-paper.github.io
\\
  Text-to-image models offer a new level of creative flexibility by allowing
users to guide the image generation process through natural language. However,
using these models to consistently portray the same subject across diverse
prompts remains challenging. Existing approaches fine-tune the model to teach
it new words that describe specific user-provided subjects or add image
conditioning to the model. These methods require lengthy per-subject
optimization or large-scale pre-training. Moreover, they struggle to align
generated images with text prompts and face difficulties in portraying multiple
subjects. Here, we present ConsiStory, a training-free approach that enables
consistent subject generation by sharing the internal activations of the
pretrained model. We introduce a subject-driven shared attention block and
correspondence-based feature injection to promote subject consistency between
images. Additionally, we develop strategies to encourage layout diversity while
maintaining subject consistency. We compare ConsiStory to a range of baselines,
and demonstrate state-of-the-art performance on subject consistency and text
alignment, without requiring a single optimization step. Finally, ConsiStory
can naturally extend to multi-subject scenarios, and even enable training-free
personalization for common objects.
\\ ( https://arxiv.org/abs/2402.03286 ,  25618kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03290 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:49:17 GMT   (17504kb,D)

Title: InstanceDiffusion: Instance-level Control for Image Generation
Authors: Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar,
  Ishan Misra
Categories: cs.CV cs.AI cs.LG
Comments: Preprint; Project page:
  https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/
\\
  Text-to-image diffusion models produce high quality images but do not offer
control over individual instances in the image. We introduce InstanceDiffusion
that adds precise instance-level control to text-to-image diffusion models.
InstanceDiffusion supports free-form language conditions per instance and
allows flexible ways to specify instance locations such as simple single
points, scribbles, bounding boxes or intricate instance segmentation masks, and
combinations thereof. We propose three major changes to text-to-image models
that enable precise instance-level control. Our UniFusion block enables
instance-level conditions for text-to-image models, the ScaleU block improves
image fidelity, and our Multi-instance Sampler improves generations for
multiple instances. InstanceDiffusion significantly surpasses specialized
state-of-the-art models for each location condition. Notably, on the COCO
dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$
for box inputs, and 25.4% IoU for mask inputs.
\\ ( https://arxiv.org/abs/2402.03290 ,  17504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03311 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:59:41 GMT   (16361kb,D)

Title: HASSOD: Hierarchical Adaptive Self-Supervised Object Detection
Authors: Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang
Categories: cs.CV cs.AI cs.LG
Comments: NeurIPS 2023
\\
  The human visual perception system demonstrates exceptional capabilities in
learning without explicit supervision and understanding the part-to-whole
composition of objects. Drawing inspiration from these two abilities, we
propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a
novel approach that learns to detect objects and understand their compositions
without human supervision. HASSOD employs a hierarchical adaptive clustering
strategy to group regions into object masks based on self-supervised visual
representations, adaptively determining the number of objects per image.
Furthermore, HASSOD identifies the hierarchical levels of objects in terms of
composition, by analyzing coverage relations between masks and constructing
tree structures. This additional self-supervised learning task leads to
improved detection performance and enhanced interpretability. Lastly, we
abandon the inefficient multi-round self-training process utilized in prior
methods and instead adapt the Mean Teacher framework from semi-supervised
learning, which leads to a smoother and more efficient training process.
Through extensive experiments on prevalent image datasets, we demonstrate the
superiority of HASSOD over existing methods, thereby advancing the state of the
art in self-supervised object detection. Notably, we improve Mask AR from 20.2
to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:
https://HASSOD-NeurIPS23.github.io.
\\ ( https://arxiv.org/abs/2402.03311 ,  16361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01682 (*cross-listing*)
Date: Mon, 22 Jan 2024 06:51:29 GMT   (857kb)

Title: Leveraging Social Media Data to Identify Factors Influencing Public
  Attitude Towards Accessibility, Socioeconomic Disparity and Public
  Transportation
Authors: Khondhaker Al Momin, Arif Mohaimin Sadri, Md Sami Hasnine
Categories: cs.CY cs.CL cs.SI
\\
  This study proposes a novel method to understand the factors affecting
individuals' perception of transport accessibility, socioeconomic disparity,
and public infrastructure. As opposed to the time consuming and expensive
survey-based approach, this method can generate organic large-scale responses
from social media and develop statistical models to understand individuals'
perceptions of various transportation issues. This study retrieved and analyzed
36,098 tweets from New York City from March 19, 2020, to May 15, 2022. A
state-of-the-art natural language processing algorithm is used for text mining
and classification. A data fusion technique has been adopted to generate a
series of socioeconomic traits that are used as explanatory variables in the
model. The model results show that females and individuals of Asian origin tend
to discuss transportation accessibility more than their counterparts, with
those experiencing high neighborhood traffic also being more vocal. However,
disadvantaged individuals, including the unemployed and those living in
low-income neighborhoods or in areas with high natural hazard risks, tend to
communicate less about such issues. As for socioeconomic disparity, individuals
of Asian origin and those experiencing various types of air pollution are more
likely to discuss these topics on Twitter, often with a negative sentiment.
However, unemployed, or disadvantaged individuals, as well as those living in
areas with high natural hazard risks or expected losses, are less inclined to
tweet about this subject. Lack of internet accessibility could be a reason why
many disadvantaged individuals do not tweet about transport accessibility and
subsidized internet could be a possible solution.
\\ ( https://arxiv.org/abs/2402.01682 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01683 (*cross-listing*)
Date: Mon, 22 Jan 2024 06:57:45 GMT   (593kb)

Title: Community-based Behavioral Understanding of Crisis Activity Concerns
  using Social Media Data: A Study on the 2023 Canadian Wildfires in New York
  City
Authors: Khondhaker Al Momin, Md Sami Hasnine, Arif Mohaimin Sadri
Categories: cs.CY cs.CL cs.SI
\\
  New York City (NYC) topped the global chart for the worst air pollution in
June 2023, owing to the wildfire smoke drifting in from Canada. This
unprecedented situation caused significant travel disruptions and shifts in
traditional activity patterns of NYC residents. This study utilized large-scale
social media data to study different crisis activity concerns (i.e.,
evacuation, staying indoors, shopping, and recreational activities among
others) in the emergence of the 2023 Canadian wildfire smoke in NYC. In this
regard, one week (June 02 through June 09, 2023) geotagged Twitter data from
NYC were retrieved and used in the analysis. The tweets were processed using
advanced text classification techniques and later integrated with national
databases such as Social Security Administration data, Census, and American
Community Survey. Finally, a model has been developed to make community
inferences of different activity concerns in a major wildfire. The findings
suggest, during wildfires, females are less likely to engage in discussions
about evacuation, trips for medical, social, or recreational purposes, and
commuting for work, likely influenced by workplaces maintaining operations
despite poor air quality. There were also racial disparities in these
discussions, with Asians being more likely than Hispanics to discuss evacuation
and work commute, and African Americans being less likely to discuss social and
recreational activities. Additionally, individuals from low-income
neighborhoods and non-higher education students expressed fewer concerns about
evacuation. This study provides valuable insights for policymakers, emergency
planners, and public health officials, aiding them in formulating targeted
communication strategies and equitable emergency response plans.
\\ ( https://arxiv.org/abs/2402.01683 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01716 (*cross-listing*)
Date: Fri, 26 Jan 2024 08:20:13 GMT   (889kb)

Title: Bloom-epistemic and sentiment analysis hierarchical classification in
  course discussion forums
Authors: H. Toba, Y. T. Hernita, M. Ayub, M. C. Wijanto
Categories: cs.CY cs.CL cs.LG
Comments: 11 pages, 7 figures
ACM-class: I.2.7
Journal-ref: International Journal of Evaluation and Research in Education 13
  (2024) 80-90
DOI: 10.11591/ijere.v13i1.26024
\\
  Online discussion forums are widely used for active textual interaction
between lecturers and students, and to see how the students have progressed in
a learning process. The objective of this study is to compare appropriate
machine-learning models to assess sentiments and Bloom\'s epistemic taxonomy
based on textual comments in educational discussion forums. Our proposed method
is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis
(BE-Sent). The research methodology consists of three main steps. The first
step is the data collection from the internal discussion forum and YouTube
comments of a Web Programming channel. The next step is text preprocessing to
annotate the text and clear unimportant words. Furthermore, with the text
dataset that has been successfully cleaned, sentiment analysis and epistemic
categorization will be done in each sentence of the text. Sentiment analysis is
divided into three categories: positive, negative, and neutral. Bloom\'s
epistemic is divided into six categories: remembering, understanding, applying,
analyzing, evaluating, and creating. This research has succeeded in producing a
course learning subsystem that assesses opinions based on text reviews of
discussion forums according to the category of sentiment and epistemic
analysis.
\\ ( https://arxiv.org/abs/2402.01716 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01775 (*cross-listing*)
Date: Thu, 1 Feb 2024 13:32:18 GMT   (8149kb,D)

Title: Design and consensus content validity of the questionnaire for
  b-learning education: A 2-Tuple Fuzzy Linguistic Delphi based Decision
  Support Tool
Authors: Rosana Montes, Cristina Zuheros, Jeovani M. Morales, Noe Zerme\~no,
  Jer\'onimo Duran, Francsico Herrera
Categories: cs.CY cs.CL
Comments: 47 pages, 7 figures
Journal-ref: Open Access Volume 147 November 2023 Article number 110755
DOI: 10.1016/j.asoc.2023.110755
\\
  Classic Delphi and Fuzzy Delphi methods are used to test content validity of
data collection tools such as questionnaires. Fuzzy Delphi takes the opinion
issued by judges from a linguistic perspective reducing ambiguity in opinions
by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic
Delphi method to deal with scenarios in which judges show different expertise
degrees by using fuzzy multigranular semantics of the linguistic terms and to
obtain intermediate and final results expressed by 2-tuple linguistic values.
The key idea of our proposal is to validate the full questionnaire by means of
the evaluation of its parts, defining the validity of each item as a Decision
Making problem. Taking the opinion of experts, we measure the degree of
consensus, the degree of consistency, and the linguistic score of each item, in
order to detect those items that affect, positively or negatively, the quality
of the instrument. Considering the real need to evaluate a b-learning
educational experience with a consensual questionnaire, we present a Decision
Making model for questionnaire validation that solves it. Additionally, we
contribute to this consensus reaching problem by developing an online tool
under GPL v3 license. The software visualizes the collective valuations for
each iteration and assists to determine which parts of the questionnaire should
be modified to reach a consensual solution.
\\ ( https://arxiv.org/abs/2402.01775 ,  8149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01796 (*cross-listing*)
Date: Fri, 2 Feb 2024 05:09:42 GMT   (2537kb,D)

Title: Exploring transfer learning for pathological speech feature prediction:
  Impact of layer selection
Authors: Daniela A. Wiepert, Rene L. Utianski, Joseph R. Duffy, John L.
  Stricker, Leland R. Barnard, David T. Jones, Hugo Botha
Categories: eess.AS cs.CL cs.LG
\\
  There is interest in leveraging AI to conduct automatic, objective
assessments of clinical speech, in turn facilitating diagnosis and treatment of
speech disorders. We explore transfer learning, focusing on the impact of layer
selection, for the downstream task of predicting the presence of pathological
speech. We find that selecting an optimal layer offers large performance
improvements (12.4% average increase in balanced accuracy), though the best
layer varies by predicted feature and does not always generalize well to unseen
data. A learned weighted sum offers comparable performance to the average best
layer in-distribution and has better generalization for out-of-distribution
data.
\\ ( https://arxiv.org/abs/2402.01796 ,  2537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01829 (*cross-listing*)
Date: Fri, 2 Feb 2024 18:42:39 GMT   (1645kb,D)

Title: Predicting ATP binding sites in protein sequences using Deep Learning
  and Natural Language Processing
Authors: Shreyas V, Swati Agarwal
Categories: q-bio.BM cs.CL cs.LG
Comments: Published at 3rd Annual AAAI Workshop on AI to Accelerate Science and
  Engineering (AI2ASE)
\\
  Predicting ATP-Protein Binding sites in genes is of great significance in the
field of Biology and Medicine. The majority of research in this field has been
conducted through time- and resource-intensive 'wet experiments' in
laboratories. Over the years, researchers have been investigating computational
methods computational methods to accomplish the same goals, utilising the
strength of advanced Deep Learning and NLP algorithms. In this paper, we
propose to develop methods to classify ATP-Protein binding sites. We conducted
various experiments mainly using PSSMs and several word embeddings as features.
We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms.
The MP3Vec and BERT models have also been subjected to testing in our study.
The outcomes of our experiments demonstrated improvement over the
state-of-the-art benchmarks.
\\ ( https://arxiv.org/abs/2402.01829 ,  1645kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01912 (*cross-listing*)
Date: Fri, 2 Feb 2024 21:29:34 GMT   (704kb,D)

Title: Natural language guidance of high-fidelity text-to-speech with synthetic
  annotations
Authors: Dan Lyth, Simon King
Categories: cs.SD cs.CL
\\
  Text-to-speech models trained on large-scale datasets have demonstrated
impressive in-context learning capabilities and naturalness. However, control
of speaker identity and style in these models typically requires conditioning
on reference speech recordings, limiting creative applications. Alternatively,
natural language prompting of speaker identity and style has demonstrated
promising results and provides an intuitive method of control. However,
reliance on human-labeled descriptions prevents scaling to large datasets.
  Our work bridges the gap between these two approaches. We propose a scalable
method for labeling various aspects of speaker identity, style, and recording
conditions. We then apply this method to a 45k hour dataset, which we use to
train a speech language model. Furthermore, we propose simple methods for
increasing audio fidelity, significantly outperforming recent work despite
relying entirely on found data.
  Our results demonstrate high-fidelity speech generation in a diverse range of
accents, prosodic styles, channel conditions, and acoustic conditions, all
accomplished with a single model and intuitive natural language conditioning.
Audio samples can be heard at https://text-description-to-speech.com/.
\\ ( https://arxiv.org/abs/2402.01912 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01916 (*cross-listing*)
Date: Fri, 2 Feb 2024 21:36:03 GMT   (38kb)

Title: CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor
  assignment in Spanish
Authors: Francisco J. Ribadas-Pena, Shuyuan Cao, Elmurod Kuriyozov
Categories: cs.IR cs.CL cs.LG
Comments: Accepted at the 8th BioASQ Workshop at the 11th Conference and Labs
  of the Evaluation Forum (CLEF) 2020. 11 pages
MSC-class: 68P20, 68T50
ACM-class: H.3.3; I.2.7
Journal-ref: Working Notes of CLEF 2020. Vol. 2696 of CEUR Workshop Proceedings
  (CEUR-WS.org)
\\
  In this paper, we describe our participation in the MESINESP Task of the
BioASQ biomedical semantic indexing challenge. The participating system follows
an approach based solely on conventional information retrieval tools. We have
evaluated various alternatives for extracting index terms from IBECS/LILACS
documents in order to be stored in an Apache Lucene index. Those indexed
representations are queried using the contents of the article to be annotated
and a ranked list of candidate labels is created from the retrieved documents.
We also have evaluated a sort of limited Label Powerset approach which creates
meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an
alternative method based on label profile matching. Results obtained in
official runs seem to confirm the suitability of this approach for languages
like Spanish.
\\ ( https://arxiv.org/abs/2402.01916 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02037 (*cross-listing*)
Date: Sat, 3 Feb 2024 05:24:39 GMT   (963kb,D)

Title: EffiBench: Benchmarking the Efficiency of Automatically Generated Code
Authors: Dong Huang, Jie M.Zhang, Yuhao Qing, Heming Cui
Categories: cs.SE cs.CL
Comments: 26 pages, 13 figures, 18 tables
\\
  Code generation models have increasingly become integral to aiding software
development, offering assistance in tasks such as code completion, debugging,
and code translation. Although current research has thoroughly examined the
correctness of code produced by code generation models, a vital aspect, i.e.,
the efficiency of the generated code, has often been neglected. This paper
presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems
for assessing the efficiency of code generated by code generation models.
EffiBench contains a diverse set of LeetCode coding problems. Each problem is
paired with an executable human-written canonical solution. With EffiBench, we
empirically examine the capability of 21 Large Language Models (13 open-sourced
and 8 closed-sourced) in generating efficient code. The results demonstrate
that GPT-4-turbo generates the most efficient code, significantly outperforming
Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5.
Nevertheless, its code efficiency is still worse than the efficiency of
human-written canonical solutions. In particular, the average and worst
execution time of GPT-4-turbo generated code is 1.69 and 45.49 times that of
the canonical solutions.
\\ ( https://arxiv.org/abs/2402.02037 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02302 (*cross-listing*)
Date: Sat, 3 Feb 2024 23:54:03 GMT   (9390kb,D)

Title: Predicting positive transfer for improved low-resource speech
  recognition using acoustic pseudo-tokens
Authors: Nay San, Georgios Paraskevopoulos, Aryaman Arora, Xiluo He, Prabhjot
  Kaur, Oliver Adams, Dan Jurafsky
Categories: eess.AS cs.CL
Comments: Accepted for SIGTYP2024
\\
  While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be
directly fine-tuned for automatic speech recognition (ASR), downstream
performance can still be relatively poor on languages that are
under-represented in the pre-training data. Continued pre-training on 70-200
hours of untranscribed speech in these languages can help -- but what about
languages without that much recorded data? For such cases, we show that
supplementing the target language with data from a similar, higher-resource
'donor' language can help. For example, continued pre-training on only 10 hours
of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as
good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing
data from less similar donors like Bengali does not improve ASR performance. To
inform donor language selection, we propose a novel similarity metric based on
the sequence distribution of induced acoustic units: the Acoustic Token
Distribution Similarity (ATDS). Across a set of typologically different target
languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between
the target language and its candidate donors precisely predicts target language
ASR performance.
\\ ( https://arxiv.org/abs/2402.02302 ,  9390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02369 (*cross-listing*)
Date: Sun, 4 Feb 2024 06:56:23 GMT   (37920kb,D)

Title: M$^3$Face: A Unified Multi-Modal Multilingual Framework for Human Face
  Generation and Editing
Authors: Mohammadreza Mofayezi, Reza Alipour, Mohammad Ali Kakavand, Ehsaneddin
  Asgari
Categories: cs.CV cs.CL cs.MM
\\
  Human face generation and editing represent an essential task in the era of
computer vision and the digital world. Recent studies have shown remarkable
progress in multi-modal face generation and editing, for instance, using face
segmentation to guide image generation. However, it may be challenging for some
users to create these conditioning modalities manually. Thus, we introduce
M3Face, a unified multi-modal multilingual framework for controllable face
generation and editing. This framework enables users to utilize only text input
to generate controlling modalities automatically, for instance, semantic
segmentation or facial landmarks, and subsequently generate face images. We
conduct extensive qualitative and quantitative experiments to showcase our
frameworks face generation and editing capabilities. Additionally, we propose
the M3CelebA Dataset, a large-scale multi-modal and multilingual face dataset
containing high-quality images, semantic segmentations, facial landmarks, and
different captions for each image in multiple languages. The code and the
dataset will be released upon publication.
\\ ( https://arxiv.org/abs/2402.02369 ,  37920kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02632 (*cross-listing*)
Date: Sun, 4 Feb 2024 22:53:38 GMT   (524kb,D)

Title: GIRT-Model: Automated Generation of Issue Report Templates
Authors: Nafiseh Nikeghbal, Amir Hossein Kargaran, Abbas Heydarnoori
Categories: cs.SE cs.CL
Comments: Accepted to be published at the 21st IEEE/ACM International
  Conference on Mining Software Repositories (MSR 2024)
\\
  Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs)
to enable more effective issue management and better alignment with developer
expectations. However, these templates are not widely adopted in most
repositories, and there is currently no tool available to aid developers in
generating them. In this work, we introduce GIRT-Model, an assistant language
model that automatically generates IRTs based on the developer's instructions
regarding the structure and necessary fields. We create GIRT-Instruct, a
dataset comprising pairs of instructions and IRTs, with the IRTs sourced from
GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model
to create the GIRT-Model. In our experiments, GIRT-Model outperforms general
language models (T5 and Flan-T5 with different parameter sizes) in IRT
generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and
human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a
user study in which participants wrote short IRTs with GIRT-Model. Our results
show that the participants find GIRT-Model useful in the automated generation
of templates. We hope that through the use of GIRT-Model, we can encourage more
developers to adopt IRTs in their repositories. We publicly release our code,
dataset, and model at https://github.com/ISE-Research/girt-model.
\\ ( https://arxiv.org/abs/2402.02632 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02987 (*cross-listing*)
Date: Mon, 5 Feb 2024 13:18:42 GMT   (3473kb,D)

Title: Conversation Reconstruction Attack Against GPT Models
Authors: Junjie Chu and Zeyang Sha and Michael Backes and Yang Zhang
Categories: cs.CR cs.CL
Comments: 17 pages, 11 figures
\\
  In recent times, significant advancements have been made in the field of
large language models (LLMs), represented by GPT series models. To optimize
task execution, users often engage in multi-round conversations with GPT models
hosted in cloud environments. These multi-round conversations, potentially
replete with private information, require transmission and storage within the
cloud. However, this operational paradigm introduces additional attack
surfaces. In this paper, we first introduce a specific Conversation
Reconstruction Attack targeting GPT models. Our introduced Conversation
Reconstruction Attack is composed of two steps: hijacking a session and
reconstructing the conversations. Subsequently, we offer an exhaustive
evaluation of the privacy risks inherent in conversations when GPT models are
subjected to the proposed attack. However, GPT-4 demonstrates certain
robustness to the proposed attacks. We then introduce two advanced attacks
aimed at better reconstructing previous conversations, specifically the UNR
attack and the PBU attack. Our experimental findings indicate that the PBU
attack yields substantial performance across all models, achieving semantic
similarity scores exceeding 0.60, while the UNR attack is effective solely on
GPT-3.5. Our results reveal the concern about privacy risks associated with
conversations involving GPT models and aim to draw the community's attention to
prevent the potential misuse of these models' remarkable capabilities. We will
responsibly disclose our findings to the suppliers of related large language
models.
\\ ( https://arxiv.org/abs/2402.02987 ,  3473kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03050 (*cross-listing*)
Date: Mon, 5 Feb 2024 14:34:14 GMT   (197kb)

Title: A Comprehensive Study of the Current State-of-the-Art in Nepali
  Automatic Speech Recognition Systems
Authors: Rupak Raj Ghimire and Bal Krishna Bal and Prakash Poudyal
Categories: cs.SD cs.CL eess.AS
Comments: Accepted in International Conference on Technologies for Computer,
  Electrical, Electronics & Communication (ICT-CEEL 2023)
\\
  In this paper, we examine the research conducted in the field of Nepali
Automatic Speech Recognition (ASR). The primary objective of this survey is to
conduct a comprehensive review of the works on Nepali Automatic Speech
Recognition Systems completed to date, explore the different datasets used,
examine the technology utilized, and take account of the obstacles encountered
in implementing the Nepali ASR system. In tandem with the global trends of
ever-increasing research on speech recognition based research, the number of
Nepalese ASR-related projects are also growing. Nevertheless, the investigation
of language and acoustic models of the Nepali language has not received
adequate attention compared to languages that possess ample resources. In this
context, we provide a framework as well as directions for future
investigations.
\\ ( https://arxiv.org/abs/2402.03050 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03161 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:30:49 GMT   (31035kb,D)

Title: Video-LaVIT: Unified Video-Language Pre-training with Decoupled
  Visual-Motional Tokenization
Authors: Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe
  Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu
Categories: cs.CV cs.CL
\\
  In light of recent advances in multimodal Large Language Models (LLMs), there
is increasing attention to scaling them from image-text data to more
informative real-world videos. Compared to static images, video poses unique
challenges for effective large-scale pre-training due to the modeling of its
spatiotemporal dynamics. In this paper, we address such limitations in
video-language pre-training with an efficient video decomposition that
represents each video as keyframes and temporal motions. These are then adapted
to an LLM using well-designed tokenizers that discretize visual and temporal
information as a few tokens, thus enabling unified generative pre-training of
videos, images, and text. At inference, the generated tokens from the LLM are
carefully recovered to the original continuous pixel space to create various
video content. Our proposed framework is both capable of comprehending and
generating image and video content, as demonstrated by its competitive
performance across 13 multimodal benchmarks in image and video understanding
and generation. Our code and models will be available at
https://video-lavit.github.io.
\\ ( https://arxiv.org/abs/2402.03161 ,  31035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03269 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:27:27 GMT   (248kb,D)

Title: ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds
Authors: Masato Hagiwara, Marius Miron, Jen-Yu Liu
Categories: cs.SD cs.CL cs.LG
Comments: Accepted at XAI-AI Workshop (IEEEXplore track) @ ICASSP 2024
\\
  Traditionally, bioacoustics has relied on spectrograms and continuous,
per-frame audio representations for the analysis of animal sounds, also serving
as input to machine learning models. Meanwhile, the International Phonetic
Alphabet (IPA) system has provided an interpretable, language-independent
method for transcribing human speech sounds. In this paper, we introduce ISPA
(Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system
designed for transcribing animal sounds into text. We compare acoustics-based
and feature-based methods for transcribing and classifying animal sounds,
demonstrating their comparable performance with baseline methods utilizing
continuous, dense audio representations. By representing animal sounds with
text, we effectively treat them as a "foreign language," and we show that
established human language ML paradigms and models, such as language models,
can be successfully applied to improve performance.
\\ ( https://arxiv.org/abs/2402.03269 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01648 (*cross-listing*)
Date: Sat, 6 Jan 2024 17:34:26 GMT   (712kb)

Title: Forecasting Imports in OECD Member Countries and Iran by Using Neural
  Network Algorithms of LSTM
Authors: Soheila Khajoui, Saeid Dehyadegari, Sayyed Abdolmajid Jalaee
Categories: cs.CY cs.LG econ.GN q-fin.EC
\\
  Artificial Neural Networks (ANN) which are a branch of artificial
intelligence, have shown their high value in lots of applications and are used
as a suitable forecasting method. Therefore, this study aims at forecasting
imports in OECD member selected countries and Iran for 20 seasons from 2021 to
2025 by means of ANN. Data related to the imports of such countries collected
over 50 years from 1970 to 2019 from valid resources including World Bank, WTO,
IFM,the data turned into seasonal data to increase the number of collected data
for better performance and high accuracy of the network by using Diz formula
that there were totally 200 data related to imports. This study has used LSTM
to analyse data in Pycharm. 75% of data considered as training data and 25%
considered as test data and the results of the analysis were forecasted with
99% accuracy which revealed the validity and reliability of the output. Since
the imports is consumption function and since the consumption is influenced
during Covid-19 Pandemic, so it is time-consuming to correct and improve it to
be influential on the imports, thus the imports in the years after Covid-19
Pandemic has had a fluctuating trend.
\\ ( https://arxiv.org/abs/2402.01648 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01658 (*cross-listing*)
Date: Fri, 12 Jan 2024 09:16:42 GMT   (990kb)

Title: Untersuchung der Wirkung von Data Storytelling auf das Datenverstaendnis
  von Dashboard-Nutzern
Authors: Valeria Zitz and Patrick Baier
Categories: cs.CY cs.LG
Comments: in German language
Journal-ref: Tagungsband zur 36. AKWI-Jahrestagung 2023
DOI: 10.15771/1794
\\
  With the increasing use of big data and business analytics, data storytelling
has gained popularity as an effective means of communicating analytical
insights to audiences to support decision making and improve business
performance. However, there is little empirical evidence on the impact of data
storytelling on data understanding. This study validates the concept of data
storytelling as a construct in terms of its impact on users' data
understanding. Based on empirical data analysis, the results of this study show
that data storytelling competence is positively associated with organizational
performance, which is partly due to the quality of the decision is conveyed.
These results provide a theoretical basis for further investigation of
potential antecedents and consequences of data storytelling.
\\ ( https://arxiv.org/abs/2402.01658 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01663 (*cross-listing*)
Date: Sun, 14 Jan 2024 12:09:40 GMT   (366kb,D)

Title: Killer Apps: Low-Speed, Large-Scale AI Weapons
Authors: Philip Feldman, Aaron Dant, James R. Foulds
Categories: cs.CY cs.CR cs.LG
Comments: 10 pages with 10 pages of appendices. 3 Figures, 2 code listings
ACM-class: I.2.7; H.4.3; J.4
\\
  The accelerating advancements in Artificial Intelligence (AI) and Machine
Learning (ML), highlighted by the development of cutting-edge Generative
Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and
Anthropic, present new challenges and opportunities in warfare and security.
Much of the current focus is on AI's integration within weapons systems and its
role in rapid decision-making in kinetic conflict. However, an equally
important but often overlooked aspect is the potential of AI-based
psychological manipulation at internet scales within the information domain.
These capabilities could pose significant threats to individuals,
organizations, and societies globally. This paper explores the concept of AI
weapons, their deployment, detection, and potential countermeasures.
\\ ( https://arxiv.org/abs/2402.01663 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01665 (*cross-listing*)
Date: Mon, 15 Jan 2024 07:47:30 GMT   (3338kb,D)

Title: Knowledge-Driven Deep Learning Paradigms for Wireless Network
  Optimization in 6G
Authors: Ruijin Sun, Nan Cheng, Changle Li, Fangjiong Chen, Wen Chen
Categories: cs.NI cs.LG
Comments: 9 pages, 5 figures
\\
  In the sixth-generation (6G) networks, newly emerging diversified services of
massive users in dynamic network environments are required to be satisfied by
multi-dimensional heterogeneous resources. The resulting large-scale
complicated network optimization problems are beyond the capability of
model-based theoretical methods due to the overwhelming computational
complexity and the long processing time. Although with fast online inference
and universal approximation ability, data-driven deep learning (DL) heavily
relies on abundant training data and lacks interpretability. To address these
issues, a new paradigm called knowledge-driven DL has emerged, aiming to
integrate proven domain knowledge into the construction of neural networks,
thereby exploiting the strengths of both methods. This article provides a
systematic review of knowledge-driven DL in wireless networks. Specifically, a
holistic framework of knowledge-driven DL in wireless networks is proposed,
where knowledge sources, knowledge representation, knowledge integration and
knowledge application are forming as a closed loop. Then, a detailed taxonomy
of knowledge integration approaches, including knowledge-assisted,
knowledge-fused, and knowledge-embedded DL, is presented. Several open issues
for future research are also discussed. The insights offered in this article
provide a basic principle for the design of network optimization that
incorporates communication-specific domain knowledge and DL, facilitating the
realization of intelligent 6G networks.
\\ ( https://arxiv.org/abs/2402.01665 ,  3338kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01687 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:11:36 GMT   (528kb)

Title: "Which LLM should I use?": Evaluating LLMs for tasks performed by
  Undergraduate Computer Science Students in India
Authors: Vibhor Agarwal, Nakul Thureja, Madhav Krishan Garg, Sahiti
  Dharmavaram, Meghna, Dhruv Kumar
Categories: cs.CY cs.HC cs.LG
Comments: Under review
\\
  This study evaluates the effectiveness of various large language models
(LLMs) in performing tasks common among undergraduate computer science
students. Although a number of research studies in the computing education
community have explored the possibility of using LLMs for a variety of tasks,
there is a lack of comprehensive research comparing different LLMs and
evaluating which LLMs are most effective for different tasks. Our research
systematically assesses some of the publicly available LLMs such as Google
Bard, ChatGPT, GitHub Copilot Chat, and Microsoft Copilot across diverse tasks
commonly encountered by undergraduate computer science students. These tasks
include code generation, explanation, project ideation, content generation,
class assignments, and email composition. Evaluation for these tasks was
carried out by junior and senior students in computer science, and provides
insights into the models' strengths and limitations. This study aims to guide
students in selecting suitable LLMs for any specific task and offers valuable
insights on how LLMs can be used constructively by students and instructors.
\\ ( https://arxiv.org/abs/2402.01687 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01710 (*cross-listing*)
Date: Thu, 25 Jan 2024 15:05:52 GMT   (2082kb)

Title: Exploring Educational Equity: A Machine Learning Approach to Unravel
  Achievement Disparities in Georgia
Authors: Yichen Ma, Dima Nazzal
Categories: cs.CY cs.LG
\\
  The COVID-19 pandemic has significantly exacerbated existing educational
disparities in Georgia's K-12 system, particularly in terms of racial and
ethnic achievement gaps. Utilizing machine learning methods, the study conducts
a comprehensive analysis of student achievement rates across different
demographics, regions, and subjects. The findings highlight a significant
decline in proficiency in English and Math during the pandemic, with a
noticeable contraction in score distribution and a greater impact on
economically disadvantaged and Black students. Socio-economic status, as
represented by the Directly Certified Percentage -- the percentage of students
eligible for free lunch, emerges as the most crucial factor, with additional
insights drawn from faculty resources such as teacher salaries and expenditure
on instruction. The study also identifies disparities in achievement rates
between urban and rural settings, as well as variations across counties,
underscoring the influence of geographical and socio-economic factors. The data
suggests that targeted interventions and resource allocation, particularly in
schools with higher percentages of economically disadvantaged students, are
essential for mitigating educational disparities.
\\ ( https://arxiv.org/abs/2402.01710 ,  2082kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01753 (*cross-listing*)
Date: Tue, 30 Jan 2024 09:17:57 GMT   (508kb,D)

Title: SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and
  Music Synthesis
Authors: Teysir Baoueb (IP Paris, LTCI, IDS, S2A), Haocheng Liu (IP Paris,
  LTCI, IDS, S2A), Mathieu Fontaine (IP Paris, LTCI, IDS, S2A), Jonathan Le
  Roux (MERL), Gael Richard (IP Paris, LTCI, IDS, S2A)
Categories: cs.SD cs.LG eess.AS eess.SP
Comments: Accepted at ICASSP 2024
Journal-ref: IEEE International Conference on Acoustics, Speech and Signal
  Processing, Apr 2024, Seoul (Korea), South Korea
\\
  Generative adversarial network (GAN) models can synthesize highquality audio
signals while ensuring fast sample generation. However, they are difficult to
train and are prone to several issues including mode collapse and divergence.
In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN,
which was initially devised for speech synthesis from mel spectrogram. In our
model, the training stability is enhanced by means of a forward diffusion
process which consists in injecting noise from a Gaussian distribution to both
real and fake samples before inputting them to the discriminator. We further
improve the model by exploiting a spectrally-shaped noise distribution with the
aim to make the discriminator's task more challenging. We then show the merits
of our proposed model for speech and music synthesis on several datasets. Our
experiments confirm that our model compares favorably in audio quality and
efficiency compared to several baselines.
\\ ( https://arxiv.org/abs/2402.01753 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01778 (*cross-listing*)
Date: Thu, 1 Feb 2024 17:54:15 GMT   (3882kb,D)

Title: Introduction to speech recognition
Authors: Gabriel Dauphin
Categories: eess.AS cs.LG
Comments: in French language
\\
  This document contains lectures and practical experimentations using Matlab
and implementing a system which is actually correctly classifying three words
(one, two and three) with the help of a very small database. To achieve this
performance, it uses speech modeling specificities, powerful computer
algorithms (dynamic time warping and Dijktra's algorithm) and machine learning
(nearest neighbor). This document introduces also some machine learning
evaluation metrics.
\\ ( https://arxiv.org/abs/2402.01778 ,  3882kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01779 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:05:47 GMT   (46748kb,D)

Title: Plug-and-Play image restoration with Stochastic deNOising REgularization
Authors: Marien Renaud, Jean Prost, Arthur Leclaire, Nicolas Papadakis
Categories: eess.IV cs.CV cs.LG stat.ML
\\
  Plug-and-Play (PnP) algorithms are a class of iterative algorithms that
address image inverse problems by combining a physical model and a deep neural
network for regularization. Even if they produce impressive image restoration
results, these algorithms rely on a non-standard use of a denoiser on images
that are less and less noisy along the iterations, which contrasts with recent
algorithms based on Diffusion Models (DM), where the denoiser is applied only
on re-noised images. We propose a new PnP framework, called Stochastic
deNOising REgularization (SNORE), which applies the denoiser only on images
with noise of the adequate level. It is based on an explicit stochastic
regularization, which leads to a stochastic gradient descent algorithm to solve
ill-posed inverse problems. A convergence analysis of this algorithm and its
annealing extension is provided. Experimentally, we prove that SNORE is
competitive with respect to state-of-the-art methods on deblurring and
inpainting tasks, both quantitatively and qualitatively.
\\ ( https://arxiv.org/abs/2402.01779 ,  46748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01795 (*cross-listing*)
Date: Fri, 2 Feb 2024 04:47:14 GMT   (184kb,D)

Title: Few-Shot Scenario Testing for Autonomous Vehicles Based on Neighborhood
  Coverage and Similarity
Authors: Shu Li, Jingxuan Yang, Honglin He, Yi Zhang, Jianming Hu, Shuo Feng
Categories: eess.SY cs.LG cs.SY
\\
  Testing and evaluating the safety performance of autonomous vehicles (AVs) is
essential before the large-scale deployment. Practically, the acceptable cost
of testing specific AV model can be restricted within an extremely small limit
because of testing cost or time. With existing testing methods, the limitations
imposed by strictly restricted testing numbers often result in significant
uncertainties or challenges in quantifying testing results. In this paper, we
formulate this problem for the first time the "few-shot testing" (FST) problem
and propose a systematic FST framework to address this challenge. To alleviate
the considerable uncertainty inherent in a small testing scenario set and
optimize scenario utilization, we frame the FST problem as an optimization
problem and search for a small scenario set based on neighborhood coverage and
similarity. By leveraging the prior information on surrogate models (SMs), we
dynamically adjust the testing scenario set and the contribution of each
scenario to the testing result under the guidance of better generalization
ability on AVs. With certain hypotheses on SMs, a theoretical upper bound of
testing error is established to verify the sufficiency of testing accuracy
within given limited number of tests. The experiments of the cut-in scenario
using FST method demonstrate a notable reduction in testing error and variance
compared to conventional testing methods, especially for situations with a
strict limitation on the number of scenarios.
\\ ( https://arxiv.org/abs/2402.01795 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01809 (*cross-listing*)
Date: Fri, 2 Feb 2024 11:35:21 GMT   (1253kb,D)

Title: PhenoLinker: Phenotype-Gene Link Prediction and Explanation using
  Heterogeneous Graph Neural Networks
Authors: Jose L. Mellina Andreu, Luis Bernal, Antonio F. Skarmeta, Mina Ryten,
  Sara \'Alvarez, Alejandro Cisterna Garc\'ia, Juan A. Bot\'ia
Categories: q-bio.GN cs.LG
Comments: 22 pages, 6 figures
ACM-class: I.2.1; J.3
\\
  The association of a given human phenotype to a genetic variant remains a
critical challenge for biology. We present a novel system called PhenoLinker
capable of associating a score to a phenotype-gene relationship by using
heterogeneous information networks and a convolutional neural network-based
model for graphs, which can provide an explanation for the predictions. This
system can aid in the discovery of new associations and in the understanding of
the consequences of human genetic variation.
\\ ( https://arxiv.org/abs/2402.01809 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01810 (*cross-listing*)
Date: Fri, 2 Feb 2024 11:41:21 GMT   (637kb,D)

Title: Misspecification uncertainties in near-deterministic regression
Authors: Thomas D Swinburne and Danny Perez
Categories: stat.ML cs.LG physics.data-an
\\
  The expected loss is an upper bound to the model generalization error which
admits robust PAC-Bayes bounds for learning. However, loss minimization is
known to ignore misspecification, where models cannot exactly reproduce
observations. This leads to significant underestimates of parameter
uncertainties in the large data, or underparameterized, limit. We analyze the
generalization error of near-deterministic, misspecified and underparametrized
surrogate models, a regime of broad relevance in science and engineering. We
show posterior distributions must cover every training point to avoid a
divergent generalization error and derive an ensemble {ansatz} that respects
this constraint, which for linear models incurs minimal overhead. The efficient
approach is demonstrated on model problems before application to high
dimensional datasets in atomistic machine learning. Parameter uncertainties
from misspecification survive in the underparametrized limit, giving accurate
prediction and bounding of test errors.
\\ ( https://arxiv.org/abs/2402.01810 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01824 (*cross-listing*)
Date: Fri, 2 Feb 2024 17:06:03 GMT   (411kb,D)

Title: Identification of Cognitive Decline from Spoken Language through Feature
  Selection and the Bag of Acoustic Words Model
Authors: Marko Niemel\"a and Mikaela von Bonsdorff and Sami \"Ayr\"am\"o and
  Tommi K\"arkk\"ainen
Categories: cs.SD cs.LG
\\
  Memory disorders are a central factor in the decline of functioning and daily
activities in elderly individuals. The confirmation of the illness, initiation
of medication to slow its progression, and the commencement of occupational
therapy aimed at maintaining and rehabilitating cognitive abilities require a
medical diagnosis. The early identification of symptoms of memory disorders,
especially the decline in cognitive abilities, plays a significant role in
ensuring the well-being of populations. Features related to speech production
are known to connect with the speaker's cognitive ability and changes. The lack
of standardized speech tests in clinical settings has led to a growing emphasis
on developing automatic machine learning techniques for analyzing naturally
spoken language. Non-lexical but acoustic properties of spoken language have
proven useful when fast, cost-effective, and scalable solutions are needed for
the rapid diagnosis of a disease. The work presents an approach related to
feature selection, allowing for the automatic selection of the essential
features required for diagnosis from the Geneva minimalistic acoustic parameter
set and relative speech pauses, intended for automatic paralinguistic and
clinical speech analysis. These features are refined into word histogram
features, in which machine learning classifiers are trained to classify control
subjects and dementia patients from the Dementia Bank's Pitt audio database.
The results show that achieving a 75% average classification accuracy with only
twenty-five features with the separate ADReSS 2020 competition test data and
the Leave-One-Subject-Out cross-validation of the entire competition data is
possible. The results rank at the top compared to international research, where
the same dataset and only acoustic features have been used to diagnose
patients.
\\ ( https://arxiv.org/abs/2402.01824 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01831 (*cross-listing*)
Date: Fri, 2 Feb 2024 18:58:34 GMT   (1143kb,D)

Title: Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and
  Dialogue Abilities
Authors: Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle,
  Bryan Catanzaro
Categories: cs.SD cs.LG eess.AS
\\
  Augmenting large language models (LLMs) to understand audio -- including
non-speech sounds and non-verbal speech -- is critically important for diverse
real-world applications of LLMs. In this paper, we propose Audio Flamingo, a
novel audio language model with 1) strong audio understanding abilities, 2) the
ability to quickly adapt to unseen tasks via in-context learning and retrieval,
and 3) strong multi-turn dialogue abilities. We introduce a series of training
techniques, architecture design, and data strategies to enhance our model with
these abilities. Extensive evaluations across various audio understanding tasks
confirm the efficacy of our method, setting new state-of-the-art benchmarks.
\\ ( https://arxiv.org/abs/2402.01831 ,  1143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01855 (*cross-listing*)
Date: Fri, 2 Feb 2024 19:18:12 GMT   (7034kb,D)

Title: SPDE priors for uncertainty quantification of end-to-end neural data
  assimilation schemes
Authors: Maxime Beauchamp, Nicolas Desassis, J. Emmanuel Johnson, Simon
  Benaichouche, Pierre Tandeo and Ronan Fablet
Categories: stat.ML cs.LG eess.IV
\\
  The spatio-temporal interpolation of large geophysical datasets has
historically been adressed by Optimal Interpolation (OI) and more sophisticated
model-based or data-driven DA techniques. In the last ten years, the link
established between Stochastic Partial Differential Equations (SPDE) and
Gaussian Markov Random Fields (GMRF) opened a new way of handling both large
datasets and physically-induced covariance matrix in Optimal Interpolation.
Recent advances in the deep learning community also enables to adress this
problem as neural architecture embedding data assimilation variational
framework. The reconstruction task is seen as a joint learning problem of the
prior involved in the variational inner cost and the gradient-based
minimization of the latter: both prior models and solvers are stated as neural
networks with automatic differentiation which can be trained by minimizing a
loss function, typically stated as the mean squared error between some ground
truth and the reconstruction. In this work, we draw from the SPDE-based
Gaussian Processes to estimate complex prior models able to handle
non-stationary covariances in both space and time and provide a stochastic
framework for interpretability and uncertainty quantification. Our neural
variational scheme is modified to embed an augmented state formulation with
both state and SPDE parametrization to estimate. Instead of a neural prior, we
use a stochastic PDE as surrogate model along the data assimilation window. The
training involves a loss function for both reconstruction task and SPDE prior
model, where the likelihood of the SPDE parameters given the true states is
involved in the training. Because the prior is stochastic, we can easily draw
samples in the prior distribution before conditioning to provide a flexible way
to estimate the posterior distribution based on thousands of members.
\\ ( https://arxiv.org/abs/2402.01855 ,  7034kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01876 (*cross-listing*)
Date: Fri, 2 Feb 2024 20:02:12 GMT   (3411kb,D)

Title: Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC
Authors: Patrick Odagiu, Zhiqiang Que, Javier Duarte, Johannes Haller, Gregor
  Kasieczka, Artur Lobanov, Vladimir Loncar, Wayne Luk, Jennifer Ngadiuba,
  Maurizio Pierini, Philipp Rincke, Arpita Seksaria, Sioni Summers, Andre
  Sznajder, Alexander Tapper, Thea K. Aarrestad
Categories: hep-ex cs.LG physics.ins-det
Comments: 13 pages, 3 figures, 3 tables
Report-no: FERMILAB-PUB-24-0030-CMS-CSAID-PPD
\\
  We study various machine learning based algorithms for performing accurate
jet flavor classification on field-programmable gate arrays and demonstrate how
latency and resource consumption scale with the input size and choice of
algorithm. These architectures provide an initial design for models that could
be used for tagging at the CERN LHC during its high-luminosity phase. The
high-luminosity upgrade will lead to a five-fold increase in its instantaneous
luminosity for proton-proton collisions and, in turn, higher data volume and
complexity, such as the availability of jet constituents. Through
quantization-aware training and efficient hardware implementations, we show
that O(100) ns inference of complex architectures such as deep sets and
interaction networks is feasible at a low computational resource cost.
\\ ( https://arxiv.org/abs/2402.01876 ,  3411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01887 (*cross-listing*)
Date: Fri, 2 Feb 2024 20:22:44 GMT   (893kb,D)

Title: On f-Divergence Principled Domain Adaptation: An Improved Framework
Authors: Ziqiao Wang, Yongyi Mao
Categories: stat.ML cs.LG
\\
  Unsupervised domain adaptation (UDA) plays a crucial role in addressing
distribution shifts in machine learning. In this work, we improve the
theoretical foundations of UDA proposed by Acuna et al. (2021) by refining
their f-divergence-based discrepancy and additionally introducing a new
measure, f-domain discrepancy (f-DD). By removing the absolute value function
and incorporating a scaling parameter, f-DD yields novel target error and
sample complexity bounds, allowing us to recover previous KL-based results and
bridging the gap between algorithms and theory presented in Acuna et al.
(2021). Leveraging a localization technique, we also develop a fast-rate
generalization bound. Empirical results demonstrate the superior performance of
f-DD-based domain learning algorithms over previous works in popular UDA
benchmarks.
\\ ( https://arxiv.org/abs/2402.01887 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01900 (*cross-listing*)
Date: Fri, 2 Feb 2024 20:59:29 GMT   (636kb,D)

Title: Distributional Off-policy Evaluation with Bellman Residual Minimization
Authors: Sungee Hong, Zhengling Qi, Raymond K. W. Wong
Categories: stat.ML cs.LG
\\
  We consider the problem of distributional off-policy evaluation which serves
as the foundation of many distributional reinforcement learning (DRL)
algorithms. In contrast to most existing works (that rely on supremum-extended
statistical distances such as supremum-Wasserstein distance), we study the
expectation-extended statistical distance for quantifying the distributional
Bellman residuals and show that it can upper bound the expected error of
estimating the return distribution. Based on this appealing property, by
extending the framework of Bellman residual minimization to DRL, we propose a
method called Energy Bellman Residual Minimizer (EBRM) to estimate the return
distribution. We establish a finite-sample error bound for the EBRM estimator
under the realizability assumption. Furthermore, we introduce a variant of our
method based on a multi-step bootstrapping procedure to enable multi-step
extension. By selecting an appropriate step level, we obtain a better error
bound for this variant of EBRM compared to a single-step EBRM, under some
non-realizability settings. Finally, we demonstrate the superior performance of
our method through simulation studies, comparing with several existing methods.
\\ ( https://arxiv.org/abs/2402.01900 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01930 (*cross-listing*)
Date: Fri, 2 Feb 2024 21:58:26 GMT   (22156kb,D)

Title: Reducing Optimism Bias in Incomplete Cooperative Games
Authors: Filip \'Uradn\'ik, David Sychrovsk\'y, Jakub \v{C}ern\'y and Martin
  \v{C}ern\'y
Categories: cs.GT cs.LG
Comments: Proc. of the 23rd International Conference on Autonomous Agents and
  Multiagent Systems (AAMAS 2024)
\\
  Cooperative game theory has diverse applications in contemporary artificial
intelligence, including domains like interpretable machine learning, resource
allocation, and collaborative decision-making. However, specifying a
cooperative game entails assigning values to exponentially many coalitions, and
obtaining even a single value can be resource-intensive in practice. Yet simply
leaving certain coalition values undisclosed introduces ambiguity regarding
individual contributions to the collective grand coalition. This ambiguity
often leads to players holding overly optimistic expectations, stemming from
either inherent biases or strategic considerations, frequently resulting in
collective claims exceeding the actual grand coalition value. In this paper, we
present a framework aimed at optimizing the sequence for revealing coalition
values, with the overarching goal of efficiently closing the gap between
players' expectations and achievable outcomes in cooperative games. Our
contributions are threefold: (i) we study the individual players' optimistic
completions of games with missing coalition values along with the arising gap,
and investigate its analytical characteristics that facilitate more efficient
optimization; (ii) we develop methods to minimize this gap over classes of
games with a known prior by disclosing values of additional coalitions in both
offline and online fashion; and (iii) we empirically demonstrate the
algorithms' performance in practical scenarios, together with an investigation
into the typical order of revealing coalition values.
\\ ( https://arxiv.org/abs/2402.01930 ,  22156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01972 (*cross-listing*)
Date: Sat, 3 Feb 2024 00:47:50 GMT   (578kb,D)

Title: Combining T-learning and DR-learning: a framework for oracle-efficient
  estimation of causal contrasts
Authors: Lars van der Laan, Marco Carone, Alex Luedtke
Categories: stat.ML cs.LG stat.ME
\\
  We introduce efficient plug-in (EP) learning, a novel framework for the
estimation of heterogeneous causal contrasts, such as the conditional average
treatment effect and conditional relative risk. The EP-learning framework
enjoys the same oracle-efficiency as Neyman-orthogonal learning strategies,
such as DR-learning and R-learning, while addressing some of their primary
drawbacks, including that (i) their practical applicability can be hindered by
loss function non-convexity; and (ii) they may suffer from poor performance and
instability due to inverse probability weighting and pseudo-outcomes that
violate bounds. To avoid these drawbacks, EP-learner constructs an efficient
plug-in estimator of the population risk function for the causal contrast,
thereby inheriting the stability and robustness properties of plug-in
estimation strategies like T-learning. Under reasonable conditions, EP-learners
based on empirical risk minimization are oracle-efficient, exhibiting
asymptotic equivalence to the minimizer of an oracle-efficient one-step
debiased estimator of the population risk function. In simulation experiments,
we illustrate that EP-learners of the conditional average treatment effect and
conditional relative risk outperform state-of-the-art competitors, including
T-learner, R-learner, and DR-learner. Open-source implementations of the
proposed methods are available in our R package hte3.
\\ ( https://arxiv.org/abs/2402.01972 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02034 (*cross-listing*)
Date: Sat, 3 Feb 2024 05:15:19 GMT   (568kb,D)

Title: Universal Post-Training Reverse-Engineering Defense Against Backdoors in
  Deep Neural Networks
Authors: Xi Li, Hang Wang, David J. Miller and George Kesidis
Categories: cs.CR cs.LG cs.NE
\\
  A variety of defenses have been proposed against backdoors attacks on deep
neural network (DNN) classifiers. Universal methods seek to reliably detect
and/or mitigate backdoors irrespective of the incorporation mechanism used by
the attacker, while reverse-engineering methods often explicitly assume one. In
this paper, we describe a new detector that: relies on internal feature map of
the defended DNN to detect and reverse-engineer the backdoor and identify its
target class; can operate post-training (without access to the training
dataset); is highly effective for various incorporation mechanisms (i.e., is
universal); and which has low computational overhead and so is scalable. Our
detection approach is evaluated for different attacks on a benchmark CIFAR-10
image classifier.
\\ ( https://arxiv.org/abs/2402.02034 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02041 (*cross-listing*)
Date: Sat, 3 Feb 2024 05:33:01 GMT   (308kb)

Title: $\alpha$-Divergence Loss Function for Neural Density Ratio Estimation
Authors: Yoshiaki Kitazawa
Categories: stat.ML cs.LG
\\
  Recently, neural networks have produced state-of-the-art results for
density-ratio estimation (DRE), a fundamental technique in machine learning.
However, existing methods bear optimization issues that arise from the loss
functions of DRE: a large sample requirement of Kullback--Leibler
(KL)-divergence, vanishing of train loss gradients, and biased gradients of the
loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that
offers concise implementation and stable optimization is proposed in this
paper. Furthermore, technical justifications for the proposed loss function are
presented. The stability of the proposed loss function is empirically
demonstrated and the estimation accuracy of DRE tasks is investigated.
Additionally, this study presents a sample requirement for DRE using the
proposed loss function in terms of the upper bound of $L_1$ error, which
connects a curse of dimensionality as a common problem in high-dimensional DRE
tasks.
\\ ( https://arxiv.org/abs/2402.02041 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02047 (*cross-listing*)
Date: Sat, 3 Feb 2024 05:52:28 GMT   (491kb,D)

Title: Quality and Trust in LLM-generated Code
Authors: Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md
  Rafiqul Islam Rabin, Susmit Jha, Prem Devanbu, Toufique Ahmed
Categories: cs.SE cs.LG
\\
  Machine learning models are widely used but can also often be wrong. Users
would benefit from a reliable indication of whether a given output from a given
model should be trusted, so a rational decision can be made whether to use the
output or not. For example, outputs can be associated with a confidence
measure; if this confidence measure is strongly associated with likelihood of
correctness, then the model is said to be well-calibrated. In this case, for
example, high-confidence outputs could be safely accepted, and low-confidence
outputs rejected.
  Calibration has so far been studied in non-generative (e.g., classification)
settings, especially in Software Engineering. However, generated code can quite
often be wrong: Developers need to know when they should e.g., directly use,
use after careful review, or discard model-generated code; thus Calibration is
vital in generative settings. However, the notion of correctness of generated
code is non-trivial, and thus so is Calibration. In this paper we make several
contributions. We develop a framework for evaluating the Calibration of
code-generating models. We consider several tasks, correctness criteria,
datasets, and approaches, and find that by and large generative code models are
not well-calibrated out of the box. We then show how Calibration can be
improved, using standard methods such as Platt scaling. Our contributions will
lead to better-calibrated decision-making in the current use of code generated
by language models, and offers a framework for future research to further
improve calibration methods for generative models in Software Engineering.
\\ ( https://arxiv.org/abs/2402.02047 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02098 (*cross-listing*)
Date: Sat, 3 Feb 2024 09:35:53 GMT   (896kb,D)

Title: Self-attention Networks Localize When QK-eigenspectrum Concentrates
Authors: Han Bao, Ryuichiro Hataya, Ryo Karakida
Categories: stat.ML cs.LG
\\
  The self-attention mechanism prevails in modern machine learning. It has an
interesting functionality of adaptively selecting tokens from an input sequence
by modulating the degree of attention localization, which many researchers
speculate is the basis of the powerful model performance but complicates the
underlying mechanism of the learning dynamics. In recent years, mainly two
arguments have connected attention localization to the model performances. One
is the rank collapse, where the embedded tokens by a self-attention block
become very similar across different tokens, leading to a less expressive
network. The other is the entropy collapse, where the attention probability
approaches non-uniform and entails low entropy, making the learning dynamics
more likely to be trapped in plateaus. These two failure modes may apparently
contradict each other because the rank and entropy collapses are relevant to
uniform and non-uniform attention, respectively. To this end, we characterize
the notion of attention localization by the eigenspectrum of query-key
parameter matrices and reveal that a small eigenspectrum variance leads
attention to be localized. Interestingly, the small eigenspectrum variance
prevents both rank and entropy collapse, leading to better model expressivity
and trainability.
\\ ( https://arxiv.org/abs/2402.02098 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02103 (*cross-listing*)
Date: Sat, 3 Feb 2024 09:55:35 GMT   (8413kb,D)

Title: D\'ej\`a Vu Memorization in Vision-Language Models
Authors: Bargav Jayaraman, Chuan Guo, Kamalika Chaudhuri
Categories: cs.CV cs.LG
\\
  Vision-Language Models (VLMs) have emerged as the state-of-the-art
representation learning solution, with myriads of downstream applications such
as image classification, retrieval and generation. A natural question is
whether these models memorize their training data, which also has implications
for generalization. We propose a new method for measuring memorization in VLMs,
which we call d\'ej\`a vu memorization. For VLMs trained on image-caption
pairs, we show that the model indeed retains information about individual
objects in the training images beyond what can be inferred from correlations or
the image caption. We evaluate d\'ej\`a vu memorization at both sample and
population level, and show that it is significant for OpenCLIP trained on as
many as 50M image-caption pairs. Finally, we show that text randomization
considerably mitigates memorization while only moderately impacting the model's
downstream task performance.
\\ ( https://arxiv.org/abs/2402.02103 ,  8413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02111 (*cross-listing*)
Date: Sat, 3 Feb 2024 10:24:30 GMT   (298kb,D)

Title: Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo
  is All you Need
Authors: Shangda Yang, Vitaly Zankin, Maximilian Balandat, Stefan Scherer,
  Kevin Carlberg, Neil Walton, and Kody J. H. Law
Categories: stat.ML cs.LG math.OC math.PR stat.CO stat.ME
\\
  We leverage multilevel Monte Carlo (MLMC) to improve the performance of
multi-step look-ahead Bayesian optimization (BO) methods that involve nested
expectations and maximizations. The complexity rate of naive Monte Carlo
degrades for nested operations, whereas MLMC is capable of achieving the
canonical Monte Carlo convergence rate for this type of problem, independently
of dimension and without any smoothness assumptions. Our theoretical study
focuses on the approximation improvements for one- and two-step look-ahead
acquisition functions, but, as we discuss, the approach is generalizable in
various ways, including beyond the context of BO. Findings are verified
numerically and the benefits of MLMC for BO are illustrated on several
benchmark examples. Code is available here
https://github.com/Shangda-Yang/MLMCBO.
\\ ( https://arxiv.org/abs/2402.02111 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02121 (*cross-listing*)
Date: Sat, 3 Feb 2024 11:07:50 GMT   (7664kb,D)

Title: Enhancing crop classification accuracy by synthetic SAR-Optical data
  generation using deep learning
Authors: Ali Mirzaei, Hossein Bagheri, and Iman Khosravi
Categories: cs.CV cs.LG
Journal-ref: ISPRS Int. J. Geo-Inf. 2023, 12(11), 450
DOI: 10.3390/ijgi12110450
\\
  Crop classification using remote sensing data has emerged as a prominent
research area in recent decades. Studies have demonstrated that fusing SAR and
optical images can significantly enhance the accuracy of classification.
However, a major challenge in this field is the limited availability of
training data, which adversely affects the performance of classifiers. In
agricultural regions, the dominant crops typically consist of one or two
specific types, while other crops are scarce. Consequently, when collecting
training samples to create a map of agricultural products, there is an
abundance of samples from the dominant crops, forming the majority classes.
Conversely, samples from other crops are scarce, representing the minority
classes. Addressing this issue requires overcoming several challenges and
weaknesses associated with traditional data generation methods. These methods
have been employed to tackle the imbalanced nature of the training data.
Nevertheless, they still face limitations in effectively handling the minority
classes. Overall, the issue of inadequate training data, particularly for
minority classes, remains a hurdle that traditional methods struggle to
overcome. In this research, We explore the effectiveness of conditional tabular
generative adversarial network (CTGAN) as a synthetic data generation method
based on a deep learning network, in addressing the challenge of limited
training data for minority classes in crop classification using the fusion of
SAR-optical data. Our findings demonstrate that the proposed method generates
synthetic data with higher quality that can significantly increase the number
of samples for minority classes leading to better performance of crop
classifiers.
\\ ( https://arxiv.org/abs/2402.02121 ,  7664kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02149 (*cross-listing*)
Date: Sat, 3 Feb 2024 13:35:39 GMT   (39505kb,D)

Title: Improving Diffusion Models for Inverse Problems Using Optimal Posterior
  Covariance
Authors: Xinyu Peng, Ziyang Zheng, Wenrui Dai, Nuoqian Xiao, Chenglin Li, Junni
  Zou, Hongkai Xiong
Categories: cs.CV cs.LG
\\
  Recent diffusion models provide a promising zero-shot solution to noisy
linear inverse problems without retraining for specific inverse problems. In
this paper, we propose the first unified interpretation for existing zero-shot
methods from the perspective of approximating the conditional posterior mean
for the reverse diffusion process of conditional sampling. We reveal that
recent methods are equivalent to making isotropic Gaussian approximations to
intractable posterior distributions over clean images given diffused noisy
images, with the only difference in the handcrafted design of isotropic
posterior covariances. Inspired by this finding, we propose a general
plug-and-play posterior covariance optimization based on maximum likelihood
estimation to improve recent methods. To achieve optimal posterior covariance
without retraining, we provide general solutions based on two approaches
specifically designed to leverage pre-trained models with and without reverse
covariances. Experimental results demonstrate that the proposed methods
significantly enhance the overall performance or robustness to hyperparameters
of recent methods. Code is available at
https://github.com/xypeng9903/k-diffusion-inverse-problems
\\ ( https://arxiv.org/abs/2402.02149 ,  39505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02152 (*cross-listing*)
Date: Sat, 3 Feb 2024 13:46:28 GMT   (46kb)

Title: Position Paper: Why the Shooting in the Dark Method Dominates
  Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
Authors: David Rohde
Categories: cs.IR cs.LG stat.ML
Comments: 11 pages
\\
  Applied recommender systems research is in a curious position. While there is
a very rigorous protocol for measuring performance by A/B testing, best
practice for finding a `B' to test does not explicitly target performance but
rather targets a proxy measure. The success or failure of a given A/B test then
depends entirely on if the proposed proxy is better correlated to performance
than the previous proxy. No principle exists to identify if one proxy is better
than another offline, leaving the practitioners shooting in the dark. The
purpose of this position paper is to question this anti-Utopian thinking and
argue that a non-standard use of the deep learning stacks actually has the
potential to unlock reward optimizing recommendation.
\\ ( https://arxiv.org/abs/2402.02152 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02154 (*cross-listing*)
Date: Sat, 3 Feb 2024 13:48:57 GMT   (10148kb,D)

Title: Evaluating the Robustness of Off-Road Autonomous Driving Segmentation
  against Adversarial Attacks: A Dataset-Centric analysis
Authors: Pankaj Deoli, Rohit Kumar, Axel Vierling, Karsten Berns
Categories: cs.CV cs.LG
Comments: 8 pages
\\
  This study investigates the vulnerability of semantic segmentation models to
adversarial input perturbations, in the domain of off- road autonomous driving.
Despite good performance in generic conditions, the state-of-the-art
classifiers are often susceptible to (even) small perturbations, ultimately
resulting in inaccurate predic- tions with high confidence. Prior research has
directed their focus on making models more robust by modifying the architecture
and training with noisy input images, but has not explored the influence of
datasets in adversarial attacks. Our study aims to address this gap by
examining the impact of non-robust features in off-road datasets and comparing
the effects of adversarial attacks on different seg- mentation network
architectures. To enable this, a robust dataset is created consisting of only
robust features and training the net- works on this robustified dataset. We
present both qualitative and quantitative analysis of our findings, which have
important impli- cations on improving the robustness of machine learning models
in off-road autonomous driving applications. Additionally, this work
contributes to the safe navigation of autonomous robot Unimog U5023 in rough
off-road unstructured environments by evaluating the robustness of segmentation
outputs. The code is publicly avail- able at https:// github.com/ rohtkumar/
adversarial_attacks_ on_segmentation
\\ ( https://arxiv.org/abs/2402.02154 ,  10148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02162 (*cross-listing*)
Date: Sat, 3 Feb 2024 14:23:36 GMT   (1016kb,D)

Title: A Bayesian cluster validity index
Authors: Nathakhun Wiroonsri and Onthada Preedasawakul
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 23 pages
MSC-class: 62H30 (Primary) 62F15, 68T10 (Secondary)
\\
  Selecting the number of clusters is one of the key processes when applying
clustering algorithms. To fulfill this task, various cluster validity indices
(CVIs) have been introduced. Most of the cluster validity indices are defined
to detect the optimal number of clusters hidden in a dataset. However, users
sometimes do not expect to get the optimal number of groups but a secondary one
which is more reasonable for their applications. This has motivated us to
introduce a Bayesian cluster validity index (BCVI) based on existing underlying
indices. This index is defined based on either Dirichlet or Generalized
Dirichlet priors which result in the same posterior distribution. Our BCVI is
then tested based on the Wiroonsri index (WI), and the Wiroonsri-Preedasawakul
index (WP) as underlying indices for hard and soft clustering, respectively. We
compare their outcomes with the original underlying indices, as well as a few
more existing CVIs including Davies and Bouldin (DB), Starczewski (STR), Xie
and Beni (XB), and KWON2 indices. Our proposed BCVI clearly benefits the use of
CVIs when experiences matter where users can specify their expected range of
the final number of clusters. This aspect is emphasized by our experiment
classified into three different cases. Finally, we present some applications to
real-world datasets including MRI brain tumor images. Our tools will be added
to a new version of the recently developed R package ``UniversalCVI''.
\\ ( https://arxiv.org/abs/2402.02162 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02171 (*cross-listing*)
Date: Sat, 3 Feb 2024 14:38:09 GMT   (7793kb,D)

Title: Off-Policy Evaluation of Slate Bandit Policies via Optimizing
  Abstraction
Authors: Haruka Kiyohara, Masahiro Nomura, Yuta Saito
Categories: stat.ML cs.LG
Comments: WebConf2024
\\
  We study off-policy evaluation (OPE) in the problem of slate contextual
bandits where a policy selects multi-dimensional actions known as slates. This
problem is widespread in recommender systems, search engines, marketing, to
medical applications, however, the typical Inverse Propensity Scoring (IPS)
estimator suffers from substantial variance due to large action spaces, making
effective OPE a significant challenge. The PseudoInverse (PI) estimator has
been introduced to mitigate the variance issue by assuming linearity in the
reward function, but this can result in significant bias as this assumption is
hard-to-verify from observed data and is often substantially violated. To
address the limitations of previous estimators, we develop a novel estimator
for OPE of slate bandits, called Latent IPS (LIPS), which defines importance
weights in a low-dimensional slate abstraction space where we optimize slate
abstractions to minimize the bias and variance of LIPS in a data-driven way. By
doing so, LIPS can substantially reduce the variance of IPS without imposing
restrictive assumptions on the reward function structure like linearity.
Through empirical evaluation, we demonstrate that LIPS substantially
outperforms existing estimators, particularly in scenarios with non-linear
rewards and large slate spaces.
\\ ( https://arxiv.org/abs/2402.02171 ,  7793kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02182 (*cross-listing*)
Date: Sat, 3 Feb 2024 15:14:51 GMT   (3275kb,D)

Title: Diffusion Cross-domain Recommendation
Authors: Yuner Xuan
Categories: cs.IR cs.LG
\\
  It is always a challenge for recommender systems to give high-quality
outcomes to cold-start users. One potential solution to alleviate the data
sparsity problem for cold-start users in the target domain is to add data from
the auxiliary domain. Finding a proper way to extract knowledge from an
auxiliary domain and transfer it into a target domain is one of the main
objectives for cross-domain recommendation (CDR) research. Among the existing
methods, mapping approach is a popular one to implement cross-domain
recommendation models (CDRs). For models of this type, a mapping module plays
the role of transforming data from one domain to another. It primarily
determines the performance of mapping approach CDRs. Recently, diffusion
probability models (DPMs) have achieved impressive success for image synthesis
related tasks. They involve recovering images from noise-added samples, which
can be viewed as a data transformation process with outstanding performance. To
further enhance the performance of CDRs, we first reveal the potential
connection between DPMs and mapping modules of CDRs, and then propose a novel
CDR model named Diffusion Cross-domain Recommendation (DiffCDR). More
specifically, we first adopt the theory of DPM and design a Diffusion Module
(DIM), which generates user's embedding in target domain. To reduce the
negative impact of randomness introduced in DIM and improve the stability, we
employ an Alignment Module to produce the aligned user embeddings. In addition,
we consider the label data of the target domain and form the task-oriented loss
function, which enables our DiffCDR to adapt to specific tasks. By conducting
extensive experiments on datasets collected from reality, we demonstrate the
effectiveness and adaptability of DiffCDR to outperform baseline models on
various CDR tasks in both cold-start and warm-start scenarios.
\\ ( https://arxiv.org/abs/2402.02182 ,  3275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02190 (*cross-listing*)
Date: Sat, 3 Feb 2024 15:31:05 GMT   (4707kb,D)

Title: Continuous Tensor Relaxation for Finding Diverse Solutions in
  Combinatorial Optimization Problems
Authors: Yuma Ichikawa, Hiroaki Iwashita
Categories: stat.ML cs.LG stat.CO stat.ME
Comments: 16 pages, 10 figures
\\
  Finding the best solution is the most common objective in combinatorial
optimization (CO) problems. However, a single solution may not be suitable in
practical scenarios, as the objective functions and constraints are only
approximations of original real-world situations. To tackle this, finding (i)
"heterogeneous solutions", diverse solutions with distinct characteristics, and
(ii) "penalty-diversified solutions", variations in constraint severity, are
natural directions. This strategy provides the flexibility to select a suitable
solution during post-processing. However, discovering these diverse solutions
is more challenging than identifying a single solution. To overcome this
challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA)
for unsupervised-learning-based CO solvers. CTRA addresses various problems
simultaneously by extending the continual relaxation approach, which transforms
discrete decision variables into continual tensors. This method finds
heterogeneous and penalty-diversified solutions through mutual interactions,
where the choice of one solution affects the other choices. Numerical
experiments show that CTRA enables UL-based solvers to find heterogeneous and
penalty-diversified solutions much faster than existing UL-based solvers.
Moreover, these experiments reveal that CTRA enhances the exploration ability.
\\ ( https://arxiv.org/abs/2402.02190 ,  4707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02196 (*cross-listing*)
Date: Sat, 3 Feb 2024 15:56:03 GMT   (983kb,D)

Title: Sample-Efficient Clustering and Conquer Procedures for Parallel
  Large-Scale Ranking and Selection
Authors: Zishi Zhang, Yijie Peng
Categories: stat.ME cs.LG
\\
  We propose novel "clustering and conquer" procedures for the parallel
large-scale ranking and selection (R&S) problem, which leverage correlation
information for clustering to break the bottleneck of sample efficiency. In
parallel computing environments, correlation-based clustering can achieve an
$\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal
reduction rate theoretically attainable. Our proposed framework is versatile,
allowing for seamless integration of various prevalent R&S methods under both
fixed-budget and fixed-precision paradigms. It can achieve improvements without
the necessity of highly accurate correlation estimation and precise clustering.
In large-scale AI applications such as neural architecture search, a
screening-free version of our procedure surprisingly surpasses fully-sequential
benchmarks in terms of sample efficiency. This suggests that leveraging
valuable structural information, such as correlation, is a viable path to
bypassing the traditional need for screening via pairwise comparison--a step
previously deemed essential for high sample efficiency but problematic for
parallelization. Additionally, we propose a parallel few-shot clustering
algorithm tailored for large-scale problems.
\\ ( https://arxiv.org/abs/2402.02196 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02198 (*cross-listing*)
Date: Sat, 3 Feb 2024 16:03:17 GMT   (8285kb)

Title: Multimodal Co-orchestration for Exploring Structure-Property
  Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization
Authors: Boris N. Slautin, Utkarsh Pratiush, Ilia N. Ivanov, Yongtao Liu, Rohit
  Pant, Xiaohang Zhang, Ichiro Takeuchi, Maxim A. Ziatdinov and Sergei V.
  Kalinin
Categories: cond-mat.mtrl-sci cs.LG
Comments: 22 pages, 9 figures
\\
  The rapid growth of automated and autonomous instrumentations brings forth an
opportunity for the co-orchestration of multimodal tools, equipped with
multiple sequential detection methods, or several characterization tools to
explore identical samples. This can be exemplified by the combinatorial
libraries that can be explored in multiple locations by multiple tools
simultaneously, or downstream characterization in automated synthesis systems.
In the co-orchestration approaches, information gained in one modality should
accelerate the discovery of other modalities. Correspondingly, the
orchestrating agent should select the measurement modality based on the
anticipated knowledge gain and measurement cost. Here, we propose and implement
a co-orchestration approach for conducting measurements with complex
observables such as spectra or images. The method relies on combining
dimensionality reduction by variational autoencoders with representation
learning for control over the latent space structure, and integrated into
iterative workflow via multi-task Gaussian Processes (GP). This approach
further allows for the native incorporation of the system's physics via a
probabilistic model as a mean function of the GP. We illustrated this method
for different modalities of piezoresponse force microscopy and micro-Raman on
combinatorial $Sm-BiFeO_3$ library. However, the proposed framework is general
and can be extended to multiple measurement modalities and arbitrary
dimensionality of measured signals. The analysis code that supports the funding
is publicly available at https://github.com/Slautin/2024_Co-orchestration.
\\ ( https://arxiv.org/abs/2402.02198 ,  8285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02208 (*cross-listing*)
Date: Sat, 3 Feb 2024 16:44:25 GMT   (22082kb,D)

Title: Implicit Neural Representation of Tileable Material Textures
Authors: Hallison Paz, Tiago Novello, Luiz Velho
Categories: cs.CV cs.GR cs.LG
\\
  We explore sinusoidal neural networks to represent periodic tileable
textures. Our approach leverages the Fourier series by initializing the first
layer of a sinusoidal neural network with integer frequencies with a period
$P$. We prove that the compositions of sinusoidal layers generate only integer
frequencies with period $P$. As a result, our network learns a continuous
representation of a periodic pattern, enabling direct evaluation at any spatial
coordinate without the need for interpolation. To enforce the resulting pattern
to be tileable, we add a regularization term, based on the Poisson equation, to
the loss function. Our proposed neural implicit representation is compact and
enables efficient reconstruction of high-resolution textures with high visual
fidelity and sharpness across multiple levels of detail. We present
applications of our approach in the domain of anti-aliased surface.
\\ ( https://arxiv.org/abs/2402.02208 ,  22082kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02242 (*cross-listing*)
Date: Sat, 3 Feb 2024 19:12:20 GMT   (99kb,D)

Title: Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
Authors: Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing
  Li, Yuntao Du
Categories: cs.CV cs.LG
Comments: Submitted to IJCAI 2024
\\
  Large-scale pre-trained vision models (PVMs) have shown great potential for
adaptability across various downstream vision tasks. However, with
state-of-the-art PVMs growing to billions or even trillions of parameters, the
standard full fine-tuning paradigm is becoming unsustainable due to high
computational and storage demands. In response, researchers are exploring
parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance
of full fine-tuning with minimal parameter modifications. This survey provides
a comprehensive overview and future directions for visual PEFT, offering a
systematic review of the latest advancements. First, we provide a formal
definition of PEFT and discuss model pre-training methods. We then categorize
existing methods into three categories: addition-based, partial-based, and
unified-based. Finally, we introduce the commonly used datasets and
applications and suggest potential future research challenges. A comprehensive
collection of resources is available at
https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning.
\\ ( https://arxiv.org/abs/2402.02242 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02245 (*cross-listing*)
Date: Sat, 3 Feb 2024 19:24:40 GMT   (42655kb,D)

Title: Revisiting Generative Adversarial Networks for Binary Semantic
  Segmentation on Imbalanced Datasets
Authors: Lei Xu and Moncef Gabbouj
Categories: cs.CV cs.LG
\\
  Anomalous pavement surface conditions detection aims to detect pixels
representing anomalous states, such as cracks, on pavement surface images
automatically by algorithms. Recently, deep learning models have been
intensively applied to related topics with outstanding performance. However,
most existing deep learning-related solutions rarely achieve a stable
performance on diverse datasets. To address this issue, in this work, we
propose a deep learning framework based on conditional Generative Adversarial
Networks for anomalous region detection on pavement images at the pixel level.
In particular, the proposed framework is developed to enhance the generator's
ability to estimate the probability feature map from heterogeneous inputs with
two training stages and multiscale feature representation. Moreover, several
attention mechanisms are incorporated into the proposed framework to mitigate
the performance deterioration of model training on severely imbalanced
datasets. We implement experiments on six accessible pavement datasets.
Extensive qualitative and quantitative experiments demonstrate that the
proposed framework can achieve SOTA results on these datasets efficiently and
robustly.
\\ ( https://arxiv.org/abs/2402.02245 ,  42655kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02290 (*cross-listing*)
Date: Sat, 3 Feb 2024 23:04:32 GMT   (6108kb,D)

Title: Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package
  in R and Python
Authors: Giovanni Saraceno and Marianthi Markatou and Raktim Mukhopadhyay and
  Mojgan Golzy
Categories: stat.CO cs.LG cs.MS stat.AP stat.ML
Comments: 54 pages, 26 figures
\\
  We introduce the QuadratiK package that incorporates innovative data analysis
methodologies. The presented software, implemented in both R and Python, offers
a comprehensive set of goodness-of-fit tests and clustering techniques using
kernel-based quadratic distances, thereby bridging the gap between the
statistical and machine learning literatures. Our software implements one, two
and k-sample tests for goodness of fit, providing an efficient and
mathematically sound way to assess the fit of probability distributions.
Expanded capabilities of our software include supporting tests for uniformity
on the $d$-dimensional Sphere based on Poisson kernel densities, and algorithms
for generating random samples from Poisson kernel densities. Particularly
noteworthy is the incorporation of a unique clustering algorithm specifically
tailored for spherical data that leverages a mixture of Poisson-kernel-based
densities on the sphere. Alongside this, our software includes additional
graphical functions, aiding the users in validating, as well as visualizing and
representing clustering results. This enhances interpretability and usability
of the analysis. In summary, our R and Python packages serve as a powerful
suite of tools, offering researchers and practitioners the means to delve
deeper into their data, draw robust inference, and conduct potentially
impactful analyses and inference across a wide array of disciplines.
\\ ( https://arxiv.org/abs/2402.02290 ,  6108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02297 (*cross-listing*)
Date: Sat, 3 Feb 2024 23:19:26 GMT   (2455kb,D)

Title: Denoising Diffusion-Based Control of Nonlinear Systems
Authors: Karthik Elamvazhuthi, Darshan Gadginmath, Fabio Pasqualetti
Categories: math.OC cs.LG cs.RO cs.SY eess.SY
\\
  We propose a novel approach based on Denoising Diffusion Probabilistic Models
(DDPMs) to control nonlinear dynamical systems. DDPMs are the state-of-art of
generative models that have achieved success in a wide variety of sampling
tasks. In our framework, we pose the feedback control problem as a generative
task of drawing samples from a target set under control system constraints. The
forward process of DDPMs constructs trajectories originating from a target set
by adding noise. We learn to control a dynamical system in reverse such that
the terminal state belongs to the target set. For control-affine systems
without drift, we prove that the control system can exactly track the
trajectory of the forward process in reverse, whenever the the Lie bracket
based condition for controllability holds. We numerically study our approach on
various nonlinear systems and verify our theoretical results. We also conduct
numerical experiments for cases beyond our theoretical results on a
physics-engine.
\\ ( https://arxiv.org/abs/2402.02297 ,  2455kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02299 (*cross-listing*)
Date: Sat, 3 Feb 2024 23:33:24 GMT   (399kb,D)

Title: A Review and Comparison of AI Enhanced Side Channel Analysis
Authors: Max Panoff, Honggang Yu, Haoqi Shan, Yier Jin
Categories: cs.CR cs.LG
Comments: This paper has been accepted by ACM Journal on Emerging Technologies
  in Computing Systems (JETC)
DOI: 10.1145/3517810
\\
  Side Channel Analysis (SCA) presents a clear threat to privacy and security
in modern computing systems. The vast majority of communications are secured
through cryptographic algorithms. These algorithms are often provably-secure
from a cryptographical perspective, but their implementation on real hardware
introduces vulnerabilities. Adversaries can exploit these vulnerabilities to
conduct SCA and recover confidential information, such as secret keys or
internal states. The threat of SCA has greatly increased as machine learning,
and in particular deep learning, enhanced attacks become more common. In this
work, we will examine the latest state-of-the-art deep learning techniques for
side channel analysis, the theory behind them, and how they are conducted. Our
focus will be on profiling attacks using deep learning techniques, but we will
also examine some new and emerging methodologies enhanced by deep learning
techniques, such as non-profiled attacks, artificial trace generation, and
others. Finally, different deep learning enhanced SCA schemes attempted against
the ANSSI SCA Database (ASCAD) and their relative performance will be evaluated
and compared. This will lead to new research directions to secure cryptographic
implementations against the latest SCA attacks.
\\ ( https://arxiv.org/abs/2402.02299 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02304 (*cross-listing*)
Date: Sun, 4 Feb 2024 00:07:05 GMT   (3243kb,D)

Title: Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep
  Learning Model
Authors: Luis Kaiser, Richard Tsai, Christian Klingenberg
Categories: math.AP cs.LG
\\
  In a variety of scientific and engineering domains, ranging from seismic
modeling to medical imaging, the need for high-fidelity and efficient solutions
for high-frequency wave propagation holds great significance. Recent advances
in wave modeling use sufficiently accurate fine solver outputs to train neural
networks that enhance the accuracy of a fast but inaccurate coarse solver. A
stable and fast solver further allows the use of Parareal, a parallel-in-time
algorithm to retrieve and correct high-frequency wave components. In this paper
we build upon the work of Nguyen and Tsai (2023) and present a novel unified
system that integrates a numerical solver with deep learning components into an
end-to-end framework. In the proposed setting, we investigate refinements to
the neural network architecture, data generation algorithm and Parareal scheme.
Our results show that the cohesive structure significantly improves performance
without sacrificing speed, and demonstrate the importance of temporal dynamics,
as well as Parareal iterations, for accurate wave propagation.
\\ ( https://arxiv.org/abs/2402.02304 ,  3243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02320 (*cross-listing*)
Date: Sun, 4 Feb 2024 02:12:15 GMT   (415kb,D)

Title: Spin: An Efficient Secure Computation Framework with GPU Acceleration
Authors: Wuxuan Jiang, Xiangjun Song, Shenbai Hong, Haijun Zhang, Wenxin Liu,
  Bo Zhao, Wei Xu, Yi Li
Categories: cs.CR cs.LG
\\
  Accuracy and efficiency remain challenges for multi-party computation (MPC)
frameworks. Spin is a GPU-accelerated MPC framework that supports multiple
computation parties and a dishonest majority adversarial setup. We propose
optimized protocols for non-linear functions that are critical for machine
learning, as well as several novel optimizations specific to attention that is
the fundamental unit of Transformer models, allowing Spin to perform
non-trivial CNNs training and Transformer inference without sacrificing
security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart
network cards for acceleration. Comprehensive evaluations demonstrate that Spin
can be up to $2\times$ faster than the state-of-the-art for deep neural network
training. For inference on a Transformer model with 18.9 million parameters,
our attention-specific optimizations enable Spin to achieve better efficiency,
less communication, and better accuracy.
\\ ( https://arxiv.org/abs/2402.02320 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02333 (*cross-listing*)
Date: Sun, 4 Feb 2024 04:00:33 GMT   (13059kb,D)

Title: Copyright Protection in Generative AI: A Technical Perspective
Authors: Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun
  Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, Jiliang Tang
Categories: cs.CR cs.LG
Comments: 26 pages
\\
  Generative AI has witnessed rapid advancement in recent years, expanding
their capabilities to create synthesized content such as text, images, audio,
and code. The high fidelity and authenticity of contents generated by these
Deep Generative Models (DGMs) have sparked significant copyright concerns.
There have been various legal debates on how to effectively safeguard
copyrights in DGMs. This work delves into this issue by providing a
comprehensive overview of copyright protection from a technical perspective. We
examine from two distinct viewpoints: the copyrights pertaining to the source
data held by the data owners and those of the generative models maintained by
the model builders. For data copyright, we delve into methods data owners can
protect their content and DGMs can be utilized without infringing upon these
rights. For model copyright, our discussion extends to strategies for
preventing model theft and identifying outputs generated by specific models.
Finally, we highlight the limitations of existing techniques and identify areas
that remain unexplored. Furthermore, we discuss prospective directions for the
future of copyright protection, underscoring its importance for the sustainable
and ethical development of Generative AI.
\\ ( https://arxiv.org/abs/2402.02333 ,  13059kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02338 (*cross-listing*)
Date: Sun, 4 Feb 2024 04:21:34 GMT   (1148kb,D)

Title: Large Language Model Adaptation for Networking
Authors: Duo Wu, Xianda Wang, Yaqi Qiao, Zhi Wang, Junchen Jiang, Shuguang Cui,
  Fangxin Wang
Categories: cs.NI cs.LG
\\
  Many networking tasks now employ deep learning (DL) to solve complex
prediction and system optimization problems. However, current design philosophy
of DL-based algorithms entails intensive engineering overhead due to the manual
design of deep neural networks (DNNs) for different networking tasks. Besides,
DNNs tend to achieve poor generalization performance on unseen data
distributions/environments.
  Motivated by the recent success of large language models (LLMs), for the
first time, this work studies the LLM adaptation for networking to explore a
more sustainable design philosophy. With the massive pre-trained knowledge and
powerful inference ability, LLM can serve as the foundation model, and is
expected to achieve "one model for all" with even better performance and
stronger generalization for various tasks. In this paper, we present NetLLM,
the first LLM adaptation framework that efficiently adapts LLMs to solve
networking problems. NetLLM addresses many practical challenges in LLM
adaptation, from how to process task-specific information with LLMs, to how to
improve the efficiency of answer generation and acquiring domain knowledge for
networking. Across three networking-related use cases - viewport prediction
(VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we
showcase the effectiveness of NetLLM in LLM adaptation for networking. Results
show that the adapted LLM surpasses state-of-the-art algorithms by 10.1-36.6%
for VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, and also achieves superior
generalization performance.
\\ ( https://arxiv.org/abs/2402.02338 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02340 (*cross-listing*)
Date: Sun, 4 Feb 2024 04:42:05 GMT   (545kb,D)

Title: Learning Semantic Proxies from Visual Prompts for Parameter-Efficient
  Fine-Tuning in Deep Metric Learning
Authors: Li Ren, Chen Chen, Liqiang Wang, Kien Hua
Categories: cs.CV cs.LG
Comments: ICLR 2024
\\
  Deep Metric Learning (DML) has long attracted the attention of the machine
learning community as a key objective. Existing solutions concentrate on
fine-tuning the pre-trained models on conventional image datasets. As a result
of the success of recent pre-trained models trained from larger-scale datasets,
it is challenging to adapt the model to the DML tasks in the local data domain
while retaining the previously gained knowledge. In this paper, we investigate
parameter-efficient methods for fine-tuning the pre-trained model for DML
tasks. In particular, we propose a novel and effective framework based on
learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT).
Based on the conventional proxy-based DML paradigm, we augment the proxy by
incorporating the semantic information from the input image and the ViT, in
which we optimize the visual prompts for each class. We demonstrate that our
new approximations with semantic information are superior to representative
capabilities, thereby improving metric learning performance. We conduct
extensive experiments to demonstrate that our proposed framework is effective
and efficient by evaluating popular DML benchmarks. In particular, we
demonstrate that our fine-tuning method achieves comparable or even better
performance than recent state-of-the-art full fine-tuning works of DML while
tuning only a small percentage of total parameters.
\\ ( https://arxiv.org/abs/2402.02340 ,  545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02346 (*cross-listing*)
Date: Sun, 4 Feb 2024 05:03:22 GMT   (12553kb,D)

Title: Closed-Loop Unsupervised Representation Disentanglement with $\beta$-VAE
  Distillation and Diffusion Probabilistic Feedback
Authors: Xin Jin, Bohan Li, BAAO Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li,
  Tao Yang, Wenjun Zeng
Categories: cs.CV cs.LG
\\
  Representation disentanglement may help AI fundamentally understand the real
world and thus benefit both discrimination and generation tasks. It currently
has at least three unresolved core issues: (i) heavy reliance on label
annotation and synthetic data -- causing poor generalization on natural
scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to
adaptively achieve an optimal training trade-off; (iii) lacking reasonable
evaluation metric, especially for the real label-free data. To address these
challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised
representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}.
Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while
resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled
representations. The strong generation ability of diffusion model and the good
disentanglement ability of VAE model are complementary. To strengthen
disentangling, VAE-latent distillation and diffusion-wise feedback are
interconnected in a closed-loop system for a further mutual promotion. Then, a
self-supervised \textbf{Navigation} strategy is introduced to identify
interpretable semantic directions in the disentangled latent space. Finally, a
new metric based on content tracking is designed to evaluate the
disentanglement effect. Experiments demonstrate the superiority of CL-Dis on
applications like real image manipulation and visual analysis.
\\ ( https://arxiv.org/abs/2402.02346 ,  12553kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02350 (*cross-listing*)
Date: Sun, 4 Feb 2024 05:27:59 GMT   (518kb,D)

Title: Interference-Aware Emergent Random Access Protocol for Downlink LEO
  Satellite Networks
Authors: Chang-Yong Lim, Jihong Park, Jinho Choi, Ju-Hyung Lee, Daesub Oh,
  Heewook Kim
Categories: cs.NI cs.LG
Comments: 2 pages, 4 figures, 1 table; submitted to IEEE for possible
  publication
\\
  In this article, we propose a multi-agent deep reinforcement learning (MADRL)
framework to train a multiple access protocol for downlink low earth orbit
(LEO) satellite networks. By improving the existing learned protocol, emergent
random access channel (eRACH), our proposed method, coined centralized and
compressed emergent signaling for eRACH (Ce2RACH), can mitigate inter-satellite
interference by exchanging additional signaling messages jointly learned
through the MADRL training process. Simulations demonstrate that Ce2RACH
achieves up to 36.65% higher network throughput compared to eRACH, while the
cost of signaling messages increase linearly with the number of users.
\\ ( https://arxiv.org/abs/2402.02350 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02356 (*cross-listing*)
Date: Sun, 4 Feb 2024 05:48:45 GMT   (250kb,D)

Title: Decentralized Sum-of-Nonconvex Optimization
Authors: Zhuanghua Liu and Bryan Kian Hsiang Low
Categories: math.OC cs.LG
\\
  We consider the optimization problem of minimizing the sum-of-nonconvex
function, i.e., a convex function that is the average of nonconvex components.
The existing stochastic algorithms for such a problem only focus on a single
machine and the centralized scenario. In this paper, we study the
sum-of-nonconvex optimization in the decentralized setting. We present a new
theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the
linear convergence of their approach. However, the convergence rate of the
PMGT-SVRG algorithm has a linear dependency on the condition number, which is
undesirable for the ill-conditioned problem. To remedy this issue, we propose
an accelerated stochastic decentralized first-order algorithm by incorporating
the techniques of acceleration, gradient tracking, and multi-consensus mixing
into the SVRG algorithm. The convergence rate of the proposed method has a
square-root dependency on the condition number. The numerical experiments
validate the theoretical guarantee of our proposed algorithms on both synthetic
and real-world datasets.
\\ ( https://arxiv.org/abs/2402.02356 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02359 (*cross-listing*)
Date: Sun, 4 Feb 2024 05:54:51 GMT   (358kb,D)

Title: Incremental Quasi-Newton Methods with Faster Superlinear Convergence
  Rates
Authors: Zhuanghua Liu and Luo Luo and Bryan Kian Hsiang Low
Categories: math.OC cs.LG
\\
  We consider the finite-sum optimization problem, where each component
function is strongly convex and has Lipschitz continuous gradient and Hessian.
The recently proposed incremental quasi-Newton method is based on BFGS update
and achieves a local superlinear convergence rate that is dependent on the
condition number of the problem. This paper proposes a more efficient
quasi-Newton method by incorporating the symmetric rank-1 update into the
incremental framework, which results in the condition-number-free local
superlinear convergence rate. Furthermore, we can boost our method by applying
the block update on the Hessian approximation, which leads to an even faster
local convergence rate. The numerical experiments show the proposed methods
significantly outperform the baseline methods.
\\ ( https://arxiv.org/abs/2402.02359 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02377 (*cross-listing*)
Date: Sun, 4 Feb 2024 07:19:40 GMT   (4922kb,D)

Title: NOAH: Learning Pairwise Object Category Attentions for Image
  Classification
Authors: Chao Li, Aojun Zhou, Anbang Yao
Categories: cs.CV cs.LG
Comments: This research work was completed in 2023. Code and pre-trained models
  are available at https://github.com/OSVAI/NOAH
\\
  A modern deep neural network (DNN) for image classification tasks typically
consists of two parts: a backbone for feature extraction, and a head for
feature encoding and class predication. We observe that the head structures of
mainstream DNNs adopt a similar feature encoding pipeline, exploiting global
feature dependencies while disregarding local ones. In this paper, we revisit
the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that
relies on a new form of dot-product attention called pairwise object category
attention (POCA), efficiently exploiting spatially dense category-specific
attentions to augment classification performance. NOAH introduces a neat
combination of feature split, transform and merge operations to learn POCAs at
local to global scales. As a drop-in design, NOAH can be easily used to replace
existing heads of various types of DNNs, improving classification performance
while maintaining similar model efficiency. We validate the effectiveness of
NOAH on ImageNet classification benchmark with 25 DNN architectures spanning
convolutional neural networks, vision transformers and multi-layer perceptrons.
In general, NOAH is able to significantly improve the performance of
lightweight DNNs, e.g., showing 3.14\%|5.3\%|1.9\% top-1 accuracy improvement
to MobileNetV2 (0.5x)|Deit-Tiny (0.5x)|gMLP-Tiny (0.5x). NOAH also generalizes
well when applied to medium-size and large-size DNNs. We further show that NOAH
retains its efficacy on other popular multi-class and multi-label image
classification benchmarks as well as in different training regimes, e.g.,
showing 3.6\%|1.1\% mAP improvement to large ResNet101|ViT-Large on MS-COCO
dataset. Project page: https://github.com/OSVAI/NOAH.
\\ ( https://arxiv.org/abs/2402.02377 ,  4922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02382 (*cross-listing*)
Date: Sun, 4 Feb 2024 07:49:02 GMT   (863kb,D)

Title: Revisiting the Power of Prompt for Visual Tuning
Authors: Yuzhu Wang, Lechao Cheng, Chaowei Fang, Dingwen Zhang, Manni Duan,
  Meng Wang
Categories: cs.CV cs.LG
Comments: 15 pages
\\
  Visual prompt tuning (VPT) is a promising solution incorporating learnable
prompt tokens to customize pre-trained models for downstream tasks. However,
VPT and its variants often encounter challenges like prompt initialization,
prompt length, and subpar performance in self-supervised pretraining, hindering
successful contextual adaptation. This study commences by exploring the
correlation evolvement between prompts and patch tokens during proficient
training. Inspired by the observation that the prompt tokens tend to share high
mutual information with patch tokens, we propose initializing prompts with
downstream token prototypes. The strategic initialization, a stand-in for the
previous initialization, substantially improves performance in fine-tuning. To
refine further, we optimize token construction with a streamlined pipeline that
maintains excellent performance with almost no increase in computational
expenses compared to VPT. Exhaustive experiments show our proposed approach
outperforms existing methods by a remarkable margin. For instance, it surpasses
full fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable
parameters on the FGVC and VTAB-1K benchmarks. Notably, our method
significantly advances the adaptation for self-supervised pretraining,
achieving impressive task performance gains of at least 10% to 30%. Besides,
the experimental results demonstrate the proposed SPT is robust to prompt
lengths and scales well with model capacity and training data size. We finally
provide an insightful exploration into the amount of target data facilitating
the adaptation of pre-trained models to downstream tasks.
\\ ( https://arxiv.org/abs/2402.02382 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02418 (*cross-listing*)
Date: Sun, 4 Feb 2024 09:34:13 GMT   (8687kb,D)

Title: eXplainable Bayesian Multi-Perspective Generative Retrieval
Authors: EuiYul Song, Philhoon Oh, Sangryul Kim, James Thorne
Categories: cs.IR cs.LG
Comments: 15 pages, 7 figures
MSC-class: 94C06
ACM-class: H.3.3
\\
  Modern deterministic retrieval pipelines prioritize achieving
state-of-the-art performance but often lack interpretability in
decision-making. These models face challenges in assessing uncertainty, leading
to overconfident predictions. To overcome these limitations, we integrate
uncertainty calibration and interpretability into a retrieval pipeline.
Specifically, we introduce Bayesian methodologies and multi-perspective
retrieval to calibrate uncertainty within a retrieval pipeline. We incorporate
techniques such as LIME and SHAP to analyze the behavior of a black-box
reranker model. The importance scores derived from these explanation
methodologies serve as supplementary relevance scores to enhance the base
reranker model. We evaluate the resulting performance enhancements achieved
through uncertainty calibration and interpretable reranking on Question
Answering and Fact Checking tasks. Our methods demonstrate substantial
performance improvements across three KILT datasets.
\\ ( https://arxiv.org/abs/2402.02418 ,  8687kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02426 (*cross-listing*)
Date: Sun, 4 Feb 2024 09:51:19 GMT   (2545kb,D)

Title: Hybrid-Prediction Integrated Planning for Autonomous Driving
Authors: Haochen Liu, Zhiyu Huang, Wenhui Huang, Haohan Yang, Xiaoyu Mo, and
  Chen Lv
Categories: cs.RO cs.LG
\\
  Autonomous driving systems require the ability to fully understand and
predict the surrounding environment to make informed decisions in complex
scenarios. Recent advancements in learning-based systems have highlighted the
importance of integrating prediction and planning modules. However, this
integration has brought forth three major challenges: inherent trade-offs by
sole prediction, consistency between prediction patterns, and social coherence
in prediction and planning. To address these challenges, we introduce a
hybrid-prediction integrated planning (HPP) system, which possesses three
novelly designed modules. First, we introduce marginal-conditioned occupancy
prediction to align joint occupancy with agent-wise perceptions. Our proposed
MS-OccFormer module achieves multi-stage alignment per occupancy forecasting
with consistent awareness from agent-wise motion predictions. Second, we
propose a game-theoretic motion predictor, GTFormer, to model the interactive
future among individual agents with their joint predictive awareness. Third,
hybrid prediction patterns are concurrently integrated with Ego Planner and
optimized by prediction guidance. HPP achieves state-of-the-art performance on
the nuScenes dataset, demonstrating superior accuracy and consistency for
end-to-end paradigms in prediction and planning. Moreover, we test the
long-term open-loop and closed-loop performance of HPP on the Waymo Open Motion
Dataset and CARLA benchmark, surpassing other integrated prediction and
planning pipelines with enhanced accuracy and compatibility.
\\ ( https://arxiv.org/abs/2402.02426 ,  2545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02430 (*cross-listing*)
Date: Sun, 4 Feb 2024 09:59:18 GMT   (9991kb,D)

Title: Exploiting Low-level Representations for Ultra-Fast Road Segmentation
Authors: Huan Zhou, Feng Xue, Yucong Li, Shi Gong, Yiqun Li, Yu Zhou
Categories: cs.CV cs.LG
Comments: 11 pages, 7 figures, TEEE TITS
\\
  Achieving real-time and accuracy on embedded platforms has always been the
pursuit of road segmentation methods. To this end, they have proposed many
lightweight networks. However, they ignore the fact that roads are "stuff"
(background or environmental elements) rather than "things" (specific
identifiable objects), which inspires us to explore the feasibility of
representing roads with low-level instead of high-level features. Surprisingly,
we find that the primary stage of mainstream network models is sufficient to
represent most pixels of the road for segmentation. Motivated by this, we
propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg).
Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail
branch is firstly designed to extract low-level feature representation for the
road by the first stage of ResNet-18. To suppress texture-less regions mistaken
as the road in the low-level feature, the context semantic branch is then
designed to extract the context feature in a fast manner. To this end, in the
second branch, we asymmetrically downsample the input image and design an
aggregation module to achieve comparable receptive fields to the third stage of
ResNet-18 but with less time consumption. Finally, to segment the road from the
low-level feature, a selective fusion module is proposed to calculate
pixel-wise attention between the low-level representation and context feature,
and suppress the non-road low-level response by this attention. On KITTI-Road,
LFD-RoadSeg achieves a maximum F1-measure (MaxF) of 95.21% and an average
precision of 93.71%, while reaching 238 FPS on a single TITAN Xp and 54 FPS on
a Jetson TX2, all with a compact model size of just 936k parameters. The source
code is available at https://github.com/zhouhuan-hust/LFD-RoadSeg.
\\ ( https://arxiv.org/abs/2402.02430 ,  9991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02431 (*cross-listing*)
Date: Sun, 4 Feb 2024 10:00:00 GMT   (2623kb,D)

Title: Learning Mutual Excitation for Hand-to-Hand and Human-to-Human
  Interaction Recognition
Authors: Mengyuan Liu, Chen Chen, Songtao Wu, Fanyang Meng, Hong Liu
Categories: cs.CV cs.LG
\\
  Recognizing interactive actions, including hand-to-hand interaction and
human-to-human interaction, has attracted increasing attention for various
applications in the field of video analysis and human-robot interaction.
Considering the success of graph convolution in modeling topology-aware
features from skeleton data, recent methods commonly operate graph convolution
on separate entities and use late fusion for interactive action recognition,
which can barely model the mutual semantic relationships between pairwise
entities. To this end, we propose a mutual excitation graph convolutional
network (me-GCN) by stacking mutual excitation graph convolution (me-GC)
layers. Specifically, me-GC uses a mutual topology excitation module to firstly
extract adjacency matrices from individual entities and then adaptively model
the mutual constraints between them. Moreover, me-GC extends the above idea and
further uses a mutual feature excitation module to extract and merge deep
features from pairwise entities. Compared with graph convolution, our proposed
me-GC gradually learns mutual information in each layer and each stage of graph
convolution operations. Extensive experiments on a challenging hand-to-hand
interaction dataset, i.e., the Assembely101 dataset, and two large-scale
human-to-human interaction datasets, i.e., NTU60-Interaction and
NTU120-Interaction consistently verify the superiority of our proposed method,
which outperforms the state-of-the-art GCN-based and Transformer-based methods.
\\ ( https://arxiv.org/abs/2402.02431 ,  2623kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02433 (*cross-listing*)
Date: Sun, 4 Feb 2024 10:11:27 GMT   (1205kb,D)

Title: Uncertainty-Aware Perceiver
Authors: EuiYul Song
Categories: cs.CV cs.LG
Comments: 8 pages, 5 figures
MSC-class: 68A06
ACM-class: I.4.0
\\
  The Perceiver makes few architectural assumptions about the relationship
among its inputs with quadratic scalability on its memory and computation time.
Indeed, the Perceiver model outpaces or is competitive with ResNet-50 and ViT
in terms of accuracy to some degree. However, the Perceiver does not take
predictive uncertainty and calibration into account. The Perceiver also
generalizes its performance on three datasets, three models, one evaluation
metric, and one hyper-parameter setting. Worst of all, the Perceiver's relative
performance improvement against other models is marginal. Furthermore, its
reduction of architectural prior is not substantial; is not equivalent to its
quality. Thereby, I invented five mutations of the Perceiver, the
Uncertainty-Aware Perceivers, that obtain uncertainty estimates and measured
their performance on three metrics. Experimented with CIFAR-10 and CIFAR-100,
the Uncertainty-Aware Perceivers make considerable performance enhancement
compared to the Perceiver.
\\ ( https://arxiv.org/abs/2402.02433 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02444 (*cross-listing*)
Date: Sun, 4 Feb 2024 10:52:43 GMT   (4506kb,D)

Title: BECLR: Batch Enhanced Contrastive Few-Shot Learning
Authors: Stylianos Poulakakis-Daktylidis and Hadi Jamali-Rad
Categories: cs.CV cs.LG
Comments: ICLR 2024 Spotlight Presentation
\\
  Learning quickly from very few labeled samples is a fundamental attribute
that separates machines and humans in the era of deep representation learning.
Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding
the reliance on annotations at training time. Intrigued by the success of
contrastive learning approaches in the realm of U-FSL, we structurally approach
their shortcomings in both pretraining and downstream inference stages. We
propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly
separable latent representation space for enhancing positive sampling at the
pretraining phase and infusing implicit class-level insights into unsupervised
contrastive learning. We then tackle the, somehow overlooked yet critical,
issue of sample bias at the few-shot inference stage. We propose an iterative
Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate
that it efficiently addresses the problem, especially in low-shot scenarios
where FSL approaches suffer the most from sample bias. We later on discuss that
DyCE and OpTA are two intertwined pieces of a novel end-to-end approach (we
coin as BECLR), constructively magnifying each other's impact. We then present
a suite of extensive quantitative and qualitative experimentation to
corroborate that BECLR sets a new state-of-the-art across ALL existing U-FSL
benchmarks (to the best of our knowledge), and significantly outperforms the
best of the current baselines (codebase available at:
https://github.com/stypoumic/BECLR).
\\ ( https://arxiv.org/abs/2402.02444 ,  4506kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02459 (*cross-listing*)
Date: Sun, 4 Feb 2024 12:15:56 GMT   (372kb,D)

Title: On Minimum Trace Factor Analysis - An Old Song Sung to a New Tune
Authors: C. Li, A. Shkolnik
Categories: stat.ML cs.LG stat.ME
\\
  Dimensionality reduction methods, such as principal component analysis (PCA)
and factor analysis, are central to many problems in data science. There are,
however, serious and well-understood challenges to finding robust low
dimensional approximations for data with significant heteroskedastic noise.
This paper introduces a relaxed version of Minimum Trace Factor Analysis
(MTFA), a convex optimization method with roots dating back to the work of
Ledermann in 1940. This relaxation is particularly effective at not overfitting
to heteroskedastic perturbations and addresses the commonly cited Heywood cases
in factor analysis and the recently identified "curse of ill-conditioning" for
existing spectral methods. We provide theoretical guarantees on the accuracy of
the resulting low rank subspace and the convergence rate of the proposed
algorithm to compute that matrix. We develop a number of interesting
connections to existing methods, including HeteroPCA, Lasso, and Soft-Impute,
to fill an important gap in the already large literature on low rank matrix
estimation. Numerical experiments benchmark our results against several recent
proposals for dealing with heteroskedastic noise.
\\ ( https://arxiv.org/abs/2402.02459 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02499 (*cross-listing*)
Date: Sun, 4 Feb 2024 14:18:20 GMT   (2451kb,D)

Title: Robot Trajectron: Trajectory Prediction-based Shared Control for Robot
  Manipulation
Authors: Pinhao Song, Pengteng Li, Erwin Aertbelien, Renaud Detry
Categories: cs.RO cs.LG
Comments: Accepted by ICRA2024
\\
  We address the problem of (a) predicting the trajectory of an arm reaching
motion, based on a few seconds of the motion's onset, and (b) leveraging this
predictor to facilitate shared-control manipulation tasks, easing the cognitive
load of the operator by assisting them in their anticipated direction of
motion. Our novel intent estimator, dubbed the \emph{Robot Trajectron} (RT),
produces a probabilistic representation of the robot's anticipated trajectory
based on its recent position, velocity and acceleration history. Taking arm
dynamics into account allows RT to capture the operator's intent better than
other SOTA models that only use the arm's position, making it particularly
well-suited to assist in tasks where the operator's intent is susceptible to
change. We derive a novel shared-control solution that combines RT's predictive
capacity to a representation of the locations of potential reaching targets.
Our experiments demonstrate RT's effectiveness in both intent estimation and
shared-control tasks. We will make the code and data supporting our experiments
publicly available at https://github.com/mousecpn/Robot-Trajectron.git.
\\ ( https://arxiv.org/abs/2402.02499 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02506 (*cross-listing*)
Date: Sun, 4 Feb 2024 14:42:13 GMT   (23326kb,D)

Title: Device Scheduling and Assignment in Hierarchical Federated Learning for
  Internet of Things
Authors: Tinghao Zhang, Kwok-Yan Lam, Jun Zhao
Categories: cs.DC cs.LG
Comments: Published in IEEE Internet of Things Journal (IoT-J)
\\
  Federated Learning (FL) is a promising machine learning approach for Internet
of Things (IoT), but it has to address network congestion problems when the
population of IoT devices grows. Hierarchical FL (HFL) alleviates this issue by
distributing model aggregation to multiple edge servers. Nevertheless, the
challenge of communication overhead remains, especially in scenarios where all
IoT devices simultaneously join the training process. For scalability,
practical HFL schemes select a subset of IoT devices to participate in the
training, hence the notion of device scheduling. In this setting, only selected
IoT devices are scheduled to participate in the global training, with each of
them being assigned to one edge server. Existing HFL assignment methods are
primarily based on search mechanisms, which suffer from high latency in finding
the optimal assignment. This paper proposes an improved K-Center algorithm for
device scheduling and introduces a deep reinforcement learning-based approach
for assigning IoT devices to edge servers. Experiments show that scheduling 50%
of IoT devices is generally adequate for achieving convergence in HFL with much
lower time delay and energy consumption. In cases where reduction in energy
consumption (such as in Green AI) and reduction of messages (to avoid burst
traffic) are key objectives, scheduling 30% IoT devices allows a substantial
reduction in energy and messages with similar model accuracy.
\\ ( https://arxiv.org/abs/2402.02506 ,  23326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02511 (*cross-listing*)
Date: Sun, 4 Feb 2024 14:51:49 GMT   (7343kb,D)

Title: PoCo: Policy Composition from and for Heterogeneous Robot Learning
Authors: Lirui Wang, Jialiang Zhao, Yilun Du, Edward H. Adelson, Russ Tedrake
Categories: cs.RO cs.LG
\\
  Training general robotic policies from heterogeneous data for different tasks
is a significant challenge. Existing robotic datasets vary in different
modalities such as color, depth, tactile, and proprioceptive information, and
collected in different domains such as simulation, real robots, and human
videos. Current methods usually collect and pool all data from one domain to
train a single policy to handle such heterogeneity in tasks and domains, which
is prohibitively expensive and difficult. In this work, we present a flexible
approach, dubbed Policy Composition, to combine information across such diverse
modalities and domains for learning scene-level and task-level generalized
manipulation skills, by composing different data distributions represented with
diffusion models. Our method can use task-level composition for multi-task
manipulation and be composed with analytic cost functions to adapt policy
behaviors at inference time. We train our method on simulation, human, and real
robot data and evaluate in tool-use tasks. The composed policy achieves robust
and dexterous performance under varying scenes and tasks and outperforms
baselines from a single data source in both simulation and real-world
experiments. See https://liruiw.github.io/policycomp for more details .
\\ ( https://arxiv.org/abs/2402.02511 ,  7343kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02514 (*cross-listing*)
Date: Sun, 4 Feb 2024 14:59:31 GMT   (1081kb,D)

Title: Deep Supervision by Gaussian Pseudo-label-based Morphological Attention
  for Abdominal Aorta Segmentation in Non-Contrast CTs
Authors: Qixiang Ma, Antoine Lucas, Adrien Kaladji, Pascal Haigron
Categories: cs.CV cs.LG
Comments: Accepted by 21st IEEE International Symposium on Biomedical Imaging
\\
  The segmentation of the abdominal aorta in non-contrast CT images is a
non-trivial task for computer-assisted endovascular navigation, particularly in
scenarios where contrast agents are unsuitable. While state-of-the-art deep
learning segmentation models have been proposed recently for this task, they
are trained on manually annotated strong labels. However, the inherent
ambiguity in the boundary of the aorta in non-contrast CT may undermine the
reliability of strong labels, leading to potential overfitting risks. This
paper introduces a Gaussian-based pseudo label, integrated into conventional
deep learning models through deep supervision, to achieve Morphological
Attention (MA) enhancement. As the Gaussian pseudo label retains the
morphological features of the aorta without explicitly representing its
boundary distribution, we suggest that it preserves aortic morphology during
training while mitigating the negative impact of ambiguous boundaries, reducing
the risk of overfitting. It is introduced in various 2D/3D deep learning models
and validated on our local data set of 30 non-contrast CT volumes comprising
5749 CT slices. The results underscore the effectiveness of MA in preserving
the morphological characteristics of the aorta and addressing overfitting
concerns, thereby enhancing the performance of the models.
\\ ( https://arxiv.org/abs/2402.02514 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02545 (*cross-listing*)
Date: Sun, 4 Feb 2024 15:48:20 GMT   (1843kb,D)

Title: Classification of Tennis Actions Using Deep Learning
Authors: Emil Hovad (1 and 2), Therese Hougaard-Jensen (2), Line Katrine Harder
  Clemmensen (2) ((1) Alexandra Instituttet A/S, Rued Langgaards Vej 7, 2300
  K{\o}benhavn S, Denmark, (2) Department of Mathematics and Computer Science,
  Technical University of Denmark, Richard Petersens Plads, Building 324, 2800
  Kgs. Lyngby, Denmark)
Categories: cs.CV cs.LG
Comments: 5 Figures
\\
  Recent advances of deep learning makes it possible to identify specific
events in videos with greater precision. This has great relevance in sports
like tennis in order to e.g., automatically collect game statistics, or replay
actions of specific interest for game strategy or player improvements. In this
paper, we investigate the potential and the challenges of using deep learning
to classify tennis actions. Three models of different size, all based on the
deep learning architecture SlowFast were trained and evaluated on the academic
tennis dataset THETIS. The best models achieve a generalization accuracy of 74
%, demonstrating a good performance for tennis action classification. We
provide an error analysis for the best model and pinpoint directions for
improvement of tennis datasets in general. We discuss the limitations of the
data set, general limitations of current publicly available tennis data-sets,
and future steps needed to make progress.
\\ ( https://arxiv.org/abs/2402.02545 ,  1843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02551 (*cross-listing*)
Date: Sun, 4 Feb 2024 15:54:03 GMT   (2064kb,D)

Title: Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner
  with Robust Low-Level Control for Robotic Manipulators
Authors: Mehdi Heydari Shahna, Seyed Adel Alizadeh Kolagar, Jouni Mattila
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: This work has been submitted for possible publication in the IEEE
\\
  In robotics, contemporary strategies are learning-based, characterized by a
complex black-box nature and a lack of interpretability, which may pose
challenges in ensuring stability and safety. To address these issues, we
propose integrating an obstacle-free deep reinforcement learning (DRL)
trajectory planner with a novel auto-tuning low- and joint-level control
strategy, all while actively engaging in the learning phase through
interactions with the environment. This approach circumvents the complexities
associated with computations while also addressing nonrepetitive and random
obstacle avoidance tasks. First, a model-free DRL agent to plan
velocity-bounded and obstacle-free motion is employed for a manipulator with
'n' degrees of freedom (DoF) in task space through joint-level reasoning. This
plan is then input into a robust subsystem-based adaptive controller, which
produces the necessary torques, while the Cuckoo Search Optimization (CSO)
algorithm enhances control gains to minimize the time required to reach, time
taken to stabilize, the maximum deviation from the desired value, and
persistent tracking error in the steady state. This approach guarantees that
position and velocity errors exponentially converge to zero, accounting for any
initial and end-point variations, unknown modeling errors, and external
disturbances. Theoretical assertions are validated through the presentation of
simulation outcomes.
\\ ( https://arxiv.org/abs/2402.02551 ,  2064kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02554 (*cross-listing*)
Date: Sun, 4 Feb 2024 15:59:35 GMT   (622kb,D)

Title: DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms
  in Vision Transformers
Authors: Oryan Yehezkel, Alon Zolfi, Amit Baras, Yuval Elovici, Asaf Shabtai
Categories: cs.CV cs.LG
Comments: 12 pages, 5 figures
\\
  Vision transformers have contributed greatly to advancements in the computer
vision domain, demonstrating state-of-the-art performance in diverse tasks
(e.g., image classification, object detection). However, their high
computational requirements grow quadratically with the number of tokens used.
Token sparsification techniques have been proposed to address this issue. These
techniques employ an input-dependent strategy, in which uninformative tokens
are discarded from the computation pipeline, improving the model's efficiency.
However, their dynamism and average-case assumption makes them vulnerable to a
new threat vector - carefully crafted adversarial examples capable of fooling
the sparsification mechanism, resulting in worst-case performance. In this
paper, we present DeSparsify, an attack targeting the availability of vision
transformers that use token sparsification mechanisms. The attack aims to
exhaust the operating system's resources, while maintaining its stealthiness.
Our evaluation demonstrates the attack's effectiveness on three token
sparsification techniques and examines the attack's transferability between
them and its effect on the GPU resources. To mitigate the impact of the attack,
we propose various countermeasures.
\\ ( https://arxiv.org/abs/2402.02554 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02569 (*cross-listing*)
Date: Sun, 4 Feb 2024 17:14:53 GMT   (76kb,D)

Title: On the Complexity of Finite-Sum Smooth Optimization under the
  Polyak-{\L}ojasiewicz Condition
Authors: Yunyan Bai, Yuxing Liu, Luo Luo
Categories: math.OC cs.LG
\\
  This paper considers the optimization problem of the form $\min_{{\bf
x}\in{\mathbb R}^d} f({\bf x})\triangleq \frac{1}{n}\sum_{i=1}^n f_i({\bf x})$,
where $f(\cdot)$ satisfies the Polyak--{\L}ojasiewicz (PL) condition with
parameter $\mu$ and $\{f_i(\cdot)\}_{i=1}^n$ is $L$-mean-squared smooth. We
show that any gradient method requires at least
$\Omega(n+\kappa\sqrt{n}\log(1/\epsilon))$ incremental first-order oracle (IFO)
calls to find an $\epsilon$-suboptimal solution, where $\kappa\triangleq L/\mu$
is the condition number of the problem. This result nearly matches upper bounds
of IFO complexity for best-known first-order methods. We also study the problem
of minimizing the PL function in the distributed setting such that the
individuals $f_1(\cdot),\dots,f_n(\cdot)$ are located on a connected network of
$n$ agents. We provide lower bounds of
$\Omega(\kappa/\sqrt{\gamma}\,\log(1/\epsilon))$,
$\Omega((\kappa+\tau\kappa/\sqrt{\gamma}\,)\log(1/\epsilon))$ and
$\Omega\big(n+\kappa\sqrt{n}\log(1/\epsilon)\big)$ for communication rounds,
time cost and local first-order oracle calls respectively, where
$\gamma\in(0,1]$ is the spectral gap of the mixing matrix associated with the
network and~$\tau>0$ is the time cost of per communication round. Furthermore,
we propose a decentralized first-order method that nearly matches above lower
bounds in expectation.
\\ ( https://arxiv.org/abs/2402.02569 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02570 (*cross-listing*)
Date: Sun, 4 Feb 2024 17:19:46 GMT   (10621kb,D)

Title: Gazebo Plants: Simulating Plant-Robot Interaction with Cosserat Rods
Authors: Junchen Deng and Samhita Marri and Jonathan Klein and Wojtek
  Pa{\l}ubicki and S\"oren Pirk and Girish Chowdhary and Dominik L. Michels
Categories: cs.RO cs.LG
Comments: Upon request, we are happy to share our GazeboPlants plugin
  open-source (MPL 2.0)
ACM-class: I.6.3; I.6.m
\\
  Robotic harvesting has the potential to positively impact agricultural
productivity, reduce costs, improve food quality, enhance sustainability, and
to address labor shortage. In the rapidly advancing field of agricultural
robotics, the necessity of training robots in a virtual environment has become
essential. Generating training data to automatize the underlying computer
vision tasks such as image segmentation, object detection and classification,
also heavily relies on such virtual environments as synthetic data is often
required to overcome the shortage and lack of variety of real data sets.
However, physics engines commonly employed within the robotics community, such
as ODE, Simbody, Bullet, and DART, primarily support motion and collision
interaction of rigid bodies. This inherent limitation hinders experimentation
and progress in handling non-rigid objects such as plants and crops. In this
contribution, we present a plugin for the Gazebo simulation platform based on
Cosserat rods to model plant motion. It enables the simulation of plants and
their interaction with the environment. We demonstrate that, using our plugin,
users can conduct harvesting simulations in Gazebo by simulating a robotic arm
picking fruits and achieve results comparable to real-world experiments.
\\ ( https://arxiv.org/abs/2402.02570 ,  10621kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02578 (*cross-listing*)
Date: Sun, 4 Feb 2024 18:06:39 GMT   (1970kb)

Title: Impact of PSF misestimation and galaxy population bias on precision
  shear measurement using a CNN
Authors: Lisa Voigt
Categories: astro-ph.CO cs.LG
Comments: 15 pages, 10 figures
Journal-ref: 2024, MNRAS, 528, 3217
DOI: 10.1093/mnras/stae038
\\
  Weak gravitational lensing of distant galaxies provides a powerful probe of
dark energy. The aim of this study is to investigate the application of
convolutional neural networks (CNNs) to precision shear estimation. In
particular, using a shallow CNN, we explore the impact of point spread function
(PSF) misestimation and `galaxy population bias' (including `distribution bias'
and `morphology bias'), focusing on the accuracy requirements of next
generation surveys. We simulate a population of noisy disk and elliptical
galaxies and adopt a PSF that is representative of a Euclid-like survey. We
quantify the accuracy achieved by the CNN assuming a linear relationship
between the estimated and true shears and measure the multiplicative ($m$) and
additive ($c$) biases. We make use of an unconventional loss function to
mitigate the effects of noise bias and measure $m$ and $c$ when we use either:
(i) an incorrect galaxy ellipticity distribution or size-magnitude relation, or
the wrong ratio of morphological types, to describe the population of galaxies
(distribution bias); (ii) an incorrect galaxy light profile (morphology bias);
or (iii) a PSF with size or ellipticity offset from its true value (PSF
misestimation). We compare our results to the Euclid requirements on the
knowledge of the PSF model shape and size. Finally, we outline further work to
build on the promising potential of CNNs in precision shear estimation.
\\ ( https://arxiv.org/abs/2402.02578 ,  1970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02583 (*cross-listing*)
Date: Sun, 4 Feb 2024 18:50:29 GMT   (14261kb,D)

Title: DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image
  Editing
Authors: Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang
Categories: cs.CV cs.LG
\\
  Large-scale Text-to-Image (T2I) diffusion models have revolutionized image
generation over the last few years. Although owning diverse and high-quality
generation capabilities, translating these abilities to fine-grained image
editing remains challenging. In this paper, we propose DiffEditor to rectify
two weaknesses in existing diffusion-based image editing: (1) in complex
scenarios, editing results often lack editing accuracy and exhibit unexpected
artifacts; (2) lack of flexibility to harmonize editing operations, e.g.,
imagine new content. In our solution, we introduce image prompts in
fine-grained image editing, cooperating with the text prompt to better describe
the editing content. To increase the flexibility while maintaining content
consistency, we locally combine stochastic differential equation (SDE) into the
ordinary differential equation (ODE) sampling. In addition, we incorporate
regional score-based gradient guidance and a time travel strategy into the
diffusion sampling, further improving the editing quality. Extensive
experiments demonstrate that our method can efficiently achieve
state-of-the-art performance on various fine-grained image editing tasks,
including editing within a single image (e.g., object moving, resizing, and
content dragging) and across images (e.g., appearance replacing and object
pasting). Our source code is released at
https://github.com/MC-E/DragonDiffusion.
\\ ( https://arxiv.org/abs/2402.02583 ,  14261kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02588 (*cross-listing*)
Date: Sun, 4 Feb 2024 19:22:26 GMT   (336kb,D)

Title: Controller Synthesis from Noisy-Input Noisy-Output Data
Authors: Lidong Li, Andrea Bisoffi, Claudio De Persis, Nima Monshizadeh
Categories: eess.SY cs.LG cs.SY
\\
  We consider the problem of synthesizing a dynamic output-feedback controller
for a linear system, using solely input-output data corrupted by measurement
noise. To handle input-output data, an auxiliary representation of the original
system is introduced. By exploiting the structure of the auxiliary system, we
design a controller that robustly stabilizes all possible systems consistent
with data. Notably, we also provide a novel solution to extend the results to
generic multi-input multi-output systems. The findings are illustrated by
numerical examples.
\\ ( https://arxiv.org/abs/2402.02588 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02626 (*cross-listing*)
Date: Sun, 4 Feb 2024 22:15:30 GMT   (563kb,D)

Title: Position bias in features
Authors: Richard Demsyn-Jones
Categories: cs.IR cs.LG
\\
  The purpose of modeling document relevance for search engines is to rank
better in subsequent searches. Document-specific historical click-through rates
can be important features in a dynamic ranking system which updates as we
accumulate more sample. This paper describes the properties of several such
features, and tests them in controlled experiments. Extending the inverse
propensity weighting method to documents creates an unbiased estimate of
document relevance. This feature can approximate relevance accurately, leading
to near-optimal ranking in ideal circumstances. However, it has high variance
that is increasing with respect to the degree of position bias. Furthermore,
inaccurate position bias estimation leads to poor performance. Under several
scenarios this feature can perform worse than biased click-through rates. This
paper underscores the need for accurate position bias estimation, and is unique
in suggesting simultaneous use of biased and unbiased position bias features.
\\ ( https://arxiv.org/abs/2402.02626 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02634 (*cross-listing*)
Date: Sun, 4 Feb 2024 23:00:24 GMT   (12350kb,D)

Title: Key-Graph Transformer for Image Restoration
Authors: Bin Ren, Yawei Li, Jingyun Liang, Rakesh Ranjan, Mengyuan Liu, Rita
  Cucchiara, Luc Van Gool, Nicu Sebe
Categories: cs.CV cs.LG
Comments: 9 pages, 6 figures
\\
  While it is crucial to capture global information for effective image
restoration (IR), integrating such cues into transformer-based methods becomes
computationally expensive, especially with high input resolution. Furthermore,
the self-attention mechanism in transformers is prone to considering
unnecessary global cues from unrelated objects or regions, introducing
computational inefficiencies. In response to these challenges, we introduce the
Key-Graph Transformer (KGT) in this paper. Specifically, KGT views patch
features as graph nodes. The proposed Key-Graph Constructor efficiently forms a
sparse yet representative Key-Graph by selectively connecting essential nodes
instead of all the nodes. Then the proposed Key-Graph Attention is conducted
under the guidance of the Key-Graph only among selected nodes with linear
computational complexity within each window. Extensive experiments across 6 IR
tasks confirm the proposed KGT's state-of-the-art performance, showcasing
advancements both quantitatively and qualitatively.
\\ ( https://arxiv.org/abs/2402.02634 ,  12350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02662 (*cross-listing*)
Date: Mon, 5 Feb 2024 01:14:07 GMT   (2784kb,D)

Title: Image-Caption Encoding for Improving Zero-Shot Generalization
Authors: Eric Yang Yu, Christopher Liao, Sathvik Ravi, Theodoros Tsiligkaridis,
  Brian Kulis
Categories: cs.CV cs.LG
\\
  Recent advances in vision-language models have combined contrastive
approaches with generative methods to achieve state-of-the-art (SOTA) on
downstream inference tasks like zero-shot image classification. However, a
persistent issue of these models for image classification is their
out-of-distribution (OOD) generalization capabilities. We first show that when
an OOD data point is misclassified, the correct class can be typically found in
the Top-K predicted classes. In order to steer the model prediction toward the
correct class within the top predicted classes, we propose the Image-Caption
Encoding (ICE) method, a straightforward approach that directly enforces
consistency between the image-conditioned and caption-conditioned predictions
at evaluation time only. Intuitively, we take advantage of unique properties of
the generated captions to guide our local search for the correct class label
within the Top-K predicted classes. We show that our method can be easily
combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on
average and up to 3% on challenging datasets. Our code:
https://github.com/Chris210634/ice
\\ ( https://arxiv.org/abs/2402.02662 ,  2784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02672 (*cross-listing*)
Date: Mon, 5 Feb 2024 02:17:21 GMT   (375kb,D)

Title: Estimation of conditional average treatment effects on distributed data:
  A privacy-preserving approach
Authors: Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya
  Sakurai
Categories: stat.ME cs.LG
Comments: 33 pages
\\
  Estimation of conditional average treatment effects (CATEs) is an important
topic in various fields such as medical and social sciences. CATEs can be
estimated with high accuracy if distributed data across multiple parties can be
centralized. However, it is difficult to aggregate such data if they contain
privacy information. To address this issue, we proposed data collaboration
double machine learning (DC-DML), a method that can estimate CATE models with
privacy preservation of distributed data, and evaluated the method through
numerical experiments. Our contributions are summarized in the following three
points. First, our method enables estimation and testing of semi-parametric
CATE models without iterative communication on distributed data.
Semi-parametric or non-parametric CATE models enable estimation and testing
that is more robust to model mis-specification than parametric models. However,
to our knowledge, no communication-efficient method has been proposed for
estimating and testing semi-parametric or non-parametric CATE models on
distributed data. Second, our method enables collaborative estimation between
different parties as well as multiple time points because the
dimensionality-reduced intermediate representations can be accumulated. Third,
our method performed as well or better than other methods in evaluation
experiments using synthetic, semi-synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2402.02672 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02686 (*cross-listing*)
Date: Mon, 5 Feb 2024 02:53:55 GMT   (5540kb,D)

Title: Multi-Region Markovian Gaussian Process: An Efficient Method to Discover
  Directional Communications Across Multiple Brain Regions
Authors: Weihan Li, Chengrui Li, Yule Wang, Anqi Wu
Categories: q-bio.NC cs.LG
\\
  Studying the complex interactions between different brain regions is crucial
in neuroscience. Various statistical methods have explored the latent
communication across multiple brain regions. Two main categories are the
Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique
strengths. The GP-based approach effectively discovers latent variables such as
frequency bands and communication directions. Conversely, the LDS-based
approach is computationally efficient but lacks powerful expressiveness in
latent representation. In this study, we merge both methodologies by creating
an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian
Process (MRM-GP). Our work is the first to establish a connection between an
LDS and a multi-output GP that explicitly models frequencies and phase delays
within the latent space of neural recordings. Consequently, the model achieves
a linear inference cost over time points and provides an interpretable
low-dimensional representation, revealing communication directions across brain
regions and separating oscillatory communications into different frequency
bands.
\\ ( https://arxiv.org/abs/2402.02686 ,  5540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02694 (*cross-listing*)
Date: Mon, 5 Feb 2024 03:12:51 GMT   (470kb,D)

Title: Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic
  Scene Classification under Domain Shift
Authors: Jisheng Bai, Mou Wang, Haohe Liu, Han Yin, Yafei Jia, Siwei Huang,
  Yutong Du, Dongzhe Zhang, Mark D. Plumbley, Dongyuan Shi, Woon-Seng Gan,
  Susanto Rahardja, Bin Xiang, Jianfeng Chen
Categories: eess.AS cs.LG cs.SD
\\
  Acoustic scene classification (ASC) is a crucial research problem in
computational auditory scene analysis, and it aims to recognize the unique
acoustic characteristics of an environment. One of the challenges of the ASC
task is domain shift caused by a distribution gap between training and testing
data. Since 2018, ASC challenges have focused on the generalization of ASC
models across different recording devices. Although this task in recent years
has achieved substantial progress in device generalization, the challenge of
domain shift between different regions, involving characteristics such as time,
space, culture, and language, remains insufficiently explored at present. In
addition, considering the abundance of unlabeled acoustic scene data in the
real world, it is important to study the possible ways to utilize these
unlabelled data. Therefore, we introduce the task Semi-supervised Acoustic
Scene Classification under Domain Shift in the ICME 2024 Grand Challenge. We
encourage participants to innovate with semi-supervised learning techniques,
aiming to develop more robust ASC models under domain shift.
\\ ( https://arxiv.org/abs/2402.02694 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02699 (*cross-listing*)
Date: Mon, 5 Feb 2024 03:23:34 GMT   (321kb,D)

Title: Adversarial Data Augmentation for Robust Speaker Verification
Authors: Zhenyu Zhou and Junhui Chen and Namin Wang and Lantian Li and Dong
  Wang
Categories: cs.SD cs.LG
\\
  Data augmentation (DA) has gained widespread popularity in deep speaker
models due to its ease of implementation and significant effectiveness. It
enriches training data by simulating real-life acoustic variations, enabling
deep neural networks to learn speaker-related representations while
disregarding irrelevant acoustic variations, thereby improving robustness and
generalization. However, a potential issue with the vanilla DA is augmentation
residual, i.e., unwanted distortion caused by different types of augmentation.
To address this problem, this paper proposes a novel approach called
adversarial data augmentation (A-DA) which combines DA with adversarial
learning. Specifically, it involves an additional augmentation classifier to
categorize various augmentation types used in data augmentation. This
adversarial learning empowers the network to generate speaker embeddings that
can deceive the augmentation classifier, making the learned speaker embeddings
more robust in the face of augmentation variations. Experiments conducted on
VoxCeleb and CN-Celeb datasets demonstrate that our proposed A-DA outperforms
standard DA in both augmentation matched and mismatched test conditions,
showcasing its superior robustness and generalization against acoustic
variations.
\\ ( https://arxiv.org/abs/2402.02699 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02724 (*cross-listing*)
Date: Mon, 5 Feb 2024 04:45:24 GMT   (2462kb,D)

Title: FDNet: Frequency Domain Denoising Network For Cell Segmentation in
  Astrocytes Derived From Induced Pluripotent Stem Cells
Authors: Haoran Li, Jiahua Shi, Huaming Chen, Bo Du, Simon Maksour, Gabrielle
  Phillips, Mirella Dottori, Jun Shen
Categories: cs.CV cs.LG
Comments: Accepted by The IEEE International Symposium on Biomedical Imaging
  (ISBI) 2024
\\
  Artificially generated induced pluripotent stem cells (iPSCs) from somatic
cells play an important role for disease modeling and drug screening of
neurodegenerative diseases. Astrocytes differentiated from iPSCs are important
targets to investigate neuronal metabolism. The astrocyte differentiation
progress can be monitored through the variations of morphology observed from
microscopy images at different differentiation stages, then determined by
molecular biology techniques upon maturation. However, the astrocytes usually
``perfectly'' blend into the background and some of them are covered by
interference information (i.e., dead cells, media sediments, and cell debris),
which makes astrocytes difficult to observe. Due to the lack of annotated
datasets, the existing state-of-the-art deep learning approaches cannot be used
to address this issue. In this paper, we introduce a new task named astrocyte
segmentation with a novel dataset, called IAI704, which contains 704 images and
their corresponding pixel-level annotation masks. Moreover, a novel frequency
domain denoising network, named FDNet, is proposed for astrocyte segmentation.
In detail, our FDNet consists of a contextual information fusion module (CIF),
an attention block (AB), and a Fourier transform block (FTB). CIF and AB fuse
multi-scale feature embeddings to localize the astrocytes. FTB transforms
feature embeddings into the frequency domain and conducts a high-pass filter to
eliminate interference information. Experimental results demonstrate the
superiority of our proposed FDNet over the state-of-the-art substitutes in
astrocyte segmentation, shedding insights for iPSC differentiation progress
prediction.
\\ ( https://arxiv.org/abs/2402.02724 ,  2462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02729 (*cross-listing*)
Date: Mon, 5 Feb 2024 05:01:28 GMT   (9886kb)

Title: Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN
Authors: Zezhong Zhang, Guangxu Zhu, Junting Chen, Shuguang Cui
Categories: cs.IT cs.CV cs.LG eess.IV math.IT
\\
  In the 6G era, real-time radio resource monitoring and management are urged
to support diverse wireless-empowered applications. This calls for fast and
accurate estimation on the distribution of the radio resources, which is
usually represented by the spatial signal power strength over the geographical
environment, known as a radio map. In this paper, we present a cooperative
radio map estimation (CRME) approach enabled by the generative adversarial
network (GAN), called as GAN-CRME, which features fast and accurate radio map
estimation without the transmitters' information. The radio map is inferred by
exploiting the interaction between distributed received signal strength (RSS)
measurements at mobile users and the geographical map using a deep neural
network estimator, resulting in low data-acquisition cost and computational
complexity. Moreover, a GAN-based learning algorithm is proposed to boost the
inference capability of the deep neural network estimator by exploiting the
power of generative AI. Simulation results showcase that the proposed GAN-CRME
is even capable of coarse error-correction when the geographical map
information is inaccurate.
\\ ( https://arxiv.org/abs/2402.02729 ,  9886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02736 (*cross-listing*)
Date: Mon, 5 Feb 2024 05:37:48 GMT   (9002kb,D)

Title: Using Motion Cues to Supervise Single-Frame Body Pose and Shape
  Estimation in Low Data Regimes
Authors: Andrey Davydov, Alexey Sidnev, Artsiom Sanakoyeu, Yuhua Chen, Mathieu
  Salzmann, Pascal Fua
Categories: cs.CV cs.LG
Comments: 21 pages; TMLR
\\
  When enough annotated training data is available, supervised deep-learning
algorithms excel at estimating human body pose and shape using a single camera.
The effects of too little such data being available can be mitigated by using
other information sources, such as databases of body shapes, to learn priors.
Unfortunately, such sources are not always available either. We show that, in
such cases, easy-to-obtain unannotated videos can be used instead to provide
the required supervisory signals. Given a trained model using too little
annotated data, we compute poses in consecutive frames along with the optical
flow between them. We then enforce consistency between the image optical flow
and the one that can be inferred from the change in pose from one frame to the
next. This provides enough additional supervision to effectively refine the
network weights and to perform on par with methods trained using far more
annotated data.
\\ ( https://arxiv.org/abs/2402.02736 ,  9002kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02738 (*cross-listing*)
Date: Mon, 5 Feb 2024 05:38:50 GMT   (10543kb,D)

Title: Improving Robustness of LiDAR-Camera Fusion Model against Weather
  Corruption from Fusion Strategy Perspective
Authors: Yihao Huang, Kaiyuan Yu, Qing Guo, Felix Juefei-Xu, Xiaojun Jia,
  Tianlin Li, Geguang Pu, Yang Liu
Categories: cs.CV cs.LG
Comments: 17 pages
\\
  In recent years, LiDAR-camera fusion models have markedly advanced 3D object
detection tasks in autonomous driving. However, their robustness against common
weather corruption such as fog, rain, snow, and sunlight in the intricate
physical world remains underexplored. In this paper, we evaluate the robustness
of fusion models from the perspective of fusion strategies on the corrupted
dataset. Based on the evaluation, we further propose a concise yet practical
fusion strategy to enhance the robustness of the fusion models, namely flexibly
weighted fusing features from LiDAR and camera sources to adapt to varying
weather scenarios. Experiments conducted on four types of fusion models, each
with two distinct lightweight implementations, confirm the broad applicability
and effectiveness of the approach.
\\ ( https://arxiv.org/abs/2402.02738 ,  10543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02739 (*cross-listing*)
Date: Mon, 5 Feb 2024 05:46:31 GMT   (9689kb,D)

Title: DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models
Authors: Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi,
  Yan Wang, Yingying Chen, Bo Yuan
Categories: cs.CR cs.CV cs.LG
\\
  In the exciting generative AI era, the diffusion model has emerged as a very
powerful and widely adopted content generation and editing tool for various
data modalities, making the study of their potential security risks very
necessary and critical. Very recently, some pioneering works have shown the
vulnerability of the diffusion model against backdoor attacks, calling for
in-depth analysis and investigation of the security challenges of this popular
and fundamental AI technique.
  In this paper, for the first time, we systematically explore the
detectability of the poisoned noise input for the backdoored diffusion models,
an important performance metric yet little explored in the existing works.
Starting from the perspective of a defender, we first analyze the properties of
the trigger pattern in the existing diffusion backdoor attacks, discovering the
important role of distribution discrepancy in Trojan detection. Based on this
finding, we propose a low-cost trigger detection mechanism that can effectively
identify the poisoned input noise. We then take a further step to study the
same problem from the attack side, proposing a backdoor attack strategy that
can learn the unnoticeable trigger to evade our proposed detection scheme.
  Empirical evaluations across various diffusion models and datasets
demonstrate the effectiveness of the proposed trigger detection and
detection-evading attack strategy. For trigger detection, our distribution
discrepancy-based solution can achieve a 100\% detection rate for the Trojan
triggers used in the existing works. For evading trigger detection, our
proposed stealthy trigger design approach performs end-to-end learning to make
the distribution of poisoned noise input approach that of benign noise,
enabling nearly 100\% detection pass rate with very high attack and benign
performance for the backdoored diffusion models.
\\ ( https://arxiv.org/abs/2402.02739 ,  9689kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02754 (*cross-listing*)
Date: Mon, 5 Feb 2024 06:20:52 GMT   (701kb,D)

Title: Focal Modulation Networks for Interpretable Sound Classification
Authors: Luca Della Libera, Cem Subakan, Mirco Ravanelli
Categories: cs.SD cs.LG
Comments: Accepted to ICASSP 2024 XAI-SA Workshop
\\
  The increasing success of deep neural networks has raised concerns about
their inherent black-box nature, posing challenges related to interpretability
and trust. While there has been extensive exploration of interpretation
techniques in vision and language, interpretability in the audio domain has
received limited attention, primarily focusing on post-hoc explanations. This
paper addresses the problem of interpretability by-design in the audio domain
by utilizing the recently proposed attention-free focal modulation networks
(FocalNets). We apply FocalNets to the task of environmental sound
classification for the first time and evaluate their interpretability
properties on the popular ESC-50 dataset. Our method outperforms a similarly
sized vision transformer both in terms of accuracy and interpretability.
Furthermore, it is competitive against PIQ, a method specifically designed for
post-hoc interpretation in the audio domain.
\\ ( https://arxiv.org/abs/2402.02754 ,  701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02774 (*cross-listing*)
Date: Mon, 5 Feb 2024 07:14:18 GMT   (61kb)

Title: Accelerating Matroid Optimization through Fast Imprecise Oracles
Authors: Franziska Eberle, Felix Hommelsheim, Alexander Lindermayr, Zhenwei
  Liu, Nicole Megow, Jens Schl\"oter
Categories: cs.DS cs.LG
\\
  Querying complex models for precise information (e.g. traffic models,
database systems, large ML models) often entails intense computations and
results in long response times. Thus, weaker models which give imprecise
results quickly can be advantageous, provided inaccuracies can be resolved
using few queries to a stronger model. In the fundamental problem of computing
a maximum-weight basis of a matroid, a well-known generalization of many
combinatorial optimization problems, algorithms have access to a clean oracle
to query matroid information. We additionally equip algorithms with a fast but
dirty oracle modelling an unknown, potentially different matroid. We design and
analyze practical algorithms which only use few clean queries w.r.t. the
quality of the dirty oracle, while maintaining robustness against arbitrarily
poor dirty matroids, approaching the performance of classic algorithms for the
given problem. Notably, we prove that our algorithms are, in many respects,
best-possible. Further, we outline extensions to other matroid oracle types,
non-free dirty oracles and other matroid problems.
\\ ( https://arxiv.org/abs/2402.02774 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02795 (*cross-listing*)
Date: Mon, 5 Feb 2024 08:06:03 GMT   (186kb,D)

Title: A Learning-Based Caching Mechanism for Edge Content Delivery
Authors: Hoda Torabi, Hamzeh Khazaei, Marin Litoiu
Categories: cs.NI cs.DC cs.LG
\\
  With the advent of 5G networks and the rise of the Internet of Things (IoT),
Content Delivery Networks (CDNs) are increasingly extending into the network
edge. This shift introduces unique challenges, particularly due to the limited
cache storage and the diverse request patterns at the edge. These edge
environments can host traffic classes characterized by varied object-size
distributions and object-access patterns. Such complexity makes it difficult
for traditional caching strategies, which often rely on metrics like request
frequency or time intervals, to be effective. Despite these complexities, the
optimization of edge caching is crucial. Improved byte hit rates at the edge
not only alleviate the load on the network backbone but also minimize
operational costs and expedite content delivery to end-users.
  In this paper, we introduce HR-Cache, a comprehensive learning-based caching
framework grounded in the principles of Hazard Rate (HR) ordering, a rule
originally formulated to compute an upper bound on cache performance. HR-Cache
leverages this rule to guide future object eviction decisions. It employs a
lightweight machine learning model to learn from caching decisions made based
on HR ordering, subsequently predicting the "cache-friendliness" of incoming
requests. Objects deemed "cache-averse" are placed into cache as priority
candidates for eviction. Through extensive experimentation, we demonstrate that
HR-Cache not only consistently enhances byte hit rates compared to existing
state-of-the-art methods but also achieves this with minimal prediction
overhead.
  Our experimental results, using three real-world traces and one synthetic
trace, indicate that HR-Cache consistently achieves 2.2-14.6% greater WAN
traffic savings than LRU. It outperforms not only heuristic caching strategies
but also the state-of-the-art learning-based algorithm.
\\ ( https://arxiv.org/abs/2402.02795 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02797 (*cross-listing*)
Date: Mon, 5 Feb 2024 08:10:16 GMT   (8229kb,D)

Title: Joint Attention-Guided Feature Fusion Network for Saliency Detection of
  Surface Defects
Authors: Xiaoheng Jiang, Feng Yan, Yang Lu, Ke Wang, Shuai Guo, Tianzhu Zhang,
  Yanwei Pang, Jianwei Niu, and Mingliang Xu
Categories: cs.CV cs.LG
\\
  Surface defect inspection plays an important role in the process of
industrial manufacture and production. Though Convolutional Neural Network
(CNN) based defect inspection methods have made huge leaps, they still confront
a lot of challenges such as defect scale variation, complex background, low
contrast, and so on. To address these issues, we propose a joint
attention-guided feature fusion network (JAFFNet) for saliency detection of
surface defects based on the encoder-decoder network. JAFFNet mainly
incorporates a joint attention-guided feature fusion (JAFF) module into
decoding stages to adaptively fuse low-level and high-level features. The JAFF
module learns to emphasize defect features and suppress background noise during
feature fusion, which is beneficial for detecting low-contrast defects. In
addition, JAFFNet introduces a dense receptive field (DRF) module following the
encoder to capture features with rich context information, which helps detect
defects of different scales. The JAFF module mainly utilizes a learned joint
channel-spatial attention map provided by high-level semantic features to guide
feature fusion. The attention map makes the model pay more attention to defect
features. The DRF module utilizes a sequence of multi-receptive-field (MRF)
units with each taking as inputs all the preceding MRF feature maps and the
original input. The obtained DRF features capture rich context information with
a large range of receptive fields. Extensive experiments conducted on
SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves
promising performance in comparison with other state-of-the-art methods.
Meanwhile, our method reaches a real-time defect detection speed of 66 FPS.
\\ ( https://arxiv.org/abs/2402.02797 ,  8229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02816 (*cross-listing*)
Date: Mon, 5 Feb 2024 08:56:24 GMT   (616kb,D)

Title: Intersectional Two-sided Fairness in Recommendation
Authors: Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang,
  Shaoping Ma
Categories: cs.IR cs.LG
\\
  Fairness of recommender systems (RS) has attracted increasing attention
recently. Based on the involved stakeholders, the fairness of RS can be divided
into user fairness, item fairness, and two-sided fairness which considers both
user and item fairness simultaneously. However, we argue that the
intersectional two-sided unfairness may still exist even if the RS is two-sided
fair, which is observed and shown by empirical studies on real-world data in
this paper, and has not been well-studied previously. To mitigate this problem,
we propose a novel approach called Intersectional Two-sided Fairness
Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive
disadvantaged groups, and then uses collaborative loss balance to develop
consistent distinguishing abilities for different intersectional groups.
Additionally, predicted score normalization is leveraged to align positive
predicted scores to fairly treat positives in different intersectional groups.
Extensive experiments and analyses on three public datasets show that our
proposed approach effectively alleviates the intersectional two-sided
unfairness and consistently outperforms previous state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.02816 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02817 (*cross-listing*)
Date: Mon, 5 Feb 2024 08:59:47 GMT   (825kb,D)

Title: Bayes-Optimal Fair Classification with Linear Disparity Constraints via
  Pre-, In-, and Post-processing
Authors: Xianli Zeng, Guang Cheng and Edgar Dobriban
Categories: stat.ML cs.LG
\\
  Machine learning algorithms may have disparate impacts on protected groups.
To address this, we develop methods for Bayes-optimal fair classification,
aiming to minimize classification error subject to given group fairness
constraints. We introduce the notion of \emph{linear disparity measures}, which
are linear functions of a probabilistic classifier; and \emph{bilinear
disparity measures}, which are also linear in the group-wise regression
functions. We show that several popular disparity measures -- the deviations
from demographic parity, equality of opportunity, and predictive equality --
are bilinear.
  We find the form of Bayes-optimal fair classifiers under a single linear
disparity measure, by uncovering a connection with the Neyman-Pearson lemma.
For bilinear disparity measures, Bayes-optimal fair classifiers become
group-wise thresholding rules. Our approach can also handle multiple fairness
constraints (such as equalized odds), and the common scenario when the
protected attribute cannot be used at the prediction phase.
  Leveraging our theoretical results, we design methods that learn fair
Bayes-optimal classifiers under bilinear disparity constraints. Our methods
cover three popular approaches to fairness-aware classification, via
pre-processing (Fair Up- and Down-Sampling), in-processing (Fair Cost-Sensitive
Classification) and post-processing (a Fair Plug-In Rule). Our methods control
disparity directly while achieving near-optimal fairness-accuracy tradeoffs. We
show empirically that our methods compare favorably to existing algorithms.
\\ ( https://arxiv.org/abs/2402.02817 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02842 (*cross-listing*)
Date: Mon, 5 Feb 2024 09:53:08 GMT   (4351kb,D)

Title: Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One
Authors: Jing Yan, Liu Jiang, Jianfei Cui, Zhichen Zhao, Xingyan Bin, Feng
  Zhang, Zuotao Liu
Categories: cs.IR cs.LG
\\
  Interest modeling in recommender system has been a constant topic for
improving user experience, and typical interest modeling tasks (e.g.
multi-interest, long-tail interest and long-term interest) have been
investigated in many existing works. However, most of them only consider one
interest in isolation, while neglecting their interrelationships. In this
paper, we argue that these tasks suffer from a common "interest amnesia"
problem, and a solution exists to mitigate it simultaneously. We figure that
long-term cues can be the cornerstone since they reveal multi-interest and
clarify long-tail interest. Inspired by the observation, we propose a novel and
unified framework in the retrieval stage, "Trinity", to solve interest amnesia
problem and improve multiple interest modeling tasks. We construct a real-time
clustering system that enables us to project items into enumerable clusters,
and calculate statistical interest histograms over these clusters. Based on
these histograms, Trinity recognizes underdelivered themes and remains stable
when facing emerging hot topics. Trinity is more appropriate for large-scale
industry scenarios because of its modest computational overheads. Its derived
retrievers have been deployed on the recommender system of Douyin,
significantly improving user experience and retention. We believe that such
practical experience can be well generalized to other scenarios.
\\ ( https://arxiv.org/abs/2402.02842 ,  4351kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02846 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:00:28 GMT   (5122kb,D)

Title: Machine Learning Resistant Amorphous Silicon Physically Unclonable
  Functions (PUFs)
Authors: Velat Kilic, Neil Macfarlane, Jasper Stround, Samuel Metais, Milad
  Alemohammad, A. Brinton Cooper, Amy C. Foster, Mark A. Foster
Categories: physics.optics cs.LG physics.app-ph
\\
  We investigate usage of nonlinear wave chaotic amorphous silicon (a-Si)
cavities as physically unclonable functions (PUF). Machine learning attacks on
integrated electronic PUFs have been demonstrated to be very effective at
modeling PUF behavior. Such attacks on integrated a-Si photonic PUFs are
investigated through application of algorithms including linear regression,
k-nearest neighbor, decision tree ensembles (random forests and gradient
boosted trees), and deep neural networks (DNNs). We found that DNNs performed
the best among all the algorithms studied but still failed to completely break
the a-Si PUF security which we quantify through a private information metric.
Furthermore, machine learning resistance of a-Si PUFs were found to be directly
related to the strength of their nonlinear response.
\\ ( https://arxiv.org/abs/2402.02846 ,  5122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02850 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:03:28 GMT   (1149kb,D)

Title: An Attention Long Short-Term Memory based system for automatic
  classification of speech intelligibility
Authors: Miguel Fern\'andez-D\'iaz and Ascensi\'on Gallardo-Antol\'in
Categories: eess.AS cs.LG
Journal-ref: Miguel Fernandez-Diaz and Ascension Gallardo-Antolin Engineering
  Applications of Artificial Intelligence 96 (2020) 103976
DOI: 10.1016/j.engappai.2020.103976
\\
  Speech intelligibility can be degraded due to multiple factors, such as noisy
environments, technical difficulties or biological conditions. This work is
focused on the development of an automatic non-intrusive system for predicting
the speech intelligibility level in this latter case. The main contribution of
our research on this topic is the use of Long Short-Term Memory (LSTM) networks
with log-mel spectrograms as input features for this purpose. In addition, this
LSTM-based system is further enhanced by the incorporation of a simple
attention mechanism that is able to determine the more relevant frames to this
task. The proposed models are evaluated with the UA-Speech database that
contains dysarthric speech with different degrees of severity. Results show
that the attention LSTM architecture outperforms both, a reference Support
Vector Machine (SVM)-based system with hand-crafted features and a LSTM-based
system with Mean-Pooling.
\\ ( https://arxiv.org/abs/2402.02850 ,  1149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02851 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:06:24 GMT   (4664kb,D)

Title: Enhancing Compositional Generalization via Compositional Feature
  Alignment
Authors: Haoxiang Wang, Haozhe Si, Huajie Shao, Han Zhao
Categories: cs.CV cs.LG stat.ML
Comments: Code is released at
  https://github.com/Haoxiang-Wang/Compositional-Feature-Alignment
\\
  Real-world applications of machine learning models often confront data
distribution shifts, wherein discrepancies exist between the training and test
data distributions. In the common multi-domain multi-class setup, as the number
of classes and domains scales up, it becomes infeasible to gather training data
for every domain-class combination. This challenge naturally leads the quest
for models with Compositional Generalization (CG) ability, where models can
generalize to unseen domain-class combinations. To delve into the CG challenge,
we develop CG-Bench, a suite of CG benchmarks derived from existing real-world
image datasets, and observe that the prevalent pretraining-finetuning paradigm
on foundational models, such as CLIP and DINOv2, struggles with the challenge.
To address this challenge, we propose Compositional Feature Alignment (CFA), a
simple two-stage finetuning technique that i) learns two orthogonal linear
heads on a pretrained encoder with respect to class and domain labels, and ii)
fine-tunes the encoder with the newly learned head frozen. We theoretically and
empirically justify that CFA encourages compositional feature learning of
pretrained models. We further conduct extensive experiments on CG-Bench for
CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment
results show that CFA outperforms common finetuning techniques in compositional
generalization, corroborating CFA's efficacy in compositional feature learning.
\\ ( https://arxiv.org/abs/2402.02851 ,  4664kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02855 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:16:20 GMT   (444kb,D)

Title: Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation
Authors: Shuyao Wang, Yongduo Sui, Jiancan Wu, Zhi Zheng, Hui Xiong
Categories: cs.IR cs.LG
Comments: 10 pages, 5 figures, 4 tables. Accecpted by WSDM 2024
DOI: 10.1145/3616855.3635780
\\
  In the realm of deep learning-based recommendation systems, the increasing
computational demands, driven by the growing number of users and items, pose a
significant challenge to practical deployment. This challenge is primarily
twofold: reducing the model size while effectively learning user and item
representations for efficient recommendations. Despite considerable
advancements in model compression and architecture search, prevalent approaches
face notable constraints. These include substantial additional computational
costs from pre-training/re-training in model compression and an extensive
search space in architecture design. Additionally, managing complexity and
adhering to memory constraints is problematic, especially in scenarios with
strict time or space limitations. Addressing these issues, this paper
introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored
for recommendation models. DSL innovatively trains a lightweight sparse model
from scratch, periodically evaluating and dynamically adjusting each weight's
significance and the model's sparsity distribution during the training. This
approach ensures a consistent and minimal parameter budget throughout the full
learning lifecycle, paving the way for "end-to-end" efficiency from training to
inference. Our extensive experimental results underline DSL's effectiveness,
significantly reducing training and inference costs while delivering comparable
recommendation performance.
\\ ( https://arxiv.org/abs/2402.02855 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02857 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:17:36 GMT   (1007kb,D)

Title: Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation
Authors: Sobihan Surendran (LPSM (UMR\_8001)), Antoine Godichon-Baggioni (LPSM
  (UMR\_8001)), Adeline Fermanian, Sylvain Le Corff (LPSM (UMR\_8001))
Categories: stat.ML cs.LG
\\
  Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for
training deep neural networks. Most theoretical results assume access to
unbiased gradient estimators, which is not the case in several recent deep
learning and reinforcement learning applications that use Monte Carlo methods.
This paper provides a comprehensive non-asymptotic analysis of SGD with biased
gradients and adaptive steps for convex and non-convex smooth functions. Our
study incorporates time-dependent bias and emphasizes the importance of
controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In
particular, we establish that Adagrad and RMSProp with biased gradients
converge to critical points for smooth non-convex functions at a rate similar
to existing results in the literature for the unbiased case. Finally, we
provide experimental results using Variational Autoenconders (VAE) that
illustrate our convergence results and show how the effect of bias can be
reduced by appropriate hyperparameter tuning.
\\ ( https://arxiv.org/abs/2402.02857 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02862 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:22:15 GMT   (480kb,D)

Title: Graph Neural Machine: A New Model for Learning with Tabular Data
Authors: Giannis Nikolentzos and Siyun Wang and Johannes Lutzeyer and Michalis
  Vazirgiannis
Categories: stat.ML cs.LG
\\
  In recent years, there has been a growing interest in mapping data from
different domains to graph structures. Among others, neural network models such
as the multi-layer perceptron (MLP) can be modeled as graphs. In fact, MLPs can
be represented as directed acyclic graphs. Graph neural networks (GNNs) have
recently become the standard tool for performing machine learning tasks on
graphs. In this work, we show that an MLP is equivalent to an asynchronous
message passing GNN model which operates on the MLP's graph representation. We
then propose a new machine learning model for tabular data, the so-called Graph
Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a
nearly complete graph and which employs a synchronous message passing scheme.
We show that a single GNM model can simulate multiple MLP models. We evaluate
the proposed model in several classification and regression datasets. In most
cases, the GNM model outperforms the MLP architecture.
\\ ( https://arxiv.org/abs/2402.02862 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02865 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:26:28 GMT   (1937kb,D)

Title: On combining acoustic and modulation spectrograms in an attention
  LSTM-based system for speech intelligibility level classification
Authors: Ascensi\'on Gallardo-Antol\'in and Juan M. Montero
Categories: eess.AS cs.LG
Journal-ref: Ascension Gallardo-Antolin and Juan M. Montero Neurocomputing 456
  (2021) 49-60
DOI: 10.1016/j.neucom.2021.05.065
\\
  Speech intelligibility can be affected by multiple factors, such as noisy
environments, channel distortions or physiological issues. In this work, we
deal with the problem of automatic prediction of the speech intelligibility
level in this latter case. Starting from our previous work, a non-intrusive
system based on LSTM networks with attention mechanism designed for this task,
we present two main contributions. In the first one, it is proposed the use of
per-frame modulation spectrograms as input features, instead of compact
representations derived from them that discard important temporal information.
In the second one, two different strategies for the combination of per-frame
acoustic log-mel and modulation spectrograms into the LSTM framework are
explored: at decision level or late fusion and at utterance level or
Weighted-Pooling (WP) fusion. The proposed models are evaluated with the
UA-Speech database that contains dysarthric speech with different degrees of
severity. On the one hand, results show that attentional LSTM networks are able
to adequately modeling the modulation spectrograms sequences producing similar
classification rates as in the case of log-mel spectrograms. On the other hand,
both combination strategies, late and WP fusion, outperform the single-feature
systems, suggesting that per-frame log-mel and modulation spectrograms carry
complementary information for the task of speech intelligibility prediction,
than can be effectively exploited by the LSTM-based architectures, being the
system with the WP fusion strategy and Attention-Pooling the one that achieves
best results.
\\ ( https://arxiv.org/abs/2402.02865 ,  1937kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02866 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:28:20 GMT   (1353kb,D)

Title: Quantum Normalizing Flows for Anomaly Detection
Authors: Bodo Rosenhahn, Christoph Hirche
Categories: quant-ph cs.LG
Comments: 11 pages, 6 figures
\\
  A Normalizing Flow computes a bijective mapping from an arbitrary
distribution to a predefined (e.g. normal) distribution. Such a flow can be
used to address different tasks, e.g. anomaly detection, once such a mapping
has been learned. In this work we introduce Normalizing Flows for Quantum
architectures, describe how to model and optimize such a flow and evaluate our
method on example datasets. Our proposed models show competitive performance
for anomaly detection compared to classical methods, e.g. based on isolation
forests, the local outlier factor (LOF) or single-class SVMs, while being fully
executable on a quantum computer.
\\ ( https://arxiv.org/abs/2402.02866 ,  1353kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02880 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:47:46 GMT   (553kb,D)

Title: Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks
Authors: Han-Xiao Tao and Jiaqi Hu and Re-Bing Wu
Categories: quant-ph cs.ET cs.LG
Comments: 9 pages; 6 figures
\\
  Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum
(NISQ) devices requires the optimal utilization of limited quantum resources.
The commonly used gate-based QML models are convenient for software engineers,
but their expressivity is restricted by the permissible circuit depth within a
finite coherence time. In contrast, pulse-based models enable the construction
of "infinitely" deep quantum neural networks within the same coherence time,
which may unleash greater expressive power for complex learning tasks. In this
paper, we investigate this potential from the perspective of quantum control
theory. We first indicate that the nonlinearity of pulse-based models comes
from the encoding process that can be viewed as the continuous limit of
data-reuploading in gate-based models. Subsequently, we prove that the
pulse-based model can approximate arbitrary nonlinear functions when the
underlying physical system is ensemble controllable. Under this condition,
numerical simulations show that the expressivity can be enhanced by either
increasing the pulse length or the number of qubits. As anticipated, we
demonstrate through numerical examples that the pulse-based model can unleash
more expressive power compared to the gate-based model. These findings
establish a theoretical foundation for understanding and designing expressive
QML models using NISQ devices.
\\ ( https://arxiv.org/abs/2402.02880 ,  553kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02887 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:55:47 GMT   (2523kb,D)

Title: Time-, Memory- and Parameter-Efficient Visual Adaptation
Authors: Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab
Categories: cs.CV cs.LG
\\
  As foundation models become more popular, there is a growing need to
efficiently finetune them for downstream tasks. Although numerous adaptation
methods have been proposed, they are designed to be efficient only in terms of
how many parameters are trained. They, however, typically still require
backpropagating gradients throughout the model, meaning that their
training-time and -memory cost does not reduce as significantly.
  We propose an adaptation method which does not backpropagate gradients
through the backbone. We achieve this by designing a lightweight network in
parallel that operates on features from the frozen, pretrained backbone. As a
result, our method is efficient not only in terms of parameters, but also in
training-time and memory usage. Our approach achieves state-of-the-art
accuracy-parameter trade-offs on the popular VTAB benchmark, and we further
show how we outperform prior works with respect to training-time and -memory
usage too. We further demonstrate the training efficiency and scalability of
our method by adapting a vision transformer backbone of 4 billion parameters
for the computationally demanding task of video classification, without any
intricate model parallelism. Here, we outperform a prior adaptor-based method
which could only scale to a 1 billion parameter backbone, or fully-finetuning a
smaller backbone, with the same GPU and less training time.
\\ ( https://arxiv.org/abs/2402.02887 ,  2523kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02889 (*cross-listing*)
Date: Mon, 5 Feb 2024 10:57:48 GMT   (682kb,D)

Title: Exploring Federated Self-Supervised Learning for General Purpose Audio
  Understanding
Authors: Yasar Abbas Ur Rehman, Kin Wai Lau, Yuyang Xie, Lan Ma, Jiajun Shen
Categories: cs.SD cs.CV cs.LG
\\
  The integration of Federated Learning (FL) and Self-supervised Learning (SSL)
offers a unique and synergetic combination to exploit the audio data for
general-purpose audio understanding, without compromising user data privacy.
However, rare efforts have been made to investigate the SSL models in the FL
regime for general-purpose audio understanding, especially when the training
data is generated by large-scale heterogeneous audio sources. In this paper, we
evaluate the performance of feature-matching and predictive audio-SSL
techniques when integrated into large-scale FL settings simulated with
non-independently identically distributed (non-iid) data. We propose a novel
Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning
intermediate feature representations from large-scale decentralized
heterogeneous clients, holding unlabelled audio data. Our study has found that
audio F-SSL approaches perform on par with the centralized audio-SSL approaches
on the audio-retrieval task. Extensive experiments demonstrate the
effectiveness and significance of FASSL as it assists in obtaining the optimal
global model for state-of-the-art FL aggregation methods.
\\ ( https://arxiv.org/abs/2402.02889 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02906 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:22:14 GMT   (823kb,D)

Title: ViewFusion: Learning Composable Diffusion Models for Novel View
  Synthesis
Authors: Bernard Spiegl, Andrea Perin, St\'ephane Deny, Alexander Ilin
Categories: cs.CV cs.LG
\\
  Deep learning is providing a wealth of new approaches to the old problem of
novel view synthesis, from Neural Radiance Field (NeRF) based approaches to
end-to-end style architectures. Each approach offers specific strengths but
also comes with specific limitations in their applicability. This work
introduces ViewFusion, a state-of-the-art end-to-end generative approach to
novel view synthesis with unparalleled flexibility. ViewFusion consists in
simultaneously applying a diffusion denoising step to any number of input views
of a scene, then combining the noise gradients obtained for each view with an
(inferred) pixel-weighting mask, ensuring that for each region of the target
scene only the most informative input views are taken into account. Our
approach resolves several limitations of previous approaches by (1) being
trainable and generalizing across multiple scenes and object classes, (2)
adaptively taking in a variable number of pose-free views at both train and
test time, (3) generating plausible views even in severely undetermined
conditions (thanks to its generative nature) -- all while generating views of
quality on par or even better than state-of-the-art methods. Limitations
include not generating a 3D embedding of the scene, resulting in a relatively
slow inference speed, and our method only being tested on the relatively small
dataset NMR. Code is available.
\\ ( https://arxiv.org/abs/2402.02906 ,  823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02909 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:25:42 GMT   (1250kb)

Title: Digital Twin for Grey Box modeling of Multistory residential building
  thermal dynamics
Authors: Lina Morkunaite, Justas Kardoka, Darius Pupeikis, Paris Fokaides,
  Vangelis Angelakis
Categories: stat.AP cs.LG
\\
  Buildings energy efficiency is a widely researched topic, which is rapidly
gaining popularity due to rising environmental concerns and the need for energy
independence. In Northern Europe heating energy alone accounts for up to 70
percent of the total building energy consumption. Industry 4.0 technologies
such as IoT, big data, cloud computing and machine learning, along with the
creation of predictive and proactive digital twins, can help to reduce this
number. However, buildings thermal dynamics is a very complex process that
depends on many variables. As a result, commonly used physics-based white box
models are time-consuming and require vast expertise. On the contrary, black
box forecasting models, which rely primarily on building energy consumption
data, lack fundamental insights and hinder re-use. In this study we propose an
architecture to facilitate grey box modelling of building thermal dynamics
while integrating real time IoT data with 3D representation of buildings. The
architecture is validated in a case study creating a digital twin platform that
enables users to define the thermal dynamics of buildings based on physical
laws and real data, thus facilitating informed decision making for the best
heating energy optimization strategy. Also, the created user interface enables
stakeholders such as facility managers, energy providers or governing bodies to
analyse, compare and evaluate buildings thermal dynamics without extensive
expertise or time resources.
\\ ( https://arxiv.org/abs/2402.02909 ,  1250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02928 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:47:45 GMT   (41136kb,D)

Title: Instance Segmentation XXL-CT Challenge of a Historic Airplane
Authors: Roland Gruber and Johann Christopher Engster and Markus Michen and
  Nele Blum and Maik Stille and Stefan Gerth and Thomas Wittenberg
Categories: cs.CV cs.LG
\\
  Instance segmentation of compound objects in XXL-CT imagery poses a unique
challenge in non-destructive testing. This complexity arises from the lack of
known reference segmentation labels, limited applicable segmentation tools, as
well as partially degraded image quality. To asses recent advancements in the
field of machine learning-based image segmentation, the "Instance Segmentation
XXL-CT Challenge of a Historic Airplane" was conducted. The challenge aimed to
explore automatic or interactive instance segmentation methods for an efficient
delineation of the different aircraft components, such as screws, rivets, metal
sheets or pressure tubes. We report the organization and outcome of this
challenge and describe the capabilities and limitations of the submitted
segmentation methods.
\\ ( https://arxiv.org/abs/2402.02928 ,  41136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02930 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:52:23 GMT   (348kb,D)

Title: Embedding Hardware Approximations in Discrete Genetic-based Training for
  Printed MLPs
Authors: Florentia Afentaki, Michael Hefenbrock, Georgios Zervakis, Mehdi B.
  Tahoori
Categories: cs.AR cs.LG
Comments: Accepted for publication at the 27th Design, Automation and Test in
  Europe Conference (DATE'24), Mar 25-27 2024, Valencia, Spain
\\
  Printed Electronics (PE) stands out as a promisingtechnology for widespread
computing due to its distinct attributes, such as low costs and flexible
manufacturing. Unlike traditional silicon-based technologies, PE enables
stretchable, conformal,and non-toxic hardware. However, PE are constrained by
larger feature sizes, making it challenging to implement complex circuits such
as machine learning (ML) classifiers. Approximate computing has been proven to
reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs).
In this paper, we maximize the benefits of approximate computing by integrating
hardware approximation into the MLP training process. Due to the discrete
nature of hardware approximation, we propose and implement a genetic-based,
approximate, hardware-aware training approach specifically designed for printed
MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction
compared to the baseline while outperforming state of-the-art approximate and
stochastic printed MLPs.
\\ ( https://arxiv.org/abs/2402.02930 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02932 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:55:30 GMT   (440kb,D)

Title: Domain Adaptation of Multilingual Semantic Search - Literature Review
Authors: Anna Bringmann, Anastasia Zhukova
Categories: cs.IR cs.LG
\\
  This literature review gives an overview of current approaches to perform
domain adaptation in a low-resource and approaches to perform multilingual
semantic search in a low-resource setting. We developed a new typology to
cluster domain adaptation approaches based on the part of dense textual
information retrieval systems, which they adapt, focusing on how to combine
them efficiently. We also explore the possibilities of combining multilingual
semantic search with domain adaptation approaches for dense retrievers in a
low-resource setting.
\\ ( https://arxiv.org/abs/2402.02932 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02936 (*cross-listing*)
Date: Mon, 5 Feb 2024 11:58:08 GMT   (5744kb,D)

Title: Panoramic Image Inpainting With Gated Convolution And Contextual
  Reconstruction Loss
Authors: Li Yu, Yanjun Gao, Farhad Pakdaman, Moncef Gabbouj
Categories: eess.IV cs.CV cs.LG cs.MM
Comments: Copyright 2024 IEEE - to appear in IEEE ICASSP 2024
\\
  Deep learning-based methods have demonstrated encouraging results in tackling
the task of panoramic image inpainting. However, it is challenging for existing
methods to distinguish valid pixels from invalid pixels and find suitable
references for corrupted areas, thus leading to artifacts in the inpainted
results. In response to these challenges, we propose a panoramic image
inpainting framework that consists of a Face Generator, a Cube Generator, a
side branch, and two discriminators. We use the Cubemap Projection (CMP) format
as network input. The generator employs gated convolutions to distinguish valid
pixels from invalid ones, while a side branch is designed utilizing contextual
reconstruction (CR) loss to guide the generators to find the most suitable
reference patch for inpainting the missing region. The proposed method is
compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in
terms of PSNR and SSIM. Experimental results and ablation study demonstrate
that the proposed method outperforms SOTA both quantitatively and
qualitatively.
\\ ( https://arxiv.org/abs/2402.02936 ,  5744kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02938 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:00:31 GMT   (943kb)

Title: Design and Implementation of an Automated Disaster-recovery System for a
  Kubernetes Cluster Using LSTM
Authors: Ji-Beom Kim, Je-Bum Choi, and Eun-Sung Jung
Categories: cs.DC cs.LG
\\
  With the increasing importance of data in the modern business environment,
effective data man-agement and protection strategies are gaining increasing
research attention. Data protection in a cloud environment is crucial for
safeguarding information assets and maintaining sustainable services. This
study introduces a system structure that integrates Kubernetes management
plat-forms with backup and restoration tools. This system is designed to
immediately detect disasters and automatically recover applications from
another kubernetes cluster. The experimental results show that this system
executes the restoration process within 15 s without human intervention,
enabling rapid recovery. This, in turn, significantly reduces the potential for
delays and errors compared with manual recovery processes, thereby enhancing
data management and recovery ef-ficiency in cloud environments. Moreover, our
research model predicts the CPU utilization of the cluster using Long
Short-Term Memory (LSTM). The necessity of scheduling through this predict is
made clearer through comparison with experiments without scheduling,
demonstrating its ability to prevent performance degradation. This research
highlights the efficiency and necessity of automatic recovery systems in cloud
environments, setting a new direction for future research.
\\ ( https://arxiv.org/abs/2402.02938 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02941 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:08:17 GMT   (2463kb)

Title: Exploring the Synergies of Hybrid CNNs and ViTs Architectures for
  Computer Vision: A survey
Authors: Haruna Yunusa, Shiyin Qin, Abdulrahman Hamman Adama Chukkol,
  Abdulganiyu Abdu Yusuf, Isah Bello, Adamu Lawan
Categories: cs.CV cs.LG
\\
  The hybrid of Convolutional Neural Network (CNN) and Vision Transformers
(ViT) architectures has emerged as a groundbreaking approach, pushing the
boundaries of computer vision (CV). This comprehensive review provides a
thorough examination of the literature on state-of-the-art hybrid CNN-ViT
architectures, exploring the synergies between these two approaches. The main
content of this survey includes: (1) a background on the vanilla CNN and ViT,
(2) systematic review of various taxonomic hybrid designs to explore the
synergy achieved through merging CNNs and ViTs models, (3) comparative analysis
and application task-specific synergy between different hybrid architectures,
(4) challenges and future directions for hybrid models, (5) lastly, the survey
concludes with a summary of key findings and recommendations. Through this
exploration of hybrid CV architectures, the survey aims to serve as a guiding
resource, fostering a deeper understanding of the intricate dynamics between
CNNs and ViTs and their collective impact on shaping the future of CV
architectures.
\\ ( https://arxiv.org/abs/2402.02941 ,  2463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02946 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:19:16 GMT   (483kb,D)

Title: HoughToRadon Transform: New Neural Network Layer for Features
  Improvement in Projection Space
Authors: Alexandra Zhabitskaya, Alexander Sheshkus, and Vladimir L. Arlazarov
Categories: cs.CV cs.LG cs.NE
\\
  In this paper, we introduce HoughToRadon Transform layer, a novel layer
designed to improve the speed of neural networks incorporated with Hough
Transform to solve semantic image segmentation problems. By placing it after a
Hough Transform layer, "inner" convolutions receive modified feature maps with
new beneficial properties, such as a smaller area of processed images and
parameter space linearity by angle and shift. These properties were not
presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer
allows us to adjust the size of intermediate feature maps using two new
parameters, thus allowing us to balance the speed and quality of the resulting
neural network. Our experiments on the open MIDV-500 dataset show that this new
approach leads to time savings in document segmentation tasks and achieves
state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger
computational complexity.
\\ ( https://arxiv.org/abs/2402.02946 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02952 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:31:18 GMT   (58kb)

Title: On Least Squares Estimation in Softmax Gating Mixture of Experts
Authors: Huy Nguyen and Nhat Ho and Alessandro Rinaldo
Categories: stat.ML cs.LG
Comments: 36 pages
\\
  Mixture of experts (MoE) model is a statistical machine learning design that
aggregates multiple expert networks using a softmax gating function in order to
form a more intricate and expressive model. Despite being commonly used in
several applications owing to their scalability, the mathematical and
statistical properties of MoE models are complex and difficult to analyze. As a
result, previous theoretical works have primarily focused on probabilistic MoE
models by imposing the impractical assumption that the data are generated from
a Gaussian MoE model. In this work, we investigate the performance of the least
squares estimators (LSE) under a deterministic MoE model where the data are
sampled according to a regression model, a setting that has remained largely
unexplored. We establish a condition called strong identifiability to
characterize the convergence behavior of various types of expert functions. We
demonstrate that the rates for estimating strongly identifiable experts, namely
the widely used feed forward networks with activation functions
$\mathrm{sigmoid}(\cdot)$ and $\tanh(\cdot)$, are substantially faster than
those of polynomial experts, which we show to exhibit a surprising slow
estimation rate. Our findings have important practical implications for expert
selection.
\\ ( https://arxiv.org/abs/2402.02952 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02953 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:31:19 GMT   (2487kb,D)

Title: Unraveling the Key of Machine Learning Solutions for Android Malware
  Detection
Authors: Jiahao Liu, Jun Zeng, Fabio Pierazzi, Lorenzo Cavallaro, Zhenkai Liang
Categories: cs.CR cs.LG
\\
  Android malware detection serves as the front line against malicious apps.
With the rapid advancement of machine learning (ML), ML-based Android malware
detection has attracted increasing attention due to its capability of
automatically capturing malicious patterns from Android APKs. These
learning-driven methods have reported promising results in detecting malware.
However, the absence of an in-depth analysis of current research progress makes
it difficult to gain a holistic picture of the state of the art in this area.
  This paper presents a comprehensive investigation to date into ML-based
Android malware detection with empirical and quantitative analysis. We first
survey the literature, categorizing contributions into a taxonomy based on the
Android feature engineering and ML modeling pipeline. Then, we design a
general-propose framework for ML-based Android malware detection, re-implement
12 representative approaches from different research communities, and evaluate
them from three primary dimensions, i.e., effectiveness, robustness, and
efficiency. The evaluation reveals that ML-based approaches still face open
challenges and provides insightful findings like more powerful ML models are
not the silver bullet for designing better malware detectors. We further
summarize our findings and put forth recommendations to guide future research.
\\ ( https://arxiv.org/abs/2402.02953 ,  2487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02954 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:33:05 GMT   (405kb,D)

Title: Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form
  Game Approach
Authors: Johan Peralez, Aur\'elien Delage, Olivier Buffet, Jilles S. Dibangoye
Categories: cs.GT cs.LG
\\
  A recent theory shows that a multi-player decentralized partially observable
Markov decision process can be transformed into an equivalent single-player
game, enabling the application of \citeauthor{bellman}'s principle of
optimality to solve the single-player game by breaking it down into
single-stage subgames. However, this approach entangles the decision variables
of all players at each single-stage subgame, resulting in backups with a
double-exponential complexity. This paper demonstrates how to disentangle these
decision variables while maintaining optimality under hierarchical information
sharing, a prominent management style in our society. To achieve this, we apply
the principle of optimality to solve any single-stage subgame by breaking it
down further into smaller subgames, enabling us to make single-player decisions
at a time. Our approach reveals that extensive-form games always exist with
solutions to a single-stage subgame, significantly reducing time complexity.
Our experimental results show that the algorithms leveraging these findings can
scale up to much larger multi-player games without compromising optimality.
\\ ( https://arxiv.org/abs/2402.02954 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02956 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:34:03 GMT   (2669kb,D)

Title: AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a
  Single High-Resolution Image
Authors: Hamed Amini Amirkolaee, Miaojing Shi, Lianghua He, and Mark Mulligan
Categories: cs.CV cs.LG
\\
  The process of estimating and counting tree density using only a single
aerial or satellite image is a difficult task in the fields of photogrammetry
and remote sensing. However, it plays a crucial role in the management of
forests. The huge variety of trees in varied topography severely hinders tree
counting models to perform well. The purpose of this paper is to propose a
framework that is learnt from the source domain with sufficient labeled trees
and is adapted to the target domain with only a limited number of labeled
trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a
hierarchical feature extraction scheme to extract robust features from the
source and target domains. It also consists of three subnets: two for
extracting self-domain attention maps from source and target domains
respectively and one for extracting cross-domain attention maps. For the
latter, an attention-to-adapt mechanism is introduced to distill relevant
information from different domains while generating tree density maps; a
hierarchical cross-domain feature alignment scheme is proposed that
progressively aligns the features from the source and target domains. We also
adopt adversarial learning into the framework to further reduce the gap between
source and target domains. Our AdaTreeFormer is evaluated on six designed
domain adaptation tasks using three tree counting datasets, ie Jiangsu,
Yosemite, and London; and outperforms the state of the art methods
significantly.
\\ ( https://arxiv.org/abs/2402.02956 ,  2669kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02957 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:36:08 GMT   (5299kb,D)

Title: Multi-Agent Reinforcement Learning for Offloading Cellular
  Communications with Cooperating UAVs
Authors: Abhishek Mondal, Deepak Mishra, Ganesh Prasad, George C.
  Alexandropoulos, Azzam Alnahari, Riku Jantti
Categories: eess.SY cs.LG cs.SY
\\
  Effective solutions for intelligent data collection in terrestrial cellular
networks are crucial, especially in the context of Internet of Things
applications. The limited spectrum and coverage area of terrestrial base
stations pose challenges in meeting the escalating data rate demands of network
users. Unmanned aerial vehicles, known for their high agility, mobility, and
flexibility, present an alternative means to offload data traffic from
terrestrial BSs, serving as additional access points. This paper introduces a
novel approach to efficiently maximize the utilization of multiple UAVs for
data traffic offloading from terrestrial BSs. Specifically, the focus is on
maximizing user association with UAVs by jointly optimizing UAV trajectories
and users association indicators under quality of service constraints. Since,
the formulated UAVs control problem is nonconvex and combinatorial, this study
leverages the multi agent reinforcement learning framework. In this framework,
each UAV acts as an independent agent, aiming to maintain inter UAV cooperative
behavior. The proposed approach utilizes the finite state Markov decision
process to account for UAVs velocity constraints and the relationship between
their trajectories and state space. A low complexity distributed state action
reward state action algorithm is presented to determine UAVs optimal sequential
decision making policies over training episodes. The extensive simulation
results validate the proposed analysis and offer valuable insights into the
optimal UAV trajectories. The derived trajectories demonstrate superior average
UAV association performance compared to benchmark techniques such as Q learning
and particle swarm optimization.
\\ ( https://arxiv.org/abs/2402.02957 ,  5299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02963 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:41:30 GMT   (3811kb,D)

Title: One-class anomaly detection through color-to-thermal AI for building
  envelope inspection
Authors: Polina Kurtser, Kailun Feng, Thomas Olofsson, Aitor De Andres
Categories: eess.IV cs.CV cs.LG
\\
  We present a label-free method for detecting anomalies during thermographic
inspection of building envelopes. It is based on the AI-driven prediction of
thermal distributions from color images. Effectively the method performs as a
one-class classifier of the thermal image regions with high mismatch between
the predicted and actual thermal distributions. The algorithm can learn to
identify certain features as normal or anomalous by selecting the target sample
used for training. We demonstrated this principle by training the algorithm
with data collected at different outdoors temperature, which lead to the
detection of thermal bridges. The method can be implemented to assist human
professionals during routine building inspections or combined with mobile
platforms for automating examination of large areas.
\\ ( https://arxiv.org/abs/2402.02963 ,  3811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02968 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:47:09 GMT   (6845kb,D)

Title: Delving into Multi-modal Multi-task Foundation Models for Road Scene
  Understanding: From Learning Paradigm Perspectives
Authors: Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang,
  Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, Ling Shao, Yi Yang, Bojun Gao,
  Qun Li and Guobin Wu
Categories: cs.CV cs.LG
Comments: 24 pages, 9 figures, 1 table
\\
  Foundation models have indeed made a profound impact on various fields,
emerging as pivotal components that significantly shape the capabilities of
intelligent systems. In the context of intelligent vehicles, leveraging the
power of foundation models has proven to be transformative, offering notable
advancements in visual understanding. Equipped with multi-modal and multi-task
learning capabilities, multi-modal multi-task visual understanding foundation
models (MM-VUFMs) effectively process and fuse data from diverse modalities and
simultaneously handle various driving-related tasks with powerful adaptability,
contributing to a more holistic understanding of the surrounding scene. In this
survey, we present a systematic analysis of MM-VUFMs specifically designed for
road scenes. Our objective is not only to provide a comprehensive overview of
common practices, referring to task-specific models, unified multi-modal
models, unified multi-task models, and foundation model prompting techniques,
but also to highlight their advanced capabilities in diverse learning
paradigms. These paradigms include open-world understanding, efficient transfer
for road scenes, continual learning, interactive and generative capability.
Moreover, we provide insights into key challenges and future trends, such as
closed-loop driving systems, interpretability, embodied driving agents, and
world models. To facilitate researchers in staying abreast of the latest
developments in MM-VUFMs for road scenes, we have established a continuously
updated repository at https://github.com/rolsheng/MM-VUFM4DS
\\ ( https://arxiv.org/abs/2402.02968 ,  6845kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02969 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:47:19 GMT   (703kb,D)

Title: Towards Understanding the Word Sensitivity of Attention Layers: A Study
  via Random Features
Authors: Simone Bombari and Marco Mondelli
Categories: stat.ML cs.LG
\\
  Unveiling the reasons behind the exceptional success of transformers requires
a better understanding of why attention layers are suitable for NLP tasks. In
particular, such tasks require predictive models to capture contextual meaning
which often depends on one or few words, even if the sentence is long. Our work
studies this key property, dubbed word sensitivity (WS), in the prototypical
setting of random features. We show that attention layers enjoy high WS,
namely, there exists a vector in the space of embeddings that largely perturbs
the random attention features map. The argument critically exploits the role of
the softmax in the attention layer, highlighting its benefit compared to other
activations (e.g., ReLU). In contrast, the WS of standard random features is of
order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and
thus it decays with the length of the context. We then translate these results
on the word sensitivity into generalization bounds: due to their low WS, random
features provably cannot learn to distinguish between two sentences that differ
only in a single word; in contrast, due to their high WS, random attention
features have higher generalization capabilities. We validate our theoretical
results with experimental evidence over the BERT-Base word embeddings of the
imdb review dataset.
\\ ( https://arxiv.org/abs/2402.02969 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02972 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:50:30 GMT   (27371kb,D)

Title: Retrieval-Augmented Score Distillation for Text-to-3D Generation
Authors: Junyoung Seo, Susung Hong, Wooseok Jang, In\`es Hyeonsu Kim, Minseop
  Kwak, Doyup Lee, Seungryong Kim
Categories: cs.CV cs.LG
Comments: Project Page: https://ku-cvlab.github.io/RetDream/
\\
  Text-to-3D generation has achieved significant success by incorporating
powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to
the inconsistency of 3D geometry. Recently, since large-scale multi-view
datasets have been released, fine-tuning the diffusion model on the multi-view
datasets becomes a mainstream to solve the 3D inconsistency problem. However,
it has confronted with fundamental difficulties regarding the limited quality
and diversity of 3D data, compared with 2D data. To sidestep these trade-offs,
we explore a retrieval-augmented approach tailored for score distillation,
dubbed RetDream. We postulate that both expressiveness of 2D diffusion models
and geometric consistency of 3D assets can be fully leveraged by employing the
semantically relevant assets directly within the optimization process. To this
end, we introduce novel framework for retrieval-based quality enhancement in
text-to-3D generation. We leverage the retrieved asset to incorporate its
geometric prior in the variational objective and adapt the diffusion model's 2D
prior toward view consistency, achieving drastic improvements in both geometry
and fidelity of generated scenes. We conduct extensive experiments to
demonstrate that RetDream exhibits superior quality with increased geometric
consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.
\\ ( https://arxiv.org/abs/2402.02972 ,  27371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02980 (*cross-listing*)
Date: Mon, 5 Feb 2024 13:12:33 GMT   (5512kb,D)

Title: Review on Fault Diagnosis and Fault-Tolerant Control Scheme for Robotic
  Manipulators: Recent Advances in AI, Machine Learning, and Digital Twin
Authors: Md Muzakkir Quamar and Ali Nasir
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: 24 pages, 6 figures
\\
  This comprehensive review article delves into the intricate realm of
fault-tolerant control (FTC) schemes tailored for robotic manipulators. Our
exploration spans the historical evolution of FTC, tracing its development over
time, and meticulously examines the recent breakthroughs fueled by the
synergistic integration of cutting-edge technologies such as artificial
intelligence (AI), machine learning (ML), and digital twin technologies (DTT).
The article places a particular emphasis on the transformative influence these
contemporary trends exert on the landscape of robotic manipulator control and
fault tolerance.
  By delving into the historical context, our aim is to provide a comprehensive
understanding of the evolution of FTC schemes. This journey encompasses the
transition from model-based and signal-based schemes to the role of sensors,
setting the stage for an exploration of the present-day paradigm shift enabled
by AI, ML, and DTT. The narrative unfolds as we dissect the intricate interplay
between these advanced technologies and their applications in enhancing fault
tolerance within the domain of robotic manipulators. Our review critically
evaluates the impact of these advancements, shedding light on the novel
methodologies, techniques, and applications that have emerged in recent times.
  The overarching goal of this article is to present a comprehensive
perspective on the current state of fault diagnosis and fault-tolerant control
within the context of robotic manipulators, positioning our exploration within
the broader framework of AI, ML, and DTT advancements. Through a meticulous
examination of both historical foundations and contemporary innovations, this
review significantly contributes to the existing body of knowledge, offering
valuable insights for researchers, practitioners, and enthusiasts navigating
the dynamic landscape of robotic manipulator control.
\\ ( https://arxiv.org/abs/2402.02980 ,  5512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02985 (*cross-listing*)
Date: Mon, 5 Feb 2024 13:16:12 GMT   (22760kb,D)

Title: Unsupervised semantic segmentation of high-resolution UAV imagery for
  road scene parsing
Authors: Zihan Ma, Yongshang Li, Ronggui Ma, Chen Liang
Categories: cs.CV cs.LG
\\
  Two challenges are presented when parsing road scenes in UAV images. First,
the high resolution of UAV images makes processing difficult. Second,
supervised deep learning methods require a large amount of manual annotations
to train robust and accurate models. In this paper, an unsupervised road
parsing framework that leverages recent advances in vision language models and
fundamental computer vision model is introduced.Initially, a vision language
model is employed to efficiently process ultra-large resolution UAV images to
quickly detect road regions of interest in the images. Subsequently, the vision
foundation model SAM is utilized to generate masks for the road regions without
category information. Following that, a self-supervised representation learning
network extracts feature representations from all masked regions. Finally, an
unsupervised clustering algorithm is applied to cluster these feature
representations and assign IDs to each cluster. The masked regions are combined
with the corresponding IDs to generate initial pseudo-labels, which initiate an
iterative self-training process for regular semantic segmentation. The proposed
method achieves an impressive 89.96% mIoU on the development dataset without
relying on any manual annotation. Particularly noteworthy is the extraordinary
flexibility of the proposed method, which even goes beyond the limitations of
human-defined categories and is able to acquire knowledge of new categories
from the dataset itself.
\\ ( https://arxiv.org/abs/2402.02985 ,  22760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02989 (*cross-listing*)
Date: Mon, 5 Feb 2024 13:27:41 GMT   (13767kb,D)

Title: DexDiffuser: Generating Dexterous Grasps with Diffusion Models
Authors: Zehang Weng, Haofei Lu, Danica Kragic, Jens Lundell
Categories: cs.RO cs.LG
Comments: 8 pages
\\
  We introduce DexDiffuser, a novel dexterous grasping method that generates,
evaluates, and refines grasps on partial object point clouds. DexDiffuser
includes the conditional diffusion-based grasp sampler DexSampler and the
dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality
grasps conditioned on object point clouds by iterative denoising of randomly
sampled grasps. We also introduce two grasp refinement strategies:
Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).
Our simulation and real-world experiments on the Allegro Hand consistently
demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger
grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp
success rate.
\\ ( https://arxiv.org/abs/2402.02989 ,  13767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03008 (*cross-listing*)
Date: Mon, 5 Feb 2024 13:47:41 GMT   (2704kb,D)

Title: Diffusive Gibbs Sampling
Authors: Wenlin Chen, Mingtian Zhang, Brooks Paige, Jos\'e Miguel
  Hern\'andez-Lobato, David Barber
Categories: stat.ML cs.LG
Comments: 15 pages, 11 figures, 4 tables, 1 algorithm
\\
  The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods
for multi-modal distributions presents a significant challenge in practical
applications such as Bayesian inference and molecular dynamics. Addressing
this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of
sampling methods designed for effective sampling from distributions
characterized by distant and disconnected modes. DiGS integrates recent
developments in diffusion models, leveraging Gaussian convolution to create an
auxiliary noisy distribution that bridges isolated modes in the original space
and applying Gibbs sampling to alternately draw samples from both spaces. Our
approach exhibits a better mixing property for sampling multi-modal
distributions than state-of-the-art methods such as parallel tempering. We
demonstrate that our sampler attains substantially improved results across
various tasks, including mixtures of Gaussians, Bayesian neural networks and
molecular dynamics.
\\ ( https://arxiv.org/abs/2402.03008 ,  2704kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03019 (*cross-listing*)
Date: Mon, 5 Feb 2024 14:00:13 GMT   (34028kb,D)

Title: Taylor Videos for Action Recognition
Authors: Lei Wang and Xiuyuan Yuan and Tom Gedeon and Liang Zheng
Categories: cs.CV cs.LG
Comments: Research report
\\
  Effectively extracting motions from video is a critical and long-standing
problem for action recognition. This problem is very challenging because
motions (i) do not have an explicit form, (ii) have various concepts such as
displacement, velocity, and acceleration, and (iii) often contain noise caused
by unstable pixels. Addressing these challenges, we propose the Taylor video, a
new video format that highlights the dominate motions (e.g., a waving hand) in
each of its frames named the Taylor frame. Taylor video is named after Taylor
series, which approximates a function at a given point using important terms.
In the scenario of videos, we define an implicit motion-extraction function
which aims to extract motions from video temporal block. In this block, using
the frames, the difference frames, and higher-order difference frames, we
perform Taylor expansion to approximate this function at the starting frame. We
show the summation of the higher-order terms in the Taylor series gives us
dominant motion patterns, where static objects, small and unstable motions are
removed. Experimentally we show that Taylor videos are effective inputs to
popular architectures including 2D CNNs, 3D CNNs, and transformers. When used
individually, Taylor videos yield competitive action recognition accuracy
compared to RGB videos and optical flow. When fused with RGB or optical flow
videos, further accuracy improvement is achieved.
\\ ( https://arxiv.org/abs/2402.03019 ,  34028kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03025 (*cross-listing*)
Date: Mon, 5 Feb 2024 14:06:15 GMT   (1774kb,D)

Title: Understanding and Guiding Weakly Supervised Entity Alignment with
  Potential Isomorphism Propagation
Authors: Yuanyi Wang, Wei Tang, Haifeng Sun, Zirui Zhuang, Xiaoyuan Fu, Jingyu
  Wang, Qi Qi and Jianxin Liao
Categories: cs.IR cs.LG
\\
  Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent
entities across diverse knowledge graphs (KGs) using only a limited number of
seed alignments. Despite substantial advances in aggregation-based weakly
supervised EA, the underlying mechanisms in this setting remain unexplored. In
this paper, we present a propagation perspective to analyze weakly supervised
EA and explain the existing aggregation-based EA models. Our theoretical
analysis reveals that these models essentially seek propagation operators for
pairwise entity similarities. We further prove that, despite the structural
heterogeneity of different KGs, the potentially aligned entities within
aggregation-based EA models have isomorphic subgraphs, which is the core
premise of EA but has not been investigated. Leveraging this insight, we
introduce a potential isomorphism propagation operator to enhance the
propagation of neighborhood information across KGs. We develop a general EA
framework, PipEA, incorporating this operator to improve the accuracy of every
type of aggregation-based model without altering the learning process.
Extensive experiments substantiate our theoretical findings and demonstrate
PipEA's significant performance gains over state-of-the-art weakly supervised
EA methods. Our work not only advances the field but also enhances our
comprehension of aggregation-based weakly supervised EA.
\\ ( https://arxiv.org/abs/2402.03025 ,  1774kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03028 (*cross-listing*)
Date: Mon, 5 Feb 2024 14:12:35 GMT   (1107kb,D)

Title: Functional SDE approximation inspired by a deep operator network
  architecture
Authors: Martin Eigel, Charles Miranda
Categories: math.NA cs.LG cs.NA
MSC-class: 65C30, 60H10, 91G60, 60H35, 68T07
\\
  A novel approach to approximate solutions of Stochastic Differential
Equations (SDEs) by Deep Neural Networks is derived and analysed. The
architecture is inspired by the notion of Deep Operator Networks (DeepONets),
which is based on operator learning in function spaces in terms of a reduced
basis also represented in the network. In our setting, we make use of a
polynomial chaos expansion (PCE) of stochastic processes and call the
corresponding architecture SDEONet. The PCE has been used extensively in the
area of uncertainty quantification (UQ) with parametric partial differential
equations. This however is not the case with SDE, where classical sampling
methods dominate and functional approaches are seen rarely. A main challenge
with truncated PCEs occurs due to the drastic growth of the number of
components with respect to the maximum polynomial degree and the number of
basis elements. The proposed SDEONet architecture aims to alleviate the issue
of exponential complexity by learning an optimal sparse truncation of the
Wiener chaos expansion. A complete convergence and complexity analysis is
presented, making use of recent Neural Network approximation results. Numerical
experiments illustrate the promising performance of the suggested approach in
1D and higher dimensions.
\\ ( https://arxiv.org/abs/2402.03028 ,  1107kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03047 (*cross-listing*)
Date: Mon, 5 Feb 2024 14:32:57 GMT   (2375kb,D)

Title: PFDM: Parser-Free Virtual Try-on via Diffusion Model
Authors: Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao
  Wang
Categories: cs.CV cs.LG
Comments: Accepted by IEEE ICASSP 2024
\\
  Virtual try-on can significantly improve the garment shopping experiences in
both online and in-store scenarios, attracting broad interest in computer
vision. However, to achieve high-fidelity try-on performance, most
state-of-the-art methods still rely on accurate segmentation masks, which are
often produced by near-perfect parsers or manual labeling. To overcome the
bottleneck, we propose a parser-free virtual try-on method based on the
diffusion model (PFDM). Given two images, PFDM can "wear" garments on the
target person seamlessly by implicitly warping without any other information.
To learn the model effectively, we synthesize many pseudo-images and construct
sample pairs by wearing various garments on persons. Supervised by the
large-scale expanded dataset, we fuse the person and garment features using a
proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that
our proposed PFDM can successfully handle complex cases, synthesize
high-fidelity images, and outperform both state-of-the-art parser-free and
parser-based models.
\\ ( https://arxiv.org/abs/2402.03047 ,  2375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03048 (*cross-listing*)
Date: Mon, 5 Feb 2024 14:33:52 GMT   (2325kb,D)

Title: Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems
  Tracking Control under Switching Topologies
Authors: Zewen Yang, Songbo Dong, Armin Lederer, Xiaobing Dai, Siyu Chen,
  Stefan Sosnowski, Georges Hattab, Sandra Hirche
Categories: cs.MA cs.LG
Comments: 8 pages
\\
  This work presents an innovative learning-based approach to tackle the
tracking control problem of Euler-Lagrange multi-agent systems with partially
unknown dynamics operating under switching communication topologies. The
approach leverages a correlation-aware cooperative algorithm framework built
upon Gaussian process regression, which adeptly captures inter-agent
correlations for uncertainty predictions. A standout feature is its exceptional
efficiency in deriving the aggregation weights achieved by circumventing the
computationally intensive posterior variance calculations. Through Lyapunov
stability analysis, the distributed control law ensures bounded tracking errors
with high probability. Simulation experiments validate the protocol's efficacy
in effectively managing complex scenarios, establishing it as a promising
solution for robust tracking control in multi-agent systems characterized by
uncertain dynamics and dynamic communication structures.
\\ ( https://arxiv.org/abs/2402.03048 ,  2325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03077 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:09:41 GMT   (86kb)

Title: Markov Persuasion Processes: Learning to Persuade from Scratch
Authors: Francesco Bacchiocchi, Francesco Emanuele Stradi, Matteo Castiglioni,
  Alberto Marchesi, Nicola Gatti
Categories: cs.GT cs.LG
\\
  In Bayesian persuasion, an informed sender strategically discloses
information to a receiver so as to persuade them to undertake desirable
actions. Recently, a growing attention has been devoted to settings in which
sender and receivers interact sequentially. Recently, Markov persuasion
processes (MPPs) have been introduced to capture sequential scenarios where a
sender faces a stream of myopic receivers in a Markovian environment. The MPPs
studied so far in the literature suffer from issues that prevent them from
being fully operational in practice, e.g., they assume that the sender knows
receivers' rewards. We fix such issues by addressing MPPs where the sender has
no knowledge about the environment. We design a learning algorithm for the
sender, working with partial feedback. We prove that its regret with respect to
an optimal information-disclosure policy grows sublinearly in the number of
episodes, as it is the case for the loss in persuasiveness cumulated while
learning. Moreover, we provide a lower bound for our setting matching the
guarantees of our algorithm.
\\ ( https://arxiv.org/abs/2402.03077 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03082 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:13:20 GMT   (19205kb,D)

Title: Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual
  Text Processing
Authors: Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou
Categories: cs.CV cs.LG
\\
  Visual text, a pivotal element in both document and scene images, speaks
volumes and attracts significant attention in the computer vision domain.
Beyond visual text detection and recognition, the field of visual text
processing has experienced a surge in research, driven by the advent of
fundamental generative models. However, challenges persist due to the unique
properties and features that distinguish text from general objects. Effectively
leveraging these unique textual characteristics is crucial in visual text
processing, as observed in our study. In this survey, we present a
comprehensive, multi-perspective analysis of recent advancements in this field.
Initially, we introduce a hierarchical taxonomy encompassing areas ranging from
text image enhancement and restoration to text image manipulation, followed by
different learning paradigms. Subsequently, we conduct an in-depth discussion
of how specific textual features such as structure, stroke, semantics, style,
and spatial context are seamlessly integrated into various tasks. Furthermore,
we explore available public datasets and benchmark the reviewed methods on
several widely-used datasets. Finally, we identify principal challenges and
potential avenues for future research. Our aim is to establish this survey as a
fundamental resource, fostering continued exploration and innovation in the
dynamic area of visual text processing.
\\ ( https://arxiv.org/abs/2402.03082 ,  19205kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03086 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:14:08 GMT   (110kb,D)

Title: Dual Lagrangian Learning for Conic Optimization
Authors: Mathieu Tanneau, Pascal Van Hentenryck
Categories: math.OC cs.LG
\\
  This paper presents Dual Lagrangian Learning (DLL), a principled learning
methodology that combines conic duality theory with the represen- tation power
of ML models. DLL leverages conic duality to provide dual-feasible solutions,
and therefore valid Lagrangian dual bounds, for para- metric linear and
nonlinear conic optimization problems. The paper introduces differentiable
conic projection layers, a systematic dual com- pletion procedure, and a
self-supervised learning framework. The effectiveness of DLL is demon- strated
on linear and nonlinear parametric opti- mization problems for which DLL
provides valid dual bounds within 0.5% of optimality.
\\ ( https://arxiv.org/abs/2402.03086 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03094 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:25:32 GMT   (5772kb)

Title: Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object
  Detector
Authors: Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan,
  Tong Liu, Lingjie Kong, Yanwei Fu, Luc Van Gool, Xingqun Jiang
Categories: cs.CV cs.LG
\\
  This paper addresses the challenge of cross-domain few-shot object detection
(CD-FSOD), aiming to develop an accurate object detector for novel domains with
minimal labeled examples. While transformer-based open-set detectors e.g.,
DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object
detection and traditional few-shot object detection, detecting categories
beyond those seen during training, we thus naturally raise two key questions:
1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no,
how to enhance the results of open-set methods when faced with significant
domain gaps? To address the first question, we introduce several metrics to
quantify domain variances and establish a new CD-FSOD benchmark with diverse
domain metric values. Some State-Of-The-Art (SOTA) open-set object detection
methods are evaluated on this benchmark, with evident performance degradation
observed across out-of-domain datasets. This indicates the failure of adopting
open-set detectors directly for CD-FSOD. Sequentially, to overcome the
performance degradation issue and also to answer the second proposed question,
we endeavor to enhance the vanilla DE-ViT. With several novel components
including finetuning, a learnable prototype module, and a lightweight attention
module, we present an improved Cross-Domain Vision Transformer for CD-FSOD
(CD-ViTO). Experiments show that our CD-ViTO achieves impressive results on
both out-of-domain and in-domain target datasets, establishing new SOTAs for
both CD-FSOD and FSOD. All the datasets, codes, and models will be released to
the community.
\\ ( https://arxiv.org/abs/2402.03094 ,  5772kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03095 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:25:40 GMT   (4355kb,D)

Title: Transcending Adversarial Perturbations: Manifold-Aided Adversarial
  Examples with Legitimate Semantics
Authors: Shuai Li, Xiaoyu Jiang, and Xiaoguang Ma
Categories: cs.CV cs.CR cs.LG
Comments: 12 pages, 6 figures
\\
  Deep neural networks were significantly vulnerable to adversarial examples
manipulated by malicious tiny perturbations. Although most conventional
adversarial attacks ensured the visual imperceptibility between adversarial
examples and corresponding raw images by minimizing their geometric distance,
these constraints on geometric distance led to limited attack transferability,
inferior visual quality, and human-imperceptible interpretability. In this
paper, we proposed a supervised semantic-transformation generative model to
generate adversarial examples with real and legitimate semantics, wherein an
unrestricted adversarial manifold containing continuous semantic variations was
constructed for the first time to realize a legitimate transition from
non-adversarial examples to adversarial ones. Comprehensive experiments on
MNIST and industrial defect datasets showed that our adversarial examples not
only exhibited better visual quality but also achieved superior attack
transferability and more effective explanations for model vulnerabilities,
indicating their great potential as generic adversarial examples. The code and
pre-trained models were available at https://github.com/shuaili1027/MAELS.git.
\\ ( https://arxiv.org/abs/2402.03095 ,  4355kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03104 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:32:10 GMT   (10802kb,D)

Title: High-dimensional Bayesian Optimization via Covariance Matrix Adaptation
  Strategy
Authors: Lam Ngo, Huong Ha, Jeffrey Chan, Vu Nguyen, Hongyu Zhang
Categories: stat.ML cs.LG
Comments: 31 pages, 17 figures
Journal-ref: Transactions on Machine Learning Research 2024
\\
  Bayesian Optimization (BO) is an effective method for finding the global
optimum of expensive black-box functions. However, it is well known that
applying BO to high-dimensional optimization problems is challenging. To
address this issue, a promising solution is to use a local search strategy that
partitions the search domain into local regions with high likelihood of
containing the global optimum, and then use BO to optimize the objective
function within these regions. In this paper, we propose a novel technique for
defining the local regions using the Covariance Matrix Adaptation (CMA)
strategy. Specifically, we use CMA to learn a search distribution that can
estimate the probabilities of data points being the global optimum of the
objective function. Based on this search distribution, we then define the local
regions consisting of data points with high probabilities of being the global
optimum. Our approach serves as a meta-algorithm as it can incorporate existing
black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global
optimum of the objective function within our derived local regions. We evaluate
our proposed method on various benchmark synthetic and real-world problems. The
results demonstrate that our method outperforms existing state-of-the-art
techniques.
\\ ( https://arxiv.org/abs/2402.03104 ,  10802kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03116 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:45:59 GMT   (4818kb,D)

Title: Feature-Action Design Patterns for Storytelling Visualizations with Time
  Series Data
Authors: Saiful Khan, Scott Jones, Benjamin Bach, Jaehoon Cha, Min Chen, Julie
  Meikle, Jonathan C Roberts, Jeyan Thiyagalingam, Jo Wood, Panagiotis D.
  Ritsos
Categories: cs.HC cs.LG
\\
  We present a method to create storytelling visualization with time series
data. Many personal decisions nowadays rely on access to dynamic data
regularly, as we have seen during the COVID-19 pandemic. It is thus desirable
to construct storytelling visualization for dynamic data that is selected by an
individual for a specific context. Because of the need to tell data-dependent
stories, predefined storyboards based on known data cannot accommodate dynamic
data easily nor scale up to many different individuals and contexts. Motivated
initially by the need to communicate time series data during the COVID-19
pandemic, we developed a novel computer-assisted method for meta-authoring of
stories, which enables the design of storyboards that include feature-action
patterns in anticipation of potential features that may appear in dynamically
arrived or selected data. In addition to meta-storyboards involving COVID-19
data, we also present storyboards for telling stories about progress in a
machine learning workflow. Our approach is complementary to traditional methods
for authoring storytelling visualization, and provides an efficient means to
construct data-dependent storyboards for different data-streams of similar
contexts.
\\ ( https://arxiv.org/abs/2402.03116 ,  4818kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03124 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:51:34 GMT   (811kb,D)

Title: Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks
Authors: Yanbo Wang, Jian Liang, Ran He
Categories: cs.CR cs.CV cs.LG
Comments: ICLR2024 poster The prior submission version had a bug in the image
  reconstruction implementation, which has been corrected without harm to the
  main conclusions
\\
  Gradient inversion attacks aim to reconstruct local training data from
intermediate gradients exposed in the federated learning framework. Despite
successful attacks, all previous methods, starting from reconstructing a single
data point and then relaxing the single-image limit to batch level, are only
tested under hard label constraints. Even for single-image reconstruction, we
still lack an analysis-based algorithm to recover augmented soft labels. In
this work, we change the focus from enlarging batchsize to investigating the
hard label constraints, considering a more realistic circumstance where label
smoothing and mixup techniques are used in the training process. In particular,
we are the first to initiate a novel algorithm to simultaneously recover the
ground-truth augmented label and the input feature of the last fully-connected
layer from single-input gradients, and provide a necessary condition for any
analytical-based label recovery methods. Extensive experiments testify to the
label recovery accuracy, as well as the benefits to the following image
reconstruction. We believe soft labels in classification tasks are worth
further attention in gradient inversion attacks.
\\ ( https://arxiv.org/abs/2402.03124 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03145 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:12:36 GMT   (190kb)

Title: SafEDMD: A certified learning architecture tailored to data-driven
  control of nonlinear dynamical systems
Authors: Robin Str\"asser, Manuel Schaller, Karl Worthmann, Julian Berberich,
  Frank Allg\"ower
Categories: eess.SY cs.LG cs.SY math.OC
\\
  The Koopman operator serves as the theoretical backbone for machine learning
of dynamical control systems, where the operator is heuristically approximated
by extended dynamic mode decomposition (EDMD). In this paper, we propose
Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning
architecture which comes along with rigorous certificates, resulting in a
reliable surrogate model generated in a data-driven fashion. To ensure
trustworthiness of SafEDMD, we derive proportional error bounds, which vanish
at the origin and are tailored for control tasks, leading to certified
controller design based on semi-definite programming. We illustrate the
developed machinery by means of several benchmark examples and highlight the
advantages over state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.03145 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03149 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:16:17 GMT   (2968kb,D)

Title: A Comparative Analysis of Microrings Based Incoherent Photonic GEMM
  Accelerators
Authors: Sairam Sri Vatsavai, Venkata Sai Praneeth Karempudi, Alo Oluwaseun,
  and Ishan Thakkar
Categories: cs.AR cs.ET cs.LG cs.NE
Comments: Accepted at ISQED 2024
\\
  Several microring resonator (MRR) based analog photonic architectures have
been proposed to accelerate general matrix-matrix multiplications (GEMMs) in
deep neural networks with exceptional throughput and energy efficiency. To
implement GEMM functions, these MRR-based architectures, in general, manipulate
optical signals in five different ways: (i) Splitting (copying) of multiple
optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing)
of multiple optical signals to achieve a certain fan-in, (iii) Modulation of
optical signals to imprint input values onto analog signal amplitude, (iv)
Weighting of modulated optical signals to achieve analog input-weight
multiplication, (v) Summation of optical signals. The MRR-based GEMM
accelerators undertake the first four ways of signal manipulation in an
arbitrary order ignoring the possible impact of the order of these
manipulations on their performance. In this paper, we conduct a detailed
analysis of accelerator organizations with three different orders of these
manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2)
Aggregation-Splitting-Modulation-Weighting (ASMW), and (3)
Splitting-Modulation-Weighting-Aggregation (SMWA). We show that these
organizations affect the crosstalk noise and optical signal losses in different
magnitudes, which renders these organizations with different levels of
processing parallelism at the circuit level, and different magnitudes of
throughput and energy-area efficiency at the system level. Our evaluation
results for four CNN models show that SMWA organization achieves up to
4.4$\times$, 5$\times$, and 5.2$\times$ better throughput, energy efficiency,
and area-energy efficiency, respectively, compared to ASMW and MASW
organizations on average.
\\ ( https://arxiv.org/abs/2402.03149 ,  2968kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03153 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:19:53 GMT   (9248kb)

Title: Learning solutions of parametric Navier-Stokes with physics-informed
  neural networks
Authors: M.Naderibeni (1), M. J.T. Reinders (1), L. Wu (2), D. M.J. Tax (1),
  ((1) Pattern Recognition and Bio-informatics Group, Delft University of
  Technology, (2) Science, Research and Innovation, dsm-firmenich)
Categories: cs.CE cs.LG
\\
  We leverage Physics-Informed Neural Networks (PINNs) to learn solution
functions of parametric Navier-Stokes Equations (NSE). Our proposed approach
results in a feasible optimization problem setup that bypasses PINNs'
limitations in converging to solutions of highly nonlinear parametric-PDEs like
NSE. We consider the parameter(s) of interest as inputs of PINNs along with
spatio-temporal coordinates, and train PINNs on generated numerical solutions
of parametric-PDES for instances of the parameters. We perform experiments on
the classical 2D flow past cylinder problem aiming to learn velocities and
pressure functions over a range of Reynolds numbers as parameter of interest.
Provision of training data from generated numerical simulations allows for
interpolation of the solution functions for a range of parameters. Therefore,
we compare PINNs with unconstrained conventional Neural Networks (NN) on this
problem setup to investigate the effectiveness of considering the PDEs
regularization in the loss function. We show that our proposed approach results
in optimizing PINN models that learn the solution functions while making sure
that flow predictions are in line with conservational laws of mass and
momentum. Our results show that PINN results in accurate prediction of
gradients compared to NN model, this is clearly visible in predicted vorticity
fields given that none of these models were trained on vorticity labels.
\\ ( https://arxiv.org/abs/2402.03153 ,  9248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03156 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:24:12 GMT   (1740kb,D)

Title: DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for
  Blind Person Navigation
Authors: Artem Bazhenov, Vladimir Berman, Sergei Satsevich, Olga Shalopanova,
  Miguel Altamirano Cabrera, Artem Lykov, Dzmitry Tsetserukou
Categories: cs.RO cs.LG
Comments: This paper has been accepted for publication at the HRI2024
  conference
DOI: 10.1145/3610978.3640606
\\
  This paper introduces DogSurf - a newapproach of using quadruped robots to
help visually impaired people navigate in real world. The presented method
allows the quadruped robot to detect slippery surfaces, and to use audio and
haptic feedback to inform the user when to stop. A state-of-the-art GRU-based
neural network architecture with mean accuracy of 99.925% was proposed for the
task of multiclass surface classification for quadruped robots. A dataset was
collected on a Unitree Go1 Edu robot. The dataset and code have been posted to
the public domain.
\\ ( https://arxiv.org/abs/2402.03156 ,  1740kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03167 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:35:30 GMT   (12049kb,D)

Title: Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic
  Update and Transient Iteration Complexity
Authors: Boao Kong, Shuchen Zhu, Songtao Lu, Xinmeng Huang, Kun Yuan
Categories: math.OC cs.LG stat.ML
Comments: 37 pages, 6 figures
\\
  Stochastic bilevel optimization (SBO) is becoming increasingly essential in
machine learning due to its versatility in handling nested structures. To
address large-scale SBO, decentralized approaches have emerged as effective
paradigms in which nodes communicate with immediate neighbors without a central
server, thereby improving communication efficiency and enhancing algorithmic
robustness. However, current decentralized SBO algorithms face challenges,
including expensive inner-loop updates and unclear understanding of the
influence of network topology, data heterogeneity, and the nested bilevel
algorithmic structures. In this paper, we introduce a single-loop decentralized
SBO (D-SOBA) algorithm and establish its transient iteration complexity, which,
for the first time, clarifies the joint influence of network topology and data
heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the
state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and
transient iteration complexity under more relaxed assumptions compared to
existing methods. Numerical experiments validate our theoretical findings.
\\ ( https://arxiv.org/abs/2402.03167 ,  12049kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03169 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:38:30 GMT   (253kb)

Title: A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation
Authors: Hugo Lebeau, Florent Chatelain, Romain Couillet
Categories: stat.ML cs.LG math.PR
\\
  This work presents a comprehensive understanding of the estimation of a
planted low-rank signal from a general spiked tensor model near the
computational threshold. Relying on standard tools from the theory of large
random matrices, we characterize the large-dimensional spectral behavior of the
unfoldings of the data tensor and exhibit relevant signal-to-noise ratios
governing the detectability of the principal directions of the signal. These
results allow to accurately predict the reconstruction performance of truncated
multilinear SVD (MLSVD) in the non-trivial regime. This is particularly
important since it serves as an initialization of the higher-order orthogonal
iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank
approximation depends entirely on its initialization. We give a sufficient
condition for the convergence of HOOI and show that the number of iterations
before convergence tends to $1$ in the large-dimensional limit.
\\ ( https://arxiv.org/abs/2402.03169 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03174 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:41:17 GMT   (121kb)

Title: Decentralized Event-Triggered Online Learning for Safe Consensus of
  Multi-Agent Systems with Gaussian Process Regression
Authors: Xiaobing Dai, Zewen Yang, Mengtian Xu, Fangzhou Liu, Georges Hattab
  and Sandra Hirche
Categories: eess.SY cs.LG cs.SY
\\
  Consensus control in multi-agent systems has received significant attention
and practical implementation across various domains. However, managing
consensus control under unknown dynamics remains a significant challenge for
control design due to system uncertainties and environmental disturbances. This
paper presents a novel learning-based distributed control law, augmented by an
auxiliary dynamics. Gaussian processes are harnessed to compensate for the
unknown components of the multi-agent system. For continuous enhancement in
predictive performance of Gaussian process model, a data-efficient online
learning strategy with a decentralized event-triggered mechanism is proposed.
Furthermore, the control performance of the proposed approach is ensured via
the Lyapunov theory, based on a probabilistic guarantee for prediction error
bounds. To demonstrate the efficacy of the proposed learning-based controller,
a comparative analysis is conducted, contrasting it with both conventional
distributed control laws and offline learning methodologies.
\\ ( https://arxiv.org/abs/2402.03174 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03179 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:45:38 GMT   (25807kb,D)

Title: Cool-chic video: Learned video coding with 800 parameters
Authors: Thomas Leguay, Th\'eo Ladune, Pierrick Philippe, Olivier D\'eforges
Categories: eess.IV cs.LG
Comments: 10 pages, published in Data Compression Conference 2024
\\
  We propose a lightweight learned video codec with 900 multiplications per
decoded pixel and 800 parameters overall. To the best of our knowledge, this is
one of the neural video codecs with the lowest decoding complexity. It is built
upon the overfitted image codec Cool-chic and supplements it with an inter
coding module to leverage the video's temporal redundancies. The proposed model
is able to compress videos using both low-delay and random access
configurations and achieves rate-distortion close to AVC while out-performing
other overfitted codecs such as FFNeRV. The system is made open-source:
orange-opensource.github.io/Cool-Chic.
\\ ( https://arxiv.org/abs/2402.03179 ,  25807kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03220 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:30:42 GMT   (2990kb,D)

Title: The Benefits of Reusing Batches for Gradient Descent in Two-Layer
  Networks: Breaking the Curse of Information and Leap Exponents
Authors: Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka
  Zdeborov\'a, and Florent Krzakala
Categories: stat.ML cs.LG
\\
  We investigate the training dynamics of two-layer neural networks when
learning multi-index target functions. We focus on multi-pass gradient descent
(GD) that reuses the batches multiple times and show that it significantly
changes the conclusion about which functions are learnable compared to
single-pass gradient descent. In particular, multi-pass GD with finite stepsize
is found to overcome the limitations of gradient flow and single-pass GD given
by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et
al., 2023) of the target function. We show that upon re-using batches, the
network achieves in just two time steps an overlap with the target subspace
even for functions not satisfying the staircase property (Abbe et al., 2021).
We characterize the (broad) class of functions efficiently learned in finite
time. The proof of our results is based on the analysis of the Dynamical
Mean-Field Theory (DMFT). We further provide a closed-form description of the
dynamical process of the low-dimensional projections of the weights, and
numerical experiments illustrating the theory.
\\ ( https://arxiv.org/abs/2402.03220 ,  2990kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03230 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:43:02 GMT   (4388kb,D)

Title: CT-based Anatomical Segmentation for Thoracic Surgical Planning: A
  Benchmark Study for 3D U-shaped Deep Learning Models
Authors: Arash Harirpoush, Amirhossein Rasoulian, Marta Kersten-Oertel, and
  Yiming Xiao
Categories: cs.CV cs.LG
\\
  Recent rising interests in patient-specific thoracic surgical planning and
simulation require efficient and robust creation of digital anatomical models
from automatic medical image segmentation algorithms. Deep learning (DL) is now
state-of-the-art in various radiological tasks, and U-shaped DL models have
particularly excelled in medical image segmentation since the inception of the
2D UNet. To date, many variants of U-shaped models have been proposed by the
integration of different attention mechanisms and network configurations.
Leveraging the recent development of large multi-label databases, systematic
benchmark studies for these models can provide valuable insights for clinical
deployment and future model designs, but such studies are still rare. We
conduct the first benchmark study for variants of 3D U-shaped models (3DUNet,
STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with
four variants) with a focus on CT-based anatomical segmentation for thoracic
surgery. Our study systematically examines the impact of different attention
mechanisms, number of resolution stages, and network configurations on
segmentation accuracy and computational complexity. To allow cross-reference
with other recent benchmarking studies, we also included a performance
assessment of the BTCV abdominal structural segmentation. With the STUNet
ranking at the top, our study demonstrated the value of CNN-based U-shaped
models for the investigated tasks and the benefit of residual blocks in network
configuration designs to boost segmentation performance.
\\ ( https://arxiv.org/abs/2402.03230 ,  4388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03231 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:44:21 GMT   (1204kb,D)

Title: Improved prediction of future user activity in online A/B testing
Authors: Lorenzo Masoero, Mario Beraha, Thomas Richardson, Stefano Favaro
Categories: stat.ME cs.LG stat.AP
\\
  In online randomized experiments or A/B tests, accurate predictions of
participant inclusion rates are of paramount importance. These predictions not
only guide experimenters in optimizing the experiment's duration but also
enhance the precision of treatment effect estimates. In this paper we present a
novel, straightforward, and scalable Bayesian nonparametric approach for
predicting the rate at which individuals will be exposed to interventions
within the realm of online A/B testing. Our approach stands out by offering
dual prediction capabilities: it forecasts both the quantity of new customers
expected in future time windows and, unlike available alternative methods, the
number of times they will be observed. We derive closed-form expressions for
the posterior distributions of the quantities needed to form predictions about
future user activity, thereby bypassing the need for numerical algorithms such
as Markov chain Monte Carlo. After a comprehensive exposition of our model, we
test its performance on experiments on real and simulated data, where we show
its superior performance with respect to existing alternatives in the
literature.
\\ ( https://arxiv.org/abs/2402.03231 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03235 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:52:58 GMT   (2570kb,D)

Title: ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object
  Detection
Authors: Ahmed Ghita, Bj{\o}rk Antoniussen, Walter Zimmer, Ross Greer,
  Christian Cre{\ss}, Andreas M{\o}gelmose, Mohan M. Trivedi, Alois C. Knoll
Categories: cs.CV cs.LG
\\
  The curation of large-scale datasets is still costly and requires much time
and resources. Data is often manually labeled, and the challenge of creating
high-quality datasets remains. In this work, we fill the research gap using
active learning for multi-modal 3D object detection. We propose ActiveAnno3D,
an active learning framework to select data samples for labeling that are of
maximum informativeness for training. We explore various continuous training
methods and integrate the most efficient method regarding computational demand
and detection performance. Furthermore, we perform extensive experiments and
ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic
Intersection dataset. We show that we can achieve almost the same performance
with PV-RCNN and the entropy-based query strategy when using only half of the
training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection
dataset. BEVFusion achieved an mAP of 64.31 when using half of the training
data and 75.0 mAP when using the complete nuScenes dataset. We integrate our
active learning framework into the proAnno labeling tool to enable AI-assisted
data selection and labeling and minimize the labeling costs. Finally, we
provide code, weights, and visualization results on our website:
https://active3d-framework.github.io/active3d-framework.
\\ ( https://arxiv.org/abs/2402.03235 ,  2570kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03241 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:56:41 GMT   (8007kb,D)

Title: FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action
  Recognition
Authors: Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han
Categories: cs.CV cs.LG
Comments: Accepted by ICLR 2024
\\
  In this paper, we introduce FROSTER, an effective framework for
open-vocabulary action recognition. The CLIP model has achieved remarkable
success in a range of image-based tasks, benefiting from its strong
generalization capability stemming from pretaining on massive image-text pairs.
However, applying CLIP directly to the open-vocabulary action recognition task
is challenging due to the absence of temporal information in CLIP's
pretraining. Further, fine-tuning CLIP on action recognition datasets may lead
to overfitting and hinder its generalizability, resulting in unsatisfactory
results when dealing with unseen actions.
  To address these issues, FROSTER employs a residual feature distillation
approach to ensure that CLIP retains its generalization capability while
effectively adapting to the action recognition task. Specifically, the residual
feature distillation treats the frozen CLIP model as a teacher to maintain the
generalizability exhibited by the original CLIP and supervises the feature
learning for the extraction of video-specific features to bridge the gap
between images and videos. Meanwhile, it uses a residual sub-network for
feature distillation to reach a balance between the two distinct objectives of
learning generalizable and video-specific features.
  We extensively evaluate FROSTER on open-vocabulary action recognition
benchmarks under both base-to-novel and cross-dataset settings. FROSTER
consistently achieves state-of-the-art performance on all datasets across the
board. Project page: https://visual-ai.github.io/froster.
\\ ( https://arxiv.org/abs/2402.03241 ,  8007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03254 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:12:28 GMT   (362kb,D)

Title: Minimum Description Length and Generalization Guarantees for
  Representation Learning
Authors: Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski
Categories: stat.ML cs.IT cs.LG math.IT
Comments: Accepted and presented at NeurIPS 2023
\\
  A major challenge in designing efficient statistical supervised learning
algorithms is finding representations that perform well not only on available
training samples but also on unseen data. While the study of representation
learning has spurred much interest, most existing such approaches are
heuristic; and very little is known about theoretical generalization
guarantees.
  In this paper, we establish a compressibility framework that allows us to
derive upper bounds on the generalization error of a representation learning
algorithm in terms of the "Minimum Description Length" (MDL) of the labels or
the latent variables (representations). Rather than the mutual information
between the encoder's input and the representation, which is often believed to
reflect the algorithm's generalization capability in the related literature but
in fact, falls short of doing so, our new bounds involve the "multi-letter"
relative entropy between the distribution of the representations (or labels) of
the training and test sets and a fixed prior. In particular, these new bounds
reflect the structure of the encoder and are not vacuous for deterministic
algorithms. Our compressibility approach, which is information-theoretic in
nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two
essential ingredients: block-coding and lossy-compression. The latter allows
our approach to subsume the so-called geometrical compressibility as a special
case. To the best knowledge of the authors, the established generalization
bounds are the first of their kind for Information Bottleneck (IB) type
encoders and representation learning. Finally, we partly exploit the
theoretical results by introducing a new data-dependent prior. Numerical
simulations illustrate the advantages of well-chosen such priors over classical
priors used in IB.
\\ ( https://arxiv.org/abs/2402.03254 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03302 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:58:11 GMT   (4378kb,D)

Title: Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining
Authors: Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong
  Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, Shanshan Wang
Categories: cs.CV cs.LG
Comments: Technical report
\\
  Accurate medical image segmentation demands the integration of multi-scale
information, spanning from local features to global dependencies. However, it
is challenging for existing methods to model long-range global information,
where convolutional neural networks (CNNs) are constrained by their local
receptive fields, and vision transformers (ViTs) suffer from high quadratic
complexity of their attention mechanism. Recently, Mamba-based models have
gained great attention for their impressive ability in long sequence modeling.
Several studies have demonstrated that these models can outperform popular
vision models in various tasks, offering higher accuracy, lower memory
consumption, and less computational burden. However, existing Mamba-based
models are mostly trained from scratch and do not explore the power of
pretraining, which has been proven to be quite effective for data-efficient
medical image analysis. This paper introduces a novel Mamba-based model,
Swin-UMamba, designed specifically for medical image segmentation tasks,
leveraging the advantages of ImageNet-based pretraining. Our experimental
results reveal the vital role of ImageNet-based training in enhancing the
performance of Mamba-based models. Swin-UMamba demonstrates superior
performance with a large margin compared to CNNs, ViTs, and latest Mamba-based
models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba
outperforms its closest counterpart U-Mamba by an average score of 3.58%. The
code and models of Swin-UMamba are publicly available at:
https://github.com/JiarunLiu/Swin-UMamba
\\ ( https://arxiv.org/abs/2402.03302 ,  4378kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03309 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:59:31 GMT   (7425kb,D)

Title: AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
Authors: Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya
  Pediredla, Christopher A. Metzler
Categories: cs.CV cs.LG
Comments: First two authors contributed equally. Paper website:
  https://aoneus.github.io/
\\
  Underwater perception and 3D surface reconstruction are challenging problems
with broad applications in construction, security, marine archaeology, and
environmental monitoring. Treacherous operating conditions, fragile
surroundings, and limited navigation control often dictate that submersibles
restrict their range of motion and, thus, the baseline over which they can
capture measurements. In the context of 3D scene reconstruction, it is
well-known that smaller baselines make reconstruction more challenging. Our
work develops a physics-based multimodal acoustic-optical neural surface
reconstruction framework (AONeuS) capable of effectively integrating
high-resolution RGB measurements with low-resolution depth-resolved imaging
sonar measurements. By fusing these complementary modalities, our framework can
reconstruct accurate high-resolution 3D surfaces from measurements captured
over heavily-restricted baselines. Through extensive simulations and in-lab
experiments, we demonstrate that AONeuS dramatically outperforms recent
RGB-only and sonar-only inverse-differentiable-rendering--based surface
reconstruction methods. A website visualizing the results of our paper is
located at this address: https://aoneus.github.io/
\\ ( https://arxiv.org/abs/2402.03309 ,  7425kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03312 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:59:52 GMT   (2631kb,D)

Title: Test-Time Adaptation for Depth Completion
Authors: Hyoungseob Park, Anjali Gupta, Alex Wong
Categories: cs.CV cs.LG
\\
  It is common to observe performance degradation when transferring models
trained on some (source) datasets to target testing data due to a domain gap
between them. Existing methods for bridging this gap, such as domain adaptation
(DA), may require the source data on which the model was trained (often not
available), while others, i.e., source-free DA, require many passes through the
testing data. We propose an online test-time adaptation method for depth
completion, the task of inferring a dense depth map from a single image and
associated sparse depth map, that closes the performance gap in a single pass.
We first present a study on how the domain shift in each data modality affects
model performance. Based on our observations that the sparse depth modality
exhibits a much smaller covariate shift than the image, we design an embedding
module trained in the source domain that preserves a mapping from features
encoding only sparse depth to those encoding image and sparse depth. During
test time, sparse depth features are projected using this map as a proxy for
source domain features and are used as guidance to train a set of auxiliary
parameters (i.e., adaptation layer) to align image and sparse depth features
from the target test domain to that of the source domain. We evaluate our
method on indoor and outdoor scenarios and show that it improves over baselines
by an average of 21.1%.
\\ ( https://arxiv.org/abs/2402.03312 ,  2631kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2306.00937
replaced with revised version Sun, 4 Feb 2024 04:57:08 GMT   (5647kb,D)

Title: STEVE-1: A Generative Model for Text-to-Behavior in Minecraft
Authors: Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila
  McIlraith
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2306.00937 ,  5647kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10409
replaced with revised version Sat, 3 Feb 2024 20:10:54 GMT   (0kb,I)

Title: Iterative Hierarchy and Ranking Process (IHRP): A Novel Effective
  Hierarchy Method for Densely Connected Systems and Case Study in Student
  Performance Assessment
Authors: Suvojit Dhara and Adrijit Goswami
Categories: cs.AI cs.MA
Comments: The paper has been rejected from the mentioned journal "International
  Journal of Information Technology and Decision Making" and also some of the
  results in this version was mis-calculated
\\ ( https://arxiv.org/abs/2306.10409 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08430
replaced with revised version Sat, 3 Feb 2024 09:14:48 GMT   (545kb,D)

Title: Long-range Meta-path Search on Large-scale Heterogeneous Graphs
Authors: Chao Li, Zijie Guo, Qiuting He, Hao Xu and Kun He
Categories: cs.AI
Comments: 17 pages, 6 figures
\\ ( https://arxiv.org/abs/2307.08430 ,  545kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16262
replaced with revised version Sat, 3 Feb 2024 22:44:45 GMT   (737kb,D)

Title: Causal Strategic Learning with Competitive Selection
Authors: Kiet Q. H. Vo, Muneeb Aadil, Siu Lun Chau, Krikamol Muandet
Categories: cs.AI
Comments: Added more discussions on assumptions and the algorithm, and expand
  the Conclusion
\\ ( https://arxiv.org/abs/2308.16262 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10625
replaced with revised version Fri, 2 Feb 2024 20:02:38 GMT   (816kb,D)

Title: NoisyNN: Exploring the Influence of Information Entropy Change in
  Learning Systems
Authors: Xiaowei Yu, Zhe Huang, Yao Xue, Lu Zhang, Li Wang, Tianming Liu,
  Dajiang Zhu
Categories: cs.AI cs.CV
Comments: Information Entropy, NoisyNN, ViT, CNN
\\ ( https://arxiv.org/abs/2309.10625 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00229
replaced with revised version Sun, 4 Feb 2024 17:57:51 GMT   (4389kb,D)

Title: Consciousness-Inspired Spatio-Temporal Abstractions for Better
  Generalization in Reinforcement Learning
Authors: Mingde Zhao, Safa Alver, Harm van Seijen, Romain Laroche, Doina
  Precup, Yoshua Bengio
Categories: cs.AI cs.LG
Comments: updated post-acceptance version for ICLR 2024
\\ ( https://arxiv.org/abs/2310.00229 ,  4389kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02054
replaced with revised version Sun, 4 Feb 2024 10:48:30 GMT   (11158kb,D)

Title: AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable
  Diffusion Model
Authors: Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing
  Hu, Tangjie Lv, Changjie Fan and Zhipeng Hu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2310.02054 ,  11158kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05212
replaced with revised version Sat, 3 Feb 2024 23:17:16 GMT   (3337kb,D)

Title: Interpretable Semiotics Networks Representing Awareness
Authors: David Kupeev and Eyal Nitcany
Categories: cs.AI cs.CV cs.SI
\\ ( https://arxiv.org/abs/2310.05212 ,  3337kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07684
replaced with revised version Mon, 5 Feb 2024 12:45:15 GMT   (22955kb,D)

Title: Hypergraph Neural Networks through the Lens of Message Passing: A Common
  Perspective to Homophily and Architecture Design
Authors: Lev Telyatnikov, Maria Sofia Bucarelli, Guillermo Bernardez, Olga
  Zaghen, Simone Scardapane, Pietro Lio
Categories: cs.AI cs.SI
\\ ( https://arxiv.org/abs/2310.07684 ,  22955kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11334
replaced with revised version Sun, 4 Feb 2024 15:17:49 GMT   (193kb,D)

Title: Agent-Specific Effects: A Causal Effect Propagation Analysis in
  Multi-Agent MDPs
Authors: Stelios Triantafyllou, Aleksa Sukovic, Debmalya Mandal, Goran
  Radanovic
Categories: cs.AI
\\ ( https://arxiv.org/abs/2310.11334 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17451
replaced with revised version Mon, 5 Feb 2024 02:50:49 GMT   (15197kb,D)

Title: Generating by Understanding: Neural Visual Generation with Logical
  Symbol Groundings
Authors: Yifei Peng, Yu Jin, Zhexu Luo, Yao-Xiang Ding, Wang-Zhou Dai, Zhong
  Ren, Kun Zhou
Categories: cs.AI cs.CV cs.GR
\\ ( https://arxiv.org/abs/2310.17451 ,  15197kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09397
replaced with revised version Sun, 4 Feb 2024 06:39:22 GMT   (47372kb,D)

Title: Large Language Models for Autonomous Driving: Real-World Experiments
Authors: Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma, Juanwu Lu, Lingxi Li,
  Yaobin Chen, Jitesh Panchal and Ziran Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.09397 ,  47372kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13434
replaced with revised version Sun, 4 Feb 2024 10:45:40 GMT   (300kb,D)

Title: Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of
  Early-bird Students towards Three Diagnostic Objectives
Authors: Weibo Gao, Qi Liu, Hao Wang, Linan Yue, Haoyang Bi, Yin Gu, Fangzhou
  Yao, Zheng Zhang, Xin Li, Yuanjing He
Categories: cs.AI cs.IR
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2312.13434 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16127
replaced with revised version Sun, 4 Feb 2024 23:50:11 GMT   (14399kb,D)

Title: LLM-SAP: Large Language Model Situational Awareness Based Planning
Authors: Liman Wang, Hanyang Zhong
Categories: cs.AI
Comments: 18 pages including appendix.
  Website:https://github.com/HanyangZhong/Situational_Planning_datasets
\\ ( https://arxiv.org/abs/2312.16127 ,  14399kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00006
replaced with revised version Mon, 5 Feb 2024 03:39:25 GMT   (20754kb,D)

Title: Building Open-Ended Embodied Agent via Language-Policy Bidirectional
  Adaptation
Authors: Shaopeng Zhai, Jie Wang, Tianyi Zhang, Fuxian Huang, Qi Zhang, Ming
  Zhou, Jing Hou, Yu Qiao and Yu Liu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.00006 ,  20754kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01836
replaced with revised version Sun, 4 Feb 2024 15:27:07 GMT   (3191kb,D)

Title: Neural Control: Concurrent System Identification and Control Learning
  with Neural ODE
Authors: Cheng Chi
Categories: cs.AI
Comments: 9 pages, code open sourced in format of Google Colab notebooks;
  Resubmitted for adding missed references in the last submission
\\ ( https://arxiv.org/abs/2401.01836 ,  3191kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07964
replaced with revised version Mon, 5 Feb 2024 17:34:17 GMT   (33kb)

Title: AI-as-exploration: Navigating intelligence space
Authors: Dimitri Coelho Mollo
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.07964 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00042
replaced with revised version Sat, 3 Feb 2024 14:17:07 GMT   (779kb)

Title: Optimized Task Assignment and Predictive Maintenance for Industrial
  Machines using Markov Decision Process
Authors: Ali Nasir, Samir Mekid, Zaid Sawlan, Omar Alsawafy
Categories: cs.AI cs.SY eess.SY
Comments: 19 pages, 11 figures, 3 tables
\\ ( https://arxiv.org/abs/2402.00042 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00715
replaced with revised version Fri, 2 Feb 2024 23:08:12 GMT   (678kb,D)

Title: Intent Assurance using LLMs guided by Intent Drift
Authors: Kristina Dzeparoska, Ali Tizghadam, Alberto Leon-Garcia
Categories: cs.AI cs.NI stat.ME
\\ ( https://arxiv.org/abs/2402.00715 ,  678kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01276
replaced with revised version Mon, 5 Feb 2024 16:11:29 GMT   (1754kb,D)

Title: Federated Unlearning: a Perspective of Stability and Fairness
Authors: Jiaqi Shao, Tao Lin, Xuanyu Cao, Bing Luo
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.01276 ,  1754kb)
------------------------------------------------------------------------------
\\
arXiv:2207.01327
replaced with revised version Sat, 3 Feb 2024 15:22:49 GMT   (269kb,D)

Title: BoAT v2 - A Web-Based Dependency Annotation Tool with Focus on
  Agglutinative Languages
Authors: Salih Furkan Akkurt and B\"u\c{s}ra Mar\c{s}an and Susan Uskudarli
Categories: cs.CL
Comments: Presented in The International Conference and Workshop on
  Agglutinative Language Technologies as a challenge of Natural Language
  Processing (ALTNLP), June 7-8, 2022, Koper, Slovenia
ACM-class: I.2.7; J.m; E.m
\\ ( https://arxiv.org/abs/2207.01327 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04684
replaced with revised version Fri, 2 Feb 2024 22:45:05 GMT   (2263kb,D)

Title: Few-Shot Character Understanding in Movies as an Assessment to
  Meta-Learning of Theory-of-Mind
Authors: Mo Yu, Qiujing Wang, Shunchi Zhang, Yisi Sang, Kangsheng Pu, Zekai
  Wei, Han Wang, Liyan Xu, Jing Li, Yue Yu, Jie Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2211.04684 ,  2263kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10923
replaced with revised version Mon, 5 Feb 2024 11:19:24 GMT   (8252kb,D)

Title: Language Models as Inductive Reasoners
Authors: Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong
  Liu, Jianfeng Gao, Furu Wei
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024
\\ ( https://arxiv.org/abs/2212.10923 ,  8252kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05737
replaced with revised version Sat, 3 Feb 2024 07:59:26 GMT   (113kb,D)

Title: A Reparameterized Discrete Diffusion Model for Text Generation
Authors: Lin Zheng and Jianbo Yuan and Lei Yu and Lingpeng Kong
Categories: cs.CL cs.LG
Comments: Code available at
  https://github.com/hkunlp/reparam-discrete-diffusion
\\ ( https://arxiv.org/abs/2302.05737 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01904
replaced with revised version Sun, 4 Feb 2024 12:15:18 GMT   (16960kb,D)

Title: REFINER: Reasoning Feedback on Intermediate Representations
Authors: Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
  Bosselut, Robert West, and Boi Faltings
Categories: cs.CL
Comments: Accepted at EACL 2024
\\ ( https://arxiv.org/abs/2304.01904 ,  16960kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11082
replaced with revised version Mon, 5 Feb 2024 17:57:06 GMT   (1327kb,D)

Title: Fundamental Limitations of Alignment in Large Language Models
Authors: Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, Amnon Shashua
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2304.11082 ,  1327kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13861
replaced with revised version Mon, 5 Feb 2024 14:41:35 GMT   (2048kb,D)

Title: The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in
  Classification Tasks
Authors: Anders Giovanni M{\o}ller, Jacob Aarup Dalsgaard, Arianna Pera, Luca
  Maria Aiello
Categories: cs.CL cs.CY physics.soc-ph
Comments: Accepted at EACL 2024. 14 pages, 4 figures, 2 tables
\\ ( https://arxiv.org/abs/2304.13861 ,  2048kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05214
replaced with revised version Sun, 4 Feb 2024 06:21:03 GMT   (647kb,D)

Title: CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine
  Translation for Extremely Low-resource Languages
Authors: Kaushal Kumar Maurya, Rahul Kejriwal, Maunendra Sankar Desarkar, Anoop
  Kunchukuttan
Categories: cs.CL
Journal-ref: EACL 2024
\\ ( https://arxiv.org/abs/2305.05214 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09859
replaced with revised version Sun, 4 Feb 2024 02:25:34 GMT   (1364kb,D)

Title: Smaller Language Models are Better Black-box Machine-Generated Text
  Detectors
Authors: Niloofar Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, Taylor
  Berg-Kirkpatrick
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2305.09859 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13015
replaced with revised version Sat, 3 Feb 2024 06:35:31 GMT   (293kb,D)

Title: 3D Rotation and Translation for Hyperbolic Knowledge Graph Embedding
Authors: Yihua Zhu, Hidetoshi Shimodaira
Categories: cs.CL
Comments: 19 pages, EACL2024 main
\\ ( https://arxiv.org/abs/2305.13015 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13654
replaced with revised version Sat, 3 Feb 2024 16:44:36 GMT   (2959kb,D)

Title: Understanding and Mitigating Spurious Correlations in Text
  Classification with Neighborhood Analysis
Authors: Oscar Chew, Hsuan-Tien Lin, Kai-Wei Chang, Kuan-Hao Huang
Categories: cs.CL
Comments: Accepted by EACL-Findings 2024
\\ ( https://arxiv.org/abs/2305.13654 ,  2959kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13954
replaced with revised version Mon, 5 Feb 2024 06:42:38 GMT   (203kb,D)

Title: Robust Prompt Optimization for Large Language Models Against
  Distribution Shifts
Authors: Moxin Li, Wenjie Wang, Fuli Feng, Yixin Cao, Jizhi Zhang, Tat-Seng
  Chua
Categories: cs.CL cs.AI
Comments: EMNLP 2023 Main
\\ ( https://arxiv.org/abs/2305.13954 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14195
replaced with revised version Mon, 5 Feb 2024 17:28:07 GMT   (7577kb,D)

Title: HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors
  of Language Models in Human-Machine Conversations
Authors: Anthony Sicilia, Jennifer C. Gates, and Malihe Alikhani
Categories: cs.CL cs.AI
Comments: 17 pages, 9 figures, 5 tables
\\ ( https://arxiv.org/abs/2305.14195 ,  7577kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14982
replaced with revised version Mon, 5 Feb 2024 07:27:18 GMT   (7168kb,D)

Title: LAraBench: Benchmarking Arabic AI with Large Language Models
Authors: Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram
  Hasanain, Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham,
  Fahim Dalvi, Majd Hawasly, Nizi Nazar, Yousseif Elshahawy, Ahmed Ali, Nadir
  Durrani, Natasa Milic-Frayling, Firoj Alam
Categories: cs.CL cs.AI
Comments: Foundation Models, Large Language Models, Arabic NLP, Arabic Speech,
  Arabic AI, GPT3.5 Evaluation, USM Evaluation, Whisper Evaluation, GPT-4,
  BLOOMZ, Jais13b
MSC-class: 68T50
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2305.14982 ,  7168kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17267
replaced with revised version Fri, 2 Feb 2024 23:34:34 GMT   (10047kb,D)

Title: CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine
  Translation
Authors: Md Mahfuz Ibn Alam, Sina Ahmadi, Antonios Anastasopoulos
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.17267 ,  10047kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18466
replaced with revised version Fri, 2 Feb 2024 20:28:27 GMT   (309kb,D)

Title: Test-Time Training on Nearest Neighbors for Large Language Models
Authors: Moritz Hardt and Yu Sun
Categories: cs.CL cs.LG
Comments: ICLR final version
\\ ( https://arxiv.org/abs/2305.18466 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07629
replaced with revised version Mon, 5 Feb 2024 05:42:32 GMT   (645kb,D)

Title: SqueezeLLM: Dense-and-Sparse Quantization
Authors: Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng
  Shen, Michael W. Mahoney, Kurt Keutzer
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2306.07629 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13588
replaced with revised version Sat, 3 Feb 2024 00:24:11 GMT   (7886kb,D)

Title: System-Level Natural Language Feedback
Authors: Weizhe Yuan, Kyunghyun Cho, Jason Weston
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024
\\ ( https://arxiv.org/abs/2306.13588 ,  7886kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06908
replaced with revised version Sun, 4 Feb 2024 09:07:54 GMT   (8461kb,D)

Title: Generating Benchmarks for Factuality Evaluation of Language Models
Authors: Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan
  Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2307.06908 ,  8461kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07262
replaced with revised version Sat, 3 Feb 2024 05:42:54 GMT   (287kb,D)

Title: MorphPiece : A Linguistic Tokenizer for Large Language Models
Authors: Haris Jabbar
Categories: cs.CL
Comments: Manuscript under review. Patent pending
\\ ( https://arxiv.org/abs/2307.07262 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10172
replaced with revised version Mon, 5 Feb 2024 08:10:52 GMT   (5079kb,D)

Title: DialogStudio: Towards Richest and Most Diverse Unified Dataset
  Collection for Conversational AI
Authors: Jianguo Zhang and Kun Qian and Zhiwei Liu and Shelby Heinecke and Rui
  Meng and Ye Liu and Zhou Yu and Huan Wang and Silvio Savarese and Caiming
  Xiong
Categories: cs.CL cs.AI
Comments: 17 pages, accepted by EACL 2024 Findings as a long paper. All
  datasets, licenses, codes, and models are available at at
  https://github.com/salesforce/DialogStudio
\\ ( https://arxiv.org/abs/2307.10172 ,  5079kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04215
replaced with revised version Mon, 5 Feb 2024 14:55:19 GMT   (1511kb,D)

Title: Hybrid Retrieval-Augmented Generation for Real-time Composition
  Assistance
Authors: Menglin Xia, Xuchao Zhang, Camille Couturier, Guoqing Zheng, Saravan
  Rajmohan, Victor Ruhle
Categories: cs.CL cs.AI cs.DC
\\ ( https://arxiv.org/abs/2308.04215 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05502
replaced with revised version Sat, 3 Feb 2024 09:54:51 GMT   (14334kb)

Title: Bringing order into the realm of Transformer-based language models for
  artificial intelligence and law
Authors: Candida M. Greco, Andrea Tagarelli
Categories: cs.CL cs.AI cs.IR cs.NE physics.soc-ph
Comments: Please refer to the published version: Greco, C.M., Tagarelli, A.
  (2023) Bringing order into the realm of Transformer-based language models for
  artificial intelligence and law. Artif Intell Law, Springer Nature. November
  2023. https://doi.org/10.1007/s10506-023-09374-7
Journal-ref: Artif Intell Law, Springer Nature. November 2023
DOI: 10.1007/s10506-023-09374-7
\\ ( https://arxiv.org/abs/2308.05502 ,  14334kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07134
replaced with revised version Sun, 4 Feb 2024 22:08:05 GMT   (8673kb,D)

Title: Language is All a Graph Needs
Authors: Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: In EACL 2024
\\ ( https://arxiv.org/abs/2308.07134 ,  8673kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11767
replaced with revised version Sun, 4 Feb 2024 06:59:45 GMT   (206kb,D)

Title: Detection of ChatGPT Fake Science with the xFakeBibs Learning Algorithm
Authors: Ahmed Abdeen Hamed and Xindong Wu
Categories: cs.CL cs.IR
Comments: 14 pages, 6 figures, 4 tables, 2 algorithms
ACM-class: I.2
\\ ( https://arxiv.org/abs/2308.11767 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12038
replaced with revised version Mon, 5 Feb 2024 16:15:38 GMT   (7473kb,D)

Title: Large Multilingual Models Pivot Zero-Shot Multimodal Learning across
  Languages
Authors: Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen,
  Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue,
  Dahai Li, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.CV
Comments: https://github.com/OpenBMB/VisCPM.git
\\ ( https://arxiv.org/abs/2308.12038 ,  7473kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06364
replaced with revised version Sun, 4 Feb 2024 19:46:38 GMT   (409kb,D)

Title: Framework-Based Qualitative Analysis of Free Responses of Large Language
  Models: Algorithmic Fidelity
Authors: Aliya Amirova, Theodora Fteropoulli, Nafiso Ahmed, Martin R. Cowie,
  Joel Z. Leibo
Categories: cs.CL cs.AI
Comments: 52 pages, 5 tables, 5 figures
\\ ( https://arxiv.org/abs/2309.06364 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07445
replaced with revised version Mon, 5 Feb 2024 12:41:09 GMT   (8742kb,D)

Title: SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic
  Classification in 200+ Languages and Dialects
Authors: David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev,
  Jesujoba O. Alabi, Yanke Mao, Haonan Gao, Annie En-Shiun Lee
Categories: cs.CL
Comments: Accepted to EACL 2024 (main conference)
\\ ( https://arxiv.org/abs/2309.07445 ,  8742kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07794
replaced with revised version Sat, 3 Feb 2024 22:42:29 GMT   (8792kb,D)

Title: Improving Multimodal Classification of Social Media Posts by Leveraging
  Image-Text Auxiliary Tasks
Authors: Danae S\'anchez Villegas, Daniel Preo\c{t}iuc-Pietro, Nikolaos Aletras
Categories: cs.CL cs.LG cs.SI
Comments: Accepted at EACL 2024 Findings
\\ ( https://arxiv.org/abs/2309.07794 ,  8792kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08777
replaced with revised version Sun, 4 Feb 2024 00:52:03 GMT   (7716kb,D)

Title: Self-training Strategies for Sentiment Analysis: An Empirical Study
Authors: Haochen Liu, Sai Krishna Rallabandi, Yijing Wu, Parag Pravin Dakle,
  Preethi Raghavan
Categories: cs.CL
Comments: Accepted by EACL Findings 2024
\\ ( https://arxiv.org/abs/2309.08777 ,  7716kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08969
replaced with revised version Sun, 4 Feb 2024 09:44:34 GMT   (134kb,D)

Title: Rethinking STS and NLI in Large Language Models
Authors: Yuxia Wang, Minghan Wang, Preslav Nakov
Categories: cs.CL
Comments: arXiv admin note: text overlap with arXiv:2212.13138 by other authors
\\ ( https://arxiv.org/abs/2309.08969 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08999
replaced with revised version Sat, 3 Feb 2024 00:11:11 GMT   (8910kb,D)

Title: Context-aware Adversarial Attack on Named Entity Recognition
Authors: Shuguang Chen, Leonardo Neves, and Thamar Solorio
Categories: cs.CL
Comments: Accepted to W-NUT at EACL 2024
\\ ( https://arxiv.org/abs/2309.08999 ,  8910kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09357
replaced with revised version Sat, 3 Feb 2024 06:32:56 GMT   (11327kb,D)

Title: Talk2Care: Facilitating Asynchronous Patient-Provider Communication with
  Large-Language-Model
Authors: Ziqi Yang, Xuhai Xu, Bingsheng Yao, Shao Zhang, Ethan Rogers, Stephen
  Intille, Nawar Shara, Guodong Gordon Gao, Dakuo Wang
Categories: cs.CL cs.AI cs.HC
Comments: Under submission to IMWUT'23, 26 pages
MSC-class: 68U35
ACM-class: H.5.2; I.2.7
\\ ( https://arxiv.org/abs/2309.09357 ,  11327kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09582
replaced with revised version Fri, 2 Feb 2024 22:53:30 GMT   (7737kb,D)

Title: Fabricator: An Open Source Toolkit for Generating Labeled Training Data
  with Teacher LLMs
Authors: Jonas Golde, Patrick Haller, Felix Hamborg, Julian Risch, Alan Akbik
Categories: cs.CL cs.AI
Comments: 3 Figures and 2 Tables
DOI: 10.18653/v1/2023.emnlp-demo.1
\\ ( https://arxiv.org/abs/2309.09582 ,  7737kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13567
replaced with revised version Sun, 4 Feb 2024 02:47:49 GMT   (3656kb,D)

Title: MentaLLaMA: Interpretable Mental Health Analysis on Social Media with
  Large Language Models
Authors: Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang,
  Sophia Ananiadou
Categories: cs.CL
Comments: Accepted by WWW 2024
\\ ( https://arxiv.org/abs/2309.13567 ,  3656kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05707
replaced with revised version Mon, 5 Feb 2024 18:33:44 GMT   (1078kb,D)

Title: Guiding Language Model Math Reasoning with Planning Tokens
Authors: Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang
  Wang, Alessandro Sordoni
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.05707 ,  1078kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10477
replaced with revised version Sun, 4 Feb 2024 13:02:39 GMT   (2245kb,D)

Title: Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake
  Analysis
Authors: Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi,
  Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng
  Shang, Xin Jiang, Qun Liu
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.10477 ,  2245kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10688
replaced with revised version Sun, 4 Feb 2024 16:19:59 GMT   (4680kb,D)

Title: A decoder-only foundation model for time-series forecasting
Authors: Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.10688 ,  4680kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12531
replaced with revised version Mon, 5 Feb 2024 10:28:43 GMT   (8136kb,D)

Title: ICU: Conquering Language Barriers in Vision-and-Language Modeling by
  Dividing the Tasks into Image Captioning and Language Understanding
Authors: Guojun Wu
Categories: cs.CL
Comments: EMNLP 2023 (Findings)
\\ ( https://arxiv.org/abs/2310.12531 ,  8136kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19923
replaced with revised version Sun, 4 Feb 2024 11:11:53 GMT   (708kb,D)

Title: Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long
  Documents
Authors: Michael G\"unther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem,
  Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba
  Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
Categories: cs.CL cs.AI cs.LG
Comments: 14 pages
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2310.19923 ,  708kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20689
replaced with revised version Sat, 3 Feb 2024 10:13:39 GMT   (5138kb,D)

Title: Learning From Mistakes Makes LLM Better Reasoner
Authors: Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou,
  Weizhu Chen
Categories: cs.CL cs.AI
Comments: 19 pages, 13 figures, 5 tables
\\ ( https://arxiv.org/abs/2310.20689 ,  5138kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01256
replaced with revised version Mon, 5 Feb 2024 11:13:59 GMT   (759kb)

Title: An energy-based comparative analysis of common approaches to text
  classification in the Legal domain
Authors: Sinan Gultekin and Achille Globo and Andrea Zugarini and Marco
  Ernandes and Leonardo Rigutini
Categories: cs.CL cs.AI cs.LG cs.PF
Comments: Presented at The 4th International Conference on NLP & Text Mining
  (NLTM 2024), January 27-28 2024, Copenhagen, Denmark - 12 pages, 1 figure, 7
  tables
Journal-ref: Computer Science & Information Technology (CS & IT) ISSN 2231-5403
  Volume 14, Number 02, January 2024
DOI: 10.5121/csit.2024.140203
\\ ( https://arxiv.org/abs/2311.01256 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01386
replaced with revised version Sun, 4 Feb 2024 15:00:11 GMT   (8046kb,D)

Title: Can Language Models Be Tricked by Language Illusions? Easier with
  Syntax, Harder with Semantics
Authors: Yuhan Zhang, Edward Gibson, Forrest Davis
Categories: cs.CL
Comments: Accepted by The SIGNLL Conference on Computational Natural Language
  Learning 2023
DOI: 10.18653/v1/2023.conll-1.1
\\ ( https://arxiv.org/abs/2311.01386 ,  8046kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02945
replaced with revised version Mon, 5 Feb 2024 00:14:40 GMT   (330kb)

Title: PhoGPT: Generative Pre-training for Vietnamese
Authors: Dat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Dinh
  Phung, Hung Bui
Categories: cs.CL
Comments: PhoGPT-4B Technical Report - 5 pages
\\ ( https://arxiv.org/abs/2311.02945 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03099
replaced with revised version Sun, 4 Feb 2024 16:28:06 GMT   (31174kb,D)

Title: Language Models are Super Mario: Absorbing Abilities from Homologous
  Models as a Free Lunch
Authors: Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li
Categories: cs.CL cs.LG
Comments: 24 pages, 21 figures
\\ ( https://arxiv.org/abs/2311.03099 ,  31174kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04076
replaced with revised version Mon, 5 Feb 2024 15:12:06 GMT   (329kb,D)

Title: Do LLMs exhibit human-like response biases? A case study in survey
  design
Authors: Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar,
  Graham Neubig
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.04076 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04534
replaced with revised version Mon, 5 Feb 2024 02:42:57 GMT   (342kb,D)

Title: Loss Masking Is Not Needed in Decoder-only Transformer for
  Discrete-token-based ASR
Authors: Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Shiliang Zhang, Chong
  Deng, Yukun Ma, Hai Yu, Jiaqing Liu, Chong Zhang
Categories: cs.CL cs.SD eess.AS
Comments: 5 pages, accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2311.04534 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06233
replaced with revised version Sat, 3 Feb 2024 18:31:30 GMT   (61kb)

Title: Data Contamination Quiz: A Tool to Detect and Estimate Contamination in
  Large Language Models
Authors: Shahriar Golchin, Mihai Surdeanu
Categories: cs.CL cs.AI cs.LG
Comments: v2 preprint
\\ ( https://arxiv.org/abs/2311.06233 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09308
replaced with revised version Mon, 5 Feb 2024 02:21:59 GMT   (8212kb,D)

Title: Divergences between Language Models and Human Brains
Authors: Yuchen Zhou, Emmy Liu, Graham Neubig, Michael J. Tarr, Leila Wehbe
Categories: cs.CL cs.AI cs.LG q-bio.NC
\\ ( https://arxiv.org/abs/2311.09308 ,  8212kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10642
replaced with revised version Sun, 4 Feb 2024 20:39:33 GMT   (637kb,D)

Title: Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as
  an Alternative to Attention Layers in Transformers
Authors: Vukasin Bozic, Danilo Dordevic, Daniele Coppola, Joseph Thommes, Sidak
  Pal Singh
Categories: cs.CL cs.LG
Comments: Accepted at AAAI24(https://aaai.org/aaai-conference/)
\\ ( https://arxiv.org/abs/2311.10642 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12664
replaced with revised version Mon, 5 Feb 2024 12:50:23 GMT   (1544kb,D)

Title: The DURel Annotation Tool: Human and Computational Measurement of
  Semantic Proximity, Sense Clusters and Semantic Change
Authors: Dominik Schlechtweg, Shafqat Mumtaz Virk, Pauline Sander, Emma
  Sk\"oldberg, Lukas Theuer Linke, Tuo Zhang, Nina Tahmasebi, Jonas Kuhn,
  Sabine Schulte im Walde
Categories: cs.CL cs.AI
Comments: EACL Demo, 7 pages
\\ ( https://arxiv.org/abs/2311.12664 ,  1544kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14736
replaced with revised version Mon, 5 Feb 2024 16:41:10 GMT   (9946kb,D)

Title: Data Diversity Matters for Robust Instruction Tuning
Authors: Alexander Bukharin and Tuo Zhao
Categories: cs.CL cs.LG
Comments: 22 pages, 18 figures
\\ ( https://arxiv.org/abs/2311.14736 ,  9946kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11282
replaced with revised version Sun, 4 Feb 2024 03:45:04 GMT   (938kb,D)

Title: Evaluating and Enhancing Large Language Models for Conversational
  Reasoning on Knowledge Graphs
Authors: Yuxuan Huang, Lida Shi, Anqi Liu and Hao Xu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.11282 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13933
replaced with revised version Sun, 4 Feb 2024 03:57:11 GMT   (1571kb,D)

Title: Structured Probabilistic Coding
Authors: Dou Hu, Lingwei Wei, Yaxin Liu, Wei Zhou, Songlin Hu
Categories: cs.CL cs.LG
Comments: 11 pages, accepted by AAAI 2024 (Oral)
\\ ( https://arxiv.org/abs/2312.13933 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17296
replaced with revised version Fri, 2 Feb 2024 20:33:28 GMT   (2769kb,D)

Title: Structured Packing in LLM Training Improves Long Context Utilization
Authors: Konrad Staniszewski, Szymon Tworkowski, Yu Zhao, Sebastian Jaszczur,
  Henryk Michalewski, {\L}ukasz Kuci\'nski, Piotr Mi{\l}o\'s
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.17296 ,  2769kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01325
replaced with revised version Sat, 3 Feb 2024 06:13:20 GMT   (506kb,D)

Title: LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning
Authors: Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu,
  Chia-Yuan Chang, Huiyuan Chen, Xia Hu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.01325 ,  506kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03735
replaced with revised version Sun, 4 Feb 2024 05:26:41 GMT   (275kb,D)

Title: Language Models Understand Numbers, at Least Partially
Authors: Fangwei Zhu, Damai Dai, Zhifang Sui
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.03735 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04700
replaced with revised version Sun, 4 Feb 2024 19:04:13 GMT   (1662kb,D)

Title: Model Editing Can Hurt General Abilities of Large Language Models
Authors: Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei
  Chang, Nanyun Peng
Categories: cs.CL
Comments: Add new results on LLaMA-2 (7B)
\\ ( https://arxiv.org/abs/2401.04700 ,  1662kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04883
replaced with revised version Sun, 4 Feb 2024 01:50:17 GMT   (4753kb,D)

Title: Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate
  Group Conversations
Authors: Manqing Mao, Paishun Ting, Yijian Xiang, Mingyang Xu, Julia Chen,
  Jianzhe Lin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.04883 ,  4753kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10521
replaced with revised version Sat, 3 Feb 2024 05:59:49 GMT   (8789kb,D)

Title: Cross-lingual Editing in Multilingual Language Models
Authors: Himanshu Beniwal, Kowsik Nandagopan D, Mayank Singh
Categories: cs.CL cs.AI
Comments: Accepted at EACL 2024
\\ ( https://arxiv.org/abs/2401.10521 ,  8789kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11268
replaced with revised version Fri, 2 Feb 2024 22:54:18 GMT   (656kb,D)

Title: Word-Level ASR Quality Estimation for Efficient Corpus Sampling and
  Post-Editing through Analyzing Attentions of a Reference-Free Metric
Authors: Golara Javadi, Kamer Ali Yuksel, Yunsu Kim, Thiago Castro Ferreira,
  Mohamed Al-Badrashiny
Categories: cs.CL cs.SD eess.AS
Journal-ref: 2024 IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP 2024), Seoul, Korea
\\ ( https://arxiv.org/abs/2401.11268 ,  656kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11373
replaced with revised version Fri, 2 Feb 2024 21:28:04 GMT   (8032kb,D)

Title: Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing
  Approach For Uncovering Edge Cases with Minimal Distribution Distortion
Authors: Aly M. Kassem, Sherif Saad
Categories: cs.CL
Comments: EACL 2024 - Main conference - Camera ready version
\\ ( https://arxiv.org/abs/2401.11373 ,  8032kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12585
replaced with revised version Mon, 5 Feb 2024 17:49:40 GMT   (9049kb,D)

Title: SLANG: New Concept Comprehension of Large Language Models
Authors: Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Cheng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.12585 ,  9049kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13565
replaced with revised version Sun, 4 Feb 2024 06:52:28 GMT   (122kb,D)

Title: Large Malaysian Language Model Based on Mistral for Enhanced Local
  Language Understanding
Authors: Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.13565 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14194
replaced with revised version Sat, 3 Feb 2024 05:42:36 GMT   (1169kb,D)

Title: Parameter-Efficient Conversational Recommender System as a Language
  Processing Task
Authors: Mathieu Ravaut, Hao Zhang, Lu Xu, Aixin Sun, Yong Liu
Categories: cs.CL
Comments: 9 pages, 4 figures, 8 tables, EACL 2024 conference
\\ ( https://arxiv.org/abs/2401.14194 ,  1169kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15222
replaced with revised version Mon, 5 Feb 2024 17:13:41 GMT   (1208kb,D)

Title: Transfer Learning for the Prediction of Entity Modifiers in Clinical
  Text: Application to Opioid Use Disorder Case Detection
Authors: Abdullateef I. Almudaifer, Whitney Covington, JaMor Hairston, Zachary
  Deitch, Ankit Anand, Caleb M. Carroll, Estera Crisan, William Bradford,
  Lauren Walter, Eaton Ellen, Sue S. Feldman and John D. Osborne
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages, 2 figures, 6 tables. To be submitted to the Journal of
  Biomedical Semantics
\\ ( https://arxiv.org/abs/2401.15222 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16332
replaced with revised version Mon, 5 Feb 2024 14:53:13 GMT   (1091kb,D)

Title: Tradeoffs Between Alignment and Helpfulness in Language Models
Authors: Yotam Wolf, Noam Wies, Dorin Shteyman, Binyamin Rothberg, Yoav Levine,
  and Amnon Shashua
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.16332 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16736
replaced with revised version Sat, 3 Feb 2024 16:34:46 GMT   (110kb,D)

Title: Engineering A Large Language Model From Scratch
Authors: Abiodun Finbarrs Oketunji
Categories: cs.CL cs.CY cs.LG cs.SE
MSC-class: I.2.7
ACM-class: I.2.7
DOI: 10.5281/zenodo.10613649; 10.13140/RG.2.2.28532.73600/1
\\ ( https://arxiv.org/abs/2401.16736 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16895
replaced with revised version Sat, 3 Feb 2024 07:26:47 GMT   (7857kb,D)

Title: Cross-Lingual Transfer from Related Languages: Treating Low-Resource
  Maltese as Multilingual Code-Switching
Authors: Kurt Micallef, Nizar Habash, Claudia Borg, Fadhl Eryani, Houda Bouamor
Categories: cs.CL
Comments: EACL 2024 camera-ready version
\\ ( https://arxiv.org/abs/2401.16895 ,  7857kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17072
replaced with revised version Mon, 5 Feb 2024 10:53:21 GMT   (7817kb,D)

Title: SemScore: Automated Evaluation of Instruction-Tuned LLMs based on
  Semantic Textual Similarity
Authors: Ansar Aynetdinov, Alan Akbik
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.17072 ,  7817kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17256
replaced with revised version Mon, 5 Feb 2024 18:19:46 GMT   (2148kb,D)

Title: Weak-to-Strong Jailbreaking on Large Language Models
Authors: Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang
  Wang, William Yang Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.17256 ,  2148kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17686
replaced with revised version Sun, 4 Feb 2024 13:18:34 GMT   (8133kb,D)

Title: Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought
  Reasoning
Authors: Tinghui Zhu, Kai Zhang, Jian Xie, Yu Su
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.17686 ,  8133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00263
replaced with revised version Sun, 4 Feb 2024 09:23:23 GMT   (407kb,D)

Title: Does DetectGPT Fully Utilize Perturbation? Selective Perturbation on
  Model-Based Contrastive Learning Detector would be Better
Authors: Shengchao Liu, Xiaoming Liu, Yichen Wang, Zehua Cheng, Chengzhengxu
  Li, Zhaohan Zhang, Yu Lan, Chao Shen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00263 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00861
replaced with revised version Sun, 4 Feb 2024 01:16:25 GMT   (6462kb,D)

Title: Evaluating Large Language Models for Generalization and Robustness via
  Data Compression
Authors: Yucheng Li, Yunhao Guo, Frank Guerin, Chenghua Lin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.00861 ,  6462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01152
replaced with revised version Mon, 5 Feb 2024 05:45:59 GMT   (21329kb,D)

Title: AccentFold: A Journey through African Accents for Zero-Shot ASR
  Adaptation to Target Accents
Authors: Abraham Toluwase Owodunni, Aditya Yadavalli, Chris Chinenye Emezue,
  Tobi Olatunji, Clinton C Mbataku
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to EACL Findings 2024
\\ ( https://arxiv.org/abs/2402.01152 ,  21329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01155
replaced with revised version Mon, 5 Feb 2024 03:42:51 GMT   (3010kb,D)

Title: CABINET: Content Relevance based Noise Reduction for Table Question
  Answering
Authors: Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman
  Kumar, Balaji Krishnamurthy
Categories: cs.CL
Comments: Accepted at ICLR 2024 (spotlight)
\\ ( https://arxiv.org/abs/2402.01155 ,  3010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01376
replaced with revised version Mon, 5 Feb 2024 12:42:52 GMT   (505kb)

Title: LoTR: Low Tensor Rank Weight Adaptation
Authors: Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, Aleksandr Mikhalev
  and Ivan Oseledets
Categories: cs.CL cs.AI cs.LG
Comments: Submitted; missing author and sections were added;
\\ ( https://arxiv.org/abs/2402.01376 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01622
replaced with revised version Mon, 5 Feb 2024 06:48:01 GMT   (2866kb,D)

Title: TravelPlanner: A Benchmark for Real-World Planning with Language Agents
Authors: Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong
  Tian, Yanghua Xiao, Yu Su
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2402.01622 ,  2866kb)
------------------------------------------------------------------------------
\\
arXiv:1810.01864
replaced with revised version Sat, 3 Feb 2024 21:49:19 GMT   (147kb,D)

Title: Agnostic Sample Compression Schemes for Regression
Authors: Idan Attias, Steve Hanneke, Aryeh Kontorovich, Menachem Sadigurschi
Categories: cs.LG cs.IT math.IT math.ST stat.ML stat.TH
Comments: New results in this version: (1) Approximate agnostic sample
  compression scheme for function classes with finite fat-shattering dimension
  and the $\ell_p$ loss (section 3), (2) Near-optimal approximate compression
  for linear functions and the $\ell_p$ loss (section 4.1) The results in
  sections 4.2 and 4.3 appear in the previous version
\\ ( https://arxiv.org/abs/1810.01864 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2105.04240
replaced with revised version Sun, 4 Feb 2024 10:20:48 GMT   (3930kb,D)

Title: A rigorous introduction to linear models
Authors: Jun Lu
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2105.04240 ,  3930kb)
------------------------------------------------------------------------------
\\
arXiv:2111.04746
replaced with revised version Sat, 3 Feb 2024 00:55:16 GMT   (98kb,D)

Title: Realizable Learning is All You Need
Authors: Max Hopkins, Daniel M. Kane, Shachar Lovett, Gaurav Mahajan
Categories: cs.LG stat.ML
MSC-class: 68Q32
Journal-ref: TheoretiCS, Volume 3 (2024), Article 2, 1-62
DOI: 10.46298/theoretics.24.2
\\ ( https://arxiv.org/abs/2111.04746 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2204.04510
replaced with revised version Mon, 5 Feb 2024 14:39:29 GMT   (588kb,D)

Title: Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient
  for Subgraph Representation Learning
Authors: Dongkwan Kim and Alice Oh
Categories: cs.LG cs.AI cs.SI
Comments: 19 pages
\\ ( https://arxiv.org/abs/2204.04510 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2207.13842
replaced with revised version Sun, 4 Feb 2024 00:44:13 GMT   (577kb,D)

Title: Dive into Machine Learning Algorithms for Influenza Virus Host
  Prediction with Hemagglutinin Sequences
Authors: Yanhua Xu and Dominik Wojtczak
Categories: cs.LG
Comments: Published at BioSystems; V1: minor typo correction; V2: minor typo
  correction and add more clarification in "Cross-validation" section
\\ ( https://arxiv.org/abs/2207.13842 ,  577kb)
------------------------------------------------------------------------------
\\
arXiv:2208.13315
replaced with revised version Mon, 5 Feb 2024 07:10:03 GMT   (875kb,D)

Title: ANAct: Adaptive Normalization for Activation Functions
Authors: Yuan Peiwen, Henan Liu, Zhu Changsheng, Yuyi Wang
Categories: cs.LG cs.AI
Comments: 14 pages, 6 figures
\\ ( https://arxiv.org/abs/2208.13315 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2211.05006
replaced with revised version Mon, 5 Feb 2024 13:00:51 GMT   (161kb,D)

Title: Almost Tight Error Bounds on Differentially Private Continual Counting
Authors: Monika Henzinger and Jalaj Upadhyay and Sarvagya Upadhyay
Categories: cs.LG cs.CR cs.DS
Comments: Updated the citations to include two papers we learned about since
  version 01
\\ ( https://arxiv.org/abs/2211.05006 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2211.15335
replaced with revised version Sun, 4 Feb 2024 19:08:53 GMT   (12882kb,D)

Title: You Can Have Better Graph Neural Networks by Not Training Weights at
  All: Finding Untrained GNNs Tickets
Authors: Tianjin Huang, Tianlong Chen, Meng Fang, Vlado Menkovski, Jiaxu Zhao,
  Lu Yin, Yulong Pei, Decebal Constantin Mocanu, Zhangyang Wang, Mykola
  Pechenizkiy, Shiwei Liu
Categories: cs.LG
Comments: Accepted by the LoG conference 2022 as a spotlight
Journal-ref: LoG 2022 (Oral & Best Paper Award)
\\ ( https://arxiv.org/abs/2211.15335 ,  12882kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12407
replaced with revised version Mon, 5 Feb 2024 11:48:16 GMT   (4612kb,D)

Title: FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based
  Model
Authors: Lin Wang, Zhichao Wang, Sai Praneeth Karimireddy and Xiaoying Tang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.12407 ,  4612kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13336
replaced with revised version Sun, 4 Feb 2024 21:32:55 GMT   (689kb,D)

Title: The Fair Value of Data Under Heterogeneous Privacy Constraints in
  Federated Learning
Authors: Justin Kang, Ramtin Pedarsani, Kannan Ramchandran
Categories: cs.LG cs.CR cs.GT
Comments: 29 pages, 5 figures, Accepted to TMLR
\\ ( https://arxiv.org/abs/2301.13336 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02450
replaced with revised version Mon, 5 Feb 2024 18:40:58 GMT   (614kb,D)

Title: Regularization and Optimization in Model-Based Clustering
Authors: Raphael Araujo Sampaio, Joaquim Dias Garcia, Marcus Poggi, Thibaut
  Vidal
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.02450 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03596
replaced with revised version Mon, 5 Feb 2024 02:22:58 GMT   (15122kb,D)

Title: Graph Generation with Diffusion Mixture
Authors: Jaehyeong Jo, Dongki Kim, Sung Ju Hwang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.03596 ,  15122kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04313
replaced with revised version Mon, 5 Feb 2024 01:22:40 GMT   (6241kb,D)

Title: Geometry-Complete Diffusion for 3D Molecule Generation and Optimization
Authors: Alex Morehead, Jianlin Cheng
Categories: cs.LG cs.AI q-bio.BM q-bio.QM stat.ML
Comments: 27 pages, 5 figures, 5 tables. Under review. Also presented at ICLR
  2023's MLDD workshop. Code available at
  https://github.com/BioinfoMachineLearning/Bio-Diffusion
ACM-class: I.2.1; J.3
\\ ( https://arxiv.org/abs/2302.04313 ,  6241kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05793
replaced with revised version Sat, 3 Feb 2024 20:31:14 GMT   (12535kb,D)

Title: Distributional GFlowNets with Quantile Flows
Authors: Dinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, Yoshua
  Bengio
Categories: cs.LG cs.AI stat.CO stat.ML
Comments: Accepted by TMLR
\\ ( https://arxiv.org/abs/2302.05793 ,  12535kb)
------------------------------------------------------------------------------
\\
arXiv:2303.01704
replaced with revised version Fri, 2 Feb 2024 21:58:19 GMT   (2653kb,D)

Title: Feature Importance Disparities for Data Bias Investigations
Authors: Peter W. Chang, Leor Fishman, Seth Neel
Categories: cs.LG cs.CY
Comments: 10 pages, 7 figures. Appendix: 15 pages, 12 figures. Edits:
  Restructuring of paper with more emphasis on usefulness as a hypothesis
  generator for biased subgroups and features. Additional experiment tying our
  work to existing fairness work by observing fairness metrics of our
  discovered subgroups and measuring the FID of subgroups found using prior
  biased subgroup discovery methods
\\ ( https://arxiv.org/abs/2303.01704 ,  2653kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10181
replaced with revised version Sun, 4 Feb 2024 09:11:08 GMT   (1228kb,D)

Title: Operating critical machine learning models in resource constrained
  regimes
Authors: Raghavendra Selvan, Julian Sch\"on, Erik B Dam
Categories: cs.LG cs.AI
Comments: Accepted to the Resource Efficient Medical Image Analysis workshop at
  MICCAI-2023. Source code available at https://github.com/raghavian/redl
Journal-ref: Lecture Notes Comp. Sci.14394 (2023)
DOI: 10.1007/978-3-031-47425-5_29
\\ ( https://arxiv.org/abs/2303.10181 ,  1228kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01285
replaced with revised version Fri, 2 Feb 2024 21:14:18 GMT   (6747kb,D)

Title: X-TIME: An in-memory engine for accelerating machine learning on tabular
  data with CAMs
Authors: Giacomo Pedretti, John Moon, Pedro Bruel, Sergey Serebryakov, Ron M.
  Roth, Luca Buonanno, Archit Gajjar, Tobias Ziegler, Cong Xu, Martin Foltin,
  Paolo Faraboschi, Jim Ignowski, Catherine E. Graves
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.01285 ,  6747kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08172
replaced with revised version Mon, 5 Feb 2024 06:05:33 GMT   (17kb)

Title: Pointwise convergence of Fourier series and deep neural network for the
  indicator function of d-dimensional ball
Authors: Ryota Kawasumi and Tsuyoshi Yoneda
Categories: cs.LG cs.IT math.AP math.IT
Comments: When the version 2 was rejected (where I submitted it to an AI
  journal), I realized I needed to further clarify the key point, and also
  realized the field is rather Fourier analysis
\\ ( https://arxiv.org/abs/2304.08172 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12770
replaced with revised version Sat, 3 Feb 2024 03:21:36 GMT   (5481kb,D)

Title: Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the
  Decoder Network
Authors: Yuri Kinoshita, Kenta Oono, Kenji Fukumizu, Yuichi Yoshida, Shin-ichi
  Maeda
Categories: cs.LG stat.ML
Comments: accepted to ICML 2023, some notations adjusted from the submitted
  version
\\ ( https://arxiv.org/abs/2304.12770 ,  5481kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02614
replaced with revised version Sat, 3 Feb 2024 16:23:06 GMT   (2406kb,D)

Title: High-Dimensional Bayesian Optimization via Semi-Supervised Learning with
  Optimized Unlabeled Data Sampling
Authors: Yuxuan Yin, Yu Wang and Peng Li
Categories: cs.LG cs.AI
Comments: 15 pages
\\ ( https://arxiv.org/abs/2305.02614 ,  2406kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10267
replaced with revised version Mon, 5 Feb 2024 10:09:29 GMT   (175kb,D)

Title: State Representation Learning Using an Unbalanced Atlas
Authors: Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad
Categories: cs.LG
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2305.10267 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12809
replaced with revised version Sat, 3 Feb 2024 07:28:51 GMT   (561kb,D)

Title: Relabeling Minimal Training Subset to Flip a Prediction
Authors: Jinghan Yang, Linjie Xu, Lequan Yu
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2305.12809 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14383
replaced with revised version Mon, 5 Feb 2024 13:23:17 GMT   (2856kb,D)

Title: A Rational Model of Dimension-reduced Human Categorization
Authors: Yifan Hong and Chen Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2305.14383 ,  2856kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17326
replaced with revised version Mon, 5 Feb 2024 12:40:24 GMT   (1877kb,D)

Title: Matrix Information Theory for Self-Supervised Learning
Authors: Yifan Zhang, Zhiquan Tan, Jingqin Yang, Weiran Huang, Yang Yuan
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2305.17326 ,  1877kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18435
replaced with revised version Mon, 5 Feb 2024 01:20:34 GMT   (2492kb,D)

Title: Statistically Efficient Bayesian Sequential Experiment Design via
  Reinforcement Learning with Cross-Entropy Estimators
Authors: Tom Blau, Iadine Chades, Amir Dezfouli, Daniel Steinberg, Edwin V.
  Bonilla
Categories: cs.LG stat.ME
\\ ( https://arxiv.org/abs/2305.18435 ,  2492kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01271
replaced with revised version Mon, 5 Feb 2024 14:21:14 GMT   (429kb,D)

Title: Towards Understanding Clean Generalization and Robust Overfitting in
  Adversarial Training
Authors: Binghui Li, Yuanzhi Li
Categories: cs.LG stat.ML
Comments: 28 pages, comments welcome
\\ ( https://arxiv.org/abs/2306.01271 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01276
replaced with revised version Mon, 5 Feb 2024 04:12:30 GMT   (3444kb,D)

Title: Symmetric Replay Training: Enhancing Sample Efficiency in Deep
  Reinforcement Learning for Combinatorial Optimization
Authors: Hyeonah Kim, Minsu Kim, Sungsoo Ahn, Jinkyoo Park
Categories: cs.LG
Comments: 24 pages (including 12 pages of the appendix)
\\ ( https://arxiv.org/abs/2306.01276 ,  3444kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01992
replaced with revised version Sun, 4 Feb 2024 19:12:16 GMT   (47kb,D)

Title: On Size-Independent Sample Complexity of ReLU Networks
Authors: Mark Sellke
Categories: cs.LG stat.ML
Comments: 4 pages
\\ ( https://arxiv.org/abs/2306.01992 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02689
replaced with revised version Mon, 5 Feb 2024 02:36:54 GMT   (2717kb,D)

Title: Equity-Transformer: Solving NP-hard Min-Max Routing Problems as
  Sequential Generation with Equity Context
Authors: Jiwoo Son, Minsu Kim, Sanghyeok Choi, Hyeonah Kim, Jinkyoo Park
Categories: cs.LG math.OC stat.ML
Comments: AAAI 2024, 16 pages, 6 figures
\\ ( https://arxiv.org/abs/2306.02689 ,  2717kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03163
replaced with revised version Sat, 3 Feb 2024 23:19:08 GMT   (925kb,D)

Title: How Can We Train Deep Learning Models Across Clouds and Continents? An
  Experimental Study
Authors: Alexander Erben, Ruben Mayer, Hans-Arno Jacobsen
Categories: cs.LG cs.DC cs.NI cs.PF
Comments: Published at VLDB 2024. Artifacts and Code:
  https://github.com/cirquit/hivemind-multi-cloud
ACM-class: I.2.11; C.2.4; C.4; D.2.8
\\ ( https://arxiv.org/abs/2306.03163 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04498
replaced with revised version Sun, 4 Feb 2024 23:42:43 GMT   (193kb,D)

Title: Fair Multi-Agent Bandits
Authors: Amir Leshem
Categories: cs.LG cs.CY cs.DC
Comments: 17 pages, 3 figures
\\ ( https://arxiv.org/abs/2306.04498 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04775
replaced with revised version Mon, 5 Feb 2024 00:25:10 GMT   (293kb,D)

Title: Exploiting Observation Bias to Improve Matrix Completion
Authors: Yassir Jedra, Sean Mann, Charlotte Park, Devavrat Shah
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2306.04775 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04940
replaced with revised version Sun, 4 Feb 2024 10:34:10 GMT   (3682kb,D)

Title: LayerAct: Advanced activation mechanism utilizing layer-direction
  normalization for CNNs with BatchNorm
Authors: Kihyuk Yoon and Chiehyeon Lim
Categories: cs.LG cs.CV cs.NE
Comments: 10 pages, 3 figures, 3 tables except appendix
MSC-class: 68T07 (Primary) 68T45 (Secondary)
\\ ( https://arxiv.org/abs/2306.04940 ,  3682kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05035
replaced with revised version Sun, 4 Feb 2024 04:42:26 GMT   (1171kb,D)

Title: Does Long-Term Series Forecasting Need Complex Attention and Extra Long
  Inputs?
Authors: Daojun Liang, Haixia Zhang, Dongfeng Yuan, Xiaoyan Ma, Dongyang Li and
  Minggao Zhang
Categories: cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2306.05035 ,  1171kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08838
replaced with revised version Sun, 4 Feb 2024 22:00:31 GMT   (108kb,D)

Title: Differentially Private Domain Adaptation with Theoretical Guarantees
Authors: Raef Bassily, Corinna Cortes, Anqi Mao, Mehryar Mohri
Categories: cs.LG cs.CR stat.ML
\\ ( https://arxiv.org/abs/2306.08838 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09912
replaced with revised version Mon, 5 Feb 2024 05:04:07 GMT   (1205kb)

Title: Towards Quantum Federated Learning
Authors: Chao Ren, Han Yu, Rudai Yan, Minrui Xu, Yuan Shen, Huihui Zhu, Dusit
  Niyato, Zhao Yang Dong, Leong Chuan Kwek
Categories: cs.LG quant-ph
Comments: Survey of quantum federated learning (QFL)
\\ ( https://arxiv.org/abs/2306.09912 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13339
replaced with revised version Sun, 4 Feb 2024 13:19:35 GMT   (9571kb,D)

Title: TrustGuard: GNN-based Robust and Explainable Trust Evaluation with
  Dynamicity Support
Authors: Jie Wang, Zheng Yan, Jiahe Lan, Elisa Bertino, Witold Pedrycz
Categories: cs.LG cs.AI
Comments: Accepted by IEEE TDSC. Code: https://github.com/Jieerbobo/TrustGuard
DOI: 10.1109/TDSC.2024.3353548
\\ ( https://arxiv.org/abs/2306.13339 ,  9571kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14275
replaced with revised version Sun, 4 Feb 2024 19:18:30 GMT   (10879kb,D)

Title: Enhancing Adversarial Training via Reweighting Optimization Trajectory
Authors: Tianjin Huang, Shiwei Liu, Tianlong Chen, Meng Fang, Li Shen, Vlaod
  Menkovski, Lu Yin, Yulong Pei and Mykola Pechenizkiy
Categories: cs.LG cs.AI
Comments: Accepted by ECML 2023
Journal-ref: ECML 2023
\\ ( https://arxiv.org/abs/2306.14275 ,  10879kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09653
replaced with revised version Sun, 4 Feb 2024 22:26:06 GMT   (453kb,D)

Title: HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual
  Learning
Authors: Xiaotian Duan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2307.09653 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10843
replaced with revised version Fri, 2 Feb 2024 22:51:17 GMT   (21392kb,D)

Title: Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals
  for GPM: A U-Net Convolutional LSTM Architecture
Authors: Reyhaneh Rahimi, Praveen Ravirathinam, Ardeshir Ebtehaj, Ali Behrangi,
  Jackson Tan, Vipin Kumar
Categories: cs.LG cs.CV physics.ao-ph
\\ ( https://arxiv.org/abs/2307.10843 ,  21392kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12971
replaced with revised version Mon, 5 Feb 2024 11:38:48 GMT   (790kb,D)

Title: Big Data - Supply Chain Management Framework for Forecasting: Data
  Preprocessing and Machine Learning Techniques
Authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed
  Ridoy, and M. F. Mridha
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2307.12971 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16375
replaced with revised version Mon, 5 Feb 2024 10:30:12 GMT   (596kb,D)

Title: UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed
  Integer Quadratic Programming
Authors: Hao Lin, Ke Wu, Jie Li, Jun Li, Wu-Jun Li
Categories: cs.LG cs.DC math.OC
Comments: 15 pages, 10 figures
\\ ( https://arxiv.org/abs/2307.16375 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01222
replaced with revised version Mon, 5 Feb 2024 09:18:18 GMT   (1487kb,D)

Title: Calibration in Deep Learning: A Survey of the State-of-the-Art
Authors: Cheng Wang
Categories: cs.LG cs.AI
Comments: 18 pages
\\ ( https://arxiv.org/abs/2308.01222 ,  1487kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07037
replaced with revised version Sat, 3 Feb 2024 20:22:57 GMT   (3155kb,D)

Title: Bayesian Flow Networks
Authors: Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.07037 ,  3155kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10699
replaced with revised version Mon, 5 Feb 2024 13:52:22 GMT   (4524kb,D)

Title: Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed
  Bandit Approach
Authors: Arman Rahbar, Niklas {\AA}kerblom, Morteza Haghir Chehreghani
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.10699 ,  4524kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11129
replaced with revised version Sat, 3 Feb 2024 08:23:09 GMT   (4013kb,D)

Title: Enhancing Graph Transformers with Hierarchical Distance Structural
  Encoding
Authors: Yuankai Luo
Categories: cs.LG cs.AI cs.SI
\\ ( https://arxiv.org/abs/2308.11129 ,  4013kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12252
replaced with revised version Fri, 2 Feb 2024 21:42:19 GMT   (14201kb,D)

Title: How Safe Am I Given What I See? Calibrated Prediction of Safety Chances
  for Image-Controlled Autonomy
Authors: Zhenjiang Mao, Carson Sobolewski, Ivan Ruchkin
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.12252 ,  14201kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13320
replaced with revised version Sun, 4 Feb 2024 19:28:06 GMT   (928kb,D)

Title: Fine-tuning can cripple your foundation model; preserving features may
  be the solution
Authors: Jishnu Mukhoti, Yarin Gal, Philip H.S. Torr, Puneet K. Dokania
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2308.13320 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01922
replaced with revised version Fri, 2 Feb 2024 19:37:09 GMT   (47kb)

Title: Regret Analysis of Policy Gradient Algorithm for Infinite Horizon
  Average Reward Markov Decision Processes
Authors: Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal
Categories: cs.LG cs.AI
Journal-ref: AAAI 2024
\\ ( https://arxiv.org/abs/2309.01922 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06256
replaced with revised version Mon, 5 Feb 2024 06:43:17 GMT   (2095kb,D)

Title: Mitigating the Alignment Tax of RLHF
Authors: Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng
  Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie
  Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, Tong Zhang
Categories: cs.LG
Comments: 28 Pages
\\ ( https://arxiv.org/abs/2309.06256 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06991
replaced with revised version Sat, 3 Feb 2024 05:52:02 GMT   (453kb,D)

Title: Unsupervised Contrast-Consistent Ranking with Language Models
Authors: Niklas Stoehr, Pengxiang Cheng, Jing Wang, Daniel Preotiuc-Pietro,
  Rajarshi Bhowmik
Categories: cs.LG cs.CL stat.ML
Comments: Long Paper at EACL 2024
\\ ( https://arxiv.org/abs/2309.06991 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09944
replaced with revised version Mon, 5 Feb 2024 16:33:03 GMT   (14429kb,D)

Title: DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by
  Generative Text-to-Image Models
Authors: Zoe De Simone and Angie Boggust and Arvind Satyanarayan and Ashia
  Wilson
Categories: cs.LG cs.AI cs.CV cs.CY
Comments: 20 pages, 8 figures
\\ ( https://arxiv.org/abs/2309.09944 ,  14429kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13950
replaced with revised version Sat, 3 Feb 2024 03:45:43 GMT   (42kb,D)

Title: Local and Global Trend Bayesian Exponential Smoothing Models
Authors: Slawek Smyl, Christoph Bergmeir, Alexander Dokumentov, Xueying Long,
  Erwin Wibowo, Daniel Schmidt
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.13950 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15395
replaced with revised version Mon, 5 Feb 2024 11:29:40 GMT   (1917kb,D)

Title: Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs
Authors: Zihan Zhou, Honghao Wei, Lei Ying
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.15395 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16519
replaced with revised version Mon, 5 Feb 2024 11:01:48 GMT   (3548kb,D)

Title: AtomSurf : Surface Representation for Learning on Protein Structures
Authors: Vincent Mallet, Souhaib Attaiki and Maks Ovsjanikov
Categories: cs.LG q-bio.BM
Comments: 10 pages
\\ ( https://arxiv.org/abs/2309.16519 ,  3548kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17196
replaced with revised version Mon, 5 Feb 2024 08:40:05 GMT   (7074kb,D)

Title: ResBit: Residual Bit Vector for Categorical Values
Authors: Masane Fuchi, Amar Zanashir, Hiroto Minami, Tomohiro Takagi
Categories: cs.LG
Comments: 20pages, 7 figures, 24 tables
\\ ( https://arxiv.org/abs/2309.17196 ,  7074kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01886
replaced with revised version Sat, 3 Feb 2024 15:22:33 GMT   (249kb,D)

Title: BYOM: Building Your Own Multi-Task Model For Free
Authors: Weisen Jiang and Baijiong Lin and Han Shi and Yu Zhang and Zhenguo Li
  and James T. Kwok
Categories: cs.LG cs.CL cs.CV
Comments: Technical Report
\\ ( https://arxiv.org/abs/2310.01886 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02025
replaced with revised version Sun, 4 Feb 2024 00:55:18 GMT   (1561kb,D)

Title: DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training
Authors: Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer, Jiancheng
  Liu, Konstantinos Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura,
  Sijia Liu
Categories: cs.LG
Comments: Accepted to ICLR'24
\\ ( https://arxiv.org/abs/2310.02025 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02299
replaced with revised version Sat, 3 Feb 2024 01:48:50 GMT   (9398kb,D)

Title: Discovering Symmetry Breaking in Physical Systems with Relaxed Group
  Convolution
Authors: Rui Wang, Elyssa Hofgard, Han Gao, Robin Walters, Tess E.Smidt
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.02299 ,  9398kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02698
replaced with revised version Sun, 4 Feb 2024 18:18:45 GMT   (2135kb,D)

Title: Enhanced Federated Optimization: Adaptive Unbiased Sampling with Reduced
  Variance
Authors: Dun Zeng, Zenglin Xu, Yu Pan, Xu Luo, Qifan Wang, Xiaoying Tang
Categories: cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2310.02698 ,  2135kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02702
replaced with revised version Sun, 4 Feb 2024 18:33:24 GMT   (3867kb,D)

Title: Tackling Hybrid Heterogeneity on Federated Optimization via Gradient
  Diversity Maximization
Authors: Dun Zeng, Zenglin Xu, Yu Pan, Qifan Wang, Xiaoying Tang
Categories: cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2310.02702 ,  3867kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02823
replaced with revised version Mon, 5 Feb 2024 02:13:16 GMT   (13625kb,D)

Title: Learning to Scale Logits for Temperature-Conditional GFlowNets
Authors: Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan,
  Woochang Kim, Jinkyoo Park, Emmanuel Bengio, Yoshua Bengio
Categories: cs.LG stat.ML
Comments: 20 pages, 21 figures
\\ ( https://arxiv.org/abs/2310.02823 ,  13625kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03052
replaced with revised version Sat, 3 Feb 2024 16:52:45 GMT   (45488kb,D)

Title: Memoria: Resolving Fateful Forgetting Problem through Human-Inspired
  Memory Architecture
Authors: Sangjun Park and JinYeong Bak
Categories: cs.LG cs.AI cs.NE
Comments: Preprint. 27 pages, 13 figures, 10 tables
\\ ( https://arxiv.org/abs/2310.03052 ,  45488kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03512
replaced with revised version Mon, 5 Feb 2024 12:28:41 GMT   (4493kb,D)

Title: Otago Exercises Monitoring for Older Adults by a Single IMU and
  Hierarchical Machine Learning Models
Authors: Meng Shang, Lenore Dedeyne, Jolan Dupont, Laura Vercauteren, Nadjia
  Amini, Laurence Lapauw, Evelien Gielen, Sabine Verschueren, Carolina Varon,
  Walter De Raedt, and Bart Vanrumste
Categories: cs.LG
Comments: 10 pages
Journal-ref: IEEE Transactions on Neural Systems and Rehabilitation
  Engineering, vol. 32, pp. 462-471, 2024
DOI: 10.1109/TNSRE.2024.3355299
\\ ( https://arxiv.org/abs/2310.03512 ,  4493kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04861
replaced with revised version Sun, 4 Feb 2024 01:49:07 GMT   (23374kb,D)

Title: Uncovering hidden geometry in Transformers via disentangling position
  and context
Authors: Jiajun Song and Yiqiao Zhong
Categories: cs.LG cs.AI stat.ML
Comments: 38 pages, 34 figures
\\ ( https://arxiv.org/abs/2310.04861 ,  23374kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05401
replaced with revised version Mon, 5 Feb 2024 03:54:42 GMT   (1449kb,D)

Title: Entropy-MCMC: Sampling from Flat Basins with Ease
Authors: Bolian Li, Ruqi Zhang
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.05401 ,  1449kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06112
replaced with revised version Sun, 4 Feb 2024 16:31:50 GMT   (264kb,D)

Title: Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK
  Approach
Authors: Shaopeng Fu, Di Wang
Categories: cs.LG stat.ML
Comments: In Twelfth International Conference on Learning Representations (ICLR
  2024)
\\ ( https://arxiv.org/abs/2310.06112 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06644
replaced with revised version Mon, 5 Feb 2024 15:02:32 GMT   (34135kb,D)

Title: Zero-Level-Set Encoder for Neural Distance Fields
Authors: Stefan Rhys Jeske and Jonathan Klein and Dominik L. Michels and Jan
  Bender
Categories: cs.LG cs.GR
\\ ( https://arxiv.org/abs/2310.06644 ,  34135kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08164
replaced with revised version Mon, 5 Feb 2024 07:02:03 GMT   (300kb,D)

Title: Beyond Training Objectives: Interpreting Reward Model Divergence in
  Large Language Models
Authors: Luke Marks, Amir Abdullah, Luna Mendez, Rauno Arike, Philip Torr, Fazl
  Barez
Categories: cs.LG
Comments: 19 pages, 5 figures
\\ ( https://arxiv.org/abs/2310.08164 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09486
replaced with revised version Sun, 4 Feb 2024 05:08:00 GMT   (1967kb,D)

Title: Mirage: Model-Agnostic Graph Distillation for Graph Classification
Authors: Mridul Gupta and Sahil Manchanda and Hariprasad Kodamana and Sayan
  Ranu
Categories: cs.LG cs.AI
Comments: 14 pages, 14 figures
\\ ( https://arxiv.org/abs/2310.09486 ,  1967kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10565
replaced with revised version Sun, 4 Feb 2024 13:28:46 GMT   (25213kb,D)

Title: HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid
  Prediction
Authors: Lanxiang Xing, Haixu Wu, Yuezhou Ma, Jianmin Wang, Mingsheng Long
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.10565 ,  25213kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11677
replaced with revised version Mon, 5 Feb 2024 15:33:18 GMT   (36kb)

Title: Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm
  with General Parameterization for Infinite Horizon Discounted Reward Markov
  Decision Processes
Authors: Washim Uddin Mondal and Vaneet Aggarwal
Categories: cs.LG cs.AI
Journal-ref: AISTATS 2024
\\ ( https://arxiv.org/abs/2310.11677 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13397
replaced with revised version Mon, 5 Feb 2024 16:37:11 GMT   (27571kb,D)

Title: Equivariant Deep Weight Space Alignment
Authors: Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik, Nadav Dym,
  Haggai Maron
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.13397 ,  27571kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15393
replaced with revised version Mon, 5 Feb 2024 16:33:05 GMT   (7719kb,D)

Title: DoGE: Domain Reweighting with Generalization Estimation
Authors: Simin Fan, Matteo Pagliardini, Martin Jaggi
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.15393 ,  7719kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15524
replaced with revised version Sat, 3 Feb 2024 20:24:38 GMT   (1906kb,D)

Title: On the Inherent Privacy Properties of Discrete Denoising Diffusion
  Models
Authors: Rongzhe Wei, Eleonora Krea\v{c}i\'c, Haoyu Wang, Haoteng Yin, Eli
  Chien, Vamsi K. Potluru, Pan Li
Categories: cs.LG
Comments: 52 pages
\\ ( https://arxiv.org/abs/2310.15524 ,  1906kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16401
replaced with revised version Sat, 3 Feb 2024 04:45:45 GMT   (1223kb,D)

Title: Graph Neural Networks with a Distribution of Parametrized Graphs
Authors: See Hian Lee, Feng Ji, Kelin Xia and Wee Peng Tay
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.16401 ,  1223kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18384
replaced with revised version Sun, 4 Feb 2024 09:54:06 GMT   (481kb,D)

Title: MicroNAS: Memory and Latency Constrained Hardware-Aware Neural
  Architecture Search for Time Series Classification on Microcontrollers
Authors: Tobias King, Yexu Zhou, Tobias R\"oddiger, Michael Beigl
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.18384 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18765
replaced with revised version Mon, 5 Feb 2024 16:37:42 GMT   (341kb,D)

Title: Rethinking Semi-Supervised Imbalanced Node Classification from
  Bias-Variance Decomposition
Authors: Divin Yan, Gengchen Wei, Chen Yang, Shengzhong Zhang, Zengfeng Huang
Categories: cs.LG
Comments: Accepted by NeurIPS 2023
Journal-ref: Thirty-seventh Conference on Neural Information Processing
  Systems. (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2310.18765 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18936
replaced with revised version Sat, 3 Feb 2024 23:35:39 GMT   (699kb)

Title: Adversarial Examples Are Not Real Features
Authors: Ang Li, Yifei Wang, Yiwen Guo, Yisen Wang
Categories: cs.LG cs.CV
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.18936 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01223
replaced with revised version Mon, 5 Feb 2024 07:03:58 GMT   (1494kb,D)

Title: Diffusion Models for Reinforcement Learning: A Survey
Authors: Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang,
  Haoquan Guo, Tingting Chen, Weinan Zhang
Categories: cs.LG cs.AI
Comments: 13 pages, 2 figures, 1 table
\\ ( https://arxiv.org/abs/2311.01223 ,  1494kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01441
replaced with revised version Mon, 5 Feb 2024 00:10:44 GMT   (1823kb,D)

Title: Distilling Out-of-Distribution Robustness from Vision-Language
  Foundation Models
Authors: Andy Zhou and Jindong Wang and Yu-Xiong Wang and Haohan Wang
Categories: cs.LG cs.AI cs.CV
Comments: Published in NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.01441 ,  1823kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01568
replaced with revised version Fri, 2 Feb 2024 20:58:42 GMT   (422kb,D)

Title: Anytime-Competitive Reinforcement Learning with Policy Prior
Authors: Jianyi Yang, Pengfei Li, Tongxin Li, Adam Wierman, Shaolei Ren
Categories: cs.LG
Comments: Accepted by NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.01568 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03340
replaced with revised version Mon, 5 Feb 2024 11:49:07 GMT   (2199kb,D)

Title: Multitask Kernel-based Learning with First-Order Logic Constraints
Authors: Michelangelo Diligenti, Marco Gori, Marco Maggini and Leonardo
  Rigutini
Categories: cs.LG cs.AI cs.LO
Comments: The 20th International Conference on Inductive Logic Programming (ILP
  2010). Florence, Italy. June 27-30 2010
Journal-ref: Proceedings of The 20th International Conference on Inductive
  Logic Programming (ILP 2010)
DOI: 10.48550/arXiv.2311.03340
\\ ( https://arxiv.org/abs/2311.03340 ,  2199kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03415
replaced with revised version Mon, 5 Feb 2024 15:16:56 GMT   (3151kb,D)

Title: PowerFlowNet: Power Flow Approximation Using Message Passing Graph
  Neural Networks
Authors: Nan Lin, Stavros Orfanoudakis, Nathan Ordonez Cardenas, Juan S.
  Giraldo, Pedro P. Vergara
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 10 pages, 7 figures
\\ ( https://arxiv.org/abs/2311.03415 ,  3151kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10246
replaced with revised version Fri, 2 Feb 2024 21:48:18 GMT   (856kb,D)

Title: Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric
  Learning
Authors: Amartya Banerjee, Christopher J. Hazard, Jacob Beel, Cade Mack, Jack
  Xia, Michael Resnick, Will Goddin
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2311.10246 ,  856kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12267
replaced with revised version Sat, 3 Feb 2024 06:56:00 GMT   (8935kb,D)

Title: Learning Causal Representations from General Environments:
  Identifiability and Intrinsic Ambiguity
Authors: Jikai Jin and Vasilis Syrgkanis
Categories: cs.LG cs.AI econ.EM stat.AP stat.ML
Comments: 42 pages
\\ ( https://arxiv.org/abs/2311.12267 ,  8935kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12495
replaced with revised version Mon, 5 Feb 2024 08:56:23 GMT   (7256kb,D)

Title: Multi-Objective Reinforcement Learning Based on Decomposition: A
  Taxonomy and Framework
Authors: Florian Felten and El-Ghazali Talbi and Gr\'egoire Danoy
Categories: cs.LG
Comments: Accepted at JAIR
\\ ( https://arxiv.org/abs/2311.12495 ,  7256kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13664
replaced with revised version Sun, 4 Feb 2024 22:29:41 GMT   (5441kb,D)

Title: Sample as You Infer: Predictive Coding With Langevin Dynamics
Authors: Umais Zahid, Qinghai Guo, Zafeirios Fountas
Categories: cs.LG cs.AI cs.CV cs.NE
Comments: FID values updated to use a fixed 50,000 samples for all experiments
  - Jeffrey's divergence now consistently best performing. Dynov2 based metrics
  removed due to inconsistency of results - and since not industry standard.
  Multiple beta values tested in Fig 4. Theta LR for VAEs; beta and inf LR for
  LPC now tuned for results. Figure 5B updated; curves now correspond to
  results in Table 1
ACM-class: I.2.0; I.2.6; I.2.10; I.4.0; I.4.8
\\ ( https://arxiv.org/abs/2311.13664 ,  5441kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14652
replaced with revised version Mon, 5 Feb 2024 18:30:30 GMT   (155kb,D)

Title: One Pass Streaming Algorithm for Super Long Token Attention
  Approximation in Sublinear Space
Authors: Raghav Addanki, Chenyang Li, Zhao Song, Chiwun Yang
Categories: cs.LG cs.CL stat.ML
\\ ( https://arxiv.org/abs/2311.14652 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16054
replaced with revised version Sat, 3 Feb 2024 08:13:50 GMT   (5859kb,D)

Title: Metric Space Magnitude for Evaluating the Diversity of Latent
  Representations
Authors: Katharina Limbeck, Rayna Andreeva, Rik Sarkar, Bastian Rieck
Categories: cs.LG math.GT stat.ML
\\ ( https://arxiv.org/abs/2311.16054 ,  5859kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16203
replaced with revised version Mon, 5 Feb 2024 02:46:11 GMT   (32368kb,D)

Title: ChatTraffic: Text-to-Traffic Generation via Diffusion Model
Authors: Chengyang Zhang, Yong Zhang, Qitan Shao, Bo Li, Yisheng Lv, Xinglin
  Piao, Baocai Yin
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.16203 ,  32368kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17539
replaced with revised version Mon, 5 Feb 2024 11:30:29 GMT   (446kb,D)

Title: Analyzing Sharpness-aware Minimization under Overparameterization
Authors: Sungbin Shin, Dongyeop Lee, Maksym Andriushchenko, Namhoon Lee
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2311.17539 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18022
replaced with revised version Fri, 2 Feb 2024 23:05:32 GMT   (543kb,D)

Title: Compelling ReLU Network Initialization and Training to Leverage
  Exponential Scaling with Depth
Authors: Max Milkert and David Hyde and Forrest Laine
Categories: cs.LG cs.AI
Comments: 13 pages, 6 figures
\\ ( https://arxiv.org/abs/2311.18022 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18703
replaced with revised version Sat, 3 Feb 2024 12:25:29 GMT   (4669kb,D)

Title: Predictable Reinforcement Learning Dynamics through Entropy Rate
  Minimization
Authors: Daniel Jarne Ornia, Giannis Delimpaltadakis, Jens Kober, Javier
  Alonso-Mora
Categories: cs.LG cs.AI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2311.18703 ,  4669kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18751
replaced with revised version Mon, 5 Feb 2024 01:13:52 GMT   (590kb,D)

Title: Exposing Limitations of Language Model Agents in Sequential-Task
  Compositions on the Web
Authors: Hiroki Furuta, Yutaka Matsuo, Aleksandra Faust, Izzeddin Gur
Categories: cs.LG cs.AI cs.CL
Comments: Code:
  https://github.com/google-research/google-research/tree/master/compositional_rl/compwob
\\ ( https://arxiv.org/abs/2311.18751 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03885
replaced with revised version Sat, 3 Feb 2024 09:00:08 GMT   (146kb,D)

Title: Adapting Newton's Method to Neural Networks through a Summary of
  Higher-Order Derivatives
Authors: Pierre Wolinski
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2312.03885 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04693
replaced with revised version Mon, 5 Feb 2024 10:10:29 GMT   (4820kb,D)

Title: GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of
  Aligned Experts
Authors: Shirley Wu, Kaidi Cao, Bruno Ribeiro, James Zou, Jure Leskovec
Categories: cs.LG
Comments: Graph Neural Networks, Mixture-of-experts, Distribution Shifts,
  Generalization
\\ ( https://arxiv.org/abs/2312.04693 ,  4820kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06837
replaced with revised version Fri, 2 Feb 2024 20:39:27 GMT   (1820kb,D)

Title: Spectral State Space Models
Authors: Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.06837 ,  1820kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11540
replaced with revised version Sat, 3 Feb 2024 11:26:49 GMT   (22kb)

Title: On the Trade-off between the Number of Nodes and the Number of Trees in
  a Random Forest
Authors: Tatsuya Akutsu, Avraham A. Melkman, Atsuhiro Takasu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.11540 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13508
replaced with revised version Sun, 4 Feb 2024 07:44:56 GMT   (41234kb,D)

Title: Multimodal Federated Learning with Missing Modality via Prototype Mask
  and Contrast
Authors: Guangyin Bao, Qi Zhang, Duoqian Miao, Zixuan Gong, Liang Hu, Ke Liu,
  Yang Liu, Chongyang Shi
Categories: cs.LG cs.AI cs.DC
Comments: 23 pages
\\ ( https://arxiv.org/abs/2312.13508 ,  41234kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16043
replaced with revised version Mon, 5 Feb 2024 01:55:13 GMT   (4705kb,D)

Title: An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced
  linear classification
Authors: Hyenkyun Woo
Categories: cs.LG cs.AI cs.CV cs.NE stat.ML
Comments: 24 pages, 9 figures, a typo is corrected
\\ ( https://arxiv.org/abs/2312.16043 ,  4705kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16046
replaced with revised version Sun, 4 Feb 2024 06:43:43 GMT   (2545kb,D)

Title: AdaNAS: Adaptively Post-processing with Self-supervised Neural
  Architecture Search for Ensemble Rainfall Forecasts
Authors: Yingpeng Wen, Weijiang Yu, Fudan Zheng, Dan Huang, Nong Xiao
Categories: cs.LG cs.AI physics.ao-ph
\\ ( https://arxiv.org/abs/2312.16046 ,  2545kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05043
replaced with revised version Fri, 2 Feb 2024 21:46:57 GMT   (246kb,D)

Title: CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation
  in Classification Tasks
Authors: Kaizheng Wang, Keivan Shariatmadar, Shireen Kudukkil Manchingal, Fabio
  Cuzzolin, David Moens, Hans Hallez
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.05043 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08875
replaced with revised version Mon, 5 Feb 2024 08:43:29 GMT   (5930kb)

Title: DCRMTA: Unbiased Causal Representation for Multi-touch Attribution
Authors: Jiaming Tang
Categories: cs.LG cs.AI stat.ME
Comments: 9 pages, 9 figures
\\ ( https://arxiv.org/abs/2401.08875 ,  5930kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09486
replaced with revised version Sun, 4 Feb 2024 03:14:08 GMT   (1371kb,D)

Title: LoMA: Lossless Compressed Memory Attention
Authors: Yumeng Wang, Zhenyang Xiao
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2401.09486 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11929
replaced with revised version Sat, 3 Feb 2024 02:24:06 GMT   (7516kb,D)

Title: The Bigger the Better? Rethinking the Effective Model Scale in Long-term
  Time Series Forecasting
Authors: Jinliang Deng, Xuan Song, Ivor W. Tsang, Hui Xiong
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.11929 ,  7516kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11940
replaced with revised version Sat, 3 Feb 2024 02:47:42 GMT   (505kb,D)

Title: Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent
Authors: Zhiyu Liu, Zhi Han, Yandong Tang, Xi-Le Zhao, Yao Wang
Categories: cs.LG math.OC stat.ML
Comments: 13 pages, 4 figures
\\ ( https://arxiv.org/abs/2401.11940 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14580
replaced with revised version Mon, 5 Feb 2024 07:06:33 GMT   (69kb,D)

Title: Design Your Own Universe: A Physics-Informed Agnostic Method for
  Enhancing Graph Neural Networks
Authors: Dai Shi, Andi Han, Lequan Lin, Yi Guo, Zhiyong Wang, Junbin Gao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.14580 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14591
replaced with revised version Sun, 4 Feb 2024 09:31:48 GMT   (10152kb,D)

Title: Ricci flow-guided autoencoders in learning time-dependent dynamics
Authors: Andrew Gracyk
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.14591 ,  10152kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15077
replaced with revised version Sun, 4 Feb 2024 17:18:34 GMT   (2228kb,D)

Title: EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty
Authors: Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2401.15077 ,  2228kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15268
replaced with revised version Fri, 2 Feb 2024 20:39:57 GMT   (0kb,I)

Title: Towards Stable Preferences for Stakeholder-aligned Machine Learning
Authors: Haleema Sheraz, Stefan C. Kremer, Joshua August Skorburg, Graham
  Taylor, Walter Sinnott-Armstrong, Kyle Boerstler
Categories: cs.LG cs.AI
Comments: Work in Progress
\\ ( https://arxiv.org/abs/2401.15268 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16452
replaced with revised version Sat, 3 Feb 2024 04:45:58 GMT   (906kb,D)

Title: Context-Former: Stitching via Latent Conditioned Sequence Modeling
Authors: Ziqi Zhang, Jingzehua Xu, Jinxin Liu, Zifeng Zhuang, Donglin Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.16452 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16808
replaced with revised version Sat, 3 Feb 2024 07:03:53 GMT   (1346kb,D)

Title: Encoding Temporal Statistical-space Priors via Augmented Representation
Authors: Insu Choi, Woosung Koh, Gimin Kang, Yuntae Jang, Woo Chang Kim
Categories: cs.LG cs.AI
Comments: pre-print
\\ ( https://arxiv.org/abs/2401.16808 ,  1346kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17263
replaced with revised version Fri, 2 Feb 2024 21:18:57 GMT   (2198kb,D)

Title: Robust Prompt Optimization for Defending Language Models Against
  Jailbreaking Attacks
Authors: Andy Zhou and Bo Li and Haohan Wang
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: website and code available at https://andyz245.github.io/rpo/
\\ ( https://arxiv.org/abs/2401.17263 ,  2198kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17612
replaced with revised version Sun, 4 Feb 2024 16:41:47 GMT   (4651kb,D)

Title: IGCN: Integrative Graph Convolutional Networks for Multi-modal Data
Authors: Cagri Ozdemir, Mohammad Al Olaimat, Yashu Vashishath, Serdar Bozdag
  and Alzheimer's Disease Neuroimaging Initiative
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.17612 ,  4651kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17870
replaced with revised version Mon, 5 Feb 2024 12:43:24 GMT   (2123kb,D)

Title: Efficient Subseasonal Weather Forecast using Teleconnection-informed
  Transformers
Authors: Shan Zhao, Zhitong Xiong, Xiao Xiang Zhu
Categories: cs.LG cs.AI
Comments: Submitted to IGARSS 2024
\\ ( https://arxiv.org/abs/2401.17870 ,  2123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00798
replaced with revised version Sun, 4 Feb 2024 22:16:48 GMT   (898kb,D)

Title: Formal-LLM: Integrating Formal Language and Natural Language for
  Controllable LLM-based Agents
Authors: Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang
Categories: cs.LG cs.AI cs.CL cs.FL
Comments: 21 pages, 6 figures; comments and suggestions are welcome
\\ ( https://arxiv.org/abs/2402.00798 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01204
replaced with revised version Mon, 5 Feb 2024 05:35:16 GMT   (295kb,D)

Title: A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
Authors: Wei-Yao Wang, Wei-Wei Du, Derek Xu, Wei Wang, Wen-Chih Peng
Categories: cs.LG cs.AI
Comments: The paper list can be found at
  https://github.com/wwweiwei/awesome-self-supervised-learning-for-tabular-data
\\ ( https://arxiv.org/abs/2402.01204 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01207
replaced with revised version Mon, 5 Feb 2024 04:44:23 GMT   (168kb,D)

Title: Efficient Causal Graph Discovery Using Large Language Models
Authors: Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, Yoshua
  Bengio
Categories: cs.LG cs.AI stat.ME
\\ ( https://arxiv.org/abs/2402.01207 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01262
replaced with revised version Mon, 5 Feb 2024 14:14:02 GMT   (515kb,D)

Title: Cascaded Scaling Classifier: class incremental learning with probability
  scaling
Authors: Jary Pomponi, Alessio Devoto, Simone Scardapane
Categories: cs.LG cs.CV
Comments: Paper under review. The official code is available
  https://github.com/jaryP/Cascaded-Scaling-Classifier
\\ ( https://arxiv.org/abs/2402.01262 ,  515kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01297
replaced with revised version Mon, 5 Feb 2024 08:58:42 GMT   (116kb,D)

Title: Characterizing Overfitting in Kernel Ridgeless Regression Through the
  Eigenspectrum
Authors: Tin Sum Cheng and Aurelien Lucchi and Anastasis Kratsios and David
  Belius
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.01297 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01327
replaced with revised version Mon, 5 Feb 2024 04:19:16 GMT   (161kb,D)

Title: Supervised Algorithmic Fairness in Distribution Shifts: A Survey
Authors: Yujie Lin, Dong Li, Chen Zhao, Xintao Wu, Qin Tian, Minglai Shao
Categories: cs.LG cs.AI cs.CY
\\ ( https://arxiv.org/abs/2402.01327 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01401
replaced with revised version Mon, 5 Feb 2024 12:39:26 GMT   (3055kb,D)

Title: Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
Authors: Jack Foster, Kyle Fogarty, Stefan Schoepf, Cengiz \"Oztireli,
  Alexandra Brintrup
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2402.01401 ,  3055kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01481
replaced with revised version Mon, 5 Feb 2024 05:55:50 GMT   (2979kb,D)

Title: Multi-level protein pre-training with Vabs-Net
Authors: Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, Shuqi Lu
Categories: cs.LG cs.AI q-bio.BM
\\ ( https://arxiv.org/abs/2402.01481 ,  2979kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03482
replaced with revised version Mon, 5 Feb 2024 12:56:43 GMT   (6500kb,D)

Title: Navigating Neural Space: Revisiting Concept Activation Vectors to
  Overcome Directional Divergence
Authors: Frederik Pahde, Maximilian Dreyer, Leander Weber, Moritz Weckbecker,
  Christopher J. Anders, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2202.03482 ,  6500kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01595
replaced with revised version Mon, 5 Feb 2024 07:59:34 GMT   (5370kb,D)

Title: FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast
  Fourier Convolutions
Authors: Bruno Berenguel-Baeta, Jesus Bermudez-Cameo and Jose J. Guerrero
Categories: cs.CV cs.AI cs.RO
Comments: 7 pages, 5 figures, 3 tables
DOI: 10.1109/ICRA48891.2023.10161142
\\ ( https://arxiv.org/abs/2210.01595 ,  5370kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10253
replaced with revised version Mon, 5 Feb 2024 13:53:45 GMT   (5948kb,D)

Title: Multiobjective Evolutionary Pruning of Deep Neural Networks with
  Transfer Learning for improving their Performance and Robustness
Authors: Javier Poyatos, Daniel Molina, Aitor Mart\'inez, Javier Del Ser,
  Francisco Herrera
Categories: cs.NE cs.AI
Comments: 28 pages, 11 figures
ACM-class: I.2; I.4
Journal-ref: Applied Soft Computing, 147 (2023), 110757
DOI: 10.1016/j.asoc.2023.110757
\\ ( https://arxiv.org/abs/2302.10253 ,  5948kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07651
replaced with revised version Fri, 2 Feb 2024 21:23:18 GMT   (10264kb,D)

Title: Context Normalization Layer with Applications
Authors: Bilal Faye, Mohamed-Djallel Dilmi, Hanane Azzag, Mustapha Lebbah,
  Djamel Bouchaffra
Categories: cs.CV cs.AI cs.LG
Journal-ref: Fifth-icdm-workshop-dlc-2023 Fifth-icdm-workshop-dlc-2023
  Fifth-icdm-workshop-dlc-2023
  https://sites.google.com/view/fifth-icdm-workshop-dlc-2023/home
\\ ( https://arxiv.org/abs/2303.07651 ,  10264kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14065
replaced with revised version Mon, 5 Feb 2024 01:29:35 GMT   (3612kb,D)

Title: Lightweight, Pre-trained Transformers for Remote Sensing Timeseries
Authors: Gabriel Tseng, Ruben Cartuyvels, Ivan Zvonkov, Mirali Purohit, David
  Rolnick, Hannah Kerner
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2304.14065 ,  3612kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14922 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 22:20:19 GMT   (5140kb,D)

Title: Supervised and Unsupervised Deep Learning Approaches for EEG Seizure
  Prediction
Authors: Zakary Georgis-Yap, Milos R. Popovic, Shehroz S. Khan
Categories: eess.SP cs.AI cs.LG
Comments: 16 figures, 9 tables
Journal-ref: Journal of Health Informatics Research, 2024
\\ ( https://arxiv.org/abs/2304.14922 ,  5140kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04228
replaced with revised version Sat, 3 Feb 2024 09:15:20 GMT   (134kb)

Title: Heterogeneous Directed Hypergraph Neural Network over abstract syntax
  tree (AST) for Code Classification
Authors: Guang Yang, Tiancheng Jin, Liang Dou
Categories: cs.SE cs.AI cs.LG
Comments: Published in the 35th International Conference on Software
  Engineering and Knowledge Engineering (SEKE 2023) as a regular paper
DOI: 10.18293/SEKE2023-136
\\ ( https://arxiv.org/abs/2305.04228 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08029
replaced with revised version Mon, 5 Feb 2024 08:35:33 GMT   (14351kb,D)

Title: REMAST: Real-time Emotion-based Music Arrangement with Soft Transition
Authors: Zihao Wang, Le Ma, Chen Zhang, Bo Han, Yunfei Xu, Yikai Wang, Xinyi
  Chen, HaoRong Hong, Wenbo Liu, Xinda Wu, Kejun Zhang
Categories: cs.SD cs.AI eess.AS
ACM-class: H.5.5; F.2.2
\\ ( https://arxiv.org/abs/2305.08029 ,  14351kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06651
replaced with revised version Sun, 4 Feb 2024 00:24:24 GMT   (16284kb,D)

Title: Predicting Software Performance with Divide-and-Learn
Authors: Jingzhi Gong, Tao Chen
Categories: cs.SE cs.AI cs.PF
Comments: This paper has been accepted by The ACM Joint European Software
  Engineering Conference and Symposium on the Foundations of Software
  Engineering (ESEC/FSE) 2023
\\ ( https://arxiv.org/abs/2306.06651 ,  16284kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10224 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 16:33:10 GMT   (7872kb,D)

Title: Bloated Disclosures: Can ChatGPT Help Investors Process Information?
Authors: Alex Kim, Maximilian Muhn, Valeri Nikolaev
Categories: econ.GN cs.AI q-fin.EC q-fin.GN
\\ ( https://arxiv.org/abs/2306.10224 ,  7872kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06924
replaced with revised version Sat, 3 Feb 2024 21:06:44 GMT   (8438kb,D)

Title: DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual
  Language Grounding
Authors: Shuijing Liu, Aamir Hasan, Kaiwen Hong, Runxuan Wang, Peixin Chang,
  Zachary Mizrachi, Justin Lin, D. Livingston McPherson, Wendy A. Rogers, and
  Katherine Driggs-Campbell
Categories: cs.RO cs.AI cs.CL cs.HC cs.LG
Comments: Published in IEEE Robotics and Automation Letters (RA-L)
DOI: 10.1109/LRA.2024.3362591
\\ ( https://arxiv.org/abs/2307.06924 ,  8438kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12714
replaced with revised version Sun, 4 Feb 2024 06:46:03 GMT   (12387kb,D)

Title: VIGC: Visual Instruction Generation and Correction
Authors: Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang,
  Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, Conghui He
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2024, Project Website:
  https://opendatalab.github.io/VIGC, Code and Pretrained Model:
  https://github.com/opendatalab/VIGC
\\ ( https://arxiv.org/abs/2308.12714 ,  12387kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14731
replaced with revised version Mon, 5 Feb 2024 18:47:42 GMT   (947kb,D)

Title: Distilled GPT for Source Code Summarization
Authors: Chia-Yi Su and Collin McMillan
Categories: cs.SE cs.AI
Comments: 19 pages + 6 figures. Accepted to Automated Software Engineering
  Journal
\\ ( https://arxiv.org/abs/2308.14731 ,  947kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01365
replaced with revised version Sun, 4 Feb 2024 07:17:28 GMT   (5506kb,D)

Title: Refined Temporal Pyramidal Compression-and-Amplification Transformer for
  3D Human Pose Estimation
Authors: Hanbing Liu, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng
  Geng and Xuansong Xie
Categories: cs.CV cs.AI
Comments: 11 pages, 5 figures
\\ ( https://arxiv.org/abs/2309.01365 ,  5506kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10426
replaced with revised version Sun, 4 Feb 2024 13:32:28 GMT   (3516kb,D)

Title: Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning
  through Compound Object Affordances
Authors: Tuba Girgin, Emre Ugur
Categories: cs.RO cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Submitted to Robotics and Automation Letters on
  February 2, 2024
\\ ( https://arxiv.org/abs/2309.10426 ,  3516kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13672
replaced with revised version Sat, 3 Feb 2024 01:40:46 GMT   (39113kb,D)

Title: Deep Reinforcement Learning for Image-to-Image Translation
Authors: Xin Wang, Ziwei Luo, Jing Hu, Chengming Feng, Shu Hu, Bin Zhu, Xi Wu,
  Xin Li, Siwei Lyu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2309.13672 ,  39113kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01378
replaced with revised version Mon, 5 Feb 2024 03:46:00 GMT   (16090kb,D)

Title: Vision-Language Foundation Models as Effective Robot Imitators
Authors: Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu,
  Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, Tao Kong
Categories: cs.RO cs.AI cs.LG
Comments: Fix typos. Project page: https://roboflamingo.github.io
\\ ( https://arxiv.org/abs/2311.01378 ,  16090kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12832
replaced with revised version Sat, 3 Feb 2024 22:22:02 GMT   (47153kb,D)

Title: Toward effective protection against diffusion based mimicry through
  score distillation
Authors: Haotian Xue, Chumeng Liang, Xiaoyu Wu, Yongxin Chen
Categories: cs.CV cs.AI
Comments: ICLR 2024 Conference Paper, the code is available in
  https://github.com/xavihart/Diff-Protect
\\ ( https://arxiv.org/abs/2311.12832 ,  47153kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13544 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 16:36:51 GMT   (3254kb,D)

Title: Piecewise Polynomial Regression of Tame Functions via Integer
  Programming
Authors: Gilles Bareilles, Johannes Aspman, Jiri Nemecek, Jakub Marecek
Categories: math.OC cs.AI cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2311.13544 ,  3254kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00050
replaced with revised version Sun, 4 Feb 2024 23:27:23 GMT   (3599kb,D)

Title: Elijah: Eliminating Backdoors Injected in Diffusion Models via
  Distribution Shift
Authors: Shengwei An, Sheng-Yen Chou, Kaiyuan Zhang, Qiuling Xu, Guanhong Tao,
  Guangyu Shen, Siyuan Cheng, Shiqing Ma, Pin-Yu Chen, Tsung-Yi Ho, Xiangyu
  Zhang
Categories: cs.CR cs.AI cs.LG
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2312.00050 ,  3599kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04792
replaced with revised version Mon, 5 Feb 2024 02:33:02 GMT   (142kb,D)

Title: Playing Large Games with Oracles and AI Debate
Authors: Xinyi Chen, Angelica Chen, Dean Foster, Elad Hazan
Categories: cs.GT cs.AI
\\ ( https://arxiv.org/abs/2312.04792 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05631
replaced with revised version Sat, 3 Feb 2024 01:23:01 GMT   (10951kb,D)

Title: DrawTalking: Building Interactive Worlds by Sketching and Speaking
Authors: Karl Toby Rosenberg, Rubaiat Habib Kazi, Li-Yi Wei, Haijun Xia, Ken
  Perlin
Categories: cs.HC cs.AI cs.CL cs.GR
ACM-class: H.5.2; D.2.2; I.2.7; D.1.7; H.5.1
\\ ( https://arxiv.org/abs/2401.05631 ,  10951kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07353
replaced with revised version Sat, 3 Feb 2024 14:55:07 GMT   (6896kb,D)

Title: Towards Engineering Fair and Equitable Software Systems for Managing
  Low-Altitude Airspace Authorizations
Authors: Usman Gohar, Michael C. Hunter, Agnieszka Marczak-Czajka, Robyn R.
  Lutz, Myra B. Cohen, Jane Cleland-Huang
Categories: cs.SE cs.AI cs.LG
Journal-ref: ICSE-SEIS 2024
DOI: 10.1145/3639475.3640103
\\ ( https://arxiv.org/abs/2401.07353 ,  6896kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13138
replaced with revised version Sun, 4 Feb 2024 10:33:36 GMT   (715kb,D)

Title: Visibility into AI Agents
Authors: Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond,
  Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt,
  Lennart Heim, Markus Anderljung
Categories: cs.CY cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2401.13138 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14831
replaced with revised version Mon, 5 Feb 2024 11:28:56 GMT   (1881kb,D)

Title: The Machine Vision Iceberg Explained: Advancing Dynamic Testing by
  Considering Holistic Environmental Circumstances
Authors: Hubert Padusinski, Thilo Braun, Christian Steinhauser, Lennart Ries,
  Eric Sax
Categories: cs.RO cs.AI cs.CV cs.SE eess.IV
Comments: Submitted at IEEE IV 2024
\\ ( https://arxiv.org/abs/2401.14831 ,  1881kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15896
replaced with revised version Sun, 4 Feb 2024 04:30:07 GMT   (8795kb,D)

Title: M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale
  Efficient Pretraining
Authors: Qingpei Guo, Furong Xu, Hanxiao Zhang, Wang Ren, Ziping Ma, Lin Ju,
  Jian Wang, Jingdong Chen, Ming Yang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.15896 ,  8795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00024 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 18:24:51 GMT   (974kb,D)

Title: Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule
  Embedding
Authors: Shaghayegh Sadeghi, Alan Bui, Ali Forooghi, Jianguo Lu, Alioune Ngom
Categories: q-bio.BM cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.00024 ,  974kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00672
replaced with revised version Sun, 4 Feb 2024 15:39:34 GMT   (13232kb,D)

Title: Exploring Homogeneous and Heterogeneous Consistent Label Associations
  for Unsupervised Visible-Infrared Person ReID
Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.00672 ,  13232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00876
replaced with revised version Mon, 5 Feb 2024 15:02:17 GMT   (6340kb,D)

Title: Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud
Authors: Siavash Alamouti, Fay Arjomandi, Michel Burger and Dr. Bashar
  Altakrouri
Categories: cs.NI cs.AI
\\ ( https://arxiv.org/abs/2402.00876 ,  6340kb)
------------------------------------------------------------------------------
\\
arXiv:2211.11419
replaced with revised version Sun, 4 Feb 2024 08:03:23 GMT   (13545kb,D)

Title: SSCFormer: Push the Limit of Chunk-wise Conformer for Streaming ASR
  Using Sequentially Sampled Chunks and Chunked Causal Convolution
Authors: Fangyuan Wang, Bo Xu, Bo Xu
Categories: cs.SD cs.CL eess.AS
Comments: This manuscript has been accepted by SPL
\\ ( https://arxiv.org/abs/2211.11419 ,  13545kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12793 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 03:24:46 GMT   (208kb,D)

Title: Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal
  Selective Self-Training
Authors: Jianfeng He, Julian Salazar, Kaisheng Yao, Haoqi Li, Jinglun Cai
Categories: eess.AS cs.CL cs.MM cs.SD
Comments: 18 pages, 7 figures
\\ ( https://arxiv.org/abs/2305.12793 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17107
replaced with revised version Fri, 2 Feb 2024 19:44:14 GMT   (20644kb,D)

Title: LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image
  Understanding
Authors: Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi
  Yang, Tong Sun
Categories: cs.CV cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2306.17107 ,  20644kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05591
replaced with revised version Mon, 5 Feb 2024 09:10:15 GMT   (11476kb,D)

Title: Linear Alignment of Vision-language Models for Image Captioning
Authors: Fabian Paischer, Markus Hofmarcher, Sepp Hochreiter, Thomas Adler
Categories: cs.CV cs.CL cs.LG
Comments: 8 pages (+ references and appendix)
\\ ( https://arxiv.org/abs/2307.05591 ,  11476kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12420
replaced with revised version Mon, 5 Feb 2024 16:06:14 GMT   (3131kb,D)

Title: Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature
Authors: Walter Hernandez, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil
  Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, and Jiahua Xu
Categories: cs.IR cs.CL cs.LG
\\ ( https://arxiv.org/abs/2308.12420 ,  3131kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15154
replaced with revised version Mon, 5 Feb 2024 10:11:37 GMT   (1732kb)

Title: The Anatomy of Conspirators: Unveiling Traits using a Comprehensive
  Twitter Dataset
Authors: Margherita Gambini, Serena Tardelli, Maurizio Tesconi
Categories: cs.SI cs.CL
Journal-ref: Computer Communications, 217 (2024), 25-40
DOI: 10.1016/j.comcom.2024.01.027
\\ ( https://arxiv.org/abs/2308.15154 ,  1732kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10456
replaced with revised version Sun, 4 Feb 2024 06:05:06 GMT   (70kb,D)

Title: Improving Speaker Diarization using Semantic Information: Joint Pairwise
  Constraints Propagation
Authors: Luyao Cheng, Siqi Zheng, Qinglin Zhang, Hui Wang, Yafeng Chen, Qian
  Chen, Shiliang Zhang
Categories: cs.SD cs.CL eess.AS
\\ ( https://arxiv.org/abs/2309.10456 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06825
replaced with revised version Sun, 4 Feb 2024 02:59:19 GMT   (518kb)

Title: Utilization of Non-verbal Behaviour and Social Gaze in Classroom
  Human-Robot Interaction Communications
Authors: Sahand Shaghaghi, Pourya Aliasghari, Bryan Tripp, Kerstin Dautenhahn,
  Chrystopher Nehaniv
Categories: cs.RO cs.CL
Comments: In WTF Workshop Proceedings (arXiv:2401.04108) held in conjunction
  with the ACM conference on Conversational User Interfaces (CUI), 19 - 21/07
  2023, in Eindhoven, The Netherlands
Report-no: WTFCUI/2023/06
\\ ( https://arxiv.org/abs/2312.06825 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10244
replaced with revised version Sat, 3 Feb 2024 17:14:46 GMT   (492kb)

Title: Knowledge Graph Driven Recommendation System Algorithm
Authors: Chaoyang Zhang, Yanan Li, Shen Chen, Siwei Fan, Wei Li
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2401.10244 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01391
replaced with revised version Mon, 5 Feb 2024 13:28:23 GMT   (324kb,D)

Title: StepCoder: Improve Code Generation with Reinforcement Learning from
  Compiler Feedback
Authors: Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen,
  Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou,
  Tao Ji, Rui Zheng, Qi Zhang, Xuanjing Huang, Tao Gui
Categories: cs.SE cs.CL
Comments: 13 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.01391 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:1910.06002 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 05:07:45 GMT   (591kb,D)

Title: Optimal Clustering from Noisy Binary Feedback
Authors: Kaito Ariu, Jungseul Ok, Alexandre Proutiere, Se-Young Yun
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/1910.06002 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2104.04310
replaced with revised version Mon, 5 Feb 2024 09:24:41 GMT   (13421kb,D)

Title: Context-self contrastive pretraining for crop type semantic segmentation
Authors: Michail Tarasiou, Riza Alp Guler, Stefanos Zafeiriou
Categories: cs.CV cs.LG
Comments: 15 pages, 17 figures
DOI: 10.1109/TGRS.2022.3198187
\\ ( https://arxiv.org/abs/2104.04310 ,  13421kb)
------------------------------------------------------------------------------
\\
arXiv:2105.09254 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 18:49:47 GMT   (79kb,D)

Title: Multiply Robust Causal Mediation Analysis with Continuous Treatments
Authors: Numair Sani, Yizhen Xu, AmirEmad Ghassami, Ilya Shpitser
Categories: math.ST cs.LG econ.EM stat.ML stat.TH
\\ ( https://arxiv.org/abs/2105.09254 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2206.05248 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 02:10:58 GMT   (252kb)

Title: Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max
  Optimization and Comonotone Inclusion
Authors: Yang Cai, Argyris Oikonomou, Weiqiang Zheng
Categories: math.OC cs.DS cs.LG
Comments: This version includes new results on point convergence
\\ ( https://arxiv.org/abs/2206.05248 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:2209.03275
replaced with revised version Mon, 5 Feb 2024 17:54:04 GMT   (4653kb,D)

Title: Multimodal Speech Enhancement Using Burst Propagation
Authors: Mohsin Raza, Leandro A. Passos, Ahmed Khubaib, Ahsan Adeel
Categories: cs.SD cs.LG cs.MM eess.AS
\\ ( https://arxiv.org/abs/2209.03275 ,  4653kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10636
replaced with revised version Fri, 2 Feb 2024 19:36:15 GMT   (35248kb,D)

Title: EVEREST: Efficient Masked Video Autoencoder by Removing Redundant
  Spatiotemporal Tokens
Authors: Sunil Hwang, Jaehong Yoon, Youngwan Lee, Sung Ju Hwang
Categories: cs.CV cs.LG
Comments: 18 pages
\\ ( https://arxiv.org/abs/2211.10636 ,  35248kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06279
replaced with revised version Mon, 5 Feb 2024 11:09:00 GMT   (4217kb,D)

Title: Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural
  Networks with Neuromorphic Data
Authors: Gorka Abad, Oguzhan Ersoy, Stjepan Picek, Aitor Urbieta
Categories: cs.CR cs.CV cs.LG
Comments: To appear in Network and Distributed System Security (NDSS) Symposium
  2024
DOI: 10.14722/ndss.2024.24334
\\ ( https://arxiv.org/abs/2302.06279 ,  4217kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09167
replaced with revised version Mon, 5 Feb 2024 18:35:36 GMT   (9215kb,D)

Title: Mixed Traffic Control and Coordination from Pixels
Authors: Michael Villarreal, Bibek Poudel, Jia Pan, Weizi Li
Categories: cs.MA cs.LG cs.RO
Comments: Accepted to IEEE International Conference on Robotics and Automation
  (ICRA), 2024
\\ ( https://arxiv.org/abs/2302.09167 ,  9215kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11831
replaced with revised version Mon, 5 Feb 2024 12:25:43 GMT   (9515kb,D)

Title: CLADE: Cycle Loss Augmented Degradation Enhancement for Unpaired
  Super-Resolution of Anisotropic Medical Images
Authors: Michele Pascale, Vivek Muthurangu, Javier Montalt Tordera, Heather E
  Fitzke, Gauraang Bhatnagar, Stuart Taylor, Jennifer Steeden
Categories: cs.CV cs.LG eess.IV physics.med-ph
\\ ( https://arxiv.org/abs/2303.11831 ,  9515kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05622 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 23:37:26 GMT   (1924kb,D)

Title: SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM
Authors: Yihao Liu, Jiaming Zhang, Zhangcong She, Amir Kheradmand and Mehran
  Armand
Categories: eess.IV cs.CV cs.LG
Comments: 5 pages, 4 figures. We added editorial changes in the text
\\ ( https://arxiv.org/abs/2304.05622 ,  1924kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11857 (*cross-listing*)
replaced with revised version Sun, 4 Feb 2024 20:51:43 GMT   (8176kb,D)

Title: Computing high-dimensional optimal transport by flow neural networks
Authors: Chen Xu, Xiuyuan Cheng, Yao Xie
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2305.11857 ,  8176kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16368 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 16:20:08 GMT   (666kb,D)

Title: Neural incomplete factorization: learning preconditioners for the
  conjugate gradient method
Authors: Paul H\"ausner, Ozan \"Oktem, Jens Sj\"olund
Categories: math.OC cs.LG cs.NA math.NA stat.ML
Comments: Under review. 18 pages, 8 figures
\\ ( https://arxiv.org/abs/2305.16368 ,  666kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16905 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 16:22:57 GMT   (4262kb,D)

Title: Improving Neural Additive Models with Bayesian Principles
Authors: Kouroche Bouchiat, Alexander Immer, Hugo Y\`eche, Gunnar R\"atsch,
  Vincent Fortuin
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.16905 ,  4262kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07392
replaced with revised version Sun, 4 Feb 2024 22:23:36 GMT   (40894kb,D)

Title: Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural
  Surface Rendering
Authors: Snehal Jauhri, Ishikaa Lunawat, Georgia Chalvatzaki
Categories: cs.RO cs.CV cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2306.07392 ,  40894kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08489 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 11:06:07 GMT   (66kb)

Title: Analysis and Approximate Inference of Large Random Kronecker Graphs
Authors: Zhenyu Liao, Yuanqian Xia, Chengmei Niu, Yong Xiao
Categories: stat.ML cs.LG math.SP
Comments: 27 pages, 5 figures, 2 tables
\\ ( https://arxiv.org/abs/2306.08489 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10374 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 22:54:23 GMT   (124kb)

Title: A Survey of Contextual Optimization Methods for Decision Making under
  Uncertainty
Authors: Utsav Sadana, Abhilash Chenreddy, Erick Delage, Alexandre Forel, Emma
  Frejinger, Thibaut Vidal
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2306.10374 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14233 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 14:25:03 GMT   (6485kb,D)

Title: Attention-Refined Unrolling for Sparse Sequential micro-Doppler
  Reconstruction
Authors: Riccardo Mazzieri, Jacopo Pegoraro and Michele Rossi
Categories: eess.SP cs.LG
Comments: 16 pages, 10 figures, 6 tables
\\ ( https://arxiv.org/abs/2306.14233 ,  6485kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00494 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 15:00:21 GMT   (3544kb,D)

Title: Improving Protein Optimization with Smoothed Fitness Landscapes
Authors: Andrew Kirjner, Jason Yim, Raman Samusevich, Shahar Bracha, Tommi
  Jaakkola, Regina Barzilay, Ila Fiete
Categories: q-bio.BM cs.LG q-bio.QM stat.ML
Comments: ICLR 2024. Code: https://github.com/kirjner/GGS
\\ ( https://arxiv.org/abs/2307.00494 ,  3544kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01171 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 13:15:39 GMT   (2278kb,AD)

Title: Quantum Neural Estimation of Entropies
Authors: Ziv Goldfeld, Dhrumil Patel, Sreejith Sreekumar, and Mark M. Wilde
Categories: quant-ph cond-mat.stat-mech cs.IT cs.LG math.IT
Comments: 14 pages, 2 figures; see also independent works of Shin, Lee, and
  Jeong at arXiv:2306.14566v1 and Lee, Kwon, and Lee at arXiv:2307.13511v2
\\ ( https://arxiv.org/abs/2307.01171 ,  2278kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03108
replaced with revised version Sun, 4 Feb 2024 04:13:47 GMT   (8945kb,D)

Title: DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion
  Models
Authors: Zhenting Wang, Chen Chen, Lingjuan Lyu, Dimitris N. Metaxas, Shiqing
  Ma
Categories: cs.CV cs.CR cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2307.03108 ,  8945kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03927 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 15:04:23 GMT   (3229kb,D)

Title: Fast Empirical Scenarios
Authors: Michael Multerer, Paul Schneider, Rohan Sen
Categories: stat.ML cs.LG cs.NA math.NA q-fin.RM
Comments: 22 pages, 7 figures
MSC-class: 11C20, 41A55, 46E22, 46N30, 60-08, 68W25
\\ ( https://arxiv.org/abs/2307.03927 ,  3229kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06324 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 00:28:58 GMT   (341kb,D)

Title: Provably Faster Gradient Descent via Long Steps
Authors: Benjamin Grimmer
Categories: math.OC cs.LG cs.NA math.NA
Comments: 20 pages
\\ ( https://arxiv.org/abs/2307.06324 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07604
replaced with revised version Sun, 4 Feb 2024 04:36:42 GMT   (61kb)

Title: Smooth Lower Bounds for Differentially Private Algorithms via
  Padding-and-Permuting Fingerprinting Codes
Authors: Naty Peter, Eliad Tsfadia, Jonathan Ullman
Categories: cs.CR cs.DS cs.LG
\\ ( https://arxiv.org/abs/2307.07604 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07941
replaced with revised version Mon, 5 Feb 2024 01:13:48 GMT   (32kb)

Title: Optimal Compression of Unit Norm Vectors in the High Distortion Regime
Authors: Heng Zhu, Avishek Ghosh, Arya Mazumdar
Categories: cs.IT cs.DC cs.LG math.IT
Comments: Appeared in ISIT 2023; Correct the proof of Theorem 1
\\ ( https://arxiv.org/abs/2307.07941 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13147 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 15:47:06 GMT   (172kb,D)

Title: Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent
  Observation Framework
Authors: William Andersson, Jakob Heiss, Florian Krach, Josef Teichmann
Categories: stat.ML cs.LG cs.NA math.NA math.PR
Journal-ref: Transactions on Machine Learning Research (TMLR) 2024
\\ ( https://arxiv.org/abs/2307.13147 ,  172kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15299
replaced with revised version Mon, 5 Feb 2024 03:20:06 GMT   (1170kb,D)

Title: Differential Evolution Algorithm based Hyper-Parameters Selection of
  Transformer Neural Network Model for Load Forecasting
Authors: Anuvab Sen, Arul Rhik Mazumder, Udayon Sen
Categories: cs.NE cs.LG
Comments: 6 Pages, 6 Figures, 2 Tables, Accepted by the 14th IEEE International
  Symposium Series on Computational Intelligence (SSCI 2023), December 5-8,
  2023, Mexico City, Mexico
Journal-ref: 14th IEEE International Symposium Series on Computational
  Intelligence (SSCI 2023)
\\ ( https://arxiv.org/abs/2307.15299 ,  1170kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02068
replaced with revised version Sat, 3 Feb 2024 01:14:57 GMT   (4592kb,D)

Title: Specious Sites: Tracking the Spread and Sway of Spurious News Stories at
  Scale
Authors: Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric
Categories: cs.SI cs.CY cs.LG
Comments: Accepted to IEEE S&P 2024. Updated Emails
\\ ( https://arxiv.org/abs/2308.02068 ,  4592kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14920 (*cross-listing*)
replaced with revised version Sun, 4 Feb 2024 17:12:47 GMT   (2545kb,D)

Title: Matbench Discovery -- A framework to evaluate machine learning crystal
  stability predictions
Authors: Janosh Riebesell, Rhys E. A. Goodall, Philipp Benner, Yuan Chiang,
  Bowen Deng, Alpha A. Lee, Anubhav Jain, Kristin A. Persson
Categories: cond-mat.mtrl-sci cs.LG
Comments: 31 pages, 18 figures, 4 tables
\\ ( https://arxiv.org/abs/2308.14920 ,  2545kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16681 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 22:59:17 GMT   (221kb,D)

Title: One Model Many Scores: Using Multiverse Analysis to Prevent Fairness
  Hacking and Evaluate the Influence of Model Design Decisions
Authors: Jan Simson and Florian Pfisterer and Christoph Kern
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2308.16681 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05649
replaced with revised version Fri, 2 Feb 2024 19:49:29 GMT   (21kb)

Title: Data efficiency, dimensionality reduction, and the generalized symmetric
  information bottleneck
Authors: K. Michael Martini and Ilya Nemenman
Categories: cs.IT cond-mat.stat-mech cs.LG math.IT physics.data-an
\\ ( https://arxiv.org/abs/2309.05649 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06642 (*cross-listing*)
replaced with revised version Sun, 4 Feb 2024 05:04:10 GMT   (46920kb,D)

Title: Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion
  Models
Authors: Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi
Categories: eess.IV cs.LG
Comments: 30 pages, 21 figures, preliminary version
ACM-class: I.2.6; I.4.5
\\ ( https://arxiv.org/abs/2309.06642 ,  46920kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16965 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 15:42:34 GMT   (2936kb,D)

Title: Controlling Continuous Relaxation for Combinatorial Optimization
Authors: Yuma Ichikawa
Categories: stat.ML cs.LG stat.CO stat.ME
Comments: 15 pages, 8 figures
\\ ( https://arxiv.org/abs/2309.16965 ,  2936kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07891 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 21:18:10 GMT   (1241kb,D)

Title: A Theory of Non-Linear Feature Learning with One Gradient Step in
  Two-Layer Neural Networks
Authors: Behrad Moniri, Donghwan Lee, Hamed Hassani, Edgar Dobriban
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.07891 ,  1241kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13681
replaced with revised version Sun, 4 Feb 2024 19:53:13 GMT   (5902kb,D)

Title: RealFM: A Realistic Mechanism to Incentivize Federated Participation and
  Contribution
Authors: Marco Bornstein, Amrit Singh Bedi, Anit Kumar Sahu, Furqan Khan, and
  Furong Huang
Categories: cs.GT cs.CY cs.DC cs.LG econ.TH
Comments: 20 pages, 11 figures
\\ ( https://arxiv.org/abs/2310.13681 ,  5902kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15330 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 05:39:28 GMT   (523kb,D)

Title: Towards the Theory of Unsupervised Federated Learning: Non-asymptotic
  Analysis of Federated EM Algorithms
Authors: Ye Tian, Haolei Weng, Yang Feng
Categories: stat.ML cs.LG
Comments: 50 pages, 3 figures
\\ ( https://arxiv.org/abs/2310.15330 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17496 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 22:44:15 GMT   (1575kb,D)

Title: Tackling Interference Induced by Data Training Loops in A/B Tests: A
  Weighted Training Approach
Authors: Nian Si
Categories: stat.ME cs.LG econ.EM
\\ ( https://arxiv.org/abs/2310.17496 ,  1575kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02467 (*cross-listing*)
replaced with revised version Sun, 4 Feb 2024 18:47:55 GMT   (520kb,D)

Title: Individualized Policy Evaluation and Learning under Clustered Network
  Interference
Authors: Yi Zhang, Kosuke Imai
Categories: stat.ME cs.LG econ.EM
\\ ( https://arxiv.org/abs/2311.02467 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14220 (*cross-listing*)
replaced with revised version Sun, 4 Feb 2024 23:26:08 GMT   (1005kb,D)

Title: Assumption-lean and Data-adaptive Post-Prediction Inference
Authors: Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, and Qiongshi Lu
Categories: stat.ME cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.14220 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16148
replaced with revised version Fri, 2 Feb 2024 19:04:02 GMT   (5392kb,D)

Title: Univariate Radial Basis Function Layers: Brain-inspired Deep Neural
  Layers for Low-Dimensional Inputs
Authors: Daniel Jost, Basavasagar Patil, Xavier Alameda-Pineda, Chris Reinke
Categories: cs.NE cs.LG
\\ ( https://arxiv.org/abs/2311.16148 ,  5392kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01187
replaced with revised version Sat, 3 Feb 2024 23:10:16 GMT   (1706kb,D)

Title: SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer
Authors: Renan A. Rojas-Gomez, Karan Singhal, Ali Etemad, Alex Bijamov, Warren
  R. Morningstar, Philip Andrew Mansfield
Categories: cs.CV cs.LG stat.ML
\\ ( https://arxiv.org/abs/2312.01187 ,  1706kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04704
replaced with revised version Fri, 2 Feb 2024 19:41:29 GMT   (2747kb,D)

Title: Efficient Parallel Reinforcement Learning Framework using the Reactor
  Model
Authors: Jacky Kwok, Marten Lohstroh, Edward A. Lee
Categories: cs.DC cs.LG
Comments: 10 pages, 11 figures
\\ ( https://arxiv.org/abs/2312.04704 ,  2747kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05332
replaced with revised version Sat, 3 Feb 2024 11:32:26 GMT   (186kb,D)

Title: Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming
  Controllers Inspired by Model Predictive Control
Authors: Yiwen Lu, Zishuo Li, Yihan Zhou, Na Li, Yilin Mo
Categories: eess.SY cs.LG cs.RO cs.SY math.OC
\\ ( https://arxiv.org/abs/2312.05332 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15574 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 17:00:41 GMT   (134kb,D)

Title: Faster Rates for Switchback Experiments
Authors: Su Jia, Nathan Kallus, Christina Lee Yu
Categories: math.ST cs.LG stat.TH
\\ ( https://arxiv.org/abs/2312.15574 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15740
replaced with revised version Sun, 4 Feb 2024 12:32:35 GMT   (5943kb,D)

Title: BiSwift: Bandwidth Orchestrator for Multi-Stream Video Analytics on Edge
Authors: Lin Sun, Weijun Wang, Tingting Yuan, Liang Mi, Haipeng Dai, Yunxin
  Liu, Xiaoming Fu
Categories: cs.NI cs.CV cs.LG
Comments: Accepted by 2024 IEEE INFOCOM
\\ ( https://arxiv.org/abs/2312.15740 ,  5943kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17612
replaced with revised version Mon, 5 Feb 2024 11:14:22 GMT   (320kb,D)

Title: Bespoke Approximation of Multiplication-Accumulation and Activation
  Targeting Printed Multilayer Perceptrons
Authors: Florentia Afentaki, Gurol Saglam, Argyris Kokkinis, Kostas Siozios,
  Georgios Zervakis, Mehdi B Tahoori
Categories: cs.AR cs.LG
Comments: Accepted for publication at the 42th IEEE/ACM International
  Conference on Computer Aided Design (ICCAD) 2023, San Francisco, USA
DOI: 10.1109/ICCAD57390.2023.10323613
\\ ( https://arxiv.org/abs/2312.17612 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17683
replaced with revised version Sun, 4 Feb 2024 04:01:04 GMT   (453kb)

Title: Malware Detection in IOT Systems Using Machine Learning Techniques
Authors: Ali Mehrban, Pegah Ahadian
Categories: cs.CR cs.LG cs.NI
MSC-class: airccse.org/
Journal-ref: International Journal of Wireless & Mobile Networks (IJWMN),
  Vol.15, No.6, December 2023
\\ ( https://arxiv.org/abs/2312.17683 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05441 (*cross-listing*)
replaced with revised version Sun, 4 Feb 2024 04:18:44 GMT   (331kb)

Title: An adaptive network-based approach for advanced forecasting of
  cryptocurrency values
Authors: Ali Mehrban, Pegah Ahadian
Categories: q-fin.ST cs.CE cs.CR cs.LG
Comments: 11 pages
Journal-ref: International Journal of Computer Science and Information
  Technology (IJCSIT), 2023
DOI: 10.5121/ijcsit.2023.15601
\\ ( https://arxiv.org/abs/2401.05441 ,  331kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08035
replaced with revised version Sun, 4 Feb 2024 17:39:07 GMT   (424kb,D)

Title: BanglaNet: Bangla Handwritten Character Recognition using Ensembling of
  Convolutional Neural Network
Authors: Chandrika Saha, Md Mostafijur Rahman
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.08035 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08224 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 08:34:57 GMT   (772kb,D)

Title: Privacy Preserving Adaptive Experiment Design
Authors: Jiachun Li, Kaining Shi and David Simchi-Levi
Categories: stat.ME cs.CR cs.LG
Comments: Add a table
\\ ( https://arxiv.org/abs/2401.08224 ,  772kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08702 (*cross-listing*)
replaced with revised version Fri, 2 Feb 2024 23:14:09 GMT   (19239kb,D)

Title: Do We Really Even Need Data?
Authors: Kentaro Hoffman, Stephen Salerno, Awan Afiaz, Jeffrey T. Leek, Tyler
  H. McCormick
Categories: stat.ME cs.LG
\\ ( https://arxiv.org/abs/2401.08702 ,  19239kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13421 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 07:43:20 GMT   (19kb,D)

Title: Federated learning with distributed fixed design quantum chips and
  quantum channels
Authors: Ammar Daskin
Categories: quant-ph cs.DC cs.LG
Comments: a few typos are corrected
\\ ( https://arxiv.org/abs/2401.13421 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15022 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 15:36:44 GMT   (718kb)

Title: Applications of artificial intelligence in the analysis of
  histopathology images of gliomas: a review
Authors: Jan-Philipp Redlich, Friedrich Feuerhake, Joachim Weis, Nadine S.
  Schaadt, Sarah Teuber-Hanselmann, Christoph Buck, Sabine Luttmann, Andrea
  Eberle, Stefan Nikolin, Arno Appenzeller, Andreas Portmann, Andr\'e Homeyer
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.15022 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15285
replaced with revised version Sun, 4 Feb 2024 03:52:09 GMT   (652kb)

Title: Ransomware threat mitigation through network traffic analysis and
  machine learning techniques
Authors: Ali Mehrban, Shirin Karimi Geransayeh
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2401.15285 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18006 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 23:32:08 GMT   (1941kb,D)

Title: EEG-GPT: Exploring Capabilities of Large Language Models for EEG
  Classification and Interpretation
Authors: Jonathan W. Kim and Ahmed Alaa and Danilo Bernardo
Categories: q-bio.QM cs.LG eess.SP
\\ ( https://arxiv.org/abs/2401.18006 ,  1941kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00515 (*cross-listing*)
replaced with revised version Sat, 3 Feb 2024 15:11:23 GMT   (671kb,D)

Title: Developing A Multi-Agent and Self-Adaptive Framework with Deep
  Reinforcement Learning for Dynamic Portfolio Risk Management
Authors: Zhenglong Li, Vincent Tam, Kwan L. Yeung
Categories: q-fin.PM cs.LG
Comments: Accepted by The 23rd International Conference on Autonomous Agents
  and Multi-Agent Systems
\\ ( https://arxiv.org/abs/2402.00515 ,  671kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
