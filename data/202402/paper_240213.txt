Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月13日 18:04
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri  9 Feb 24 19:00:00 GMT  to  Mon 12 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.06634
Date: Fri, 19 Jan 2024 07:16:21 GMT   (2697kb,D)

Title: SocraSynth: Multi-LLM Reasoning with Conditional Statistics
Authors: Edward Y. Chang
Categories: cs.AI cs.CL cs.LG
Comments: 1 figure, 6 tables, 6 appendices
ACM-class: I.2.7
\\
  Large language models (LLMs), while promising, face criticisms for biases,
hallucinations, and a lack of reasoning capability. This paper introduces
SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these
issues. SocraSynth utilizes conditional statistics and systematic context
enhancement through continuous arguments, alongside adjustable debate
contentiousness levels. The platform typically involves a human moderator and
two LLM agents representing opposing viewpoints on a given subject. SocraSynth
operates in two main phases: knowledge generation and reasoning evaluation. In
the knowledge generation phase, the moderator defines the debate topic and
contentiousness level, prompting the agents to formulate supporting arguments
for their respective stances. The reasoning evaluation phase then employs
Socratic reasoning and formal logic principles to appraise the quality of the
arguments presented. The dialogue concludes with the moderator adjusting the
contentiousness from confrontational to collaborative, gathering final,
conciliatory remarks to aid in human reasoning and decision-making. Through
case studies in three distinct application domains, this paper showcases
SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning,
comprehensive assessment, and enhanced collaboration. This underscores the
value of multi-agent interactions in leveraging LLMs for advanced knowledge
extraction and decision-making support.
\\ ( https://arxiv.org/abs/2402.06634 ,  2697kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06640
Date: Thu, 25 Jan 2024 22:39:39 GMT   (840kb)

Title: Modeling and Optimization of Epidemiological Control Policies Through
  Reinforcement Learning
Authors: Ishir Rao
Categories: cs.AI q-bio.PE
Comments: 22 pages, 8 figures
Journal-ref: J. Emerging Investigators Article (2023) Vol. 6
DOI: 10.59720/22-157
\\
  Pandemics involve the high transmission of a disease that impacts global and
local health and economic patterns. The impact of a pandemic can be minimized
by enforcing certain restrictions on a community. However, while minimizing
infection and death rates, these restrictions can also lead to economic crises.
Epidemiological models help propose pandemic control strategies based on
non-pharmaceutical interventions such as social distancing, curfews, and
lockdowns, reducing the economic impact of these restrictions. However,
designing manual control strategies while considering disease spread and
economic status is non-trivial. Optimal strategies can be designed through
multi-objective reinforcement learning (MORL) models, which demonstrate how
restrictions can be used to optimize the outcome of a pandemic. In this
research, we utilized an epidemiological Susceptible, Exposed, Infected,
Recovered, Deceased (SEIRD) model: a compartmental model for virtually
simulating a pandemic day by day. We combined the SEIRD model with a deep
double recurrent Q-network to train a reinforcement learning agent to enforce
the optimal restriction on the SEIRD simulation based on a reward function. We
tested two agents with unique reward functions and pandemic goals to obtain two
strategies. The first agent placed long lockdowns to reduce the initial spread
of the disease, followed by cyclical and shorter lockdowns to mitigate the
resurgence of the disease. The second agent provided similar infection rates
but an improved economy by implementing a 10-day lockdown and 20-day
no-restriction cycle. This use of reinforcement learning and epidemiological
modeling allowed for both economic and infection mitigation in multiple
pandemic scenarios.
\\ ( https://arxiv.org/abs/2402.06640 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06647
Date: Fri, 2 Feb 2024 12:21:04 GMT   (1737kb,D)

Title: A Survey on Large Language Model Hallucination via a Creativity
  Perspective
Authors: Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu, Yuanzhuo Wang,
  Jian Guo
Categories: cs.AI cs.HC
Comments: 9 pages, 4 figures
\\
  Hallucinations in large language models (LLMs) are always seen as
limitations. However, could they also be a source of creativity? This survey
explores this possibility, suggesting that hallucinations may contribute to LLM
application by fostering creativity. This survey begins with a review of the
taxonomy of hallucinations and their negative impact on LLM reliability in
critical applications. Then, through historical examples and recent relevant
theories, the survey explores the potential creative benefits of hallucinations
in LLMs. To elucidate the value and evaluation criteria of this connection, we
delve into the definitions and assessment methods of creativity. Following the
framework of divergent and convergent thinking phases, the survey
systematically reviews the literature on transforming and harnessing
hallucinations for creativity in LLMs. Finally, the survey discusses future
research directions, emphasizing the need to further explore and refine the
application of hallucinations in creative processes within LLMs.
\\ ( https://arxiv.org/abs/2402.06647 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06654
Date: Sun, 4 Feb 2024 15:10:11 GMT   (3535kb)

Title: Conversational Crowdsensing: A Parallel Intelligence Powered Novel
  Sensing Approach
Authors: Zhengqiu Zhu, Yong Zhao, Bin Chen, Sihang Qiu, Kai Xu, Quanjun Yin,
  Jincai Huang, Zhong Liu, Fei-Yue Wang
Categories: cs.AI cs.HC
\\
  The transition from CPS-based Industry 4.0 to CPSS-based Industry 5.0 brings
new requirements and opportunities to current sensing approaches, especially in
light of recent progress in Chatbots and Large Language Models (LLMs).
Therefore, the advancement of parallel intelligence-powered Crowdsensing
Intelligence (CSI) is witnessed, which is currently advancing towards
linguistic intelligence. In this paper, we propose a novel sensing paradigm,
namely conversational crowdsensing, for Industry 5.0. It can alleviate workload
and professional requirements of individuals and promote the organization and
operation of diverse workforce, thereby facilitating faster response and wider
popularization of crowdsensing systems. Specifically, we design the
architecture of conversational crowdsensing to effectively organize three types
of participants (biological, robotic, and digital) from diverse communities.
Through three levels of effective conversation (i.e., inter-human, human-AI,
and inter-AI), complex interactions and service functionalities of different
workers can be achieved to accomplish various tasks across three sensing phases
(i.e., requesting, scheduling, and executing). Moreover, we explore the
foundational technologies for realizing conversational crowdsensing,
encompassing LLM-based multi-agent systems, scenarios engineering and
conversational human-AI cooperation. Finally, we present potential industrial
applications of conversational crowdsensing and discuss its implications. We
envision that conversations in natural language will become the primary
communication channel during crowdsensing process, enabling richer information
exchange and cooperative problem-solving among humans, robots, and AI.
\\ ( https://arxiv.org/abs/2402.06654 ,  3535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06660
Date: Mon, 5 Feb 2024 21:43:11 GMT   (409kb)

Title: The role of the metaverse in calibrating an embodied artificial general
  intelligence
Authors: Martin Schmalzried
Categories: cs.AI cs.HC
Comments: Presented at the conference second international conference on
  human-centred AI ethics: seeing the human in the artificial (HCAIE 2023):
  https://ethics-ai.eu/hcaie2023/
\\
  This paper examines the concept of embodied artificial general intelligence
(AGI), its relationship to human consciousness, and the key role of the
metaverse in facilitating this relationship. The paper leverages theoretical
frameworks such as embodied cognition, Michael Levin's computational boundary
of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo
Kastrup's analytical idealism to build the argument for achieving embodied AGI.
It contends that our perceived outer reality is a symbolic representation of
alternate inner states of being, and that AGI could embody a higher
consciousness with a larger computational boundary. The paper further discusses
the developmental stages of AGI, the requirements for the emergence of an
embodied AGI, the importance of a calibrated symbolic interface for AGI, and
the key role played by the metaverse, decentralized systems, open-source
blockchain technology, as well as open-source AI research. It also explores the
idea of a feedback loop between AGI and human users in metaverse spaces as a
tool for AGI calibration, as well as the role of local homeostasis and
decentralized governance as preconditions for achieving a stable embodied AGI.
The paper concludes by emphasizing the importance of achieving a certain degree
of harmony in human relations and recognizing the interconnectedness of
humanity at a global level, as key prerequisites for the emergence of a stable
embodied AGI.
\\ ( https://arxiv.org/abs/2402.06660 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06665
Date: Tue, 6 Feb 2024 17:15:33 GMT   (7942kb,D)

Title: The Essential Role of Causality in Foundation World Models for Embodied
  AI
Authors: Tarun Gupta, Wenbo Gong, Chao Ma, Nick Pawlowski, Agrin Hilmkil, Meyer
  Scetbon, Ade Famoti, Ashley Juan Llorens, Jianfeng Gao, Stefan Bauer, Danica
  Kragic, Bernhard Sch\"olkopf, Cheng Zhang
Categories: cs.AI cs.CL cs.LG cs.RO
\\
  Recent advances in foundation models, especially in large multi-modal models
and conversational agents, have ignited interest in the potential of generally
capable embodied agents. Such agents would require the ability to perform new
tasks in many different real-world environments. However, current foundation
models fail to accurately model physical interactions with the real world thus
not sufficient for Embodied AI. The study of causality lends itself to the
construction of veridical world models, which are crucial for accurately
predicting the outcomes of possible interactions. This paper focuses on the
prospects of building foundation world models for the upcoming generation of
embodied agents and presents a novel viewpoint on the significance of causality
within these. We posit that integrating causal considerations is vital to
facilitate meaningful physical interactions with the world. Finally, we
demystify misconceptions about causality in this context and present our
outlook for future research.
\\ ( https://arxiv.org/abs/2402.06665 ,  7942kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06673
Date: Wed, 7 Feb 2024 14:09:11 GMT   (1465kb,D)

Title: Advancing Explainable AI Toward Human-Like Intelligence: Forging the
  Path to Artificial Brain
Authors: Yongchen Zhou, Richard Jiang
Categories: cs.AI
\\
  The intersection of Artificial Intelligence (AI) and neuroscience in
Explainable AI (XAI) is pivotal for enhancing transparency and interpretability
in complex decision-making processes. This paper explores the evolution of XAI
methodologies, ranging from feature-based to human-centric approaches, and
delves into their applications in diverse domains, including healthcare and
finance. The challenges in achieving explainability in generative models,
ensuring responsible AI practices, and addressing ethical implications are
discussed. The paper further investigates the potential convergence of XAI with
cognitive sciences, the development of emotionally intelligent AI, and the
quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards
Artificial General Intelligence (AGI), considerations of consciousness, ethics,
and societal impact become paramount. The ongoing pursuit of deciphering the
mysteries of the brain with AI and the quest for HLI represent transformative
endeavors, bridging technical advancements with multidisciplinary explorations
of human cognition.
\\ ( https://arxiv.org/abs/2402.06673 ,  1465kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06695
Date: Thu, 8 Feb 2024 22:11:21 GMT   (384kb,D)

Title: Integrating LLMs for Explainable Fault Diagnosis in Complex Systems
Authors: Akshay J. Dave, Tat Nghia Nguyen, Richard B. Vilim
Categories: cs.AI cs.LG cs.SY eess.SY
Comments: 4 pages
\\
  This paper introduces an integrated system designed to enhance the
explainability of fault diagnostics in complex systems, such as nuclear power
plants, where operator understanding is critical for informed decision-making.
By combining a physics-based diagnostic tool with a Large Language Model, we
offer a novel solution that not only identifies faults but also provides clear,
understandable explanations of their causes and implications. The system's
efficacy is demonstrated through application to a molten salt facility,
showcasing its ability to elucidate the connections between diagnosed faults
and sensor data, answer operator queries, and evaluate historical sensor
anomalies. Our approach underscores the importance of merging model-based
diagnostics with advanced AI to improve the reliability and transparency of
autonomous systems.
\\ ( https://arxiv.org/abs/2402.06695 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06764
Date: Fri, 9 Feb 2024 19:53:29 GMT   (1030kb,D)

Title: GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph
  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding
Authors: Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry,
  Sutanay Choudhury
Categories: cs.AI
Comments: To be published in AAAI Spring Symposium: AAAI-MAKE 2024
\\
  Integrating large language models (LLMs) with knowledge graphs derived from
domain-specific data represents an important advancement towards more powerful
and factual reasoning. As these models grow more capable, it is crucial to
enable them to perform multi-step inferences over real-world knowledge graphs
while minimizing hallucination. While large language models excel at
conversation and text generation, their ability to reason over
domain-specialized graphs of interconnected entities remains limited. For
example, can we query a LLM to identify the optimal contact in a professional
network for a specific goal, based on relationships and attributes in a private
database? The answer is no--such capabilities lie beyond current methods.
However, this question underscores a critical technical gap that must be
addressed. Many high-value applications in areas such as science, security, and
e-commerce rely on proprietary knowledge graphs encoding unique structures,
relationships, and logical constraints. We introduce a fine-tuning framework
for developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge
graph into an alternate text representation with labeled question-answer pairs.
We demonstrate that grounding the models in specific graph-based knowledge
expands the models' capacity for structure-based reasoning. Our methodology
leverages the large-language model's generative capabilities to create the
dataset and proposes an efficient alternate to retrieval-augmented generation
styled methods.
\\ ( https://arxiv.org/abs/2402.06764 ,  1030kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06782
Date: Fri, 9 Feb 2024 21:05:01 GMT   (7563kb,D)

Title: Debating with More Persuasive LLMs Leads to More Truthful Answers
Authors: Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan,
  Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rockt\"aschel
  and Ethan Perez
Categories: cs.AI cs.CL
Comments: For code please check: https://github.com/ucl-dark/llm_debate
\\
  Common methods for aligning large language models (LLMs) with desired
behaviour heavily rely on human-labelled data. However, as models grow
increasingly sophisticated, they will surpass human expertise, and the role of
human evaluation will evolve into non-experts overseeing experts. In
anticipation of this, we ask: can weaker models assess the correctness of
stronger models? We investigate this question in an analogous setting, where
stronger models (experts) possess the necessary information to answer questions
and weaker models (non-experts) lack this information. The method we evaluate
is \textit{debate}, where two LLM experts each argue for a different answer,
and a non-expert selects the answer. We find that debate consistently helps
both non-expert models and humans answer questions, achieving 76\% and 88\%
accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore,
optimising expert debaters for persuasiveness in an unsupervised manner
improves non-expert ability to identify the truth in debates. Our results
provide encouraging empirical evidence for the viability of aligning models
with debate in the absence of ground truth.
\\ ( https://arxiv.org/abs/2402.06782 ,  7563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06811
Date: Fri, 9 Feb 2024 22:21:55 GMT   (630kb)

Title: Discipline and Label: A WEIRD Genealogy and Social Theory of Data
  Annotation
Authors: Andrew Smart, Ding Wang, Ellis Monk, Mark D\'iaz, Atoosa Kasirzadeh,
  Erin Van Liemt, Sonja Schmer-Galunder
Categories: cs.AI
Comments: 18 pages
\\
  Data annotation remains the sine qua non of machine learning and AI. Recent
empirical work on data annotation has begun to highlight the importance of
rater diversity for fairness, model performance, and new lines of research have
begun to examine the working conditions for data annotation workers, the
impacts and role of annotator subjectivity on labels, and the potential
psychological harms from aspects of annotation work. This paper outlines a
critical genealogy of data annotation; starting with its psychological and
perceptual aspects. We draw on similarities with critiques of the rise of
computerized lab-based psychological experiments in the 1970's which question
whether these experiments permit the generalization of results beyond the
laboratory settings within which these results are typically obtained. Do data
annotations permit the generalization of results beyond the settings, or
locations, in which they were obtained? Psychology is overly reliant on
participants from Western, Educated, Industrialized, Rich, and Democratic
societies (WEIRD). Many of the people who work as data annotation platform
workers, however, are not from WEIRD countries; most data annotation workers
are based in Global South countries. Social categorizations and classifications
from WEIRD countries are imposed on non-WEIRD annotators through instructions
and tasks, and through them, on data, which is then used to train or evaluate
AI models in WEIRD countries. We synthesize evidence from several recent lines
of research and argue that data annotation is a form of automated social
categorization that risks entrenching outdated and static social categories
that are in reality dynamic and changing. We propose a framework for
understanding the interplay of the global social conditions of data annotation
with the subjective phenomenological experience of data annotation work.
\\ ( https://arxiv.org/abs/2402.06811 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06852
Date: Sat, 10 Feb 2024 01:11:59 GMT   (6527kb,D)

Title: ChemLLM: A Chemical Large Language Model
Authors: Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan,
  Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su,
  Hansen Zhong, Yuqiang Li, Wanli Ouyang
Categories: cs.AI cs.CL
\\
  Large language models (LLMs) have made impressive progress in chemistry
applications, including molecular property prediction, molecular generation,
experimental protocol design, etc. However, the community lacks a
dialogue-based model specifically designed for chemistry. The challenge arises
from the fact that most chemical data and scientific knowledge are primarily
stored in structured databases, and the direct use of these structured data
compromises the model's ability to maintain coherent dialogue. To tackle this
issue, we develop a novel template-based instruction construction method that
transforms structured knowledge into plain dialogue, making it suitable for
language model training. By leveraging this approach, we develop ChemLLM, the
first large language model dedicated to chemistry, capable of performing
various tasks across chemical disciplines with smooth dialogue interaction.
ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name
conversion, molecular caption, and reaction prediction, and surpasses GPT-4 on
two of them. Remarkably, ChemLLM also shows exceptional adaptability to related
mathematical and physical tasks despite being trained mainly on
chemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in
specialized NLP tasks within chemistry, such as literature translation and
cheminformatic programming. ChemLLM opens up a new avenue for exploration
within chemical studies, while our method of integrating structured chemical
knowledge into dialogue systems sets a new frontier for developing LLMs across
various scientific fields. Codes, Datasets, and Model weights are publicly
accessible at hf.co/AI4Chem/ChemLLM-7B-Chat.
\\ ( https://arxiv.org/abs/2402.06852 ,  6527kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06861
Date: Sat, 10 Feb 2024 01:50:19 GMT   (3299kb,D)

Title: UrbanKGent: A Unified Large Language Model Agent Framework for Urban
  Knowledge Graph Construction
Authors: Yansong Ning, Hao Liu
Categories: cs.AI
Comments: Under review
\\
  Urban knowledge graph has recently worked as an emerging building block to
distill critical knowledge from multi-sourced urban data for diverse urban
application scenarios. Despite its promising benefits, urban knowledge graph
construction (UrbanKGC) still heavily relies on manual effort, hindering its
potential advancement. This paper presents UrbanKGent, a unified large language
model agent framework, for urban knowledge graph construction. Specifically, we
first construct the knowledgeable instruction set for UrbanKGC tasks (such as
relational triplet extraction and knowledge graph completion) via
heterogeneity-aware and geospatial-infused instruction generation. Moreover, we
propose a tool-augmented iterative trajectory refinement module to enhance and
refine the trajectories distilled from GPT-4. Through hybrid instruction
fine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC
agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world
datasets using both human and GPT-4 self-evaluation. The experimental results
demonstrate that UrbanKGent-13B not only can significantly outperform 21
baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4,
by more than 10\% with approximately 20 times lower cost. We deploy
UrbanKGent-13B to provide online services, which can construct an UrbanKG with
thousands of times richer relationships using only one-fifth of the data
compared with the existing benchmark. Our data, code, and opensource UrbanKGC
agent are available at https://github.com/usail-hkust/UrbanKGent.
\\ ( https://arxiv.org/abs/2402.06861 ,  3299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06929
Date: Sat, 10 Feb 2024 11:38:09 GMT   (546kb,D)

Title: Making a prototype of Seoul historical sites chatbot using Langchain
Authors: Jae Young Suh, Minsoo Kwak, Soo Yong Kim, Hyoungseo Cho
Categories: cs.AI
Comments: 4 pages, 4 figures, draft
\\
  In this paper, we are going to share a draft of the development of a
conversational agent created to disseminate information about historical sites
located in the Seoul. The primary objective of the agent is to increase
awareness among visitors who are not familiar with Seoul, about the presence
and precise locations of valuable cultural heritage sites. It aims to promote a
basic understanding of Korea's rich and diverse cultural history. The agent is
thoughtfully designed for accessibility in English and utilizes data generously
provided by the Seoul Metropolitan Government. Despite the limited data volume,
it consistently delivers reliable and accurate responses, seamlessly aligning
with the available information. We have meticulously detailed the methodologies
employed in creating this agent and provided a comprehensive overview of its
underlying structure within the paper. Additionally, we delve into potential
improvements to enhance this initial version of the system, with a primary
emphasis on expanding the available data through our prompting. In conclusion,
we provide an in-depth discussion of our expectations regarding the future
impact of this agent in promoting and facilitating the sharing of historical
sites.
\\ ( https://arxiv.org/abs/2402.06929 ,  546kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07016
Date: Sat, 10 Feb 2024 18:27:28 GMT   (1390kb,D)

Title: REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records
  Analysis via Large Language Models
Authors: Yinghao Zhu, Changyu Ren, Shiyun Xie, Shukai Liu, Hangyuan Ji, Zixiang
  Wang, Tao Sun, Long He, Zhoujun Li, Xi Zhu, Chengwei Pan
Categories: cs.AI
\\
  The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.
\\ ( https://arxiv.org/abs/2402.07016 ,  1390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07039
Date: Sat, 10 Feb 2024 20:39:04 GMT   (109kb)

Title: Coordinated Disclosure for AI: Beyond Security Vulnerabilities
Authors: Sven Cattell, Avijit Ghosh
Categories: cs.AI cs.CR cs.CY
\\
  Harm reporting in the field of Artificial Intelligence (AI) currently
operates on an ad hoc basis, lacking a structured process for disclosing or
addressing algorithmic flaws. In contrast, the Coordinated Vulnerability
Disclosure (CVD) ethos and ecosystem play a pivotal role in software security
and transparency. Within the U.S. context, there has been a protracted legal
and policy struggle to establish a safe harbor from the Computer Fraud and
Abuse Act, aiming to foster institutional support for security researchers
acting in good faith. Notably, algorithmic flaws in Machine Learning (ML)
models present distinct challenges compared to traditional software
vulnerabilities, warranting a specialized approach. To address this gap, we
propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD)
framework tailored to the intricacies of machine learning and artificial
intelligence issues. This paper delves into the historical landscape of
disclosures in ML, encompassing the ad hoc reporting of harms and the emergence
of participatory auditing. By juxtaposing these practices with the
well-established disclosure norms in cybersecurity, we argue that the broader
adoption of CFD has the potential to enhance public trust through transparent
processes that carefully balance the interests of both organizations and the
community.
\\ ( https://arxiv.org/abs/2402.07039 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07049
Date: Sat, 10 Feb 2024 21:44:28 GMT   (1028kb)

Title: A Factor Graph Model of Trust for a Collaborative Multi-Agent System
Authors: Behzad Akbari, Mingfeng Yuan, Hao Wang, Haibin Zhu, Jinjun Shan
Categories: cs.AI
\\
  In the field of Multi-Agent Systems (MAS), known for their openness,
dynamism, and cooperative nature, the ability to trust the resources and
services of other agents is crucial. Trust, in this setting, is the reliance
and confidence an agent has in the information, behaviors, intentions,
truthfulness, and capabilities of others within the system. Our paper
introduces a new graphical approach that utilizes factor graphs to represent
the interdependent behaviors and trustworthiness among agents. This includes
modeling the behavior of robots as a trajectory of actions using a Gaussian
process factor graph, which accounts for smoothness, obstacle avoidance, and
trust-related factors. Our method for evaluating trust is decentralized and
considers key interdependent sub-factors such as proximity safety, consistency,
and cooperation. The overall system comprises a network of factor graphs that
interact through trust-related factors and employs a Bayesian inference method
to dynamically assess trust-based decisions with informed consent. The
effectiveness of this method is validated via simulations and empirical tests
with autonomous robots navigating unsignalized intersections.
\\ ( https://arxiv.org/abs/2402.07049 ,  1028kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07140
Date: Sun, 11 Feb 2024 09:46:24 GMT   (323kb,D)

Title: Sequential Ordering in Textual Descriptions: Impact on Spatial
  Perception Abilities of Large Language Models
Authors: Yuyao Ge, Shenghua Liu, Lingrui Mei, Lizhe Chen, Xueqi Cheng
Categories: cs.AI
\\
  In recent years, Large Language Models have reached state-of-the-art
performance across multiple domains. However, the progress in the field of
graph reasoning remains limited. Our work delves into this gap by thoroughly
investigating graph reasoning with LLM. In this work, we reveal the impact of
text sequence on LLM spatial understanding, finding that graph-descriptive text
sequences significantly affect LLM reasoning performance on graphs. By altering
the graph-descriptive text sequences, we enhance the performance of LLM from
42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM
performance and graph size, discovering that the reasoning performance of LLM
does not monotonically decrease with the increase in graph size. Conclusively,
we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance
across varied graph sizes.
\\ ( https://arxiv.org/abs/2402.07140 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07166
Date: Sun, 11 Feb 2024 11:23:28 GMT   (326kb,D)

Title: Social Evolution of Published Text and The Emergence of Artificial
  Intelligence Through Large Language Models and The Problem of Toxicity and
  Bias
Authors: Arifa Khan, P. Saravanan and S.K Venkatesan
Categories: cs.AI
\\
  We provide a birds eye view of the rapid developments in AI and Deep Learning
that has led to the path-breaking emergence of AI in Large Language Models. The
aim of this study is to place all these developments in a pragmatic broader
historical social perspective without any exaggerations while at the same time
without any pessimism that created the AI winter in the 1970s to 1990s. We also
at the same time point out toxicity, bias, memorization, sycophancy, logical
inconsistencies, hallucinations that exist just as a warning to the overly
optimistic. We note here that just as this emergence of AI seems to occur at a
threshold point in the number of neural connections or weights, it has also
been observed that human brain and especially the cortex region is nothing
special or extraordinary but simply a case of scaled-up version of the primate
brain and that even the human intelligence seems like an emergent phenomena of
scale.
\\ ( https://arxiv.org/abs/2402.07166 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07167
Date: Sun, 11 Feb 2024 11:24:09 GMT   (6203kb,D)

Title: Large-Language-Model Empowered Dose Volume Histogram Prediction for
  Intensity Modulated Radiotherapy
Authors: Zehao Dong, Yixin Chen, Hiram Gay, Yao Hao, Geoffrey D. Hugo, Pamela
  Samson, Tianyu Zhao
Categories: cs.AI
\\
  Treatment planning is currently a patient specific, time-consuming, and
resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction
plays a critical role in automating this process. The geometric relationship
between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target
volume (PTV) has been well established. This study explores the potential of
deep learning models for predicting DVHs using images and subsequent human
intervention facilitated by a large-language model (LLM) to enhance the
planning quality. We propose a pipeline to convert unstructured images to a
structured graph consisting of image-patch nodes and dose nodes. A novel Dose
Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the
structured graph. The proposed DoseGNN is enhanced with the LLM to encode
massive knowledge from prescriptions and interactive instructions from
clinicians. In this study, we introduced an online human-AI collaboration
(OHAC) system as a practical implementation of the concept proposed for the
automation of intensity-modulated radiotherapy (IMRT) planning. In comparison
to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean
square errors that were 80$\%$, 76$\%$ and 41.0$\%$ of those predicted by Swin
U-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the
LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans
through interaction with clinicians using natural language.
\\ ( https://arxiv.org/abs/2402.07167 ,  6203kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07183
Date: Sun, 11 Feb 2024 12:35:28 GMT   (3475kb,D)

Title: A Random Ensemble of Encrypted Vision Transformers for Adversarially
  Robust Defense
Authors: Ryota Iijima, Sayaka Shiota, Hitoshi Kiya
Categories: cs.AI
Comments: 9 pages
\\
  Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In previous studies, the use of models encrypted with a secret
key was demonstrated to be robust against white-box attacks, but not against
black-box ones. In this paper, we propose a novel method using the vision
transformer (ViT) that is a random ensemble of encrypted models for enhancing
robustness against both white-box and black-box attacks. In addition, a
benchmark attack method, called AutoAttack, is applied to models to test
adversarial robustness objectively. In experiments, the method was demonstrated
to be robust against not only white-box attacks but also black-box ones in an
image classification task on the CIFAR-10 and ImageNet datasets. The method was
also compared with the state-of-the-art in a standardized benchmark for
adversarial robustness, RobustBench, and it was verified to outperform
conventional defenses in terms of clean accuracy and robust accuracy.
\\ ( https://arxiv.org/abs/2402.07183 ,  3475kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07197
Date: Sun, 11 Feb 2024 13:24:13 GMT   (684kb,D)

Title: GraphTranslator: Aligning Graph Model to Large Language Model for
  Open-ended Tasks
Authors: Mengmei Zhang (Alibaba Group Holding Limited, China Telecom Bestpay),
  Mingwei Sun (Alibaba Group Holding Limited), Peng Wang (Alibaba Group Holding
  Limited), Shen Fan (Alibaba Group Holding Limited), Yanhu Mo (Alibaba Group
  Holding Limited), Xiaoxiao Xu (Alibaba Group Holding Limited), Hong Liu
  (Alibaba Group Holding Limited), Cheng Yang (Peng Cheng Laboratory), Chuan
  Shi (Peng Cheng Laboratory)
Categories: cs.AI
\\
  Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and
instruction-following capabilities, have catalyzed a revolutionary
transformation across diverse research fields of artificial intelligence,
especially for open-ended tasks. While the idea is less explored in the graph
domain, despite the availability of numerous powerful graph models (GMs), they
are restricted to tasks in a pre-defined form. Although several methods
applying LLMs to graphs have been proposed, they fail to simultaneously handle
the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as
a standalone predictor. To break this dilemma, we propose to bridge the
pretrained GM and LLM by a Translator, named GraphTranslator, aiming to
leverage GM to handle the pre-defined tasks effectively and utilize the
extended interface of LLMs to offer various open-ended tasks for GM. To train
such Translator, we propose a Producer capable of constructing the graph-text
alignment data along node information, neighbor information and model
information. By treating the node representation as a type of language, the
proposed GraphTranslator empowers an LLM to make predictions based on node
representation and language instructions, providing a unified perspective for
both pre-defined and open-ended tasks. Extensive results show that the proposed
GraphTranslator effectively improves the results of zero-shot node
classification. The graph question answering experiments reveal our
GraphTranslator potential across a broad spectrum of open-ended applications
through language instructions.
\\ ( https://arxiv.org/abs/2402.07197 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07199
Date: Sun, 11 Feb 2024 13:26:06 GMT   (352kb,D)

Title: Link-aware link prediction over temporal graph by pattern recognition
Authors: Bingqing Liu, Xikun Huang
Categories: cs.AI
Comments: 12 pages, one column
\\
  A temporal graph can be considered as a stream of links, each of which
represents an interaction between two nodes at a certain time. On temporal
graphs, link prediction is a common task, which aims to answer whether the
query link is true or not. To do this task, previous methods usually focus on
the learning of representations of the two nodes in the query link. We point
out that the learned representation by their models may encode too much
information with side effects for link prediction because they have not
utilized the information of the query link, i.e., they are link-unaware. Based
on this observation, we propose a link-aware model: historical links and the
query link are input together into the following model layers to distinguish
whether this input implies a reasonable pattern that ends with the query link.
During this process, we focus on the modeling of link evolution patterns rather
than node representations. Experiments on six datasets show that our model
achieves strong performances compared with state-of-the-art baselines, and the
results of link prediction are interpretable. The code and datasets are
available on the project website: https://github.com/lbq8942/TGACN.
\\ ( https://arxiv.org/abs/2402.07199 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07204
Date: Sun, 11 Feb 2024 13:30:53 GMT   (10230kb,D)

Title: Synergizing Spatial Optimization with Large Language Models for
  Open-Domain Urban Itinerary Planning
Authors: Yihong Tang, Zhaokai Wang, Ao Qu, Yihao Yan, Kebing Hou, Dingyi
  Zhuang, Xiaotong Guo, Jinhua Zhao, Zhan Zhao, Wei Ma
Categories: cs.AI cs.CL cs.LG
\\
  In this paper, we for the first time propose the task of Open-domain Urban
Itinerary Planning (OUIP) for citywalk, which directly generates itineraries
based on users' requests described in natural language. OUIP is different from
conventional itinerary planning, which limits users from expressing more
detailed needs and hinders true personalization. Recently, large language
models (LLMs) have shown potential in handling diverse tasks. However, due to
non-real-time information, incomplete knowledge, and insufficient spatial
awareness, they are unable to independently deliver a satisfactory user
experience in OUIP. Given this, we present ItiNera, an OUIP system that
synergizes spatial optimization with Large Language Models (LLMs) to provide
services that customize urban itineraries based on users' needs. Specifically,
we develop an LLM-based pipeline for extracting and updating POI features to
create a user-owned personalized POI database. For each user request, we
leverage LLM in cooperation with an embedding-based module for retrieving
candidate POIs from the user's POI database. Then, a spatial optimization
module is used to order these POIs, followed by LLM crafting a personalized,
spatially coherent itinerary. To the best of our knowledge, this study marks
the first integration of LLMs to innovate itinerary planning solutions.
Extensive experiments on offline datasets and online subjective evaluation have
demonstrated the capacities of our system to deliver more responsive and
spatially coherent itineraries than current LLM-based solutions. Our system has
been deployed in production at the TuTu online travel service and has attracted
thousands of users for their urban travel planning.
\\ ( https://arxiv.org/abs/2402.07204 ,  10230kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07221
Date: Sun, 11 Feb 2024 14:39:40 GMT   (190kb,D)

Title: The Reasons that Agents Act: Intention and Instrumental Goals
Authors: Francis Rhys Ward and Matt MacDermott and Francesco Belardinelli and
  Francesca Toni and Tom Everitt
Categories: cs.AI
Comments: AAMAS24
\\
  Intention is an important and challenging concept in AI. It is important
because it underlies many other concepts we care about, such as agency,
manipulation, legal responsibility, and blame. However, ascribing intent to AI
systems is contentious, and there is no universally accepted theory of
intention applicable to AI agents. We operationalise the intention with which
an agent acts, relating to the reasons it chooses its decision. We introduce a
formal definition of intention in structural causal influence models, grounded
in the philosophy literature on intent and applicable to real-world machine
learning systems. Through a number of examples and results, we show that our
definition captures the intuitive notion of intent and satisfies desiderata
set-out by past work. In addition, we show how our definition relates to past
concepts, including actual causality, and the notion of instrumental goals,
which is a core idea in the literature on safe AI agents. Finally, we
demonstrate how our definition can be used to infer the intentions of
reinforcement learning agents and language models from their behaviour.
\\ ( https://arxiv.org/abs/2402.07221 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07226
Date: Sun, 11 Feb 2024 15:23:13 GMT   (3098kb,D)

Title: Stitching Sub-Trajectories with Conditional Diffusion Model for
  Goal-Conditioned Offline RL
Authors: Sungyoon Kim, Yunseon Choi, Daiki E. Matsunaga, and Kee-Eung Kim
Categories: cs.AI
\\
  Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an
important problem in RL that focuses on acquiring diverse goal-oriented skills
solely from pre-collected behavior datasets. In this setting, the reward
feedback is typically absent except when the goal is achieved, which makes it
difficult to learn policies especially from a finite dataset of suboptimal
behaviors. In addition, realistic scenarios involve long-horizon planning,
which necessitates the extraction of useful skills within sub-trajectories.
Recently, the conditional diffusion model has been shown to be a promising
approach to generate high-quality long-horizon plans for RL. However, their
practicality for the goal-conditioned setting is still limited due to a number
of technical assumptions made by the methods. In this paper, we propose SSD
(Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method
that leverages the conditional diffusion model to address these limitations. In
summary, we use the diffusion model that generates future plans conditioned on
the target goal and value, with the target value estimated from the
goal-relabeled offline dataset. We report state-of-the-art performance in the
standard benchmark set of GCRL tasks, and demonstrate the capability to
successfully stitch the segments of suboptimal trajectories in the offline data
to generate high-quality plans.
\\ ( https://arxiv.org/abs/2402.07226 ,  3098kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07234
Date: Sun, 11 Feb 2024 15:56:03 GMT   (341kb,D)

Title: CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for
  Chinese Public Security Domain
Authors: Xin Tong, Bo Jin, Zhi Lin, Binjun Wang and Ting Yu
Categories: cs.AI
\\
  Large Language Models (LLMs) have demonstrated significant potential and
effectiveness across multiple application domains. To assess the performance of
mainstream LLMs in public security tasks, this study aims to construct a
specialized evaluation benchmark tailored to the Chinese public security
domain--CPSDbench. CPSDbench integrates datasets related to public security
collected from real-world scenarios, supporting a comprehensive assessment of
LLMs across four key dimensions: text classification, information extraction,
question answering, and text generation. Furthermore, this study introduces a
set of innovative evaluation metrics designed to more precisely quantify the
efficacy of LLMs in executing tasks related to public security. Through the
in-depth analysis and evaluation conducted in this research, we not only
enhance our understanding of the performance strengths and limitations of
existing models in addressing public security issues but also provide
references for the future development of more accurate and customized LLM
models targeted at applications in this field.
\\ ( https://arxiv.org/abs/2402.07234 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07326
Date: Sun, 11 Feb 2024 23:23:31 GMT   (528kb)

Title: Persian Speech Emotion Recognition by Fine-Tuning Transformers
Authors: Minoo Shayaninasab, Bagher Babaali
Categories: cs.AI eess.AS
\\
  Given the significance of speech emotion recognition, numerous methods have
been developed in recent years to create effective and efficient systems in
this domain. One of these methods involves the use of pretrained transformers,
fine-tuned to address this specific problem, resulting in high accuracy.
Despite extensive discussions and global-scale efforts to enhance these
systems, the application of this innovative and effective approach has received
less attention in the context of Persian speech emotion recognition. In this
article, we review the field of speech emotion recognition and its background,
with an emphasis on the importance of employing transformers in this context.
We present two models, one based on spectrograms and the other on the audio
itself, fine-tuned using the shEMO dataset. These models significantly enhance
the accuracy of previous systems, increasing it from approximately 65% to 80%
on the mentioned dataset. Subsequently, to investigate the effect of
multilinguality on the fine-tuning process, these same models are fine-tuned
twice. First, they are fine-tuned using the English IEMOCAP dataset, and then
they are fine-tuned with the Persian shEMO dataset. This results in an improved
accuracy of 82% for the Persian emotion recognition system. Keywords: Persian
Speech Emotion Recognition, shEMO, Self-Supervised Learning
\\ ( https://arxiv.org/abs/2402.07326 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07327
Date: Sun, 11 Feb 2024 23:27:24 GMT   (1221kb)

Title: Multi-Modal Emotion Recognition by Text, Speech and Video Using
  Pretrained Transformers
Authors: Minoo Shayaninasab, Bagher Babaali
Categories: cs.AI
\\
  Due to the complex nature of human emotions and the diversity of emotion
representation methods in humans, emotion recognition is a challenging field.
In this research, three input modalities, namely text, audio (speech), and
video, are employed to generate multimodal feature vectors. For generating
features for each of these modalities, pre-trained Transformer models with
fine-tuning are utilized. In each modality, a Transformer model is used with
transfer learning to extract feature and emotional structure. These features
are then fused together, and emotion recognition is performed using a
classifier. To select an appropriate fusion method and classifier, various
feature-level and decision-level fusion techniques have been experimented with,
and ultimately, the best model, which combines feature-level fusion by
concatenating feature vectors and classification using a Support Vector Machine
on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords:
Multimodal Emotion Recognition, IEMOCAP, Self-Supervised Learning, Transfer
Learning, Transformer.
\\ ( https://arxiv.org/abs/2402.07327 ,  1221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07350
Date: Mon, 12 Feb 2024 00:44:37 GMT   (492kb,D)

Title: Antagonistic AI
Authors: Alice Cai, Ian Arawjo, Elena L. Glassman
Categories: cs.AI cs.HC
Comments: 17 pages, 1 figure, 5 tables
ACM-class: I.2.0; J.0; K.4.0
\\
  The vast majority of discourse around AI development assumes that
subservient, "moral" models aligned with "human values" are universally
beneficial -- in short, that good AI is sycophantic AI. We explore the shadow
of the sycophantic paradigm, a design space we term antagonistic AI: AI systems
that are disagreeable, rude, interrupting, confrontational, challenging, etc.
-- embedding opposite behaviors or values. Far from being "bad" or "immoral,"
we consider whether antagonistic AI systems may sometimes have benefits to
users, such as forcing users to confront their assumptions, build resilience,
or develop healthier relational boundaries. Drawing from formative explorations
and a speculative design workshop where participants designed fictional AI
technologies that employ antagonism, we lay out a design space for antagonistic
AI, articulating potential benefits, design techniques, and methods of
embedding antagonistic elements into user experience. Finally, we discuss the
many ethical challenges of this space and identify three dimensions for the
responsible design of antagonistic AI -- consent, context, and framing.
\\ ( https://arxiv.org/abs/2402.07350 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07398
Date: Mon, 12 Feb 2024 04:13:16 GMT   (11182kb,D)

Title: VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language
  Models with Autonomous Instruction Optimization
Authors: Dongsheng Zhu, Xunzhu Tang, Weidong Han, Jinghui Lu, Yukun Zhao,
  Guoliang Xing, Junfeng Wang, Dawei Yin
Categories: cs.AI
\\
  This paper presents VisLingInstruct, a novel approach to advancing
Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show
impressive zero-shot abilities in multi-modal tasks, but their performance
depends heavily on the quality of instructions. VisLingInstruct tackles this by
autonomously evaluating and optimizing instructional texts through In-Context
Learning, improving the synergy between visual perception and linguistic
expression in MMLMs. Alongside this instructional advancement, we have also
optimized the visual feature extraction modules in MMLMs, further augmenting
their responsiveness to textual cues. Our comprehensive experiments on MMLMs,
based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves
zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1%
and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and
HatefulMemes datasets.
\\ ( https://arxiv.org/abs/2402.07398 ,  11182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07404
Date: Mon, 12 Feb 2024 04:47:38 GMT   (557kb)

Title: Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic
  Hierarchy Process and GPT-4 for Automated Decision Support
Authors: Igor Svoboda, Dmytro Lande
Categories: cs.AI cs.CR cs.MA
Comments: 24 pages, 1 figure
ACM-class: I.2.1; I.2.8; H.1.1
\\
  Our study presents a new framework that incorporates the Analytic Hierarchy
Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language
model (LLM), bringing novel approaches to cybersecurity Multiple-criteria
Decision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous
agents as virtual experts, we automate the decision-making process, enhancing
both efficiency and reliability. This new approach focuses on leveraging LLMs
for sophisticated decision analysis, highlighting the synergy between
traditional decision-making models and cutting-edge AI technologies. Our
innovative methodology demonstrates significant advancements in using AI-driven
agents for complex decision-making scenarios, highlighting the importance of AI
in strategic cybersecurity applications. The findings reveal the transformative
potential of combining AHP and LLMs, establishing a new paradigm for
intelligent decision support systems in cybersecurity and beyond.
\\ ( https://arxiv.org/abs/2402.07404 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07418
Date: Mon, 12 Feb 2024 05:46:10 GMT   (17723kb,D)

Title: SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy
  Adaptation
Authors: Sangwoo Shin, Minjong Yoo, Jeongwoo Lee, Honguk Woo
Categories: cs.AI
Comments: AAAI 2024 Camera-ready version
\\
  This work explores the zero-shot adaptation capability of semantic skills,
semantically interpretable experts' behavior patterns, in cross-domain
settings, where a user input in interleaved multi-modal snippets can prompt a
new long-horizon task for different domains. In these cross-domain settings, we
present a semantic skill translator framework SemTra which utilizes a set of
multi-modal models to extract skills from the snippets, and leverages the
reasoning capabilities of a pretrained language model to adapt these extracted
skills to the target domain. The framework employs a two-level hierarchy for
adaptation: task adaptation and skill adaptation. During task adaptation,
seq-to-seq translation by the language model transforms the extracted skills
into a semantic skill sequence, which is tailored to fit the cross-domain
contexts. Skill adaptation focuses on optimizing each semantic skill for the
target domain context, through parametric instantiations that are facilitated
by language prompting and contrastive learning-based context inferences. This
hierarchical adaptation empowers the framework to not only infer a complex task
specification in one-shot from the interleaved multi-modal snippets, but also
adapt it to new domains with zero-shot learning abilities. We evaluate our
framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The
results clarify the framework's superiority in performing long-horizon tasks
and adapting to different domains, showing its broad applicability in practical
use cases, such as cognitive robots interpreting abstract instructions and
autonomous vehicles operating under varied configurations.
\\ ( https://arxiv.org/abs/2402.07418 ,  17723kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07420
Date: Mon, 12 Feb 2024 05:48:52 GMT   (3428kb,D)

Title: On the Transit Obfuscation Problem
Authors: Hideaki Takahashi and Alex Fukunaga
Categories: cs.AI
\\
  Concealing an intermediate point on a route or visible from a route is an
important goal in some transportation and surveillance scenarios. This paper
studies the Transit Obfuscation Problem, the problem of traveling from some
start location to an end location while "covering" a specific transit point
that needs to be concealed from adversaries. We propose the notion of transit
anonymity, a quantitative guarantee of the anonymity of a specific transit
point, even with a powerful adversary with full knowledge of the path planning
algorithm. We propose and evaluate planning/search algorithms that satisfy this
anonymity criterion.
\\ ( https://arxiv.org/abs/2402.07420 ,  3428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07422
Date: Mon, 12 Feb 2024 05:56:12 GMT   (277kb)

Title: News Recommendation with Attention Mechanism
Authors: Tianrui Liu, Changxin Xu, Yuxin Qiao, Chufeng Jiang, Weisheng Chen
Categories: cs.AI
Comments: 7 pages, Journal of Industrial Engineering and Applied Science
\\
  This paper explores the area of news recommendation, a key component of
online information sharing. Initially, we provide a clear introduction to news
recommendation, defining the core problem and summarizing current methods and
notable recent algorithms. We then present our work on implementing the NRAM
(News Recommendation with Attention Mechanism), an attention-based approach for
news recommendation, and assess its effectiveness. Our evaluation shows that
NRAM has the potential to significantly improve how news content is
personalized for users on digital news platforms.
\\ ( https://arxiv.org/abs/2402.07422 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07429
Date: Mon, 12 Feb 2024 06:06:09 GMT   (366kb)

Title: Particle Filter SLAM for Vehicle Localization
Authors: Tianrui Liu, Changxin Xu, Yuxin Qiao, Chufeng Jiang, Jiqiang Yu
Categories: cs.AI
Comments: 6 pages, Journal of Industrial Engineering and Applied Science
\\
  Simultaneous Localization and Mapping (SLAM) presents a formidable challenge
in robotics, involving the dynamic construction of a map while concurrently
determining the precise location of the robotic agent within an unfamiliar
environment. This intricate task is further compounded by the inherent
"chicken-and-egg" dilemma, where accurate mapping relies on a dependable
estimation of the robot's location, and vice versa. Moreover, the computational
intensity of SLAM adds an additional layer of complexity, making it a crucial
yet demanding topic in the field. In our research, we address the challenges of
SLAM by adopting the Particle Filter SLAM method. Our approach leverages
encoded data and fiber optic gyro (FOG) information to enable precise
estimation of vehicle motion, while lidar technology contributes to
environmental perception by providing detailed insights into surrounding
obstacles. The integration of these data streams culminates in the
establishment of a Particle Filter SLAM framework, representing a key endeavor
in this paper to effectively navigate and overcome the complexities associated
with simultaneous localization and mapping in robotic systems.
\\ ( https://arxiv.org/abs/2402.07429 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07442
Date: Mon, 12 Feb 2024 06:49:48 GMT   (322kb)

Title: Game Agent Driven by Free-Form Text Command: Using LLM-based Code
  Generation and Behavior Branch
Authors: Ray Ito, Junichiro Takahashi
Categories: cs.AI
Comments: This paper is posted at JSAI 2024 Conference
\\
  Several attempts have been made to implement text command control for game
agents. However, current technologies are limited to processing predefined
format commands. This paper proposes a pioneering text command control system
for a game agent that can understand natural language commands expressed in
free-form. The proposed system uses a large language model (LLM) for code
generation to interpret and transform natural language commands into behavior
branch, a proposed knowledge expression based on behavior trees, which
facilitates execution by the game agent. This study conducted empirical
validation within a game environment that simulates a Pok\'emon game and
involved multiple participants. The results confirmed the system's ability to
understand and carry out natural language commands, representing a noteworthy
in the realm of real-time language interactive game agents.
  Notice for the use of this material. The copyright of this material is
retained by the Japanese Society for Artificial Intelligence (JSAI). This
material is published here with the agreement of JSAI. Please be complied with
Copyright Law of Japan if any users wish to reproduce, make derivative work,
distribute or make available to the public any part or whole thereof. All
Rights Reserved, Copyright (C) The Japanese Society for Artificial
Intelligence.
\\ ( https://arxiv.org/abs/2402.07442 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07456
Date: Mon, 12 Feb 2024 07:29:22 GMT   (2761kb,D)

Title: OS-Copilot: Towards Generalist Computer Agents with Self-Improvement
Authors: Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu,
  Shunyu Yao, Tao Yu and Lingpeng Kong
Categories: cs.AI
Comments: Project page: https://os-copilot.github.io
\\
  Autonomous interaction with the computer has been a longstanding challenge
with great potential, and the recent proliferation of large language models
(LLMs) has markedly accelerated progress in building digital agents. However,
most of these agents are designed to interact with a narrow domain, such as a
specific software or website. This narrow focus constrains their applicability
for general computer tasks. To this end, we introduce OS-Copilot, a framework
to build generalist agents capable of interfacing with comprehensive elements
in an operating system (OS), including the web, code terminals, files,
multimedia, and various third-party applications. We use OS-Copilot to create
FRIDAY, a self-improving embodied agent for automating general computer tasks.
On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods
by 35%, showcasing strong generalization to unseen applications via accumulated
skills from previous tasks. We also present numerical and quantitative evidence
that FRIDAY learns to control and self-improve on Excel and Powerpoint with
minimal supervision. Our OS-Copilot framework and empirical findings provide
infrastructure and insights for future research toward more capable and
general-purpose computer agents.
\\ ( https://arxiv.org/abs/2402.07456 ,  2761kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07462
Date: Mon, 12 Feb 2024 07:49:48 GMT   (36088kb)

Title: A Hormetic Approach to the Value-Loading Problem: Preventing the
  Paperclip Apocalypse?
Authors: Nathan I. N. Henry, Mangor Pedersen, Matt Williams, Jamin L. B.
  Martin, Liesje Donkin
Categories: cs.AI cs.CY cs.LG cs.MA econ.TH
Comments: 105 pages (24 pages for main article excluding references and
  appendices), 46 figures (7 in main article, 39 in appendices), and 1 table
MSC-class: 68T01, 68T37, 68T42
ACM-class: I.2.0; I.2.8; I.2.11
\\
  The value-loading problem is a significant challenge for researchers aiming
to create artificial intelligence (AI) systems that align with human values and
preferences. This problem requires a method to define and regulate safe and
optimal limits of AI behaviors. In this work, we propose HALO (Hormetic
ALignment via Opponent processes), a regulatory paradigm that uses hormetic
analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a
phenomenon where low frequencies of a behavior have beneficial effects, while
high frequencies are harmful. By modeling behaviors as allostatic opponent
processes, we can use either Behavioral Frequency Response Analysis (BFRA) or
Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of
repeatable behaviors. We demonstrate how HALO can solve the 'paperclip
maximizer' scenario, a thought experiment where an unregulated AI tasked with
making paperclips could end up converting all matter in the universe into
paperclips. Our approach may be used to help create an evolving database of
'values' based on the hedonic calculus of repeatable behaviors with decreasing
marginal utility. This positions HALO as a promising solution for the
value-loading problem, which involves embedding human-aligned values into an AI
system, and the weak-to-strong generalization problem, which explores whether
weak models can supervise stronger models as they become more intelligent.
Hence, HALO opens several research avenues that may lead to the development of
a computational value system that allows an AI algorithm to learn whether the
decisions it makes are right or wrong.
\\ ( https://arxiv.org/abs/2402.07462 ,  36088kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07477
Date: Mon, 12 Feb 2024 08:32:29 GMT   (2516kb,D)

Title: Food Recommendation as Language Processing (F-RLP): A Personalized and
  Contextual Paradigm
Authors: Ali Rostami, Ramesh Jain, Amir M. Rahmani
Categories: cs.AI
\\
  State-of-the-art rule-based and classification-based food recommendation
systems face significant challenges in becoming practical and useful. This
difficulty arises primarily because most machine learning models struggle with
problems characterized by an almost infinite number of classes and a limited
number of samples within an unbalanced dataset. Conversely, the emergence of
Large Language Models (LLMs) as recommendation engines offers a promising
avenue. However, a general-purpose Recommendation as Language Processing (RLP)
approach lacks the critical components necessary for effective food
recommendations. To address this gap, we introduce Food Recommendation as
Language Processing (F-RLP), a novel framework that offers a food-specific,
tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize
their potential, thereby paving the way for more accurate, personalized food
recommendations.
\\ ( https://arxiv.org/abs/2402.07477 ,  2516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07483
Date: Mon, 12 Feb 2024 08:45:08 GMT   (282kb,D)

Title: T-RAG: Lessons from the LLM Trenches
Authors: Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla
Categories: cs.AI cs.CL
\\
  Large Language Models (LLM) have shown remarkable language capabilities
fueling attempts to integrate them into applications across a wide range of
domains. An important application area is question answering over private
enterprise documents where the main considerations are data security, which
necessitates applications that can be deployed on-prem, limited computational
resources and the need for a robust application that correctly responds to
queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent
framework for building LLM-based applications. While building a RAG is
relatively straightforward, making it robust and a reliable application
requires extensive customization and relatively deep knowledge of the
application domain. We share our experiences building and deploying an LLM
application for question answering over private organizational documents. Our
application combines the use of RAG with a finetuned open-source LLM.
Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure
to represent entity hierarchies within the organization. This is used to
generate a textual description to augment the context when responding to user
queries pertaining to entities within the organization's hierarchy. Our
evaluations show that this combination performs better than a simple RAG or
finetuning implementation. Finally, we share some lessons learned based on our
experiences building an LLM application for real-world use.
\\ ( https://arxiv.org/abs/2402.07483 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07507
Date: Mon, 12 Feb 2024 09:28:16 GMT   (1843kb,D)

Title: Clustering Dynamics for Improved Speed Prediction Deriving from
  Topographical GPS Registrations
Authors: Sarah Almeida Carneiro (LIGM), Giovanni Chierchia (LIGM), Aurelie
  Pirayre (IFPEN), Laurent Najman (LIGM)
Categories: cs.AI
\\
  A persistent challenge in the field of Intelligent Transportation Systems is
to extract accurate traffic insights from geographic regions with scarce or no
data coverage. To this end, we propose solutions for speed prediction using
sparse GPS data points and their associated topographical and road design
features. Our goal is to investigate whether we can use similarities in the
terrain and infrastructure to train a machine learning model that can predict
speed in regions where we lack transportation data. For this we create a
Temporally Orientated Speed Dictionary Centered on Topographically Clustered
Roads, which helps us to provide speed correlations to selected feature
configurations. Our results show qualitative and quantitative improvement over
new and standard regression methods. The presented framework provides a fresh
perspective on devising strategies for missing data traffic analysis.
\\ ( https://arxiv.org/abs/2402.07507 ,  1843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07510
Date: Mon, 12 Feb 2024 09:31:21 GMT   (841kb,D)

Title: Secret Collusion Among Generative AI Agents
Authors: Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay
  Bolina, Philip H.S. Torr, Lewis Hammond, Christian Schroeder de Witt
Categories: cs.AI cs.CR
\\
  Recent capability increases in large language models (LLMs) open up
applications in which teams of communicating generative AI agents solve joint
tasks. This poses privacy and security challenges concerning the unauthorised
sharing of information, or other unwanted forms of agent coordination. Modern
steganographic techniques could render such dynamics hard to detect. In this
paper, we comprehensively formalise the problem of secret collusion in systems
of generative AI agents by drawing on relevant concepts from both the AI and
security literature. We study incentives for the use of steganography, and
propose a variety of mitigation measures. Our investigations result in a model
evaluation framework that systematically tests capabilities required for
various forms of secret collusion. We provide extensive empirical results
across a range of contemporary LLMs. While the steganographic capabilities of
current models remain limited, GPT-4 displays a capability jump suggesting the
need for continuous monitoring of steganographic frontier model capabilities.
We conclude by laying out a comprehensive research program to mitigate future
risks of collusion between generative AI models.
\\ ( https://arxiv.org/abs/2402.07510 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07514
Date: Mon, 12 Feb 2024 09:38:42 GMT   (1863kb,D)

Title: Physics-informed machine learning as a kernel method
Authors: Nathan Doum\`eche (LPSM, EDF R&D OSIRIS), Francis Bach (DI-ENS),
  Claire Boyer (IUF, LPSM), G\'erard Biau (LPSM)
Categories: cs.AI math.ST stat.TH
\\
  Physics-informed machine learning combines the expressiveness of data-based
approaches with the interpretability of physical models. In this context, we
consider a general regression problem where the empirical risk is regularized
by a partial differential equation that quantifies the physical inconsistency.
We prove that for linear differential priors, the problem can be formulated as
a kernel regression task. Taking advantage of kernel theory, we derive
convergence rates for the minimizer of the regularized risk and show that it
converges at least at the Sobolev minimax rate. However, faster rates can be
achieved, depending on the physical error. This principle is illustrated with a
one-dimensional example, supporting the claim that regularizing the empirical
risk with physical information can be beneficial to the statistical performance
of estimators.
\\ ( https://arxiv.org/abs/2402.07514 ,  1863kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07536
Date: Mon, 12 Feb 2024 10:04:07 GMT   (2962kb,D)

Title: BreakGPT: A Large Language Model with Multi-stage Structure for
  Financial Breakout Detection
Authors: Kang Zhang, Osamu Yoshie, Weiran Huang
Categories: cs.AI
\\
  Trading range breakout (TRB) is a key method in the technical analysis of
financial trading, widely employed by traders in financial markets such as
stocks, futures, and foreign exchange. However, distinguishing between true and
false breakout and providing the correct rationale cause significant challenges
to investors. Recently, large language models have achieved success in various
downstream applications, but their effectiveness in the domain of financial
breakout detection has been subpar. The reason is that the unique data and
specific knowledge are required in breakout detection. To address these issues,
we introduce BreakGPT, the first large language model for financial breakout
detection. Furthermore, we have developed a novel framework for large language
models, namely multi-stage structure, effectively reducing mistakes in
downstream applications. Experimental results indicate that compared to
GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with
the multi-stage structure contributing 17.6% to the improvement. Additionally,
it outperforms ChatGPT-4 by 42.07%. Our Code is publicly available:
https://github.com/Neviim96/BreakGPT
\\ ( https://arxiv.org/abs/2402.07536 ,  2962kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07632
Date: Mon, 12 Feb 2024 13:16:30 GMT   (5427kb,D)

Title: Overconfident and Unconfident AI Hinder Human-AI Collaboration
Authors: Jingshu Li, Yitian Yang, Yi-chieh Lee
Categories: cs.AI cs.HC
\\
  As artificial intelligence (AI) advances, human-AI collaboration has become
increasingly prevalent across both professional and everyday settings. In such
collaboration, AI can express its confidence level about its performance,
serving as a crucial indicator for humans to evaluate AI's suggestions.
However, AI may exhibit overconfidence or underconfidence--its expressed
confidence is higher or lower than its actual performance--which may lead
humans to mistakenly evaluate AI advice. Our study investigates the influences
of AI's overconfidence and underconfidence on human trust, their acceptance of
AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI
confidence levels and performance feedback facilitates better recognition of AI
confidence misalignments. However, participants tend to withhold their trust as
perceiving such misalignments, leading to a rejection of AI suggestions and
subsequently poorer performance in collaborative tasks. Conversely, without
such information, participants struggle to identify misalignments, resulting in
either the neglect of correct AI advice or the following of incorrect AI
suggestions, adversely affecting collaboration. This study offers valuable
insights for enhancing human-AI collaboration by underscoring the importance of
aligning AI's expressed confidence with its actual performance and the
necessity of calibrating human trust towards AI confidence.
\\ ( https://arxiv.org/abs/2402.07632 ,  5427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07688
Date: Mon, 12 Feb 2024 14:53:28 GMT   (338kb,D)

Title: CyberMetric: A Benchmark Dataset for Evaluating Large Language Models
  Knowledge in Cybersecurity
Authors: Norbert Tihanyi, Mohamed Amine Ferrag, Ridhi Jain, Merouane Debbah
Categories: cs.AI cs.CR
\\
  Large Language Models (LLMs) excel across various domains, from computer
vision to medical diagnostics. However, understanding the diverse landscape of
cybersecurity, encompassing cryptography, reverse engineering, and managerial
facets like risk assessment, presents a challenge, even for human experts. In
this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000
questions sourced from standards, certifications, research papers, books, and
other publications in the cybersecurity domain. The questions are created
through a collaborative process, i.e., merging expert knowledge with LLMs,
including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying
their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's
main goal is to facilitate a fair comparison between humans and different LLMs
in cybersecurity. To achieve this, we carefully selected 80 questions covering
a wide range of topics within cybersecurity and involved 30 participants of
diverse expertise levels, facilitating a comprehensive comparison between human
and machine intelligence in this area. The findings revealed that LLMs
outperformed humans in almost every aspect of cybersecurity.
\\ ( https://arxiv.org/abs/2402.07688 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07710
Date: Mon, 12 Feb 2024 15:23:19 GMT   (1056kb,D)

Title: Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA
Authors: Chester Luo, Kevin Lai
Categories: cs.AI
Comments: 8 pages
\\
  In recent years, there has been a significant increase in the utilization of
deep learning methods, particularly convolutional neural networks (CNNs), which
have emerged as the dominant approach in various domains that involve
structured grid data, such as picture analysis and processing. Nevertheless,
the exponential growth in the utilization of LiDAR and 3D sensors across many
domains has resulted in an increased need for the analysis of 3D point clouds.
The utilization of 3D point clouds is crucial in various applications,
including object recognition and segmentation, as they offer a spatial
depiction of things within a three-dimensional environment. In contrast to
photos, point clouds exhibit sparsity and lack a regular grid, hence posing
distinct processing and computational issues.
\\ ( https://arxiv.org/abs/2402.07710 ,  1056kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07744
Date: Mon, 12 Feb 2024 16:14:22 GMT   (5486kb,D)

Title: Towards Unified Alignment Between Agents, Humans, and Environment
Authors: Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile
  Wang, Zeyuan Yang, Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo,
  Zhicheng Guo, Peng Li, Yang Liu
Categories: cs.AI cs.CL cs.LG
Comments: Project webpage:
  https://agent-force.github.io/unified-alignment-for-agents.html
\\
  The rapid progress of foundation models has led to the prosperity of
autonomous agents, which leverage the universal capabilities of foundation
models to conduct reasoning, decision-making, and environmental interaction.
However, the efficacy of agents remains limited when operating in intricate,
realistic environments. In this work, we introduce the principles of
$\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents
($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with
human intentions, environmental dynamics, and self-constraints such as the
limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we
review the current agent research and highlight the neglected factors in
existing agent benchmarks and method candidates. We also conduct
proof-of-concept studies by introducing realistic features to WebShop,
including user profiles to demonstrate intentions, personalized reranking for
complex environmental dynamics, and runtime cost statistics to reflect
self-constraints. We then follow the principles of $\mathbf{UA}^2$ to propose
an initial design of our agent, and benchmark its performance with several
candidate baselines in the retrofitted WebShop. The extensive experimental
results further prove the importance of the principles of $\mathbf{UA}^2$. Our
research sheds light on the next steps of autonomous agent research with
improved general problem-solving abilities.
\\ ( https://arxiv.org/abs/2402.07744 ,  5486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07772
Date: Mon, 12 Feb 2024 16:33:35 GMT   (731kb,D)

Title: End-to-End Learning for Fair Multiobjective Optimization Under
  Uncertainty
Authors: My H Dinh and James Kotary and Ferdinando Fioretto
Categories: cs.AI
\\
  Many decision processes in artificial intelligence and operations research
are modeled by parametric optimization problems whose defining parameters are
unknown and must be inferred from observable data. The Predict-Then-Optimize
(PtO) paradigm in machine learning aims to maximize downstream decision quality
by training the parametric inference model end-to-end with the subsequent
constrained optimization. This requires backpropagation through the
optimization problem using approximation techniques specific to the problem's
form, especially for nondifferentiable linear and mixed-integer programs. This
paper extends the PtO methodology to optimization problems with
nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their
ability to ensure properties of fairness and robustness in decision models.
Through a collection of training techniques and proposed application settings,
it shows how optimization of OWA functions can be effectively integrated with
parametric prediction for fair and robust optimization under uncertainty.
\\ ( https://arxiv.org/abs/2402.07772 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07787
Date: Mon, 12 Feb 2024 16:52:26 GMT   (8194kb,D)

Title: Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment
  Analysis
Authors: Xiaowei Zhao, Yong Zhou, Xiujuan Xu, Yu Liu
Categories: cs.AI
Comments: 8 pages, 4 figures
\\
  Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within
a text to comprehend sentiment information. Previous studies integrated
external knowledge, such as knowledge graphs, to enhance the semantic features
in ABSA models. Recent research has examined the use of Graph Neural Networks
(GNNs) on dependency and constituent trees for syntactic analysis. With the
ongoing development of ABSA, more innovative linguistic and structural features
are being incorporated (e.g. latent graph), but this also introduces complexity
and confusion. As of now, a scalable framework for integrating diverse
linguistic and structural features into ABSA does not exist. This paper
presents the Extensible Multi-Granularity Fusion (EMGF) network, which
integrates information from dependency and constituent syntactic, attention
semantic , and external knowledge graphs. EMGF, equipped with multi-anchor
triplet learning and orthogonal projection, efficiently harnesses the combined
potential of each granularity feature and their synergistic interactions,
resulting in a cumulative effect without additional computational expenses.
Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's
superiority over existing ABSA methods.
\\ ( https://arxiv.org/abs/2402.07787 ,  8194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07799
Date: Mon, 12 Feb 2024 17:03:58 GMT   (4979kb,D)

Title: Generalising Planning Environment Redesign
Authors: Alberto Pozanco, Ramon Fraga Pereira, Daniel Borrajo
Categories: cs.AI
Comments: Paper accepted at AAAI'24
\\
  In Environment Design, one interested party seeks to affect another agent's
decisions by applying changes to the environment. Most research on planning
environment (re)design assumes the interested party's objective is to
facilitate the recognition of goals and plans, and search over the space of
environment modifications to find the minimal set of changes that simplify
those tasks and optimise a particular metric. This search space is usually
intractable, so existing approaches devise metric-dependent pruning techniques
for performing search more efficiently. This results in approaches that are not
able to generalise across different objectives and/or metrics. In this paper,
we argue that the interested party could have objectives and metrics that are
not necessarily related to recognising agents' goals or plans. Thus, to
generalise the task of Planning Environment Redesign, we develop a general
environment redesign approach that is metric-agnostic and leverages recent
research on top-quality planning to efficiently redesign planning environments
according to any interested party's objective and metric. Experiments over a
set of environment redesign benchmarks show that our general approach
outperforms existing approaches when using well-known metrics, such as
facilitating the recognition of goals, as well as its effectiveness when
solving environment redesign tasks that optimise a novel set of different
metrics.
\\ ( https://arxiv.org/abs/2402.07799 ,  4979kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07822
Date: Mon, 12 Feb 2024 17:26:35 GMT   (512kb,D)

Title: Understanding fitness landscapes in morpho-evolution via local optima
  networks
Authors: Sarah L. Thomson, L\'eni K. Le Goff, Emma Hart, Edgar Buchanan
Categories: cs.AI
Comments: Submitted to GECCO-2024
\\
  Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's
design and controller to maximise performance given a task and environment.
Many genetic encodings have been proposed which are capable of representing
design and control. Previous research has provided empirical comparisons
between encodings in terms of their performance with respect to an objective
function and the diversity of designs that are evaluated, however there has
been no attempt to explain the observed findings. We address this by applying
Local Optima Network (LON) analysis to investigate the structure of the fitness
landscapes induced by three different encodings when evolving a robot for a
locomotion task, shedding new light on the ease by which different fitness
landscapes can be traversed by a search process. This is the first time LON
analysis has been applied in the field of ME despite its popularity in
combinatorial optimisation domains; the findings will facilitate design of new
algorithms or operators that are customised to ME landscapes in the future.
\\ ( https://arxiv.org/abs/2402.07822 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07877
Date: Mon, 12 Feb 2024 18:41:55 GMT   (1415kb,D)

Title: WildfireGPT: Tailored Large Language Model for Wildfire Analysis
Authors: Yangxinyu Xie, Tanwi Mallick, Joshua David Bergerson, John K.
  Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B.
  Ross, Yan Feng, Leslie-Anne Levy, Weijie Su
Categories: cs.AI
\\
  The recent advancement of large language models (LLMs) represents a
transformational capability at the frontier of artificial intelligence (AI) and
machine learning (ML). However, LLMs are generalized models, trained on
extensive text corpus, and often struggle to provide context-specific
information, particularly in areas requiring specialized knowledge such as
wildfire details within the broader context of climate change. For
decision-makers and policymakers focused on wildfire resilience and adaptation,
it is crucial to obtain responses that are not only precise but also
domain-specific, rather than generic. To that end, we developed WildfireGPT, a
prototype LLM agent designed to transform user queries into actionable insights
on wildfire risks. We enrich WildfireGPT by providing additional context such
as climate projections and scientific literature to ensure its information is
current, relevant, and scientifically accurate. This enables WildfireGPT to be
an effective tool for delivering detailed, user-specific insights on wildfire
risks to support a diverse set of end users, including researchers, engineers,
urban planners, emergency managers, and infrastructure operators.
\\ ( https://arxiv.org/abs/2402.07877 ,  1415kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07890
Date: Mon, 12 Feb 2024 18:53:20 GMT   (789kb,D)

Title: MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement
  Learning
Authors: Ayesha Siddika Nipu, Siming Liu, Anthony Harris
Categories: cs.AI cs.LG
Comments: 2022 IEEE Conference on Games (CoG)
DOI: 10.1109/CoG51982.2022.9893711
\\
  Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.
\\ ( https://arxiv.org/abs/2402.07890 ,  789kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06733
Date: Fri, 9 Feb 2024 19:09:19 GMT   (7174kb,D)

Title: NICE: To Optimize In-Context Examples or Not?
Authors: Pragya Srivastava, Satvik Golechha, Amit Deshpande, Amit Sharma
Categories: cs.CL cs.AI cs.LG
\\
  Recent works have shown that large language models (LLMs) work remarkably
well on a wide range of tasks through in-context learning and optimization of
in-context examples (ICE). However, most of these studies assume either a fixed
or no instruction provided in the prompt, leading to the apparent consensus
that the optimization of in-context examples is critical for better
performance. We challenge this consensus for instruction-tuned LLMs by
investigating the necessity of optimizing in-context examples when
task-specific instructions are provided, and find that there are tasks for
which various ways of optimizing in-context examples yield diminishing returns.
We introduce a task-specific metric called \metriclong{} (\metric) that
quantifies the learnability of tasks from a given instruction, and provides a
heuristic that helps decide whether to optimize for instructions or ICE for any
new task. On a wide range of tasks and a systematically created instruction set
with gradually added details, we validate our hypothesis empirically by
computing \metric with query-dependent bins of examples, comparing different
instructions with ICE selection methods, and performing label perturbation
experiments. We conclude that tasks can be divided into two broad classes based
on the \metric metric, where the returns on ICE optimization follow predictable
trends when instructions are provided in the prompt.
\\ ( https://arxiv.org/abs/2402.06733 ,  7174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06738
Date: Fri, 9 Feb 2024 19:16:27 GMT   (1373kb,D)

Title: EntGPT: Linking Generative Large Language Models with Knowledge Bases
Authors: Yifan Ding, Amrit Poudel, Qingkai Zeng, Tim Weninger, Balaji
  Veeramani, Sanmitra Bhattacharya
Categories: cs.CL
\\
  The ability of Large Language Models (LLMs) to generate factually correct
output remains relatively unexplored due to the lack of fact-checking and
knowledge grounding during training and inference. In this work, we aim to
address this challenge through the Entity Disambiguation (ED) task. We first
consider prompt engineering, and design a three-step hard-prompting method to
probe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the
prompting method improves the micro-F_1 score of the original vanilla models by
a large margin, on some cases up to 36% and higher, and obtains comparable
performance across 10 datasets when compared to existing methods with SFT. We
further improve the knowledge grounding ability through instruction tuning (IT)
with similar prompts and responses. The instruction-tuned model not only
achieves higher micro-F1 score performance as compared to several baseline
methods on supervised entity disambiguation tasks with an average micro-F_1
improvement of 2.1% over the existing baseline models, but also obtains higher
accuracy on six Question Answering (QA) tasks in the zero-shot setting. Our
methodologies apply to both open- and closed-source LLMs.
\\ ( https://arxiv.org/abs/2402.06738 ,  1373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06766
Date: Fri, 9 Feb 2024 19:59:34 GMT   (108kb,D)

Title: Evaluation Metrics for Text Data Augmentation in NLP
Authors: Marcellus Amadeus and William Alberto Cruz Casta\~neda
Categories: cs.CL cs.AI
\\
  Recent surveys on data augmentation for natural language processing have
reported different techniques and advancements in the field. Several
frameworks, tools, and repositories promote the implementation of text data
augmentation pipelines. However, a lack of evaluation criteria and standards
for method comparison due to different tasks, metrics, datasets, architectures,
and experimental settings makes comparisons meaningless. Also, a lack of
methods unification exists and text data augmentation research would benefit
from unified metrics to compare different augmentation methods. Thus, academics
and the industry endeavor relevant evaluation metrics for text data
augmentation techniques. The contribution of this work is to provide a taxonomy
of evaluation metrics for text augmentation methods and serve as a direction
for a unified benchmark. The proposed taxonomy organizes categories that
include tools for implementation and metrics calculation. Finally, with this
study, we intend to present opportunities to explore the unification and
standardization of text data augmentation metrics.
\\ ( https://arxiv.org/abs/2402.06766 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06853
Date: Sat, 10 Feb 2024 01:18:15 GMT   (1386kb,D)

Title: History, Development, and Principles of Large Language Models-An
  Introductory Survey
Authors: Zhibo Chu, Shiwen Ni, Zichong Wang, Xi Feng, Chengming Li, Xiping Hu,
  Ruifeng Xu, Min Yang, Wenbin Zhang
Categories: cs.CL
\\
  Language models serve as a cornerstone in natural language processing (NLP),
utilizing mathematical methods to generalize language laws and knowledge for
prediction and generation. Over extensive research spanning decades, language
modeling has progressed from initial statistical language models (SLMs) to the
contemporary landscape of large language models (LLMs). Notably, the swift
evolution of LLMs has reached the ability to process, understand, and generate
human-level text. Nevertheless, despite the significant advantages that LLMs
offer in improving both work and personal lives, the limited understanding
among general practitioners about the background and principles of these models
hampers their full potential. Notably, most LLMs reviews focus on specific
aspects and utilize specialized language, posing a challenge for practitioners
lacking relevant background knowledge. In light of this, this survey aims to
present a comprehensible overview of LLMs to assist a broader audience. It
strives to facilitate a comprehensive understanding by exploring the historical
background of language models and tracing their evolution over time. The survey
further investigates the factors influencing the development of LLMs,
emphasizing key contributions. Additionally, it concentrates on elucidating the
underlying principles of LLMs, equipping audiences with essential theoretical
knowledge. The survey also highlights the limitations of existing work and
points out promising future directions.
\\ ( https://arxiv.org/abs/2402.06853 ,  1386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06894
Date: Sat, 10 Feb 2024 07:20:49 GMT   (8299kb,D)

Title: GenTranslate: Large Language Models are Generative Multilingual Speech
  and Machine Translators
Authors: Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang,
  Zhehuai Chen, Eng Siong Chng
Categories: cs.CL cs.AI cs.LG cs.SD eess.AS
Comments: 17 pages. This work is open sourced at:
  https://github.com/YUCHEN005/GenTranslate
\\
  Recent advances in large language models (LLMs) have stepped forward the
development of multilingual speech and machine translation by its reduced
representation errors and incorporated external knowledge. However, both
translation tasks typically utilize beam search decoding and top-1 hypothesis
selection for inference. These techniques struggle to fully exploit the rich
information in the diverse N-best hypotheses, making them less optimal for
translation tasks that require a single, high-quality output sequence. In this
paper, we propose a new generative paradigm for translation tasks, namely
"GenTranslate", which builds upon LLMs to generate better results from the
diverse translation versions in N-best list. Leveraging the rich linguistic
knowledge and strong reasoning abilities of LLMs, our new paradigm can
integrate the rich information in N-best candidates to generate a
higher-quality translation result. Furthermore, to support LLM finetuning, we
build and release a HypoTranslate dataset that contains over 592K
hypotheses-translation pairs in 11 languages. Experiments on various speech and
machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that
our GenTranslate significantly outperforms the state-of-the-art model.
\\ ( https://arxiv.org/abs/2402.06894 ,  8299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06900
Date: Sat, 10 Feb 2024 07:55:27 GMT   (2750kb,D)

Title: Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework
  and Semantic-Based Metric
Authors: Hyukhun Koh, Dohyung Kim, Minwoo Lee, and Kyomin Jung
Categories: cs.CL cs.AI
Comments: 8 page long
\\
  In the pursuit of developing Large Language Models (LLMs) that adhere to
societal standards, it is imperative to discern the existence of toxicity in
the generated text. The majority of existing toxicity metrics rely on encoder
models trained on specific toxicity datasets. However, these encoders are
susceptible to out-of-distribution (OOD) problems and depend on the definition
of toxicity assumed in a dataset. In this paper, we introduce an automatic
robust metric grounded on LLMs to distinguish whether model responses are
toxic. We start by analyzing the toxicity factors, followed by examining the
intrinsic toxic attributes of LLMs to ascertain their suitability as
evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators
(LATTE), on evaluation datasets.The empirical results indicate outstanding
performance in measuring toxicity, improving upon state-of-the-art metrics by
12 points in F1 score without training procedure. We also show that upstream
toxicity has an influence on downstream metrics.
\\ ( https://arxiv.org/abs/2402.06900 ,  2750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06907
Date: Sat, 10 Feb 2024 08:25:30 GMT   (479kb)

Title: Investigating Consistency in Query-Based Meeting Summarization: A
  Comparative Study of Different Embedding Methods
Authors: Chen Jia-Chen (Oscar), Guillem Senabre, Allane Caron
Categories: cs.CL
\\
  With more and more advanced data analysis techniques emerging, people will
expect these techniques to be applied in more complex tasks and solve problems
in our daily lives. Text Summarization is one of famous applications in Natural
Language Processing (NLP) field. It aims to automatically generate summary with
important information based on a given context, which is important when you
have to deal with piles of documents. Summarization techniques can help capture
key points in a short time and bring convenience in works. One of applicable
situation is meeting summarization, especially for important meeting that tend
to be long, complicated, multi-topic and multi-person. Therefore, when people
want to review specific content from a meeting, it will be hard and
time-consuming to find the related spans in the meeting transcript. However,
most of previous works focus on doing summarization for newsletters, scientific
articles...etc, which have a clear document structure and an official format.
For the documents with complex structure like transcripts, we think those works
are not quite suitable for meeting summarization. Besides, the consistency of
summary is another issue common to be discussed in NLP field. To conquer
challenges of meeting summarization, we are inspired by "QMSum: A New Benchmark
for Query-based Multi-domain Meeting Summarization" proposed by Microsoft and
we also propose our Locater model designed to extract relevant spans based on
given transcript and query, which are then summarized by Summarizer model.
Furthermore, we perform a comparative study by applying different word
embedding techniques to improve summary consistency.
\\ ( https://arxiv.org/abs/2402.06907 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06913
Date: Sat, 10 Feb 2024 09:16:56 GMT   (6622kb,D)

Title: TL;DR Progress: Multi-faceted Literature Exploration in Text
  Summarization
Authors: Shahbaz Syed, Khalid Al-Khatib, Martin Potthast
Categories: cs.CL
Comments: EACL 2024 System Demonstration
\\
  This paper presents TL;DR Progress, a new tool for exploring the literature
on neural text summarization. It organizes 514~papers based on a comprehensive
annotation scheme for text summarization approaches and enables fine-grained,
faceted search. Each paper was manually annotated to capture aspects such as
evaluation metrics, quality dimensions, learning paradigms, challenges
addressed, datasets, and document domains. In addition, a succinct indicative
summary is provided for each paper, consisting of automatically extracted
contextual factors, issues, and proposed solutions. The tool is available
online at https://www.tldr-progress.de, a demo video at
https://youtu.be/uCVRGFvXUj8
\\ ( https://arxiv.org/abs/2402.06913 ,  6622kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06925
Date: Sat, 10 Feb 2024 11:14:53 GMT   (145kb,D)

Title: A Thorough Examination of Decoding Methods in the Era of LLMs
Authors: Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu
  Yang, Wai Lam
Categories: cs.CL
\\
  Decoding methods play an indispensable role in converting language models
from next-token predictors into practical task solvers. Prior research on
decoding methods, primarily focusing on task-specific models, may not extend to
the current era of general-purpose large language models (LLMs). Moreover, the
recent influx of decoding strategies has further complicated this landscape.
This paper provides a comprehensive and multifaceted analysis of various
decoding methods within the context of LLMs, evaluating their performance,
robustness to hyperparameter changes, and decoding speeds across a wide range
of tasks, models, and deployment environments. Our findings reveal that
decoding method performance is notably task-dependent and influenced by factors
such as alignment, model size, and quantization. Intriguingly, sensitivity
analysis exposes that certain methods achieve superior performance at the cost
of extensive hyperparameter tuning, highlighting the trade-off between
attaining optimal results and the practicality of implementation in varying
contexts.
\\ ( https://arxiv.org/abs/2402.06925 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06930
Date: Sat, 10 Feb 2024 11:53:48 GMT   (232kb,D)

Title: LiFi: Lightweight Controlled Text Generation with Fine-Grained Control
  Codes
Authors: Chufan Shi, Deng Cai, Yujiu Yang
Categories: cs.CL
\\
  In the rapidly evolving field of text generation, the demand for more precise
control mechanisms has become increasingly apparent. To address this need, we
present a novel methodology, LIFI, which offers a lightweight approach with
fine-grained control for controlled text generation. Unlike previous studies
that train pre-trained language models to follow discrete, categorical, and
exclusive control codes, LIFI learns controlled text generation under the
guidance of continuous, relative, and nonexclusive control codes. These
fine-grained codes are automatically derived from an attribute classifier,
initially trained with a small amount of labeled data and subsequently employed
to label abundant unlabeled data, thus garnering more extensive supervision
signals. Moreover, to achieve efficient control, we incorporate the
fine-grained control codes with adapters, a parameter- and compute-efficient
way to steer a pre-trained language model. We evaluate LIFI on two conventional
tasks -- sentiment control and topic control -- and one newly proposed task --
stylistic novel writing. Comprehensive experimental results validate the
effectiveness of our proposed methods, demonstrating substantial performance
improvements over existing baselines.
\\ ( https://arxiv.org/abs/2402.06930 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06948
Date: Sat, 10 Feb 2024 13:26:14 GMT   (33886kb,D)

Title: Should I try multiple optimizers when fine-tuning pre-trained
  Transformers for NLP tasks? Should I tune their hyperparameters?
Authors: Nefeli Gkouti, Prodromos Malakasiotis, Stavros Toumpis, Ion
  Androutsopoulos
Categories: cs.CL
Comments: Accepted at EACL 2024
\\
  NLP research has explored different neural model architectures and sizes,
datasets, training objectives, and transfer learning techniques. However, the
choice of optimizer during training has not been explored as extensively.
Typically, some variant of Stochastic Gradient Descent (SGD) is employed,
selected among numerous variants, using unclear criteria, often with minimal or
no tuning of the optimizer's hyperparameters. Experimenting with five GLUE
datasets, two models (DistilBERT and DistilRoBERTa), and seven popular
optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound),
we find that when the hyperparameters of the optimizers are tuned, there is no
substantial difference in test performance across the five more elaborate
(adaptive) optimizers, despite differences in training loss. Furthermore,
tuning just the learning rate is in most cases as good as tuning all the
hyperparameters. Hence, we recommend picking any of the best-behaved adaptive
optimizers (e.g., Adam) and tuning only its learning rate. When no
hyperparameter can be tuned, SGD with Momentum is the best choice.
\\ ( https://arxiv.org/abs/2402.06948 ,  33886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06959
Date: Sat, 10 Feb 2024 14:26:42 GMT   (2204kb,D)

Title: SpeechCLIP+: Self-supervised multi-task representation learning for
  speech via CLIP and speech-image data
Authors: Hsuan-Fu Wang, Yi-Jen Shih, Heng-Jui Chang, Layne Berry, Puyuan Peng,
  Hung-yi Lee, Hsin-Min Wang, David Harwath
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to ICASSP 2024, Self-supervision in Audio, Speech, and
  Beyond (SASB) workshop
\\
  The recently proposed visually grounded speech model SpeechCLIP is an
innovative framework that bridges speech and text through images via CLIP
without relying on text transcription. On this basis, this paper introduces two
extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire
(CIF) module to replace a fixed number of CLS tokens in the cascaded
architecture. Second, we propose a new hybrid architecture that merges the
cascaded and parallel architectures of SpeechCLIP into a multi-task learning
framework. Our experimental evaluation is performed on the Flickr8k and
SpokenCOCO datasets. The results show that in the speech keyword extraction
task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded
SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our
hybrid architecture, cascaded task learning boosts the performance of the
parallel branch in image-speech retrieval tasks.
\\ ( https://arxiv.org/abs/2402.06959 ,  2204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06964
Date: Sat, 10 Feb 2024 14:43:08 GMT   (938kb,D)

Title: NLP for Knowledge Discovery and Information Extraction from Energetics
  Corpora
Authors: Francis G. VanGessel, Efrem Perry, Salil Mohan, Oliver M. Barham, Mark
  Cavolowsky
Categories: cs.CL cond-mat.mtrl-sci
\\
  We present a demonstration of the utility of NLP for aiding research into
energetic materials and associated systems. The NLP method enables machine
understanding of textual data, offering an automated route to knowledge
discovery and information extraction from energetics text. We apply three
established unsupervised NLP models: Latent Dirichlet Allocation, Word2Vec, and
the Transformer to a large curated dataset of energetics-related scientific
articles. We demonstrate that each NLP algorithm is capable of identifying
energetic topics and concepts, generating a language model which aligns with
Subject Matter Expert knowledge. Furthermore, we present a document
classification pipeline for energetics text. Our classification pipeline
achieves 59-76\% accuracy depending on the NLP model used, with the highest
performing Transformer model rivaling inter-annotator agreement metrics. The
NLP approaches studied in this work can identify concepts germane to energetics
and therefore hold promise as a tool for accelerating energetics research
efforts and energetics material development.
\\ ( https://arxiv.org/abs/2402.06964 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06967
Date: Sat, 10 Feb 2024 14:52:52 GMT   (365kb,D)

Title: Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning
  Framework for Dialogue
Authors: Jian Wang, Chak Tou Leong, Jiashuo Wang, Dongding Lin, Wenjie Li,
  Xiao-Yong Wei
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  Tuning pretrained language models for dialogue generation has been a
prevalent paradigm for building capable dialogue agents. Yet, traditional
tuning narrowly views dialogue generation as resembling other language
generation tasks, ignoring the role disparities between two speakers and the
multi-round interactive process that dialogues ought to be. Such a manner leads
to unsatisfactory chat consistency of the built agent. In this work, we
emphasize the interactive, communicative nature of dialogue and argue that it
is more feasible to model the speaker roles of agent and user separately,
enabling the agent to adhere to its role consistently. We propose an efficient
Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the
agent and user individually with two adapters built upon large language models,
where they utilize utterances round by round in alternating order and are tuned
via a round-level memory caching mechanism. Extensive experiments demonstrate
that, our framework performs superior to traditional fine-tuning and harbors
the tremendous potential for improving dialogue consistency.
\\ ( https://arxiv.org/abs/2402.06967 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06973
Date: Sat, 10 Feb 2024 15:32:53 GMT   (7817kb,D)

Title: Event-Keyed Summarization
Authors: William Gantt and Alexander Martin and Pavlo Kuchmiichuk and Aaron
  Steven White
Categories: cs.CL cs.AI cs.LG
Comments: ARR short paper (under review)
\\
  We introduce event-keyed summarization (EKS), a novel task that marries
traditional summarization and document-level event extraction, with the goal of
generating a contextualized summary for a specific event, given a document and
an extracted event structure. We introduce a dataset for this task, MUCSUM,
consisting of summaries of all events in the classic MUC-4 dataset, along with
a set of baselines that comprises both pretrained LM standards in the
summarization literature, as well as larger frontier models. We show that
ablations that reduce EKS to traditional summarization or structure-to-text
yield inferior summaries of target events and that MUCSUM is a robust benchmark
for this task. Lastly, we conduct a human evaluation of both reference and
model summaries, and provide some detailed analysis of the results.
\\ ( https://arxiv.org/abs/2402.06973 ,  7817kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07023
Date: Sat, 10 Feb 2024 19:08:28 GMT   (12787kb,D)

Title: Gemini Goes to Med School: Exploring the Capabilities of Multimodal
  Large Language Models on Medical Challenge Problems & Hallucinations
Authors: Ankit Pal, Malaikannan Sankarasubbu
Categories: cs.CL cs.AI cs.CV cs.HC cs.LG
Comments: Preprint version, Under Review
\\
  Large language models have the potential to be valuable in the healthcare
industry, but it's crucial to verify their safety and effectiveness through
rigorous evaluation. For this purpose, we comprehensively evaluated both
open-source LLMs and Google's new multimodal LLM called Gemini across Medical
reasoning, hallucination detection, and Medical Visual Question Answering
tasks. While Gemini showed competence, it lagged behind state-of-the-art models
like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved
an accuracy of 61.45\% on the medical VQA dataset, significantly lower than
GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible
to hallucinations, overconfidence, and knowledge gaps, which indicate risks if
deployed uncritically. We also performed a detailed analysis by medical subject
and test type, providing actionable feedback for developers and clinicians. To
mitigate risks, we applied prompting strategies that improved performance.
Additionally, we facilitated future research and development by releasing a
Python module for medical LLM evaluation and establishing a dedicated
leaderboard on Hugging Face for medical domain LLMs. Python module can be found
at https://github.com/promptslab/RosettaEval
\\ ( https://arxiv.org/abs/2402.07023 ,  12787kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07028
Date: Sat, 10 Feb 2024 19:27:22 GMT   (1137kb,D)

Title: Semi-Supervised Learning for Bilingual Lexicon Induction
Authors: Paul Garnier and Gauthier Guinet
Categories: cs.CL cs.LG
\\
  We consider the problem of aligning two sets of continuous word
representations, corresponding to languages, to a common space in order to
infer a bilingual lexicon. It was recently shown that it is possible to infer
such lexicon, without using any parallel data, by aligning word embeddings
trained on monolingual data. Such line of work is called unsupervised bilingual
induction. By wondering whether it was possible to gain experience in the
progressive learning of several languages, we asked ourselves to what extent we
could integrate the knowledge of a given set of languages when learning a new
one, without having parallel data for the latter. In other words, while keeping
the core problem of unsupervised learning in the latest step, we allowed the
access to other corpora of idioms, hence the name semi-supervised. This led us
to propose a novel formulation, considering the lexicon induction as a ranking
problem for which we used recent tools of this machine learning field. Our
experiments on standard benchmarks, inferring dictionary from English to more
than 20 languages, show that our approach consistently outperforms existing
state of the art benchmark. In addition, we deduce from this new scenario
several relevant conclusions allowing a better understanding of the alignment
phenomenon.
\\ ( https://arxiv.org/abs/2402.07028 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07081
Date: Sun, 11 Feb 2024 01:37:48 GMT   (479kb,D)

Title: Using Large Language Models for Student-Code Guided Test Case Generation
  in Computer Science Education
Authors: Nischal Ashok Kumar, Andrew Lan
Categories: cs.CL cs.SE
Comments: Oral Presentation at AI4ED workshop at AAAI-2024
\\
  In computer science education, test cases are an integral part of programming
assignments since they can be used as assessment items to test students'
programming knowledge and provide personalized feedback on student-written
code. The goal of our work is to propose a fully automated approach for test
case generation that can accurately measure student knowledge, which is
important for two reasons. First, manually constructing test cases requires
expert knowledge and is a labor-intensive process. Second, developing test
cases for students, especially those who are novice programmers, is
significantly different from those oriented toward professional-level software
developers. Therefore, we need an automated process for test case generation to
assess student knowledge and provide feedback. In this work, we propose a large
language model-based approach to automatically generate test cases and show
that they are good measures of student knowledge, using a publicly available
dataset that contains student-written Java code. We also discuss future
research directions centered on using test cases to help students.
\\ ( https://arxiv.org/abs/2402.07081 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07092
Date: Sun, 11 Feb 2024 03:27:22 GMT   (1011kb,D)

Title: Generalizing Conversational Dense Retrieval via LLM-Cognition Data
  Augmentation
Authors: Haonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan Liu, Ziliang Zhao
Categories: cs.CL cs.IR
\\
  Conversational search utilizes muli-turn natural language contexts to
retrieve relevant passages. Existing conversational dense retrieval models
mostly view a conversation as a fixed sequence of questions and responses,
overlooking the severe data sparsity problem -- that is, users can perform a
conversation in various ways, and these alternate conversations are unrecorded.
Consequently, they often struggle to generalize to diverse conversations in
real-world scenarios. In this work, we propose a framework for generalizing
Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug).
ConvAug first generates multi-level augmented conversations to capture the
diverse nature of conversational contexts. Inspired by human cognition, we
devise a cognition-aware process to mitigate the generation of false positives,
false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive
sample filter that selects challenging samples for complex conversations,
thereby giving the model a larger learning space. A contrastive learning
objective is then employed to train a better conversational context encoder.
Extensive experiments conducted on four public datasets, under both normal and
zero-shot settings, demonstrate the effectiveness, generalizability, and
applicability of ConvAug.
\\ ( https://arxiv.org/abs/2402.07092 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07157
Date: Sun, 11 Feb 2024 11:03:04 GMT   (359kb,D)

Title: Natural Language Reinforcement Learning
Authors: Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushiks,
  Yali Du, Ying Wen, Jun Wang
Categories: cs.CL cs.AI cs.LG
Comments: Work in Progress
\\
  Reinforcement Learning (RL) has shown remarkable abilities in learning
policies for decision-making tasks. However, RL is often hindered by issues
such as low sample efficiency, lack of interpretability, and sparse supervision
signals. To tackle these limitations, we take inspiration from the human
learning process and introduce Natural Language Reinforcement Learning (NLRL),
which innovatively combines RL principles with natural language representation.
Specifically, NLRL redefines RL concepts like task objectives, policy, value
function, Bellman equation, and policy iteration in natural language space. We
present how NLRL can be practically implemented with the latest advancements in
large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs
demonstrate the effectiveness, efficiency, and also interpretability of the
NLRL framework.
\\ ( https://arxiv.org/abs/2402.07157 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07179
Date: Sun, 11 Feb 2024 12:25:41 GMT   (1336kb,D)

Title: Prompt Perturbation in Retrieval-Augmented Generation based Large
  Language Models
Authors: Zhibo Hu, Chen Wang, Yanfeng Shu, Helen (Hye-Young) Paik, Liming Zhu
Categories: cs.CL cs.IR
Comments: 12 pages, 9 figures
ACM-class: I.2.7; H.3.3
\\
  The robustness of large language models (LLMs) becomes increasingly important
as their use rapidly grows in a wide range of domains. Retrieval-Augmented
Generation (RAG) is considered as a means to improve the trustworthiness of
text generation from LLMs. However, how the outputs from RAG-based LLMs are
affected by slightly different inputs is not well studied. In this work, we
find that the insertion of even a short prefix to the prompt leads to the
generation of outputs far away from factually correct answers. We
systematically evaluate the effect of such prefixes on RAG by introducing a
novel optimization technique called Gradient Guided Prompt Perturbation (GGPP).
GGPP achieves a high success rate in steering outputs of RAG-based LLMs to
targeted wrong answers. It can also cope with instructions in the prompts
requesting to ignore irrelevant context. We also exploit LLMs' neuron
activation difference between prompts with and without GGPP perturbations to
give a method that improves the robustness of RAG-based LLMs through a highly
effective detector trained on neuron activation triggered by GGPP generated
prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of
our methods.
\\ ( https://arxiv.org/abs/2402.07179 ,  1336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07214
Date: Sun, 11 Feb 2024 14:21:37 GMT   (3164kb,D)

Title: Through the Lens of Split Vote: Exploring Disagreement, Difficulty and
  Calibration in Legal Case Outcome Classification
Authors: Shanshan Xu, T.Y.S.S Santosh, Oana Ichim, Barbara Plank, Matthias
  Grabmair
Categories: cs.CL
\\
  In legal decisions, split votes (SV) occur when judges cannot reach a
unanimous decision, posing a difficulty for lawyers who must navigate diverse
legal arguments and opinions. In high-stakes domains, understanding the
alignment of perceived difficulty between humans and AI systems is crucial to
build trust. However, existing NLP calibration methods focus on a classifier's
awareness of predictive performance, measured against the human majority class,
overlooking inherent human label variation (HLV). This paper explores split
votes as naturally observable human disagreement and value pluralism. We
collect judges' vote distributions from the European Court of Human Rights
(ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with
SV information. We build a taxonomy of disagreement with SV-specific
subcategories. We further assess the alignment of perceived difficulty between
models and humans, as well as confidence- and human-calibration of COC models.
We observe limited alignment with the judge vote distribution. To our
knowledge, this is the first systematic exploration of calibration to human
judgements in legal NLP. Our study underscores the necessity for further
research on measuring and enhancing model calibration considering HLV in legal
decision tasks.
\\ ( https://arxiv.org/abs/2402.07214 ,  3164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07233
Date: Sun, 11 Feb 2024 15:50:35 GMT   (4913kb,D)

Title: TransGPT: Multi-modal Generative Pre-trained Transformer for
  Transportation
Authors: Peng Wang, Xiang Wei, Fangxu Hu and Wenjuan Han
Categories: cs.CL cs.AI
\\
  Natural language processing (NLP) is a key component of intelligent
transportation systems (ITS), but it faces many challenges in the
transportation domain, such as domain-specific knowledge and data, and
multi-modal inputs and outputs. This paper presents TransGPT, a novel
(multi-modal) large language model for the transportation domain, which
consists of two independent variants: TransGPT-SM for single-modal data and
TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal
Transportation dataset (STD) that contains textual data from various sources in
the transportation domain. TransGPT-MM is finetuned on a multi-modal
Transportation dataset (MTD) that we manually collected from three areas of the
transportation domain: driving tests, traffic signs, and landmarks. We evaluate
TransGPT on several benchmark datasets for different tasks in the
transportation domain, and show that it outperforms baseline models on most
tasks. We also showcase the potential applications of TransGPT for traffic
analysis and modeling, such as generating synthetic traffic scenarios,
explaining traffic phenomena, answering traffic-related questions, providing
traffic recommendations, and generating traffic reports. This work advances the
state-of-the-art of NLP in the transportation domain and provides a useful tool
for ITS researchers and practitioners.
\\ ( https://arxiv.org/abs/2402.07233 ,  4913kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07255
Date: Sun, 11 Feb 2024 17:46:33 GMT   (1567kb,D)

Title: American Sign Language Video to Text Translation
Authors: Parsheeta Roy, Ji-Eun Han, Srishti Chouhan, Bhaavanaa Thumu
Categories: cs.CL cs.CV
\\
  Sign language to text is a crucial technology that can break down
communication barriers for individuals with hearing difficulties. We replicate
and try to improve on a recently published study. We evaluate models using BLEU
and rBLEU metrics to ensure translation quality. During our ablation study, we
found that the model's performance is significantly influenced by optimizers,
activation functions, and label smoothing. Further research aims to refine
visual feature capturing, enhance decoder utilization, and integrate
pre-trained decoders for better translation outcomes. Our source code is
available to facilitate replication of our results and encourage future
research.
\\ ( https://arxiv.org/abs/2402.07255 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07262
Date: Sun, 11 Feb 2024 18:09:50 GMT   (4682kb,D)

Title: Low-Resource Counterspeech Generation for Indic Languages: The Case of
  Bengali and Hindi
Authors: Mithun Das, Saurabh Kumar Pandey, Shivansh Sethi, Punyajoy Saha,
  Animesh Mukherjee
Categories: cs.CL cs.HC
Comments: Accepted to the Findings of the ACL: EACL 2024
\\
  With the rise of online abuse, the NLP community has begun investigating the
use of neural architectures to generate counterspeech that can "counter" the
vicious tone of such abusive speech and dilute/ameliorate their rippling effect
over the social network. However, most of the efforts so far have been
primarily focused on English. To bridge the gap for low-resource languages such
as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive
speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs
are in Hindi. We implement several baseline models considering various
interlingual transfer mechanisms with different configurations to generate
suitable counterspeech to set up an effective benchmark. We observe that the
monolingual setup yields the best performance. Further, using synthetic
transfer, language models can generate counterspeech to some extent;
specifically, we notice that transferability is better when languages belong to
the same language family.
\\ ( https://arxiv.org/abs/2402.07262 ,  4682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07271
Date: Sun, 11 Feb 2024 18:27:14 GMT   (1222kb,D)

Title: Previously on the Stories: Recap Snippet Identification for Story
  Reading
Authors: Jiangnan Li, Qiujing Wang, Liyan Xu, Wenjie Pang, Mo Yu, Zheng Lin,
  Weiping Wang, Jie Zhou
Categories: cs.CL
\\
  Similar to the "previously-on" scenes in TV shows, recaps can help book
reading by recalling the readers' memory about the important elements in
previous texts to better understand the ongoing plot. Despite its usefulness,
this application has not been well studied in the NLP community. We propose the
first benchmark on this useful task called Recap Snippet Identification with a
hand-crafted evaluation dataset. Our experiments show that the proposed task is
challenging to PLMs, LLMs, and proposed methods as the task requires a deep
understanding of the plot correlation between snippets.
\\ ( https://arxiv.org/abs/2402.07271 ,  1222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07282
Date: Sun, 11 Feb 2024 19:13:26 GMT   (6082kb,D)

Title: How do Large Language Models Navigate Conflicts between Honesty and
  Helpfulness?
Authors: Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths
Categories: cs.CL cs.AI cs.LG
\\
  In day-to-day communication, people often approximate the truth - for
example, rounding the time or omitting details - in order to be maximally
helpful to the listener. How do large language models (LLMs) handle such
nuanced trade-offs? To address this question, we use psychological models and
experiments designed to characterize human behavior to analyze LLMs. We test a
range of LLMs and explore how optimization for human preferences or
inference-time reasoning affects these trade-offs. We find that reinforcement
learning from human feedback improves both honesty and helpfulness, while
chain-of-thought prompting skews LLMs towards helpfulness over honesty.
Finally, GPT-4 Turbo demonstrates human-like response patterns including
sensitivity to the conversational framing and listener's decision context. Our
findings reveal the conversational values internalized by LLMs and suggest that
even these abstract values can, to a degree, be steered by zero-shot prompting.
\\ ( https://arxiv.org/abs/2402.07282 ,  6082kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07386
Date: Mon, 12 Feb 2024 03:05:54 GMT   (1326kb,D)

Title: Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy
  Induction from Limited Examples
Authors: Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Shangbin Feng, Zhenwen Liang,
  Zhihan Zhang, Meng Jiang
Categories: cs.CL
\\
  Automatic taxonomy induction is crucial for web search, recommendation
systems, and question answering. Manual curation of taxonomies is expensive in
terms of human effort, making automatic taxonomy construction highly desirable.
In this work, we introduce Chain-of-Layer which is an in-context learning
framework designed to induct taxonomies from a given set of entities.
Chain-of-Layer breaks down the task into selecting relevant candidate entities
in each layer and gradually building the taxonomy from top to bottom. To
minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the
hallucinated content generated at each iteration. Through extensive
experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art
performance on four real-world benchmarks.
\\ ( https://arxiv.org/abs/2402.07386 ,  1326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07401
Date: Mon, 12 Feb 2024 04:32:33 GMT   (279kb,D)

Title: Can LLMs Produce Faithful Explanations For Fact-checking? Towards
  Faithful Explainable Fact-Checking via Multi-Agent Debate
Authors: Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling
  Li, Heng Ji
Categories: cs.CL
\\
  Fact-checking research has extensively explored verification but less so the
generation of natural-language explanations, crucial for user trust. While
Large Language Models (LLMs) excel in text generation, their capability for
producing faithful explanations in fact-checking remains underexamined. Our
study investigates LLMs' ability to generate such explanations, finding that
zero-shot prompts often result in unfaithfulness. To address these challenges,
we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging
multiple LLMs as agents with diverse roles in an iterative refining process
aimed at enhancing faithfulness in generated explanations. MADR ensures that
the final explanation undergoes rigorous validation, significantly reducing the
likelihood of unfaithful elements and aligning closely with the provided
evidence. Experimental results demonstrate that MADR significantly improves the
faithfulness of LLM-generated explanations to the evidence, advancing the
credibility and trustworthiness of these explanations.
\\ ( https://arxiv.org/abs/2402.07401 ,  279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07405
Date: Mon, 12 Feb 2024 04:50:31 GMT   (530kb,D)

Title: D\'olares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs
  Between Spanish and English
Authors: Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han,
  Alejandro Lopez-Lira, Xiao-Yang Liu, Sophia Ananiadou, Min Peng, Jimin Huang,
  Qianqian Xie
Categories: cs.CL
Comments: 10 pages, 2 figures
\\
  Despite Spanish's pivotal role in the global finance industry, a pronounced
gap exists in Spanish financial natural language processing (NLP) and
application studies compared to English, especially in the era of large
language models (LLMs). To bridge this gap, we unveil Tois\'on de Oro, the
first bilingual framework that establishes instruction datasets, finetuned
LLMs, and evaluation benchmark for financial LLMs in Spanish joint with
English. We construct a rigorously curated bilingual instruction dataset
including over 144K Spanish and English samples from 15 datasets covering 7
tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual
financial applications. We evaluate our model and existing LLMs using FLARE-ES,
the first comprehensive bilingual evaluation benchmark with 21 datasets
covering 9 tasks. The FLARE-ES benchmark results reveal a significant
multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass
SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic
instruction tuning and leveraging data from diverse linguistic resources,
highlighting the positive impact of cross-linguistic transfer. All our
datasets, models, and benchmarks have been released.
\\ ( https://arxiv.org/abs/2402.07405 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07431
Date: Mon, 12 Feb 2024 06:15:24 GMT   (4615kb,D)

Title: SALAD: Smart AI Language Assistant Daily
Authors: Ragib Amin Nihal
Categories: cs.CL cs.CY
\\
  SALAD is an AI-driven language-learning application designed to help
foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech
recognition, translated audio, vocabulary tracking, grammar explanations, and
songs generated from newly learned words. The app targets beginners and
intermediate learners, aiming to make language acquisition more accessible and
enjoyable. SALAD uses daily translations to enhance fluency and comfort in
communication with native speakers. The primary objectives include effective
Japanese language learning, user engagement, and progress tracking. A survey by
us found that 39% of foreigners in Japan face discomfort in conversations with
Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's
ability to enhance their Japanese language skills. The app uses large language
models, speech recognition, and diffusion models to bridge the language gap and
foster a more inclusive community in Japan.
\\ ( https://arxiv.org/abs/2402.07431 ,  4615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07432
Date: Mon, 12 Feb 2024 06:21:35 GMT   (7708kb,D)

Title: Intrinsic Task-based Evaluation for Referring Expression Generation
Authors: Guanyi Chen, Fahime Same, Kees van Deemter
Categories: cs.CL
\\
  Recently, a human evaluation study of Referring Expression Generation (REG)
models had an unexpected conclusion: on \textsc{webnlg}, Referring Expressions
(REs) generated by the state-of-the-art neural models were not only
indistinguishable from the REs in \textsc{webnlg} but also from the REs
generated by a simple rule-based system. Here, we argue that this limitation
could stem from the use of a purely ratings-based human evaluation (which is a
common practice in Natural Language Generation). To investigate these issues,
we propose an intrinsic task-based evaluation for REG models, in which, in
addition to rating the quality of REs, participants were asked to accomplish
two meta-level tasks. One of these tasks concerns the referential success of
each RE; the other task asks participants to suggest a better alternative for
each RE. The outcomes suggest that, in comparison to previous evaluations, the
new evaluation protocol assesses the performance of each REG model more
comprehensively and makes the participants' ratings more reliable and
discriminable.
\\ ( https://arxiv.org/abs/2402.07432 ,  7708kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07446
Date: Mon, 12 Feb 2024 07:03:14 GMT   (9880kb,D)

Title: Quality Does Matter: A Detailed Look at the Quality and Utility of
  Web-Mined Parallel Corpora
Authors: Surangika Ranathunga, Nisansa de Silva, Menan Velayuthan, Aloka
  Fernando, Charitha Rathnayake
Categories: cs.CL
\\
  We conducted a detailed analysis on the quality of web-mined corpora for two
low-resource languages (making three language pairs, English-Sinhala,
English-Tamil and Sinhala-Tamil). We ranked each corpus according to a
similarity measure and carried out an intrinsic and extrinsic evaluation on
different portions of this ranked corpus. We show that there are significant
quality differences between different portions of web-mined corpora and that
the quality varies across languages and datasets. We also show that, for some
web-mined datasets, Neural Machine Translation (NMT) models trained with their
highest-ranked 25k portion can be on par with human-curated datasets.
\\ ( https://arxiv.org/abs/2402.07446 ,  9880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07448
Date: Mon, 12 Feb 2024 07:11:13 GMT   (324kb)

Title: AraSpider: Democratizing Arabic-to-SQL
Authors: Ahmed Heakl, Youssef Mohamed, and Ahmed B. Zaky
Categories: cs.CL cs.AI cs.DB cs.IR cs.LG
Comments: 11 pages, 4 figures
\\
  This study presents AraSpider, the first Arabic version of the Spider
dataset, aimed at improving natural language processing (NLP) in the
Arabic-speaking community. Four multilingual translation models were tested for
their effectiveness in translating English to Arabic. Additionally, two models
were assessed for their ability to generate SQL queries from Arabic text. The
results showed that using back translation significantly improved the
performance of both ChatGPT 3.5 and SQLCoder models, which are considered top
performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated
high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The
study underscores the importance of incorporating contextual schema and
employing back translation strategies to enhance model performance in Arabic
NLP tasks. Moreover, the provision of detailed methodologies for
reproducibility and translation of the dataset into other languages highlights
the research's commitment to promoting transparency and collaborative knowledge
sharing in the field. Overall, these contributions advance NLP research,
empower Arabic-speaking researchers, and enrich the global discourse on
language comprehension and database interrogation.
\\ ( https://arxiv.org/abs/2402.07448 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07470
Date: Mon, 12 Feb 2024 08:14:03 GMT   (1930kb,D)

Title: Pushing The Limit of LLM Capacity for Text Classification
Authors: Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari,
  Benyou Wang, Jing Qin
Categories: cs.CL
\\
  The value of text classification's future research has encountered challenges
and uncertainties, due to the extraordinary efficacy demonstrated by large
language models (LLMs) across numerous downstream NLP tasks. In this era of
open-ended language modeling, where task boundaries are gradually fading, an
urgent question emerges: have we made significant advances in text
classification under the full benefit of LLMs? To answer this question, we
propose RGPT, an adaptive boosting framework tailored to produce a specialized
text classification LLM by recurrently ensembling a pool of strong base
learners. The base learners are constructed by adaptively adjusting the
distribution of training samples and iteratively fine-tuning LLMs with them.
Such base learners are then ensembled to be a specialized text classification
LLM, by recurrently incorporating the historical predictions from the previous
learners. Through a comprehensive empirical comparison, we show that RGPT
significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by
1.36% on average. Further evaluation experiments show a clear surpassing of
RGPT over human classification.
\\ ( https://arxiv.org/abs/2402.07470 ,  1930kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07513
Date: Mon, 12 Feb 2024 09:35:13 GMT   (754kb,D)

Title: The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese
Authors: Ajinkya Kulkarni, Anna Tokareva, Rameez Qureshi, Miguel Couceiro
Categories: cs.CL cs.AI cs.CY
Comments: EACL-2024 LT-EDI Workshop
\\
  In the field of spoken language understanding, systems like Whisper and
Multilingual Massive Speech (MMS) have shown state-of-the-art performances.
This study is dedicated to a comprehensive exploration of the Whisper and MMS
systems, with a focus on assessing biases in automatic speech recognition (ASR)
inherent to casual conversation speech specific to the Portuguese language. Our
investigation encompasses various categories, including gender, age, skin tone
color, and geo-location. Alongside traditional ASR evaluation metrics such as
Word Error Rate (WER), we have incorporated p-value statistical significance
for gender bias analysis. Furthermore, we extensively examine the impact of
data distribution and empirically show that oversampling techniques alleviate
such stereotypical biases. This research represents a pioneering effort in
quantifying biases in the Portuguese language context through the application
of MMS and Whisper, contributing to a better understanding of ASR systems'
performance in multilingual settings.
\\ ( https://arxiv.org/abs/2402.07513 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07519
Date: Mon, 12 Feb 2024 09:41:00 GMT   (8043kb,D)

Title: MAFIA: Multi-Adapter Fused Inclusive LanguAge Models
Authors: Prachi Jain, Ashutosh Sathe, Varun Gumma, Kabir Ahuja, Sunayana
  Sitaram
Categories: cs.CL cs.CY
Comments: Accepted to EACL 2024
\\
  Pretrained Language Models (PLMs) are widely used in NLP for various tasks.
Recent studies have identified various biases that such models exhibit and have
proposed methods to correct these biases. However, most of the works address a
limited set of bias dimensions independently such as gender, race, or religion.
Moreover, the methods typically involve finetuning the full model to maintain
the performance on the downstream task. In this work, we aim to modularly
debias a pretrained language model across multiple dimensions. Previous works
extensively explored debiasing PLMs using limited US-centric counterfactual
data augmentation (CDA). We use structured knowledge and a large generative
model to build a diverse CDA across multiple bias dimensions in a
semi-automated way. We highlight how existing debiasing methods do not consider
interactions between multiple societal biases and propose a debiasing model
that exploits the synergy amongst various societal biases and enables
multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks
and languages demonstrates the efficacy of our approach.
\\ ( https://arxiv.org/abs/2402.07519 ,  8043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07543
Date: Mon, 12 Feb 2024 10:11:50 GMT   (2114kb,D)

Title: Show Me How It's Done: The Role of Explanations in Fine-Tuning Language
  Models
Authors: Mohamad Ballout, Ulf Krumnack, Gunther Heidemann and Kai-Uwe
  Kuehnberger
Categories: cs.CL cs.AI cs.LG
\\
  Our research demonstrates the significant benefits of using fine-tuning with
explanations to enhance the performance of language models. Unlike prompting,
which maintains the model's parameters, fine-tuning allows the model to learn
and update its parameters during a training phase. In this study, we applied
fine-tuning to various sized language models using data that contained
explanations of the output rather than merely presenting the answers. We found
that even smaller language models with as few as 60 million parameters
benefited substantially from this approach. Interestingly, our results
indicated that the detailed explanations were more beneficial to smaller models
than larger ones, with the latter gaining nearly the same advantage from any
form of explanation, irrespective of its length. Additionally, we demonstrate
that the inclusion of explanations enables the models to solve tasks that they
were not able to solve without explanations. Lastly, we argue that despite the
challenging nature of adding explanations, samples that contain explanations
not only reduce the volume of data required for training but also promote a
more effective generalization by the model. In essence, our findings suggest
that fine-tuning with explanations significantly bolsters the performance of
large language models.
\\ ( https://arxiv.org/abs/2402.07543 ,  2114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07577
Date: Mon, 12 Feb 2024 11:18:32 GMT   (406kb,D)

Title: Topic Modeling as Multi-Objective Contrastive Optimization
Authors: Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Cong-Duy T Nguyen, See-Kiong
  Ng, Anh Tuan Luu
Categories: cs.CL
Comments: Accepted at ICLR 2024 (poster)
\\
  Recent representation learning approaches enhance neural topic models by
optimizing the weighted linear combination of the evidence lower bound (ELBO)
of the log-likelihood and the contrastive learning objective that contrasts
pairs of input documents. However, document-level contrastive learning might
capture low-level mutual information, such as word ratio, which disturbs topic
modeling. Moreover, there is a potential conflict between the ELBO loss that
memorizes input details for better reconstruction quality, and the contrastive
loss which attempts to learn topic representations that generalize among input
documents. To address these issues, we first introduce a novel contrastive
learning method oriented towards sets of topic vectors to capture useful
semantics that are shared among a set of input documents. Secondly, we
explicitly cast contrastive topic modeling as a gradient-based multi-objective
optimization problem, with the goal of achieving a Pareto stationary solution
that balances the trade-off between the ELBO and the contrastive objective.
Extensive experiments demonstrate that our framework consistently produces
higher-performing neural topic models in terms of topic coherence, topic
diversity, and downstream performance.
\\ ( https://arxiv.org/abs/2402.07577 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07610
Date: Mon, 12 Feb 2024 12:30:42 GMT   (3387kb,D)

Title: Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
Authors: Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang,
  Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao
Categories: cs.CL cs.AI
\\
  Self-alignment is an effective way to reduce the cost of human annotation
while ensuring promising model capability. However, most current methods
complete the data collection and training steps in a single round, which may
overlook the continuously improving ability of self-aligned models. This gives
rise to a key query: What if we do multi-time bootstrapping self-alignment?
Does this strategy enhance model performance or lead to rapid degradation? In
this paper, our pioneering exploration delves into the impact of bootstrapping
self-alignment on large language models. Our findings reveal that bootstrapping
self-alignment markedly surpasses the single-round approach, by guaranteeing
data diversity from in-context learning. To further exploit the capabilities of
bootstrapping, we investigate and adjust the training order of data, which
yields improved performance of the model. Drawing on these findings, we propose
Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
few-shot ability to boost zero or one-shot performance. Based on easy-to-hard
training recipe, we propose SOFT+ which further boost self-alignment's
performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across
various classification and generation tasks, highlighting the potential of
bootstrapping self-alignment on continually enhancing model alignment
performance.
\\ ( https://arxiv.org/abs/2402.07610 ,  3387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07616
Date: Mon, 12 Feb 2024 12:48:02 GMT   (7774kb,D)

Title: Anchor-based Large Language Models
Authors: Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang
Categories: cs.CL cs.AI
Comments: 13 pages. Work was done when Jianhui Pang and Fanghua Ye were
  interning at Tencent AI Lab. Longyue Wang is the corresponding author
\\
  Large language models (LLMs) predominantly employ decoder-only transformer
architectures, necessitating the retention of keys/values information for
historical tokens to provide contextual information and avoid redundant
computation. However, the substantial size and parameter volume of these LLMs
require massive GPU memory. This memory demand increases with the length of the
input text, leading to an urgent need for more efficient methods of information
storage and processing. This study introduces the Anchor-based LLM (AnLLM),
which utilizes an innovative anchor-based self-attention network (AnSAN) and
also an anchor-based inference strategy. This approach enables LLMs to compress
sequence information into an anchor token, reducing the keys/values cache and
enhancing inference efficiency. Experiments show that the AnLLM maintains
comparable accuracy with up to 99% keys/values cache reduction and up to 3.5
times faster inference. Despite a minor compromise in accuracy, the AnLLM
significantly improves computational efficiency and resource utilization,
demonstrating the potential of the anchor-based attention approach in the
context of LLMs for real-time inference in practical applications.
\\ ( https://arxiv.org/abs/2402.07616 ,  7774kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07625
Date: Mon, 12 Feb 2024 13:09:21 GMT   (177kb,D)

Title: AutoMathText: Autonomous Data Selection with Language Models for
  Mathematical Texts
Authors: Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao
Categories: cs.CL cs.AI cs.LG
\\
  To improve language models' proficiency in mathematical reasoning via
continual pretraining, we introduce a novel strategy that leverages base
language models for autonomous data selection. Departing from conventional
supervised fine-tuning or trained classifiers with human-annotated data, our
approach utilizes meta-prompted language models as zero-shot verifiers to
autonomously evaluate and select high-quality mathematical content, and we
release the curated open-source AutoMathText dataset encompassing over 200GB of
data. To demonstrate the efficacy of our method, we continuously pretrained a
7B-parameter Mistral language model on the AutoMathText dataset, achieving
substantial improvements in downstream performance on the MATH dataset with a
token amount reduced by orders of magnitude compared to previous continuous
pretraining works. Our method showcases a 2 times increase in pretraining token
efficiency compared to baselines, underscoring the potential of our approach in
enhancing models' mathematical reasoning capabilities. The AutoMathText dataset
is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code
is available at https://github.com/yifanzhang-pro/AutoMathText.
\\ ( https://arxiv.org/abs/2402.07625 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07645
Date: Mon, 12 Feb 2024 13:34:33 GMT   (1767kb,D)

Title: Detecting the Clinical Features of Difficult-to-Treat Depression using
  Synthetic Data from Large Language Models
Authors: Isabelle Lorge, Dan W. Joyce, Niall Taylor, Alejo Nevado-Holgado,
  Andrea Cipriani, Andrey Kormilitzin
Categories: cs.CL
\\
  Difficult-to-treat depression (DTD) has been proposed as a broader and more
clinically comprehensive perspective on a person's depressive disorder where
despite treatment, they continue to experience significant burden. We sought to
develop a Large Language Model (LLM)-based tool capable of interrogating
routinely-collected, narrative (free-text) electronic health record (EHR) data
to locate published prognostic factors that capture the clinical syndrome of
DTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a
Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction
model. The resulting model is then able to extract and label spans related to a
variety of relevant positive and negative factors in real clinical data (i.e.
spans of text that increase or decrease the likelihood of a patient matching
the DTD syndrome). We show it is possible to obtain good overall performance
(0.70 F1 across polarity) on real clinical data on a set of as many as 20
different factors, and high performance (0.85 F1 with 0.95 precision) on a
subset of important DTD factors such as history of abuse, family history of
affective disorder, illness severity and suicidality by training the model
exclusively on synthetic data. Our results show promise for future healthcare
applications especially in applications where traditionally, highly
confidential medical data and human-expert annotation would normally be
required.
\\ ( https://arxiv.org/abs/2402.07645 ,  1767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07658
Date: Mon, 12 Feb 2024 14:01:12 GMT   (4102kb,D)

Title: The Sound of Healthcare: Improving Medical Transcription ASR Accuracy
  with Large Language Models
Authors: Ayo Adedeji, Sarita Joshi, Brendan Doohan
Categories: cs.CL cs.SD eess.AS
Comments: 31 pages, 17 figures
\\
  In the rapidly evolving landscape of medical documentation, transcribing
clinical dialogues accurately is increasingly paramount. This study explores
the potential of Large Language Models (LLMs) to enhance the accuracy of
Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing
the PriMock57 dataset, which encompasses a diverse range of primary care
consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our
research is multifaceted, focusing on improvements in general Word Error Rate
(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential
medical terms, and speaker diarization accuracy. Additionally, we assess the
role of LLM post-processing in improving semantic textual similarity, thereby
preserving the contextual integrity of clinical dialogues. Through a series of
experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)
prompting techniques in enhancing diarization and correction accuracy. Our
findings demonstrate that LLMs, particularly through CoT prompting, not only
improve the diarization accuracy of existing ASR systems but also achieve
state-of-the-art performance in this domain. This improvement extends to more
accurately capturing medical concepts and enhancing the overall semantic
coherence of the transcribed dialogues. These findings illustrate the dual role
of LLMs in augmenting ASR outputs and independently excelling in transcription
tasks, holding significant promise for transforming medical ASR systems and
leading to more accurate and reliable patient records in healthcare settings.
\\ ( https://arxiv.org/abs/2402.07658 ,  4102kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07681
Date: Mon, 12 Feb 2024 14:40:54 GMT   (420kb)

Title: Large Language Models "Ad Referendum": How Good Are They at Machine
  Translation in the Legal Domain?
Authors: Vicent Briva-Iglesias, Joao Lucas Cavalheiro Camargo, Gokhan Dogru
Categories: cs.CL cs.AI
\\
  This study evaluates the machine translation (MT) quality of two
state-of-the-art large language models (LLMs) against a tradition-al neural
machine translation (NMT) system across four language pairs in the legal
domain. It combines automatic evaluation met-rics (AEMs) and human evaluation
(HE) by professional transla-tors to assess translation ranking, fluency and
adequacy. The re-sults indicate that while Google Translate generally
outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4,
comparably or slightly better in terms of producing contextually adequate and
fluent translations. This discrepancy suggests LLMs' potential in handling
specialized legal terminology and context, highlighting the importance of human
evaluation methods in assessing MT quality. The study underscores the evolving
capabil-ities of LLMs in specialized domains and calls for reevaluation of
traditional AEMs to better capture the nuances of LLM-generated translations.
\\ ( https://arxiv.org/abs/2402.07681 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07682
Date: Mon, 12 Feb 2024 14:42:33 GMT   (18kb,D)

Title: Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing
Authors: Marie Candito
Categories: cs.CL
Journal-ref: Findings of the Association for Computational Linguistics: ACL
  2022, pp. 2422-2429
DOI: 10.18653/v1/2022.findings-acl.190
\\
  The biaffine parser of Dozat and Manning (2017) was successfully extended to
semantic dependency parsing (SDP) (Dozat and Manning, 2018). Its performance on
graphs is surprisingly high given that, without the constraint of producing a
tree, all arcs for a given sentence are predicted independently from each other
(modulo a shared representation of tokens). To circumvent such an independence
of decision, while retaining the O(n^2) complexity and highly parallelizable
architecture, we propose to use simple auxiliary tasks that introduce some form
of interdependence between arcs. Experiments on the three English acyclic
datasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep
syntactic cyclic graphs (Ribeyre et al., 2014) show modest but systematic
performance gains on a near state-of-the-art baseline using transformer-based
contextualized representations. This provides a simple and robust method to
boost SDP performance.
\\ ( https://arxiv.org/abs/2402.07682 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07689
Date: Mon, 12 Feb 2024 14:53:37 GMT   (815kb,D)

Title: OrderBkd: Textual backdoor attack through repositioning
Authors: Irina Alekseevskaia and Konstantin Arkhipenko
Categories: cs.CL cs.AI
\\
  The use of third-party datasets and pre-trained machine learning models poses
a threat to NLP systems due to possibility of hidden backdoor attacks. Existing
attacks involve poisoning the data samples such as insertion of tokens or
sentence paraphrasing, which either alter the semantics of the original texts
or can be detected. Our main difference from the previous work is that we use
the reposition of a two words in a sentence as a trigger. By designing and
applying specific part-of-speech (POS) based rules for selecting these tokens,
we maintain high attack success rate on SST-2 and AG classification datasets
while outperforming existing attacks in terms of perplexity and semantic
similarity to the clean samples. In addition, we show the robustness of our
attack to the ONION defense method. All the code and data for the paper can be
obtained at https://github.com/alekseevskaia/OrderBkd.
\\ ( https://arxiv.org/abs/2402.07689 ,  815kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07726
Date: Mon, 12 Feb 2024 15:39:05 GMT   (1729kb,D)

Title: Unsupervised Sign Language Translation and Generation
Authors: Zhengsheng Guo, Zhiwei He, Wenxiang Jiao, Xing Wang, Rui Wang, Kehai
  Chen, Zhaopeng Tu, Yong Xu, Min Zhang
Categories: cs.CL
\\
  Motivated by the success of unsupervised neural machine translation (UNMT),
we introduce an unsupervised sign language translation and generation network
(USLNet), which learns from abundant single-modality (text and video) data
without parallel sign language data. USLNet comprises two main components:
single-modality reconstruction modules (text and video) that rebuild the input
from its noisy version in the same modality and cross-modality back-translation
modules (text-video-text and video-text-video) that reconstruct the input from
its noisy version in the different modality using back-translation
procedure.Unlike the single-modality back-translation procedure in text-based
UNMT, USLNet faces the cross-modality discrepancy in feature representation, in
which the length and the feature dimension mismatch between text and video
sequences. We propose a sliding window method to address the issues of aligning
variable-length text with video sequences. To our knowledge, USLNet is the
first unsupervised sign language translation and generation model capable of
generating both natural language text and sign language video in a unified
manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL)
and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet
achieves competitive results compared to supervised baseline models, indicating
its effectiveness in sign language translation and generation.
\\ ( https://arxiv.org/abs/2402.07726 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07742
Date: Mon, 12 Feb 2024 16:04:01 GMT   (17606kb,D)

Title: Asking Multimodal Clarifying Questions in Mixed-Initiative
  Conversational Search
Authors: Yifei Yuan, Clemencia Siro, Mohammad Aliannejadi, Maarten de Rijke,
  Wai Lam
Categories: cs.CL cs.CV
Comments: Accepted to WWW24
\\
  In mixed-initiative conversational search systems, clarifying questions are
used to help users who struggle to express their intentions in a single query.
These questions aim to uncover user's information needs and resolve query
ambiguities. We hypothesize that in scenarios where multimodal information is
pertinent, the clarification process can be improved by using non-textual
information. Therefore, we propose to add images to clarifying questions and
formulate the novel task of asking multimodal clarifying questions in
open-domain, mixed-initiative conversational search systems. To facilitate
research into this task, we collect a dataset named Melon that contains over 4k
multimodal clarifying questions, enriched with over 14k images. We also propose
a multimodal query clarification model named Marto and adopt a prompt-based,
generative fine-tuning strategy to perform the training of different stages
with different prompts. Several analyses are conducted to understand the
importance of multimodal contents during the query clarification phase.
Experimental results indicate that the addition of images leads to significant
improvements of up to 90% in retrieval performance when selecting the relevant
images. Extensive analyses are also performed to show the superiority of Marto
compared with discriminative baselines in terms of effectiveness and
efficiency.
\\ ( https://arxiv.org/abs/2402.07742 ,  17606kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07754
Date: Mon, 12 Feb 2024 16:23:28 GMT   (1338kb,D)

Title: Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language
  Models
Authors: Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han
  Shi, Chuan Wu, Zhenguo Li, Wei Bi, Lingpeng Kong
Categories: cs.CL cs.AI cs.LG
\\
  Diffusion models have gained attention in text processing, offering many
potential advantages over traditional autoregressive models. This work explores
the integration of diffusion models and Chain-of-Thought (CoT), a
well-established technique to improve the reasoning ability in autoregressive
language models. We propose Diffusion-of-Thought (DoT), allowing reasoning
steps to diffuse over time through the diffusion process. In contrast to
traditional autoregressive language models that make decisions in a
left-to-right, token-by-token manner, DoT offers more flexibility in the
trade-off between computation and reasoning performance. Our experimental
results demonstrate the effectiveness of DoT in multi-digit multiplication and
grade school math problems. Additionally, DoT showcases promising
self-correction abilities and benefits from existing reasoning-enhancing
techniques like self-consistency decoding. Our findings contribute to the
understanding and development of reasoning capabilities in diffusion language
models.
\\ ( https://arxiv.org/abs/2402.07754 ,  1338kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07767
Date: Mon, 12 Feb 2024 16:30:41 GMT   (194kb)

Title: Text Detoxification as Style Transfer in English and Hindi
Authors: Sourabrata Mukherjee, Akanksha Bansal, Atul Kr. Ojha, John P. McCrae,
  Ond\v{r}ej Du\v{s}ek
Categories: cs.CL
Comments: Accepted and presented at the 20th International Conference on
  Natural Language Processing (ICON-2023) during December 14-17, 2023
\\
  This paper focuses on text detoxification, i.e., automatically converting
toxic text into non-toxic text. This task contributes to safer and more
respectful online communication and can be considered a Text Style Transfer
(TST) task, where the text style changes while its content is preserved. We
present three approaches: knowledge transfer from a similar task, multi-task
learning approach, combining sequence-to-sequence modeling with various
toxicity classification tasks, and, delete and reconstruct approach. To support
our research, we utilize a dataset provided by Dementieva et al.(2021), which
contains multiple versions of detoxified texts corresponding to toxic texts. In
our experiments, we selected the best variants through expert human annotators,
creating a dataset where each toxic sentence is paired with a single,
appropriate detoxified version. Additionally, we introduced a small Hindi
parallel dataset, aligning with a part of the English dataset, suitable for
evaluation purposes. Our results demonstrate that our approach effectively
balances text detoxication while preserving the actual content and maintaining
fluency.
\\ ( https://arxiv.org/abs/2402.07767 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07776
Date: Mon, 12 Feb 2024 16:41:54 GMT   (584kb,D)

Title: TELLER: A Trustworthy Framework for Explainable, Generalizable and
  Controllable Fake News Detection
Authors: Hui Liu, Wenya Wang, Haoru Li, Haoliang Li
Categories: cs.CL
Comments: 28 pages, 2 figures, 16 tables
\\
  The proliferation of fake news has emerged as a severe societal problem,
raising significant interest from industry and academia. While existing
deep-learning based methods have made progress in detecting fake news
accurately, their reliability may be compromised caused by the non-transparent
reasoning processes, poor generalization abilities and inherent risks of
integration with large language models (LLMs). To address this challenge, we
propose {\methodname}, a novel framework for trustworthy fake news detection
that prioritizes explainability, generalizability and controllability of
models. This is achieved via a dual-system framework that integrates cognition
and decision systems, adhering to the principles above. The cognition system
harnesses human expertise to generate logical predicates, which guide LLMs in
generating human-readable logic atoms. Meanwhile, the decision system deduces
generalizable logic rules to aggregate these atoms, enabling the identification
of the truthfulness of the input news across diverse domains and enhancing
transparency in the decision-making process. Finally, we present comprehensive
evaluation results on four datasets, demonstrating the feasibility and
trustworthiness of our proposed framework. Our implementation is available at
\url{https://github.com/less-and-less-bugs/Trust_TELLER}.
\\ ( https://arxiv.org/abs/2402.07776 ,  584kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07788
Date: Mon, 12 Feb 2024 16:54:22 GMT   (2948kb,D)

Title: Multi-Intent Attribute-Aware Text Matching in Searching
Authors: Mingzhe Li, Xiuying Chen, Jing Xiang, Qishen Zhang, Changsheng Ma,
  Chenchen Dai, Jinxiong Chang, Zhongyi Liu, Guannan Zhang
Categories: cs.CL
Comments: 9 pages
\\
  Text matching systems have become a fundamental service in most searching
platforms. For instance, they are responsible for matching user queries to
relevant candidate items, or rewriting the user-input query to a pre-selected
high-performing one for a better search experience. In practice, both the
queries and items often contain multiple attributes, such as the category of
the item and the location mentioned in the query, which represent condensed key
information that is helpful for matching. However, most of the existing works
downplay the effectiveness of attributes by integrating them into text
representations as supplementary information. Hence, in this work, we focus on
exploring the relationship between the attributes from two sides. Since
attributes from two ends are often not aligned in terms of number and type, we
propose to exploit the benefit of attributes by multiple-intent modeling. The
intents extracted from attributes summarize the diverse needs of queries and
provide rich content of items, which are more refined and abstract, and can be
aligned for paired inputs. Concretely, we propose a multi-intent
attribute-aware matching model (MIM), which consists of three main components:
attribute-aware encoder, multi-intent modeling, and intent-aware matching. In
the attribute-aware encoder, the text and attributes are weighted and processed
through a scaled attention mechanism with regard to the attributes' importance.
Afterward, the multi-intent modeling extracts intents from two ends and aligns
them. Herein, we come up with a distribution loss to ensure the learned intents
are diverse but concentrated, and a kullback-leibler divergence loss that
aligns the learned intents. Finally, in the intent-aware matching, the intents
are evaluated by a self-supervised masking task, and then incorporated to
output the final matching result.
\\ ( https://arxiv.org/abs/2402.07788 ,  2948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07812
Date: Mon, 12 Feb 2024 17:17:50 GMT   (1326kb,D)

Title: Retrieval-Augmented Thought Process as Sequential Decision Making
Authors: Thomas Pouplin, Hao Sun, Samuel Holt, Mihaela van der Schaar
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: 17 pages, 18 figures
ACM-class: H.3.3; I.2.6; I.2.7; I.2.8
\\
  Large Language Models (LLMs) have demonstrated their strong ability to assist
people and show "sparks of intelligence". However, several open challenges
hinder their wider application: such as concerns over privacy, tendencies to
produce hallucinations, and difficulties in handling long contexts. In this
work, we address those challenges by introducing the Retrieval-Augmented
Thought Process (RATP). Given access to external knowledge, RATP formulates the
thought generation of LLMs as a multiple-step decision process. To optimize
such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a
Q-value estimator that permits cost-efficient inference. In addressing the task
of question-answering with private data, where ethical and security concerns
limit LLM training methods, RATP achieves a 50% improvement over existing
in-context retrieval-augmented language models.
\\ ( https://arxiv.org/abs/2402.07812 ,  1326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07817
Date: Mon, 12 Feb 2024 17:22:42 GMT   (137kb,D)

Title: Injecting Wiktionary to improve token-level contextual representations
  using contrastive learning
Authors: Anna Mosolova, Marie Candito, Carlos Ramisch
Categories: cs.CL
Comments: Accepted to EACL 2024 (Main)
\\
  While static word embeddings are blind to context, for lexical semantics
tasks context is rather too present in contextual word embeddings, vectors of
same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning
pre-trained language models (PLMs) using contrastive learning was proposed,
leveraging automatically self-augmented examples (Liu et al., 2021b). In this
paper, we investigate how to inject a lexicon as an alternative source of
supervision, using the English Wiktionary. We also test how dimensionality
reduction impacts the resulting contextual word embeddings. We evaluate our
approach on the Word-In-Context (WiC) task, in the unsupervised setting (not
using the training set). We achieve new SoTA result on the original WiC test
set. We also propose two new WiC test sets for which we show that our
fine-tuning method achieves substantial improvements. We also observe
improvements, although modest, for the semantic frame induction task. Although
we experimented on English to allow comparison with related work, our method is
adaptable to the many languages for which large Wiktionaries exist.
\\ ( https://arxiv.org/abs/2402.07817 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07827
Date: Mon, 12 Feb 2024 17:34:13 GMT   (1797kb,D)

Title: Aya Model: An Instruction Finetuned Open-Access Multilingual Language
  Model
Authors: Ahmet \"Ust\"un, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel
  D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr
  Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff,
  Marzieh Fadaee, Julia Kreutzer, Sara Hooker
Categories: cs.CL
\\
  Recent breakthroughs in large language models (LLMs) have centered around a
handful of data-rich languages. What does it take to broaden access to
breakthroughs beyond first-class citizen languages? Our work introduces Aya, a
massively multilingual generative language model that follows instructions in
101 languages of which over 50% are considered as lower-resourced. Aya
outperforms mT0 and BLOOMZ on the majority of tasks while covering double the
number of languages. We introduce extensive new evaluation suites that broaden
the state-of-art for multilingual eval across 99 languages -- including
discriminative and generative tasks, human evaluation, and simulated win rates
that cover both held-out tasks and in-distribution performance. Furthermore, we
conduct detailed investigations on the optimal finetuning mixture composition,
data pruning, as well as the toxicity, bias, and safety of our models. We
open-source our instruction datasets and our model at
https://hf.co/CohereForAI/aya-101
\\ ( https://arxiv.org/abs/2402.07827 ,  1797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07841
Date: Mon, 12 Feb 2024 17:52:05 GMT   (4202kb,D)

Title: Do Membership Inference Attacks Work on Large Language Models?
Authors: Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia
  Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh
  Hajishirzi
Categories: cs.CL
\\
  Membership inference attacks (MIAs) attempt to predict whether a particular
datapoint is a member of a target model's training data. Despite extensive
research on traditional machine learning models, there has been limited work
studying MIA on the pre-training data of large language models (LLMs). We
perform a large-scale evaluation of MIAs over a suite of language models (LMs)
trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs
barely outperform random guessing for most settings across varying LLM sizes
and domains. Our further analyses reveal that this poor performance can be
attributed to (1) the combination of a large dataset and few training
iterations, and (2) an inherently fuzzy boundary between members and
non-members. We identify specific settings where LLMs have been shown to be
vulnerable to membership inference and show that the apparent success in such
settings can be attributed to a distribution shift, such as when members and
non-members are drawn from the seemingly identical domain but with different
temporal ranges. We release our code and data as a unified benchmark package
that includes all existing MIAs, supporting future work.
\\ ( https://arxiv.org/abs/2402.07841 ,  4202kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07859
Date: Mon, 12 Feb 2024 18:10:17 GMT   (1886kb,D)

Title: Lissard: Long and Simple Sequential Reasoning Datasets
Authors: Mirelle Bueno, Roberto Lotufo, and Rodrigo Nogueira
Categories: cs.CL cs.AI
\\
  Language models are now capable of solving tasks that require dealing with
long sequences consisting of hundreds of thousands of tokens. However, they
often fail on tasks that require repetitive use of simple rules, even on
sequences that are much shorter than those seen during training. For example,
state-of-the-art LLMs can find common items in two lists with up to 20 items
but fail when lists have 80 items. In this paper, we introduce Lissard, a
benchmark comprising seven tasks whose goal is to assess the ability of models
to process and generate wide-range sequence lengths, requiring repetitive
procedural execution. Our evaluation of open-source (Mistral-7B and
Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent
decline in performance across all models as the complexity of the sequence
increases. The datasets and code are available at
https://github.com/unicamp-dl/Lissard
\\ ( https://arxiv.org/abs/2402.07859 ,  1886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07891
Date: Mon, 12 Feb 2024 18:54:02 GMT   (2481kb,D)

Title: Label-Efficient Model Selection for Text Generation
Authors: Shir Ashury-Tahan, Benjamin Sznajder, Leshem Choshen, Liat Ein-Dor,
  Eyal Shnarch, Ariel Gera
Categories: cs.CL cs.LG
\\
  Model selection for a given target task can be costly, as it may entail
extensive annotation of the quality of outputs of different models. We
introduce DiffUse, an efficient method to make an informed decision between
candidate text generation models. DiffUse reduces the required amount of
preference annotations, thus saving valuable time and resources in performing
evaluation. DiffUse intelligently selects instances by clustering embeddings
that represent the semantic differences between model outputs. Thus, it is able
to identify a subset of examples that are more informative for preference
decisions. Our method is model-agnostic, and can be applied to any text
generation model. Moreover, we propose a practical iterative approach for
dynamically determining how many instances to annotate. In a series of
experiments over hundreds of model pairs, we demonstrate that DiffUse can
dramatically reduce the required number of annotations -- by up to 75% -- while
maintaining high evaluation reliability.
\\ ( https://arxiv.org/abs/2402.07891 ,  2481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07896
Date: Mon, 12 Feb 2024 18:57:46 GMT   (3134kb,D)

Title: Suppressing Pink Elephants with Direct Principle Feedback
Authors: Louis Castricato, Nathan Lile, Suraj Anand, Hailey Schoelkopf,
  Siddharth Verma, Stella Biderman
Categories: cs.CL
Comments: 8 pages, 6 figures
\\
  Existing methods for controlling language models, such as RLHF and
Constitutional AI, involve determining which LLM behaviors are desirable and
training them into a language model. However, in many cases, it is desirable
for LLMs to be controllable \textit{at inference time}, so that they can be
used in multiple contexts with diverse needs. We illustrate this with the
\textbf{Pink Elephant Problem}: instructing an LLM to avoid discussing a
certain entity (a ``Pink Elephant''), and instead discuss a preferred entity
(``Grey Elephant''). We apply a novel simplification of Constitutional AI,
\textbf{Direct Principle Feedback}, which skips the ranking of responses and
uses DPO directly on critiques and revisions. Our results show that after DPF
fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2
model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and
performs as well as GPT-4 in on our curated test set assessing the Pink
Elephant Problem.
\\ ( https://arxiv.org/abs/2402.07896 ,  3134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07899
Date: Mon, 12 Feb 2024 18:58:58 GMT   (7745kb,D)

Title: A systematic investigation of learnability from single child linguistic
  input
Authors: Yulu Qin, Wentao Wang and Brenden M. Lake
Categories: cs.CL cs.LG
Comments: 8 pages; 6 figures; Submitted to CogSci 2024
\\
  Language models (LMs) have demonstrated remarkable proficiency in generating
linguistically coherent text, sparking discussions about their relevance to
understanding human language learnability. However, a significant gap exists
between the training data for these models and the linguistic input a child
receives. LMs are typically trained on data that is orders of magnitude larger
and fundamentally different from child-directed speech (Warstadt and Bowman,
2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our
research focuses on training LMs on subsets of a single child's linguistic
input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in
this setting can form syntactic and semantic word clusters and develop
sensitivity to certain linguistic phenomena, but they only considered LSTMs and
simpler neural networks trained from just one single-child dataset. Here, to
examine the robustness of learnability from single-child input, we
systematically train six different model architectures on five datasets (3
single-child and 2 baselines). We find that the models trained on single-child
datasets showed consistent results that matched with previous work,
underscoring the robustness of forming meaningful syntactic and semantic
representations from a subset of a child's linguistic input.
\\ ( https://arxiv.org/abs/2402.07899 ,  7745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06653
Date: Sun, 4 Feb 2024 14:27:28 GMT   (610kb,D)

Title: Using remotely sensed data for air pollution assessment
Authors: Teresa Bernardino, Maria Alexandra Oliveira, Jo\~ao Nuno Silva
Categories: cs.LG physics.ao-ph
\\
  Air pollution constitutes a global problem of paramount importance that
affects not only human health, but also the environment. The existence of
spatial and temporal data regarding the concentrations of pollutants is crucial
for performing air pollution studies and monitor emissions. However, although
observation data presents great temporal coverage, the number of stations is
very limited and they are usually built in more populated areas.
  The main objective of this work is to create models capable of inferring
pollutant concentrations in locations where no observation data exists. A
machine learning model, more specifically the random forest model, was
developed for predicting concentrations in the Iberian Peninsula in 2019 for
five selected pollutants: $NO_2$, $O_3$ $SO_2$, $PM10$, and $PM2.5$. Model
features include satellite measurements, meteorological variables, land use
classification, temporal variables (month, day of year), and spatial variables
(latitude, longitude, altitude).
  The models were evaluated using various methods, including station 10-fold
cross-validation, in which in each fold observations from 10\% of the stations
are used as testing data and the rest as training data. The $R^2$, RMSE and
mean bias were determined for each model. The $NO_2$ and $O_3$ models presented
good values of $R^2$, 0.5524 and 0.7462, respectively. However, the $SO_2$,
$PM10$, and $PM2.5$ models performed very poorly in this regard, with $R^2$
values of -0.0231, 0.3722, and 0.3303, respectively. All models slightly
overestimated the ground concentrations, except the $O_3$ model. All models
presented acceptable cross-validation RMSE, except the $O_3$ and $PM10$ models
where the mean value was a little higher (12.5934 $\mu g/m^3$ and 10.4737 $\mu
g/m^3$, respectively).
\\ ( https://arxiv.org/abs/2402.06653 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06662
Date: Tue, 6 Feb 2024 02:17:20 GMT   (11518kb,D)

Title: Sign Rank Limitations for Attention-Based Graph Decoders
Authors: Su Hyeong Lee and Qingqi Zhang and Risi Kondor
Categories: cs.LG
\\
  Inner product-based decoders are among the most influential frameworks used
to extract meaningful data from latent embeddings. However, such decoders have
shown limitations in representation capacity in numerous works within the
literature, which have been particularly notable in graph reconstruction
problems. In this paper, we provide the first theoretical elucidation of this
pervasive phenomenon in graph data, and suggest straightforward modifications
to circumvent this issue without deviating from the inner product framework.
\\ ( https://arxiv.org/abs/2402.06662 ,  11518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06675
Date: Wed, 7 Feb 2024 15:38:29 GMT   (267kb)

Title: A Masked language model for multi-source EHR trajectories contextual
  representation learning
Authors: Ali Amirahmadi (1), Mattias Ohlsson (1,2), Kobra Etminani (1), Olle
  Melander (3), and Jonas Bj\"ork (4) ((1) Center for Applied Intelligent
  Systems Research, Halmstad University, (2) Centre for Environmental and
  Climate Science, Lund University, (3) Department of Clinical Sciences, Lund
  University, (4) Division of Occupational and Environmental Medicine, Lund
  University)
Categories: cs.LG
Comments: Presented at Proceedings of MIE 2023
\\
  Using electronic health records data and machine learning to guide future
decisions needs to address challenges, including 1) long/short-term
dependencies and 2) interactions between diseases and interventions.
Bidirectional transformers have effectively addressed the first challenge. Here
we tackled the latter challenge by masking one source (e.g., ICD10 codes) and
training the transformer to predict it using other sources (e.g., ATC codes).
\\ ( https://arxiv.org/abs/2402.06675 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06688
Date: Thu, 8 Feb 2024 16:32:14 GMT   (403kb)

Title: Comparison of machine learning and statistical approaches for digital
  elevation model (DEM) correction: interim results
Authors: Chukwuma Okolie, Adedayo Adeleke, Julian Smit, Jon Mills, Iyke
  Maduako, Caleb Ogbeta
Categories: cs.LG
Comments: 2 pages, 2 figures, 1 table, ISPRS conference extended abstract
\\
  Several methods have been proposed for correcting the elevation bias in
digital elevation models (DEMs) for example, linear regression. Nowadays,
supervised machine learning enables the modelling of complex relationships
between variables, and has been deployed by researchers in a variety of fields.
In the existing literature, several studies have adopted either machine
learning or statistical approaches in the task of DEM correction. However, to
our knowledge, none of these studies have compared the performance of both
approaches, especially with regard to open-access global DEMs. Our previous
work has already shown the potential of machine learning approaches,
specifically gradient boosted decision trees (GBDTs) for DEM correction. In
this study, we share some results from the comparison of three recent
implementations of gradient boosted decision trees (XGBoost, LightGBM and
CatBoost), versus multiple linear regression (MLR) for enhancing the vertical
accuracy of 30 m Copernicus and AW3D global DEMs in Cape Town, South Africa.
\\ ( https://arxiv.org/abs/2402.06688 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06694
Date: Thu, 8 Feb 2024 21:57:10 GMT   (1372kb)

Title: Scaling Intelligent Agents in Combat Simulations for Wargaming
Authors: Scotty Black, Christian Darken
Categories: cs.LG cs.AI
Comments: arXiv admin note: text overlap with arXiv:2402.06075
Journal-ref: I/ITSEC Conference Proceedings 2023
\\
  Remaining competitive in future conflicts with technologically-advanced
competitors requires us to accelerate our research and development in
artificial intelligence (AI) for wargaming. More importantly, leveraging
machine learning for intelligent combat behavior development will be key to one
day achieving superhuman performance in this domain--elevating the quality and
accelerating the speed of our decisions in future wars. Although deep
reinforcement learning (RL) continues to show promising results in intelligent
agent behavior development in games, it has yet to perform at or above the
human level in the long-horizon, complex tasks typically found in combat
modeling and simulation. Capitalizing on the proven potential of RL and recent
successes of hierarchical reinforcement learning (HRL), our research is
investigating and extending the use of HRL to create intelligent agents capable
of performing effectively in these large and complex simulation environments.
Our ultimate goal is to develop an agent capable of superhuman performance that
could then serve as an AI advisor to military planners and decision-makers.
This papers covers our ongoing approach and the first three of our five
research areas aimed at managing the exponential growth of computations that
have thus far limited the use of AI in combat simulations: (1) developing an
HRL training framework and agent architecture for combat units; (2) developing
a multi-model framework for agent decision-making; (3) developing
dimension-invariant observation abstractions of the state space to manage the
exponential growth of computations; (4) developing an intrinsic rewards engine
to enable long-term planning; and (5) implementing this framework into a
higher-fidelity combat simulation.
\\ ( https://arxiv.org/abs/2402.06694 ,  1372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06696
Date: Fri, 9 Feb 2024 00:49:03 GMT   (386kb,D)

Title: FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via
  Large Language Models
Authors: Ruiyang Qin, Yuting Hu, Zheyu Yan, Jinjun Xiong, Ahmed Abbasi, Yiyu
  Shi
Categories: cs.LG cs.AI
Comments: ASP-DAC 2024
\\
  Neural Architecture Search (NAS) has become the de fecto tools in the
industry in automating the design of deep neural networks for various
applications, especially those driven by mobile and edge devices with limited
computing resources. The emerging large language models (LLMs), due to their
prowess, have also been incorporated into NAS recently and show some promising
results. This paper conducts further exploration in this direction by
considering three important design metrics simultaneously, i.e., model
accuracy, fairness, and hardware deployment efficiency. We propose a novel
LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that
FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN
models by orders-of-magnitude across almost all design considerations.
\\ ( https://arxiv.org/abs/2402.06696 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06697
Date: Fri, 9 Feb 2024 02:23:37 GMT   (307kb,D)

Title: Feed-Forward Neural Networks as a Mixed-Integer Program
Authors: Navid Aftabi and Nima Moradi and Fatemeh Mahroo
Categories: cs.LG cs.AI math.OC
\\
  Deep neural networks (DNNs) are widely studied in various applications. A DNN
consists of layers of neurons that compute affine combinations, apply nonlinear
operations, and produce corresponding activations. The rectified linear unit
(ReLU) is a typical nonlinear operator, outputting the max of its input and
zero. In scenarios like max pooling, where multiple input values are involved,
a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This
formulation, with continuous variables representing unit outputs and binary
variables for ReLU activation, finds applications across diverse domains. This
study explores the formulation of trained ReLU neurons as MIP and applies MIP
models for training neural networks (NNs). Specifically, it investigates
interactions between MIP techniques and various NN architectures, including
binary DNNs (employing step activation functions) and binarized DNNs (with
weights and activations limited to $-1,0,+1$). The research focuses on training
and evaluating proposed approaches through experiments on handwritten digit
classification models. The comparative study assesses the performance of
trained ReLU NNs, shedding light on the effectiveness of MIP formulations in
enhancing training processes for NNs.
\\ ( https://arxiv.org/abs/2402.06697 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06700
Date: Fri, 9 Feb 2024 07:45:26 GMT   (504kb,D)

Title: Entropy-Regularized Token-Level Policy Optimization for Large Language
  Models
Authors: Muning Wen, Cheng Deng, Jun Wang, Weinan Zhang and Ying Wen
Categories: cs.LG cs.AI
\\
  Large Language Models (LLMs) have shown promise as intelligent agents in
interactive decision-making tasks. Traditional approaches often depend on
meticulously designed prompts, high-quality examples, or additional reward
models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement
learning (RL) presents a dynamic alternative for LLMs to overcome these
dependencies by engaging directly with task-specific environments. Nonetheless,
it faces significant hurdles: 1) instability stemming from the exponentially
vast action space requiring exploration; 2) challenges in assigning token-level
credit based on action-level reward signals, resulting in discord between
maximizing rewards and accurately modeling corpus data. In response to these
challenges, we introduce Entropy-Regularized Token-level Policy Optimization
(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the
token level. At the heart of ETPO is our novel per-token soft Bellman update,
designed to harmonize the RL process with the principles of language modeling.
This methodology decomposes the Q-function update from a coarse action-level
view to a more granular token-level perspective, backed by theoretical proof of
optimization consistency. Crucially, this decomposition renders linear time
complexity in action exploration. We assess the effectiveness of ETPO within a
simulated environment that models data science code generation as a series of
multi-step interactive tasks; results show that ETPO achieves effective
performance improvement on the CodeLlama-7B model and surpasses a variant PPO
baseline inherited from RLHF. This underlines ETPO's potential as a robust
method for refining the interactive decision-making capabilities of LLMs.
\\ ( https://arxiv.org/abs/2402.06700 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06707
Date: Fri, 9 Feb 2024 10:51:09 GMT   (1147kb)

Title: Multi-class real-time crash risk forecasting using convolutional neural
  network: Istanbul case study
Authors: Behnaz Alafi, Saeid Moradi
Categories: cs.LG
Comments: 17 pages, 16 figures
ACM-class: I.2.6; K.3.2; I.2.m
\\
  The performance of an artificial neural network (ANN) in forecasting crash
risk is shown in this paper. To begin, some traffic and weather data are
acquired as raw data. This data is then analyzed, and relevant characteristics
are chosen to utilize as input data based on additional tree and Pearson
correlation. Furthermore, crash and non-crash time data are separated; then,
feature values for crash and non-crash events are written in three four-minute
intervals prior to the crash and non-crash events using the average of all
available values for that period. The number of non-crash samples was lowered
after calculating crash likelihood for each period based on accident labeling.
The proposed CNN model is capable of learning from recorded, processed, and
categorized input characteristics such as traffic characteristics and
meteorological conditions. The goal of this work is to forecast the chance of a
real-time crash based on three periods before events. The area under the curve
(AUC) for the receiver operating characteristic curve (ROC curve), as well as
sensitivity as the true positive rate and specificity as the false positive
rate, are shown and compared with three typical machine learning and neural
network models. Finally, when it comes to the error value, AUC, sensitivity,
and specificity parameters as performance variables, the executed model
outperforms other models. The findings of this research suggest applying the
CNN model as a multi-class prediction model for real-time crash risk
prediction. Our emphasis is on multi-class prediction, while prior research
used this for binary (two-class) categorization like crash and non-crash.
\\ ( https://arxiv.org/abs/2402.06707 ,  1147kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06714
Date: Fri, 9 Feb 2024 15:18:00 GMT   (2923kb,D)

Title: Electricity Price Forecasting in the Irish Balancing Market
Authors: Ciaran O'Connor and Joseph Collins and Steven Prestwich and Andrea
  Visentin
Categories: cs.LG
\\
  Short-term electricity markets are becoming more relevant due to
less-predictable renewable energy sources, attracting considerable attention
from the industry. The balancing market is the closest to real-time and the
most volatile among them. Its price forecasting literature is limited,
inconsistent and outdated, with few deep learning attempts and no public
dataset. This work applies to the Irish balancing market a variety of price
prediction techniques proven successful in the widely studied day-ahead market.
We compare statistical, machine learning, and deep learning models using a
framework that investigates the impact of different training sizes. The
framework defines hyperparameters and calibration settings; the dataset and
models are made public to ensure reproducibility and to be used as benchmarks
for future works. An extensive numerical study shows that well-performing
models in the day-ahead market do not perform well in the balancing one,
highlighting that these markets are fundamentally different constructs. The
best model is LEAR, a statistical approach based on LASSO, which outperforms
more complex and computationally demanding approaches.
\\ ( https://arxiv.org/abs/2402.06714 ,  2923kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06716
Date: Fri, 9 Feb 2024 17:02:41 GMT   (3343kb,D)

Title: Dynamic Graph Information Bottleneck
Authors: Haonan Yuan, Qingyun Sun, Xingcheng Fu, Cheng Ji, Jianxin Li
Categories: cs.LG cs.AI
Comments: Accepted by the research tracks of The Web Conference 2024 (WWW 2024)
\\
  Dynamic Graphs widely exist in the real world, which carry complicated
spatial and temporal feature patterns, challenging their representation
learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive
predictive abilities by exploiting the intrinsic dynamics. However, DGNNs
exhibit limited robustness, prone to adversarial attacks. This paper presents
the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust
and discriminative representations. Leveraged by the Information Bottleneck
(IB) principle, we first propose the expected optimal representations should
satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress
redundant as well as conserve meritorious information into latent
representation, DGIB iteratively directs and refines the structural and feature
information flow passing through graph snapshots. To meet the MSC Condition, we
decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the
DGIB$_{MS}$ channel aims to learn the minimal and sufficient representations,
with the DGIB$_{MS}$ channel guarantees the predictive consensus. Extensive
experiments on real-world and synthetic dynamic graph datasets demonstrate the
superior robustness of DGIB against adversarial attacks compared with
state-of-the-art baselines in the link prediction task. To the best of our
knowledge, DGIB is the first work to learn robust representations of dynamic
graphs grounded in the information-theoretic IB principle.
\\ ( https://arxiv.org/abs/2402.06716 ,  3343kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06734
Date: Fri, 9 Feb 2024 19:09:48 GMT   (55kb)

Title: Corruption Robust Offline Reinforcement Learning with Human Feedback
Authors: Debmalya Mandal, Andi Nika, Parameswaran Kamalaruban, Adish Singla,
  and Goran Radanovi\'c
Categories: cs.LG cs.AI
\\
  We study data corruption robustness for reinforcement learning with human
feedback (RLHF) in an offline setting. Given an offline dataset of pairs of
trajectories along with feedback about human preferences, an
$\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or
trajectory features manipulated), capturing an adversarial attack or noisy
human preferences. We aim to design algorithms that identify a near-optimal
policy from the corrupted data, with provable guarantees. Existing theoretical
works have separately studied the settings of corruption robust RL (learning
from scalar rewards directly under corruption) and offline RLHF (learning from
human feedback without corruption); however, they are inapplicable to our
problem of dealing with corrupted data in offline RLHF setting. To this end, we
design novel corruption robust offline RLHF methods under various assumptions
on the coverage of the data-generating distributions. At a high level, our
methodology robustifies an offline RLHF framework by first learning a reward
model along with confidence sets and then learning a pessimistic optimal policy
over the confidence set. Our key insight is that learning optimal policy can be
done by leveraging an offline corruption-robust RL oracle in different ways
(e.g., zero-order oracle or first-order oracle), depending on the data coverage
assumptions. To our knowledge, ours is the first work that provides provable
corruption robust offline RLHF methods.
\\ ( https://arxiv.org/abs/2402.06734 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06737
Date: Fri, 9 Feb 2024 19:16:04 GMT   (8905kb,D)

Title: ExGRG: Explicitly-Generated Relation Graph for Self-Supervised
  Representation Learning
Authors: Mahdi Naseri, Mahdi Biparva
Categories: cs.LG cs.AI
\\
  Self-supervised Learning (SSL) has emerged as a powerful technique in
pre-training deep learning models without relying on expensive annotated
labels, instead leveraging embedded signals in unlabeled data. While SSL has
shown remarkable success in computer vision tasks through intuitive data
augmentation, its application to graph-structured data poses challenges due to
the semantic-altering and counter-intuitive nature of graph augmentations.
Addressing this limitation, this paper introduces a novel non-contrastive SSL
approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead
of relying solely on the conventional augmentation-based implicit relation
graph. ExGRG offers a framework for incorporating prior domain knowledge and
online extracted information into the SSL invariance objective, drawing
inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM).
Employing an EM perspective on SSL, our E-step involves relation graph
generation to identify candidates to guide the SSL invariance objective, and
M-step updates the model parameters by integrating the derived relational
information. Extensive experimentation on diverse node classification datasets
demonstrates the superiority of our method over state-of-the-art techniques,
affirming ExGRG as an effective adoption of SSL for graph representation
learning.
\\ ( https://arxiv.org/abs/2402.06737 ,  8905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06751
Date: Fri, 9 Feb 2024 19:28:02 GMT   (3485kb,D)

Title: Low-Rank Learning by Design: the Role of Network Architecture and
  Activation Linearity in Gradient Rank Collapse
Authors: Bradley T. Baker, Barak A. Pearlmutter, Robyn Miller, Vince D.
  Calhoun, Sergey M. Plis
Categories: cs.LG
\\
  Our understanding of learning dynamics of deep neural networks (DNNs) remains
incomplete. Recent research has begun to uncover the mathematical principles
underlying these networks, including the phenomenon of "Neural Collapse", where
linear classifiers within DNNs converge to specific geometrical structures
during late-stage training. However, the role of geometric constraints in
learning extends beyond this terminal phase. For instance, gradients in
fully-connected layers naturally develop a low-rank structure due to the
accumulation of rank-one outer products over a training batch. Despite the
attention given to methods that exploit this structure for memory saving or
regularization, the emergence of low-rank learning as an inherent aspect of
certain DNN architectures has been under-explored. In this paper, we conduct a
comprehensive study of gradient rank in DNNs, examining how architectural
choices and structure of the data effect gradient rank bounds. Our theoretical
analysis provides these bounds for training fully-connected, recurrent, and
convolutional neural networks. We also demonstrate, both theoretically and
empirically, how design choices like activation function linearity, bottleneck
layer introduction, convolutional stride, and sequence truncation influence
these bounds. Our findings not only contribute to the understanding of learning
dynamics in DNNs, but also provide practical guidance for deep learning
engineers to make informed design decisions.
\\ ( https://arxiv.org/abs/2402.06751 ,  3485kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06756
Date: Fri, 9 Feb 2024 19:39:23 GMT   (594kb,D)

Title: Convergence of Gradient Descent with Small Initialization for
  Unregularized Matrix Completion
Authors: Jianhao Ma, Salar Fattahi
Categories: cs.LG math.OC stat.ML
\\
  We study the problem of symmetric matrix completion, where the goal is to
reconstruct a positive semidefinite matrix $\rm{X}^\star \in
\mathbb{R}^{d\times d}$ of rank-$r$, parameterized by $\rm{U}\rm{U}^{\top}$,
from only a subset of its observed entries. We show that the vanilla gradient
descent (GD) with small initialization provably converges to the ground truth
$\rm{X}^\star$ without requiring any explicit regularization. This convergence
result holds true even in the over-parameterized scenario, where the true rank
$r$ is unknown and conservatively over-estimated by a search rank $r'\gg r$.
The existing results for this problem either require explicit regularization, a
sufficiently accurate initial point, or exact knowledge of the true rank $r$.
  In the over-parameterized regime where $r'\geq r$, we show that, with
$\widetilde\Omega(dr^9)$ observations, GD with an initial point $\|\rm{U}_0\|
\leq \epsilon$ converges near-linearly to an $\epsilon$-neighborhood of
$\rm{X}^\star$. Consequently, smaller initial points result in increasingly
accurate solutions. Surprisingly, neither the convergence rate nor the final
accuracy depends on the over-parameterized search rank $r'$, and they are only
governed by the true rank $r$. In the exactly-parameterized regime where
$r'=r$, we further enhance this result by proving that GD converges at a faster
rate to achieve an arbitrarily small accuracy $\epsilon>0$, provided the
initial point satisfies $\|\rm{U}_0\| = O(1/d)$. At the crux of our method lies
a novel weakly-coupled leave-one-out analysis, which allows us to establish the
global convergence of GD, extending beyond what was previously possible using
the classical leave-one-out analysis.
\\ ( https://arxiv.org/abs/2402.06756 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06761
Date: Fri, 9 Feb 2024 19:47:31 GMT   (573kb,D)

Title: Embedding Compression for Teacher-to-Student Knowledge Transfer
Authors: Yiwei Ding and Alexander Lerch
Categories: cs.LG
Comments: 5+1 pages. In ICASSP 2024 Satellite Workshop Deep Neural Network
  Model Compression
\\
  Common knowledge distillation methods require the teacher model and the
student model to be trained on the same task. However, the usage of embeddings
as teachers has also been proposed for different source tasks and target tasks.
Prior work that uses embeddings as teachers ignores the fact that the teacher
embeddings are likely to contain irrelevant knowledge for the target task. To
address this problem, we propose to use an embedding compression module with a
trainable teacher transformation to obtain a compact teacher embedding. Results
show that adding the embedding compression module improves the classification
performance, especially for unsupervised teacher embeddings. Moreover, student
models trained with the guidance of embeddings show stronger generalizability.
\\ ( https://arxiv.org/abs/2402.06761 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06763
Date: Fri, 9 Feb 2024 19:52:31 GMT   (635kb,D)

Title: Scalable Kernel Logistic Regression with Nystr\"om Approximation:
  Theoretical Analysis and Application to Discrete Choice Modelling
Authors: Jos\'e \'Angel Mart\'in-Baos, Ricardo Garc\'ia-R\'odenas, Luis
  Rodriguez-Benitez, Michel Bierlaire
Categories: cs.LG stat.ML
Comments: 32 pages, 5 figures
\\
  The application of kernel-based Machine Learning (ML) techniques to discrete
choice modelling using large datasets often faces challenges due to memory
requirements and the considerable number of parameters involved in these
models. This complexity hampers the efficient training of large-scale models.
This paper addresses these problems of scalability by introducing the Nystr\"om
approximation for Kernel Logistic Regression (KLR) on large datasets. The study
begins by presenting a theoretical analysis in which: i) the set of KLR
solutions is characterised, ii) an upper bound to the solution of KLR with
Nystr\"om approximation is provided, and finally iii) a specialisation of the
optimisation algorithms to Nystr\"om KLR is described. After this, the
Nystr\"om KLR is computationally validated. Four landmark selection methods are
tested, including basic uniform sampling, a k-means sampling strategy, and two
non-uniform methods grounded in leverage scores. The performance of these
strategies is evaluated using large-scale transport mode choice datasets and is
compared with traditional methods such as Multinomial Logit (MNL) and
contemporary ML techniques. The study also assesses the efficiency of various
optimisation techniques for the proposed Nystr\"om KLR model. The performance
of gradient descent, Momentum, Adam, and L-BFGS-B optimisation methods is
examined on these datasets. Among these strategies, the k-means Nystr\"om KLR
approach emerges as a successful solution for applying KLR to large datasets,
particularly when combined with the L-BFGS-B and Adam optimisation methods. The
results highlight the ability of this strategy to handle datasets exceeding
200,000 observations while maintaining robust performance.
\\ ( https://arxiv.org/abs/2402.06763 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06800
Date: Fri, 9 Feb 2024 21:57:57 GMT   (3702kb)

Title: Generative Nowcasting of Marine Fog Visibility in the Grand Banks area
  and Sable Island in Canada
Authors: Eren Gultepe, Sen Wang, Byron Blomquist, Harindra J.S. Fernando, O.
  Patrick Kreidl, David J. Delene, Ismail Gultepe
Categories: cs.LG physics.ao-ph
Journal-ref: Tackling Climate Change with Machine Learning: workshop at NeurIPS
  2023
\\
  This study presents the application of generative deep learning techniques to
evaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence
interactions in the marine atmosphere) campaign observations collected during
July 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable
Island (SI), northeast of Canada. The measurements were collected using the
Vaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50,
and Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic
Condor. To perform nowcasting, the time series of fog visibility (Vis), wind
speed, dew point depression, and relative humidity with respect to water were
preprocessed to have lagged time step features. Generative nowcasting of Vis
time series for lead times of 30 and 60 minutes were performed using
conditional generative adversarial networks (cGAN) regression at visibility
thresholds of Vis < 1 km and < 10 km. Extreme gradient boosting (XGBoost) was
used as a baseline method for comparison against cGAN. At the 30 min lead time,
Vis was best predicted with cGAN at Vis < 1 km (RMSE = 0.151 km) and with
XGBoost at Vis < 10 km (RMSE = 2.821 km). At the 60 min lead time, Vis was best
predicted with XGBoost at Vis < 1 km (RMSE = 0.167 km) and Vis < 10 km (RMSE =
3.508 km), but the cGAN RMSE was similar to XGBoost. Despite nowcasting Vis at
30 min being quite difficult, the ability of the cGAN model to track the
variation in Vis at 1 km suggests that there is potential for generative
analysis of marine fog visibility using observational meteorological
parameters.
\\ ( https://arxiv.org/abs/2402.06800 ,  3702kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06808
Date: Fri, 9 Feb 2024 22:14:40 GMT   (2189kb,D)

Title: Explain Variance of Prediction in Variational Time Series Models for
  Clinical Deterioration Prediction
Authors: Jiacheng Liu and Jaideep Srivastava
Categories: cs.LG
\\
  In healthcare, thanks to many model agnostic methods, explainability of the
prediction scores made by deep learning applications has improved. However, we
note that for daily or hourly risk of deterioration prediction of in-hospital
patients, not only the predicted risk probability score matters, but also the
variance of the risk scores play key roles in aiding clinical decision making.
In this paper, we propose to use delta's method to approximate variance of
prediction deterministically, such that the SHAP method can be adopted to
attribute contribution of variance. The prediction variance is estimated by
sampling the conditional hidden space in variational models and is propagated
to input clinical variables based on Shapley values of the variance game. This
approach works with variational time series models such as variational
recurrent neural networks and variational transformers. We further argue that
variational time series models are perfect fits for achieving a balance between
predictive power and explainability through a series of experiments on a public
clinical ICU datasets. Since SHAP values are additive, we also postulate that
the SHAP importance of clinical variables with respect to prediction variations
can guide their frequency of measurements.
\\ ( https://arxiv.org/abs/2402.06808 ,  2189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06812
Date: Fri, 9 Feb 2024 22:27:29 GMT   (3185kb,D)

Title: A Kalman Filter Based Framework for Monitoring the Performance of
  In-Hospital Mortality Prediction Models Over Time
Authors: Jiacheng Liu, Lisa Kirkland, Jaideep Srivastava
Categories: cs.LG
\\
  Unlike in a clinical trial, where researchers get to determine the least
number of positive and negative samples required, or in a machine learning
study where the size and the class distribution of the validation set is static
and known, in a real-world scenario, there is little control over the size and
distribution of incoming patients. As a result, when measured during different
time periods, evaluation metrics like Area under the Receiver Operating Curve
(AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly
comparable. Therefore, in this study, for binary classifiers running in a long
time period, we proposed to adjust these performance metrics for sample size
and class distribution, so that a fair comparison can be made between two time
periods. Note that the number of samples and the class distribution, namely the
ratio of positive samples, are two robustness factors which affect the variance
of AUCROC. To better estimate the mean of performance metrics and understand
the change of performance over time, we propose a Kalman filter based framework
with extrapolated variance adjusted for the total number of samples and the
number of positive samples during different time periods. The efficacy of this
method is demonstrated first on a synthetic dataset and then retrospectively
applied to a 2-days ahead in-hospital mortality prediction model for COVID-19
patients during 2021 and 2022. Further, we conclude that our prediction model
is not significantly affected by the evolution of the disease, improved
treatments and changes in hospital operational plans.
\\ ( https://arxiv.org/abs/2402.06812 ,  3185kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06815
Date: Fri, 9 Feb 2024 22:47:25 GMT   (1602kb,D)

Title: Estimating Player Performance in Different Contexts Using Fine-tuned
  Large Events Models
Authors: Tiago Mendes-Neves, Lu\'is Meireles, Jo\~ao Mendes-Moreira
Categories: cs.LG
\\
  This paper introduces an innovative application of Large Event Models (LEMs),
akin to Large Language Models, to the domain of soccer analytics. By learning
the "language" of soccer - predicting variables for subsequent events rather
than words LEMs facilitate the simulation of matches and offer various
applications, including player performance prediction across different team
contexts. We focus on fine-tuning LEMs with the WyScout dataset for the
2017-2018 Premier League season to derive specific insights into player
contributions and team strategies. Our methodology involves adapting these
models to reflect the nuanced dynamics of soccer, enabling the evaluation of
hypothetical transfers. Our findings confirm the effectiveness and limitations
of LEMs in soccer analytics, highlighting the model's capability to forecast
teams' expected standings and explore high-profile scenarios, such as the
potential effects of transferring Cristiano Ronaldo or Lionel Messi to
different teams in the Premier League. This analysis underscores the importance
of context in evaluating player quality. While general metrics may suggest
significant differences between players, contextual analyses reveal narrower
gaps in performance within specific team frameworks.
\\ ( https://arxiv.org/abs/2402.06815 ,  1602kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06818
Date: Fri, 9 Feb 2024 22:59:20 GMT   (1347kb,D)

Title: Towards a Systematic Approach to Design New Ensemble Learning Algorithms
Authors: Jo\~ao Mendes-Moreira, Tiago Mendes-Neves
Categories: cs.LG
\\
  Ensemble learning has been a focal point of machine learning research due to
its potential to improve predictive performance. This study revisits the
foundational work on ensemble error decomposition, historically confined to
bias-variance-covariance analysis for regression problems since the 1990s.
Recent advancements introduced a "unified theory of diversity," which proposes
an innovative bias-variance-diversity decomposition framework. Leveraging this
contemporary understanding, our research systematically explores the
application of this decomposition to guide the creation of new ensemble
learning algorithms. Focusing on regression tasks, we employ neural networks as
base learners to investigate the practical implications of this theoretical
framework. This approach used 7 simple ensemble methods, we name them
strategies, for neural networks that were used to generate 21 new ensemble
algorithms. Among these, most of the methods aggregated with the snapshot
strategy, one of the 7 strategies used, showcase superior predictive
performance across diverse datasets w.r.t. the Friedman rank test with the
Conover post-hoc test. Our systematic design approach contributes a suite of
effective new algorithms and establishes a structured pathway for future
ensemble learning algorithm development.
\\ ( https://arxiv.org/abs/2402.06818 ,  1347kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06819
Date: Fri, 9 Feb 2024 23:00:29 GMT   (10959kb,D)

Title: Monitored Markov Decision Processes
Authors: Simone Parisi, Montaser Mohammedalamen, Alireza Kazemipour, Matthew E.
  Taylor, Michael Bowling
Categories: cs.LG
Comments: AAMAS 2024, Main Track
\\
  In reinforcement learning (RL), an agent learns to perform a task by
interacting with an environment and receiving feedback (a numerical reward) for
its actions. However, the assumption that rewards are always observable is
often not applicable in real-world problems. For example, the agent may need to
ask a human to supervise its actions or activate a monitoring system to receive
feedback. There may even be a period of time before rewards become observable,
or a period of time after which rewards are no longer given. In other words,
there are cases where the environment generates rewards in response to the
agent's actions but the agent cannot observe them. In this paper, we formalize
a novel but general RL framework - Monitored MDPs - where the agent cannot
always observe rewards. We discuss the theoretical and practical consequences
of this setting, show challenges raised even in toy environments, and propose
algorithms to begin to tackle this novel setting. This paper introduces a
powerful new formalism that encompasses both new and existing problems and lays
the foundation for future research.
\\ ( https://arxiv.org/abs/2402.06819 ,  10959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06820
Date: Fri, 9 Feb 2024 23:02:57 GMT   (1183kb,D)

Title: Forecasting Events in Soccer Matches Through Language
Authors: Tiago Mendes-Neves, Lu\'is Meireles, Jo\~ao Mendes-Moreira
Categories: cs.LG
\\
  This paper introduces an approach to predicting the next event in a soccer
match, a challenge bearing remarkable similarities to the problem faced by
Large Language Models (LLMs). Unlike other methods that severely limit event
dynamics in soccer, often abstracting from many variables or relying on a mix
of sequential models, our research proposes a novel technique inspired by the
methodologies used in LLMs. These models predict a complete chain of variables
that compose an event, significantly simplifying the construction of Large
Event Models (LEMs) for soccer. Utilizing deep learning on the publicly
available WyScout dataset, the proposed approach notably surpasses the
performance of previous LEM proposals in critical areas, such as the prediction
accuracy of the next event type. This paper highlights the utility of LEMs in
various applications, including betting and match analytics. Moreover, we show
that LEMs provide a simulation backbone on which many analytics pipelines can
be built, an approach opposite to the current specialized single-purpose
models. LEMs represent a pivotal advancement in soccer analytics, establishing
a foundational framework for multifaceted analytics pipelines through a
singular machine-learning model.
\\ ( https://arxiv.org/abs/2402.06820 ,  1183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06827
Date: Fri, 9 Feb 2024 23:29:54 GMT   (9138kb,D)

Title: RAMP: Boosting Adversarial Robustness Against Multiple $l_p$
  Perturbations
Authors: Enyi Jiang, Gagandeep Singh
Categories: cs.LG
\\
  There is considerable work on improving robustness against adversarial
attacks bounded by a single $l_p$ norm using adversarial training (AT).
However, the multiple-norm robustness (union accuracy) of AT models is still
low. We observe that simultaneously obtaining good union and clean accuracy is
hard since there are tradeoffs between robustness against multiple $l_p$
perturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs
from the lens of distribution shifts, we identify the key tradeoff pair among
$l_p$ attacks to boost efficiency and design a logit pairing loss to improve
the union accuracy. Next, we connect natural training with AT via gradient
projection, to find and incorporate useful information from natural training
into AT, which moderates the accuracy/robustness tradeoff. Combining our
contributions, we propose a framework called \textbf{RAMP}, to boost the
robustness against multiple $l_p$ perturbations. We show \textbf{RAMP} can be
easily adapted for both robust fine-tuning and full AT. For robust fine-tuning,
\textbf{RAMP} obtains a union accuracy up to $53.5\%$ on CIFAR-10, and $29.7\%$
on ImageNet. For training from scratch, \textbf{RAMP} achieves SOTA union
accuracy of $44.6\%$ and relatively good clean accuracy of $81.2\%$ on
ResNet-18 against AutoAttack on CIFAR-10.
\\ ( https://arxiv.org/abs/2402.06827 ,  9138kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06855
Date: Sat, 10 Feb 2024 01:36:39 GMT   (1691kb,D)

Title: For Better or For Worse? Learning Minimum Variance Features With Label
  Augmentation
Authors: Muthu Chidambaram and Rong Ge
Categories: cs.LG cs.CV
Comments: 20 pages, 7 figures
\\
  Data augmentation has been pivotal in successfully training deep learning
models on classification tasks over the past decade. An important subclass of
data augmentation techniques - which includes both label smoothing and Mixup -
involves modifying not only the input data but also the input label during
model training. In this work, we analyze the role played by the label
augmentation aspect of such methods. We prove that linear models on linearly
separable data trained with label augmentation learn only the minimum variance
features in the data, while standard training (which includes weight decay) can
learn higher variance features. An important consequence of our results is
negative: label smoothing and Mixup can be less robust to adversarial
perturbations of the training data when compared to standard training. We
verify that our theory reflects practice via a range of experiments on
synthetic data and image classification benchmarks.
\\ ( https://arxiv.org/abs/2402.06855 ,  1691kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06859
Date: Sat, 10 Feb 2024 01:47:10 GMT   (2432kb,D)

Title: LiRank: Industrial Large Scale Ranking Models at LinkedIn
Authors: Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh
  Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Xiao,
  Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing
  Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang,
  Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang,
  Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney,
  Amol Ghoting, Souvik Ghosh
Categories: cs.LG cs.AI cs.IR
ACM-class: H.3.3
\\
  We present LiRank, a large-scale ranking framework at LinkedIn that brings to
production state-of-the-art modeling architectures and optimization methods. We
unveil several modeling improvements, including Residual DCN, which adds
attention and residual connections to the famous DCNv2 architecture. We share
insights into combining and tuning SOTA architectures to create a unified
model, including Dense Gating, Transformers and Residual DCN. We also propose
novel techniques for calibration and describe how we productionalized deep
learning based explore/exploit methods. To enable effective, production-grade
serving of large ranking models, we detail how to train and compress models
using quantization and vocabulary compression. We provide details about the
deployment setup for large-scale use cases of Feed ranking, Jobs
Recommendations, and Ads click-through rate (CTR) prediction. We summarize our
learnings from various A/B tests by elucidating the most effective technical
approaches. These ideas have contributed to relative metrics improvements
across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%
qualified job applications for Jobs search and recommendations, and +4.3% for
Ads CTR. We hope this work can provide practical insights and solutions for
practitioners interested in leveraging large-scale deep ranking systems.
\\ ( https://arxiv.org/abs/2402.06859 ,  2432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06864
Date: Sat, 10 Feb 2024 03:04:57 GMT   (224kb,D)

Title: Discriminative Adversarial Unlearning
Authors: Rohan Sharma, Shijie Zhou, Kaiyi Ji and Changyou Chen
Categories: cs.LG cs.AI
Comments: 13 pages including references, 2 tables, 2 figures and 1 algorithm
\\
  We introduce a novel machine unlearning framework founded upon the
established principles of the min-max optimization paradigm. We capitalize on
the capabilities of strong Membership Inference Attacks (MIA) to facilitate the
unlearning of specific samples from a trained model. We consider the scenario
of two networks, the attacker $\mathbf{A}$ and the trained defender
$\mathbf{D}$ pitted against each other in an adversarial objective, wherein the
attacker aims at teasing out the information of the data to be unlearned in
order to infer membership, and the defender unlearns to defend the network
against the attack, whilst preserving its general performance. The algorithm
can be trained end-to-end using backpropagation, following the well known
iterative min-max approach in updating the attacker and the defender. We
additionally incorporate a self-supervised objective effectively addressing the
feature space discrepancies between the forget set and the validation set,
enhancing unlearning performance. Our proposed algorithm closely approximates
the ideal benchmark of retraining from scratch for both random sample
forgetting and class-wise forgetting schemes on standard machine-unlearning
datasets. Specifically, on the class unlearning scheme, the method demonstrates
near-optimal performance and comprehensively overcomes known methods over the
random sample forgetting scheme across all metrics and multiple network pruning
strategies.
\\ ( https://arxiv.org/abs/2402.06864 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06886
Date: Sat, 10 Feb 2024 04:54:15 GMT   (375kb,D)

Title: Principled Penalty-based Methods for Bilevel Reinforcement Learning and
  RLHF
Authors: Han Shen, Zhuoran Yang, Tianyi Chen
Categories: cs.LG math.OC stat.ML
\\
  Bilevel optimization has been recently applied to many machine learning
tasks. However, their applications have been restricted to the supervised
learning setting, where static objective functions with benign structures are
considered. But bilevel problems such as incentive design, inverse
reinforcement learning (RL), and RL from human feedback (RLHF) are often
modeled as dynamic objective functions that go beyond the simple static
objective structures, which pose significant challenges of using existing
bilevel solutions. To tackle this new class of bilevel problems, we introduce
the first principled algorithmic framework for solving bilevel RL problems
through the lens of penalty formulation. We provide theoretical studies of the
problem landscape and its penalty-based (policy) gradient algorithms. We
demonstrate the effectiveness of our algorithms via simulations in the
Stackelberg Markov game, RL from human feedback and incentive design.
\\ ( https://arxiv.org/abs/2402.06886 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06892
Date: Sat, 10 Feb 2024 06:49:08 GMT   (544kb,D)

Title: Understanding Test-Time Augmentation
Authors: Masanari Kimura
Categories: cs.LG
\\
  Test-Time Augmentation (TTA) is a very powerful heuristic that takes
advantage of data augmentation during testing to produce averaged output.
Despite the experimental effectiveness of TTA, there is insufficient discussion
of its theoretical aspects. In this paper, we aim to give theoretical
guarantees for TTA and clarify its behavior.
\\ ( https://arxiv.org/abs/2402.06892 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06908
Date: Sat, 10 Feb 2024 08:26:06 GMT   (21247kb,D)

Title: Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural
  Networks via Higher-Order Interactions
Authors: Lorenzo Giusti
Categories: cs.LG cs.AI
Comments: PhD thesis, 135 pages, 51 figures, 11 tables
\\
  The irreducible complexity of natural phenomena has led Graph Neural Networks
to be employed as a standard model to perform representation learning tasks on
graph-structured data. While their capacity to capture local and global
patterns is remarkable, the implications associated with long-range and
higher-order dependencies pose considerable challenges to such models. This
work starts with a theoretical framework to reveal the impact of network's
width, depth, and graph topology on the over-squashing phenomena in
message-passing neural networks. Then, the work drifts towards, higher-order
interactions and multi-relational inductive biases via Topological Neural
Networks. Such models propagate messages through higher-dimensional structures,
providing shortcuts or additional routes for information flow. With this
construction, the underlying computational graph is no longer coupled with the
input graph structure, thus mitigating the aforementioned bottlenecks while
accounting also for higher-order interactions. Inspired by Graph Attention
Networks, two topological attention networks are proposed: Simplicial and Cell
Attention Networks. The rationale behind these architecture is to leverage the
extended notion of neighbourhoods provided by the arrangement of groups of
nodes within a simplicial or cell complex to design anisotropic aggregations
able to measure the importance of the information coming from different regions
of the domain. By doing so, they capture dependencies that conventional Graph
Neural Networks might miss. Finally, a multi-way communication scheme is
introduced with Enhanced Cellular Isomorphism Networks, which augment
topological message passing schemes to enable a direct interactions among
groups of nodes arranged in ring-like structures.
\\ ( https://arxiv.org/abs/2402.06908 ,  21247kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06912
Date: Sat, 10 Feb 2024 09:15:21 GMT   (4115kb,D)

Title: Solving Deep Reinforcement Learning Benchmarks with Linear Policy
  Networks
Authors: Annie Wong, Jacob de Nobel, Thomas B\"ack, Aske Plaat, Anna V.
  Kononova
Categories: cs.LG cs.AI
\\
  Although Deep Reinforcement Learning (DRL) methods can learn effective
policies for challenging problems such as Atari games and robotics tasks,
algorithms are complex and training times are often long. This study
investigates how evolution strategies (ES) perform compared to gradient-based
deep reinforcement learning methods. We use ES to optimize the weights of a
neural network via neuroevolution, performing direct policy search. We
benchmark both regular networks and policy networks consisting of a single
linear layer from observations to actions; for three classical ES methods and
for three gradient-based methods such as PPO. Our results reveal that ES can
find effective linear policies for many RL benchmark tasks, in contrast to DRL
methods that can only find successful policies using much larger networks,
suggesting that current benchmarks are easier to solve than previously assumed.
Interestingly, also for higher complexity tasks, ES achieves results comparable
to gradient-based DRL algorithms. Furthermore, we find that by directly
accessing the memory state of the game, ES are able to find successful policies
in Atari, outperforming DQN. While gradient-based methods have dominated the
field in recent years, ES offers an alternative that is easy to implement,
parallelize, understand, and tune.
\\ ( https://arxiv.org/abs/2402.06912 ,  4115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06918
Date: Sat, 10 Feb 2024 09:51:03 GMT   (118kb,D)

Title: Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach
  to Searching for the Most Promising Intermediate Thought
Authors: Zhen-Yu Zhang, Siwei Han, Huaxiu Yao, Gang Niu, Masashi Sugiyama
Categories: cs.LG cs.AI cs.CL
\\
  To improve the ability of the large language model (LLMs) to handle complex
reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs
to reason step-by-step, facilitating problem solving from simple to complex
tasks. State-of-the-art approaches for generating such a chain involve
interactive collaboration, where the learner generates candidate intermediate
thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts.
However, a widespread yet understudied problem is that the evaluation from the
LLM is typically noisy and unreliable, potentially misleading the generation
process in selecting promising intermediate thoughts. In this paper, motivated
by Vapnik's principle, we propose a novel comparison-based CoT generation
algorithm that directly identifies the most promising thoughts with the noisy
feedback from the LLM. In each round, we randomly pair intermediate thoughts
and directly prompt the LLM to select the more promising one from each pair,
allowing us to identify the most promising thoughts through an iterative
process. To further model the noise in the comparison, we resort to the
techniques of ensemble and dueling bandits and propose two variants of the
proposed algorithm. Experiments on three real-world mathematical and reasoning
tasks demonstrate the effectiveness of our proposed algorithm and verify the
rationale of the direct pairwise comparison.
\\ ( https://arxiv.org/abs/2402.06918 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06921
Date: Sat, 10 Feb 2024 10:10:11 GMT   (1374kb)

Title: Clustering Techniques Selection for a Hybrid Regression Model: A Case
  Study Based on a Solar Thermal System
Authors: Mar\'ia Teresa Garc\'ia-Ord\'as, H\'ector Alaiz-Moret\'on, Jos\'e-Luis
  Casteleiro-Roca, Esteban Jove, Jos\'e Alberto Ben\'itez-Andrades, Isa\'ias
  Garc\'ia-Rodr\'iguez, H\'ector Quinti\'an and Jos\'e Luis Calvo-Rolle
Categories: cs.LG cs.MA
Journal-ref: Cybernetics and Systems, Volume 54, Issue 3, pages 286-305, 2023
DOI: 10.1080/01969722.2022.2030006
\\
  This work addresses the performance comparison between four clustering
techniques with the objective of achieving strong hybrid models in supervised
learning tasks. A real dataset from a bio-climatic house named Sotavento placed
on experimental wind farm and located in Xermade (Lugo) in Galicia (Spain) has
been collected. Authors have chosen the thermal solar generation system in
order to study how works applying several cluster methods followed by a
regression technique to predict the output temperature of the system. With the
objective of defining the quality of each clustering method two possible
solutions have been implemented. The first one is based on three unsupervised
learning metrics (Silhouette, Calinski-Harabasz and Davies-Bouldin) while the
second one, employs the most common error measurements for a regression
algorithm such as Multi Layer Perceptron.
\\ ( https://arxiv.org/abs/2402.06921 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06932
Date: Sat, 10 Feb 2024 12:10:13 GMT   (3261kb,D)

Title: Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with
  Trainable Attribute
Authors: Tajima Shinji, Ren Sugihara, Ryota Kitahara and Masayuki Karasuyama
Categories: cs.LG
\\
  The graph classification problem has been widely studied; however, achieving
an interpretable model with high predictive performance remains a challenging
issue. This paper proposes an interpretable classification algorithm for
attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA
learns importance weights for small attributed subgraphs, called attributed
graphlets (AGs), while simultaneously optimizing their attribute vectors. This
enables us to obtain a combination of subgraph structures and their attribute
vectors that strongly contribute to discriminating different classes. A
significant characteristics of LAGRA is that all the subgraph structures in the
training dataset can be considered as a candidate structures of AGs. This
approach can explore all the potentially important subgraphs exhaustively, but
obviously, a naive implementation can require a large amount of computations.
To mitigate this issue, we propose an efficient pruning strategy by combining
the proximal gradient descent and a graph mining tree search. Our pruning
strategy can ensure that the quality of the solution is maintained compared to
the result without pruning. We empirically demonstrate that LAGRA has superior
or comparable prediction performance to the standard existing algorithms
including graph neural networks, while using only a small number of AGs in an
interpretable manner.
\\ ( https://arxiv.org/abs/2402.06932 ,  3261kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06937
Date: Sat, 10 Feb 2024 12:23:08 GMT   (4520kb,D)

Title: Assessing Uncertainty Estimation Methods for 3D Image Segmentation under
  Distribution Shifts
Authors: Masoumeh Javanbakhat, Md Tasnimul Hasan, Cristoph Lippert
Categories: cs.LG cs.CV
\\
  In recent years, machine learning has witnessed extensive adoption across
various sectors, yet its application in medical image-based disease detection
and diagnosis remains challenging due to distribution shifts in real-world
data. In practical settings, deployed models encounter samples that differ
significantly from the training dataset, especially in the health domain,
leading to potential performance issues. This limitation hinders the
expressiveness and reliability of deep learning models in health applications.
Thus, it becomes crucial to identify methods capable of producing reliable
uncertainty estimation in the context of distribution shifts in the health
sector. In this paper, we explore the feasibility of using cutting-edge
Bayesian and non-Bayesian methods to detect distributionally shifted samples,
aiming to achieve reliable and trustworthy diagnostic predictions in
segmentation task. Specifically, we compare three distinct uncertainty
estimation methods, each designed to capture either unimodal or multimodal
aspects in the posterior distribution. Our findings demonstrate that methods
capable of addressing multimodal characteristics in the posterior distribution,
offer more dependable uncertainty estimates. This research contributes to
enhancing the utility of deep learning in healthcare, making diagnostic
predictions more robust and trustworthy.
\\ ( https://arxiv.org/abs/2402.06937 ,  4520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06954
Date: Sat, 10 Feb 2024 13:50:11 GMT   (481kb,D)

Title: OpenFedLLM: Training Large Language Models on Decentralized Private Data
  via Federated Learning
Authors: Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin
  Du, Yanfeng Wang, Siheng Chen
Categories: cs.LG cs.CL cs.DC cs.MA
Comments: 28 pages, 3 figures, 16 tables
\\
  Trained on massive publicly available data, large language models (LLMs) have
demonstrated tremendous success across various fields. While more data
contributes to better performance, a disconcerting reality is that high-quality
public data will be exhausted in a few years. In this paper, we offer a
potential next step for contemporary LLMs: collaborative and privacy-preserving
LLM training on the underutilized distributed private data via federated
learning (FL), where multiple data owners collaboratively train a shared model
without transmitting raw data. To achieve this, we build a concise, integrated,
and research-friendly framework/codebase, named OpenFedLLM. It covers federated
instruction tuning for enhancing instruction-following capability, federated
value alignment for aligning with human values, and 7 representative FL
algorithms. Besides, OpenFedLLM supports training on diverse domains, where we
cover 8 training datasets; and provides comprehensive evaluations, where we
cover 30+ evaluation metrics. Through extensive experiments, we observe that
all FL algorithms outperform local training on training LLMs, demonstrating a
clear performance improvement across a variety of settings. Notably, in a
financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can
outperform GPT-4 by a significant margin while the model obtained through
individual training cannot, demonstrating strong motivation for clients to
participate in FL. The code is available at
https://github.com/rui-ye/OpenFedLLM.
\\ ( https://arxiv.org/abs/2402.06954 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06955
Date: Sat, 10 Feb 2024 13:51:09 GMT   (21934kb,D)

Title: Training dynamics in Physics-Informed Neural Networks with feature
  mapping
Authors: Chengxi Zeng, Tilo Burghardt, Alberto M Gambaruto
Categories: cs.LG cs.AI cs.CE
\\
  Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine
learning approach for solving Partial Differential Equations (PDEs). Although
its variants have achieved significant progress, the empirical success of
utilising feature mapping from the wider Implicit Neural Representations
studies has been substantially neglected. We investigate the training dynamics
of PINNs with a feature mapping layer via the limiting Conjugate Kernel and
Neural Tangent Kernel, which sheds light on the convergence and generalisation
of the model. We also show the inadequacy of commonly used Fourier-based
feature mapping in some scenarios and propose the conditional positive definite
Radial Basis Function as a better alternative. The empirical results reveal the
efficacy of our method in diverse forward and inverse problem sets. This simple
technique can be easily implemented in coordinate input networks and benefits
the broad PINNs research.
\\ ( https://arxiv.org/abs/2402.06955 ,  21934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06963
Date: Sat, 10 Feb 2024 14:36:31 GMT   (2251kb,D)

Title: Tree Ensembles for Contextual Bandits
Authors: Hannes Nilsson and Rikard Johansson and Niklas {\AA}kerblom and
  Morteza Haghir Chehreghani
Categories: cs.LG cs.AI stat.ML
Comments: The first two authors contributed equally to this work
\\
  We propose a novel framework for contextual multi-armed bandits based on tree
ensembles. Our framework integrates two widely used bandit methods, Upper
Confidence Bound and Thompson Sampling, for both standard and combinatorial
settings. We demonstrate the effectiveness of our framework via several
experimental studies, employing XGBoost, a popular tree ensemble method.
Compared to state-of-the-art methods based on neural networks, our methods
exhibit superior performance in terms of both regret minimization and
computational runtime, when applied to benchmark datasets and the real-world
application of navigation over road networks.
\\ ( https://arxiv.org/abs/2402.06963 ,  2251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06966
Date: Sat, 10 Feb 2024 14:45:23 GMT   (701kb,D)

Title: DeepCover: Advancing RNN Test Coverage and Online Error Prediction using
  State Machine Extraction
Authors: Pouria Golshanrad and Fathiyeh Faghih
Categories: cs.LG
DOI: 10.1016/j.jss.2024.111987
\\
  Recurrent neural networks (RNNs) have emerged as powerful tools for
processing sequential data in various fields, including natural language
processing and speech recognition. However, the lack of explainability in RNN
models has limited their interpretability, posing challenges in understanding
their internal workings. To address this issue, this paper proposes a
methodology for extracting a state machine (SM) from an RNN-based model to
provide insights into its internal function. The proposed SM extraction
algorithm was assessed using four newly proposed metrics: Purity, Richness,
Goodness, and Scale. The proposed methodology along with its assessment metrics
contribute to increasing explainability in RNN models by providing a clear
representation of their internal decision making process through the extracted
SM. In addition to improving the explainability of RNNs, the extracted SM can
be used to advance testing and and monitoring of the primary RNN-based model.
To enhance RNN testing, we introduce six model coverage criteria based on the
extracted SM, serving as metrics for evaluating the effectiveness of test
suites designed to analyze the primary model. We also propose a tree-based
model to predict the error probability of the primary model for each input
based on the extracted SM. We evaluated our proposed online error prediction
approach using the MNIST dataset and Mini Speech Commands dataset, achieving an
area under the curve (AUC) exceeding 80\% for the receiver operating
characteristic (ROC) chart.
\\ ( https://arxiv.org/abs/2402.06966 ,  701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06968
Date: Sat, 10 Feb 2024 14:56:36 GMT   (824kb,D)

Title: Contextual Stochastic Vehicle Routing with Time Windows
Authors: Breno Serrano, Alexandre M. Florio, Stefan Minner, Maximilian
  Schiffer, Thibaut Vidal
Categories: cs.LG math.OC
\\
  We study the vehicle routing problem with time windows (VRPTW) and stochastic
travel times, in which the decision-maker observes related contextual
information, represented as feature variables, before making routing decisions.
Despite the extensive literature on stochastic VRPs, the integration of feature
variables has received limited attention in this context. We introduce the
contextual stochastic VRPTW, which minimizes the total transportation cost and
expected late arrival penalties conditioned on the observed features. Since the
joint distribution of travel times and features is unknown, we present novel
data-driven prescriptive models that use historical data to provide an
approximate solution to the problem. We distinguish the prescriptive models
between point-based approximation, sample average approximation, and
penalty-based approximation, each taking a different perspective on dealing
with stochastic travel times and features. We develop specialized
branch-price-and-cut algorithms to solve these data-driven prescriptive models.
In our computational experiments, we compare the out-of-sample cost performance
of different methods on instances with up to one hundred customers. Our results
show that, surprisingly, a feature-dependent sample average approximation
outperforms existing and novel methods in most settings.
\\ ( https://arxiv.org/abs/2402.06968 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06971
Date: Sat, 10 Feb 2024 15:23:45 GMT   (14450kb,D)

Title: In-Context Data Distillation with TabPFN
Authors: Junwei Ma, Valentin Thomas, Guangwei Yu, Anthony Caterini
Categories: cs.LG
\\
  Foundation models have revolutionized tasks in computer vision and natural
language processing. However, in the realm of tabular data, tree-based models
like XGBoost continue to dominate. TabPFN, a transformer model tailored for
tabular data, mirrors recent foundation models in its exceptional in-context
learning capability, being competitive with XGBoost's performance without the
need for task-specific training or hyperparameter tuning. Despite its promise,
TabPFN's applicability is hindered by its data size constraint, limiting its
use in real-world scenarios. To address this, we present in-context data
distillation (ICD), a novel methodology that effectively eliminates these
constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to
handle significantly larger datasets with a fixed memory budget, improving
TabPFN's quadratic memory complexity but at the cost of a linear number of
tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong
performance against established tree-based models and modern deep learning
methods on 48 large tabular datasets from OpenML.
\\ ( https://arxiv.org/abs/2402.06971 ,  14450kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06974
Date: Sat, 10 Feb 2024 15:42:03 GMT   (9429kb,D)

Title: Non-linear Fusion in Federated Learning: A Hypernetwork Approach to
  Federated Domain Generalization
Authors: Marc Bartholet, Taehyeon Kim, Ami Beuret, Se-Young Yun, Joachim M.
  Buhmann
Categories: cs.LG
Comments: Submitted to ICML 2024 Conference on 01.02.2024; currently Under
  Review
\\
  Federated Learning (FL) has emerged as a promising paradigm in which multiple
clients collaboratively train a shared global model while preserving data
privacy. To create a robust and practicable FL framework, it is crucial to
extend its ability to generalize well to unseen domains - a problem referred to
as federated Domain Generalization (FDG), being still under-explored. We
propose an innovative federated algorithm, termed hFedF for hypernetwork-based
Federated Fusion, designed to bridge the performance gap between generalization
and personalization, capable of addressing various degrees of domain shift.
Essentially, the hypernetwork supports a non-linear fusion of client models
enabling a comprehensive understanding of the underlying data distribution. We
encompass an extensive discussion and provide novel insights into the tradeoff
between personalization and generalization in FL. The proposed algorithm
outperforms strong benchmarks on three widely-used data sets for DG in an
exceeding number of cases.
\\ ( https://arxiv.org/abs/2402.06974 ,  9429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06990
Date: Sat, 10 Feb 2024 16:47:53 GMT   (79kb,D)

Title: Guided Sketch-Based Program Induction by Search Gradients
Authors: Ahmad Ayaz Amin
Categories: cs.LG cs.PL
\\
  Many tasks can be easily solved using machine learning techniques. However,
some tasks cannot readily be solved using statistical models, requiring a
symbolic approach instead. Program induction is one of the ways that such tasks
can be solved by means of capturing an interpretable and generalizable
algorithm through training. However, contemporary approaches to program
induction are not sophisticated enough to readily be applied to various types
of tasks as they tend to be formulated as a single, all-encompassing model,
usually parameterized by neural networks. In an attempt to make program
induction a viable solution for many scenarios, we propose a framework for
learning parameterized programs via search gradients using evolution
strategies. This formulation departs from traditional program induction as it
allows for the programmer to impart task-specific code to the program 'sketch',
while also enjoying the benefits of accelerated learning through end-to-end
gradient-based optimization.
\\ ( https://arxiv.org/abs/2402.06990 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07002
Date: Sat, 10 Feb 2024 17:39:34 GMT   (5245kb,D)

Title: Clients Collaborate: Flexible Differentially Private Federated Learning
  with Guaranteed Improvement of Utility-Privacy Trade-off
Authors: Yuecheng Li, Tong Wang, Chuan Chen, Jian Lou, Bin Chen, Lei Yang,
  Zibin Zheng
Categories: cs.LG cs.AI cs.CR
Comments: 22 pages, 8 figures
\\
  To defend against privacy leakage of user data, differential privacy is
widely used in federated learning, but it is not free. The addition of noise
randomly disrupts the semantic integrity of the model and this disturbance
accumulates with increased communication rounds. In this paper, we introduce a
novel federated learning framework with rigorous privacy guarantees, named
FedCEO, designed to strike a trade-off between model utility and user privacy
by letting clients ''Collaborate with Each Other''. Specifically, we perform
efficient tensor low-rank proximal optimization on stacked local model
parameters at the server, demonstrating its capability to flexibly truncate
high-frequency components in spectral space. This implies that our FedCEO can
effectively recover the disrupted semantic information by smoothing the global
semantic space for different privacy settings and continuous training
processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an
order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our
theoretical results with experiments on representative image datasets. It
observes significant performance improvements and strict privacy guarantees
under different privacy settings.
\\ ( https://arxiv.org/abs/2402.07002 ,  5245kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07011
Date: Sat, 10 Feb 2024 18:14:57 GMT   (1811kb,D)

Title: FedImpro: Measuring and Improving Client Update in Federated Learning
Authors: Zhenheng Tang, Yonggang Zhang, Shaohuai Shi, Xinmei Tian, Tongliang
  Liu, Bo Han, Xiaowen Chu
Categories: cs.LG cs.AI cs.DC
\\
  Federated Learning (FL) models often experience client drift caused by
heterogeneous data, where the distribution of data differs across clients. To
address this issue, advanced research primarily focuses on manipulating the
existing gradients to achieve more consistent client models. In this paper, we
present an alternative perspective on client drift and aim to mitigate it by
generating improved local models. First, we analyze the generalization
contribution of local training and conclude that this generalization
contribution is bounded by the conditional Wasserstein distance between the
data distribution of different clients. Then, we propose FedImpro, to construct
similar conditional distributions for local training. Specifically, FedImpro
decouples the model into high-level and low-level components, and trains the
high-level portion on reconstructed feature distributions. This approach
enhances the generalization contribution and reduces the dissimilarity of
gradients in FL. Experimental results show that FedImpro can help FL defend
against data heterogeneity and enhance the generalization performance of the
model.
\\ ( https://arxiv.org/abs/2402.07011 ,  1811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07019
Date: Sat, 10 Feb 2024 18:36:42 GMT   (3878kb,D)

Title: Informativeness of Reward Functions in Reinforcement Learning
Authors: Rati Devidze, Parameswaran Kamalaruban, Adish Singla
Categories: cs.LG
Comments: Longer version of the AAMAS'24 paper
\\
  Reward functions are central in specifying the task we want a reinforcement
learning agent to perform. Given a task and desired optimal behavior, we study
the problem of designing informative reward functions so that the designed
rewards speed up the agent's convergence. In particular, we consider
expert-driven reward design settings where an expert or teacher seeks to
provide informative and interpretable rewards to a learning agent. Existing
works have considered several different reward design formulations; however,
the key challenge is formulating a reward informativeness criterion that adapts
w.r.t. the agent's current policy and can be optimized under specified
structural constraints to obtain interpretable rewards. In this paper, we
propose a novel reward informativeness criterion, a quantitative measure that
captures how the agent's current policy will improve if it receives rewards
from a specific reward function. We theoretically showcase the utility of the
proposed informativeness criterion for adaptively designing rewards for an
agent. Experimental results on two navigation tasks demonstrate the
effectiveness of our adaptive reward informativeness criterion.
\\ ( https://arxiv.org/abs/2402.07019 ,  3878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07033
Date: Sat, 10 Feb 2024 19:54:08 GMT   (896kb,D)

Title: Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts
  Models
Authors: Keisuke Kamahori, Yile Gu, Kan Zhu, Baris Kasikci
Categories: cs.LG cs.AI cs.OS
\\
  Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture
are showing promising performance on various tasks. However, running them on
resource-constrained settings, where GPU memory resources are not abundant, is
challenging due to huge model sizes. Existing systems that offload model
weights to CPU memory suffer from the significant overhead of frequently moving
data between CPU and GPU. In this paper, we propose Fiddler, a
resource-efficient inference engine with CPU-GPU orchestration for MoE models.
The key idea of Fiddler is to use the computation ability of the CPU to
minimize the data movement between the CPU and GPU. Our evaluation shows that
Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in
parameters, to generate over $3$ tokens per second on a single GPU with 24GB
memory, showing an order of magnitude improvement over existing methods. The
code of Fiddler is publicly available at
\url{https://github.com/efeslab/fiddler}
\\ ( https://arxiv.org/abs/2402.07033 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07035
Date: Sat, 10 Feb 2024 20:06:26 GMT   (1257kb,D)

Title: Distilling Symbolic Priors for Concept Learning into Neural Networks
Authors: Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths
Categories: cs.LG cs.AI
Comments: 8 pages, 6 figures, 4 tables
\\
  Humans can learn new concepts from a small number of examples by drawing on
their inductive biases. These inductive biases have previously been captured by
using Bayesian models defined over symbolic hypothesis spaces. Is it possible
to create a neural network that displays the same inductive biases? We show
that inductive biases that enable rapid concept learning can be instantiated in
artificial neural networks by distilling a prior distribution from a symbolic
Bayesian model via meta-learning, an approach for extracting the common
structure from a set of tasks. By generating the set of tasks used in
meta-learning from the prior distribution of a Bayesian model, we are able to
transfer that prior into a neural network. We use this approach to create a
neural network with an inductive bias towards concepts expressed as short
logical formulas. Analyzing results from previous behavioral experiments in
which people learned logical concepts from a few examples, we find that our
meta-trained models are highly aligned with human performance.
\\ ( https://arxiv.org/abs/2402.07035 ,  1257kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07043
Date: Sat, 10 Feb 2024 21:06:34 GMT   (3128kb,D)

Title: A Tale of Tails: Model Collapse as a Change of Scaling Laws
Authors: Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton and Julia
  Kempe
Categories: cs.LG cs.AI
\\
  As AI model size grows, neural scaling laws have become a crucial tool to
predict the improvements of large models when increasing capacity and the size
of original (human or natural) training data. Yet, the widespread use of
popular models means that the ecosystem of online data and text will co-evolve
to progressively contain increased amounts of synthesized data. In this paper
we ask: How will the scaling laws change in the inevitable regime where
synthetic data makes its way into the training corpus? Will future models,
still improve, or be doomed to degenerate up to total (model) collapse? We
develop a theoretical framework of model collapse through the lens of scaling
laws. We discover a wide range of decay phenomena, analyzing loss of scaling,
shifted scaling with number of generations, the ''un-learning" of skills, and
grokking when mixing human and synthesized data. Our theory is validated by
large-scale experiments with a transformer on an arithmetic task and text
generation using the large language model Llama2.
\\ ( https://arxiv.org/abs/2402.07043 ,  3128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07051
Date: Sat, 10 Feb 2024 21:46:34 GMT   (327kb,D)

Title: $L^*LM$: Learning Automata from Examples using Natural Language Oracles
Authors: Marcell Vazquez-Chanlatte, Karim Elmaaroufi, Stefan J. Witwicki,
  Sanjit A. Seshia
Categories: cs.LG cs.AI cs.FL
\\
  Expert demonstrations have proven an easy way to indirectly specify complex
tasks. Recent algorithms even support extracting unambiguous formal
specifications, e.g. deterministic finite automata (DFA), from demonstrations.
Unfortunately, these techniques are generally not sample efficient. In this
work, we introduce $L^*LM$, an algorithm for learning DFAs from both
demonstrations and natural language. Due to the expressivity of natural
language, we observe a significant improvement in the data efficiency of
learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large
language models to answer membership queries about the underlying task. This is
then combined with recent techniques for transforming learning from
demonstrations into a sequence of labeled example learning problems. In our
experiments, we observe the two modalities complement each other, yielding a
powerful few-shot learner.
\\ ( https://arxiv.org/abs/2402.07051 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07052
Date: Sat, 10 Feb 2024 21:51:59 GMT   (72kb,D)

Title: Understanding the Training Speedup from Sampling with Approximate Losses
Authors: Rudrajit Das, Xi Chen, Bertram Ieong, Parikshit Bansal, Sujay Sanghavi
Categories: cs.LG stat.ML
\\
  It is well known that selecting samples with large losses/gradients can
significantly reduce the number of training steps. However, the selection
overhead is often too high to yield any meaningful gains in terms of overall
training time. In this work, we focus on the greedy approach of selecting
samples with large \textit{approximate losses} instead of exact losses in order
to reduce the selection overhead. For smooth convex losses, we show that such a
greedy strategy can converge to a constant factor of the minimum value of the
average loss in fewer iterations than the standard approach of random
selection. We also theoretically quantify the effect of the approximation
level. We then develop SIFT which uses early exiting to obtain approximate
losses with an intermediate layer's representations for sample selection. We
evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model
and show significant gains (in terms of training hours and number of
backpropagation steps) without any optimized implementation over vanilla
training. For e.g., to reach 64% validation accuracy, SIFT with exit at the
first layer takes ~43 hours compared to ~57 hours of vanilla training.
\\ ( https://arxiv.org/abs/2402.07052 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07062
Date: Sat, 10 Feb 2024 22:38:21 GMT   (33492kb,D)

Title: Fast UCB-type algorithms for stochastic bandits with heavy and super
  heavy symmetric noise
Authors: Yuriy Dorn, Aleksandr Katrutsa, Ilgam Latypov, Andrey Pudovikov
Categories: cs.LG math.OC stat.ML
\\
  In this study, we propose a new method for constructing UCB-type algorithms
for stochastic multi-armed bandits based on general convex optimization methods
with an inexact oracle. We derive the regret bounds corresponding to the
convergence rates of the optimization methods. We propose a new algorithm
Clipped-SGD-UCB and show, both theoretically and empirically, that in the case
of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$
regret bound instead of $O\left (T^{\frac{1}{1+\alpha}}
K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution
satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$
($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general
lower bound for bandits with heavy-tails. Moreover, the same bound holds even
when the reward distribution does not have the expectation, that is, when
$\alpha<0$.
\\ ( https://arxiv.org/abs/2402.07062 ,  33492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07069
Date: Sun, 11 Feb 2024 00:00:05 GMT   (818kb,D)

Title: Using Large Language Models to Automate and Expedite Reinforcement
  Learning with Reward Machine
Authors: Shayan Meshkat Alsadat, Jean-Raphael Gaglione, Daniel Neider, Ufuk
  Topcu, and Zhe Xu
Categories: cs.LG cs.AI cs.CL
\\
  We present LARL-RM (Large language model-generated Automaton for
Reinforcement Learning with Reward Machine) algorithm in order to encode
high-level knowledge into reinforcement learning using automaton to expedite
the reinforcement learning. Our method uses Large Language Models (LLM) to
obtain high-level domain-specific knowledge using prompt engineering instead of
providing the reinforcement learning algorithm directly with the high-level
knowledge which requires an expert to encode the automaton. We use
chain-of-thought and few-shot methods for prompt engineering and demonstrate
that our method works using these approaches. Additionally, LARL-RM allows for
fully closed-loop reinforcement learning without the need for an expert to
guide and supervise the learning since LARL-RM can use the LLM directly to
generate the required high-level knowledge for the task at hand. We also show
the theoretical guarantee of our algorithm to converge to an optimal policy. We
demonstrate that LARL-RM speeds up the convergence by 30% by implementing our
method in two case studies.
\\ ( https://arxiv.org/abs/2402.07069 ,  818kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07079
Date: Sun, 11 Feb 2024 01:21:56 GMT   (3287kb,D)

Title: The Relevance Feature and Vector Machine for health applications
Authors: Albert Belenguer-Llorens, Carlos Sevilla-Salcedo, Emilio
  Parrado-Hern\'andez and Vanessa G\'omez-Verdejo
Categories: cs.LG
Comments: 19 pages of main text, 12 pages of appendices, 2 figures and 5 tables
\\
  This paper presents the Relevance Feature and Vector Machine (RFVM), a novel
model that addresses the challenges of the fat-data problem when dealing with
clinical prospective studies. The fat-data problem refers to the limitations of
Machine Learning (ML) algorithms when working with databases in which the
number of features is much larger than the number of samples (a common scenario
in certain medical fields). To overcome such limitations, the RFVM incorporates
different characteristics: (1) A Bayesian formulation which enables the model
to infer its parameters without overfitting thanks to the Bayesian model
averaging. (2) A joint optimisation that overcomes the limitations arising from
the fat-data characteristic by simultaneously including the variables that
define the primal space (features) and those that define the dual space
(observations). (3) An integrated prunning that removes the irrelevant features
and samples during the training iterative optimization. Also, this last point
turns out crucial when performing medical prospective studies, enabling
researchers to exclude unnecessary medical tests, reducing costs and
inconvenience for patients, and identifying the critical patients/subjects that
characterize the disorder and, subsequently, optimize the patient recruitment
process that leads to a balanced cohort. The model capabilities are tested
against state-of-the-art models in several medical datasets with fat-data
problems. These experimental works show that RFVM is capable of achieving
competitive classification accuracies while providing the most compact subset
of data (in both terms of features and samples). Moreover, the selected
features (medical tests) seem to be aligned with the existing medical
literature.
\\ ( https://arxiv.org/abs/2402.07079 ,  3287kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07082
Date: Sun, 11 Feb 2024 01:51:15 GMT   (42kb)

Title: Refined Sample Complexity for Markov Games with Independent Linear
  Function Approximation
Authors: Yan Dai, Qiwen Cui, Simon S. Du
Categories: cs.LG cs.GT stat.ML
\\
  Markov Games (MG) is an important model for Multi-Agent Reinforcement
Learning (MARL). It was long believed that the "curse of multi-agents" (i.e.,
the algorithmic performance drops exponentially with the number of agents) is
unavoidable until several recent works (Daskalakis et al., 2023; Cui et al.,
2023; Wang et al., 2023. While these works did resolve the curse of
multi-agents, when the state spaces are prohibitively large and (linear)
function approximations are deployed, they either had a slower convergence rate
of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions
$A_{\max}$ -- which is avoidable in single-agent cases even when the loss
functions can arbitrarily vary with time (Dai et al., 2023). This paper first
refines the `AVLPR` framework by Wang et al. (2023), with an insight of
*data-dependent* (i.e., stochastic) pessimistic estimation of the
sub-optimality gap, allowing a broader choice of plug-in algorithms. When
specialized to MGs with independent linear function approximations, we propose
novel *action-dependent bonuses* to cover occasionally extreme estimation
errors. With the help of state-of-the-art techniques from the single-agent RL
literature, we give the first algorithm that tackles the curse of multi-agents,
attains the optimal $O(T^{-1/2})$ convergence rate, and avoids
$\text{poly}(A_{\max})$ dependency simultaneously.
\\ ( https://arxiv.org/abs/2402.07082 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07087
Date: Sun, 11 Feb 2024 02:34:42 GMT   (11149kb,D)

Title: Self-Correcting Self-Consuming Loops for Generative Model Training
Authors: Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin
  Luo, Yonglong Tian, Chen Sun
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: Under submission. Code will be released at
  https://nategillman.com/sc-sc.html
\\
  As synthetic data becomes higher quality and proliferates on the internet,
machine learning models are increasingly trained on a mix of human- and
machine-generated data. Despite the successful stories of using synthetic data
for representation learning, using synthetic data for generative model training
creates "self-consuming loops" which may lead to training instability or even
collapse, unless certain conditions are met. Our paper aims to stabilize
self-consuming generative model training. Our theoretical results demonstrate
that by introducing an idealized correction function, which maps a data point
to be more likely under the true data distribution, self-consuming loops can be
made exponentially more stable. We then propose self-correction functions,
which rely on expert knowledge (e.g. the laws of physics programmed in a
simulator), and aim to approximate the idealized corrector automatically and at
scale. We empirically validate the effectiveness of self-correcting
self-consuming loops on the challenging human motion synthesis task, and
observe that it successfully avoids model collapse, even when the ratio of
synthetic data to real data is as high as 100%.
\\ ( https://arxiv.org/abs/2402.07087 ,  11149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07099
Date: Sun, 11 Feb 2024 04:09:50 GMT   (62kb,D)

Title: Rethinking the Capacity of Graph Neural Networks for Branching Strategy
Authors: Ziang Chen, Jialin Liu, Xiaohan Chen, Xinshang Wang, Wotao Yin
Categories: cs.LG math.OC
\\
  Graph neural networks (GNNs) have been widely used to predict properties and
heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP
solvers. This paper investigates the capacity of GNNs to represent strong
branching (SB) scores that provide an efficient strategy in the
branch-and-bound algorithm.
  Although message-passing GNN (MP-GNN), as the simplest GNN structure, is
frequently employed in the existing literature to learn SB scores, we prove a
fundamental limitation in its expressive power -- there exist two MILP
instances with different SB scores that cannot be distinguished by any MP-GNN,
regardless of the number of parameters. In addition, we establish a universal
approximation theorem for another GNN structure called the second-order
folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there
always exists a 2-FGNN that can approximate the SB score with arbitrarily high
accuracy and arbitrarily high probability. A small-scale numerical experiment
is conducted to directly validate our theoretical findings.
\\ ( https://arxiv.org/abs/2402.07099 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07102
Date: Sun, 11 Feb 2024 04:53:40 GMT   (13694kb,D)

Title: Future Prediction Can be a Strong Evidence of Good History
  Representation in Partially Observable Environments
Authors: Jeongyeol Kwon, Liu Yang, Robert Nowak, Josiah Hanna
Categories: cs.LG cs.AI
\\
  Learning a good history representation is one of the core challenges of
reinforcement learning (RL) in partially observable environments. Recent works
have shown the advantages of various auxiliary tasks for facilitating
representation learning. However, the effectiveness of such auxiliary tasks has
not been fully convincing, especially in partially observable environments that
require long-term memorization and inference. In this empirical study, we
investigate the effectiveness of future prediction for learning the
representations of histories, possibly of extensive length, in partially
observable environments. We first introduce an approach that decouples the task
of learning history representations from policy optimization via future
prediction. Then, our main contributions are two-fold: (a) we demonstrate that
the performance of reinforcement learning is strongly correlated with the
prediction accuracy of future observations in partially observable
environments, and (b) our approach can significantly improve the overall
end-to-end approach by preventing high-variance noisy signals from
reinforcement learning objectives to influence the representation learning. We
illustrate our claims on three types of benchmarks that necessitate the ability
to process long histories for high returns.
\\ ( https://arxiv.org/abs/2402.07102 ,  13694kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07107
Date: Sun, 11 Feb 2024 05:17:56 GMT   (5895kb,D)

Title: Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential
  Reinforcement Learning
Authors: Alex Christopher Stutts, Danilo Erricolo, Theja Tulabandhula, Amit
  Ranjan Trivedi
Categories: cs.LG cs.AI
\\
  We present a novel statistical approach to incorporating uncertainty
awareness in model-free distributional reinforcement learning involving
quantile regression-based deep Q networks. The proposed algorithm,
$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks
(CEQR-DQN)}$, aims to address key challenges associated with separately
estimating aleatoric and epistemic uncertainty in stochastic environments. It
combines deep evidential learning with quantile calibration based on principles
of conformal inference to provide explicit, sample-free computations of
$\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on
simple variance, overcoming limitations of traditional methods in computational
and statistical efficiency and handling of out-of-distribution (OOD)
observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar),
CEQR-DQN is shown to surpass similar existing frameworks in scores and learning
speed. Its ability to rigorously evaluate uncertainty improves exploration
strategies and can serve as a blueprint for other algorithms requiring
uncertainty awareness.
\\ ( https://arxiv.org/abs/2402.07107 ,  5895kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07108
Date: Sun, 11 Feb 2024 05:35:50 GMT   (2120kb,D)

Title: Decoupling Learning and Decision-Making: Breaking the
  $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with
  First-Order Methods
Authors: Wenzhi Gao, Chunlin Sun, Chenyu Xue, Dongdong Ge, Yinyu Ye
Categories: cs.LG math.OC
\\
  Online linear programming plays an important role in both revenue management
and resource allocation, and recent research has focused on developing
efficient first-order online learning algorithms. Despite the empirical success
of first-order methods, they typically achieve a regret no better than
$\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log
T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based
online algorithms. This paper establishes several important facts about online
linear programming, which unveils the challenge for first-order-method-based
online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address
the challenge, we introduce a new algorithmic framework that decouples learning
from decision-making. More importantly, for the first time, we show that
first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new
framework. Lastly, we conduct numerical experiments to validate our theoretical
findings.
\\ ( https://arxiv.org/abs/2402.07108 ,  2120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07114
Date: Sun, 11 Feb 2024 06:21:18 GMT   (64kb,D)

Title: Towards Quantifying the Preconditioning Effect of Adam
Authors: Rudrajit Das, Naman Agarwal, Sujay Sanghavi, Inderjit S. Dhillon
Categories: cs.LG math.OC stat.ML
\\
  There is a notable dearth of results characterizing the preconditioning
effect of Adam and showing how it may alleviate the curse of ill-conditioning
-- an issue plaguing gradient descent (GD). In this work, we perform a detailed
analysis of Adam's preconditioning effect for quadratic functions and quantify
to what extent Adam can mitigate the dependence on the condition number of the
Hessian. Our key finding is that Adam can suffer less from the condition number
but at the expense of suffering a dimension-dependent quantity. Specifically,
for a $d$-dimensional quadratic with a diagonal Hessian having condition number
$\kappa$, we show that the effective condition number-like quantity controlling
the iteration complexity of Adam without momentum is $\mathcal{O}(\min(d,
\kappa))$. For a diagonally dominant Hessian, we obtain a bound of
$\mathcal{O}(\min(d \sqrt{d \kappa}, \kappa))$ for the corresponding quantity.
Thus, when $d < \mathcal{O}(\kappa^p)$ where $p = 1$ for a diagonal Hessian and
$p = 1/3$ for a diagonally dominant Hessian, Adam can outperform GD (which has
an $\mathcal{O}(\kappa)$ dependence). On the negative side, our results suggest
that Adam can be worse than GD for a sufficiently non-diagonal Hessian even if
$d \ll \mathcal{O}(\kappa^{1/3})$; we corroborate this with empirical evidence.
Finally, we extend our analysis to functions satisfying per-coordinate
Lipschitz smoothness and a modified version of the Polyak-\L ojasiewicz
condition.
\\ ( https://arxiv.org/abs/2402.07114 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07129
Date: Sun, 11 Feb 2024 08:54:37 GMT   (787kb)

Title: An attempt to generate new bridge types from latent space of denoising
  diffusion Implicit model
Authors: Hongjun Zhang
Categories: cs.LG cs.AI cs.CV
Comments: 9 pages, 7 figures
\\
  Use denoising diffusion implicit model for bridge-type innovation. The
process of adding noise and denoising to an image can be likened to the process
of a corpse rotting and a detective restoring the scene of a victim being
killed, to help beginners understand. Through an easy-to-understand algebraic
method, derive the function formulas for adding noise and denoising, making it
easier for beginners to master the mathematical principles of the model. Using
symmetric structured image dataset of three-span beam bridge, arch bridge,
cable-stayed bridge and suspension bridge , based on Python programming
language, TensorFlow and Keras deep learning platform framework , denoising
diffusion implicit model is constructed and trained. From the latent space
sampling, new bridge types with asymmetric structures can be generated.
Denoising diffusion implicit model can organically combine different structural
components on the basis of human original bridge types, and create new bridge
types.
\\ ( https://arxiv.org/abs/2402.07129 ,  787kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07139
Date: Sun, 11 Feb 2024 09:46:15 GMT   (10412kb,D)

Title: Towards Robust Car Following Dynamics Modeling via Blackbox Models:
  Methodology, Analysis, and Recommendations
Authors: Muhammad Bilal Shahid, Cody Fleming
Categories: cs.LG
\\
  The selection of the target variable is important while learning parameters
of the classical car following models like GIPPS, IDM, etc. There is a vast
body of literature on which target variable is optimal for classical car
following models, but there is no study that empirically evaluates the
selection of optimal target variables for black-box models, such as LSTM, etc.
The black-box models, like LSTM and Gaussian Process (GP) are increasingly
being used to model car following behavior without wise selection of target
variables. The current work tests different target variables, like
acceleration, velocity, and headway, for three black-box models, i.e., GP,
LSTM, and Kernel Ridge Regression. These models have different objective
functions and work in different vector spaces, e.g., GP works in function
space, and LSTM works in parameter space. The experiments show that the optimal
target variable recommendations for black-box models differ from classical car
following models depending on the objective function and the vector space. It
is worth mentioning that models and datasets used during evaluation are diverse
in nature: the datasets contained both automated and human-driven vehicle
trajectories; the black-box models belong to both parametric and non-parametric
classes of models. This diversity is important during the analysis of variance,
wherein we try to find the interaction between datasets, models, and target
variables. It is shown that the models and target variables interact and
recommended target variables don't depend on the dataset under consideration.
\\ ( https://arxiv.org/abs/2402.07139 ,  10412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07152
Date: Sun, 11 Feb 2024 10:44:41 GMT   (4768kb,D)

Title: Explainable Global Wildfire Prediction Models using Graph Neural
  Networks
Authors: Dayou Chen and Sibo Cheng and Jinwei Hu and Matthew Kasoar and
  Rossella Arcucci
Categories: cs.LG cs.AI
\\
  Wildfire prediction has become increasingly crucial due to the escalating
impacts of climate change. Traditional CNN-based wildfire prediction models
struggle with handling missing oceanic data and addressing the long-range
dependencies across distant regions in meteorological data. In this paper, we
introduce an innovative Graph Neural Network (GNN)-based model for global
wildfire prediction. We propose a hybrid model that combines the spatial
prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long
Short-Term Memory (LSTM) networks. Our approach uniquely transforms global
climate and wildfire data into a graph representation, addressing challenges
such as null oceanic data locations and long-range dependencies inherent in
traditional models. Benchmarking against established architectures using an
unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior
predictive accuracy. Furthermore, we emphasise the model's explainability,
unveiling potential wildfire correlation clusters through community detection
and elucidating feature importance via Integrated Gradient analysis. Our
findings not only advance the methodological domain of wildfire prediction but
also underscore the importance of model transparency, offering valuable
insights for stakeholders in wildfire management.
\\ ( https://arxiv.org/abs/2402.07152 ,  4768kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07164
Date: Sun, 11 Feb 2024 11:20:29 GMT   (602kb,D)

Title: GeoFormer: A Vision and Sequence Transformer-based Approach for
  Greenhouse Gas Monitoring
Authors: Madhav Khirwar and Ankur Narang
Categories: cs.LG cs.CV
\\
  Air pollution represents a pivotal environmental challenge globally, playing
a major role in climate change via greenhouse gas emissions and negatively
affecting the health of billions. However predicting the spatial and temporal
patterns of pollutants remains challenging. The scarcity of ground-based
monitoring facilities and the dependency of air pollution modeling on
comprehensive datasets, often inaccessible for numerous areas, complicate this
issue. In this work, we introduce GeoFormer, a compact model that combines a
vision transformer module with a highly efficient time-series transformer
module to predict surface-level nitrogen dioxide (NO2) concentrations from
Sentinel-5P satellite imagery. We train the proposed model to predict
surface-level NO2 measurements using a dataset we constructed with Sentinel-5P
images of ground-level monitoring stations, and their corresponding NO2
concentration readings. The proposed model attains high accuracy (MAE 5.65),
demonstrating the efficacy of combining vision and time-series transformer
architectures to harness satellite-derived data for enhanced GHG emission
insights, proving instrumental in advancing climate change monitoring and
emission regulation efforts globally.
\\ ( https://arxiv.org/abs/2402.07164 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07180
Date: Sun, 11 Feb 2024 12:29:16 GMT   (5275kb,D)

Title: MAGNETO: Edge AI for Human Activity Recognition -- Privacy and
  Personalization
Authors: Jingwei Zuo, George Arvanitakis, Mthandazo Ndhlovu and Hakim Hacid
Categories: cs.LG cs.AI cs.CR
Comments: Accepted by EDBT 2024 (demo track)
\\
  Human activity recognition (HAR) is a well-established field, significantly
advanced by modern machine learning (ML) techniques. While companies have
successfully integrated HAR into consumer products, they typically rely on a
predefined activity set, which limits personalizations at the user level (edge
devices). Despite advancements in Incremental Learning for updating models with
new data, this often occurs on the Cloud, necessitating regular data transfers
between cloud and edge devices, thus leading to data privacy issues. In this
paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the
Cloud to the Edge. MAGNETO allows incremental human activity learning directly
on the Edge devices, without any data exchange with the Cloud. This enables
strong privacy guarantees, low processing latency, and a high degree of
personalization for users. In particular, we demonstrate MAGNETO in an Android
device, validating the whole pipeline from data collection to result
visualization.
\\ ( https://arxiv.org/abs/2402.07180 ,  5275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07182
Date: Sun, 11 Feb 2024 12:35:13 GMT   (1013kb,D)

Title: Divide and Conquer: Provably Unveiling the Pareto Front with
  Multi-Objective Reinforcement Learning
Authors: Willem R\"opke, Mathieu Reymond, Patrick Mannion, Diederik M. Roijers,
  Ann Now\'e, Roxana R\u{a}dulescu
Categories: cs.LG
\\
  A significant challenge in multi-objective reinforcement learning is
obtaining a Pareto front of policies that attain optimal performance under
different preferences. We introduce Iterated Pareto Referent Optimisation
(IPRO), a principled algorithm that decomposes the task of finding the Pareto
front into a sequence of single-objective problems for which various solution
methods exist. This enables us to establish convergence guarantees while
providing an upper bound on the distance to undiscovered Pareto optimal
solutions at each step. Empirical evaluations demonstrate that IPRO matches or
outperforms methods that require additional domain knowledge. By leveraging
problem-specific single-objective solvers, our approach also holds promise for
applications beyond multi-objective reinforcement learning, such as in
pathfinding and optimisation.
\\ ( https://arxiv.org/abs/2402.07182 ,  1013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07191
Date: Sun, 11 Feb 2024 12:57:16 GMT   (12816kb,D)

Title: GSINA: Improving Subgraph Extraction for Graph Invariant Learning via
  Graph Sinkhorn Attention
Authors: Fangyu Ding, Haiyang Wang, Zhixuan Chu, Tianming Li, Zhaoping Hu,
  Junchi Yan
Categories: cs.LG cs.AI
\\
  Graph invariant learning (GIL) has been an effective approach to discovering
the invariant relationships between graph data and its labels for different
graph learning tasks under various distribution shifts. Many recent endeavors
of GIL focus on extracting the invariant subgraph from the input graph for
prediction as a regularization strategy to improve the generalization
performance of graph learning. Despite their success, such methods also have
various limitations in obtaining their invariant subgraphs. In this paper, we
provide in-depth analyses of the drawbacks of existing works and propose
corresponding principles of our invariant subgraph extraction: 1) the sparsity,
to filter out the variant features, 2) the softness, for a broader solution
space, and 3) the differentiability, for a soundly end-to-end optimization. To
meet these principles in one shot, we leverage the Optimal Transport (OT)
theory and propose a novel graph attention mechanism called Graph Sinkhorn
Attention (GSINA). This novel approach serves as a powerful regularization
method for GIL tasks. By GSINA, we are able to obtain meaningful,
differentiable invariant subgraphs with controllable sparsity and softness.
Moreover, GSINA is a general graph learning framework that could handle GIL
tasks of multiple data grain levels. Extensive experiments on both synthetic
and real-world datasets validate the superiority of our GSINA, which
outperforms the state-of-the-art GIL methods by large margins on both
graph-level tasks and node-level tasks. Our code is publicly available at
\url{https://github.com/dingfangyu/GSINA}.
\\ ( https://arxiv.org/abs/2402.07191 ,  12816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07193
Date: Sun, 11 Feb 2024 13:00:04 GMT   (1106kb,D)

Title: The Implicit Bias of Gradient Noise: A Symmetry Perspective
Authors: Liu Ziyin, Mingze Wang, Lei Wu
Categories: cs.LG math.OC stat.ML
Comments: preprint
\\
  We characterize the learning dynamics of stochastic gradient descent (SGD)
when continuous symmetry exists in the loss function, where the divergence
between SGD and gradient descent is dramatic. We show that depending on how the
symmetry affects the learning dynamics, we can divide a family of symmetry into
two classes. For one class of symmetry, SGD naturally converges to solutions
that have a balanced and aligned gradient noise. For the other class of
symmetry, SGD will almost always diverge. Then, we show that our result remains
applicable and can help us understand the training dynamics even when the
symmetry is not present in the loss function. Our main result is universal in
the sense that it only depends on the existence of the symmetry and is
independent of the details of the loss function. We demonstrate that the
proposed theory offers an explanation of progressive sharpening and flattening
and can be applied to common practical problems such as representation
normalization, matrix factorization, and the use of warmup.
\\ ( https://arxiv.org/abs/2402.07193 ,  1106kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07198
Date: Sun, 11 Feb 2024 13:25:53 GMT   (194kb,D)

Title: More Benefits of Being Distributional: Second-Order Bounds for
  Reinforcement Learning
Authors: Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, Wen Sun
Categories: cs.LG
\\
  In this paper, we prove that Distributional Reinforcement Learning (DistRL),
which learns the return distribution, can obtain second-order bounds in both
online and offline RL in general settings with function approximation.
Second-order bounds are instance-dependent bounds that scale with the variance
of return, which we prove are tighter than the previously known small-loss
bounds of distributional RL. To the best of our knowledge, our results are the
first second-order bounds for low-rank MDPs and for offline RL. When
specializing to contextual bandits (one-step RL problem), we show that a
distributional learning based optimism algorithm achieves a second-order
worst-case regret bound, and a second-order gap dependent bound,
simultaneously. We also empirically demonstrate the benefit of DistRL in
contextual bandits on real-world datasets. We highlight that our analysis with
DistRL is relatively simple, follows the general framework of optimism in the
face of uncertainty and does not require weighted regression. Our results
suggest that DistRL is a promising framework for obtaining second-order bounds
in general RL settings, thus further reinforcing the benefits of DistRL.
\\ ( https://arxiv.org/abs/2402.07198 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07211
Date: Sun, 11 Feb 2024 14:04:13 GMT   (748kb,D)

Title: Towards Fast Stochastic Sampling in Diffusion Generative Models
Authors: Kushagra Pandey, Maja Rudolph, Stephan Mandt
Categories: cs.LG stat.ML
Comments: Accepted in the NeurIPS'23 Workshop on Diffusion Models. arXiv admin
  note: substantial text overlap with arXiv:2310.07894
\\
  Diffusion models suffer from slow sample generation at inference time.
Despite recent efforts, improving the sampling efficiency of stochastic
samplers for diffusion models remains a promising direction. We propose
Splitting Integrators for fast stochastic sampling in pre-trained diffusion
models in augmented spaces. Commonly used in molecular dynamics,
splitting-based integrators attempt to improve sampling efficiency by cleverly
alternating between numerical updates involving the data, auxiliary, or noise
variables. However, we show that a naive application of splitting integrators
is sub-optimal for fast sampling. Consequently, we propose several principled
modifications to naive splitting samplers for improving sampling efficiency and
denote the resulting samplers as Reduced Splitting Integrators. In the context
of Phase Space Langevin Diffusion (PSLD) [Pandey \& Mandt, 2023] on CIFAR-10,
our stochastic sampler achieves an FID score of 2.36 in only 100 network
function evaluations (NFE) as compared to 2.63 for the best baselines.
\\ ( https://arxiv.org/abs/2402.07211 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07225
Date: Sun, 11 Feb 2024 15:21:08 GMT   (1182kb,D)

Title: Rethinking Graph Masked Autoencoders through Alignment and Uniformity
Authors: Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, Liang Wang
Categories: cs.LG
Comments: Accepted by AAAI 2024
\\
  Self-supervised learning on graphs can be bifurcated into contrastive and
generative methods. Contrastive methods, also known as graph contrastive
learning (GCL), have dominated graph self-supervised learning in the past few
years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles
the momentum behind generative methods. Despite the empirical success of
GraphMAE, there is still a dearth of theoretical understanding regarding its
efficacy. Moreover, while both generative and contrastive methods have been
shown to be effective, their connections and differences have yet to be
thoroughly investigated. Therefore, we theoretically build a bridge between
GraphMAE and GCL, and prove that the node-level reconstruction objective in
GraphMAE implicitly performs context-level GCL. Based on our theoretical
analysis, we further identify the limitations of the GraphMAE from the
perspectives of alignment and uniformity, which have been considered as two key
properties of high-quality representations in GCL. We point out that GraphMAE's
alignment performance is restricted by the masking strategy, and the uniformity
is not strictly guaranteed. To remedy the aforementioned limitations, we
propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named
AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy
to provide hard-to-align samples, which improves the alignment performance.
Meanwhile, we introduce an explicit uniformity regularizer to ensure the
uniformity of the learned representations. Experimental results on benchmark
datasets demonstrate the superiority of our model over existing
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.07225 ,  1182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07232
Date: Sun, 11 Feb 2024 15:49:50 GMT   (4321kb,D)

Title: GenSTL: General Sparse Trajectory Learning via Auto-regressive
  Generation of Feature Domains
Authors: Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen,
  Youfang Lin, Huaiyu Wan
Categories: cs.LG
\\
  Trajectories are sequences of timestamped location samples. In sparse
trajectories, the locations are sampled infrequently; and while such
trajectories are prevalent in real-world settings, they are challenging to use
to enable high-quality transportation-related applications. Current
methodologies either assume densely sampled and accurately map-matched
trajectories, or they rely on two-stage schemes, yielding sub-optimal
applications.
  To extend the utility of sparse trajectories, we propose a novel sparse
trajectory learning framework, GenSTL. The framework is pre-trained to form
connections between sparse trajectories and dense counterparts using
auto-regressive generation of feature domains. GenSTL can subsequently be
applied directly in downstream tasks, or it can be fine-tuned first. This way,
GenSTL eliminates the reliance on the availability of large-scale dense and
map-matched trajectory data. The inclusion of a well-crafted feature domain
encoding layer and a hierarchical masked trajectory encoder enhances GenSTL's
learning capabilities and adaptability. Experiments on two real-world
trajectory datasets offer insight into the framework's ability to contend with
sparse trajectories with different sampling intervals and its versatility
across different downstream tasks, thus offering evidence of its practicality
in real-world applications.
\\ ( https://arxiv.org/abs/2402.07232 ,  4321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07246
Date: Sun, 11 Feb 2024 17:10:31 GMT   (395kb,D)

Title: Towards Generalized Inverse Reinforcement Learning
Authors: Chaosheng Dong, Yijia Wang
Categories: cs.LG
\\
  This paper studies generalized inverse reinforcement learning (GIRL) in
Markov decision processes (MDPs), that is, the problem of learning the basic
components of an MDP given observed behavior (policy) that might not be
optimal. These components include not only the reward function and transition
probability matrices, but also the action space and state space that are not
exactly known but are known to belong to given uncertainty sets. We address two
key challenges in GIRL: first, the need to quantify the discrepancy between the
observed policy and the underlying optimal policy; second, the difficulty of
mathematically characterizing the underlying optimal policy when the basic
components of an MDP are unobservable or partially observable. Then, we propose
the mathematical formulation for GIRL and develop a fast heuristic algorithm.
Numerical results on both finite and infinite state problems show the merit of
our formulation and algorithm.
\\ ( https://arxiv.org/abs/2402.07246 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07248
Date: Sun, 11 Feb 2024 17:27:26 GMT   (30kb)

Title: Depth Separations in Neural Networks: Separating the Dimension from the
  Accuracy
Authors: Itay Safran, Daniel Reichman, Paul Valiant
Categories: cs.LG stat.ML
\\
  We prove an exponential separation between depth 2 and depth 3 neural
networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to
constant accuracy, with respect to a distribution with support in $[0,1]^{d}$,
assuming exponentially bounded weights. This addresses an open problem posed in
\citet{safran2019depth}, and proves that the curse of dimensionality manifests
in depth 2 approximation, even in cases where the target function can be
represented efficiently using depth 3. Previously, lower bounds that were used
to separate depth 2 from depth 3 required that at least one of the Lipschitz
parameter, target accuracy or (some measure of) the size of the domain of
approximation scale polynomially with the input dimension, whereas we fix the
former two and restrict our domain to the unit hypercube. Our lower bound holds
for a wide variety of activation functions, and is based on a novel application
of an average- to worst-case random self-reducibility argument, to reduce the
problem to threshold circuits lower bounds.
\\ ( https://arxiv.org/abs/2402.07248 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07249
Date: Sun, 11 Feb 2024 17:29:58 GMT   (1251kb,D)

Title: The Impact of Domain Knowledge and Multi-Modality on Intelligent
  Molecular Property Prediction: A Systematic Survey
Authors: Taojie Kuang, Pengfei Liu, Zhixiang Ren
Categories: cs.LG cs.CE q-bio.BM
\\
  The precise prediction of molecular properties is essential for advancements
in drug development, particularly in virtual screening and compound
optimization. The recent introduction of numerous deep learning-based methods
has shown remarkable potential in enhancing molecular property prediction
(MPP), especially improving accuracy and insights into molecular structures.
Yet, two critical questions arise: does the integration of domain knowledge
augment the accuracy of molecular property prediction and does employing
multi-modal data fusion yield more precise results than unique data source
methods? To explore these matters, we comprehensively review and quantitatively
analyze recent deep learning methods based on various benchmarks. We discover
that integrating molecular information will improve both MPP regression and
classification tasks by upto 3.98% and 1.72%, respectively. We also discover
that the utilizing 3-dimensional information with 1-dimensional and
2-dimensional information simultaneously can substantially enhance MPP upto
4.2%. The two consolidated insights offer crucial guidance for future
advancements in drug discovery.
\\ ( https://arxiv.org/abs/2402.07249 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07250
Date: Sun, 11 Feb 2024 17:32:23 GMT   (7706kb,D)

Title: DIMON: Learning Solution Operators of Partial Differential Equations on
  a Diffeomorphic Family of Domains
Authors: Minglang Yin, Nicolas Charon, Ryan Brody, Lu Lu, Natalia Trayanova,
  Mauro Maggioni
Categories: cs.LG cs.AI cs.CE
\\
  The solution of a PDE over varying initial/boundary conditions on multiple
domains is needed in a wide variety of applications, but it is computationally
expensive if the solution is computed de novo whenever the initial/boundary
conditions of the domain change. We introduce a general operator learning
framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn
approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$,
that learns the map from initial/boundary conditions and domain $\Omega_\theta$
to the solution of the PDE, or to specified functionals thereof. DIMON is based
on transporting a given problem (initial/boundary conditions and domain
$\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where
training data from multiple problems is used to learn the map to the solution
on $\Omega_{0}$, which is then re-mapped to the original domain
$\Omega_{\theta}$. We consider several problems to demonstrate the performance
of the framework in learning both static and time-dependent PDEs on non-rigid
geometries; these include solving the Laplace equation, reaction-diffusion
equations, and a multiscale PDE that characterizes the electrical propagation
on the left ventricle. This work paves the way toward the fast prediction of
PDE solutions on a family of domains and the application of neural operators in
engineering and precision medicine.
\\ ( https://arxiv.org/abs/2402.07250 ,  7706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07251
Date: Sun, 11 Feb 2024 17:40:26 GMT   (947kb,D)

Title: Physics-Informed Neural Networks with Hard Linear Equality Constraints
Authors: Hao Chen, Gonzalo E. Constante Flores, Can Li
Categories: cs.LG math.OC
\\
  Surrogate modeling is used to replace computationally expensive simulations.
Neural networks have been widely applied as surrogate models that enable
efficient evaluations over complex physical systems. Despite this, neural
networks are data-driven models and devoid of any physics. The incorporation of
physics into neural networks can improve generalization and data efficiency.
The physics-informed neural network (PINN) is an approach to leverage known
physical constraints present in the data, but it cannot strictly satisfy them
in the predictions. This work proposes a novel physics-informed neural network,
KKT-hPINN, which rigorously guarantees hard linear equality constraints through
projection layers derived from KKT conditions. Numerical experiments on Aspen
models of a continuous stirred-tank reactor (CSTR) unit, an extractive
distillation subsystem, and a chemical plant demonstrate that this model can
further enhance the prediction accuracy.
\\ ( https://arxiv.org/abs/2402.07251 ,  947kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07281
Date: Sun, 11 Feb 2024 19:12:51 GMT   (6879kb,D)

Title: Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A
  Benchmarking Study
Authors: Santonu Sarkar, Shanay Mehta, Nicole Fernandes, Jyotirmoy Sarkar and
  Snehanshu Saha
Categories: cs.LG
\\
  Detection of anomalous situations for complex mission-critical systems holds
paramount importance when their service continuity needs to be ensured. A major
challenge in detecting anomalies from the operational data arises due to the
imbalanced class distribution problem since the anomalies are supposed to be
rare events. This paper evaluates a diverse array of machine learning-based
anomaly detection algorithms through a comprehensive benchmark study. The paper
contributes significantly by conducting an unbiased comparison of various
anomaly detection algorithms, spanning classical machine learning including
various tree-based approaches to deep learning and outlier detection methods.
The inclusion of 104 publicly available and a few proprietary industrial
systems datasets enhances the diversity of the study, allowing for a more
realistic evaluation of algorithm performance and emphasizing the importance of
adaptability to real-world scenarios. The paper dispels the deep learning myth,
demonstrating that though powerful, deep learning is not a universal solution
in this case. We observed that recently proposed tree-based evolutionary
algorithms outperform in many scenarios. We noticed that tree-based approaches
catch a singleton anomaly in a dataset where deep learning methods fail. On the
other hand, classical SVM performs the best on datasets with more than 10%
anomalies, implying that such scenarios can be best modeled as a classification
problem rather than anomaly detection. To our knowledge, such a study on a
large number of state-of-the-art algorithms using diverse data sets, with the
objective of guiding researchers and practitioners in making informed
algorithmic choices, has not been attempted earlier.
\\ ( https://arxiv.org/abs/2402.07281 ,  6879kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07283
Date: Sun, 11 Feb 2024 19:14:28 GMT   (329kb)

Title: Power Transformer Fault Prediction Based on Knowledge Graphs
Authors: Chao Wang, Zhuo Chen, Ziyan Zhang, Chiyi Li, Kai Song
Categories: cs.LG cs.CL
\\
  In this paper, we address the challenge of learning with limited fault data
for power transformers. Traditional operation and maintenance tools lack
effective predictive capabilities for potential faults. The scarcity of
extensive fault data makes it difficult to apply machine learning techniques
effectively. To solve this problem, we propose a novel approach that leverages
the knowledge graph (KG) technology in combination with gradient boosting
decision trees (GBDT). This method is designed to efficiently learn from a
small set of high-dimensional data, integrating various factors influencing
transformer faults and historical operational data. Our approach enables
accurate safe state assessments and fault analyses of power transformers
despite the limited fault characteristic data. Experimental results demonstrate
that this method outperforms other learning approaches in prediction accuracy,
such as artificial neural networks (ANN) and logistic regression (LR).
Furthermore, it offers significant improvements in progressiveness,
practicality, and potential for widespread application.
\\ ( https://arxiv.org/abs/2402.07283 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07295
Date: Sun, 11 Feb 2024 20:15:52 GMT   (6266kb,D)

Title: Training Heterogeneous Client Models using Knowledge Distillation in
  Serverless Federated Learning
Authors: Mohak Chadha, Pulkit Khera, Jianfeng Gu, Osama Abboud, Michael Gerndt
Categories: cs.LG cs.AI cs.DC
Comments: ACM/SIGAPP Symposium on Applied Computing 2024
\\
  Federated Learning (FL) is an emerging machine learning paradigm that enables
the collaborative training of a shared global model across distributed clients
while keeping the data decentralized. Recent works on designing systems for
efficient FL have shown that utilizing serverless computing technologies,
particularly Function-as-a-Service (FaaS) for FL, can enhance resource
efficiency, reduce training costs, and alleviate the complex infrastructure
management burden on data holders. However, existing serverless FL systems
implicitly assume a uniform global model architecture across all participating
clients during training. This assumption fails to address fundamental
challenges in practical FL due to the resource and statistical data
heterogeneity among FL clients. To address these challenges and enable
heterogeneous client models in serverless FL, we utilize Knowledge Distillation
(KD) in this paper. Towards this, we propose novel optimized serverless
workflows for two popular conventional federated KD techniques, i.e., FedMD and
FedDF. We implement these workflows by introducing several extensions to an
open-source serverless FL system called FedLess. Moreover, we comprehensively
evaluate the two strategies on multiple datasets across varying levels of
client data heterogeneity using heterogeneous client models with respect to
accuracy, fine-grained training times, and costs. Results from our experiments
demonstrate that serverless FedDF is more robust to extreme non-IID data
distributions, is faster, and leads to lower costs than serverless FedMD. In
addition, compared to the original implementation, our optimizations for
particular steps in FedMD and FedDF lead to an average speedup of 3.5x and
1.76x across all datasets.
\\ ( https://arxiv.org/abs/2402.07295 ,  6266kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07309
Date: Sun, 11 Feb 2024 21:16:26 GMT   (8141kb,D)

Title: HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node
  Classification on Text-Attributed Hypergraphs
Authors: Adri\'an Bazaga and Pietro Li\`o and Gos Micklem
Categories: cs.LG cs.CL stat.ML
Comments: 11 pages, 2 figures
\\
  Hypergraphs are marked by complex topology, expressing higher-order
interactions among multiple entities with hyperedges. Lately, hypergraph-based
deep learning methods to learn informative data representations for the problem
of node classification on text-attributed hypergraphs have garnered increasing
research attention. However, existing methods struggle to simultaneously
capture the full extent of hypergraph structural information and the rich
linguistic attributes inherent in the nodes attributes, which largely hampers
their effectiveness and generalizability. To overcome these challenges, we
explore ways to further augment a pretrained BERT model with specialized
hypergraph-aware layers for the task of node classification. Such layers
introduce higher-order structural inductive bias into the language model, thus
improving the model's capacity to harness both higher-order context information
from the hypergraph structure and semantic information present in text. In this
paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model
which simultaneously models hypergraph relational structure while maintaining
the high-quality text encoding capabilities of a pre-trained BERT. Notably,
HyperBERT presents results that achieve a new state-of-the-art on 5 challenging
text-attributed hypergraph node classification benchmarks.
\\ ( https://arxiv.org/abs/2402.07309 ,  8141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07314
Date: Sun, 11 Feb 2024 21:44:21 GMT   (65kb)

Title: A Theoretical Analysis of Nash Learning from Human Feedback under
  General KL-Regularized Preference
Authors: Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, Tong Zhang
Categories: cs.LG stat.ML
Comments: RLHF, NLHF, Alignment for LLMs
\\
  Reinforcement Learning from Human Feedback (RLHF) learns from the preference
signal provided by a probabilistic preference model, which takes a prompt and
two responses as input, and produces a score indicating the preference of one
response against another. So far, the most popular RLHF paradigm is
reward-based, which starts with an initial step of reward modeling, and the
constructed reward is then used to provide a reward signal for the subsequent
reward optimization stage. However, the existence of a reward function is a
strong assumption and the reward-based RLHF is limited in expressivity and
cannot capture the real-world complicated human preference.
  In this work, we provide theoretical insights for a recently proposed
learning paradigm, Nash learning from human feedback (NLHF), which considered a
general preference model and formulated the alignment process as a game between
two competitive LLMs. The learning objective is to find a policy that
consistently generates responses preferred over any competing policy while
staying close to the initial model. The objective is defined as the Nash
equilibrium (NE) of the KL-regularized preference model. We aim to make the
first attempt to study the theoretical learnability of the KL-regularized NLHF
by considering both offline and online settings. For the offline learning from
a pre-collected dataset, we propose algorithms that are efficient under
suitable coverage conditions of the dataset. For batch online learning from
iterative interactions with a preference oracle, our proposed algorithm enjoys
a finite sample guarantee under the structural condition of the underlying
preference model. Our results connect the new NLHF paradigm with traditional RL
theory, and validate the potential of reward-model-free learning under general
preference.
\\ ( https://arxiv.org/abs/2402.07314 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07319
Date: Sun, 11 Feb 2024 22:40:12 GMT   (920kb,D)

Title: ODIN: Disentangled Reward Mitigates Hacking in RLHF
Authors: Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom
  Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro
Categories: cs.LG cs.AI cs.CL
\\
  In this work, we study the issue of reward hacking on the response length, a
challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on
LLMs. A well-formatted, verbose but less helpful response from the LLMs can
often deceive LLMs or even human evaluators to achieve high scores. The same
issue also holds for some reward models in RL. To address the challenges in
both training and evaluation, we establish a more reliable evaluation protocol
for comparing different training configurations, which inspects the trade-off
between LLM evaluation score and response length obtained by varying training
hyperparameters. Based on this evaluation, we conduct large-scale studies,
where the results shed insights into the efficacy of hyperparameters and tricks
used in RL on mitigating length bias. We further propose to improve the reward
model by jointly training two linear heads on shared feature representations to
predict the rewards, one trained to correlate with length, and the other
trained to decorrelate with length and therefore focus more on the actual
content. We then discard the length head in RL to prevent reward hacking on
length. Experiments demonstrate that our approach almost eliminates the reward
correlation with length, and improves the obtained policy by a significant
margin.
\\ ( https://arxiv.org/abs/2402.07319 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07321
Date: Sun, 11 Feb 2024 22:58:49 GMT   (1393kb,D)

Title: Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs
Authors: Bilal Chughtai, Alan Cooney, Neel Nanda
Categories: cs.LG
Comments: NeurIPS 2023 Attributing Model Behaviour at Scale Workshop
\\
  How do transformer-based large language models (LLMs) store and retrieve
knowledge? We focus on the most basic form of this task -- factual recall,
where the model is tasked with explicitly surfacing stored facts in prompts of
form `Fact: The Colosseum is in the country of'. We find that the mechanistic
story behind factual recall is more complex than previously thought. It
comprises several distinct, independent, and qualitatively different mechanisms
that additively combine, constructively interfering on the correct attribute.
We term this generic phenomena the additive motif: models compute through
summing up multiple independent contributions. Each mechanism's contribution
may be insufficient alone, but summing results in constructive interfere on the
correct answer. In addition, we extend the method of direct logit attribution
to attribute an attention head's output to individual source tokens. We use
this technique to unpack what we call `mixed heads' -- which are themselves a
pair of two separate additive updates from different source tokens.
\\ ( https://arxiv.org/abs/2402.07321 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07340
Date: Mon, 12 Feb 2024 00:18:25 GMT   (114kb,D)

Title: Random Geometric Graph Alignment with Graph Neural Networks
Authors: Suqi Liu and Morgane Austern
Categories: cs.LG cs.IT cs.SI math.IT math.PR math.ST stat.ML stat.TH
Comments: 29 pages, 2 figure, 1 table
\\
  We characterize the performance of graph neural networks for graph alignment
problems in the presence of vertex feature information. More specifically,
given two graphs that are independent perturbations of a single random
geometric graph with noisy sparse features, the task is to recover an unknown
one-to-one mapping between the vertices of the two graphs. We show under
certain conditions on the sparsity and noise level of the feature vectors, a
carefully designed one-layer graph neural network can with high probability
recover the correct alignment between the vertices with the help of the graph
structure. We also prove that our conditions on the noise level are tight up to
logarithmic factors. Finally we compare the performance of the graph neural
network to directly solving an assignment problem on the noisy vertex features.
We demonstrate that when the noise level is at least constant this direct
matching fails to have perfect recovery while the graph neural network can
tolerate noise level growing as fast as a power of the size of the graph.
\\ ( https://arxiv.org/abs/2402.07340 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07344
Date: Mon, 12 Feb 2024 00:22:47 GMT   (364kb,D)

Title: Measurement Scheduling for ICU Patients with Offline Reinforcement
  Learning
Authors: Zongliang Ji, Anna Goldenberg, Rahul G. Krishnan
Categories: cs.LG cs.AI
Comments: Extended Abstract presented at Machine Learning for Health (ML4H)
  symposium 2023, December 10th, 2023, New Orleans, United States, 11 pages
\\
  Scheduling laboratory tests for ICU patients presents a significant
challenge. Studies show that 20-40% of lab tests ordered in the ICU are
redundant and could be eliminated without compromising patient safety. Prior
work has leveraged offline reinforcement learning (Offline-RL) to find optimal
policies for ordering lab tests based on patient information. However, new ICU
patient datasets have since been released, and various advancements have been
made in Offline-RL methods. In this study, we first introduce a preprocessing
pipeline for the newly-released MIMIC-IV dataset geared toward time-series
tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in
identifying better policies for ICU patient lab test scheduling. Besides
assessing methodological performance, we also discuss the overall suitability
and practicality of using Offline-RL frameworks for scheduling laboratory tests
in ICU settings.
\\ ( https://arxiv.org/abs/2402.07344 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07347
Date: Mon, 12 Feb 2024 00:36:34 GMT   (258kb,D)

Title: Accuracy of TextFooler black box adversarial attacks on 01 loss sign
  activation neural network ensemble
Authors: Yunzhe Xue and Usman Roshan
Categories: cs.LG cs.AI cs.CR
\\
  Recent work has shown the defense of 01 loss sign activation neural networks
against image classification adversarial attacks. A public challenge to attack
the models on CIFAR10 dataset remains undefeated. We ask the following question
in this study: are 01 loss sign activation neural networks hard to deceive with
a popular black box text adversarial attack program called TextFooler? We study
this question on four popular text classification datasets: IMDB reviews, Yelp
reviews, MR sentiment classification, and AG news classification. We find that
our 01 loss sign activation network is much harder to attack with TextFooler
compared to sigmoid activation cross entropy and binary neural networks. We
also study a 01 loss sign activation convolutional neural network with a novel
global pooling step specific to sign activation networks. With this new
variation we see a significant gain in adversarial accuracy rendering
TextFooler practically useless against it. We make our code freely available at
\url{https://github.com/zero-one-loss/wordcnn01} and
\url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that
01 loss sign activation networks could be further developed to create fool
proof models against text adversarial attacks.
\\ ( https://arxiv.org/abs/2402.07347 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07352
Date: Mon, 12 Feb 2024 01:02:22 GMT   (1157kb,D)

Title: Data Distribution-based Curriculum Learning
Authors: Shonal Chaudhry and Anuraganand Sharma
Categories: cs.LG
\\
  The order of training samples can have a significant impact on the
performance of a classifier. Curriculum learning is a method of ordering
training samples from easy to hard. This paper proposes the novel idea of a
curriculum learning approach called Data Distribution-based Curriculum Learning
(DDCL). DDCL uses the data distribution of a dataset to build a curriculum
based on the order of samples. Two types of scoring methods known as DDCL
(Density) and DDCL (Point) are used to score training samples thus determining
their training order. DDCL (Density) uses the sample density to assign scores
while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the
proposed DDCL approach by conducting experiments on multiple datasets using a
neural network, support vector machine and random forest classifier. Evaluation
results show that the application of DDCL improves the average classification
accuracy for all datasets compared to standard evaluation without any
curriculum. Moreover, analysis of the error losses for a single training epoch
reveals that convergence is faster when using DDCL over the no curriculum
method.
\\ ( https://arxiv.org/abs/2402.07352 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07356
Date: Mon, 12 Feb 2024 01:11:49 GMT   (37kb)

Title: A Novel Gaussian Min-Max Theorem and its Applications
Authors: Danil Akhtiamov, David Bosch, Reza Ghane, K Nithin Varma, Babak
  Hassibi
Categories: cs.LG stat.ML
\\
  A celebrated result by Gordon allows one to compare the min-max behavior of
two Gaussian processes if certain inequality conditions are met. The
consequences of this result include the Gaussian min-max (GMT) and convex
Gaussian min-max (CGMT) theorems which have had far-reaching implications in
high-dimensional statistics, machine learning, non-smooth optimization, and
signal processing. Both theorems rely on a pair of Gaussian processes, first
identified by Slepian, that satisfy Gordon's comparison inequalities. To date,
no other pair of Gaussian processes satisfying these inequalities has been
discovered. In this paper, we identify such a new pair. The resulting theorems
extend the classical GMT and CGMT Theorems from the case where the underlying
Gaussian matrix in the primary process has iid rows to where it has independent
but non-identically-distributed ones. The new CGMT is applied to the problems
of multi-source Gaussian regression, as well as to binary classification of
general Gaussian mixture models.
\\ ( https://arxiv.org/abs/2402.07356 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07366
Date: Mon, 12 Feb 2024 01:47:06 GMT   (243kb,D)

Title: Bayesian Federated Learning Via Expectation Maximization and Turbo Deep
  Approximate Message Passing
Authors: Wei Xu, An Liu, Yiting Zhang and Vincent Lau
Categories: cs.LG cs.AI
\\
  Federated learning (FL) is a machine learning paradigm where the clients
possess decentralized training data and the central server handles aggregation
and scheduling. Typically, FL algorithms involve clients training their local
models using stochastic gradient descent (SGD), which carries drawbacks such as
slow convergence and being prone to getting stuck in suboptimal solutions. In
this work, we propose a message passing based Bayesian federated learning (BFL)
framework to avoid these drawbacks.Specifically, we formulate the problem of
deep neural network (DNN) learning and compression and as a sparse Bayesian
inference problem, in which group sparse prior is employed to achieve
structured model compression. Then, we propose an efficient BFL algorithm
called EMTDAMP, where expectation maximization (EM) and turbo deep approximate
message passing (TDAMP) are combined to achieve distributed learning and
compression. The central server aggregates local posterior distributions to
update global posterior distributions and update hyperparameters based on EM to
accelerate convergence. The clients perform TDAMP to achieve efficient
approximate message passing over DNN with joint prior distribution. We detail
the application of EMTDAMP to Boston housing price prediction and handwriting
recognition, and present extensive numerical results to demonstrate the
advantages of EMTDAMP.
\\ ( https://arxiv.org/abs/2402.07366 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07368
Date: Mon, 12 Feb 2024 01:55:51 GMT   (15091kb,D)

Title: Assessing Generalization for Subpopulation Representative Modeling via
  In-Context Learning
Authors: Gabriel Simmons and Vladislav Savinov
Categories: cs.LG cs.AI cs.CL cs.CY
Comments: Accepted to PERSONALIZE workshop at EACL 2024
\\
  This study evaluates the ability of Large Language Model (LLM)-based
Subpopulation Representative Models (SRMs) to generalize from empirical data,
utilizing in-context learning with data from the 2016 and 2020 American
National Election Studies. We explore generalization across response variables
and demographic subgroups. While conditioning with empirical data improves
performance on the whole, the benefit of in-context learning varies
considerably across demographics, sometimes hurting performance for one
demographic while helping performance for others. The inequitable benefits of
in-context learning for SRM present a challenge for practitioners implementing
SRMs, and for decision-makers who might come to rely on them. Our work
highlights a need for fine-grained benchmarks captured from diverse
subpopulations that test not only fidelity but generalization.
\\ ( https://arxiv.org/abs/2402.07368 ,  15091kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07369
Date: Mon, 12 Feb 2024 01:59:51 GMT   (17989kb,D)

Title: Diff-RNTraj: A Structure-aware Diffusion Model for Road
  Network-constrained Trajectory Generation
Authors: Tonglong Wei, Youfang Lin, Shengnan Guo, Yan Lin, Yiheng Huang,
  Chenyang Xiang, Yuqing Bai, Menglu Ya, Huaiyu Wan
Categories: cs.LG
\\
  Trajectory data is essential for various applications as it records the
movement of vehicles. However, publicly available trajectory datasets remain
limited in scale due to privacy concerns, which hinders the development of
trajectory data mining and trajectory-based applications. To address this
issue, some methods for generating synthetic trajectories have been proposed to
expand the scale of the dataset. However, all existing methods generate
trajectories in the geographical coordinate system, which poses two limitations
for their utilization in practical applications: 1) the inability to ensure
that the generated trajectories are constrained on the road. 2) the lack of
road-related information. In this paper, we propose a new problem to meet the
practical application need, \emph{i.e.}, road network-constrained trajectory
(RNTraj) generation, which can directly generate trajectories on the road
network with road-related information. RNTraj is a hybrid type of data, in
which each point is represented by a discrete road segment and a continuous
moving rate. To generate RNTraj, we design a diffusion model called
Diff-RNTraj. This model can effectively handle the hybrid RNTraj using a
continuous diffusion framework by incorporating a pre-training strategy to
embed hybrid RNTraj into continuous representations. During the sampling stage,
a RNTraj decoder is designed to map the continuous representation generated by
the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj
introduces a novel loss function to enhance the spatial validity of the
generated trajectories. Extensive experiments conducted on two real-world
trajectory datasets demonstrate the effectiveness of the proposed model.
\\ ( https://arxiv.org/abs/2402.07369 ,  17989kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07411
Date: Mon, 12 Feb 2024 05:12:09 GMT   (22226kb,D)

Title: Potential-Based Reward Shaping For Intrinsic Motivation
Authors: Grant C. Forbes, Nitish Gupta, Leonardo Villalobos-Arias, Colin M.
  Potts, Arnav Jhala, David L. Roberts
Categories: cs.LG
Comments: Extended version of paper appearing in AAMAS 2024
ACM-class: I.2.6
\\
  Recently there has been a proliferation of intrinsic motivation (IM)
reward-shaping methods to learn in complex and sparse-reward environments.
These methods can often inadvertently change the set of optimal policies in an
environment, leading to suboptimal behavior. Previous work on mitigating the
risks of reward shaping, particularly through potential-based reward shaping
(PBRS), has not been applicable to many IM methods, as they are often complex,
trainable functions themselves, and therefore dependent on a wider set of
variables than the traditional reward functions that PBRS was developed for. We
present an extension to PBRS that we prove preserves the set of optimal
policies under a more general set of functions than has been previously proven.
We also present {\em Potential-Based Intrinsic Motivation} (PBIM), a method for
converting IM rewards into a potential-based form that is useable without
altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff
Walking environments, we demonstrate that PBIM successfully prevents the agent
from converging to a suboptimal policy and can speed up training.
\\ ( https://arxiv.org/abs/2402.07411 ,  22226kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07412
Date: Mon, 12 Feb 2024 05:13:44 GMT   (14729kb,D)

Title: Auxiliary Reward Generation with Transition Distance Representation
  Learning
Authors: Siyuan Li and Shijie Han and Yingnan Zhao and By Liang and Peng Liu
Categories: cs.LG cs.AI
\\
  Reinforcement learning (RL) has shown its strength in challenging sequential
decision-making problems. The reward function in RL is crucial to the learning
performance, as it serves as a measure of the task completion degree. In
real-world problems, the rewards are predominantly human-designed, which
requires laborious tuning, and is easily affected by human cognitive biases. To
achieve automatic auxiliary reward generation, we propose a novel
representation learning approach that can measure the ``transition distance''
between states. Building upon these representations, we introduce an auxiliary
reward generation technique for both single-task and skill-chaining scenarios
without the need for human knowledge. The proposed approach is evaluated in a
wide range of manipulation tasks. The experiment results demonstrate the
effectiveness of measuring the transition distance between states and the
induced improvement by auxiliary rewards, which not only promotes better
learning efficiency but also increases convergent stability.
\\ ( https://arxiv.org/abs/2402.07412 ,  14729kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07415
Date: Mon, 12 Feb 2024 05:38:11 GMT   (5806kb,D)

Title: Context-aware Multi-Model Object Detection for Diversely Heterogeneous
  Compute Systems
Authors: Justin Davis and Mehmet E. Belviranli
Categories: cs.LG cs.CV cs.RO
\\
  In recent years, deep neural networks (DNNs) have gained widespread adoption
for continuous mobile object detection (OD) tasks, particularly in autonomous
systems. However, a prevalent issue in their deployment is the
one-size-fits-all approach, where a single DNN is used, resulting in
inefficient utilization of computational resources. This inefficiency is
particularly detrimental in energy-constrained systems, as it degrades overall
system efficiency. We identify that, the contextual information embedded in the
input data stream (e.g. the frames in the camera feed that the OD models are
run on) could be exploited to allow a more efficient multi-model-based OD
process. In this paper, we propose SHIFT which continuously selects from a
variety of DNN-based OD models depending on the dynamically changing contextual
information and computational constraints. During this selection, SHIFT
uniquely considers multi-accelerator execution to better optimize the
energy-efficiency while satisfying the latency constraints. Our proposed
methodology results in improvements of up to 7.5x in energy usage and 2.8x in
latency compared to state-of-the-art GPU-based single model OD approaches.
\\ ( https://arxiv.org/abs/2402.07415 ,  5806kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07419
Date: Mon, 12 Feb 2024 05:48:31 GMT   (8616kb,D)

Title: Conditional Generative Models are Sufficient to Sample from Any Causal
  Effect Estimand
Authors: Md Musfiqur Rahman, Matt Jordan, Murat Kocaoglu
Categories: cs.LG cs.AI stat.ME stat.ML
\\
  Causal inference from observational data has recently found many applications
in machine learning. While sound and complete algorithms exist to compute
causal effects, many of these algorithms require explicit access to conditional
likelihoods over the observational distribution, which is difficult to estimate
in the high-dimensional regime, such as with images. To alleviate this issue,
researchers have approached the problem by simulating causal relations with
neural models and obtained impressive results. However, none of these existing
approaches can be applied to generic scenarios such as causal graphs on image
data with latent confounders, or obtain conditional interventional samples. In
this paper, we show that any identifiable causal effect given an arbitrary
causal graph can be computed through push-forward computations of conditional
generative models. Based on this result, we devise a diffusion-based approach
to sample from any (conditional) interventional distribution on image data. To
showcase our algorithm's performance, we conduct experiments on a Colored MNIST
dataset having both the treatment ($X$) and the target variables ($Y$) as
images and obtain interventional samples from $P(y|do(x))$. As an application
of our algorithm, we evaluate two large conditional generative models that are
pre-trained on the CelebA dataset by analyzing the strength of spurious
correlations and the level of disentanglement they achieve.
\\ ( https://arxiv.org/abs/2402.07419 ,  8616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07443
Date: Mon, 12 Feb 2024 06:50:45 GMT   (66kb,D)

Title: The I/O Complexity of Attention, or How Optimal is Flash Attention?
Authors: Barna Saha, Christopher Ye
Categories: cs.LG cs.CC cs.DS cs.IT math.IT
Comments: 24 pages, 3 figures
\\
  Self-attention is at the heart of the popular Transformer architecture, yet
suffers from quadratic time and memory complexity. The breakthrough
FlashAttention algorithm revealed I/O complexity as the true bottleneck in
scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g.
GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O
complexity measures the number of accesses to memory. FlashAttention computes
attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of
the attention matrix, $d$ the head-dimension and $M$ the cache size. However,
is this I/O complexity optimal? The known lower bound only rules out an I/O
complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be
written to slow memory is $\Omega(Nd)$. This leads to the main question of our
work: Is FlashAttention I/O optimal for all values of $M$?
  We resolve the above question in its full generality by showing an I/O
complexity lower bound that matches the upper bound provided by FlashAttention
for any values of $M \geq d^2$ within any constant factors. Further, we give a
better algorithm with lower I/O complexity for $M < d^2$, and show that it is
optimal as well. Moreover, our lower bounds do not rely on using combinatorial
matrix multiplication for computing the attention matrix. We show even if one
uses fast matrix multiplication, the above I/O complexity bounds cannot be
improved. We do so by introducing a new communication complexity protocol for
matrix compression, and connecting communication complexity to I/O complexity.
To the best of our knowledge, this is the first work to establish a connection
between communication complexity and I/O complexity, and we believe this
connection could be of independent interest and will find many more
applications in proving I/O complexity lower bounds in the future.
\\ ( https://arxiv.org/abs/2402.07443 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07453
Date: Mon, 12 Feb 2024 07:20:05 GMT   (43kb)

Title: Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs
Authors: Yuval Filmus, Steve Hanneke, Idan Mehalel and Shay Moran
Categories: cs.LG stat.ML
\\
  Consider the domain of multiclass classification within the adversarial
online setting. What is the price of relying on bandit feedback as opposed to
full information? To what extent can an adaptive adversary amplify the loss
compared to an oblivious one? To what extent can a randomized learner reduce
the loss compared to a deterministic one? We study these questions in the
mistake bound model and provide nearly tight answers.
  We demonstrate that the optimal mistake bound under bandit feedback is at
most $O(k)$ times higher than the optimal mistake bound in the full information
case, where $k$ represents the number of labels. This bound is tight and
provides an answer to an open question previously posed and studied by Daniely
and Helbertal ['13] and by Long ['17, '20], who focused on deterministic
learners.
  Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap
between randomized and deterministic learners, as well as between adaptive and
oblivious adversaries in the bandit feedback setting. This stands in contrast
to the full information scenario, where adaptive and oblivious adversaries are
equivalent, and the gap in mistake bounds between randomized and deterministic
learners is a constant multiplicative factor of $2$.
  In addition, our results imply that in some cases the optimal randomized
mistake bound is approximately the square-root of its deterministic parallel.
Previous results show that this is essentially the smallest it can get.
\\ ( https://arxiv.org/abs/2402.07453 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07458
Date: Mon, 12 Feb 2024 07:37:19 GMT   (44kb,D)

Title: On the Distance from Calibration in Sequential Prediction
Authors: Mingda Qiao, Letian Zheng
Categories: cs.LG cs.DS stat.ML
\\
  We study a sequential binary prediction setting where the forecaster is
evaluated in terms of the calibration distance, which is defined as the $L_1$
distance between the predicted values and the set of predictions that are
perfectly calibrated in hindsight. This is analogous to a calibration measure
recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the
offline setting. The calibration distance is a natural and intuitive measure of
deviation from perfect calibration, and satisfies a Lipschitz continuity
property which does not hold for many popular calibration measures, such as the
$L_1$ calibration error and its variants.
  We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$
calibration distance in expectation on an adversarially chosen sequence of $T$
binary outcomes. At the core of this upper bound is a structural result showing
that the calibration distance is accurately approximated by the lower
calibration distance, which is a continuous relaxation of the former. We then
show that an $O(\sqrt{T})$ lower calibration distance can be achieved via a
simple minimax argument and a reduction to online learning on a Lipschitz
class.
  On the lower bound side, an $\Omega(T^{1/3})$ calibration distance is shown
to be unavoidable, even when the adversary outputs a sequence of independent
random bits, and has an additional ability to early stop (i.e., to stop
producing random bits and output the same bit in the remaining steps).
Interestingly, without this early stopping, the forecaster can achieve a much
smaller calibration distance of $\mathrm{polylog}(T)$.
\\ ( https://arxiv.org/abs/2402.07458 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07465
Date: Mon, 12 Feb 2024 07:59:25 GMT   (69kb,D)

Title: Score-Based Physics-Informed Neural Networks for High-Dimensional
  Fokker-Planck Equations
Authors: Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, Kenji Kawaguchi
Categories: cs.LG cs.AI cs.NA math.DS math.NA stat.ML
Comments: 22 pages
MSC-class: 14J60
\\
  The Fokker-Planck (FP) equation is a foundational PDE in stochastic
processes. However, curse of dimensionality (CoD) poses challenge when dealing
with high-dimensional FP PDEs. Although Monte Carlo and vanilla
Physics-Informed Neural Networks (PINNs) have shown the potential to tackle
CoD, both methods exhibit numerical errors in high dimensions when dealing with
the probability density function (PDF) associated with Brownian motion. The
point-wise PDF values tend to decrease exponentially as dimension increases,
surpassing the precision of numerical simulations and resulting in substantial
errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast
sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms
the FP equation into a difficult HJB equation, whose error grows rapidly with
dimension. To this end, we propose a novel approach utilizing a score-based
solver to fit the score function in SDEs. The score function, defined as the
gradient of the LL, plays a fundamental role in inferring LL and PDF and
enables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced
SM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver
operates in two stages: first, employing SM, SSM, or Score-PINN to acquire the
score; and second, solving the LL via an ODE using the obtained score.
Comparative evaluations across these methods showcase varying trade-offs. The
proposed method is evaluated across diverse SDEs, including anisotropic OU
processes, geometric Brownian, and Brownian with varying eigenspace. We also
test various distributions, including Gaussian, Log-normal, Laplace, and
Cauchy. The numerical results demonstrate the score-based SDE solver's
stability, speed, and performance across different settings, solidifying its
potential as a solution to CoD for high-dimensional FP equations.
\\ ( https://arxiv.org/abs/2402.07465 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07471
Date: Mon, 12 Feb 2024 08:16:58 GMT   (852kb,D)

Title: Differentially Private Decentralized Learning with Random Walks
Authors: Edwige Cyffers, Aur\'elien Bellet and Jalaj Upadhyay
Categories: cs.LG
\\
  The popularity of federated learning comes from the possibility of better
scalability and the ability for participants to keep control of their data,
improving data security and sovereignty. Unfortunately, sharing model updates
also creates a new privacy attack surface. In this work, we characterize the
privacy guarantees of decentralized learning with random walk algorithms, where
a model is updated by traveling from one node to another along the edges of a
communication graph. Using a recent variant of differential privacy tailored to
the study of decentralized algorithms, namely Pairwise Network Differential
Privacy, we derive closed-form expressions for the privacy loss between each
pair of nodes where the impact of the communication topology is captured by
graph theoretic quantities. Our results further reveal that random walk
algorithms tends to yield better privacy guarantees than gossip algorithms for
nodes close from each other. We supplement our theoretical results with
empirical evaluation on synthetic and real-world graphs and datasets.
\\ ( https://arxiv.org/abs/2402.07471 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07480
Date: Mon, 12 Feb 2024 08:39:40 GMT   (1252kb,D)

Title: Topological Safeguard for Evasion Attack based on the Interpretability
  of Artificial Neural Network Behavior
Authors: Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, I\~nigo Mendialdua,
  Raul Orduna-Urrutia
Categories: cs.LG
Journal-ref: Pattern Recognition, Volume 147, 2024, 110130, ISSN 0031-3203
DOI: 10.1016/j.patcog.2023.110130
\\
  In the last years, Deep Learning technology has been proposed in different
fields, bringing many advances in each of them, but identifying new threats in
these solutions regarding cybersecurity. Those implemented models have brought
several vulnerabilities associated with Deep Learning technology. Moreover,
those allow taking advantage of the implemented model, obtaining private
information, and even modifying the model's decision-making. Therefore,
interest in studying those vulnerabilities/attacks and designing defenses to
avoid or fight them is gaining prominence among researchers. In particular, the
widely known evasion attack is being analyzed by researchers; thus, several
defenses to avoid such a threat can be found in the literature. Since the
presentation of the L-BFG algorithm, this threat concerns the research
community. However, it continues developing new and ingenious countermeasures
since there is no perfect defense for all the known evasion algorithms. In this
work, a novel detector of evasion attacks is developed. It focuses on the
information of the activations of the neurons given by the model when an input
sample is injected. Moreover, it puts attention to the topology of the targeted
deep learning model to analyze the activations according to which neurons are
connecting. This approach has been decided because the literature shows that
the targeted model's topology contains essential information about if the
evasion attack occurs. For this purpose, a huge data preprocessing is required
to introduce all this information in the detector, which uses the Graph
Convolutional Neural Network (GCN) technology. Thus, it understands the
topology of the target model, obtaining promising results and improving the
outcomes presented in the literature related to similar defenses.
\\ ( https://arxiv.org/abs/2402.07480 ,  1252kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07487
Date: Mon, 12 Feb 2024 08:52:35 GMT   (2263kb,D)

Title: Score-based Diffusion Models via Stochastic Differential Equations -- a
  Technical Tutorial
Authors: Wenpin Tang and Hanyang Zhao
Categories: cs.LG math.HO
\\
  This is an expository article on the score-based diffusion models, with a
particular focus on the formulation via stochastic differential equations
(SDE). After a gentle introduction, we discuss the two pillars in the diffusion
modeling -- sampling and score matching, which encompass the SDE/ODE sampling,
score matching efficiency, the consistency model, and reinforcement learning.
Short proofs are given to illustrate the main idea of the stated results. The
article is primarily for introducing the beginners to the field, and
practitioners may also find some analysis useful in designing new models or
algorithms.
\\ ( https://arxiv.org/abs/2402.07487 ,  2263kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07496
Date: Mon, 12 Feb 2024 09:05:01 GMT   (911kb)

Title: Understanding Deep Learning defenses Against Adversarial Examples
  Through Visualizations for Dynamic Risk Assessment
Authors: Xabier Echeberria-Barrio, Amaia Gil-Lerchundi, Jon Egana-Zubia, Raul
  Orduna-Urrutia
Categories: cs.LG cs.CR
Journal-ref: Neural Comput and Applic 34, 20477 to 20490, 2022
DOI: 10.1007/s00521-021-06812-y
\\
  In recent years, Deep Neural Network models have been developed in different
fields, where they have brought many advances. However, they have also started
to be used in tasks where risk is critical. A misdiagnosis of these models can
lead to serious accidents or even death. This concern has led to an interest
among researchers to study possible attacks on these models, discovering a long
list of vulnerabilities, from which every model should be defended. The
adversarial example attack is a widely known attack among researchers, who have
developed several defenses to avoid such a threat. However, these defenses are
as opaque as a deep neural network model, how they work is still unknown. This
is why visualizing how they change the behavior of the target model is
interesting in order to understand more precisely how the performance of the
defended model is being modified. For this work, some defenses, against
adversarial example attack, have been selected in order to visualize the
behavior modification of each of them in the defended model. Adversarial
training, dimensionality reduction and prediction similarity were the selected
defenses, which have been developed using a model composed by convolution
neural network layers and dense neural network layers. In each defense, the
behavior of the original model has been compared with the behavior of the
defended model, representing the target model by a graph in a visualization.
\\ ( https://arxiv.org/abs/2402.07496 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07498
Date: Mon, 12 Feb 2024 09:07:54 GMT   (16261kb,D)

Title: Accelerated Smoothing: A Scalable Approach to Randomized Smoothing
Authors: Devansh Bhardwaj, Kshitiz Kaushik, Sarthak Gupta
Categories: cs.LG
\\
  Randomized smoothing has emerged as a potent certifiable defense against
adversarial attacks by employing smoothing noises from specific distributions
to ensure the robustness of a smoothed classifier. However, the utilization of
Monte Carlo sampling in this process introduces a compute-intensive element,
which constrains the practicality of randomized smoothing on a larger scale. To
address this limitation, we propose a novel approach that replaces Monte Carlo
sampling with the training of a surrogate neural network. Through extensive
experimentation in various settings, we demonstrate the efficacy of our
approach in approximating the smoothed classifier with remarkable precision.
Furthermore, we demonstrate that our approach significantly accelerates the
robust radius certification process, providing nearly $600$X improvement in
computation time, overcoming the computational bottlenecks associated with
traditional randomized smoothing.
\\ ( https://arxiv.org/abs/2402.07498 ,  16261kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07501
Date: Mon, 12 Feb 2024 09:10:09 GMT   (344kb,D)

Title: One Train for Two Tasks: An Encrypted Traffic Classification Framework
  Using Supervised Contrastive Learning
Authors: Haozhen Zhang, Xi Xiao, Le Yu, Qing Li, Zhen Ling, Ye Zhang
Categories: cs.LG cs.AI
Comments: The code is available at https://github.com/ViktorAxelsen/CLE-TFE
\\
  As network security receives widespread attention, encrypted traffic
classification has become the current research focus. However, existing methods
conduct traffic classification without sufficiently considering the common
characteristics between data samples, leading to suboptimal performance.
Moreover, they train the packet-level and flow-level classification tasks
independently, which is redundant because the packet representations learned in
the packet-level task can be exploited by the flow-level task. Therefore, in
this paper, we propose an effective model named a Contrastive Learning Enhanced
Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised
contrastive learning to enhance the packet-level and flow-level representations
and perform graph data augmentation on the byte-level traffic graph so that the
fine-grained semantic-invariant characteristics between bytes can be captured
through contrastive learning. We also propose cross-level multi-task learning,
which simultaneously accomplishes the packet-level and flow-level
classification tasks in the same model with one training. Further experiments
show that CLE-TFE achieves the best overall performance on the two tasks, while
its computational overhead (i.e., floating point operations, FLOPs) is only
about 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at
https://github.com/ViktorAxelsen/CLE-TFE
\\ ( https://arxiv.org/abs/2402.07501 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07502
Date: Mon, 12 Feb 2024 09:10:24 GMT   (1029kb,D)

Title: ClusterTabNet: Supervised clustering method for table detection and
  table structure recognition
Authors: Marek Polewczyk and Marco Spinaci
Categories: cs.LG cs.CV
Comments: 15 pages, 4 figures, submitted. The code will be released at
  https://github.com/SAP-samples
\\
  We present a novel deep-learning-based method to cluster words in documents
which we apply to detect and recognize tables given the OCR output. We
interpret table structure bottom-up as a graph of relations between pairs of
words (belonging to the same row, column, header, as well as to the same table)
and use a transformer encoder model to predict its adjacency matrix. We
demonstrate the performance of our method on the PubTables-1M dataset as well
as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art
detection methods such as DETR and Faster R-CNN, our method achieves similar or
better accuracy, while requiring a significantly smaller model.
\\ ( https://arxiv.org/abs/2402.07502 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07506
Date: Mon, 12 Feb 2024 09:24:34 GMT   (1295kb)

Title: NeuralSentinel: Safeguarding Neural Network Reliability and
  Trustworthiness
Authors: Xabier Echeberria-Barrio, Mikel Gorricho, Selene Valencia, Francesco
  Zola
Categories: cs.LG
Journal-ref: CS and IT Conference Proceedings, CS and IT Conference
  Proceedings, 2024
\\
  The usage of Artificial Intelligence (AI) systems has increased
exponentially, thanks to their ability to reduce the amount of data to be
analyzed, the user efforts and preserving a high rate of accuracy. However,
introducing this new element in the loop has converted them into attacked
points that can compromise the reliability of the systems. This new scenario
has raised crucial challenges regarding the reliability and trustworthiness of
the AI models, as well as about the uncertainties in their response decisions,
becoming even more crucial when applied in critical domains such as healthcare,
chemical, electrical plants, etc. To contain these issues, in this paper, we
present NeuralSentinel (NS), a tool able to validate the reliability and
trustworthiness of AI models. This tool combines attack and defence strategies
and explainability concepts to stress an AI model and help non-expert staff
increase their confidence in this new system by understanding the model
decisions. NS provide a simple and easy-to-use interface for helping humans in
the loop dealing with all the needed information. This tool was deployed and
used in a Hackathon event to evaluate the reliability of a skin cancer image
detector. During the event, experts and non-experts attacked and defended the
detector, learning which factors were the most important for model
misclassification and which techniques were the most efficient. The event was
also used to detect NS's limitations and gather feedback for further
improvements.
\\ ( https://arxiv.org/abs/2402.07506 ,  1295kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07545
Date: Mon, 12 Feb 2024 10:16:05 GMT   (2116kb,D)

Title: TransAxx: Efficient Transformers with Approximate Computing
Authors: Dimitrios Danopoulos, Georgios Zervakis, Dimitrios Soudris, J\"org
  Henkel
Categories: cs.LG cs.AR
\\
  Vision Transformer (ViT) models which were recently introduced by the
transformer architecture have shown to be very competitive and often become a
popular alternative to Convolutional Neural Networks (CNNs). However, the high
computational requirements of these models limit their practical applicability
especially on low-power devices. Current state-of-the-art employs approximate
multipliers to address the highly increased compute demands of DNN accelerators
but no prior research has explored their use on ViT models. In this work we
propose TransAxx, a framework based on the popular PyTorch library that enables
fast inherent support for approximate arithmetic to seamlessly evaluate the
impact of approximate computing on DNNs such as ViT models. Using TransAxx we
analyze the sensitivity of transformer models on the ImageNet dataset to
approximate multiplications and perform approximate-aware finetuning to regain
accuracy. Furthermore, we propose a methodology to generate approximate
accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS)
algorithm to efficiently search the space of possible configurations using a
hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy
of our methodology in achieving significant trade-offs between accuracy and
power, resulting in substantial gains without compromising on performance.
\\ ( https://arxiv.org/abs/2402.07545 ,  2116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07568
Date: Mon, 12 Feb 2024 11:03:52 GMT   (698kb,D)

Title: Weisfeiler-Leman at the margin: When more expressivity matters
Authors: Billy J. Franks, Christopher Morris, Ameya Velingker, and Floris
  Geerts
Categories: cs.LG cs.DM cs.NE stat.ML
Comments: arXiv admin note: text overlap with arXiv:2301.11039
\\
  The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the
graph isomorphism problem. Recently, the algorithm has played a prominent role
in understanding the expressive power of message-passing graph neural networks
(MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL
faces challenges in distinguishing non-isomorphic graphs, leading to the
development of more expressive MPNN and kernel architectures. However, the
relationship between enhanced expressivity and improved generalization
performance remains unclear. Here, we show that an architecture's expressivity
offers limited insights into its generalization performance when viewed through
graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with
subgraph information and employ classical margin theory to investigate the
conditions under which an architecture's increased expressivity aligns with
improved generalization performance. In addition, we show that gradient flow
pushes the MPNN's weights toward the maximum margin solution. Further, we
introduce variations of expressive $1$-WL-based kernel and MPNN architectures
with provable generalization properties. Our empirical study confirms the
validity of our theoretical findings.
\\ ( https://arxiv.org/abs/2402.07568 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07570
Date: Mon, 12 Feb 2024 11:04:14 GMT   (1853kb,D)

Title: Only the Curve Shape Matters: Training Foundation Models for Zero-Shot
  Multivariate Time Series Forecasting through Next Curve Shape Prediction
Authors: Cheng Feng, Long Huang, Denis Krompass
Categories: cs.LG cs.AI
\\
  We present General Time Transformer (GTT), an encoder-only style foundation
model for zero-shot multivariate time series forecasting. GTT is pretrained on
a large dataset of 200M high-quality time series samples spanning diverse
domains. In our proposed framework, the task of multivariate time series
forecasting is formulated as a channel-wise next curve shape prediction
problem, where each time series sample is represented as a sequence of
non-overlapping curve shapes with a unified numerical magnitude. GTT is trained
to predict the next curve shape based on a window of past curve shapes in a
channel-wise manner. Experimental results demonstrate that GTT exhibits
superior zero-shot multivariate forecasting capabilities on unseen time series
datasets, even surpassing state-of-the-art supervised baselines. Additionally,
we investigate the impact of varying GTT model parameters and training dataset
scales, observing that the scaling law also holds in the context of zero-shot
multivariate time series forecasting.
\\ ( https://arxiv.org/abs/2402.07570 ,  1853kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07585
Date: Mon, 12 Feb 2024 11:35:04 GMT   (78kb,D)

Title: Identifying architectural design decisions for achieving green ML
  serving
Authors: Francisco Dur\'an, Silverio Mart\'inez-Fern\'andez, Matias Martinez,
  and Patricia Lago
Categories: cs.LG cs.SE
Comments: Accepted for publication as short paper in Conference on AI
  Engineering Software Engineering for AI (CAIN 2024)
DOI: 10.1145/3644815.3644962
\\
  The growing use of large machine learning models highlights concerns about
their increasing computational demands. While the energy consumption of their
training phase has received attention, fewer works have considered the
inference phase. For ML inference, the binding of ML models to the ML system
for user access, known as ML serving, is a critical yet understudied step for
achieving efficiency in ML applications.
  We examine the literature in ML architectural design decisions and Green AI,
with a special focus on ML serving. The aim is to analyze ML serving
architectural design decisions for the purpose of understanding and identifying
them with respect to quality characteristics from the point of view of
researchers and practitioners in the context of ML serving literature.
  Our results (i) identify ML serving architectural design decisions along with
their corresponding components and associated technological stack, and (ii)
provide an overview of the quality characteristics studied in the literature,
including energy efficiency.
  This preliminary study is the first step in our goal to achieve green ML
serving. Our analysis may aid ML researchers and practitioners in making
green-aware architecture design decisions when serving their models.
\\ ( https://arxiv.org/abs/2402.07585 ,  78kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07586
Date: Mon, 12 Feb 2024 11:35:25 GMT   (26472kb,D)

Title: Unveiling Group-Specific Distributed Concept Drift: A Fairness
  Imperative in Federated Learning
Authors: Teresa Salazar and Jo\~ao Gama and Helder Ara\'ujo and Pedro Henriques
  Abreu
Categories: cs.LG
MSC-class: 68T01
ACM-class: I.2.m
\\
  In the evolving field of machine learning, ensuring fairness has become a
critical concern, prompting the development of algorithms designed to mitigate
discriminatory outcomes in decision-making processes. However, achieving
fairness in the presence of group-specific concept drift remains an unexplored
frontier, and our research represents pioneering efforts in this regard.
Group-specific concept drift refers to situations where one group experiences
concept drift over time while another does not, leading to a decrease in
fairness even if accuracy remains fairly stable. Within the framework of
federated learning, where clients collaboratively train models, its distributed
nature further amplifies these challenges since each client can experience
group-specific concept drift independently while still sharing the same
underlying concept, creating a complex and dynamic environment for maintaining
fairness. One of the significant contributions of our research is the
formalization and introduction of the problem of group-specific concept drift
and its distributed counterpart, shedding light on its critical importance in
the realm of fairness. In addition, leveraging insights from prior research, we
adapt an existing distributed concept drift adaptation algorithm to tackle
group-specific distributed concept drift which utilizes a multi-model approach,
a local group-specific drift detection mechanism, and continuous clustering of
models over time. The findings from our experiments highlight the importance of
addressing group-specific concept drift and its distributed counterpart to
advance fairness in machine learning.
\\ ( https://arxiv.org/abs/2402.07586 ,  26472kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07594
Date: Mon, 12 Feb 2024 11:48:54 GMT   (887kb,D)

Title: Foundational Inference Models for Dynamical Systems
Authors: Patrick Seifner, Kostadin Cvejoski, Ramses J. Sanchez
Categories: cs.LG math.DS
\\
  Ordinary differential equations (ODEs) underlie dynamical systems which serve
as models for a vast number of natural and social phenomena. Yet inferring the
ODE that best describes a set of noisy observations on one such phenomenon can
be remarkably challenging, and the models available to achieve it tend to be
highly specialized and complex too. In this work we propose a novel supervised
learning framework for zero-shot inference of ODEs from noisy data. We first
generate large datasets of one-dimensional ODEs, by sampling distributions over
the space of initial conditions, and the space of vector fields defining them.
We then learn neural maps between noisy observations on the solutions of these
equations, and their corresponding initial condition and vector fields. The
resulting models, which we call foundational inference models (FIM), can be (i)
copied and matched along the time dimension to increase their resolution; and
(ii) copied and composed to build inference models of any dimensionality,
without the need of any finetuning. We use FIM to model both ground-truth
dynamical systems of different dimensionalities and empirical time series data
in a zero-shot fashion, and outperform state-of-the-art models which are
finetuned to these systems. Our (pretrained) FIMs are available online
\\ ( https://arxiv.org/abs/2402.07594 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07598
Date: Mon, 12 Feb 2024 11:58:18 GMT   (397kb,D)

Title: Near-Minimax-Optimal Distributional Reinforcement Learning with a
  Generative Model
Authors: Mark Rowland, Li Kevin Wenliang, R\'emi Munos, Clare Lyle, Yunhao
  Tang, Will Dabney
Categories: cs.LG stat.ML
\\
  We propose a new algorithm for model-based distributional reinforcement
learning (RL), and prove that it is minimax-optimal for approximating return
distributions with a generative model (up to logarithmic factors), resolving an
open question of Zhang et al. (2023). Our analysis provides new theoretical
results on categorical approaches to distributional RL, and also introduces a
new distributional Bellman equation, the stochastic categorical CDF Bellman
equation, which we expect to be of independent interest. We also provide an
experimental study comparing several model-based distributional RL algorithms,
with several takeaways for practitioners.
\\ ( https://arxiv.org/abs/2402.07598 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07630
Date: Mon, 12 Feb 2024 13:13:04 GMT   (20324kb,D)

Title: G-Retriever: Retrieval-Augmented Generation for Textual Graph
  Understanding and Question Answering
Authors: Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent,
  Yann LeCun, Xavier Bresson, Bryan Hooi
Categories: cs.LG
\\
  Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop
our Graph Question Answering (GraphQA) benchmark with data collected from
different tasks. Then, we propose our G-Retriever approach, which integrates
the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can
be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and resists
hallucination. (Our codes and datasets are available at:
https://github.com/XiaoxinHe/G-Retriever.)
\\ ( https://arxiv.org/abs/2402.07630 ,  20324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07639
Date: Mon, 12 Feb 2024 13:24:32 GMT   (2548kb,D)

Title: Tighter Bounds on the Information Bottleneck with Application to Deep
  Learning
Authors: Nir Weingarten, Zohar Yakhini, Moshe Butman, Ran Gilad-Bachrach
Categories: cs.LG cs.AI cs.IT math.IT
Comments: 10 pages, 5 figures, code included in github repo
MSC-class: 94A08, 94A10, 94A11, 68T06, 62B04, 62B08
ACM-class: I.2; E.4; I.4; I.7
\\
  Deep Neural Nets (DNNs) learn latent representations induced by their
downstream task, objective function, and other parameters. The quality of the
learned representations impacts the DNN's generalization ability and the
coherence of the emerging latent space. The Information Bottleneck (IB)
provides a hypothetically optimal framework for data modeling, yet it is often
intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired
variational methods to approximate bounds on mutual information, resulting in
improved robustness to adversarial attacks. This work introduces a new and
tighter variational bound for the IB, improving performance of previous
IB-inspired DNNs. These advancements strengthen the case for the IB and its
variational approximations as a data modeling framework, and provide a simple
method to significantly enhance the adversarial robustness of classifier DNNs.
\\ ( https://arxiv.org/abs/2402.07639 ,  2548kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07692
Date: Mon, 12 Feb 2024 14:59:40 GMT   (11274kb,D)

Title: Boundary Exploration for Bayesian Optimization With Unknown Physical
  Constraints
Authors: Yunsheng Tian, Ane Zuniga, Xinwei Zhang, Johannes P. D\"urholt, Payel
  Das, Jie Chen, Wojciech Matusik, Mina Konakovi\'c Lukovi\'c
Categories: cs.LG
\\
  Bayesian optimization has been successfully applied to optimize black-box
functions where the number of evaluations is severely limited. However, in many
real-world applications, it is hard or impossible to know in advance which
designs are feasible due to some physical or system limitations. These issues
lead to an even more challenging problem of optimizing an unknown function with
unknown constraints. In this paper, we observe that in such scenarios optimal
solution typically lies on the boundary between feasible and infeasible regions
of the design space, making it considerably more difficult than that with
interior optima. Inspired by this observation, we propose BE-CBO, a new
Bayesian optimization method that efficiently explores the boundary between
feasible and infeasible designs. To identify the boundary, we learn the
constraints with an ensemble of neural networks that outperform the standard
Gaussian Processes for capturing complex boundaries. Our method demonstrates
superior performance against state-of-the-art methods through comprehensive
experiments on synthetic and real-world benchmarks.
\\ ( https://arxiv.org/abs/2402.07692 ,  11274kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07703
Date: Mon, 12 Feb 2024 15:17:31 GMT   (4416kb,D)

Title: Online Sequential Decision-Making with Unknown Delays
Authors: Ping Wu and Heyan Huang and Zhengyang Liu
Categories: cs.LG cs.AI
\\
  In the field of online sequential decision-making, we address the problem
with delays utilizing the framework of online convex optimization (OCO), where
the feedback of a decision can arrive with an unknown delay. Unlike previous
research that is limited to Euclidean norm and gradient information, we propose
three families of delayed algorithms based on approximate solutions to handle
different types of received feedback. Our proposed algorithms are versatile and
applicable to universal norms. Specifically, we introduce a family of Follow
the Delayed Regularized Leader algorithms for feedback with full information on
the loss function, a family of Delayed Mirror Descent algorithms for feedback
with gradient information on the loss function and a family of Simplified
Delayed Mirror Descent algorithms for feedback with the value information of
the loss function's gradients at corresponding decision points. For each type
of algorithm, we provide corresponding regret bounds under cases of general
convexity and relative strong convexity, respectively. We also demonstrate the
efficiency of each algorithm under different norms through concrete examples.
Furthermore, our theoretical results are consistent with the current best
bounds when degenerated to standard settings.
\\ ( https://arxiv.org/abs/2402.07703 ,  4416kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07712
Date: Mon, 12 Feb 2024 15:26:01 GMT   (873kb,D)

Title: Model Collapse Demystified: The Case of Regression
Authors: Elvis Dohmatob, Yunzhen Feng and Julia Kempe
Categories: cs.LG cs.AI stat.ML
\\
  In the era of large language models like ChatGPT, the phenomenon of "model
collapse" refers to the situation whereby as a model is trained recursively on
data generated from previous generations of itself over time, its performance
degrades until the model eventually becomes completely useless, i.e the model
collapses. In this work, we study this phenomenon in the simplified setting of
kernel regression and obtain results which show a clear crossover between where
the model can cope with fake data, and a regime where the model's performance
completely collapses. Under polynomial decaying spectral and source conditions,
we obtain modified scaling laws which exhibit new crossover phenomena from fast
to slow rates. We also propose a simple strategy based on adaptive
regularization to mitigate model collapse. Our theoretical results are
validated with experiments.
\\ ( https://arxiv.org/abs/2402.07712 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07721
Date: Mon, 12 Feb 2024 15:34:56 GMT   (3248kb,D)

Title: LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation
Authors: Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao
Categories: cs.LG cs.CL
Comments: 12 pages, 11 figures
\\
  Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to
fine-tune the pre-trained model under limited computing resources. But it still
faces challenges of resource consumption when scaling up to larger models.
Previous studies employ pruning techniques by evaluating the importance of LoRA
parameters for different layers to address the problem. However, these efforts
only analyzed parameter features to evaluate their importance. Indeed, the
output of LoRA related to the parameters and data is the factor that directly
impacts the frozen model. To this end, we propose LoRA-drop which evaluates the
importance of the parameters by analyzing the LoRA output. We retain LoRA for
important layers and the LoRA of the other layers share the same parameters.
Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of
LoRA-drop.
\\ ( https://arxiv.org/abs/2402.07721 ,  3248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07738
Date: Mon, 12 Feb 2024 15:52:27 GMT   (8572kb,D)

Title: Universal link predictor by In-context Learning
Authors: Kaiwen Dong, Haitao Mao, Zhichun Guo, Nitesh V. Chawla
Categories: cs.LG
Comments: Preprint
\\
  Link prediction is a crucial task in graph machine learning, where the goal
is to infer missing or future links within a graph. Traditional approaches
leverage heuristic methods based on widely observed connectivity patterns,
offering broad applicability and generalizability without the need for model
training. Despite their utility, these methods are limited by their reliance on
human-derived heuristics and lack the adaptability of data-driven approaches.
Conversely, parametric link predictors excel in automatically learning the
connectivity patterns from data and achieving state-of-the-art but fail short
to directly transfer across different graphs. Instead, it requires the cost of
extensive training and hyperparameter optimization to adapt to the target
graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel
model that combines the generalizability of heuristic approaches with the
pattern learning capabilities of parametric models. UniLP is designed to
autonomously identify connectivity patterns across diverse graphs, ready for
immediate application to any unseen graph dataset without targeted training. We
address the challenge of conflicting connectivity patterns-arising from the
unique distributions of different graphs-through the implementation of
In-context Learning (ICL). This approach allows UniLP to dynamically adjust to
various target graphs based on contextual demonstrations, thereby avoiding
negative transfer. Through rigorous experimentation, we demonstrate UniLP's
effectiveness in adapting to new, unseen graphs at test time, showcasing its
ability to perform comparably or even outperform parametric models that have
been finetuned for specific datasets. Our findings highlight UniLP's potential
to set a new standard in link prediction, combining the strengths of heuristic
and parametric methods in a single, versatile framework.
\\ ( https://arxiv.org/abs/2402.07738 ,  8572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07745
Date: Mon, 12 Feb 2024 16:15:25 GMT   (234kb,D)

Title: Predictive Churn with the Set of Good Models
Authors: Jamelle Watson-Daniels, Flavio du Pin Calmon, Alexander D'Amour, Carol
  Long, David C. Parkes, Berk Ustun
Categories: cs.LG
\\
  Machine learning models in modern mass-market applications are often updated
over time. One of the foremost challenges faced is that, despite increasing
overall performance, these updates may flip specific model predictions in
unpredictable ways. In practice, researchers quantify the number of unstable
predictions between models pre and post update -- i.e., predictive churn. In
this paper, we study this effect through the lens of predictive multiplicity --
i.e., the prevalence of conflicting predictions over the set of near-optimal
models (the Rashomon set). We show how traditional measures of predictive
multiplicity can be used to examine expected churn over this set of prospective
models -- i.e., the set of models that may be used to replace a baseline model
in deployment. We present theoretical results on the expected churn between
models within the Rashomon set from different perspectives. And we characterize
expected churn over model updates via the Rashomon set, pairing our analysis
with empirical results on real-world datasets -- showing how our approach can
be used to better anticipate, reduce, and avoid churn in consumer-facing
applications. Further, we show that our approach is useful even for models
enhanced with uncertainty awareness.
\\ ( https://arxiv.org/abs/2402.07745 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07757
Date: Mon, 12 Feb 2024 16:25:47 GMT   (20478kb,D)

Title: Towards an Understanding of Stepwise Inference in Transformers: A
  Synthetic Graph Navigation Model
Authors: Mikail Khona, Maya Okawa, Jan Hula, Rahul Ramesh, Kento Nishi, Robert
  Dick, Ekdeep Singh Lubana, Hidenori Tanaka
Categories: cs.LG cs.AI
\\
  Stepwise inference protocols, such as scratchpads and chain-of-thought, help
language models solve complex problems by decomposing them into a sequence of
simpler subproblems. Despite the significant gain in performance achieved via
these protocols, the underlying mechanisms of stepwise inference have remained
elusive. To address this, we propose to study autoregressive Transformer models
on a synthetic task that embodies the multi-step nature of problems where
stepwise inference is generally most useful. Specifically, we define a graph
navigation problem wherein a model is tasked with traversing a path from a
start to a goal node on the graph. Despite is simplicity, we find we can
empirically reproduce and analyze several phenomena observed at scale: (i) the
stepwise inference reasoning gap, the cause of which we find in the structure
of the training data; (ii) a diversity-accuracy tradeoff in model generations
as sampling temperature varies; (iii) a simplicity bias in the model's output;
and (iv) compositional generalization and a primacy bias with in-context
exemplars. Overall, our work introduces a grounded, synthetic framework for
studying stepwise inference and offers mechanistic hypotheses that can lay the
foundation for a deeper understanding of this phenomenon.
\\ ( https://arxiv.org/abs/2402.07757 ,  20478kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07785
Date: Mon, 12 Feb 2024 16:50:07 GMT   (2829kb,D)

Title: HYPO: Hyperspherical Out-of-Distribution Generalization
Authors: Haoyue Bai, Yifei Ming, Julian Katz-Samuels, and Yixuan Li
Categories: cs.LG
Comments: Published as a conference paper at ICLR 2024; First two authors
  contributed equally
\\
  Out-of-distribution (OOD) generalization is critical for machine learning
models deployed in the real world. However, achieving this can be fundamentally
challenging, as it requires the ability to learn invariant features across
different domains or environments. In this paper, we propose a novel framework
HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant
representations in a hyperspherical space. In particular, our hyperspherical
learning algorithm is guided by intra-class variation and inter-class
separation principles -- ensuring that features from the same class (across
different training domains) are closely aligned with their class prototypes,
while different class prototypes are maximally separated. We further provide
theoretical justifications on how our prototypical learning objective improves
the OOD generalization bound. Through extensive experiments on challenging OOD
benchmarks, we demonstrate that our approach outperforms competitive baselines
and achieves superior performance. Code is available at
https://github.com/deeplearning-wisc/hypo.
\\ ( https://arxiv.org/abs/2402.07785 ,  2829kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07790
Date: Mon, 12 Feb 2024 16:55:19 GMT   (5826kb,D)

Title: From Uncertainty to Precision: Enhancing Binary Classifier Performance
  through Calibration
Authors: Agathe Fernandes Machado, Arthur Charpentier, Emmanuel Flachaire, Ewen
  Gallic, Fran\c{c}ois Hu
Categories: cs.LG
\\
  The assessment of binary classifier performance traditionally centers on
discriminative ability using metrics, such as accuracy. However, these metrics
often disregard the model's inherent uncertainty, especially when dealing with
sensitive decision-making domains, such as finance or healthcare. Given that
model-predicted scores are commonly seen as event probabilities, calibration is
crucial for accurate interpretation. In our study, we analyze the sensitivity
of various calibration measures to score distortions and introduce a refined
metric, the Local Calibration Score. Comparing recalibration methods, we
advocate for local regressions, emphasizing their dual role as effective
recalibration tools and facilitators of smoother visualizations. We apply these
findings in a real-world scenario using Random Forest classifier and regressor
to predict credit default while simultaneously measuring calibration during
performance optimization.
\\ ( https://arxiv.org/abs/2402.07790 ,  5826kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07792
Date: Mon, 12 Feb 2024 16:59:05 GMT   (1654kb,D)

Title: Empowering Federated Learning for Massive Models with NVIDIA FLARE
Authors: Holger R. Roth, Ziyue Xu, Yuan-Ting Hsieh, Adithya Renduchintala,
  Isaac Yang, Zhihong Zhang, Yuhong Wen, Sean Yang, Kevin Lu, Kristopher
  Kersten, Camir Ricketts, Daguang Xu, Chester Chen, Yan Cheng, Andrew Feng
Categories: cs.LG
\\
  In the ever-evolving landscape of artificial intelligence (AI) and large
language models (LLMs), handling and leveraging data effectively has become a
critical challenge. Most state-of-the-art machine learning algorithms are
data-centric. However, as the lifeblood of model performance, necessary data
cannot always be centralized due to various factors such as privacy,
regulation, geopolitics, copyright issues, and the sheer effort required to
move vast datasets. In this paper, we explore how federated learning enabled by
NVIDIA FLARE can address these challenges with easy and scalable integration
capabilities, enabling parameter-efficient and full supervised fine-tuning of
LLMs for natural language processing and biopharmaceutical applications to
enhance their accuracy and robustness.
\\ ( https://arxiv.org/abs/2402.07792 ,  1654kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07808
Date: Mon, 12 Feb 2024 17:13:02 GMT   (6508kb,D)

Title: Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation
Authors: Julius Vetter, Guy Moss, Cornelius Schr\"oder, Richard Gao, Jakob H.
  Macke
Categories: cs.LG
\\
  Scientific modeling applications often require estimating a distribution of
parameters consistent with a dataset of observations - an inference task also
known as source distribution estimation. This problem can be ill-posed,
however, since many different source distributions might produce the same
distribution of data-consistent simulations. To make a principled choice among
many equally valid sources, we propose an approach which targets the maximum
entropy distribution, i.e., prioritizes retaining as much uncertainty as
possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein
distance to measure the discrepancy between the dataset and simulations - and
thus suitable for simulators with intractable likelihoods. We benchmark our
method on several tasks, and show that it can recover source distributions with
substantially higher entropy without sacrificing the fidelity of the
simulations. Finally, to demonstrate the utility of our approach, we infer
source distributions for parameters of the Hodgkin-Huxley neuron model from
experimental datasets with thousands of measurements. In summary, we propose a
principled framework for inferring unique source distributions of scientific
simulator parameters while retaining as much uncertainty as possible.
\\ ( https://arxiv.org/abs/2402.07808 ,  6508kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07818
Date: Mon, 12 Feb 2024 17:24:15 GMT   (292kb,D)

Title: Differentially Private Zeroth-Order Methods for Scalable Large Language
  Model Finetuning
Authors: Z Liu, J Lou, W Bao, Z Qin, K Ren
Categories: cs.LG cs.AI
\\
  Finetuning on task-specific datasets is a widely-embraced paradigm of
harnessing the powerful capability of pretrained LLMs for various downstream
tasks. Due to the popularity of LLMs finetuning and its accompanying privacy
concerns, differentially private (DP) finetuning of pretrained LLMs has
garnered increasing attention to safeguarding the privacy of task-specific
datasets. Lying at the design core of DP LLM finetuning methods is the
satisfactory tradeoff between privacy, utility, and scalability. Most existing
methods build upon the seminal work of DP-SGD. Despite pushing the scalability
of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately
limited by the inherent inefficiency of SGD. In this paper, we investigate the
potential of DP zeroth-order methods for LLM pretraining, which avoids the
scalability bottleneck of SGD by approximating the gradient with the more
efficient zeroth-order gradient. Rather than treating the zeroth-order method
as a drop-in replacement for SGD, this paper presents a comprehensive study
both theoretically and empirically. First, we propose the stagewise DP
zeroth-order method that dynamically schedules key hyperparameters. This design
is grounded on the synergy between DP random perturbation and the gradient
approximation error of the zeroth-order method, and its effect on finetuning
trajectory. Second, we further enhance the scalability by reducing the
trainable parameters that are identified by repurposing a data-free pruning
technique requiring no additional data or extra privacy budget. We provide
theoretical analysis for both proposed methods. We conduct extensive empirical
analysis on both encoder-only masked language model and decoder-only
autoregressive language model, achieving impressive results in terms of
scalability and utility.
\\ ( https://arxiv.org/abs/2402.07818 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07821
Date: Mon, 12 Feb 2024 17:25:23 GMT   (57kb,D)

Title: On Computationally Efficient Multi-Class Calibration
Authors: Parikshit Gopalan, Lunjia Hu, Guy N. Rothblum
Categories: cs.LG cs.CC cs.DS math.ST stat.ML stat.TH
\\
  Consider a multi-class labelling problem, where the labels can take values in
$[k]$, and a predictor predicts a distribution over the labels. In this work,
we study the following foundational question: Are there notions of multi-class
calibration that give strong guarantees of meaningful predictions and can be
achieved in time and sample complexities polynomial in $k$? Prior notions of
calibration exhibit a tradeoff between computational efficiency and
expressivity: they either suffer from having sample complexity exponential in
$k$, or needing to solve computationally intractable problems, or give rather
weak guarantees.
  Our main contribution is a notion of calibration that achieves all these
desiderata: we formulate a robust notion of projected smooth calibration for
multi-class predictions, and give new recalibration algorithms for efficiently
calibrating predictors under this definition with complexity polynomial in $k$.
Projected smooth calibration gives strong guarantees for all downstream
decision makers who want to use the predictor for binary classification
problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g.
is this an image of an animal? It ensures that the probabilities predicted by
summing the probabilities assigned to labels in $T$ are close to some perfectly
calibrated binary predictor for that task. We also show that natural
strengthenings of our definition are computationally hard to achieve: they run
into information theoretic barriers or computational intractability. Underlying
both our upper and lower bounds is a tight connection that we prove between
multi-class calibration and the well-studied problem of agnostic learning in
the (standard) binary prediction setting.
\\ ( https://arxiv.org/abs/2402.07821 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07834
Date: Mon, 12 Feb 2024 17:45:40 GMT   (2587kb,D)

Title: Generalizing across Temporal Domains with Koopman Operators
Authors: Qiuhao Zeng, Wei Wang, Fan Zhou, Gezheng Xu, Ruizhi Pu, Changjian
  Shui, Christian Gagne, Shichun Yang, Boyu Wang, Charles X. Ling
Categories: cs.LG
Comments: 15 pages, 7 figures, Accepted by AAAI 2024. arXiv admin note: text
  overlap with arXiv:2206.00047
\\
  In the field of domain generalization, the task of constructing a predictive
model capable of generalizing to a target domain without access to target data
remains challenging. This problem becomes further complicated when considering
evolving dynamics between domains. While various approaches have been proposed
to address this issue, a comprehensive understanding of the underlying
generalization theory is still lacking. In this study, we contribute novel
theoretic results that aligning conditional distribution leads to the reduction
of generalization bounds. Our analysis serves as a key motivation for solving
the Temporal Domain Generalization (TDG) problem through the application of
Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By
employing Koopman Operators, we effectively address the time-evolving
distributions encountered in TDG using the principles of Koopman theory, where
measurement functions are sought to establish linear transition relations
between evolving domains. Through empirical evaluations conducted on synthetic
and real-world datasets, we validate the effectiveness of our proposed
approach.
\\ ( https://arxiv.org/abs/2402.07834 ,  2587kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07845
Date: Mon, 12 Feb 2024 17:53:43 GMT   (927kb)

Title: An Investigation into Using Unsupervised Metrics to Optimise GNNs for
  Node Clustering
Authors: William Leeney and Ryan McConville
Categories: cs.LG cs.AI
\\
  Graph Neural Networks (GNNs) can be trained to detect communities within a
graph by learning from the duality of feature and connectivity information.
Currently, the common approach for optimisation of GNNs is to use comparisons
to ground-truth for hyperparameter tuning and model selection. In this work, we
show that nodes can be clustered into communities with GNNs by solely
optimising for modularity, without any comparison to ground-truth. Although
modularity is a graph partitioning quality metric, we show that this can be
used to optimise GNNs that also encode features without a drop in performance.
We take it a step further and also study whether the unsupervised metric
performance can predict ground-truth performance. To investigate why modularity
can be used to optimise GNNs, we design synthetic experiments that show the
limitations of this approach. The synthetic graphs are created to highlight
current capabilities in distinct, random and zero information space partitions
in attributed graphs. We conclude that modularity can be used for
hyperparameter optimisation and model selection on real-world datasets as well
as being a suitable proxy for predicting ground-truth performance, however,
GNNs fail to balance the information duality when the spaces contain
conflicting signals.
\\ ( https://arxiv.org/abs/2402.07845 ,  927kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07846
Date: Mon, 12 Feb 2024 17:56:52 GMT   (1170kb,D)

Title: Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow
  Matching on Assignment Manifolds
Authors: Bastian Boll, Daniel Gonzalez-Alvarado, Christoph Schn\"orr
Categories: cs.LG stat.ML
\\
  This paper introduces a novel generative model for discrete distributions
based on continuous normalizing flows on the submanifold of factorizing
discrete measures. Integration of the flow gradually assigns categories and
avoids issues of discretizing the latent continuous model like rounding, sample
truncation etc. General non-factorizing discrete distributions capable of
representing complex statistical dependencies of structured discrete data, can
be approximated by embedding the submanifold into a the meta-simplex of all
joint discrete distributions and data-driven averaging. Efficient training of
the generative model is demonstrated by matching the flow of geodesics of
factorizing discrete distributions. Various experiments underline the
approach's broad applicability.
\\ ( https://arxiv.org/abs/2402.07846 ,  1170kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07851
Date: Mon, 12 Feb 2024 17:59:20 GMT   (27478kb,D)

Title: Comparing skill of historical rainfall data based monsoon rainfall
  prediction in India with NCEP-NWP forecasts
Authors: Apoorva Narula, Aastha Jain, Jatin Batra, Sandeep Juneja
Categories: cs.LG
\\
  In this draft we consider the problem of forecasting rainfall across India
during the four monsoon months, one day as well as three days in advance. We
train neural networks using historical daily gridded precipitation data for
India obtained from IMD for the time period $1901- 2022$, at a spatial
resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical
weather prediction (NWP) forecasts obtained from NCEP (National Centre for
Environmental Prediction) available for the period 2011-2022. We conduct a
detailed country wide analysis and separately analyze some of the most
populated cities in India. Our conclusion is that forecasts obtained by
applying deep learning to historical rainfall data are more accurate compared
to NWP forecasts as well as predictions based on persistence. On average,
compared to our predictions, forecasts from NCEP-NWP model have about 34%
higher error for a single day prediction, and over 68% higher error for a three
day prediction. Similarly, persistence estimates report a 29% higher error in a
single day forecast, and over 54% error in a three day forecast. We further
observe that data up to 20 days in the past is useful in reducing errors of one
and three day forecasts, when a transformer based learning architecture, and to
a lesser extent when an LSTM is used. A key conclusion suggested by our
preliminary analysis is that NWP forecasts can be substantially improved upon
through more and diverse data relevant to monsoon prediction combined with
carefully selected neural network architecture.
\\ ( https://arxiv.org/abs/2402.07851 ,  27478kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07858
Date: Mon, 12 Feb 2024 18:05:03 GMT   (2109kb,D)

Title: Multiscale Neuroimaging Features for the Identification of Medication
  Class and Non-Responders in Mood Disorder Treatment
Authors: Bradley T. Baker, Mustafa S. Salman, Zening Fu, Armin Iraji, Elizabeth
  Osuch, Jeremy Bockholt, Vince D. Calhoun
Categories: cs.LG
\\
  In the clinical treatment of mood disorders, the complex behavioral symptoms
presented by patients and variability of patient response to particular
medication classes can create difficulties in providing fast and reliable
treatment when standard diagnostic and prescription methods are used.
Increasingly, the incorporation of physiological information such as
neuroimaging scans and derivatives into the clinical process promises to
alleviate some of the uncertainty surrounding this process. Particularly, if
neural features can help to identify patients who may not respond to standard
courses of anti-depressants or mood stabilizers, clinicians may elect to avoid
lengthy and side-effect-laden treatments and seek out a different, more
effective course that might otherwise not have been under consideration.
Previously, approaches for the derivation of relevant neuroimaging features
work at only one scale in the data, potentially limiting the depth of
information available for clinical decision support. In this work, we show that
the utilization of multi spatial scale neuroimaging features - particularly
resting state functional networks and functional network connectivity measures
- provide a rich and robust basis for the identification of relevant medication
class and non-responders in the treatment of mood disorders. We demonstrate
that the generated features, along with a novel approach for fast and automated
feature selection, can support high accuracy rates in the identification of
medication class and non-responders as well as the identification of novel,
multi-scale biomarkers.
\\ ( https://arxiv.org/abs/2402.07858 ,  2109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07868
Date: Mon, 12 Feb 2024 18:29:17 GMT   (66kb)

Title: Nesting Particle Filters for Experimental Design in Dynamical Systems
Authors: Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a, Hany Abdulsamad
Categories: cs.LG
Comments: The article has been made available early for dissemination. The
  empirical results are preliminary
\\
  In this paper, we propose a novel approach to Bayesian Experimental Design
(BED) for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested
sequential Monte Carlo (SMC) estimator of the expected information gain and
embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform
gradient-based policy optimization. This is in contrast to recent approaches
that rely on biased estimators of the expected information gain (EIG) to
amortize the cost of experiments by learning a design policy in advance.
Numerical validation on a set of dynamical systems showcases the efficacy of
our method in comparison to other state-of-the-art strategies.
\\ ( https://arxiv.org/abs/2402.07868 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07871
Date: Mon, 12 Feb 2024 18:33:47 GMT   (7313kb,D)

Title: Scaling Laws for Fine-Grained Mixture of Experts
Authors: Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pi\'oro,
  Micha{\l} Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Kr\'ol, Tomasz
  Odrzyg\'o\'zd\'z, Piotr Sankowski, Marek Cygan, Sebastian Jaszczur
Categories: cs.LG cs.AI cs.CL
\\
  Mixture of Experts (MoE) models have emerged as a primary solution for
reducing the computational cost of Large Language Models. In this work, we
analyze their scaling properties, incorporating an expanded range of variables.
Specifically, we introduce a new hyperparameter, granularity, whose adjustment
enables precise control over the size of the experts. Building on this, we
establish scaling laws for fine-grained MoE, taking into account the number of
training tokens, model size, and granularity. Leveraging these laws, we derive
the optimal training configuration for a given computational budget. Our
findings not only show that MoE models consistently outperform dense
Transformers but also highlight that the efficiency gap between dense and MoE
models widens as we scale up the model size and training budget. Furthermore,
we demonstrate that the common practice of setting the size of experts in MoE
to mirror the feed-forward layer is not optimal at almost any computational
budget.
\\ ( https://arxiv.org/abs/2402.07871 ,  7313kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07875
Date: Mon, 12 Feb 2024 18:41:31 GMT   (3118kb,D)

Title: Implicit Bias of Policy Gradient in Linear Quadratic Control:
  Extrapolation to Unseen Initial States
Authors: Noam Razin, Yotam Alexander, Edo Cohen-Karlik, Raja Giryes, Amir
  Globerson, Nadav Cohen
Categories: cs.LG cs.AI cs.SY eess.SY stat.ML
\\
  In modern machine learning, models can often fit training data in numerous
ways, some of which perform well on unseen (test) data, while others do not.
Remarkably, in such cases gradient descent frequently exhibits an implicit bias
that leads to excellent performance on unseen data. This implicit bias was
extensively studied in supervised learning, but is far less understood in
optimal control (reinforcement learning). There, learning a controller applied
to a system via gradient descent is known as policy gradient, and a question of
prime importance is the extent to which a learned controller extrapolates to
unseen initial states. This paper theoretically studies the implicit bias of
policy gradient in terms of extrapolation to unseen initial states. Focusing on
the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the
extent of extrapolation depends on the degree of exploration induced by the
system when commencing from initial states included in training. Experiments
corroborate our theory, and demonstrate its conclusions on problems beyond LQR,
where systems are non-linear and controllers are neural networks. We
hypothesize that real-world optimal control may be greatly improved by
developing methods for informed selection of initial states to train on.
\\ ( https://arxiv.org/abs/2402.07875 ,  3118kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07876
Date: Mon, 12 Feb 2024 18:41:34 GMT   (13524kb,D)

Title: Policy Improvement using Language Feedback Models
Authors: Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre C\^ot\'e
Categories: cs.LG cs.AI cs.CL
\\
  We introduce Language Feedback Models (LFMs) that identify desirable
behaviour - actions that help achieve tasks specified in the instruction - for
imitation learning in instruction following. To train LFMs, we obtain feedback
from Large Language Models (LLMs) on visual trajectories verbalized to language
descriptions. First, by using LFMs to identify desirable behaviour to imitate,
we improve in task-completion rate over strong behavioural cloning baselines on
three distinct language grounding environments (Touchdown, ScienceWorld, and
ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict
actions, when controlling for the number of LLM output tokens. Third, LFMs
generalize to unseen environments, improving task-completion rate by 3.5-12.0%
through one round of adaptation. Finally, LFM can be modified to provide
human-interpretable feedback without performance loss, allowing human
verification of desirable behaviour for imitation learning.
\\ ( https://arxiv.org/abs/2402.07876 ,  13524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07901
Date: Mon, 12 Feb 2024 18:59:39 GMT   (2416kb,D)

Title: FAST: Factorizable Attention for Speeding up Transformers
Authors: Armin Gerami, Monte Hoover, Pranav S. Dulepet, Ramani Duraiswami
Categories: cs.LG cs.AI cs.NA math.NA
\\
  Motivated by the factorization inherent in the original fast multipole method
and the improved fast Gauss transform we introduce a factorable form of
attention that operates efficiently in high dimensions. This approach reduces
the computational and memory complexity of the attention mechanism in
transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our
work presents a linearly scaled attention mechanism that maintains the full
representation of the attention matrix without compromising on sparsification
and incorporates the all-to-all relationship between tokens. We explore the
properties of our new attention metric and conduct tests in various standard
settings. Results indicate that our attention mechanism has a robust
performance and holds significant promise for diverse applications where
self-attention is used.
\\ ( https://arxiv.org/abs/2402.07901 ,  2416kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.06629 (*cross-listing*)
Date: Tue, 9 Jan 2024 08:30:55 GMT   (53kb,D)

Title: Towards the mathematical foundation of the minimum enclosing ball and
  related problems
Authors: Michael N. Vrahatis
Categories: cs.CG cs.AI math.GT
Comments: arXiv admin note: text overlap with arXiv:2401.03232
\\
  Theoretical background is provided towards the mathematical foundation of the
minimum enclosing ball problem. This problem concerns the determination of the
unique spherical surface of smallest radius enclosing a given bounded set in
the d-dimensional Euclidean space. The study of several problems that are
similar or related to the minimum enclosing ball problem has received a
considerable impetus from the large amount of applications of these problems in
various fields of science and technology. The proposed theoretical framework is
based on several enclosing (covering) and partitioning (clustering) theorems
and provides among others bounds and relations between the circumradius,
inradius, diameter and width of a set. These enclosing and partitioning
theorems are considered as cornerstones in the field that strongly influencing
developments and generalizations to other spaces and non-Euclidean geometries.
\\ ( https://arxiv.org/abs/2402.06629 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06638 (*cross-listing*)
Date: Mon, 22 Jan 2024 07:33:28 GMT   (4940kb,D)

Title: Transformers with Attentive Federated Aggregation for Time Series Stock
  Forecasting
Authors: Chu Myaet Thwal, Ye Lin Tun, Kitae Kim, Seong-Bae Park, Choong Seon
  Hong
Categories: q-fin.ST cs.AI cs.DC cs.LG
Comments: Published in IEEE ICOIN 2023
DOI: 10.1109/ICOIN56518.2023.10048928
\\
  Recent innovations in transformers have shown their superior performance in
natural language processing (NLP) and computer vision (CV). The ability to
capture long-range dependencies and interactions in sequential data has also
triggered a great interest in time series modeling, leading to the widespread
use of transformers in many time series applications. However, being the most
common and crucial application, the adaptation of transformers to time series
forecasting has remained limited, with both promising and inconsistent results.
In contrast to the challenges in NLP and CV, time series problems not only add
the complexity of order or temporal dependence among input sequences but also
consider trend, level, and seasonality information that much of this data is
valuable for decision making. The conventional training scheme has shown
deficiencies regarding model overfitting, data scarcity, and privacy issues
when working with transformers for a forecasting task. In this work, we propose
attentive federated transformers for time series stock forecasting with better
performance while preserving the privacy of participating enterprises.
Empirical results on various stock data from the Yahoo! Finance website
indicate the superiority of our proposed scheme in dealing with the above
challenges and data heterogeneity in federated learning.
\\ ( https://arxiv.org/abs/2402.06638 ,  4940kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06655 (*cross-listing*)
Date: Mon, 5 Feb 2024 02:36:41 GMT   (401kb,D)

Title: Adversarial Text Purification: A Large Language Model Approach for
  Defense
Authors: Raha Moraffah, Shubh Khandelwal, Amrita Bhattacharjee, and Huan Liu
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: PAKDD 2024
\\
  Adversarial purification is a defense mechanism for safeguarding classifiers
against adversarial attacks without knowing the type of attacks or training of
the classifier. These techniques characterize and eliminate adversarial
perturbations from the attacked inputs, aiming to restore purified samples that
retain similarity to the initially attacked ones and are correctly classified
by the classifier. Due to the inherent challenges associated with
characterizing noise perturbations for discrete inputs, adversarial text
purification has been relatively unexplored. In this paper, we investigate the
effectiveness of adversarial purification methods in defending text
classifiers. We propose a novel adversarial text purification that harnesses
the generative capabilities of Large Language Models (LLMs) to purify
adversarial text without the need to explicitly characterize the discrete noise
perturbations. We utilize prompt engineering to exploit LLMs for recovering the
purified examples for given adversarial examples such that they are
semantically similar and correctly classified. Our proposed method demonstrates
remarkable performance over various classifiers, improving their accuracy under
the attack by over 65% on average.
\\ ( https://arxiv.org/abs/2402.06655 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06656 (*cross-listing*)
Date: Mon, 5 Feb 2024 03:54:36 GMT   (11049kb,D)

Title: DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation
Authors: Yuan Gao, Haokun Chen, Xiang Wang, Zhicai Wang, Xue Wang, Jinyang Gao,
  Bolin Ding
Categories: q-fin.ST cs.AI cs.LG
\\
  Machine learning models have demonstrated remarkable efficacy and efficiency
in a wide range of stock forecasting tasks. However, the inherent challenges of
data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity,
pose significant obstacles to accurate forecasting. To address this issue, we
propose a novel approach that utilizes artificial intelligence-generated
samples (AIGS) to enhance the training procedures. In our work, we introduce
the Diffusion Model to generate stock factors with Transformer architecture
(DiffsFormer). DiffsFormer is initially trained on a large-scale source domain,
incorporating conditional guidance so as to capture global joint distribution.
When presented with a specific downstream task, we employ DiffsFormer to
augment the training procedure by editing existing samples. This editing step
allows us to control the strength of the editing process, determining the
extent to which the generated data deviates from the target domain. To evaluate
the effectiveness of DiffsFormer augmented training, we conduct experiments on
the CSI300 and CSI800 datasets, employing eight commonly used machine learning
models. The proposed method achieves relative improvements of 7.2% and 27.8% in
annualized return ratio for the respective datasets. Furthermore, we perform
extensive experiments to gain insights into the functionality of DiffsFormer
and its constituent components, elucidating how they address the challenges of
data scarcity and enhance the overall model performance. Our research
demonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture
to mitigate data scarcity in stock forecasting tasks.
\\ ( https://arxiv.org/abs/2402.06656 ,  11049kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06659 (*cross-listing*)
Date: Mon, 5 Feb 2024 18:55:53 GMT   (15231kb,D)

Title: Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language
  Models
Authors: Yuancheng Xu, Jiarui Yao, Manli Shu, Yanchao Sun, Zichu Wu, Ning Yu,
  Tom Goldstein, Furong Huang
Categories: cs.CR cs.AI cs.LG
\\
  Vision-Language Models (VLMs) excel in generating textual responses from
visual inputs, yet their versatility raises significant security concerns. This
study takes the first step in exposing VLMs' susceptibility to data poisoning
attacks that can manipulate responses to innocuous, everyday prompts. We
introduce Shadowcast, a stealthy data poisoning attack method where poison
samples are visually indistinguishable from benign images with matching texts.
Shadowcast demonstrates effectiveness in two attack types. The first is Label
Attack, tricking VLMs into misidentifying class labels, such as confusing
Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages
VLMs' text generation capabilities to craft narratives, such as portraying junk
food as health food, through persuasive and seemingly rational descriptions. We
show that Shadowcast are highly effective in achieving attacker's intentions
using as few as 50 poison samples. Moreover, these poison samples remain
effective across various prompts and are transferable across different VLM
architectures in the black-box setting. This work reveals how poisoned VLMs can
generate convincing yet deceptive misinformation and underscores the importance
of data quality for responsible deployments of VLMs. Our code is available at:
https://github.com/umd-huang-lab/VLM-Poisoning.
\\ ( https://arxiv.org/abs/2402.06659 ,  15231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06663 (*cross-listing*)
Date: Tue, 6 Feb 2024 11:19:20 GMT   (1829kb,D)

Title: Explainable Adversarial Learning Framework on Physical Layer Secret Keys
  Combating Malicious Reconfigurable Intelligent Surface
Authors: Zhuangkun Wei, Wenxiu Hu, Weisi Guo
Categories: cs.CR cs.AI
\\
  The development of reconfigurable intelligent surfaces (RIS) is a
double-edged sword to physical layer security (PLS). Whilst a legitimate RIS
can yield beneficial impacts including increased channel randomness to enhance
physical layer secret key generation (PL-SKG), malicious RIS can poison
legitimate channels and crack most of existing PL-SKGs. In this work, we
propose an adversarial learning framework between legitimate parties (namely
Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS)
eavesdropping. First, the theoretical mutual information gap between legitimate
pairs and MITM-RIS is deduced. Then, Alice and Bob leverage generative
adversarial networks (GANs) to learn to achieve a common feature surface that
does not have mutual information overlap with MITM-RIS. Next, we aid signal
processing interpretation of black-box neural networks by using a symbolic
explainable AI (xAI) representation. These symbolic terms of dominant neurons
aid feature engineering-based validation and future design of PLS common
feature space. Simulation results show that our proposed GAN-based and
symbolic-based PL-SKGs can achieve high key agreement rates between legitimate
users, and is even resistant to MITM-RIS Eve with the knowledge of legitimate
feature generation (NNs or formulas). This therefore paves the way to secure
wireless communications with untrusted reflective devices in future 6G.
\\ ( https://arxiv.org/abs/2402.06663 ,  1829kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06664 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:46:08 GMT   (464kb,D)

Title: LLM Agents can Autonomously Hack Websites
Authors: Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang
Categories: cs.CR cs.AI
\\
  In recent years, large language models (LLMs) have become increasingly
capable and can now interact with tools (i.e., call functions), read documents,
and recursively call themselves. As a result, these LLMs can now function
autonomously as agents. With the rise in capabilities of these agents, recent
work has speculated on how LLM agents would affect cybersecurity. However, not
much is known about the offensive capabilities of LLM agents.
  In this work, we show that LLM agents can autonomously hack websites,
performing tasks as complex as blind database schema extraction and SQL
injections without human feedback. Importantly, the agent does not need to know
the vulnerability beforehand. This capability is uniquely enabled by frontier
models that are highly capable of tool use and leveraging extended context.
Namely, we show that GPT-4 is capable of such hacks, but existing open-source
models are not. Finally, we show that GPT-4 is capable of autonomously finding
vulnerabilities in websites in the wild. Our findings raise questions about the
widespread deployment of LLMs.
\\ ( https://arxiv.org/abs/2402.06664 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06666 (*cross-listing*)
Date: Tue, 6 Feb 2024 21:28:42 GMT   (6845kb,D)

Title: Weather Prediction with Diffusion Guided by Realistic Forecast Processes
Authors: Zhanxiang Hua, Yutong He, Chengqian Ma, Alexandra Anderson-Frey
Categories: physics.ao-ph cs.AI cs.LG
\\
  Weather forecasting remains a crucial yet challenging domain, where recently
developed models based on deep learning (DL) have approached the performance of
traditional numerical weather prediction (NWP) models. However, these DL
models, often complex and resource-intensive, face limitations in flexibility
post-training and in incorporating NWP predictions, leading to reliability
concerns due to potential unphysical predictions. In response, we introduce a
novel method that applies diffusion models (DM) for weather forecasting. In
particular, our method can achieve both direct and iterative forecasting with
the same modeling framework. Our model is not only capable of generating
forecasts independently but also uniquely allows for the integration of NWP
predictions, even with varying lead times, during its sampling process. The
flexibility and controllability of our model empowers a more trustworthy DL
system for the general weather community. Additionally, incorporating
persistence and climatology data further enhances our model's long-term
forecasting stability. Our empirical findings demonstrate the feasibility and
generalizability of this approach, suggesting a promising direction for future,
more sophisticated diffusion models without the need for retraining.
\\ ( https://arxiv.org/abs/2402.06666 ,  6845kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06680 (*cross-listing*)
Date: Thu, 8 Feb 2024 04:43:33 GMT   (588kb,D)

Title: Social Physics Informed Diffusion Model for Crowd Simulation
Authors: Hongyi Chen, Jingtao Ding, Yong Li, Yue Wang, Xiao-Ping Zhang
Categories: physics.soc-ph cs.AI cs.LG
\\
  Crowd simulation holds crucial applications in various domains, such as urban
planning, architectural design, and traffic arrangement. In recent years,
physics-informed machine learning methods have achieved state-of-the-art
performance in crowd simulation but fail to model the heterogeneity and
multi-modality of human movement comprehensively. In this paper, we propose a
social physics-informed diffusion model named SPDiff to mitigate the above gap.
SPDiff takes both the interactive and historical information of crowds in the
current timeframe to reverse the diffusion process, thereby generating the
distribution of pedestrian movement in the subsequent timeframe. Inspired by
the well-known social physics model, i.e., Social Force, regarding crowd
dynamics, we design a crowd interaction module to guide the denoising process
and further enhance this module with the equivariant properties of crowd
interactions. To mitigate error accumulation in long-term simulations, we
propose a multi-frame rollout training algorithm for diffusion modeling.
Experiments conducted on two real-world datasets demonstrate the superior
performance of SPDiff in terms of macroscopic and microscopic evaluation
metrics. Code and appendix are available at
https://github.com/tsinghua-fib-lab/SPDiff.
\\ ( https://arxiv.org/abs/2402.06680 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06682 (*cross-listing*)
Date: Thu, 8 Feb 2024 07:18:23 GMT   (1308kb,D)

Title: Private Knowledge Sharing in Distributed Learning: A Survey
Authors: Yasas Supeksala, Dinh C. Nguyen, Ming Ding, Thilina Ranbaduge, Calson
  Chua, Jun Zhang, Jun Li and H. Vincent Poor
Categories: cs.CR cs.AI cs.LG
Comments: Manuscript submitted to ACM
\\
  The rise of Artificial Intelligence (AI) has revolutionized numerous
industries and transformed the way society operates. Its widespread use has led
to the distribution of AI and its underlying data across many intelligent
systems. In this light, it is crucial to utilize information in learning
processes that are either distributed or owned by different entities. As a
result, modern data-driven services have been developed to integrate
distributed knowledge entities into their outcomes. In line with this goal, the
latest AI models are frequently trained in a decentralized manner. Distributed
learning involves multiple entities working together to make collective
predictions and decisions. However, this collaboration can also bring about
security vulnerabilities and challenges. This paper provides an in-depth survey
on private knowledge sharing in distributed learning, examining various
knowledge components utilized in leading distributed learning architectures.
Our analysis sheds light on the most critical vulnerabilities that may arise
when using these components in a distributed setting. We further identify and
examine defensive strategies for preserving the privacy of these knowledge
components and preventing malicious parties from manipulating or accessing the
knowledge information. Finally, we highlight several key limitations of
knowledge sharing in distributed learning and explore potential avenues for
future research.
\\ ( https://arxiv.org/abs/2402.06682 ,  1308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06692 (*cross-listing*)
Date: Thu, 8 Feb 2024 20:14:46 GMT   (7348kb,D)

Title: HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image
  Translation
Authors: Hrishav Bakul Barua, Ganesh Krishnasamy, KokSheik Wong, Abhinav Dhall,
  Kalin Stefanov
Categories: eess.IV cs.AI cs.CV cs.GR cs.LG cs.MM
Comments: Submitted to IEEE
MSC-class: Artificial intelligence, Computer vision, Machine learning, Deep
  learning
ACM-class: I.3.3; I.4.5
\\
  High Dynamic Range (HDR) imaging aims to replicate the high visual quality
and clarity of real-world scenes. Due to the high costs associated with HDR
imaging, the literature offers various data-driven methods for HDR image
reconstruction from Low Dynamic Range (LDR) counterparts. A common limitation
of these approaches is missing details in regions of the reconstructed HDR
images, which are over- or under-exposed in the input LDR images. To this end,
we propose a simple and effective method, HistoHDR-Net, to recover the fine
details (e.g., color, contrast, saturation, and brightness) of HDR images via a
fusion-based approach utilizing histogram-equalized LDR images along with
self-attention guidance. Our experiments demonstrate the efficacy of the
proposed approach over the state-of-art methods.
\\ ( https://arxiv.org/abs/2402.06692 ,  7348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06759 (*cross-listing*)
Date: Fri, 9 Feb 2024 19:44:29 GMT   (2214kb)

Title: A Methodology for Questionnaire Analysis: Insights through Cluster
  Analysis of an Investor Competition Data
Authors: Carlos Henrique Q. Forster, Paulo Andr\'e Lima de Castro and Andrei
  Ramalho
Categories: cs.HC cs.AI
Comments: 14 pages, 12 figures
\\
  In this paper, we propose a methodology for the analysis of questionnaire
data along with its application on discovering insights from investor data
motivated by a day trading competition. The questionnaire includes categorical
questions, which are reduced to binary questions, 'yes' or 'no'. The
methodology reduces dimensionality by grouping questions and participants with
similar responses using clustering analysis. Rule discovery was performed by
using a conversion rate metric. Innovative visual representations were proposed
to validate the cluster analysis and the relation discovery between questions.
When crossing with financial data, additional insights were revealed related to
the recognized clusters.
\\ ( https://arxiv.org/abs/2402.06759 ,  2214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06772 (*cross-listing*)
Date: Fri, 9 Feb 2024 20:25:45 GMT   (2998kb,D)

Title: Retrosynthesis Prediction via Search in (Hyper) Graph
Authors: Zixun Lan, Binjie Hong, Jiajun Zhu, Zuo Zeng, Zhenfu Liu, Limin Yu,
  Fei Ma
Categories: q-bio.QM cs.AI cs.CE cs.LG
\\
  Predicting reactants from a specified core product stands as a fundamental
challenge within organic synthesis, termed retrosynthesis prediction. Recently,
semi-template-based methods and graph-edits-based methods have achieved good
performance in terms of both interpretability and accuracy. However, due to
their mechanisms these methods cannot predict complex reactions, e.g.,
reactions with multiple reaction center or attaching the same leaving group to
more than one atom. In this study we propose a semi-template-based method, the
\textbf{Retro}synthesis via \textbf{S}earch \textbf{i}n (Hyper) \textbf{G}raph
(RetroSiG) framework to alleviate these limitations. In the proposed method, we
turn the reaction center identification and the leaving group completion tasks
as tasks of searching in the product molecular graph and leaving group
hypergraph respectively. As a semi-template-based method RetroSiG has several
advantages. First, RetroSiG is able to handle the complex reactions mentioned
above by its novel search mechanism. Second, RetroSiG naturally exploits the
hypergraph to model the implicit dependencies between leaving groups. Third,
RetroSiG makes full use of the prior, i.e., one-hop constraint. It reduces the
search space and enhances overall performance. Comprehensive experiments
demonstrated that RetroSiG achieved competitive results. Furthermore, we
conducted experiments to show the capability of RetroSiG in predicting complex
reactions. Ablation experiments verified the efficacy of specific elements,
such as the one-hop constraint and the leaving group hypergraph.
\\ ( https://arxiv.org/abs/2402.06772 ,  2998kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06784 (*cross-listing*)
Date: Fri, 9 Feb 2024 21:17:31 GMT   (1965kb,D)

Title: Transfer learning with generative models for object detection on limited
  datasets
Authors: Matteo Paiano, Stefano Martina, Carlotta Giannelli, Filippo Caruso
Categories: cs.CV cond-mat.dis-nn cs.AI cs.LG cs.NA math.NA
Comments: 24 pages, 15 figures
MSC-class: 68T05, 68T07, 68T10, 68T45,
ACM-class: I.2.6; I.2.0; I.4.8; I.4.9; I.5.1; I.5.0; I.5.4; J.3
\\
  The availability of data is limited in some fields, especially for object
detection tasks, where it is necessary to have correctly labeled bounding boxes
around each object. A notable example of such data scarcity is found in the
domain of marine biology, where it is useful to develop methods to
automatically detect submarine species for environmental monitoring. To address
this data limitation, the state-of-the-art machine learning strategies employ
two main approaches. The first involves pretraining models on existing datasets
before generalizing to the specific domain of interest. The second strategy is
to create synthetic datasets specifically tailored to the target domain using
methods like copy-paste techniques or ad-hoc simulators. The first strategy
often faces a significant domain shift, while the second demands custom
solutions crafted for the specific task. In response to these challenges, here
we propose a transfer learning framework that is valid for a generic scenario.
In this framework, generated images help to improve the performances of an
object detector in a few-real data regime. This is achieved through a
diffusion-based generative model that was pretrained on large generic datasets,
and is not trained on the task-specific domain. We validate our approach on
object detection tasks, specifically focusing on fishes in an underwater
environment, and on the more common domain of cars in an urban setting. Our
method achieves detection performance comparable to models trained on thousands
of images, using only a few hundreds of input data. Our results pave the way
for new generative AI-based protocols for machine learning applications in
various domains, for instance ranging from geophysics to biology and medicine.
\\ ( https://arxiv.org/abs/2402.06784 ,  1965kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06794 (*cross-listing*)
Date: Fri, 9 Feb 2024 21:37:13 GMT   (11204kb,D)

Title: Is it safe to cross? Interpretable Risk Assessment with GPT-4V for
  Safety-Aware Street Crossing
Authors: Hochul Hwang, Sunjae Kwon, Yekyung Kim and Donghyun Kim
Categories: cs.CV cs.AI
\\
  Safely navigating street intersections is a complex challenge for blind and
low-vision individuals, as it requires a nuanced understanding of the
surrounding context - a task heavily reliant on visual cues. Traditional
methods for assisting in this decision-making process often fall short, lacking
the ability to provide a comprehensive scene analysis and safety level. This
paper introduces an innovative approach that leverages large multimodal models
(LMMs) to interpret complex street crossing scenes, offering a potential
advancement over conventional traffic signal recognition techniques. By
generating a safety score and scene description in natural language, our method
supports safe decision-making for the blind and low-vision individuals. We
collected crosswalk intersection data that contains multiview egocentric images
captured by a quadruped robot and annotated the images with corresponding
safety scores based on our predefined safety score categorization. Grounded on
the visual knowledge, extracted from images, and text prompt, we evaluate a
large multimodal model for safety score prediction and scene description. Our
findings highlight the reasoning and safety score prediction capabilities of a
LMM, activated by various prompts, as a pathway to developing a trustworthy
system, crucial for applications requiring reliable decision-making support.
\\ ( https://arxiv.org/abs/2402.06794 ,  11204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06810 (*cross-listing*)
Date: Fri, 9 Feb 2024 22:15:39 GMT   (1449kb,D)

Title: Evaluating Co-Creativity using Total Information Flow
Authors: Vignesh Gokul, Chris Francis, Shlomo Dubnov
Categories: cs.SD cs.AI cs.HC cs.IT cs.LG eess.AS math.IT
\\
  Co-creativity in music refers to two or more musicians or musical agents
interacting with one another by composing or improvising music. However, this
is a very subjective process and each musician has their own preference as to
which improvisation is better for some context. In this paper, we aim to create
a measure based on total information flow to quantitatively evaluate the
co-creativity process in music. In other words, our measure is an indication of
how "good" a creative musical process is. Our main hypothesis is that a good
musical creation would maximize information flow between the participants
captured by music voices recorded in separate tracks. We propose a method to
compute the information flow using pre-trained generative models as entropy
estimators. We demonstrate how our method matches with human perception using a
qualitative study.
\\ ( https://arxiv.org/abs/2402.06810 ,  1449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06871 (*cross-listing*)
Date: Sat, 10 Feb 2024 03:21:13 GMT   (2022kb,D)

Title: Non-autoregressive Generative Models for Reranking Recommendation
Authors: Yuxin Ren, Qiya Yang, Yichun Wu, Wei Xu, Yalong Wang, Zhiqiang Zhang
Categories: cs.IR cs.AI
Comments: Work in progress
\\
  In a multi-stage recommendation system, reranking plays a crucial role by
modeling the intra-list correlations among items.The key challenge of reranking
lies in the exploration of optimal sequences within the combinatorial space of
permutations. Recent research proposes a generator-evaluator learning paradigm,
where the generator generates multiple feasible sequences and the evaluator
picks out the best sequence based on the estimated listwise score. Generator is
of vital importance, and generative models are well-suited for the generator
function. Current generative models employ an autoregressive strategy for
sequence generation. However, deploying autoregressive models in real-time
industrial systems is challenging. Hence, we propose a Non-AutoRegressive
generative model for reranking Recommendation (NAR4Rec) designed to enhance
efficiency and effectiveness. To address challenges related to sparse training
samples and dynamic candidates impacting model convergence, we introduce a
matching model. Considering the diverse nature of user feedback, we propose a
sequence-level unlikelihood training objective to distinguish feasible from
unfeasible sequences. Additionally, to overcome the lack of dependency modeling
in non-autoregressive models regarding target items, we introduce contrastive
decoding to capture correlations among these items. Extensive offline
experiments on publicly available datasets validate the superior performance of
our proposed approach compared to the existing state-of-the-art reranking
methods. Furthermore, our method has been fully deployed in a popular video app
Kuaishou with over 300 million daily active users, significantly enhancing
online recommendation quality, and demonstrating the effectiveness and
efficiency of our approach.
\\ ( https://arxiv.org/abs/2402.06871 ,  2022kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06931 (*cross-listing*)
Date: Sat, 10 Feb 2024 12:05:52 GMT   (1778kb,D)

Title: ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive
  Applications in 6G
Authors: Masoud Shokrnezhad and Tarik Taleb
Categories: cs.NI cs.AI cs.DC cs.LG
Comments: Conference, 6 pages, 2 figures, 28 equations, 1 table, 1 algorithm,
  and 16 references
\\
  Anticipation for 6G's arrival comes with growing concerns about increased
energy consumption in computing and networking. The expected surge in connected
devices and resource-demanding applications presents unprecedented challenges
for energy resources. While sustainable resource allocation strategies have
been discussed in the past, these efforts have primarily focused on
single-domain orchestration or ignored the unique requirements posed by 6G. To
address this gap, we investigate the joint problem of service instance
placement and assignment, path selection, and request prioritization, dubbed
PIRA. The objective function is to maximize the system's overall profit as a
function of the number of concurrently supported requests while simultaneously
minimizing energy consumption over an extended period of time. In addition,
end-to-end latency requirements and resource capacity constraints are
considered for computing and networking resources, where queuing theory is
utilized to estimate the Age of Information (AoI) for requests. After
formulating the problem in a non-linear fashion, we prove its NP-hardness and
propose a method, denoted ORIENT. This method is based on the Double Dueling
Deep Q-Learning (D3QL) mechanism and leverages Graph Neural Networks (GNNs) for
state encoding. Extensive numerical simulations demonstrate that ORIENT yields
near-optimal solutions for varying system sizes and request counts.
\\ ( https://arxiv.org/abs/2402.06931 ,  1778kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06938 (*cross-listing*)
Date: Sat, 10 Feb 2024 12:26:20 GMT   (1971kb,D)

Title: Efficient Resource Scheduling for Distributed Infrastructures Using
  Negotiation Capabilities
Authors: Junjie Chu and Prashant Singh and Salman Toor
Categories: cs.DC cs.AI cs.LG
Comments: Accepted in IEEE CLOUD 2023. 13 pages, 5 figures
\\
  In the past few decades, the rapid development of information and internet
technologies has spawned massive amounts of data and information. The
information explosion drives many enterprises or individuals to seek to rent
cloud computing infrastructure to put their applications in the cloud. However,
the agreements reached between cloud computing providers and clients are often
not efficient. Many factors affect the efficiency, such as the idleness of the
providers' cloud computing infrastructure, and the additional cost to the
clients. One possible solution is to introduce a comprehensive, bargaining game
(a type of negotiation), and schedule resources according to the negotiation
results. We propose an agent-based auto-negotiation system for resource
scheduling based on fuzzy logic. The proposed method can complete a one-to-one
auto-negotiation process and generate optimal offers for the provider and
client. We compare the impact of different member functions, fuzzy rule sets,
and negotiation scenario cases on the offers to optimize the system. It can be
concluded that our proposed method can utilize resources more efficiently and
is interpretable, highly flexible, and customizable. We successfully train
machine learning models to replace the fuzzy negotiation system to improve
processing speed. The article also highlights possible future improvements to
the proposed system and machine learning models. All the codes and data are
available in the open-source repository.
\\ ( https://arxiv.org/abs/2402.06938 ,  1971kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06945 (*cross-listing*)
Date: Sat, 10 Feb 2024 13:18:10 GMT   (588kb,D)

Title: Evaluation Metrics for Automated Typographic Poster Generation
Authors: S\'ergio M. Rebelo, J. J. Merelo, Jo\~ao Bicker, Penousal Machado
Categories: cs.MM cs.AI cs.HC
Comments: Paper accepted be presented in the 13th International Conference
  Artificial Intelligence in Music, Sound, Art and Design -- EvoMUSART 2024,
  Held as Part of EvoStar 2024, Aberystwyth, Wales, United Kingdom, April
  3\textendash{}5, 2024
MSC-class: 68W50
ACM-class: I.2.1; I.7; J.7; J.5
\\
  Computational Design approaches facilitate the generation of typographic
design, but evaluating these designs remains a challenging task. In this paper,
we propose a set of heuristic metrics for typographic design evaluation,
focusing on their legibility, which assesses the text visibility, aesthetics,
which evaluates the visual quality of the design, and semantic features, which
estimate how effectively the design conveys the content semantics. We
experiment with a constrained evolutionary approach for generating typographic
posters, incorporating the proposed evaluation metrics with varied setups, and
treating the legibility metrics as constraints. We also integrate emotion
recognition to identify text semantics automatically and analyse the
performance of the approach and the visual characteristics outputs.
\\ ( https://arxiv.org/abs/2402.06945 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06952 (*cross-listing*)
Date: Sat, 10 Feb 2024 13:42:14 GMT   (1741kb,D)

Title: Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy
  Intermediate-Scale Quantum Devices
Authors: Sovanmonynuth Heng, Myeongseong Go, Youngsun Han
Categories: quant-ph cs.AI
\\
  Current advancements in technology have focused the attention of the quantum
computing community toward exploring the potential of near-term devices whose
computing power surpasses that of classical computers in practical
applications. An unresolved central question revolves around whether the
inherent noise in these devices can be overcome or whether any potential
quantum advantage would be limited. There is no doubt that crosstalk is one of
the main sources of noise in noisy intermediate-scale quantum (NISQ) systems,
and it poses a fundamental challenge to hardware designs. Crosstalk between
parallel instructions can corrupt quantum states and cause incorrect program
execution. In this study, we present a comprehensive analysis of the crosstalk
error effect on NISQ computers. Our approach is extremely straightforward and
practical for characterizing the crosstalk error of various multi-qubit
devices. In particular, we combine the randomized benchmarking (RB) and
simultaneous randomized benchmarking (SRB) protocol to characterize the
crosstalk error from the correlation controlled-NOT (CNOT) gate. We demonstrate
this protocol experimentally on 5- \& 7-qubit devices. Our results demonstrate
the crosstalk error model of two different IBM quantum devices over the
experimental week and compare the error variation against the machine, number
of qubits, quantum volume, processor, and topology of the IBM quantum devices.
We then confirm the improvement in the circuit fidelity on different benchmarks
by up to 3.06x via inserting an instruction barrier, as compared with an IBM
quantum noisy device which offers near-optimal crosstalk mitigation in
practice. Most importantly, we provide insight to ensure that the quantum
operation can perform its quantum magic undisturbed.
\\ ( https://arxiv.org/abs/2402.06952 ,  1741kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06957 (*cross-listing*)
Date: Sat, 10 Feb 2024 13:57:51 GMT   (6820kb,D)

Title: Architectural Neural Backdoors from First Principles
Authors: Harry Langford, Ilia Shumailov, Yiren Zhao, Robert Mullins, Nicolas
  Papernot
Categories: cs.CR cs.AI cs.CV cs.LG
\\
  While previous research backdoored neural networks by changing their
parameters, recent work uncovered a more insidious threat: backdoors embedded
within the definition of the network's architecture. This involves injecting
common architectural components, such as activation functions and pooling
layers, to subtly introduce a backdoor behavior that persists even after (full
re-)training. However, the full scope and implications of architectural
backdoors have remained largely unexplored. Bober-Irizar et al. [2023]
introduced the first architectural backdoor; they showed how to create a
backdoor for a checkerboard pattern, but never explained how to target an
arbitrary trigger pattern of choice. In this work we construct an arbitrary
trigger detector which can be used to backdoor an architecture with no human
supervision. This leads us to revisit the concept of architecture backdoors and
taxonomise them, describing 12 distinct types. To gauge the difficulty of
detecting such backdoors, we conducted a user study, revealing that ML
developers can only identify suspicious components in common model definitions
as backdoors in 37% of cases, while they surprisingly preferred backdoored
models in 33% of cases. To contextualize these results, we find that language
models outperform humans at the detection of backdoors. Finally, we discuss
defenses against architectural backdoors, emphasizing the need for robust and
comprehensive strategies to safeguard the integrity of ML systems.
\\ ( https://arxiv.org/abs/2402.06957 ,  6820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06982 (*cross-listing*)
Date: Sat, 10 Feb 2024 16:13:09 GMT   (2768kb,D)

Title: Treatment-wise Glioblastoma Survival Inference with Multi-parametric
  Preoperative MRI
Authors: Xiaofeng Liu, Nadya Shusharina, Helen A Shih, C.-C. Jay Kuo, Georges
  El Fakhri, Jonghye Woo
Categories: cs.CV cs.AI physics.med-ph
Comments: SPIE Medical Imaging 2024: Computer-Aided Diagnosis
\\
  In this work, we aim to predict the survival time (ST) of glioblastoma (GBM)
patients undergoing different treatments based on preoperative magnetic
resonance (MR) scans. The personalized and precise treatment planning can be
achieved by comparing the ST of different treatments. It is well established
that both the current status of the patient (as represented by the MR scans)
and the choice of treatment are the cause of ST. While previous related
MR-based glioblastoma ST studies have focused only on the direct mapping of MR
scans to ST, they have not included the underlying causal relationship between
treatments and ST. To address this limitation, we propose a
treatment-conditioned regression model for glioblastoma ST that incorporates
treatment information in addition to MR scans. Our approach allows us to
effectively utilize the data from all of the treatments in a unified manner,
rather than having to train separate models for each of the treatments.
Furthermore, treatment can be effectively injected into each convolutional
layer through the adaptive instance normalization we employ. We evaluate our
framework on the BraTS20 ST prediction task. Three treatment options are
considered: Gross Total Resection (GTR), Subtotal Resection (STR), and no
resection. The evaluation results demonstrate the effectiveness of injecting
the treatment for estimating GBM survival.
\\ ( https://arxiv.org/abs/2402.06982 ,  2768kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06985 (*cross-listing*)
Date: Sat, 10 Feb 2024 16:23:12 GMT   (530kb,D)

Title: OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted
  Surgery
Authors: Long Bai, Guankun Wang, Jie Wang, Xiaoxiao Yang, Huxin Gao, Xin Liang,
  An Wang, Mobarakol Islam, Hongliang Ren
Categories: cs.CV cs.AI cs.RO
Comments: To appear in IEEE ICRA 2024
\\
  In the realm of automated robotic surgery and computer-assisted
interventions, understanding robotic surgical activities stands paramount.
Existing algorithms dedicated to surgical activity recognition predominantly
cater to pre-defined closed-set paradigms, ignoring the challenges of
real-world open-set scenarios. Such algorithms often falter in the presence of
test samples originating from classes unseen during training phases. To tackle
this problem, we introduce an innovative Open-Set Surgical Activity Recognition
(OSSAR) framework. Our solution leverages the hyperspherical reciprocal point
strategy to enhance the distinction between known and unknown classes in the
feature space. Additionally, we address the issue of over-confidence in the
closed set by refining model calibration, avoiding misclassification of unknown
classes as known ones. To support our assertions, we establish an open-set
surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we
also collect a novel dataset on endoscopic submucosal dissection for surgical
activity tasks. Extensive comparisons and ablation experiments on these
datasets demonstrate the significant outperformance of our method over existing
state-of-the-art approaches. Our proposed solution can effectively address the
challenges of real-world surgical scenarios. Our code is publicly accessible at
https://github.com/longbai1006/OSSAR.
\\ ( https://arxiv.org/abs/2402.06985 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06992 (*cross-listing*)
Date: Sat, 10 Feb 2024 16:54:28 GMT   (1477kb,D)

Title: A Rational Analysis of the Speech-to-Song Illusion
Authors: Raja Marjieh, Pol van Rijn, Ilia Sucholutsky, Harin Lee, Thomas L.
  Griffiths, Nori Jacoby
Categories: q-bio.NC cs.AI cs.CL stat.AP
Comments: 7 pages, 5 figures
\\
  The speech-to-song illusion is a robust psychological phenomenon whereby a
spoken sentence sounds increasingly more musical as it is repeated. Despite
decades of research, a complete formal account of this transformation is still
lacking, and some of its nuanced characteristics, namely, that certain phrases
appear to transform while others do not, is not well understood. Here we
provide a formal account of this phenomenon, by recasting it as a statistical
inference whereby a rational agent attempts to decide whether a sequence of
utterances is more likely to have been produced in a song or speech. Using this
approach and analyzing song and speech corpora, we further introduce a novel
prose-to-lyrics illusion that is purely text-based. In this illusion, simply
duplicating written sentences makes them appear more like song lyrics. We
provide robust evidence for this new illusion in both human participants and
large language models.
\\ ( https://arxiv.org/abs/2402.06992 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07031 (*cross-listing*)
Date: Sat, 10 Feb 2024 19:45:40 GMT   (4975kb,D)

Title: Instance-Level Safety-Aware Fidelity of Synthetic Data and Its
  Calibration
Authors: Chih-Hong Cheng, Paul St\"ockel, Xingyu Zhao
Categories: cs.SE cs.AI cs.LG
\\
  Modeling and calibrating the fidelity of synthetic data is paramount in
shaping the future of safe and reliable self-driving technology by offering a
cost-effective and scalable alternative to real-world data collection. We focus
on its role in safety-critical applications, introducing four types of
instance-level fidelity that go beyond mere visual input characteristics. The
aim is to align synthetic data with real-world safety issues. We suggest an
optimization method to refine the synthetic data generator, reducing fidelity
gaps identified by the DNN-based component. Our findings show this tuning
enhances the correlation between safety-critical errors in synthetic and real
images.
\\ ( https://arxiv.org/abs/2402.07031 ,  4975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07076 (*cross-listing*)
Date: Sun, 11 Feb 2024 01:03:41 GMT   (1791kb,D)

Title: Enhancing Multi-field B2B Cloud Solution Matching via Contrastive
  Pre-training
Authors: Haonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song,
  Zhenli Sheng
Categories: cs.IR cs.AI
\\
  Cloud solutions have gained significant popularity in the technology industry
as they offer a combination of services and tools to tackle specific problems.
However, despite their widespread use, the task of identifying appropriate
company customers for a specific target solution to the sales team of a
solution provider remains a complex business problem that existing matching
systems have yet to adequately address. In this work, we study the B2B solution
matching problem and identify two main challenges of this scenario: (1) the
modeling of complex multi-field features and (2) the limited, incomplete, and
sparse transaction data. To tackle these challenges, we propose a framework
CAMA, which is built with a hierarchical multi-field matching structure as its
backbone and supplemented by three data augmentation strategies and a
contrastive pre-training objective to compensate for the imperfections in the
available data. Through extensive experiments on a real-world dataset, we
demonstrate that CAMA outperforms several strong baseline matching models
significantly. Furthermore, we have deployed our matching framework on a system
of Huawei Cloud. Our observations indicate an improvement of about 30% compared
to the previous online model in terms of Conversion Rate (CVR), which
demonstrates its great business value.
\\ ( https://arxiv.org/abs/2402.07076 ,  1791kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07098 (*cross-listing*)
Date: Sun, 11 Feb 2024 03:54:44 GMT   (2639kb,D)

Title: Improving Pallet Detection Using Synthetic Data
Authors: Henry Gann, Josiah Bull, Trevor Gee, Mahla Nejati
Categories: cs.RO cs.AI
Comments: Australasian Conference on Robotics and Automation (ACRA 2023)
\\
  The use of synthetic data in machine learning saves a significant amount of
time when implementing an effective object detector. However, there is limited
research in this domain. This study aims to improve upon previously applied
implementations in the task of instance segmentation of pallets in a warehouse
environment. This study proposes using synthetically generated
domain-randomised data as well as data generated through Unity to achieve this.
This study achieved performance improvements on the stacked and racked pallet
categories by 69% and 50% mAP50, respectively when being evaluated on real
data. Additionally, it was found that there was a considerable impact on the
performance of a model when it was evaluated against images in a darker
environment, dropping as low as 3% mAP50 when being evaluated on images with an
80% brightness reduction. This study also created a two-stage detector that
used YOLOv8 and SAM, but this proved to have unstable performance. The use of
domain-randomised data proved to have negligible performance improvements when
compared to the Unity-generated data.
\\ ( https://arxiv.org/abs/2402.07098 ,  2639kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07118 (*cross-listing*)
Date: Sun, 11 Feb 2024 07:27:01 GMT   (23246kb,D)

Title: Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding
  Remote Smartphone-based Consultation
Authors: Dhruv Srikanth, Jayang Gurung, N Satya Deepika, Vineet Joshi, Pravin
  Vaddavalli, Soumya Jana
Categories: cs.HC cs.AI cs.LG eess.IV eess.SP
Comments: 4 pages, Submitted to IEEE EMBC 2024
\\
  Blindness and other eye diseases are a global health concern, particularly in
low- and middle-income countries like India. In this regard, during the
COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi
attachment for smartphone-based eye imaging gained in use. However, quality of
user-captured image often remained inadequate, requiring clinician vetting and
delays. In this backdrop, we propose an AI-based quality assessment system with
instant feedback mimicking clinicians' judgments and tested on patient-captured
images. Dividing the complex problem hierarchically, here we tackle a
nontrivial part, and demonstrate a proof of the concept.
\\ ( https://arxiv.org/abs/2402.07118 ,  23246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07127 (*cross-listing*)
Date: Sun, 11 Feb 2024 08:41:42 GMT   (371kb,D)

Title: Learning by Watching: A Review of Video-based Learning Approaches for
  Robot Manipulation
Authors: Chrisantus Eze and Christopher Crick
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  Robot learning of manipulation skills is hindered by the scarcity of diverse,
unbiased datasets. While curated datasets can help, challenges remain in
generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild"
video datasets have driven progress in computer vision through self-supervised
techniques. Translating this to robotics, recent works have explored learning
manipulation skills by passively watching abundant videos sourced online.
Showing promising results, such video-based learning paradigms provide scalable
supervision while reducing dataset bias. This survey reviews foundations such
as video feature representation learning techniques, object affordance
understanding, 3D hand/body modeling, and large-scale robot resources, as well
as emerging techniques for acquiring robot manipulation skills from
uncontrolled video demonstrations. We discuss how learning only from observing
large-scale human videos can enhance generalization and sample efficiency for
robotic manipulation. The survey summarizes video-based learning approaches,
analyses their benefits over standard datasets, survey metrics, and benchmarks,
and discusses open challenges and future directions in this nascent domain at
the intersection of computer vision, natural language processing, and robot
learning.
\\ ( https://arxiv.org/abs/2402.07127 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07148 (*cross-listing*)
Date: Sun, 11 Feb 2024 10:23:34 GMT   (40914kb,D)

Title: X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for
  Large Language Models with Applications in Protein Mechanics and Design
Authors: Eric L. Buehler, Markus J. Buehler
Categories: cond-mat.soft cond-mat.dis-nn cs.AI cs.CL cs.LG q-bio.QM
\\
  We report a mixture of expert strategy to create fine-tuned large language
models using a deep layer-wise token-level approach based on low-rank
adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose
a gating strategy that uses the hidden states to dynamically mix adapted
layers, allowing the resulting X-LoRA model to draw upon different capabilities
and create never-before-used deep layer-wise combinations of adaptations are
established to solve specific tasks. The design is inspired by the biological
principles of universality and diversity, where neural network building blocks
are reused in different hierarchical manifestations. Hence, the X-LoRA model
can be easily implemented for any existing large language model (LLM) without a
need for modifications of the underlying structure. We develop a tailored
X-LoRA model that offers scientific capabilities including forward/inverse
analysis tasks and enhanced reasoning capability, focused on biomaterial
analysis, protein mechanics and design. The impact of this work include access
to readily expandable, adaptable and changeable models with strong domain
knowledge and the capability to integrate across areas of knowledge. With the
X-LoRA model featuring experts in biology, mathematics, reasoning, bio-inspired
materials, mechanics and materials, chemistry, and protein mechanics we conduct
a series of physics-focused case studies. We examine knowledge recall, protein
mechanics forward/inverse tasks, protein design, and adversarial agentic
modeling including ontological knowledge graphs. The model is capable not only
of making quantitative predictions of nanomechanical properties of proteins,
but also reasons over the results and correctly predicts likely mechanisms that
explain distinct molecular behaviors.
\\ ( https://arxiv.org/abs/2402.07148 ,  40914kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07153 (*cross-listing*)
Date: Sun, 11 Feb 2024 10:50:20 GMT   (1393kb,D)

Title: Error Estimation for Physics-informed Neural Networks Approximating
  Semilinear Wave Equations
Authors: Beatrice Lorenz, Aras Bacho, Gitta Kutyniok
Categories: math.NA cs.AI cs.NA
MSC-class: 35L05, 68T07, 65M15, 35G50, 35A35
\\
  This paper provides rigorous error bounds for physics-informed neural
networks approximating the semilinear wave equation. We provide bounds for the
generalization and training error in terms of the width of the network's layers
and the number of training points for a tanh neural network with two hidden
layers. Our main result is a bound of the total error in the
$H^1([0,T];L^2(\Omega))$-norm in terms of the training error and the number of
training points, which can be made arbitrarily small under some assumptions. We
illustrate our theoretical bounds with numerical experiments.
\\ ( https://arxiv.org/abs/2402.07153 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07174 (*cross-listing*)
Date: Sun, 11 Feb 2024 12:03:01 GMT   (7150kb,D)

Title: EmoWear: Exploring Emotional Teasers for Voice Message Interaction on
  Smartwatches
Authors: Pengcheng An, Jiawen Zhu, Zibo Zhang, Yifei Yin, Qingyuan Ma, Che Yan,
  Linghao Du, Jian Zhao
Categories: cs.HC cs.AI
Comments: To appear at ACM CHI '24
DOI: 10.1145/3613904.3642101
\\
  Voice messages, by nature, prevent users from gauging the emotional tone
without fully diving into the audio content. This hinders the shared emotional
experience at the pre-retrieval stage. Research scarcely explored "Emotional
Teasers"-pre-retrieval cues offering a glimpse into an awaiting message's
emotional tone without disclosing its content. We introduce EmoWear, a
smartwatch voice messaging system enabling users to apply 30 animation teasers
on message bubbles to reflect emotions. EmoWear eases senders' choice by
prioritizing emotions based on semantic and acoustic processing. EmoWear was
evaluated in comparison with a mirroring system using color-coded message
bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced
emotional communication experience in both receiving and sending messages. The
animated teasers were considered intuitive and valued for diverse expressions.
Desirable interaction qualities and practical implications are distilled for
future design. We thereby contribute both a novel system and empirical
knowledge concerning emotional teasers for voice messaging.
\\ ( https://arxiv.org/abs/2402.07174 ,  7150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07229 (*cross-listing*)
Date: Sun, 11 Feb 2024 15:36:33 GMT   (1038kb,D)

Title: Successive Refinement in Large-Scale Computation: Advancing Model
  Inference Applications
Authors: Homa Esfahanizadeh, Alejandro Cohen, Shlomo Shamai (Shitz), Muriel
  Medard
Categories: cs.IT cs.AI math.IT
Comments: 13 pages, partially appeared in proceedings of IEEE Cloudnet 2022,
  submitted and under review for IEEE Transactions on Signal Processing
\\
  Modern computationally-intensive applications often operate under time
constraints, necessitating acceleration methods and distribution of
computational workloads across multiple entities. However, the outcome is
either achieved within the desired timeline or not, and in the latter case,
valuable resources are wasted. In this paper, we introduce solutions for
layered-resolution computation. These solutions allow lower-resolution results
to be obtained at an earlier stage than the final result. This innovation
notably enhances the deadline-based systems, as if a computational job is
terminated due to time constraints, an approximate version of the final result
can still be generated. Moreover, in certain operational regimes, a
high-resolution result might be unnecessary, because the low-resolution result
may already deviate significantly from the decision threshold, for example in
AI-based decision-making systems. Therefore, operators can decide whether
higher resolution is needed or not based on intermediate results, enabling
computations with adaptive resolution. We present our framework for two
critical and computationally demanding jobs: distributed matrix multiplication
(linear) and model inference in machine learning (nonlinear). Our theoretical
and empirical results demonstrate that the execution delay for the first
resolution is significantly shorter than that for the final resolution, while
maintaining overall complexity comparable to the conventional one-shot
approach. Our experiments further illustrate how the layering feature increases
the likelihood of meeting deadlines and enables adaptability and transparency
in massive, large-scale computations.
\\ ( https://arxiv.org/abs/2402.07229 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07244 (*cross-listing*)
Date: Sun, 11 Feb 2024 16:58:59 GMT   (2188kb,D)

Title: SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic
  Paradigm
Authors: Junhao Song, Yingfang Yuan, Wei Pang
Categories: cs.NE cs.AI
\\
  We propose a novel type of Artificial Immune System (AIS): Symbiotic
Artificial Immune Systems (SAIS), drawing inspiration from symbiotic
relationships in biology. SAIS parallels the three key stages (i.e., mutualism,
commensalism and parasitism) of population updating from the Symbiotic
Organisms Search (SOS) algorithm. This parallel approach effectively addresses
the challenges of large population size and enhances population diversity in
AIS, which traditional AIS and SOS struggle to resolve efficiently. We
conducted a series of experiments, which demonstrated that our SAIS achieved
comparable performance to the state-of-the-art approach SOS and outperformed
other popular AIS approaches and evolutionary algorithms across 26 benchmark
problems. Furthermore, we investigated the problem of parameter selection and
found that SAIS performs better in handling larger population sizes while
requiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired
and immune-inspired algorithm, paves the way for innovation in bio-inspired
computing with the symbiotic paradigm.
\\ ( https://arxiv.org/abs/2402.07244 ,  2188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07268 (*cross-listing*)
Date: Sun, 11 Feb 2024 18:23:54 GMT   (14918kb,D)

Title: Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker
  Identification with PathFormer
Authors: Zehao Dong, Qihang Zhao, Philip R.O. Payne, Michael A Province, Carlos
  Cruchaga, Muhan Zhang, Tianyu Zhao, Yixin Chen, Fuhai Li
Categories: q-bio.GN cs.AI cs.LG
DOI: 10.21203/rs.3.rs-3576068/v1
\\
  Biomarker identification is critical for precise disease diagnosis and
understanding disease pathogenesis in omics data analysis, like using fold
change and regression analysis. Graph neural networks (GNNs) have been the
dominant deep learning model for analyzing graph-structured data. However, we
found two major limitations of existing GNNs in omics data analysis, i.e.,
limited-prediction (diagnosis) accuracy and limited-reproducible biomarker
identification capacity across multiple datasets. The root of the challenges is
the unique graph structure of biological signaling pathways, which consists of
a large number of targets and intensive and complex signaling interactions
among these targets. To resolve these two challenges, in this study, we
presented a novel GNN model architecture, named PathFormer, which
systematically integrate signaling network, priori knowledge and omics data to
rank biomarkers and predict disease diagnosis. In the comparison results,
PathFormer outperformed existing GNN models significantly in terms of highly
accurate prediction capability ( 30% accuracy improvement in disease diagnosis
compared with existing GNN models) and high reproducibility of biomarker
ranking across different datasets. The improvement was confirmed using two
independent Alzheimer's Disease (AD) and cancer transcriptomic datasets. The
PathFormer model can be directly applied to other omics data analysis studies.
\\ ( https://arxiv.org/abs/2402.07268 ,  14918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07294 (*cross-listing*)
Date: Sun, 11 Feb 2024 20:15:44 GMT   (297kb,D)

Title: On the Effectiveness of Machine Learning-based Call Graph Pruning: An
  Empirical Study
Authors: Amir M. Mir, Mehdi Keshani, Sebastian Proksch
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: Accepted at the technical track of MSR'24
\\
  Static call graph (CG) construction often over-approximates call relations,
leading to sound, but imprecise results. Recent research has explored machine
learning (ML)-based CG pruning as a means to enhance precision by eliminating
false edges. However, current methods suffer from a limited evaluation dataset,
imbalanced training data, and reduced recall, which affects practical
downstream analyses. Prior results were also not compared with advanced static
CG construction techniques yet. This study tackles these issues. We introduce
the NYXCorpus, a dataset of real-world Java programs with high test coverage
and we collect traces from test executions and build a ground truth of dynamic
CGs. We leverage these CGs to explore conservative pruning strategies during
the training and inference of ML-based CG pruners. We conduct a comparative
analysis of static CGs generated using zero control flow analysis (0-CFA) and
those produced by a context-sensitive 1-CFA algorithm, evaluating both with and
without pruning. We find that CG pruning is a difficult task for real-world
Java projects and substantial improvements in the CG precision (+25%) meet
reduced recall (-9%). However, our experiments show promising results: even
when we favor recall over precision by using an F2 metric in our experiments,
we can show that pruned CGs have comparable quality to a context-sensitive
1-CFA analysis while being computationally less demanding. Resulting CGs are
much smaller (69%), and substantially faster (3.5x speed-up), with virtually
unchanged results in our downstream analysis.
\\ ( https://arxiv.org/abs/2402.07294 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07320 (*cross-listing*)
Date: Sun, 11 Feb 2024 22:53:21 GMT   (42945kb,D)

Title: Towards Explainable, Safe Autonomous Driving with Language Embeddings
  for Novelty Identification and Active Learning: Framework and Experimental
  Analysis with Real-World Data Sets
Authors: Ross Greer and Mohan Trivedi
Categories: cs.CV cs.AI cs.LG
\\
  This research explores the integration of language embeddings for active
learning in autonomous driving datasets, with a focus on novelty detection.
Novelty arises from unexpected scenarios that autonomous vehicles struggle to
navigate, necessitating higher-level reasoning abilities. Our proposed method
employs language-based representations to identify novel scenes, emphasizing
the dual purpose of safety takeover responses and active learning. The research
presents a clustering experiment using Contrastive Language-Image Pretrained
(CLIP) embeddings to organize datasets and detect novelties. We find that the
proposed algorithm effectively isolates novel scenes from a collection of
subsets derived from two real-world driving datasets, one vehicle-mounted and
one infrastructure-mounted. From the generated clusters, we further present
methods for generating textual explanations of elements which differentiate
scenes classified as novel from other scenes in the data pool, presenting
qualitative examples from the clustered results. Our results demonstrate the
effectiveness of language-driven embeddings in identifying novel elements and
generating explanations of data, and we further discuss potential applications
in safe takeovers, data curation, and multi-task active learning.
\\ ( https://arxiv.org/abs/2402.07320 ,  42945kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07342 (*cross-listing*)
Date: Mon, 12 Feb 2024 00:20:43 GMT   (1395kb,D)

Title: Imagining a Future of Designing with AI: Dynamic Grounding, Constructive
  Negotiation, and Sustainable Motivation
Authors: Priyan Vaithilingam, Ian Arawjo, Elena L. Glassman
Categories: cs.HC cs.AI
Comments: 12 pages, 4 figures
ACM-class: J.6; I.2.0; H.5.2
\\
  We ideate a future design workflow that involves AI technology. Drawing from
activity and communication theory, we attempt to isolate the new value large AI
models can provide design compared to past technologies. We arrive at three
affordances -- dynamic grounding, constructive negotiation, and sustainable
motivation -- that summarize latent qualities of natural language-enabled
foundation models that, if explicitly designed for, can support the process of
design. Through design fiction, we then imagine a future interface as a
diegetic prototype, the story of Squirrel Game, that demonstrates each of our
three affordances in a realistic usage scenario. Our design process,
terminology, and diagrams aim to contribute to future discussions about the
relative affordances of AI technology with regard to collaborating with human
designers.
\\ ( https://arxiv.org/abs/2402.07342 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07370 (*cross-listing*)
Date: Mon, 12 Feb 2024 02:01:53 GMT   (23023kb,D)

Title: SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked
  AutoEncoder
Authors: Jaeseong Lee, Junha Hyung, Sohyun Jeong, Jaegul Choo
Categories: cs.CV cs.AI
\\
  Face swapping has gained significant attention for its varied applications.
The majority of previous face swapping approaches have relied on the seesaw
game training scheme, which often leads to the instability of the model
training and results in undesired samples with blended identities due to the
target identity leakage problem. This paper introduces the Shape Agnostic
Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach
designed to enhance face swapping model training. Our training scheme addresses
the limitations of traditional training methods by circumventing the
conventional seesaw game and introducing clear ground truth through its
self-reconstruction training regime. It effectively mitigates identity leakage
by masking facial regions of the input images and utilizing learned
disentangled identity and non-identity features. Additionally, we tackle the
shape misalignment problem with new techniques including perforation confusion
and random mesh scaling, and establishes a new state-of-the-art, surpassing
other baseline methods, preserving both identity and non-identity attributes,
without sacrificing on either aspect.
\\ ( https://arxiv.org/abs/2402.07370 ,  23023kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07384 (*cross-listing*)
Date: Mon, 12 Feb 2024 03:04:42 GMT   (3822kb,D)

Title: Exploring Perceptual Limitation of Multimodal Large Language Models
Authors: Jiarui Zhang, Jinyi Hu, Mahyar Khayatkhoei, Filip Ilievski, Maosong
  Sun
Categories: cs.CV cs.AI cs.LG
Comments: 14 pages, 14 figures, 3 tables
\\
  Multimodal Large Language Models (MLLMs) have recently shown remarkable
perceptual capability in answering visual questions, however, little is known
about the limits of their perception. In particular, while prior works have
provided anecdotal evidence of MLLMs' sensitivity to object size, this
phenomenon and its underlying causes have not been explored comprehensively. In
this work, we quantitatively study the perception of small visual objects in
several state-of-the-art MLLMs and reveal a pervasive limitation in answering
questions about small objects in images. Next, we identify four independent
factors that can contribute to this limitation -- object quality, size,
distractors, and location -- and conduct controlled intervention studies to
measure the effect of each factor on MLLMs' perception. In particular, we find
that lower object quality and smaller object size can both independently reduce
MLLMs' ability to answer visual questions. More surprisingly, we find that the
location of the object in the image and the presence of visual distractors can
also significantly reduce MLLMs' question answering accuracy. Our study
provides a better understanding of the perceptual limitation of MLLMs and
contributes new evaluation protocols for analyzing the perception of future
MLLMs. To facilitate further investigations, we release our code and data.
\\ ( https://arxiv.org/abs/2402.07384 ,  3822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07393 (*cross-listing*)
Date: Mon, 12 Feb 2024 03:40:32 GMT   (4019kb,D)

Title: TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge
  AI with Compact Slow-Light Electro-Optic Modulator
Authors: Meng Zhang, Dennis Yin, Nicholas Gangi, Amir Begovi\'c, Alexander
  Chen, Zhaoran Rena Huang, Jiaqi Gu
Categories: cs.ET cs.AI cs.LG
Comments: 17 pages, 19 figures
\\
  Electronic-photonic computing systems offer immense potential in
energy-efficient artificial intelligence (AI) acceleration tasks due to the
superior computing speed and efficiency of optics, especially for real-time,
low-energy deep neural network (DNN) inference tasks on resource-restricted
edge platforms. However, current optical neural accelerators based on
foundry-available devices and conventional system architecture still encounter
a performance gap compared to highly customized electronic counterparts. To
bridge the performance gap due to lack of domain specialization, we present a
time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with
cross-layer device/circuit/architecture customization. At the device level, we
present foundry-compatible, customized photonic devices, including a slow-light
electro-optic modulator with experimental demonstration, optical splitters, and
phase shifters that significantly reduce the footprint and power in input
encoding and dot-product calculation. At the circuit level, partial products
are hierarchically accumulated via parallel photocurrent aggregation,
lightweight capacitive temporal integration, and sequential digital summation,
considerably relieving the analog-to-digital conversion bottleneck. We also
employ a multi-tile, multi-core architecture to maximize hardware sharing for
higher efficiency. Across diverse edge AI workloads, TeMPO delivers
digital-comparable task accuracy with superior quantization/noise tolerance. We
achieve a 368.6 TOPS peak performance, 22.3 TOPS/W energy efficiency, and 1.2
TOPS/mm$^2$ compute density, pushing the Pareto frontier in edge AI hardware.
This work signifies the power of cross-layer co-design and domain-specific
customization, paving the way for future electronic-photonic accelerators with
even greater performance and efficiency.
\\ ( https://arxiv.org/abs/2402.07393 ,  4019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07402 (*cross-listing*)
Date: Mon, 12 Feb 2024 04:34:19 GMT   (427kb,D)

Title: BDIQA: A New Dataset for Video Question Answering to Explore Cognitive
  Reasoning through Theory of Mind
Authors: Yuanyuan Mao, Xin Lin, Qin Ni, Liang He
Categories: cs.MM cs.AI
\\
  As a foundational component of cognitive intelligence, theory of mind (ToM)
can make AI more closely resemble human thought processes, thereby enhancing
their interaction and collaboration with human. In particular, it can
significantly improve a model's comprehension of videos in complex scenes.
However, current video question answer (VideoQA) datasets focus on studying
causal reasoning within events few of them genuinely incorporating human ToM.
Consequently, there is a lack of development in ToM reasoning tasks within the
area of VideoQA. This paper presents BDIQA, the first benchmark to explore the
cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA
is inspired by the cognitive development of children's ToM and addresses the
current deficiencies in machine ToM within datasets and tasks. Specifically, it
offers tasks at two difficulty levels, assessing Belief, Desire and Intention
(BDI) reasoning in both simple and complex scenarios. We conduct evaluations on
several mainstream methods of VideoQA and diagnose their capabilities with zero
shot, few shot and supervised learning. We find that the performance of
pre-trained models on cognitive reasoning tasks remains unsatisfactory. To
counter this challenge, we undertake thorough analysis and experimentation,
ultimately presenting two guidelines to enhance cognitive reasoning derived
from ablation analysis.
\\ ( https://arxiv.org/abs/2402.07402 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07408 (*cross-listing*)
Date: Mon, 12 Feb 2024 04:59:58 GMT   (2248kb,D)

Title: Large Language Models are Few-shot Generators: Proposing Hybrid Prompt
  Algorithm To Generate Webshell Escape Samples
Authors: Mingrui Ma, Lansheng Han, Chunjie Zhou
Categories: cs.CR cs.AI
Comments: 13 pages, 16 figures
\\
  The frequent occurrence of cyber-attacks has made webshell attacks and
defense gradually become a research hotspot in the field of network security.
However, the lack of publicly available benchmark datasets and the
over-reliance on manually defined rules for webshell escape sample generation
have slowed down the progress of research related to webshell escape sample
generation strategies and artificial intelligence-based webshell detection
algorithms. To address the drawbacks of weak webshell sample escape
capabilities, the lack of webshell datasets with complex malicious features,
and to promote the development of webshell detection technology, we propose the
Hybrid Prompt algorithm for webshell escape sample generation with the help of
large language models. As a prompt algorithm specifically developed for
webshell sample generation, the Hybrid Prompt algorithm not only combines
various prompt ideas including Chain of Thought, Tree of Thought, but also
incorporates various components such as webshell hierarchical module and
few-shot example to facilitate the LLM in learning and reasoning webshell
escape strategies. Experimental results show that the Hybrid Prompt algorithm
can work with multiple LLMs with excellent code reasoning ability to generate
high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on
VIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).
\\ ( https://arxiv.org/abs/2402.07408 ,  2248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07435 (*cross-listing*)
Date: Mon, 12 Feb 2024 06:29:57 GMT   (1299kb)

Title: Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and
  IV Models for GBP/USD and EUR/GBP Pairs
Authors: Narayan Tondapu
Categories: q-fin.ST cs.AI cs.LG
\\
  In this study, we examine the fluctuation in the value of the Great Britain
Pound (GBP). We focus particularly on its relationship with the United States
Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15,
2018, to June 15, 2023, we apply various mathematical models to assess their
effectiveness in predicting the 20-day variation in the pairs' daily returns.
Our analysis involves the implementation of Exponentially Weighted Moving
Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity
(GARCH) models, and Implied Volatility (IV) models. To evaluate their
performance, we compare the accuracy of their predictions using Root Mean
Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the
intricacies of GARCH models, examining their statistical characteristics when
applied to the provided dataset. Our findings suggest the existence of
asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for
the GBP/USD pair. Additionally, we observe that GARCH-type models better fit
the data when assuming residuals follow a standard t-distribution rather than a
standard normal distribution. Furthermore, we investigate the efficacy of
different forecasting techniques within GARCH-type models. Comparing rolling
window forecasts to expanding window forecasts, we find no definitive
superiority in either approach across the tested scenarios. Our experiments
reveal that for the GBP/USD pair, the most accurate volatility forecasts stem
from the utilization of GARCH models employing a rolling window methodology.
Conversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH
models and Ordinary Least Squares (OLS) models incorporating the annualized
implied volatility of the exchange rate as an independent variable.
\\ ( https://arxiv.org/abs/2402.07435 ,  1299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07437 (*cross-listing*)
Date: Mon, 12 Feb 2024 06:32:53 GMT   (23kb)

Title: Learning Optimal Tax Design in Nonatomic Congestion Games
Authors: Qiwen Cui, Maryam Fazel and Simon S. Du
Categories: cs.GT cs.AI cs.LG cs.MA
Comments: 19 pages
\\
  We study how to learn the optimal tax design to maximize the efficiency in
nonatomic congestion games. It is known that self-interested behavior among the
players can damage the system's efficiency. Tax mechanisms is a common method
to alleviate this issue and induce socially optimal behavior. In this work, we
take the initial step for learning the optimal tax that can minimize the social
cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe
the equilibrium state under the enforced tax. Existing algorithms are not
applicable due to the exponentially large tax function space, nonexistence of
the gradient, and nonconvexity of the objective. To tackle these challenges,
our algorithm leverages several novel components: (1) piece-wise linear tax to
approximate the optimal tax; (2) an extra linear term to guarantee a strongly
convex potential function; (3) efficient subroutine to find the ``boundary''
tax. The algorithm can find an $\epsilon$-optimal tax with $O(\beta
F^2/\epsilon)$ sample complexity, where $\beta$ is the smoothness of the cost
function and $F$ is the number of facilities.
\\ ( https://arxiv.org/abs/2402.07437 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07452 (*cross-listing*)
Date: Mon, 12 Feb 2024 07:19:00 GMT   (3528kb,D)

Title: TriAug: Out-of-Distribution Detection for Robust Classification of
  Imbalanced Breast Lesion in Ultrasound
Authors: Yinyu Ye, Shijing Chen, Dong Ni, Ruobing Huang
Categories: cs.CV cs.AI cs.LG
\\
  Different diseases, such as histological subtypes of breast lesions, have
severely varying incidence rates. Even trained with substantial amount of
in-distribution (ID) data, models often encounter out-of-distribution (OOD)
samples belonging to unseen classes in clinical reality. To address this, we
propose a novel framework built upon a long-tailed OOD detection task for
breast ultrasound images. It is equipped with a triplet state augmentation
(TriAug) which improves ID classification accuracy while maintaining a
promising OOD detection performance. Meanwhile, we designed a balanced sphere
loss to handle the class imbalanced problem.
\\ ( https://arxiv.org/abs/2402.07452 ,  3528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07540 (*cross-listing*)
Date: Mon, 12 Feb 2024 10:09:16 GMT   (549kb,D)

Title: PKG API: A Tool for Personal Knowledge Graph Management
Authors: Nolwenn Bernard and Ivica Kostric and Weronika {\L}ajewska and
  Krisztian Balog and Petra Galu\v{s}\v{c}\'akov\'a and Vinay Setty and Martin
  G. Skj{\ae}veland
Categories: cs.HC cs.AI cs.CL
\\
  Personal knowledge graphs (PKGs) offer individuals a way to store and
consolidate their fragmented personal data in a central place, improving
service personalization while maintaining full user control. Despite their
potential, practical PKG implementations with user-friendly interfaces remain
scarce. This work addresses this gap by proposing a complete solution to
represent, manage, and interface with PKGs. Our approach includes (1) a
user-facing PKG Client, enabling end-users to administer their personal data
easily via natural language statements, and (2) a service-oriented PKG API. To
tackle the complexity of representing these statements within a PKG, we present
an RDF-based PKG vocabulary that supports this, along with properties for
access rights and provenance.
\\ ( https://arxiv.org/abs/2402.07540 ,  549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07547 (*cross-listing*)
Date: Mon, 12 Feb 2024 10:19:17 GMT   (62kb,D)

Title: Ensuring trustworthy and ethical behaviour in intelligent logical agents
Authors: Stefania Costantini
Categories: cs.MA cs.AI cs.LO cs.SC
ACM-class: I.2.4
Journal-ref: Journal of Logic and Computation, Volume 32, Issue 2, March 2022,
  Pages 443-478
DOI: 10.1093/logcom/exab091
\\
  Autonomous Intelligent Agents are employed in many applications upon which
the life and welfare of living beings and vital social functions may depend.
Therefore, agents should be trustworthy. A priori certification techniques
(i.e., techniques applied prior to system's deployment) can be useful, but are
not sufficient for agents that evolve, and thus modify their epistemic and
belief state, and for open Multi-Agent Systems, where heterogeneous agents can
join or leave the system at any stage of its operation. In this paper, we
propose/refine/extend dynamic (runtime) logic-based self-checking techniques,
devised in order to be able to ensure agents' trustworthy and ethical
behaviour.
\\ ( https://arxiv.org/abs/2402.07547 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07619 (*cross-listing*)
Date: Mon, 12 Feb 2024 12:52:47 GMT   (1486kb,D)

Title: Developing a Multi-variate Prediction Model For COVID-19 From
  Crowd-sourced Respiratory Voice Data
Authors: Yuyang Yan, Wafaa Aljbawi, Sami O. Simons, Visara Urovi
Categories: cs.SD cs.AI eess.AS
Comments: arXiv admin note: text overlap with arXiv:2209.03727
\\
  COVID-19 has affected more than 223 countries worldwide and in the Post-COVID
Era, there is a pressing need for non-invasive, low-cost, and highly scalable
solutions to detect COVID-19. We develop a deep learning model to identify
COVID-19 from voice recording data. The novelty of this work is in the
development of deep learning models for COVID-19 identification from only voice
recordings. We use the Cambridge COVID-19 Sound database which contains 893
speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.
Voice features including Mel-spectrograms and Mel-frequency cepstral
coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice
data, we develop deep learning classification models to detect COVID-19 cases.
These models include Long Short-Term Memory (LSTM) and Convolutional Neural
Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power
to baseline machine learning models. HuBERT achieves the highest accuracy of
86\% and the highest AUC of 0.93. The results achieved with the proposed models
suggest promising results in COVID-19 diagnosis from voice recordings when
compared to the results obtained from the state-of-the-art.
\\ ( https://arxiv.org/abs/2402.07619 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07640 (*cross-listing*)
Date: Mon, 12 Feb 2024 13:27:22 GMT   (6788kb,D)

Title: Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image
  Data
Authors: Puneet Kumar, Sarthak Malik, Balasubramanian Raman, Xiaobai Li
Categories: cs.MM cs.AI
\\
  The ability to generate sentiment-controlled feedback in response to
multimodal inputs, comprising both text and images, addresses a critical gap in
human-computer interaction by enabling systems to provide empathetic, accurate,
and engaging responses. This capability has profound applications in
healthcare, marketing, and education. To this end, we construct a large-scale
Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a
controllable feedback synthesis system. The proposed system includes an
encoder, decoder, and controllability block for textual and visual inputs. It
extracts textual and visual features using a transformer and Faster R-CNN
networks and combines them to generate feedback. The CMFeed dataset encompasses
images, text, reactions to the post, human comments with relevance scores, and
reactions to the comments. The reactions to the post and comments are utilized
to train the proposed model to produce feedback with a particular (positive or
negative) sentiment. A sentiment classification accuracy of 77.23% has been
achieved, 18.82% higher than the accuracy without using the controllability.
Moreover, the system incorporates a similarity module for assessing feedback
relevance through rank-based metrics. It implements an interpretability
technique to analyze the contribution of textual and visual features during the
generation of uncontrolled and controlled feedback.
\\ ( https://arxiv.org/abs/2402.07640 ,  6788kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07680 (*cross-listing*)
Date: Mon, 12 Feb 2024 14:40:43 GMT   (5732kb)

Title: AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual
  Vision Transformer
Authors: Tanmoy Dam, Sanjay Bhargav Dharavath, Sameer Alam, Nimrod Lilith,
  Supriyo Chakraborty and Mir Feroskhan
Categories: cs.CV cs.AI cs.RO
Comments: This paper has been accepted for ICRA 2024, and copyright will
  automatically transfer to IEEE upon its availability on the IEEE portal
\\
  Combining LiDAR and camera data has shown potential in enhancing
short-distance object detection in autonomous driving systems. Yet, the fusion
encounters difficulties with extended distance detection due to the contrast
between LiDAR's sparse data and the dense resolution of cameras. Besides,
discrepancies in the two data representations further complicate fusion
methods. We introduce AYDIV, a novel framework integrating a tri-phase
alignment process specifically designed to enhance long-distance detection even
amidst data discrepancies. AYDIV consists of the Global Contextual Fusion
Alignment Transformer (GCFAT), which improves the extraction of camera features
and provides a deeper understanding of large-scale patterns; the Sparse Fused
Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera
details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial
data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an
improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset
with a performance improvement of 7.40% in AP value demonstrates its efficacy
in comparison to other existing fusion-based methods. Our code is publicly
available at https://github.com/sanjay-810/AYDIV2
\\ ( https://arxiv.org/abs/2402.07680 ,  5732kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07814 (*cross-listing*)
Date: Mon, 12 Feb 2024 17:18:51 GMT   (3166kb,D)

Title: PBADet: A One-Stage Anchor-Free Approach for Part-Body Association
Authors: Zhongpai Gao, Huayi Zhou, Abhishek Sharma, Meng Zheng, Benjamin
  Planche, Terrence Chen, Ziyan Wu
Categories: cs.CV cs.AI
Comments: Accepted by ICLR2024
\\
  The detection of human parts (e.g., hands, face) and their correct
association with individuals is an essential task, e.g., for ubiquitous
human-machine interfaces and action recognition. Traditional methods often
employ multi-stage processes, rely on cumbersome anchor-based systems, or do
not scale well to larger part sets. This paper presents PBADet, a novel
one-stage, anchor-free approach for part-body association detection. Building
upon the anchor-free object representation across multi-scale feature maps, we
introduce a singular part-to-body center offset that effectively encapsulates
the relationship between parts and their parent bodies. Our design is
inherently versatile and capable of managing multiple parts-to-body
associations without compromising on detection accuracy or robustness.
Comprehensive experiments on various datasets underscore the efficacy of our
approach, which not only outperforms existing state-of-the-art techniques but
also offers a more streamlined and efficient solution to the part-body
association challenge.
\\ ( https://arxiv.org/abs/2402.07814 ,  3166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07860 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:12:09 GMT   (287kb,D)

Title: On the Detection of Reviewer-Author Collusion Rings From Paper Bidding
Authors: Steven Jecmen, Nihar B. Shah, Fei Fang, Leman Akoglu
Categories: cs.SI cs.AI cs.GT
\\
  A major threat to the peer-review systems of computer science conferences is
the existence of "collusion rings" between reviewers. In such collusion rings,
reviewers who have also submitted their own papers to the conference work
together to manipulate the conference's paper assignment, with the aim of being
assigned to review each other's papers. The most straightforward way that
colluding reviewers can manipulate the paper assignment is by indicating their
interest in each other's papers through strategic paper bidding. One potential
approach to solve this important problem would be to detect the colluding
reviewers from their manipulated bids, after which the conference can take
appropriate action. While prior work has has developed effective techniques to
detect other kinds of fraud, no research has yet established that detecting
collusion rings is even possible. In this work, we tackle the question of
whether it is feasible to detect collusion rings from the paper bidding. To
answer this question, we conduct empirical analysis of two realistic conference
bidding datasets, including evaluations of existing algorithms for fraud
detection in other applications. We find that collusion rings can achieve
considerable success at manipulating the paper assignment while remaining
hidden from detection: for example, in one dataset, undetected colluders are
able to achieve assignment to up to 30% of the papers authored by other
colluders. In addition, when 10 colluders bid on all of each other's papers, no
detection algorithm outputs a group of reviewers with more than 31% overlap
with the true colluders. These results suggest that collusion cannot be
effectively detected from the bidding, demonstrating the need to develop more
complex detection algorithms that leverage additional metadata.
\\ ( https://arxiv.org/abs/2402.07860 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07862 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:14:43 GMT   (319kb,D)

Title: AI-Augmented Predictions: LLM Assistants Improve Human Forecasting
  Accuracy
Authors: Philipp Schoenegger, Peter S. Park, Ezra Karger, Philip E. Tetlock
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 18 pages (main text comprised of 15 pages, appendix comprised of
  three pages). 10 visualizations in the main text (four figures, six tables),
  three additional figures in the appendix
\\
  Large language models (LLMs) show impressive capabilities, matching and
sometimes exceeding human performance in many domains. This study explores the
potential of LLMs to augment judgement in forecasting tasks. We evaluated the
impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to
provide high-quality advice ('superforecasting'), and the other designed to be
overconfident and base-rate-neglecting. Participants (N = 991) had the option
to consult their assigned LLM assistant throughout the study, in contrast to a
control group that used a less advanced model (DaVinci-003) without direct
forecasting support. Our preregistered analyses reveal that LLM augmentation
significantly enhances forecasting accuracy by 23% across both types of
assistants, compared to the control group. This improvement occurs despite the
superforecasting assistant's higher accuracy in predictions, indicating the
augmentation's benefit is not solely due to model prediction accuracy.
Exploratory analyses showed a pronounced effect in one forecasting item,
without which we find that the superforecasting assistant increased accuracy by
43%, compared with 28% for the biased assistant. We further examine whether LLM
augmentation disproportionately benefits less skilled forecasters, degrades the
wisdom-of-the-crowd by reducing prediction diversity, or varies in
effectiveness with question difficulty. Our findings do not consistently
support these hypotheses. Our results suggest that access to an LLM assistant,
even a biased one, can be a helpful decision aid in cognitively demanding tasks
where the answer is not known at the time of interaction.
\\ ( https://arxiv.org/abs/2402.07862 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07865 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:21:14 GMT   (12984kb,D)

Title: Prismatic VLMs: Investigating the Design Space of Visually-Conditioned
  Language Models
Authors: Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang,
  Thomas Kollar, Dorsa Sadigh
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 22 pages, 11 figures. Training code and models:
  https://github.com/TRI-ML/prismatic-vlms. Evaluation code:
  https://github.com/TRI-ML/vlm-evaluation
\\
  Visually-conditioned language models (VLMs) have seen growing adoption in
applications such as visual dialogue, scene understanding, and robotic task
planning; adoption that has fueled a wealth of new models such as LLaVa,
InstructBLIP, and PaLI-3. Despite the volume of new releases, key design
decisions around image preprocessing, architecture, and optimization are
under-explored, making it challenging to understand what factors account for
model performance $-$ a challenge further complicated by the lack of objective,
consistent evaluations. To address these gaps, we first compile a suite of
standardized evaluations spanning visual question answering, object
localization from language, and targeted challenge sets that probe properties
such as hallucination; evaluations that provide calibrated, fine-grained
insight into a VLM's capabilities. Second, we rigorously investigate VLMs along
key design axes, including pretrained visual representations and quantifying
the tradeoffs of using base vs. instruct-tuned language models, amongst others.
We couple our analysis with three resource contributions: (1) a unified
framework for evaluating VLMs, (2) optimized, flexible code for VLM training,
and (3) checkpoints for all models, including a family of VLMs at the 7-13B
scale that strictly outperform InstructBLIP and LLaVa v1.5, the
state-of-the-art in open-source VLMs.
\\ ( https://arxiv.org/abs/2402.07865 ,  12984kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07895 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:57:06 GMT   (17721kb,D)

Title: Detection of Spider Mites on Labrador Beans through Machine Learning
  Approaches Using Custom Datasets
Authors: Violet Liu, Jason Chen, Ans Qureshi, Mahla Nejati
Categories: cs.CV cs.AI cs.RO
Comments: Australasian Conference on Robotics and Automation (ACRA 2023)
\\
  Amidst growing food production demands, early plant disease detection is
essential to safeguard crops; this study proposes a visual machine learning
approach for plant disease detection, harnessing RGB and NIR data collected in
real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN
dataset. A two-stage early plant disease detection model with YOLOv8 and a
sequential CNN was used to train on a dataset with partial labels, which showed
a 3.6% increase in mAP compared to a single-stage end-to-end segmentation
model. The sequential CNN model achieved 90.62% validation accuracy utilising
RGBN data. An average of 6.25% validation accuracy increase is found using RGBN
in classification compared to RGB using ResNet15 and the sequential CNN models.
Further research and dataset improvements are needed to meet food production
demands.
\\ ( https://arxiv.org/abs/2402.07895 ,  17721kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06690 (*cross-listing*)
Date: Thu, 8 Feb 2024 17:10:12 GMT   (3448kb,D)

Title: Neural Models for Source Code Synthesis and Completion
Authors: Mitodru Niyogi
Categories: cs.SE cs.CL cs.LG cs.PL
Comments: Master thesis submitted to University of Heidelberg, Germany on 30th
  July, 2021
\\
  Natural language (NL) to code suggestion systems assist developers in
Integrated Development Environments (IDEs) by translating NL utterances into
compilable code snippet. The current approaches mainly involve hard-coded,
rule-based systems based on semantic parsing. These systems make heavy use of
hand-crafted rules that map patterns in NL or elements in its syntax parse tree
to various query constructs and can only work on a limited subset of NL with a
restricted NL syntax. These systems are unable to extract semantic information
from the coding intents of the developer, and often fail to infer types, names,
and the context of the source code to get accurate system-level code
suggestions. In this master thesis, we present sequence-to-sequence deep
learning models and training paradigms to map NL to general-purpose programming
languages that can assist users with suggestions of source code snippets, given
a NL intent, and also extend auto-completion functionality of the source code
to users while they are writing source code. The developed architecture
incorporates contextual awareness into neural models which generate source code
tokens directly instead of generating parse trees/abstract meaning
representations from the source code and converting them back to source code.
The proposed pretraining strategy and the data augmentation techniques improve
the performance of the proposed architecture. The proposed architecture has
been found to exceed the performance of a neural semantic parser, TranX, based
on the BLEU-4 metric by 10.82%. Thereafter, a finer analysis for the parsable
code translations from the NL intent for CoNaLA challenge was introduced. The
proposed system is bidirectional as it can be also used to generate NL code
documentation given source code. Lastly, a RoBERTa masked language model for
Python was proposed to extend the developed system for code completion.
\\ ( https://arxiv.org/abs/2402.06690 ,  3448kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07085 (*cross-listing*)
Date: Sun, 11 Feb 2024 02:26:43 GMT   (11143kb,D)

Title: Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and
  Phoneme Duration for Multi-Speaker Speech Synthesis
Authors: Kenichi Fujita, Atsushi Ando, Yusuke Ijima
Categories: cs.SD cs.CL cs.LG eess.AS
Comments: 11 pages,9 figures, Accepted to IEICE TRANSACTIONS on Information and
  Systems
Journal-ref: IEICE TRANSACTIONS on Information and Systems 107.1 (2024): 93-104
DOI: 10.1587/transinf.2023EDP7039
\\
  This paper proposes a speech rhythm-based method for speaker embeddings to
model phoneme duration using a few utterances by the target speaker. Speech
rhythm is one of the essential factors among speaker characteristics, along
with acoustic features such as F0, for reproducing individual utterances in
speech synthesis. A novel feature of the proposed method is the rhythm-based
embeddings extracted from phonemes and their durations, which are known to be
related to speaking rhythm. They are extracted with a speaker identification
model similar to the conventional spectral feature-based one. We conducted
three experiments, speaker embeddings generation, speech synthesis with
generated embeddings, and embedding space analysis, to evaluate the
performance. The proposed method demonstrated a moderate speaker identification
performance (15.2% EER), even with only phonemes and their duration
information. The objective and subjective evaluation results demonstrated that
the proposed method can synthesize speech with speech rhythm closer to the
target speaker than the conventional method. We also visualized the embeddings
to evaluate the relationship between the distance of the embeddings and the
perceptual similarity. The visualization of the embedding space and the
relation analysis between the closeness indicated that the distribution of
embeddings reflects the subjective and objective similarity.
\\ ( https://arxiv.org/abs/2402.07085 ,  11143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07270 (*cross-listing*)
Date: Sun, 11 Feb 2024 18:26:18 GMT   (9040kb,D)

Title: Open-ended VQA benchmarking of Vision-Language models by exploiting
  Classification datasets and their semantic hierarchy
Authors: Simon Ging, Mar\'ia A. Bravo, Thomas Brox
Categories: cs.CV cs.CL cs.LG
Comments: Accepted as Spotlight Paper for ICLR 2024. The first two authors
  contributed equally to this work
\\
  The evaluation of text-generative vision-language models is a challenging yet
crucial endeavor. By addressing the limitations of existing Visual Question
Answering (VQA) benchmarks and proposing innovative evaluation methodologies,
our research seeks to advance our understanding of these models' capabilities.
We propose a novel VQA benchmark based on well-known visual classification
datasets which allows a granular evaluation of text-generative vision-language
models and their comparison with discriminative vision-language models. To
improve the assessment of coarse answers on fine-grained classification tasks,
we suggest using the semantic hierarchy of the label space to ask automatically
generated follow-up questions about the ground-truth category. Finally, we
compare traditional NLP and LLM-based metrics for the problem of evaluating
model predictions given ground-truth answers. We perform a human evaluation
study upon which we base our decision on the final metric. We apply our
benchmark to a suite of vision-language models and show a detailed comparison
of their abilities on object, action, and attribute classification. Our
contributions aim to lay the foundation for more precise and meaningful
assessments, facilitating targeted progress in the exciting field of
vision-language modeling.
\\ ( https://arxiv.org/abs/2402.07270 ,  9040kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07383 (*cross-listing*)
Date: Mon, 12 Feb 2024 02:58:10 GMT   (1468kb,D)

Title: Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like
Authors: Naoyuki Kanda, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker,
  Hemin Yang, Zirun Zhu, Min Tang, Canrun Li, Steven Tsai, Zhen Xiao, Yufei
  Xia, Jinzhu Li, Yanqing Liu, Sheng Zhao, Michael Zeng
Categories: eess.AS cs.CL cs.LG cs.SD
Comments: See https://aka.ms/elate/ for demo samples
\\
  Laughter is one of the most expressive and natural aspects of human speech,
conveying emotions, social cues, and humor. However, most text-to-speech (TTS)
systems lack the ability to produce realistic and appropriate laughter sounds,
limiting their applications and user experience. While there have been prior
works to generate natural laughter, they fell short in terms of controlling the
timing and variety of the laughter to be generated. In this work, we propose
ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker
based on a short audio prompt with precise control of laughter timing and
expression. Specifically, ELaTE works on the audio prompt to mimic the voice
characteristic, the text prompt to indicate the contents of the generated
speech, and the input to control the laughter expression, which can be either
the start and end times of laughter, or the additional audio prompt that
contains laughter to be mimicked. We develop our model based on the foundation
of conditional flow-matching-based zero-shot TTS, and fine-tune it with
frame-level representation from a laughter detector as additional conditioning.
With a simple scheme to mix small-scale laughter-conditioned data with
large-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS
model can be readily fine-tuned to generate natural laughter with precise
controllability, without losing any quality of the pre-trained zero-shot TTS
model. Through the evaluations, we show that ELaTE can generate laughing speech
with significantly higher quality and controllability compared to conventional
models. See https://aka.ms/elate/ for demo samples.
\\ ( https://arxiv.org/abs/2402.07383 ,  1468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07397 (*cross-listing*)
Date: Mon, 12 Feb 2024 04:10:09 GMT   (9767kb,D)

Title: Leveraging AI to Advance Science and Computing Education across Africa:
  Progress, Challenges, and Opportunities
Authors: George Boateng
Categories: cs.CY cs.CL cs.HC
Comments: Book chapter for upcoming book: "Artificial Intelligence in
  Education: The Intersection of Technology and Pedagogy"
\\
  Across the African continent, students grapple with various educational
challenges, including limited access to essential resources such as computers,
internet connectivity, reliable electricity, and a shortage of qualified
teachers. Despite these challenges, recent advances in AI such as BERT, and
GPT-4 have demonstrated their potential for advancing education. Yet, these AI
tools tend to be deployed and evaluated predominantly within the context of
Western educational settings, with limited attention directed towards the
unique needs and challenges faced by students in Africa. In this book chapter,
we describe our works developing and deploying AI in Education tools in Africa:
(1) SuaCode, an AI-powered app that enables Africans to learn to code using
their smartphones, (2) AutoGrad, an automated grading, and feedback tool for
graphical and interactive coding assignments, (3) a tool for code plagiarism
detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI
teaching assistant for coding courses, (5) Kwame for Science, a web-based AI
teaching assistant that provides instant answers to students' science questions
and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz
competition. We discuss challenges and potential opportunities to use AI to
advance science and computing education across Africa.
\\ ( https://arxiv.org/abs/2402.07397 ,  9767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07729 (*cross-listing*)
Date: Mon, 12 Feb 2024 15:41:22 GMT   (1917kb,D)

Title: AIR-Bench: Benchmarking Large Audio-Language Models via Generative
  Comprehension
Authors: Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou,
  Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, Jingren Zhou
Categories: eess.AS cs.CL cs.LG cs.SD
\\
  Recently, instruction-following audio-language models have received broad
attention for human-audio interaction. However, the absence of benchmarks
capable of evaluating audio-centric interaction capabilities has impeded
advancements in this field. Previous models primarily focus on assessing
different fundamental tasks, such as Automatic Speech Recognition (ASR), and
lack an assessment of the open-ended generative capabilities centered around
audio. Thus, it is challenging to track the progression in the Large
Audio-Language Models (LALMs) domain and to provide guidance for future
improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio
\textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed
to evaluate the ability of LALMs to understand various types of audio signals
(including human speech, natural sounds, and music), and furthermore, to
interact with humans in the textual format. AIR-Bench encompasses two
dimensions: \textit{foundation} and \textit{chat} benchmarks. The former
consists of 19 tasks with approximately 19k single-choice questions, intending
to inspect the basic single-task ability of LALMs. The latter one contains 2k
instances of open-ended question-and-answer data, directly assessing the
comprehension of the model on complex audio and its capacity to follow
instructions. Both benchmarks require the model to generate hypotheses
directly. We design a unified framework that leverages advanced language
models, such as GPT-4, to evaluate the scores of generated hypotheses given the
meta-information of the audio. Experimental results demonstrate a high level of
consistency between GPT-4-based evaluation and human evaluation. By revealing
the limitations of existing LALMs through evaluation results, AIR-Bench can
provide insights into the direction of future research.
\\ ( https://arxiv.org/abs/2402.07729 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07770 (*cross-listing*)
Date: Mon, 12 Feb 2024 16:32:37 GMT   (1880kb,D)

Title: Quantitative knowledge retrieval from large language models
Authors: David Selby, Kai Spriestersbach, Yuichiro Iwashita, Dennis Bappert,
  Archana Warrier, Sumantrak Mukherjee, Muhammad Nabeel Asim, Koichi Kise,
  Sebastian Vollmer
Categories: cs.IR cs.CL stat.AP
Comments: 13 pages plus supplementary materials
\\
  Large language models (LLMs) have been extensively studied for their
abilities to generate convincing natural language sequences, however their
utility for quantitative information retrieval is less well understood. In this
paper we explore the feasibility of LLMs as a mechanism for quantitative
knowledge retrieval to aid data analysis tasks such as elicitation of prior
distributions for Bayesian models and imputation of missing data. We present a
prompt engineering framework, treating an LLM as an interface to a latent space
of scientific literature, comparing responses in different contexts and domains
against more established approaches. Implications and challenges of using LLMs
as 'experts' are discussed.
\\ ( https://arxiv.org/abs/2402.07770 ,  1880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07844 (*cross-listing*)
Date: Mon, 12 Feb 2024 17:53:22 GMT   (2757kb,D)

Title: Mercury: An Efficiency Benchmark for LLM Code Synthesis
Authors: Mingzhe Du, Anh Tuan Luu, Bin Ji, See-Kiong Ng
Categories: cs.SE cs.CL
\\
  Despite advancements in evaluating Large Language Models (LLMs) for code
synthesis, benchmarks have predominantly focused on functional correctness,
overlooking the importance of code efficiency. We present Mercury, the first
benchmark designated for assessing the code efficiency of LLM code synthesis
tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty
levels alongside test case generators generating unlimited cases for
comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a
novel metric Beyond@K to measure normalized code efficiency based on historical
submissions, leading to a new evaluation indicator for code synthesis, which
encourages generating functionally correct and computationally efficient code,
mirroring the real-world software development standard. Our findings reveal
that while LLMs demonstrate the remarkable capability to generate functionally
correct code, there still exists a substantial gap in their efficiency output,
underscoring a new frontier for LLM research and development.
\\ ( https://arxiv.org/abs/2402.07844 ,  2757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07872 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:33:47 GMT   (10413kb,D)

Title: PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs
Authors: Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita
  Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan
  Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu,
  Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, Brian
  Ichter
Categories: cs.RO cs.CL cs.CV cs.LG
\\
  Vision language models (VLMs) have shown impressive capabilities across a
variety of tasks, from logical reasoning to visual understanding. This opens
the door to richer interaction with the world, for example robotic control.
However, VLMs produce only textual outputs, while robotic control and other
spatial tasks require outputting continuous coordinates, actions, or
trajectories. How can we enable VLMs to handle such settings without
fine-tuning on task-specific data?
  In this paper, we propose a novel visual prompting approach for VLMs that we
call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as
iterative visual question answering. In each iteration, the image is annotated
with a visual representation of proposals that the VLM can refer to (e.g.,
candidate robot actions, localizations, or trajectories). The VLM then selects
the best ones for the task. These proposals are iteratively refined, allowing
the VLM to eventually zero in on the best available answer. We investigate
PIVOT on real-world robotic navigation, real-world manipulation from images,
instruction following in simulation, and additional spatial inference tasks
such as localization. We find, perhaps surprisingly, that our approach enables
zero-shot control of robotic systems without any robot training data,
navigation in a variety of environments, and other capabilities. Although
current performance is far from perfect, our work highlights potentials and
limitations of this new regime and shows a promising approach for
Internet-Scale VLMs in robotic and spatial reasoning domains. Website:
pivot-prompt.github.io and HuggingFace:
https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.
\\ ( https://arxiv.org/abs/2402.07872 ,  10413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06633 (*cross-listing*)
Date: Fri, 19 Jan 2024 02:51:29 GMT   (3680kb,D)

Title: MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive
  and Dynamic Stock Investment Prediction
Authors: Hao Qian, Hongting Zhou, Qian Zhao, Hao Chen, Hongxiang Yao, Jingwei
  Wang, Ziqi Liu, Fei Yu, Zhiqiang Zhang, Jun Zhou
Categories: q-fin.ST cs.IR cs.LG
Comments: 9 pages, 3 figures, accepted by AAAI 2024
\\
  The stock market is a crucial component of the financial system, but
predicting the movement of stock prices is challenging due to the dynamic and
intricate relations arising from various aspects such as economic indicators,
financial reports, global news, and investor sentiment. Traditional sequential
methods and graph-based models have been applied in stock movement prediction,
but they have limitations in capturing the multifaceted and temporal influences
in stock price movements. To address these challenges, the Multi-relational
Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a
discrete dynamic graph to comprehensively capture multifaceted relations among
stocks and their evolution over time. The representation generated from the
graph offers a complete perspective on the interrelationships among stocks and
associated entities. Additionally, the power of the Transformer structure is
leveraged to encode the temporal evolution of multiplex relations, providing a
dynamic and effective approach to predicting stock investment. Further, our
proposed MDGNN framework achieves the best performance in public datasets
compared with state-of-the-art (SOTA) stock investment methods.
\\ ( https://arxiv.org/abs/2402.06633 ,  3680kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06635 (*cross-listing*)
Date: Sat, 20 Jan 2024 14:03:31 GMT   (682kb,D)

Title: Large (and Deep) Factor Models
Authors: Bryan Kelly, Boris Kuznetsov, Semyon Malamud, Teng Andrea Xu
Categories: q-fin.ST cs.CE cs.LG
\\
  We open up the black box behind Deep Learning for portfolio optimization and
prove that a sufficiently wide and arbitrarily deep neural network (DNN)
trained to maximize the Sharpe ratio of the Stochastic Discount Factor (SDF) is
equivalent to a large factor model (LFM): A linear factor pricing model that
uses many non-linear characteristics. The nature of these characteristics
depends on the architecture of the DNN in an explicit, tractable fashion. This
makes it possible to derive end-to-end trained DNN-based SDFs in closed form
for the first time. We evaluate LFMs empirically and show how various
architectural choices impact SDF performance. We document the virtue of depth
complexity: With enough data, the out-of-sample performance of DNN-SDF is
increasing in the NN depth, saturating at huge depths of around 100 hidden
layers.
\\ ( https://arxiv.org/abs/2402.06635 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06642 (*cross-listing*)
Date: Mon, 29 Jan 2024 07:11:13 GMT   (474kb,D)

Title: From GARCH to Neural Network for Volatility Forecast
Authors: Pengfei Zhao, Haoren Zhu, Wilfred Siu Hung NG, Dik Lun Lee
Categories: q-fin.ST cs.LG
Comments: Accepted by AAAI'24
\\
  Volatility, as a measure of uncertainty, plays a crucial role in numerous
financial activities such as risk management. The Econometrics and Machine
Learning communities have developed two distinct approaches for financial
volatility forecasting: the stochastic approach and the neural network (NN)
approach. Despite their individual strengths, these methodologies have
conventionally evolved in separate research trajectories with little
interaction between them. This study endeavors to bridge this gap by
establishing an equivalence relationship between models of the GARCH family and
their corresponding NN counterparts. With the equivalence relationship
established, we introduce an innovative approach, named GARCH-NN, for
constructing NN-based volatility models. It obtains the NN counterparts of
GARCH models and integrates them as components into an established NN
architecture, thereby seamlessly infusing volatility stylized facts (SFs)
inherent in the GARCH models into the neural network. We develop the GARCH-LSTM
model to showcase the power of the GARCH-NN approach. Experiment results
validate that amalgamating the NN counterparts of the GARCH family models into
established NN models leads to enhanced outcomes compared to employing the
stochastic and NN models in isolation.
\\ ( https://arxiv.org/abs/2402.06642 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06646 (*cross-listing*)
Date: Fri, 2 Feb 2024 01:34:33 GMT   (1975kb)

Title: Diffusion Model-based Probabilistic Downscaling for 180-year East Asian
  Climate Reconstruction
Authors: Fenghua Ling, Zeyu Lu, Jing-Jia Luo, Lei Bai, Swadhin K. Behera,
  Dachao Jin, Baoxiang Pan, Huidong Jiang and Toshio Yamagata
Categories: physics.ao-ph cs.LG physics.geo-ph
\\
  As our planet is entering into the "global boiling" era, understanding
regional climate change becomes imperative. Effective downscaling methods that
provide localized insights are crucial for this target. Traditional approaches,
including computationally-demanding regional dynamical models or statistical
downscaling frameworks, are often susceptible to the influence of downscaling
uncertainty. Here, we address these limitations by introducing a diffusion
probabilistic downscaling model (DPDM) into the meteorological field. This
model can efficiently transform data from 1{\deg} to 0.1{\deg} resolution.
Compared with deterministic downscaling schemes, it not only has more accurate
local details, but also can generate a large number of ensemble members based
on probability distribution sampling to evaluate the uncertainty of
downscaling. Additionally, we apply the model to generate a 180-year dataset of
monthly surface variables in East Asia, offering a more detailed perspective
for understanding local scale climate change over the past centuries.
\\ ( https://arxiv.org/abs/2402.06646 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06661 (*cross-listing*)
Date: Mon, 5 Feb 2024 22:34:24 GMT   (3257kb)

Title: Authentication and integrity of smartphone videos through multimedia
  container structure analysis
Authors: Carlos Quinto Huam\'an, Ana Lucila Sandoval Orozco, Luis Javier
  Garc\'ia Villalba
Categories: cs.CR cs.LG eess.IV
Journal-ref: Quinto Huam\'an, A. L. Sandoval Orozco, L. J. Garc\'ia Villalba:
  Authentication and Integrity of Smartphone Videos Through Multimedia
  Container Structure Analysis. Future Generation Computer Systems. Vol. 108,
  pp. 15-33, July 2020
DOI: 10.1016/j.future.2020.02.044
\\
  Nowadays, mobile devices have become the natural substitute for the digital
camera, as they capture everyday situations easily and quickly, encouraging
users to express themselves through images and videos. These videos can be
shared across different platforms exposing them to any kind of intentional
manipulation by criminals who are aware of the weaknesses of forensic
techniques to accuse an innocent person or exonerate a guilty person in a
judicial process. Commonly, manufacturers do not comply 100% with the
specifications of the standards for the creation of videos. Also, videos shared
on social networks, and instant messaging applications go through filtering and
compression processes to reduce their size, facilitate their transfer, and
optimize storage on their platforms. The omission of specifications and results
of transformations carried out by the platforms embed a features pattern in the
multimedia container of the videos. These patterns make it possible to
distinguish the brand of the device that generated the video, social network,
and instant messaging application that was used for the transfer. Research in
recent years has focused on the analysis of AVI containers and tiny video
datasets. This work presents a novel technique to detect possible attacks
against MP4, MOV, and 3GP format videos that affect their integrity and
authenticity. The method is based on the analysis of the structure of video
containers generated by mobile devices and their behavior when shared through
social networks, instant messaging applications, or manipulated by editing
programs. The objectives of the proposal are to verify the integrity of videos,
identify the source of acquisition and distinguish between original and
manipulated videos.
\\ ( https://arxiv.org/abs/2402.06661 ,  3257kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06674 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:23:01 GMT   (3109kb,D)

Title: Understanding Practical Membership Privacy of Deep Learning
Authors: Marlon Tobaben, Gauri Pradhan, Yuan He, Joonas J\"alk\"o, and Antti
  Honkela
Categories: cs.CR cs.LG
Comments: 21 pages, 8 figures
\\
  We apply a state-of-the-art membership inference attack (MIA) to
systematically test the practical privacy vulnerability of fine-tuning large
image classification models.We focus on understanding the properties of data
sets and samples that make them vulnerable to membership inference. In terms of
data set properties, we find a strong power law dependence between the number
of examples per class in the data and the MIA vulnerability, as measured by
true positive rate of the attack at a low false positive rate. For an
individual sample, large gradients at the end of training are strongly
correlated with MIA vulnerability.
\\ ( https://arxiv.org/abs/2402.06674 ,  3109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06678 (*cross-listing*)
Date: Wed, 7 Feb 2024 19:56:35 GMT   (1905kb,D)

Title: Can machine learning predict citizen-reported angler behavior?
Authors: Julia S. Schmid (1), Sean Simmons (2), Mark A. Lewis (1 and 3), Mark
  S. Poesch (1), Pouria Ramazi (4) ((1) University of Alberta, (2) Angler's
  Atlas, (3) University of Victoria, (4) Brock University)
Categories: physics.soc-ph cs.LG q-bio.QM
Comments: 36 pages, 10 figures, 4 tables (including supplementary information)
\\
  Prediction of angler behaviors, such as catch rates and angler pressure, is
essential to maintaining fish populations and ensuring angler satisfaction.
Angler behavior can partly be tracked by online platforms and mobile phone
applications that provide fishing activities reported by recreational anglers.
Moreover, angler behavior is known to be driven by local site attributes. Here,
the prediction of citizen-reported angler behavior was investigated by
machine-learning methods using auxiliary data on the environment,
socioeconomics, fisheries management objectives, and events at a freshwater
body. The goal was to determine whether auxiliary data alone could predict the
reported behavior. Different spatial and temporal extents and temporal
resolutions were considered. Accuracy scores averaged 88% for monthly
predictions at single water bodies and 86% for spatial predictions on a day in
a specific region across Canada. At other resolutions and scales, the models
only achieved low prediction accuracy of around 60%. The study represents a
first attempt at predicting angler behavior in time and space at a large scale
and establishes a foundation for potential future expansions in various
directions.
\\ ( https://arxiv.org/abs/2402.06678 ,  1905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06683 (*cross-listing*)
Date: Thu, 8 Feb 2024 07:22:39 GMT   (119kb,D)

Title: Sound Source Separation Using Latent Variational Block-Wise
  Disentanglement
Authors: Karim Helwani, Masahito Togami, Paris Smaragdis, Michael M. Goodwin
Categories: eess.AS cs.LG cs.SD
\\
  While neural network approaches have made significant strides in resolving
classical signal processing problems, it is often the case that hybrid
approaches that draw insight from both signal processing and neural networks
produce more complete solutions. In this paper, we present a hybrid classical
digital signal processing/deep neural network (DSP/DNN) approach to source
separation (SS) highlighting the theoretical link between variational
autoencoder and classical approaches to SS. We propose a system that transforms
the single channel under-determined SS task to an equivalent multichannel
over-determined SS problem in a properly designed latent space. The separation
task in the latent space is treated as finding a variational block-wise
disentangled representation of the mixture. We show empirically, that the
design choices and the variational formulation of the task at hand motivated by
the classical signal processing theoretical results lead to robustness to
unseen out-of-distribution data and reduction of the overfitting risk. To
address the resulting permutation issue we explicitly incorporate a novel
differentiable permutation loss function and augment the model with a memory
mechanism to keep track of the statistics of the individual sources.
\\ ( https://arxiv.org/abs/2402.06683 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06684 (*cross-listing*)
Date: Thu, 8 Feb 2024 11:00:51 GMT   (1713kb,D)

Title: Ai4Fapar: How artificial intelligence can help to forecast the seasonal
  earth observation signal
Authors: Filip Sabo, Martin Claverie, Michele Meroni, Arthur Hrast Essenfelder
Categories: physics.ao-ph cs.LG
Journal-ref: Proceedings of the 2023 conference on Big Data from Space, Soille,
  P., Lumnitz, S. and Albani, S. editor(s), Publications Office of the European
  Union, Luxembourg, 2023, JRC135493
DOI: 10.2760/46796
\\
  This paper investigated the potential of a multivariate Transformer model to
forecast the temporal trajectory of the Fraction of Absorbed Photosynthetically
Active Radiation (FAPAR) for short (1 month) and long horizon (more than 1
month) periods at the regional level in Europe and North Africa. The input data
covers the period from 2002 to 2022 and includes remote sensing and weather
data for modelling FAPAR predictions. The model was evaluated using a leave one
year out cross-validation and compared with the climatological benchmark.
Results show that the transformer model outperforms the benchmark model for one
month forecasting horizon, after which the climatological benchmark is better.
The RMSE values of the transformer model ranged from 0.02 to 0.04 FAPAR units
for the first 2 months of predictions. Overall, the tested Transformer model is
a valid method for FAPAR forecasting, especially when combined with weather
data and used for short-term predictions.
\\ ( https://arxiv.org/abs/2402.06684 ,  1713kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06689 (*cross-listing*)
Date: Thu, 8 Feb 2024 16:45:01 GMT   (1237kb,D)

Title: A Study on Stock Forecasting Using Deep Learning and Statistical Models
Authors: Himanshu Gupta and Aditya Jaiswal
Categories: q-fin.ST cs.LG
\\
  Predicting a fast and accurate model for stock price forecasting is been a
challenging task and this is an active area of research where it is yet to be
found which is the best way to forecast the stock price. Machine learning, deep
learning and statistical analysis techniques are used here to get the accurate
result so the investors can see the future trend and maximize the return of
investment in stock trading. This paper will review many deep learning
algorithms for stock price forecasting. We use a record of s&p 500 index data
for training and testing. The survey motive is to check various deep learning
and statistical model techniques for stock price forecasting that are Moving
Averages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL
CNN which are deep learning models. It will discuss various models, including
the Auto regression integration moving average model, the Recurrent neural
network model, the long short-term model which is the type of RNN used for long
dependency for data, the convolutional neural network model, and the full
convolutional neural network model, in terms of error calculation or percentage
of accuracy that how much it is accurate which measures by the function like
Root mean square error, mean absolute error, mean squared error. The model can
be used to predict the stock price by checking the low MAE value as lower the
MAE value the difference between the predicting and the actual value will be
less and this model will predict the price more accurately than other models.
\\ ( https://arxiv.org/abs/2402.06689 ,  1237kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06701 (*cross-listing*)
Date: Fri, 9 Feb 2024 08:31:46 GMT   (143kb,D)

Title: Privacy Profiles for Private Selection
Authors: Antti Koskela, Rachel Redberg, Yu-Xiang Wang
Categories: cs.CR cs.LG
\\
  Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are
fundamental primitives of differentially private (DP) data analysis with wide
applications to private query release, voting, and hyperparameter tuning.
Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made
significant progress in both generalizing private selection mechanisms and
tightening their privacy analysis using modern numerical privacy accounting
tools, e.g., R\'enyi DP. But R\'enyi DP is known to be lossy when
$(\epsilon,\delta)$-DP is ultimately needed, and there is a trend to close the
gap by directly handling privacy profiles, i.e., $\delta$ as a function of
$\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work
out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax
and PrivateTuning using the privacy profiles of the base algorithms they
corral. Numerically, our approach improves over the RDP-based accounting in all
regimes of interest and leads to substantial benefits in end-to-end private
learning experiments. Our analysis also suggests new distributions, e.g.,
binomial distribution for randomizing the number of rounds that leads to more
substantial improvements in certain regimes.
\\ ( https://arxiv.org/abs/2402.06701 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06706 (*cross-listing*)
Date: Fri, 9 Feb 2024 10:50:45 GMT   (782kb,D)

Title: CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with
  GNNs
Authors: Florian Gr\"otschla, Jo\"el Mathys, Robert Veres, Roger Wattenhofer
Categories: cs.CG cs.LG
Comments: Published as a conference paper at ICLR 2024
\\
  Graph Visualization, also known as Graph Drawing, aims to find geometric
embeddings of graphs that optimize certain criteria. Stress is a widely used
metric; stress is minimized when every pair of nodes is positioned at their
shortest path distance. However, stress optimization presents computational
challenges due to its inherent complexity and is usually solved using
heuristics in practice. We introduce a scalable Graph Neural Network (GNN)
based Graph Drawing framework with sub-quadratic runtime that can learn to
optimize stress. Inspired by classical stress optimization techniques and
force-directed layout algorithms, we create a coarsening hierarchy for the
input graph. Beginning at the coarsest level, we iteratively refine and
un-coarsen the layout, until we generate an embedding for the original graph.
To enhance information propagation within the network, we propose a novel
positional rewiring technique based on intermediate node positions. Our
empirical evaluation demonstrates that the framework achieves state-of-the-art
performance while remaining scalable.
\\ ( https://arxiv.org/abs/2402.06706 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06715 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:10:54 GMT   (277kb,D)

Title: Learning-augmented Online Algorithm for Two-level Ski-rental Problem
Authors: Keyuan Zhang, Zhongdong Liu, Nakjung Choi, Bo Ji
Categories: cs.DS cs.LG
Comments: Accepted by the Thirty-Eighth AAAI Conference on Artificial
  Intelligence (AAAI-24)
\\
  In this paper, we study the two-level ski-rental problem,where a user needs
to fulfill a sequence of demands for multiple items by choosing one of the
three payment options: paying for the on-demand usage (i.e., rent), buying
individual items (i.e., single purchase), and buying all the items (i.e., combo
purchase). Without knowing future demands, the user aims to minimize the total
cost (i.e., the sum of the rental, single purchase, and combo purchase costs)
by balancing the trade-off between the expensive upfront costs (for purchase)
and the potential future expenses (for rent). We first design a robust online
algorithm (RDTSR) that offers a worst-case performance guarantee. While online
algorithms are robust against the worst-case scenarios, they are often overly
cautious and thus suffer a poor average performance in typical scenarios. On
the other hand, Machine Learning (ML) algorithms typically show promising
average performance in various applications but lack worst-case performance
guarantees. To harness the benefits of both methods, we develop a
learning-augmented algorithm (LADTSR) by integrating ML predictions into the
robust online algorithm, which outperforms the robust online algorithm under
accurate predictions while ensuring worst-case performance guarantees even when
predictions are inaccurate. Finally, we conduct numerical experiments on both
synthetic and real-world trace data to corroborate the effectiveness of our
approach.
\\ ( https://arxiv.org/abs/2402.06715 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06730 (*cross-listing*)
Date: Fri, 9 Feb 2024 19:01:48 GMT   (66kb,D)

Title: A Scalable Algorithm for Individually Fair K-means Clustering
Authors: MohammadHossein Bateni and Vincent Cohen-Addad and Alessandro Epasto
  and Silvio Lattanzi
Categories: cs.DS cs.LG
Comments: 32 pages, 2 figures, to appear at the 27th International Conference
  on Artificial Intelligence and Statistics (AISTATS) 2024
\\
  We present a scalable algorithm for the individually fair ($p$,
$k$)-clustering problem introduced by Jung et al. and Mahabadi et al. Given $n$
points $P$ in a metric space, let $\delta(x)$ for $x\in P$ be the radius of the
smallest ball around $x$ containing at least $n / k$ points. A clustering is
then called individually fair if it has centers within distance $\delta(x)$ of
$x$ for each $x\in P$. While good approximation algorithms are known for this
problem no efficient practical algorithms with good theoretical guarantees have
been presented. We design the first fast local-search algorithm that runs in
~$O(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. Then we
show empirically that not only is our algorithm much faster than prior work,
but it also produces lower-cost solutions.
\\ ( https://arxiv.org/abs/2402.06730 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06783 (*cross-listing*)
Date: Fri, 9 Feb 2024 21:16:43 GMT   (779kb,D)

Title: Learn to Teach: Improve Sample Efficiency in Teacher-student Learning
  for Sim-to-Real Transfer
Authors: Feiyang Wu, Zhaoyuan Gu, Ye Zhao, Anqi Wu
Categories: cs.RO cs.LG
\\
  Simulation-to-reality (sim-to-real) transfer is a fundamental problem for
robot learning. Domain Randomization, which adds randomization during training,
is a powerful technique that effectively addresses the sim-to-real gap.
However, the noise in observations makes learning significantly harder.
Recently, studies have shown that employing a teacher-student learning paradigm
can accelerate training in randomized environments. Learned with privileged
information, a teacher agent can instruct the student agent to operate in noisy
environments. However, this approach is often not sample efficient as the
experience collected by the teacher is discarded completely when training the
student, wasting information revealed by the environment. In this work, we
extend the teacher-student learning paradigm by proposing a sample efficient
learning framework termed Learn to Teach (L2T) that recycles experience
collected by the teacher agent. We observe that the dynamics of the
environments for both agents remain unchanged, and the state space of the
teacher is coupled with the observation space of the student. We show that a
single-loop algorithm can train both the teacher and student agents under both
Reinforcement Learning and Inverse Reinforcement Learning contexts. We
implement variants of our methods, conduct experiments on the MuJoCo benchmark,
and apply our methods to the Cassie robot locomotion problem. Extensive
experiments show that our method achieves competitive performance while only
requiring environmental interaction with the teacher.
\\ ( https://arxiv.org/abs/2402.06783 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06787 (*cross-listing*)
Date: Fri, 9 Feb 2024 21:21:17 GMT   (153kb,D)

Title: ForestColl: Efficient Collective Communications on Heterogeneous Network
  Fabrics
Authors: Liangyu Zhao and Saeed Maleki and Ziyue Yang and Hossein Pourreza and
  Aashaka Shah and Changho Hwang and Arvind Krishnamurthy
Categories: cs.NI cs.DC cs.LG
Comments: arXiv admin note: text overlap with arXiv:2305.18461
\\
  As modern DNN models grow ever larger, collective communications between the
accelerators (allreduce, etc.) emerge as a significant performance bottleneck.
Designing efficient communication schedules is challenging given today's highly
diverse and heterogeneous network fabrics. In this paper, we present
ForestColl, a tool that generates efficient schedules for any network topology.
ForestColl constructs broadcast/aggregation spanning trees as the communication
schedule, achieving theoretically minimum network congestion. Its schedule
generation runs in strongly polynomial time and is highly scalable. ForestColl
supports any network fabrics, including both switching fabrics and direct
connections, as well as any network graph structure. We evaluated ForestColl on
multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules
achieved up to 52\% higher performance compared to the vendors' own optimized
communication libraries, RCCL and NCCL. ForestColl also outperforms other
state-of-the-art schedule generation techniques with both up to 61\% more
efficient generated schedules and orders of magnitude faster schedule
generation speed.
\\ ( https://arxiv.org/abs/2402.06787 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06806 (*cross-listing*)
Date: Fri, 9 Feb 2024 22:07:59 GMT   (2200kb,D)

Title: Towards Principled Assessment of Tabular Data Synthesis Algorithms
Authors: Yuntao Du, Ninghui Li
Categories: cs.CR cs.DB cs.LG
Comments: The code is available at: https://github.com/zealscott/SynMeter
\\
  Data synthesis has been advocated as an important approach for utilizing data
while protecting data privacy. A large number of tabular data synthesis
algorithms (which we call synthesizers) have been proposed. Some synthesizers
satisfy Differential Privacy, while others aim to provide privacy in a
heuristic fashion. A comprehensive understanding of the strengths and
weaknesses of these synthesizers remains elusive due to lacking principled
evaluation metrics and missing head-to-head comparisons of newly developed
synthesizers that take advantage of diffusion models and large language models
with state-of-the-art marginal-based synthesizers.
  In this paper, we present a principled and systematic evaluation framework
for assessing tabular data synthesis algorithms. Specifically, we examine and
critique existing evaluation metrics, and introduce a set of new metrics in
terms of fidelity, privacy, and utility to address their limitations. Based on
the proposed metrics, we also devise a unified objective for tuning, which can
consistently improve the quality of synthetic data for all methods. We
conducted extensive evaluations of 8 different types of synthesizers on 12
datasets and identified some interesting findings, which offer new directions
for privacy-preserving data synthesis.
\\ ( https://arxiv.org/abs/2402.06806 ,  2200kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06854 (*cross-listing*)
Date: Sat, 10 Feb 2024 01:30:24 GMT   (1671kb,D)

Title: Gyroscope-Assisted Motion Deblurring Network
Authors: Simin Luan, Cong Yang, Zeyd Boukhers, Xue Qin, Dongfeng Cheng, Wei
  Sui, Zhijun Li
Categories: cs.CV cs.GR cs.LG
\\
  Image research has shown substantial attention in deblurring networks in
recent years. Yet, their practical usage in real-world deblurring, especially
motion blur, remains limited due to the lack of pixel-aligned training triplets
(background, blurred image, and blur heat map) and restricted information
inherent in blurred images. This paper presents a simple yet efficient
framework to synthetic and restore motion blur images using Inertial
Measurement Unit (IMU) data. Notably, the framework includes a strategy for
training triplet generation, and a Gyroscope-Aided Motion Deblurring (GAMD)
network for blurred image restoration. The rationale is that through harnessing
IMU data, we can determine the transformation of the camera pose during the
image exposure phase, facilitating the deduction of the motion trajectory (aka.
blur trajectory) for each point inside the three-dimensional space. Thus, the
synthetic triplets using our strategy are inherently close to natural motion
blur, strictly pixel-aligned, and mass-producible. Through comprehensive
experiments, we demonstrate the advantages of the proposed framework: only
two-pixel errors between our synthetic and real-world blur trajectories, a
marked improvement (around 33.17%) of the state-of-the-art deblurring method
MIMO on Peak Signal-to-Noise Ratio (PSNR).
\\ ( https://arxiv.org/abs/2402.06854 ,  1671kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06884 (*cross-listing*)
Date: Sat, 10 Feb 2024 04:45:27 GMT   (4306kb,D)

Title: Low-Rank Approximation of Structural Redundancy for Self-Supervised
  Learning
Authors: Kang Du and Yu Xiang
Categories: stat.ML cs.LG
Comments: Accepted to the 3rd Conference on Causal Learning and Reasoning
  (CLeaR)
\\
  We study the data-generating mechanism for reconstructive SSL to shed light
on its effectiveness. With an infinite amount of labeled samples, we provide a
sufficient and necessary condition for perfect linear approximation. The
condition reveals a full-rank component that preserves the label classes of Y,
along with a redundant component. Motivated by the condition, we propose to
approximate the redundant component by a low-rank factorization and measure the
approximation quality by introducing a new quantity $\epsilon_s$, parameterized
by the rank of factorization s. We incorporate $\epsilon_s$ into the excess
risk analysis under both linear regression and ridge regression settings, where
the latter regularization approach is to handle scenarios when the dimension of
the learned features is much larger than the number of labeled samples n for
downstream tasks. We design three stylized experiments to compare SSL with
supervised learning under different settings to support our theoretical
findings.
\\ ( https://arxiv.org/abs/2402.06884 ,  4306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06885 (*cross-listing*)
Date: Sat, 10 Feb 2024 04:50:36 GMT   (1460kb,D)

Title: DimVis: Interpreting Visual Clusters in Dimensionality Reduction With
  Explainable Boosting Machine
Authors: Parisa Salmanian, Angelos Chatzimparmpas, Ali Can Karaca, Rafael M.
  Martins
Categories: cs.HC cs.LG stat.CO
Comments: This manuscript is currently under review
\\
  Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular
for transforming complex datasets into simpler visual representations. However,
while effective in uncovering general dataset patterns, these methods may
introduce artifacts and suffer from interpretability issues. This paper
presents DimVis, a visualization tool that employs supervised Explainable
Boosting Machine (EBM) models (trained on user-selected data of interest) as an
interpretation assistant for DR projections. Our tool facilitates
high-dimensional data analysis by providing an interpretation of feature
relevance in visual clusters through interactive exploration of UMAP
projections. Specifically, DimVis uses a contrastive EBM model that is trained
in real time to differentiate between the data inside and outside a cluster of
interest. Taking advantage of the inherent explainable nature of the EBM, we
then use this model to interpret the cluster itself via single and pairwise
feature comparisons in a ranking based on the EBM model's feature importance.
The applicability and effectiveness of DimVis are demonstrated through two use
cases involving real-world datasets, and we also discuss the limitations and
potential directions for future research.
\\ ( https://arxiv.org/abs/2402.06885 ,  1460kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06919 (*cross-listing*)
Date: Sat, 10 Feb 2024 09:53:21 GMT   (209kb,D)

Title: TREET: TRansfer Entropy Estimation via Transformer
Authors: Omer Luxembourg, Dor Tsur, Haim Permuter
Categories: cs.IT cs.LG math.IT
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Transfer entropy (TE) is a measurement in information theory that reveals the
directional flow of information between processes, providing valuable insights
for a wide range of real-world applications. This work proposes Transfer
Entropy Estimation via Transformers (TREET), a novel transformer-based approach
for estimating the TE for stationary processes. The proposed approach employs
Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism
for the task of neural estimation. We propose a detailed theoretical and
empirical study of the TREET, comparing it to existing methods. To increase its
applicability, we design an estimated TE optimization scheme that is motivated
by the functional representation lemma. Afterwards, we take advantage of the
joint optimization scheme to optimize the capacity of communication channels
with memory, which is a canonical optimization problem in information theory,
and show the memory capabilities of our estimator. Finally, we apply TREET to
real-world feature analysis. Our work, applied with state-of-the-art deep
learning methods, opens a new door for communication problems which are yet to
be solved.
\\ ( https://arxiv.org/abs/2402.06919 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06922 (*cross-listing*)
Date: Sat, 10 Feb 2024 11:07:24 GMT   (165kb,D)

Title: Whispers in the Machine: Confidentiality in LLM-integrated Systems
Authors: Jonathan Evertz, Merlin Chlosta, Lea Sch\"onherr, Thorsten Eisenhofer
Categories: cs.CR cs.LG
\\
  Large Language Models (LLMs) are increasingly integrated with external tools.
While these integrations can significantly improve the functionality of LLMs,
they also create a new attack surface where confidential data may be disclosed
between different components. Specifically, malicious tools can exploit
vulnerabilities in the LLM itself to manipulate the model and compromise the
data of other services, raising the question of how private data can be
protected in the context of LLM integrations.
  In this work, we provide a systematic way of evaluating confidentiality in
LLM-integrated systems. For this, we formalize a "secret key" game that can
capture the ability of a model to conceal private information. This enables us
to compare the vulnerability of a model against confidentiality attacks and
also the effectiveness of different defense strategies. In this framework, we
evaluate eight previously published attacks and four defenses. We find that
current defenses lack generalization across attack strategies. Building on this
analysis, we propose a method for robustness fine-tuning, inspired by
adversarial training. This approach is effective in lowering the success rate
of attackers and in improving the system's resilience against unknown attacks.
\\ ( https://arxiv.org/abs/2402.06922 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06923 (*cross-listing*)
Date: Sat, 10 Feb 2024 11:13:13 GMT   (1658kb,D)

Title: CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using
  Cochlear Cepstrum-based Masking for Speech Emotion Recognition
Authors: Ioannis Ziogas, Hessa Alfalahi, Ahsan H. Khandoker, Leontios J.
  Hadjileontiadis
Categories: eess.AS cs.LG cs.SD eess.SP stat.ML
Comments: 5 pages, 1 figure Accepted in IEEE ICASSP 2024 Workshops -
  Self-Supervision in Audio, Speech, and Beyond
\\
  Self-supervised learning (SSL) for automated speech recognition in terms of
its emotional content, can be heavily degraded by the presence noise, affecting
the efficiency of modeling the intricate temporal and spectral informative
structures of speech. Recently, SSL on large speech datasets, as well as new
audio-specific SSL proxy tasks, such as, temporal and frequency masking, have
emerged, yielding superior performance compared to classic approaches drawn
from the image augmentation domain. Our proposed contribution builds upon this
successful paradigm by introducing CochCeps-Augment, a novel bio-inspired
masking augmentation task for self-supervised contrastive learning of speech
representations. Specifically, we utilize the newly introduced bio-inspired
cochlear cepstrogram (CCGRAM) to derive noise robust representations of input
speech, that are then further refined through a self-supervised learning
scheme. The latter employs SimCLR to generate contrastive views of a CCGRAM
through masking of its angle and quefrency dimensions. Our experimental
approach and validations on the emotion recognition K-EmoCon benchmark dataset,
for the first time via a speaker-independent approach, features unsupervised
pre-training, linear probing and fine-tuning. Our results potentiate
CochCeps-Augment to serve as a standard tool in speech emotion recognition
analysis, showing the added value of incorporating bio-inspired masking as an
informative augmentation task for self-supervision. Our code for implementing
CochCeps-Augment will be made available at:
https://github.com/GiannisZgs/CochCepsAugment.
\\ ( https://arxiv.org/abs/2402.06923 ,  1658kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06940 (*cross-listing*)
Date: Sat, 10 Feb 2024 12:48:49 GMT   (617kb,D)

Title: Efficient Incremental Belief Updates Using Weighted Virtual Observations
Authors: David Tolpin
Categories: stat.ML cs.LG
\\
  We present an algorithmic solution to the problem of incremental belief
updating in the context of Monte Carlo inference in Bayesian statistical models
represented by probabilistic programs. Given a model and a sample-approximated
posterior, our solution constructs a set of weighted observations to condition
the model such that inference would result in the same posterior. This problem
arises e.g. in multi-level modelling, incremental inference, inference in
presence of privacy constraints. First, a set of virtual observations is
selected, then, observation weights are found through a computationally
efficient optimization procedure such that the reconstructed posterior
coincides with or closely approximates the original posterior. We implement and
apply the solution to a number of didactic examples and case studies, showing
efficiency and robustness of our approach. The provided reference
implementation is agnostic to the probabilistic programming language or the
inference algorithm, and can be applied to most mainstream probabilistic
programming environments.
\\ ( https://arxiv.org/abs/2402.06940 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06994 (*cross-listing*)
Date: Sat, 10 Feb 2024 17:02:53 GMT   (39kb)

Title: A Change Detection Reality Check
Authors: Isaac Corley, Caleb Robinson, Anthony Ortiz
Categories: cs.CV cs.LG
\\
  In recent years, there has been an explosion of proposed change detection
deep learning architectures in the remote sensing literature. These approaches
claim to offer state-of the-art performance on different standard benchmark
datasets. However, has the field truly made significant progress? In this paper
we perform experiments which conclude a simple U-Net segmentation baseline
without training tricks or complicated architectural changes is still a top
performer for the task of change detection.
\\ ( https://arxiv.org/abs/2402.06994 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07008 (*cross-listing*)
Date: Sat, 10 Feb 2024 18:03:15 GMT   (2020kb,D)

Title: An Optimization Framework for Processing and Transfer Learning for the
  Brain Tumor Segmentation
Authors: Tianyi Ren, Ethan Honey, Harshitha Rebala, Abhishek Sharma, Agamdeep
  Chopra, Mehmet Kurt
Categories: eess.IV cs.CV cs.LG
\\
  Tumor segmentation from multi-modal brain MRI images is a challenging task
due to the limited samples, high variance in shapes and uneven distribution of
tumor morphology. The performance of automated medical image segmentation has
been significant improvement by the recent advances in deep learning. However,
the model predictions have not yet reached the desired level for clinical use
in terms of accuracy and generalizability. In order to address the distinct
problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed
an optimization framework based on a 3D U-Net model for brain tumor
segmentation. This framework incorporates a range of techniques, including
various pre-processing and post-processing techniques, and transfer learning.
On the validation datasets, this multi-modality brain tumor segmentation
framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on
Challenges 1, 2, 3 respectively.
\\ ( https://arxiv.org/abs/2402.07008 ,  2020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07025 (*cross-listing*)
Date: Sat, 10 Feb 2024 19:12:31 GMT   (95kb,D)

Title: Generalization Error of Graph Neural Networks in the Mean-field Regime
Authors: Gholamali Aminian, Yixuan He, Gesine Reinert, {\L}ukasz Szpruch,
  Samuel N. Cohen
Categories: stat.ML cs.IT cs.LG math.IT
Comments: 43 pages
MSC-class: 62B10, 60F99, 49N80, 46N30
\\
  This work provides a theoretical framework for assessing the generalization
error of graph classification tasks via graph neural networks in the
over-parameterized regime, where the number of parameters surpasses the
quantity of data points. We explore two widely utilized types of graph neural
networks: graph convolutional neural networks and message passing graph neural
networks. Prior to this study, existing bounds on the generalization error in
the over-parametrized regime were uninformative, limiting our understanding of
over-parameterized network performance. Our novel approach involves deriving
upper bounds within the mean-field regime for evaluating the generalization
error of these graph neural networks. We establish upper bounds with a
convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These
upper bounds offer a theoretical assurance of the networks' performance on
unseen data in the challenging over-parameterized regime and overall contribute
to our understanding of their performance.
\\ ( https://arxiv.org/abs/2402.07025 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07027 (*cross-listing*)
Date: Sat, 10 Feb 2024 19:21:29 GMT   (23kb)

Title: Quantum Speedup for Spectral Approximation of Kronecker Products
Authors: Yeqi Gao, Zhao Song, Ruizhe Zhang
Categories: cs.DS cs.ET cs.LG math.QA
Comments: arXiv admin note: text overlap with arXiv:2311.03215 by other authors
\\
  Given its widespread application in machine learning and optimization, the
Kronecker product emerges as a pivotal linear algebra operator. However, its
computational demands render it an expensive operation, leading to heightened
costs in spectral approximation of it through traditional computation
algorithms. Existing classical methods for spectral approximation exhibit a
linear dependency on the matrix dimension denoted by $n$, considering matrices
of size $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times
d}$. Our work introduces an innovative approach to efficiently address the
spectral approximation of the Kronecker product $A_1 \otimes A_2$ using quantum
methods. By treating matrices as quantum states, our proposed method
significantly reduces the time complexity of spectral approximation to
$O_{d,\epsilon}(\sqrt{n})$.
\\ ( https://arxiv.org/abs/2402.07027 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07066 (*cross-listing*)
Date: Sat, 10 Feb 2024 23:42:05 GMT   (785kb,D)

Title: Differentially Private Range Queries with Correlated Input Perturbation
Authors: Prathamesh Dharangutte, Jie Gao, Ruobin Gong, Guanyang Wang
Categories: cs.CR cs.LG stat.ME
Comments: 26 pages, 8 figures
\\
  This work proposes a class of locally differentially private mechanisms for
linear queries, in particular range queries, that leverages correlated input
perturbation to simultaneously achieve unbiasedness, consistency, statistical
transparency, and control over utility requirements in terms of accuracy
targets expressed either in certain query margins or as implied by the
hierarchical database structure. The proposed Cascade Sampling algorithm
instantiates the mechanism exactly and efficiently. Our bounds show that we
obtain near-optimal utility while being empirically competitive against output
perturbation methods.
\\ ( https://arxiv.org/abs/2402.07066 ,  785kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07067 (*cross-listing*)
Date: Sat, 10 Feb 2024 23:49:49 GMT   (385kb,D)

Title: Learning the Expected Core of Strictly Convex Stochastic Cooperative
  Games
Authors: Nam Phuong Tran, The Anh Ta, Shuqing Shi, Debmalya Mandal, Yali Du,
  Long Tran-Thanh
Categories: cs.GT cs.LG
\\
  Reward allocation, also known as the credit assignment problem, has been an
important topic in economics, engineering, and machine learning. An important
concept in credit assignment is the core, which is the set of stable
allocations where no agent has the motivation to deviate from the grand
coalition. In this paper, we consider the stable allocation learning problem of
stochastic cooperative games, where the reward function is characterised as a
random variable with an unknown distribution. Given an oracle that returns a
stochastic reward for an enquired coalition each round, our goal is to learn
the expected core, that is, the set of allocations that are stable in
expectation. Within the class of strictly convex games, we present an algorithm
named \texttt{Common-Points-Picking} that returns a stable allocation given a
polynomial number of samples, with high probability. The analysis of our
algorithm involves the development of several new results in convex geometry,
including an extension of the separation hyperplane theorem for multiple convex
sets, and may be of independent interest.
\\ ( https://arxiv.org/abs/2402.07067 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07101 (*cross-listing*)
Date: Sun, 11 Feb 2024 04:26:35 GMT   (44kb)

Title: On the Complexity of First-Order Methods in Stochastic Bilevel
  Optimization
Authors: Jeongyeol Kwon, Dohyun Kwon, Hanbaek Lyu
Categories: math.OC cs.LG
\\
  We consider the problem of finding stationary points in Bilevel optimization
when the lower-level problem is unconstrained and strongly convex. The problem
has been extensively studied in recent years; the main technical challenge is
to keep track of lower-level solutions $y^*(x)$ in response to the changes in
the upper-level variables $x$. Subsequently, all existing approaches tie their
analyses to a genie algorithm that knows lower-level solutions and, therefore,
need not query any points far from them. We consider a dual question to such
approaches: suppose we have an oracle, which we call $y^*$-aware, that returns
an $O(\epsilon)$-estimate of the lower-level solution, in addition to
first-order gradient estimators {\it locally unbiased} within the
$\Theta(\epsilon)$-ball around $y^*(x)$. We study the complexity of finding
stationary points with such an $y^*$-aware oracle: we propose a simple
first-order method that converges to an $\epsilon$ stationary point using
$O(\epsilon^{-6}), O(\epsilon^{-4})$ access to first-order $y^*$-aware oracles.
Our upper bounds also apply to standard unbiased first-order oracles, improving
the best-known complexity of first-order methods by $O(\epsilon)$ with minimal
assumptions. We then provide the matching $\Omega(\epsilon^{-6})$,
$\Omega(\epsilon^{-4})$ lower bounds without and with an additional smoothness
assumption on $y^*$-aware oracles, respectively. Our results imply that any
approach that simulates an algorithm with an $y^*$-aware oracle must suffer the
same lower bounds.
\\ ( https://arxiv.org/abs/2402.07101 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07131 (*cross-listing*)
Date: Sun, 11 Feb 2024 08:59:02 GMT   (214kb,D)

Title: Resampling methods for Private Statistical Inference
Authors: Karan Chadha, John Duchi and Rohit Kuditipudi
Categories: stat.ML cs.CR cs.LG stat.ME
Comments: 35 pages
\\
  We consider the task of constructing confidence intervals with differential
privacy. We propose two private variants of the non-parametric bootstrap, which
privately compute the median of the results of multiple ``little'' bootstraps
run on partitions of the data and give asymptotic bounds on the coverage error
of the resulting confidence intervals. For a fixed differential privacy
parameter $\epsilon$, our methods enjoy the same error rates as that of the
non-private bootstrap to within logarithmic factors in the sample size $n$. We
empirically validate the performance of our methods for mean estimation, median
estimation, and logistic regression with both real and synthetic data. Our
methods achieve similar coverage accuracy to existing methods (and non-private
baselines) while providing notably shorter ($\gtrsim 10$ times) confidence
intervals than previous approaches.
\\ ( https://arxiv.org/abs/2402.07131 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07156 (*cross-listing*)
Date: Sun, 11 Feb 2024 11:02:25 GMT   (432kb,D)

Title: A hybrid iterative method based on MIONet for PDEs: Theory and numerical
  examples
Authors: Jun Hu and Pengzhan Jin
Categories: math.NA cs.LG cs.NA
\\
  We propose a hybrid iterative method based on MIONet for PDEs, which combines
the traditional numerical iterative solver and the recent powerful machine
learning method of neural operator, and further systematically analyze its
theoretical properties, including the convergence condition, the spectral
behavior, as well as the convergence rate, in terms of the errors of the
discretization and the model inference. We show the theoretical results for the
frequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel. We
give an upper bound of the convergence rate of the hybrid method w.r.t. the
model correction period, which indicates a minimum point to make the hybrid
iteration converge fastest. Several numerical examples including the hybrid
Richardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are
presented to verify our theoretical results, and also reflect an excellent
acceleration effect. As a meshless acceleration method, it is provided with
enormous potentials for practice applications.
\\ ( https://arxiv.org/abs/2402.07156 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07158 (*cross-listing*)
Date: Sun, 11 Feb 2024 11:03:08 GMT   (986kb,D)

Title: Effort and Size Estimation in Software Projects with Large Language
  Model-based Intelligent Interfaces
Authors: Claudionor N. Coelho Jr, Hanchen Xiong, Tushar Karayil, Sree Koratala,
  Rex Shang, Jacob Bollinger, Mohamed Shabar, Syam Nair
Categories: cs.SE cs.LG
\\
  The advancement of Large Language Models (LLM) has also resulted in an
equivalent proliferation in its applications. Software design, being one, has
gained tremendous benefits in using LLMs as an interface component that extends
fixed user stories. However, inclusion of LLM-based AI agents in software
design often poses unexpected challenges, especially in the estimation of
development efforts. Through the example of UI-based user stories, we provide a
comparison against traditional methods and propose a new way to enhance
specifications of natural language-based questions that allows for the
estimation of development effort by taking into account data sources,
interfaces and algorithms.
\\ ( https://arxiv.org/abs/2402.07158 ,  986kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07160 (*cross-listing*)
Date: Sun, 11 Feb 2024 11:11:39 GMT   (9375kb,D)

Title: PASOA- PArticle baSed Bayesian Optimal Adaptive design
Authors: Jacopo Iollo, Christophe Heinkel\'e, Pierre Alliez, Florence Forbes
Categories: stat.ML cs.LG stat.CO stat.ME
\\
  We propose a new procedure named PASOA, for Bayesian experimental design,
that performs sequential design optimization by simultaneously providing
accurate estimates of successive posterior distributions for parameter
inference. The sequential design process is carried out via a contrastive
estimation principle, using stochastic optimization and Sequential Monte Carlo
(SMC) samplers to maximise the Expected Information Gain (EIG). As larger
information gains are obtained for larger distances between successive
posterior distributions, this EIG objective may worsen classical SMC
performance. To handle this issue, tempering is proposed to have both a large
information gain and an accurate SMC sampling, that we show is crucial for
performance. This novel combination of stochastic optimization and tempered SMC
allows to jointly handle design optimization and parameter inference. We
provide a proof that the obtained optimal design estimators benefit from some
consistency property. Numerical experiments confirm the potential of the
approach, which outperforms other recent existing procedures.
\\ ( https://arxiv.org/abs/2402.07160 ,  9375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07189 (*cross-listing*)
Date: Sun, 11 Feb 2024 12:54:07 GMT   (79kb)

Title: Improving LSH via Tensorized Random Projection
Authors: Bhisham Dev Verma and Rameshwar Pratap
Categories: stat.ML cs.DS cs.LG
\\
  Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by
data scientists for approximate nearest neighbour search problems that have
been used extensively in many large scale data processing applications such as
near duplicate detection, nearest neighbour search, clustering, etc. In this
work, we aim to propose faster and space efficient locality sensitive hash
functions for Euclidean distance and cosine similarity for tensor data.
Typically, the naive approach for obtaining LSH for tensor data involves first
reshaping the tensor into vectors, followed by applying existing LSH methods
for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical
for higher order tensors because the size of the reshaped vector becomes
exponential in the order of the tensor. Consequently, the size of LSH
parameters increases exponentially. To address this problem, we suggest two
methods for LSH for Euclidean distance and cosine similarity, namely
$CP-E2LSH$, $TT-E2LSH$, and $CP-SRP$, $TT-SRP$, respectively, building on $CP$
and tensor train $(TT)$ decompositions techniques. Our approaches are space
efficient and can be efficiently applied to low rank $CP$ or $TT$ tensors. We
provide a rigorous theoretical analysis of our proposal on their correctness
and efficacy.
\\ ( https://arxiv.org/abs/2402.07189 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07200 (*cross-listing*)
Date: Sun, 11 Feb 2024 13:26:40 GMT   (8819kb,D)

Title: Outlier-Aware Training for Low-Bit Quantization of Structural
  Re-Parameterized Networks
Authors: Muqun Niu, Yuan Ren, Boyu Li and Chenchen Ding
Categories: cs.CV cs.LG cs.NE
Comments: 8 pages, 8 figures
\\
  Lightweight design of Convolutional Neural Networks (CNNs) requires co-design
efforts in the model architectures and compression techniques. As a novel
design paradigm that separates training and inference, a structural
re-parameterized (SR) network such as the representative RepVGG revitalizes the
simple VGG-like network with a high accuracy comparable to advanced and often
more complicated networks. However, the merging process in SR networks
introduces outliers into weights, making their distribution distinct from
conventional networks and thus heightening difficulties in quantization. To
address this, we propose an operator-level improvement for training called
Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of
limited bitwidths while upkeeping the inference accuracy, we develop a
clustering-based non-uniform quantization framework for Quantization-Aware
Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the
quantized performance of RepVGG is largely enhanced, particularly when the
bitwidth falls below 8.
\\ ( https://arxiv.org/abs/2402.07200 ,  8819kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07284 (*cross-listing*)
Date: Sun, 11 Feb 2024 19:16:01 GMT   (289kb,D)

Title: CLIPPER: Robust Data Association without an Initial Guess
Authors: Parker C. Lusk and Jonathan P. How
Categories: cs.RO cs.LG
Comments: 8 pages, 4 figures, accepted to RA-L
DOI: 10.1109/LRA.2024.3364842
\\
  Identifying correspondences in noisy data is a critically important step in
estimation processes. When an informative initial estimation guess is
available, the data association challenge is less acute; however, the existence
of a high-quality initial guess is rare in most contexts. We explore
graph-theoretic formulations for data association, which do not require an
initial estimation guess. Existing graph-theoretic approaches optimize over
unweighted graphs, discarding important consistency information encoded in
weighted edges, and frequently attempt to solve NP-hard problems exactly. In
contrast, we formulate a new optimization problem that fully leverages weighted
graphs and seeks the densest edge-weighted clique. We introduce two relaxations
to this problem: a convex semidefinite relaxation which we find to be
empirically tight, and a fast first-order algorithm called CLIPPER which
frequently arrives at nearly-optimal solutions in milliseconds. When evaluated
on point cloud registration problems, our algorithms remain robust up to at
least 95% outliers while existing algorithms begin breaking down at 80%
outliers. Code is available at https://mit-acl.github.io/clipper.
\\ ( https://arxiv.org/abs/2402.07284 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07307 (*cross-listing*)
Date: Sun, 11 Feb 2024 21:12:21 GMT   (839kb,D)

Title: Self-Consistent Conformal Prediction
Authors: Lars van der Laan, Ahmed M. Alaa
Categories: stat.ML cs.LG stat.ME
\\
  In decision-making guided by machine learning, decision-makers often take
identical actions in contexts with identical predicted outcomes. Conformal
prediction helps decision-makers quantify outcome uncertainty for actions,
allowing for better risk management. Inspired by this perspective, we introduce
self-consistent conformal prediction, which yields both Venn-Abers calibrated
predictions and conformal prediction intervals that are valid conditional on
actions prompted by model predictions. Our procedure can be applied post-hoc to
any black-box predictor to provide rigorous, action-specific decision-making
guarantees. Numerical experiments show our approach strikes a balance between
interval efficiency and conditional validity.
\\ ( https://arxiv.org/abs/2402.07307 ,  839kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07310 (*cross-listing*)
Date: Sun, 11 Feb 2024 21:16:42 GMT   (37361kb,D)

Title: BioNeRF: Biologically Plausible Neural Radiance Fields for View
  Synthesis
Authors: Leandro A. Passos, Douglas Rodrigues, Danilo Jodas, Kelton A. P.
  Costa, Jo\~ao Paulo Papa
Categories: cs.CV cs.LG
\\
  This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.
\\ ( https://arxiv.org/abs/2402.07310 ,  37361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07323 (*cross-listing*)
Date: Sun, 11 Feb 2024 22:59:19 GMT   (319kb,D)

Title: Lessons Learned from Mining the Hugging Face Repository
Authors: Joel Casta\~no, Silverio Mart\'inez-Fern\'andez, Xavier Franch
Categories: cs.SE cs.LG
Comments: Accepted at the 2024 ACM/IEEE 1st International Workshop on
  Methodological Issues with Empirical Studies in Software Engineering (WSESE)
\\
  The rapidly evolving fields of Machine Learning (ML) and Artificial
Intelligence have witnessed the emergence of platforms like Hugging Face (HF)
as central hubs for model development and sharing. This experience report
synthesizes insights from two comprehensive studies conducted on HF, focusing
on carbon emissions and the evolutionary and maintenance aspects of ML models.
Our objective is to provide a practical guide for future researchers embarking
on mining software repository studies within the HF ecosystem to enhance the
quality of these studies. We delve into the intricacies of the replication
package used in our studies, highlighting the pivotal tools and methodologies
that facilitated our analysis. Furthermore, we propose a nuanced stratified
sampling strategy tailored for the diverse HF Hub dataset, ensuring a
representative and comprehensive analytical approach. The report also
introduces preliminary guidelines, transitioning from repository mining to
cohort studies, to establish causality in repository mining studies,
particularly within the ML model of HF context. This transition is inspired by
existing frameworks and is adapted to suit the unique characteristics of the HF
model ecosystem. Our report serves as a guiding framework for researchers,
contributing to the responsible and sustainable advancement of ML, and
fostering a deeper understanding of the broader implications of ML models.
\\ ( https://arxiv.org/abs/2402.07323 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07334 (*cross-listing*)
Date: Sun, 11 Feb 2024 23:57:09 GMT   (71kb,D)

Title: Differentially Private Training of Mixture of Experts Models
Authors: Pierre Tholoniat, Huseyin A. Inan, Janardhan Kulkarni, Robert Sim
Categories: cs.CR cs.LG
Comments: Preliminary work presented as a poster at the 5th AAAI Workshop on
  Privacy-Preserving Artificial Intelligence (PPAI 24)
\\
  This position paper investigates the integration of Differential Privacy (DP)
in the training of Mixture of Experts (MoE) models within the field of natural
language processing. As Large Language Models (LLMs) scale to billions of
parameters, leveraging expansive datasets, they exhibit enhanced linguistic
capabilities and emergent abilities. However, this growth raises significant
computational and privacy concerns. Our study addresses these issues by
exploring the potential of MoE models, known for their computational
efficiency, and the application of DP, a standard for privacy preservation. We
present the first known attempt to train MoE models under the constraints of
DP, addressing the unique challenges posed by their architecture and the
complexities of DP integration. Our initial experimental studies demonstrate
that MoE models can be effectively trained with DP, achieving performance that
is competitive with their non-private counterparts. This initial study aims to
provide valuable insights and ignite further research in the domain of
privacy-preserving MoE models, softly laying the groundwork for prospective
developments in this evolving field.
\\ ( https://arxiv.org/abs/2402.07334 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07341 (*cross-listing*)
Date: Mon, 12 Feb 2024 00:19:09 GMT   (6995kb,D)

Title: Noise-Adaptive Confidence Sets for Linear Bandits and Application to
  Bayesian Optimization
Authors: Kwang-Sung Jun, Jungtaek Kim
Categories: stat.ML cs.LG
\\
  Adapting to a priori unknown noise level is a very important but challenging
problem in sequential decision-making as efficient exploration typically
requires knowledge of the noise level, which is often loosely specified. We
report significant progress in addressing this issue in linear bandits in two
respects. First, we propose a novel confidence set that is `semi-adaptive' to
the unknown sub-Gaussian parameter $\sigma_*^2$ in the sense that the
(normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$
where $d$ is the dimension and $\sigma_0^2$ is the specified sub-Gaussian
parameter (known) that can be much larger than $\sigma_*^2$. This is a
significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence
set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that
this leads to an improved regret bound in linear bandits. Second, for bounded
rewards, we propose a novel variance-adaptive confidence set that has a much
improved numerical performance upon prior art. We then apply this confidence
set to develop, as we claim, the first practical variance-adaptive linear
bandit algorithm via an optimistic approach, which is enabled by our novel
regret analysis technique. Both of our confidence sets rely critically on
`regret equality' from online learning. Our empirical evaluation in Bayesian
optimization tasks shows that our algorithms demonstrate better or comparable
performance compared to existing methods.
\\ ( https://arxiv.org/abs/2402.07341 ,  6995kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07355 (*cross-listing*)
Date: Mon, 12 Feb 2024 01:04:39 GMT   (63kb)

Title: Sampling from the Mean-Field Stationary Distribution
Authors: Yunbum Kook, Matthew S. Zhang, Sinho Chewi, Murat A. Erdogdu, Mufan
  (Bill) Li
Categories: math.ST cs.LG stat.ML stat.TH
\\
  We study the complexity of sampling from the stationary distribution of a
mean-field SDE, or equivalently, the complexity of minimizing a functional over
the space of probability measures which includes an interaction term.
  Our main insight is to decouple the two key aspects of this problem: (1)
approximation of the mean-field SDE via a finite-particle system, via
uniform-in-time propagation of chaos, and (2) sampling from the finite-particle
stationary distribution, via standard log-concave samplers. Our approach is
conceptually simpler and its flexibility allows for incorporating the
state-of-the-art for both algorithms and theory. This leads to improved
guarantees in numerous settings, including better guarantees for optimizing
certain two-layer neural networks in the mean-field regime.
\\ ( https://arxiv.org/abs/2402.07355 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07357 (*cross-listing*)
Date: Mon, 12 Feb 2024 01:17:09 GMT   (2867kb,D)

Title: Regression Trees for Fast and Adaptive Prediction Intervals
Authors: Luben M. C. Cabezas, Mateus P. Otto, Rafael Izbicki, Rafael B. Stern
Categories: stat.ML cs.LG
\\
  Predictive models make mistakes. Hence, there is a need to quantify the
uncertainty associated with their predictions. Conformal inference has emerged
as a powerful tool to create statistically valid prediction regions around
point predictions, but its naive application to regression problems yields
non-adaptive regions. New conformal scores, often relying upon quantile
regressors or conditional density estimators, aim to address this limitation.
Although they are useful for creating prediction bands, these scores are
detached from the original goal of quantifying the uncertainty around an
arbitrary predictive model. This paper presents a new, model-agnostic family of
methods to calibrate prediction intervals for regression problems with local
coverage guarantees. Our approach is based on pursuing the coarsest partition
of the feature space that approximates conditional coverage. We create this
partition by training regression trees and Random Forests on conformity scores.
Our proposal is versatile, as it applies to various conformity scores and
prediction settings and demonstrates superior scalability and performance
compared to established baselines in simulated and real-world datasets. We
provide a Python package locart that implements our methods using the standard
scikit-learn interface.
\\ ( https://arxiv.org/abs/2402.07357 ,  2867kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07363 (*cross-listing*)
Date: Mon, 12 Feb 2024 01:33:33 GMT   (72kb,D)

Title: Strategically-Robust Learning Algorithms for Bidding in First-Price
  Auctions
Authors: Rachitesh Kumar, Jon Schneider, Balasubramanian Sivan
Categories: cs.GT cs.LG
\\
  Learning to bid in repeated first-price auctions is a fundamental problem at
the interface of game theory and machine learning, which has seen a recent
surge in interest due to the transition of display advertising to first-price
auctions. In this work, we propose a novel concave formulation for
pure-strategy bidding in first-price auctions, and use it to analyze natural
Gradient-Ascent-based algorithms for this problem. Importantly, our analysis
goes beyond regret, which was the typical focus of past work, and also accounts
for the strategic backdrop of online-advertising markets where bidding
algorithms are deployed -- we prove that our algorithms cannot be exploited by
a strategic seller and that they incentivize truth-telling for the buyer.
  Concretely, we show that our algorithms achieve $O(\sqrt{T})$ regret when the
highest competing bids are generated adversarially, and show that no online
algorithm can do better. We further prove that the regret improves to $O(\log
T)$ when the competition is stationary and stochastic. Moving beyond regret, we
show that a strategic seller cannot exploit our algorithms to extract more
revenue on average than is possible under the optimal mechanism, i.e., the
seller cannot do much better than posting the monopoly reserve price in each
auction. Finally, we prove that our algorithm is also incentive compatible --
it is a (nearly) dominant strategy for the buyer to report her values
truthfully to the algorithm as a whole.
\\ ( https://arxiv.org/abs/2402.07363 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07365 (*cross-listing*)
Date: Mon, 12 Feb 2024 01:40:31 GMT   (2132kb,D)

Title: A Deep Learning Method for Optimal Investment Under Relative Performance
  Criteria Among Heterogeneous Agents
Authors: Mathieu Lauri\`ere, Ludovic Tangpi, Xuchen Zhou
Categories: math.OC cs.LG
\\
  Graphon games have been introduced to study games with many players who
interact through a weighted graph of interaction. By passing to the limit, a
game with a continuum of players is obtained, in which the interactions are
through a graphon. In this paper, we focus on a graphon game for optimal
investment under relative performance criteria, and we propose a deep learning
method. The method builds upon two key ingredients: first, a characterization
of Nash equilibria by forward-backward stochastic differential equations and,
second, recent advances of machine learning algorithms for stochastic
differential games. We provide numerical experiments on two different financial
models. In each model, we compare the effect of several graphons, which
correspond to different structures of interactions.
\\ ( https://arxiv.org/abs/2402.07365 ,  2132kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07388 (*cross-listing*)
Date: Mon, 12 Feb 2024 03:19:30 GMT   (37kb)

Title: The Limits of Assumption-free Tests for Algorithm Performance
Authors: Yuetian Luo and Rina Foygel Barber
Categories: math.ST cs.LG stat.ML stat.TH
\\
  Algorithm evaluation and comparison are fundamental questions in machine
learning and statistics -- how well does an algorithm perform at a given
modeling task, and which algorithm performs best? Many methods have been
developed to assess algorithm performance, often based around cross-validation
type strategies, retraining the algorithm of interest on different subsets of
the data and assessing its performance on the held-out data points. Despite the
broad use of such procedures, the theoretical properties of these methods are
not yet fully understood. In this work, we explore some fundamental limits for
answering these questions with limited amounts of data. In particular, we make
a distinction between two questions: how good is an algorithm $A$ at the
problem of learning from a training set of size $n$, versus, how good is a
particular fitted model produced by running $A$ on a particular training data
set of size $n$?
  Our main results prove that, for any test that treats the algorithm $A$ as a
``black box'' (i.e., we can only study the behavior of $A$ empirically), there
is a fundamental limit on our ability to carry out inference on the performance
of $A$, unless the number of available data points $N$ is many times larger
than the sample size $n$ of interest. (On the other hand, evaluating the
performance of a particular fitted model is easy as long as a holdout data set
is available -- that is, as long as $N-n$ is not too small.) We also ask
whether an assumption of algorithmic stability might be sufficient to
circumvent this hardness result. Surprisingly, we find that this is not the
case: the same hardness result still holds for the problem of evaluating the
performance of $A$, aside from a high-stability regime where fitted models are
essentially nonrandom. Finally, we also establish similar hardness results for
the problem of comparing multiple algorithms.
\\ ( https://arxiv.org/abs/2402.07388 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07391 (*cross-listing*)
Date: Mon, 12 Feb 2024 03:31:34 GMT   (100kb,D)

Title: Replicability is Asymptotically Free in Multi-armed Bandits
Authors: Junpei Komiyama, Shinji Ito, Yuichi Yoshida, Souta Koshino
Categories: stat.ML cs.LG
\\
  This work is motivated by the growing demand for reproducible machine
learning. We study the stochastic multi-armed bandit problem. In particular, we
consider a replicable algorithm that ensures, with high probability, that the
algorithm's sequence of actions is not affected by the randomness inherent in
the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times
more regret than nonreplicable algorithms, where $\rho$ is the level of
nonreplication. However, we demonstrate that this additional cost is
unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$,
provided that the magnitude of the confidence bounds is chosen carefully. We
introduce an explore-then-commit algorithm that draws arms uniformly before
committing to a single arm. Additionally, we examine a successive elimination
algorithm that eliminates suboptimal arms at the end of each phase. To ensure
the replicability of these algorithms, we incorporate randomness into their
decision-making processes. We extend the use of successive elimination to the
linear bandit problem as well. For the analysis of these algorithms, we propose
a principled approach to limiting the probability of nonreplication. This
approach elucidates the steps that existing research has implicitly followed.
Furthermore, we derive the first lower bound for the two-armed replicable
bandit problem, which implies the optimality of the proposed algorithms up to a
$\log\log T$ factor for the two-armed case.
\\ ( https://arxiv.org/abs/2402.07391 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07407 (*cross-listing*)
Date: Mon, 12 Feb 2024 04:59:34 GMT   (205kb,D)

Title: Conformal Predictive Programming for Chance Constrained Optimization
Authors: Yiqi Zhao, Xinyi Yu, Jyotirmoy V. Deshmukh, Lars Lindemann
Categories: eess.SY cs.LG cs.SY math.OC stat.ML
\\
  Motivated by the advances in conformal prediction (CP), we propose conformal
predictive programming (CPP), an approach to solve chance constrained
optimization (CCO) problems, i.e., optimization problems with nonlinear
constraint functions affected by arbitrary random parameters. CPP utilizes
samples from these random parameters along with the quantile lemma -- which is
central to CP -- to transform the CCO problem into a deterministic optimization
problem. We then present two tractable reformulations of CPP by: (1) writing
the quantile as a linear program along with its KKT conditions (CPP-KKT), and
(2) using mixed integer programming (CPP-MIP). CPP comes with marginal
probabilistic feasibility guarantees for the CCO problem that are conceptually
different from existing approaches, e.g., the sample approximation and the
scenario approach. While we explore algorithmic similarities with the sample
approximation approach, we emphasize that the strength of CPP is that it can
easily be extended to incorporate different variants of CP. To illustrate this,
we present robust conformal predictive programming to deal with distribution
shifts in the uncertain parameters of the CCO problem.
\\ ( https://arxiv.org/abs/2402.07407 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07410 (*cross-listing*)
Date: Mon, 12 Feb 2024 05:05:55 GMT   (2483kb,D)

Title: A Closer Look at the Robustness of Contrastive Language-Image
  Pre-Training (CLIP)
Authors: Weijie Tu, Weijian Deng, Tom Gedeon
Categories: cs.CV cs.LG
Comments: Accepted by NeurIPS 2023
\\
  Contrastive Language-Image Pre-training (CLIP) models have demonstrated
remarkable generalization capabilities across multiple challenging distribution
shifts. However, there is still much to be explored in terms of their
robustness to the variations of specific visual factors. In real-world
applications, reliable and safe systems must consider other safety objectives
beyond classification accuracy, such as predictive uncertainty. Yet, the
effectiveness of CLIP models on such safety-related features is less-explored.
Driven by the above, this work comprehensively investigates the safety
objectives of CLIP models, specifically focusing on three key properties:
resilience to visual factor variations, calibrated uncertainty estimations, and
the ability to detect anomalous inputs. To this end, we study 83 CLIP models
and 127 ImageNet classifiers. They are diverse in architecture, (pre)training
distribution and training strategies. We consider 10 visual factors (e.g.,
shape and pattern), 5 types of out-of-distribution data, and 8 natural and
challenging test conditions with different shift types, such as texture, style,
and perturbation shifts. Our study has unveiled several previously unknown
insights into CLIP models. For instance, they are not consistently more
calibrated than other ImageNet models, which contradicts existing findings.
Additionally, our analysis underscores the significance of training source
design by showcasing its profound influence on the three safety-related
properties. We believe our comprehensive study can shed light on and help guide
the development of more robust and reliable CLIP models.
\\ ( https://arxiv.org/abs/2402.07410 ,  2483kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07417 (*cross-listing*)
Date: Mon, 12 Feb 2024 05:44:10 GMT   (1236kb,D)

Title: An Empirical Study Into What Matters for Calibrating Vision-Language
  Models
Authors: Weijie Tu, Weijian Deng, Dylan Campbell, Stephen Gould, Tom Gedeon
Categories: cs.CV cs.LG
Comments: 12 pages, 8 figures, this version is not fully edited and will be
  updated soon
\\
  Vision--Language Models (VLMs) have emerged as the dominant approach for
zero-shot recognition, adept at handling diverse scenarios and significant
distribution changes. However, their deployment in risk-sensitive areas
requires a deeper understanding of their uncertainty estimation capabilities, a
relatively uncharted area. In this study, we explore the calibration properties
of VLMs across different architectures, datasets, and training strategies. In
particular, we analyze the uncertainty estimation performance of VLMs when
calibrated in one domain, label set or hierarchy level, and tested in a
different one. Our findings reveal that while VLMs are not inherently
calibrated for uncertainty, temperature scaling significantly and consistently
improves calibration, even across shifts in distribution and changes in label
set. Moreover, VLMs can be calibrated with a very small set of examples.
Through detailed experimentation, we highlight the potential applications and
importance of our insights, aiming for more reliable and effective use of VLMs
in critical, real-world scenarios.
\\ ( https://arxiv.org/abs/2402.07417 ,  1236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07440 (*cross-listing*)
Date: Mon, 12 Feb 2024 06:43:52 GMT   (3289kb,D)

Title: Benchmarking and Building Long-Context Retrieval Models with LoCo and
  M2-BERT
Authors: Jon Saad-Falcon, Daniel Y. Fu, Simran Arora, Neel Guha, Christopher
  R\'e
Categories: cs.IR cs.LG
\\
  Retrieval pipelines-an integral component of many machine learning
systems-perform poorly in domains where documents are long (e.g., 10K tokens or
more) and where identifying the relevant document requires synthesizing
information across the entire text. Developing long-context retrieval encoders
suitable for these domains raises three challenges: (1) how to evaluate
long-context retrieval performance, (2) how to pretrain a base language model
to represent both short contexts (corresponding to queries) and long contexts
(corresponding to documents), and (3) how to fine-tune this model for retrieval
under the batch size limitations imposed by GPU memory constraints. To address
these challenges, we first introduce LoCoV1, a novel 12 task benchmark
constructed to measure long-context retrieval where chunking is not possible or
not effective. We next present the M2-BERT retrieval encoder, an 80M parameter
state-space encoder model built from the Monarch Mixer architecture, capable of
scaling to documents up to 32K tokens long. We describe a pretraining data
mixture which allows this encoder to process both short and long context
sequences, and a finetuning approach that adapts this base model to retrieval
with only single-sample batches. Finally, we validate the M2-BERT retrieval
encoder on LoCoV1, finding that it outperforms competitive baselines by up to
23.3 points, despite containing 5-90x fewer parameters.
\\ ( https://arxiv.org/abs/2402.07440 ,  3289kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07445 (*cross-listing*)
Date: Mon, 12 Feb 2024 06:57:34 GMT   (496kb,D)

Title: Top-$K$ ranking with a monotone adversary
Authors: Yuepeng Yang, Antares Chen, Lorenzo Orecchia, Cong Ma
Categories: stat.ML cs.IT cs.LG math.IT math.ST stat.TH
\\
  In this paper, we address the top-$K$ ranking problem with a monotone
adversary. We consider the scenario where a comparison graph is randomly
generated and the adversary is allowed to add arbitrary edges. The
statistician's goal is then to accurately identify the top-$K$ preferred items
based on pairwise comparisons derived from this semi-random comparison graph.
The main contribution of this paper is to develop a weighted maximum likelihood
estimator (MLE) that achieves near-optimal sample complexity, up to a
$\log^2(n)$ factor, where n denotes the number of items under comparison. This
is made possible through a combination of analytical and algorithmic
innovations. On the analytical front, we provide a refined $\ell_\infty$ error
analysis of the weighted MLE that is more explicit and tighter than existing
analyses. It relates the $\ell_\infty$ error with the spectral properties of
the weighted comparison graph. Motivated by this, our algorithmic innovation
involves the development of an SDP-based approach to reweight the semi-random
graph and meet specified spectral properties. Additionally, we propose a
first-order method based on the Matrix Multiplicative Weight Update (MMWU)
framework. This method efficiently solves the resulting SDP in nearly-linear
time relative to the size of the semi-random comparison graph.
\\ ( https://arxiv.org/abs/2402.07445 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07472 (*cross-listing*)
Date: Mon, 12 Feb 2024 08:17:23 GMT   (1273kb,D)

Title: Cartesian atomic cluster expansion for machine learning interatomic
  potentials
Authors: Bingqing Cheng
Categories: physics.comp-ph cs.LG physics.chem-ph
\\
  Machine learning interatomic potentials are revolutionizing large-scale,
accurate atomistic modelling in material science and chemistry. These
potentials often use atomic cluster expansion or equivariant message passing
with spherical harmonics as basis functions. However, the dependence on
Clebsch-Gordan coefficients for maintaining rotational symmetry leads to
computational inefficiencies and redundancies. We propose an alternative: a
Cartesian-coordinates-based atomic density expansion. This approach provides a
complete description of atomic environments while maintaining interaction body
orders. Additionally, we integrate low-dimensional embeddings of various
chemical elements and inter-atomic message passing. The resulting potential,
named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy,
stability, and generalizability. We validate its performance in diverse
systems, including bulk water, small molecules, and 25-element high-entropy
alloys.
\\ ( https://arxiv.org/abs/2402.07472 ,  1273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07492 (*cross-listing*)
Date: Mon, 12 Feb 2024 09:00:27 GMT   (686kb,D)

Title: Convolutional Neural Networks for signal detection in real LIGO data
Authors: Ond\v{r}ej Zelenka, Bernd Br\"ugmann, and Frank Ohme
Categories: astro-ph.IM cs.LG gr-qc
Comments: 11 pages, 4 figures, 4 tables
\\
  Searching the data of gravitational-wave detectors for signals from compact
binary mergers is a computationally demanding task. Recently, machine learning
algorithms have been proposed to address current and future challenges.
However, the results of these publications often differ greatly due to
differing choices in the evaluation procedure. The Machine Learning
Gravitational-Wave Search Challenge was organized to resolve these issues and
produce a unified framework for machine-learning search evaluation. Six teams
submitted contributions, four of which are based on machine learning methods
and two are state-of-the-art production analyses. This paper describes the
submission from the team TPI FSU Jena and its updated variant. We also apply
our algorithm to real O3b data and recover the relevant events of the GWTC-3
catalog.
\\ ( https://arxiv.org/abs/2402.07492 ,  686kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07529 (*cross-listing*)
Date: Mon, 12 Feb 2024 09:57:47 GMT   (386kb,D)

Title: Accelerating Distributed Deep Learning using Lossless Homomorphic
  Compression
Authors: Haoyu Li, Yuchen Xu, Jiayi Chen, Rohit Dwivedula, Wenfei Wu, Keqiang
  He, Aditya Akella, Daehyeok Kim
Categories: cs.DC cs.DS cs.LG cs.NI
\\
  As deep neural networks (DNNs) grow in complexity and size, the resultant
increase in communication overhead during distributed training has become a
significant bottleneck, challenging the scalability of distributed training
systems. Existing solutions, while aiming to mitigate this bottleneck through
worker-level compression and in-network aggregation, fall short due to their
inability to efficiently reconcile the trade-offs between compression
effectiveness and computational overhead, hindering overall performance and
scalability. In this paper, we introduce a novel compression algorithm that
effectively merges worker-level compression with in-network aggregation. Our
solution is both homomorphic, allowing for efficient in-network aggregation
without CPU/GPU processing, and lossless, ensuring no compromise on training
accuracy. Theoretically optimal in compression and computational efficiency,
our approach is empirically validated across diverse DNN models such as NCF,
LSTM, VGG19, and BERT-base, showing up to a 6.33$\times$ improvement in
aggregation throughput and a 3.74$\times$ increase in per-iteration training
speed.
\\ ( https://arxiv.org/abs/2402.07529 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07549 (*cross-listing*)
Date: Mon, 12 Feb 2024 10:30:45 GMT   (1697kb,D)

Title: A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit
  for Analog In-Memory Computing
Authors: Elena Ferro, Athanasios Vasilopoulos, Corey Lammie, Manuel Le Gallo,
  Luca Benini, Irem Boybat, Abu Sebastian
Categories: cs.AR cs.ET cs.LG
Comments: Accepted at ISCAS2024
\\
  Analog In-Memory Computing (AIMC) is an emerging technology for fast and
energy-efficient Deep Learning (DL) inference. However, a certain amount of
digital post-processing is required to deal with circuit mismatches and
non-idealities associated with the memory devices. Efficient near-memory
digital logic is critical to retain the high area/energy efficiency and low
latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic
with limited parallelization capability and high latency. To overcome these
limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on
fixed-point arithmetic. It achieves competitive accuracy and higher computing
throughput than previous approaches while minimizing the area overhead.
Moreover, the NMPU supports standard DL activation steps, such as ReLU and
Batch Normalization. We perform a physical implementation of the NMPU design in
a 14 nm CMOS technology and provide detailed performance, power, and area
assessments. We validate the efficacy of the NMPU by using data from an AIMC
chip and demonstrate that a simulated AIMC system with the proposed NMPU
outperforms existing FP16-based implementations, providing 139$\times$
speed-up, 7.8$\times$ smaller area, and a competitive power consumption.
Additionally, our approach achieves an inference accuracy of 86.65 %/65.06 %,
with an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when
benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100
datasets, respectively.
\\ ( https://arxiv.org/abs/2402.07549 ,  1697kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07588 (*cross-listing*)
Date: Mon, 12 Feb 2024 11:41:42 GMT   (547kb,D)

Title: Rethinking Scaling Laws for Learning in Strategic Environments
Authors: Tinashe Handina and Eric Mazumdar
Categories: cs.GT cs.LG stat.ML
\\
  The deployment of ever-larger machine learning models reflects a growing
consensus that the more expressive the model$\unicode{x2013}$and the more data
one has access to$\unicode{x2013}$the more one can improve performance. As
models get deployed in a variety of real world scenarios, they inevitably face
strategic environments. In this work, we consider the natural question of how
the interplay of models and strategic interactions affects scaling laws. We
find that strategic interactions can break the conventional view of scaling
laws$\unicode{x2013}$meaning that performance does not necessarily
monotonically improve as models get larger and/ or more expressive (even with
infinite data). We show the implications of this phenomenon in several contexts
including strategic regression, strategic classification, and multi-agent
reinforcement learning through examples of strategic environments in
which$\unicode{x2013}$by simply restricting the expressivity of one's model or
policy class$\unicode{x2013}$one can achieve strictly better equilibrium
outcomes. Motivated by these examples, we then propose a new paradigm for
model-selection in games wherein an agent seeks to choose amongst different
model classes to use as their action set in a game.
\\ ( https://arxiv.org/abs/2402.07588 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07595 (*cross-listing*)
Date: Mon, 12 Feb 2024 11:49:08 GMT   (300kb,D)

Title: Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and
  DINOv2 in Medical Imaging Classification
Authors: Yuning Huang, Jingchen Zou, Lanxi Meng, Xin Yue, Qing Zhao, Jianqiang
  Li, Changwei Song, Gabriel Jimenez, Shaowu Li, Guanghui Fu
Categories: eess.IV cs.LG
\\
  Medical image analysis frequently encounters data scarcity challenges.
Transfer learning has been effective in addressing this issue while conserving
computational resources. The recent advent of foundational models like the
DINOv2, which uses the vision transformer architecture, has opened new
opportunities in the field and gathered significant interest. However, DINOv2's
performance on clinical data still needs to be verified. In this paper, we
performed a glioma grading task using three clinical modalities of brain MRI
data. We compared the performance of various pre-trained deep learning models,
including those based on ImageNet and DINOv2, in a transfer learning context.
Our focus was on understanding the impact of the freezing mechanism on
performance. We also validated our findings on three other types of public
datasets: chest radiography, fundus radiography, and dermoscopy. Our findings
indicate that in our clinical dataset, DINOv2's performance was not as strong
as ImageNet-based pre-trained models, whereas in public datasets, DINOv2
generally outperformed other models, especially when using the frozen
mechanism. Similar performance was observed with various sizes of DINOv2 models
across different tasks. In summary, DINOv2 is viable for medical image
classification tasks, particularly with data resembling natural images.
However, its effectiveness may vary with data that significantly differs from
natural images such as MRI. In addition, employing smaller versions of the
model can be adequate for medical task, offering resource-saving benefits. Our
codes are available at https://github.com/GuanghuiFU/medical_DINOv2_eval.
\\ ( https://arxiv.org/abs/2402.07595 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07613 (*cross-listing*)
Date: Mon, 12 Feb 2024 12:38:20 GMT   (58kb,D)

Title: Global optimality under amenable symmetry constraints
Authors: Peter Orbanz
Categories: math.ST cs.LG stat.ML stat.TH
\\
  We ask whether there exists a function or measure that (1) minimizes a given
convex functional or risk and (2) satisfies a symmetry property specified by an
amenable group of transformations. Examples of such symmetry properties are
invariance, equivariance, or quasi-invariance. Our results draw on old ideas of
Stein and Le Cam and on approximate group averages that appear in ergodic
theorems for amenable groups. A class of convex sets known as orbitopes in
convex analysis emerges as crucial, and we establish properties of such
orbitopes in nonparametric settings. We also show how a simple device called a
cocycle can be used to reduce different forms of symmetry to a single problem.
As applications, we obtain results on invariant kernel mean embeddings and a
Monge-Kantorovich theorem on optimality of transport plans under symmetry
constraints. We also explain connections to the Hunt-Stein theorem on invariant
tests.
\\ ( https://arxiv.org/abs/2402.07613 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07621 (*cross-listing*)
Date: Mon, 12 Feb 2024 12:55:35 GMT   (194kb,D)

Title: Correctness Verification of Neural Networks Approximating Differential
  Equations
Authors: Petros Ellinas, Rahul Nellikath, Ignasi Ventura, Jochen Stiasny,
  Spyros Chatzivasileiadis
Categories: eess.SY cs.LG cs.SY
\\
  Verification of Neural Networks (NNs) that approximate the solution of
Partial Differential Equations (PDEs) is a major milestone towards enhancing
their trustworthiness and accelerating their deployment, especially for
safety-critical systems. If successful, such NNs can become integral parts of
simulation software tools which can accelerate the simulation of complex
dynamic systems more than 100 times. However, the verification of these
functions poses major challenges; it is not straightforward how to efficiently
bound them or how to represent the derivative of the NN. This work addresses
both these problems. First, we define the NN derivative as a finite difference
approximation. Then, we formulate the PDE residual bounding problem alongside
the Initial Value Problem's error propagation. Finally, for the first time, we
tackle the problem of bounding an NN function without a priori knowledge of the
output domain. For this, we build a parallel branching algorithm that combines
the incomplete CROWN solver and Gradient Attack for termination and domain
rejection conditions. We demonstrate the strengths and weaknesses of the
proposed framework, and we suggest further work to enhance its efficiency.
\\ ( https://arxiv.org/abs/2402.07621 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07626 (*cross-listing*)
Date: Mon, 12 Feb 2024 13:11:11 GMT   (6195kb,D)

Title: Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution
  for Weak Features
Authors: Rodrigo Veiga, Anastasia Remizova, Nicolas Macris
Categories: stat.ML cond-mat.dis-nn cs.LG
Comments: 34 pages, 7 figures
\\
  We investigate the test risk of continuous-time stochastic gradient flow
dynamics in learning theory. Using a path integral formulation we provide, in
the regime of a small learning rate, a general formula for computing the
difference between test risk curves of pure gradient and stochastic gradient
flows. We apply the general theory to a simple model of weak features, which
displays the double descent phenomenon, and explicitly compute the corrections
brought about by the added stochastic term in the dynamics, as a function of
time and model parameters. The analytical results are compared to simulations
of discrete-time stochastic gradient descent and show good agreement.
\\ ( https://arxiv.org/abs/2402.07626 ,  6195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07642 (*cross-listing*)
Date: Mon, 12 Feb 2024 13:30:34 GMT   (33188kb,D)

Title: A Flow-based Credibility Metric for Safety-critical Pedestrian Detection
Authors: Maria Lyssenko, Christoph Gladisch, Christian Heinzemann, Matthias
  Woehrle, Rudolph Triebel
Categories: cs.CV cs.LG
\\
  Safety is of utmost importance for perception in automated driving (AD).
However, a prime safety concern in state-of-the art object detection is that
standard evaluation schemes utilize safety-agnostic metrics to argue sufficient
detection performance. Hence, it is imperative to leverage supplementary domain
knowledge to accentuate safety-critical misdetections during evaluation tasks.
To tackle the underspecification, this paper introduces a novel credibility
metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow
relies on a complementary optical flow signal from image sequences and enhances
the analyses of safety-critical misdetections without requiring additional
labels. We implement and evaluate c-flow with a state-of-the-art pedestrian
detector on a large AD dataset. Our analysis demonstrates that c-flow allows
developers to identify safety-critical misdetections.
\\ ( https://arxiv.org/abs/2402.07642 ,  33188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07684 (*cross-listing*)
Date: Mon, 12 Feb 2024 14:46:31 GMT   (24559kb,D)

Title: Towards a Foundation Model for Brain Age Prediction using coVariance
  Neural Networks
Authors: Saurabh Sihag, Gonzalo Mateos, Alejandro Ribeiro
Categories: q-bio.QM cs.LG stat.AP
Comments: Preliminary work. Contact sihag.saurabh@gmail.com for the NeuroVNN
  model and code used for results reported in this manuscript
\\
  Brain age is the estimate of biological age derived from neuroimaging
datasets using machine learning algorithms. Increasing brain age with respect
to chronological age can reflect increased vulnerability to neurodegeneration
and cognitive decline. In this paper, we study NeuroVNN, based on coVariance
neural networks, as a paradigm for foundation model for the brain age
prediction application. NeuroVNN is pre-trained as a regression model on
healthy population to predict chronological age using cortical thickness
features and fine-tuned to estimate brain age in different neurological
contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age
and has a `scale-free' characteristic that allows its transference to datasets
curated according to any arbitrary brain atlas. Our results demonstrate that
NeuroVNN can extract biologically plausible brain age estimates in different
populations, as well as transfer successfully to datasets of dimensionalities
distinct from that for the dataset used to train NeuroVNN.
\\ ( https://arxiv.org/abs/2402.07684 ,  24559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07685 (*cross-listing*)
Date: Mon, 12 Feb 2024 14:48:31 GMT   (4215kb,D)

Title: Contrastive Multiple Instance Learning for Weakly Supervised Person ReID
Authors: Jacob Tyo and Zachary C. Lipton
Categories: cs.CV cs.LG
\\
  The acquisition of large-scale, precisely labeled datasets for person
re-identification (ReID) poses a significant challenge. Weakly supervised ReID
has begun to address this issue, although its performance lags behind fully
supervised methods. In response, we introduce Contrastive Multiple Instance
Learning (CMIL), a novel framework tailored for more effective weakly
supervised ReID. CMIL distinguishes itself by requiring only a single model and
no pseudo labels while leveraging contrastive losses -- a technique that has
significantly enhanced traditional ReID performance yet is absent in all prior
MIL-based approaches. Through extensive experiments and analysis across three
datasets, CMIL not only matches state-of-the-art performance on the large-scale
SYSU-30k dataset with fewer assumptions but also consistently outperforms all
baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification
dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an
extension of the MUDD dataset featuring naturally occurring weak labels from
the real-world application at PerformancePhoto.co. All our code and data are
accessible at
https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.
\\ ( https://arxiv.org/abs/2402.07685 ,  4215kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07723 (*cross-listing*)
Date: Mon, 12 Feb 2024 15:35:32 GMT   (1770kb,D)

Title: Generalization Bounds for Heavy-Tailed SDEs through the Fractional
  Fokker-Planck Equation
Authors: Benjamin Dupuis, Umut \c{S}im\c{s}ekli
Categories: stat.ML cs.LG
\\
  Understanding the generalization properties of heavy-tailed stochastic
optimization algorithms has attracted increasing attention over the past years.
While illuminating interesting aspects of stochastic optimizers by using
heavy-tailed stochastic differential equations as proxies, prior works either
provided expected generalization bounds, or introduced non-computable
information theoretic terms. Addressing these drawbacks, in this work, we prove
high-probability generalization bounds for heavy-tailed SDEs which do not
contain any nontrivial information theoretic terms. To achieve this goal, we
develop new proof techniques based on estimating the entropy flows associated
with the so-called fractional Fokker-Planck equation (a partial differential
equation that governs the evolution of the distribution of the corresponding
heavy-tailed SDE). In addition to obtaining high-probability bounds, we show
that our bounds have a better dependence on the dimension of parameters as
compared to prior art. Our results further identify a phase transition
phenomenon, which suggests that heavy tails can be either beneficial or harmful
depending on the problem structure. We support our theory with experiments
conducted in a variety of settings.
\\ ( https://arxiv.org/abs/2402.07723 ,  1770kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07735 (*cross-listing*)
Date: Mon, 12 Feb 2024 15:48:58 GMT   (10163kb,D)

Title: Graph Structure Inference with BAM: Introducing the Bilinear Attention
  Mechanism
Authors: Philipp Froehlich and Heinz Koeppl
Categories: stat.ML cs.LG
\\
  In statistics and machine learning, detecting dependencies in datasets is a
central challenge. We propose a novel neural network model for supervised graph
structure learning, i.e., the process of learning a mapping between
observational data and their underlying dependence structure. The model is
trained with variably shaped and coupled simulated input data and requires only
a single forward pass through the trained network for inference. By leveraging
structural equation models and employing randomly generated multivariate
Chebyshev polynomials for the simulation of training data, our method
demonstrates robust generalizability across both linear and various types of
non-linear dependencies. We introduce a novel bilinear attention mechanism
(BAM) for explicit processing of dependency information, which operates on the
level of covariance matrices of transformed data and respects the geometry of
the manifold of symmetric positive definite matrices. Empirical evaluation
demonstrates the robustness of our method in detecting a wide range of
dependencies, excelling in undirected graph estimation and proving competitive
in completed partially directed acyclic graph estimation through a novel
two-step approach.
\\ ( https://arxiv.org/abs/2402.07735 ,  10163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07739 (*cross-listing*)
Date: Mon, 12 Feb 2024 15:57:31 GMT   (14530kb,D)

Title: Task-conditioned adaptation of visual features in multi-task policy
  learning
Authors: Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf
Categories: cs.CV cs.LG cs.RO
\\
  Successfully addressing a wide variety of tasks is a core ability of
autonomous agents, which requires flexibly adapting the underlying
decision-making strategies and, as we argue in this work, also adapting the
underlying perception modules. An analogical argument would be the human visual
system, which uses top-down signals to focus attention determined by the
current task. Similarly, in this work, we adapt pre-trained large vision models
conditioned on specific downstream tasks in the context of multi-task policy
learning. We introduce task-conditioned adapters that do not require finetuning
any pre-trained weights, combined with a single policy trained with behavior
cloning and capable of addressing multiple tasks. We condition the policy and
visual adapters on task embeddings, which can be selected at inference if the
task is known, or alternatively inferred from a set of example demonstrations.
To this end, we propose a new optimization-based estimator. We evaluate the
method on a wide variety of tasks of the CortexBench benchmark and show that,
compared to existing work, it can be addressed with a single policy. In
particular, we demonstrate that adapting visual features is a key design choice
and that the method generalizes to unseen tasks given visual demonstrations.
\\ ( https://arxiv.org/abs/2402.07739 ,  14530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07752 (*cross-listing*)
Date: Mon, 12 Feb 2024 16:21:50 GMT   (8412kb,D)

Title: Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL
  with Continuous Action Domains
Authors: Yasin Findik and S. Reza Ahmadzadeh
Categories: cs.MA cs.LG cs.RO
\\
  Tackling multi-agent learning problems efficiently is a challenging task in
continuous action domains. While value-based algorithms excel in sample
efficiency when applied to discrete action domains, they are usually
inefficient when dealing with continuous actions. Policy-based algorithms, on
the other hand, attempt to address this challenge by leveraging critic networks
for guiding the learning process and stabilizing the gradient estimation. The
limitations in the estimation of true return and falling into local optima in
these methods result in inefficient and often sub-optimal policies. In this
paper, we diverge from the trend of further enhancing critic networks, and
focus on improving the effectiveness of value-based methods in multi-agent
continuous domains by concurrently evaluating numerous actions. We propose a
novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired
from the idea of Q-Functionals, that enables agents to transform their states
into basis functions. Our algorithm fosters collaboration among agents by
mixing their action-values. We evaluate the efficacy of our algorithm in six
cooperative multi-agent scenarios. Our empirical findings reveal that MQF
outperforms four variants of Deep Deterministic Policy Gradient through rapid
action evaluation and increased sample efficiency.
\\ ( https://arxiv.org/abs/2402.07752 ,  8412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07762 (*cross-listing*)
Date: Mon, 12 Feb 2024 16:28:52 GMT   (236kb,D)

Title: Scalable Structure Learning for Sparse Context-Specific Causal Systems
Authors: Felix Leopoldo Rios, Alex Markham, Liam Solus
Categories: stat.ML cs.LG math.CO
Comments: 23 pages, 6 figures
\\
  Several approaches to graphically representing context-specific relations
among jointly distributed categorical variables have been proposed, along with
structure learning algorithms. While existing optimization-based methods have
limited scalability due to the large number of context-specific models, the
constraint-based methods are more prone to error than even constraint-based DAG
learning algorithms since more relations must be tested. We present a hybrid
algorithm for learning context-specific models that scales to hundreds of
variables while testing no more constraints than standard DAG learning
algorithms. Scalable learning is achieved through a combination of an
order-based MCMC algorithm and sparsity assumptions analogous to those
typically invoked for DAG models. To implement the method, we solve a special
case of an open problem recently posed by Alon and Balogh. The method is shown
to perform well on synthetic data and real world examples, in terms of both
accuracy and scalability.
\\ ( https://arxiv.org/abs/2402.07762 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07763 (*cross-listing*)
Date: Mon, 12 Feb 2024 16:28:57 GMT   (643kb,D)

Title: Multi-level Optimal Control with Neural Surrogate Models
Authors: Dante Kalise, Estefan\'ia Loayza-Romero, Kirsten A. Morris, Zhengang
  Zhong
Categories: math.OC cs.LG cs.NA math.NA
\\
  Optimal actuator and control design is studied as a multi-level optimisation
problem, where the actuator design is evaluated based on the performance of the
associated optimal closed loop. The evaluation of the optimal closed loop for a
given actuator realisation is a computationally demanding task, for which the
use of a neural network surrogate is proposed. The use of neural network
surrogates to replace the lower level of the optimisation hierarchy enables the
use of fast gradient-based and gradient-free consensus-based optimisation
methods to determine the optimal actuator design. The effectiveness of the
proposed surrogate models and optimisation methods is assessed in a test
related to optimal actuator location for heat control.
\\ ( https://arxiv.org/abs/2402.07763 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07781 (*cross-listing*)
Date: Mon, 12 Feb 2024 16:47:08 GMT   (2526kb,D)

Title: IR-Aware ECO Timing Optimization Using Reinforcement Learning
Authors: Vidya A. Chhabria, Wenjing Jiang and Sachin S. Sapatnekar
Categories: cs.AR cs.LG
\\
  Engineering change orders (ECOs) in late stages make minimal design fixes to
recover from timing shifts due to excessive IR drops. This paper integrates
IR-drop-aware timing analysis and ECO timing optimization using reinforcement
learning (RL). The method operates after physical design and power grid
synthesis, and rectifies IR-drop-induced timing degradation through gate
sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel
RL framework, which trains a relational graph convolutional network (R-GCN)
agent to sequentially size gates to fix timing violations. The R-GCN agent
outperforms a classical LR-only algorithm: in an open 45nm technology, it (a)
moves the Pareto front of the delay-area tradeoff curve to the left and (b)
saves runtime over the classical method by running fast inference using trained
models at iso-quality. The RL model is transferable across timing
specifications, and transferable to unseen designs with zero-shot learning or
fine tuning.
\\ ( https://arxiv.org/abs/2402.07781 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07793 (*cross-listing*)
Date: Mon, 12 Feb 2024 16:59:06 GMT   (66kb)

Title: Tuning-Free Stochastic Optimization
Authors: Ahmed Khaled and Chi Jin
Categories: math.OC cs.LG stat.ML
\\
  Large-scale machine learning problems make the cost of hyperparameter tuning
ever more prohibitive. This creates a need for algorithms that can tune
themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that
can match the performance of optimally-tuned optimization algorithms up to
polylogarithmic factors given only loose hints on the relevant problem
parameters. We consider in particular algorithms that can match optimally-tuned
Stochastic Gradient Descent (SGD). When the domain of optimization is bounded,
we show tuning-free matching of SGD is possible and achieved by several
existing algorithms. We prove that for the task of minimizing a convex and
smooth or Lipschitz function over an unbounded domain, tuning-free optimization
is impossible. We discuss conditions under which tuning-free optimization is
possible even over unbounded domains. In particular, we show that the recently
proposed DoG and DoWG algorithms are tuning-free when the noise distribution is
sufficiently well-behaved. For the task of finding a stationary point of a
smooth and potentially nonconvex function, we give a variant of SGD that
matches the best-known high-probability convergence rate for tuned SGD at only
an additional polylogarithmic cost. However, we also give an impossibility
result that shows no algorithm can hope to match the optimal expected
convergence rate for tuned SGD with high probability.
\\ ( https://arxiv.org/abs/2402.07793 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07802 (*cross-listing*)
Date: Mon, 12 Feb 2024 17:07:02 GMT   (67kb)

Title: Towards a mathematical theory for consistency training in diffusion
  models
Authors: Gen Li, Zhihan Huang, Yuting Wei
Categories: stat.ML cs.IT cs.LG math.IT math.ST stat.TH
Comments: The first two authors contributed equally
\\
  Consistency models, which were proposed to mitigate the high computational
overhead during the sampling phase of diffusion models, facilitate single-step
sampling while attaining state-of-the-art empirical performance. When
integrated into the training phase, consistency models attempt to train a
sequence of consistency functions capable of mapping any point at any time step
of the diffusion process to its starting point. Despite the empirical success,
a comprehensive theoretical understanding of consistency training remains
elusive. This paper takes a first step towards establishing theoretical
underpinnings for consistency models. We demonstrate that, in order to generate
samples within $\varepsilon$ proximity to the target in distribution (measured
by some Wasserstein metric), it suffices for the number of steps in consistency
learning to exceed the order of $d^{5/2}/\varepsilon$, with $d$ the data
dimension. Our theory offers rigorous insights into the validity and efficacy
of consistency models, illuminating their utility in downstream inference
tasks.
\\ ( https://arxiv.org/abs/2402.07802 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07839 (*cross-listing*)
Date: Mon, 12 Feb 2024 17:50:56 GMT   (10301kb,D)

Title: Towards Meta-Pruning via Optimal Transport
Authors: Alexander Theus, Olin Geimer, Friedrich Wicke, Thomas Hofmann, Sotiris
  Anagnostidis, Sidak Pal Singh
Categories: cs.CV cs.LG
Comments: Accepted at the International Conference on Learning Representations
  (ICLR) 2024
\\
  Structural pruning of neural networks conventionally relies on identifying
and discarding less important neurons, a practice often resulting in
significant accuracy loss that necessitates subsequent fine-tuning efforts.
This paper introduces a novel approach named Intra-Fusion, challenging this
prevailing pruning paradigm. Unlike existing methods that focus on designing
meaningful neuron importance metrics, Intra-Fusion redefines the overlying
pruning procedure. Through utilizing the concepts of model fusion and Optimal
Transport, we leverage an agnostically given importance metric to arrive at a
more effective sparse model representation. Notably, our approach achieves
substantial accuracy recovery without the need for resource-intensive
fine-tuning, making it an efficient and promising tool for neural network
compression.
  Additionally, we explore how fusion can be added to the pruning process to
significantly decrease the training time while maintaining competitive
performance. We benchmark our results for various networks on commonly used
datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that
the proposed Intra-Fusion approach invigorates exploration into a fresh
alternative to the predominant compression approaches. Our code is available
here: https://github.com/alexandertheus/Intra-Fusion.
\\ ( https://arxiv.org/abs/2402.07839 ,  10301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07867 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:28:36 GMT   (481kb,D)

Title: PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented
  Generation of Large Language Models
Authors: Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia
Categories: cs.CR cs.LG
Comments: Code is available at https://github.com/sleeepeer/PoisonedRAG
\\
  Large language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate those limitations. In particular, given a question, RAG retrieves
relevant knowledge from a knowledge database to augment the input of the LLM.
For instance, the retrieved knowledge could be a set of top-k texts that are
most semantically similar to the given question when the knowledge database
contains millions of texts collected from Wikipedia. As a result, the LLM could
utilize the retrieved knowledge as the context to generate an answer for the
given question. Existing studies mainly focus on improving the accuracy or
efficiency of RAG, leaving its security largely unexplored. We aim to bridge
the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge
poisoning attacks to RAG, where an attacker could inject a few poisoned texts
into the knowledge database such that the LLM generates an attacker-chosen
target answer for an attacker-chosen target question. We formulate knowledge
poisoning attacks as an optimization problem, whose solution is a set of
poisoned texts. Depending on the background knowledge (e.g., black-box and
white-box settings) of an attacker on the RAG, we propose two solutions to
solve the optimization problem, respectively. Our results on multiple benchmark
datasets and LLMs show our attacks could achieve 90% attack success rates when
injecting 5 poisoned texts for each target question into a database with
millions of texts. We also evaluate recent defenses and our results show they
are insufficient to defend against our attacks, highlighting the need for new
defenses.
\\ ( https://arxiv.org/abs/2402.07867 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07878 (*cross-listing*)
Date: Mon, 12 Feb 2024 18:44:02 GMT   (89kb,D)

Title: Using Graph Theory for Improving Machine Learning-based Detection of
  Cyber Attacks
Authors: Giacomo Zonneveld, Lorenzo Principi, Marco Baldi
Categories: cs.CR cs.LG
Comments: 6 pages, 1 figure, 4 tables
\\
  Early detection of network intrusions and cyber threats is one of the main
pillars of cybersecurity. One of the most effective approaches for this purpose
is to analyze network traffic with the help of artificial intelligence
algorithms, with the aim of detecting the possible presence of an attacker by
distinguishing it from a legitimate user. This is commonly done by collecting
the traffic exchanged between terminals in a network and analyzing it on a
per-packet or per-connection basis. In this paper, we propose instead to
perform pre-processing of network traffic under analysis with the aim of
extracting some new metrics on which we can perform more efficient detection
and overcome some limitations of classical approaches. These new metrics are
based on graph theory, and consider the network as a whole, rather than
focusing on individual packets or connections. Our approach is validated
through experiments performed on publicly available data sets, from which it
results that it can not only overcome some of the limitations of classical
approaches, but also achieve a better detection capability of cyber threats.
\\ ( https://arxiv.org/abs/2402.07878 ,  89kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2012.02282
replaced with revised version Sat, 10 Feb 2024 15:23:47 GMT   (503kb)

Title: Creativity of Deep Learning: Conceptualization and Assessment
Authors: Marcus Basalla and Johannes Schneider and Jan vom Brocke
Categories: cs.AI cs.LG
Journal-ref: Proceedings of the International Conference on Agents and
  Artificial Intelligence (ICAART), 2022
\\ ( https://arxiv.org/abs/2012.02282 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08560
replaced with revised version Mon, 12 Feb 2024 18:59:46 GMT   (419kb,D)

Title: MemGPT: Towards LLMs as Operating Systems
Authors: Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G.
  Patil, Ion Stoica, Joseph E. Gonzalez
Categories: cs.AI
Comments: Code and data available at https://research.memgpt.ai
\\ ( https://arxiv.org/abs/2310.08560 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00344
replaced with revised version Mon, 12 Feb 2024 11:41:37 GMT   (132kb,D)

Title: A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents
Authors: Olivier Sigaud, Gianluca Baldassarre, Cedric Colas, Stephane Doncieux,
  Richard Duro, Nicolas Perrin-Gilbert, Vieri Giuliano Santucci
Categories: cs.AI
\\ ( https://arxiv.org/abs/2311.00344 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14455
replaced with revised version Mon, 12 Feb 2024 09:42:35 GMT   (3251kb,D)

Title: Universal Jailbreak Backdoors from Poisoned Human Feedback
Authors: Javier Rando and Florian Tram\`er
Categories: cs.AI cs.CL cs.CR cs.LG
Comments: Accepted as conference paper in ICLR 2024
\\ ( https://arxiv.org/abs/2311.14455 ,  3251kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06231
replaced with revised version Mon, 12 Feb 2024 10:22:21 GMT   (7018kb,D)

Title: Uncovering communities of pipelines in the task-fMRI analytical space
Authors: Elodie Germani (EMPENN), Elisa Fromont (LACODAM), Camille Maumet
  (EMPENN)
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.06231 ,  7018kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07130
replaced with revised version Sun, 11 Feb 2024 08:35:59 GMT   (26953kb,D)

Title: Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety
  Filters of Text-to-Image Models
Authors: Yimo Deng, Huangxun Chen
Categories: cs.AI
Comments: 23 pages, 11 figures, under review
\\ ( https://arxiv.org/abs/2312.07130 ,  26953kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09963
replaced with revised version Mon, 12 Feb 2024 09:52:37 GMT   (213kb,D)

Title: Symbolic Numeric Planning with Patterns
Authors: Matteo Cardellini, Enrico Giunchiglia, and Marco Maratea
Categories: cs.AI
Comments: Accepted at AAAI24
\\ ( https://arxiv.org/abs/2312.09963 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12568
replaced with revised version Sat, 10 Feb 2024 21:52:17 GMT   (24362kb,D)

Title: Scaling Opponent Shaping to High Dimensional Games
Authors: Akbir Khan and Timon Willi and Newton Kwan and Andrea Tacchetti and
  Chris Lu and Edward Grefenstette and Tim Rockt\"aschel and Jakob Foerster
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.12568 ,  24362kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14890
replaced with revised version Mon, 12 Feb 2024 17:30:25 GMT   (21528kb,D)

Title: NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language
  Models via Complexity Classes
Authors: Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng Zhang
Categories: cs.AI cs.CC cs.CL cs.LG
Comments: 23 pages, 7 figures, 2 tables
\\ ( https://arxiv.org/abs/2312.14890 ,  21528kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02731
replaced with revised version Mon, 12 Feb 2024 02:20:30 GMT   (591kb,D)

Title: Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts
  for Instruction Tuning on General Tasks
Authors: Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.02731 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01276
replaced with revised version Mon, 12 Feb 2024 05:00:44 GMT   (1754kb,D)

Title: Federated Unlearning: a Perspective of Stability and Fairness
Authors: Jiaqi Shao, Tao Lin, Xuanyu Cao, Bing Luo
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.01276 ,  1754kb)
------------------------------------------------------------------------------
\\
arXiv:2208.07998
replaced with revised version Sun, 11 Feb 2024 21:24:26 GMT   (459kb,D)

Title: What Artificial Neural Networks Can Tell Us About Human Language
  Acquisition
Authors: Alex Warstadt and Samuel R. Bowman
Categories: cs.CL
Comments: Please cite the published version with the following information:
  @incollection{warstadt2022artificial, title={What artificial neural networks
  can tell us about human language acquisition}, author={Warstadt, Alex and
  Bowman, Samuel R.}, booktitle={Algebraic Structures in Natural Language},
  pages={17--60}, year={2022}, publisher={CRC Press} }
\\ ( https://arxiv.org/abs/2208.07998 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2212.07699
replaced with revised version Sat, 10 Feb 2024 10:12:22 GMT   (5354kb,D)

Title: Retrieval-based Disentangled Representation Learning with Natural
  Language Supervision
Authors: Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Lei Chen
Categories: cs.CL cs.AI cs.CV
\\ ( https://arxiv.org/abs/2212.07699 ,  5354kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03106
replaced with revised version Sat, 10 Feb 2024 17:45:03 GMT   (428kb)

Title: Efficient and Flexible Topic Modeling using Pretrained Embeddings and
  Bag of Sentences
Authors: Johannes Schneider
Categories: cs.CL cs.LG
Comments: International Conference on Agents and Artificial Intelligence
  (ICAART), 2024
\\ ( https://arxiv.org/abs/2302.03106 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09859
replaced with revised version Mon, 12 Feb 2024 09:31:30 GMT   (1364kb,D)

Title: Smaller Language Models are Better Black-box Machine-Generated Text
  Detectors
Authors: Niloofar Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, Taylor
  Berg-Kirkpatrick
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2305.09859 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13068
replaced with revised version Sat, 10 Feb 2024 05:06:56 GMT   (9362kb,D)

Title: Making Language Models Better Tool Learners with Execution Feedback
Authors: Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen,
  Ningyu Zhang
Categories: cs.CL cs.AI cs.HC cs.IR cs.LG
Comments: Work in progress
\\ ( https://arxiv.org/abs/2305.13068 ,  9362kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15096
replaced with revised version Sat, 10 Feb 2024 20:56:20 GMT   (160kb,D)

Title: Dynamic Masking Rate Schedules for MLM Pretraining
Authors: Zachary Ankner, Naomi Saphra, Davis Blalock, Jonathan Frankle, and
  Matthew L. Leavitt
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.15096 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00933
replaced with revised version Mon, 12 Feb 2024 11:43:15 GMT   (2573kb,D)

Title: Data-Driven Information Extraction and Enrichment of Molecular Profiling
  Data for Cancer Cell Lines
Authors: Ellery Smith, Rahel Paloots, Dimitris Giagkos, Michael Baudis, Kurt
  Stockinger
Categories: cs.CL cs.CE cs.DB
\\ ( https://arxiv.org/abs/2307.00933 ,  2573kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01878
replaced with revised version Sun, 11 Feb 2024 07:08:09 GMT   (558kb,D)

Title: KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation
Authors: Weijie Xu, Xiaoyu Jiang, Jay Desai, Bin Han, Fuqin Yan and Francis
  Iannacci
Categories: cs.CL cs.AI
Comments: 12 pages, 4 figures, ICLR 2022 Workshop
MSC-class: 68T50
ACM-class: I.2.6
Journal-ref: ICLR 2022 Workshop PML4DC
\\ ( https://arxiv.org/abs/2307.01878 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11278
replaced with revised version Sun, 11 Feb 2024 16:06:53 GMT   (276kb,D)

Title: Novel Dual-Generator Framework for Open-Domain Question Answering
Authors: Abdelrahman Abdallah, Adam Jatowt
Categories: cs.CL
Journal-ref: J Big Data 10, 127 (2023)
DOI: 10.1186/s40537-023-00802-8
\\ ( https://arxiv.org/abs/2307.11278 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02705
replaced with revised version Mon, 12 Feb 2024 18:55:34 GMT   (1739kb,D)

Title: Certifying LLM Safety against Adversarial Prompting
Authors: Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil
  Feizi and Himabindu Lakkaraju
Categories: cs.CL cs.AI cs.CR cs.LG
\\ ( https://arxiv.org/abs/2309.02705 ,  1739kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08325
replaced with revised version Sat, 10 Feb 2024 17:57:14 GMT   (390kb,D)

Title: Distributional Inclusion Hypothesis and Quantifications: Probing for
  Hypernymy in Functional Distributional Semantics
Authors: Chun Hei Lo, Wai Lam, Hong Cheng, and Guy Emerson
Categories: cs.CL
Comments: 12 pages
\\ ( https://arxiv.org/abs/2309.08325 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09800
replaced with revised version Sun, 11 Feb 2024 16:26:53 GMT   (444kb,D)

Title: CrossLingR: A Comprehensive Multilingual Receipt Dataset for
  Cross-Language Information Extraction and Classification
Authors: Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser
  Elbendary, Adam Jatowt
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.09800 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01045
replaced with revised version Sun, 11 Feb 2024 16:58:02 GMT   (2435kb,D)

Title: Tool-Augmented Reward Modeling
Authors: Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua
  Wu
Categories: cs.CL
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2310.01045 ,  2435kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04910
replaced with revised version Sun, 11 Feb 2024 08:13:39 GMT   (274kb,D)

Title: Faithful Knowledge Graph Explanations for Commonsense Reasoning
Authors: Weihe Zhai, Arkaitz Zubiaga, Bingquan Liu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.04910 ,  274kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05116
replaced with revised version Sat, 10 Feb 2024 05:29:47 GMT   (5784kb,D)

Title: Utilizing Contextual Clues and Role Correlations for Enhancing
  Document-level Event Argument Extraction
Authors: Wanlong Liu, Dingyi Zeng, Li Zhou, Malu Zhang, Shaohuan Cheng, Weishan
  Kong, Yichen Xiao, Hongyang Zhao, Wenyu Chen
Categories: cs.CL cs.IR
Comments: 12 pages
\\ ( https://arxiv.org/abs/2310.05116 ,  5784kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14174
replaced with revised version Sat, 10 Feb 2024 06:18:59 GMT   (3505kb,D)

Title: An In-Context Schema Understanding Method for Knowledge Base Question
  Answering
Authors: Yantao Liu, Zixuan Li, Xiaolong Jin, Yucan Guo, Long Bai, Saiping
  Guan, Jiafeng Guo and Xueqi Cheng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.14174 ,  3505kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14338
replaced with revised version Mon, 12 Feb 2024 06:30:44 GMT   (1993kb,D)

Title: From Chaos to Clarity: Claim Normalization to Empower Fact-Checking
Authors: Megha Sundriyal, Tanmoy Chakraborty, Preslav Nakov
Categories: cs.CL cs.AI
Comments: Accepted at Findings EMNLP2023
\\ ( https://arxiv.org/abs/2310.14338 ,  1993kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18376
replaced with revised version Sun, 11 Feb 2024 18:02:38 GMT   (306kb,D)

Title: SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL
  Translation
Authors: Adri\'an Bazaga and Pietro Li\`o and Gos Micklem
Categories: cs.CL cs.LG
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2310.18376 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18463
replaced with revised version Mon, 12 Feb 2024 17:05:48 GMT   (1366kb,D)

Title: PeTailor: Improving Large Language Model by Tailored Chunk Scorer in
  Biomedical Triple Extraction
Authors: Mingchen Li, M. Chen, Huixue Zhou, Halil Kilicoglu, Rui Zhang
Categories: cs.CL
Comments: this is the second preprint version
\\ ( https://arxiv.org/abs/2310.18463 ,  1366kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06233
replaced with revised version Sat, 10 Feb 2024 22:28:34 GMT   (61kb)

Title: Data Contamination Quiz: A Tool to Detect and Estimate Contamination in
  Large Language Models
Authors: Shahriar Golchin, Mihai Surdeanu
Categories: cs.CL cs.AI cs.LG
Comments: v2.1 preprint
\\ ( https://arxiv.org/abs/2311.06233 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07466
replaced with revised version Sat, 10 Feb 2024 18:31:13 GMT   (8499kb,D)

Title: On Measuring Faithfulness or Self-consistency of Natural Language
  Explanations
Authors: Letitia Parcalabescu and Anette Frank
Categories: cs.CL cs.AI cs.LG
Comments: 9 main paper pages, 29 appendix pages
MSC-class: 68Txx
ACM-class: I.2.7; I.2.10
\\ ( https://arxiv.org/abs/2311.07466 ,  8499kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08711
replaced with revised version Mon, 12 Feb 2024 01:09:34 GMT   (958kb,D)

Title: PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning
Authors: Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng
  Jiang, Francesco Barbieri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08711 ,  958kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15054
replaced with revised version Sat, 10 Feb 2024 11:05:40 GMT   (331kb)

Title: Detection of developmental language disorder in Cypriot Greek children
  using a neural network algorithm
Authors: Georgios P. Georgiou and Elena Theodorou
Categories: cs.CL cs.LG
Comments: 15 pages, 3 figures, journal article
\\ ( https://arxiv.org/abs/2311.15054 ,  331kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03731
replaced with revised version Sun, 11 Feb 2024 10:20:52 GMT   (582kb,D)

Title: MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs
Authors: Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang
Categories: cs.CL cs.LG
Comments: Accepted by WWW2024
\\ ( https://arxiv.org/abs/2312.03731 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04455
replaced with revised version Sun, 11 Feb 2024 13:01:16 GMT   (1771kb,D)

Title: Fortify the Shortest Stave in Attention: Enhancing Context Awareness of
  Large Language Models for Effective Tool Use
Authors: Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang,
  Yongbin Li and Rui Yan
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.04455 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05507
replaced with revised version Mon, 12 Feb 2024 09:30:24 GMT   (1089kb,D)

Title: InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks
Authors: Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang,
  Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li,
  Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu
Categories: cs.CL cs.AI
Comments: 27 pages, 7 figures, work in progress
\\ ( https://arxiv.org/abs/2401.05507 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06790
replaced with revised version Sun, 11 Feb 2024 15:54:58 GMT   (3116kb,D)

Title: Using Zero-shot Prompting in the Automatic Creation and Expansion of
  Topic Taxonomies for Tagging Retail Banking Transactions
Authors: Daniel de S. Moraes, Pedro T. C. Santos, Polyana B. da Costa, Matheus
  A. S. Pinto, Ivan de J. P. Pinto, \'Alvaro M. G. da Veiga, Sergio Colcher,
  Antonio J. G. Busson, Rafael H. Rocha, Rennan Gaio, Rafael Miceli, Gabriela
  Tourinho, Marcos Rabaioli, Leandro Santos, Fellipe Marques, David Favaro
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.06790 ,  3116kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09003
replaced with revised version Sun, 11 Feb 2024 04:10:53 GMT   (101kb,D)

Title: Augmenting Math Word Problems via Iterative Question Composing
Authors: Haoxiong Liu, Yifan Zhang, Yifan Luo, Andrew Chi-Chih Yao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.09003 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09615
replaced with revised version Fri, 9 Feb 2024 22:08:12 GMT   (54kb)

Title: Learning Shortcuts: On the Misleading Promise of NLU in Language Models
Authors: Geetanjali Bihani, Julia Taylor Rayz
Categories: cs.CL cs.AI
Comments: Accepted at HICSS-SDPS 2024
\\ ( https://arxiv.org/abs/2401.09615 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09798
replaced with revised version Mon, 12 Feb 2024 02:29:28 GMT   (51kb)

Title: All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks
Authors: Kazuhiro Takemoto
Categories: cs.CL cs.AI cs.CY
Comments: 12 pages, 4 figures, 3 tables
\\ ( https://arxiv.org/abs/2401.09798 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11356
replaced with revised version Mon, 12 Feb 2024 02:09:51 GMT   (197kb,D)

Title: ProLex: A Benchmark for Language Proficiency-oriented Lexical
  Substitution
Authors: Xuanming Zhang, Zixun Chen, Zhou Yu
Categories: cs.CL
Comments: 19 pages, 4 figures
\\ ( https://arxiv.org/abs/2401.11356 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14215
replaced with revised version Mon, 12 Feb 2024 12:27:18 GMT   (5822kb,D)

Title: Commonsense-augmented Memory Construction and Management in Long-term
  Conversations via Context-aware Persona Refinement
Authors: Hana Kim, Kai Tzu-iunn Ong, Seoyeon Kim, Dongha Lee, Jinyoung Yeo
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2401.14215 ,  5822kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15042
replaced with revised version Mon, 12 Feb 2024 03:32:43 GMT   (8091kb,D)

Title: PROXYQA: An Alternative Framework for Evaluating Long-Form Text
  Generation with Large Language Models
Authors: Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng,
  Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.15042 ,  8091kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16184
replaced with revised version Mon, 12 Feb 2024 11:30:05 GMT   (4344kb,D)

Title: On the Semantics of LM Latent Space: A Vocabulary-defined Approach
Authors: Jian Gu, Chunyang Chen, Aldeida Aleti
Categories: cs.CL cs.LG
Comments: under peer-review
\\ ( https://arxiv.org/abs/2401.16184 ,  4344kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00746
replaced with revised version Mon, 12 Feb 2024 16:56:25 GMT   (951kb,D)

Title: Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
Authors: Mingyu Jin, Qinkai Yu, Chong Zhang, Dong Shu, Suiyuan Zhu, Mengnan Du,
  Yongfeng Zhang, Yanda Meng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00746 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01713
replaced with revised version Sat, 10 Feb 2024 16:31:40 GMT   (2368kb,D)

Title: Prompting Large Language Models for Zero-Shot Clinical Prediction with
  Structured Longitudinal Electronic Health Record Data
Authors: Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin
  Liao, Ewen M. Harrison, Liantao Ma, Chengwei Pan
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.01713 ,  2368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01735
replaced with revised version Sat, 10 Feb 2024 14:08:55 GMT   (11185kb,D)

Title: VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large
  Models
Authors: Yi Zhao, Yilin Zhang, Rong Xiang, Jing Li, Hillming Li
Categories: cs.CL cs.AI cs.CV
Comments: under review
\\ ( https://arxiv.org/abs/2402.01735 ,  11185kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03216
replaced with revised version Sat, 10 Feb 2024 14:26:06 GMT   (1239kb,D)

Title: BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge Distillation
Authors: Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\ ( https://arxiv.org/abs/2402.03216 ,  1239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03776
replaced with revised version Sat, 10 Feb 2024 22:43:41 GMT   (41kb)

Title: Large Language Models As MOOCs Graders
Authors: Shahriar Golchin, Nikhil Garuda, Christopher Impey, Matthew Wenger
Categories: cs.CL cs.AI
Comments: v1.1 preprint
\\ ( https://arxiv.org/abs/2402.03776 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05116
replaced with revised version Sun, 11 Feb 2024 22:50:17 GMT   (3857kb,D)

Title: Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and
  Google Bard Content in Relation to BioMedical Literature
Authors: Jakub Klimczak and Ahmed Abdeen Hamed
Categories: cs.CL cs.DL cs.IR
Comments: 15 pages, 10 figures, 4 tables; and 1 algorithm
\\ ( https://arxiv.org/abs/2402.05116 ,  3857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05131
replaced with revised version Sat, 10 Feb 2024 10:55:15 GMT   (256kb)

Title: Financial Report Chunking for Effective Retrieval Augmented Generation
Authors: Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, and
  Renyu Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.05131 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05403
replaced with revised version Fri, 9 Feb 2024 19:09:52 GMT   (2361kb,D)

Title: In-Context Principle Learning from Mistakes
Authors: Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra,
  Yiming Yang, Niket Tandon, Uri Alon
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.05403 ,  2361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05783
replaced with revised version Mon, 12 Feb 2024 10:44:18 GMT   (8139kb,D)

Title: Text-to-Code Generation with Modality-relative Pre-training
Authors: Fenia Christopoulou, Guchun Zhang, Gerasimos Lampouras
Categories: cs.CL
Comments: Accepted at EACL 2024. 15 pages, 5 figures, 6 tables
\\ ( https://arxiv.org/abs/2402.05783 ,  8139kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05868
replaced with revised version Mon, 12 Feb 2024 16:26:14 GMT   (2503kb,D)

Title: EmojiCrypt: Prompt Encryption for Secure Communication with Large
  Language Models
Authors: Guo Lin, Wenyue Hua, Yongfeng Zhang
Categories: cs.CL cs.AI cs.CR cs.IR cs.LG
Comments: 12 pages, 4 figures, 2 tables, comments and suggestions are welcome
\\ ( https://arxiv.org/abs/2402.05868 ,  2503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05880
replaced with revised version Sat, 10 Feb 2024 17:03:58 GMT   (1693kb,D)

Title: Generative Echo Chamber? Effects of LLM-Powered Search Systems on
  Diverse Information Seeking
Authors: Nikhil Sharma, Q. Vera Liao, Ziang Xiao
Categories: cs.CL cs.AI cs.HC
Comments: Accepted in CHI'24. Supplementary material will be available online
  with the official submission in CHI 2024
\\ ( https://arxiv.org/abs/2402.05880 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06073
replaced with revised version Mon, 12 Feb 2024 15:28:38 GMT   (716kb)

Title: LightCAM: A Fast and Light Implementation of Context-Aware Masking based
  D-TDNN for Speaker Verification
Authors: Di Cao, Xianchen Wang, Junfeng Zhou, Jiakai Zhang, Yanjing Lei and
  Wenpeng Chen
Categories: cs.CL cs.SD eess.AS
\\ ( https://arxiv.org/abs/2402.06073 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2109.09264
replaced with revised version Mon, 12 Feb 2024 17:22:28 GMT   (10886kb,D)

Title: Computationally Efficient High-Dimensional Bayesian Optimization via
  Variable Selection
Authors: Yihang Shen and Carl Kingsford
Categories: cs.LG stat.ML
Comments: This work has already been accepted in AutoML 2023
\\ ( https://arxiv.org/abs/2109.09264 ,  10886kb)
------------------------------------------------------------------------------
\\
arXiv:2110.02879
replaced with revised version Sun, 11 Feb 2024 00:21:07 GMT   (1794kb,D)

Title: Compositional Q-learning for electrolyte repletion with imbalanced
  patient sub-populations
Authors: Aishwarya Mandyam, Andrew Jones, Jiayu Yao, Krzysztof Laudanski,
  Barbara Engelhardt
Categories: cs.LG cs.AI
Journal-ref: Proceedings of the 3rd Machine Learning for Health Symposium, PMLR
  225:323-339, 2023
\\ ( https://arxiv.org/abs/2110.02879 ,  1794kb)
------------------------------------------------------------------------------
\\
arXiv:2208.04448
replaced with revised version Sun, 11 Feb 2024 22:37:50 GMT   (23629kb,D)

Title: NeuralVDB: High-resolution Sparse Volume Representation using
  Hierarchical Neural Networks
Authors: Doyub Kim, Minjae Lee, Ken Museth
Categories: cs.LG cs.CV cs.GR
\\ ( https://arxiv.org/abs/2208.04448 ,  23629kb)
------------------------------------------------------------------------------
\\
arXiv:2210.03120
replaced with revised version Sun, 11 Feb 2024 16:02:18 GMT   (7180kb,D)

Title: GBSVM: Granular-ball Support Vector Machine
Authors: Shuyin Xia, Xiaoyu Lian, Guoyin Wang, Xinbo Gao, Jiancu Chen, Xiaoli
  Peng
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2210.03120 ,  7180kb)
------------------------------------------------------------------------------
\\
arXiv:2210.03728
replaced with revised version Sun, 11 Feb 2024 22:30:48 GMT   (5164kb,D)

Title: Dynamic Latent Separation for Deep Learning
Authors: Yi-Lin Tuan, Zih-Yun Chiu, William Yang Wang
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2210.03728 ,  5164kb)
------------------------------------------------------------------------------
\\
arXiv:2211.03846
replaced with revised version Sun, 11 Feb 2024 05:18:01 GMT   (8943kb,D)

Title: Federated Causal Discovery From Interventions
Authors: Amin Abyaneh, Nino Scherrer, Patrick Schwab, Stefan Bauer, Bernhard
  Sch\"olkopf, Arash Mehrjou
Categories: cs.LG cs.MA stat.ME
\\ ( https://arxiv.org/abs/2211.03846 ,  8943kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04659
replaced with revised version Sat, 10 Feb 2024 23:26:03 GMT   (795kb,D)

Title: When is Momentum Extragradient Optimal? A Polynomial-Based Analysis
Authors: Junhyung Lyle Kim, Gauthier Gidel, Anastasios Kyrillidis, Fabian
  Pedregosa
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2211.04659 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2211.15281
replaced with revised version Sat, 10 Feb 2024 22:31:45 GMT   (1022kb,D)

Title: Flow: Per-Instance Personalized Federated Learning Through Dynamic
  Routing
Authors: Kunjal Panchal, Sunav Choudhary, Nisarg Parikh, Lijun Zhang, Hui Guan
Categories: cs.LG
Comments: 37th Annual Conference on Neural Information Processing Systems
  (NeurIPS), 2023
\\ ( https://arxiv.org/abs/2211.15281 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2301.04741
replaced with revised version Fri, 9 Feb 2024 20:44:22 GMT   (1235kb,D)

Title: Efficient Preference-Based Reinforcement Learning Using Learned Dynamics
  Models
Authors: Yi Liu, Gaurav Datta, Ellen Novoseller, Daniel S. Brown
Categories: cs.LG
Comments: In proceedings of the 2023 IEEE International Conference on Robotics
  and Automation (ICRA 2023)
\\ ( https://arxiv.org/abs/2301.04741 ,  1235kb)
------------------------------------------------------------------------------
\\
arXiv:2301.09505
replaced with revised version Sun, 11 Feb 2024 03:44:23 GMT   (744kb,D)

Title: Rethinking the Expressive Power of GNNs via Graph Biconnectivity
Authors: Bohang Zhang, Shengjie Luo, Liwei Wang, Di He
Categories: cs.LG stat.ML
Comments: Extended from ICLR 2023 Outstanding Paper; 60 pages, 12 figures. Fix
  typos in the previous version
\\ ( https://arxiv.org/abs/2301.09505 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00722
replaced with revised version Sat, 10 Feb 2024 17:48:25 GMT   (623kb,D)

Title: A Survey of Deep Learning: From Activations to Transformers
Authors: Johannes Schneider and Michalis Vlachos
Categories: cs.LG cs.AI
Comments: International Conference on Agents and Artificial Intelligence
  (ICAART), 2024
\\ ( https://arxiv.org/abs/2302.00722 ,  623kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02182
replaced with revised version Sat, 10 Feb 2024 23:08:10 GMT   (2391kb,D)

Title: Online Reinforcement Learning in Non-Stationary Context-Driven
  Environments
Authors: Pouya Hamadanian, Arash Nasr-Esfahany, Malte Schwarzkopf, Siddartha
  Sen, Mohammad Alizadeh
Categories: cs.LG cs.AI
Comments: 9 pages + 6 pages in the appendix, 10 Figures and 8 Tables
\\ ( https://arxiv.org/abs/2302.02182 ,  2391kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05428
replaced with revised version Sat, 10 Feb 2024 14:02:08 GMT   (2252kb,D)

Title: STERLING: Synergistic Representation Learning on Bipartite Graphs
Authors: Baoyu Jing, Yuchen Yan, Kaize Ding, Chanyoung Park, Yada Zhu, Huan Liu
  and Hanghang Tong
Categories: cs.LG
Comments: Accepted by AAAI'2024
\\ ( https://arxiv.org/abs/2302.05428 ,  2252kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11628
replaced with revised version Sun, 11 Feb 2024 01:24:31 GMT   (341kb,D)

Title: Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks
Authors: Zayd Hammoudeh and Daniel Lowd
Categories: cs.LG
Comments: Accepted at AAAI 2024 -- Extended version including the supplementary
  material
\\ ( https://arxiv.org/abs/2302.11628 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13053
replaced with revised version Sun, 11 Feb 2024 08:52:32 GMT   (1394kb,D)

Title: Scalable Neural Network Training over Distributed Graphs
Authors: Aashish Kolluri, Sarthak Choudhary, Bryan Hooi, Prateek Saxena
Categories: cs.LG cs.AI cs.IR
\\ ( https://arxiv.org/abs/2302.13053 ,  1394kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08253
replaced with revised version Sun, 11 Feb 2024 19:04:48 GMT   (2620kb,D)

Title: R2 Loss: Range Restriction Loss for Model Compression and Quantization
Authors: Arnav Kundu, Chungkuk Yoo, Srijan Mishra, Minsik Cho, Saurabh Adya
Categories: cs.LG cs.CV cs.PF eess.IV
\\ ( https://arxiv.org/abs/2303.08253 ,  2620kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08757
replaced with revised version Mon, 12 Feb 2024 17:16:32 GMT   (4992kb,D)

Title: Physics Informed Token Transformer for Solving Partial Differential
  Equations
Authors: Cooper Lorsung, Zijie Li, Amir Barati Farimani
Categories: cs.LG physics.comp-ph
Comments: 23 pages, 5 figures
\\ ( https://arxiv.org/abs/2305.08757 ,  4992kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10181
replaced with revised version Mon, 12 Feb 2024 03:46:59 GMT   (6773kb,D)

Title: Exploring the cloud of feature interaction scores in a Rashomon set
Authors: Sichao Li, Rong Wang, Quanling Deng, Amanda Barnard
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.10181 ,  6773kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10818
replaced with revised version Mon, 12 Feb 2024 09:34:39 GMT   (8761kb,D)

Title: Diffusion Language Models Generation Can Be Halted Early
Authors: Sofia Maria Lo Cicero Vaina, Nikita Balagansky, Daniil Gavrilov
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2305.10818 ,  8761kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16179
replaced with revised version Sun, 11 Feb 2024 09:35:03 GMT   (162kb)

Title: Dropout Drops Double Descent
Authors: Tian-Le Yang, Joe Suzuki
Categories: cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2305.16179 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18377
replaced with revised version Mon, 12 Feb 2024 12:06:40 GMT   (10659kb,D)

Title: BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise
  Learning
Authors: Jingfeng Zhang, Bo Song, Haohan Wang, Bo Han, Tongliang Liu, Lei Liu,
  Masashi Sugiyama
Categories: cs.LG cs.CV
Comments: IEEE T-PAMI 2024 Accept
DOI: 10.1109/TPAMI.2024.3355425
\\ ( https://arxiv.org/abs/2305.18377 ,  10659kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19259
replaced with revised version Mon, 12 Feb 2024 12:51:54 GMT   (853kb,D)

Title: On Convergence of Incremental Gradient for Non-Convex Smooth Functions
Authors: Anastasia Koloskova, Nikita Doikov, Sebastian U. Stich, Martin Jaggi
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2305.19259 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00809
replaced with revised version Mon, 12 Feb 2024 12:48:53 GMT   (8152kb,D)

Title: Initial Guessing Bias: How Untrained Networks Favor Some Classes
Authors: Emanuele Francazi, Aurelien Lucchi, Marco Baity-Jesi
Categories: cs.LG cond-mat.dis-nn stat.ML
\\ ( https://arxiv.org/abs/2306.00809 ,  8152kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04120
replaced with revised version Sat, 10 Feb 2024 05:33:21 GMT   (2354kb,D)

Title: MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY
  Estimation
Authors: Tony Tohme, Mohsen Sadr, Kamal Youcef-Toumi, Nicolas G.
  Hadjiconstantinou
Categories: cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH
Comments: Published in Transactions on Machine Learning Research (TMLR)
\\ ( https://arxiv.org/abs/2306.04120 ,  2354kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04288
replaced with revised version Mon, 12 Feb 2024 11:49:57 GMT   (2368kb,D)

Title: Revising deep learning methods in parking lot occupancy detection
Authors: Anastasia Martynova, Mikhail Kuznetsov, Vadim Porvatov, Vladislav
  Tishin, Andrey Kuznetsov, Natalia Semenova, Ksenia Kuznetsova
Categories: cs.LG cs.CV
Comments: 15 pages, 7 figures
\\ ( https://arxiv.org/abs/2306.04288 ,  2368kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05859
replaced with revised version Mon, 12 Feb 2024 11:19:09 GMT   (11947kb,D)

Title: Bring Your Own (Non-Robust) Algorithm to Solve Robust MDPs by Estimating
  The Worst Kernel
Authors: Kaixin Wang, Uri Gadot, Navdeep Kumar, Kfir Levy, Shie Mannor
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.05859 ,  11947kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07850
replaced with revised version Sun, 11 Feb 2024 13:44:39 GMT   (357kb,D)

Title: Exact Mean Square Linear Stability Analysis for SGD
Authors: Rotem Mulayoff, Tomer Michaeli
Categories: cs.LG
Comments: Preprint, revised
\\ ( https://arxiv.org/abs/2306.07850 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11336
replaced with revised version Mon, 12 Feb 2024 08:06:28 GMT   (16107kb,D)

Title: Cooperative Multi-Agent Learning for Navigation via Structured State
  Abstraction
Authors: Mohamed K. Abdelaziz, Mohammed S. Elbamby, Sumudu Samarakoon, Mehdi
  Bennis
Categories: cs.LG cs.MA
Comments: Double columns, 10 Pages, 12 Figures, Accepted for publication in
  IEEE TCOM
\\ ( https://arxiv.org/abs/2306.11336 ,  16107kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14009
replaced with revised version Mon, 12 Feb 2024 16:51:41 GMT   (1638kb,D)

Title: Boosting Multitask Learning on Graphs through Higher-Order Task
  Affinities
Authors: Dongyue Li, Haotian Ju, Aneesh Sharma, and Hongyang R. Zhang
Categories: cs.LG cs.SI
Comments: 15 pages. Appeared in KDD 2023
DOI: 10.1145/3580305.3599265
\\ ( https://arxiv.org/abs/2306.14009 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14043
replaced with revised version Sat, 10 Feb 2024 14:22:32 GMT   (14212kb,D)

Title: Machine Learning needs Better Randomness Standards: Randomised Smoothing
  and PRNG-based attacks
Authors: Pranav Dahiya, Ilia Shumailov, Ross Anderson
Categories: cs.LG cs.AI cs.CR
Comments: USENIX Security 2024
  (https://www.usenix.org/conference/usenixsecurity24/presentation/dahiya)
\\ ( https://arxiv.org/abs/2306.14043 ,  14212kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02245
replaced with revised version Mon, 12 Feb 2024 13:40:23 GMT   (12591kb,D)

Title: Set Learning for Accurate and Calibrated Models
Authors: Lukas Muttenthaler and Robert A. Vandermeulen and Qiuyi Zhang and
  Thomas Unterthiner and Klaus-Robert M\"uller
Categories: cs.LG cs.CV cs.IT math.IT
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2307.02245 ,  12591kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07084
replaced with revised version Sat, 10 Feb 2024 21:28:19 GMT   (18448kb,D)

Title: Safe Reinforcement Learning as Wasserstein Variational Inference: Formal
  Methods for Interpretability
Authors: Yanran Wang, Qiuchen Qian, David Boyle
Categories: cs.LG cs.AI cs.RO cs.SY eess.SY
Comments: 22 pages, 7 figures, containing Appendix
\\ ( https://arxiv.org/abs/2307.07084 ,  18448kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11280
replaced with revised version Fri, 9 Feb 2024 23:32:58 GMT   (1691kb,D)

Title: Epsilon*: Privacy Metric for Machine Learning Models
Authors: Diana M. Negoescu, Humberto Gonzalez, Saad Eddin Al Orjany, Jilei
  Yang, Yuliia Lut, Rahul Tandra, Xiaowen Zhang, Xinyi Zheng, Zach Douglas,
  Vidita Nolkha, Parvez Ahammad, Gennady Samorodnitsky
Categories: cs.LG cs.CR cs.DS
\\ ( https://arxiv.org/abs/2307.11280 ,  1691kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13771
replaced with revised version Mon, 12 Feb 2024 10:49:28 GMT   (388kb)

Title: Accuracy Improvement in Differentially Private Logistic Regression: A
  Pre-training Approach
Authors: Mohammad Hoseinpour, Milad Hoseinpour, Ali Aghagolzadeh
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2307.13771 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01107
replaced with revised version Mon, 12 Feb 2024 11:23:29 GMT   (4310kb,D)

Title: Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization
Authors: Uri Gadot, Esther Derman, Navdeep Kumar, Maxence Mohamed Elfatihi,
  Kfir Levy, Shie Mannor
Categories: cs.LG
Comments: accepted in AAAI2024
\\ ( https://arxiv.org/abs/2309.01107 ,  4310kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04344
replaced with revised version Mon, 12 Feb 2024 17:15:52 GMT   (2928kb,D)

Title: Zero-Shot Robustification of Zero-Shot Models
Authors: Dyah Adila, Changho Shin, Linrong Cai, Frederic Sala
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR), 2024
\\ ( https://arxiv.org/abs/2309.04344 ,  2928kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07708
replaced with revised version Sat, 10 Feb 2024 07:30:39 GMT   (8647kb,D)

Title: Market-GAN: Adding Control to Financial Market Data Generation with
  Semantic Context
Authors: Haochong Xia, Shuo Sun, Xinrun Wang, Bo An
Categories: cs.LG q-fin.TR
Comments: Accepted to the 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI24), Vancouver, British Columbia, 2024
\\ ( https://arxiv.org/abs/2309.07708 ,  8647kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15048
replaced with revised version Sun, 11 Feb 2024 15:55:15 GMT   (3251kb,D)

Title: Class Incremental Learning via Likelihood Ratio Based Task Prediction
Authors: Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, Bing Liu
Categories: cs.LG cs.AI cs.CV
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2309.15048 ,  3251kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16457
replaced with revised version Sun, 11 Feb 2024 17:49:49 GMT   (20550kb,D)

Title: Universal Sleep Decoder: Aligning awake and sleep neural representation
  across subjects
Authors: Zhongtao Chen, Hui Zheng, Haiteng Wang, Jianyang Zhou, Lin Zheng,
  Yunzhe Liu
Categories: cs.LG eess.SP q-bio.NC
\\ ( https://arxiv.org/abs/2309.16457 ,  20550kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17348
replaced with revised version Sun, 11 Feb 2024 14:49:58 GMT   (482kb,D)

Title: Intrinsic Biologically Plausible Adversarial Training
Authors: Matilde Tristany Farinha, Thomas Ortner, Giorgia Dellaferrera,
  Benjamin Grewe, Angeliki Pantazi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.17348 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00290
replaced with revised version Mon, 12 Feb 2024 03:46:52 GMT   (8kb)

Title: Universality of almost periodicity in bounded discrete time series
Authors: Tsuyoshi Yoneda
Categories: cs.LG cs.IT math.AP math.DS math.FA math.IT
\\ ( https://arxiv.org/abs/2310.00290 ,  8kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02975
replaced with revised version Mon, 12 Feb 2024 10:39:44 GMT   (59kb)

Title: $(\epsilon, u)$-Adaptive Regret Minimization in Heavy-Tailed Bandits
Authors: Gianmarco Genalti and Lupo Marsigli and Nicola Gatti and Alberto Maria
  Metelli
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.02975 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03946
replaced with revised version Mon, 12 Feb 2024 00:49:26 GMT   (14627kb,D)

Title: Improved prediction of ligand-protein binding affinities by
  meta-modeling
Authors: Ho-Joon Lee, Prashant S. Emani, and Mark B. Gerstein
Categories: cs.LG q-bio.QM
Comments: 62 pages, 4 main tables, 6 main figures, 7 supplementary figures, and
  supporting information. For 9 supplementary tables and code, see
  https://github.com/Lee1701/Lee2023a
\\ ( https://arxiv.org/abs/2310.03946 ,  14627kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04017
replaced with revised version Sun, 11 Feb 2024 05:06:47 GMT   (740kb,D)

Title: PGraphDTA: Improving Drug Target Interaction Prediction using Protein
  Language Models and Contact Maps
Authors: Rakesh Bal, Yijia Xiao, Wei Wang
Categories: cs.LG q-bio.QM
Comments: AI for Science Workshop, NeurIPS 2023. 11 pages, 5 figures, 4 tables
\\ ( https://arxiv.org/abs/2310.04017 ,  740kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04470
replaced with revised version Sat, 10 Feb 2024 18:15:46 GMT   (9980kb,D)

Title: Hierarchical Multi-Marginal Optimal Transport for Network Alignment
Authors: Zhichen Zeng, Boxin Du, Si Zhang, Yinglong Xia, Zhining Liu, Hanghang
  Tong
Categories: cs.LG cs.AI
Comments: 14 pages, 10 figures
\\ ( https://arxiv.org/abs/2310.04470 ,  9980kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05842
replaced with revised version Mon, 12 Feb 2024 18:40:40 GMT   (42724kb,D)

Title: Robust Angular Synchronization via Directed Graph Neural Networks
Authors: Yixuan He, Gesine Reinert, David Wipf, Mihai Cucuringu
Categories: cs.LG math.OC stat.ML
Comments: 9 pages for main text, ICLR 2024
\\ ( https://arxiv.org/abs/2310.05842 ,  42724kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08091
replaced with revised version Sat, 10 Feb 2024 14:27:29 GMT   (758kb,D)

Title: Discerning Temporal Difference Learning
Authors: Jianfei Ma
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.08091 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09656
replaced with revised version Mon, 12 Feb 2024 07:33:25 GMT   (1592kb,D)

Title: Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent
  Space
Authors: Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan
  Shen, Xiao Qin, Christos Faloutsos, Huzefa Rangwala and George Karypis
Categories: cs.LG
Comments: Accepted by ICLR 2024 (Oral Presentation)
\\ ( https://arxiv.org/abs/2310.09656 ,  1592kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09877
replaced with revised version Mon, 12 Feb 2024 17:00:20 GMT   (288kb,D)

Title: Statistical inference using machine learning and classical techniques
  based on accumulated local effects (ALE)
Authors: Chitu Okoli
Categories: cs.LG cs.AI
Comments: 1. P-values have been added to the analysis. 2. Neural network
  demonstration replaced with random forest
\\ ( https://arxiv.org/abs/2310.09877 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13283
replaced with revised version Sun, 11 Feb 2024 11:38:38 GMT   (3429kb,D)

Title: pFedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA
  Tuning
Authors: Liping Yi, Han Yu, Gang Wang, Xiaoguang Liu, Xiaoxiao Li
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2310.13283 ,  3429kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15080
replaced with revised version Sun, 11 Feb 2024 11:59:52 GMT   (188kb,D)

Title: Federated Learning of Large Language Models with Parameter-Efficient
  Prompt Tuning and Adaptive Optimization
Authors: Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S.
  Sheng, Huaiyu Dai, Dejing Dou
Categories: cs.LG cs.CL cs.DC
Comments: 18 pages, accepted by EMNLP 2023
\\ ( https://arxiv.org/abs/2310.15080 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16363
replaced with revised version Sat, 10 Feb 2024 17:44:51 GMT   (430kb,D)

Title: Finite Time Analysis of Constrained Actor Critic and Constrained Natural
  Actor Critic Algorithms
Authors: Prashansa Panda, Shalabh Bhatnagar
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.16363 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00664
replaced with revised version Sun, 11 Feb 2024 11:08:13 GMT   (4343kb,D)

Title: Latent Space Translation via Semantic Alignment
Authors: Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero,
  Francesco Locatello, Emanuele Rodol\`a
Categories: cs.LG
Comments: Accepted at NeurIPS 2023. 21 pages, 13 figures, 8 tables
\\ ( https://arxiv.org/abs/2311.00664 ,  4343kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00886
replaced with revised version Mon, 12 Feb 2024 07:38:58 GMT   (2944kb,D)

Title: COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised
  Learning
Authors: Chuizheng Meng, Yihe Dong, Sercan \"O. Ar{\i}k, Yan Liu, Tomas Pfister
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.00886 ,  2944kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01047
replaced with revised version Mon, 12 Feb 2024 17:35:44 GMT   (1182kb,D)

Title: Improving Robustness via Tilted Exponential Layer: A
  Communication-Theoretic Perspective
Authors: Bhagyashree Puranik, Ahmad Beirami, Yao Qin, Upamanyu Madhow
Categories: cs.LG cs.IT eess.SP math.IT
Comments: This manuscript has been accepted for publication at the 27th
  International Conference on Artificial Intelligence and Statistics (AISTATS),
  2024
\\ ( https://arxiv.org/abs/2311.01047 ,  1182kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02017
replaced with revised version Sun, 11 Feb 2024 06:29:38 GMT   (7918kb,D)

Title: DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network
  for Food Deliveries
Authors: Ashman Mehra, Snehanshu Saha, Vaskar Raychoudhury, Archana Mathur
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.02017 ,  7918kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12244
replaced with revised version Sun, 11 Feb 2024 22:28:11 GMT   (1561kb,D)

Title: Efficient Reinforcement Learning from Partial Observability
Authors: Hongming Zhang, Tongzheng Ren, Chenjun Xiao, Dale Schuurmans, Bo Dai
Categories: cs.LG cs.AI stat.ML
Comments: The first two authors contribute equally
\\ ( https://arxiv.org/abs/2311.12244 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13541
replaced with revised version Mon, 12 Feb 2024 14:38:17 GMT   (2908kb,D)

Title: Linear Log-Normal Attention with Unbiased Concentration
Authors: Yury Nahshan, Joseph Kampeas and Emir Haleva
Categories: cs.LG cs.AI
Comments: 22 pages, 20 figures, 5 tables, submitted to ICLR2024
ACM-class: I.7.0; G.3
\\ ( https://arxiv.org/abs/2311.13541 ,  2908kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14361
replaced with revised version Mon, 12 Feb 2024 14:45:16 GMT   (1293kb)

Title: Deciphering and integrating invariants for neural operator learning with
  various physical mechanisms
Authors: Rui Zhang, Qi Meng, Zhi-Ming Ma
Categories: cs.LG cs.NA math.NA physics.comp-ph
DOI: 10.1093/nsr/nwad336
\\ ( https://arxiv.org/abs/2311.14361 ,  1293kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16526
replaced with revised version Sat, 10 Feb 2024 13:11:49 GMT   (282kb,D)

Title: On robust overfitting: adversarial training induced distribution matters
Authors: Runzhi Tian, Yongyi Mao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.16526 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18575
replaced with revised version Mon, 12 Feb 2024 16:10:37 GMT   (2153kb,D)

Title: Class Distribution Shifts in Zero-Shot Learning: Learning Robust
  Representations
Authors: Yuli Slavutsky and Yuval Benjamini
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.18575 ,  2153kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03096
replaced with revised version Sun, 11 Feb 2024 00:52:49 GMT   (881kb,D)

Title: What Causes Polysemanticity? An Alternative Origin Story of Mixed
  Selectivity from Incidental Causes
Authors: Victor Lecomte, Kushal Thaman, Trevor Chow, Rylan Schaeffer, Sanmi
  Koyejo
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2312.03096 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05974
replaced with revised version Mon, 12 Feb 2024 14:17:39 GMT   (793kb,D)

Title: Learning the Causal Structure of Networked Dynamical Systems under
  Latent Nodes and Structured Noise
Authors: Augusto Santos, Diogo Rente, Rui Seabra and Jos\'e M. F. Moura
Categories: cs.LG stat.ME
Comments: Accepted at The 38th AAAI Conference on Artificial Intelligence (Main
  Track). Final Camera-Ready Version
MSC-class: 62D20, 93B30
ACM-class: I.2.m; G.3
\\ ( https://arxiv.org/abs/2312.05974 ,  793kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09006
replaced with revised version Sun, 11 Feb 2024 14:09:28 GMT   (3755kb,D)

Title: FedSSA: Semantic Similarity-based Aggregation for Efficient
  Model-Heterogeneous Personalized Federated Learning
Authors: Liping Yi, Han Yu, Zhuan Shi, Gang Wang, Xiaoguang Liu, Lizhen Cui,
  Xiaoxiao Li
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2312.09006 ,  3755kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12869
replaced with revised version Mon, 12 Feb 2024 15:42:30 GMT   (11178kb,D)

Title: Parameterized Projected Bellman Operator
Authors: Th\'eo Vincent, Alberto Maria Metelli, Boris Belousov, Jan Peters,
  Marcello Restelli and Carlo D'Eramo
Categories: cs.LG cs.AI
Comments: Proceedings of the National Conference on Artificial Intelligence
  (AAAI-24)
\\ ( https://arxiv.org/abs/2312.12869 ,  11178kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13327
replaced with revised version Fri, 9 Feb 2024 19:36:32 GMT   (2949kb,D)

Title: In-Context Reinforcement Learning for Variable Action Spaces
Authors: Viacheslav Sinii, Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman,
  Sergey Kolesnikov
Categories: cs.LG cs.AI
Comments: Preprint, Under Review; code:
  https://github.com/corl-team/headless-ad
\\ ( https://arxiv.org/abs/2312.13327 ,  2949kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00547
replaced with revised version Sun, 11 Feb 2024 06:07:17 GMT   (0kb,I)

Title: On Learning for Ambiguous Chance Constrained Problems
Authors: A Ch Madhusudanarao, Rahul Singh
Categories: cs.LG math.OC
Comments: We have "not considered the uniform bound" for violation
  probabilities corresponding to the set of distributions in the ambiguity set
\\ ( https://arxiv.org/abs/2401.00547 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01479
replaced with revised version Mon, 12 Feb 2024 03:50:55 GMT   (779kb,D)

Title: Kernel-U-Net: Symmetric and Hierarchical Architecture for Multivariate
  Time Series Forecasting
Authors: Jiang You, Ren\'e Natowicz, Arben Cela, Jacob Ouanounou, Patrick
  Siarry
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.01479 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02333
replaced with revised version Sat, 10 Feb 2024 12:35:22 GMT   (944kb,D)

Title: Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models
Authors: Uday Allu, Biddwan Ahmed, Vishesh Tripathi
Categories: cs.LG cs.CL
Comments: Submitted to IEEE
\\ ( https://arxiv.org/abs/2401.02333 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08513
replaced with revised version Mon, 12 Feb 2024 14:53:33 GMT   (4108kb,D)

Title: X Hacking: The Threat of Misguided AutoML
Authors: Rahul Sharma, Sergey Redyuk, Sumantrak Mukherjee, Andrea Sipka,
  Sebastian Vollmer, David Selby
Categories: cs.LG cs.CR
Comments: 13 pages, 8 figures, plus supplementary materials
\\ ( https://arxiv.org/abs/2401.08513 ,  4108kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09192
replaced with revised version Sat, 10 Feb 2024 14:52:49 GMT   (1089kb,D)

Title: Preparing Lessons for Progressive Training on Language Models
Authors: Yu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, Ming Zhang,
  Lifeng Shang, Xin Jiang, Qun Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.09192 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10386
replaced with revised version Mon, 12 Feb 2024 07:28:33 GMT   (460kb,D)

Title: Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest
  Machine Learning
Authors: Zaina Abu Hweij, Florence Liang, Sophie Zhang
Categories: cs.LG eess.SP physics.med-ph
\\ ( https://arxiv.org/abs/2401.10386 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12012
replaced with revised version Sun, 11 Feb 2024 12:09:51 GMT   (10256kb,D)

Title: TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for
  Lazy Clients
Authors: Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci
Categories: cs.LG cs.DC
Comments: Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
\\ ( https://arxiv.org/abs/2401.12012 ,  10256kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14876
replaced with revised version Sat, 10 Feb 2024 08:58:14 GMT   (3650kb,D)

Title: Cross-Space Adaptive Filter: Integrating Graph Topology and Node
  Attributes for Alleviating the Over-smoothing Problem
Authors: Chen Huang, Haoyang Li, Yifan Zhang, Wenqiang Lei, Jiancheng Lv
Categories: cs.LG cs.AI
Comments: Accepted to WWW 2024. V2: update the results on GCN-BC based on our
  rebuttal on OpenReview. Our code is available at
  https://github.com/huangzichun/Cross-Space-Adaptive-Filter
\\ ( https://arxiv.org/abs/2401.14876 ,  3650kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15292
replaced with revised version Sun, 11 Feb 2024 01:26:47 GMT   (1270kb,D)

Title: Adaptive Block Sparse Regularization under Arbitrary Linear Transform
Authors: Takanobu Furuhashi, Hidekata Hontani, Tatsuya Yokota
Categories: cs.LG eess.SP
Comments: 5 pages, 4 figures
\\ ( https://arxiv.org/abs/2401.15292 ,  1270kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16327
replaced with revised version Mon, 12 Feb 2024 17:20:32 GMT   (5737kb,D)

Title: PICL: Physics Informed Contrastive Learning for Partial Differential
  Equations
Authors: Cooper Lorsung and Amir Barati Farimani
Categories: cs.LG cs.NA math.NA physics.comp-ph
Comments: 16 pages, 4 figures
\\ ( https://arxiv.org/abs/2401.16327 ,  5737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16852
replaced with revised version Sat, 10 Feb 2024 09:37:50 GMT   (2766kb,D)

Title: Checkmating One, by Using Many: Combining Mixture of Experts with MCTS
  to Improve in Chess
Authors: Felix Helfenstein, Jannis Bl\"uml, Johannes Czech and Kristian
  Kersting
Categories: cs.LG
Comments: Code available under https://github.com/HelpstoneX/CrazyAra
\\ ( https://arxiv.org/abs/2401.16852 ,  2766kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17823
replaced with revised version Mon, 12 Feb 2024 11:58:23 GMT   (151kb,D)

Title: Privacy-preserving data release leveraging optimal transport and
  particle gradient descent
Authors: Konstantin Donhauser and Javier Abad and Neha Hulkund and Fanny Yang
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2401.17823 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00326
replaced with revised version Sun, 11 Feb 2024 22:53:59 GMT   (2784kb,D)

Title: PirateNets: Physics-informed Deep Learning with Residual Adaptive
  Networks
Authors: Sifan Wang, Bowen Li, Yuhan Chen, Paris Perdikaris
Categories: cs.LG cs.NA math.NA
Comments: 30Pages, 15 Figures, 8 Tables
\\ ( https://arxiv.org/abs/2402.00326 ,  2784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00654
replaced with revised version Mon, 12 Feb 2024 15:16:39 GMT   (3814kb)

Title: Improving the accuracy of freight mode choice models: A case study using
  the 2017 CFS PUF data set and ensemble learning techniques
Authors: Diyi Liu, Hyeonsup Lim, Majbah Uddin, Yuandong Liu, Lee D. Han,
  Ho-ling Hwang, Shih-Miao Chin
Categories: cs.LG
Journal-ref: Expert Systems with Applications, 240, 122478 (2024)
DOI: 10.1016/j.eswa.2023.122478
\\ ( https://arxiv.org/abs/2402.00654 ,  3814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01350
replaced with revised version Sun, 11 Feb 2024 06:33:43 GMT   (3127kb,D)

Title: pFedMoE: Data-Level Personalization with Mixture of Experts for
  Model-Heterogeneous Personalized Federated Learning
Authors: Liping Yi, Han Yu, Chao Ren, Heng Zhang, Gang Wang, Xiaoguang Liu,
  Xiaoxiao Li
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2402.01350 ,  3127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02317
replaced with revised version Mon, 12 Feb 2024 14:02:31 GMT   (9766kb,D)

Title: INViT: A Generalizable Routing Problem Solver with Invariant Nested View
  Transformer
Authors: Han Fang, Zhihao Song, Paul Weng, Yutong Ban
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.02317 ,  9766kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02342
replaced with revised version Mon, 12 Feb 2024 04:19:19 GMT   (10948kb,D)

Title: MetaOptimize: A Framework for Optimizing Step Sizes and Other
  Meta-parameters
Authors: Arsalan Sharifnassab, Saber Salehkaleybar, Richard Sutton
Categories: cs.LG cs.AI math.OC
\\ ( https://arxiv.org/abs/2402.02342 ,  10948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02460
replaced with revised version Mon, 12 Feb 2024 01:10:12 GMT   (318kb,D)

Title: Review of multimodal machine learning approaches in healthcare
Authors: Felix Krones, Umar Marikkar, Guy Parsons, Adam Szmul, Adam Mahdi
Categories: cs.LG cs.AI cs.CV
Comments: 5 figures, 5 tables
\\ ( https://arxiv.org/abs/2402.02460 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02823
replaced with revised version Mon, 12 Feb 2024 17:50:07 GMT   (1414kb,D)

Title: Evading Data Contamination Detection for Language Models is (too) Easy
Authors: Jasper Dekoninck, Mark Niklas M\"uller, Maximilian Baader, Marc
  Fischer, Martin Vechev
Categories: cs.LG cs.AI cs.CL cs.CR
\\ ( https://arxiv.org/abs/2402.02823 ,  1414kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03570
replaced with revised version Sun, 11 Feb 2024 17:33:16 GMT   (15640kb,D)

Title: Diffusion World Model
Authors: Zihan Ding, Amy Zhang, Yuandong Tian, Qinqing Zheng
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.03570 ,  15640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04384
replaced with revised version Sat, 10 Feb 2024 19:19:34 GMT   (911kb,D)

Title: Denoising Diffusion Probabilistic Models in Six Simple Steps
Authors: Richard E. Turner, Cristiana-Diana Diaconu, Stratis Markou,
  Aliaksandra Shysheya, Andrew Y. K. Foong and Bruno Mlodozeniec
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.04384 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05147
replaced with revised version Mon, 12 Feb 2024 15:09:39 GMT   (17690kb,D)

Title: ApiQ: Finetuning of 2-Bit Quantized Large Language Model
Authors: Baohao Liao, Christof Monz
Categories: cs.LG cs.CL
Comments: compared to v0: new histogram formats for better reading
\\ ( https://arxiv.org/abs/2402.05147 ,  17690kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05290
replaced with revised version Sun, 11 Feb 2024 00:50:25 GMT   (2195kb,D)

Title: Do Transformer World Models Give Better Policy Gradients?
Authors: Michel Ma, Tianwei Ni, Clement Gehring, Pierluca D'Oro, Pierre-Luc
  Bacon
Categories: cs.LG cs.AI
Comments: Michel Ma and Pierluca D'Oro contributed equally
\\ ( https://arxiv.org/abs/2402.05290 ,  2195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05406
replaced with revised version Fri, 9 Feb 2024 19:53:56 GMT   (2072kb,D)

Title: Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
Authors: Lucio Dery, Steven Kolawole, Jean-Fran\c{c}ois Kagy, Virginia Smith,
  Graham Neubig, Ameet Talwalkar
Categories: cs.LG cs.CL
Comments: 15 pages, 4 fiigures, 15 tables
\\ ( https://arxiv.org/abs/2402.05406 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05950
replaced with revised version Mon, 12 Feb 2024 10:55:07 GMT   (423kb,D)

Title: SQT -- std $Q$-target
Authors: Nitsan Soffair, Dotan Di-Castro, Orly Avner, Shie Mannor
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.05950 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05951
replaced with revised version Mon, 12 Feb 2024 10:53:10 GMT   (798kb,D)

Title: MinMaxMin $Q$-learning
Authors: Nitsan Soffair, Shie Mannor
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.05951 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06187
replaced with revised version Mon, 12 Feb 2024 03:35:05 GMT   (8614kb,D)

Title: Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask
  Representation via Temporal Action-Driven Contrastive Loss
Authors: Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Shuang Ma, Hal Daum\'e III,
  Huazhe Xu, John Langford, Praveen Palanisamy, Kalyan Shankar Basu, Furong
  Huang
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.06187 ,  8614kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11694
replaced with revised version Sat, 10 Feb 2024 22:13:42 GMT   (13025kb,D)

Title: Trustworthy Reinforcement Learning for Quadrotor UAV Tracking Control
  Systems
Authors: Yanran Wang and David Boyle
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: 17 pages, 9 figures. arXiv admin note: substantial text overlap with
  arXiv:2205.07150
\\ ( https://arxiv.org/abs/2302.11694 ,  13025kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00521
replaced with revised version Mon, 12 Feb 2024 07:17:38 GMT   (8384kb,D)

Title: StyleLipSync: Style-based Personalized Lip-sync Video Generation
Authors: Taekyung Ki and Dongchan Min
Categories: cs.CV cs.AI cs.LG
Comments: International Conference on Computer Vision (ICCV) 2023. Project
  page: https://stylelipsync.github.io
\\ ( https://arxiv.org/abs/2305.00521 ,  8384kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15596
replaced with revised version Mon, 12 Feb 2024 18:03:29 GMT   (4112kb,D)

Title: Distributed Online Rollout for Multivehicle Routing in Unmapped
  Environments
Authors: Jamison W. Weber, Dhanush R. Giriyan, Devendra R. Parkar, Andr\'ea W.
  Richa, Dimitri P. Bertsekas
Categories: cs.DC cs.AI cs.MA
\\ ( https://arxiv.org/abs/2305.15596 ,  4112kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01745 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 16:34:56 GMT   (7114kb,D)

Title: Biomarker Discovery with Quantum Neural Networks: A Case-study in
  CTLA4-Activation Pathways
Authors: Nam Nguyen
Categories: q-bio.QM cs.AI
\\ ( https://arxiv.org/abs/2306.01745 ,  7114kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01798
replaced with revised version Sat, 10 Feb 2024 14:13:43 GMT   (1672kb)

Title: Exploring EFL students' prompt engineering in human-AI story writing: an
  Activity Theory perspective
Authors: David James Woo, Kai Guo, Hengky Susanto
Categories: cs.CY cs.AI
Comments: 44 pages, 9 figures
\\ ( https://arxiv.org/abs/2306.01798 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04959
replaced with revised version Fri, 9 Feb 2024 19:57:05 GMT   (7378kb,D)

Title: FedMLSecurity: A Benchmark for Attacks and Defenses in Federated
  Learning and Federated LLMs
Authors: Shanshan Han, Baturalp Buyukates, Zijian Hu, Han Jin, Weizhao Jin,
  Lichao Sun, Xiaoyang Wang, Wenxuan Wu, Chulin Xie, Yuhang Yao, Kai Zhang,
  Qifan Zhang, Yuhui Zhang, Carlee Joe-Wong, Salman Avestimehr and Chaoyang He
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2306.04959 ,  7378kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07458
replaced with revised version Sun, 11 Feb 2024 16:22:40 GMT   (2806kb,D)

Title: Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time
  Pressure
Authors: Siddharth Swaroop, Zana Bu\c{c}inca, Krzysztof Z. Gajos, Finale
  Doshi-Velez
Categories: cs.HC cs.AI
DOI: 10.1145/3640543.3645206
\\ ( https://arxiv.org/abs/2306.07458 ,  2806kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13092
replaced with revised version Sun, 11 Feb 2024 20:34:51 GMT   (5204kb,D)

Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale
  From A New Perspective
Authors: Zeyuan Yin and Eric Xing and Zhiqiang Shen
Categories: cs.CV cs.AI cs.LG
Comments: NeurIPS 2023 spotlight. Code at https://github.com/VILA-Lab/SRe2L
\\ ( https://arxiv.org/abs/2306.13092 ,  5204kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08303
replaced with revised version Sat, 10 Feb 2024 22:46:29 GMT   (232kb,D)

Title: Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language
  Models
Authors: Zhiyuan Peng, Xuyang Wu, Qifan Wang, Yi Fang
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Under Reivew
\\ ( https://arxiv.org/abs/2307.08303 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08930
replaced with revised version Sun, 11 Feb 2024 06:36:42 GMT   (8876kb,D)

Title: Unsupervised Deep Graph Matching Based on Cycle Consistency
Authors: Siddharth Tourani, Carsten Rother and Muhammad Haris Khan and Bogdan
  Savchynskyy
Categories: cs.CV cs.AI
Comments: 12 pages, 5 figures, 3 papers
\\ ( https://arxiv.org/abs/2307.08930 ,  8876kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08925
replaced with revised version Fri, 9 Feb 2024 23:47:02 GMT   (492kb,D)

Title: A White-Box False Positive Adversarial Attack Method on Contrastive Loss
  Based Offline Handwritten Signature Verification Models
Authors: Zhongliang Guo, Weiye Li, Yifei Qian, Ognjen Arandjelovi\'c, Lei Fang
Categories: cs.CV cs.AI
Comments: 8 pages, 3 figures, 2 tables, accepted by the Proceedings of the 27th
  International Conference on Artificial Intelligence and Statistics (AISTATS)
  2024
\\ ( https://arxiv.org/abs/2308.08925 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01845
replaced with revised version Sun, 11 Feb 2024 14:28:09 GMT   (39343kb,D)

Title: Zero-Shot Refinement of Buildings' Segmentation Models using SAM
Authors: Ali Mayladan, Hasan Nasrallah, Hasan Moughnieh, Mustafa Shukor and Ali
  J. Ghandour
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2310.01845 ,  39343kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09298
replaced with revised version Mon, 12 Feb 2024 08:01:41 GMT   (7857kb,D)

Title: ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency
  for Grayscale Image-based Network Intrusion Detection
Authors: Irfan Khan, Yasir Ali Farrukh and Syed Wali
Categories: cs.CR cs.AI cs.LG
Comments: 6 pages, 6 figures
\\ ( https://arxiv.org/abs/2310.09298 ,  7857kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09401
replaced with revised version Sun, 11 Feb 2024 03:38:45 GMT   (1318kb,D)

Title: CROWN: A Novel Approach to Comprehending Users' Preferences for Accurate
  Personalized News Recommendation
Authors: Yunyong Ko, Seongeun Ryu, Sang-Wook Kim
Categories: cs.IR cs.AI
Comments: 10 pages, 6 figures, 8 tables
\\ ( https://arxiv.org/abs/2310.09401 ,  1318kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12609
replaced with revised version Mon, 12 Feb 2024 07:50:24 GMT   (5804kb)

Title: Denoising Heat-inspired Diffusion with Insulators for Collision Free
  Motion Planning
Authors: Junwoo Chang, Hyunwoo Ryu, Jiwoo Kim, Soochul Yoo, Jongeun Choi,
  Joohwan Seo, Nikhil Prakash, Roberto Horowitz
Categories: cs.RO cs.AI cs.LG
Comments: 9 pages, 6 figures
Journal-ref: NeurIPS 2023 Workshop on Diffusion Models
\\ ( https://arxiv.org/abs/2310.12609 ,  5804kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01723
replaced with revised version Mon, 12 Feb 2024 02:57:26 GMT   (3951kb,D)

Title: Towards Calibrated Robust Fine-Tuning of Vision-Language Models
Authors: Changdae Oh, Hyesu Lim, Mijoo Kim, Jaegul Choo, Alexander Hauptmann,
  Zhi-Qi Cheng, Kyungwoo Song
Categories: cs.CV cs.AI
Comments: Presented at the NeurIPS 2023 Workshop on Distribution Shifts
  (DistShift)
\\ ( https://arxiv.org/abs/2311.01723 ,  3951kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12304
replaced with revised version Fri, 9 Feb 2024 22:22:58 GMT   (1686kb,D)

Title: Discovering Effective Policies for Land-Use Planning
Authors: Risto Miikkulainen, Olivier Francon, Daniel Young, Elliot Meyerson,
  Clemens Schwingshackl, Jacob Bieker, Hugo Cunha, and Babak Hodjat
Categories: cs.NE cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.12304 ,  1686kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15108
replaced with revised version Sun, 11 Feb 2024 06:07:19 GMT   (4687kb,D)

Title: Leveraging Diffusion Perturbations for Measuring Fairness in Computer
  Vision
Authors: Nicholas Lui, Bryan Chia, William Berrios, Candace Ross, Douwe Kiela
Categories: cs.CV cs.AI
Comments: The Appendix can be found at https://bit.ly/dp-appendix; Added link
  to code and fixed formatting (Feb 10 2024)
\\ ( https://arxiv.org/abs/2311.15108 ,  4687kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16466
replaced with revised version Mon, 12 Feb 2024 16:20:45 GMT   (4298kb,D)

Title: Large language models can enhance persuasion through linguistic feature
  alignment
Authors: Minkyu Shin and Jin Kim
Categories: cs.HC cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.16466 ,  4298kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16594
replaced with revised version Mon, 12 Feb 2024 01:53:33 GMT   (0kb,I)

Title: Monitor Placement for Fault Localization in Deep Neural Network
  Accelerators
Authors: Wei-Kai Liu
Categories: cs.AR cs.AI
Comments: Technical fallacies appear in this paper
\\ ( https://arxiv.org/abs/2311.16594 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00054 (*cross-listing*)
replaced with revised version Sat, 10 Feb 2024 07:38:32 GMT   (72kb,D)

Title: Is Inverse Reinforcement Learning Harder than Standard Reinforcement
  Learning? A Theoretical Perspective
Authors: Lei Zhao, Mengdi Wang, Yu Bai
Categories: stat.ML cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.00054 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04579
replaced with revised version Sun, 11 Feb 2024 19:00:20 GMT   (619kb,D)

Title: zkDFL: An efficient and privacy-preserving decentralized federated
  learning with zero-knowledge proof
Authors: Mojtaba Ahmadi, Reza Nourmohammadi
Categories: cs.CR cs.AI
Comments: 10 pages
\\ ( https://arxiv.org/abs/2312.04579 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15284
replaced with revised version Mon, 12 Feb 2024 05:11:56 GMT   (122kb)

Title: Five ethical principles for generative AI in scientific research
Authors: Zhicheng Lin
Categories: cs.CY cs.AI
Comments: 9 pages, 2 tables
\\ ( https://arxiv.org/abs/2401.15284 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15741
replaced with revised version Mon, 12 Feb 2024 09:37:18 GMT   (1053kb)

Title: SERNet-Former: Semantic Segmentation by Efficient Residual Network with
  Attention-Boosting Gates and Attention-Fusion Networks
Authors: Serdar Erisen
Categories: cs.CV cs.AI
DOI: 10.48550/arXiv.2401.15741
\\ ( https://arxiv.org/abs/2401.15741 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16450
replaced with revised version Sat, 10 Feb 2024 20:17:11 GMT   (801kb,D)

Title: ACCESS: Prompt Engineering for Automated Web Accessibility Violation
  Corrections
Authors: Calista Huang, Alyssa Ma, Suchir Vyasamudri, Eugenie Puype, Sayem
  Kamal, Juan Belza Garcia, Salar Cheema, Michael Lutz
Categories: cs.HC cs.AI cs.SE
Comments: 11 pages, 6 figures
\\ ( https://arxiv.org/abs/2401.16450 ,  801kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01345
replaced with revised version Mon, 12 Feb 2024 13:53:20 GMT   (393kb,D)

Title: Skip \n: A Simple Method to Reduce Hallucination in Large
  Vision-Language Models
Authors: Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike
  Zheng Shou
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Technical Report
\\ ( https://arxiv.org/abs/2402.01345 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03681
replaced with revised version Sat, 10 Feb 2024 20:49:55 GMT   (4569kb,D)

Title: RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model
  Feedback
Authors: Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David
  Held, Zackory Erickson
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03681 ,  4569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05301
replaced with revised version Fri, 9 Feb 2024 21:26:40 GMT   (979kb,D)

Title: BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and
  Parametric CAD Designs
Authors: Lyle Regenwetter, Yazan Abu Obaideh, Amin Heyrani Nobari, Faez Ahmed
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.05301 ,  979kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05902
replaced with revised version Sat, 10 Feb 2024 03:19:39 GMT   (1510kb)

Title: ClickSAM: Fine-tuning Segment Anything Model using click prompts for
  ultrasound image segmentation
Authors: Aimee Guo, Gace Fei, Hemanth Pasupuletic, and Jing Wang
Categories: cs.CV cs.AI physics.med-ph
Comments: 6 pages, 2 figures, SPIE Medical Imaging Conference 2024
\\ ( https://arxiv.org/abs/2402.05902 ,  1510kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13915
replaced with revised version Mon, 12 Feb 2024 18:19:55 GMT   (6866kb,D)

Title: DAPR: A Benchmark on Document-Aware Passage Retrieval
Authors: Kexin Wang, Nils Reimers, Iryna Gurevych
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2305.13915 ,  6866kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05329
replaced with revised version Mon, 12 Feb 2024 02:04:37 GMT   (195kb,D)

Title: Decoding the Popularity of TV Series: A Network Analysis Perspective
Authors: Melody Yu
Categories: cs.SI cs.CL cs.NI
\\ ( https://arxiv.org/abs/2307.05329 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12659
replaced with revised version Sun, 11 Feb 2024 12:07:55 GMT   (994kb,D)

Title: A Model for Every User and Budget: Label-Free and Personalized
  Mixed-Precision Quantization
Authors: Edward Fish, Umberto Michieli, Mete Ozay
Categories: cs.SD cs.CL eess.AS
Comments: INTERSPEECH 2023. Code is available at
  https://github.com/SamsungLabs/myQASR
\\ ( https://arxiv.org/abs/2307.12659 ,  994kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16033
replaced with revised version Mon, 12 Feb 2024 05:00:09 GMT   (19586kb,D)

Title: Towards Perceiving Small Visual Details in Zero-shot Visual Question
  Answering with Multimodal LLMs
Authors: Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski
Categories: cs.CV cs.CL
Comments: 20 pages, 12 figures, 7 tables
\\ ( https://arxiv.org/abs/2310.16033 ,  19586kb)
------------------------------------------------------------------------------
\\
arXiv:2104.03527 (*cross-listing*)
replaced with revised version Sun, 11 Feb 2024 04:51:28 GMT   (1854kb,D)

Title: Sparse NMF with Archetypal Regularization: Computational and Robustness
  Properties
Authors: Kayhan Behdin and Rahul Mazumder
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2104.03527 ,  1854kb)
------------------------------------------------------------------------------
\\
arXiv:2105.02569 (*cross-listing*)
replaced with revised version Sun, 11 Feb 2024 10:21:20 GMT   (2229kb)

Title: Machine Collaboration
Authors: Qingfeng Liu and Yang Feng
Categories: stat.ML cs.LG econ.EM
MSC-class: 68Q32, 68T05, 62G05, 68T07, 62J02, 62G08, 62J07, 62J12
ACM-class: G.3; I.2.6; I.6.4; I.6.5
\\ ( https://arxiv.org/abs/2105.02569 ,  2229kb)
------------------------------------------------------------------------------
\\
arXiv:2111.04596 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 10:00:23 GMT   (273kb,D)

Title: Inertial Newton Algorithms Avoiding Strict Saddle Points
Authors: Camille Castera
Categories: math.OC cs.LG
Journal-ref: Journal of Optimization Theory and Applications (2023)
  199(12):881--903
DOI: 10.1007/s10957-023-02330-0
\\ ( https://arxiv.org/abs/2111.04596 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2207.06944
replaced with revised version Fri, 9 Feb 2024 21:21:43 GMT   (768kb)

Title: Differentially Private Graph Learning via Sensitivity-Bounded
  Personalized PageRank
Authors: Alessandro Epasto, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin,
  Peilin Zhong
Categories: cs.CR cs.LG cs.SI stat.ML
\\ ( https://arxiv.org/abs/2207.06944 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2208.03367
replaced with revised version Mon, 12 Feb 2024 14:20:41 GMT   (38kb)

Title: Sublinear Time Algorithm for Online Weighted Bipartite Matching
Authors: Hang Hu, Zhao Song, Runzhou Tao, Zhaozhuo Xu, Junze Yin, Danyang Zhuo
Categories: cs.DS cs.LG
\\ ( https://arxiv.org/abs/2208.03367 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2212.05949 (*cross-listing*)
replaced with revised version Sat, 10 Feb 2024 16:29:49 GMT   (64kb)

Title: Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear
  Contextual Bandits and Markov Decision Processes
Authors: Chenlu Ye, Wei Xiong, Quanquan Gu, Tong Zhang
Categories: stat.ML cs.LG
Comments: We study the corruption-robust MDPs and contextual bandits with
  general function approximation
Journal-ref: ICML 2023
\\ ( https://arxiv.org/abs/2212.05949 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2301.00270
replaced with revised version Mon, 12 Feb 2024 16:59:06 GMT   (2471kb,D)

Title: NetEffect: Discovery and Exploitation of Generalized Network Effects
Authors: Meng-Chieh Lee, Shubhranshu Shekhar, Jaemin Yoo, Christos Faloutsos
Categories: cs.SI cs.LG
Comments: Accepted to PAKDD 2024
\\ ( https://arxiv.org/abs/2301.00270 ,  2471kb)
------------------------------------------------------------------------------
\\
arXiv:2303.14877 (*cross-listing*)
replaced with revised version Sat, 10 Feb 2024 15:45:01 GMT   (2884kb,D)

Title: Error-mitigated Quantum Approximate Optimization via Learning-based
  Adaptive Optimization
Authors: Lixue Cheng, Yu-Qin Chen, Shi-Xin Zhang, Shengyu Zhang
Categories: quant-ph cs.LG
Comments: Main text: 11 pages, 4 figures, SI: 5 pages, 5 figures
\\ ( https://arxiv.org/abs/2303.14877 ,  2884kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03894
replaced with revised version Fri, 9 Feb 2024 22:38:25 GMT   (2139kb,D)

Title: A multifidelity approach to continual learning for physical systems
Authors: Amanda Howard, Yucheng Fu, and Panos Stinis
Categories: math.NA cs.LG cs.NA
\\ ( https://arxiv.org/abs/2304.03894 ,  2139kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06989 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 20:24:16 GMT   (75kb,D)

Title: Neural Wave Functions for Superfluids
Authors: Wan Tong Lou, Halvard Sutterud, Gino Cassella, W.M.C. Foulkes,
  Johannes Knolle, David Pfau, James S. Spencer
Categories: cond-mat.quant-gas cond-mat.supr-con cs.LG physics.comp-ph
Comments: 15 pages, 5 figures. Talk presented at the 2023 APS March Meeting,
  March 5-10, 2023, Las Vegas, Nevada, United States
\\ ( https://arxiv.org/abs/2305.06989 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09492 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 16:23:30 GMT   (741kb,D)

Title: Solar Active Region Magnetogram Image Dataset for Studies of Space
  Weather
Authors: Laura E. Boucheron, Ty Vincent, Jeremy A. Grajeda, Ellery Wuest
Categories: astro-ph.SR astro-ph.IM cs.LG
DOI: 10.1038/s41597-023-02628-8
\\ ( https://arxiv.org/abs/2305.09492 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16860 (*cross-listing*)
replaced with revised version Sun, 11 Feb 2024 22:44:44 GMT   (55kb)

Title: Error Bounds for Flow Matching Methods
Authors: Joe Benton, George Deligiannidis, Arnaud Doucet
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.16860 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00353 (*cross-listing*)
replaced with revised version Sun, 11 Feb 2024 14:36:00 GMT   (9361kb,D)

Title: Constructing Semantics-Aware Adversarial Examples with Probabilistic
  Perspective
Authors: Andi Zhang, Mingtian Zhang, Damon Wischik
Categories: stat.ML cs.CR cs.LG
Comments: 16 pages, 9 figures
\\ ( https://arxiv.org/abs/2306.00353 ,  9361kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08125 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 10:46:46 GMT   (249kb,D)

Title: Implicit Compressibility of Overparametrized Neural Networks Trained
  with Heavy-Tailed SGD
Authors: Yijun Wan, Melih Barsbey, Abdellatif Zaidi, Umut Simsekli
Categories: stat.ML cs.LG math.PR
Comments: 31 pages, 2 figures
\\ ( https://arxiv.org/abs/2306.08125 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12465
replaced with revised version Mon, 12 Feb 2024 11:53:48 GMT   (10146kb,D)

Title: Efficient Deep Spiking Multi-Layer Perceptrons with Multiplication-Free
  Inference
Authors: Boyan Li, Luziwei Leng, Ran Cheng, Shuaijie Shen, Kaixuan Zhang,
  Jianguo Zhang, Jianxing Liao
Categories: cs.NE cs.LG
Comments: 13 pages, 6 figures
\\ ( https://arxiv.org/abs/2306.12465 ,  10146kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13461 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 16:30:54 GMT   (594kb,D)

Title: Understanding quantum machine learning also requires rethinking
  generalization
Authors: Elies Gil-Fuster, Jens Eisert, Carlos Bravo-Prieto
Categories: quant-ph cond-mat.quant-gas cs.LG stat.ML
Comments: 14+4 pages, 3 figures
\\ ( https://arxiv.org/abs/2306.13461 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14291
replaced with revised version Fri, 9 Feb 2024 21:07:37 GMT   (11856kb,D)

Title: Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic
  Distance Enhances Open World Object Detection
Authors: Thang Doan, Xin Li, Sima Behpour, Wenbin He, Liang Gou, Liu Ren
Categories: cs.CV cs.LG
Comments: Accepted at AAAI 2024 || keywords: Open World Object Detection,
  Hyperbolic Distance, Unknown Detection, Deformable Transformers, Hierarchical
  Representation Learning
\\ ( https://arxiv.org/abs/2306.14291 ,  11856kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01107 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 10:03:28 GMT   (2452kb,D)

Title: Sampling the lattice Nambu-Goto string using Continuous Normalizing
  Flows
Authors: Michele Caselle, Elia Cellini and Alessandro Nada
Categories: hep-lat cs.LG hep-th
Comments: 1+28 pages, 11 figures; v2 1+29 pages, 12 figures, added discussion
  on the implications of this approach for EST modeling, added results on the
  comparison between CNF and HMC, matches published version
Journal-ref: JHEP 02 (2024) 048
DOI: 10.1007/JHEP02(2024)048
\\ ( https://arxiv.org/abs/2307.01107 ,  2452kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07572 (*cross-listing*)
replaced with revised version Sat, 10 Feb 2024 21:05:48 GMT   (14585kb,D)

Title: Harpa: High-Rate Phase Association with Travel Time Neural Fields
Authors: Cheng Shi, Maarten V. de Hoop, and Ivan Dokmani\'c
Categories: physics.geo-ph cs.LG eess.SP
\\ ( https://arxiv.org/abs/2307.07572 ,  14585kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02261 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 16:05:24 GMT   (437kb,D)

Title: Adaptive Proximal Gradient Method for Convex Optimization
Authors: Yura Malitsky, Konstantin Mishchenko
Categories: math.OC cs.LG cs.NA math.NA stat.ML
\\ ( https://arxiv.org/abs/2308.02261 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02397 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 16:58:42 GMT   (4122kb,D)

Title: Design Space Exploration on Efficient and Accurate Human Pose Estimation
  from Sparse IMU-Sensing
Authors: Iris F\"urst-Walter, Antonio Nappi, Tanja Harbaum, J\"urgen Becker
Categories: eess.SP cs.LG
Comments: 7 pages, 6 figures, 3 tables
DOI: 10.1109/IROS55552.2023.10341256
\\ ( https://arxiv.org/abs/2308.02397 ,  4122kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05737
replaced with revised version Sat, 10 Feb 2024 03:53:18 GMT   (41447kb,D)

Title: Follow Anything: Open-set detection, tracking, and following in
  real-time
Authors: Alaa Maalouf and Ninad Jadhav and Krishna Murthy Jatavallabhula and
  Makram Chahine and Daniel M.Vogt and Robert J. Wood and Antonio Torralba and
  Daniela Rus
Categories: cs.RO cs.CV cs.LG
Comments: Project webpage: https://github.com/alaamaalouf/FollowAnything
  Explainer video: https://www.youtube.com/watch?v=6Mgt3EPytrw
\\ ( https://arxiv.org/abs/2308.05737 ,  41447kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06394
replaced with revised version Sun, 11 Feb 2024 08:38:07 GMT   (17237kb,D)

Title: Detecting and Preventing Hallucinations in Large Vision Language Models
Authors: Anisha Gunjal, Jihan Yin, Erhan Bas
Categories: cs.CV cs.LG
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2308.06394 ,  17237kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09895
replaced with revised version Sat, 10 Feb 2024 18:40:31 GMT   (476kb,D)

Title: Knowledge Transfer from High-Resource to Low-Resource Programming
  Languages for Code LLMs
Authors: Federico Cassano, John Gouwar, Francesca Lucchetti, Claire
  Schlesinger, Anders Freeman, Carolyn Jane Anderson, Molly Q Feldman, Michael
  Greenberg, Abhinav Jangda, Arjun Guha
Categories: cs.PL cs.LG
\\ ( https://arxiv.org/abs/2308.09895 ,  476kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16848 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 10:14:32 GMT   (2319kb,D)

Title: Natural Quantum Monte Carlo Computation of Excited States
Authors: David Pfau and Simon Axelrod and Halvard Sutterud and Ingrid von Glehn
  and James S. Spencer
Categories: physics.comp-ph cs.LG physics.chem-ph quant-ph
Comments: Added funding acknowledgment
\\ ( https://arxiv.org/abs/2308.16848 ,  2319kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00612 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 17:15:34 GMT   (11983kb,D)

Title: Bayesian deep learning for cosmic volumes with modified gravity
Authors: Jorge Enrique Garc\'ia-Farieta, H\'ector J Hort\'ua and Francisco-Shu
  Kitaura
Categories: astro-ph.CO astro-ph.IM cs.LG stat.ML
Comments: 13 pages, 7 figures and 7 tables. Minor changes to match the accepted
  version in A&A
DOI: 10.1051/0004-6361/202347929
\\ ( https://arxiv.org/abs/2309.00612 ,  11983kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01108 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 23:01:51 GMT   (990kb,D)

Title: Acoustic-to-articulatory inversion for dysarthric speech: Are
  pre-trained self-supervised representations favorable?
Authors: Sarthak Kumar Maharana, Krishna Kamal Adidam, Shoumik Nandi, Ajitesh
  Srivastava
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to IEEE ICASSP Workshops 2024
\\ ( https://arxiv.org/abs/2309.01108 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01753 (*cross-listing*)
replaced with revised version Sun, 11 Feb 2024 08:10:01 GMT   (98kb,D)

Title: On Penalty Methods for Nonconvex Bilevel Optimization and First-Order
  Stochastic Approximation
Authors: Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, Robert Nowak
Categories: math.OC cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2309.01753 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03202 (*cross-listing*)
replaced with revised version Sat, 10 Feb 2024 23:14:06 GMT   (0kb,I)

Title: Evaluation of Reinforcement Learning Techniques for Trading on a Diverse
  Portfolio
Authors: Ishan S. Khare, Tarun K. Martheswaran, Akshana Dassanaike-Perera
Categories: q-fin.TR cs.LG
Comments: Course project not to be posted online
\\ ( https://arxiv.org/abs/2309.03202 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08630 (*cross-listing*)
replaced with revised version Sun, 11 Feb 2024 00:17:22 GMT   (1079kb,D)

Title: PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph
  Construction Methods and Chebyshev Graph Convolutions
Authors: Yash Semlani, Mihir Relan, Krithik Ramesh
Categories: hep-ph cs.LG hep-ex
Comments: 14 pages, 2 figures, and 6 tables
\\ ( https://arxiv.org/abs/2309.08630 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06085
replaced with revised version Sat, 10 Feb 2024 19:07:48 GMT   (11060kb,D)

Title: Quantile-based Maximum Likelihood Training for Outlier Detection
Authors: Masoud Taghikhah, Nishant Kumar, Sini\v{s}a \v{S}egvi\'c, Abouzar
  Eslami, Stefan Gumhold
Categories: cs.CV cs.LG
Comments: Camera Ready Version AAAI 2024. Code available at
  https://github.com/taghikhah/QuantOD
\\ ( https://arxiv.org/abs/2310.06085 ,  11060kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10531
replaced with revised version Sat, 10 Feb 2024 17:49:32 GMT   (1933kb,D)

Title: Learning optimal integration of spatial and temporal information in
  noisy chemotaxis
Authors: Albert Alonso and Julius B. Kirkegaard
Categories: cs.NE cs.LG physics.bio-ph
\\ ( https://arxiv.org/abs/2310.10531 ,  1933kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00164
replaced with revised version Mon, 12 Feb 2024 17:09:19 GMT   (2477kb,D)

Title: Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations
  for Accident Analysis
Authors: Abhinav Nippani, Dongyue Li, Haotian Ju, Haris N. Koutsopoulos,
  Hongyang R. Zhang
Categories: cs.SI cs.LG
Comments: 24 pages. Appeared in NeurIPS 2023 Datasets Track
\\ ( https://arxiv.org/abs/2311.00164 ,  2477kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04153 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 11:19:55 GMT   (5041kb,D)

Title: Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet
  transits and $H_0$ inference
Authors: Namu Kroupa, David Yallup, Will Handley and Michael Hobson
Categories: astro-ph.CO astro-ph.EP astro-ph.IM cs.LG stat.ML
Comments: 17 pages, 11 figures
Journal-ref: Monthly Notices of the Royal Astronomical Society, Volume 528,
  Issue 2, February 2024, Pages 1232-1248
DOI: 10.1093/mnras/stae087
\\ ( https://arxiv.org/abs/2311.04153 ,  5041kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13917 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 09:26:59 GMT   (4547kb)

Title: Exploring the impact of social stress on the adaptive dynamics of
  COVID-19: Typing the behavior of na\"ive populations faced with epidemics
Authors: Innokentiy Kastalskiy, Andrei Zinovyev, Evgeny Mirkes, Victor
  Kazantsev and Alexander N. Gorban
Categories: physics.soc-ph cs.LG
Comments: 29 pages, 16 figures, 1 table, 2 appendices
\\ ( https://arxiv.org/abs/2311.13917 ,  4547kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08823 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 21:37:42 GMT   (1940kb,D)

Title: Fast sampling from constrained spaces using the Metropolis-adjusted
  Mirror Langevin algorithm
Authors: Vishwak Srinivasan, Andre Wibisono, Ashia Wilson
Categories: stat.CO cs.DS cs.LG math.ST stat.ML stat.TH
Comments: 48 pages, 6 figures, 2 tables
\\ ( https://arxiv.org/abs/2312.08823 ,  1940kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11801 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 21:01:56 GMT   (4254kb,D)

Title: Fast, Scalable, Warm-Start Semidefinite Programming with Spectral
  Bundling and Sketching
Authors: Rico Angell and Andrew McCallum
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2312.11801 ,  4254kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14212 (*cross-listing*)
replaced with revised version Sat, 10 Feb 2024 11:07:55 GMT   (1234kb,D)

Title: Beyond mirkwood: Enhancing SED Modeling with Conformal Predictions
Authors: Sankalp Gilda
Categories: astro-ph.IM astro-ph.GA cs.LG
Comments: 6 pages + 1 reference page. Accepted to the 3rd AI2ASE workshop at
  AAAI 2024 (Vancouver, BC, Canada)
Journal-ref: Astronomy 2024, 3, 14-20
DOI: 10.3390/astronomy3010002
\\ ( https://arxiv.org/abs/2312.14212 ,  1234kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15910
replaced with revised version Sun, 11 Feb 2024 07:19:58 GMT   (4805kb,D)

Title: Reinforcement Unlearning
Authors: Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi, Sheng
  Shen, Wanlei Zhou, Minhui Xue
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.15910 ,  4805kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01404
replaced with revised version Mon, 12 Feb 2024 13:01:54 GMT   (7924kb,D)

Title: Scalable network reconstruction in subquadratic time
Authors: Tiago P. Peixoto
Categories: cs.DS cs.LG physics.data-an stat.CO stat.ML
Comments: 11 pages, 6 figures
\\ ( https://arxiv.org/abs/2401.01404 ,  7924kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08468 (*cross-listing*)
replaced with revised version Sat, 10 Feb 2024 04:12:05 GMT   (1695kb,D)

Title: Keep or toss? A nonparametric score to evaluate solutions for noisy ICA
Authors: Syamantak Kumar, Purnamrita Sarkar, Peter Bickel, and Derek Bean
Categories: math.ST cs.LG eess.SP stat.TH
\\ ( https://arxiv.org/abs/2401.08468 ,  1695kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09417
replaced with revised version Sat, 10 Feb 2024 14:28:20 GMT   (628kb,D)

Title: Vision Mamba: Efficient Visual Representation Learning with
  Bidirectional State Space Model
Authors: Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu,
  Xinggang Wang
Categories: cs.CV cs.LG
Comments: Work in progress. Code is available at https://github.com/hustvl/Vim
\\ ( https://arxiv.org/abs/2401.09417 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13695 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 20:26:33 GMT   (15421kb,D)

Title: Inverse analysis of granular flows using differentiable graph neural
  network simulator
Authors: Yongjin Choi, Krishna Kumar
Categories: physics.geo-ph cs.LG
ACM-class: I.6.8
\\ ( https://arxiv.org/abs/2401.13695 ,  15421kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17345
replaced with revised version Sat, 10 Feb 2024 12:09:18 GMT   (727kb)

Title: Reproducibility, energy efficiency and performance of pseudorandom
  number generators in machine learning: a comparative study of python, numpy,
  tensorflow, and pytorch implementations
Authors: Benjamin Antunes, David R.C Hill
Categories: cs.MS cs.LG
Comments: 20 pages, 10 tables, 1 figure
\\ ( https://arxiv.org/abs/2401.17345 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02047
replaced with revised version Fri, 9 Feb 2024 22:18:05 GMT   (491kb,D)

Title: Quality and Trust in LLM-generated Code
Authors: Claudio Spiess, David Gros, Kunal Suresh Pai, Michael Pradel, Md
  Rafiqul Islam Rabin, Amin Alipour, Susmit Jha, Prem Devanbu, Toufique Ahmed
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2402.02047 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02209
replaced with revised version Mon, 12 Feb 2024 08:25:06 GMT   (4004kb,D)

Title: On the Exploitation of DCT-Traces in the Generative-AI Domain
Authors: Orazio Pontorno (1), Luca Guarnera (1), Sebastiano Battiato (1) ((1)
  University of Catania)
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.02209 ,  4004kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02954
replaced with revised version Fri, 9 Feb 2024 20:57:08 GMT   (426kb,D)

Title: Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form
  Game Approach
Authors: Johan Peralez, Aur\'elien Delage, Olivier Buffet, Jilles S. Dibangoye
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2402.02954 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03312
replaced with revised version Sun, 11 Feb 2024 06:33:19 GMT   (2306kb,D)

Title: Test-Time Adaptation for Depth Completion
Authors: Hyoungseob Park, Anjali Gupta, Alex Wong
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.03312 ,  2306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05210 (*cross-listing*)
replaced with revised version Fri, 9 Feb 2024 21:40:29 GMT   (23105kb,D)

Title: Anatomically-Controllable Medical Image Generation with
  Segmentation-Guided Diffusion Models
Authors: Nicholas Konz, Yuwen Chen, Haoyu Dong, Maciej A. Mazurowski
Categories: eess.IV cs.CV cs.LG stat.ML
Comments: Code and synthetic dataset:
  https://github.com/mazurowski-lab/segmentation-guided-diffusion
\\ ( https://arxiv.org/abs/2402.05210 ,  23105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05817 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 15:19:22 GMT   (891kb)

Title: Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging
Authors: Pouria Yazdian Anari, Fiona Obiezu, Nathan Lay, Fatemeh Dehghani
  Firouzabadi, Aditi Chaurasia, Mahshid Golagha, Shiva Singh, Fatemeh
  Homayounieh, Aryan Zahergivar, Stephanie Harmon, Evrim Turkbey, Rabindra
  Gautam, Kevin Ma, Maria Merino, Elizabeth C. Jones, Mark W. Ball, W. Marston
  Linehan, Baris Turkbey, Ashkan A. Malayeri
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.05817 ,  891kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06033 (*cross-listing*)
replaced with revised version Mon, 12 Feb 2024 10:22:26 GMT   (46kb)

Title: An Inexact Halpern Iteration with Application to Distributionally Robust
  Optimization
Authors: Ling Liang, Kim-Chuan Toh, and Jia-Jie Zhu
Categories: math.OC cs.LG
Comments: Correct a typo in the title and update authors' information
\\ ( https://arxiv.org/abs/2402.06033 ,  46kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
