Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月8日 17:08
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue  6 Feb 24 19:00:00 GMT  to  Wed  7 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.04338
Date: Tue, 6 Feb 2024 19:20:58 GMT   (3503kb)

Title: Logical recognition method for solving the problem of identification in
  the Internet of Things
Authors: Islambek Saymanov
Categories: cs.AI
Comments: 2 figures
\\
  A new area of application of methods of algebra of logic and to valued logic,
which has emerged recently, is the problem of recognizing a variety of objects
and phenomena, medical or technical diagnostics, constructing modern machines,
checking test problems, etc., which can be reduced to constructing an optimal
extension of the logical function to the entire feature space. For example, in
logical recognition systems, logical methods based on discrete analysis and
propositional calculus based on it are used to build their own recognition
algorithms. In the general case, the use of a logical recognition method
provides for the presence of logical connections expressed by the optimal
continuation of a k-valued function over the entire feature space, in which the
variables are the logical features of the objects or phenomena being
recognized. The goal of this work is to develop a logical method for object
recognition consisting of a reference table with logical features and classes
of non-intersecting objects, which are specified as vectors from a given
feature space. The method consists of considering the reference table as a
logical function that is not defined everywhere and constructing an optimal
continuation of the logical function to the entire feature space, which
determines the extension of classes to the entire space.
\\ ( https://arxiv.org/abs/2402.04338 ,  3503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04370
Date: Tue, 6 Feb 2024 20:13:34 GMT   (6060kb,D)

Title: Pedestrian crossing decisions can be explained by bounded optimal
  decision-making under noisy visual perception
Authors: Yueyang Wang, Aravinda Ramakrishnan Srinivasan, Jussi P.P. Jokinen,
  Antti Oulasvirta, Gustav Markkula
Categories: cs.AI
\\
  This paper presents a model of pedestrian crossing decisions, based on the
theory of computational rationality. It is assumed that crossing decisions are
boundedly optimal, with bounds on optimality arising from human cognitive
limitations. While previous models of pedestrian behaviour have been either
'black-box' machine learning models or mechanistic models with explicit
assumptions about cognitive factors, we combine both approaches. Specifically,
we model mechanistically noisy human visual perception and assumed rewards in
crossing, but we use reinforcement learning to learn bounded optimal behaviour
policy. The model reproduces a larger number of known empirical phenomena than
previous models, in particular: (1) the effect of the time to arrival of an
approaching vehicle on whether the pedestrian accepts the gap, the effect of
the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of
crossing in front of yielding vehicles, and (4) the effect on this crossing
timing of the stopping distance of the yielding vehicle. Notably, our findings
suggest that behaviours previously framed as 'biases' in decision-making, such
as speed-dependent gap acceptance, might instead be a product of rational
adaptation to the constraints of visual perception. Our approach also permits
fitting the parameters of cognitive constraints and rewards per individual, to
better account for individual differences. To conclude, by leveraging both RL
and mechanistic modelling, our model offers novel insights about pedestrian
behaviour, and may provide a useful foundation for more accurate and scalable
pedestrian models.
\\ ( https://arxiv.org/abs/2402.04370 ,  6060kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04382
Date: Tue, 6 Feb 2024 20:39:49 GMT   (1971kb,D)

Title: Counterfactual Generation with Answer Set Programming
Authors: Sopam Dasgupta, Farhad Shakerin, Joaqu\'in Arias, Elmer Salazar, Gopal
  Gupta
Categories: cs.AI
Comments: 16 Pages
\\
  Machine learning models that automate decision-making are increasingly being
used in consequential areas such as loan approvals, pretrial bail approval,
hiring, and many more. Unfortunately, most of these models are black-boxes,
i.e., they are unable to reveal how they reach these prediction decisions. A
need for transparency demands justification for such predictions. An affected
individual might also desire explanations to understand why a decision was
made. Ethical and legal considerations may further require informing the
individual of changes in the input attribute that could be made to produce a
desirable outcome. This paper focuses on the latter problem of automatically
generating counterfactual explanations. We propose a framework Counterfactual
Generation with s(CASP) (CFGS) that utilizes answer set programming (ASP) and
the s(CASP) goal-directed ASP system to automatically generate counterfactual
explanations from rules generated by rule-based machine learning (RBML)
algorithms. In our framework, we show how counterfactual explanations are
computed and justified by imagining worlds where some or all factual
assumptions are altered/changed. More importantly, we show how we can navigate
between these worlds, namely, go from our original world/scenario where we
obtain an undesired outcome to the imagined world/scenario where we obtain a
desired/favourable outcome.
\\ ( https://arxiv.org/abs/2402.04382 ,  1971kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04464
Date: Tue, 6 Feb 2024 23:16:41 GMT   (6120kb)

Title: Ten Hard Problems in Artificial Intelligence We Must Get Right
Authors: Gavin Leech and Simson Garfinkel and Misha Yagudin and Alexander
  Briand and Aleksandr Zhuravlev
Categories: cs.AI cs.CY
Comments: 71 + 19 pages
\\
  We explore the AI2050 "hard problems" that block the promise of AI and cause
AI risks: (1) developing general capabilities of the systems; (2) assuring the
performance of AI systems and their training processes; (3) aligning system
goals with human goals; (4) enabling great applications of AI in real life; (5)
addressing economic disruptions; (6) ensuring the participation of all; (7) at
the same time ensuring socially responsible deployment; (8) addressing any
geopolitical disruptions that AI causes; (9) promoting sound governance of the
technology; and (10) managing the philosophical disruptions for humans living
in the age of AI. For each problem, we outline the area, identify significant
recent work, and suggest ways forward. [Note: this paper reviews literature
through January 2023.]
\\ ( https://arxiv.org/abs/2402.04464 ,  6120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04559
Date: Wed, 7 Feb 2024 03:37:19 GMT   (1869kb,D)

Title: Can Large Language Model Agents Simulate Human Trust Behaviors?
Authors: Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi,
  Ziniu Hu, Philip Torr, Bernard Ghanem, Guohao Li
Categories: cs.AI cs.CL cs.HC
Comments: The first two authors contributed equally. Project website:
  https://www.camel-ai.org/research/agent-trust
\\
  Large Language Model (LLM) agents have been increasingly adopted as
simulation tools to model humans in applications such as social science.
However, one fundamental question remains: can LLM agents really simulate human
behaviors? In this paper, we focus on one of the most critical behaviors in
human interactions, trust, and aim to investigate whether or not LLM agents can
simulate human trust behaviors. We first find that LLM agents generally exhibit
trust behaviors, referred to as agent trust, under the framework of Trust
Games, which are widely recognized in behavioral economics. Then, we discover
that LLM agents can have high behavioral alignment with humans regarding trust
behaviors, indicating the feasibility to simulate human trust behaviors with
LLM agents. In addition, we probe into the biases in agent trust and the
differences in agent trust towards agents and humans. We also explore the
intrinsic properties of agent trust under conditions including advanced
reasoning strategies and external manipulations. We further offer important
implications for various scenarios where trust is paramount. Our study
represents a significant step in understanding the behaviors of LLM agents and
the LLM-human analogy.
\\ ( https://arxiv.org/abs/2402.04559 ,  1869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04578
Date: Wed, 7 Feb 2024 04:36:31 GMT   (33284kb,D)

Title: S-Agents: self-organizing agents in open-ended environment
Authors: Jiaqi Chen and Yuxian Jiang and Jiachen Lu and Li Zhang
Categories: cs.AI cs.MA
Comments: Preview, 23 pages, 12 figure
\\
  Leveraging large language models (LLMs), autonomous agents have significantly
improved, gaining the ability to handle a variety of tasks. In open-ended
settings, optimizing collaboration for efficiency and effectiveness demands
flexible adjustments. Despite this, current research mainly emphasizes fixed,
task-oriented workflows and overlooks agent-centric organizational structures.
Drawing inspiration from human organizational behavior, we introduce a
self-organizing agent system (S-Agents) with a "tree of agents" structure for
dynamic workflow, an "hourglass agent architecture" for balancing information
priorities, and a "non-obstructive collaboration" method to allow asynchronous
task execution among agents. This structure can autonomously coordinate a group
of agents, efficiently addressing the challenges of an open and dynamic
environment without human intervention. Our experiments demonstrate that
S-Agents proficiently execute collaborative building tasks and resource
collection in the Minecraft environment, validating their effectiveness.
\\ ( https://arxiv.org/abs/2402.04578 ,  33284kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04597
Date: Wed, 7 Feb 2024 05:43:57 GMT   (1138kb,D)

Title: CMSA algorithm for solving the prioritized pairwise test data generation
  problem in software product lines
Authors: Javier Ferrer, Francisco Chicano, Jos\'e Antonio Ortega Toro
Categories: cs.AI cs.SE
Comments: Preprint of the submitted version of the article in Journal of
  Heuristics
Journal-ref: J. Heuristics 27(1-2): 229-249 (2021)
DOI: 10.1007/s10732-020-09462-w
\\
  In Software Product Lines (SPLs) it may be difficult or even impossible to
test all the products of the family because of the large number of valid
feature combinations that may exist. Thus, we want to find a minimal subset of
the product family that allows us to test all these possible combinations
(pairwise). Furthermore, when testing a single product is a great effort, it is
desirable to first test products composed of a set of priority features. This
problem is called Prioritized Pairwise Test Data Generation Problem.
  State-of-the-art algorithms based on Integer Linear Programming for this
problema are faster enough for small and medium instances. However, there
exists some real instances that are too large to be computed with these
algorithms in a reasonable time because of the exponential growth of the number
of candidate solutions. Also, these heuristics not always lead us to the best
solutions. In this work we propose a new approach based on a hybrid
metaheuristic algorithm called Construct, Merge, Solve & Adapt. We compare this
matheuristic with four algorithms: a Hybrid algorithm based on Integer Linear
Programming ((HILP), a Hybrid algorithm based on Integer Nonlinear Programming
(HINLP), the Parallel Prioritized Genetic Solver (PPGS), and a greedy algorithm
called prioritized-ICPL. The analysis reveals that CMSA results in
statistically significantly better quality solutions in most instances and for
most levels of weighted coverage, although it requires more execution time.
\\ ( https://arxiv.org/abs/2402.04597 ,  1138kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04627
Date: Wed, 7 Feb 2024 07:24:01 GMT   (3545kb,D)

Title: SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question
  Answering over a Life Science Knowledge Graph
Authors: Julio C. Rangel, Tarcisio Mendes de Farias, Ana Claudia Sima and Norio
  Kobayashi
Categories: cs.AI cs.CL cs.DB cs.IR
Comments: To appear in Proceedings of SWAT4HCLS 2024: Semantic Web Tools and
  Applications for Healthcare and Life Sciences
\\
  The recent success of Large Language Models (LLM) in a wide range of Natural
Language Processing applications opens the path towards novel Question
Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the
main obstacles preventing their implementation is the scarcity of training data
for the task of translating questions into corresponding SPARQL queries,
particularly in the case of domain-specific KGs. To overcome this challenge, in
this study, we evaluate several strategies for fine-tuning the OpenLlama LLM
for question answering over life science knowledge graphs. In particular, we
propose an end-to-end data augmentation approach for extending a set of
existing queries over a given knowledge graph towards a larger dataset of
semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even
for datasets where these pairs are scarce. In this context, we also investigate
the role of semantic "clues" in the queries, such as meaningful variable names
and inline comments. Finally, we evaluate our approach over the real-world Bgee
gene expression knowledge graph and we show that semantic clues can improve
model performance by up to 33% compared to a baseline with random variable
names and no comments included.
\\ ( https://arxiv.org/abs/2402.04627 ,  3545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04792
Date: Wed, 7 Feb 2024 12:31:13 GMT   (2947kb,D)

Title: Direct Language Model Alignment from Online AI Feedback
Authors: Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
  Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan
  Ferret, Mathieu Blondel
Categories: cs.AI cs.CL cs.HC
Comments: 18 pages, 8 figures, 4 tables
\\
  Direct alignment from preferences (DAP) methods, such as DPO, have recently
emerged as efficient alternatives to reinforcement learning from human feedback
(RLHF), that do not require a separate reward model. However, the preference
datasets used in DAP methods are usually collected ahead of training and never
updated, thus the feedback is purely offline. Moreover, responses in these
datasets are often sampled from a language model distinct from the one being
aligned, and since the model evolves over training, the alignment phase is
inevitably off-policy. In this study, we posit that online feedback is key and
improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as
annotator: on each training iteration, we sample two responses from the current
model and prompt the LLM annotator to choose which one is preferred, thus
providing online feedback. Despite its simplicity, we demonstrate via human
evaluation in several tasks that OAIF outperforms both offline DAP and RLHF
methods. We further show that the feedback leveraged in OAIF is easily
controllable, via instruction prompts to the LLM annotator.
\\ ( https://arxiv.org/abs/2402.04792 ,  2947kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04832
Date: Wed, 7 Feb 2024 13:31:59 GMT   (23kb)

Title: Structured d-DNNF Is Not Closed Under Negation
Authors: Harry Vinall-Smeeth
Categories: cs.AI cs.LO
Comments: 9 pages, 2 figures
\\
  Both structured d-DNNF and SDD can be exponentially more succinct than OBDD.
Moreover, SDD is essentially as tractable as OBDD. But this has left two
important open questions. Firstly, does OBDD support more tractable
transformations than structured d-DNNF? And secondly, is structured d-DNNF more
succinct than SDD? In this paper, we answer both questions in the affirmative.
For the first question we show that, unlike OBDD, structured d-DNNF does not
support polytime negation, disjunction, or existential quantification
operations. As a corollary, we deduce that there are functions with an
equivalent polynomial-sized structured d-DNNF but with no such representation
as an SDD, thus answering the second question. We also lift this second result
to arithmetic circuits (AC) to show a succinctness gap between PSDD and the
monotone AC analogue to structured d-DNNF.
\\ ( https://arxiv.org/abs/2402.04832 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04856
Date: Wed, 7 Feb 2024 13:54:38 GMT   (4824kb,D)

Title: Explaining Learned Reward Functions with Counterfactual Trajectories
Authors: Jan Wehner, Frans Oliehoek, Luciano Cavalcante Siebert
Categories: cs.AI cs.LG
\\
  Learning rewards from human behaviour or feedback is a promising approach to
aligning AI systems with human values but fails to consistently extract correct
reward functions. Interpretability tools could enable users to understand and
evaluate possible flaws in learned reward functions. We propose Counterfactual
Trajectory Explanations (CTEs) to interpret reward functions in reinforcement
learning by contrasting an original with a counterfactual partial trajectory
and the rewards they each receive. We derive six quality criteria for CTEs and
propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises
these quality criteria. Finally, we measure how informative the generated
explanations are to a proxy-human model by training it on CTEs. CTEs are
demonstrably informative for the proxy-human model, increasing the similarity
between its predictions and the reward function on unseen trajectories.
Further, it learns to accurately judge differences in rewards between
trajectories and generalises to out-of-distribution examples. Although CTEs do
not lead to a perfect understanding of the reward, our method, and more
generally the adaptation of XAI methods, are presented as a fruitful approach
for interpreting learned reward functions.
\\ ( https://arxiv.org/abs/2402.04856 ,  4824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04858
Date: Wed, 7 Feb 2024 13:55:27 GMT   (792kb,D)

Title: CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
Authors: Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone, David
  Zhang, Micha\"el Defferrard, Taco Cohen
Categories: cs.AI cs.CL cs.LG
Comments: 8 pages, 11 figures
\\
  Large language models are increasingly solving tasks that are commonly
believed to require human-level reasoning ability. However, these models still
perform very poorly on benchmarks of general intelligence such as the
Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a
programming-by-examples problem, and introduce a novel and scalable method for
language model self-improvement called Code Iteration (CodeIt). Our method
iterates between 1) program sampling and hindsight relabeling, and 2) learning
from prioritized experience replay. By relabeling the goal of an episode (i.e.,
the target program output given input) to the realized output produced by the
sampled program, our method effectively deals with the extreme sparsity of
rewards in program synthesis. Applying CodeIt to the ARC dataset, we
demonstrate that prioritized hindsight replay, along with pre-training and
data-augmentation, leads to successful inter-task generalization. CodeIt is the
first neuro-symbolic approach that scales to the full ARC evaluation dataset.
Our method solves 15% of ARC evaluation tasks, achieving state-of-the-art
performance and outperforming existing neural and symbolic baselines.
\\ ( https://arxiv.org/abs/2402.04858 ,  792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04870
Date: Tue, 6 Feb 2024 11:23:33 GMT   (1069kb,D)

Title: Embedding Knowledge Graphs in Degenerate Clifford Algebras
Authors: Louis Mozart Kamdem, Caglar Demir and Axel-Cyrille Ngonga
Categories: cs.AI cs.LG
\\
  Clifford algebras are a natural generalization of the real numbers, the
complex numbers, and the quaternions. So far, solely Clifford algebras of the
form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been
studied in the context of knowledge graph embeddings. We propose to consider
nilpotent base vectors with a nilpotency index of two. In these spaces, denoted
$Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which
cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from
the absence of higher-order interactions between real and complex parts of
entity embeddings. We design two new models for the discovery of the parameters
$p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$,
and $r$. The second predicts $(p, q,r)$ based on an embedding of the input
knowledge graph computed using neural networks. The results of our evaluation
on seven benchmark datasets suggest that nilpotent vectors can help capture
embeddings better. Our comparison against the state of the art suggests that
our approach generalizes better than other approaches on all datasets w.r.t.
the MRR it achieves on validation data. We also show that a greedy search
suffices to discover values of $p$, $q$ and $r$ that are close to optimal.
\\ ( https://arxiv.org/abs/2402.04870 ,  1069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04874
Date: Thu, 25 Jan 2024 13:04:27 GMT   (727kb,D)

Title: Choosing a Classical Planner with Graph Neural Networks
Authors: Jana Vatter, Ruben Mayer, Hans-Arno Jacobsen, Horst Samulowitz,
  Michael Katz
Categories: cs.AI cs.LG
\\
  Online planner selection is the task of choosing a solver out of a predefined
set for a given planning problem. As planning is computationally hard, the
performance of solvers varies greatly on planning problems. Thus, the ability
to predict their performance on a given problem is of great importance. While a
variety of learning methods have been employed, for classical cost-optimal
planning the prevailing approach uses Graph Neural Networks (GNNs). In this
work, we continue the line of work on using GNNs for online planner selection.
We perform a thorough investigation of the impact of the chosen GNN model,
graph representation and node features, as well as prediction task. Going
further, we propose using the graph representation obtained by a GNN as an
input to the Extreme Gradient Boosting (XGBoost) model, resulting in a more
resource-efficient yet accurate approach. We show the effectiveness of a
variety of GNN-based online planner selection methods, opening up new exciting
avenues for research on online planner selection.
\\ ( https://arxiv.org/abs/2402.04874 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04892
Date: Wed, 7 Feb 2024 14:24:04 GMT   (3991kb,D)

Title: A Unified Framework for Probabilistic Verification of AI Systems via
  Weighted Model Integration
Authors: Paolo Morettin, Andrea Passerini and Roberto Sebastiani
Categories: cs.AI cs.LG
\\
  The probabilistic formal verification (PFV) of AI systems is in its infancy.
So far, approaches have been limited to ad-hoc algorithms for specific classes
of models and/or properties.
  We propose a unifying framework for the PFV of AI systems based onWeighted
Model Integration (WMI), which allows to frame the problem in very general
terms.
  Crucially, this reduction enables the verification of many properties of
interest, like fairness, robustness or monotonicity, over a wide range of
machine learning models, without making strong distributional assumptions.
  We support the generality of the approach by solving multiple verification
tasks with a single, off-the-shelf WMI solver, then discuss the scalability
challenges and research directions related to this promising framework.
\\ ( https://arxiv.org/abs/2402.04892 ,  3991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04898
Date: Wed, 7 Feb 2024 14:28:04 GMT   (2608kb)

Title: The Strain of Success: A Predictive Model for Injury Risk Mitigation and
  Team Success in Soccer
Authors: Gregory Everett, Ryan Beal, Tim Matthews, Timothy J. Norman, Sarvapali
  D. Ramchurn
Categories: cs.AI cs.LG
Comments: 19 pages (16 main, 2 references, 1 appendix), 10 figures (9 main, 1
  appendix). Accepted at the MIT Sloan Sports Analytics Conference 2024
  Research Paper Competition
\\
  In this paper, we present a novel sequential team selection model in soccer.
Specifically, we model the stochastic process of player injury and
unavailability using player-specific information learned from real-world soccer
data. Monte-Carlo Tree Search is used to select teams for games that optimise
long-term team performance across a soccer season by reasoning over player
injury probability. We validate our approach compared to benchmark solutions
for the 2018/19 English Premier League season. Our model achieves similar
season expected points to the benchmark whilst reducing first-team injuries by
~13% and the money inefficiently spent on injured players by ~11% -
demonstrating the potential to reduce costs and improve player welfare in
real-world soccer teams.
\\ ( https://arxiv.org/abs/2402.04898 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04938
Date: Wed, 7 Feb 2024 15:16:21 GMT   (1228kb)

Title: An approach to automated videogame beta testing
Authors: Jennifer Hern\'andez-B\'ecares, Luis Costero, Pedro Pablo
  G\'omez-Mart\'in
Categories: cs.AI
Journal-ref: Entertainment Computing, Elsevier. 18. pp 79 to 92. (2017)
DOI: 10.1016/j.entcom.2016.08.002
\\
  Videogames developed in the 1970s and 1980s were modest programs created in a
couple of months by a single person, who played the roles of designer, artist
and programmer. Since then, videogames have evolved to become a multi-million
dollar industry. Today, AAA game development involves hundreds of people
working together over several years. Management and engineering requirements
have changed at the same pace. Although many of the processes have been adapted
over time, this is not quite true for quality assurance tasks, which are still
done mainly manually by human beta testers due to the specific peculiarities of
videogames. This paper presents an approach to automate this beta testing.
\\ ( https://arxiv.org/abs/2402.04938 ,  1228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04971
Date: Wed, 7 Feb 2024 15:50:20 GMT   (2539kb,D)

Title: Multi-Sender Persuasion -- A Computational Perspective
Authors: Safwan Hossain, Tonghan Wang, Tao Lin, Yiling Chen, David C. Parkes,
  Haifeng Xu
Categories: cs.AI cs.GT
\\
  We consider multiple senders with informational advantage signaling to
convince a single self-interested actor towards certain actions. Generalizing
the seminal Bayesian Persuasion framework, such settings are ubiquitous in
computational economics, multi-agent learning, and machine learning with
multiple objectives. The core solution concept here is the Nash equilibrium of
senders' signaling policies. Theoretically, we prove that finding an
equilibrium in general is PPAD-Hard; in fact, even computing a sender's best
response is NP-Hard. Given these intrinsic difficulties, we turn to finding
local Nash equilibria. We propose a novel differentiable neural network to
approximate this game's non-linear and discontinuous utilities. Complementing
this with the extra-gradient algorithm, we discover local equilibria that
Pareto dominates full-revelation equilibria and those found by existing neural
networks. Broadly, our theoretical and empirical contributions are of interest
to a large class of economic problems.
\\ ( https://arxiv.org/abs/2402.04971 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05048
Date: Wed, 7 Feb 2024 17:41:15 GMT   (618kb,D)

Title: How VADER is your AI? Towards a definition of artificial intelligence
  systems appropriate for regulation
Authors: Leonardo C. T. Bezerra, Alexander E. I. Brownlee, Luana Ferraz
  Alvarenga, Renan Cipriano Moioli, Thais Vasconcelos Batista
Categories: cs.AI
ACM-class: I.2.0
\\
  Artificial intelligence (AI) has driven many information and communication
technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has
expanded far beyond AI since the Turing test proposal. Critically, recent AI
regulation proposals adopt AI definitions affecting ICT techniques, approaches,
and systems that are not AI. In some cases, even works from mathematics,
statistics, and engineering would be affected. Worryingly, AI misdefinitions
are observed from Western societies to the Global South. In this paper, we
propose a framework to score how \textit{validated as appropriately-defined for
regulation} (VADER) an AI definition is. Our online, publicly-available VADER
framework scores the coverage of premises that should underlie AI definitions
for regulation, which aim to (i) reproduce principles observed in other
successful technology regulations, and (ii) include all AI techniques and
approaches while excluding non-AI works. Regarding the latter, our score is
based on a dataset of representative AI, non-AI ICT, and non-ICT examples. We
demonstrate our contribution by reviewing the AI regulation proposals of key
players, namely the United States, United Kingdom, European Union, and Brazil.
Importantly, none of the proposals assessed achieve the appropriateness score,
ranging from a revision need to a concrete risk to ICT systems and works from
other fields.
\\ ( https://arxiv.org/abs/2402.05048 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05070
Date: Wed, 7 Feb 2024 18:21:17 GMT   (5684kb,D)

Title: A Roadmap to Pluralistic Alignment
Authors: Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon,
  Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang,
  Ximing Lu, Nouha Dziri, Tim Althoff, Yejin Choi
Categories: cs.AI cs.CL cs.IR
\\
  With increased power and prevalence of AI systems, it is ever more critical
that AI systems are designed to serve all, i.e., people with diverse values and
perspectives. However, aligning models to serve pluralistic human values
remains an open research question. In this piece, we propose a roadmap to
pluralistic alignment, specifically using language models as a test bed. We
identify and formalize three possible ways to define and operationalize
pluralism in AI systems: 1) Overton pluralistic models that present a spectrum
of reasonable responses; 2) Steerably pluralistic models that can steer to
reflect certain perspectives; and 3) Distributionally pluralistic models that
are well-calibrated to a given population in distribution. We also propose and
formalize three possible classes of pluralistic benchmarks: 1) Multi-objective
benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to
steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which
explicitly model diverse human ratings. We use this framework to argue that
current alignment techniques may be fundamentally limited for pluralistic AI;
indeed, we highlight empirical evidence, both from our own experiments and from
other work, that standard alignment procedures might reduce distributional
pluralism in models, motivating the need for further research on pluralistic
alignment.
\\ ( https://arxiv.org/abs/2402.05070 ,  5684kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04315
Date: Tue, 6 Feb 2024 19:00:40 GMT   (7723kb,D)

Title: Training Language Models to Generate Text with Citations via
  Fine-grained Rewards
Authors: Chengyu Huang, Zeqiu Wu, Yushi Hu, Wenya Wang
Categories: cs.CL
\\
  While recent Large Language Models (LLMs) have proven useful in answering
user queries, they are prone to hallucination, and their responses often lack
credibility due to missing references to reliable sources. An intuitive
solution to these issues would be to include in-text citations referring to
external documents as evidence. While previous works have directly prompted
LLMs to generate in-text citations, their performances are far from
satisfactory, especially when it comes to smaller LLMs. In this work, we
propose an effective training framework using fine-grained rewards to teach
LLMs to generate highly supportive and relevant citations, while ensuring the
correctness of their responses. We also conduct a systematic analysis of
applying these fine-grained rewards to common LLM training strategies,
demonstrating its advantage over conventional practices. We conduct extensive
experiments on Question Answering (QA) datasets taken from the ALCE benchmark
and validate the model's generalizability using EXPERTQA. On LLaMA-2-7B, the
incorporation of fine-grained rewards achieves the best performance among the
baselines, even surpassing that of GPT-3.5-turbo.
\\ ( https://arxiv.org/abs/2402.04315 ,  7723kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04333
Date: Tue, 6 Feb 2024 19:18:04 GMT   (1784kb,D)

Title: LESS: Selecting Influential Data for Targeted Instruction Tuning
Authors: Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi
  Chen
Categories: cs.CL cs.AI cs.LG
Comments: Code and data are available at https://github.com/princeton-nlp/LESS
\\
  Instruction tuning has unlocked powerful capabilities in large language
models (LLMs), effectively using combined datasets to develop generalpurpose
chatbots. However, real-world applications often require a specialized suite of
skills (e.g., reasoning). The challenge lies in identifying the most relevant
data from these extensive datasets to effectively develop specific
capabilities, a setting we frame as targeted instruction tuning. We propose
LESS, an optimizer-aware and practically efficient algorithm to effectively
estimate data influences and perform Low-rank gradiEnt Similarity Search for
instruction data selection. Crucially, LESS adapts existing influence
formulations to work with the Adam optimizer and variable-length instruction
data. LESS first constructs a highly reusable and transferable gradient
datastore with low-dimensional gradient features and then selects examples
based on their similarity to few-shot examples embodying a specific capability.
Experiments show that training on a LESS-selected 5% of the data can often
outperform training on the full dataset across diverse downstream tasks.
Furthermore, the selected data is highly transferable: smaller models can be
leveraged to select useful data for larger models and models from different
families. Our qualitative analysis shows that our method goes beyond surface
form cues to identify data that exemplifies the necessary reasoning skills for
the intended downstream application.
\\ ( https://arxiv.org/abs/2402.04333 ,  1784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04335
Date: Tue, 6 Feb 2024 19:18:56 GMT   (9729kb,D)

Title: LegalLens: Leveraging LLMs for Legal Violation Identification in
  Unstructured Text
Authors: Dor Bernsohn, Gil Semo, Yaron Vazana, Gila Hayat, Ben Hagag, Joel
  Niklaus, Rohit Saha, Kyryl Truskovskyi
Categories: cs.CL cs.AI cs.LG
\\
  In this study, we focus on two main tasks, the first for detecting legal
violations within unstructured textual data, and the second for associating
these violations with potentially affected individuals. We constructed two
datasets using Large Language Models (LLMs) which were subsequently validated
by domain expert annotators. Both tasks were designed specifically for the
context of class-action cases. The experimental design incorporated fine-tuning
models from the BERT family and open-source LLMs, and conducting few-shot
experiments using closed-source LLMs. Our results, with an F1-score of 62.69\%
(violation identification) and 81.02\% (associating victims), show that our
datasets and setups can be used for both tasks. Finally, we publicly release
the datasets and the code used for the experiments in order to advance further
research in the area of legal natural language processing (NLP).
\\ ( https://arxiv.org/abs/2402.04335 ,  9729kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04401
Date: Tue, 6 Feb 2024 21:03:52 GMT   (1851kb,D)

Title: Democratizing Large Language Models via Personalized Parameter-Efficient
  Fine-tuning
Authors: Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, Meng
  Jiang
Categories: cs.CL
\\
  Personalization in large language models (LLMs) is increasingly important,
aiming to align LLM's interactions, content, and recommendations with
individual user preferences. Recent advances in LLM personalization have
spotlighted effective prompt design, by enriching user queries with
non-parametric knowledge through behavior history retrieval and textual
profiles. However, these approaches were limited due to a lack of model
ownership, resulting in constrained customization and privacy issues. Moreover,
they often failed to accurately capture user behavior patterns, especially in
cases where user data were complex and dynamic. To address these shortcomings,
we introduce One PEFT Per User (OPPU), which employs personalized
parameter-efficient fine-tuning (PEFT) modules, to store user-specific behavior
patterns and preferences. By plugging in users' personal PEFT parameters, they
can own and use their LLMs personally. OPPU integrates parametric user
knowledge in the personal PEFT parameters with the non-parametric knowledge
acquired through retrieval and profile. This integration adapts individual LLMs
to user behavior shifts. Experimental results demonstrate that OPPU
significantly outperforms existing prompt-based methods across seven diverse
tasks in the LaMP benchmark. Further in-depth studies reveal OPPU's enhanced
capabilities in handling user behavior shifts, modeling users at different
active levels, maintaining robustness across various user history formats, and
displaying versatility with different PEFT methods.
\\ ( https://arxiv.org/abs/2402.04401 ,  1851kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04411
Date: Tue, 6 Feb 2024 21:14:45 GMT   (4888kb,D)

Title: Chatbot Meets Pipeline: Augment Large Language Model with Definite
  Finite Automaton
Authors: Yiyou Sun and Junjie Hu and Wei Cheng and Haifeng Chen
Categories: cs.CL cs.LG
Comments: 21 pages, 11 figures
\\
  This paper introduces the Definite Finite Automaton augmented large language
model (DFA-LLM), a novel framework designed to enhance the capabilities of
conversational agents using large language models (LLMs). Traditional LLMs face
challenges in generating regulated and compliant responses in special scenarios
with predetermined response guidelines, like emotional support and customer
service. Our framework addresses these challenges by embedding a Definite
Finite Automaton (DFA), learned from training dialogues, within the LLM. This
structured approach enables the LLM to adhere to a deterministic response
pathway, guided by the DFA. The advantages of DFA-LLM include an interpretable
structure through human-readable DFA, context-aware retrieval for responses in
conversations, and plug-and-play compatibility with existing LLMs. Extensive
benchmarks validate DFA-LLM's effectiveness, indicating its potential as a
valuable contribution to the conversational agent.
\\ ( https://arxiv.org/abs/2402.04411 ,  4888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04437
Date: Tue, 6 Feb 2024 22:15:09 GMT   (2323kb,D)

Title: Structured Entity Extraction Using Large Language Models
Authors: Haolun Wu, Ye Yuan, Liana Mikaelyan, Alexander Meulemans, Xue Liu,
  James Hensman, Bhaskar Mitra
Categories: cs.CL cs.LG
\\
  Recent advances in machine learning have significantly impacted the field of
information extraction, with Large Language Models (LLMs) playing a pivotal
role in extracting structured information from unstructured text. This paper
explores the challenges and limitations of current methodologies in structured
entity extraction and introduces a novel approach to address these issues. We
contribute to the field by first introducing and formalizing the task of
Structured Entity Extraction (SEE), followed by proposing Approximate Entity
Set OverlaP (AESOP) Metric designed to appropriately assess model performance
on this task. Later, we propose a new model that harnesses the power of LLMs
for enhanced effectiveness and efficiency through decomposing the entire
extraction task into multiple stages. Quantitative evaluation and human
side-by-side evaluation confirm that our model outperforms baselines, offering
promising directions for future advancements in structured entity extraction.
\\ ( https://arxiv.org/abs/2402.04437 ,  2323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04442
Date: Tue, 6 Feb 2024 22:24:56 GMT   (352kb,D)

Title: Evaluating Embeddings for One-Shot Classification of Doctor-AI
  Consultations
Authors: Olumide Ebenezer Ojo, Olaronke Oluwayemisi Adebanji, Alexander
  Gelbukh, Hiram Calvo and Anna Feldman
Categories: cs.CL
\\
  Effective communication between healthcare providers and patients is crucial
to providing high-quality patient care. In this work, we investigate how
Doctor-written and AI-generated texts in healthcare consultations can be
classified using state-of-the-art embeddings and one-shot classification
systems. By analyzing embeddings such as bag-of-words, character n-grams,
Word2Vec, GloVe, fastText, and GPT2 embeddings, we examine how well our
one-shot classification systems capture semantic information within medical
consultations. Results show that the embeddings are capable of capturing
semantic features from text in a reliable and adaptable manner. Overall,
Word2Vec, GloVe and Character n-grams embeddings performed well, indicating
their suitability for modeling targeted to this task. GPT2 embedding also shows
notable performance, indicating its suitability for models tailored to this
task as well. Our machine learning architectures significantly improved the
quality of health conversations when training data are scarce, improving
communication between patients and healthcare providers.
\\ ( https://arxiv.org/abs/2402.04442 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04477
Date: Tue, 6 Feb 2024 23:52:58 GMT   (69kb,D)

Title: Detecting Mode Collapse in Language Models via Narration
Authors: Sil Hamilton
Categories: cs.CL cs.AI
Comments: To appear in the proceedings of the first Workshop on the Scaling
  Behavior of Large Language Models (EACL 2024)
\\
  No two authors write alike. Personal flourishes invoked in written
narratives, from lexicon to rhetorical devices, imply a particular author--what
literary theorists label the implied or virtual author; distinct from the real
author or narrator of a text. Early large language models trained on unfiltered
training sets drawn from a variety of discordant sources yielded incoherent
personalities, problematic for conversational tasks but proving useful for
sampling literature from multiple perspectives. Successes in alignment research
in recent years have allowed researchers to impose subjectively consistent
personae on language models via instruction tuning and reinforcement learning
from human feedback (RLHF), but whether aligned models retain the ability to
model an arbitrary virtual author has received little scrutiny. By studying
4,374 stories sampled from three OpenAI language models, we show successive
versions of GPT-3 suffer from increasing degrees of "mode collapse" whereby
overfitting the model during alignment constrains it from generalizing over
authorship: models suffering from mode collapse become unable to assume a
multiplicity of perspectives. Our method and results are significant for
researchers seeking to employ language models in sociological simulations.
\\ ( https://arxiv.org/abs/2402.04477 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04505
Date: Wed, 7 Feb 2024 01:18:55 GMT   (2970kb,D)

Title: Developments in Sheaf-Theoretic Models of Natural Language Ambiguities
Authors: Kin Ian Lo, Mehrnoosh Sadrzadeh, Shane Mansfield
Categories: cs.CL quant-ph
Comments: arXiv admin note: text overlap with arXiv:2308.16498
\\
  Sheaves are mathematical objects consisting of a base which constitutes a
topological space and the data associated with each open set thereof, e.g.
continuous functions defined on the open sets. Sheaves have originally been
used in algebraic topology and logic. Recently, they have also modelled events
such as physical experiments and natural language disambiguation processes. We
extend the latter models from lexical ambiguities to discourse ambiguities
arising from anaphora. To begin, we calculated a new measure of contextuality
for a dataset of basic anaphoric discourses, resulting in a higher proportion
of contextual models--82.9%--compared to previous work which only yielded 3.17%
contextual models. Then, we show how an extension of the natural language
processing challenge, known as the Winograd Schema, which involves anaphoric
ambiguities can be modelled on the Bell-CHSH scenario with a contextual
fraction of 0.096.
\\ ( https://arxiv.org/abs/2402.04505 ,  2970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04542
Date: Wed, 7 Feb 2024 02:59:18 GMT   (8371kb,D)

Title: Share What You Already Know: Cross-Language-Script Transfer and
  Alignment for Sentiment Detection in Code-Mixed Data
Authors: Niraj Pahari and Kazutaka Shimada
Categories: cs.CL
\\
  Code-switching entails mixing multiple languages. It is an increasingly
occurring phenomenon in social media texts. Usually, code-mixed texts are
written in a single script, even though the languages involved have different
scripts. Pre-trained multilingual models primarily utilize the data in the
native script of the language. In existing studies, the code-switched texts are
utilized as they are. However, using the native script for each language can
generate better representations of the text owing to the pre-trained knowledge.
Therefore, a cross-language-script knowledge sharing architecture utilizing the
cross attention and alignment of the representations of text in individual
language scripts was proposed in this study. Experimental results on two
different datasets containing Nepali-English and Hindi-English code-switched
texts, demonstrate the effectiveness of the proposed method. The interpretation
of the model using model explainability technique illustrates the sharing of
language-specific knowledge between language-specific representations.
\\ ( https://arxiv.org/abs/2402.04542 ,  8371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04588
Date: Wed, 7 Feb 2024 05:05:53 GMT   (751kb,D)

Title: UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised
  Fine-tuning Dataset
Authors: Haoyu Wang, Shuo Wang, Yukun Yan, Xujia Wang, Zhiyu Yang, Yuzhuang Xu,
  Zhenghao Liu, Ning Ding, Xu Han, Zhiyuan Liu, Maosong Sun
Categories: cs.CL
Comments: Work in Progress
\\
  Open-source large language models (LLMs) have gained significant strength
across diverse fields. Nevertheless, the majority of studies primarily
concentrate on English, with only limited exploration into the realm of
multilingual supervised fine-tuning. In this work, we therefore construct an
open-source multilingual supervised fine-tuning dataset. Different from
previous works that simply translate English instructions, we consider both the
language-specific and language-agnostic abilities of LLMs. For
language-specific abilities, we introduce a knowledge-grounded data
augmentation approach to elicit more culture-specific knowledge of LLMs,
improving their ability to serve users from different countries. For
language-agnostic abilities, we find through experiments that modern LLMs
exhibit strong cross-lingual transfer capabilities, thus repeatedly learning
identical content in various languages is not necessary. Consequently, we can
substantially prune the language-agnostic SFT data without any performance
degradation, making the SFT process more efficient. The resulting UltraLink
dataset comprises approximately 1 million samples across five languages, and
the proposed data construction method can also be easily extended to other
languages. UltraLink-LM, which is trained on UltraLink, outperforms several
representative baselines across many tasks.
\\ ( https://arxiv.org/abs/2402.04588 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04601
Date: Wed, 7 Feb 2024 05:56:54 GMT   (229kb,D)

Title: Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector
Authors: Haihui Yang and Xiaojun Quan
Categories: cs.CL cs.AI
\\
  Chinese grammatical error correction (CGEC) faces serious overcorrection
challenges when employing autoregressive generative models such as
sequence-to-sequence (Seq2Seq) models and decoder-only large language models
(LLMs). While previous methods aim to address overcorrection in Seq2Seq models,
they are difficult to adapt to decoder-only LLMs. In this paper, we propose an
alignment-enhanced corrector for the overcorrection problem that applies to
both Seq2Seq models and decoder-only LLMs. Our method first trains a correction
model to generate an initial correction of the source sentence. Then, we
combine the source sentence with the initial correction and feed it through an
alignment model for another round of correction, aiming to enforce the
alignment model to focus on potential overcorrection. Moreover, to enhance the
model's ability to identify nuances, we further explore the reverse alignment
of the source sentence and the initial correction. Finally, we transfer the
alignment knowledge from two alignment models to the correction model,
instructing it on how to avoid overcorrection. Experimental results on three
CGEC datasets demonstrate the effectiveness of our approach in alleviating
overcorrection and improving overall performance.
\\ ( https://arxiv.org/abs/2402.04601 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04609
Date: Wed, 7 Feb 2024 06:13:14 GMT   (634kb,D)

Title: Improving Cross-Domain Low-Resource Text Generation through LLM
  Post-Editing: A Programmer-Interpreter Approach
Authors: Zhuang Li, Levon Haroutunian, Raj Tumuluri, Philip Cohen, Gholamreza
  Haffari
Categories: cs.CL
Comments: EACL 2024 (findings), short paper, 5 pages
\\
  Post-editing has proven effective in improving the quality of text generated
by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when
direct updating of their parameters to enhance text quality is infeasible or
expensive. However, relying solely on smaller language models for post-editing
can limit the LLMs' ability to generalize across domains. Moreover, the editing
strategies in these methods are not optimally designed for text-generation
tasks. To address these limitations, we propose a neural programmer-interpreter
approach that preserves the domain generalization ability of LLMs when editing
their output. The editing actions in this framework are specifically devised
for text generation. Extensive experiments demonstrate that the
programmer-interpreter significantly enhances GPT-3.5's performance in logical
form-to-text conversion and low-resource machine translation, surpassing other
state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.
\\ ( https://arxiv.org/abs/2402.04609 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04614
Date: Wed, 7 Feb 2024 06:32:50 GMT   (791kb,D)

Title: Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations
  from Large Language Models
Authors: Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju
Categories: cs.CL
\\
  Large Language Models (LLMs) are deployed as powerful tools for several
natural language processing (NLP) applications. Recent works show that modern
LLMs can generate self-explanations (SEs), which elicit their intermediate
reasoning steps for explaining their behavior. Self-explanations have seen
widespread adoption owing to their conversational and plausible nature.
However, there is little to no understanding of their faithfulness. In this
work, we discuss the dichotomy between faithfulness and plausibility in SEs
generated by LLMs. We argue that while LLMs are adept at generating plausible
explanations -- seemingly logical and coherent to human users -- these
explanations do not necessarily align with the reasoning processes of the LLMs,
raising concerns about their faithfulness. We highlight that the current trend
towards increasing the plausibility of explanations, primarily driven by the
demand for user-friendly interfaces, may come at the cost of diminishing their
faithfulness. We assert that the faithfulness of explanations is critical in
LLMs employed for high-stakes decision-making. Moreover, we urge the community
to identify the faithfulness requirements of real-world applications and ensure
explanations meet those needs. Finally, we propose some directions for future
work, emphasizing the need for novel methodologies and frameworks that can
enhance the faithfulness of self-explanations without compromising their
plausibility, essential for the transparent deployment of LLMs in diverse
high-stakes domains.
\\ ( https://arxiv.org/abs/2402.04614 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04616
Date: Wed, 7 Feb 2024 06:48:24 GMT   (550kb,D)

Title: TinyLLM: Learning a Small Student from Multiple Large Language Models
Authors: Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, Nitesh V. Chawla
Categories: cs.CL cs.AI cs.LG
\\
  Transferring the reasoning capability from stronger large language models
(LLMs) to smaller ones has been quite appealing, as smaller LLMs are more
flexible to deploy with less expense. Among the existing solutions, knowledge
distillation stands out due to its outstanding efficiency and generalization.
However, existing methods suffer from several drawbacks, including limited
knowledge diversity and the lack of rich contextual information. To solve the
problems and facilitate the learning of compact language models, we propose
TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM
from multiple large teacher LLMs. In particular, we encourage the student LLM
to not only generate the correct answers but also understand the rationales
behind these answers. Given that different LLMs possess diverse reasoning
skills, we guide the student model to assimilate knowledge from various teacher
LLMs. We further introduce an in-context example generator and a
teacher-forcing Chain-of-Thought strategy to ensure that the rationales are
accurate and grounded in contextually appropriate scenarios. Extensive
experiments on six datasets across two reasoning tasks demonstrate the
superiority of our method. Results show that TinyLLM can outperform large
teacher LLMs significantly, despite having a considerably smaller model size.
\\ ( https://arxiv.org/abs/2402.04616 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04617
Date: Wed, 7 Feb 2024 06:50:42 GMT   (2123kb,D)

Title: InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding
  Extremely Long Sequences with Training-Free Memory
Authors: Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin,
  Zhengyan Zhang, Zhiyuan Liu, Song Han, Maosong Sun
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have emerged as a cornerstone in real-world
applications with lengthy streaming inputs, such as LLM-driven agents. However,
existing LLMs, pre-trained on sequences with restricted maximum length, cannot
generalize to longer sequences due to the out-of-domain and distraction issues.
To alleviate these issues, existing efforts employ sliding attention windows
and discard distant tokens to achieve the processing of extremely long
sequences. Unfortunately, these approaches inevitably fail to capture
long-distance dependencies within sequences to deeply understand semantics.
This paper introduces a training-free memory-based method, InfLLM, to unveil
the intrinsic ability of LLMs to process streaming long sequences.
Specifically, InfLLM stores distant contexts into additional memory units and
employs an efficient mechanism to lookup token-relevant units for attention
computation. Thereby, InfLLM allows LLMs to efficiently process long sequences
while maintaining the ability to capture long-distance dependencies. Without
any training, InfLLM enables LLMs pre-trained on sequences of a few thousand
tokens to achieve superior performance than competitive baselines continually
training these LLMs on long sequences. Even when the sequence length is scaled
to $1,024$K, InfLLM still effectively captures long-distance dependencies.
\\ ( https://arxiv.org/abs/2402.04617 ,  2123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04624
Date: Wed, 7 Feb 2024 07:14:11 GMT   (1297kb,D)

Title: MEMORYLLM: Towards Self-Updatable Large Language Models
Authors: Yu Wang, Xiusi Chen, Jingbo Shang, Julian McAuley
Categories: cs.CL
Comments: 13 pages, 9 figures
\\
  Existing Large Language Models (LLMs) usually remain static after deployment,
which might make it hard to inject new knowledge into the model. We aim to
build models containing a considerable portion of self-updatable parameters,
enabling the model to integrate new knowledge effectively and efficiently. To
this end, we introduce MEMORYLLM, a model that comprises a transformer and a
fixed-size memory pool within the latent space of the transformer. MEMORYLLM
can self-update with text knowledge and memorize the knowledge injected
earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively
incorporate new knowledge, as evidenced by its performance on model editing
benchmarks. Meanwhile, the model exhibits long-term information retention
capacity, which is validated through our custom-designed evaluations and
long-context benchmarks. MEMORYLLM also shows operational integrity without any
sign of performance degradation even after nearly a million memory updates.
\\ ( https://arxiv.org/abs/2402.04624 ,  1297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04631
Date: Wed, 7 Feb 2024 07:28:34 GMT   (16478kb,D)

Title: The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents:
  New Perspectives and Trends
Authors: Mengqi Chen, Bin Guo, Hao Wang, Haoyu Li, Qian Zhao, Jingqi Liu, Yasan
  Ding, Yan Pan, Zhiwen Yu
Categories: cs.CL
Comments: 36 pages, 6 figures
\\
  Persuasion, as one of the crucial abilities in human communication, has
garnered extensive attention from researchers within the field of intelligent
dialogue systems. We humans tend to persuade others to change their viewpoints,
attitudes or behaviors through conversations in various scenarios (e.g.,
persuasion for social good, arguing in online platforms). Developing dialogue
agents that can persuade others to accept certain standpoints is essential to
achieving truly intelligent and anthropomorphic dialogue system. Benefiting
from the substantial progress of Large Language Models (LLMs), dialogue agents
have acquired an exceptional capability in context understanding and response
generation. However, as a typical and complicated cognitive psychological
system, persuasive dialogue agents also require knowledge from the domain of
cognitive psychology to attain a level of human-like persuasion. Consequently,
the cognitive strategy-enhanced persuasive dialogue agent (defined as
CogAgent), which incorporates cognitive strategies to achieve persuasive
targets through conversation, has become a predominant research paradigm. To
depict the research trends of CogAgent, in this paper, we first present several
fundamental cognitive psychology theories and give the formalized definition of
three typical cognitive strategies, including the persuasion strategy, the
topic path planning strategy, and the argument structure prediction strategy.
Then we propose a new system architecture by incorporating the formalized
definition to lay the foundation of CogAgent. Representative works are detailed
and investigated according to the combined cognitive strategy, followed by the
summary of authoritative benchmarks and evaluation metrics. Finally, we
summarize our insights on open issues and future directions of CogAgent for
upcoming researchers.
\\ ( https://arxiv.org/abs/2402.04631 ,  16478kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04636
Date: Wed, 7 Feb 2024 07:39:27 GMT   (7836kb,D)

Title: TransLLaMa: LLM-based Simultaneous Translation System
Authors: Roman Koshkin, Katsuhito Sudoh and Satoshi Nakamura
Categories: cs.CL
\\
  Decoder-only large language models (LLMs) have recently demonstrated
impressive capabilities in text generation and reasoning. Nonetheless, they
have limited applications in simultaneous machine translation (SiMT), currently
dominated by encoder-decoder transformers. This study demonstrates that, after
fine-tuning on a small dataset comprising causally aligned source and target
sentence pairs, a pre-trained open-source LLM can control input segmentation
directly by generating a special "wait" token. This obviates the need for a
separate policy and enables the LLM to perform English-German and
English-Russian SiMT tasks with BLEU scores that are comparable to those of
specific state-of-the-art baselines. We also evaluated closed-source models
such as GPT-4, which displayed encouraging results in performing the SiMT task
without prior training (zero-shot), indicating a promising avenue for enhancing
future SiMT systems.
\\ ( https://arxiv.org/abs/2402.04636 ,  7836kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04677
Date: Wed, 7 Feb 2024 09:09:09 GMT   (106kb,D)

Title: Source Identification in Abstractive Summarization
Authors: Yoshi Suhara and Dimitris Alikaniotis
Categories: cs.CL
Comments: EACL 2024
\\
  Neural abstractive summarization models make summaries in an end-to-end
manner, and little is known about how the source information is actually
converted into summaries. In this paper, we define input sentences that contain
essential information in the generated summary as $\textit{source sentences}$
and study how abstractive summaries are made by analyzing the source sentences.
To this end, we annotate source sentences for reference summaries and system
summaries generated by PEGASUS on document-summary pairs sampled from the
CNN/DailyMail and XSum datasets. We also formulate automatic source sentence
detection and compare multiple methods to establish a strong baseline for the
task. Experimental results show that the perplexity-based method performs well
in highly abstractive settings, while similarity-based methods perform robustly
in relatively extractive settings. Our code and data are available at
https://github.com/suhara/sourcesum.
\\ ( https://arxiv.org/abs/2402.04677 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04678
Date: Wed, 7 Feb 2024 09:09:14 GMT   (491kb,D)

Title: Large Language Models As Faithful Explainers
Authors: Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Ruixiang Tang, Fan
  Yang, Mengnan Du, Xuanting Cai, and Xia Hu
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have recently become proficient in addressing
complex tasks by utilizing their rich internal knowledge and reasoning ability.
Consequently, this complexity hinders traditional input-focused explanation
algorithms for explaining the complex decision-making processes of LLMs. Recent
advancements have thus emerged for self-explaining their predictions through a
single feed-forward inference in a natural language format. However, natural
language explanations are often criticized for lack of faithfulness since these
explanations may not accurately reflect the decision-making behaviors of the
LLMs. In this work, we introduce a generative explanation framework, xLLM, to
improve the faithfulness of the explanations provided in natural language
formats for LLMs. Specifically, we propose an evaluator to quantify the
faithfulness of natural language explanation and enhance the faithfulness by an
iterative optimization process of xLLM, with the goal of maximizing the
faithfulness scores. Experiments conducted on three NLU datasets demonstrate
that xLLM can significantly improve the faithfulness of generated explanations,
which are in alignment with the behaviors of LLMs.
\\ ( https://arxiv.org/abs/2402.04678 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04779
Date: Wed, 7 Feb 2024 12:01:02 GMT   (11487kb,D)

Title: StableMask: Refining Causal Masking in Decoder-only Transformer
Authors: Qingyu Yin, Xuzheng He, Xiang Zhuang, Yu Zhao, Jianhua Yao, Xiaoyu
  Shen, Qiang Zhang
Categories: cs.CL cs.AI
Comments: Preprint
\\
  The decoder-only Transformer architecture with causal masking and relative
position encoding (RPE) has become the de facto choice in language modeling.
Despite its exceptional performance across various tasks, we have identified
two limitations: First, it requires all attention scores to be non-zero and sum
up to 1, even if the current embedding has sufficient self-contained
information. This compels the model to assign disproportional excessive
attention to specific tokens. Second, RPE-based Transformers are not universal
approximators due to their limited capacity at encoding absolute positional
information, which limits their application in position-critical tasks. In this
work, we propose StableMask: a parameter-free method to address both
limitations by refining the causal mask. It introduces pseudo-attention values
to balance attention distributions and encodes absolute positional information
via a progressively decreasing mask ratio. StableMask's effectiveness is
validated both theoretically and empirically, showing significant enhancements
in language models with parameter sizes ranging from 71M to 1.4B across diverse
datasets and encoding methods. We further show that it naturally supports (1)
efficient extrapolation without special tricks such as StreamingLLM and (2)
easy integration with existing attention optimization techniques.
\\ ( https://arxiv.org/abs/2402.04779 ,  11487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04787
Date: Wed, 7 Feb 2024 12:26:12 GMT   (7981kb,D)

Title: A Hypothesis-Driven Framework for the Analysis of Self-Rationalising
  Models
Authors: Marc Braun, Jenny Kunz
Categories: cs.CL
\\
  The self-rationalising capabilities of LLMs are appealing because the
generated explanations can give insights into the plausibility of the
predictions. However, how faithful the explanations are to the predictions is
questionable, raising the need to explore the patterns behind them further. To
this end, we propose a hypothesis-driven statistical framework. We use a
Bayesian network to implement a hypothesis about how a task (in our example,
natural language inference) is solved, and its internal states are translated
into natural language with templates. Those explanations are then compared to
LLM-generated free-text explanations using automatic and human evaluations.
This allows us to judge how similar the LLM's and the Bayesian network's
decision processes are. We demonstrate the usage of our framework with an
example hypothesis and two realisations in Bayesian networks. The resulting
models do not exhibit a strong similarity to GPT-3.5. We discuss the
implications of this as well as the framework's potential to approximate LLM
decisions better in future work.
\\ ( https://arxiv.org/abs/2402.04787 ,  7981kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04788
Date: Wed, 7 Feb 2024 12:28:32 GMT   (3194kb,D)

Title: MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with
  Vision-Language Benchmark
Authors: Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang,
  Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun
Categories: cs.CL cs.AI cs.CV
\\
  Multimodal Large Language Models (MLLMs) have gained significant attention
recently, showing remarkable potential in artificial general intelligence.
However, assessing the utility of MLLMs presents considerable challenges,
primarily due to the absence multimodal benchmarks that align with human
preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel
benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting
judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and
Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable
human-like discernment in Pair Comparisons, there is a significant divergence
from human preferences in Scoring Evaluation and Batch Ranking tasks.
Furthermore, MLLMs still face challenges in judgment, including diverse biases,
hallucinatory responses, and inconsistencies, even for advanced models such as
GPT-4V. These findings emphasize the pressing need for enhancements and further
research efforts regarding MLLMs as fully reliable evaluators. Code and dataset
are available at https://github.com/Dongping-Chen/MLLM-as-a-Judge.
\\ ( https://arxiv.org/abs/2402.04788 ,  3194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04812
Date: Wed, 7 Feb 2024 13:01:43 GMT   (48kb,D)

Title: Aspect-Based Sentiment Analysis for Open-Ended HR Survey Responses
Authors: Lois Rink and Job Meijdam and David Graus
Categories: cs.CL
Comments: Accepted at NLP4HR Workshop at EACL2024
\\
  Understanding preferences, opinions, and sentiment of the workforce is
paramount for effective employee lifecycle management. Open-ended survey
responses serve as a valuable source of information. This paper proposes a
machine learning approach for aspect-based sentiment analysis (ABSA) of Dutch
open-ended responses in employee satisfaction surveys. Our approach aims to
overcome the inherent noise and variability in these responses, enabling a
comprehensive analysis of sentiments that can support employee lifecycle
management. Through response clustering we identify six key aspects (salary,
schedule, contact, communication, personal attention, agreements), which we
validate by domain experts. We compile a dataset of 1,458 Dutch survey
responses, revealing label imbalance in aspects and sentiments. We propose
few-shot approaches for ABSA based on Dutch BERT models, and compare them
against bag-of-words and zero-shot baselines. Our work significantly
contributes to the field of ABSA by demonstrating the first successful
application of Dutch pre-trained language models to aspect-based sentiment
analysis in the domain of human resources (HR).
\\ ( https://arxiv.org/abs/2402.04812 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04824
Date: Wed, 7 Feb 2024 13:22:17 GMT   (475kb,D)

Title: Learning Communication Policies for Different Follower Behaviors in a
  Collaborative Reference Game
Authors: Philipp Sadler, Sherzod Hakimov and David Schlangen
Categories: cs.CL
Comments: Work presented at the "Cooperative Multi-Agent Systems
  Decision-making and Learning" workshop (AAAI'24)
\\
  Albrecht and Stone (2018) state that modeling of changing behaviors remains
an open problem "due to the essentially unconstrained nature of what other
agents may do". In this work we evaluate the adaptability of neural artificial
agents towards assumed partner behaviors in a collaborative reference game. In
this game success is achieved when a knowledgeable Guide can verbally lead a
Follower to the selection of a specific puzzle piece among several distractors.
We frame this language grounding and coordination task as a reinforcement
learning problem and measure to which extent a common reinforcement training
algorithm (PPO) is able to produce neural agents (the Guides) that perform well
with various heuristic Follower behaviors that vary along the dimensions of
confidence and autonomy. We experiment with a learning signal that in addition
to the goal condition also respects an assumed communicative effort. Our
results indicate that this novel ingredient leads to communicative strategies
that are less verbose (staying silent in some of the steps) and that with
respect to that the Guide's strategies indeed adapt to the partner's level of
confidence and autonomy.
\\ ( https://arxiv.org/abs/2402.04824 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04833
Date: Wed, 7 Feb 2024 13:32:11 GMT   (1015kb,D)

Title: Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for
  Instruction Fine-Tuning
Authors: Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion
Categories: cs.CL
Comments: Preprint. 25 pages, 24 figures
\\
  There is a consensus that instruction fine-tuning of LLMs requires
high-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR
2024) are state-of-the-art methods for selecting such high-quality examples,
either via manual curation or using GPT-3.5-Turbo as a quality scorer. We show
that the extremely simple baseline of selecting the 1,000 instructions with
longest responses from standard datasets can consistently outperform these
sophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining
competitive on the OpenLLM benchmarks that test factual knowledge. We
demonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B,
and Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a
lightweight refinement of such long instructions can further improve the
abilities of the fine-tuned LLMs, and allows us to obtain the 2nd
highest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only
1,000 examples and no extra preference data. We also conduct a thorough
analysis of our models to ensure that their enhanced performance is not simply
due to GPT-4's preference for longer responses, thus ruling out any artificial
improvement. In conclusion, our findings suggest that fine-tuning on the
longest instructions should be the default baseline for any research on
instruction fine-tuning.
\\ ( https://arxiv.org/abs/2402.04833 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04838
Date: Wed, 7 Feb 2024 13:39:38 GMT   (319kb,D)

Title: PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity
  Recognition
Authors: Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, Can Huang
Categories: cs.CL cs.AI
\\
  In this study, we aim to reduce generation latency for Named Entity
Recognition (NER) with Large Language Models (LLMs). The main cause of high
latency in LLMs is the sequential decoding process, which autoregressively
generates all labels and mentions for NER, significantly increase the sequence
length. To this end, we introduce Parallel Decoding in LLM for NE}
(PaDeLLM-NER), a approach that integrates seamlessly into existing generative
model frameworks without necessitating additional modules or architectural
modifications. PaDeLLM-NER allows for the simultaneous decoding of all
mentions, thereby reducing generation latency. Experiments reveal that
PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times
faster than the autoregressive approach for both English and Chinese.
Simultaneously it maintains the quality of predictions as evidenced by the
performance that is on par with the state-of-the-art across various datasets.
\\ ( https://arxiv.org/abs/2402.04838 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04914
Date: Wed, 7 Feb 2024 14:41:08 GMT   (8835kb,D)

Title: Personalized Text Generation with Fine-Grained Linguistic Control
Authors: Bashar Alhafni, Vivek Kulkarni, Dhruv Kumar, Vipul Raheja
Categories: cs.CL
\\
  As the text generation capabilities of large language models become
increasingly prominent, recent studies have focused on controlling particular
aspects of the generated text to make it more personalized. However, most
research on controllable text generation focuses on controlling the content or
modeling specific high-level/coarse-grained attributes that reflect authors'
writing styles, such as formality, domain, or sentiment. In this paper, we
focus on controlling fine-grained attributes spanning multiple linguistic
dimensions, such as lexical and syntactic attributes. We introduce a novel
benchmark to train generative models and evaluate their ability to generate
personalized text based on multiple fine-grained linguistic attributes. We
systematically investigate the performance of various large language models on
our benchmark and draw insights from the factors that impact their performance.
We make our code, data, and pretrained models publicly available.
\\ ( https://arxiv.org/abs/2402.04914 ,  8835kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04918
Date: Wed, 7 Feb 2024 14:44:42 GMT   (111kb,D)

Title: Prompting Implicit Discourse Relation Annotation
Authors: Frances Yung, Mansoor Ahmad, Merel Scholman, Vera Demberg
Categories: cs.CL cs.AI
Comments: To appear at the Linguistic Annotation Workshop 2024
\\
  Pre-trained large language models, such as ChatGPT, archive outstanding
performance in various reasoning tasks without supervised training and were
found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's
performance in the task of implicit discourse relation classification, prompted
by a standard multiple-choice question, is still far from satisfactory and
considerably inferior to state-of-the-art supervised approaches. This work
investigates several proven prompting techniques to improve ChatGPT's
recognition of discourse relations. In particular, we experimented with
breaking down the classification task that involves numerous abstract labels
into smaller subtasks. Nonetheless, experiment results show that the inference
accuracy hardly changes even with sophisticated prompt engineering, suggesting
that implicit discourse relation classification is not yet resolvable under
zero-shot or few-shot settings.
\\ ( https://arxiv.org/abs/2402.04918 ,  111kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04957
Date: Wed, 7 Feb 2024 15:40:22 GMT   (1155kb,D)

Title: Reconfidencing LLMs from the Grouping Loss Perspective
Authors: Lihu Chen, Alexandre Perez-Lebel, Fabian M. Suchanek, Ga\"el Varoquaux
Categories: cs.CL
\\
  Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to
generating hallucinated answers in a confident tone. While efforts to elicit
and calibrate confidence scores have proven useful, recent findings show that
controlling uncertainty must go beyond calibration: predicted scores may
deviate significantly from the actual posterior probabilities due to the impact
of grouping loss. In this work, we construct a new evaluation dataset derived
from a knowledge base to assess confidence scores given to answers of Mistral
and LLaMA. Experiments show that they tend to be overconfident. Further, we
show that they are more overconfident on some answers than others, \emph{eg}
depending on the nationality of the person in the query. In
uncertainty-quantification theory, this is grouping loss. To address this, we
propose a solution to reconfidence LLMs, canceling not only calibration but
also grouping loss. The LLMs, after the reconfidencing process, indicate
improved confidence alignment with the accuracy of their responses.
\\ ( https://arxiv.org/abs/2402.04957 ,  1155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04967
Date: Wed, 7 Feb 2024 15:44:55 GMT   (6511kb,D)

Title: Text or Image? What is More Important in Cross-Domain Generalization
  Capabilities of Hate Meme Detection Models?
Authors: Piush Aggarwal, Jawar Mehrabanian, Weigang Huang, \"Ozge Alacam and
  Torsten Zesch
Categories: cs.CL cs.AI cs.CV
Comments: Accepted at EACL'2024 Findings
\\
  This paper delves into the formidable challenge of cross-domain
generalization in multimodal hate meme detection, presenting compelling
findings. We provide enough pieces of evidence supporting the hypothesis that
only the textual component of hateful memes enables the existing multimodal
classifier to generalize across different domains, while the image component
proves highly sensitive to a specific training dataset. The evidence includes
demonstrations showing that hate-text classifiers perform similarly to
hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction
of captions generated from images of memes to the hate-meme classifier worsens
performance by an average F1 of 0.02. Through blackbox explanations, we
identify a substantial contribution of the text modality (average of 83%),
which diminishes with the introduction of meme's image captions (52%).
Additionally, our evaluation on a newly created confounder dataset reveals
higher performance on text confounders as compared to image confounders with an
average $\Delta$F1 of 0.18.
\\ ( https://arxiv.org/abs/2402.04967 ,  6511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04978
Date: Wed, 7 Feb 2024 15:56:17 GMT   (637kb)

Title: An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge
  Graph-Integrated Collaboration
Authors: Yihao Li, Ru Zhang, Jianyi Liu, Gongshen Liu
Categories: cs.CL cs.AI
\\
  While Large Language Models (LLMs) demonstrate exceptional performance in a
multitude of Natural Language Processing (NLP) tasks, they encounter challenges
in practical applications, including issues with hallucinations, inadequate
knowledge updating, and limited transparency in the reasoning process. To
overcome these limitations, this study innovatively proposes a collaborative
training-free reasoning scheme involving tight cooperation between Knowledge
Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively
explore KG, selectively retrieving a task-relevant knowledge subgraph to
support reasoning. The LLMs are then guided to further combine inherent
implicit knowledge to reason on the subgraph while explicitly elucidating the
reasoning process. Through such a cooperative approach, our scheme achieves
more reliable knowledge-based reasoning and facilitates the tracing of the
reasoning results. Experimental results show that our scheme significantly
progressed across multiple datasets, notably achieving over a 10% improvement
on the QALD10 dataset compared to the best baseline and the fine-tuned
state-of-the-art (SOTA) work. Building on this success, this study hopes to
offer a valuable reference for future research in the fusion of KG and LLMs,
thereby enhancing LLMs' proficiency in solving complex issues.
\\ ( https://arxiv.org/abs/2402.04978 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05000
Date: Wed, 7 Feb 2024 16:15:59 GMT   (793kb,D)

Title: Pedagogical Alignment of Large Language Models
Authors: Shashank Sonkar, Kangqi Ni, Sapana Chaudhary, Richard G. Baraniuk
Categories: cs.CL
\\
  In this paper, we introduce the novel concept of pedagogically aligned Large
Language Models (LLMs) that signifies a transformative shift in the application
of LLMs within educational contexts. Rather than providing direct responses to
user queries, pedagogically-aligned LLMs function as scaffolding tools,
breaking complex problems into manageable subproblems and guiding students
towards the final answer through constructive feedback and hints. The objective
is to equip learners with problem-solving strategies that deepen their
understanding and internalization of the subject matter. Previous research in
this field has primarily applied the supervised finetuning approach without
framing the objective as an alignment problem, hence not employing
reinforcement learning through human feedback (RLHF) methods. This study
reinterprets the narrative by viewing the task through the lens of alignment
and demonstrates how RLHF methods emerge naturally as a superior alternative
for aligning LLM behaviour. Building on this perspective, we propose a novel
approach for constructing a reward dataset specifically designed for the
pedagogical alignment of LLMs. We apply three state-of-the-art RLHF algorithms
and find that they outperform SFT significantly. Our qualitative analyses
across model differences and hyperparameter sensitivity further validate the
superiority of RLHF over SFT. Also, our study sheds light on the potential of
online feedback for enhancing the performance of pedagogically-aligned LLMs,
thus providing valuable insights for the advancement of these models in
educational settings.
\\ ( https://arxiv.org/abs/2402.05000 ,  793kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05034
Date: Wed, 7 Feb 2024 17:07:53 GMT   (6650kb,D)

Title: How BERT Speaks Shakespearean English? Evaluating Historical Bias in
  Contextual Language Models
Authors: Miriam Cuscito, Alfio Ferrara, Martin Ruskov
Categories: cs.CL cs.CY
ACM-class: I.2.7; J.5
\\
  In this paper, we explore the idea of analysing the historical bias of
contextual language models based on BERT by measuring their adequacy with
respect to Early Modern (EME) and Modern (ME) English. In our preliminary
experiments, we perform fill-in-the-blank tests with 60 masked sentences (20
EME-specific, 20 ME-specific and 20 generic) and three different models (i.e.,
BERT Base, MacBERTh, English HLM). We then rate the model predictions according
to a 5-point bipolar scale between the two language varieties and derive a
weighted score to measure the adequacy of each model to EME and ME varieties of
English.
\\ ( https://arxiv.org/abs/2402.05034 ,  6650kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05044
Date: Wed, 7 Feb 2024 17:33:54 GMT   (12088kb,D)

Title: SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large
  Language Models
Authors: Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin,
  Yu Qiao, Jing Shao
Categories: cs.CL cs.AI cs.CR cs.LG
\\
  In the rapidly evolving landscape of Large Language Models (LLMs), ensuring
robust safety measures is paramount. To meet this crucial need, we propose
\emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating
LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench
transcends conventional benchmarks through its large scale, rich diversity,
intricate taxonomy spanning three levels, and versatile
functionalities.SALAD-Bench is crafted with a meticulous array of questions,
from standard queries to complex ones enriched with attack, defense
modifications and multiple-choice. To effectively manage the inherent
complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for
QA pairs with a particular focus on attack-enhanced queries, ensuring a
seamless, and reliable evaluation. Above components extend SALAD-Bench from
standard LLM safety evaluation to both LLM attack and defense methods
evaluation, ensuring the joint-purpose utility. Our extensive experiments shed
light on the resilience of LLMs against emerging threats and the efficacy of
contemporary defense tactics. Data and evaluator are released under
\url{https://github.com/OpenSafetyLab/SALAD-BENCH}. Warning: this paper
includes examples that may be offensive or harmful.
\\ ( https://arxiv.org/abs/2402.05044 ,  12088kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05111
Date: Wed, 7 Feb 2024 18:59:31 GMT   (9382kb,D)

Title: Edu-ConvoKit: An Open-Source Library for Education Conversation Data
Authors: Rose E. Wang, Dorottya Demszky
Categories: cs.CL cs.AI
Comments: https://github.com/stanfordnlp/edu-convokit
  https://edu-convokit.readthedocs.io/en/latest/
\\
  We introduce Edu-ConvoKit, an open-source library designed to handle
pre-processing, annotation and analysis of conversation data in education.
Resources for analyzing education conversation data are scarce, making the
research challenging to perform and therefore hard to access. We address these
challenges with Edu-ConvoKit. Edu-ConvoKit is open-source
(https://github.com/stanfordnlp/edu-convokit ), pip-installable
(https://pypi.org/project/edu-convokit/ ), with comprehensive documentation
(https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available
at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional
resources, such as Colab applications of Edu-ConvoKit to three diverse
education datasets and a repository of Edu-ConvoKit related papers, that can be
found in our GitHub repository.
\\ ( https://arxiv.org/abs/2402.05111 ,  9382kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04284
Date: Tue, 6 Feb 2024 01:34:56 GMT   (1160kb,D)

Title: PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks
Authors: Junwei Su, Difan Zou, Chuan Wu
Categories: cs.LG
\\
  Memory-based Dynamic Graph Neural Networks (MDGNNs) are a family of dynamic
graph neural networks that leverage a memory module to extract, distill, and
memorize long-term temporal dependencies, leading to superior performance
compared to memory-less counterparts. However, training MDGNNs faces the
challenge of handling entangled temporal and structural dependencies, requiring
sequential and chronological processing of data sequences to capture accurate
temporal patterns. During the batch training, the temporal data points within
the same batch will be processed in parallel, while their temporal dependencies
are neglected. This issue is referred to as temporal discontinuity and
restricts the effective temporal batch size, limiting data parallelism and
reducing MDGNNs' flexibility in industrial applications. This paper studies the
efficient training of MDGNNs at scale, focusing on the temporal discontinuity
in training MDGNNs with large temporal batch sizes. We first conduct a
theoretical study on the impact of temporal batch size on the convergence of
MDGNN training. Based on the analysis, we propose PRES, an iterative
prediction-correction scheme combined with a memory coherence learning
objective to mitigate the effect of temporal discontinuity, enabling MDGNNs to
be trained with significantly larger temporal batches without sacrificing
generalization performance. Experimental results demonstrate that our approach
enables up to a 4x larger temporal batch (3.4x speed-up) during MDGNN training.
\\ ( https://arxiv.org/abs/2402.04284 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04290
Date: Tue, 6 Feb 2024 08:30:47 GMT   (39565kb,D)

Title: CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded
  Modelling
Authors: Junchao Gong, Lei Bai, Peng Ye, Wanghan Xu, Na Liu, Jianhua Dai,
  Xiaokang Yang, Wanli Ouyang
Categories: cs.LG cs.AI
\\
  Precipitation nowcasting based on radar data plays a crucial role in extreme
weather prediction and has broad implications for disaster management. Despite
progresses have been made based on deep learning, two key challenges of
precipitation nowcasting are not well-solved: (i) the modeling of complex
precipitation system evolutions with different scales, and (ii) accurate
forecasts for extreme precipitation. In this work, we propose CasCast, a
cascaded framework composed of a deterministic and a probabilistic part to
decouple the predictions for mesoscale precipitation distributions and
small-scale patterns. Then, we explore training the cascaded framework at the
high resolution and conducting the probabilistic modeling in a low dimensional
latent space with a frame-wise-guided diffusion transformer for enhancing the
optimization of extreme events while reducing computational costs. Extensive
experiments on three benchmark radar precipitation datasets show that CasCast
achieves competitive performance. Especially, CasCast significantly surpasses
the baseline (up to +91.8%) for regional extreme-precipitation nowcasting.
\\ ( https://arxiv.org/abs/2402.04290 ,  39565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04291
Date: Tue, 6 Feb 2024 09:26:34 GMT   (7361kb,D)

Title: BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
Authors: Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang,
  Xianglong Liu, Michele Magno, Xiaojuan Qi
Categories: cs.LG cs.AI cs.CL
Comments: 19 pages
\\
  Pretrained large language models (LLMs) exhibit exceptional general language
processing capabilities but come with significant demands on memory and
computational resources. As a powerful compression technology, binarization can
extremely reduce model weights to a mere 1 bit, lowering the expensive
computation and memory requirements. However, existing quantization techniques
fall short of maintaining LLM performance under ultra-low bit-widths. In
response to this challenge, we present BiLLM, a groundbreaking 1-bit
post-training quantization scheme tailored for pretrained LLMs. Based on the
weight distribution of LLMs, BiLLM first identifies and structurally selects
salient weights, and minimizes the compression loss through an effective binary
residual approximation strategy. Moreover, considering the bell-shaped
distribution of the non-salient weights, we propose an optimal splitting search
to group and binarize them accurately. BiLLM achieving for the first time
high-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit
weights across various LLMs families and evaluation metrics, outperforms SOTA
quantization methods of LLM by significant margins. Moreover, BiLLM enables the
binarization process of the LLM with 7 billion weights within 0.5 hours on a
single GPU, demonstrating satisfactory time efficiency.
\\ ( https://arxiv.org/abs/2402.04291 ,  7361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04292
Date: Tue, 6 Feb 2024 10:15:38 GMT   (2963kb,D)

Title: AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies
Authors: Xixi Hu, Bo Liu, Xingchao Liu and Qiang Liu
Categories: cs.LG cs.AI
Comments: 18 pages
\\
  Diffusion-based imitation learning improves Behavioral Cloning (BC) on
multi-modal decision-making, but comes at the cost of significantly slower
inference due to the recursion in the diffusion process. It urges us to design
efficient policy generators while keeping the ability to generate diverse
actions. To address this challenge, we propose AdaFlow, an imitation learning
framework based on flow-based generative modeling. AdaFlow represents the
policy with state-conditioned ordinary differential equations (ODEs), which are
known as probability flows. We reveal an intriguing connection between the
conditional variance of their training loss and the discretization error of the
ODEs. With this insight, we propose a variance-adaptive ODE solver that can
adjust its step size in the inference stage, making AdaFlow an adaptive
decision-maker, offering rapid inference without sacrificing diversity.
Interestingly, it automatically reduces to a one-step generator when the action
distribution is uni-modal. Our comprehensive empirical evaluation shows that
AdaFlow achieves high performance across all dimensions, including success
rate, behavioral diversity, and inference speed. The code is available at
https://github.com/hxixixh/AdaFlow
\\ ( https://arxiv.org/abs/2402.04292 ,  2963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04296
Date: Tue, 6 Feb 2024 14:40:26 GMT   (4915kb,D)

Title: LightHGNN: Distilling Hypergraph Neural Networks into MLPs for
  $100\times$ Faster Inference
Authors: Yifan Feng, Yihe Luo, Shihui Ying, Yue Gao
Categories: cs.LG
Comments: ICLR 2024
\\
  Hypergraph Neural Networks (HGNNs) have recently attracted much attention and
exhibited satisfactory performance due to their superiority in high-order
correlation modeling. However, it is noticed that the high-order modeling
capability of hypergraph also brings increased computation complexity, which
hinders its practical industrial deployment. In practice, we find that one key
barrier to the efficient deployment of HGNNs is the high-order structural
dependencies during inference. In this paper, we propose to bridge the gap
between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to
eliminate the hypergraph dependency of HGNNs and thus reduce computational
complexity as well as improve inference speed. Specifically, we introduce
LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN
directly distills the knowledge from teacher HGNNs to student MLPs via soft
labels, and LightHGNN$^+$ further explicitly injects reliable high-order
correlations into the student MLPs to achieve topology-aware distillation and
resistance to over-smoothing. Experiments on eight hypergraph datasets
demonstrate that even without hypergraph dependency, the proposed LightHGNNs
can still achieve competitive or even better performance than HGNNs and
outperform vanilla MLPs by $16.3$ on average. Extensive experiments on three
graph datasets further show the average best performance of our LightHGNNs
compared with all other methods. Experiments on synthetic hypergraphs with 5.5w
vertices indicate LightHGNNs can run $100\times$ faster than HGNNs, showcasing
their ability for latency-sensitive deployments.
\\ ( https://arxiv.org/abs/2402.04296 ,  4915kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04298
Date: Tue, 6 Feb 2024 15:53:49 GMT   (216kb,D)

Title: Multi-View Symbolic Regression
Authors: Etienne Russeil, Fabr\'icio Olivetti de Fran\c{c}a, Konstantin
  Malanchev, Bogdan Burlacu, Emille E. O. Ishida, Marion Leroux, Cl\'ement
  Michelin, Guillaume Moinard, Emmanuel Gangler
Categories: cs.LG astro-ph.IM stat.AP
Comments: Submitted to GECCO-2024. 10 pages, 6 figures
\\
  Symbolic regression (SR) searches for analytical expressions representing the
relationship between a set of explanatory and response variables. Current SR
methods assume a single dataset extracted from a single experiment.
Nevertheless, frequently, the researcher is confronted with multiple sets of
results obtained from experiments conducted with different setups. Traditional
SR methods may fail to find the underlying expression since the parameters of
each experiment can be different. In this work we present Multi-View Symbolic
Regression (MvSR), which takes into account multiple datasets simultaneously,
mimicking experimental environments, and outputs a general parametric solution.
This approach fits the evaluated expression to each independent dataset and
returns a parametric family of functions f(x; \theta) simultaneously capable of
accurately fitting all datasets. We demonstrate the effectiveness of MvSR using
data generated from known expressions, as well as real-world data from
astronomy, chemistry and economy, for which an a priori analytical expression
is not available. Results show that MvSR obtains the correct expression more
frequently and is robust to hyperparameters change. In real-world data, it is
able to grasp the group behaviour, recovering known expressions from the
literature as well as promising alternatives, thus enabling the use SR to a
large range of experimental scenarios.
\\ ( https://arxiv.org/abs/2402.04298 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04325
Date: Tue, 6 Feb 2024 19:09:32 GMT   (847kb,D)

Title: Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to
  Non-Essential Neurons
Authors: Zhenyu Liu, Garrett Gagnon, Swagath Venkataramani, Liu Liu
Categories: cs.LG cs.AI cs.CR
\\
  Deep Neural Networks (DNNs) have revolutionized a wide range of industries,
from healthcare and finance to automotive, by offering unparalleled
capabilities in data analysis and decision-making. Despite their transforming
impact, DNNs face two critical challenges: the vulnerability to adversarial
attacks and the increasing computational costs associated with more complex and
larger models. In this paper, we introduce an effective method designed to
simultaneously enhance adversarial robustness and execution efficiency. Unlike
prior studies that enhance robustness via uniformly injecting noise, we
introduce a non-uniform noise injection algorithm, strategically applied at
each DNN layer to disrupt adversarial perturbations introduced in attacks. By
employing approximation techniques, our approach identifies and protects
essential neurons while strategically introducing noise into non-essential
neurons. Our experimental results demonstrate that our method successfully
enhances both robustness and efficiency across several attack scenarios, model
architectures, and datasets.
\\ ( https://arxiv.org/abs/2402.04325 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04344
Date: Tue, 6 Feb 2024 19:27:48 GMT   (269kb,D)

Title: Does Confidence Calibration Help Conformal Prediction?
Authors: Huajun Xi, Jianguo Huang, Lei Feng, Hongxin Wei
Categories: cs.LG
\\
  Conformal prediction, as an emerging uncertainty qualification technique,
constructs prediction sets that are guaranteed to contain the true label with
high probability. Previous works usually employ temperature scaling to
calibrate the classifier, assuming that confidence calibration can benefit
conformal prediction. In this work, we first show that post-hoc calibration
methods surprisingly lead to larger prediction sets with improved calibration,
while over-confidence with small temperatures benefits the conformal prediction
performance instead. Theoretically, we prove that high confidence reduces the
probability of appending a new class in the prediction set. Inspired by the
analysis, we propose a novel method, $\textbf{Conformal Temperature Scaling}$
(ConfTS), which rectifies the objective through the gap between the threshold
and the non-conformity score of the ground-truth label. In this way, the new
objective of ConfTS will optimize the temperature value toward an optimal set
that satisfies the $\textit{marginal coverage}$. Experiments demonstrate that
our method can effectively improve widely-used conformal prediction methods.
\\ ( https://arxiv.org/abs/2402.04344 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04347
Date: Tue, 6 Feb 2024 19:31:26 GMT   (19731kb,D)

Title: The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax
  Mimicry
Authors: Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher R\'e
Categories: cs.LG cs.CL
Comments: 30 pages, 20 figures, 15 tables, ICLR 2024
\\
  Linear attentions have shown potential for improving Transformer efficiency,
reducing attention's quadratic complexity to linear in sequence length. This
holds exciting promise for (1) training linear Transformers from scratch, (2)
"finetuned-conversion" of task-specific Transformers into linear versions that
recover task performance, and (3) "pretrained-conversion" of Transformers such
as large language models into linear versions finetunable on downstream tasks.
However, linear attentions often underperform standard softmax attention in
quality. To close this performance gap, we find prior linear attentions lack
key properties of softmax attention tied to good performance: low-entropy (or
"spiky") weights and dot-product monotonicity. We further observe surprisingly
simple feature maps that retain these properties and match softmax performance,
but are inefficient to compute in linear attention. We thus propose Hedgehog, a
learnable linear attention that retains the spiky and monotonic properties of
softmax attention while maintaining linear complexity. Hedgehog uses simple
trainable MLPs to produce attention weights mimicking softmax attention.
Experiments show Hedgehog recovers over 99% of standard Transformer quality in
train-from-scratch and finetuned-conversion settings, outperforming prior
linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,
and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also
enables pretrained-conversion. Converting a pretrained GPT-2 into a linear
attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for
125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into
a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B
achieves 28.1 higher ROUGE-1 points over the base standard attention model,
where prior linear attentions lead to 16.5 point drops.
\\ ( https://arxiv.org/abs/2402.04347 ,  19731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04359
Date: Tue, 6 Feb 2024 19:49:23 GMT   (676kb,D)

Title: Adaptive Inference: Theoretical Limits and Unexplored Opportunities
Authors: Soheil Hor, Ying Qian, Mert Pilanci, Amin Arbabian
Categories: cs.LG
\\
  This paper introduces the first theoretical framework for quantifying the
efficiency and performance gain opportunity size of adaptive inference
algorithms. We provide new approximate and exact bounds for the achievable
efficiency and performance gains, supported by empirical evidence demonstrating
the potential for 10-100x efficiency improvements in both Computer Vision and
Natural Language Processing tasks without incurring any performance penalties.
Additionally, we offer insights on improving achievable efficiency gains
through the optimal selection and design of adaptive inference state spaces.
\\ ( https://arxiv.org/abs/2402.04359 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04362
Date: Tue, 6 Feb 2024 20:03:35 GMT   (2200kb,D)

Title: Neural Networks Learn Statistics of Increasing Complexity
Authors: Nora Belrose, Quintin Pope, Lucia Quirke, Alex Mallen, Xiaoli Fern
Categories: cs.LG
\\
  The distributional simplicity bias (DSB) posits that neural networks learn
low-order moments of the data distribution first, before moving on to
higher-order correlations. In this work, we present compelling new evidence for
the DSB by showing that networks automatically learn to perform well on
maximum-entropy distributions whose low-order statistics match those of the
training set early in training, then lose this ability later. We also extend
the DSB to discrete domains by proving an equivalence between token $n$-gram
frequencies and the moments of embedding vectors, and by finding empirical
evidence for the bias in LLMs. Finally we use optimal transport methods to
surgically edit the low-order statistics of one class to match those of
another, and show that early-training networks treat the edited samples as if
they were drawn from the target class. Code is available at
https://github.com/EleutherAI/features-across-time.
\\ ( https://arxiv.org/abs/2402.04362 ,  2200kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04375
Date: Tue, 6 Feb 2024 20:24:07 GMT   (726kb,D)

Title: Bounding the Excess Risk for Linear Models Trained on
  Marginal-Preserving, Differentially-Private, Synthetic Data
Authors: Yvonne Zhou, Mingyu Liang, Ivan Brugere, Dana Dachman-Soled, Danial
  Dervovic, Antigoni Polychroniadou, Min Wu
Categories: cs.LG cs.CR
\\
  The growing use of machine learning (ML) has raised concerns that an ML model
may reveal private information about an individual who has contributed to the
training dataset. To prevent leakage of sensitive data, we consider using
differentially-private (DP), synthetic training data instead of real training
data to train an ML model. A key desirable property of synthetic data is its
ability to preserve the low-order marginals of the original distribution. Our
main contribution comprises novel upper and lower bounds on the excess
empirical risk of linear models trained on such synthetic data, for continuous
and Lipschitz loss functions. We perform extensive experimentation alongside
our theoretical results.
\\ ( https://arxiv.org/abs/2402.04375 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04376
Date: Tue, 6 Feb 2024 20:30:19 GMT   (1070kb,D)

Title: Scaling laws for learning with real and surrogate data
Authors: Ayush Jain, Andrea Montanari and Eren Sasoglu
Categories: cs.LG cs.AI stat.ML
\\
  Collecting large quantities of high-quality data is often prohibitively
expensive or impractical, and a crucial bottleneck in machine learning. One may
instead augment a small set of $n$ data points from the target distribution
with data from more accessible sources like public datasets, data collected
under different circumstances, or synthesized by generative models. Blurring
distinctions, we refer to such data as `surrogate data'.
  We define a simple scheme for integrating surrogate data into training and
use both theoretical models and empirical studies to explore its behavior. Our
main findings are: $(i)$ Integrating surrogate data can significantly reduce
the test error on the original distribution; $(ii)$ In order to reap this
benefit, it is crucial to use optimally weighted empirical risk minimization;
$(iii)$ The test error of models trained on mixtures of real and surrogate data
is well described by a scaling law. This can be used to predict the optimal
weighting and the gain from surrogate data.
\\ ( https://arxiv.org/abs/2402.04376 ,  1070kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04377
Date: Tue, 6 Feb 2024 20:31:15 GMT   (970kb,D)

Title: $\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient
  Distributed Prediction Serving Systems
Authors: Parsa Moradi, Mohammad Ali Maddah-Ali
Categories: cs.LG cs.DC cs.IT math.IT
\\
  Resilience against stragglers is a critical element of prediction serving
systems, tasked with executing inferences on input data for a pre-trained
machine-learning model. In this paper, we propose NeRCC, as a general
straggler-resistant framework for approximate coded computing. NeRCC includes
three layers: (1) encoding regression and sampling, which generates coded data
points, as a combination of original data points, (2) computing, in which a
cluster of workers run inference on the coded data points, (3) decoding
regression and sampling, which approximately recovers the predictions of the
original data points from the available predictions on the coded data points.
We argue that the overall objective of the framework reveals an underlying
interconnection between two regression models in the encoding and decoding
layers. We propose a solution to the nested regressions problem by summarizing
their dependence on two regularization terms that are jointly optimized. Our
extensive experiments on different datasets and various machine learning
models, including LeNet5, RepVGG, and Vision Transformer (ViT), demonstrate
that NeRCC accurately approximates the original predictions in a wide range of
stragglers, outperforming the state-of-the-art by up to 23%.
\\ ( https://arxiv.org/abs/2402.04377 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04379
Date: Tue, 6 Feb 2024 20:35:28 GMT   (986kb,D)

Title: Fine-Tuned Language Models Generate Stable Inorganic Materials as Text
Authors: Nate Gruver, Anuroop Sriram, Andrea Madotto, Andrew Gordon Wilson, C.
  Lawrence Zitnick, Zachary Ulissi
Categories: cs.LG cond-mat.mtrl-sci
Comments: ICLR 2024. Code available at:
  https://github.com/facebookresearch/crystal-llm
\\
  We propose fine-tuning large language models for generation of stable
materials. While unorthodox, fine-tuning large language models on text-encoded
atomistic data is simple to implement yet reliable, with around 90% of sampled
structures obeying physical constraints on atom positions and charges. Using
energy above hull calculations from both learned ML potentials and
gold-standard DFT calculations, we show that our strongest model (fine-tuned
LLaMA-2 70B) can generate materials predicted to be metastable at about twice
the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text
prompting's inherent flexibility, our models can simultaneously be used for
unconditional generation of stable material, infilling of partial structures
and text-conditional generation. Finally, we show that language models' ability
to capture key symmetries of crystal structures improves with model scale,
suggesting that the biases of pretrained LLMs are surprisingly well-suited for
atomistic data.
\\ ( https://arxiv.org/abs/2402.04379 ,  986kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04383
Date: Tue, 6 Feb 2024 20:43:00 GMT   (3612kb,D)

Title: FairWire: Fair Graph Generation
Authors: O. Deniz Kose and Yanning Shen
Categories: cs.LG cs.CY
Comments: 16 pages, 1 figure, 7 tables
\\
  Machine learning over graphs has recently attracted growing attention due to
its ability to analyze and learn complex relations within critical
interconnected systems. However, the disparate impact that is amplified by the
use of biased graph structures in these algorithms has raised significant
concerns for the deployment of them in real-world decision systems. In
addition, while synthetic graph generation has become pivotal for privacy and
scalability considerations, the impact of generative learning algorithms on the
structural bias has not yet been investigated. Motivated by this, this work
focuses on the analysis and mitigation of structural bias for both real and
synthetic graphs. Specifically, we first theoretically analyze the sources of
structural bias that result in disparity for the predictions of dyadic
relations. To alleviate the identified bias factors, we design a novel fairness
regularizer that offers a versatile use. Faced with the bias amplification in
graph generation models that is brought to light in this work, we further
propose a fair graph generation framework, FairWire, by leveraging our fair
regularizer design in a generative model. Experimental results on real-world
networks validate that the proposed tools herein deliver effective structural
bias mitigation for both real and synthetic graphs.
\\ ( https://arxiv.org/abs/2402.04383 ,  3612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04384
Date: Tue, 6 Feb 2024 20:43:04 GMT   (911kb,D)

Title: Denoising Diffusion Probabilistic Models in Six Simple Steps
Authors: Richard E. Turner, Cristiana-Diana Diaconu, Stratis Markou,
  Aliaksandra Shysheya, Andrew Y. K. Foong and Bruno Mlodozeniec
Categories: cs.LG stat.ML
\\
  Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of
deep generative model that have been successfully applied to a diverse range of
problems including image and video generation, protein and material synthesis,
weather forecasting, and neural surrogates of partial differential equations.
Despite their ubiquity it is hard to find an introduction to DDPMs which is
simple, comprehensive, clean and clear. The compact explanations necessary in
research papers are not able to elucidate all of the different design steps
taken to formulate the DDPM and the rationale of the steps that are presented
is often omitted to save space. Moreover, the expositions are typically
presented from the variational lower bound perspective which is unnecessary and
arguably harmful as it obfuscates why the method is working and suggests
generalisations that do not perform well in practice. On the other hand,
perspectives that take the continuous time-limit are beautiful and general, but
they have a high barrier-to-entry as they require background knowledge of
stochastic differential equations and probability flow. In this note, we
distill down the formulation of the DDPM into six simple steps each of which
comes with a clear rationale. We assume that the reader is familiar with
fundamental topics in machine learning including basic probabilistic modelling,
Gaussian distributions, maximum likelihood estimation, and deep learning.
\\ ( https://arxiv.org/abs/2402.04384 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04390
Date: Tue, 6 Feb 2024 20:45:31 GMT   (955kb)

Title: Densely Multiplied Physics Informed Neural Network
Authors: Feilong Jiang, Xiaonan Hou, Min Xia
Categories: cs.LG cs.AI
Comments: 15 pages, 9 figures
\\
  Although physics-informed neural networks (PINNs) have shown great potential
in dealing with nonlinear partial differential equations (PDEs), it is common
that PINNs will suffer from the problem of insufficient precision or obtaining
incorrect outcomes. Unlike most of the existing solutions trying to enhance the
ability of PINN by optimizing the training process, this paper improved the
neural network architecture to improve the performance of PINN. We propose a
densely multiply PINN (DM-PINN) architecture, which multiplies the output of a
hidden layer with the outputs of all the behind hidden layers. Without
introducing more trainable parameters, this effective mechanism can
significantly improve the accuracy of PINNs. The proposed architecture is
evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation,
Burgers equation and 1D convection equation). Comparisons between the proposed
architecture and different PINN structures demonstrate the superior performance
of the DM-PINN in both accuracy and efficiency.
\\ ( https://arxiv.org/abs/2402.04390 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04396
Date: Tue, 6 Feb 2024 20:52:12 GMT   (2058kb,D)

Title: QuIP#: Even Better LLM Quantization with Hadamard Incoherence and
  Lattice Codebooks
Authors: Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher
  De Sa
Categories: cs.LG cs.AI cs.CL
Comments: Preprint
\\
  Post-training quantization (PTQ) reduces the memory footprint of LLMs by
quantizing their weights to low-precision. In this work, we introduce QuIP#, a
weight-only PTQ method that achieves state-of-the-art results in extreme
compression regimes ($\le$ 4 bits per weight) using three novel techniques.
First, QuIP# improves the incoherence processing from QuIP by using the
randomized Hadamard transform, which is faster and has better theoretical
properties. Second, QuIP# uses vector quantization techniques to take advantage
of the ball-shaped sub-Gaussian distribution that incoherent weights possess:
specifically, we introduce a set of hardware-efficient codebooks based on the
highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit
ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original
model. Our experiments show that QuIP# outperforms existing PTQ methods,
enables new behaviors in PTQ scaling, and supports fast inference.
\\ ( https://arxiv.org/abs/2402.04396 ,  2058kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04398
Date: Tue, 6 Feb 2024 20:56:31 GMT   (2021kb,D)

Title: Learning from Time Series under Temporal Label Noise
Authors: Sujay Nagaraj, Walter Gerych, Sana Tonekaboni, Anna Goldenberg, Berk
  Ustun, Thomas Hartvigsen
Categories: cs.LG cs.AI stat.ML
\\
  Many sequential classification tasks are affected by label noise that varies
over time. Such noise can cause label quality to improve, worsen, or
periodically change over time. We first propose and formalize temporal label
noise, an unstudied problem for sequential classification of time series. In
this setting, multiple labels are recorded in sequence while being corrupted by
a time-dependent noise function. We first demonstrate the importance of
modelling the temporal nature of the label noise function and how existing
methods will consistently underperform. We then propose methods that can train
noise-tolerant classifiers by estimating the temporal label noise function
directly from data. We show that our methods lead to state-of-the-art
performance in the presence of diverse temporal label noise functions using
real and synthetic data.
\\ ( https://arxiv.org/abs/2402.04398 ,  2021kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04400
Date: Tue, 6 Feb 2024 20:58:36 GMT   (10987kb,D)

Title: CEHR-GPT: Generating Electronic Health Records with Chronological
  Patient Timelines
Authors: Chao Pang, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S.
  Kalluri, Elise L. Minto, Jason Patterson, Linying Zhang, George Hripcsak,
  No\'emie Elhadad, Karthik Natarajan
Categories: cs.LG cs.AI cs.CY
\\
  Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in
advancing healthcare applications and machine learning models, particularly for
researchers without direct access to healthcare data. Although existing
methods, like rule-based approaches and generative adversarial networks (GANs),
generate synthetic data that resembles real-world EHR data, these methods often
use a tabular format, disregarding temporal dependencies in patient histories
and limiting data replication. Recently, there has been a growing interest in
leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables
applications like disease progression analysis, population estimation,
counterfactual reasoning, and synthetic data generation. In this work, we focus
on synthetic data generation and demonstrate the capability of training a GPT
model using a particular patient representation derived from CEHR-BERT,
enabling us to generate patient sequences that can be seamlessly converted to
the Observational Medical Outcomes Partnership (OMOP) data format.
\\ ( https://arxiv.org/abs/2402.04400 ,  10987kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04409
Date: Tue, 6 Feb 2024 21:07:12 GMT   (183kb,D)

Title: Towards Fair, Robust and Efficient Client Contribution Evaluation in
  Federated Learning
Authors: Meiying Zhang, Huan Zhao, Sheldon Ebron, Kan Yang
Categories: cs.LG cs.AI cs.CR cs.DC
\\
  The performance of clients in Federated Learning (FL) can vary due to various
reasons. Assessing the contributions of each client is crucial for client
selection and compensation. It is challenging because clients often have
non-independent and identically distributed (non-iid) data, leading to
potentially noisy or divergent updates. The risk of malicious clients amplifies
the challenge especially when there's no access to clients' local data or a
benchmark root dataset. In this paper, we introduce a novel method called Fair,
Robust, and Efficient Client Assessment (FRECA) for quantifying client
contributions in FL. FRECA employs a framework called FedTruth to estimate the
global model's ground truth update, balancing contributions from all clients
while filtering out impacts from malicious ones. This approach is robust
against Byzantine attacks and incorporates a Byzantine-resilient aggregation
algorithm. FRECA is also efficient, as it operates solely on local model
updates and requires no validation operations or datasets. Our experimental
results show that FRECA can accurately and efficiently quantify client
contributions in a robust manner.
\\ ( https://arxiv.org/abs/2402.04409 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04412
Date: Tue, 6 Feb 2024 21:18:34 GMT   (29934kb,D)

Title: The VampPrior Mixture Model
Authors: Andrew Stirn and David A. Knowles
Categories: cs.LG cs.AI stat.ML
\\
  Current clustering priors for deep latent variable models (DLVMs) require
defining the number of clusters a-priori and are susceptible to poor
initializations. Addressing these deficiencies could greatly benefit deep
learning-based scRNA-seq analysis by performing integration and clustering
simultaneously. We adapt the VampPrior (Tomczak & Welling, 2018) into a
Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture
Model (VMM), a novel prior for DLVMs. We propose an inference procedure that
alternates between variational inference and Empirical Bayes to cleanly
distinguish variational and prior parameters. Using the VMM in a Variational
Autoencoder attains highly competitive clustering performance on benchmark
datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration
method, with the VMM significantly improves its performance and automatically
arranges cells into biologically meaningful clusters.
\\ ( https://arxiv.org/abs/2402.04412 ,  29934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04417
Date: Tue, 6 Feb 2024 21:33:34 GMT   (39kb)

Title: Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit
Authors: Mengfan Xu, Diego Klabjan
Categories: cs.LG cs.MA
Comments: 16 pages
\\
  We study a robust multi-agent multi-armed bandit problem where multiple
clients or participants are distributed on a fully decentralized blockchain,
with the possibility of some being malicious. The rewards of arms are
homogeneous among the clients, following time-invariant stochastic
distributions that are revealed to the participants only when the system is
secure enough. The system's objective is to efficiently ensure the cumulative
rewards gained by the honest participants. To this end and to the best of our
knowledge, we are the first to incorporate advanced techniques from
blockchains, as well as novel mechanisms, into the system to design optimal
strategies for honest participants. This allows various malicious behaviors and
the maintenance of participant privacy. More specifically, we randomly select a
pool of validators who have access to all participants, design a brand-new
consensus mechanism based on digital signatures for these validators, invent a
UCB-based strategy that requires less information from participants through
secure multi-party computation, and design the chain-participant interaction
and an incentive mechanism to encourage participants' participation. Notably,
we are the first to prove the theoretical guarantee of the proposed algorithms
by regret analyses in the context of optimality in blockchains. Unlike existing
work that integrates blockchains with learning problems such as federated
learning which mainly focuses on numerical optimality, we demonstrate that the
regret of honest participants is upper bounded by $log{T}$. This is consistent
with the multi-agent multi-armed bandit problem without malicious participants
and the robust multi-agent multi-armed bandit problem with purely Byzantine
attacks.
\\ ( https://arxiv.org/abs/2402.04417 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04435
Date: Tue, 6 Feb 2024 22:13:49 GMT   (412kb,D)

Title: PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep
  Intellectual Property Protection
Authors: Enyan Dai, Minhua Lin, Suhang Wang
Categories: cs.LG cs.AI
\\
  Pretraining on Graph Neural Networks (GNNs) has shown great power in
facilitating various downstream tasks. As pretraining generally requires huge
amount of data and computational resources, the pretrained GNNs are high-value
Intellectual Properties (IP) of the legitimate owner. However, adversaries may
illegally copy and deploy the pretrained GNN models for their downstream tasks.
Though initial efforts have been made to watermark GNN classifiers for IP
protection, these methods require the target classification task for
watermarking, and thus are not applicable to self-supervised pretraining of GNN
models. Hence, in this work, we propose a novel framework named PreGIP to
watermark the pretraining of GNN encoder for IP protection while maintain the
high-quality of the embedding space. PreGIP incorporates a task-free
watermarking loss to watermark the embedding space of pretrained GNN encoder. A
finetuning-resistant watermark injection is further deployed. Theoretical
analysis and extensive experiments show the effectiveness of {\method} in IP
protection and maintaining high-performance for downstream tasks.
\\ ( https://arxiv.org/abs/2402.04435 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04440
Date: Tue, 6 Feb 2024 22:20:53 GMT   (25978kb,D)

Title: Exploring higher-order neural network node interactions with total
  correlation
Authors: Thomas Kerby, Teresa White, Kevin Moon
Categories: cs.LG stat.ML
\\
  In domains such as ecological systems, collaborations, and the human brain
the variables interact in complex ways. Yet accurately characterizing
higher-order variable interactions (HOIs) is a difficult problem that is
further exacerbated when the HOIs change across the data. To solve this problem
we propose a new method called Local Correlation Explanation (CorEx) to capture
HOIs at a local scale by first clustering data points based on their proximity
on the data manifold. We then use a multivariate version of the mutual
information called the total correlation, to construct a latent factor
representation of the data within each cluster to learn the local HOIs. We use
Local CorEx to explore HOIs in synthetic and real world data to extract hidden
insights about the data structure. Lastly, we demonstrate Local CorEx's
suitability to explore and interpret the inner workings of trained neural
networks.
\\ ( https://arxiv.org/abs/2402.04440 ,  25978kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04467
Date: Tue, 6 Feb 2024 23:26:12 GMT   (10699kb,D)

Title: DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic
  Systems
Authors: Yair Schiff, Zhong Yi Wan, Jeffrey B. Parker, Stephan Hoyer, Volodymyr
  Kuleshov, Fei Sha, Leonardo Zepeda-N\'u\~nez
Categories: cs.LG math.DS
\\
  Learning dynamics from dissipative chaotic systems is notoriously difficult
due to their inherent instability, as formalized by their positive Lyapunov
exponents, which exponentially amplify errors in the learned dynamics. However,
many of these systems exhibit ergodicity and an attractor: a compact and highly
complex manifold, to which trajectories converge in finite-time, that supports
an invariant measure, i.e., a probability distribution that is invariant under
the action of the dynamics, which dictates the long-term statistical behavior
of the system. In this work, we leverage this structure to propose a new
framework that targets learning the invariant measure as well as the dynamics,
in contrast with typical methods that only target the misfit between
trajectories, which often leads to divergence as the trajectories' length
increases. We use our framework to propose a tractable and sample efficient
objective that can be used with any existing learning objectives. Our Dynamics
Stable Learning by Invariant Measures (DySLIM) objective enables model training
that achieves better point-wise tracking and long-term statistical accuracy
relative to other learning objectives. By targeting the distribution with a
scalable regularization term, we hope that this approach can be extended to
more complex systems exhibiting slowly-variant distributions, such as weather
and climate models.
\\ ( https://arxiv.org/abs/2402.04467 ,  10699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04469
Date: Tue, 6 Feb 2024 23:28:15 GMT   (403kb,D)

Title: IoT Network Traffic Analysis with Deep Learning
Authors: Mei Liu and Leon Yang
Categories: cs.LG cs.CR cs.NI
Comments: PerCom 2024 Workshop
\\
  As IoT networks become more complex and generate massive amounts of dynamic
data, it is difficult to monitor and detect anomalies using traditional
statistical methods and machine learning methods. Deep learning algorithms can
process and learn from large amounts of data and can also be trained using
unsupervised learning techniques, meaning they don't require labelled data to
detect anomalies. This makes it possible to detect new and unknown anomalies
that may not have been detected before. Also, deep learning algorithms can be
automated and highly scalable; thereby, they can run continuously in the
backend and make it achievable to monitor large IoT networks instantly. In this
work, we conduct a literature review on the most recent works using deep
learning techniques and implement a model using ensemble techniques on the KDD
Cup 99 dataset. The experimental results showcase the impressive performance of
our deep anomaly detection model, achieving an accuracy of over 98\%.
\\ ( https://arxiv.org/abs/2402.04469 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04485
Date: Wed, 7 Feb 2024 00:23:20 GMT   (1379kb,D)

Title: Incentivized Truthful Communication for Federated Bandits
Authors: Zhepei Wei, Chuanhao Li, Tianze Ren, Haifeng Xu, Hongning Wang
Categories: cs.LG cs.GT
Comments: 20 pages, 2 figures. Accepted at ICLR 2024
\\
  To enhance the efficiency and practicality of federated bandit learning,
recent advances have introduced incentives to motivate communication among
clients, where a client participates only when the incentive offered by the
server outweighs its participation cost. However, existing incentive mechanisms
naively assume the clients are truthful: they all report their true cost and
thus the higher cost one participating client claims, the more the server has
to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming
to optimize their own utility by misreporting. To address this issue, we
propose an incentive compatible (i.e., truthful) communication protocol, named
Truth-FedBan, where the incentive for each participant is independent of its
self-reported cost, and reporting the true cost is the only way to achieve the
best utility. More importantly, Truth-FedBan still guarantees the sub-linear
regret and communication cost without any overheads. In other words, the core
conceptual contribution of this paper is, for the first time, demonstrating the
possibility of simultaneously achieving incentive compatibility and nearly
optimal regret in federated bandit learning. Extensive numerical studies
further validate the effectiveness of our proposed solution.
\\ ( https://arxiv.org/abs/2402.04485 ,  1379kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04489
Date: Wed, 7 Feb 2024 00:30:58 GMT   (3634kb,D)

Title: De-amplifying Bias from Differential Privacy in Language Model
  Fine-tuning
Authors: Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat,
  Anupam Datta, John C Mitchell
Categories: cs.LG cs.CR cs.CY stat.ME
\\
  Fairness and privacy are two important values machine learning (ML)
practitioners often seek to operationalize in models. Fairness aims to reduce
model bias for social/demographic sub-groups. Privacy via differential privacy
(DP) mechanisms, on the other hand, limits the impact of any individual's
training data on the resulting model. The trade-offs between privacy and
fairness goals of trustworthy ML pose a challenge to those wishing to address
both. We show that DP amplifies gender, racial, and religious bias when
fine-tuning large language models (LLMs), producing models more biased than
ones fine-tuned without DP. We find the cause of the amplification to be a
disparity in convergence of gradients across sub-groups. Through the case of
binary gender bias, we demonstrate that Counterfactual Data Augmentation (CDA),
a known method for addressing bias, also mitigates bias amplification by DP. As
a consequence, DP and CDA together can be used to fine-tune models while
maintaining both fairness and privacy.
\\ ( https://arxiv.org/abs/2402.04489 ,  3634kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04494
Date: Wed, 7 Feb 2024 00:36:24 GMT   (2737kb,D)

Title: Grandmaster-Level Chess Without Search
Authors: Anian Ruoss, Gr\'egoire Del\'etang, Sourabh Medapati, Jordi Grau-Moya,
  Li Kevin Wenliang, Elliot Catt, John Reid, Tim Genewein
Categories: cs.LG cs.AI stat.ML
\\
  The recent breakthrough successes in machine learning are mainly attributed
to scale: namely large-scale attention-based architectures and datasets of
unprecedented scale. This paper investigates the impact of training at scale
for chess. Unlike traditional chess engines that rely on complex heuristics,
explicit search, or a combination of both, we train a 270M parameter
transformer model with supervised learning on a dataset of 10 million chess
games. We annotate each board in the dataset with action-values provided by the
powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our
largest model reaches a Lichess blitz Elo of 2895 against humans, and
successfully solves a series of challenging chess puzzles, without any
domain-specific tweaks or explicit search algorithms. We also show that our
model outperforms AlphaZero's policy and value networks (without MCTS) and
GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size
shows that strong chess performance only arises at sufficient scale. To
validate our results, we perform an extensive series of ablations of design
choices and hyperparameters.
\\ ( https://arxiv.org/abs/2402.04494 ,  2737kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04497
Date: Wed, 7 Feb 2024 00:45:31 GMT   (28kb)

Title: The Fine-Grained Complexity of Gradient Computation for Training Large
  Language Models
Authors: Josh Alman, Zhao Song
Categories: cs.LG cs.CC cs.CL cs.DS
\\
  Large language models (LLMs) have made fundamental contributions over the
last a few years. To train an LLM, one needs to alternatingly run `forward'
computations and `backward' computations. The forward computation can be viewed
as attention function evaluation, and the backward computation can be viewed as
a gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it
was proved that the forward step can be performed in almost-linear time in
certain parameter regimes, but that there is no truly sub-quadratic time
algorithm in the remaining parameter regimes unless the popular hypothesis SETH
is false. In this work, we show nearly identical results for the harder-seeming
problem of computing the gradient of loss function of one layer attention
network, and thus for the entire process of LLM training. This completely
characterizes the fine-grained complexity of every step of LLM training.
\\ ( https://arxiv.org/abs/2402.04497 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04513
Date: Wed, 7 Feb 2024 01:46:50 GMT   (1975kb,D)

Title: Online Cascade Learning for Efficient Inference over Streams
Authors: Lunyiu Nie, Zhimin Ding, Erdong Hu, Christopher Jermaine, Swarat
  Chaudhuri
Categories: cs.LG cs.CL
\\
  Large Language Models (LLMs) have a natural role in answering complex queries
about data streams, but the high computational cost of LLM inference makes them
infeasible in many such tasks. We propose online cascade learning, the first
approach to addressing this challenge. The objective here is to learn a
"cascade" of models, starting with lower-capacity models (such as logistic
regressors) and ending with a powerful LLM, along with a deferral policy that
determines the model that is used on a given input. We formulate the task of
learning cascades online as an imitation-learning problem and give a no-regret
algorithm for the problem. Experimental results across four benchmarks show
that our method parallels LLMs in accuracy while cutting down inference costs
by as much as 90%, underscoring its efficacy and adaptability in stream
processing.
\\ ( https://arxiv.org/abs/2402.04513 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04520
Date: Wed, 7 Feb 2024 01:58:21 GMT   (43kb)

Title: On Computational Limits of Modern Hopfield Models: A Fine-Grained
  Complexity Analysis
Authors: Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, Han Liu
Categories: cs.LG cs.AI stat.ML
Comments: 28 pages
\\
  We investigate the computational limits of the memory retrieval dynamics of
modern Hopfield models from the fine-grained complexity analysis. Our key
contribution is the characterization of a phase transition behavior in the
efficiency of all possible modern Hopfield models based on the norm of
patterns. Specifically, we establish an upper bound criterion for the norm of
input query patterns and memory patterns. Only below this criterion,
sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming
the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we
provide a formal example of efficient constructions of modern Hopfield models
using low-rank approximation when the efficient criterion holds. This includes
a derivation of a lower bound on the computational time, scaling linearly with
$\Max\{$# of stored memory patterns, length of input query sequence$\}$. In
addition, we prove its memory retrieval error bound and exponential memory
capacity.
\\ ( https://arxiv.org/abs/2402.04520 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04523
Date: Wed, 7 Feb 2024 02:06:48 GMT   (7128kb,D)

Title: SumRec: A Framework for Recommendation using Open-Domain Dialogue
Authors: Ryutaro Asahara, Masaki Takahashi, Chiho Iwahashi, Michimasa Inaba
Categories: cs.LG cs.CL
Comments: Accepted to PACLIC 2023
\\
  Chat dialogues contain considerable useful information about a speaker's
interests, preferences, and experiences.Thus, knowledge from open-domain chat
dialogue can be used to personalize various systems and offer recommendations
for advanced information.This study proposed a novel framework SumRec for
recommending information from open-domain chat dialogue.The study also examined
the framework using ChatRec, a newly constructed dataset for training and
evaluation. To extract the speaker and item characteristics, the SumRec
framework employs a large language model (LLM) to generate a summary of the
speaker information from a dialogue and to recommend information about an item
according to the type of user.The speaker and item information are then input
into a score estimation model, generating a recommendation score.Experimental
results show that the SumRec framework provides better recommendations than the
baseline method of using dialogues and item descriptions in their original
form. Our dataset and code is publicly available at
https://github.com/Ryutaro-A/SumRec
\\ ( https://arxiv.org/abs/2402.04523 ,  7128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04538
Date: Wed, 7 Feb 2024 02:53:06 GMT   (13212kb,D)

Title: Triplet Interaction Improves Graph Transformers: Accurate Molecular
  Graph Learning with Triplet Graph Transformers
Authors: Md Shamim Hussain, Mohammed J. Zaki and Dharmashankar Subramanian
Categories: cs.LG
Comments: First preprint, 24 pages, 10 figures, 18 tables
\\
  Graph transformers typically lack direct pair-to-pair communication, instead
forcing neighboring pairs to exchange information via a common node. We propose
the Triplet Graph Transformer (TGT) that enables direct communication between
two neighboring pairs in a graph via novel triplet attention and aggregation
mechanisms. TGT is applied to molecular property prediction by first predicting
interatomic distances from 2D graphs and then using these distances for
downstream tasks. A novel three-stage training procedure and stochastic
inference further improve training efficiency and model performance. Our model
achieves new state-of-the-art (SOTA) results on open challenge benchmarks
PCQM4Mv2 and OC20 IS2RE. We also obtain SOTA results on QM9, MOLPCBA, and
LIT-PCBA molecular property prediction benchmarks via transfer learning. We
also demonstrate the generality of TGT with SOTA results on the traveling
salesman problem (TSP).
\\ ( https://arxiv.org/abs/2402.04538 ,  13212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04539
Date: Wed, 7 Feb 2024 02:53:50 GMT   (2429kb,D)

Title: Learning Diverse Policies with Soft Self-Generated Guidance
Authors: Guojian Wang, Faguo Wu, Xiao Zhang, Jianxiang Liu
Categories: cs.LG cs.AI
Comments: 23 pages, 19 figures
Journal-ref: International Journal of Intelligent Systems, Volume 2023
DOI: 10.1155/2023/4705291
\\
  Reinforcement learning (RL) with sparse and deceptive rewards is challenging
because non-zero rewards are rarely obtained. Hence, the gradient calculated by
the agent can be stochastic and without valid information. Recent studies that
utilize memory buffers of previous experiences can lead to a more efficient
learning process. However, existing methods often require these experiences to
be successful and may overly exploit them, which can cause the agent to adopt
suboptimal behaviors. This paper develops an approach that uses diverse past
trajectories for faster and more efficient online RL, even if these
trajectories are suboptimal or not highly rewarded. The proposed algorithm
combines a policy improvement step with an additional exploration step using
offline demonstration data. The main contribution of this paper is that by
regarding diverse past trajectories as guidance, instead of imitating them, our
method directs its policy to follow and expand past trajectories while still
being able to learn without rewards and approach optimality. Furthermore, a
novel diversity measurement is introduced to maintain the team's diversity and
regulate exploration. The proposed algorithm is evaluated on discrete and
continuous control tasks with sparse and deceptive rewards. Compared with the
existing RL methods, the experimental results indicate that our proposed
algorithm is significantly better than the baseline methods regarding diverse
exploration and avoiding local optima.
\\ ( https://arxiv.org/abs/2402.04539 ,  2429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04553
Date: Wed, 7 Feb 2024 03:18:00 GMT   (4797kb,D)

Title: Curvature-Informed SGD via General Purpose Lie-Group Preconditioners
Authors: Omead Pooladzandi and Xi-Lin Li
Categories: cs.LG
\\
  We present a novel approach to accelerate stochastic gradient descent (SGD)
by utilizing curvature information obtained from Hessian-vector products or
finite differences of parameters and gradients, similar to the BFGS algorithm.
Our approach involves two preconditioners: a matrix-free preconditioner and a
low-rank approximation preconditioner. We update both preconditioners online
using a criterion that is robust to stochastic gradient noise and does not
require line search or damping. To preserve the corresponding symmetry or
invariance, our preconditioners are constrained to certain connected Lie
groups. The Lie group's equivariance property simplifies the preconditioner
fitting process, while its invariance property eliminates the need for damping,
which is commonly required in second-order optimizers. As a result, the
learning rate for parameter updating and the step size for preconditioner
fitting are naturally normalized, and their default values work well in most
scenarios. Our proposed approach offers a promising direction for improving the
convergence of SGD with low computational overhead. We demonstrate that
Preconditioned SGD (PSGD) outperforms SoTA on Vision, NLP, and RL tasks across
multiple modern deep-learning architectures. We have provided code for
reproducing toy and large scale experiments in this paper.
\\ ( https://arxiv.org/abs/2402.04553 ,  4797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04567
Date: Wed, 7 Feb 2024 04:06:53 GMT   (693kb,D)

Title: OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences
Authors: Chen Wang, Sarah Erfani, Tansu Alpcan, Christopher Leckie
Categories: cs.LG cs.AI
\\
  Anomaly detection in decision-making sequences is a challenging problem due
to the complexity of normality representation learning and the sequential
nature of the task. Most existing methods based on Reinforcement Learning (RL)
are difficult to implement in the real world due to unrealistic assumptions,
such as having access to environment dynamics, reward signals, and online
interactions with the environment. To address these limitations, we propose an
unsupervised method named Offline Imitation Learning based Anomaly Detection
(OIL-AD), which detects anomalies in decision-making sequences using two
extracted behaviour features: action optimality and sequential association. Our
offline learning model is an adaptation of behavioural cloning with a
transformer policy network, where we modify the training process to learn a Q
function and a state value function from normal trajectories. We propose that
the Q function and the state value function can provide sufficient information
about agents' behavioural data, from which we derive two features for anomaly
detection. The intuition behind our method is that the action optimality
feature derived from the Q function can differentiate the optimal action from
others at each local state, and the sequential association feature derived from
the state value function has the potential to maintain the temporal
correlations between decisions (state-action pairs). Our experiments show that
OIL-AD can achieve outstanding online anomaly detection performance with up to
34.8% improvement in F1 score over comparable baselines.
\\ ( https://arxiv.org/abs/2402.04567 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04579
Date: Wed, 7 Feb 2024 04:39:23 GMT   (4300kb,D)

Title: Collective Counterfactual Explanations via Optimal Transport
Authors: Ahmad-Reza Ehyaei, Ali Shirali, Samira Samadi
Categories: cs.LG stat.ME
\\
  Counterfactual explanations provide individuals with cost-optimal actions
that can alter their labels to desired classes. However, if substantial
instances seek state modification, such individual-centric methods can lead to
new competitions and unanticipated costs. Furthermore, these recommendations,
disregarding the underlying data distribution, may suggest actions that users
perceive as outliers. To address these issues, our work proposes a collective
approach for formulating counterfactual explanations, with an emphasis on
utilizing the current density of the individuals to inform the recommended
actions. Our problem naturally casts as an optimal transport problem.
Leveraging the extensive literature on optimal transport, we illustrate how
this collective method improves upon the desiderata of classical counterfactual
explanations. We support our proposal with numerical simulations, illustrating
the effectiveness of the proposed approach and its relation to classic methods.
\\ ( https://arxiv.org/abs/2402.04579 ,  4300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04596
Date: Wed, 7 Feb 2024 05:38:53 GMT   (1567kb,D)

Title: Towards Improved Imbalance Robustness in Continual Multi-Label Learning
  with Dual Output Spiking Architecture (DOSA)
Authors: Sourav Mishra, Shirin Dora and Suresh Sundaram
Categories: cs.LG cs.AI cs.CV
Comments: 8 pages, 4 figures, 4 tables, 45 references. Submitted to IJCNN 2024
\\
  Algorithms designed for addressing typical supervised classification problems
can only learn from a fixed set of samples and labels, making them unsuitable
for the real world, where data arrives as a stream of samples often associated
with multiple labels over time. This motivates the study of task-agnostic
continual multi-label learning problems. While algorithms using deep learning
approaches for continual multi-label learning have been proposed in the recent
literature, they tend to be computationally heavy. Although spiking neural
networks (SNNs) offer a computationally efficient alternative to artificial
neural networks, existing literature has not used SNNs for continual
multi-label learning. Also, accurately determining multiple labels with SNNs is
still an open research problem. This work proposes a dual output spiking
architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss
function is also proposed, improving the multi-label classification performance
of the model by making it more robust to data imbalance. A modified F1 score is
presented to evaluate the effectiveness of the proposed loss function in
handling imbalance. Experiments on several benchmark multi-label datasets show
that DOSA trained with the proposed loss function shows improved robustness to
data imbalance and obtains better continual multi-label learning performance
than CIFDM, a previous state-of-the-art algorithm.
\\ ( https://arxiv.org/abs/2402.04596 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04621
Date: Wed, 7 Feb 2024 07:09:15 GMT   (3857kb,D)

Title: Feature Distribution on Graph Topology Mediates the Effect of Graph
  Convolution: Homophily Perspective
Authors: Soo Yong Lee, Sunwoo Kim, Fanchen Bu, Jaemin Yoo, Jiliang Tang, Kijung
  Shin
Categories: cs.LG
\\
  How would randomly shuffling feature vectors among nodes from the same class
affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs
the dependence between graph topology and features (A-X dependence) for GNNs to
learn from. Surprisingly, we observe a consistent and significant improvement
in GNN performance following the feature shuffle. Having overlooked the impact
of A-X dependence on GNNs, the prior literature does not provide a satisfactory
understanding of the phenomenon. Thus, we raise two research questions. First,
how should A-X dependence be measured, while controlling for potential
confounds? Second, how does A-X dependence affect GNNs? In response, we (i)
propose a principled measure for A-X dependence, (ii) design a random graph
model that controls A-X dependence, (iii) establish a theory on how A-X
dependence relates to graph convolution, and (iv) present empirical analysis on
real-world graphs that aligns with the theory. We conclude that A-X dependence
mediates the effect of graph convolution, such that smaller dependence improves
GNN-based node classification.
\\ ( https://arxiv.org/abs/2402.04621 ,  3857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04640
Date: Wed, 7 Feb 2024 07:57:43 GMT   (637kb,D)

Title: Domain Bridge: Generative model-based domain forensic for black-box
  models
Authors: Jiyi Zhang, Han Fang, Ee-Chien Chang
Categories: cs.LG
\\
  In forensic investigations of machine learning models, techniques that
determine a model's data domain play an essential role, with prior work relying
on large-scale corpora like ImageNet to approximate the target model's domain.
Although such methods are effective in finding broad domains, they often
struggle in identifying finer-grained classes within those domains. In this
paper, we introduce an enhanced approach to determine not just the general data
domain (e.g., human face) but also its specific attributes (e.g., wearing
glasses). Our approach uses an image embedding model as the encoder and a
generative model as the decoder. Beginning with a coarse-grained description,
the decoder generates a set of images, which are then presented to the unknown
target model. Successful classifications by the model guide the encoder to
refine the description, which in turn, are used to produce a more specific set
of images in the subsequent iteration. This iterative refinement narrows down
the exact class of interest. A key strength of our approach lies in leveraging
the expansive dataset, LAION-5B, on which the generative model Stable Diffusion
is trained. This enlarges our search space beyond traditional corpora, such as
ImageNet. Empirical results showcase our method's performance in identifying
specific attributes of a model's input domain, paving the way for more detailed
forensic analyses of deep learning models.
\\ ( https://arxiv.org/abs/2402.04640 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04644
Date: Wed, 7 Feb 2024 08:16:40 GMT   (1391kb,D)

Title: LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different
  Views
Authors: Yuji Roh, Qingyun Liu, Huan Gui, Zhe Yuan, Yujin Tang, Steven Euijong
  Whang, Liang Liu, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao
Categories: cs.LG cs.AI
\\
  Fine-tuning is becoming widely used for leveraging the power of pre-trained
foundation models in new downstream tasks. While there are many successes of
fine-tuning on various tasks, recent studies have observed challenges in the
generalization of fine-tuned models to unseen distributions (i.e.,
out-of-distribution; OOD). To improve OOD generalization, some previous studies
identify the limitations of fine-tuning data and regulate fine-tuning to
preserve the general representation learned from pre-training data. However,
potential limitations in the pre-training data and models are often ignored. In
this paper, we contend that overly relying on the pre-trained representation
may hinder fine-tuning from learning essential representations for downstream
tasks and thus hurt its OOD generalization. It can be especially catastrophic
when new tasks are from different (sub)domains compared to pre-training data.
To address the issues in both pre-training and fine-tuning data, we propose a
novel generalizable fine-tuning method LEVI, where the pre-trained model is
adaptively ensembled layer-wise with a small task-specific model, while
preserving training and inference efficiencies. By combining two complementing
models, LEVI effectively suppresses problematic features in both the
fine-tuning data and pre-trained model and preserves useful features for new
tasks. Broad experiments with large language and vision models show that LEVI
greatly improves fine-tuning generalization via emphasizing different views
from fine-tuning data and pre-trained features.
\\ ( https://arxiv.org/abs/2402.04644 ,  1391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04646
Date: Wed, 7 Feb 2024 08:18:06 GMT   (1726kb,D)

Title: Learning with Diversification from Block Sparse Signal
Authors: Yanhao Zhang, Zhihan Zhu and Yong Xia
Categories: cs.LG math.OC
Comments: 12 pages, 12 figures, 3 tables
\\
  This paper introduces a novel prior called Diversified Block Sparse Prior to
characterize the widespread block sparsity phenomenon in real-world data. By
allowing diversification on variance and correlation matrix, we effectively
address the sensitivity issue of existing block sparse learning methods to
pre-defined block information, which enables adaptive block estimation while
mitigating the risk of overfitting. Based on this, a diversified block sparse
Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual
ascent method for hyperparameter estimation. Moreover, we establish the global
and local optimality theory of our model. Experiments validate the advantages
of DivSBL over existing algorithms.
\\ ( https://arxiv.org/abs/2402.04646 ,  1726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04647
Date: Wed, 7 Feb 2024 08:18:09 GMT   (5408kb,D)

Title: Latent Plan Transformer: Planning as Latent Variable Inference
Authors: Deqian Kong, Dehong Xu, Minglu Zhao, Bo Pang, Jianwen Xie, Andrew
  Lizarraga, Yuhao Huang, Sirui Xie, Ying Nian Wu
Categories: cs.LG
\\
  In tasks aiming for long-term returns, planning becomes necessary. We study
generative modeling for planning with datasets repurposed from offline
reinforcement learning. Specifically, we identify temporal consistency in the
absence of step-wise rewards as one key technical challenge. We introduce the
Latent Plan Transformer (LPT), a novel model that leverages a latent space to
connect a Transformer-based trajectory generator and the final return. LPT can
be learned with maximum likelihood estimation on trajectory-return pairs. In
learning, posterior sampling of the latent variable naturally gathers
sub-trajectories to form a consistent abstraction despite the finite context.
During test time, the latent variable is inferred from an expected return
before policy execution, realizing the idea of planning as inference. It then
guides the autoregressive policy throughout the episode, functioning as a plan.
Our experiments demonstrate that LPT can discover improved decisions from
suboptimal trajectories. It achieves competitive performance across several
benchmarks, including Gym-Mujoco, Maze2D, and Connect Four, exhibiting
capabilities of nuanced credit assignments, trajectory stitching, and
adaptation to environmental contingencies. These results validate that latent
variable inference can be a strong alternative to step-wise reward prompting.
\\ ( https://arxiv.org/abs/2402.04647 ,  5408kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04653
Date: Wed, 7 Feb 2024 08:38:12 GMT   (8094kb,D)

Title: An Over Complete Deep Learning Method for Inverse Problems
Authors: Moshe Eliasof, Eldad Haber, Eran Treister
Categories: cs.LG cs.CV
\\
  Obtaining meaningful solutions for inverse problems has been a major
challenge with many applications in science and engineering. Recent machine
learning techniques based on proximal and diffusion-based methods have shown
promising results. However, as we show in this work, they can also face
challenges when applied to some exemplary problems. We show that similar to
previous works on over-complete dictionaries, it is possible to overcome these
shortcomings by embedding the solution into higher dimensions. The novelty of
the work proposed is that we jointly design and learn the embedding and the
regularizer for the embedding vector. We demonstrate the merit of this approach
on several exemplary and common inverse problems.
\\ ( https://arxiv.org/abs/2402.04653 ,  8094kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04655
Date: Wed, 7 Feb 2024 08:42:48 GMT   (1436kb,D)

Title: Open-Vocabulary Calibration for Vision-Language Models
Authors: Shuoyuan Wang, Jindong Wang, Guoqing Wang, Bob Zhang, Kaiyang Zhou,
  Hongxin Wei
Categories: cs.LG
Comments: Preprrint
\\
  Vision-language models (VLMs) have emerged as formidable tools, showing their
strong capability in handling various open-vocabulary tasks in image
recognition, text-driven visual content generation, and visual chatbots, to
name a few. In recent years, considerable efforts and resources have been
devoted to adaptation methods for improving downstream performance of VLMs,
particularly on parameter-efficient fine-tuning methods like prompt learning.
However, a crucial aspect that has been largely overlooked is the confidence
calibration problem in fine-tuned VLMs, which could greatly reduce reliability
when deploying such models in the real world. This paper bridges the gap by
systematically investigating the confidence calibration problem in the context
of prompt learning and reveals that existing calibration methods are
insufficient to address the problem, especially in the open-vocabulary setting.
To solve the problem, we present a simple and effective approach called
Distance-Aware Calibration (DAC), which is based on scaling the temperature
using as guidance the distance between predicted text labels and base classes.
The experiments with 7 distinct prompt learning methods applied across 11
diverse downstream datasets demonstrate the effectiveness of DAC, which
achieves high efficacy without sacrificing the inference speed.
\\ ( https://arxiv.org/abs/2402.04655 ,  1436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04668
Date: Wed, 7 Feb 2024 08:53:46 GMT   (541kb,D)

Title: A Perspective on Individualized Treatment Effects Estimation from
  Time-series Health Data
Authors: Ghadeer O. Ghosheh, Moritz G\"ogl and Tingting Zhu
Categories: cs.LG
\\
  The burden of diseases is rising worldwide, with unequal treatment efficacy
for patient populations that are underrepresented in clinical trials.
Healthcare, however, is driven by the average population effect of medical
treatments and, therefore, operates in a "one-size-fits-all" approach, not
necessarily what best fits each patient. These facts suggest a pressing need
for methodologies to study individualized treatment effects (ITE) to drive
personalized treatment. Despite the increased interest in
machine-learning-driven ITE estimation models, the vast majority focus on
tabular data with limited review and understanding of methodologies proposed
for time-series electronic health records (EHRs). To this end, this work
provides an overview of ITE works for time-series data and insights into future
research. The work summarizes the latest work in the literature and reviews it
in light of theoretical assumptions, types of treatment settings, and
computational frameworks. Furthermore, this work discusses challenges and
future research directions for ITEs in a time-series setting. We hope this work
opens new directions and serves as a resource for understanding one of the
exciting yet under-studied research areas.
\\ ( https://arxiv.org/abs/2402.04668 ,  541kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04676
Date: Wed, 7 Feb 2024 09:03:04 GMT   (14436kb,D)

Title: Group Distributionally Robust Dataset Distillation with Risk
  Minimization
Authors: Saeed Vahidian, Mingyu Wang, Jianyang Gu, Vyacheslav Kungurtsev, Wei
  Jiang, Yiran Chen
Categories: cs.LG cs.AI cs.CV
\\
  Dataset distillation (DD) has emerged as a widely adopted technique for
crafting a synthetic dataset that captures the essential information of a
training dataset, facilitating the training of accurate neural models. Its
applications span various domains, including transfer learning, federated
learning, and neural architecture search. The most popular methods for
constructing the synthetic data rely on matching the convergence properties of
training the model with the synthetic dataset and the training dataset.
However, targeting the training dataset must be thought of as auxiliary in the
same sense that the training set is an approximate substitute for the
population distribution, and the latter is the data of interest. Yet despite
its popularity, an aspect that remains unexplored is the relationship of DD to
its generalization, particularly across uncommon subgroups. That is, how can we
ensure that a model trained on the synthetic dataset performs well when faced
with samples from regions with low population density? Here, the
representativeness and coverage of the dataset become salient over the
guaranteed training error at inference. Drawing inspiration from
distributionally robust optimization, we introduce an algorithm that combines
clustering with the minimization of a risk measure on the loss to conduct DD.
We provide a theoretical rationale for our approach and demonstrate its
effective generalization and robustness across subgroups through numerical
experiments.
\\ ( https://arxiv.org/abs/2402.04676 ,  14436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04710
Date: Wed, 7 Feb 2024 09:57:39 GMT   (17771kb,D)

Title: Incorporating Retrieval-based Causal Learning with Information
  Bottlenecks for Interpretable Graph Neural Networks
Authors: Jiahua Rao, Jiancong Xie, Hanjing Lin, Shuangjia Zheng, Zhen Wang,
  Yuedong Yang
Categories: cs.LG
\\
  Graph Neural Networks (GNNs) have gained considerable traction for their
capability to effectively process topological data, yet their interpretability
remains a critical concern. Current interpretation methods are dominated by
post-hoc explanations to provide a transparent and intuitive understanding of
GNNs. However, they have limited performance in interpreting complicated
subgraphs and can't utilize the explanation to advance GNN predictions. On the
other hand, transparent GNN models are proposed to capture critical subgraphs.
While such methods could improve GNN predictions, they usually don't perform
well on explanations. Thus, it is desired for a new strategy to better couple
GNN explanation and prediction. In this study, we have developed a novel
interpretable causal GNN framework that incorporates retrieval-based causal
learning with Graph Information Bottleneck (GIB) theory. The framework could
semi-parametrically retrieve crucial subgraphs detected by GIB and compress the
explanatory subgraphs via a causal module. The framework was demonstrated to
consistently outperform state-of-the-art methods, and to achieve 32.71\% higher
precision on real-world explanation scenarios with diverse explanation types.
More importantly, the learned explanations were shown able to also improve GNN
prediction performance.
\\ ( https://arxiv.org/abs/2402.04710 ,  17771kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04732
Date: Wed, 7 Feb 2024 10:33:09 GMT   (74kb,D)

Title: Graph Cuts with Arbitrary Size Constraints Through Optimal Transport
Authors: Chakib Fettal, Lazhar Labiod, Mohamed Nadif
Categories: cs.LG
\\
  A common way of partitioning graphs is through minimum cuts. One drawback of
classical minimum cut methods is that they tend to produce small groups, which
is why more balanced variants such as normalized and ratio cuts have seen more
success. However, we believe that with these variants, the balance constraints
can be too restrictive for some applications like for clustering of imbalanced
datasets, while not being restrictive enough for when searching for perfectly
balanced partitions. Here, we propose a new graph cut algorithm for
partitioning graphs under arbitrary size constraints. We formulate the graph
cut problem as a regularized Gromov-Wasserstein problem. We then propose to
solve it using accelerated proximal GD algorithm which has global convergence
guarantees, results in sparse solutions and only incurs an additional ratio of
$\mathcal{O}(\log(n))$ compared to the classical spectral clustering algorithm
but was seen to be more efficient.
\\ ( https://arxiv.org/abs/2402.04732 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04744
Date: Wed, 7 Feb 2024 10:55:59 GMT   (2625kb,D)

Title: Progressive Gradient Flow for Robust N:M Sparsity Training in
  Transformers
Authors: Abhimanyu Rajeshkumar Bambhaniya, Amir Yazdanbakhsh, Suvinay
  Subramanian, Sheng-Chun Kao, Shivani Agrawal, Utku Evci, Tushar Krishna
Categories: cs.LG cs.AR
Comments: 18 pages, 8 figures, 17 tables. Code is available at
  https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity
\\
  N:M Structured sparsity has garnered significant interest as a result of
relatively modest overhead and improved efficiency. Additionally, this form of
sparsity holds considerable appeal for reducing the memory footprint owing to
their modest representation overhead. There have been efforts to develop
training recipes for N:M structured sparsity, they primarily focus on
low-sparsity regions ($\sim$50\%). Nonetheless, performance of models trained
using these approaches tends to decline when confronted with high-sparsity
regions ($>$80\%). In this work, we study the effectiveness of existing sparse
training recipes at \textit{high-sparsity regions} and argue that these methods
fail to sustain the model quality on par with low-sparsity regions. We
demonstrate that the significant factor contributing to this disparity is the
presence of elevated levels of induced noise in the gradient magnitudes. To
mitigate this undesirable effect, we employ decay mechanisms to progressively
restrict the flow of gradients towards pruned elements. Our approach improves
the model quality by up to 2$\%$ and 5$\%$ in vision and language models at
high sparsity regime, respectively. We also evaluate the trade-off between
model accuracy and training compute cost in terms of FLOPs. At iso-training
FLOPs, our method yields better performance compared to conventional sparse
training recipes, exhibiting an accuracy improvement of up to 2$\%$. The source
code is available at
https://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity.
\\ ( https://arxiv.org/abs/2402.04744 ,  2625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04764
Date: Wed, 7 Feb 2024 11:27:45 GMT   (2489kb,D)

Title: Code as Reward: Empowering Reinforcement Learning with VLMs
Authors: David Venuto, Sami Nur Islam, Martin Klissarov, Doina Precup, Sherry
  Yang, Ankit Anand
Categories: cs.LG
\\
  Pre-trained Vision-Language Models (VLMs) are able to understand visual
concepts, describe and decompose complex tasks into sub-tasks, and provide
feedback on task completion. In this paper, we aim to leverage these
capabilities to support the training of reinforcement learning (RL) agents. In
principle, VLMs are well suited for this purpose, as they can naturally analyze
image-based observations and provide feedback (reward) on learning progress.
However, inference in VLMs is computationally expensive, so querying them
frequently to compute rewards would significantly slowdown the training of an
RL agent. To address this challenge, we propose a framework named Code as
Reward (VLM-CaR). VLM-CaR produces dense reward functions from VLMs through
code generation, thereby significantly reducing the computational burden of
querying the VLM directly. We show that the dense rewards generated through our
approach are very accurate across a diverse set of discrete and continuous
environments, and can be more effective in training RL policies than the
original sparse environment rewards.
\\ ( https://arxiv.org/abs/2402.04764 ,  2489kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04783
Date: Wed, 7 Feb 2024 12:06:52 GMT   (349kb,D)

Title: Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate
  Networks
Authors: Hemanth Saratchandran, Shin-Fang Chng, Simon Lucey
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2402.02711
\\
  Recently, neural networks utilizing periodic activation functions have been
proven to demonstrate superior performance in vision tasks compared to
traditional ReLU-activated networks. However, there is still a limited
understanding of the underlying reasons for this improved performance. In this
paper, we aim to address this gap by providing a theoretical understanding of
periodically activated networks through an analysis of their Neural Tangent
Kernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the
finite width setting, using a fairly general network architecture which
requires only one wide layer that grows at least linearly with the number of
data samples. Our findings indicate that periodically activated networks are
\textit{notably more well-behaved}, from the NTK perspective, than ReLU
activated networks. Additionally, we give an application to the memorization
capacity of such networks and verify our theoretical predictions empirically.
Our study offers a deeper understanding of the properties of periodically
activated neural networks and their potential in the field of deep learning.
\\ ( https://arxiv.org/abs/2402.04783 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04794
Date: Wed, 7 Feb 2024 12:35:31 GMT   (333kb,D)

Title: Scalable Multi-view Clustering via Explicit Kernel Features Maps
Authors: Chakib Fettal, Lazhar Labiod, Mohamed Nadif
Categories: cs.LG
\\
  A growing awareness of multi-view learning as an important component in data
science and machine learning is a consequence of the increasing prevalence of
multiple views in real-world applications, especially in the context of
networks. In this paper we introduce a new scalability framework for multi-view
subspace clustering. An efficient optimization strategy is proposed, leveraging
kernel feature maps to reduce the computational burden while maintaining good
clustering performance. The scalability of the algorithm means that it can be
applied to large-scale datasets, including those with millions of data points,
using a standard machine, in a few minutes. We conduct extensive experiments on
real-world benchmark networks of various sizes in order to evaluate the
performance of our algorithm against state-of-the-art multi-view subspace
clustering methods and attributed-network multi-view approaches.
\\ ( https://arxiv.org/abs/2402.04794 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04814
Date: Wed, 7 Feb 2024 13:04:35 GMT   (246kb,D)

Title: BOWLL: A Deceptively Simple Open World Lifelong Learner
Authors: Roshni Kamath, Rupert Mitchell, Subarnaduti Paul, Kristian Kersting,
  Martin Mundt
Categories: cs.LG
\\
  The quest to improve scalar performance numbers on predetermined benchmarks
seems to be deeply engraved in deep learning. However, the real world is seldom
carefully curated and applications are seldom limited to excelling on test
sets. A practical system is generally required to recognize novel concepts,
refrain from actively including uninformative data, and retain previously
acquired knowledge throughout its lifetime. Despite these key elements being
rigorously researched individually, the study of their conjunction, open world
lifelong learning, is only a recent trend. To accelerate this multifaceted
field's exploration, we introduce its first monolithic and much-needed
baseline. Leveraging the ubiquitous use of batch normalization across deep
neural networks, we propose a deceptively simple yet highly effective way to
repurpose standard models for open world lifelong learning. Through extensive
empirical evaluation, we highlight why our approach should serve as a future
standard for models that are able to effectively maintain their knowledge,
selectively focus on informative data, and accelerate future learning.
\\ ( https://arxiv.org/abs/2402.04814 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04821
Date: Wed, 7 Feb 2024 13:21:41 GMT   (3996kb,D)

Title: E(3)-Equivariant Mesh Neural Networks
Authors: Thuan Trang, Nhat Khang Ngo, Daniel Levy, Thieu N. Vo, Siamak
  Ravanbakhsh, Truong Son Hy
Categories: cs.LG
\\
  Triangular meshes are widely used to represent three-dimensional objects. As
a result, many recent works have address the need for geometric deep learning
on 3D mesh. However, we observe that the complexities in many of these
architectures does not translate to practical performance, and simple deep
models for geometric graphs are competitive in practice. Motivated by this
observation, we minimally extend the update equations of E(n)-Equivariant Graph
Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face
information, and further improve it to account for long-range interactions
through hierarchy. The resulting architecture, Equivariant Mesh Neural Network
(EMNN), outperforms other, more complicated equivariant methods on mesh tasks,
with a fast run-time and no expensive pre-processing.
\\ ( https://arxiv.org/abs/2402.04821 ,  3996kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04823
Date: Wed, 7 Feb 2024 13:22:05 GMT   (1400kb,D)

Title: How Realistic Is Your Synthetic Data? Constraining Deep Generative
  Models for Tabular Data
Authors: Mihaela C\u{a}t\u{a}lina Stoian, Salijona Dyrmishi, Maxime Cordy,
  Thomas Lukasiewicz, Eleonora Giunchiglia
Categories: cs.LG
Comments: Accepted at ICLR 2024
\\
  Deep Generative Models (DGMs) have been shown to be powerful tools for
generating tabular data, as they have been increasingly able to capture the
complex distributions that characterize them. However, to generate realistic
synthetic data, it is often not enough to have a good approximation of their
distribution, as it also requires compliance with constraints that encode
essential background knowledge on the problem at hand. In this paper, we
address this limitation and show how DGMs for tabular data can be transformed
into Constrained Deep Generative Models (C-DGMs), whose generated samples are
guaranteed to be compliant with the given constraints. This is achieved by
automatically parsing the constraints and transforming them into a Constraint
Layer (CL) seamlessly integrated with the DGM. Our extensive experimental
analysis with various DGMs and tasks reveals that standard DGMs often violate
constraints, some exceeding $95\%$ non-compliance, while their corresponding
C-DGMs are never non-compliant. Then, we quantitatively demonstrate that, at
training time, C-DGMs are able to exploit the background knowledge expressed by
the constraints to outperform their standard counterparts with up to $6.5\%$
improvement in utility and detection. Further, we show how our CL does not
necessarily need to be integrated at training time, as it can be also used as a
guardrail at inference time, still producing some improvements in the overall
performance of the models. Finally, we show that our CL does not hinder the
sample generation time of the models.
\\ ( https://arxiv.org/abs/2402.04823 ,  1400kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04830
Date: Wed, 7 Feb 2024 13:26:10 GMT   (166kb,D)

Title: Closing the Gap Between SGP4 and High-Precision Propagation via
  Differentiable Programming
Authors: Giacomo Acciarini, At{\i}l{\i}m G\"une\c{s} Baydin, Dario Izzo
Categories: cs.LG astro-ph.EP
\\
  The Simplified General Perturbations 4 (SGP4) orbital propagation method is
widely used for predicting the positions and velocities of Earth-orbiting
objects rapidly and reliably. Despite continuous refinement, SGP models still
lack the precision of numerical propagators, which offer significantly smaller
errors. This study presents dSGP4, a novel differentiable version of SGP4
implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates
various space-related applications, including spacecraft orbit determination,
state conversion, covariance transformation, state transition matrix
computation, and covariance propagation. Additionally, dSGP4's PyTorch
implementation allows for embarrassingly parallel orbital propagation across
batches of Two-Line Element Sets (TLEs), leveraging the computational power of
CPUs, GPUs, and advanced hardware for distributed prediction of satellite
positions at future times. Furthermore, dSGP4's differentiability enables
integration with modern machine learning techniques. Thus, we propose a novel
orbital propagation paradigm, ML-dSGP4, where neural networks are integrated
into the orbital propagator. Through stochastic gradient descent, this combined
model's inputs, outputs, and parameters can be iteratively refined, surpassing
SGP4's precision. Neural networks act as identity operators by default,
adhering to SGP4's behavior. However, dSGP4's differentiability allows
fine-tuning with ephemeris data, enhancing precision while maintaining
computational speed. This empowers satellite operators and researchers to train
the model using specific ephemeris or high-precision numerical propagation
data, significantly advancing orbital prediction capabilities.
\\ ( https://arxiv.org/abs/2402.04830 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04836
Date: Wed, 7 Feb 2024 13:32:53 GMT   (1199kb,D)

Title: On the Completeness of Invariant Geometric Deep Learning Models
Authors: Zian Li, Xiyuan Wang, Shijia Kang, Muhan Zhang
Categories: cs.LG cs.AI
\\
  Invariant models, one important class of geometric deep learning models, are
capable of generating meaningful geometric representations by leveraging
informative geometric features. These models are characterized by their
simplicity, good experimental results and computational efficiency. However,
their theoretical expressive power still remains unclear, restricting a deeper
understanding of the potential of such models. In this work, we concentrate on
characterizing the theoretical expressiveness of invariant models. We first
rigorously bound the expressiveness of the most classical invariant model,
Vanilla DisGNN (message passing neural networks incorporating distance),
restricting its unidentifiable cases to be only those highly symmetric
geometric graphs. To break these corner cases' symmetry, we introduce a simple
yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN.
Leveraging GeoNGNN as a theoretical tool, we for the first time prove the
E(3)-completeness of three well-established geometric models: DimeNet, GemNet
and SphereNet. Our results fill the gap in the theoretical power of invariant
models, contributing to a rigorous and comprehensive understanding of their
capabilities. Experimentally, GeoNGNN exhibits good inductive bias in capturing
local environments, and achieves competitive results w.r.t. complicated models
relying on high-order invariant/equivariant representations while exhibiting
significantly faster computational speed.
\\ ( https://arxiv.org/abs/2402.04836 ,  1199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04852
Date: Wed, 7 Feb 2024 13:51:26 GMT   (1967kb,D)

Title: Multi-Patch Prediction: Adapting LLMs for Time Series Representation
  Learning
Authors: Yuxuan Bian, Xuan Ju, Jiangtong Li, Zhijian Xu, Dawei Cheng, Qiang Xu
Categories: cs.LG
\\
  In this study, we present aLLM4TS, an innovative framework that adapts Large
Language Models (LLMs) for time-series representation learning. Central to our
approach is that we reconceive time-series forecasting as a self-supervised,
multi-patch prediction task, which, compared to traditional
mask-and-reconstruction methods, captures temporal dynamics in patch
representations more effectively. Our strategy encompasses two-stage training:
(i). a causal continual pre-training phase on various time-series datasets,
anchored on next patch prediction, effectively syncing LLM capabilities with
the intricacies of time-series data; (ii). fine-tuning for multi-patch
prediction in the targeted time-series context. A distinctive element of our
framework is the patch-wise decoding layer, which departs from previous methods
reliant on sequence-level decoding. Such a design directly transposes
individual patches into temporal sequences, thereby significantly bolstering
the model's proficiency in mastering temporal patch-based representations.
aLLM4TS demonstrates superior performance in several downstream tasks, proving
its effectiveness in deriving temporal representations with enhanced
transferability and marking a pivotal advancement in the adaptation of LLMs for
time-series analysis.
\\ ( https://arxiv.org/abs/2402.04852 ,  1967kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04869
Date: Wed, 7 Feb 2024 14:09:34 GMT   (480kb,D)

Title: Learning by Doing: An Online Causal Reinforcement Learning Framework
  with Causal-Aware Policy
Authors: Ruichu Cai, Siyang Huang, Jie Qiao, Wei Chen, Yan Zeng, Keli Zhang,
  Fuchun Sun, Yang Yu, Zhifeng Hao
Categories: cs.LG cs.AI
\\
  As a key component to intuitive cognition and reasoning solutions in human
intelligence, causal knowledge provides great potential for reinforcement
learning (RL) agents' interpretability towards decision-making by helping
reduce the searching space. However, there is still a considerable gap in
discovering and incorporating causality into RL, which hinders the rapid
development of causal RL. In this paper, we consider explicitly modeling the
generation process of states with the causal graphical model, based on which we
augment the policy. We formulate the causal structure updating into the RL
interaction process with active intervention learning of the environment. To
optimize the derived objective, we propose a framework with theoretical
performance guarantees that alternates between two steps: using interventions
for causal structure learning during exploration and using the learned causal
structure for policy guidance during exploitation. Due to the lack of public
benchmarks that allow direct intervention in the state space, we design the
root cause localization task in our simulated fault alarm environment and then
empirically show the effectiveness and robustness of the proposed method
against state-of-the-art baselines. Theoretical analysis shows that our
performance improvement attributes to the virtuous cycle of causal-guided
policy learning and causal structure learning, which aligns with our
experimental results.
\\ ( https://arxiv.org/abs/2402.04869 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04875
Date: Wed, 7 Feb 2024 14:16:28 GMT   (310kb,D)

Title: On Provable Length and Compositional Generalization
Authors: Kartik Ahuja, Amin Mansouri
Categories: cs.LG cs.CL stat.ML
\\
  Length generalization -- the ability to generalize to longer sequences than
ones seen during training, and compositional generalization -- the ability to
generalize to token combinations not seen during training, are crucial forms of
out-of-distribution generalization in sequence-to-sequence models. In this
work, we take the first steps towards provable length and compositional
generalization for a range of architectures, including deep sets, transformers,
state space models, and simple recurrent neural nets. Depending on the
architecture, we prove different degrees of representation identification,
e.g., a linear or a permutation relation with ground truth representation, is
necessary for length and compositional generalization.
\\ ( https://arxiv.org/abs/2402.04875 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04902
Date: Wed, 7 Feb 2024 14:35:05 GMT   (264kb,D)

Title: L4Q: Parameter Efficient Quantization-Aware Training on Large Language
  Models via LoRA-wise LSQ
Authors: Hyesung Jeon, Yulhwa Kim, Jae-joon Kim
Categories: cs.LG cs.CL
Comments: 8 pages, 2 figures
\\
  Post-training quantization (PTQ) and quantization-aware training (QAT)
methods are gaining popularity in mitigating the high memory and computational
costs associated with Large Language Models (LLMs). In resource-constrained
scenarios, PTQ, with its reduced training overhead, is often preferred over
QAT, despite the latter's potential for higher accuracy. Meanwhile,
parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA)
have been introduced, and recent efforts have explored quantization-aware PEFT
techniques. However, these approaches may lack generality due to their reliance
on the pre-quantized model's configuration. Their effectiveness may be
compromised by non-linearly quantized or mixed-precision weights, and the
retraining of specific quantization parameters might impede optimal
performance. To address these challenges, we propose L4Q, an algorithm for
parameter-efficient quantization-aware training. L4Q leverages LoRA-wise
learned quantization step size for LLMs, aiming to enhance generality. The
simultaneous quantization-and-fine-tuning process of L4Q is applicable to
high-precision models, yielding linearly quantized weights with superior
accuracy. Our experiments, conducted on the LLaMA and LLaMA2 model families
using an instructional dataset, showcase L4Q's capabilities in language
comprehension and few-shot in-context learning, achieving sub-4-bit precision
while maintaining comparable training times to applying PEFT on a quantized
model.
\\ ( https://arxiv.org/abs/2402.04902 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04906
Date: Wed, 7 Feb 2024 14:35:25 GMT   (291kb,D)

Title: Conformal Monte Carlo Meta-learners for Predictive Inference of
  Individual Treatment Effects
Authors: Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau,
  Sofie Van Hoecke
Categories: cs.LG stat.ML
Comments: 21 pages, 8 figures
\\
  Knowledge of the effect of interventions, called the treatment effect, is
paramount for decision-making. Approaches to estimating this treatment effect,
e.g. by using Conditional Average Treatment Effect (CATE) estimators, often
only provide a point estimate of this treatment effect, while additional
uncertainty quantification is frequently desired instead. Therefore, we present
a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging
conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to
instead produce a predictive distribution usable in individualized
decision-making. Furthermore, we show how specific assumptions on the noise
distribution of the outcome heavily affect these uncertainty predictions.
Nonetheless, the CMC framework shows strong experimental coverage while
retaining small interval widths to provide estimates of the true individual
treatment effect.
\\ ( https://arxiv.org/abs/2402.04906 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04915
Date: Wed, 7 Feb 2024 14:41:17 GMT   (313kb,D)

Title: Moco: A Learnable Meta Optimizer for Combinatorial Optimization
Authors: Tim Dernedde, Daniela Thyssens, S\"oren Dittrich, Maximilan
  Stubbemann, Lars Schmidt-Thieme
Categories: cs.LG
Comments: 13 pages, 3 figures
\\
  Relevant combinatorial optimization problems (COPs) are often NP-hard. While
they have been tackled mainly via handcrafted heuristics in the past, advances
in neural networks have motivated the development of general methods to learn
heuristics from data. Many approaches utilize a neural network to directly
construct a solution, but are limited in further improving based on already
constructed solutions at inference time. Our approach, Moco, learns a graph
neural network that updates the solution construction procedure based on
features extracted from the current search state. This meta training procedure
targets the overall best solution found during the search procedure given
information such as the search budget. This allows Moco to adapt to varying
circumstances such as different computational budgets. Moco is a fully
learnable meta optimizer that does not utilize any problem specific local
search or decomposition. We test Moco on the Traveling Salesman Problem (TSP)
and Maximum Independent Set (MIS) and show that it outperforms other approaches
on MIS and is overall competitive on the TSP, especially outperforming related
approaches, partially even if they use additional local search.
\\ ( https://arxiv.org/abs/2402.04915 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04924
Date: Wed, 7 Feb 2024 14:49:10 GMT   (10753kb,D)

Title: Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient
  Matching
Authors: Tianle Zhang and Yuchen Zhang and Kun Wang and Kai Wang and Beining
  Yang and Kaipeng Zhang and Wenqi Shao and Ping Liu and Joey Tianyi Zhou and
  Yang You
Categories: cs.LG
Comments: An effective method for graph condensation
\\
  Training on large-scale graphs has achieved remarkable results in graph
representation learning, but its cost and storage have raised growing concerns.
As one of the most promising directions, graph condensation methods address
these issues by employing gradient matching, aiming to condense the full graph
into a more concise yet information-rich synthetic set. Though encouraging,
these strategies primarily emphasize matching directions of the gradients,
which leads to deviations in the training trajectories. Such deviations are
further magnified by the differences between the condensation and evaluation
phases, culminating in accumulated errors, which detrimentally affect the
performance of the condensed graphs. In light of this, we propose a novel graph
condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L}
trajectory (\textbf{CTRL}), which offers an optimized starting point closer to
the original dataset's feature distribution and a more refined strategy for
gradient matching. Theoretically, CTRL can effectively neutralize the impact of
accumulated errors on the performance of condensed graphs. We provide extensive
experiments on various graph datasets and downstream tasks to support the
effectiveness of CTRL. Code is released at
https://github.com/NUS-HPC-AI-Lab/CTRL.
\\ ( https://arxiv.org/abs/2402.04924 ,  10753kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04933
Date: Wed, 7 Feb 2024 15:11:37 GMT   (12386kb,D)

Title: A Bayesian Approach to Online Learning for Contextual Restless Bandits
  with Applications to Public Health
Authors: Biyonka Liang, Lily Xu, Aparna Taneja, Milind Tambe, Lucas Janson
Categories: cs.LG stat.AP
Comments: 26 pages, 18 figures
\\
  Restless multi-armed bandits (RMABs) are used to model sequential resource
allocation in public health intervention programs. In these settings, the
underlying transition dynamics are often unknown a priori, requiring online
reinforcement learning (RL). However, existing methods in online RL for RMABs
cannot incorporate properties often present in real-world public health
applications, such as contextual information and non-stationarity. We present
Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs
that novelly combines techniques in Bayesian modeling with Thompson sampling to
flexibly model a wide range of complex RMAB settings, such as contextual and
non-stationary RMABs. A key contribution of our approach is its ability to
leverage shared information within and between arms to learn unknown RMAB
transition dynamics quickly in budget-constrained settings with relatively
short time horizons. Empirically, we show that BCoR achieves substantially
higher finite-sample performance than existing approaches over a range of
experimental settings, including one constructed from a real-world public
health campaign in India.
\\ ( https://arxiv.org/abs/2402.04933 ,  12386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04982
Date: Wed, 7 Feb 2024 15:58:51 GMT   (3774kb,D)

Title: Beyond explaining: XAI-based Adaptive Learning with SHAP Clustering for
  Energy Consumption Prediction
Authors: Tobias Clement and Hung Truong Thanh Nguyen and Nils Kemmerzell and
  Mohamed Abdelaal and Davor Stjelja
Categories: cs.LG cs.DB
Comments: A short version of this paper was published at the Australasian Joint
  Conference on Artificial Intelligence in 2023
\\
  This paper presents an approach integrating explainable artificial
intelligence (XAI) techniques with adaptive learning to enhance energy
consumption prediction models, with a focus on handling data distribution
shifts. Leveraging SHAP clustering, our method provides interpretable
explanations for model predictions and uses these insights to adaptively refine
the model, balancing model complexity with predictive performance. We introduce
a three-stage process: (1) obtaining SHAP values to explain model predictions,
(2) clustering SHAP values to identify distinct patterns and outliers, and (3)
refining the model based on the derived SHAP clustering characteristics. Our
approach mitigates overfitting and ensures robustness in handling data
distribution shifts. We evaluate our method on a comprehensive dataset
comprising energy consumption records of buildings, as well as two additional
datasets to assess the transferability of our approach to other domains,
regression, and classification problems. Our experiments demonstrate the
effectiveness of our approach in both task types, resulting in improved
predictive performance and interpretable model explanations.
\\ ( https://arxiv.org/abs/2402.04982 ,  3774kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04987
Date: Wed, 7 Feb 2024 16:06:20 GMT   (1191kb,D)

Title: PriorBoost: An Adaptive Algorithm for Learning from Aggregate Responses
Authors: Adel Javanmard, Matthew Fahrbach, Vahab Mirrokni
Categories: cs.LG cs.DS
Comments: 29 pages, 4 figures
\\
  This work studies algorithms for learning from aggregate responses. We focus
on the construction of aggregation sets (called bags in the literature) for
event-level loss functions. We prove for linear regression and generalized
linear models (GLMs) that the optimal bagging problem reduces to
one-dimensional size-constrained $k$-means clustering. Further, we
theoretically quantify the advantage of using curated bags over random bags. We
then propose the PriorBoost algorithm, which adaptively forms bags of samples
that are increasingly homogeneous with respect to (unobserved) individual
responses to improve model quality. We study label differential privacy for
aggregate learning, and we also provide extensive experiments showing that
PriorBoost regularly achieves optimal model quality for event-level
predictions, in stark contrast to non-adaptive algorithms.
\\ ( https://arxiv.org/abs/2402.04987 ,  1191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05002
Date: Wed, 7 Feb 2024 16:18:59 GMT   (636kb,D)

Title: Randomized Confidence Bounds for Stochastic Partial Monitoring
Authors: Maxime Heuillet, Ola Ahmad, Audrey Durand
Categories: cs.LG
\\
  The partial monitoring (PM) framework provides a theoretical formulation of
sequential learning problems with incomplete feedback. On each round, a
learning agent plays an action while the environment simultaneously chooses an
outcome. The agent then observes a feedback signal that is only partially
informative about the (unobserved) outcome. The agent leverages the received
feedback signals to select actions that minimize the (unobserved) cumulative
loss. In contextual PM, the outcomes depend on some side information that is
observable by the agent before selecting the action on each round. In this
paper, we consider the contextual and non-contextual PM settings with
stochastic outcomes. We introduce a new class of strategies based on the
randomization of deterministic confidence bounds, that extend regret guarantees
to settings where existing stochastic strategies are not applicable. Our
experiments show that the proposed RandCBP and RandCBPside* strategies improve
state-of-the-art baselines in PM games. To encourage the adoption of the PM
framework, we design a use case on the real-world problem of monitoring the
error rate of any deployed classification system.
\\ ( https://arxiv.org/abs/2402.05002 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05007
Date: Wed, 7 Feb 2024 16:28:04 GMT   (1072kb,D)

Title: Example-based Explanations for Random Forests using Machine Unlearning
Authors: Tanmay Surve and Romila Pradhan
Categories: cs.LG cs.AI
\\
  Tree-based machine learning models, such as decision trees and random
forests, have been hugely successful in classification tasks primarily because
of their predictive power in supervised learning tasks and ease of
interpretation. Despite their popularity and power, these models have been
found to produce unexpected or discriminatory outcomes. Given their
overwhelming success for most tasks, it is of interest to identify sources of
their unexpected and discriminatory behavior. However, there has not been much
work on understanding and debugging tree-based classifiers in the context of
fairness.
  We introduce FairDebugger, a system that utilizes recent advances in machine
unlearning research to identify training data subsets responsible for instances
of fairness violations in the outcomes of a random forest classifier.
FairDebugger generates top-$k$ explanations (in the form of coherent training
data subsets) for model unfairness. Toward this goal, FairDebugger first
utilizes machine unlearning to estimate the change in the tree structures of
the random forest when parts of the underlying training data are removed, and
then leverages the Apriori algorithm from frequent itemset mining to reduce the
subset search space. We empirically evaluate our approach on three real-world
datasets, and demonstrate that the explanations generated by FairDebugger are
consistent with insights from prior studies on these datasets.
\\ ( https://arxiv.org/abs/2402.05007 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05011
Date: Wed, 7 Feb 2024 16:32:02 GMT   (3660kb,D)

Title: Navigating Complexity: Toward Lossless Graph Condensation via Expanding
  Window Matching
Authors: Yuchen Zhang and Tianle Zhang and Kai Wang and Ziyao Guo and Yuxuan
  Liang and Xavier Bresson and Wei Jin and Yang You
Categories: cs.LG
Comments: Lossless graph condensation method
\\
  Graph condensation aims to reduce the size of a large-scale graph dataset by
synthesizing a compact counterpart without sacrificing the performance of Graph
Neural Networks (GNNs) trained on it, which has shed light on reducing the
computational cost for training GNNs. Nevertheless, existing methods often fall
short of accurately replicating the original graph for certain datasets,
thereby failing to achieve the objective of lossless condensation. To
understand this phenomenon, we investigate the potential reasons and reveal
that the previous state-of-the-art trajectory matching method provides biased
and restricted supervision signals from the original graph when optimizing the
condensed one. This significantly limits both the scale and efficacy of the
condensed graph. In this paper, we make the first attempt toward
\textit{lossless graph condensation} by bridging the previously neglected
supervision signals. Specifically, we employ a curriculum learning strategy to
train expert trajectories with more diverse supervision signals from the
original graph, and then effectively transfer the information into the
condensed graph with expanding window matching. Moreover, we design a loss
function to further extract knowledge from the expert trajectories. Theoretical
analysis justifies the design of our method and extensive experiments verify
its superiority across different datasets. Code is released at
https://github.com/NUS-HPC-AI-Lab/GEOM.
\\ ( https://arxiv.org/abs/2402.05011 ,  3660kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05013
Date: Wed, 7 Feb 2024 16:32:29 GMT   (1064kb,D)

Title: Compression of Structured Data with Autoencoders: Provable Benefit of
  Nonlinearities and Depth
Authors: Kevin K\"ogler, Alexander Shevchenko, Hamed Hassani, Marco Mondelli
Categories: cs.LG cs.IT math.IT stat.ML
\\
  Autoencoders are a prominent model in many empirical branches of machine
learning and lossy data compression. However, basic theoretical questions
remain unanswered even in a shallow two-layer setting. In particular, to what
degree does a shallow autoencoder capture the structure of the underlying data
distribution? For the prototypical case of the 1-bit compression of sparse
Gaussian data, we prove that gradient descent converges to a solution that
completely disregards the sparse structure of the input. Namely, the
performance of the algorithm is the same as if it was compressing a Gaussian
source - with no sparsity. For general data distributions, we give evidence of
a phase transition phenomenon in the shape of the gradient descent minimizer,
as a function of the data sparsity: below the critical sparsity level, the
minimizer is a rotation taken uniformly at random (just like in the compression
of non-sparse data); above the critical sparsity, the minimizer is the identity
(up to a permutation). Finally, by exploiting a connection with approximate
message passing algorithms, we show how to improve upon Gaussian performance
for the compression of sparse data: adding a denoising function to a shallow
architecture already reduces the loss provably, and a suitable multi-layer
decoder leads to a further improvement. We validate our findings on image
datasets, such as CIFAR-10 and MNIST.
\\ ( https://arxiv.org/abs/2402.05013 ,  1064kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05015
Date: Wed, 7 Feb 2024 16:32:58 GMT   (2536kb,D)

Title: A Sober Look at LLMs for Material Discovery: Are They Actually Good for
  Bayesian Optimization Over Molecules?
Authors: Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal
  Poupart, Al\'an Aspuru-Guzik, Geoff Pleiss
Categories: cs.LG
\\
  Automation is one of the cornerstones of contemporary material discovery.
Bayesian optimization (BO) is an essential part of such workflows, enabling
scientists to leverage prior domain knowledge into efficient exploration of a
large molecular space. While such prior knowledge can take many forms, there
has been significant fanfare around the ancillary scientific knowledge
encapsulated in large language models (LLMs). However, existing work thus far
has only explored LLMs for heuristic materials searches. Indeed, recent work
obtains the uncertainty estimate -- an integral part of BO -- from
point-estimated, non-Bayesian LLMs. In this work, we study the question of
whether LLMs are actually useful to accelerate principled Bayesian optimization
in the molecular space. We take a sober, dispassionate stance in answering this
question. This is done by carefully (i) viewing LLMs as fixed feature
extractors for standard but principled BO surrogate models and by (ii)
leveraging parameter-efficient finetuning methods and Bayesian neural networks
to obtain the posterior of the LLM surrogate. Our extensive experiments with
real-world chemistry problems show that LLMs can be useful for BO over
molecules, but only if they have been pretrained or finetuned with
domain-specific data.
\\ ( https://arxiv.org/abs/2402.05015 ,  2536kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05025
Date: Wed, 7 Feb 2024 16:47:07 GMT   (997kb,D)

Title: Strong convexity-guided hyper-parameter optimization for flatter losses
Authors: Rahul Yedida, Snehanshu Saha
Categories: cs.LG
Comments: v1
\\
  We propose a novel white-box approach to hyper-parameter optimization.
Motivated by recent work establishing a relationship between flat minima and
generalization, we first establish a relationship between the strong convexity
of the loss and its flatness. Based on this, we seek to find hyper-parameter
configurations that improve flatness by minimizing the strong convexity of the
loss. By using the structure of the underlying neural network, we derive
closed-form equations to approximate the strong convexity parameter, and
attempt to find hyper-parameters that minimize it in a randomized fashion.
Through experiments on 14 classification datasets, we show that our method
achieves strong performance at a fraction of the runtime.
\\ ( https://arxiv.org/abs/2402.05025 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05033
Date: Wed, 7 Feb 2024 17:07:41 GMT   (695kb,D)

Title: Simulated Overparameterization
Authors: Hanna Mazzawi, Pranjal Awasthi, Xavi Gonzalvo, Srikumar Ramalingam
Categories: cs.LG
\\
  In this work, we introduce a novel paradigm called Simulated
Overparametrization (SOP). SOP merges the computational efficiency of compact
models with the advanced learning proficiencies of overparameterized models.
SOP proposes a unique approach to model training and inference, where a model
with a significantly larger number of parameters is trained in such a way that
a smaller, efficient subset of these parameters is used for the actual
computation during inference. Building upon this framework, we present a novel,
architecture agnostic algorithm called "majority kernels", which seamlessly
integrates with predominant architectures, including Transformer models.
Majority kernels enables the simulated training of overparameterized models,
resulting in performance gains across architectures and tasks. Furthermore, our
approach adds minimal overhead to the cost incurred (wall clock time) at
training time. The proposed approach shows strong performance on a wide variety
of datasets and models, even outperforming strong baselines such as
combinatorial optimization methods based on submodular optimization.
\\ ( https://arxiv.org/abs/2402.05033 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05039
Date: Wed, 7 Feb 2024 17:23:15 GMT   (3102kb,D)

Title: PAC Learnability under Explanation-Preserving Graph Perturbations
Authors: Xu Zheng, Farhad Shirani, Tianchun Wang, Shouwei Gao, Wenqian Dong,
  Wei Cheng, Dongsheng Luo
Categories: cs.LG
Comments: 21 pages, 6 figures, 4 tables
\\
  Graphical models capture relations between entities in a wide range of
applications including social networks, biology, and natural language
processing, among others. Graph neural networks (GNN) are neural models that
operate over graphs, enabling the model to leverage the complex relationships
and dependencies in graph-structured data. A graph explanation is a subgraph
which is an `almost sufficient' statistic of the input graph with respect to
its classification label. Consequently, the classification label is invariant,
with high probability, to perturbations of graph edges not belonging to its
explanation subgraph. This work considers two methods for leveraging such
perturbation invariances in the design and training of GNNs. First,
explanation-assisted learning rules are considered. It is shown that the sample
complexity of explanation-assisted learning can be arbitrarily smaller than
explanation-agnostic learning. Next, explanation-assisted data augmentation is
considered, where the training set is enlarged by artificially producing new
training samples via perturbation of the non-explanation edges in the original
training set. It is shown that such data augmentation methods may improve
performance if the augmented data is in-distribution, however, it may also lead
to worse sample complexity compared to explanation-agnostic learning rules if
the augmented data is out-of-distribution. Extensive empirical evaluations are
provided to verify the theoretical analysis.
\\ ( https://arxiv.org/abs/2402.05039 ,  3102kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05050
Date: Wed, 7 Feb 2024 17:46:37 GMT   (461kb,D)

Title: Federated Learning Can Find Friends That Are Beneficial
Authors: Nazarii Tupitsa and Samuel Horv\'ath and Martin Tak\'a\v{c} and Eduard
  Gorbunov
Categories: cs.LG math.OC
\\
  In Federated Learning (FL), the distributed nature and heterogeneity of
client data present both opportunities and challenges. While collaboration
among clients can significantly enhance the learning process, not all
collaborations are beneficial; some may even be detrimental. In this study, we
introduce a novel algorithm that assigns adaptive aggregation weights to
clients participating in FL training, identifying those with data distributions
most conducive to a specific learning objective. We demonstrate that our
aggregation method converges no worse than the method that aggregates only the
updates received from clients with the same data distribution. Furthermore,
empirical evaluations consistently reveal that collaborations guided by our
algorithm outperform traditional FL approaches. This underscores the critical
role of judicious client selection and lays the foundation for more streamlined
and effective FL implementations in the coming years.
\\ ( https://arxiv.org/abs/2402.05050 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05052
Date: Wed, 7 Feb 2024 17:51:38 GMT   (1104kb,D)

Title: Causal Representation Learning from Multiple Distributions: A General
  Setting
Authors: Kun Zhang, Shaoan Xie, Ignavier Ng, Yujia Zheng
Categories: cs.LG stat.ML
\\
  In many problems, the measured variables (e.g., image pixels) are just
mathematical functions of the hidden causal variables (e.g., the underlying
concepts or objects). For the purpose of making predictions in changing
environments or making proper changes to the system, it is helpful to recover
the hidden causal variables $Z_i$ and their causal relations represented by
graph $\mathcal{G}_Z$. This problem has recently been known as causal
representation learning. This paper is concerned with a general, completely
nonparametric setting of causal representation learning from multiple
distributions (arising from heterogeneous data or nonstationary time series),
without assuming hard interventions behind distribution changes. We aim to
develop general solutions in this fundamental case; as a by product, this helps
see the unique benefit offered by other assumptions such as parametric causal
models or hard interventions. We show that under the sparsity constraint on the
recovered graph over the latent variables and suitable sufficient change
conditions on the causal influences, interestingly, one can recover the
moralized graph of the underlying directed acyclic graph, and the recovered
latent variables and their relations are related to the underlying causal model
in a specific, nontrivial way. In some cases, each latent variable can even be
recovered up to component-wise transformations. Experimental results verify our
theoretical claims.
\\ ( https://arxiv.org/abs/2402.05052 ,  1104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05073
Date: Wed, 7 Feb 2024 18:27:29 GMT   (9914kb,D)

Title: NITO: Neural Implicit Fields for Resolution-free Topology Optimization
Authors: Amin Heyrani Nobari, Giorgio Giannone, Lyle Regenwetter, Faez Ahmed
Categories: cs.LG cs.CE
\\
  Topology optimization is a critical task in engineering design, where the
goal is to optimally distribute material in a given space for maximum
performance. We introduce Neural Implicit Topology Optimization (NITO), a novel
approach to accelerate topology optimization problems using deep learning. NITO
stands out as one of the first frameworks to offer a resolution-free and
domain-agnostic solution in deep learning-based topology optimization. NITO
synthesizes structures with up to seven times better structural efficiency
compared to SOTA diffusion models and does so in a tenth of the time. In the
NITO framework, we introduce a novel method, the Boundary Point Order-Invariant
MLP (BPOM), to represent boundary conditions in a sparse and domain-agnostic
manner, moving away from expensive simulation-based approaches. Crucially, NITO
circumvents the domain and resolution limitations that restrict Convolutional
Neural Network (CNN) models to a structured domain of fixed size -- limitations
that hinder the widespread adoption of CNNs in engineering applications. This
generalizability allows a single NITO model to train and generate solutions in
countless domains, eliminating the need for numerous domain-specific CNNs and
their extensive datasets. Despite its generalizability, NITO outperforms SOTA
models even in specialized tasks, is an order of magnitude smaller, and is
practically trainable at high resolutions that would be restrictive for CNNs.
This combination of versatility, efficiency, and performance underlines NITO's
potential to transform the landscape of engineering design optimization
problems through implicit fields.
\\ ( https://arxiv.org/abs/2402.05073 ,  9914kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05098
Date: Wed, 7 Feb 2024 18:51:49 GMT   (9161kb,D)

Title: On diffusion models for amortized inference: Benchmarking and improving
  stochastic control and sampling
Authors: Marcin Sendera, Minsu Kim, Sarthak Mittal, Pablo Lemos, Luca Scimeca,
  Jarrid Rector-Brooks, Alexandre Adam, Yoshua Bengio, Nikolay Malkin
Categories: cs.LG stat.ML
Comments: 21 pages; code: https://github.com/GFNOrg/gfn-diffusion
\\
  We study the problem of training diffusion models to sample from a
distribution with a given unnormalized density or energy function. We benchmark
several diffusion-structured inference methods, including simulation-based
variational approaches and off-policy methods (continuous generative flow
networks). Our results shed light on the relative advantages of existing
algorithms while bringing into question some claims from past work. We also
propose a novel exploration strategy for off-policy methods, based on local
search in the target space with the use of a replay buffer, and show that it
improves the quality of samples on a variety of target distributions. Our code
for the sampling methods and benchmarks studied is made public at
https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion
models for amortized inference.
\\ ( https://arxiv.org/abs/2402.05098 ,  9161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05099
Date: Wed, 7 Feb 2024 18:53:01 GMT   (1750kb,D)

Title: Hydragen: High-Throughput LLM Inference with Shared Prefixes
Authors: Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Y. Fu,
  Christopher R\'e, Azalia Mirhoseini
Categories: cs.LG
\\
  Transformer-based large language models (LLMs) are now deployed to hundreds
of millions of users. LLM inference is commonly performed on batches of
sequences that share a prefix, such as few-shot examples or a chatbot system
prompt. Decoding in this large-batch setting can be bottlenecked by the
attention operation, which reads large key-value (KV) caches from memory and
computes inefficient matrix-vector products for every sequence in the batch. In
this work, we introduce Hydragen, a hardware-aware exact implementation of
attention with shared prefixes. Hydragen computes attention over the shared
prefix and unique suffixes separately. This decomposition enables efficient
prefix attention by batching queries together across sequences, reducing
redundant memory reads and enabling the use of hardware-friendly matrix
multiplications. Our method can improve end-to-end LLM throughput by up to 32x
against competitive baselines, with speedup growing with the batch size and
shared prefix length. Hydragen also enables the use of very long shared
contexts: with a high batch size, increasing the prefix length from 1K to 16K
tokens decreases Hydragen throughput by less than 15%, while the throughput of
baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix
decomposition and can be applied to tree-based prompt sharing patterns,
allowing us to further reduce inference time on competitive programming
problems by 55%.
\\ ( https://arxiv.org/abs/2402.05099 ,  1750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05109
Date: Wed, 7 Feb 2024 18:58:50 GMT   (144kb,D)

Title: Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding
Authors: Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher
  Rinard, Jonathan Ragan-Kelley, William Brandon
Categories: cs.LG
\\
  To combat the memory bandwidth-bound nature of autoregressive LLM inference,
previous research has proposed the speculative decoding framework. To perform
speculative decoding, a small draft model proposes candidate continuations of
the input sequence, that are then verified in parallel by the base model. One
way to specify the draft model, as used in the recent Medusa decoding
framework, is as a collection of light-weight heads, called draft heads, that
operate on the base model's hidden states. To date, all existing draft heads
have been sequentially independent, meaning that they speculate tokens in the
candidate continuation independently of any preceding tokens in the candidate
continuation. In this work, we propose Hydra heads, a sequentially dependent,
drop-in replacement for standard draft heads that significantly improves
speculation accuracy. Decoding with Hydra heads improves throughput compared to
Medusa decoding with standard draft heads. We further explore the design space
of Hydra head training objectives and architectures, and propose a
carefully-tuned Hydra head recipe, which we call Hydra++, that improves
decoding throughput by 1.31x and 2.71x compared to Medusa decoding and
autoregressive decoding, respectively. Overall, Hydra heads are a simple
intervention on standard draft heads that significantly improve the end-to-end
speed of draft head based speculative decoding.
\\ ( https://arxiv.org/abs/2402.05109 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05110
Date: Wed, 7 Feb 2024 18:59:12 GMT   (677kb,D)

Title: Opening the AI black box: program synthesis via mechanistic
  interpretability
Authors: Eric J. Michaud, Isaac Liao, Vedang Lad, Ziming Liu, Anish Mudide,
  Chloe Loughridge, Zifan Carl Guo, Tara Rezaei Kheirkhah, Mateja Vukeli\'c,
  Max Tegmark
Categories: cs.LG
Comments: 24 pages
\\
  We present MIPS, a novel method for program synthesis based on automated
mechanistic interpretability of neural networks trained to perform the desired
task, auto-distilling the learned algorithm into Python code. We test MIPS on a
benchmark of 62 algorithmic tasks that can be learned by an RNN and find it
highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are
not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to
convert the RNN into a finite state machine, then applies Boolean or integer
symbolic regression to capture the learned algorithm. As opposed to large
language models, this program synthesis technique makes no use of (and is
therefore not limited by) human training data such as algorithms and code from
GitHub. We discuss opportunities and challenges for scaling up this approach to
make machine-learned models more interpretable and trustworthy.
\\ ( https://arxiv.org/abs/2402.05110 ,  677kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.02452 (*cross-listing*)
Date: Sun, 4 Feb 2024 11:42:16 GMT   (219kb,D)
Date (revised v2): Wed, 7 Feb 2024 09:00:09 GMT   (219kb,D)

Title: XAI-CF -- Examining the Role of Explainable Artificial Intelligence in
  Cyber Forensics
Authors: Shahid Alam and Zeynep Altiparmak
Categories: cs.CR cs.AI
\\
  With the rise of complex cyber devices Cyber Forensics (CF) is facing many
new challenges. For example, there are dozens of systems running on
smartphones, each with more than millions of downloadable applications. Sifting
through this large amount of data and making sense requires new techniques,
such as from the field of Artificial Intelligence (AI). To apply these
techniques successfully in CF, we need to justify and explain the results to
the stakeholders of CF, such as forensic analysts and members of the court, for
them to make an informed decision. If we want to apply AI successfully in CF,
there is a need to develop trust in AI systems. Some other factors in accepting
the use of AI in CF are to make AI authentic, interpretable, understandable,
and interactive. This way, AI systems will be more acceptable to the public and
ensure alignment with legal standards. An explainable AI (XAI) system can play
this role in CF, and we call such a system XAI-CF. XAI-CF is indispensable and
is still in its infancy. In this paper, we explore and make a case for the
significance and advantages of XAI-CF. We strongly emphasize the need to build
a successful and practical XAI-CF system and discuss some of the main
requirements and prerequisites of such a system. We present a formal definition
of the terms CF and XAI-CF and a comprehensive literature review of previous
works that apply and utilize XAI to build and increase trust in CF. We discuss
some challenges facing XAI-CF. We also provide some concrete solutions to these
challenges. We identify key insights and future research directions for
building XAI applications for CF. This paper is an effort to explore and
familiarize the readers with the role of XAI applications in CF, and we believe
that our work provides a promising basis for future researchers interested in
XAI-CF.
\\ ( https://arxiv.org/abs/2402.02452 ,  219kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04267 (*cross-listing*)
Date: Fri, 26 Jan 2024 07:58:09 GMT   (593kb)

Title: Application analysis of ai technology combined with spiral CT scanning
  in early lung cancer screening
Authors: Shulin Li, Liqiang Yu, Bo Liu, Qunwei Lin, Jiaxin Huang
Categories: physics.med-ph cs.AI cs.CV eess.IV
Comments: This article was accepted by Frontiers in Computing and Intelligent
  Systems https://drpress.org/ojs/index.php/fcis/article/view/15781. arXiv
  admin note: text overlap with arXiv:nlin/0508031 by other authors
DOI: 10.54097/LAwfJzEA
\\
  At present, the incidence and fatality rate of lung cancer in China rank
first among all malignant tumors. Despite the continuous development and
improvement of China's medical level, the overall 5-year survival rate of lung
cancer patients is still lower than 20% and is staged. A number of studies have
confirmed that early diagnosis and treatment of early stage lung cancer is of
great significance to improve the prognosis of patients. In recent years,
artificial intelligence technology has gradually begun to be applied in
oncology. ai is used in cancer screening, clinical diagnosis, radiation therapy
(image acquisition, at-risk organ segmentation, image calibration and delivery)
and other aspects of rapid development. However, whether medical ai can be
socialized depends on the public's attitude and acceptance to a certain extent.
However, at present, there are few studies on the diagnosis of early lung
cancer by AI technology combined with SCT scanning. In view of this, this study
applied the combined method in early lung cancer screening, aiming to find a
safe and efficient screening mode and provide a reference for clinical
diagnosis and treatment.
\\ ( https://arxiv.org/abs/2402.04267 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04268 (*cross-listing*)
Date: Sat, 27 Jan 2024 20:19:49 GMT   (14892kb,D)

Title: ProtAgents: Protein discovery via large language model multi-agent
  collaborations combining physics and machine learning
Authors: A. Ghafarollahi, M.J. Buehler
Categories: cond-mat.soft cs.AI cs.CL q-bio.BM
\\
  Designing de novo proteins beyond those found in nature holds significant
promise for advancements in both scientific and engineering applications.
Current methodologies for protein design often rely on AI-based models, such as
surrogate models that address end-to-end problems by linking protein structure
to material properties or vice versa. However, these models frequently focus on
specific material objectives or structural properties, limiting their
flexibility when incorporating out-of-domain knowledge into the design process
or comprehensive data analysis is required. In this study, we introduce
ProtAgents, a platform for de novo protein design based on Large Language
Models (LLMs), where multiple AI agents with distinct capabilities
collaboratively address complex tasks within a dynamic environment. The
versatility in agent development allows for expertise in diverse domains,
including knowledge retrieval, protein structure analysis, physics-based
simulations, and results analysis. The dynamic collaboration between agents,
empowered by LLMs, provides a versatile approach to tackling protein design and
analysis problems, as demonstrated through diverse examples in this study. The
problems of interest encompass designing new proteins, analyzing protein
structures and obtaining new first-principles data -- natural vibrational
frequencies -- via physics simulations. The concerted effort of the system
allows for powerful automated and synergistic design of de novo proteins with
targeted mechanical properties. The flexibility in designing the agents, on one
hand, and their capacity in autonomous collaboration through the dynamic
LLM-based multi-agent environment on the other hand, unleashes great potentials
of LLMs in addressing multi-objective materials problems and opens up new
avenues for autonomous materials discovery and design.
\\ ( https://arxiv.org/abs/2402.04268 ,  14892kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04275 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:11:25 GMT   (491kb)

Title: Motion Mapping Cognition: A Nondecomposable Primary Process in Human
  Vision
Authors: Zhenping Xie
Categories: q-bio.NC cs.AI cs.CV
Comments: 7 pages, 3 figures
\\
  Human intelligence seems so mysterious that we have not successfully
understood its foundation until now. Here, I want to present a basic cognitive
process, motion mapping cognition (MMC), which should be a nondecomposable
primary function in human vision. Wherein, I point out that, MMC process can be
used to explain most of human visual functions in fundamental, but can not be
effectively modelled by traditional visual processing ways including image
segmentation, object recognition, object tracking etc. Furthermore, I state
that MMC may be looked as an extension of Chen's theory of topological
perception on human vision, and seems to be unsolvable using existing
intelligent algorithm skills. Finally, along with the requirements of MMC
problem, an interesting computational model, quantized topological matching
principle can be derived by developing the idea of optimal transport theory.
Above results may give us huge inspiration to develop more robust and
interpretable machine vision models.
\\ ( https://arxiv.org/abs/2402.04275 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04286 (*cross-listing*)
Date: Tue, 6 Feb 2024 02:29:17 GMT   (2567kb)

Title: Progress and Opportunities of Foundation Models in Bioinformatics
Authors: Qing Li, Zhihang Hu, Yixuan Wang, Lei Li, Yimin Fan, Irwin King, Le
  Song, Yu Li
Categories: q-bio.QM cs.AI cs.LG
Comments: 27 pages, 3 figures, 2 tables
MSC-class: cs.CL, 92-02
ACM-class: I.2.1
\\
  Bioinformatics has witnessed a paradigm shift with the increasing integration
of artificial intelligence (AI), particularly through the adoption of
foundation models (FMs). These AI techniques have rapidly advanced, addressing
historical challenges in bioinformatics such as the scarcity of annotated data
and the presence of data noise. FMs are particularly adept at handling
large-scale, unlabeled data, a common scenario in biological contexts due to
the time-consuming and costly nature of experimentally determining labeled
data. This characteristic has allowed FMs to excel and achieve notable results
in various downstream validation tasks, demonstrating their ability to
represent diverse biological entities effectively. Undoubtedly, FMs have
ushered in a new era in computational biology, especially in the realm of deep
learning. The primary goal of this survey is to conduct a systematic
investigation and summary of FMs in bioinformatics, tracing their evolution,
current research status, and the methodologies employed. Central to our focus
is the application of FMs to specific biological problems, aiming to guide the
research community in choosing appropriate FMs for their research needs. We
delve into the specifics of the problem at hand including sequence analysis,
structure prediction, function annotation, and multimodal integration,
comparing the structures and advancements against traditional methods.
Furthermore, the review analyses challenges and limitations faced by FMs in
biology, such as data noise, model explainability, and potential biases.
Finally, we outline potential development paths and strategies for FMs in
future biological research, setting the stage for continued innovation and
application in this rapidly evolving field. This comprehensive review serves
not only as an academic resource but also as a roadmap for future explorations
and applications of FMs in biology.
\\ ( https://arxiv.org/abs/2402.04286 ,  2567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04355 (*cross-listing*)
Date: Tue, 6 Feb 2024 19:39:26 GMT   (2669kb,D)

Title: PQMass: Probabilistic Assessment of the Quality of Generative Models
  using Probability Mass Estimation
Authors: Pablo Lemos, Sammy Sharief, Nikolay Malkin, Laurence
  Perreault-Levasseur, Yashar Hezaveh
Categories: stat.ML cs.AI cs.LG stat.ME
Comments: 14 pages, 13 figures
\\
  We propose a comprehensive sample-based method for assessing the quality of
generative models. The proposed approach enables the estimation of the
probability that two sets of samples are drawn from the same distribution,
providing a statistically rigorous method for assessing the performance of a
single generative model or the comparison of multiple competing models trained
on the same dataset. This comparison can be conducted by dividing the space
into non-overlapping regions and comparing the number of data samples in each
region. The method only requires samples from the generative model and the test
data. It is capable of functioning directly on high-dimensional data, obviating
the need for dimensionality reduction. Significantly, the proposed method does
not depend on assumptions regarding the density of the true distribution, and
it does not rely on training or fitting any auxiliary models. Instead, it
focuses on approximating the integral of the density (probability mass) across
various sub-regions within the data space.
\\ ( https://arxiv.org/abs/2402.04355 ,  2669kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04420 (*cross-listing*)
Date: Tue, 6 Feb 2024 21:39:13 GMT   (12172kb,D)

Title: Measuring machine learning harms from stereotypes: requires
  understanding who is being harmed by which errors in what ways
Authors: Angelina Wang and Xuechunzi Bai and Solon Barocas and Su Lin Blodgett
Categories: cs.CY cs.AI
Comments: earlier draft non-archival at EAAMO 2023
\\
  As machine learning applications proliferate, we need an understanding of
their potential for harm. However, current fairness metrics are rarely grounded
in human psychological experiences of harm. Drawing on the social psychology of
stereotypes, we use a case study of gender stereotypes in image search to
examine how people react to machine learning errors. First, we use survey
studies to show that not all machine learning errors reflect stereotypes nor
are equally harmful. Then, in experimental studies we randomly expose
participants to stereotype-reinforcing, -violating, and -neutral machine
learning errors. We find stereotype-reinforcing errors induce more
experientially (i.e., subjectively) harmful experiences, while having minimal
changes to cognitive beliefs, attitudes, or behaviors. This experiential harm
impacts women more than men. However, certain stereotype-violating errors are
more experientially harmful for men, potentially due to perceived threats to
masculinity. We conclude that harm cannot be the sole guide in fairness
mitigation, and propose a nuanced perspective depending on who is experiencing
what harm and why.
\\ ( https://arxiv.org/abs/2402.04420 ,  12172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04421 (*cross-listing*)
Date: Tue, 6 Feb 2024 21:39:55 GMT   (64kb,D)

Title: Studying Vulnerable Code Entities in R
Authors: Zixiao Zhao, Millon Madhur Das, Fatemeh H. Fard
Categories: cs.SE cs.AI
Comments: 5 pages, 3 figures, and 2 tables. to be published in ICPC 2024
\\
  Pre-trained Code Language Models (Code-PLMs) have shown many advancements and
achieved state-of-the-art results for many software engineering tasks in the
past few years. These models are mainly targeted for popular programming
languages such as Java and Python, leaving out many other ones like R. Though R
has a wide community of developers and users, there is little known about the
applicability of Code-PLMs for R. In this preliminary study, we aim to
investigate the vulnerability of Code-PLMs for code entities in R. For this
purpose, we use an R dataset of code and comment pairs and then apply
CodeAttack, a black-box attack model that uses the structure of code to
generate adversarial code samples. We investigate how the model can attack
different entities in R. This is the first step towards understanding the
importance of R token types, compared to popular programming languages (e.g.,
Java). We limit our study to code summarization. Our results show that the most
vulnerable code entity is the identifier, followed by some syntax tokens
specific to R. The results can shed light on the importance of token types and
help in developing models for code summarization and method name prediction for
the R language.
\\ ( https://arxiv.org/abs/2402.04421 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04466 (*cross-listing*)
Date: Tue, 6 Feb 2024 23:20:34 GMT   (1394kb,D)

Title: Towards Deterministic End-to-end Latency for Medical AI Systems in
  NVIDIA Holoscan
Authors: Soham Sinha, Shekhar Dwivedi, Mahdi Azizian
Categories: cs.SE cs.AI cs.LG cs.OS
ACM-class: C.3; J.7; D.2.11; D.2.10; D.4.8
\\
  The introduction of AI and ML technologies into medical devices has
revolutionized healthcare diagnostics and treatments. Medical device
manufacturers are keen to maximize the advantages afforded by AI and ML by
consolidating multiple applications onto a single platform. However, concurrent
execution of several AI applications, each with its own visualization
components, leads to unpredictable end-to-end latency, primarily due to GPU
resource contentions. To mitigate this, manufacturers typically deploy separate
workstations for distinct AI applications, thereby increasing financial,
energy, and maintenance costs. This paper addresses these challenges within the
context of NVIDIA's Holoscan platform, a real-time AI system for streaming
sensor data and images. We propose a system design optimized for heterogeneous
GPU workloads, encompassing both compute and graphics tasks. Our design
leverages CUDA MPS for spatial partitioning of compute workloads and isolates
compute and graphics processing onto separate GPUs. We demonstrate significant
performance improvements across various end-to-end latency determinism metrics
through empirical evaluation with real-world Holoscan medical device
applications. For instance, the proposed design reduces maximum latency by
21-30% and improves latency distribution flatness by 17-25% for up to five
concurrent endoscopy tool tracking AI applications, compared to a single-GPU
baseline. Against a default multi-GPU setup, our optimizations decrease maximum
latency by 35% for up to six concurrent applications by improving GPU
utilization by 42%. This paper provides clear design insights for AI
applications in the edge-computing domain including medical systems, where
performance predictability of concurrent and heterogeneous GPU workloads is a
critical requirement.
\\ ( https://arxiv.org/abs/2402.04466 ,  1394kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04476 (*cross-listing*)
Date: Tue, 6 Feb 2024 23:52:10 GMT   (2309kb,D)

Title: Dual-View Visual Contextualization for Web Navigation
Authors: Jihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang Deng, Yu Su, Wei-Lun
  Chao
Categories: cs.CV cs.AI cs.CL
\\
  Automatic web navigation aims to build a web agent that can follow language
instructions to execute complex and diverse tasks on real-world websites.
Existing work primarily takes HTML documents as input, which define the
contents and action spaces (i.e., actionable elements and operations) of
webpages. Nevertheless, HTML documents may not provide a clear task-related
context for each element, making it hard to select the right (sequence of)
actions. In this paper, we propose to contextualize HTML elements through their
"dual views" in webpage screenshots: each HTML element has its corresponding
bounding box and visual content in the screenshot. We build upon the insight --
web developers tend to arrange task-related elements nearby on webpages to
enhance user experiences -- and propose to contextualize each element with its
neighbor elements, using both textual and visual features. The resulting
representations of HTML elements are more informative for the agent to take
action. We validate our method on the recently released Mind2Web dataset, which
features diverse navigation domains and tasks on real-world websites. Our
method consistently outperforms the baseline in all the scenarios, including
cross-task, cross-website, and cross-domain ones.
\\ ( https://arxiv.org/abs/2402.04476 ,  2309kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04515 (*cross-listing*)
Date: Wed, 7 Feb 2024 01:48:29 GMT   (2539kb,D)

Title: A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in
  Next-gen Networks
Authors: Akshita Abrol, Purnima Murali Mohan, Tram Truong-Huu
Categories: cs.NI cs.AI
Comments: Accepted for publication in the Proceedings of the IEEE International
  Conference on Communications (IEEE ICC 2024)
\\
  Next-gen networks require significant evolution of management to enable
automation and adaptively adjust network configuration based on traffic
dynamics. The advent of software-defined networking (SDN) and programmable
switches enables flexibility and programmability. However, traditional
techniques that decide traffic policies are usually based on hand-crafted
programming optimization and heuristic algorithms. These techniques make
non-realistic assumptions, e.g., considering static network load and topology,
to obtain tractable solutions, which are inadequate for next-gen networks. In
this paper, we design and develop a deep reinforcement learning (DRL) approach
for adaptive traffic routing. We design a deep graph convolutional neural
network (DGCNN) integrated into the DRL framework to learn the traffic behavior
from not only the network topology but also link and node attributes. We adopt
the Deep Q-Learning technique to train the DGCNN model in the DRL framework
without the need for a labeled training dataset, enabling the framework to
quickly adapt to traffic dynamics. The model leverages q-value estimates to
select the routing path for every traffic flow request, balancing exploration
and exploitation. We perform extensive experiments with various traffic
patterns and compare the performance of the proposed approach with the Open
Shortest Path First (OSPF) protocol. The experimental results show the
effectiveness and adaptiveness of the proposed framework by increasing the
network throughput by up to 7.8% and reducing the traffic delay by up to 16.1%
compared to OSPF.
\\ ( https://arxiv.org/abs/2402.04515 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04527 (*cross-listing*)
Date: Wed, 7 Feb 2024 02:14:58 GMT   (2842kb,D)

Title: RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based
  Recommendation
Authors: Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang, Zhongrui Ma
Categories: cs.IR cs.AI
Comments: 10 pages
\\
  Large language models (LLM) have recently emerged as a powerful tool for a
variety of natural language processing tasks, bringing a new surge of combining
LLM with recommendation systems, termed as LLM-based RS. Current approaches
generally fall into two main paradigms, the ID direct usage paradigm and the ID
translation paradigm, noting their core weakness stems from lacking
recommendation knowledge and uniqueness. To address this limitation, we propose
a new paradigm, ID representation, which incorporates pre-trained ID embeddings
into LLMs in a complementary manner. In this work, we present RA-Rec, an
efficient ID representation alignment framework for LLM-based recommendation,
which is compatible with multiple ID-based methods and LLM architectures.
Specifically, we treat ID embeddings as soft prompts and design an innovative
alignment module and an efficient tuning method with tailored data construction
for alignment. Extensive experiments demonstrate RA-Rec substantially
outperforms current state-of-the-art methods, achieving up to 3.0% absolute
HitRate@100 improvements while utilizing less than 10x training data.
\\ ( https://arxiv.org/abs/2402.04527 ,  2842kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04536 (*cross-listing*)
Date: Wed, 7 Feb 2024 02:50:56 GMT   (18561kb,D)

Title: Tactile-based Object Retrieval From Granular Media
Authors: Jingxi Xu, Yinsen Jia, Dongxiao Yang, Patrick Meng, Xinyue Zhu, Zihan
  Guo, Shuran Song, Matei Ciocarlie
Categories: cs.RO cs.AI cs.LG
\\
  We introduce GEOTACT, a robotic manipulation method capable of retrieving
objects buried in granular media. This is a challenging task due to the need to
interact with granular media, and doing so based exclusively on tactile
feedback, since a buried object can be completely hidden from vision. Tactile
feedback is in itself challenging in this context, due to ubiquitous contact
with the surrounding media, and the inherent noise level induced by the tactile
readings. To address these challenges, we use a learning method trained
end-to-end with simulated sensor noise. We show that our problem formulation
leads to the natural emergence of learned pushing behaviors that the
manipulator uses to reduce uncertainty and funnel the object to a stable grasp
despite spurious and noisy tactile readings. We also introduce a training
curriculum that enables learning these behaviors in simulation, followed by
zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is
the first method to reliably retrieve a number of different objects from a
granular environment, doing so on real hardware and with integrated tactile
sensing. Videos and additional information can be found at
https://jxu.ai/geotact.
\\ ( https://arxiv.org/abs/2402.04536 ,  18561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04563 (*cross-listing*)
Date: Wed, 7 Feb 2024 03:43:56 GMT   (24154kb,D)

Title: Attention Guided CAM: Visual Explanations of Vision Transformer Guided
  by Self-Attention
Authors: Saebom Leem, Hyunseok Seo
Categories: cs.CV cs.AI
Comments: AAAI2024. Code available at
  https://github.com/LeemSaebom/Attention-Guided-CAM-Visual-Explanations-of-Vision-Transformer-Guided-by-Self-Attention.git
\\
  Vision Transformer(ViT) is one of the most widely used models in the computer
vision field with its great performance on various tasks. In order to fully
utilize the ViT-based architecture in various applications, proper
visualization methods with a decent localization performance are necessary, but
these methods employed in CNN-based models are still not available in ViT due
to its unique structure. In this work, we propose an attention-guided
visualization method applied to ViT that provides a high-level semantic
explanation for its decision. Our method selectively aggregates the gradients
directly propagated from the classification output to each self-attention,
collecting the contribution of image features extracted from each location of
the input image. These gradients are additionally guided by the normalized
self-attention scores, which are the pairwise patch correlation scores. They
are used to supplement the gradients on the patch-level context information
efficiently detected by the self-attention mechanism. This approach of our
method provides elaborate high-level semantic explanations with great
localization performance only with the class labels. As a result, our method
outperforms the previous leading explainability methods of ViT in the
weakly-supervised localization task and presents great capability in capturing
the full instances of the target class object. Meanwhile, our method provides a
visualization that faithfully explains the model, which is demonstrated in the
perturbation comparison test.
\\ ( https://arxiv.org/abs/2402.04563 ,  24154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04580 (*cross-listing*)
Date: Wed, 7 Feb 2024 04:43:41 GMT   (881kb,D)

Title: A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied
  Agents
Authors: Haoyi Niu, Jianming Hu, Guyue Zhou, Xianyuan Zhan
Categories: cs.RO cs.AI cs.LG
\\
  The burgeoning fields of robot learning and embodied AI have triggered an
increasing demand for large quantities of data. However, collecting sufficient
unbiased data from the target domain remains a challenge due to costly data
collection processes and stringent safety requirements. Consequently,
researchers often resort to data from easily accessible source domains, such as
simulation and laboratory environments, for cost-effective data acquisition and
rapid model iteration. Nevertheless, the environments and embodiments of these
source domains can be quite different from their target domain counterparts,
underscoring the need for effective cross-domain policy transfer approaches. In
this paper, we conduct a systematic review of existing cross-domain policy
transfer methods. Through a nuanced categorization of domain gaps, we
encapsulate the overarching insights and design considerations of each problem
setting. We also provide a high-level discussion about the key methodologies
used in cross-domain policy transfer problems. Lastly, we summarize the open
challenges that lie beyond the capabilities of current paradigms and discuss
potential future directions in this field.
\\ ( https://arxiv.org/abs/2402.04580 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04599 (*cross-listing*)
Date: Wed, 7 Feb 2024 05:47:31 GMT   (10371kb,D)

Title: Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via
  Temporal-Viewpoint Alignment
Authors: Lei Wang and Jun Liu and Liang Zheng and Tom Gedeon and Piotr Koniusz
Categories: cs.CV cs.AI cs.LG
Comments: Under minor revision with IJCV. An extension of our ACCV'22 paper
  [arXiv:arXiv:2210.16820] which was distinguished by the Sang Uk Lee Best
  Student Paper Award. arXiv admin note: text overlap with arXiv:2112.12668
\\
  Video sequences exhibit significant nuisance variations (undesired effects)
of speed of actions, temporal locations, and subjects' poses, leading to
temporal-viewpoint misalignment when comparing two sets of frames or evaluating
the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera
viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D
skeleton sequences whose camera and subjects' poses can be easily manipulated
in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where
matching well temporal blocks (temporal chunks that make up a sequence) of
support-query sequence pairs (by factoring out nuisance variations) is
essential due to limited samples of novel classes. Given a query sequence, we
create its several views by simulating several camera locations. For a support
sequence, we match it with view-simulated query sequences, as in the popular
Dynamic Time Warping (DTW). Specifically, each support temporal block can be
matched to the query temporal block with the same or adjacent (next) temporal
index, and adjacent camera views to achieve joint local temporal-viewpoint
warping. JEANIE selects the smallest distance among matching paths with
different temporal-viewpoint warping patterns, an advantage over DTW which only
performs temporal alignment. We also propose an unsupervised FSAR akin to
clustering of sequences with JEANIE as a distance measure. JEANIE achieves
state-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D
Multiview Activity II on supervised and unsupervised FSAR, and their
meta-learning inspired fusion.
\\ ( https://arxiv.org/abs/2402.04599 ,  10371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04615 (*cross-listing*)
Date: Wed, 7 Feb 2024 06:42:33 GMT   (6360kb,D)

Title: ScreenAI: A Vision-Language Model for UI and Infographics Understanding
Authors: Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan
  Mansoor, Vincent Etter, Victor C\u{a}rbune, Jason Lin, Jindong Chen, Abhanshu
  Sharma
Categories: cs.CV cs.AI
Comments: 7 pages main tex with 5 figures, 2 page bib, 6 pages appendix
\\
  Screen user interfaces (UIs) and infographics, sharing similar visual
language and design principles, play important roles in human communication and
human-machine interaction. We introduce ScreenAI, a vision-language model that
specializes in UI and infographics understanding. Our model improves upon the
PaLI architecture with the flexible patching strategy of pix2struct and is
trained on a unique mixture of datasets. At the heart of this mixture is a
novel screen annotation task in which the model has to identify the type and
location of UI elements. We use these text annotations to describe screens to
Large Language Models and automatically generate question-answering (QA), UI
navigation, and summarization training datasets at scale. We run ablation
studies to demonstrate the impact of these design choices. At only 5B
parameters, ScreenAI achieves new state-of-the-artresults on UI- and
infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget
Captioning), and new best-in-class performance on others (Chart QA, DocVQA, and
InfographicVQA) compared to models of similar size. Finally, we release three
new datasets: one focused on the screen annotation task and two others focused
on question answering.
\\ ( https://arxiv.org/abs/2402.04615 ,  6360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04660 (*cross-listing*)
Date: Wed, 7 Feb 2024 08:49:33 GMT   (3034kb,D)

Title: Adversarial Robustness Through Artifact Design
Authors: Tsufit Shua and Mahmood Sharif
Categories: cs.CR cs.AI cs.CV cs.LG
\\
  Adversarial examples arose as a challenge for machine learning. To hinder
them, most defenses alter how models are trained (e.g., adversarial training)
or inference is made (e.g., randomized smoothing). Still, while these
approaches markedly improve models' adversarial robustness, models remain
highly susceptible to adversarial examples. Identifying that, in certain
domains such as traffic-sign recognition, objects are implemented per standards
specifying how artifacts (e.g., signs) should be designed, we propose a novel
approach for improving adversarial robustness. Specifically, we offer a method
to redefine standards, making minor changes to existing ones, to defend against
adversarial examples. We formulate the problem of artifact design as a robust
optimization problem, and propose gradient-based and greedy search methods to
solve it. We evaluated our approach in the domain of traffic-sign recognition,
allowing it to alter traffic-sign pictograms (i.e., symbols within the signs)
and their colors. We found that, combined with adversarial training, our
approach led to up to 25.18\% higher robust accuracy compared to
state-of-the-art methods against two adversary types, while further increasing
accuracy on benign inputs.
\\ ( https://arxiv.org/abs/2402.04660 ,  3034kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04699 (*cross-listing*)
Date: Wed, 7 Feb 2024 09:39:29 GMT   (5051kb,D)

Title: EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World
  Illusions
Authors: Shashank Kotyan, PoYuan Mao, Danilo Vasconcellos Vargas
Categories: cs.CV cs.AI cs.LG cs.NE
\\
  Deep neural networks are exploited using natural adversarial samples, which
have no impact on human perception but are misclassified. Current approaches
often rely on the white-box nature of deep neural networks to generate these
adversarial samples or alter the distribution of adversarial samples compared
to training distribution. To alleviate the limitations of current approaches,
we propose EvoSeed, a novel evolutionary strategy-based search algorithmic
framework to generate natural adversarial samples. Our EvoSeed framework uses
auxiliary Diffusion and Classifier models to operate in a model-agnostic
black-box setting. We employ CMA-ES to optimize the search for an adversarial
seed vector, which, when processed by the Conditional Diffusion Model, results
in an unrestricted natural adversarial sample misclassified by the Classifier
Model. Experiments show that generated adversarial images are of high image
quality and are transferable to different classifiers. Our approach
demonstrates promise in enhancing the quality of adversarial samples using
evolutionary algorithms. We hope our research opens new avenues to enhance the
robustness of deep neural networks in real-world scenarios. Project Website can
be accessed at \url{https://shashankkotyan.github.io/EvoSeed}.
\\ ( https://arxiv.org/abs/2402.04699 ,  5051kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04763 (*cross-listing*)
Date: Wed, 7 Feb 2024 11:26:53 GMT   (1990kb,D)

Title: Emergence of specialized Collective Behaviors in Evolving Heterogeneous
  Swarms
Authors: Fuda van Diggelen, Matteo De Carlo, Nicolas Cambier, Eliseo Ferrante,
  A.E. Eiben
Categories: cs.RO cs.AI
\\
  Natural groups of animals, such as swarms of social insects, exhibit
astonishing degrees of task specialization, useful to address complex tasks and
to survive. This is supported by phenotypic plasticity: individuals sharing the
same genotype that is expressed differently for different classes of
individuals, each specializing in one task. In this work, we evolve a swarm of
simulated robots with phenotypic plasticity to study the emergence of
specialized collective behavior during an emergent perception task. Phenotypic
plasticity is realized in the form of heterogeneity of behavior by dividing the
genotype into two components, with one different neural network controller
associated to each component. The whole genotype, expressing the behavior of
the whole group through the two components, is subject to evolution with a
single fitness function. We analyse the obtained behaviors and use the insights
provided by these results to design an online regulatory mechanism. Our
experiments show three main findings: 1) The sub-groups evolve distinct
emergent behaviors. 2) The effectiveness of the whole swarm depends on the
interaction between the two sub-groups, leading to a more robust performance
than with singular sub-group behavior. 3) The online regulatory mechanism
enhances overall performance and scalability.
\\ ( https://arxiv.org/abs/2402.04763 ,  1990kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04880 (*cross-listing*)
Date: Sat, 20 Jan 2024 06:14:22 GMT   (3985kb,D)

Title: Combining Cloud and Mobile Computing for Machine Learning
Authors: Ruiqi Xu and Tianchi Zhang contributed equally to this work
Categories: cs.DC cs.AI cs.LG
\\
  Although the computing power of mobile devices is increasing, machine
learning models are also growing in size. This trend creates problems for
mobile devices due to limitations like their memory capacity and battery life.
While many services, like ChatGPT and Midjourney, run all the inferences in the
cloud, we believe a flexible and fine-grained task distribution is more
desirable. In this work, we consider model segmentation as a solution to
improving the user experience, dividing the computation between mobile devices
and the cloud in a way that offloads the compute-heavy portion of the model
while minimizing the data transfer required. We show that the division not only
reduces the wait time for users but can also be fine-tuned to optimize the
workloads of the cloud. To achieve that, we design a scheduler that collects
information about network quality, client device capability, and job
requirements, making decisions to achieve consistent performance across a range
of devices while reducing the work the cloud needs to perform.
\\ ( https://arxiv.org/abs/2402.04880 ,  3985kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04882 (*cross-listing*)
Date: Sat, 20 Jan 2024 01:10:18 GMT   (864kb,D)

Title: LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre
  Memory Units
Authors: Zeyu Liu, Gourav Datta, Anni Li, Peter Anthony Beerel
Categories: cs.NE cs.AI cs.LG cs.SD eess.AS
Comments: The 12th International Conference on Learning Representations (ICLR
  2024)
\\
  Transformer models have demonstrated high accuracy in numerous applications
but have high complexity and lack sequential processing capability making them
ill-suited for many streaming applications at the edge where devices are
heavily resource-constrained. Thus motivated, many researchers have proposed
reformulating the transformer models as RNN modules which modify the
self-attention computation with explicit states. However, these approaches
often incur significant performance degradation. The ultimate goal is to
develop a model that has the following properties: parallel training, streaming
and low-cost inference, and SOTA performance. In this paper, we propose a new
direction to achieve this goal. We show how architectural modifications to a
recurrent model can help push its performance toward Transformer models while
retaining its sequential processing capability. Specifically, inspired by the
recent success of Legendre Memory Units (LMU) in sequence learning tasks, we
propose LMUFormer, which augments the LMU with convolutional patch embedding
and convolutional channel mixer. Moreover, we present a spiking version of this
architecture, which introduces the benefit of states within the patch embedding
and channel mixer modules while simultaneously reducing the computing
complexity. We evaluated our architectures on multiple sequence datasets. In
comparison to SOTA transformer-based models within the ANN domain on the SCv2
dataset, our LMUFormer demonstrates comparable performance while necessitating
a remarkable 53 times reduction in parameters and a substantial 65 times
decrement in FLOPs. Additionally, owing to our model's proficiency in real-time
data processing, we can achieve a 32.03% reduction in sequence length, all
while incurring an inconsequential decline in performance. Our code is publicly
available at https://github.com/zeyuliu1037/LMUFormer.git.
\\ ( https://arxiv.org/abs/2402.04882 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04885 (*cross-listing*)
Date: Fri, 19 Jan 2024 21:11:32 GMT   (4147kb,D)

Title: A Unified Gaussian Process for Branching and Nested Hyperparameter
  Optimization
Authors: Jiazhao Zhang and Ying Hung and Chung-Ching Lin and Zicheng Liu
Categories: stat.ML cs.AI cs.LG
\\
  Choosing appropriate hyperparameters plays a crucial role in the success of
neural networks as hyper-parameters directly control the behavior and
performance of the training algorithms. To obtain efficient tuning, Bayesian
optimization methods based on Gaussian process (GP) models are widely used.
Despite numerous applications of Bayesian optimization in deep learning, the
existing methodologies are developed based on a convenient but restrictive
assumption that the tuning parameters are independent of each other. However,
tuning parameters with conditional dependence are common in practice. In this
paper, we focus on two types of them: branching and nested parameters. Nested
parameters refer to those tuning parameters that exist only within a particular
setting of another tuning parameter, and a parameter within which other
parameters are nested is called a branching parameter. To capture the
conditional dependence between branching and nested parameters, a unified
Bayesian optimization framework is proposed. The sufficient conditions are
rigorously derived to guarantee the validity of the kernel function, and the
asymptotic convergence of the proposed optimization framework is proven under
the continuum-armed-bandit setting. Based on the new GP model, which accounts
for the dependent structure among input variables through a new kernel
function, higher prediction accuracy and better optimization efficiency are
observed in a series of synthetic simulations and real data applications of
neural networks. Sensitivity analysis is also performed to provide insights
into how changes in hyperparameter values affect prediction accuracy.
\\ ( https://arxiv.org/abs/2402.04885 ,  4147kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04888 (*cross-listing*)
Date: Fri, 19 Jan 2024 20:30:23 GMT   (163kb,D)

Title: RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing
Authors: Borna Barahimi, Hakam Singh, Hina Tabassum, Omer Waqar, Mohammad Omer
Categories: cs.IT cs.AI cs.HC cs.LG eess.SP math.IT
Comments: The paper has been accepted by IEEE International Conference on
  Communications (ICC) 2024
\\
  WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere
communication devices to sensing instruments, leveraging Channel State
Information (CSI) extraction capabilities. Nevertheless, resource-constrained
IoT devices and the intricacies of deep neural networks necessitate
transmitting CSI to cloud servers for sensing. Although feasible, this leads to
considerable communication overhead. In this context, this paper develops a
novel Real-time Sensing and Compression Network (RSCNet) which enables sensing
with compressed CSI; thereby reducing the communication overheads. RSCNet
facilitates optimization across CSI windows composed of a few CSI frames. Once
transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to
harness data from prior windows, thus bolstering both the sensing accuracy and
CSI reconstruction. RSCNet adeptly balances the trade-off between CSI
compression and sensing precision, thus streamlining real-time cloud-based WiFi
sensing with reduced communication costs. Numerical findings demonstrate the
gains of RSCNet over the existing benchmarks like SenseFi, showcasing a sensing
accuracy of 97.4% with minimal CSI reconstruction error. Numerical results also
show a computational analysis of the proposed RSCNet as a function of the
number of CSI frames.
\\ ( https://arxiv.org/abs/2402.04888 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04929 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:56:13 GMT   (14307kb,D)

Title: Source-Free Domain Adaptation with Diffusion-Guided Source Data
  Generation
Authors: Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha
Categories: cs.CV cs.AI cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2310.01701
\\
  This paper introduces a novel approach to leverage the generalizability
capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our
proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image
diffusion model to generate source domain images using features from the target
images to guide the diffusion process. Specifically, the pre-trained diffusion
model is fine-tuned to generate source samples that minimize entropy and
maximize confidence for the pre-trained source model. We then apply established
unsupervised domain adaptation techniques to align the generated source images
with target domain data. We validate our approach through comprehensive
experiments across a range of datasets, including Office-31, Office-Home, and
VisDA. The results highlight significant improvements in SFDA performance,
showcasing the potential of diffusion models in generating contextually
relevant, domain-specific images.
\\ ( https://arxiv.org/abs/2402.04929 ,  14307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04955 (*cross-listing*)
Date: Wed, 7 Feb 2024 15:39:07 GMT   (399kb,D)

Title: Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based
  Systems
Authors: Samuel Kernan Freire, Chaofan Wang, Evangelos Niforatos
Categories: cs.HC cs.AI
Comments: 10 pages, 7 figures, under review at an ACM venue
\\
  Cognitive assistants (CA) are chatbots that provide context-aware support to
human workers in knowledge-intensive tasks. Traditionally, cognitive assistants
respond in specific ways to predefined user intents and conversation patterns.
However, this rigidness does not handle the diversity of natural language well.
Recent advances in natural language processing (NLP), powering large language
models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in
a more flexible, human-like manner. However, the additional degrees of freedom
may have unforeseen consequences, especially in knowledge-intensive contexts
where accuracy is crucial. As a preliminary step to assessing the potential of
using LLMs in these contexts, we conducted a user study comparing an LLM-based
CA to an intent-based system regarding interaction efficiency, user experience,
workload, and usability. This revealed that LLM-based CAs exhibited better user
experience, task completion rate, usability, and perceived performance than
intent-based systems, suggesting that switching NLP techniques should be
investigated further.
\\ ( https://arxiv.org/abs/2402.04955 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04975 (*cross-listing*)
Date: Wed, 7 Feb 2024 15:55:51 GMT   (19871kb,D)

Title: ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming
  Learning for Children Aged 6-12
Authors: Liuqing Chen, Shuhong Xiao, Yunnong Chen, Ruoyu Wu, Yaxuan Song,
  Lingyun Sun
Categories: cs.HC cs.AI cs.PL
Comments: 29 pages, 7 figures, accepted by CHI 2024
DOI: 10.1145/3613904.3642229
\\
  As Computational Thinking (CT) continues to permeate younger age groups in
K-12 education, established CT platforms such as Scratch face challenges in
catering to these younger learners, particularly those in the elementary school
(ages 6-12). Through formative investigation with Scratch experts, we uncover
three key obstacles to children's autonomous Scratch learning: artist's block
in project planning, bounded creativity in asset creation, and inadequate
coding guidance during implementation. To address these barriers, we introduce
ChatScratch, an AI-augmented system to facilitate autonomous programming
learning for young children. ChatScratch employs structured interactive
storyboards and visual cues to overcome artist's block, integrates digital
drawing and advanced image generation technologies to elevate creativity, and
leverages Scratch-specialized Large Language Models (LLMs) for professional
coding guidance. Our study shows that, compared to Scratch, ChatScratch
efficiently fosters autonomous programming learning, and contributes to the
creation of high-quality, personally meaningful Scratch projects for children.
\\ ( https://arxiv.org/abs/2402.04975 ,  19871kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04979 (*cross-listing*)
Date: Wed, 7 Feb 2024 15:57:28 GMT   (23579kb,D)

Title: Detection and Pose Estimation of flat, Texture-less Industry Objects on
  HoloLens using synthetic Training
Authors: Thomas P\"ollabauer, Fabian R\"ucker, Andreas Franek, Felix
  Gorschl\"uter
Categories: cs.CV cs.AI
Comments: Scandinavian Conference on Image Analysis 2023
Journal-ref: In Scandinavian Conference on Image Analysis 2023 (pp. 569-585).
  Cham: Springer Nature Switzerland
\\
  Current state-of-the-art 6d pose estimation is too compute intensive to be
deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both
used for an increasing number of augmented reality applications. The quality of
AR is greatly dependent on its capabilities to detect and overlay geometry
within the scene. We propose a synthetically trained client-server-based
augmented reality application, demonstrating state-of-the-art object pose
estimation of metallic and texture-less industry objects on edge devices.
Synthetic data enables training without real photographs, i.e. for
yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted
sorting task, and quantitative evaluation on both renderings, as well as
real-world data recorded on HoloLens 2, sheds light on its real-world
applicability.
\\ ( https://arxiv.org/abs/2402.04979 ,  23579kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05008 (*cross-listing*)
Date: Wed, 7 Feb 2024 16:28:36 GMT   (2998kb,D)

Title: EfficientViT-SAM: Accelerated Segment Anything Model Without Performance
  Loss
Authors: Zhuoyang Zhang, Han Cai, Song Han
Categories: cs.CV cs.AI cs.LG
Comments: tech report
\\
  We present EfficientViT-SAM, a new family of accelerated segment anything
models. We retain SAM's lightweight prompt encoder and mask decoder while
replacing the heavy image encoder with EfficientViT. For the training, we begin
with the knowledge distillation from the SAM-ViT-H image encoder to
EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B
dataset. Benefiting from EfficientViT's efficiency and capacity,
EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over
SAM-ViT-H without sacrificing performance. Our code and pre-trained models are
released at https://github.com/mit-han-lab/efficientvit.
\\ ( https://arxiv.org/abs/2402.05008 ,  2998kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05027 (*cross-listing*)
Date: Wed, 7 Feb 2024 16:53:09 GMT   (1686kb,D)

Title: Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs
  with Recurrent Message Passing
Authors: Jannis Weil and Zhenghua Bao and Osama Abboud and Tobias Meuser
Categories: cs.MA cs.AI
Comments: Accepted at AAMAS 2024, version with appendix
\\
  Graph-based environments pose unique challenges to multi-agent reinforcement
learning. In decentralized approaches, agents operate within a given graph and
make decisions based on partial or outdated observations. The size of the
observed neighborhood limits the generalizability to different graphs and
affects the reactivity of agents, the quality of the selected actions, and the
communication overhead. This work focuses on generalizability and resolves the
trade-off in observed neighborhood size with a continuous information flow in
the whole graph. We propose a recurrent message-passing model that iterates
with the environment's steps and allows nodes to create a global representation
of the graph by exchanging messages with their neighbors. Agents receive the
resulting learned graph observations based on their location in the graph. Our
approach can be used in a decentralized manner at runtime and in combination
with a reinforcement learning algorithm of choice. We evaluate our method
across 1000 diverse graphs in the context of routing in communication networks
and find that it enables agents to generalize and adapt to changes in the
graph.
\\ ( https://arxiv.org/abs/2402.05027 ,  1686kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05106 (*cross-listing*)
Date: Wed, 7 Feb 2024 18:57:37 GMT   (1347kb,D)

Title: Image captioning for Brazilian Portuguese using GRIT model
Authors: Rafael Silva de Alencar and William Alberto Cruz Casta\~neda and
  Marcellus Amadeus
Categories: cs.CV cs.AI cs.CL
Comments: arXiv admin note: text overlap with arXiv:2207.09666 by other authors
\\
  This work presents the early development of a model of image captioning for
the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based
Image captioning Transformer) model to accomplish this work. GRIT is a
Transformer-only neural architecture that effectively utilizes two visual
features to generate better captions. The GRIT method emerged as a proposal to
be a more efficient way to generate image captioning. In this work, we adapt
the GRIT model to be trained in a Brazilian Portuguese dataset to have an image
captioning method for the Brazilian Portuguese Language.
\\ ( https://arxiv.org/abs/2402.05106 ,  1347kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04373 (*cross-listing*)
Date: Tue, 6 Feb 2024 20:18:32 GMT   (4961kb,D)

Title: The World of Generative AI: Deepfakes and Large Language Models
Authors: Alakananda Mitra, Saraju P. Mohanty, and Elias Kougianos
Categories: cs.CY cs.CL
\\
  We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes
and Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in
particular, pose an alarming threat to society as they are capable of spreading
misinformation and changing the truth. LLMs are powerful language models that
generate general-purpose language. However due to its generative aspect, it can
also be a risk for people if used with ill intentions. The ethical use of these
technologies is a big concern. This short article tries to find out the
interrelationship between them.
\\ ( https://arxiv.org/abs/2402.04373 ,  4961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04492 (*cross-listing*)
Date: Wed, 7 Feb 2024 00:31:49 GMT   (13881kb,D)

Title: ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation
Authors: Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, Tristan Thrush
Categories: cs.CV cs.CL
\\
  This paper introduces the ColorSwap dataset, designed to assess and improve
the proficiency of multimodal models in matching objects with their colors. The
dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000
examples. Each example includes a caption-image pair, along with a
``color-swapped'' pair. We follow the Winoground schema: the two captions in an
example have the same words, but the color words have been rearranged to modify
different objects. The dataset was created through a novel blend of automated
caption and image generation with humans in the loop. We evaluate image-text
matching (ITM) and visual language models (VLMs) and find that even the latest
ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on
our main VLM metric, although they may improve with more advanced prompting
techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP
perform close to chance (at 12% and 30%, respectively), although the
non-contrastive BLIP ITM model is stronger (87%). We also find that finetuning
on fewer than 2,000 examples yields significant performance gains on this
out-of-distribution word-order understanding task. The dataset is here:
https://github.com/Top34051/colorswap.
\\ ( https://arxiv.org/abs/2402.04492 ,  13881kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04854 (*cross-listing*)
Date: Wed, 7 Feb 2024 13:54:06 GMT   (8364kb,D)

Title: Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
Authors: Jinghong Li, Huy Phan, Wen Gu, Koichi Ota, Shinobu Hasegawa
Categories: cs.DL cs.CL cs.LG
Comments: This paper will submit to '27th International Symposium on
  Methodologies for Intelligent Systems'(ISMIS 2024)
\\
  Research surveys have always posed a challenge for beginner researchers who
lack of research training. These researchers struggle to understand the
directions within their research topic, and the discovery of new research
findings within a short time. One way to provide intuitive assistance to
beginner researchers is by offering relevant knowledge graphs(KG) and
recommending related academic papers. However, existing navigation knowledge
graphs primarily rely on keywords in the research field and often fail to
present the logical hierarchy among multiple related papers clearly. Moreover,
most recommendation systems for academic papers simply rely on high text
similarity, which can leave researchers confused as to why a particular article
is being recommended. They may lack of grasp important information about the
insight connection between "Issue resolved" and "Issue finding" that they hope
to obtain. To address these issues, this study aims to support research insight
surveys for beginner researchers by establishing a hierarchical tree-structured
knowledge graph that reflects the inheritance insight of research topics and
the relevance insight among the academic papers.
\\ ( https://arxiv.org/abs/2402.04854 ,  8364kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04889 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:22:51 GMT   (659kb,D)

Title: Detecting Generated Native Ads in Conversational Search
Authors: Sebastian Schmidt, Ines Zelch, Janek Bevendorff, Benno Stein, Matthias
  Hagen, Martin Potthast
Categories: cs.IR cs.CL
Comments: Submitted to WWW'24 Short Papers Track; 4 pages
\\
  Conversational search engines such as YouChat and Microsoft Copilot use large
language models (LLMs) to generate answers to queries. It is only a small step
to also use this technology to generate and integrate advertising within these
answers - instead of placing ads separately from the organic search results.
This type of advertising is reminiscent of native advertising and product
placement, both of which are very effective forms of subtle and manipulative
advertising. It is likely that information seekers will be confronted with such
use of LLM technology in the near future, especially when considering the high
computational costs associated with LLMs, for which providers need to develop
sustainable business models. This paper investigates whether LLMs can also be
used as a countermeasure against generated native ads, i.e., to block them. For
this purpose we compile a large dataset of ad-prone queries and of generated
answers with automatically integrated ads to experiment with fine-tuned
sentence transformers and state-of-the-art LLMs on the task of recognizing the
ads. In our experiments sentence transformers achieve detection precision and
recall values above 0.9, while the investigated LLMs struggle with the task.
\\ ( https://arxiv.org/abs/2402.04889 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04254 (*cross-listing*)
Date: Sun, 15 Oct 2023 13:07:41 GMT   (477kb)

Title: Large Vocabulary Spontaneous Speech Recognition for Tigrigna
Authors: Ataklti Kahsu, Solomon Teferra
Categories: eess.AS cs.LG cs.SD
Comments: 15 pages, 1 figures
MSC-class: 68T50 (Primary)
ACM-class: H.1.2
\\
  This thesis proposes and describes a research attempt at designing and
developing a speaker independent spontaneous automatic speech recognition
system for Tigrigna The acoustic model of the Speech Recognition System is
developed using Carnegie Mellon University Automatic Speech Recognition
development tool (Sphinx) while the SRIM tool is used for the development of
the language model.
  Keywords Automatic Speech Recognition Tigrigna language
\\ ( https://arxiv.org/abs/2402.04254 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04273 (*cross-listing*)
Date: Tue, 6 Feb 2024 20:09:52 GMT   (16473kb,D)

Title: Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception
  from Independent Private Sources
Authors: Jinlong Li, Baolu Li, Xinyu Liu, Runsheng Xu, Jiaqi Ma, Hongkai Yu
Categories: cs.CV cs.LG
Comments: Accepted by the 2024 IEEE International Conference on Robotics and
  Automation (ICRA)
\\
  The diverse agents in multi-agent perception systems may be from different
companies. Each company might use the identical classic neural network
architecture based encoder for feature extraction. However, the data source to
train the various agents is independent and private in each company, leading to
the Distribution Gap of different private data for training distinct agents in
multi-agent perception system. The data silos by the above Distribution Gap
could result in a significant performance decline in multi-agent perception. In
this paper, we thoroughly examine the impact of the distribution gap on
existing multi-agent perception systems. To break the data silos, we introduce
the Feature Distribution-aware Aggregation (FDA) framework for cross-domain
learning to mitigate the above Distribution Gap in multi-agent perception. FDA
comprises two key components: Learnable Feature Compensation Module and
Distribution-aware Statistical Consistency Module, both aimed at enhancing
intermediate features to minimize the distribution gap among multi-agent
features. Intensive experiments on the public OPV2V and V2XSet datasets
underscore FDA's effectiveness in point cloud-based 3D object detection,
presenting it as an invaluable augmentation to existing multi-agent perception
systems.
\\ ( https://arxiv.org/abs/2402.04273 ,  16473kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04274 (*cross-listing*)
Date: Fri, 2 Feb 2024 07:52:20 GMT   (1494kb,D)

Title: FPGA Deployment of LFADS for Real-time Neuroscience Experiments
Authors: Xiaohan Liu, ChiJui Chen, YanLun Huang, LingChi Yang, Elham E Khoda,
  Yihui Chen, Scott Hauck, Shih-Chieh Hsu, Bo-Cheng Lai
Categories: q-bio.NC cs.LG cs.NE
Comments: 6 pages, 8 figures
Journal-ref: Fast Machine Learning for Science, ICCAD 2023
\\
  Large-scale recordings of neural activity are providing new opportunities to
study neural population dynamics. A powerful method for analyzing such
high-dimensional measurements is to deploy an algorithm to learn the
low-dimensional latent dynamics. LFADS (Latent Factor Analysis via Dynamical
Systems) is a deep learning method for inferring latent dynamics from
high-dimensional neural spiking data recorded simultaneously in single trials.
This method has shown a remarkable performance in modeling complex brain
signals with an average inference latency in milliseconds. As our capacity of
simultaneously recording many neurons is increasing exponentially, it is
becoming crucial to build capacity for deploying low-latency inference of the
computing algorithms. To improve the real-time processing ability of LFADS, we
introduce an efficient implementation of the LFADS models onto Field
Programmable Gate Arrays (FPGA). Our implementation shows an inference latency
of 41.97 $\mu$s for processing the data in a single trial on a Xilinx U55C.
\\ ( https://arxiv.org/abs/2402.04274 ,  1494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04278 (*cross-listing*)
Date: Mon, 5 Feb 2024 06:05:30 GMT   (112480kb,D)

Title: Gaussian Plane-Wave Neural Operator for Electron Density Estimation
Authors: Seongsu Kim, Sungsoo Ahn
Categories: physics.chem-ph cs.LG
\\
  This work studies machine learning for electron density prediction, which is
fundamental for understanding chemical systems and density functional theory
(DFT) simulations. To this end, we introduce the Gaussian plane-wave neural
operator (GPWNO), which operates in the infinite-dimensional functional space
using the plane-wave and Gaussian-type orbital bases, widely recognized in the
context of DFT. In particular, both high- and low-frequency components of the
density can be effectively represented due to the complementary nature of the
two bases. Extensive experiments on QM9, MD, and material project datasets
demonstrate GPWNO's superior performance over ten baselines.
\\ ( https://arxiv.org/abs/2402.04278 ,  112480kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04301 (*cross-listing*)
Date: Tue, 6 Feb 2024 17:00:19 GMT   (2444kb,D)

Title: Deep PCCT: Photon Counting Computed Tomography Deep Learning
  Applications Review
Authors: Ana Carolina Alves, Andr\'e Ferreira, Gijs Luijten, Jens Kleesiek,
  Behrus Puladi, Jan Egger, Victor Alves
Categories: eess.IV cs.CE cs.CV cs.LG
\\
  Medical imaging faces challenges such as limited spatial resolution,
interference from electronic noise and poor contrast-to-noise ratios. Photon
Counting Computed Tomography (PCCT) has emerged as a solution, addressing these
issues with its innovative technology. This review delves into the recent
developments and applications of PCCT in pre-clinical research, emphasizing its
potential to overcome traditional imaging limitations. For example PCCT has
demonstrated remarkable efficacy in improving the detection of subtle
abnormalities in breast, providing a level of detail previously unattainable.
Examining the current literature on PCCT, it presents a comprehensive analysis
of the technology, highlighting the main features of scanners and their varied
applications. In addition, it explores the integration of deep learning into
PCCT, along with the study of radiomic features, presenting successful
applications in data processing. While acknowledging these advances, it also
discusses the existing challenges in this field, paving the way for future
research and improvements in medical imaging technologies. Despite the limited
number of articles on this subject, due to the recent integration of PCCT at a
clinical level, its potential benefits extend to various diagnostic
applications.
\\ ( https://arxiv.org/abs/2402.04301 ,  2444kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04326 (*cross-listing*)
Date: Tue, 6 Feb 2024 19:09:44 GMT   (1182kb,D)

Title: Personality Trait Recognition using ECG Spectrograms and Deep Learning
Authors: Muhammad Mohsin Altaf, Saadat Ullah Khan, Muhammad Majd, Syed Muhammad
  Anwar
Categories: cs.HC cs.LG eess.SP
\\
  This paper presents an innovative approach to recognizing personality traits
using deep learning (DL) methods applied to electrocardiogram (ECG) signals.
Within the framework of detecting the big five personality traits model
encompassing extra-version, neuroticism, agreeableness, conscientiousness, and
openness, the research explores the potential of ECG-derived spectrograms as
informative features. Optimal window sizes for spectrogram generation are
determined, and a convolutional neural network (CNN), specifically Resnet-18,
and visual transformer (ViT) are employed for feature extraction and
personality trait classification. The study utilizes the publicly available
ASCERTAIN dataset, which comprises various physiological signals, including ECG
recordings, collected from 58 participants during the presentation of video
stimuli categorized by valence and arousal levels. The outcomes of this study
demonstrate noteworthy performance in personality trait classification,
consistently achieving F1-scores exceeding 0.9 across different window sizes
and personality traits. These results emphasize the viability of ECG signal
spectrograms as a valuable modality for personality trait recognition, with
Resnet-18 exhibiting effectiveness in discerning distinct personality traits.
\\ ( https://arxiv.org/abs/2402.04326 ,  1182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04403 (*cross-listing*)
Date: Tue, 6 Feb 2024 21:04:57 GMT   (92kb,D)

Title: Edge-Parallel Graph Encoder Embedding
Authors: Ariel Lubonja (1), Cencheng Shen (2), Carey Priebe (1) and Randal
  Burns (1) ((1) Johns Hopkins University, (2) University of Delaware)
Categories: cs.DC cs.LG
Comments: 4 pages, 4 figures
\\
  New algorithms for embedding graphs have reduced the asymptotic complexity of
finding low-dimensional representations. One-Hot Graph Encoder Embedding (GEE)
uses a single, linear pass over edges and produces an embedding that converges
asymptotically to the spectral embedding. The scaling and performance benefits
of this approach have been limited by a serial implementation in an interpreted
language. We refactor GEE into a parallel program in the Ligra graph engine
that maps functions over the edges of the graph and uses lock-free atomic
instrutions to prevent data races. On a graph with 1.8B edges, this results in
a 500 times speedup over the original implementation and a 17 times speedup
over a just-in-time compiled version.
\\ ( https://arxiv.org/abs/2402.04403 ,  92kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04416 (*cross-listing*)
Date: Tue, 6 Feb 2024 21:29:37 GMT   (3041kb,D)

Title: A Data Centric Approach for Unsupervised Domain Generalization via
  Retrieval from Web Scale Multimodal Data
Authors: Christopher Liao, Theodoros Tsiligkaridis, Brian Kulis
Categories: cs.CV cs.LG
\\
  Domain generalization (DG) is an important problem that learns a model that
can generalize to unseen test domains leveraging one or more source domains,
under the assumption of shared label spaces. However, most DG methods assume
access to abundant source data in the target label space, a requirement that
proves overly stringent for numerous real-world applications, where acquiring
the same label space as the target task is prohibitively expensive. For this
setting, we tackle the multimodal version of the unsupervised domain
generalization (UDG) problem, which uses a large task-agnostic unlabeled source
dataset, such as LAION-2B during finetuning. Our framework does not explicitly
assume any relationship between the source dataset and target task. Instead, it
relies only on the premise that the source dataset can be efficiently searched
in a joint vision-language space. For this multimodal UDG setting, we propose a
novel method to build a small ($<$100K) subset of the source data in three
simple steps: (1) diversified retrieval using label names as queries, (2) rank
pseudo-labeling, and (3) clustering to find representative samples. To
demonstrate the value of studying the multimodal UDG problem, we compare our
results against state-of-the-art source-free DG and zero-shot (ZS) methods on
their respective benchmarks and show up to 10% improvement in accuracy on 20
diverse target datasets. Additionally, our multi-stage dataset construction
method achieves 3% improvement on average over nearest neighbors retrieval.
Code is available: https://github.com/Chris210634/mudg
\\ ( https://arxiv.org/abs/2402.04416 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04419 (*cross-listing*)
Date: Tue, 6 Feb 2024 21:38:29 GMT   (701kb)

Title: What limits performance of weakly supervised deep learning for chest CT
  classification?
Authors: Fakrul Islam Tushar, Vincent M. D'Anniballe, Geoffrey D. Rubin, Joseph
  Y. Lo
Categories: eess.IV cs.LG
Comments: 16 pages , 8 figures. arXiv admin note: text overlap with
  arXiv:2202.11709
\\
  Weakly supervised learning with noisy data has drawn attention in the medical
imaging community due to the sparsity of high-quality disease labels. However,
little is known about the limitations of such weakly supervised learning and
the effect of these constraints on disease classification performance. In this
paper, we test the effects of such weak supervision by examining model
tolerance for three conditions. First, we examined model tolerance for noisy
data by incrementally increasing error in the labels within the training data.
Second, we assessed the impact of dataset size by varying the amount of
training data. Third, we compared performance differences between binary and
multi-label classification. Results demonstrated that the model could endure up
to 10% added label error before experiencing a decline in disease
classification performance. Disease classification performance steadily rose as
the amount of training data was increased for all disease classes, before
experiencing a plateau in performance at 75% of training data. Last, the binary
model outperformed the multilabel model in every disease category. However,
such interpretations may be misleading, as the binary model was heavily
influenced by co-occurring diseases and may not have learned the specific
features of the disease in the image. In conclusion, this study may help the
medical imaging community understand the benefits and risks of weak supervision
with noisy labels. Such studies demonstrate the need to build diverse,
large-scale datasets and to develop explainable and responsible AI.
\\ ( https://arxiv.org/abs/2402.04419 ,  701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04436 (*cross-listing*)
Date: Tue, 6 Feb 2024 22:13:51 GMT   (12kb,D)

Title: Continuous Multidimensional Scaling
Authors: Michael W. Trosset, Carey E. Priebe
Categories: stat.ML cs.LG
Comments: 15 pages
MSC-class: 62H99
\\
  Multidimensional scaling (MDS) is the act of embedding proximity information
about a set of $n$ objects in $d$-dimensional Euclidean space. As originally
conceived by the psychometric community, MDS was concerned with embedding a
fixed set of proximities associated with a fixed set of objects. Modern
concerns, e.g., that arise in developing asymptotic theories for statistical
inference on random graphs, more typically involve studying the limiting
behavior of a sequence of proximities associated with an increasing set of
objects. Standard results from the theory of point-to-set maps imply that, if
$n$ is fixed, then the limit of the embedded structures is the embedded
structure of the limiting proximities. But what if $n$ increases? It then
becomes necessary to reformulate MDS so that the entire sequence of embedding
problems can be viewed as a sequence of optimization problems in a fixed space.
We present such a reformulation and derive some consequences.
\\ ( https://arxiv.org/abs/2402.04436 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04446 (*cross-listing*)
Date: Tue, 6 Feb 2024 22:32:05 GMT   (718kb,D)

Title: Pushing the limits of cell segmentation models for imaging mass
  cytometry
Authors: Kimberley M. Bird, Xujiong Ye, Alan M. Race, James M. Brown
Categories: eess.IV cs.CV cs.LG
Comments: International Symposium on Biomedical Imaging (ISBI) 2024 Submission
ACM-class: I.2; I.4; I.4.6
\\
  Imaging mass cytometry (IMC) is a relatively new technique for imaging
biological tissue at subcellular resolution. In recent years, learning-based
segmentation methods have enabled precise quantification of cell type and
morphology, but typically rely on large datasets with fully annotated ground
truth (GT) labels. This paper explores the effects of imperfect labels on
learning-based segmentation models and evaluates the generalisability of these
models to different tissue types. Our results show that removing 50% of cell
annotations from GT masks only reduces the dice similarity coefficient (DSC)
score to 0.874 (from 0.889 achieved by a model trained on fully annotated GT
masks). This implies that annotation time can in fact be reduced by at least
half without detrimentally affecting performance. Furthermore, training our
single-tissue model on imperfect labels only decreases DSC by 0.031 on an
unseen tissue type compared to its multi-tissue counterpart, with negligible
qualitative differences in segmentation. Additionally, bootstrapping the
worst-performing model (with 5% of cell annotations) a total of ten times
improves its original DSC score of 0.720 to 0.829. These findings imply that
less time and work can be put into the process of producing comparable
segmentation models; this includes eliminating the need for multiple IMC tissue
types during training, whilst also providing the potential for models with very
few labels to improve on themselves. Source code is available on GitHub:
https://github.com/kimberley/ISBI2024.
\\ ( https://arxiv.org/abs/2402.04446 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04453 (*cross-listing*)
Date: Tue, 6 Feb 2024 22:42:28 GMT   (1428kb,D)

Title: The Potential of AutoML for Recommender Systems
Authors: Tobias Vente, Joeran Beel
Categories: cs.IR cs.LG
\\
  Automated Machine Learning (AutoML) has greatly advanced applications of
Machine Learning (ML) including model compression, machine translation, and
computer vision. Recommender Systems (RecSys) can be seen as an application of
ML. Yet, AutoML has found little attention in the RecSys community; nor has
RecSys found notable attention in the AutoML community. Only few and relatively
simple Automated Recommender Systems (AutoRecSys) libraries exist that adopt
AutoML techniques. However, these libraries are based on student projects and
do not offer the features and thorough development of AutoML libraries. We set
out to determine how AutoML libraries perform in the scenario of an
inexperienced user who wants to implement a recommender system. We compared the
predictive performance of 60 AutoML, AutoRecSys, ML, and RecSys algorithms from
15 libraries, including a mean predictor baseline, on 14 explicit feedback
RecSys datasets. To simulate the perspective of an inexperienced user, the
algorithms were evaluated with default hyperparameters. We found that AutoML
and AutoRecSys libraries performed best. AutoML libraries performed best for
six of the 14 datasets (43%), but it was not always the same AutoML library
performing best. The single-best library was the AutoRecSys library
Auto-Surprise, which performed best on five datasets (36%). On three datasets
(21%), AutoML libraries performed poorly, and RecSys libraries with default
parameters performed best. Although, while obtaining 50% of all placements in
the top five per dataset, RecSys algorithms fall behind AutoML on average. ML
algorithms generally performed the worst.
\\ ( https://arxiv.org/abs/2402.04453 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04493 (*cross-listing*)
Date: Wed, 7 Feb 2024 00:33:11 GMT   (38kb)

Title: A Primal-Dual Algorithm for Offline Constrained Reinforcement Learning
  with Low-Rank MDPs
Authors: Kihyuk Hong, Ambuj Tewari
Categories: stat.ML cs.LG
\\
  Offline reinforcement learning (RL) aims to learn a policy that maximizes the
expected cumulative reward using a pre-collected dataset. Offline RL with
low-rank MDPs or general function approximation has been widely studied
recently, but existing algorithms with sample complexity $O(\epsilon^{-2})$ for
finding an $\epsilon$-optimal policy either require a uniform data coverage
assumptions or are computationally inefficient. In this paper, we propose a
primal dual algorithm for offline RL with low-rank MDPs in the discounted
infinite-horizon setting. Our algorithm is the first computationally efficient
algorithm in this setting that achieves sample complexity of $O(\epsilon^{-2})$
with partial data coverage assumption. This improves upon a recent work that
requires $O(\epsilon^{-4})$ samples. Moreover, our algorithm extends the
previous work to the offline constrained RL setting by supporting constraints
on additional reward signals.
\\ ( https://arxiv.org/abs/2402.04493 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04498 (*cross-listing*)
Date: Wed, 7 Feb 2024 00:54:35 GMT   (535kb,D)

Title: Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing
  Time-course Data
Authors: Chaitra Agrahar, William Poole, Simone Bianco, Hana El-Samad
Categories: stat.ML cs.LG q-bio.QM
Comments: 28 pages, 7 figures, Submitted for review
\\
  Kalman Filter (KF) is an optimal linear state prediction algorithm, with
applications in fields as diverse as engineering, economics, robotics, and
space exploration. Here, we develop an extension of the KF, called a Pathspace
Kalman Filter (PKF) which allows us to a) dynamically track the uncertainties
associated with the underlying data and prior knowledge, and b) take as input
an entire trajectory and an underlying mechanistic model, and using a Bayesian
methodology quantify the different sources of uncertainty. An application of
this algorithm is to automatically detect temporal windows where the internal
mechanistic model deviates from the data in a time-dependent manner. First, we
provide theorems characterizing the convergence of the PKF algorithm. Then, we
numerically demonstrate that the PKF outperforms conventional KF methods on a
synthetic dataset lowering the mean-squared-error by several orders of
magnitude. Finally, we apply this method to biological time-course dataset
involving over 1.8 million gene expression measurements.
\\ ( https://arxiv.org/abs/2402.04498 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04516 (*cross-listing*)
Date: Wed, 7 Feb 2024 01:49:03 GMT   (645kb,D)

Title: Generalized Sobolev Transport for Probability Measures on a Graph
Authors: Tam Le, Truyen Nguyen, Kenji Fukumizu
Categories: stat.ML cs.LG
\\
  We study the optimal transport (OT) problem for measures supported on a graph
metric space. Recently, Le et al. (2022) leverage the graph structure and
propose a variant of OT, namely Sobolev transport (ST), which yields a
closed-form expression for a fast computation. However, ST is essentially
coupled with the $L^p$ geometric structure within its definition which makes it
nontrivial to utilize ST for other prior structures. In contrast, the classic
OT has the flexibility to adapt to various geometric structures by modifying
the underlying cost function. An important instance is the Orlicz-Wasserstein
(OW) which moves beyond the $L^p$ structure by leveraging the \emph{Orlicz
geometric structure}. Comparing to the usage of standard $p$-order Wasserstein,
OW remarkably helps to advance certain machine learning approaches.
Nevertheless, OW brings up a new challenge on its computation due to its
two-level optimization formulation. In this work, we leverage a specific class
of convex functions for Orlicz structure to propose the generalized Sobolev
transport (GST). GST encompasses the ST as its special case, and can be
utilized for prior structures beyond the $L^p$ geometry. In connection with the
OW, we show that one only needs to simply solve a univariate optimization
problem to compute the GST, unlike the complex two-level optimization problem
in OW. We empirically illustrate that GST is several-order faster than the OW.
Moreover, we provide preliminary evidences on the advantages of GST for
document classification and for several tasks in topological data analysis.
\\ ( https://arxiv.org/abs/2402.04516 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04550 (*cross-listing*)
Date: Wed, 7 Feb 2024 03:13:11 GMT   (181kb,D)

Title: Riemann-Lebesgue Forest for Regression
Authors: Tian Qin, Wei-Min Huang
Categories: stat.ML cs.LG
\\
  We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for
regression. The core idea of RLF is to mimic the way how a measurable function
can be approximated by partitioning its range into a few intervals. With this
idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree which
has a chance to split the node from response $Y$ or a direction in feature
space $\mathbf{X}$ at each non-terminal node. We generalize the asymptotic
performance of RLF under different parameter settings mainly through Hoeffding
decomposition \cite{Vaart} and Stein's method \cite{Chen2010NormalAB}. When the
underlying function $Y=f(\mathbf{X})$ follows an additive regression model, RLF
is consistent with the argument from \cite{Scornet2014ConsistencyOR}. The
competitive performance of RLF against original random forest
\cite{Breiman2001RandomF} is demonstrated by experiments in simulation data and
real world datasets.
\\ ( https://arxiv.org/abs/2402.04550 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04557 (*cross-listing*)
Date: Wed, 7 Feb 2024 03:25:08 GMT   (1189kb)

Title: An Artificial Intelligence (AI) workflow for catalyst design and
  optimization
Authors: Nung Siong Lai, Yi Shen Tew, Xialin Zhong, Jun Yin, Jiali Li, Binhang
  Yan, Xiaonan Wang
Categories: physics.chem-ph cs.LG
Comments: 31 pages, 7 figures
Journal-ref: Ind. Eng. Chem. Res. 2023, 62, 43, 17835-17848
DOI: 10.1021/acs.iecr.3c02520
\\
  In the pursuit of novel catalyst development to address pressing
environmental concerns and energy demand, conventional design and optimization
methods often fall short due to the complexity and vastness of the catalyst
parameter space. The advent of Machine Learning (ML) has ushered in a new era
in the field of catalyst optimization, offering potential solutions to the
shortcomings of traditional techniques. However, existing methods fail to
effectively harness the wealth of information contained within the burgeoning
body of scientific literature on catalyst synthesis. To address this gap, this
study proposes an innovative Artificial Intelligence (AI) workflow that
integrates Large Language Models (LLMs), Bayesian optimization, and an active
learning loop to expedite and enhance catalyst optimization. Our methodology
combines advanced language understanding with robust optimization strategies,
effectively translating knowledge extracted from diverse literature into
actionable parameters for practical experimentation and optimization. In this
article, we demonstrate the application of this AI workflow in the optimization
of catalyst synthesis for ammonia production. The results underscore the
workflow's ability to streamline the catalyst development process, offering a
swift, resource-efficient, and high-precision alternative to conventional
methods.
\\ ( https://arxiv.org/abs/2402.04557 ,  1189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04613 (*cross-listing*)
Date: Wed, 7 Feb 2024 06:30:39 GMT   (10195kb,D)

Title: Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in
  Reproducing Kernel Hilbert Spaces
Authors: Sebastian Neumayer, Viktor Stein, Gabriele Steidl
Categories: stat.ML cs.LG math.FA math.OC
Comments: 42 pages, 13 figures
MSC-class: 46N10 (Primary) 46E22, 94A15 (Secondary)
\\
  Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler
divergence, are subject to limitations regarding the support of the involved
measures. A remedy consists of regularizing the $f$-divergence by a squared
maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In
this paper, we use the so-called kernel mean embedding to show that the
corresponding regularization can be rewritten as the Moreau envelope of some
function in the reproducing kernel Hilbert space associated with $K$. Then, we
exploit well-known results on Moreau envelopes in Hilbert spaces to prove
properties of the MMD-regularized $f$-divergences and, in particular, their
gradients. Subsequently, we use our findings to analyze Wasserstein gradient
flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein
gradient flows starting from empirical measures and provide
proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
\\ ( https://arxiv.org/abs/2402.04613 ,  10195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04620 (*cross-listing*)
Date: Wed, 7 Feb 2024 07:07:02 GMT   (9282kb,D)

Title: CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract
  Patients
Authors: Pragnya Ramjee, Bhuvan Sachdeva, Satvik Golechha, Shreyas Kulkarni,
  Geeta Fulari, Kaushik Murali, Mohit Jain
Categories: cs.HC cs.LG
\\
  The healthcare landscape is evolving, with patients seeking more reliable
information about their health conditions, treatment options, and potential
risks. Despite the abundance of information sources, the digital age overwhelms
individuals with excess, often inaccurate information. Patients primarily trust
doctors and hospital staff, highlighting the need for expert-endorsed health
information. However, the pressure on experts has led to reduced communication
time, impacting information sharing. To address this gap, we propose
CataractBot, an experts-in-the-loop chatbot powered by large language models
(LLMs). Developed in collaboration with a tertiary eye hospital in India,
CataractBot answers cataract surgery related questions instantly by querying a
curated knowledge base, and provides expert-verified responses asynchronously.
CataractBot features multimodal support and multilingual capabilities. In an
in-the-wild deployment study with 49 participants, CataractBot proved valuable,
providing anytime accessibility, saving time, and accommodating diverse
literacy levels. Trust was established through expert verification. Broadly,
our results could inform future work on designing expert-mediated LLM bots.
\\ ( https://arxiv.org/abs/2402.04620 ,  9282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04691 (*cross-listing*)
Date: Wed, 7 Feb 2024 09:31:01 GMT   (50kb)

Title: Learning Operators with Stochastic Gradient Descent in General Hilbert
  Spaces
Authors: Lei Shi and Jia-Qi Yang
Categories: stat.ML cs.LG math.FA math.ST stat.TH
Comments: 56 pages
\\
  This study investigates leveraging stochastic gradient descent (SGD) to learn
operators between general Hilbert spaces. We propose weak and strong regularity
conditions for the target operator to depict its intrinsic structure and
complexity. Under these conditions, we establish upper bounds for convergence
rates of the SGD algorithm and conduct a minimax lower bound analysis, further
illustrating that our convergence analysis and regularity conditions
quantitatively characterize the tractability of solving operator learning
problems using the SGD algorithm. It is crucial to highlight that our
convergence analysis is still valid for nonlinear operator learning. We show
that the SGD estimator will converge to the best linear approximation of the
nonlinear target operator. Moreover, applying our analysis to operator learning
problems based on vector-valued and real-valued reproducing kernel Hilbert
spaces yields new convergence results, thereby refining the conclusions of
existing literature.
\\ ( https://arxiv.org/abs/2402.04691 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04692 (*cross-listing*)
Date: Wed, 7 Feb 2024 09:32:32 GMT   (91kb,D)

Title: From explained variance of correlated components to PCA without
  orthogonality constraints
Authors: Marie Chavent (IMB), Guy Chavent
Categories: stat.ML cs.LG
\\
  Block Principal Component Analysis (Block PCA) of a data matrix A, where
loadings Z are determined by maximization of AZ 2 over unit norm orthogonal
loadings, is difficult to use for the design of sparse PCA by 1 regularization,
due to the difficulty of taking care of both the orthogonality constraint on
loadings and the non differentiable 1 penalty. Our objective in this paper is
to relax the orthogonality constraint on loadings by introducing new objective
functions expvar(Y) which measure the part of the variance of the data matrix A
explained by correlated components Y = AZ. So we propose first a comprehensive
study of mathematical and numerical properties of expvar(Y) for two existing
definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions.
Then we show that only two of these explained variance are fit to use as
objective function in block PCA formulations for A rid of orthogonality
constraints.
\\ ( https://arxiv.org/abs/2402.04692 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04713 (*cross-listing*)
Date: Wed, 7 Feb 2024 10:05:42 GMT   (2180kb,D)

Title: Theoretical and Empirical Analysis of Adaptive Entry Point Selection for
  Graph-based Approximate Nearest Neighbor Search
Authors: Yutaro Oguri and Yusuke Matsui
Categories: cs.IR cs.DB cs.LG
\\
  We present a theoretical and empirical analysis of the adaptive entry point
selection for graph-based approximate nearest neighbor search (ANNS). We
introduce novel concepts: $b\textit{-monotonic path}$ and $B\textit{-MSNET}$,
which better capture an actual graph in practical algorithms than existing
concepts like MSNET. We prove that adaptive entry point selection offers better
performance upper bound than the fixed central entry point under more general
conditions than previous work. Empirically, we validate the method's
effectiveness in accuracy, speed, and memory usage across various datasets,
especially in challenging scenarios with out-of-distribution data and hard
instances. Our comprehensive study provides deeper insights into optimizing
entry points for graph-based ANNS for real-world high-dimensional data
applications.
\\ ( https://arxiv.org/abs/2402.04713 ,  2180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04740 (*cross-listing*)
Date: Wed, 7 Feb 2024 10:51:11 GMT   (2122kb,D)

Title: Non-Parametric Estimation of Multi-dimensional Marked Hawkes Processes
Authors: Sobin Joseph and Shashi Jain
Categories: stat.ML cs.LG q-fin.CP q-fin.ST
\\
  An extension of the Hawkes process, the Marked Hawkes process distinguishes
itself by featuring variable jump size across each event, in contrast to the
constant jump size observed in a Hawkes process without marks. While extensive
literature has been dedicated to the non-parametric estimation of both the
linear and non-linear Hawkes process, there remains a significant gap in the
literature regarding the marked Hawkes process. In response to this, we propose
a methodology for estimating the conditional intensity of the marked Hawkes
process. We introduce two distinct models: \textit{Shallow Neural Hawkes with
marks}- for Hawkes processes with excitatory kernels and \textit{Neural Network
for Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these
approaches take the past arrival times and their corresponding marks as the
input to obtain the arrival intensity. This approach is entirely
non-parametric, preserving the interpretability associated with the marked
Hawkes process. To validate the efficacy of our method, we subject the method
to synthetic datasets with known ground truth. Additionally, we apply our
method to model cryptocurrency order book data, demonstrating its applicability
to real-world scenarios.
\\ ( https://arxiv.org/abs/2402.04740 ,  2122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04754 (*cross-listing*)
Date: Wed, 7 Feb 2024 11:12:41 GMT   (2994kb,D)

Title: Towards Aligned Layout Generation via Diffusion Model with Aesthetic
  Constraints
Authors: Jian Chen, Ruiyi Zhang, Yufan Zhou, Changyou Chen
Categories: cs.CV cs.LG
Comments: Accepted by ICLR 2024
\\
  Controllable layout generation refers to the process of creating a plausible
visual arrangement of elements within a graphic design (e.g., document and web
designs) with constraints representing design intentions. Although recent
diffusion-based models have achieved state-of-the-art FID scores, they tend to
exhibit more pronounced misalignment compared to earlier transformer-based
models. In this work, we propose the $\textbf{LA}$yout $\textbf{C}$onstraint
diffusion mod$\textbf{E}$l (LACE), a unified model to handle a broad range of
layout generation tasks, such as arranging elements with specified attributes
and refining or completing a coarse layout design. The model is based on
continuous diffusion models. Compared with existing methods that use discrete
diffusion models, continuous state-space design can enable the incorporation of
differentiable aesthetic constraint functions in training. For conditional
generation, we introduce conditions via masked input. Extensive experiment
results show that LACE produces high-quality layouts and outperforms existing
state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.04754 ,  2994kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04762 (*cross-listing*)
Date: Wed, 7 Feb 2024 11:26:00 GMT   (1060kb,D)

Title: Color Recognition in Challenging Lighting Environments: CNN Approach
Authors: Nizamuddin Maitlo, Nooruddin Noonari, Sajid Ahmed Ghanghro,
  Sathishkumar Duraisamy, Fayaz Ahmed
Categories: cs.CV cs.LG
\\
  Light plays a vital role in vision either human or machine vision, the
perceived color is always based on the lighting conditions of the surroundings.
Researchers are working to enhance the color detection techniques for the
application of computer vision. They have implemented proposed several methods
using different color detection approaches but still, there is a gap that can
be filled. To address this issue, a color detection method, which is based on a
Convolutional Neural Network (CNN), is proposed. Firstly, image segmentation is
performed using the edge detection segmentation technique to specify the object
and then the segmented object is fed to the Convolutional Neural Network
trained to detect the color of an object in different lighting conditions. It
is experimentally verified that our method can substantially enhance the
robustness of color detection in different lighting conditions, and our method
performed better results than existing methods.
\\ ( https://arxiv.org/abs/2402.04762 ,  1060kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04777 (*cross-listing*)
Date: Wed, 7 Feb 2024 11:56:34 GMT   (1374kb,D)

Title: A fast score-based search algorithm for maximal ancestral graphs using
  entropy
Authors: Zhongyi Hu and Robin Evans
Categories: stat.ML cs.LG math.ST stat.TH
\\
  \emph{Maximal ancestral graph} (MAGs) is a class of graphical model that
extend the famous \emph{directed acyclic graph} in the presence of latent
confounders. Most score-based approaches to learn the unknown MAG from
empirical data rely on BIC score which suffers from instability and heavy
computations. We propose to use the framework of imsets
\citep{studeny2006probabilistic} to score MAGs using empirical entropy
estimation and the newly proposed \emph{refined Markov property}
\citep{hu2023towards}. Our graphical search procedure is similar to
\citet{claassen2022greedy} but improved from our theoretical results. We show
that our search algorithm is polynomial in number of nodes by restricting
degree, maximal head size and number of discriminating paths. In simulated
experiment, our algorithm shows superior performance compared to other state of
art MAG learning algorithms.
\\ ( https://arxiv.org/abs/2402.04777 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04785 (*cross-listing*)
Date: Wed, 7 Feb 2024 12:15:56 GMT   (1103kb,D)

Title: Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time
  Complexity Under Arbitrary Computation and Communication Heterogeneity
Authors: Alexander Tyurin, Marta Pozzi, Ivan Ilin, Peter Richt\'arik
Categories: math.OC cs.LG
\\
  We consider nonconvex stochastic optimization problems in the asynchronous
centralized distributed setup where the communication times from workers to a
server can not be ignored, and the computation and communication times are
potentially different for all workers. Using an unbiassed compression
technique, we develop a new method-Shadowheart SGD-that provably improves the
time complexities of all previous centralized methods. Moreover, we show that
the time complexity of Shadowheart SGD is optimal in the family of centralized
methods with compressed communication. We also consider the bidirectional
setup, where broadcasting from the server to the workers is non-negligible, and
develop a corresponding method.
\\ ( https://arxiv.org/abs/2402.04785 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04825 (*cross-listing*)
Date: Wed, 7 Feb 2024 13:23:25 GMT   (547kb,D)

Title: Fast Timing-Conditioned Latent Audio Diffusion
Authors: Zach Evans, CJ Carr, Josiah Taylor, Scott H. Hawley, Jordi Pons
Categories: cs.SD cs.LG eess.AS
Comments: Code: https://github.com/Stability-AI/stable-audio-tools. Metrics:
  https://github.com/Stability-AI/stable-audio-metrics. Demo:
  https://stability-ai.github.io/stable-audio-demo
\\
  Generating long-form 44.1kHz stereo audio from text prompts can be
computationally demanding. Further, most previous works do not tackle that
music and sound effects naturally vary in their duration. Our research focuses
on the efficient generation of long-form, variable-length stereo music and
sounds at 44.1kHz using text prompts with a generative model. Stable Audio is
based on latent diffusion, with its latent defined by a fully-convolutional
variational autoencoder. It is conditioned on text prompts as well as timing
embeddings, allowing for fine control over both the content and length of the
generated music and sounds. Stable Audio is capable of rendering stereo signals
of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute
efficiency and fast inference, it is one of the best in two public
text-to-music and -audio benchmarks and, differently from state-of-the-art
models, can generate music with structure and stereo sounds.
\\ ( https://arxiv.org/abs/2402.04825 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04835 (*cross-listing*)
Date: Wed, 7 Feb 2024 13:32:47 GMT   (1105kb,D)

Title: SARI: Simplistic Average and Robust Identification based Noisy Partial
  Label Learning
Authors: Darshana Saravanan, Naresh Manwani, Vineet Gandhi
Categories: cs.CV cs.LG
Comments: 13 pages, 6 tables, 2 figures
\\
  Partial label learning (PLL) is a weakly-supervised learning paradigm where
each training instance is paired with a set of candidate labels (partial
label), one of which is the true label. Noisy PLL (NPLL) relaxes this
constraint by allowing some partial labels to not contain the true label,
enhancing the practicality of the problem. Our work centers on NPLL and
presents a minimalistic framework called SARI that initially assigns
pseudo-labels to images by exploiting the noisy partial labels through a
weighted nearest neighbour algorithm. These pseudo-label and image pairs are
then used to train a deep neural network classifier with label smoothing and
standard regularization techniques. The classifier's features and predictions
are subsequently employed to refine and enhance the accuracy of pseudo-labels.
SARI combines the strengths of Average Based Strategies (in pseudo labelling)
and Identification Based Strategies (in classifier training) from the
literature. We perform thorough experiments on seven datasets and compare SARI
against nine NPLL and PLL methods from the prior art. SARI achieves
state-of-the-art results in almost all studied settings, obtaining substantial
gains in fine-grained classification and extreme noise settings.
\\ ( https://arxiv.org/abs/2402.04835 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04845 (*cross-listing*)
Date: Wed, 7 Feb 2024 13:44:47 GMT   (8301kb,D)

Title: AlphaFold Meets Flow Matching for Generating Protein Ensembles
Authors: Bowen Jing, Bonnie Berger, Tommi Jaakkola
Categories: q-bio.BM cs.LG
\\
  The biological functions of proteins often depend on dynamic structural
ensembles. In this work, we develop a flow-based generative modeling approach
for learning and sampling the conformational landscapes of proteins. We
repurpose highly accurate single-state predictors such as AlphaFold and ESMFold
and fine-tune them under a custom flow matching framework to obtain
sequence-conditoned generative models of protein structure called AlphaFlow and
ESMFlow. When trained and evaluated on the PDB, our method provides a superior
combination of precision and diversity compared to AlphaFold with MSA
subsampling. When further trained on ensembles from all-atom MD, our method
accurately captures conformational flexibility, positional distributions, and
higher-order ensemble observables for unseen proteins. Moreover, our method can
diversify a static PDB structure with faster wall-clock convergence to certain
equilibrium properties than replicate MD trajectories, demonstrating its
potential as a proxy for expensive physics-based simulations. Code is available
at https://github.com/bjing2016/alphaflow.
\\ ( https://arxiv.org/abs/2402.04845 ,  8301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04866 (*cross-listing*)
Date: Thu, 1 Feb 2024 21:16:40 GMT   (1844kb,D)

Title: Room transfer function reconstruction using complex-valued neural
  networks and irregularly distributed microphones
Authors: Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci,
  Augusto Sarti
Categories: eess.AS cs.LG cs.SD eess.SP
\\
  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several important real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of room transfer functions measured at scattered points in the room. In
this study, we employ complex-valued neural networks to estimate room transfer
functions in the frequency range of the first room resonances, using a few
irregularly distributed microphones. To the best of our knowledge, this is the
first time complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex-valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
real-valued neural network method and a state-of-the-art kernel-based signal
processing approach for sound field reconstruction, showing that the proposed
technique exhibits relevant advantages in terms of phase accuracy and overall
quality of the reconstructed sound field.
\\ ( https://arxiv.org/abs/2402.04866 ,  1844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04894 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:24:41 GMT   (17723kb,D)

Title: Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative
  Path Planning
Authors: Apoorva Vashisth, Julius R\"uckin, Federico Magistri, Cyrill
  Stachniss, Marija Popovi\'c
Categories: cs.RO cs.LG
Comments: 8 pages, 6 figures
\\
  Autonomous robots are often employed for data collection due to their
efficiency and low labour costs. A key task in robotic data acquisition is
planning paths through an initially unknown environment to collect observations
given platform-specific resource constraints, such as limited battery life.
Adaptive online path planning in 3D environments is challenging due to the
large set of valid actions and the presence of unknown occlusions. To address
these issues, we propose a novel deep reinforcement learning approach for
adaptively replanning robot paths to map targets of interest in unknown 3D
environments. A key aspect of our approach is a dynamically constructed graph
that restricts planning actions local to the robot, allowing us to quickly
react to newly discovered obstacles and targets of interest. For replanning, we
propose a new reward function that balances between exploring the unknown
environment and exploiting online-collected data about the targets of interest.
Our experiments show that our method enables more efficient target detection
compared to state-of-the-art learning and non-learning baselines. We also show
the applicability of our approach for orchard monitoring using an unmanned
aerial vehicle in a photorealistic simulator.
\\ ( https://arxiv.org/abs/2402.04894 ,  17723kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04896 (*cross-listing*)
Date: Tue, 23 Jan 2024 12:21:57 GMT   (483kb,D)

Title: Learning from the Best: Active Learning for Wireless Communications
Authors: Nasim Soltani, Jifan Zhang, Batool Salehi, Debashri Roy, Robert Nowak,
  Kaushik Chowdhury
Categories: cs.NI cs.LG
\\
  Collecting an over-the-air wireless communications training dataset for deep
learning-based communication tasks is relatively simple. However, labeling the
dataset requires expert involvement and domain knowledge, may involve private
intellectual properties, and is often computationally and financially
expensive. Active learning is an emerging area of research in machine learning
that aims to reduce the labeling overhead without accuracy degradation. Active
learning algorithms identify the most critical and informative samples in an
unlabeled dataset and label only those samples, instead of the complete set. In
this paper, we introduce active learning for deep learning applications in
wireless communications, and present its different categories. We present a
case study of deep learning-based mmWave beam selection, where labeling is
performed by a compute-intensive algorithm based on exhaustive search. We
evaluate the performance of different active learning algorithms on a publicly
available multi-modal dataset with different modalities including image and
LiDAR. Our results show that using an active learning algorithm for
class-imbalanced datasets can reduce labeling overhead by up to 50% for this
dataset while maintaining the same accuracy as classical training.
\\ ( https://arxiv.org/abs/2402.04896 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04907 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:37:37 GMT   (68kb)

Title: On a Combinatorial Problem Arising in Machine Teaching
Authors: Brigt H{\aa}vardstun, Jan Kratochv\'il, Joakim Sunde, Jan Arne
Categories: math.CO cs.LG
Comments: 14 pages, 1 figure
MSC-class: 05
ACM-class: G.2.1
\\
  We study a model of machine teaching where the teacher mapping is constructed
from a size function on both concepts and examples. The main question in
machine teaching is the minimum number of examples needed for any concept, the
so-called teaching dimension. A recent paper [7] conjectured that the worst
case for this model, as a function of the size of the concept class, occurs
when the consistency matrix contains the binary representations of numbers from
zero and up. In this paper we prove their conjecture. The result can be seen as
a generalization of a theorem resolving the edge isoperimetry problem for
hypercubes [12], and our proof is based on a lemma of [10].
\\ ( https://arxiv.org/abs/2402.04907 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04912 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:39:11 GMT   (18817kb,D)

Title: Towards Biologically Plausible and Private Gene Expression Data
  Generation
Authors: Dingfan Chen, Marie Oestreich, Tejumade Afonja, Raouf Kerkouche,
  Matthias Becker, Mario Fritz
Categories: cs.CR cs.LG
Journal-ref: Proceedings on Privacy Enhancing Technologies (PoPETs 2024)
\\
  Generative models trained with Differential Privacy (DP) are becoming
increasingly prominent in the creation of synthetic data for downstream
applications. Existing literature, however, primarily focuses on basic
benchmarking datasets and tends to report promising results only for elementary
metrics and relatively simple data distributions. In this paper, we initiate a
systematic analysis of how DP generative models perform in their natural
application scenarios, specifically focusing on real-world gene expression
data. We conduct a comprehensive analysis of five representative DP generation
methods, examining them from various angles, such as downstream utility,
statistical properties, and biological plausibility. Our extensive evaluation
illuminates the unique characteristics of each DP generation method, offering
critical insights into the strengths and weaknesses of each approach, and
uncovering intriguing possibilities for future developments. Perhaps
surprisingly, our analysis reveals that most methods are capable of achieving
seemingly reasonable downstream utility, according to the standard evaluation
metrics considered in existing literature. Nevertheless, we find that none of
the DP methods are able to accurately capture the biological characteristics of
the real dataset. This observation suggests a potential over-optimistic
assessment of current methodologies in this field and underscores a pressing
need for future enhancements in model design.
\\ ( https://arxiv.org/abs/2402.04912 ,  18817kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04922 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:47:13 GMT   (25314kb,D)

Title: Voronoi Candidates for Bayesian Optimization
Authors: Nathan Wycoff, John W. Smith, Annie S. Booth, Robert B. Gramacy
Categories: stat.ML cs.LG
Comments: comments very welcome
\\
  Bayesian optimization (BO) offers an elegant approach for efficiently
optimizing black-box functions. However, acquisition criteria demand their own
challenging inner-optimization, which can induce significant overhead. Many
practical BO methods, particularly in high dimension, eschew a formal,
continuous optimization of the acquisition function and instead search
discretely over a finite set of space-filling candidates. Here, we propose to
use candidates which lie on the boundary of the Voronoi tessellation of the
current design points, so they are equidistant to two or more of them. We
discuss strategies for efficient implementation by directly sampling the
Voronoi boundary without explicitly generating the tessellation, thus
accommodating large designs in high dimension. On a battery of test problems
optimized via Gaussian processes with expected improvement, our proposed
approach significantly improves the execution time of a multi-start continuous
search without a loss in accuracy.
\\ ( https://arxiv.org/abs/2402.04922 ,  25314kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04925 (*cross-listing*)
Date: Mon, 15 Jan 2024 08:01:40 GMT   (1964kb,D)

Title: TP-Aware Dequantization
Authors: Adnan Hoque, Mudhakar Srivatsa, Chih-Chieh Yang, Raghu Ganti
Categories: cs.DC cs.LG
\\
  In this paper, we present a novel method that reduces model inference latency
during distributed deployment of Large Language Models (LLMs). Our contribution
is an optimized inference deployment scheme that address the current
limitations of state-of-the-art quantization kernels when used in conjunction
with Tensor Parallel (TP). Our method preserves data locality in GPU memory
access patterns and exploits a priori knowledge of TP to reduce global
communication. We demonstrate an up to 1.81x speedup over existing methods for
Llama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer
problem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.
\\ ( https://arxiv.org/abs/2402.04925 ,  1964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04930 (*cross-listing*)
Date: Wed, 7 Feb 2024 14:59:25 GMT   (25954kb,D)

Title: Blue noise for diffusion models
Authors: Xingchang Huang, Corentin Sala\"un, Cristina Vasconcelos, Christian
  Theobalt, Cengiz \"Oztireli, Gurprit Singh
Categories: cs.CV cs.GR cs.LG
Comments: 10 pages, 12 figures
\\
  Most of the existing diffusion models use Gaussian noise for training and
sampling across all time steps, which may not optimally account for the
frequency contents reconstructed by the denoising network. Despite the diverse
applications of correlated noise in computer graphics, its potential for
improving the training process has been underexplored. In this paper, we
introduce a novel and general class of diffusion models taking correlated noise
within and across images into account. More specifically, we propose a
time-varying noise model to incorporate correlated noise into the training
process, as well as a method for fast generation of correlated noise mask. Our
model is built upon deterministic diffusion models and utilizes blue noise to
help improve the generation quality compared to using Gaussian white (random)
noise only. Further, our framework allows introducing correlation across images
within a single mini-batch to improve gradient flow. We perform both
qualitative and quantitative evaluations on a variety of datasets using our
method, achieving improvements on different tasks over existing deterministic
diffusion models in terms of FID metric.
\\ ( https://arxiv.org/abs/2402.04930 ,  25954kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04980 (*cross-listing*)
Date: Wed, 7 Feb 2024 15:57:30 GMT   (374kb,D)

Title: Asymptotics of feature learning in two-layer networks after one
  gradient-step
Authors: Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue M. Lu, Lenka
  Zdeborov\'a, Bruno Loureiro
Categories: stat.ML cond-mat.dis-nn cs.LG
\\
  In this manuscript we investigate the problem of how two-layer neural
networks learn features from data, and improve over the kernel regime, after
being trained with a single gradient descent step. Leveraging a connection from
(Ba et al., 2022) with a non-linear spiked matrix model and recent progress on
Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic
description of the generalization error in the high-dimensional limit where the
number of samples $n$, the width $p$ and the input dimension $d$ grow at a
proportional rate. We characterize exactly how adapting to the data is crucial
for the network to efficiently learn non-linear functions in the direction of
the gradient -- where at initialization it can only express linear functions in
this regime. To our knowledge, our results provides the first tight description
of the impact of feature learning in the generalization of two-layer neural
networks in the large learning rate regime $\eta=\Theta_{d}(d)$, beyond
perturbative finite width corrections of the conjugate and neural tangent
kernels.
\\ ( https://arxiv.org/abs/2402.04980 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04997 (*cross-listing*)
Date: Wed, 7 Feb 2024 16:15:36 GMT   (3518kb,D)

Title: Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows
  with Applications to Protein Co-Design
Authors: Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, Tommi
  Jaakkola
Categories: stat.ML cs.LG q-bio.QM
Comments: 52 pages, 11 figures, 5 tables
\\
  Combining discrete and continuous data is an important capability for
generative models. We present Discrete Flow Models (DFMs), a new flow-based
model of discrete data that provides the missing link in enabling flow-based
generative models to be applied to multimodal continuous and discrete data
problems. Our key insight is that the discrete equivalent of continuous space
flow matching can be realized using Continuous Time Markov Chains. DFMs benefit
from a simple derivation that includes discrete diffusion models as a specific
instance while allowing improved performance over existing diffusion-based
approaches. We utilize our DFMs method to build a multimodal flow-based
modeling framework. We apply this capability to the task of protein co-design,
wherein we learn a model for jointly generating protein structure and sequence.
Our approach achieves state-of-the-art co-design performance while allowing the
same multimodal model to be used for flexible generation of the sequence or
structure.
\\ ( https://arxiv.org/abs/2402.04997 ,  3518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05067 (*cross-listing*)
Date: Wed, 7 Feb 2024 18:19:51 GMT   (4803kb,D)

Title: Multiscale Modelling with Physics-informed Neural Network: from
  Large-scale Dynamics to Small-scale Predictions in Complex Systems
Authors: Jing Wang and Zheng Li and Pengyu Lai and Rui Wang and Di Yang and Hui
  Xu
Categories: physics.flu-dyn cs.LG physics.comp-ph
\\
  Multiscale phenomena manifest across various scientific domains, presenting a
ubiquitous challenge in accurately and effectively predicting multiscale
dynamics in complex systems. In this paper, a novel solving mode is proposed
for characterizing multiscale dynamics through a decoupling method. By
modelling large-scale dynamics independently and treating small-scale dynamics
as a slaved system, a Spectral PINN is developed to approach the small-scale
system in an orthogonal basis functional space. The effectiveness of the method
is demonstrated through extensive numerical experiments, including
one-dimensional Kuramot-Sivashinsky (KS) equation, two- and three-dimensional
Navier-Stokes (NS) equations, showcasing its versatility in addressing problems
of fluid dynamics. Furthermore, we also delve into the application of the
proposed approach to more complex problems, including non-uniform meshes,
complex geometries, large-scale data with noise, and high-dimensional
small-scale dynamics. The discussions about these scenarios contribute to a
comprehensive understanding of the method's capabilities and limitations. This
novel decoupling approach simplifies the analysis and prediction of
spatiotemporal systems, where large-scale data can be obtained with low
computational demands, followed by Spectral PINNs for capturing small-scale
dynamics with improved efficiency and accuracy.
\\ ( https://arxiv.org/abs/2402.05067 ,  4803kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05071 (*cross-listing*)
Date: Wed, 7 Feb 2024 18:22:41 GMT   (46kb,D)

Title: Extending the Reach of First-Order Algorithms for Nonconvex Min-Max
  Problems with Cohypomonotonicity
Authors: Ahmet Alacaoglu, Donghwan Kim, Stephen J. Wright
Categories: math.OC cs.LG stat.ML
\\
  We focus on constrained, $L$-smooth, nonconvex-nonconcave min-max problems
either satisfying $\rho$-cohypomonotonicity or admitting a solution to the
$\rho$-weakly Minty Variational Inequality (MVI), where larger values of the
parameter $\rho>0$ correspond to a greater degree of nonconvexity. These
problem classes include examples in two player reinforcement learning,
interaction dominant min-max problems, and certain synthetic test problems on
which classical min-max algorithms fail. It has been conjectured that
first-order methods can tolerate value of $\rho$ no larger than $\frac{1}{L}$,
but existing results in the literature have stagnated at the tighter
requirement $\rho < \frac{1}{2L}$. With a simple argument, we obtain optimal or
best-known complexity guarantees with cohypomonotonicity or weak MVI conditions
for $\rho < \frac{1}{L}$. The algorithms we analyze are inexact variants of
Halpern and Krasnosel'ski\u{\i}-Mann (KM) iterations. We also provide
algorithms and complexity guarantees in the stochastic case with the same range
on $\rho$. Our main insight for the improvements in the convergence analyses is
to harness the recently proposed "conic nonexpansiveness" property of
operators. As byproducts, we provide a refined analysis for inexact Halpern
iteration and propose a stochastic KM iteration with a multilevel Monte Carlo
estimator.
\\ ( https://arxiv.org/abs/2402.05071 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05101 (*cross-listing*)
Date: Wed, 7 Feb 2024 18:55:22 GMT   (51kb)

Title: Tighter Generalisation Bounds via Interpolation
Authors: Paul Viallard, Maxime Haddouche, Umut \c{S}im\c{s}ekli, Benjamin Guedj
Categories: stat.ML cs.LG
\\
  This paper contains a recipe for deriving new PAC-Bayes generalisation bounds
based on the $(f, \Gamma)$-divergence, and, in addition, presents PAC-Bayes
generalisation bounds where we interpolate between a series of probability
divergences (including but not limited to KL, Wasserstein, and total
variation), making the best out of many worlds depending on the posterior
distributions properties. We explore the tightness of these bounds and connect
them to earlier results from statistical learning, which are specific cases. We
also instantiate our bounds as training objectives, yielding non-trivial
guarantees and practical performances.
\\ ( https://arxiv.org/abs/2402.05101 ,  51kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2210.01426
replaced with revised version Wed, 7 Feb 2024 15:56:30 GMT   (2879kb,D)

Title: Continuous Monte Carlo Graph Search
Authors: Kalle Kujanp\"a\"a, Amin Babadi, Yi Zhao, Juho Kannala, Alexander
  Ilin, Joni Pajarinen
Categories: cs.AI cs.LG cs.RO
Comments: Accepted at AAMAS 2024 (full paper & oral)
\\ ( https://arxiv.org/abs/2210.01426 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01285
replaced with revised version Wed, 7 Feb 2024 10:15:53 GMT   (3482kb,D)

Title: Flows: Building Blocks of Reasoning and Collaborating AI
Authors: Martin Josifoski, Lars Klein, Maxime Peyrard, Nicolas Baldwin, Yifei
  Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul,
  Robert West
Categories: cs.AI cs.HC
\\ ( https://arxiv.org/abs/2308.01285 ,  3482kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09992
replaced with revised version Wed, 7 Feb 2024 16:40:22 GMT   (840kb)

Title: OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?
Authors: Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme
Categories: cs.AI cs.CL
Comments: 5 pages
ACM-class: I.2.7; I.2.0
Journal-ref: 180 TAX NOTES FEDERAL 1101 (AUG. 14, 2023)
\\ ( https://arxiv.org/abs/2309.09992 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13007
replaced with revised version Wed, 7 Feb 2024 18:07:13 GMT   (1107kb,D)

Title: A Critical Survey on Fairness Benefits of XAI
Authors: Luca Deck, Jakob Schoeffer, Maria De-Arteaga, Niklas K\"uhl
Categories: cs.AI
\\ ( https://arxiv.org/abs/2310.13007 ,  1107kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14455
replaced with revised version Wed, 7 Feb 2024 13:36:21 GMT   (3249kb,D)

Title: Universal Jailbreak Backdoors from Poisoned Human Feedback
Authors: Javier Rando and Florian Tram\`er
Categories: cs.AI cs.CL cs.CR cs.LG
Comments: Accepted as conference paper in ICLR 2024
\\ ( https://arxiv.org/abs/2311.14455 ,  3249kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12568
replaced with revised version Wed, 7 Feb 2024 10:00:20 GMT   (24845kb,D)

Title: Scaling Opponent Shaping to High Dimensional Games
Authors: Akbir Khan and Timon Willi and Newton Kwan and Andrea Tacchetti and
  Chris Lu and Edward Grefenstette and Tim Rockt\"aschel and Jakob Foerster
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.12568 ,  24845kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15692
replaced with revised version Wed, 7 Feb 2024 08:14:57 GMT   (8571kb,D)

Title: Instruction Fusion: Advancing Prompt Evolution through Hybridization
Authors: Weidong Guo, Jiuding Yang, Kaitong Yang, Xiangyang Li, Zhuwei Rao, Yu
  Xu, Di Niu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.15692 ,  8571kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03962
replaced with revised version Wed, 7 Feb 2024 08:33:23 GMT   (8410kb,D)

Title: Position Paper: Against Spurious Sparks $-$ Dovelating Inflated AI
  Claims
Authors: Patrick Altmeyer, Andrew M. Demetriou, Antony Bartlett, Cynthia C. S.
  Liem
Categories: cs.AI cs.CL
Comments: 20 pages, 15 figures. Preliminary work. Under review by the
  International Conference on Machine Learning (ICML)
\\ ( https://arxiv.org/abs/2402.03962 ,  8410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04140
replaced with revised version Wed, 7 Feb 2024 14:48:27 GMT   (890kb)

Title: Advancing Legal Reasoning: The Integration of AI to Navigate
  Complexities and Biases in Global Jurisprudence with Semi-Automated
  Arbitration Processes (SAAPs)
Authors: Michael De'Shazer
Categories: cs.AI cs.CY cs.HC
\\ ( https://arxiv.org/abs/2402.04140 ,  890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04232
replaced with revised version Wed, 7 Feb 2024 17:27:09 GMT   (10104kb,D)

Title: Can Generative Agents Predict Emotion?
Authors: Ciaran Regan, Nanami Iwahashi, Shogo Tanaka, Mizuki Oka
Categories: cs.AI cs.CL
Comments: 14 pages, 6 figures
\\ ( https://arxiv.org/abs/2402.04232 ,  10104kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03693
replaced with revised version Wed, 7 Feb 2024 05:43:31 GMT   (25519kb,D)

Title: Concept Algebra for (Score-Based) Text-Controlled Generative Models
Authors: Zihao Wang, Lin Gui, Jeffrey Negrea, Victor Veitch
Categories: cs.CL cs.LG stat.ML
\\ ( https://arxiv.org/abs/2302.03693 ,  25519kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08809
replaced with revised version Tue, 6 Feb 2024 22:30:07 GMT   (3331kb,D)

Title: Interpretability at Scale: Identifying Causal Mechanisms in Alpaca
Authors: Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, Noah D.
  Goodman
Categories: cs.CL
Comments: NeurIPS 2023 with Author Corrections
\\ ( https://arxiv.org/abs/2305.08809 ,  3331kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14459
replaced with revised version Wed, 7 Feb 2024 06:28:44 GMT   (8635kb,D)

Title: Advancing Precise Outline-Conditioned Text Generation with Task Duality
  and Explicit Outline Control
Authors: Yunzhe Li, Qian Chen, Weixiang Yan, Wen Wang, Qinglin Zhang, Hari
  Sundaram
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024
\\ ( https://arxiv.org/abs/2305.14459 ,  8635kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06985
replaced with revised version Wed, 7 Feb 2024 05:42:12 GMT   (3451kb)

Title: Engineering Design Knowledge Graphs from Patented Artefact Descriptions
  for Retrieval-Augmented Generation in the Design Process
Authors: L Siddharth, Jianxi Luo
Categories: cs.CL cs.DB cs.IR
\\ ( https://arxiv.org/abs/2307.06985 ,  3451kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13549
replaced with revised version Wed, 7 Feb 2024 12:01:49 GMT   (9093kb,D)

Title: The Perils & Promises of Fact-checking with Large Language Models
Authors: Dorian Quelle, Alexandre Bovet
Categories: cs.CL cs.CY cs.HC
Journal-ref: Frontiers in Artificial Intelligence, Volume 7, 2024
DOI: 10.3389/frai.2024.1341697
\\ ( https://arxiv.org/abs/2310.13549 ,  9093kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16421
replaced with revised version Wed, 7 Feb 2024 02:38:02 GMT   (17208kb,D)

Title: CDEval: A Benchmark for Measuring the Cultural Dimensions of Large
  Language Models
Authors: Yuhang Wang, Yanxu Zhu, Chao Kong, Shuyu Wei, Xiaoyuan Yi, Xing Xie
  and Jitao Sang
Categories: cs.CL cs.CY
Comments: Work in process
\\ ( https://arxiv.org/abs/2311.16421 ,  17208kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04828
replaced with revised version Wed, 7 Feb 2024 11:01:25 GMT   (9583kb,D)

Title: Human-Readable Fingerprint for Large Language Models
Authors: Boyi Zeng, Chenghu Zhou, Xinbing Wang, Zhouhan Lin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.04828 ,  9583kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08906
replaced with revised version Wed, 7 Feb 2024 10:12:03 GMT   (232kb,D)

Title: Using eye tracking to investigate what native Chinese speakers notice
  about linguistic landscape images
Authors: Zichao Wei, Yewei Qin
Categories: cs.CL q-bio.QM
ACM-class: J.4
\\ ( https://arxiv.org/abs/2312.08906 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02122
replaced with revised version Wed, 7 Feb 2024 07:36:34 GMT   (262kb,D)

Title: PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and
  Ensemble Techniques
Authors: Tzu-Han Lin, How-Shing Wang, Hao-Yung Weng, Kuang-Chen Peng, Zih-Ching
  Chen, Hung-yi Lee
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond
  (SASB) workshop
\\ ( https://arxiv.org/abs/2401.02122 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04319
replaced with revised version Wed, 7 Feb 2024 15:01:21 GMT   (3546kb,D)

Title: Know Your Needs Better: Towards Structured Understanding of Marketer
  Demands with Analogical Reasoning Augmented LLMs
Authors: Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Ziqi Liu, Wen Zhang,
  Jinjie Gu, Zhiqiang Zhang
Categories: cs.CL cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2401.04319 ,  3546kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05861
replaced with revised version Wed, 7 Feb 2024 08:37:15 GMT   (18932kb,D)

Title: Towards Boosting Many-to-Many Multilingual Machine Translation with
  Large Language Models
Authors: Pengzhi Gao, Zhongjun He, Hua Wu, Haifeng Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.05861 ,  18932kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00838
replaced with revised version Wed, 7 Feb 2024 18:53:02 GMT   (323kb,D)

Title: OLMo: Accelerating the Science of Language Models
Authors: Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney
  Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson,
  Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi
  Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel,
  Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha
  Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha
  Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant
  Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle
  Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A.
  Smith, Hannaneh Hajishirzi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00838 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01035
replaced with revised version Wed, 7 Feb 2024 10:51:11 GMT   (1376kb,D)

Title: Getting the most out of your tokenizer for pre-training and domain
  adaptation
Authors: Gautier Dagan, Gabriel Synnaeve, Baptiste Rozi\`ere
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.01035 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01364
replaced with revised version Wed, 7 Feb 2024 07:14:39 GMT   (1653kb,D)

Title: Continual Learning for Large Language Models: A Survey
Authors: Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu,
  Gholamreza Haffari
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.01364 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01697
replaced with revised version Wed, 7 Feb 2024 16:17:02 GMT   (288kb,D)

Title: APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data
  Annotation
Authors: Yiming Zhu, Zhizhuo Yin, Ehsan-Ul Haq, Lik-Hang Lee, Gareth Tyson, Pan
  Hui
Categories: cs.CL
Comments: Just accepted by WWW 2024
\\ ( https://arxiv.org/abs/2402.01697 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01726
replaced with revised version Wed, 7 Feb 2024 17:04:31 GMT   (4308kb,D)

Title: AI Does Not Alter Perceptions of Text Messages
Authors: N'yoma Diamond
Categories: cs.CL cs.AI cs.HC
\\ ( https://arxiv.org/abs/2402.01726 ,  4308kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01729
replaced with revised version Wed, 7 Feb 2024 00:31:40 GMT   (8052kb,D)

Title: Contextualization Distillation from Large Language Model for Knowledge
  Graph Completion
Authors: Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024 findings v2: revise the citation problem
\\ ( https://arxiv.org/abs/2402.01729 ,  8052kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01734
replaced with revised version Wed, 7 Feb 2024 03:51:42 GMT   (1282kb,D)

Title: CFTM: Continuous time fractional topic model
Authors: Kei Nakagawa, Kohei Hayashi, Yugo Fujimoto
Categories: cs.CL cs.LG q-fin.CP stat.AP
\\ ( https://arxiv.org/abs/2402.01734 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03780
replaced with revised version Wed, 7 Feb 2024 09:23:42 GMT   (2305kb,D)

Title: Exposing propaganda: an analysis of stylistic cues comparing human
  annotations and machine classification
Authors: G\'eraud Faye, Benjamin Icard, Morgane Casanova, Julien Chanson,
  Fran\c{c}ois Maine, Fran\c{c}ois Bancilhon, Guillaume Gadek, Guillaume
  Gravier, Paul \'Egr\'e
Categories: cs.CL cs.AI cs.LG
Comments: Paper to appear in the EACL 2024 Proceedings of the Third Workshop on
  Understanding Implicit and Underspecified Language (UnImplicit 2024)
\\ ( https://arxiv.org/abs/2402.03780 ,  2305kb)
------------------------------------------------------------------------------
\\
arXiv:2002.00178
replaced with revised version Wed, 7 Feb 2024 13:17:55 GMT   (439kb,D)

Title: An Equivalence between Bayesian Priors and Penalties in Variational
  Inference
Authors: Pierre Wolinski, Guillaume Charpiat, Yann Ollivier
Categories: cs.LG math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2002.00178 ,  439kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14839
replaced with revised version Wed, 7 Feb 2024 09:47:25 GMT   (20kb)

Title: Adversarial Bandits against Arbitrary Strategies
Authors: Jung-hun Kim, Se-Young Yun
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2205.14839 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2301.03962
replaced with revised version Wed, 7 Feb 2024 10:11:39 GMT   (2564kb,D)

Title: A Unified Theory of Diversity in Ensemble Learning
Authors: Danny Wood and Tingting Mu and Andrew Webb and Henry Reeve and Mikel
  Luj\'an and Gavin Brown
Categories: cs.LG cs.AI stat.ML
Journal-ref: Journal of Machine Learning Research, 24(359), 2023
\\ ( https://arxiv.org/abs/2301.03962 ,  2564kb)
------------------------------------------------------------------------------
\\
arXiv:2301.08918
replaced with revised version Tue, 6 Feb 2024 23:12:44 GMT   (777kb,D)

Title: Revisiting Signed Propagation for Multi-Class Graph Neural Networks
Authors: Yoonhyuk Choi, Jiho Choi, Taewook Ko, Chong-Kwon Kim
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2301.08918 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17235
replaced with revised version Wed, 7 Feb 2024 15:45:43 GMT   (776kb,D)

Title: Kaizen: Practical Self-supervised Continual Learning with Continual
  Fine-tuning
Authors: Chi Ian Tang, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Cecilia
  Mascolo, Akhil Mathur
Categories: cs.LG
Comments: Presented at IEEE/CVF Winter Conference on Applications of Computer
  Vision (WACV) 2024. The code for this work is available at
  https://github.com/dr-bell/kaizen
Journal-ref: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2024, pp. 2841-2850
\\ ( https://arxiv.org/abs/2303.17235 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01300
replaced with revised version Wed, 7 Feb 2024 12:20:13 GMT   (1317kb)

Title: On Mitigating the Utility-Loss in Differentially Private Learning: A new
  Perspective by a Geometrically Inspired Kernel Approach
Authors: Mohit Kumar, Bernhard A. Moser, Lukas Fischer
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2304.01300 ,  1317kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15371
replaced with revised version Tue, 6 Feb 2024 19:27:52 GMT   (710kb,D)

Title: Stochastic Unrolled Federated Learning
Authors: Samar Hadou, Navid NaderiAlizadeh, and Alejandro Ribeiro
Categories: cs.LG eess.SP
\\ ( https://arxiv.org/abs/2305.15371 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15611
replaced with revised version Wed, 7 Feb 2024 03:27:12 GMT   (23031kb,D)

Title: Size Generalization of Graph Neural Networks on Biological Data:
  Insights and Practices from the Spectral Perspective
Authors: Gaotang Li, Danai Koutra, Yujun Yan
Categories: cs.LG cs.AI
Comments: 21 pages, including appendix
\\ ( https://arxiv.org/abs/2305.15611 ,  23031kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15613
replaced with revised version Wed, 7 Feb 2024 11:28:16 GMT   (267kb,D)

Title: O$n$ Learning Deep O($n$)-Equivariant Hyperspheres
Authors: Pavlo Melnyk, Michael Felsberg, M{\aa}rten Wadenb\"ack, Andreas
  Robinson, Cuong Le
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.15613 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19510
replaced with revised version Wed, 7 Feb 2024 02:51:46 GMT   (1175kb,D)

Title: Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape
Authors: Kedar Karhadkar, Michael Murray, Hanna Tseran, Guido Mont\'ufar
Categories: cs.LG math.CO stat.ML
Comments: 40 pages
\\ ( https://arxiv.org/abs/2305.19510 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01237
replaced with revised version Tue, 6 Feb 2024 21:04:29 GMT   (539kb,D)

Title: Bayesian Regret Minimization in Offline Bandits
Authors: Mohammad Ghavamzadeh, Marek Petrik, Guy Tennenholtz
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2306.01237 ,  539kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01731
replaced with revised version Wed, 7 Feb 2024 18:41:12 GMT   (22508kb,D)

Title: PAGAR: Taming Reward Misalignment in Inverse Reinforcement
  Learning-Based Imitation Learning with Protagonist Antagonist Guided
  Adversarial Reward
Authors: Weichao Zhou, Wenchao Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.01731 ,  22508kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03801
replaced with revised version Wed, 7 Feb 2024 11:03:48 GMT   (2703kb)

Title: Stable Vectorization of Multiparameter Persistent Homology using Signed
  Barcodes as Measures
Authors: David Loiseaux, Luis Scoccola, Mathieu Carri\`ere, Magnus Bakke
  Botnan, Steve Oudot
Categories: cs.LG cs.CG math.AT stat.ML
Comments: 26 pages, 4 figures, 9 tables; v2: final version in NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.03801 ,  2703kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08191
replaced with revised version Wed, 7 Feb 2024 18:18:54 GMT   (317kb,D)

Title: Solving Large-scale Spatial Problems with Convolutional Neural Networks
Authors: Damian Owerko, Charilaos I. Kanatsoulis, Alejandro Ribeiro
Categories: cs.LG eess.SP
Comments: 6 pages, 2 figures, submitted to Asilomar Conference on Signals,
  Systems, and Computers 2023
\\ ( https://arxiv.org/abs/2306.08191 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08754
replaced with revised version Wed, 7 Feb 2024 02:39:56 GMT   (18111kb,D)

Title: ClimSim: A large multi-scale dataset for hybrid physics-ML climate
  emulation
Authors: Sungduk Yu, Walter Hannah, Liran Peng, Jerry Lin, Mohamed Aziz Bhouri,
  Ritwik Gupta, Bj\"orn L\"utjens, Justus Christopher Will, Gunnar Behrens,
  Julius Busecke, Nora Loose, Charles I Stern, Tom Beucler, Bryce Harrop,
  Benjamin R Hillman, Andrea Jenney, Savannah Ferretti, Nana Liu, Anima
  Anandkumar, Noah D Brenowitz, Veronika Eyring, Nicholas Geneva, Pierre
  Gentine, Stephan Mandt, Jaideep Pathak, Akshay Subramaniam, Carl Vondrick,
  Rose Yu, Laure Zanna, Tian Zheng, Ryan Abernathey, Fiaz Ahmed, David C Bader,
  Pierre Baldi, Elizabeth Barnes, Christopher Bretherton, Peter Caldwell, Wayne
  Chuang, Yilun Han, Yu Huang, Fernando Iglesias-Suarez, Sanket Jantre, Karthik
  Kashinath, Marat Khairoutdinov, Thorsten Kurth, Nicholas Lutsko, Po-Lun Ma,
  Griffin Mooers, J. David Neelin, David Randall, Sara Shamekh, et al. (5
  additional authors not shown)
Categories: cs.LG physics.ao-ph
Comments: NeurIPS 2023 Outstanding Datasets and Benchmarks Track Paper
\\ ( https://arxiv.org/abs/2306.08754 ,  18111kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10947
replaced with revised version Wed, 7 Feb 2024 10:41:48 GMT   (24502kb,D)

Title: PAC-Chernoff Bounds: Understanding Generalization in the Interpolation
  Regime
Authors: Andr\'es R. Masegosa and Luis A. Ortega
Categories: cs.LG math.ST stat.ML stat.TH
Comments: 34 pages, 10 figures, Pre-print
\\ ( https://arxiv.org/abs/2306.10947 ,  24502kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11903
replaced with revised version Wed, 7 Feb 2024 17:18:09 GMT   (2298kb,D)

Title: Deep Fusion: Efficient Network Training via Pre-trained Initializations
Authors: Hanna Mazzawi, Xavi Gonzalvo, Michael Wunder, Sammy Jerome, Benoit
  Dherin
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.11903 ,  2298kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16120
replaced with revised version Wed, 7 Feb 2024 13:19:29 GMT   (8688kb,D)

Title: Deep Unrolling Networks with Recurrent Momentum Acceleration for
  Nonlinear Inverse Problems
Authors: Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li
Categories: cs.LG
MSC-class: 68U10, 94A08, 68T99
\\ ( https://arxiv.org/abs/2307.16120 ,  8688kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01070
replaced with revised version Wed, 7 Feb 2024 17:13:43 GMT   (11kb)

Title: When Analytic Calculus Cracks AdaBoost Code
Authors: Jean-Marc Brossier, Olivier Lafitte, Lenny R\'ethor\'e
Categories: cs.LG
Comments: 9 pages, 1 figure
\\ ( https://arxiv.org/abs/2308.01070 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08055
replaced with revised version Tue, 6 Feb 2024 20:04:38 GMT   (19kb)

Title: Simple online learning with consistent oracle
Authors: Alexander Kozachinskiy, Tomasz Steifer
Categories: cs.LG cs.AI stat.ML
Comments: Changes to previous version: added 3^d lower bound
\\ ( https://arxiv.org/abs/2308.08055 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13838
replaced with revised version Wed, 7 Feb 2024 08:41:19 GMT   (521kb,D)

Title: Price-Discrimination Game for Distributed Resource Management in
  Federated Learning
Authors: Han Zhang, Halvin Yang and Guopeng Zhang
Categories: cs.LG cs.GT
\\ ( https://arxiv.org/abs/2308.13838 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01775
replaced with revised version Wed, 7 Feb 2024 11:30:01 GMT   (3778kb,D)

Title: Gated recurrent neural networks discover attention
Authors: Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes von Oswald,
  Maxime Larcher, Angelika Steger, Jo\~ao Sacramento
Categories: cs.LG cs.NE
\\ ( https://arxiv.org/abs/2309.01775 ,  3778kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01945
replaced with revised version Wed, 7 Feb 2024 06:52:29 GMT   (4424kb,D)

Title: OHQ: On-chip Hardware-aware Quantization
Authors: Wei Huang, Haotong Qin, Yangdong Liu, Jingzhuo Liang, Yulun Zhang,
  Ying Li, Xianglong Liu
Categories: cs.LG cs.AI cs.AR
Comments: 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2309.01945 ,  4424kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10980
replaced with revised version Wed, 7 Feb 2024 12:12:40 GMT   (861kb,D)

Title: Adaptive Multi-Agent Deep Reinforcement Learning for Timely Healthcare
  Interventions
Authors: Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, Hong-Ning Dai, and
  Jianming Yong
Categories: cs.LG cs.AI
Comments: This work has been submitted to the ELSEVIER for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible. arXiv admin note: text overlap with
  arXiv:2309.10576
\\ ( https://arxiv.org/abs/2309.10980 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16825
replaced with revised version Tue, 6 Feb 2024 22:14:09 GMT   (1345kb,D)

Title: FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical
  Datasets
Authors: Fatemeh Tavakoli, D.B. Emerson, Sana Ayromlou, John Jewell, Amrit
  Krishnan, Yuchong Zhang, Amol Verma, Fahad Razak
Categories: cs.LG
Comments: 23 pages, 5 figures, 11 tables, 1 algorithm Update includes a
  significant number of new experiments, a new format, and additional results
MSC-class: 68T07
\\ ( https://arxiv.org/abs/2309.16825 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00841
replaced with revised version Tue, 6 Feb 2024 23:14:12 GMT   (1731kb,D)

Title: Drug Discovery with Dynamic Goal-aware Fragments
Authors: Seul Lee, Seanie Lee, Kenji Kawaguchi, Sung Ju Hwang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.00841 ,  1731kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01655
replaced with revised version Wed, 7 Feb 2024 03:13:56 GMT   (407kb,D)

Title: PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels
Authors: Praneeth Kacham, Vahab Mirrokni, Peilin Zhong
Categories: cs.LG
Comments: Adding learned sketches and results on downstream tasks
\\ ( https://arxiv.org/abs/2310.01655 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03272
replaced with revised version Tue, 6 Feb 2024 19:56:35 GMT   (1375kb,D)

Title: Network Alignment with Transferable Graph Autoencoders
Authors: Jiashu He, Charilaos I. Kanatsoulis, Alejandro Ribeiro
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.03272 ,  1375kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03311
replaced with revised version Wed, 7 Feb 2024 03:48:51 GMT   (3924kb,D)

Title: Deep Variational Multivariate Information Bottleneck -- A Framework for
  Variational Losses
Authors: Eslam Abdelaleem and Ilya Nemenman and K. Michael Martini
Categories: cs.LG cond-mat.stat-mech cs.IT math.IT physics.data-an
\\ ( https://arxiv.org/abs/2310.03311 ,  3924kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05401
replaced with revised version Wed, 7 Feb 2024 14:49:08 GMT   (1449kb,D)

Title: Entropy-MCMC: Sampling from Flat Basins with Ease
Authors: Bolian Li, Ruqi Zhang
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.05401 ,  1449kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08164
replaced with revised version Wed, 7 Feb 2024 11:13:15 GMT   (300kb,D)

Title: Beyond Training Objectives: Interpreting Reward Model Divergence in
  Large Language Models
Authors: Luke Marks, Amir Abdullah, Clement Neo, Rauno Arike, Philip Torr, Fazl
  Barez
Categories: cs.LG
Comments: 19 pages, 5 figures
\\ ( https://arxiv.org/abs/2310.08164 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08320
replaced with revised version Wed, 7 Feb 2024 14:13:05 GMT   (25819kb,D)

Title: Defending Our Privacy With Backdoors
Authors: Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting
Categories: cs.LG cs.CL cs.CR cs.CV
Comments: 18 pages, 11 figures
\\ ( https://arxiv.org/abs/2310.08320 ,  25819kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15830
replaced with revised version Wed, 7 Feb 2024 13:50:48 GMT   (5132kb,D)

Title: Localizing Anomalies in Critical Infrastructure using Model-Based Drift
  Explanations
Authors: Valerie Vaquet and Fabian Hinder and Jonas Vaquet and Kathrin Lammers
  and Lars Quakernack and Barbara Hammer
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.15830 ,  5132kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20285
replaced with revised version Wed, 7 Feb 2024 08:37:16 GMT   (5862kb,D)

Title: Accelerating Generalized Linear Models by Trading off Computation for
  Uncertainty
Authors: Lukas Tatzel, Jonathan Wenger, Frank Schneider, Philipp Hennig
Categories: cs.LG stat.ML
Comments: Main text: 11 pages, 6 figures; Supplements: 13 pages, 2 figures
\\ ( https://arxiv.org/abs/2310.20285 ,  5862kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06973
replaced with revised version Tue, 6 Feb 2024 21:40:46 GMT   (672kb)

Title: Analytical Verification of Deep Neural Network Performance for
  Time-Synchronized Distribution System State Estimation
Authors: Behrouz Azimian, Shiva Moshtagh, Anamitra Pal, Shanshan Ma
Categories: cs.LG cs.SY eess.SY
Comments: 8 pages, in Journal of Modern Power Systems and Clean Energy, 2023
DOI: 10.35833/MPCE.2023.000432
\\ ( https://arxiv.org/abs/2311.06973 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07558
replaced with revised version Tue, 6 Feb 2024 20:41:01 GMT   (3576kb,D)

Title: Data-Efficient Task Generalization via Probabilistic Model-based Meta
  Reinforcement Learning
Authors: Arjun Bhardwaj, Jonas Rothfuss, Bhavya Sukhija, Yarden As, Marco
  Hutter, Stelian Coros, Andreas Krause
Categories: cs.LG cs.RO
\\ ( https://arxiv.org/abs/2311.07558 ,  3576kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09438
replaced with revised version Wed, 7 Feb 2024 14:41:40 GMT   (2641kb,D)

Title: Labeled Interactive Topic Models
Authors: Kyle Seelman, Mozhi Zhang, Jordan Boyd-Graber
Categories: cs.LG cs.CL cs.HC cs.IR
\\ ( https://arxiv.org/abs/2311.09438 ,  2641kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01538
replaced with revised version Wed, 7 Feb 2024 13:29:29 GMT   (2431kb,D)

Title: Recurrent Distance Filtering for Graph Representation Learning
Authors: Yuhui Ding, Antonio Orvieto, Bobby He, Thomas Hofmann
Categories: cs.LG cs.NE
\\ ( https://arxiv.org/abs/2312.01538 ,  2431kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05253
replaced with revised version Wed, 7 Feb 2024 18:59:55 GMT   (1792kb,D)

Title: DiSK: A Diffusion Model for Structured Knowledge
Authors: Ouail Kitouni, Niklas Nolte, James Hensman, Bhaskar Mitra
Categories: cs.LG cs.AI
Comments: 24 pages, 12 figures
\\ ( https://arxiv.org/abs/2312.05253 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05596
replaced with revised version Wed, 7 Feb 2024 18:52:26 GMT   (1120kb,D)

Title: Factorized Explainer for Graph Neural Networks
Authors: Rundong Huang, Farhad Shirani, Dongsheng Luo
Categories: cs.LG
Comments: AAAI 24
\\ ( https://arxiv.org/abs/2312.05596 ,  1120kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07930
replaced with revised version Tue, 6 Feb 2024 21:01:28 GMT   (147kb,D)

Title: Towards Optimal Statistical Watermarking
Authors: Baihe Huang and Hanlin Zhu and Banghua Zhu and Kannan Ramchandran and
  Michael I. Jordan and Jason D. Lee and Jiantao Jiao
Categories: cs.LG cs.CL cs.CR cs.IT math.IT stat.ML
\\ ( https://arxiv.org/abs/2312.07930 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09108
replaced with revised version Wed, 7 Feb 2024 08:34:53 GMT   (595kb)

Title: Greedy Shapley Client Selection for Communication-Efficient Federated
  Learning
Authors: Pranava Singhal, Shashi Raj Pandey, Petar Popovski
Categories: cs.LG cs.DC
Comments: Accepted for publication in IEEE Networking Letters
DOI: 10.1109/LNET.2024.3363620
\\ ( https://arxiv.org/abs/2312.09108 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10396
replaced with revised version Wed, 7 Feb 2024 13:27:04 GMT   (72kb)

Title: How Far Can Fairness Constraints Help Recover From Biased Data?
Authors: Mohit Sharma, Amit Deshpande
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.10396 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11549
replaced with revised version Wed, 7 Feb 2024 02:00:39 GMT   (6117kb,D)

Title: Label-Free Multivariate Time Series Anomaly Detection
Authors: Qihang Zhou, Shibo He, Haoyu Liu, Jiming Chen, Wenchao Meng
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2208.02108
\\ ( https://arxiv.org/abs/2312.11549 ,  6117kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12275
replaced with revised version Wed, 7 Feb 2024 10:20:55 GMT   (1046kb,D)

Title: Emergence of In-Context Reinforcement Learning from Noise Distillation
Authors: Ilya Zisman, Vladislav Kurenkov, Alexander Nikulin, Viacheslav Sinii,
  Sergey Kolesnikov
Categories: cs.LG
Comments: Preprint, Under Review; code: https://github.com/corl-team/ad-eps
\\ ( https://arxiv.org/abs/2312.12275 ,  1046kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04472
replaced with revised version Wed, 7 Feb 2024 08:33:18 GMT   (272kb,D)

Title: A Survey on Efficient Federated Learning Methods for Foundation Model
  Training
Authors: Herbert Woisetschl\"ager, Alexander Isenko, Shiqiang Wang, Ruben
  Mayer, Hans-Arno Jacobsen
Categories: cs.LG cs.AI cs.DC
ACM-class: I.2.11; C.2
\\ ( https://arxiv.org/abs/2401.04472 ,  272kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14694
replaced with revised version Tue, 6 Feb 2024 20:26:15 GMT   (1283kb)

Title: TA-RNN: an Attention-based Time-aware Recurrent Neural Network
  Architecture for Electronic Health Records
Authors: Mohammad Al Olaimat, Serdar Bozdag (for the Alzheimer's Disease
  Neuroimaging Initiative)
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.14694 ,  1283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15604
replaced with revised version Tue, 6 Feb 2024 19:38:04 GMT   (65kb)

Title: Neural Network-Based Score Estimation in Diffusion Models: Optimization
  and Generalization
Authors: Yinbin Han, Meisam Razaviyayn, Renyuan Xu
Categories: cs.LG stat.ML
Comments: 38 pages
\\ ( https://arxiv.org/abs/2401.15604 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15879
replaced with revised version Wed, 7 Feb 2024 05:52:28 GMT   (4709kb,D)

Title: lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold
  Gap
Authors: Tzu-Hsien Tsai, Yun-Da Tsai, Shou-De Lin
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.15879 ,  4709kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18079
replaced with revised version Wed, 7 Feb 2024 08:39:28 GMT   (1062kb,D)

Title: KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
  Quantization
Authors: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney,
  Yakun Sophia Shao, Kurt Keutzer, Amir Gholami
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.18079 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01263
replaced with revised version Wed, 7 Feb 2024 18:44:41 GMT   (1559kb,D)

Title: A Differentiable Partially Observable Generalized Linear Model with
  Forward-Backward Message Passing
Authors: Chengrui Li, Weihan Li, Yule Wang, and Anqi Wu
Categories: cs.LG q-bio.NC
\\ ( https://arxiv.org/abs/2402.01263 ,  1559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01995
replaced with revised version Wed, 7 Feb 2024 14:48:35 GMT   (5766kb,D)

Title: Online Uniform Risk Times Sampling: First Approximation Algorithms,
  Learning Augmentation with Full Confidence Interval Integration
Authors: Xueqing Liu, Kyra Gan, Esmaeil Keyvanshokooh, Susan Murphy
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2402.01995 ,  5766kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02018
replaced with revised version Wed, 7 Feb 2024 01:51:21 GMT   (371kb,D)

Title: The Landscape and Challenges of HPC Research and LLMs
Authors: Le Chen, Nesreen K. Ahmed, Akash Dutta, Arijit Bhattacharjee, Sixing
  Yu, Quazi Ishtiaque Mahmud, Waqwoya Abebe, Hung Phan, Aishwarya Sarkar,
  Branden Butler, Niranjan Hasabnis, Gal Oren, Vy A. Vo, Juan Pablo Munoz,
  Theodore L. Willke, Tim Mattson, Ali Jannesari
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.02018 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02216
replaced with revised version Tue, 6 Feb 2024 19:29:54 GMT   (144kb,D)

Title: Graph Foundation Models
Authors: Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao,
  Neil Shah, Mikhail Galkin, Jiliang Tang
Categories: cs.LG
Comments: 18 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.02216 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02347
replaced with revised version Wed, 7 Feb 2024 06:17:13 GMT   (32558kb,D)

Title: Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
Authors: Fangzhao Zhang, Mert Pilanci
Categories: cs.LG cs.NA math.NA math.OC
\\ ( https://arxiv.org/abs/2402.02347 ,  32558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02355
replaced with revised version Wed, 7 Feb 2024 02:38:52 GMT   (29637kb,D)

Title: Symbol: Generating Flexible Black-Box Optimizers through Symbolic
  Equation Learning
Authors: Jiacheng Chen, Zeyuan Ma, Hongshu Guo, Yining Ma, Jie Zhang, Yue-Jiao
  Gong
Categories: cs.LG cs.NE
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2402.02355 ,  29637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02441
replaced with revised version Wed, 7 Feb 2024 05:42:00 GMT   (33kb,D)

Title: TopoX: A Suite of Python Packages for Machine Learning on Topological
  Domains
Authors: Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Agerberg,
  Ibrahem AlJabea, Ruben Ballester, Claudio Battiloro, Guillermo Bern\'ardez,
  Tolga Birdal, Aiden Brent, Peter Chin, Sergio Escalera, Simone Fiorellino,
  Odin Hoff Gardaa, Gurusankar Gopalakrishnan, Devendra Govil, Josef Hoppe,
  Maneel Reddy Karri, Jude Khouja, Manuel Lecha, Neal Livesay, Jan Mei{\ss}ner,
  Soham Mukherjee, Alexander Nikitin, Theodore Papamarkou, Jaro Pr\'ilepok,
  Karthikeyan Natesan Ramamurthy, Paul Rosen, Aldo Guzm\'an-S\'aenz, Alessandro
  Salatiello, Shreyas N. Samaga, Simone Scardapane, Michael T. Schaub, Luca
  Scofano, Indro Spinelli, Lev Telyatnikov, Quang Truong, Robin Walters,
  Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali Zia, Nina Miolane
Categories: cs.LG cs.AI cs.MS stat.CO
\\ ( https://arxiv.org/abs/2402.02441 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02619
replaced with revised version Tue, 6 Feb 2024 20:37:36 GMT   (833kb,D)

Title: Increasing Trust in Language Models through the Reuse of Verified
  Circuits
Authors: Philip Quirke, Clement Neo, Fazl Barez
Categories: cs.LG cs.CL
Comments: 8 pages, 10 figures
\\ ( https://arxiv.org/abs/2402.02619 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02692
replaced with revised version Wed, 7 Feb 2024 16:16:08 GMT   (537kb,D)

Title: Statistical Guarantees for Link Prediction using Graph Neural Networks
Authors: Alan Chung, Amin Saberi, Morgane Austern
Categories: cs.LG cs.SI math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2402.02692 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02910
replaced with revised version Wed, 7 Feb 2024 14:21:37 GMT   (4141kb,D)

Title: DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage
  Temporal Convolutional Network
Authors: Meng Shang, Lenore Dedeyne, Jolan Dupont, Laura Vercauteren, Nadjia
  Amini, Laurence Lapauw, Evelien Gielen, Sabine Verschueren, Carolina Varon,
  Walter De Raedt, Bart Vanrumste
Categories: cs.LG cs.AI eess.SP
\\ ( https://arxiv.org/abs/2402.02910 ,  4141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03292
replaced with revised version Wed, 7 Feb 2024 03:21:57 GMT   (38761kb,D)

Title: Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
Authors: Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian
  Q. Weinberger, Dung D. Le
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2402.03292 ,  38761kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03659
replaced with revised version Wed, 7 Feb 2024 04:12:35 GMT   (3324kb,D)

Title: Learning to Generate Explainable Stock Predictions using Self-Reflective
  Large Language Models
Authors: Kelvin J.L. Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua
Categories: cs.LG cs.CL q-fin.ST
Comments: WWW 2024
\\ ( https://arxiv.org/abs/2402.03659 ,  3324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03784
replaced with revised version Wed, 7 Feb 2024 02:10:11 GMT   (13576kb,D)

Title: AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality
  Prediction
Authors: Kethmi Hirushini Hettige, Jiahao Ji, Shili Xiang, Cheng Long, Gao
  Cong, Jingyuan Wang
Categories: cs.LG cs.AI physics.app-ph
Comments: Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2402.03784 ,  13576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03979
replaced with revised version Wed, 7 Feb 2024 03:09:43 GMT   (419kb,D)

Title: Cross Entropy versus Label Smoothing: A Neural Collapse Perspective
Authors: Li Guo, Keith Ross, Zifan Zhao, George Andriopoulos, Shuyang Ling,
  Yufeng Xu, Zixuan Dong
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.03979 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04193
replaced with revised version Wed, 7 Feb 2024 06:13:23 GMT   (745kb,D)

Title: Gradient Coding in Decentralized Learning for Evading Stragglers
Authors: Chengxi Li and Mikael Skoglund
Categories: cs.LG eess.SP
\\ ( https://arxiv.org/abs/2402.04193 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2209.08996
replaced with revised version Wed, 7 Feb 2024 07:14:55 GMT   (10863kb,D)

Title: EDO-Net: Learning Elastic Properties of Deformable Objects from Graph
  Dynamics
Authors: Alberta Longhini, Marco Moletta, Alfredo Reichlin, Michael C. Welle,
  David Held, Zackory Erickson, and Danica Kragic
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2209.08996 ,  10863kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00720
replaced with revised version Wed, 7 Feb 2024 13:01:24 GMT   (5663kb,D)

Title: A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive
  Coding Networks
Authors: Tommaso Salvatori, Yuhang Song, Yordan Yordanov, Beren Millidge,
  Zhenghua Xu, Lei Sha, Cornelius Emde, Rafal Bogacz, Thomas Lukasiewicz
Categories: cs.NE cs.AI cs.LG
Comments: Change of title and abstract, that now reflect the version accepted
  for publication. One co-author also added, that performed the additional
  experiments
\\ ( https://arxiv.org/abs/2212.00720 ,  5663kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12138
replaced with revised version Wed, 7 Feb 2024 10:07:35 GMT   (2752kb,D)

Title: LMs: Understanding Code Syntax and Semantics for Code Analysis
Authors: Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen
  Zhang, Liming Nie, Li Li, Yang Liu
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2305.12138 ,  2752kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00107
replaced with revised version Wed, 7 Feb 2024 11:12:27 GMT   (393kb,D)

Title: MERT: Acoustic Music Understanding Model with Large-Scale
  Self-supervised Training
Authors: Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin,
  Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge,
  Roger Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang,
  Zili Wang, Yike Guo, Jie Fu
Categories: cs.SD cs.AI cs.CL cs.LG eess.AS
Comments: accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2306.00107 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03933 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 18:16:43 GMT   (427kb,D)

Title: High-dimensional and Permutation Invariant Anomaly Detection
Authors: Vinicius Mikuni, Benjamin Nachman
Categories: hep-ph cs.AI cs.LG hep-ex
Comments: 7 pages, 5 figures
\\ ( https://arxiv.org/abs/2306.03933 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11305
replaced with revised version Wed, 7 Feb 2024 01:52:30 GMT   (48190kb,D)

Title: Progressive Fourier Neural Representation for Sequential Video
  Compilation
Authors: Haeyong Kang, Jaehong Yoon, DaHyun Kim, Sung Ju Hwang, and Chang D Yoo
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2306.11305 ,  48190kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16517
replaced with revised version Wed, 7 Feb 2024 04:53:54 GMT   (2349kb,D)

Title: Select2Col: Leveraging Spatial-Temporal Importance of Semantic
  Information for Efficient Collaborative Perception
Authors: Yuntao Liu, Qian Huang, Rongpeng Li, Xianfu Chen, Zhifeng Zhao,
  Shuyuan Zhao, Yongdong Zhu and Honggang Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.16517 ,  2349kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05869
replaced with revised version Wed, 7 Feb 2024 01:50:14 GMT   (5586kb,D)

Title: Shared Memory-contention-aware Concurrent DNN Execution for Diversely
  Heterogeneous System-on-Chips
Authors: Ismet Dagli, Mehmet Belviranli
Categories: cs.DC cs.AI cs.PF
Journal-ref: 29th ACM SIGPLAN Annual Symposium on Principles and Practice of
  Parallel Programming, 2024
DOI: 10.1145/3627535.3638502
\\ ( https://arxiv.org/abs/2308.05869 ,  5586kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01701
replaced with revised version Tue, 6 Feb 2024 21:50:42 GMT   (0kb,I)

Title: Transcending Domains through Text-to-Image Diffusion: A Source-Free
  Approach to Domain Adaptation
Authors: Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha
Categories: cs.CV cs.AI
Comments: Revamped the whole paper; new version will be re-submitted
\\ ( https://arxiv.org/abs/2310.01701 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07433
replaced with revised version Wed, 7 Feb 2024 14:43:41 GMT   (14138kb,D)

Title: Imitation Learning from Observation with Automatic Discount Scheduling
Authors: Yuyang Liu, Weijun Dong, Yingdong Hu, Chuan Wen, Zhao-Heng Yin,
  Chongjie Zhang, Yang Gao
Categories: cs.RO cs.AI cs.LG
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.07433 ,  14138kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18562
replaced with revised version Wed, 7 Feb 2024 08:47:47 GMT   (2020kb,D)

Title: Optimization-Free Test-Time Adaptation for Cross-Person Activity
  Recognition
Authors: Shuoyuan Wang, Jindong Wang, HuaJun Xi, Bob Zhang, Lei Zhang, Hongxin
  Wei
Categories: cs.CV cs.AI cs.LG
Comments: To be presented at UbiComp 2024; Accepted by Proceedings of the ACM
  on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)
\\ ( https://arxiv.org/abs/2310.18562 ,  2020kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20007 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 22:56:40 GMT   (41kb)

Title: Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement
  Learning
Authors: Ahmadreza Moradipari, Mohammad Pedramfar, Modjtaba Shokrian Zini,
  Vaneet Aggarwal
Categories: stat.ML cs.AI cs.LG
Comments: 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
\\ ( https://arxiv.org/abs/2310.20007 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12610
replaced with revised version Tue, 6 Feb 2024 19:49:11 GMT   (8822kb,D)

Title: VALUED -- Vision and Logical Understanding Evaluation Dataset
Authors: Soumadeep Saha, Saptarshi Saha, Utpal Garain
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.12610 ,  8822kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14395
replaced with revised version Tue, 6 Feb 2024 21:51:04 GMT   (743kb,D)

Title: Unsupervised Deep Learning Image Verification Method
Authors: Enoch Solomon, Abraham Woubie and Eyael Solomon Emiru
Categories: cs.CV cs.AI cs.CY
\\ ( https://arxiv.org/abs/2312.14395 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04122
replaced with revised version Tue, 6 Feb 2024 23:58:44 GMT   (1231kb,D)

Title: From Prompt Engineering to Prompt Science With Human in the Loop
Authors: Chirag Shah
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2401.04122 ,  1231kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07836
replaced with revised version Tue, 6 Feb 2024 21:50:30 GMT   (59kb)

Title: Two Types of AI Existential Risk: Decisive and Accumulative
Authors: Atoosa Kasirzadeh
Categories: cs.CY cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.07836 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10034
replaced with revised version Wed, 7 Feb 2024 07:37:34 GMT   (242kb,D)

Title: Evolutionary Computation in the Era of Large Language Model: Survey and
  Roadmap
Authors: Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, Kay Chen Tan
Categories: cs.NE cs.AI cs.CL
Comments: evolutionary algorithm (EA), large language model (LLM), optimization
  problem, prompt optimization, architecture search, code generation
\\ ( https://arxiv.org/abs/2401.10034 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15753
replaced with revised version Wed, 7 Feb 2024 11:47:38 GMT   (14346kb,D)

Title: An objective comparison of methods for augmented reality in laparoscopic
  liver resection by preoperative-to-intraoperative image fusion
Authors: Sharib Ali, Yamid Espinel, Yueming Jin, Peng Liu, Bianca G\"uttner,
  Xukun Zhang, Lihua Zhang, Tom Dowrick, Matthew J. Clarkson, Shiting Xiao,
  Yifan Wu, Yijun Yang, Lei Zhu, Dai Sun, Lan Li, Micha Pfeiffer, Shahid Farid,
  Lena Maier-Hein, Emmanuel Buc, Adrien Bartoli
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: 24 pages
\\ ( https://arxiv.org/abs/2401.15753 ,  14346kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16123
replaced with revised version Wed, 7 Feb 2024 11:25:28 GMT   (14078kb,D)

Title: Looking for a better fit? An Incremental Learning Multimodal Object
  Referencing Framework adapting to Individual Drivers
Authors: Amr Gomaa and Guillermo Reyes and Michael Feld and Antonio Kr\"uger
Categories: cs.HC cs.AI cs.CV cs.LG
Comments: Accepted for publication in the Proceedings of the 29th International
  Conference on Intelligent User Interfaces (IUI'24), March 18--21, 2024, in
  Greenville, SC, USA
DOI: 10.1145/3640543.3645152
\\ ( https://arxiv.org/abs/2401.16123 ,  14078kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00045
replaced with revised version Wed, 7 Feb 2024 06:27:12 GMT   (34237kb,D)

Title: Detecting Multimedia Generated by Large AI Models: A Survey
Authors: Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding,
  Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu
Categories: cs.MM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.00045 ,  34237kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00350
replaced with revised version Wed, 7 Feb 2024 06:03:15 GMT   (763kb,D)

Title: Large Language Models Based Fuzzing Techniques: A Survey
Authors: Linghan Huang, Peizhou Zhao, Huaming Chen, Lei Ma
Categories: cs.SE cs.AI
Comments: 9 pages submission under review
\\ ( https://arxiv.org/abs/2402.00350 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01345
replaced with revised version Wed, 7 Feb 2024 08:07:02 GMT   (388kb,D)

Title: Skip \n: A Simple Method to Reduce Hallucination in Large
  Vision-Language Models
Authors: Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike
  Zheng Shou
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Technical Report
\\ ( https://arxiv.org/abs/2402.01345 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01580
replaced with revised version Wed, 7 Feb 2024 01:11:10 GMT   (3109kb,D)

Title: Generative AI for Education (GAIED): Advances, Opportunities, and
  Challenges
Authors: Paul Denny, Sumit Gulwani, Neil T. Heffernan, Tanja K\"aser, Steven
  Moore, Anna N. Rafferty, Adish Singla
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2402.01580 ,  3109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01748
replaced with revised version Wed, 7 Feb 2024 17:55:11 GMT   (1497kb,D)

Title: Large Multi-Modal Models (LMMs) as Universal Foundation Models for
  AI-Native Wireless Systems
Authors: Shengzhe Xu, Christo Kurisummoottil Thomas, Omar Hashash, Nikhil
  Muralidhar, Walid Saad, Naren Ramakrishnan
Categories: cs.NI cs.AI cs.CL cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2402.01748 ,  1497kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02544
replaced with revised version Wed, 7 Feb 2024 03:28:12 GMT   (18432kb,D)

Title: LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal
  Language Model
Authors: Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao
Categories: cs.CV cs.AI cs.LG
Comments: 32 pages, 8 figures. Github https://github.com/NJU-LHRS/LHRS-Bot
\\ ( https://arxiv.org/abs/2402.02544 ,  18432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03214
replaced with revised version Tue, 6 Feb 2024 18:57:10 GMT   (20417kb,D)

Title: Organic or Diffused: Can We Distinguish Human Art from AI-generated
  Images?
Authors: Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan,
  Reid Southen, Haitao Zheng, Ben Y. Zhao
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03214 ,  20417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03358
replaced with revised version Wed, 7 Feb 2024 03:30:41 GMT   (242kb,D)

Title: A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening,
  and Condensation
Authors: Mohammad Hashemi, Shengbo Gong, Juntong Ni, Wenqi Fan, B. Aditya
  Prakash, Wei Jin
Categories: cs.SI cs.AI cs.DS cs.LG
Comments: 16 pages, 3 tables, 2 figures
\\ ( https://arxiv.org/abs/2402.03358 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03561
replaced with revised version Wed, 7 Feb 2024 18:02:51 GMT   (9474kb,D)

Title: VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language
  Navigation
Authors: Jialu Li, Aishwarya Padmakumar, Gaurav Sukhatme, Mohit Bansal
Categories: cs.CV cs.AI cs.CL
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2402.03561 ,  9474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03583
replaced with revised version Wed, 7 Feb 2024 03:03:06 GMT   (360kb,D)

Title: MQuinE: a cure for "Z-paradox" in knowledge graph embedding models
Authors: Yang Liu, Huang Fang, Yunfeng Cai, Mingming Sun
Categories: cs.SI cs.AI cs.LG
Comments: 18pages, 1 figure
\\ ( https://arxiv.org/abs/2402.03583 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04247
replaced with revised version Wed, 7 Feb 2024 14:26:02 GMT   (12836kb,D)

Title: Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
Authors: Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang,
  Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman
  Cohan, Zhiyong Lu, Mark Gerstein
Categories: cs.CY cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.04247 ,  12836kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07235 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 09:48:07 GMT   (568kb,D)

Title: What does self-attention learn from Masked Language Modelling?
Authors: Riccardo Rende, Federica Gerace, Alessandro Laio, Sebastian Goldt
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.CL stat.ML
Comments: 4 pages, 3 figures
\\ ( https://arxiv.org/abs/2304.07235 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08030 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 22:12:51 GMT   (1145kb,D)

Title: AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised
  Features for Audio-Visual Speech Enhancement
Authors: Ju-Chieh Chou, Chung-Ming Chien, Karen Livescu
Categories: eess.AS cs.CL cs.SD
Comments: ICASSP 2024
\\ ( https://arxiv.org/abs/2309.08030 ,  1145kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11123
replaced with revised version Tue, 6 Feb 2024 20:32:41 GMT   (195kb,D)

Title: (Why) Is My Prompt Getting Worse? Rethinking Regression Testing for
  Evolving LLM APIs
Authors: Wanqin Ma, Chenyang Yang, Christian K\"astner
Categories: cs.SE cs.CL
Comments: conference version
\\ ( https://arxiv.org/abs/2311.11123 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2011.08388
replaced with revised version Wed, 7 Feb 2024 13:23:08 GMT   (4638kb,D)

Title: Domain Adaptation based Interpretable Image Emotion Recognition using
  Facial Expression Recognition
Authors: Puneet Kumar and Balasubramanian Raman
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2011.08388 ,  4638kb)
------------------------------------------------------------------------------
\\
arXiv:2012.14600
replaced with revised version Wed, 7 Feb 2024 14:04:43 GMT   (23609kb,D)

Title: A Comprehensive Guide to CAN IDS Data & Introduction of the ROAD Dataset
Authors: Miki E. Verma and Robert A. Bridges and Michael D. Iannacone and
  Samuel C. Hollifield and Pablo Moriano and Steven C. Hespeler and Bill Kay
  and Frank L. Combs
Categories: cs.CR cs.LG
Comments: title changed and author added from original version
Journal-ref: PLoS one 19, no. 1 (2024): e0296879
DOI: 10.1371/journal.pone.0296879 10.5281/zenodo.10462795
\\ ( https://arxiv.org/abs/2012.14600 ,  23609kb)
------------------------------------------------------------------------------
\\
arXiv:2105.00582 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 21:41:26 GMT   (536kb)

Title: Semi-supervised learning for generalizable intracranial hemorrhage
  detection and segmentation
Authors: Emily Lin, Esther Yuh
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2105.00582 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2111.12581
replaced with revised version Tue, 6 Feb 2024 21:53:00 GMT   (425kb,D)

Title: Medium Access Control protocol for Collaborative Spectrum Learning in
  Wireless Networks
Authors: Tomer Boyarski, Wenbo Wang, Amir Leshem
Categories: cs.NI cs.LG cs.MA
Journal-ref: IEEE Trans. on Signal Processing, 2023, pages: 3149-3163
DOI: 10.1109/TSP.2023.3300630
\\ ( https://arxiv.org/abs/2111.12581 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09869
replaced with revised version Tue, 6 Feb 2024 21:12:24 GMT   (12338kb,D)

Title: RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and
  Generation
Authors: Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan
  Bilen, Niloy J. Mitra, Paul Guerrero
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2023. Project page:
  https://github.com/Anciukevicius/RenderDiffusion
\\ ( https://arxiv.org/abs/2211.09869 ,  12338kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10805 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 02:06:44 GMT   (47kb,D)

Title: On the Pointwise Behavior of Recursive Partitioning and Its Implications
  for Heterogeneous Causal Effect Estimation
Authors: Matias D. Cattaneo, Jason M. Klusowski, Peter M. Tian
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2211.10805 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13692
replaced with revised version Wed, 7 Feb 2024 06:45:30 GMT   (7782kb,D)

Title: To be or not to be stable, that is the question: understanding neural
  networks for inverse problems
Authors: Davide Evangelista, James Nagy, Elena Morotti, Elena Loli Piccolomini
Categories: math.NA cs.LG cs.NA
Comments: 21 pages, 6 figure. Paper will be sent for publication on a journal
  soon. This is a preliminary version, updated versions will be uploaded on
  ArXiv
MSC-class: 65K10, 68T07, 68U10
\\ ( https://arxiv.org/abs/2211.13692 ,  7782kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10019 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 20:39:45 GMT   (1695kb,AD)

Title: Multivariate Probabilistic CRPS Learning with an Application to
  Day-Ahead Electricity Prices
Authors: Jonathan Berrisch, Florian Ziel
Categories: stat.ML cs.LG econ.EM q-fin.CP stat.AP
\\ ( https://arxiv.org/abs/2303.10019 ,  1695kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15919
replaced with revised version Wed, 7 Feb 2024 13:46:35 GMT   (2384kb,D)

Title: Fully Hyperbolic Convolutional Neural Networks for Computer Vision
Authors: Ahmad Bdeir and Kristian Schwethelm and Niels Landwehr
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2303.15919 ,  2384kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17152
replaced with revised version Wed, 7 Feb 2024 13:53:38 GMT   (5318kb,D)

Title: Mixed Autoencoder for Self-supervised Visual Representation Learning
Authors: Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung
Categories: cs.CV cs.LG
Comments: Accepted by CVPR 2023
\\ ( https://arxiv.org/abs/2303.17152 ,  5318kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12498 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 18:06:54 GMT   (416kb,D)

Title: Empirical Risk Minimization with Shuffled SGD: A Primal-Dual Perspective
  and Improved Bounds
Authors: Xufeng Cai, Cheuk Yin Lin, Jelena Diakonikolas
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2306.12498 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01037 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 16:00:59 GMT   (11559kb,D)

Title: Vector Quantile Regression on Manifolds
Authors: Marco Pegoraro, Sanketh Vedula, Aviv A. Rosenberg, Irene Tallini,
  Emanuele Rodol\`a, Alex M. Bronstein
Categories: stat.ME cs.LG
\\ ( https://arxiv.org/abs/2307.01037 ,  11559kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01946
replaced with revised version Wed, 7 Feb 2024 02:56:52 GMT   (33847kb,D)

Title: ECG-Image-Kit: A Synthetic Image Generation Toolbox to Facilitate Deep
  Learning-Based Electrocardiogram Digitization
Authors: Kshama Kodthalu Shivashankara, Deepanshi, Afagh Mehri Shervedani, Gari
  D. Clifford, Matthew A. Reyna, Reza Sameni
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.01946 ,  33847kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04081
replaced with revised version Wed, 7 Feb 2024 16:00:21 GMT   (9135kb,D)

Title: Score-based Conditional Generation with Fewer Labeled Data by
  Self-calibrating Classifier Guidance
Authors: Paul Kuo-Ming Huang, Si-An Chen, Hsuan-Tien Lin
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.04081 ,  9135kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19283
replaced with revised version Wed, 7 Feb 2024 05:39:40 GMT   (269kb,D)

Title: rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature
  Extraction for IMU-based Human Activity Recognition
Authors: Yu Enokibori
Categories: cs.HC cs.LG
Comments: Updating abstract length to clear a submission target's requirement.
  Updating English quality. Updating the best results of OPPORTUNITY (not iSPL
  version) and PAMAP2
\\ ( https://arxiv.org/abs/2310.19283 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20172 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 16:58:38 GMT   (7908kb,D)

Title: Compact Binary Systems Waveform Generation with Generative Pre-trained
  Transformer
Authors: Ruijun Shi, Yue Zhou, Tianyu Zhao, Zhoujian Cao, Zhixiang Ren
Categories: gr-qc astro-ph.IM cs.LG
\\ ( https://arxiv.org/abs/2310.20172 ,  7908kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06650 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 16:12:06 GMT   (834kb,D)

Title: Heuristic Optimal Transport in Branching Networks
Authors: M. Andrecut
Categories: math.OC cs.LG
Comments: Accepted in Int. J. Mod. Phys. C, 11 pages, 6 figures
\\ ( https://arxiv.org/abs/2311.06650 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06918
replaced with revised version Tue, 6 Feb 2024 20:27:27 GMT   (376kb,D)

Title: Resource-Aware Hierarchical Federated Learning for Video Caching in
  Wireless Networks
Authors: Md Ferdous Pervej and Andreas F Molisch
Categories: cs.NI cs.LG cs.SY eess.SY
Comments: Accepted for publication in IEEE ICC 2024. \c{opyright} 2024 IEEE.
  Personal use of this material is permitted. Permission from IEEE must be
  obtained for all other uses
\\ ( https://arxiv.org/abs/2311.06918 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14220 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 21:23:09 GMT   (1005kb,D)

Title: Assumption-lean and Data-adaptive Post-Prediction Inference
Authors: Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, and Qiongshi Lu
Categories: stat.ME cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.14220 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00639
replaced with revised version Wed, 7 Feb 2024 10:57:28 GMT   (37530kb,D)

Title: RefinedFields: Radiance Fields Refinement for Unconstrained Scenes
Authors: Karim Kassab, Antoine Schnepf, Jean-Yves Franceschi, Laurent Caraffa,
  Jeremie Mary, Val\'erie Gouet-Brunet
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.00639 ,  37530kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03506 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 22:38:24 GMT   (177kb,D)

Title: DiarizationLM: Speaker Diarization Post-Processing with Large Language
  Models
Authors: Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, Hank Liao
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2401.03506 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16356 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 15:37:24 GMT   (3781kb,D)

Title: cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and
  Glitch Generation
Authors: Tom Dooney, Lyana Curier, Daniel Tan, Melissa Lopez, Chris Van Den
  Broeck, Stefano Bromuri
Categories: physics.ins-det cs.LG gr-qc
\\ ( https://arxiv.org/abs/2401.16356 ,  3781kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16803
replaced with revised version Wed, 7 Feb 2024 06:48:12 GMT   (1399kb,D)

Title: PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset
Authors: Arhan Jain, Alec Bunn, Austin Pham, and TJ Tsai
Categories: cs.SD cs.LG eess.AS
Comments: 15 pages, 4 figures
\\ ( https://arxiv.org/abs/2401.16803 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17760 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 06:49:41 GMT   (3053kb)

Title: Regularized Linear Discriminant Analysis Using a Nonlinear Covariance
  Matrix Estimator
Authors: Maaz Mahadi, Tarig Ballal, Muhammad Moinuddin, Tareq Y. Al-Naffouri,
  and Ubaid M. Al-Saggaf
Categories: stat.ML cs.LG eess.SP
Comments: \c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\ ( https://arxiv.org/abs/2401.17760 ,  3053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01000 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 16:53:25 GMT   (642kb,D)

Title: Multivariate Probabilistic Time Series Forecasting with Correlated
  Errors
Authors: Vincent Zhihao Zheng, Lijun Sun
Categories: stat.ML cs.LG
Comments: This paper extends the work presented in arXiv:2305.17028 to a
  multivariate setting
\\ ( https://arxiv.org/abs/2402.01000 ,  642kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01598 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 19:32:28 GMT   (609kb,D)

Title: Learning from Two Decades of Blood Pressure Data: Demography-Specific
  Patterns Across 75 Million Patient Encounters
Authors: Seyedeh Somayyeh Mousavi and Yuting Guo and Abeed Sarker and Reza
  Sameni
Categories: q-bio.QM cs.LG stat.AP
\\ ( https://arxiv.org/abs/2402.01598 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02304 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 17:42:57 GMT   (1403kb,D)

Title: Efficient Numerical Wave Propagation Enhanced By An End-to-End Deep
  Learning Model
Authors: Luis Kaiser, Richard Tsai, Christian Klingenberg
Categories: math.AP cs.LG
\\ ( https://arxiv.org/abs/2402.02304 ,  1403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03019
replaced with revised version Wed, 7 Feb 2024 05:50:11 GMT   (34028kb,D)

Title: Taylor Videos for Action Recognition
Authors: Lei Wang and Xiuyuan Yuan and Tom Gedeon and Liang Zheng
Categories: cs.CV cs.LG
Comments: Research report
\\ ( https://arxiv.org/abs/2402.03019 ,  34028kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03363 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 02:15:40 GMT   (1185kb,D)

Title: Exploring Prime Number Classification: Achieving High Recall Rate and
  Rapid Convergence with Sparse Encoding
Authors: Serin Lee and S. Kim
Categories: math.NT cs.LG
MSC-class: 11, 68
ACM-class: G.0; G.1.0; G.1.10; G.1.m; I.0; I.m; I.1.1; I.2.0; I.2.6; I.2.m; J.2
\\ ( https://arxiv.org/abs/2402.03363 ,  1185kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04022 (*cross-listing*)
replaced with revised version Wed, 7 Feb 2024 18:36:18 GMT   (2249kb,D)

Title: A General Theory for Kernel Packets: from state space model to compactly
  supported basis
Authors: Liang Ding and Tuo Rui
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.04022 ,  2249kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
