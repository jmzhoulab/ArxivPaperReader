Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月7日 16:02
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon  5 Feb 24 19:00:00 GMT  to  Tue  6 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.03375
Date: Sat, 3 Feb 2024 08:00:12 GMT   (522kb,D)

Title: BetterV: Controlled Verilog Generation with Discriminative Guidance
Authors: Zehua Pei, Hui-Ling Zhen, Mingxuan Yuan, Yu Huang, Bei Yu
Categories: cs.AI cs.PL
\\
  Due to the growing complexity of modern Integrated Circuits (ICs), there is a
need for automated circuit design methods. Recent years have seen rising
research in hardware design language generation to facilitate the design
process. In this work, we propose a Verilog generation framework, BetterV,
which fine-tunes the large language models (LLMs) on processed domain-specific
datasets and incorporates generative discriminators for guidance on particular
design demands. The Verilog modules are collected, filtered and processed from
internet to form a clean and abundant dataset. Instruct-tuning methods are
specially designed to fine-tuned the LLMs to understand the knowledge about
Verilog. Furthermore, data are augmented to enrich the training set and also
used to train a generative discriminator on particular downstream task, which
leads a guidance for the LLMs to optimize the Verilog implementation. BetterV
has the ability to generate syntactically and functionally correct Verilog,
which can outperform GPT-4 on the VerilogEval-machine benchmark. With the help
of task-specific generative discriminator, BetterV can achieve remarkable
improvement on various electronic design automation (EDA) downstream tasks,
including the netlist node reduction for synthesis and verification runtime
reduction with Boolean Satisfiability (SAT) solving.
\\ ( https://arxiv.org/abs/2402.03375 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03388
Date: Sun, 4 Feb 2024 10:18:33 GMT   (4016kb,D)

Title: Delivery Optimized Discovery in Behavioral User Segmentation under
  Budget Constrain
Authors: Harshita Chopra, Atanu R. Sinha, Sunav Choudhary, Ryan A. Rossi,
  Paavan Kumar Indela, Veda Pranav Parwatala, Srinjayee Paul, Aurghya Maiti
Categories: cs.AI cs.IR cs.LG
DOI: 10.1145/3583780.3614839
\\
  Users' behavioral footprints online enable firms to discover behavior-based
user segments (or, segments) and deliver segment specific messages to users.
Following the discovery of segments, delivery of messages to users through
preferred media channels like Facebook and Google can be challenging, as only a
portion of users in a behavior segment find match in a medium, and only a
fraction of those matched actually see the message (exposure). Even high
quality discovery becomes futile when delivery fails. Many sophisticated
algorithms exist for discovering behavioral segments; however, these ignore the
delivery component. The problem is compounded because (i) the discovery is
performed on the behavior data space in firms' data (e.g., user clicks), while
the delivery is predicated on the static data space (e.g., geo, age) as defined
by media; and (ii) firms work under budget constraint. We introduce a
stochastic optimization based algorithm for delivery optimized discovery of
behavioral user segmentation and offer new metrics to address the joint
optimization. We leverage optimization under a budget constraint for delivery
combined with a learning-based component for discovery. Extensive experiments
on a public dataset from Google and a proprietary dataset show the
effectiveness of our approach by simultaneously improving delivery metrics,
reducing budget spend and achieving strong predictive performance in discovery.
\\ ( https://arxiv.org/abs/2402.03388 ,  4016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03494
Date: Mon, 5 Feb 2024 20:11:56 GMT   (4689kb,D)

Title: Beyond Text: Improving LLM's Decision Making for Robot Navigation via
  Vocal Cues
Authors: Xingpeng Sun, Haoming Meng, Souradip Chakraborty, Amrit Singh Bedi,
  Aniket Bera
Categories: cs.AI cs.RO
Comments: 20 pages, 8 figures
\\
  This work highlights a critical shortcoming in text-based Large Language
Models (LLMs) used for human-robot interaction, demonstrating that text alone
as a conversation modality falls short in such applications. While LLMs excel
in processing text in these human conversations, they struggle with the nuances
of verbal instructions in scenarios like social navigation, where ambiguity and
uncertainty can erode trust in robotic and other AI systems. We can address
this shortcoming by moving beyond text and additionally focusing on the
paralinguistic features of these audio responses. These features are the
aspects of spoken communication that do not involve the literal wording
(lexical content) but convey meaning and nuance through how something is said.
We present "Beyond Text"; an approach that improves LLM decision-making by
integrating audio transcription along with a subsection of these features,
which focus on the affect and more relevant in human-robot conversations. This
approach not only achieves a 70.26% winning rate, outperforming existing LLMs
by 48.30%, but also enhances robustness against token manipulation adversarial
attacks, highlighted by a 22.44% less decrease ratio than the text-only
language model in winning rate. "Beyond Text" marks an advancement in social
robot navigation and broader Human-Robot interactions, seamlessly integrating
text-based guidance with human-audio-informed language models.
\\ ( https://arxiv.org/abs/2402.03494 ,  4689kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03507
Date: Mon, 5 Feb 2024 20:48:57 GMT   (1909kb,D)

Title: Neural networks for abstraction and reasoning: Towards broad
  generalization in machines
Authors: Mikel Bober-Irizar, Soumya Banerjee
Categories: cs.AI cs.CL cs.LG
Comments: 32 pages main text, 17 pages
\\
  For half a century, artificial intelligence research has attempted to
reproduce the human qualities of abstraction and reasoning - creating computer
systems that can learn new concepts from a minimal set of examples, in settings
where humans find this easy. While specific neural networks are able to solve
an impressive range of problems, broad generalisation to situations outside
their training data has proved elusive.In this work, we look at several novel
approaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of
abstract visual reasoning tasks introduced to test algorithms on broad
generalization. Despite three international competitions with $100,000 in
prizes, the best algorithms still fail to solve a majority of ARC tasks and
rely on complex hand-crafted rules, without using machine learning at all. We
revisit whether recent advances in neural networks allow progress on this task.
  First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.
DreamCoder automatically writes programs in a bespoke domain-specific language
to perform reasoning, using a neural network to mimic human intuition. We
present the Perceptual Abstraction and Reasoning Language (PeARL) language,
which allows DreamCoder to solve ARC tasks, and propose a new recognition model
that allows us to significantly improve on the previous best implementation.We
also propose a new encoding and augmentation scheme that allows large language
models (LLMs) to solve ARC tasks, and find that the largest models can solve
some ARC tasks. LLMs are able to solve a different group of problems to
state-of-the-art solvers, and provide an interesting way to complement other
approaches. We perform an ensemble analysis, combining models to achieve better
results than any system alone. Finally, we publish the arckit Python library to
make future research on ARC easier.
\\ ( https://arxiv.org/abs/2402.03507 ,  1909kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03539
Date: Mon, 5 Feb 2024 21:51:36 GMT   (146kb,D)

Title: Extended Version of: On the Structural Hardness of Answer Set
  Programming: Can Structure Efficiently Confine the Power of Disjunctions?
Authors: Markus Hecher, Rafael Kiesel
Categories: cs.AI cs.LO
\\
  Answer Set Programming (ASP) is a generic problem modeling and solving
framework with a strong focus on knowledge representation and a rapid growth of
industrial applications. So far, the study of complexity resulted in
characterizing hardness and determining their sources, fine-grained insights in
the form of dichotomy-style results, as well as detailed parameterized
complexity landscapes. Unfortunately, for the well-known parameter treewidth
disjunctive programs require double-exponential runtime under reasonable
complexity assumptions. This quickly becomes out of reach. We deal with the
classification of structural parameters for disjunctive ASP on the program's
rule structure (incidence graph).
  First, we provide a polynomial kernel to obtain single-exponential runtime in
terms of vertex cover size, despite subset-minimization being not represented
in the program's structure. Then we turn our attention to strictly better
structural parameters between vertex cover size and treewidth. Here, we provide
double-exponential lower bounds for the most prominent parameters in that
range: treedepth, feedback vertex size, and cliquewidth. Based on this, we
argue that unfortunately our options beyond vertex cover size are limited. Our
results provide an in-depth hardness study, relying on a novel reduction from
normal to disjunctive programs, trading the increase of complexity for an
exponential parameter compression.
\\ ( https://arxiv.org/abs/2402.03539 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03575
Date: Mon, 5 Feb 2024 22:55:33 GMT   (47961kb,D)

Title: Toward Human-AI Alignment in Large-Scale Multi-Player Games
Authors: Sugandha Sharma, Guy Davidson, Khimya Khetarpal, Anssi Kanervisto,
  Udit Arora, Katja Hofmann, Ida Momennejad
Categories: cs.AI cs.HC
\\
  Achieving human-AI alignment in complex multi-agent games is crucial for
creating trustworthy AI agents that enhance gameplay. We propose a method to
evaluate this alignment using an interpretable task-sets framework, focusing on
high-level behavioral tasks instead of low-level policies. Our approach has
three components. First, we analyze extensive human gameplay data from Xbox's
Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task
space. This task space serves as a basis set for a behavior manifold capturing
interpretable axes: fight-flight, explore-exploit, and solo-multi-agent.
Second, we train an AI agent to play Bleeding Edge using a Generative
Pretrained Causal Transformer and measure its behavior. Third, we project human
and AI gameplay to the proposed behavior manifold to compare and contrast. This
allows us to interpret differences in policy as higher-level behavioral
concepts, e.g., we find that while human players exhibit variability in
fight-flight and explore-exploit behavior, AI players tend towards uniformity.
Furthermore, AI agents predominantly engage in solo play, while humans often
engage in cooperative and competitive multi-agent patterns. These stark
differences underscore the need for interpretable evaluation, design, and
integration of AI in human-aligned applications. Our study advances the
alignment discussion in AI and especially generative AI research, offering a
measurable framework for interpretable human-agent alignment in multiplayer
gaming.
\\ ( https://arxiv.org/abs/2402.03575 ,  47961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03607
Date: Tue, 6 Feb 2024 00:51:27 GMT   (41019kb,D)

Title: Improving Contextual Congruence Across Modalities for Effective
  Multimodal Marketing using Knowledge-infused Learning
Authors: Trilok Padhi, Ugur Kursuncu, Yaman Kumar, Valerie L. Shalin, Lane
  Peterson Fronczek
Categories: cs.AI cs.CL cs.CV cs.CY cs.HC
ACM-class: I.2.7; I.2.10; I.2.4; I.2.1
\\
  The prevalence of smart devices with the ability to capture moments in
multiple modalities has enabled users to experience multimodal information
online. However, large Language (LLMs) and Vision models (LVMs) are still
limited in capturing holistic meaning with cross-modal semantic relationships.
Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual
Language Models (VLMs) only learn implicit representations by capturing
high-level patterns in vast corpora, missing essential contextual cross-modal
cues. In this work, we design a framework to couple explicit commonsense
knowledge in the form of knowledge graphs with large VLMs to improve the
performance of a downstream task, predicting the effectiveness of multi-modal
marketing campaigns. While the marketing application provides a compelling
metric for assessing our methods, our approach enables the early detection of
likely persuasive multi-modal campaigns and the assessment and augmentation of
marketing theory.
\\ ( https://arxiv.org/abs/2402.03607 ,  41019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03618
Date: Tue, 6 Feb 2024 01:07:56 GMT   (1701kb,D)

Title: Comparing Abstraction in Humans and Large Language Models Using
  Multimodal Serial Reproduction
Authors: Sreejan Kumar, Raja Marjieh, Byron Zhang, Declan Campbell, Michael Y.
  Hu, Umang Bhatt, Brenden Lake, Thomas L. Griffiths
Categories: cs.AI cs.CL q-bio.NC
\\
  Humans extract useful abstractions of the world from noisy sensory data.
Serial reproduction allows us to study how people construe the world through a
paradigm similar to the game of telephone, where one person observes a stimulus
and reproduces it for the next to form a chain of reproductions. Past serial
reproduction experiments typically employ a single sensory modality, but humans
often communicate abstractions of the world to each other through language. To
investigate the effect language on the formation of abstractions, we implement
a novel multimodal serial reproduction framework by asking people who receive a
visual stimulus to reproduce it in a linguistic format, and vice versa. We ran
unimodal and multimodal chains with both humans and GPT-4 and find that adding
language as a modality has a larger effect on human reproductions than GPT-4's.
This suggests human visual and linguistic representations are more dissociable
than those of GPT-4.
\\ ( https://arxiv.org/abs/2402.03618 ,  1701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03620
Date: Tue, 6 Feb 2024 01:13:53 GMT   (1783kb,D)

Title: Self-Discover: Large Language Models Self-Compose Reasoning Structures
Authors: Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V.
  Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven Zheng
Categories: cs.AI cs.CL
Comments: 17 pages, 11 figures, 5 tables
\\
  We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the
task-intrinsic reasoning structures to tackle complex reasoning problems that
are challenging for typical prompting methods. Core to the framework is a
self-discovery process where LLMs select multiple atomic reasoning modules such
as critical thinking and step-by-step thinking, and compose them into an
explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER
substantially improves GPT-4 and PaLM 2's performance on challenging reasoning
benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as
much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER
outperforms inference-intensive methods such as CoT-Self-Consistency by more
than 20%, while requiring 10-40x fewer inference compute. Finally, we show that
the self-discovered reasoning structures are universally applicable across
model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share
commonalities with human reasoning patterns.
\\ ( https://arxiv.org/abs/2402.03620 ,  1783kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03640
Date: Tue, 6 Feb 2024 02:33:00 GMT   (371kb,D)

Title: torchmSAT: A GPU-Accelerated Approximation To The Maximum Satisfiability
  Problem
Authors: Abdelrahman Hosny, Sherief Reda
Categories: cs.AI
\\
  The remarkable achievements of machine learning techniques in analyzing
discrete structures have drawn significant attention towards their integration
into combinatorial optimization algorithms. Typically, these methodologies
improve existing solvers by injecting learned models within the solving loop to
enhance the efficiency of the search process. In this work, we derive a single
differentiable function capable of approximating solutions for the Maximum
Satisfiability Problem (MaxSAT). Then, we present a novel neural network
architecture to model our differentiable function, and progressively solve
MaxSAT using backpropagation. This approach eliminates the need for labeled
data or a neural network training phase, as the training process functions as
the solving algorithm. Additionally, we leverage the computational power of
GPUs to accelerate these computations. Experimental results on challenging
MaxSAT instances show that our proposed methodology outperforms two existing
MaxSAT solvers, and is on par with another in terms of solution cost, without
necessitating any training or access to an underlying SAT solver. Given that
numerous NP-hard problems can be reduced to MaxSAT, our novel technique paves
the way for a new generation of solvers poised to benefit from neural network
GPU acceleration.
\\ ( https://arxiv.org/abs/2402.03640 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03678
Date: Tue, 6 Feb 2024 04:00:21 GMT   (1856kb,D)

Title: Logical Specifications-guided Dynamic Task Sampling for Reinforcement
  Learning Agents
Authors: Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert
  Wright and Jivko Sinapov
Categories: cs.AI cs.LG cs.RO
\\
  Reinforcement Learning (RL) has made significant strides in enabling
artificial agents to learn diverse behaviors. However, learning an effective
policy often requires a large number of environment interactions. To mitigate
sample complexity issues, recent approaches have used high-level task
specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward
Machines (RM), to guide the learning progress of the agent. In this work, we
propose a novel approach, called Logical Specifications-guided Dynamic Task
Sampling (LSTS), that learns a set of RL policies to guide an agent from an
initial state to a goal state based on a high-level task specification, while
minimizing the number of environmental interactions. Unlike previous work, LSTS
does not assume information about the environment dynamics or the Reward
Machine, and dynamically samples promising tasks that lead to successful goal
policies. We evaluate LSTS on a gridworld and show that it achieves improved
time-to-threshold performance on complex sequential decision-making problems
compared to state-of-the-art RM and Automaton-guided RL baselines, such as
Q-Learning for Reward Machines and Compositional RL from logical Specifications
(DIRL). Moreover, we demonstrate that our method outperforms RM and
Automaton-guided RL baselines in terms of sample-efficiency, both in a
partially observable robotic task and in a continuous control robotic
manipulation task.
\\ ( https://arxiv.org/abs/2402.03678 ,  1856kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03728
Date: Tue, 6 Feb 2024 05:50:04 GMT   (650kb,D)

Title: Consistent Joint Decision-Making with Heterogeneous Learning Models
Authors: Hossein Rajaby Faghihi and Parisa Kordjamshidi
Categories: cs.AI cs.CL cs.LG cs.LO
Comments: EACL 2024 Findings - Short Paper
Journal-ref: EACL 2024
\\
  This paper introduces a novel decision-making framework that promotes
consistency among decisions made by diverse models while utilizing external
knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map
predictions from various models into globally normalized and comparable values
by incorporating information about decisions' prior probability, confidence
(uncertainty), and the models' expected accuracy. Our empirical study
demonstrates the superiority of our approach over conventional baselines on
multiple datasets.
\\ ( https://arxiv.org/abs/2402.03728 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03732
Date: Tue, 6 Feb 2024 05:58:15 GMT   (4790kb,D)

Title: Deep Outdated Fact Detection in Knowledge Graphs
Authors: Huiling Tu, Shuo Yu, Vidya Saikrishna, Feng Xia, Karin Verspoor
Categories: cs.AI cs.CL cs.DL cs.LG
Comments: 10 pages, 6 figures
MSC-class: 68T09, 68T30, 68P20
ACM-class: I.2.6; I.2.4; H.3.7; H.3.3
Journal-ref: 2023 IEEE International Conference on Data Mining Workshops
  (ICDMW), December 1-4, 2023, Shanghai, China
DOI: 10.1109/ICDMW60847.2023.00184
\\
  Knowledge graphs (KGs) have garnered significant attention for their vast
potential across diverse domains. However, the issue of outdated facts poses a
challenge to KGs, affecting their overall quality as real-world information
evolves. Existing solutions for outdated fact detection often rely on manual
recognition. In response, this paper presents DEAN (Deep outdatEd fAct
detectioN), a novel deep learning-based framework designed to identify outdated
facts within KGs. DEAN distinguishes itself by capturing implicit structural
information among facts through comprehensive modeling of both entities and
relations. To effectively uncover latent out-of-date information, DEAN employs
a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph,
weighted by the number of entities. Experimental results demonstrate the
effectiveness and superiority of DEAN over state-of-the-art baseline methods.
\\ ( https://arxiv.org/abs/2402.03732 ,  4790kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03755
Date: Tue, 6 Feb 2024 06:47:14 GMT   (886kb,D)

Title: QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large
  Language Model
Authors: Saizhuo Wang, Hang Yuan, Lionel M. Ni, Jian Guo
Categories: cs.AI q-fin.CP
\\
  Autonomous agents based on Large Language Models (LLMs) that devise plans and
tackle real-world challenges have gained prominence.However, tailoring these
agents for specialized domains like quantitative investment remains a
formidable task. The core challenge involves efficiently building and
integrating a domain-specific knowledge base for the agent's learning process.
This paper introduces a principled framework to address this challenge,
comprising a two-layer loop.In the inner loop, the agent refines its responses
by drawing from its knowledge base, while in the outer loop, these responses
are tested in real-world scenarios to automatically enhance the knowledge base
with new insights.We demonstrate that our approach enables the agent to
progressively approximate optimal behavior with provable
efficiency.Furthermore, we instantiate this framework through an autonomous
agent for mining trading signals named QuantAgent. Empirical results showcase
QuantAgent's capability in uncovering viable financial signals and enhancing
the accuracy of financial forecasts.
\\ ( https://arxiv.org/abs/2402.03755 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03822
Date: Tue, 6 Feb 2024 09:10:35 GMT   (2888kb,D)

Title: RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
Authors: Si Shen, Peijun Shen, Danhao Zhu
Categories: cs.AI cs.CL cs.LG
\\
  This paper presents RevOrder, a novel technique aimed at improving arithmetic
operations in large language models (LLMs) by reversing the output digits in
addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks.
Our method significantly reduces the Count of Sequential Intermediate Digits
(CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation
complexity. Through comprehensive testing, RevOrder not only achieves perfect
accuracy in basic arithmetic operations but also substantially boosts LLM
performance in division tasks, particularly with large numbers where
traditional models struggle. Implementation of RevOrder is cost-effective for
both training and inference phases. Moreover, applying RevOrder to fine-tune
the LLaMA2-7B model on the GSM8K math task results in a considerable
improvement, reducing equation calculation errors by 46% and increasing overall
scores from 41.6 to 44.4.
\\ ( https://arxiv.org/abs/2402.03822 ,  2888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03824
Date: Tue, 6 Feb 2024 09:11:20 GMT   (75kb)

Title: A call for embodied AI
Authors: Giuseppe Paolo, Jonas Gonzalez-Billandon, Bal\'azs K\'egl
Categories: cs.AI
Comments: Submitted to ICML 2024 Position paper track
\\
  We propose Embodied AI as the next fundamental step in the pursuit of
Artificial General Intelligence, juxtaposing it against current AI
advancements, particularly Large Language Models. We traverse the evolution of
the embodiment concept across diverse fields - philosophy, psychology,
neuroscience, and robotics - to highlight how EAI distinguishes itself from the
classical paradigm of static learning. By broadening the scope of Embodied AI,
we introduce a theoretical framework based on cognitive architectures,
emphasizing perception, action, memory, and learning as essential components of
an embodied agent. This framework is aligned with Friston's active inference
principle, offering a comprehensive approach to EAI development. Despite the
progress made in the field of AI, substantial challenges, such as the
formulation of a novel AI learning theory and the innovation of advanced
hardware, persist. Our discussion lays down a foundational guideline for future
Embodied AI research. Highlighting the importance of creating Embodied AI
agents capable of seamless communication, collaboration, and coexistence with
humans and other intelligent entities within real-world environments, we aim to
steer the AI community towards addressing the multifaceted challenges and
seizing the opportunities that lie ahead in the quest for AGI.
\\ ( https://arxiv.org/abs/2402.03824 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03962
Date: Tue, 6 Feb 2024 12:42:21 GMT   (8410kb,D)

Title: Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
Authors: Patrick Altmeyer, Andrew M. Demetriou, Antony Bartlett, Cynthia C. S.
  Liem
Categories: cs.AI cs.CL
Comments: 20 pages, 15 figures. Preliminary work. Under review by the
  International Conference on Machine Learning (ICML)
\\
  Humans have a tendency to see 'human'-like qualities in objects around them.
We name our cars, and talk to pets and even household appliances, as if they
could understand us as other humans do. This behavior, called anthropomorphism,
is also seeing traction in Machine Learning (ML), where human-like intelligence
is claimed to be perceived in Large Language Models (LLMs). In this position
paper, considering professional incentives, human biases, and general
methodological setups, we discuss how the current search for Artificial General
Intelligence (AGI) is a perfect storm for over-attributing human-like qualities
to LLMs. In several experiments, we demonstrate that the discovery of
human-interpretable patterns in latent spaces should not be a surprising
outcome. Also in consideration of common AI portrayal in the media, we call for
the academic community to exercise extra caution, and to be extra aware of
principles of academic integrity, in interpreting and communicating about AI
research outcomes.
\\ ( https://arxiv.org/abs/2402.03962 ,  8410kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04140
Date: Tue, 6 Feb 2024 16:47:34 GMT   (890kb)

Title: Advancing Legal Reasoning: The Integration of AI to Navigate
  Complexities and Biases in Global Jurisprudence with Semi-Automated
  Arbitration Processes (SAAPs)
Authors: Michael De'Shazer
Categories: cs.AI cs.CY cs.HC
\\
  This study consists of a novel approach toward the analysis of court
judgments spanning five countries, including the United States, the United
Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the
intersection of the latest advancements in artificial intelligence (AI) and
legal analysis, emphasizing the role of AI (specifically generative AI) in
identifying human biases and facilitating automated, valid, and coherent
multisided argumentation of court judgments with the goal of ensuring
consistent application of laws in and across various jurisdictions. By
incorporating Advanced Language Models (ALMs) and a newly introduced human-AI
collaborative framework, this paper seeks to analyze Grounded Theory-based
research design with Advanced Language Models (ALMs) in the practice of law.
SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT
technology), focusing on detecting logical inconsistencies and biases across
various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a
comparison-oriented AI-based application called SAM (also an ALM) to identify
relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated
within semi-autonomous arbitration process via the ALM, SARA. A novel approach
is introduced in the utilization of an AI arbitrator to critically evaluate
biases and qualitative-in-nature nuances identified by the aforementioned AI
applications (SAM in concert with SHIRLEY), based on the Hague Rules on
Business and Human Rights Arbitration. This Semi-Automated Arbitration Process
(SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring
a nuanced debate-resultant "understanding" through a hybrid system of AI and
human-based collaborative analysis.
\\ ( https://arxiv.org/abs/2402.04140 ,  890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04154
Date: Tue, 6 Feb 2024 17:09:25 GMT   (1540kb,D)

Title: Read to Play (R2-Play): Decision Transformer with Multimodal Game
  Instruction
Authors: Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu
  Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He and Jie Fu
Categories: cs.AI cs.LG
\\
  Developing a generalist agent is a longstanding objective in artificial
intelligence. Previous efforts utilizing extensive offline datasets from
various tasks demonstrate remarkable performance in multitasking scenarios
within Reinforcement Learning.However, these works encounter challenges in
extending their capabilities to new tasks.Recent approaches integrate textual
guidance or visual trajectory into decision networks to provide task-specific
contextual cues, representing a promising direction.However, it is observed
that relying solely on textual guidance or visual trajectory is insufficient
for accurately conveying the contextual information of tasks.This paper
explores enhanced forms of task guidance for agents, enabling them to
comprehend gameplay instructions, thereby facilitating a "read-to-play"
capability.Drawing inspiration from the success of multimodal instruction
tuning in visual tasks, we treat the visual-based RL task as a long-horizon
vision task and construct a set of multimodal game instructions to incorporate
instruction tuning into a decision transformer.Experimental results demonstrate
that incorporating multimodal game instructions significantly enhances the
decision transformer's multitasking and generalization capabilities.
\\ ( https://arxiv.org/abs/2402.04154 ,  1540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04203
Date: Tue, 6 Feb 2024 17:59:46 GMT   (2627kb,D)

Title: Human-Like Geometric Abstraction in Large Pre-trained Neural Networks
Authors: Declan Campbell, Sreejan Kumar, Tyler Giallanza, Thomas L. Griffiths,
  Jonathan D. Cohen
Categories: cs.AI q-bio.NC
\\
  Humans possess a remarkable capacity to recognize and manipulate abstract
structure, which is especially apparent in the domain of geometry. Recent
research in cognitive science suggests neural networks do not share this
capacity, concluding that human geometric abilities come from discrete symbolic
structure in human mental representations. However, progress in artificial
intelligence (AI) suggests that neural networks begin to demonstrate more
human-like reasoning after scaling up standard architectures in both model size
and amount of training data. In this study, we revisit empirical results in
cognitive science on geometric visual processing and identify three key biases
in geometric visual processing: a sensitivity towards complexity, regularity,
and the perception of parts and relations. We test tasks from the literature
that probe these biases in humans and find that large pre-trained neural
network models used in AI demonstrate more human-like abstract geometric
processing.
\\ ( https://arxiv.org/abs/2402.04203 ,  2627kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04210
Date: Tue, 6 Feb 2024 18:07:43 GMT   (32009kb,D)

Title: "Task Success" is not Enough: Investigating the Use of Video-Language
  Models as Behavior Critics for Catching Undesirable Agent Behaviors
Authors: Lin Guan, Yifan Zhou, Denis Liu, Yantian Zha, Heni Ben Amor, Subbarao
  Kambhampati
Categories: cs.AI cs.RO
\\
  Large-scale generative models are shown to be useful for sampling meaningful
candidate solutions, yet they often overlook task constraints and user
preferences. Their full power is better harnessed when the models are coupled
with external verifiers and the final solutions are derived iteratively or
progressively according to the verification feedback. In the context of
embodied AI, verification often solely involves assessing whether goal
conditions specified in the instructions have been met. Nonetheless, for these
agents to be seamlessly integrated into daily life, it is crucial to account
for a broader range of constraints and preferences beyond bare task success
(e.g., a robot should grasp bread with care to avoid significant deformations).
However, given the unbounded scope of robot tasks, it is infeasible to
construct scripted verifiers akin to those used for explicit-knowledge tasks
like the game of Go and theorem proving. This begs the question: when no sound
verifier is available, can we use large vision and language models (VLMs),
which are approximately omniscient, as scalable Behavior Critics to catch
undesirable robot behaviors in videos? To answer this, we first construct a
benchmark that contains diverse cases of goal-reaching yet undesirable robot
policies. Then, we comprehensively evaluate VLM critics to gain a deeper
understanding of their strengths and failure modes. Based on the evaluation, we
provide guidelines on how to effectively utilize VLM critiques and showcase a
practical way to integrate the feedback into an iterative process of policy
refinement. The dataset and codebase are released at:
https://guansuns.github.io/pages/vlm-critic.
\\ ( https://arxiv.org/abs/2402.04210 ,  32009kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04232
Date: Tue, 6 Feb 2024 18:39:43 GMT   (10104kb,D)

Title: Can Generative Agents Predict Emotion?
Authors: Ciaran Regan, Nanami Iwahashi, Shogo Tanaka, Mizuki Oka
Categories: cs.AI cs.CL
Comments: 14 pages, 6 figures
\\
  Large Language Models (LLMs) have demonstrated a number of human-like
abilities, however the empathic understanding and emotional state of LLMs is
yet to be aligned to that of humans. In this work, we investigate how the
emotional state of generative LLM agents evolves as they perceive new events,
introducing a novel architecture in which new experiences are compared to past
memories. Through this comparison, the agent gains the ability to understand
new experiences in context, which according to the appraisal theory of emotion
is vital in emotion creation. First, the agent perceives new experiences as
time series text data. After perceiving each new input, the agent generates a
summary of past relevant memories, referred to as the norm, and compares the
new experience to this norm. Through this comparison we can analyse how the
agent reacts to the new experience in context. The PANAS, a test of affect, is
administered to the agent, capturing the emotional state of the agent after the
perception of the new event. Finally, the new experience is then added to the
agents memory to be used in the creation of future norms. By creating multiple
experiences in natural language from emotionally charged situations, we test
the proposed architecture on a wide range of scenarios. The mixed results
suggests that introducing context can occasionally improve the emotional
alignment of the agent, but further study and comparison with human evaluators
is necessary. We hope that this paper is another step towards the alignment of
generative agents.
\\ ( https://arxiv.org/abs/2402.04232 ,  10104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03339
Date: Thu, 18 Jan 2024 06:11:06 GMT   (3986kb,D)

Title: Interplay of Semantic Communication and Knowledge Learning
Authors: Fei Ni, Bingyan Wang, Rongpeng Li, Zhifeng Zhao and Honggang Zhang
Categories: cs.CL
Comments: Contributing to a Wiley book, copyright might be transferred without
  further notice; And the paper "Knowledge Enhanced Semantic Communication
  Receiver" (available at arXiv:2302.07727) constitutes a segment of this work
\\
  In the swiftly advancing realm of communication technologies, Semantic
Communication (SemCom), which emphasizes knowledge understanding and
processing, has emerged as a hot topic. By integrating artificial intelligence
technologies, SemCom facilitates a profound understanding, analysis and
transmission of communication content. In this chapter, we clarify the means of
knowledge learning in SemCom with a particular focus on the utilization of
Knowledge Graphs (KGs). Specifically, we first review existing efforts that
combine SemCom with knowledge learning. Subsequently, we introduce a
KG-enhanced SemCom system, wherein the receiver is carefully calibrated to
leverage knowledge from its static knowledge base for ameliorating the decoding
performance. Contingent upon this framework, we further explore potential
approaches that can empower the system to operate in evolving knowledge base
more effectively. Furthermore, we investigate the possibility of integration
with Large Language Models (LLMs) for data augmentation, offering additional
perspective into the potential implementation means of SemCom. Extensive
numerical results demonstrate that the proposed framework yields superior
performance on top of the KG-enhanced decoding and manifests its versatility
under different scenarios.
\\ ( https://arxiv.org/abs/2402.03339 ,  3986kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03435
Date: Mon, 5 Feb 2024 19:00:02 GMT   (41kb,D)

Title: Psychological Assessments with Large Language Models: A Privacy-Focused
  and Cost-Effective Approach
Authors: Sergi Blanco-Cuaresma
Categories: cs.CL cs.AI cs.CY
Comments: Accepted to the Workshop on Computational Linguistics and Clinical
  Psychology (CLPsych) at EACL 2024
\\
  This study explores the use of Large Language Models (LLMs) to analyze text
comments from Reddit users, aiming to achieve two primary objectives: firstly,
to pinpoint critical excerpts that support a predefined psychological
assessment of suicidal risk; and secondly, to summarize the material to
substantiate the preassigned suicidal risk level. The work is circumscribed to
the use of "open-source" LLMs that can be run locally, thereby enhancing data
privacy. Furthermore, it prioritizes models with low computational
requirements, making it accessible to both individuals and institutions
operating on limited computing budgets. The implemented strategy only relies on
a carefully crafted prompt and a grammar to guide the LLM's text completion.
Despite its simplicity, the evaluation metrics show outstanding results, making
it a valuable privacy-focused and cost-effective approach. This work is part of
the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared
task.
\\ ( https://arxiv.org/abs/2402.03435 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03477
Date: Mon, 5 Feb 2024 19:39:07 GMT   (7814kb,D)

Title: Arabic Synonym BERT-based Adversarial Examples for Text Classification
Authors: Norah Alshahrani, Saied Alshahrani, Esma Wali, Jeanna Matthews
Categories: cs.CL
Comments: This paper is accepted at The 18th Conference of the European Chapter
  of the Association for Computational Linguistics (Student Research Workshop),
  March 17-22, 2024
\\
  Text classification systems have been proven vulnerable to adversarial text
examples, modified versions of the original text examples that are often
unnoticed by human eyes, yet can force text classification models to alter
their classification. Often, research works quantifying the impact of
adversarial text attacks have been applied only to models trained in English.
In this paper, we introduce the first word-level study of adversarial attacks
in Arabic. Specifically, we use a synonym (word-level) attack using a Masked
Language Modeling (MLM) task with a BERT model in a black-box setting to assess
the robustness of the state-of-the-art text classification models to
adversarial attacks in Arabic. To evaluate the grammatical and semantic
similarities of the newly produced adversarial examples using our synonym
BERT-based attack, we invite four human evaluators to assess and compare the
produced adversarial examples with their original examples. We also study the
transferability of these newly produced Arabic adversarial examples to various
models and investigate the effectiveness of defense mechanisms against these
adversarial examples on the BERT models. We find that fine-tuned BERT models
were more susceptible to our synonym attacks than the other Deep Neural
Networks (DNN) models like WordCNN and WordLSTM we trained. We also find that
fine-tuned BERT models were more susceptible to transferred attacks. We,
lastly, find that fine-tuned BERT models successfully regain at least 2% in
accuracy after applying adversarial training as an initial defense mechanism.
\\ ( https://arxiv.org/abs/2402.03477 ,  7814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03483
Date: Mon, 5 Feb 2024 19:55:06 GMT   (2383kb,D)

Title: SWAG: Storytelling With Action Guidance
Authors: Zeeshan Patel, Karim El-Refai, Jonathan Pei, Tianle Li
Categories: cs.CL cs.AI
\\
  Automated long-form story generation typically employs long-context large
language models (LLMs) for one-shot creation, which can produce cohesive but
not necessarily engaging content. We introduce Storytelling With Action
Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach
reduces story writing to a search problem through a two-model feedback loop:
one LLM generates story content, and another auxiliary LLM is used to choose
the next best "action" to steer the story's future direction. Our results show
that SWAG can substantially outperform previous end-to-end story generation
techniques when evaluated by GPT-4 and through human evaluation, and our SWAG
pipeline using only open-source models surpasses GPT-3.5-Turbo.
\\ ( https://arxiv.org/abs/2402.03483 ,  2383kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03509
Date: Mon, 5 Feb 2024 20:51:11 GMT   (1067kb,D)

Title: Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains
Authors: Sanjana Ramprasad, Kundan Krishna, Zachary C Lipton and Byron C
  Wallace
Categories: cs.CL cs.AI cs.LG
\\
  Recent work has shown that large language models (LLMs) are capable of
generating summaries zero-shot (i.e., without explicit supervision) that, under
human assessment, are often comparable or even preferred to manually composed
reference summaries. However, this prior work has focussed almost exclusively
on evaluating news article summarization. How do zero-shot summarizers perform
in other (potentially more specialized) domains? In this work we evaluate
zero-shot generated summaries across specialized domains including biomedical
articles, and legal bills (in addition to standard news benchmarks for
reference). We focus especially on the factuality of outputs. We acquire
annotations from domain experts to identify inconsistencies in summaries and
systematically categorize these errors. We analyze whether the prevalence of a
given domain in the pretraining corpus affects extractiveness and faithfulness
of generated summaries of articles in this domain. We release all collected
annotations to facilitate additional research toward measuring and realizing
factually accurate summarization, beyond news articles. The dataset can be
downloaded from https://github.com/sanjanaramprasad/zero_shot_faceval_domains
\\ ( https://arxiv.org/abs/2402.03509 ,  1067kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03519
Date: Mon, 5 Feb 2024 21:05:35 GMT   (7753kb,D)

Title: Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical
  System for Punctuation Restoration
Authors: Xiliang Zhu, Chia-Tien Chang, Shayna Gardiner, David Rossouw, Jonas
  Robertson
Categories: cs.CL cs.AI
Comments: Accepted to UnImplicit workshop at EACL 2024
\\
  Punctuation restoration is a crucial step after Automatic Speech Recognition
(ASR) systems to enhance transcript readability and facilitate subsequent NLP
tasks. Nevertheless, conventional lexical-based approaches are inadequate for
solving the punctuation restoration task in Spanish, where ambiguity can be
often found between unpunctuated declaratives and questions. In this study, we
propose a novel hybrid acoustic-lexical punctuation restoration system for
Spanish transcription, which consolidates acoustic and lexical signals through
a modular process. Our experiment results show that the proposed system can
effectively improve F1 score of question marks and overall punctuation
restoration on both public and internal Spanish conversational datasets.
Additionally, benchmark comparison against LLMs (Large Language Model)
indicates the superiority of our approach in accuracy, reliability and latency.
Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module
also benefits from our proposed system.
\\ ( https://arxiv.org/abs/2402.03519 ,  7753kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03597
Date: Tue, 6 Feb 2024 00:14:53 GMT   (1204kb)

Title: Identifying Reasons for Contraceptive Switching from Real-World Data
  Using Large Language Models
Authors: Brenda Y. Miao, Christopher YK Williams, Ebenezer Chinedu-Eneh, Travis
  Zack, Emily Alsentzer, Atul J. Butte, Irene Y. Chen
Categories: cs.CL cs.IR cs.LG
\\
  Prescription contraceptives play a critical role in supporting women's
reproductive health. With nearly 50 million women in the United States using
contraceptives, understanding the factors that drive contraceptives selection
and switching is of significant interest. However, many factors related to
medication switching are often only captured in unstructured clinical notes and
can be difficult to extract. Here, we evaluate the zero-shot abilities of a
recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft
Azure API), to identify reasons for switching between classes of contraceptives
from the UCSF Information Commons clinical notes dataset. We demonstrate that
GPT-4 can accurately extract reasons for contraceptive switching, outperforming
baseline BERT-based models with microF1 scores of 0.849 and 0.881 for
contraceptive start and stop extraction, respectively. Human evaluation of
GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal
hallucinations. Using extracted reasons, we identified patient preference,
adverse events, and insurance as key reasons for switching using unsupervised
topic modeling approaches. Notably, we also showed using our approach that
"weight gain/mood change" and "insurance coverage" are disproportionately found
as reasons for contraceptive switching in specific demographic populations. Our
code and supplemental data are available at
https://github.com/BMiao10/contraceptive-switching.
\\ ( https://arxiv.org/abs/2402.03597 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03616
Date: Tue, 6 Feb 2024 01:05:14 GMT   (179kb,D)

Title: Leveraging Large Language Models for Hybrid Workplace Decision Support
Authors: Yujin Kim, Chin-Chia Hsu
Categories: cs.CL cs.AI cs.HC cs.IR
\\
  Large Language Models (LLMs) hold the potential to perform a variety of text
processing tasks and provide textual explanations for proposed actions or
decisions. In the era of hybrid work, LLMs can provide intelligent decision
support for workers who are designing their hybrid work plans. In particular,
they can offer suggestions and explanations to workers balancing numerous
decision factors, thereby enhancing their work experience. In this paper, we
present a decision support model for workspaces in hybrid work environments,
leveraging the reasoning skill of LLMs. We first examine LLM's capability of
making suitable workspace suggestions. We find that its reasoning extends
beyond the guidelines in the prompt and the LLM can manage the trade-off among
the available resources in the workspaces. We conduct an extensive user study
to understand workers' decision process for workspace choices and evaluate the
effectiveness of the system. We observe that a worker's decision could be
influenced by the LLM's suggestions and explanations. The participants in our
study find the system to be convenient, regardless of whether reasons are
provided or not. Our results show that employees can benefit from the
LLM-empowered system for their workspace selection in hybrid workplace.
\\ ( https://arxiv.org/abs/2402.03616 ,  179kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03627
Date: Tue, 6 Feb 2024 01:44:38 GMT   (1190kb)

Title: Partially Recentralization Softmax Loss for Vision-Language Models
  Robustness
Authors: Hao Wang, Xin Zhang, Jinzhe Jiang, Yaqian Zhao and Chen Li
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\
  As Large Language Models make a breakthrough in natural language processing
tasks (NLP), multimodal technique becomes extremely popular. However, it has
been shown that multimodal NLP are vulnerable to adversarial attacks, where the
outputs of a model can be dramatically changed by a perturbation to the input.
While several defense techniques have been proposed both in computer vision and
NLP models, the multimodal robustness of models have not been fully explored.
In this paper, we study the adversarial robustness provided by modifying loss
function of pre-trained multimodal models, by restricting top K softmax
outputs. Based on the evaluation and scoring, our experiments show that after a
fine-tuning, adversarial robustness of pre-trained models can be significantly
improved, against popular attacks. Further research should be studying, such as
output diversity, generalization and the robustness-performance trade-off of
this kind of loss functions. Our code will be available after this paper is
accepted
\\ ( https://arxiv.org/abs/2402.03627 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03628
Date: Tue, 6 Feb 2024 01:48:53 GMT   (1681kb,D)

Title: Professional Agents -- Evolving Large Language Models into Autonomous
  Experts with Human-Level Competencies
Authors: Zhixuan Chu, Yan Wang, Feng Zhu, Lu Yu, Longfei Li, Jinjie Gu
Categories: cs.CL
Comments: 14 pages, 1 figure
\\
  The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4
has catalyzed remarkable advances in natural language processing, demonstrating
human-like language fluency and reasoning capacities. This position paper
introduces the concept of Professional Agents (PAgents), an application
framework harnessing LLM capabilities to create autonomous agents with
controllable, specialized, interactive, and professional-level competencies. We
posit that PAgents can reshape professional services through continuously
developed expertise. Our proposed PAgents framework entails a tri-layered
architecture for genesis, evolution, and synergy: a base tool layer, a middle
agent layer, and a top synergy layer. This paper aims to spur discourse on
promising real-world applications of LLMs. We argue the increasing
sophistication and integration of PAgents could lead to AI systems exhibiting
professional mastery over complex domains, serving critical needs, and
potentially achieving artificial general intelligence.
\\ ( https://arxiv.org/abs/2402.03628 ,  1681kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03642
Date: Tue, 6 Feb 2024 02:39:59 GMT   (354kb,D)

Title: Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish
  Misinformation
Authors: Anton Lavrouk, Ian Ligon, Tarek Naous, Jonathan Zheng, Alan Ritter,
  Wei Xu
Categories: cs.CL cs.CY cs.LG cs.SI
Comments: WNUT2024
\\
  The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide
high-quality, annotated, 5-way stance data extracted from Twitter, suitable for
analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus
2.0 iteration, we extend this framework to encompass Russian and Spanish. The
former is of current significance due to prevalent misinformation amid
escalating tensions with the West and the violent incursion into Ukraine. The
latter, meanwhile, represents an enormous community that has been largely
overlooked on major social media platforms. By incorporating an additional
3,874 Spanish and Russian tweets over 41 misinformation claims, our objective
is to support research focused on these issues. To demonstrate the value of
this data, we employed zero-shot cross-lingual transfer on multilingual BERT,
yielding results on par with the initial Stanceosaurus study with a macro F1
score of 43 for both languages. This underlines the viability of stance
classification as an effective tool for identifying multicultural
misinformation.
\\ ( https://arxiv.org/abs/2402.03642 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03658
Date: Tue, 6 Feb 2024 03:14:46 GMT   (916kb,D)

Title: Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue
Authors: Kun Ouyang and Liqiang Jing and Xuemeng Song and Meng Liu and Yupeng
  Hu and Liqiang Nie
Categories: cs.CL cs.MM
\\
  Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which
aims to generate a natural language explanation for the given sarcastic
dialogue that involves multiple modalities (i.e., utterance, video, and audio).
Although existing studies have achieved great success based on the generative
pretrained language model BART, they overlook exploiting the sentiments
residing in the utterance, video and audio, which are vital clues for sarcasm
explanation. In fact, it is non-trivial to incorporate sentiments for boosting
SED performance, due to three main challenges: 1) diverse effects of utterance
tokens on sentiments; 2) gap between video-audio sentiment signals and the
embedding space of BART; and 3) various relations among utterances, utterance
sentiments, and video-audio sentiments. To tackle these challenges, we propose
a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation
framework, named EDGE. In particular, we first propose a lexicon-guided
utterance sentiment inference module, where a heuristic utterance sentiment
refinement strategy is devised. We then develop a module named Joint Cross
Attention-based Sentiment Inference (JCA-SI) by extending the multimodal
sentiment analysis model JCA to derive the joint sentiment label for each
video-audio clip. Thereafter, we devise a context-sentiment graph to
comprehensively model the semantic relations among the utterances, utterance
sentiments, and video-audio sentiments, to facilitate sarcasm explanation
generation. Extensive experiments on the publicly released dataset WITS verify
the superiority of our model over cutting-edge methods.
\\ ( https://arxiv.org/abs/2402.03658 ,  916kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03667
Date: Tue, 6 Feb 2024 03:41:12 GMT   (1842kb,D)

Title: Large Language Models as an Indirect Reasoner: Contrapositive and
  Contradiction for Automated Reasoning
Authors: Yanfang Zhang, Yiliu Sun, Yibing Zhan, Dapeng Tao, Dacheng Tao, Chen
  Gong
Categories: cs.CL cs.AI
Comments: 20 pages,13 figures,4 tables
\\
  Recently, increasing attention has been focused drawn on to improve the
ability of Large Language Models (LLMs) to perform complex reasoning. However,
previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow
Direct Reasoning (DR) frameworks, so they will meet difficulty in solving
numerous real-world tasks which can hardly be solved via DR. Therefore, to
strengthen the reasoning power of LLMs, this paper proposes a novel Indirect
Reasoning (IR) method that employs the logic of contrapositives and
contradictions to tackle IR tasks such as factual reasoning and mathematic
proof. Specifically, our methodology comprises two steps. Firstly, we leverage
the logical equivalence of contrapositive to augment the data and rules to
enhance the comprehensibility of LLMs. Secondly, we design a set of prompt
templates to trigger LLMs to conduct IR based on proof by contradiction that is
logically equivalent to the original DR process. Our IR method is simple yet
effective and can be straightforwardly integrated with existing DR methods to
further boost the reasoning abilities of LLMs. The experimental results on
popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method
enhances the overall accuracy of factual reasoning by 27.33% and mathematical
proof by 31.43%, when compared with traditional DR methods. Moreover, the
methods combining IR and DR significantly outperform the methods solely using
IR or DR, further demonstrating the effectiveness of our strategy.
\\ ( https://arxiv.org/abs/2402.03667 ,  1842kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03686
Date: Tue, 6 Feb 2024 04:14:09 GMT   (835kb,D)

Title: Minds versus Machines: Rethinking Entailment Verification with Language
  Models
Authors: Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, Xiang Ren
Categories: cs.CL cs.AI
\\
  Humans make numerous inferences in text comprehension to understand
discourse. This paper aims to understand the commonalities and disparities in
the inference judgments between humans and state-of-the-art Large Language
Models (LLMs). Leveraging a comprehensively curated entailment verification
benchmark, we evaluate both human and LLM performance across various reasoning
categories. Our benchmark includes datasets from three categories (NLI,
contextual QA, and rationales) that include multi-sentence premises and
different knowledge types, thereby evaluating the inference capabilities in
complex reasoning instances. Notably, our findings reveal LLMs' superiority in
multi-hop reasoning across extended contexts, while humans excel in tasks
necessitating simple deductive reasoning. Leveraging these insights, we
introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with
GPT-4, offering a robust open-source solution for entailment verification. As a
practical application, we showcase the efficacy of our finetuned model in
enhancing self-consistency in model-generated explanations, resulting in a 6%
performance boost on average across three multiple-choice question-answering
datasets.
\\ ( https://arxiv.org/abs/2402.03686 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03719
Date: Tue, 6 Feb 2024 05:24:16 GMT   (1105kb,D)

Title: Empowering Language Models with Active Inquiry for Deeper Understanding
Authors: Jing-Cheng Pang, Heng-Bo Fan, Pengyuan Wang, Jia-Hao Xiao, Nan Tang,
  Si-Hang Yang, Chengxing Jia, Sheng-Jun Huang, Yang Yu
Categories: cs.CL cs.AI
\\
  The rise of large language models (LLMs) has revolutionized the way that we
interact with artificial intelligence systems through natural language.
However, LLMs often misinterpret user queries because of their uncertain
intention, leading to less helpful responses. In natural human interactions,
clarification is sought through targeted questioning to uncover obscure
information. Thus, in this paper, we introduce LaMAI (Language Model with
Active Inquiry), designed to endow LLMs with this same level of interactive
engagement. LaMAI leverages active learning techniques to raise the most
informative questions, fostering a dynamic bidirectional dialogue. This
approach not only narrows the contextual gap but also refines the output of the
LLMs, aligning it more closely with user expectations. Our empirical studies,
across a variety of complex datasets where LLMs have limited conversational
context, demonstrate the effectiveness of LaMAI. The method improves answer
accuracy from 31.9% to 50.9%, outperforming other leading question-answering
frameworks. Moreover, in scenarios involving human participants, LaMAI
consistently generates responses that are superior or comparable to baseline
methods in more than 82% of the cases. The applicability of LaMAI is further
evidenced by its successful integration with various LLMs, highlighting its
potential for the future of interactive language models.
\\ ( https://arxiv.org/abs/2402.03719 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03744
Date: Tue, 6 Feb 2024 06:23:12 GMT   (391kb,D)

Title: INSIDE: LLMs' Internal States Retain the Power of Hallucination
  Detection
Authors: Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu,
  Jieping Ye
Categories: cs.CL
Comments: Accepted by ICLR-2024
\\
  Knowledge hallucination have raised widespread concerns for the security and
reliability of deployed LLMs. Previous efforts in detecting hallucinations have
been employed at logit-level uncertainty estimation or language-level
self-consistency evaluation, where the semantic information is inevitably lost
during the token-decoding procedure. Thus, we propose to explore the dense
semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates
for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular,
a simple yet effective \textbf{EigenScore} metric is proposed to better
evaluate responses' self-consistency, which exploits the eigenvalues of
responses' covariance matrix to measure the semantic consistency/diversity in
the dense embedding space. Furthermore, from the perspective of self-consistent
hallucination detection, a test time feature clipping approach is explored to
truncate extreme activations in the internal states, which reduces
overconfident generations and potentially benefits the detection of
overconfident hallucinations. Extensive experiments and ablation studies are
performed on several popular LLMs and question-answering (QA) benchmarks,
showing the effectiveness of our proposal.
\\ ( https://arxiv.org/abs/2402.03744 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03776
Date: Tue, 6 Feb 2024 07:43:07 GMT   (39kb)

Title: Large Language Models As MOOCs Graders
Authors: Shahriar Golchin, Nikhil Garuda, Christopher Impey, Matthew Wenger
Categories: cs.CL cs.AI
Comments: v1 preprint
\\
  Massive open online courses (MOOCs) unlock the doors to free education for
anyone around the globe with access to a computer and the internet. Despite
this democratization of learning, the massive enrollment in these courses means
it is almost impossible for one instructor to assess every student's writing
assignment. As a result, peer grading, often guided by a straightforward
rubric, is the method of choice. While convenient, peer grading often falls
short in terms of reliability and validity. In this study, using 18 distinct
settings, we explore the feasibility of leveraging large language models (LLMs)
to replace peer grading in MOOCs. Specifically, we focus on two
state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses:
Introductory Astronomy, Astrobiology, and the History and Philosophy of
Astronomy. To instruct LLMs, we use three different prompts based on a variant
of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique:
Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT
in conjunction with both instructor-formulated answers and rubrics; and
Zero-shot-CoT with instructor-offered correct answers and LLM-generated
rubrics. Our results show that Zero-shot-CoT, when integrated with
instructor-provided answers and rubrics, produces grades that are more aligned
with those assigned by instructors compared to peer grading. However, the
History and Philosophy of Astronomy course proves to be more challenging in
terms of grading as opposed to other courses. Finally, our study reveals a
promising direction for automating grading systems for MOOCs, especially in
subjects with well-defined rubrics.
\\ ( https://arxiv.org/abs/2402.03776 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03780
Date: Tue, 6 Feb 2024 07:51:54 GMT   (2305kb,D)

Title: Exposing propaganda: an analysis of stylistic cues comparing human
  annotations and machine classification
Authors: G\'eraud Faye, Benjamin Icard, Morgane Casanova, Julien Chanson,
  Fran\c{c}ois Maine, Fran\c{c}ois Bancilhon, Guillaume Gadek, Guillaume
  Gravier, Paul \'Egr\'e
Categories: cs.CL cs.AI cs.LG
Comments: Paper to appear in the EACL 2024 Proceedings of the Third Workshop on
  Understanding Implicit and Underspecified Language (UnImplicit 2024)
\\
  This paper investigates the language of propaganda and its stylistic
features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a
multisource, multilingual, multimodal dataset composed of news articles
extracted from websites identified as propaganda sources by expert agencies. A
limited sample from this set was randomly mixed with papers from the regular
French press, and their URL masked, to conduct an annotation-experiment by
humans, using 11 distinct labels. The results show that human annotators were
able to reliably discriminate between the two types of press across each of the
labels. We propose different NLP techniques to identify the cues used by the
annotators, and to compare them with machine classification. They include the
analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to
serve as a baseline, and four different classifiers: two RoBERTa-based models,
CATS using syntax, and one XGBoost combining syntactic and semantic features.
  Keywords: Propaganda, Fake News, Explainability, AI alignment, Vagueness,
Subjectivity, Exaggeration, Stylistic analysis
\\ ( https://arxiv.org/abs/2402.03780 ,  2305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03782
Date: Tue, 6 Feb 2024 07:52:30 GMT   (2630kb,D)

Title: Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More
Authors: Fred Philippy, Siwen Guo, Shohreh Haddadan, Cedric Lothritz, Jacques
  Klein, Tegawend\'e F. Bissyand\'e
Categories: cs.CL cs.AI
Comments: Accepted at the 1st Workshop on Modular and Open Multilingual NLP
  (co-located with EACL 2024)
\\
  Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting
pre-trained language models (PLMs) to specific tasks by inserting learnable
embeddings, or soft prompts, at the input layer of the PLM, without modifying
its parameters. This paper investigates the potential of SPT for cross-lingual
transfer. Unlike previous studies on SPT for cross-lingual transfer that often
fine-tune both the soft prompt and the model parameters, we adhere to the
original intent of SPT by keeping the model parameters frozen and only training
the soft prompt. This does not only reduce the computational cost and storage
overhead of full-model fine-tuning, but we also demonstrate that this very
parameter efficiency intrinsic to SPT can enhance cross-lingual transfer
performance to linguistically distant languages. Moreover, we explore how
different factors related to the prompt, such as the length or its
reparameterization, affect cross-lingual transfer performance.
\\ ( https://arxiv.org/abs/2402.03782 ,  2630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03832
Date: Tue, 6 Feb 2024 09:23:26 GMT   (8063kb,D)

Title: Rethinking Skill Extraction in the Job Market Domain using Large
  Language Models
Authors: Khanh Cao Nguyen, Mike Zhang, Syrielle Montariol, Antoine Bosselut
Categories: cs.CL
Comments: Published at NLP4HR 2024 (EACL Workshop)
\\
  Skill Extraction involves identifying skills and qualifications mentioned in
documents such as job postings and resumes. The task is commonly tackled by
training supervised models using a sequence labeling approach with BIO tags.
However, the reliance on manually annotated data limits the generalizability of
such approaches. Moreover, the common BIO setting limits the ability of the
models to capture complex skill patterns and handle ambiguous mentions. In this
paper, we explore the use of in-context learning to overcome these challenges,
on a benchmark of 6 uniformized skill extraction datasets. Our approach
leverages the few-shot learning capabilities of large language models (LLMs) to
identify and extract skills from sentences. We show that LLMs, despite not
being on par with traditional supervised models in terms of performance, can
better handle syntactically complex skill mentions in skill extraction tasks.
\\ ( https://arxiv.org/abs/2402.03832 ,  8063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03848
Date: Tue, 6 Feb 2024 09:50:08 GMT   (184kb,D)

Title: ANLS* -- A Universal Document Processing Metric for Generative Large
  Language Models
Authors: David Peer, Philemon Sch\"opf, Volckmar Nebendahl, Alexander Rietzler,
  Sebastian Stabinger
Categories: cs.CL cs.AI
\\
  Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs. This paper introduces a new metric for generative models called ANLS*
for evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets and 3 different GLLMs using the
ANLS* metric is also provided, demonstrating the importance of the proposed
metric. We also benchmark a novel approach to generate prompts for documents,
called SFT, against other prompting techniques such as LATIN. In 15 out of 21
cases, SFT outperforms other techniques and improves the state-of-the-art,
sometimes by as much as $15$ percentage points.
  Sources are available at https://github.com/deepopinion/anls_star_metric
\\ ( https://arxiv.org/abs/2402.03848 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03870
Date: Tue, 6 Feb 2024 10:32:34 GMT   (965kb)

Title: Less than one percent of words would be affected by gender-inclusive
  language in German press texts
Authors: Carolin M\"uller-Spitzer, Samira Ochs, Alexander Koplenig, Jan-Oliver
  R\"udiger, Sascha Wolfer
Categories: cs.CL stat.AP
Comments: 27 pages, 7 figures, 2 tables
\\
  Research on gender and language is tightly knitted to social debates on
gender equality and non-discriminatory language use. Psycholinguistic scholars
have made significant contributions in this field. However, corpus-based
studies that investigate these matters within the context of language use are
still rare. In our study, we address the question of how much textual material
would actually have to be changed if non-gender-inclusive texts were rewritten
to be gender-inclusive. This quantitative measure is an important empirical
insight, as a recurring argument against the use of gender-inclusive German is
that it supposedly makes written texts too long and complicated. It is also
argued that gender-inclusive language has negative effects on language
learners. However, such effects are only likely if gender-inclusive texts are
very different from those that are not gender-inclusive. In our
corpus-linguistic study, we manually annotated German press texts to identify
the parts that would have to be changed. Our results show that, on average,
less than 1% of all tokens would be affected by gender-inclusive language. This
small proportion calls into question whether gender-inclusive German presents a
substantial barrier to understanding and learning the language, particularly
when we take into account the potential complexities of interpreting masculine
generics.
\\ ( https://arxiv.org/abs/2402.03870 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03877
Date: Tue, 6 Feb 2024 10:37:21 GMT   (18407kb,D)

Title: Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large
  Language Models
Authors: Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski
Categories: cs.CL cs.AI
Comments: Preprint. Work in progress
\\
  Large Language Models (LLMs) demonstrate ever-increasing abilities in
mathematical and algorithmic tasks, yet their geometric reasoning skills are
underexplored. We investigate LLMs' abilities in constructive geometric
problem-solving one of the most fundamental steps in the development of human
mathematical reasoning. Our work reveals notable challenges that the
state-of-the-art LLMs face in this domain despite many successes in similar
areas. LLMs exhibit biases in target variable selection and struggle with 2D
spatial relationships, often misrepresenting and hallucinating objects and
their placements. To this end, we introduce a framework that formulates an
LLMs-based multi-agents system that enhances their existing reasoning potential
by conducting an internal dialogue. This work underscores LLMs' current
limitations in geometric reasoning and improves geometric reasoning
capabilities through self-correction, collaboration, and diverse role
specializations.
\\ ( https://arxiv.org/abs/2402.03877 ,  18407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03887
Date: Tue, 6 Feb 2024 10:49:28 GMT   (1262kb)

Title: Shifting social norms as a driving force for linguistic change:
  Struggles about language and gender in the German Bundestag
Authors: Carolin M\"uller-Spitzer, Samira Ochs
Categories: cs.CL
Comments: 40 pages, 9 figures
\\
  This paper focuses on language change based on shifting social norms, in
particular with regard to the debate on language and gender. It is a recurring
argument in this debate that language develops "naturally" and that "severe
interventions" - such as gender-inclusive language is often claimed to be - in
the allegedly "organic" language system are inappropriate and even "dangerous".
Such interventions are, however, not unprecedented. Socially motivated
processes of language change are neither unusual nor new. We focus in our
contribution on one important political-social space in Germany, the German
Bundestag. Taking other struggles about language and gender in the plenaries of
the Bundestag as a starting point, our article illustrates that language and
gender has been a recurring issue in the German Bundestag since the 1980s. We
demonstrate how this is reflected in linguistic practices of the Bundestag, by
the use of a) designations for gays and lesbians; b) pair forms such as
B\"urgerinnen und B\"urger (female and male citizens); and c) female forms of
addresses and personal nouns ('Pr\"asidentin' in addition to 'Pr\"asident').
Lastly, we will discuss implications of these earlier language battles for the
currently very heated debate about gender-inclusive language, especially
regarding new forms with gender symbols like the asterisk or the colon
(Lehrer*innen, Lehrer:innen; male*female teachers) which are intended to
encompass all gender identities.
\\ ( https://arxiv.org/abs/2402.03887 ,  1262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03898
Date: Tue, 6 Feb 2024 11:10:35 GMT   (1690kb,D)

Title: DistiLLM: Towards Streamlined Distillation for Large Language Models
Authors: Jongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young Yun
Categories: cs.CL cs.AI cs.LG
Comments: Code is available at https://github.com/jongwooko/distillm
\\
  Knowledge distillation (KD) is widely used for compressing a teacher model to
a smaller student model, reducing its inference cost and memory footprint while
preserving model capabilities. However, current KD methods for auto-regressive
sequence models (e.g., large language models) suffer from missing a
standardized objective function. Moreover, the recent use of student-generated
outputs to address training-inference mismatches has significantly escalated
computational costs. To tackle these issues, we introduce DistiLLM, a more
effective and efficient KD framework for auto-regressive language models.
DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence
loss, where we unveil and leverage its theoretical properties, and (2) an
adaptive off-policy approach designed to enhance the efficiency in utilizing
student-generated outputs. Extensive experiments, including
instruction-following tasks, demonstrate the effectiveness of DistiLLM in
building high-performing student models while achieving up to 4.3$\times$
speedup compared to recent KD methods.
\\ ( https://arxiv.org/abs/2402.03898 ,  1690kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03900
Date: Tue, 6 Feb 2024 11:12:09 GMT   (64kb,D)

Title: Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based
  Spoken Language Understanding
Authors: Dechuan Teng, Chunlin Lu, Xiao Xu, Wanxiang Che, Libo Qin
Categories: cs.CL
Comments: Accepted at ICASSP 2024
\\
  Recently, Profile-based Spoken Language Understanding (SLU) has gained
increasing attention, which aims to incorporate various types of supplementary
profile information (i.e., Knowledge Graph, User Profile, Context Awareness) to
eliminate the prevalent ambiguities in user utterances. However, existing
approaches can only separately model different profile information, without
considering their interrelationships or excluding irrelevant and conflicting
information within them. To address the above issues, we introduce a
Heterogeneous Graph Attention Network to perform reasoning across multiple
Profile information, called Pro-HAN. Specifically, we design three types of
edges, denoted as intra-Pro, inter-Pro, and utterance-Pro, to capture
interrelationships among multiple Pros. We establish a new state-of-the-art on
the ProSLU dataset, with an improvement of approximately 8% across all three
metrics. Further analysis experiments also confirm the effectiveness of our
method in modeling multi-source profile information.
\\ ( https://arxiv.org/abs/2402.03900 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03927
Date: Tue, 6 Feb 2024 11:54:23 GMT   (9055kb,D)

Title: Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in
  Closed-Source LLMs
Authors: Simone Balloccu, Patr\'icia Schmidtov\'a, Mateusz Lango, and
  Ond\v{r}ej Du\v{s}ek
Categories: cs.CL cs.AI
\\
  Natural Language Processing (NLP) research is increasingly focusing on the
use of Large Language Models (LLMs), with some of the most popular ones being
either fully or partially closed-source. The lack of access to model details,
especially regarding training data, has repeatedly raised concerns about data
contamination among researchers. Several attempts have been made to address
this issue, but they are limited to anecdotal evidence and trial and error.
Additionally, they overlook the problem of \emph{indirect} data leaking, where
models are iteratively improved by using data coming from users. In this work,
we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and
GPT-4, the most prominently used LLMs today, in the context of data
contamination. By analysing 255 papers and considering OpenAI's data usage
policy, we extensively document the amount of data leaked to these models
during the first year after the model's release. We report that these models
have been globally exposed to $\sim$4.7M samples from 263 benchmarks. At the
same time, we document a number of evaluation malpractices emerging in the
reviewed papers, such as unfair or missing baseline comparisons and
reproducibility issues. We release our results as a collaborative project on
https://leak-llm.github.io/, where other researchers can contribute to our
efforts.
\\ ( https://arxiv.org/abs/2402.03927 ,  9055kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03957
Date: Tue, 6 Feb 2024 12:34:15 GMT   (7602kb,D)

Title: Sparse Graph Representations for Procedural Instructional Documents
Authors: Shruti Singh and Rishabh Gupta
Categories: cs.CL
\\
  Computation of document similarity is a critical task in various NLP domains
that has applications in deduplication, matching, and recommendation.
Traditional approaches for document similarity computation include learning
representations of documents and employing a similarity or a distance function
over the embeddings. However, pairwise similarities and differences are not
efficiently captured by individual representations. Graph representations such
as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a
joint undirected weighted graph. JCIGs facilitate an interpretable
representation of document pairs as a graph. However, JCIGs are undirected, and
don't consider the sequential flow of sentences in documents. We propose two
approaches to model document similarity by representing document pairs as a
directed and sparse JCIG that incorporates sequential information. We propose
two algorithms inspired by Supergenome Sorting and Hamiltonian Path that
replace the undirected edges with directed edges. Our approach also sparsifies
the graph to $O(n)$ edges from JCIG's worst case of $O(n^2)$. We show that our
sparse directed graph model architecture consisting of a Siamese encoder and
GCN achieves comparable results to the baseline on datasets not containing
sequential information and beats the baseline by ten points on an instructional
documents dataset containing sequential information.
\\ ( https://arxiv.org/abs/2402.03957 ,  7602kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04023
Date: Tue, 6 Feb 2024 14:16:32 GMT   (839kb)

Title: Google Translate Error Analysis for Mental Healthcare Information:
  Evaluating Accuracy, Comprehensibility, and Implications for Multilingual
  Healthcare Communication
Authors: Jaleh Delfani, Constantin Orasan, Hadeel Saadany, Ozlem Temizoz,
  Eleanor Taylor-Stilgoe, Diptesh Kanojia, Sabine Braun, Barbara Schouten
Categories: cs.CL
\\
  This study explores the use of Google Translate (GT) for translating mental
healthcare (MHealth) information and evaluates its accuracy, comprehensibility,
and implications for multilingual healthcare communication through analysing GT
output in the MHealth domain from English to Persian, Arabic, Turkish,
Romanian, and Spanish. Two datasets comprising MHealth information from the UK
National Health Service website and information leaflets from The Royal College
of Psychiatrists were used. Native speakers of the target languages manually
assessed the GT translations, focusing on medical terminology accuracy,
comprehensibility, and critical syntactic/semantic errors. GT output analysis
revealed challenges in accurately translating medical terminology, particularly
in Arabic, Romanian, and Persian. Fluency issues were prevalent across various
languages, affecting comprehension, mainly in Arabic and Spanish. Critical
errors arose in specific contexts, such as bullet-point formatting,
specifically in Persian, Turkish, and Romanian. Although improvements are seen
in longer-text translations, there remains a need to enhance accuracy in
medical and mental health terminology and fluency, whilst also addressing
formatting issues for a more seamless user experience. The findings highlight
the need to use customised translation engines for Mhealth translation and the
challenges when relying solely on machine-translated medical content,
emphasising the crucial role of human reviewers in multilingual healthcare
communication.
\\ ( https://arxiv.org/abs/2402.04023 ,  839kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04028
Date: Tue, 6 Feb 2024 14:24:28 GMT   (41kb,D)

Title: AlbNews: A Corpus of Headlines for Topic Modeling in Albanian
Authors: Erion \c{C}ano, Dario Lamaj
Categories: cs.CL cs.AI cs.LG
\\
  The scarcity of available text corpora for low-resource languages like
Albanian is a serious hurdle for research in natural language processing tasks.
This paper introduces AlbNews, a collection of 600 topically labeled news
headlines and 2600 unlabeled ones in Albanian. The data can be freely used for
conducting topic modeling research. We report the initial classification scores
of some traditional machine learning classifiers trained with the AlbNews
samples. These results show that basic models outrun the ensemble learning ones
and can serve as a baseline for future experiments.
\\ ( https://arxiv.org/abs/2402.04028 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04049
Date: Tue, 6 Feb 2024 14:51:55 GMT   (719kb,D)

Title: Systematic Biases in LLM Simulations of Debates
Authors: Amir Taubenfeld, Yaniv Dover, Roi Reichart, Ariel Goldstein
Categories: cs.CL cs.AI
\\
  Recent advancements in natural language processing, especially the emergence
of Large Language Models (LLMs), have opened exciting possibilities for
constructing computational simulations designed to replicate human behavior
accurately. However, LLMs are complex statistical learners without
straightforward deductive rules, making them prone to unexpected behaviors. In
this study, we highlight the limitations of LLMs in simulating human
interactions, particularly focusing on LLMs' ability to simulate political
debates. Our findings indicate a tendency for LLM agents to conform to the
model's inherent social biases despite being directed to debate from certain
political perspectives. This tendency results in behavioral patterns that seem
to deviate from well-established social dynamics among humans. We reinforce
these observations using an automatic self-fine-tuning method, which enables us
to manipulate the biases within the LLM and demonstrate that agents
subsequently align with the altered biases. These results underscore the need
for further research to develop methods that help agents overcome these biases,
a critical step toward creating more realistic simulations.
\\ ( https://arxiv.org/abs/2402.04049 ,  719kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04075
Date: Tue, 6 Feb 2024 15:25:09 GMT   (327kb,D)

Title: Iterative Prompt Refinement for Radiation Oncology Symptom Extraction
  Using Teacher-Student Large Language Models
Authors: Reza Khanmohammadi, Ahmed I Ghanem, Kyle Verdecchia, Ryan Hall,
  Mohamed Elshaikh, Benjamin Movsas, Hassan Bagher-Ebadian, Indrin Chetty,
  Mohammad M. Ghassemi, Kundan Thind
Categories: cs.CL
\\
  This study introduces a novel teacher-student architecture utilizing Large
Language Models (LLMs) to improve prostate cancer radiotherapy symptom
extraction from clinical notes. Mixtral, the student model, initially extracts
symptoms, followed by GPT-4, the teacher model, which refines prompts based on
Mixtral's performance. This iterative process involved 294 single symptom
clinical notes across 12 symptoms, with up to 16 rounds of refinement per
epoch. Results showed significant improvements in extracting symptoms from both
single and multi-symptom notes. For 59 single symptom notes, accuracy increased
from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and
F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24
to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score
from 0.20 to 0.44. These results demonstrate the effectiveness of advanced
prompt engineering in LLMs for radiation oncology use.
\\ ( https://arxiv.org/abs/2402.04075 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04088
Date: Tue, 6 Feb 2024 15:46:31 GMT   (790kb)

Title: The Use of a Large Language Model for Cyberbullying Detection
Authors: Bayode Ogunleye, Babitha Dharmaraj
Categories: cs.CL cs.AI cs.LG stat.AP
Comments: 14 pages, Journal of Analytics
ACM-class: H.3.3
Journal-ref: Analytics 2 (2023), no. 3: 694-707
DOI: 10.3390/analytics2030038
\\
  The dominance of social media has added to the channels of bullying for
perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent
phenomenon in todays cyber world, and is a severe threat to the mental and
physical health of citizens. This opens the need to develop a robust system to
prevent bullying content from online forums, blogs, and social media platforms
to manage the impact in our society. Several machine learning (ML) algorithms
have been proposed for this purpose. However, their performances are not
consistent due to high class imbalance and generalisation issues. In recent
years, large language models (LLMs) like BERT and RoBERTa have achieved
state-of-the-art (SOTA) results in several natural language processing (NLP)
tasks. Unfortunately, the LLMs have not been applied extensively for CB
detection. In our paper, we explored the use of these models for cyberbullying
(CB) detection. We have prepared a new dataset (D2) from existing studies
(Formspring and Twitter). Our experimental results for dataset D1 and D2 showed
that RoBERTa outperformed other models.
\\ ( https://arxiv.org/abs/2402.04088 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04110
Date: Tue, 6 Feb 2024 16:03:57 GMT   (606kb,D)

Title: Behind the Screen: Investigating ChatGPT's Dark Personality Traits and
  Conspiracy Beliefs
Authors: Erik Weber, J\'er\^ome Rutinowski, Markus Pauly
Categories: cs.CL
Comments: 15 pages, 5 figures
\\
  ChatGPT is notorious for its intransparent behavior. This paper tries to shed
light on this, providing an in-depth analysis of the dark personality traits
and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and
questionnaires were employed, including the Dark Factor Test, the Mach-IV
Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale.
The responses were analyzed computing average scores, standard deviations, and
significance tests to investigate differences between GPT-3.5 and GPT-4. For
traits that have shown to be interdependent in human studies, correlations were
considered. Additionally, system roles corresponding to groups that have shown
distinct answering behavior in the corresponding questionnaires were applied to
examine the models' ability to reflect characteristics associated with these
roles in their responses. Dark personality traits and conspiracy beliefs were
not particularly pronounced in either model with little differences between
GPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in
information withholding. This is particularly intriguing given that GPT-4 is
trained on a significantly larger dataset than GPT-3.5. Apparently, in this
case an increased data exposure correlates with a greater belief in the control
of information. An assignment of extreme political affiliations increased the
belief in conspiracy theories. Test sequencing affected the models' responses
and the observed correlations, indicating a form of contextual memory.
\\ ( https://arxiv.org/abs/2402.04110 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04160
Date: Tue, 6 Feb 2024 17:18:25 GMT   (2287kb,D)

Title: Harnessing the Plug-and-Play Controller by Prompting
Authors: Hao Wang, Lei Sha
Categories: cs.CL
Comments: The Third Version of the Generation, Evaluation & Metrics (GEM)
  Workshop in EMNLP 2023
\\
  Controllable text generation is a growing field within natural language
generation (NLG) that focuses on producing text that meets specific constraints
in real-world applications. Previous approaches, such as plug-and-play
controllers (PPCs), aimed to steer the properties of generated text in a
flexible manner. However, these methods often compromised the integrity of the
language model's decoding process, resulting in less smooth text generation.
Alternatively, other techniques utilized multiple attribute prompts to align
the generated text with desired attributes, but this approach required prompt
design for each attribute and was dependent on the size of the language model.
This paper introduces a novel method for flexible attribute control in text
generation using pre-trained language models (PLMs). The proposed approach aims
to enhance the fluency of generated text by guiding the generation process with
PPCs. The key idea is to dynamically adjust the distribution of generated text
by modifying prompts, effectively constraining the output space of the language
model and influencing the desired attribute. To enable smooth cooperation
between the PLM and the PPC, our work innovatively proposes a new model
fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback
(RLDAF).This fine-tuning process adapts a small subset of the language model's
parameters based on the generating actions taken during the PPC control
process. The resulting harmonious collaboration between the PLM and PPC leads
to improved smoothness in text generation during inference. Extensive
experiments were conducted on the SST2 dataset, and the proposed method
outperformed previous approaches in various evaluation metrics, including text
fluency and attribute consistency.
\\ ( https://arxiv.org/abs/2402.04160 ,  2287kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04177
Date: Tue, 6 Feb 2024 17:31:20 GMT   (991kb,D)

Title: Scaling Laws for Downstream Task Performance of Large Language Models
Authors: Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas,
  Sergei Vassilvitskii, Sanmi Koyejo
Categories: cs.CL cs.LG stat.ML
\\
  Scaling laws provide important insights that can guide the design of large
language models (LLMs). Existing work has primarily focused on studying scaling
laws for pretraining (upstream) loss. However, in transfer learning settings,
in which LLMs are pretrained on an unsupervised dataset and then finetuned on a
downstream task, we often also care about the downstream performance. In this
work, we study the scaling behavior in a transfer learning setting, where LLMs
are finetuned for machine translation tasks. Specifically, we investigate how
the choice of the pretraining data and its size affect downstream performance
(translation quality) as judged by two metrics: downstream cross-entropy and
BLEU score. Our experiments indicate that the size of the finetuning dataset
and the distribution alignment between the pretraining and downstream data
significantly influence the scaling behavior. With sufficient alignment, both
downstream cross-entropy and BLEU score improve monotonically with more
pretraining data. In such cases, we show that it is possible to predict the
downstream BLEU score with good accuracy using a log-law. However, there are
also cases where moderate misalignment causes the BLEU score to fluctuate or
get worse with more pretraining, whereas downstream cross-entropy monotonically
improves. By analyzing these observations, we provide new practical insights
for choosing appropriate pretraining data.
\\ ( https://arxiv.org/abs/2402.04177 ,  991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04222
Date: Tue, 6 Feb 2024 18:29:39 GMT   (1574kb,D)

Title: What is 'Typological Diversity' in NLP?
Authors: Esther Ploeger, Wessel Poelman, Miryam de Lhoneux, Johannes Bjerva
Categories: cs.CL
\\
  The NLP research community has devoted increased attention to languages
beyond English, resulting in considerable improvements for multilingual NLP.
However, these improvements only apply to a small subset of the world's
languages. Aiming to extend this, an increasing number of papers aspires to
enhance generalizable multilingual performance across languages. To this end,
linguistic typology is commonly used to motivate language selection, on the
basis that a broad typological sample ought to imply generalization across a
broad range of languages. These selections are often described as being
'typologically diverse'. In this work, we systematically investigate NLP
research that includes claims regarding 'typological diversity'. We find there
are no set definitions or criteria for such claims. We introduce metrics to
approximate the diversity of language selection along several axes and find
that the results vary considerably across papers. Furthermore, we show that
skewed language selection can lead to overestimated multilingual performance.
We recommend future work to include an operationalization of 'typological
diversity' that empirically justifies the diversity of language samples.
\\ ( https://arxiv.org/abs/2402.04222 ,  1574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04251
Date: Tue, 6 Feb 2024 18:59:30 GMT   (47kb,D)

Title: Linear-time Minimum Bayes Risk Decoding with Reference Aggregation
Authors: Jannis Vamvas and Rico Sennrich
Categories: cs.CL
\\
  Minimum Bayes Risk (MBR) decoding is a text generation technique that has
been shown to improve the quality of machine translations, but is expensive,
even if a sampling-based approximation is used. Besides requiring a large
number of sampled sequences, it requires the pairwise calculation of a utility
metric, which has quadratic complexity. In this paper, we propose to
approximate pairwise metric scores with scores calculated against aggregated
reference representations. This changes the complexity of utility estimation
from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains
of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr
\\ ( https://arxiv.org/abs/2402.04251 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04253
Date: Tue, 6 Feb 2024 18:59:57 GMT   (751kb,D)

Title: AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls
Authors: Yu Du, Fangyun Wei, Hongyang Zhang
Categories: cs.CL
\\
  We introduce AnyTool, a large language model agent designed to revolutionize
the utilization of a vast array of tools in addressing user queries. We utilize
over 16,000 APIs from Rapid API, operating under the assumption that a subset
of these APIs could potentially resolve the queries. AnyTool primarily
incorporates three elements: an API retriever with a hierarchical structure, a
solver aimed at resolving user queries using a selected set of API candidates,
and a self-reflection mechanism, which re-activates AnyTool if the initial
solution proves impracticable. AnyTool is powered by the function calling
feature of GPT-4, eliminating the need for training external modules. We also
revisit the evaluation protocol introduced by previous works and identify a
limitation in this protocol that leads to an artificially high pass rate. By
revising the evaluation protocol to better reflect practical application
scenarios, we introduce an additional benchmark, termed AnyToolBench.
Experiments across various datasets demonstrate the superiority of our AnyTool
over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool
utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of
average pass rate on ToolBench. Code will be available at
https://github.com/dyabel/AnyTool.
\\ ( https://arxiv.org/abs/2402.04253 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03386
Date: Sun, 4 Feb 2024 09:22:52 GMT   (1685kb)

Title: A generalized decision tree ensemble based on the NeuralNetworks
  architecture: Distributed Gradient Boosting Forest (DGBF)
Authors: \'Angel Delgado-Panadero, Jos\'e Alberto Ben\'itez-Andrades and
  Mar\'ia Teresa Garc\'ia-Ord\'as
Categories: cs.LG cs.AI
Journal-ref: Applied Intelligence, Volume 53, July 2023, pages 22991-23003
DOI: 10.1007/s10489-023-04735-w
\\
  Tree ensemble algorithms as RandomForest and GradientBoosting are currently
the dominant methods for modeling discrete or tabular data, however, they are
unable to perform a hierarchical representation learning from raw data as
NeuralNetworks does thanks to its multi-layered structure, which is a key
feature for DeepLearning problems and modeling unstructured data. This
limitation is due to the fact that tree algorithms can not be trained with
back-propagation because of their mathematical nature. However, in this work,
we demonstrate that the mathematical formulation of bagging and boosting can be
combined together to define a graph-structured-tree-ensemble algorithm with a
distributed representation learning process between trees naturally (without
using back-propagation). We call this novel approach Distributed Gradient
Boosting Forest (DGBF) and we demonstrate that both RandomForest and
GradientBoosting can be expressed as particular graph architectures of DGBT.
Finally, we see that the distributed learning outperforms both RandomForest and
GradientBoosting in 7 out of 9 datasets.
\\ ( https://arxiv.org/abs/2402.03386 ,  1685kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03448
Date: Mon, 5 Feb 2024 19:02:19 GMT   (275kb,D)

Title: Decentralized Sporadic Federated Learning: A Unified Methodology with
  Generalized Convergence Guarantees
Authors: Shahryar Zehtabi, Dong-Jun Han, Rohit Parasnis, Seyyedali
  Hosseinalipour, Christopher G. Brinton
Categories: cs.LG cs.DC
\\
  Decentralized Federated Learning (DFL) has received significant recent
research attention, capturing settings where both model updates and model
aggregations -- the two key FL processes -- are conducted by the clients. In
this work, we propose Decentralized Sporadic Federated Learning
($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of
sporadicity in both of these processes, modeling the impact of different forms
of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$
unifies many of the prominent decentralized optimization methods, e.g.,
distributed gradient descent (DGD), randomized gossip (RG), and decentralized
federated averaging (DFedAvg), under a single modeling framework. We
analytically characterize the convergence behavior of $\texttt{DSpodFL}$,
showing, among other insights, that we can match a geometric convergence rate
to a finite optimality gap under more general assumptions than in existing
works. Through experiments, we demonstrate that $\texttt{DSpodFL}$ achieves
significantly improved training speeds and robustness to variations in system
parameters compared to the state-of-the-art.
\\ ( https://arxiv.org/abs/2402.03448 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03457
Date: Mon, 5 Feb 2024 19:09:42 GMT   (5003kb,D)

Title: Efficient and Interpretable Traffic Destination Prediction using
  Explainable Boosting Machines
Authors: Yasin Yousif and J\"org M\"uller
Categories: cs.LG
\\
  Developing accurate models for traffic trajectory predictions is crucial for
achieving fully autonomous driving. Various deep neural network models have
been employed to address this challenge, but their black-box nature hinders
transparency and debugging capabilities in a deployed system. Glass-box models
offer a solution by providing full interpretability through methods like
\ac{GAM}. In this study, we evaluate an efficient additive model called
\ac{EBM} for traffic prediction on three popular mixed traffic datasets:
\ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models
perform competitively in predicting pedestrian destinations within \ac{SDD} and
\ac{InD} while providing modest predictions for vehicle-dominant Argoverse
dataset. Additionally, our transparent trained models allow us to analyse
feature importance and interactions, as well as provide qualitative examples of
predictions explanation. The full training code will be made public upon
publication.
\\ ( https://arxiv.org/abs/2402.03457 ,  5003kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03467
Date: Fri, 2 Feb 2024 14:29:38 GMT   (37kb)

Title: Stochastic Modified Flows for Riemannian Stochastic Gradient Descent
Authors: Benjamin Gess, Sebastian Kassing, Nimit Rana
Categories: cs.LG math.OC math.PR stat.ML
MSC-class: Primary 62L20, Secondary 58J65, 60J20, 65K05
\\
  We give quantitative estimates for the rate of convergence of Riemannian
stochastic gradient descent (RSGD) to Riemannian gradient flow and to a
diffusion process, the so-called Riemannian stochastic modified flow (RSMF).
Using tools from stochastic differential geometry we show that, in the small
learning rate regime, RSGD can be approximated by the solution to the RSMF
driven by an infinite-dimensional Wiener process. The RSMF accounts for the
random fluctuations of RSGD and, thereby, increases the order of approximation
compared to the deterministic Riemannian gradient flow. The RSGD is build using
the concept of a retraction map, that is, a cost efficient approximation of the
exponential map, and we prove quantitative bounds for the weak error of the
diffusion approximation under assumptions on the retraction map, the geometry
of the manifold, and the random estimators of the gradient.
\\ ( https://arxiv.org/abs/2402.03467 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03468
Date: Fri, 2 Feb 2024 13:26:38 GMT   (30851kb,D)

Title: Exact Tensor Completion Powered by Arbitrary Linear Transforms
Authors: Li Ge, Xue Jiang, Lin Chen
Categories: cs.LG eess.SP
\\
  In this work, a tensor completion problem is studied, which aims to perfectly
recover the tensor from partial observations. Existing theoretical guarantee
requires the involved transform to be orthogonal, which hinders its
applications. In this paper, jumping out of the constraints of isotropy or
self-adjointness, the theoretical guarantee of exact tensor completion with
arbitrary linear transforms is established. To that end, we define a new
tensor-tensor product, which leads us to a new definition of the tensor nuclear
norm. Equipped with these tools, an efficient algorithm based on alternating
direction of multipliers is designed to solve the transformed tensor completion
program and the theoretical bound is obtained. Our model and proof greatly
enhance the flexibility of tensor completion and extensive experiments validate
the superiority of the proposed method.
\\ ( https://arxiv.org/abs/2402.03468 ,  30851kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03469
Date: Fri, 2 Feb 2024 11:58:08 GMT   (1736kb,D)

Title: Preference-free Alignment Learning with Regularized Relevance Reward
Authors: Sungdong Kim and Minjoon Seo
Categories: cs.LG cs.AI cs.CL
Comments: Preprint; Under review
\\
  Learning from human preference has been considered key to aligning Large
Language Models (LLMs) with human values. However, contrary to popular belief,
our preliminary study reveals that reward models trained on human preference
datasets tend to give higher scores to long off-topic responses than short
on-topic ones. Motivated by this observation, we explore a preference-free
approach utilizing `relevance' as a key objective for alignment. On our first
attempt, we find that the relevance score obtained by a retriever alone is
vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when
we utilize the score as a reward for reinforcement learning. To mitigate it, we
integrate effective inductive biases into the vanilla relevance to regularize
each other, resulting in a mixture of reward functions: Regularized Relevance
Reward ($R^3$). $R^3$ significantly improves performance on preference
benchmarks by providing a robust reward signal. Notably, $R^3$ does not require
any human preference datasets (i.e., preference-free), outperforming
open-source reward models in improving human preference. Our analysis
demonstrates that $R^3$ has advantages in elevating human preference while
minimizing its side effects. Finally, we show the generalizability of $R^3$,
consistently improving instruction-tuned models in various backbones and sizes
without additional dataset cost. Our code is available at
https://github.com/naver-ai/RRR.
\\ ( https://arxiv.org/abs/2402.03469 ,  1736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03471
Date: Thu, 1 Feb 2024 12:50:43 GMT   (305kb,D)

Title: The Information of Large Language Model Geometry
Authors: Zhiquan Tan, Chenghai Li, Weiran Huang
Categories: cs.LG cs.AI cs.CL cs.IT math.IT
\\
  This paper investigates the information encoded in the embeddings of large
language models (LLMs). We conduct simulations to analyze the representation
entropy and discover a power law relationship with model sizes. Building upon
this observation, we propose a theory based on (conditional) entropy to
elucidate the scaling law phenomenon. Furthermore, we delve into the
auto-regressive structure of LLMs and examine the relationship between the last
token and previous context tokens using information theory and regression
techniques. Specifically, we establish a theoretical connection between the
information gain of new tokens and ridge regression. Additionally, we explore
the effectiveness of Lasso regression in selecting meaningful tokens, which
sometimes outperforms the closely related attention weights. Finally, we
conduct controlled experiments, and find that information is distributed across
tokens, rather than being concentrated in specific "meaningful" tokens alone.
\\ ( https://arxiv.org/abs/2402.03471 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03478
Date: Mon, 5 Feb 2024 19:39:52 GMT   (8336kb,D)

Title: Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a
  Single Model
Authors: Matthew A. Chan, Maria J. Molina, Christopher A. Metzler
Categories: cs.LG cs.CV
Comments: 10 pages, 7 figures
\\
  Estimating and disentangling epistemic uncertainty (uncertainty that can be
reduced with more training data) and aleatoric uncertainty (uncertainty that is
inherent to the task at hand) is critically important when applying machine
learning (ML) to high-stakes applications such as medical imaging and weather
forecasting. Conditional diffusion models' breakthrough ability to accurately
and efficiently sample from the posterior distribution of a dataset now makes
uncertainty estimation conceptually straightforward: One need only train and
sample from a large ensemble of diffusion models. Unfortunately, training such
an ensemble becomes computationally intractable as the complexity of the model
architecture grows.
  In this work we introduce a new approach to ensembling, hyper-diffusion,
which allows one to accurately estimate epistemic and aleatoric uncertainty
with a single model. Unlike existing Monte Carlo dropout based single-model
ensembling methods, hyper-diffusion offers the same prediction accuracy as
multi-model ensembles. We validate our approach on two distinct tasks: x-ray
computed tomography (CT) reconstruction and weather temperature forecasting.
\\ ( https://arxiv.org/abs/2402.03478 ,  8336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03479
Date: Mon, 5 Feb 2024 19:47:45 GMT   (5016kb,D)

Title: ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context
  Environment Design
Authors: Samuel Garcin, James Doran, Shangmin Guo, Christopher G. Lucas,
  Stefano V. Albrecht
Categories: cs.LG cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2310.03494
\\
  Autonomous agents trained using deep reinforcement learning (RL) often lack
the ability to successfully generalise to new environments, even when they
share characteristics with the environments they have encountered during
training. In this work, we investigate how the sampling of individual
environment instances, or levels, affects the zero-shot generalisation (ZSG)
ability of RL agents. We discover that, for deep actor-critic architectures
sharing their base layers, prioritising levels according to their value loss
minimises the mutual information between the agent's internal representation
and the set of training levels in the generated training data. This provides a
novel theoretical justification for the implicit regularisation achieved by
certain adaptive sampling strategies. We then turn our attention to
unsupervised environment design (UED) methods, which have more control over the
data generation mechanism. We find that existing UED methods can significantly
shift the training distribution, which translates to low ZSG performance. To
prevent both overfitting and distributional shift, we introduce in-context
environment design (ICED). ICED generates levels using a variational
autoencoder trained over an initial set of level parameters, reducing
distributional shift, and achieves significant improvements in ZSG over
adaptive level sampling strategies and UED methods.
\\ ( https://arxiv.org/abs/2402.03479 ,  5016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03480
Date: Mon, 5 Feb 2024 19:48:31 GMT   (879kb,D)

Title: Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A
  Survey and Vision
Authors: Nathaniel Hudson, J. Gregory Pauloski, Matt Baughman, Alok Kamatar,
  Mansi Sakarvadia, Logan Ward, Ryan Chard, Andr\'e Bauer, Maksim Levental,
  Wenyi Wang, Will Engler, Owen Price Skelly, Ben Blaiszik, Rick Stevens, Kyle
  Chard, Ian Foster
Categories: cs.LG cs.AI cs.DC
Comments: 10 pages, 3 figures, accepted for publication in the proceedings of
  the 10th IEEE/ACM International Conference on Big Data Computing,
  Applications and Technologies (BDCAT2023)
\\
  Deep learning methods are transforming research, enabling new techniques, and
ultimately leading to new discoveries. As the demand for more capable AI models
continues to grow, we are now entering an era of Trillion Parameter Models
(TPM), or models with more than a trillion parameters -- such as Huawei's
PanGu-$\Sigma$. We describe a vision for the ecosystem of TPM users and
providers that caters to the specific needs of the scientific community. We
then outline the significant technical challenges and open problems in system
design for serving TPMs to enable scientific research and discovery.
Specifically, we describe the requirements of a comprehensive software stack
and interfaces to support the diverse and flexible requirements of researchers.
\\ ( https://arxiv.org/abs/2402.03480 ,  879kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03486
Date: Mon, 5 Feb 2024 19:58:40 GMT   (1688kb,D)

Title: Early prediction of onset of sepsis in Clinical Setting
Authors: Fahim Mohammad, Lakshmi Arunachalam, Samanway Sadhu, Boudewijn Aasman,
  Shweta Garg, Adil Ahmed, Silvie Colman, Meena Arunachalam, Sudhir Kulkarni,
  Parsa Mirhaji
Categories: cs.LG cs.AI cs.IR
Comments: 16 pages, 6 figures and 7 tables
\\
  This study proposes the use of Machine Learning models to predict the early
onset of sepsis using deidentified clinical data from Montefiore Medical Center
in Bronx, NY, USA. A supervised learning approach was adopted, wherein an
XGBoost model was trained utilizing 80\% of the train dataset, encompassing 107
features (including the original and derived features). Subsequently, the model
was evaluated on the remaining 20\% of the test data. The model was validated
on prospective data that was entirely unseen during the training phase. To
assess the model's performance at the individual patient level and timeliness
of the prediction, a normalized utility score was employed, a widely recognized
scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis
Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag
Rate were also devised. The model achieved a normalized utility score of 0.494
on test data and 0.378 on prospective data at threshold 0.3. The F1 scores were
80.8\% and 67.1\% respectively for the test data and the prospective data for
the same threshold, highlighting its potential to be integrated into clinical
decision-making processes effectively. These results bear testament to the
model's robust predictive capabilities and its potential to substantially
impact clinical decision-making processes.
\\ ( https://arxiv.org/abs/2402.03486 ,  1688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03495
Date: Mon, 5 Feb 2024 20:15:19 GMT   (2644kb,D)

Title: Partially Stochastic Infinitely Deep Bayesian Neural Networks
Authors: Sergio Calvo-Ordonez, Matthieu Meunier, Francesco Piatti, Yuantao Shi
Categories: cs.LG math.PR
Comments: 16 pages including supplementary material, 6 figures
\\
  In this paper, we present Partially Stochastic Infinitely Deep Bayesian
Neural Networks, a novel family of architectures that integrates partial
stochasticity into the framework of infinitely deep neural networks. Our new
class of architectures is designed to improve the limitations of existing
architectures around computational efficiency at training and inference time.
To do this, we leverage the advantages of partial stochasticity in the
infinite-depth limit which include the benefits of full stochasticity e.g.
robustness, uncertainty quantification, and memory efficiency, whilst improving
their limitations around computational efficiency at training and inference
time. We present a variety of architectural configurations, offering
flexibility in network design including different methods for weight partition.
We also provide mathematical guarantees on the expressivity of our models by
establishing that our network family qualifies as Universal Conditional
Distribution Approximators. Lastly, empirical evaluations across multiple tasks
show that our proposed architectures achieve better downstream task performance
and uncertainty quantification than their counterparts while being
significantly more efficient.
\\ ( https://arxiv.org/abs/2402.03495 ,  2644kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03496
Date: Mon, 5 Feb 2024 20:15:19 GMT   (651kb,D)

Title: Can We Remove the Square-Root in Adaptive Gradient Methods? A
  Second-Order Perspective
Authors: Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner,
  Alireza Makhzani
Categories: cs.LG math.OC
\\
  Adaptive gradient optimizers like Adam(W) are the default training algorithms
for many deep learning architectures, such as transformers. Their diagonal
preconditioner is based on the gradient outer product which is incorporated
into the parameter update via a square root. While these methods are often
motivated as approximate second-order methods, the square root represents a
fundamental difference. In this work, we investigate how the behavior of
adaptive methods changes when we remove the root, i.e. strengthen their
second-order motivation. Surprisingly, we find that such square-root-free
adaptive methods close the generalization gap to SGD on convolutional
architectures, while maintaining their root-based counterpart's performance on
transformers. The second-order perspective also has practical benefits for the
development of adaptive methods with non-diagonal preconditioner. In contrast
to root-based counterparts like Shampoo, they do not require numerically
unstable matrix square roots and therefore work well in low precision, which we
demonstrate empirically. This raises important questions regarding the
currently overlooked role of adaptivity for the success of adaptive methods.
\\ ( https://arxiv.org/abs/2402.03496 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03502
Date: Mon, 5 Feb 2024 20:36:33 GMT   (503kb,D)

Title: How Does Unlabeled Data Provably Help Out-of-Distribution Detection?
Authors: Xuefeng Du, Zhen Fang, Ilias Diakonikolas, Yixuan Li
Categories: cs.LG stat.ML
Comments: ICLR 2024
\\
  Using unlabeled data to regularize the machine learning models has
demonstrated promise for improving safety and reliability in detecting
out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild
data is non-trivial due to the heterogeneity of both in-distribution (ID) and
OOD data. This lack of a clean set of OOD samples poses significant challenges
in learning an optimal OOD classifier. Currently, there is a lack of research
on formally understanding how unlabeled data helps OOD detection. This paper
bridges the gap by introducing a new learning framework SAL (Separate And
Learn) that offers both strong theoretical guarantees and empirical
effectiveness. The framework separates candidate outliers from the unlabeled
data and then trains an OOD classifier using the candidate outliers and the
labeled ID data. Theoretically, we provide rigorous error bounds from the lens
of separability and learnability, formally justifying the two components in our
algorithm. Our theory shows that SAL can separate the candidate outliers with
small error rates, which leads to a generalization guarantee for the learned
OOD classifier. Empirically, SAL achieves state-of-the-art performance on
common benchmarks, reinforcing our theoretical insights. Code is publicly
available at https://github.com/deeplearning-wisc/sal.
\\ ( https://arxiv.org/abs/2402.03502 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03525
Date: Mon, 5 Feb 2024 21:25:45 GMT   (32kb)

Title: Deep Reinforcement Learning for Picker Routing Problem in Warehousing
Authors: George Dunn, Hadi Charkhgard, Ali Eshragh, Sasan Mahmoudinazlou and
  Elizabeth Stojanovski
Categories: cs.LG cs.AI
\\
  Order Picker Routing is a critical issue in Warehouse Operations Management.
Due to the complexity of the problem and the need for quick solutions,
suboptimal algorithms are frequently employed in practice. However,
Reinforcement Learning offers an appealing alternative to traditional
heuristics, potentially outperforming existing methods in terms of speed and
accuracy. We introduce an attention based neural network for modeling picker
tours, which is trained using Reinforcement Learning. Our method is evaluated
against existing heuristics across a range of problem parameters to demonstrate
its efficacy. A key advantage of our proposed method is its ability to offer an
option to reduce the perceived complexity of routes.
\\ ( https://arxiv.org/abs/2402.03525 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03531
Date: Mon, 5 Feb 2024 21:38:23 GMT   (43kb)

Title: Fairness and Privacy Guarantees in Federated Contextual Bandits
Authors: Sambhav Solanki, Shweta Jain, Sujit Gujar
Categories: cs.LG
Comments: 16 pages, 2 figures
\\
  This paper considers the contextual multi-armed bandit (CMAB) problem with
fairness and privacy guarantees in a federated environment. We consider
merit-based exposure as the desired fair outcome, which provides exposure to
each action in proportion to the reward associated. We model the algorithm's
effectiveness using fairness regret, which captures the difference between fair
optimal policy and the policy output by the algorithm. Applying fair CMAB
algorithm to each agent individually leads to fairness regret linear in the
number of agents. We propose that collaborative -- federated learning can be
more effective and provide the algorithm Fed-FairX-LinUCB that also ensures
differential privacy. The primary challenge in extending the existing privacy
framework is designing the communication protocol for communicating required
information across agents. A naive protocol can either lead to weaker privacy
guarantees or higher regret. We design a novel communication protocol that
allows for (i) Sub-linear theoretical bounds on fairness regret for
Fed-FairX-LinUCB and comparable bounds for the private counterpart,
Priv-FairX-LinUCB (relative to single-agent learning), (ii) Effective use of
privacy budget in Priv-FairX-LinUCB. We demonstrate the efficacy of our
proposed algorithm with extensive simulations-based experiments. We show that
both Fed-FairX-LinUCB and Priv-FairX-LinUCB achieve near-optimal fairness
regret.
\\ ( https://arxiv.org/abs/2402.03531 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03540
Date: Mon, 5 Feb 2024 21:54:28 GMT   (11480kb,D)

Title: Regulation Games for Trustworthy Machine Learning
Authors: Mohammad Yaghini, Patty Liu, Franziska Boenisch, Nicolas Papernot
Categories: cs.LG cs.GT stat.ML
\\
  Existing work on trustworthy machine learning (ML) often concentrates on
individual aspects of trust, such as fairness or privacy. Additionally, many
techniques overlook the distinction between those who train ML models and those
responsible for assessing their trustworthiness. To address these issues, we
propose a framework that views trustworthy ML as a multi-objective multi-agent
optimization problem. This naturally lends itself to a game-theoretic
formulation we call regulation games. We illustrate a particular game instance,
the SpecGame in which we model the relationship between an ML model builder and
fairness and privacy regulators. Regulators wish to design penalties that
enforce compliance with their specification, but do not want to discourage
builders from participation. Seeking such socially optimal (i.e., efficient for
all agents) solutions to the game, we introduce ParetoPlay. This novel
equilibrium search algorithm ensures that agents remain on the Pareto frontier
of their objectives and avoids the inefficiencies of other equilibria.
Simulating SpecGame through ParetoPlay can provide policy guidance for ML
Regulation. For instance, we show that for a gender classification application,
regulators can enforce a differential privacy budget that is on average 4.0
lower if they take the initiative to specify their desired guarantee first.
\\ ( https://arxiv.org/abs/2402.03540 ,  11480kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03541
Date: Mon, 5 Feb 2024 21:55:24 GMT   (2120kb,D)

Title: HAMLET: Graph Transformer Neural Operator for Partial Differential
  Equations
Authors: Andrey Bryutkin, Jiahao Huang, Zhongying Deng, Guang Yang,
  Carola-Bibiane Sch\"onlieb, Angelica Aviles-Rivero
Categories: cs.LG cs.NA math.NA
Comments: 17 pages, 7 figures, 6 tables
\\
  We present a novel graph transformer framework, HAMLET, designed to address
the challenges in solving partial differential equations (PDEs) using neural
networks. The framework uses graph transformers with modular input encoders to
directly incorporate differential equation information into the solution
process. This modularity enhances parameter correspondence control, making
HAMLET adaptable to PDEs of arbitrary geometries and varied input formats.
Notably, HAMLET scales effectively with increasing data complexity and noise,
showcasing its robustness. HAMLET is not just tailored to a single type of
physical simulation, but can be applied across various domains. Moreover, it
boosts model resilience and performance, especially in scenarios with limited
data. We demonstrate, through extensive experiments, that our framework is
capable of outperforming current techniques for PDEs.
\\ ( https://arxiv.org/abs/2402.03541 ,  2120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03545
Date: Mon, 5 Feb 2024 22:03:25 GMT   (1727kb,D)

Title: Online Feature Updates Improve Online (Generalized) Label Shift
  Adaptation
Authors: Ruihan Wu and Siddhartha Datta and Yi Su and Dheeraj Baby and Yu-Xiang
  Wang and Kilian Q. Weinberger
Categories: cs.LG
\\
  This paper addresses the prevalent issue of label shift in an online setting
with missing labels, where data distributions change over time and obtaining
timely labels is challenging. While existing methods primarily focus on
adjusting or updating the final layer of a pre-trained classifier, we explore
the untapped potential of enhancing feature representations using unlabeled
data at test-time. Our novel method, Online Label Shift adaptation with Online
Feature Updates (OLS-OFU), leverages self-supervised learning to refine the
feature extraction process, thereby improving the prediction model. Theoretical
analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on
self-supervised learning for feature refinement. Empirical studies on various
datasets, under both online label shift and generalized label shift conditions,
underscore the effectiveness and robustness of OLS-OFU, especially in cases of
domain shifts.
\\ ( https://arxiv.org/abs/2402.03545 ,  1727kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03548
Date: Mon, 5 Feb 2024 22:07:58 GMT   (4640kb,D)

Title: Single-GPU GNN Systems: Traps and Pitfalls
Authors: Yidong Gong, Arnab Tarafder, Saima Afrin, and Pradeep Kumar
Categories: cs.LG cs.DC
\\
  The current graph neural network (GNN) systems have established a clear trend
of not showing training accuracy results, and directly or indirectly relying on
smaller datasets for evaluations majorly. Our in-depth analysis shows that it
leads to a chain of pitfalls in the system design and evaluation process,
questioning the practicality of many of the proposed system optimizations, and
affecting conclusions and lessons learned. We analyze many single-GPU systems
and show the fundamental impact of these pitfalls. We further develop
hypotheses, recommendations, and evaluation methodologies, and provide future
directions. Finally, a new reference system is developed to establish a new
line of optimizations rooted in solving the system-design pitfalls efficiently
and practically. The proposed design can productively be integrated into prior
works, thereby truly advancing the state-of-the-art.
\\ ( https://arxiv.org/abs/2402.03548 ,  4640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03558
Date: Mon, 5 Feb 2024 22:16:05 GMT   (2859kb,D)

Title: Path Signatures and Graph Neural Networks for Slow Earthquake Analysis:
  Better Together?
Authors: Hans Riess, Manolis Veveakis, Michael M. Zavlanos
Categories: cs.LG physics.geo-ph
\\
  The path signature, having enjoyed recent success in the machine learning
community, is a theoretically-driven method for engineering features from
irregular paths. On the other hand, graph neural networks (GNN), neural
architectures for processing data on graphs, excel on tasks with irregular
domains, such as sensor networks. In this paper, we introduce a novel approach,
Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path
signatures into graph convolutional neural networks (GCNN), and leveraging the
strengths of both path signatures, for feature extraction, and GCNNs, for
handling spatial interactions. We apply our method to analyze slow earthquake
sequences, also called slow slip events (SSE), utilizing data from GPS
timeseries, with a case study on a GPS sensor network on the east coast of New
Zealand's north island. We also establish benchmarks for our method on
simulated stochastic differential equations, which model similar
reaction-diffusion phenomenon. Our methodology shows promise for future
advancement in earthquake prediction and sensor network analysis.
\\ ( https://arxiv.org/abs/2402.03558 ,  2859kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03559
Date: Mon, 5 Feb 2024 22:18:16 GMT   (5581kb,D)

Title: Projected Generative Diffusion Models for Constraint Satisfaction
Authors: Jacob K Christopher, Stephen Baek, Ferdinando Fioretto
Categories: cs.LG cs.AI
\\
  Generative diffusion models excel at robustly synthesizing coherent content
from raw noise through a sequential process. However, their direct application
in scenarios requiring outputs to adhere to specific, stringent criteria faces
several severe challenges. This paper aims at overcome these challenges and
introduces Projected Generative Diffusion Models (PGDM), an approach that
recast traditional diffusion models sampling into a constrained-optimization
problem. This enables the application of an iterative projections method to
ensure that generated data faithfully adheres to specified constraints or
physical principles. This paper provides theoretical support for the ability of
PGDM to synthesize outputs from a feasible subdistribution under a restricted
class of constraints while also providing large empirical evidence in the case
of complex non-convex constraints and ordinary differential equations. These
capabilities are demonstrated by physics-informed motion in video generation,
trajectory optimization in path planning, and morphometric properties adherence
in material science.
\\ ( https://arxiv.org/abs/2402.03559 ,  5581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03563
Date: Mon, 5 Feb 2024 22:22:49 GMT   (3347kb,D)

Title: Distinguishing the Knowable from the Unknowable with Language Models
Authors: Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, Benjamin L. Edelman
Categories: cs.LG cs.AI cs.CL
\\
  We study the feasibility of identifying epistemic uncertainty (reflecting a
lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in
the underlying distribution), in the outputs of large language models (LLMs)
over free-form text. In the absence of ground-truth probabilities, we explore a
setting where, in order to (approximately) disentangle a given LLM's
uncertainty, a significantly larger model stands in as a proxy for the ground
truth. We show that small linear probes trained on the embeddings of frozen,
pretrained models accurately predict when larger models will be more confident
at the token level and that probes trained on one text domain generalize to
others. Going further, we propose a fully unsupervised method that achieves
non-trivial accuracy on the same task. Taken together, we interpret these
results as evidence that LLMs naturally contain internal representations of
different types of uncertainty that could potentially be leveraged to devise
more informative indicators of model confidence in diverse practical settings.
\\ ( https://arxiv.org/abs/2402.03563 ,  3347kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03564
Date: Mon, 5 Feb 2024 22:24:19 GMT   (1588kb,D)

Title: SkipPredict: When to Invest in Predictions for Scheduling
Authors: Rana Shahout, Michael Mitzenmacher
Categories: cs.LG cs.DC
\\
  In light of recent work on scheduling with predicted job sizes, we consider
the effect of the cost of predictions in queueing systems, removing the
assumption in prior research that predictions are external to the system's
resources and/or cost-free. In particular, we introduce a novel approach to
utilizing predictions, SkipPredict, designed to address their inherent cost.
Rather than uniformly applying predictions to all jobs, we propose a tailored
approach that categorizes jobs based on their prediction requirements. To
achieve this, we employ one-bit "cheap predictions" to classify jobs as either
short or long. SkipPredict prioritizes predicted short jobs over long jobs, and
for the latter, SkipPredict applies a second round of more detailed "expensive
predictions" to approximate Shortest Remaining Processing Time for these jobs.
Our analysis takes into account the cost of prediction. We examine the effect
of this cost for two distinct models. In the external cost model, predictions
are generated by some external method without impacting job service times but
incur a cost. In the server time cost model, predictions themselves require
server processing time, and are scheduled on the same server as the jobs.
\\ ( https://arxiv.org/abs/2402.03564 ,  1588kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03570
Date: Mon, 5 Feb 2024 22:43:57 GMT   (15662kb,D)

Title: Diffusion World Model
Authors: Zihan Ding, Amy Zhang, Yuandong Tian, Qinqing Zheng
Categories: cs.LG cs.AI
\\
  We introduce Diffusion World Model (DWM), a conditional diffusion model
capable of predicting multistep future states and rewards concurrently. As
opposed to traditional one-step dynamics models, DWM offers long-horizon
predictions in a single forward pass, eliminating the need for recursive
quires. We integrate DWM into model-based value estimation, where the
short-term return is simulated by future trajectories sampled from DWM. In the
context of offline reinforcement learning, DWM can be viewed as a conservative
value regularization through generative modeling. Alternatively, it can be seen
as a data source that enables offline Q-learning with synthetic data. Our
experiments on the D4RL dataset confirm the robustness of DWM to long-horizon
simulation. In terms of absolute performance, DWM significantly surpasses
one-step dynamics models with a $44\%$ performance gain, and achieves
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2402.03570 ,  15662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03576
Date: Mon, 5 Feb 2024 22:57:33 GMT   (40kb)

Title: Generalization Properties of Adversarial Training for $\ell_0$-Bounded
  Adversarial Attacks
Authors: Payam Delgosha, Hamed Hassani, Ramtin Pedarsani
Categories: cs.LG cs.CR
\\
  We have widely observed that neural networks are vulnerable to small additive
perturbations to the input causing misclassification. In this paper, we focus
on the $\ell_0$-bounded adversarial attacks, and aim to theoretically
characterize the performance of adversarial training for an important class of
truncated classifiers. Such classifiers are shown to have strong performance
empirically, as well as theoretically in the Gaussian mixture model, in the
$\ell_0$-adversarial setting. The main contribution of this paper is to prove a
novel generalization bound for the binary classification setting with
$\ell_0$-bounded adversarial perturbation that is distribution-independent.
Deriving a generalization bound in this setting has two main challenges: (i)
the truncated inner product which is highly non-linear; and (ii) maximization
over the $\ell_0$ ball due to adversarial training is non-convex and highly
non-smooth. To tackle these challenges, we develop new coding techniques for
bounding the combinatorial dimension of the truncated hypothesis class.
\\ ( https://arxiv.org/abs/2402.03576 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03577
Date: Mon, 5 Feb 2024 22:58:06 GMT   (4687kb,D)

Title: Revisiting the Dataset Bias Problem from a Statistical Perspective
Authors: Kien Do, Dung Nguyen, Hung Le, Thao Le, Dang Nguyen, Haripriya
  Harikumar, Truyen Tran, Santu Rana, Svetha Venkatesh
Categories: cs.LG
\\
  In this paper, we study the "dataset bias" problem from a statistical
standpoint, and identify the main cause of the problem as the strong
correlation between a class attribute u and a non-class attribute b in the
input x, represented by p(u|b) differing significantly from p(u). Since p(u|b)
appears as part of the sampling distributions in the standard maximum
log-likelihood (MLL) objective, a model trained on a biased dataset via MLL
inherently incorporates such correlation into its parameters, leading to poor
generalization to unbiased test data. From this observation, we propose to
mitigate dataset bias via either weighting the objective of each sample n by
\frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to
\frac{1}{p(u_{n}|b_{n})}. While both methods are statistically equivalent, the
former proves more stable and effective in practice. Additionally, we establish
a connection between our debiasing approach and causal reasoning, reinforcing
our method's theoretical foundation. However, when the bias label is
unavailable, computing p(u|b) exactly is difficult. To overcome this challenge,
we propose to approximate \frac{1}{p(u|b)} using a biased classifier trained
with "bias amplification" losses. Extensive experiments on various biased
datasets demonstrate the superiority of our method over existing debiasing
techniques in most settings, validating our theoretical analysis.
\\ ( https://arxiv.org/abs/2402.03577 ,  4687kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03579
Date: Mon, 5 Feb 2024 23:06:48 GMT   (11878kb,D)

Title: Deconstructing the Goldilocks Zone of Neural Network Initialization
Authors: Artem Vysogorets, Anna Dawid, and Julia Kempe
Categories: cs.LG math.OC
\\
  The second-order properties of the training loss have a massive impact on the
optimization dynamics of deep learning models. Fort & Scherlis (2019)
discovered that a high positive curvature and local convexity of the loss
Hessian are associated with highly trainable initial points located in a region
coined the "Goldilocks zone". Only a handful of subsequent studies touched upon
this relationship, so it remains largely unexplained. In this paper, we present
a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous
neural networks. In particular, we derive the fundamental condition resulting
in non-zero positive curvature of the loss Hessian and argue that it is only
incidentally related to the initialization norm, contrary to prior beliefs.
Further, we relate high positive curvature to model confidence, low initial
loss, and a previously unknown type of vanishing cross-entropy loss gradient.
To understand the importance of positive curvature for trainability of deep
networks, we optimize both fully-connected and convolutional architectures
outside the Goldilocks zone and analyze the emergent behaviors. We find that
strong model performance is not necessarily aligned with the Goldilocks zone,
which questions the practical significance of this concept.
\\ ( https://arxiv.org/abs/2402.03579 ,  11878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03587
Date: Mon, 5 Feb 2024 23:33:57 GMT   (39561kb,D)

Title: Effective Acquisition Functions for Active Correlation Clustering
Authors: Linus Aronsson, Morteza Haghir Chehreghani
Categories: cs.LG stat.ML
\\
  Correlation clustering is a powerful unsupervised learning paradigm that
supports positive and negative similarities. In this paper, we assume the
similarities are not known in advance. Instead, we employ active learning to
iteratively query similarities in a cost-efficient way. In particular, we
develop three effective acquisition functions to be used in this setting. One
is based on the notion of inconsistency (i.e., when similarities violate the
transitive property). The remaining two are based on information-theoretic
quantities, i.e., entropy and information gain.
\\ ( https://arxiv.org/abs/2402.03587 ,  39561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03588
Date: Mon, 5 Feb 2024 23:46:03 GMT   (655kb,D)

Title: Continual Domain Adversarial Adaptation via Double-Head Discriminators
Authors: Yan Shen and Zhanghexuan Ji and Chunwei Ma and Mingchen Gao
Categories: cs.LG cs.AI
Comments: AISTATS 2024
\\
  Domain adversarial adaptation in a continual setting poses a significant
challenge due to the limitations on accessing previous source domain data.
Despite extensive research in continual learning, the task of adversarial
adaptation cannot be effectively accomplished using only a small number of
stored source domain data, which is a standard setting in memory replay
approaches. This limitation arises from the erroneous empirical estimation of
$\gH$-divergence with few source domain samples. To tackle this problem, we
propose a double-head discriminator algorithm, by introducing an addition
source-only domain discriminator that are trained solely on source learning
phase. We prove that with the introduction of a pre-trained source-only domain
discriminator, the empirical estimation error of $\gH$-divergence related
adversarial loss is reduced from the source domain side. Further experiments on
existing domain adaptation benchmark show that our proposed algorithm achieves
more than 2$\%$ improvement on all categories of target domain adaptation task
while significantly mitigating the forgetting on source domain.
\\ ( https://arxiv.org/abs/2402.03588 ,  655kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03589
Date: Mon, 5 Feb 2024 23:46:42 GMT   (6573kb,D)

Title: A Reinforcement Learning Approach for Dynamic Rebalancing in
  Bike-Sharing System
Authors: Jiaqi Liang, Sanjay Dominik Jena, Defeng Liu, Andrea Lodi
Categories: cs.LG math.OC
\\
  Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the
alleviation of traffic congestion and to healthier lifestyles. Efficiently
operating such systems and maintaining high customer satisfaction is
challenging due to the stochastic nature of trip demand, leading to full or
empty stations. Devising effective rebalancing strategies using vehicles to
redistribute bikes among stations is therefore of uttermost importance for
operators. As a promising alternative to classical mathematical optimization,
reinforcement learning is gaining ground to solve sequential decision-making
problems. This paper introduces a spatio-temporal reinforcement learning
algorithm for the dynamic rebalancing problem with multiple vehicles. We first
formulate the problem as a Multi-agent Markov Decision Process in a continuous
time framework. This allows for independent and cooperative vehicle
rebalancing, eliminating the impractical restriction of time-discretized models
where vehicle departures are synchronized. A comprehensive simulator under the
first-arrive-first-serve rule is then developed to facilitate the learning
process by computing immediate rewards under diverse demand scenarios. To
estimate the value function and learn the rebalancing policy, various Deep
Q-Network configurations are tested, minimizing the lost demand. Experiments
are carried out on various datasets generated from historical data, affected by
both temporal and weather factors. The proposed algorithms outperform
benchmarks, including a multi-period Mixed-Integer Programming model, in terms
of lost demand. Once trained, it yields immediate decisions, making it suitable
for real-time applications. Our work offers practical insights for operators
and enriches the integration of reinforcement learning into dynamic rebalancing
problems, paving the way for more intelligent and robust urban mobility
solutions.
\\ ( https://arxiv.org/abs/2402.03589 ,  6573kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03590
Date: Mon, 5 Feb 2024 23:50:55 GMT   (3282kb,D)

Title: Assessing the Impact of Distribution Shift on Reinforcement Learning
  Performance
Authors: Ted Fujimoto and Joshua Suetterlein and Samrat Chatterjee and Auroop
  Ganguly
Categories: cs.LG cs.AI cs.MA
Comments: Poster at the Workshop on Regulatable Machine Learning at the 37th
  Conference on Neural Information Processing Systems (RegML @ NeurIPS 2023)
\\
  Research in machine learning is making progress in fixing its own
reproducibility crisis. Reinforcement learning (RL), in particular, faces its
own set of unique challenges. Comparison of point estimates, and plots that
show successful convergence to the optimal policy during training, may
obfuscate overfitting or dependence on the experimental setup. Although
researchers in RL have proposed reliability metrics that account for
uncertainty to better understand each algorithm's strengths and weaknesses, the
recommendations of past work do not assume the presence of out-of-distribution
observations. We propose a set of evaluation methods that measure the
robustness of RL algorithms under distribution shifts. The tools presented here
argue for the need to account for performance over time while the agent is
acting in its environment. In particular, we recommend time series analysis as
a method of observational RL evaluation. We also show that the unique
properties of RL and simulated dynamic environments allow us to make stronger
assumptions to justify the measurement of causal impact in our evaluations. We
then apply these tools to single-agent and multi-agent environments to show the
impact of introducing distribution shifts during test time. We present this
methodology as a first step toward rigorous RL evaluation in the presence of
distribution shifts.
\\ ( https://arxiv.org/abs/2402.03590 ,  3282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03610
Date: Tue, 6 Feb 2024 00:53:27 GMT   (1812kb,D)

Title: RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal
  LLM Agents
Authors: Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar,
  Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, Yang You
Categories: cs.LG cs.AI cs.CL
\\
  Owing to recent advancements, Large Language Models (LLMs) can now be
deployed as agents for increasingly complex decision-making applications in
areas including robotics, gaming, and API integration. However, reflecting past
experiences in current decision-making processes, an innate human behavior,
continues to pose significant challenges. Addressing this, we propose
Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage
past experiences corresponding to the current situation and context, thereby
enhancing agents' planning capabilities. RAP distinguishes itself by being
versatile: it excels in both text-only and multimodal environments, making it
suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's
effectiveness, where it achieves SOTA performance in textual scenarios and
notably enhances multimodal LLM agents' performance for embodied tasks. These
results highlight RAP's potential in advancing the functionality and
applicability of LLM agents in complex, real-world applications.
\\ ( https://arxiv.org/abs/2402.03610 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03614
Date: Tue, 6 Feb 2024 01:01:23 GMT   (227kb,D)

Title: Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series
  Data
Authors: He Zhao and Edwin V. Bonilla
Categories: cs.LG stat.ML
\\
  We study the problem of automatically discovering Granger causal relations
from observational multivariate time-series data. Vector autoregressive (VAR)
models have been time-tested for this problem, including Bayesian variants and
more recent developments using deep neural networks. Most existing VAR methods
for Granger causality use sparsity-inducing penalties/priors or post-hoc
thresholds to interpret their coefficients as Granger causal graphs. Instead,
we propose a new Bayesian VAR model with a hierarchical graph prior over binary
Granger causal graphs, separately from the VAR coefficients. We develop an
efficient algorithm to infer the posterior over binary Granger causal graphs.
Our method provides better uncertainty quantification, has less
hyperparameters, and achieves better performance than competing approaches,
especially on sparse multivariate time-series data.
\\ ( https://arxiv.org/abs/2402.03614 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03621
Date: Tue, 6 Feb 2024 01:15:06 GMT   (473kb,D)

Title: Neural Network Approximators for Marginal MAP in Probabilistic Circuits
Authors: Shivvrat Arya, Tahrima Rahman, Vibhav Gogate
Categories: cs.LG cs.AI
Comments: Will appear in AAAI 2024
\\
  Probabilistic circuits (PCs) such as sum-product networks efficiently
represent large multi-variate probability distributions. They are preferred in
practice over other probabilistic representations such as Bayesian and Markov
networks because PCs can solve marginal inference (MAR) tasks in time that
scales linearly in the size of the network. Unfortunately, the
maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in
these models. Inspired by the recent work on using neural networks for
generating near-optimal solutions to optimization problems such as integer
linear programming, we propose an approach that uses neural networks to
approximate (M)MAP inference in PCs. The key idea in our approach is to
approximate the cost of an assignment to the query variables using a continuous
multilinear function, and then use the latter as a loss function. The two main
benefits of our new method are that it is self-supervised and after the neural
network is learned, it requires only linear time to output a solution. We
evaluate our new approach on several benchmark datasets and show that it
outperforms three competing linear time approximations, max-product inference,
max-marginal inference and sequential estimation, which are used in practice to
solve MMAP tasks in PCs.
\\ ( https://arxiv.org/abs/2402.03621 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03625
Date: Tue, 6 Feb 2024 01:29:35 GMT   (174kb,D)

Title: Convex Relaxations of ReLU Neural Networks Approximate Global Optima in
  Polynomial Time
Authors: Sungyoon Kim, Mert Pilanci
Categories: cs.LG math.OC
\\
  In this paper, we study the optimality gap between two-layer ReLU networks
regularized with weight decay and their convex relaxations. We show that when
the training data is random, the relative optimality gap between the original
problem and its relaxation can be bounded by a factor of $O(\sqrt{\log n})$,
where $n$ is the number of training samples. A simple application leads to a
tractable polynomial-time algorithm that is guaranteed to solve the original
non-convex problem up to a logarithmic factor. Moreover, under mild
assumptions, we show that with random initialization on the parameters local
gradient methods almost surely converge to a point that has low training loss.
Our result is an exponential improvement compared to existing results and sheds
new light on understanding why local gradient methods work well.
\\ ( https://arxiv.org/abs/2402.03625 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03629
Date: Tue, 6 Feb 2024 01:56:29 GMT   (1018kb,D)

Title: Disparate Impact on Group Accuracy of Linearization for Private
  Inference
Authors: Saswat Das, Marco Romanelli, Ferdinando Fioretto
Categories: cs.LG cs.CR cs.CY
\\
  Ensuring privacy-preserving inference on cryptographically secure data is a
well-known computational challenge. To alleviate the bottleneck of costly
cryptographic computations in non-linear activations, recent methods have
suggested linearizing a targeted portion of these activations in neural
networks. This technique results in significantly reduced runtimes with often
negligible impacts on accuracy. In this paper, we demonstrate that such
computational benefits may lead to increased fairness costs. Specifically, we
find that reducing the number of ReLU activations disproportionately decreases
the accuracy for minority groups compared to majority groups. To explain these
observations, we provide a mathematical interpretation under restricted
assumptions about the nature of the decision boundary, while also showing the
prevalence of this problem across widely used datasets and architectures.
Finally, we show how a simple procedure altering the fine-tuning step for
linearized models can serve as an effective mitigation strategy.
\\ ( https://arxiv.org/abs/2402.03629 ,  1018kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03646
Date: Tue, 6 Feb 2024 02:45:13 GMT   (3302kb,D)

Title: Lens: A Foundation Model for Network Traffic
Authors: Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Huajie Shao
Categories: cs.LG cs.NI
\\
  Network traffic refers to the amount of information being sent and received
over the internet or any system that connects computers. Analyzing and
understanding network traffic is vital for improving network security and
management. However, the analysis of network traffic poses great challenges due
to the unique characteristics of data packets, such as heterogeneous headers
and encrypted payload lacking semantics. To capture the latent semantics of
traffic, a few studies have adopted pre-training techniques based on the
Transformer encoder or decoder to learn the representations from large-scale
traffic data. However, these methods typically excel only in traffic
understanding (classification) or traffic generation tasks. To address this
issue, we develop Lens, a foundational network traffic model that leverages the
T5 architecture to learn the pre-trained representations from large-scale
unlabeled data. Harnessing the strength of the encoder-decoder framework, which
captures the global information while preserving the generative ability, our
model can better learn the representations from large-scale network traffic. To
further enhance pre-training performance, we design a novel loss that
integrates three distinct tasks, namely Masked Span Prediction (MSP), Packet
Order Prediction (POP), and Homologous Traffic Prediction (HTP). Evaluation
results on multiple benchmark datasets demonstrate that the proposed Lens
outperforms the baselines in most downstream tasks related to both traffic
understanding and traffic generation. Notably, it also requires considerably
less labeled data for fine-tuning compared to current methods.
\\ ( https://arxiv.org/abs/2402.03646 ,  3302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03647
Date: Tue, 6 Feb 2024 02:47:16 GMT   (1882kb,D)

Title: CAMBranch: Contrastive Learning with Augmented MILPs for Branching
Authors: Jiacheng Lin, Meng Xu, Zhihua Xiong, Huangang Wang
Categories: cs.LG cs.AI
\\
  Recent advancements have introduced machine learning frameworks to enhance
the Branch and Bound (B\&B) branching policies for solving Mixed Integer Linear
Programming (MILP). These methods, primarily relying on imitation learning of
Strong Branching, have shown superior performance. However, collecting expert
samples for imitation learning, particularly for Strong Branching, is a
time-consuming endeavor. To address this challenge, we propose
\textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for
\textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs
(AMILPs) by applying variable shifting to limited expert data from their
original MILPs. This approach enables the acquisition of a considerable number
of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for
imitation learning and employs contrastive learning to enhance the model's
ability to capture MILP features, thereby improving the quality of branching
decisions. Experimental results demonstrate that CAMBranch, trained with only
10\% of the complete dataset, exhibits superior performance. Ablation studies
further validate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.03647 ,  1882kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03655
Date: Tue, 6 Feb 2024 03:06:06 GMT   (814kb,D)

Title: Operator SVD with Neural Networks via Nested Low-Rank Approximation
Authors: J. Jon Ryu, Xiangxiang Xu, H. S. Melihcan Erol, Yuheng Bu, Lizhong
  Zheng, Gregory W. Wornell
Categories: cs.LG cs.NA math.NA stat.ML
Comments: 44 pages, 7 figures
\\
  Computing eigenvalue decomposition (EVD) of a given linear operator, or
finding its leading eigenvalues and eigenfunctions, is a fundamental task in
many machine learning and scientific computing problems. For high-dimensional
eigenvalue problems, training neural networks to parameterize the
eigenfunctions is considered as a promising alternative to the classical
numerical linear algebra techniques. This paper proposes a new optimization
framework based on the low-rank approximation characterization of a truncated
singular value decomposition, accompanied by new techniques called nesting for
learning the top-$L$ singular values and singular functions in the correct
order. The proposed method promotes the desired orthogonality in the learned
functions implicitly and efficiently via an unconstrained optimization
formulation, which is easy to solve with off-the-shelf gradient-based
optimization algorithms. We demonstrate the effectiveness of the proposed
optimization framework for use cases in computational physics and machine
learning.
\\ ( https://arxiv.org/abs/2402.03655 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03659
Date: Tue, 6 Feb 2024 03:18:58 GMT   (3325kb,D)

Title: Learning to Generate Explainable Stock Predictions using Self-Reflective
  Large Language Models
Authors: Kelvin J.L. Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua
Categories: cs.LG cs.CL q-fin.ST
Comments: WWW 2024
\\
  Explaining stock predictions is generally a difficult task for traditional
non-generative deep learning models, where explanations are limited to
visualizing the attention weights on important texts. Today, Large Language
Models (LLMs) present a solution to this problem, given their known
capabilities to generate human-readable explanations for their decision-making
process. However, the task of stock prediction remains challenging for LLMs, as
it requires the ability to weigh the varying impacts of chaotic social texts on
stock prices. The problem gets progressively harder with the introduction of
the explanation component, which requires LLMs to explain verbally why certain
factors are more important than the others. On the other hand, to fine-tune
LLMs for such a task, one would need expert-annotated samples of explanation
for every stock movement in the training set, which is expensive and
impractical to scale. To tackle these issues, we propose our
Summarize-Explain-Predict (SEP) framework, which utilizes a self-reflective
agent and Proximal Policy Optimization (PPO) to let a LLM teach itself how to
generate explainable stock predictions in a fully autonomous manner. The
reflective agent learns how to explain past stock movements through
self-reasoning, while the PPO trainer trains the model to generate the most
likely explanations from input texts. The training samples for the PPO trainer
are also the responses generated during the reflective process, which
eliminates the need for human annotators. Using our SEP framework, we fine-tune
a LLM that can outperform both traditional deep-learning and LLM methods in
prediction accuracy and Matthews correlation coefficient for the stock
classification task. To justify the generalization capability of our framework,
we further test it on the portfolio construction task, and demonstrate its
effectiveness through various portfolio metrics.
\\ ( https://arxiv.org/abs/2402.03659 ,  3325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03660
Date: Tue, 6 Feb 2024 03:28:36 GMT   (22063kb,D)

Title: Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm
Authors: Zhanpeng Zhou, Zijun Chen, Yilan Chen, Bo Zhang, Junchi Yan
Categories: cs.LG cs.AI
Comments: 27 pages, 24 figures
\\
  The pretraining-finetuning paradigm has become the prevailing trend in modern
deep learning. In this work, we discover an intriguing linear phenomenon in
models that are initialized from a common pretrained checkpoint and finetuned
on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we
linearly interpolate the weights of two finetuned models, the features in the
weight-interpolated model are approximately equal to the linear interpolation
of features in two finetuned models at each layer. Such cross-task linearity
has not been noted in peer literature. We provide comprehensive empirical
evidence supporting that CTL consistently occurs for finetuned models that
start from the same pretrained checkpoint. We conjecture that in the
pretraining-finetuning paradigm, neural networks essentially function as linear
maps, mapping from the parameter space to the feature space. Based on this
viewpoint, our study unveils novel insights into explaining model
merging/editing, particularly by translating operations from the parameter
space to the feature space. Furthermore, we delve deeper into the underlying
factors for the emergence of CTL, emphasizing the impact of pretraining.
\\ ( https://arxiv.org/abs/2402.03660 ,  22063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03661
Date: Tue, 6 Feb 2024 03:31:28 GMT   (4872kb,D)

Title: Transductive Reward Inference on Graph
Authors: Bohao Qu, Xiaofeng Cao, Qing Guo, Yi Chang, Ivor W. Tsang, Chengqi
  Zhang
Categories: cs.LG cs.AI
\\
  In this study, we present a transductive inference approach on that reward
information propagation graph, which enables the effective estimation of
rewards for unlabelled data in offline reinforcement learning. Reward inference
is the key to learning effective policies in practical scenarios, while direct
environmental interactions are either too costly or unethical and the reward
functions are rarely accessible, such as in healthcare and robotics. Our
research focuses on developing a reward inference method based on the
contextual properties of information propagation on graphs that capitalizes on
a constrained number of human reward annotations to infer rewards for
unlabelled data. We leverage both the available data and limited reward
annotations to construct a reward propagation graph, wherein the edge weights
incorporate various influential factors pertaining to the rewards.
Subsequently, we employ the constructed graph for transductive reward
inference, thereby estimating rewards for unlabelled data. Furthermore, we
establish the existence of a fixed point during several iterations of the
transductive inference process and demonstrate its at least convergence to a
local optimum. Empirical evaluations on locomotion and robotic manipulation
tasks validate the effectiveness of our approach. The application of our
inferred rewards improves the performance in offline reinforcement learning
tasks.
\\ ( https://arxiv.org/abs/2402.03661 ,  4872kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03663
Date: Tue, 6 Feb 2024 03:33:50 GMT   (344kb,D)

Title: Symbol Correctness in Deep Neural Networks Containing Symbolic Layers
Authors: Aaron Bembenek, Toby Murray
Categories: cs.LG cs.AI
\\
  To handle AI tasks that combine perception and logical reasoning, recent work
introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in
addition to traditional neural layers -- symbolic layers: symbolic expressions
(e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers
during inference. We identify and formalize an intuitive, high-level principle
that can guide the design and analysis of NS-DNNs: symbol correctness, the
correctness of the intermediate symbols predicted by the neural layers with
respect to a (generally unknown) ground-truth symbolic representation of the
input data. We demonstrate that symbol correctness is a necessary property for
NS-DNN explainability and transfer learning (despite being in general
impossible to train for). Moreover, we show that the framework of symbol
correctness provides a precise way to reason and communicate about model
behavior at neural-symbolic boundaries, and gives insight into the fundamental
tradeoffs faced by NS-DNN training algorithms. In doing so, we both identify
significant points of ambiguity in prior work, and provide a framework to
support further NS-DNN developments.
\\ ( https://arxiv.org/abs/2402.03663 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03664
Date: Tue, 6 Feb 2024 03:36:05 GMT   (10558kb,D)

Title: Efficient Solvers for Partial Gromov-Wasserstein
Authors: Yikun Bai, Rocio Diaz Martin, Hengrong Du, Ashkan Shahbazi, and Soheil
  Kolouri
Categories: cs.LG stat.ML
\\
  The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of
measures with unequal masses residing in potentially distinct metric spaces,
thereby enabling unbalanced and partial matching across these spaces. In this
paper, we demonstrate that the PGW problem can be transformed into a variant of
the Gromov-Wasserstein problem, akin to the conversion of the partial optimal
transport problem into an optimal transport problem. This transformation leads
to two new solvers, mathematically and computationally equivalent, based on the
Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We
further establish that the PGW problem constitutes a metric for metric measure
spaces. Finally, we validate the effectiveness of our proposed solvers in terms
of computation time and performance on shape-matching and positive-unlabeled
learning problems, comparing them against existing baselines.
\\ ( https://arxiv.org/abs/2402.03664 ,  10558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03687
Date: Tue, 6 Feb 2024 04:17:44 GMT   (1359kb,D)

Title: Pard: Permutation-Invariant Autoregressive Diffusion for Graph
  Generation
Authors: Lingxiao Zhao, Xueying Ding, Leman Akoglu
Categories: cs.LG stat.ML
Comments: Diffusion Model on Graphs
\\
  Graph generation has been dominated by autoregressive models due to their
simplicity and effectiveness, despite their sensitivity to ordering. Yet
diffusion models have garnered increasing attention, as they offer comparable
performance while being permutation-invariant. Current graph diffusion models
generate graphs in a one-shot fashion, but they require extra features and
thousands of denoising steps to achieve optimal performance. We introduce PARD,
a Permutation-invariant Auto Regressive Diffusion model that integrates
diffusion models with autoregressive methods. PARD harnesses the effectiveness
and efficiency of the autoregressive model while maintaining permutation
invariance without ordering sensitivity. Specifically, we show that contrary to
sets, elements in a graph are not entirely unordered and there is a unique
partial order for nodes and edges. With this partial order, PARD generates a
graph in a block-by-block, autoregressive fashion, where each block's
probability is conditionally modeled by a shared diffusion model with an
equivariant network. To ensure efficiency while being expressive, we further
propose a higher-order graph transformer, which integrates transformer with
PPGN. Like GPT, we extend the higher-order graph transformer to support
parallel training of all blocks. Without any extra features, PARD achieves
state-of-the-art performance on molecular and non-molecular datasets, and
scales to large datasets like MOSES containing 1.9M molecules.
\\ ( https://arxiv.org/abs/2402.03687 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03698
Date: Tue, 6 Feb 2024 04:37:09 GMT   (4871kb,D)

Title: Estimating the Local Learning Coefficient at Scale
Authors: Zach Furman, Edmund Lau
Categories: cs.LG stat.ML
Comments: 23 pages, 12 figures
MSC-class: 68T07, 14B05, 62F15
\\
  The \textit{local learning coefficient} (LLC) is a principled way of
quantifying model complexity, originally derived in the context of Bayesian
statistics using singular learning theory (SLT). Several methods are known for
numerically estimating the local learning coefficient, but so far these methods
have not been extended to the scale of modern deep learning architectures or
data sets. Using a method developed in {\tt arXiv:2308.12108 [stat.ML]} we
empirically show how the LLC may be measured accurately and self-consistently
for deep linear networks (DLNs) up to 100M parameters. We also show that the
estimated LLC has the rescaling invariance that holds for the theoretical
quantity.
\\ ( https://arxiv.org/abs/2402.03698 ,  4871kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03701
Date: Tue, 6 Feb 2024 04:42:36 GMT   (574kb,D)

Title: Improving and Unifying Discrete&Continuous-time Discrete Denoising
  Diffusion
Authors: Lingxiao Zhao, Xueying Ding, Lijun Yu, Leman Akoglu
Categories: cs.LG stat.ML
Comments: Unify Discrete Denoising Diffusion
\\
  Discrete diffusion models have seen a surge of attention with applications on
naturally discrete data such as language and graphs. Although discrete-time
discrete diffusion has been established for a while, only recently Campbell et
al. (2022) introduced the first framework for continuous-time discrete
diffusion. However, their training and sampling processes differ significantly
from the discrete-time version, necessitating nontrivial approximations for
tractability. In this paper, we first present a series of mathematical
simplifications of the variational lower bound that enable more accurate and
easy-to-optimize training for discrete diffusion. In addition, we derive a
simple formulation for backward denoising that enables exact and accelerated
sampling, and importantly, an elegant unification of discrete-time and
continuous-time discrete diffusion. Thanks to simpler analytical formulations,
both forward and now also backward probabilities can flexibly accommodate any
noise distribution, including different noise distributions for multi-element
objects. Experiments show that our proposed USD3 (for Unified Simplified
Discrete Denoising Diffusion) outperform all SOTA baselines on established
datasets. We open-source our unified code at
https://github.com/LingxiaoShawn/USD3.
\\ ( https://arxiv.org/abs/2402.03701 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03715
Date: Tue, 6 Feb 2024 05:11:38 GMT   (5956kb,D)

Title: Clarify: Improving Model Robustness With Natural Language Corrections
Authors: Yoonho Lee, Michelle S. Lam, Helena Vasconcelos, Michael S. Bernstein,
  Chelsea Finn
Categories: cs.LG cs.AI cs.CL
\\
  In supervised learning, models are trained to extract correlations from a
static dataset. This often leads to models that rely on high-level
misconceptions. To prevent such misconceptions, we must necessarily provide
additional information beyond the training data. Existing methods incorporate
forms of additional instance-level supervision, such as labels for spurious
features or additional labeled data from a balanced distribution. Such
strategies can become prohibitively costly for large-scale datasets since they
require additional annotation at a scale close to the original training data.
We hypothesize that targeted natural language feedback about a model's
misconceptions is a more efficient form of additional supervision. We introduce
Clarify, a novel interface and method for interactively correcting model
misconceptions. Through Clarify, users need only provide a short text
description to describe a model's consistent failure patterns. Then, in an
entirely automated way, we use such descriptions to improve the training
process by reweighting the training data or gathering additional targeted data.
Our user studies show that non-expert users can successfully describe model
misconceptions via Clarify, improving worst-group accuracy by an average of
17.1% in two datasets. Additionally, we use Clarify to find and rectify 31
novel hard subpopulations in the ImageNet dataset, improving minority-split
accuracy from 21.1% to 28.7%.
\\ ( https://arxiv.org/abs/2402.03715 ,  5956kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03720
Date: Tue, 6 Feb 2024 05:29:05 GMT   (126kb,D)

Title: Similarity-based Neighbor Selection for Graph LLMs
Authors: Rui Li, Jiwei Li, Jiawei Han, Guoyin Wang
Categories: cs.LG cs.AI cs.CL cs.SI
\\
  Text-attributed graphs (TAGs) present unique challenges for direct processing
by Language Learning Models (LLMs), yet their extensive commonsense knowledge
and robust reasoning capabilities offer great promise for node classification
in TAGs. Prior research in this field has grappled with issues such as
over-squashing, heterophily, and ineffective graph information integration,
further compounded by inconsistencies in dataset partitioning and
underutilization of advanced LLMs. To address these challenges, we introduce
Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor
selection techniques, SNS effectively improves the quality of selected
neighbors, thereby improving graph representation and alleviating issues like
over-squashing and heterophily. Besides, as an inductive and training-free
approach, SNS demonstrates superior generalization and scalability over
traditional GNN methods. Our comprehensive experiments, adhering to standard
dataset partitioning practices, demonstrate that SNS, through simple prompt
interactions with LLMs, consistently outperforms vanilla GNNs and achieves
state-of-the-art results on datasets like PubMed in node classification,
showcasing LLMs' potential in graph structure understanding. Our research
further underscores the significance of graph structure integration in LLM
applications and identifies key factors for their success in node
classification. Code is available at https://github.com/ruili33/SNS.
\\ ( https://arxiv.org/abs/2402.03720 ,  126kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03726
Date: Tue, 6 Feb 2024 05:46:51 GMT   (1252kb,D)

Title: Learning Granger Causality from Instance-wise Self-attentive Hawkes
  Processes
Authors: Dongxia Wu, Tsuyoshi Id\'e, Aur\'elie Lozano, Georgios Kollias,
  Ji\v{r}\'i Navr\'atil, Naoki Abe, Yi-An Ma, Rose Yu
Categories: cs.LG stat.ML
\\
  We address the problem of learning Granger causality from asynchronous,
interdependent, multi-type event sequences. In particular, we are interested in
discovering instance-level causal structures in an unsupervised manner.
Instance-level causality identifies causal relationships among individual
events, providing more fine-grained information for decision-making. Existing
work in the literature either requires strong assumptions, such as linearity in
the intensity function, or heuristically defined model parameters that do not
necessarily meet the requirements of Granger causality. We propose
Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning
framework that can directly infer the Granger causality at the event instance
level. ISAHP is the first neural point process model that meets the
requirements of Granger causality. It leverages the self-attention mechanism of
the transformer to align with the principles of Granger causality. We
empirically demonstrate that ISAHP is capable of discovering complex
instance-level causal structures that cannot be handled by classical models. We
also show that ISAHP achieves state-of-the-art performance in proxy tasks
involving type-level causal discovery and instance-level event type prediction.
\\ ( https://arxiv.org/abs/2402.03726 ,  1252kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03737
Date: Tue, 6 Feb 2024 06:10:46 GMT   (28kb)

Title: Differentially Private High Dimensional Bandits
Authors: Apurv Shukla
Categories: cs.LG cs.CR cs.SY eess.SY math.OC stat.ML
\\
  We consider a high-dimensional stochastic contextual linear bandit problem
when the parameter vector is $s_{0}$-sparse and the decision maker is subject
to privacy constraints under both central and local models of differential
privacy. We present PrivateLASSO, a differentially private LASSO bandit
algorithm. PrivateLASSO is based on two sub-routines: (i) a sparse
hard-thresholding-based privacy mechanism and (ii) an episodic thresholding
rule for identifying the support of the parameter $\theta$. We prove minimax
private lower bounds and establish privacy and utility guarantees for
PrivateLASSO for the central model under standard assumptions.
\\ ( https://arxiv.org/abs/2402.03737 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03741
Date: Tue, 6 Feb 2024 06:18:16 GMT   (2155kb,D)

Title: SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent
  Reinforcement Learning Systems
Authors: Oubo Ma, Yuwen Pu, Linkang Du, Yang Dai, Ruo Wang, Xiaolei Liu,
  Yingcai Wu, Shouling Ji
Categories: cs.LG cs.AI cs.CR
\\
  Recent advances in multi-agent reinforcement learning (MARL) have opened up
vast application prospects, including swarm control of drones, collaborative
manipulation by robotic arms, and multi-target encirclement. However, potential
security threats during the MARL deployment need more attention and thorough
investigation. Recent researches reveal that an attacker can rapidly exploit
the victim's vulnerabilities and generate adversarial policies, leading to the
victim's failure in specific tasks. For example, reducing the winning rate of a
superhuman-level Go AI to around 20%. They predominantly focus on two-player
competitive environments, assuming attackers possess complete global state
observation.
  In this study, we unveil, for the first time, the capability of attackers to
generate adversarial policies even when restricted to partial observations of
the victims in multi-agent competitive environments. Specifically, we propose a
novel black-box attack (SUB-PLAY), which incorporates the concept of
constructing multiple subgames to mitigate the impact of partial observability
and suggests the sharing of transitions among subpolicies to improve the
exploitative ability of attackers. Extensive evaluations demonstrate the
effectiveness of SUB-PLAY under three typical partial observability
limitations. Visualization results indicate that adversarial policies induce
significantly different activations of the victims' policy networks.
Furthermore, we evaluate three potential defenses aimed at exploring ways to
mitigate security threats posed by adversarial policies, providing constructive
recommendations for deploying MARL in competitive environments.
\\ ( https://arxiv.org/abs/2402.03741 ,  2155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03747
Date: Tue, 6 Feb 2024 06:28:17 GMT   (3341kb)

Title: An invariance constrained deep learning network for PDE discovery
Authors: Chao Chen, Hui Li, Xiaowei Jin
Categories: cs.LG
\\
  The discovery of partial differential equations (PDEs) from datasets has
attracted increased attention. However, the discovery of governing equations
from sparse data with high noise is still very challenging due to the
difficulty of derivatives computation and the disturbance of noise. Moreover,
the selection principles for the candidate library to meet physical laws need
to be further studied. The invariance is one of the fundamental laws for
governing equations. In this study, we propose an invariance constrained deep
learning network (ICNet) for the discovery of PDEs. Considering that temporal
and spatial translation invariance (Galilean invariance) is a fundamental
property of physical laws, we filter the candidates that cannot meet the
requirement of the Galilean transformations. Subsequently, we embedded the
fixed and possible terms into the loss function of neural network,
significantly countering the effect of sparse data with high noise. Then, by
filtering out redundant terms without fixing learnable parameters during the
training process, the governing equations discovered by the ICNet method can
effectively approximate the real governing equations. We select the 2D Burgers
equation, the equation of 2D channel flow over an obstacle, and the equation of
3D intracranial aneurysm as examples to verify the superiority of the ICNet for
fluid mechanics. Furthermore, we extend similar invariance methods to the
discovery of wave equation (Lorentz Invariance) and verify it through Single
and Coupled Klein-Gordon equation. The results show that the ICNet method with
physical constraints exhibits excellent performance in governing equations
discovery from sparse and noisy data.
\\ ( https://arxiv.org/abs/2402.03747 ,  3341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03750
Date: Tue, 6 Feb 2024 06:37:43 GMT   (520kb,D)

Title: Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning
  Approach
Authors: Xin Chen, Mingliang Hou, Tao Tang, Achhardeep Kaur and Feng Xia
Categories: cs.LG cs.AI cs.HC
Comments: 10 pages, 7 figures
MSC-class: 68T09, 68T30, 68U35
ACM-class: I.2.6; I.2.4; H.1.2
Journal-ref: The 7th IEEE International Conference on Data Science and Systems
  (DSS), Dec 20 - 22, 2021, Haikou, China
DOI: 10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00182
\\
  With the arrival of the big data era, mobility profiling has become a viable
method of utilizing enormous amounts of mobility data to create an intelligent
transportation system. Mobility profiling can extract potential patterns in
urban traffic from mobility data and is critical for a variety of
traffic-related applications. However, due to the high level of complexity and
the huge amount of data, mobility profiling faces huge challenges. Digital Twin
(DT) technology paves the way for cost-effective and performance-optimised
management by digitally creating a virtual representation of the network to
simulate its behaviour. In order to capture the complex spatio-temporal
features in traffic scenario, we construct alignment diagrams to assist in
completing the spatio-temporal correlation representation and design dilated
alignment convolution network (DACN) to learn the fine-grained correlations,
i.e., spatio-temporal interactions. We propose a digital twin mobility
profiling (DTMP) framework to learn node profiles on a mobility network DT
model. Extensive experiments have been conducted upon three real-world
datasets. Experimental results demonstrate the effectiveness of DTMP.
\\ ( https://arxiv.org/abs/2402.03750 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03753
Date: Tue, 6 Feb 2024 06:42:51 GMT   (6950kb,D)

Title: Enhanced sampling of robust molecular datasets with uncertainty-based
  collective variables
Authors: Aik Rui Tan, Johannes C. B. Dietschreit, Rafael Gomez-Bombarelli
Categories: cs.LG physics.comp-ph
Comments: 13 pages, 4 figures, 10 pages of Supplementary Information
\\
  Generating a data set that is representative of the accessible configuration
space of a molecular system is crucial for the robustness of machine learned
interatomic potentials (MLIP). However, the complexity of molecular systems,
characterized by intricate potential energy surfaces (PESs) with numerous local
minima and energy barriers, presents a significant challenge. Traditional
methods of data generation, such as random sampling or exhaustive exploration,
are either intractable or may not capture rare, but highly informative
configurations. In this study, we propose a method that leverages uncertainty
as the collective variable (CV) to guide the acquisition of chemically-relevant
data points, focusing on regions of the configuration space where ML model
predictions are most uncertain. This approach employs a Gaussian Mixture
Model-based uncertainty metric from a single model as the CV for biased
molecular dynamics simulations. The effectiveness of our approach in overcoming
energy barriers and exploring unseen energy minima, thereby enhancing the data
set in an active learning framework, is demonstrated on the alanine dipeptide
benchmark system.
\\ ( https://arxiv.org/abs/2402.03753 ,  6950kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03770
Date: Tue, 6 Feb 2024 07:25:21 GMT   (208kb,D)

Title: Fed-CVLC: Compressing Federated Learning Communications with
  Variable-Length Codes
Authors: Xiaoxin Su, Yipeng Zhou, Laizhong Cui, John C.S. Lui and Jiangchuan
  Liu
Categories: cs.LG
Comments: To appear in 2024 IEEE International Conference on Computer
  Communications(INFOCOM 2024)
\\
  In Federated Learning (FL) paradigm, a parameter server (PS) concurrently
communicates with distributed participating clients for model collection,
update aggregation, and model distribution over multiple rounds, without
touching private data owned by individual clients. FL is appealing in
preserving data privacy; yet the communication between the PS and scattered
clients can be a severe bottleneck. Model compression algorithms, such as
quantization and sparsification, have been suggested but they generally assume
a fixed code length, which does not reflect the heterogeneity and variability
of model updates. In this paper, through both analysis and experiments, we show
strong evidences that variable-length is beneficial for compression in FL. We
accordingly present Fed-CVLC (Federated Learning Compression with
Variable-Length Codes), which fine-tunes the code length in response of the
dynamics of model updates. We develop optimal tuning strategy that minimizes
the loss function (equivalent to maximizing the model utility) subject to the
budget for communication. We further demonstrate that Fed-CVLC is indeed a
general compression design that bridges quantization and sparsification, with
greater flexibility. Extensive experiments have been conducted with public
datasets to demonstrate that Fed-CVLC remarkably outperforms state-of-the-art
baselines, improving model utility by 1.50%-5.44%, or shrinking communication
traffic by 16.67%-41.61%.
\\ ( https://arxiv.org/abs/2402.03770 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03771
Date: Tue, 6 Feb 2024 07:26:44 GMT   (3428kb,D)

Title: Reinforcement Learning from Bagged Reward: A Transformer-based Approach
  for Instance-Level Reward Redistribution
Authors: Yuting Tang and Xin-Qiang Cai and Yao-Xiang Ding and Qiyu Wu and
  Guoqing Liu and Masashi Sugiyama
Categories: cs.LG
\\
  In reinforcement Learning (RL), an instant reward signal is generated for
each action of the agent, such that the agent learns to maximize the cumulative
reward to obtain the optimal policy. However, in many real-world applications,
the instant reward signals are not obtainable by the agent. Instead, the
learner only obtains rewards at the ends of bags, where a bag is defined as a
partial sequence of a complete trajectory. In this situation, the learner has
to face the significant difficulty of exploring the unknown instant rewards in
the bags, which could not be addressed by existing approaches, including those
trajectory-based approaches that consider only complete trajectories and ignore
the inner reward distributions. To formally study this situation, we introduce
a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR),
where only the bagged rewards of sequences can be obtained. We provide the
theoretical study to establish the connection between RLBR and standard RL in
Markov Decision Processes (MDPs). To effectively explore the reward
distributions within the bagged rewards, we propose a Transformer-based reward
model, the Reward Bag Transformer (RBT), which uses the self-attention
mechanism for interpreting the contextual nuances and temporal dependencies
within each bag. Extensive experimental analyses demonstrate the superiority of
our method, particularly in its ability to mimic the original MDP's reward
distribution, highlighting its proficiency in contextual understanding and
adaptability to environmental dynamics.
\\ ( https://arxiv.org/abs/2402.03771 ,  3428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03774
Date: Tue, 6 Feb 2024 07:40:53 GMT   (1110kb,D)

Title: Learning a Decision Tree Algorithm with Transformers
Authors: Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao
Categories: cs.LG cs.AI cs.CL
\\
  Decision trees are renowned for their interpretability capability to achieve
high predictive performance, especially on tabular data. Traditionally, they
are constructed through recursive algorithms, where they partition the data at
every node in a tree. However, identifying the best partition is challenging,
as decision trees optimized for local segments may not bring global
generalization. To address this, we introduce MetaTree, which trains a
transformer-based model on filtered outputs from classical algorithms to
produce strong decision trees for classification. Specifically, we fit both
greedy decision trees and optimized decision trees on a large number of
datasets. We then train MetaTree to produce the trees that achieve strong
generalization performance. This training enables MetaTree to not only emulate
these algorithms, but also to intelligently adapt its strategy according to the
context, thereby achieving superior generalization performance.
\\ ( https://arxiv.org/abs/2402.03774 ,  1110kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03784
Date: Tue, 6 Feb 2024 07:55:54 GMT   (13576kb,D)

Title: AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality
  Prediction
Authors: Kethmi Hirushini Hettige, Jiahao Ji, Shili Xiang, Cheng Long, Gao
  Cong, Jingyuan Wang
Categories: cs.LG cs.AI physics.app-ph
Comments: Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)
\\
  Air quality prediction and modelling plays a pivotal role in public health
and environment management, for individuals and authorities to make informed
decisions. Although traditional data-driven models have shown promise in this
domain, their long-term prediction accuracy can be limited, especially in
scenarios with sparse or incomplete data and they often rely on black-box deep
learning structures that lack solid physical foundation leading to reduced
transparency and interpretability in predictions. To address these limitations,
this paper presents a novel approach named Physics guided Neural Network for
Air Quality Prediction (AirPhyNet). Specifically, we leverage two
well-established physics principles of air particle movement (diffusion and
advection) by representing them as differential equation networks. Then, we
utilize a graph structure to integrate physics knowledge into a neural network
architecture and exploit latent representations to capture spatio-temporal
relationships within the air quality data. Experiments on two real-world
benchmark datasets demonstrate that AirPhyNet outperforms state-of-the-art
models for different testing scenarios including different lead time (24h, 48h,
72h), sparse data and sudden change prediction, achieving reduction in
prediction errors up to 10%. Moreover, a case study further validates that our
model captures underlying physical processes of particle movement and generates
accurate predictions with real physical meaning.
\\ ( https://arxiv.org/abs/2402.03784 ,  13576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03785
Date: Tue, 6 Feb 2024 07:57:13 GMT   (7348kb,D)

Title: Weakly Supervised Anomaly Detection via Knowledge-Data Alignment
Authors: Haihong Zhao, Chenyi Zi, Yang Liu, Chen Zhang, Yan Zhou and Jia Li
Categories: cs.LG
Comments: Accepted by WWW 2024
\\
  Anomaly detection (AD) plays a pivotal role in numerous web-based
applications, including malware detection, anti-money laundering, device
failure detection, and network fault analysis. Most methods, which rely on
unsupervised learning, are hard to reach satisfactory detection accuracy due to
the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been
introduced with a limited number of labeled anomaly samples to enhance model
performance. Nevertheless, it is still challenging for models, trained on an
inadequate amount of labeled data, to generalize to unseen anomalies. In this
paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to
integrate rule knowledge, typically summarized by human experts, to supplement
the limited labeled data. Specifically, we transpose these rules into the
knowledge space and subsequently recast the incorporation of knowledge as the
alignment of knowledge and data. To facilitate this alignment, we employ the
Optimal Transport (OT) technique. We then incorporate the OT distance as an
additional loss term to the original objective function of WSAD methodologies.
Comprehensive experimental results on five real-world datasets demonstrate that
our proposed KDAlign framework markedly surpasses its state-of-the-art
counterparts, achieving superior performance across various anomaly types.
\\ ( https://arxiv.org/abs/2402.03785 ,  7348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03792
Date: Tue, 6 Feb 2024 08:18:14 GMT   (535kb,D)

Title: No-Regret Reinforcement Learning in Smooth MDPs
Authors: Davide Maran, Alberto Maria Metelli, Matteo Papini, Marcello Restell
Categories: cs.LG cs.AI
\\
  Obtaining no-regret guarantees for reinforcement learning (RL) in the case of
problems with continuous state and/or action spaces is still one of the major
open challenges in the field. Recently, a variety of solutions have been
proposed, but besides very specific settings, the general problem remains
unsolved. In this paper, we introduce a novel structural assumption on the
Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes
most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To
face this challenging scenario, we propose two algorithms for regret
minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of
constructing an MDP representation through an orthogonal feature map based on
Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives
the no-regret property under weaker assumptions but is computationally
inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial
time, although for a smaller class of problems. After analyzing their regret
properties, we compare our results with state-of-the-art ones from RL theory,
showing that our algorithms achieve the best guarantees.
\\ ( https://arxiv.org/abs/2402.03792 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03804
Date: Tue, 6 Feb 2024 08:45:51 GMT   (1595kb,D)

Title: ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse
  LLMs
Authors: Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun
  Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong Sun
Categories: cs.LG cs.AI
\\
  Sparse computation offers a compelling solution for the inference of Large
Language Models (LLMs) in low-resource scenarios by dynamically skipping the
computation of inactive neurons. While traditional approaches focus on
ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of
sparse LLMs beyond zero activation values. We introduce a general method that
defines neuron activation through neuron output magnitudes and a tailored
magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse
activation. To find the most efficient activation function for sparse
computation, we propose a systematic framework to examine the sparsity of LLMs
from three aspects: the trade-off between sparsity and performance, the
predictivity of sparsity, and the hardware affinity. We conduct thorough
experiments on LLMs utilizing different activation functions, including ReLU,
SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing
ReLU$^2$ excel across all three evaluation aspects, highlighting its potential
as an efficient activation function for sparse LLMs. We will release the code
to facilitate future research.
\\ ( https://arxiv.org/abs/2402.03804 ,  1595kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03807
Date: Tue, 6 Feb 2024 08:48:01 GMT   (7091kb,D)

Title: SEABO: A Simple Search-Based Method for Offline Imitation Learning
Authors: Jiafei Lyu, Xiaoteng Ma, Le Wan, Runze Liu, Xiu Li, Zongqing Lu
Categories: cs.LG cs.AI
Comments: To appear in ICLR2024
\\
  Offline reinforcement learning (RL) has attracted much attention due to its
ability in learning from static offline datasets and eliminating the need of
interacting with the environment. Nevertheless, the success of offline RL
relies heavily on the offline transitions annotated with reward labels. In
practice, we often need to hand-craft the reward function, which is sometimes
difficult, labor-intensive, or inefficient. To tackle this challenge, we set
our focus on the offline imitation learning (IL) setting, and aim at getting a
reward function based on the expert data and unlabeled data. To that end, we
propose a simple yet effective search-based offline IL method, tagged SEABO.
SEABO allocates a larger reward to the transition that is close to its closest
neighbor in the expert demonstration, and a smaller reward otherwise, all in an
unsupervised learning manner. Experimental results on a variety of D4RL
datasets indicate that SEABO can achieve competitive performance to offline RL
algorithms with ground-truth rewards, given only a single expert trajectory,
and can outperform prior reward learning and offline IL methods across many
tasks. Moreover, we demonstrate that SEABO also works well if the expert
demonstrations contain only observations. Our code is publicly available at
https://github.com/dmksjfl/SEABO.
\\ ( https://arxiv.org/abs/2402.03807 ,  7091kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03814
Date: Tue, 6 Feb 2024 08:57:49 GMT   (3011kb,D)

Title: Masked Graph Autoencoder with Non-discrete Bandwidths
Authors: Ziwen Zhao, Yuhua Li, Yixiong Zou, Jiliang Tang, Ruixuan Li
Categories: cs.LG cs.SI
Comments: Full version (17 pages, 8 figures, 12 tables), accepted by TheWebConf
  2024 (WWW 2024)
\\
  Masked graph autoencoders have emerged as a powerful graph self-supervised
learning method that has yet to be fully explored. In this paper, we unveil
that the existing discrete edge masking and binary link reconstruction
strategies are insufficient to learn topologically informative representations,
from the perspective of message propagation on graph neural networks. These
limitations include blocking message flows, vulnerability to over-smoothness,
and suboptimal neighborhood discriminability. Inspired by these understandings,
we explore non-discrete edge masks, which are sampled from a continuous and
dispersive probability distribution instead of the discrete Bernoulli
distribution. These masks restrict the amount of output messages for each edge,
referred to as "bandwidths". We propose a novel, informative, and effective
topological masked graph autoencoder using bandwidth masking and a layer-wise
bandwidth prediction objective. We demonstrate its powerful graph topological
learning ability both theoretically and empirically. Our proposed framework
outperforms representative baselines in both self-supervised link prediction
(improving the discrete edge reconstructors by at most 20%) and node
classification on numerous datasets, solely with a structure-learning pretext.
Our implementation is available at https://github.com/Newiz430/Bandana.
\\ ( https://arxiv.org/abs/2402.03814 ,  3011kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03815
Date: Tue, 6 Feb 2024 09:00:05 GMT   (216kb,D)

Title: Expediting In-Network Federated Learning by Voting-Based Consensus Model
  Compression
Authors: Xiaoxin Su, Yipeng Zhou, Laizhong Cui and Song Guo
Categories: cs.LG
Comments: To appear in 2024 IEEE International Conference on Computer
  Communications(INFOCOM 2024)
\\
  Recently, federated learning (FL) has gained momentum because of its
capability in preserving data privacy. To conduct model training by FL,
multiple clients exchange model updates with a parameter server via Internet.
To accelerate the communication speed, it has been explored to deploy a
programmable switch (PS) in lieu of the parameter server to coordinate clients.
The challenge to deploy the PS in FL lies in its scarce memory space,
prohibiting running memory consuming aggregation algorithms on the PS. To
overcome this challenge, we propose Federated Learning in-network Aggregation
with Compression (FediAC) algorithm, consisting of two phases: client voting
and model aggregating. In the former phase, clients report their significant
model update indices to the PS to estimate global significant model updates. In
the latter phase, clients upload global significant model updates to the PS for
aggregation. FediAC consumes much less memory space and communication traffic
than existing works because the first phase can guarantee consensus compression
across clients. The PS easily aligns model update indices to swiftly complete
aggregation in the second phase. Finally, we conduct extensive experiments by
using public datasets to demonstrate that FediAC remarkably surpasses the
state-of-the-art baselines in terms of model accuracy and communication
traffic.
\\ ( https://arxiv.org/abs/2402.03815 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03818
Date: Tue, 6 Feb 2024 09:07:26 GMT   (246kb,D)

Title: Asymptotic generalization error of a single-layer graph convolutional
  network
Authors: O. Duranthon, L. Zdeborov\'a
Categories: cs.LG cond-mat.dis-nn
\\
  While graph convolutional networks show great practical promises, the
theoretical understanding of their generalization properties as a function of
the number of samples is still in its infancy compared to the more broadly
studied case of supervised fully connected neural networks. In this article, we
predict the performances of a single-layer graph convolutional network (GCN)
trained on data produced by attributed stochastic block models (SBMs) in the
high-dimensional limit. Previously, only ridge regression on contextual-SBM
(CSBM) has been considered in Shi et al. 2022; we generalize the analysis to
arbitrary convex loss and regularization for the CSBM and add the analysis for
another data model, the neural-prior SBM. We also study the high
signal-to-noise ratio limit, detail the convergence rates of the GCN and show
that, while consistent, it does not reach the Bayes-optimal rate for any of the
considered cases.
\\ ( https://arxiv.org/abs/2402.03818 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03828
Date: Tue, 6 Feb 2024 09:17:07 GMT   (12654kb,D)

Title: Estimating Barycenters of Distributions with Neural Optimal Transport
Authors: Alexander Kolesov, Petr Mokrov, Igor Udovichenko, Milena Gazdieva,
  Gudmund Pammer, Evgeny Burnaev, Alexander Korotin
Categories: cs.LG
\\
  Given a collection of probability measures, a practitioner sometimes needs to
find an "average" distribution which adequately aggregates reference
distributions. A theoretically appealing notion of such an average is the
Wasserstein barycenter, which is the primal focus of our work. By building upon
the dual formulation of Optimal Transport (OT), we propose a new scalable
approach for solving the Wasserstein barycenter problem. Our methodology is
based on the recent Neural OT solver: it has bi-level adversarial learning
objective and works for general cost functions. These are key advantages of our
method, since the typical adversarial algorithms leveraging barycenter tasks
utilize tri-level optimization and focus mostly on quadratic cost. We also
establish theoretical error bounds for our proposed approach and showcase its
applicability and effectiveness on illustrative scenarios and image data
setups.
\\ ( https://arxiv.org/abs/2402.03828 ,  12654kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03845
Date: Tue, 6 Feb 2024 09:41:43 GMT   (9576kb,D)

Title: On gauge freedom, conservativity and intrinsic dimensionality estimation
  in diffusion models
Authors: Christian Horvat and Jean-Pascal Pfister
Categories: cs.LG
\\
  Diffusion models are generative models that have recently demonstrated
impressive performances in terms of sampling quality and density estimation in
high dimensions. They rely on a forward continuous diffusion process and a
backward continuous denoising process, which can be described by a
time-dependent vector field and is used as a generative model. In the original
formulation of the diffusion model, this vector field is assumed to be the
score function (i.e. it is the gradient of the log-probability at a given time
in the diffusion process). Curiously, on the practical side, most studies on
diffusion models implement this vector field as a neural network function and
do not constrain it be the gradient of some energy function (that is, most
studies do not constrain the vector field to be conservative). Even though some
studies investigated empirically whether such a constraint will lead to a
performance gain, they lead to contradicting results and failed to provide
analytical results. Here, we provide three analytical results regarding the
extent of the modeling freedom of this vector field. {Firstly, we propose a
novel decomposition of vector fields into a conservative component and an
orthogonal component which satisfies a given (gauge) freedom. Secondly, from
this orthogonal decomposition, we show that exact density estimation and exact
sampling is achieved when the conservative component is exactly equals to the
true score and therefore conservativity is neither necessary nor sufficient to
obtain exact density estimation and exact sampling. Finally, we show that when
it comes to inferring local information of the data manifold, constraining the
vector field to be conservative is desirable.
\\ ( https://arxiv.org/abs/2402.03845 ,  9576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03846
Date: Tue, 6 Feb 2024 09:48:33 GMT   (2668kb,D)

Title: Efficient Generation of Hidden Outliers for Improved Outlier Detection
Authors: Jose Cribeiro-Ramallo, Vadim Arzamasov, Klemens B\"ohm
Categories: cs.LG
\\
  Outlier generation is a popular technique used for solving important outlier
detection tasks. Generating outliers with realistic behavior is challenging.
Popular existing methods tend to disregard the 'multiple views' property of
outliers in high-dimensional spaces. The only existing method accounting for
this property falls short in efficiency and effectiveness. We propose BISECT, a
new outlier generation method that creates realistic outliers mimicking said
property. To do so, BISECT employs a novel proposition introduced in this
article stating how to efficiently generate said realistic outliers. Our method
has better guarantees and complexity than the current methodology for
recreating 'multiple views'. We use the synthetic outliers generated by BISECT
to effectively enhance outlier detection in diverse datasets, for multiple use
cases. For instance, oversampling with BISECT reduced the error by up to 3
times when compared with the baselines.
\\ ( https://arxiv.org/abs/2402.03846 ,  2668kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03855
Date: Tue, 6 Feb 2024 10:06:13 GMT   (1604kb,D)

Title: Position Paper: Toward New Frameworks for Studying Model Representations
Authors: Satvik Golechha, James Dao
Categories: cs.LG cs.AI
\\
  Mechanistic interpretability (MI) aims to understand AI models by
reverse-engineering the exact algorithms neural networks learn. Most works in
MI so far have studied behaviors and capabilities that are trivial and
token-aligned. However, most capabilities are not that trivial, which advocates
for the study of hidden representations inside these networks as the unit of
analysis. We do a literature review, formalize representations for features and
behaviors, highlight their importance and evaluation, and perform some basic
exploration in the mechanistic interpretability of representations. With
discussion and exploratory results, we justify our position that studying
representations is an important and under-studied field, and that currently
established methods in MI are not sufficient to understand representations,
thus pushing for the research community to work toward new frameworks for
studying representations.
\\ ( https://arxiv.org/abs/2402.03855 ,  1604kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03864
Date: Tue, 6 Feb 2024 10:24:36 GMT   (1371kb,D)

Title: The Challenges of the Nonlinear Regime for Physics-Informed Neural
  Networks
Authors: Andrea Bonfanti, Giuseppe Bruno, Cristina Cipriani
Categories: cs.LG
Comments: 8 pages, 10 figures, appendix of 10 additional pages
\\
  The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to
examine the training dynamics of Physics-Informed Neural Networks (PINNs) in
the infinite width limit. We leverage this perspective and focus on the case of
nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide
theoretical results on the different behaviors of the NTK depending on the
linearity of the differential operator. Moreover, inspired by our theoretical
results, we emphasize the advantage of employing second-order methods for
training PINNs. Additionally, we explore the convergence capabilities of
second-order methods and address the challenges of spectral bias and slow
convergence. Every theoretical result is supported by numerical examples with
both linear and nonlinear PDEs, and we validate our training method on
benchmark test cases.
\\ ( https://arxiv.org/abs/2402.03864 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03885
Date: Tue, 6 Feb 2024 10:48:46 GMT   (3105kb,D)

Title: MOMENT: A Family of Open Time-series Foundation Models
Authors: Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li,
  Artur Dubrawski
Categories: cs.LG cs.AI
\\
  We introduce MOMENT, a family of open-source foundation models for
general-purpose time-series analysis. Pre-training large models on time-series
data is challenging due to (1) the absence of a large and cohesive public
time-series repository, and (2) diverse time-series characteristics which make
multi-dataset training onerous. Additionally, (3) experimental benchmarks to
evaluate these models, especially in scenarios with limited resources, time,
and supervision, are still in their nascent stages. To address these
challenges, we compile a large and diverse collection of public time-series,
called the Time-series Pile, and systematically tackle time-series-specific
challenges to unlock large-scale multi-dataset pre-training. Finally, we build
on recent work to design a benchmark to evaluate time-series foundation models
on diverse tasks and datasets in limited supervision settings. Experiments on
this benchmark demonstrate the effectiveness of our pre-trained models with
minimal data and task-specific fine-tuning. Finally, we present several
interesting empirical observations about large pre-trained time-series models.
Our code is available anonymously at anonymous.4open.science/r/BETT-773F/.
\\ ( https://arxiv.org/abs/2402.03885 ,  3105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03902
Date: Tue, 6 Feb 2024 11:13:54 GMT   (910kb,D)

Title: A phase transition between positional and semantic learning in a
  solvable model of dot-product attention
Authors: Hugo Cui, Freya Behrens, Florent Krzakala, Lenka Zdeborov\'a
Categories: cs.LG
\\
  We investigate how a dot-product attention layer learns a positional
attention matrix (with tokens attending to each other based on their respective
positions) and a semantic attention matrix (with tokens attending to each other
based on their meaning). For an algorithmic task, we experimentally show how
the same simple architecture can learn to implement a solution using either the
positional or semantic mechanism. On the theoretical side, we study the
learning of a non-linear self-attention layer with trainable tied and low-rank
query and key matrices. In the asymptotic limit of high-dimensional data and a
comparably large number of training samples, we provide a closed-form
characterization of the global minimum of the non-convex empirical loss
landscape. We show that this minimum corresponds to either a positional or a
semantic mechanism and evidence an emergent phase transition from the former to
the latter with increasing sample complexity. Finally, we compare the
dot-product attention layer to linear positional baseline, and show that it
outperforms the latter using the semantic mechanism provided it has access to
sufficient data.
\\ ( https://arxiv.org/abs/2402.03902 ,  910kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03903
Date: Tue, 6 Feb 2024 11:13:57 GMT   (207kb,D)

Title: Compound Returns Reduce Variance in Reinforcement Learning
Authors: Brett Daley, Martha White, Marlos C. Machado
Categories: cs.LG
Comments: Preprint. 8 pages, 5 figures, 3 tables
\\
  Multistep returns, such as $n$-step returns and $\lambda$-returns, are
commonly used to improve the sample efficiency of reinforcement learning (RL)
methods. The variance of the multistep returns becomes the limiting factor in
their length; looking too far into the future increases variance and reverses
the benefits of multistep learning. In our work, we demonstrate the ability of
compound returns -- weighted averages of $n$-step returns -- to reduce
variance. We prove for the first time that any compound return with the same
contraction modulus as a given $n$-step return has strictly lower variance. We
additionally prove that this variance-reduction property improves the
finite-sample complexity of temporal-difference learning under linear function
approximation. Because general compound returns can be expensive to implement,
we introduce two-bootstrap returns which reduce variance while remaining
efficient, even when using minibatched experience replay. We conduct
experiments showing that two-bootstrap returns can improve the sample
efficiency of $n$-step deep RL agents, with little additional computational
cost.
\\ ( https://arxiv.org/abs/2402.03903 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03905
Date: Tue, 6 Feb 2024 11:17:16 GMT   (691kb)

Title: Employee Turnover Analysis Using Machine Learning Algorithms
Authors: Mahyar Karimi, Kamyar Seyedkazem Viliyani
Categories: cs.LG
Comments: 6 pages, 11 feagures, 2 tables
\\
  Employee's knowledge is an organization asset. Turnover may impose apparent
and hidden costs and irreparable damages. To overcome and mitigate this risk,
employee's condition should be monitored. Due to high complexity of analyzing
well-being features, employee's turnover predicting can be delegated to machine
learning techniques. In this paper, we discuss employee's attrition rate. Three
different supervised learning algorithms comprising AdaBoost, SVM and
RandomForest are used to benchmark employee attrition accuracy. Attained models
can help out at establishing predictive analytics.
\\ ( https://arxiv.org/abs/2402.03905 ,  691kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03915
Date: Tue, 6 Feb 2024 11:31:04 GMT   (11492kb,D)

Title: Learning Metrics that Maximise Power for Accelerated A/B-Tests
Authors: Olivier Jeunen and Aleksei Ustimenko
Categories: cs.LG cs.IR stat.AP stat.ML
\\
  Online controlled experiments are a crucial tool to allow for confident
decision-making in technology companies. A North Star metric is defined (such
as long-term revenue or user retention), and system variants that statistically
significantly improve on this metric in an A/B-test can be considered superior.
North Star metrics are typically delayed and insensitive. As a result, the cost
of experimentation is high: experiments need to run for a long time, and even
then, type-II errors (i.e. false negatives) are prevalent.
  We propose to tackle this by learning metrics from short-term signals that
directly maximise the statistical power they harness with respect to the North
Star. We show that existing approaches are prone to overfitting, in that higher
average metric sensitivity does not imply improved type-II errors, and propose
to instead minimise the $p$-values a metric would have produced on a log of
past experiments. We collect such datasets from two social media applications
with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.
Empirical results show that we are able to increase statistical power by up to
78% when using our learnt metrics stand-alone, and by up to 210% when used in
tandem with the North Star. Alternatively, we can obtain constant statistical
power at a sample size that is down to 12% of what the North Star requires,
significantly reducing the cost of experimentation.
\\ ( https://arxiv.org/abs/2402.03915 ,  11492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03921
Date: Tue, 6 Feb 2024 11:44:06 GMT   (9596kb,D)

Title: Large Language Models to Enhance Bayesian Optimization
Authors: Tennison Liu and Nicol\'as Astorga and Nabeel Seedat and Mihaela van
  der Schaar
Categories: cs.LG cs.AI
Comments: Accepted as Poster at ICLR2024
\\
  Bayesian optimization (BO) is a powerful approach for optimizing complex and
expensive-to-evaluate black-box functions. Its importance is underscored in
many applications, notably including hyperparameter tuning, but its efficacy
depends on efficiently balancing exploration and exploitation. While there has
been substantial progress in BO methods, striking this balance still remains a
delicate process. In this light, we present \texttt{LLAMBO}, a novel approach
that integrates the capabilities of large language models (LLM) within BO. At a
high level, we frame the BO problem in natural language terms, enabling LLMs to
iteratively propose promising solutions conditioned on historical evaluations.
More specifically, we explore how combining contextual understanding, few-shot
learning proficiency, and domain knowledge of LLMs can enhance various
components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is
effective at zero-shot warmstarting, and improves surrogate modeling and
candidate sampling, especially in the early stages of search when observations
are sparse. Our approach is performed in context and does not require LLM
finetuning. Additionally, it is modular by design, allowing individual
components to be integrated into existing BO frameworks, or function cohesively
as an end-to-end method. We empirically validate \texttt{LLAMBO}'s efficacy on
the problem of hyperparameter tuning, highlighting strong empirical performance
across a range of diverse benchmarks, proprietary, and synthetic tasks.
\\ ( https://arxiv.org/abs/2402.03921 ,  9596kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03923
Date: Tue, 6 Feb 2024 11:46:47 GMT   (212kb,D)

Title: Return-Aligned Decision Transformer
Authors: Tsunehiko Tanaka, Kenshi Abe, Kaito Ariu, Tetsuro Morimura, Edgar
  Simo-Serra
Categories: cs.LG
\\
  Traditional approaches in offline reinforcement learning aim to learn the
optimal policy that maximizes the cumulative reward, also known as return.
However, as applications broaden, it becomes increasingly crucial to train
agents that not only maximize the returns, but align the actual return with a
specified target return, giving control over the agent's performance. Decision
Transformer (DT) optimizes a policy that generates actions conditioned on the
target return through supervised learning and is equipped with a mechanism to
control the agent using the target return. Despite being designed to align the
actual return with the target return, we have empirically identified a
discrepancy between the actual return and the target return in DT. In this
paper, we propose Return-Aligned Decision Transformer (RADT), designed to
effectively align the actual return with the target return. Our model decouples
returns from the conventional input sequence, which typically consists of
returns, states, and actions, to enhance the relationships between returns and
states, as well as returns and actions. Extensive experiments show that RADT
reduces the discrepancies between the actual return and the target return of
DT-based methods.
\\ ( https://arxiv.org/abs/2402.03923 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03941
Date: Tue, 6 Feb 2024 12:18:54 GMT   (4077kb,D)

Title: Discovery of the Hidden World with Large Language Models
Authors: Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng,
  Bo Han, Kun Zhang
Categories: cs.LG cs.AI stat.ME
Comments: Preliminary version of an ongoing project; Chenxi and Yongqiang
  contributed equally; 26 pages, 41 figures; Project page:
  https://causalcoat.github.io/
\\
  Science originates with discovering new causal knowledge from a combination
of known facts and observations. Traditional causal discovery approaches mainly
rely on high-quality measured variables, usually given by human experts, to
find causal relations. However, the causal variables are usually unavailable in
a wide range of real-world applications. The rise of large language models
(LLMs) that are trained to learn rich knowledge from the massive observations
of the world, provides a new opportunity to assist with discovering high-level
hidden variables from the raw observational data. Therefore, we introduce COAT:
Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer
that extracts the potential causal factors from unstructured data. Moreover,
LLMs can also be instructed to provide additional information used to collect
data values (e.g., annotation criteria) and to further parse the raw
unstructured data into structured data. The annotated data will be fed to a
causal learning module (e.g., the FCI algorithm) that provides both rigorous
explanations of the data, as well as useful feedback to further improve the
extraction of causal factors by LLMs. We verify the effectiveness of COAT in
uncovering the underlying causal system with two case studies of review rating
analysis and neuropathic diagnosis.
\\ ( https://arxiv.org/abs/2402.03941 ,  4077kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03966
Date: Tue, 6 Feb 2024 12:56:55 GMT   (2698kb,D)

Title: On dimensionality of feature vectors in MPNNs
Authors: C\'esar Bravo, Alexander Kozachinskiy, Crist\'obal Rojas
Categories: cs.LG
Comments: 15 pages, 2 figures
\\
  We revisit the classical result of Morris et al.~(AAAI'19) that
message-passing graphs neural networks (MPNNs) are equal in their
distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.
  Morris et al.~show their simulation result with ReLU activation function and
$O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the
graph. Recently, by introducing randomness into the architecture, Aamand et
al.~(NeurIPS'22) were able to improve this bound to $O(\log n)$-dimensional
feature vectors, although at the expense of guaranteeing perfect simulation
only with high probability.
  In all these constructions, to guarantee equivalence to the WL test, the
dimension of feature vectors in the MPNN has to increase with the size of the
graphs. However, architectures used in practice have feature vectors of
constant dimension. Thus, there is a gap between the guarantees provided by
these results and the actual characteristics of architectures used in practice.
In this paper we close this gap by showing that, for \emph{any} non-polynomial
analytic (like the sigmoid) activation function, to guarantee that MPNNs are
equivalent to the WL test, feature vectors of dimension $d=1$ is all we need,
independently of the size of the graphs.
  Our main technical insight is that for simulating multi-sets in the WL-test,
it is enough to use linear independence of feature vectors over rationals
instead of reals. Countability of the set of rationals together with nice
properties of analytic functions allow us to carry out the simulation invariant
over the iterations of the WL test without increasing the dimension of the
feature vectors.
\\ ( https://arxiv.org/abs/2402.03966 ,  2698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03969
Date: Tue, 6 Feb 2024 12:58:38 GMT   (936kb,D)

Title: In-context learning agents are asymmetric belief updaters
Authors: Johannes A. Schubert, Akshay K. Jagadish, Marcel Binz, Eric Schulz
Categories: cs.LG
\\
  We study the in-context learning dynamics of large language models (LLMs)
using three instrumental learning tasks adapted from cognitive psychology. We
find that LLMs update their beliefs in an asymmetric manner and learn more from
better-than-expected outcomes than from worse-than-expected ones. Furthermore,
we show that this effect reverses when learning about counterfactual feedback
and disappears when no agency is implied. We corroborate these findings by
investigating idealized in-context learning agents derived through
meta-reinforcement learning, where we observe similar patterns. Taken together,
our results contribute to our understanding of how in-context learning works by
highlighting that the framing of a problem significantly influences how
learning occurs, a phenomenon also observed in human cognition.
\\ ( https://arxiv.org/abs/2402.03969 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03970
Date: Tue, 6 Feb 2024 12:59:02 GMT   (185kb,D)

Title: Tabular Data: Is Attention All You Need?
Authors: Guri Zab\"ergja, Arlind Kadra, Josif Grabocka
Categories: cs.LG cs.AI
\\
  Deep Learning has revolutionized the field of AI and led to remarkable
achievements in applications involving image and text data. Unfortunately,
there is inconclusive evidence on the merits of neural networks for structured
tabular data. In this paper, we introduce a large-scale empirical study
comparing neural networks against gradient-boosted decision trees on tabular
data, but also transformer-based architectures against traditional multi-layer
perceptrons (MLP) with residual connections. In contrast to prior work, our
empirical findings indicate that neural networks are competitive against
decision trees. Furthermore, we assess that transformer-based architectures do
not outperform simpler variants of traditional MLP architectures on tabular
datasets. As a result, this paper helps the research and practitioner
communities make informed choices on deploying neural networks on future
tabular data applications.
\\ ( https://arxiv.org/abs/2402.03970 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03979
Date: Tue, 6 Feb 2024 13:16:50 GMT   (419kb,D)

Title: Cross Entropy versus Label Smoothing: A Neural Collapse Perspective
Authors: Li Guo, Keith Ross, Zifan Zhao, Andriopoulos George, Shuyang Ling,
  Yufeng Xu, Zixuan Dong
Categories: cs.LG
\\
  Label smoothing loss is a widely adopted technique to mitigate overfitting in
deep neural networks. This paper studies label smoothing from the perspective
of Neural Collapse (NC), a powerful empirical and theoretical framework which
characterizes model behavior during the terminal phase of training. We first
show empirically that models trained with label smoothing converge faster to
neural collapse solutions and attain a stronger level of neural collapse.
Additionally, we show that at the same level of NC1, models under label
smoothing loss exhibit intensified NC2. These findings provide valuable
insights into the performance benefits and enhanced model calibration under
label smoothing loss. We then leverage the unconstrained feature model to
derive closed-form solutions for the global minimizers for both loss functions
and further demonstrate that models under label smoothing have a lower
conditioning number and, therefore, theoretically converge faster. Our study,
combining empirical evidence and theoretical results, not only provides nuanced
insights into the differences between label smoothing and cross-entropy losses,
but also serves as an example of how the powerful neural collapse framework can
be used to improve our understanding of DNNs.
\\ ( https://arxiv.org/abs/2402.03979 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03985
Date: Tue, 6 Feb 2024 13:20:46 GMT   (799kb,D)

Title: A Bias-Variance Decomposition for Ensembles over Multiple Synthetic
  Datasets
Authors: Ossi R\"ais\"a, Antti Honkela
Categories: cs.LG stat.ML
\\
  Recent studies have highlighted the benefits of generating multiple synthetic
datasets for supervised learning, from increased accuracy to more effective
model selection and uncertainty estimation. These benefits have clear empirical
support, but the theoretical understanding of them is currently very light. We
seek to increase the theoretical understanding by deriving bias-variance
decompositions for several settings of using multiple synthetic datasets. Our
theory predicts multiple synthetic datasets to be especially beneficial for
high-variance downstream predictors, and yields a simple rule of thumb to
select the appropriate number of synthetic datasets in the case of mean-squared
error and Brier score. We investigate how our theory works in practice by
evaluating the performance of an ensemble over many synthetic datasets for
several real datasets and downstream predictors. The results follow our theory,
showing that our insights are also practically relevant.
\\ ( https://arxiv.org/abs/2402.03985 ,  799kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03991
Date: Tue, 6 Feb 2024 13:44:39 GMT   (170kb,D)

Title: Neural Rank Collapse: Weight Decay and Small Within-Class Variability
  Yield Low-Rank Bias
Authors: Emanuele Zangrando, Piero Deidda, Simone Brugiapaglia, Nicola
  Guglielmi, Francesco Tudisco
Categories: cs.LG cs.NA math.NA stat.ML
\\
  Recent work in deep learning has shown strong empirical and theoretical
evidence of an implicit low-rank bias: weight matrices in deep networks tend to
be approximately low-rank and removing relatively small singular values during
training or from available trained models may significantly reduce model size
while maintaining or even improving model performance. However, the majority of
the theoretical investigations around low-rank bias in neural networks deal
with oversimplified deep linear networks. In this work, we consider general
networks with nonlinear activations and the weight decay parameter, and we show
the presence of an intriguing neural rank collapse phenomenon, connecting the
low-rank bias of trained networks with networks' neural collapse properties: as
the weight decay parameter grows, the rank of each layer in the network
decreases proportionally to the within-class variability of the hidden-space
embeddings of the previous layers. Our theoretical findings are supported by a
range of experimental evaluations illustrating the phenomenon.
\\ ( https://arxiv.org/abs/2402.03991 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03992
Date: Tue, 6 Feb 2024 13:45:01 GMT   (1189kb,D)

Title: Space Group Constrained Crystal Generation
Authors: Rui Jiao, Wenbing Huang, Yu Liu, Deli Zhao, Yang Liu
Categories: cs.LG cond-mat.mtrl-sci
Comments: ICLR 2024 poster
\\
  Crystals are the foundation of numerous scientific and industrial
applications. While various learning-based approaches have been proposed for
crystal generation, existing methods seldom consider the space group constraint
which is crucial in describing the geometry of crystals and closely relevant to
many desirable properties. However, considering space group constraint is
challenging owing to its diverse and nontrivial forms. In this paper, we reduce
the space group constraint into an equivalent formulation that is more
tractable to be handcrafted into the generation process. In particular, we
translate the space group constraint into two parts: the basis constraint of
the invariant logarithmic space of the lattice matrix and the Wyckoff position
constraint of the fractional coordinates. Upon the derived constraints, we then
propose DiffCSP++, a novel diffusion model that has enhanced a previous work
DiffCSP by further taking space group constraint into account. Experiments on
several popular datasets verify the benefit of the involvement of the space
group constraint, and show that our DiffCSP++ achieves promising performance on
crystal structure prediction, ab initio crystal generation and controllable
generation with customized space groups.
\\ ( https://arxiv.org/abs/2402.03992 ,  1189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03994
Date: Tue, 6 Feb 2024 13:47:12 GMT   (354kb,D)

Title: Gradient Sketches for Training Data Attribution and Studying the Loss
  Landscape
Authors: Andrea Schioppa
Categories: cs.LG stat.ML
\\
  Random projections or sketches of gradients and Hessian vector products play
an essential role in applications where one needs to store many such vectors
while retaining accurate information about their relative geometry. Two
important scenarios are training data attribution (tracing a model's behavior
to the training data), where one needs to store a gradient for each training
example, and the study of the spectrum of the Hessian (to analyze the training
dynamics), where one needs to store multiple Hessian vector products. While
sketches that use dense matrices are easy to implement, they are memory bound
and cannot be scaled to modern neural networks. Motivated by work on the
intrinsic dimension of neural networks, we propose and study a design space for
scalable sketching algorithms. We demonstrate the efficacy of our approach in
three applications: training data attribution, the analysis of the Hessian
spectrum and the computation of the intrinsic dimension when fine-tuning
pre-trained language models.
\\ ( https://arxiv.org/abs/2402.03994 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04004
Date: Tue, 6 Feb 2024 13:59:56 GMT   (1430kb,D)

Title: Understanding the Effect of Noise in LLM Training Data with Algorithmic
  Chains of Thought
Authors: Alex Havrilla, Maia Iyer
Categories: cs.LG
\\
  During both pretraining and fine-tuning, Large Language Models
(\textbf{LLMs}) are trained on trillions of tokens of text of widely varying
quality. Both phases of training typically involve heuristically filtering out
``low-quality'' or \textit{noisy} training samples, yet little is known
quantitatively about how the type or intensity of noise affects downstream
performance. In this work, we study how noise in chain of thought
(\textbf{CoT}) impacts task performance in the highly-controlled setting of
algorithmically solvable tasks. First, we develop the Traced Integer
(\textbf{TInt}) framework to generate highly customizable noised execution
traces for any arithmetic function on lists of integers. We then define two
types of noise: \textit{static} noise, a local form of noise which is applied
after the CoT trace is computed, and \textit{dynamic} noise, a global form of
noise which propagates errors in the trace as it is computed. We then evaluate
the test performance of pretrained models both prompted and fine-tuned on
noised datasets with varying levels of dataset contamination and intensity. We
find fine-tuned models are extremely robust to high levels of static noise but
struggle significantly more with lower levels of dynamic noise. In contrast,
few-shot prompted models appear more sensitive to even static noise. We
conclude with a discussion of how our findings impact noise filtering
best-practices, in particular emphasizing the importance of removing samples
containing destructive dynamic noise with global errors.
\\ ( https://arxiv.org/abs/2402.04004 ,  1430kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04005
Date: Tue, 6 Feb 2024 14:00:43 GMT   (648kb,D)

Title: Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning
Authors: Idan Achituve, Idit Diamant, Arnon Netzer, Gal Chechik, Ethan Fetaya
Categories: cs.LG
\\
  As machine learning becomes more prominent there is a growing demand to
perform several inference tasks in parallel. Running a dedicated model for each
task is computationally expensive and therefore there is a great interest in
multi-task learning (MTL). MTL aims at learning a single model that solves
several tasks efficiently. Optimizing MTL models is often achieved by computing
a single gradient per task and aggregating them for obtaining a combined update
direction. However, these approaches do not consider an important aspect, the
sensitivity in the gradient dimensions. Here, we introduce a novel gradient
aggregation approach using Bayesian inference. We place a probability
distribution over the task-specific parameters, which in turn induce a
distribution over the gradients of the tasks. This additional valuable
information allows us to quantify the uncertainty in each of the gradients
dimensions, which can then be factored in when aggregating them. We empirically
demonstrate the benefits of our approach in a variety of datasets, achieving
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2402.04005 ,  648kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04010
Date: Tue, 6 Feb 2024 14:05:05 GMT   (12674kb,D)

Title: Efficient Availability Attacks against Supervised and Contrastive
  Learning Simultaneously
Authors: Yihan Wang and Yifan Zhu and Xiao-Shan Gao
Categories: cs.LG stat.ML
\\
  Availability attacks can prevent the unauthorized use of private data and
commercial datasets by generating imperceptible noise and making unlearnable
examples before release. Ideally, the obtained unlearnability prevents
algorithms from training usable models. When supervised learning (SL)
algorithms have failed, a malicious data collector possibly resorts to
contrastive learning (CL) algorithms to bypass the protection. Through
evaluation, we have found that most of the existing methods are unable to
achieve both supervised and contrastive unlearnability, which poses risks to
data protection. Different from recent methods based on contrastive error
minimization, we employ contrastive-like data augmentations in supervised error
minimization or maximization frameworks to obtain attacks effective for both SL
and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case
unlearnability across SL and CL algorithms with less computation consumption,
showcasing prospects in real-world applications.
\\ ( https://arxiv.org/abs/2402.04010 ,  12674kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04019
Date: Fri, 2 Feb 2024 15:47:01 GMT   (922kb)

Title: Exploring the Effects of Population and Employment Characteristics on
  Truck Flows: An Analysis of NextGen NHTS Origin-Destination Data
Authors: Majbah Uddin, Yuandong Liu, and Hyeonsup Lim
Categories: cs.LG
Journal-ref: In International Conference on Transportation and Development 2023
  (pp. 503-513)
DOI: 10.1061/9780784484883.044
\\
  Truck transportation remains the dominant mode of US freight transportation
because of its advantages, such as the flexibility of accessing pickup and
drop-off points and faster delivery. Because of the massive freight volume
transported by trucks, understanding the effects of population and employment
characteristics on truck flows is critical for better transportation planning
and investment decisions. The US Federal Highway Administration published a
truck travel origin-destination data set as part of the Next Generation
National Household Travel Survey program. This data set contains the total
number of truck trips in 2020 within and between 583 predefined zones
encompassing metropolitan and nonmetropolitan statistical areas within each
state and Washington, DC. In this study, origin-destination-level truck trip
flow data was augmented to include zone-level population and employment
characteristics from the US Census Bureau. Census population and County
Business Patterns data were included. The final data set was used to train a
machine learning algorithm-based model, Extreme Gradient Boosting (XGBoost),
where the target variable is the number of total truck trips. Shapley Additive
ExPlanation (SHAP) was adopted to explain the model results. Results showed
that the distance between the zones was the most important variable and had a
nonlinear relationship with truck flows.
\\ ( https://arxiv.org/abs/2402.04019 ,  922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04029
Date: Tue, 6 Feb 2024 14:24:29 GMT   (11314kb,D)

Title: Positive concave deep equilibrium models
Authors: Mateusz Gabor, Tomasz Piotrowski, Renato L. G. Cavalcante
Categories: cs.LG
\\
  Deep equilibrium (DEQ) models are widely recognized as a memory efficient
alternative to standard neural networks, achieving state-of-the-art performance
in language modeling and computer vision tasks. These models solve a fixed
point equation instead of explicitly computing the output, which sets them
apart from standard neural networks. However, existing DEQ models often lack
formal guarantees of the existence and uniqueness of the fixed point, and the
convergence of the numerical scheme used for computing the fixed point is not
formally established. As a result, DEQ models are potentially unstable in
practice. To address these drawbacks, we introduce a novel class of DEQ models
called positive concave deep equilibrium (pcDEQ) models. Our approach, which is
based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and
activation functions that are concave on the positive orthant. By imposing
these constraints, we can easily ensure the existence and uniqueness of the
fixed point without relying on additional complex assumptions commonly found in
the DEQ literature, such as those based on monotone operator theory in convex
analysis. Furthermore, the fixed point can be computed with the standard fixed
point algorithm, and we provide theoretical guarantees of geometric
convergence, which, in particular, simplifies the training process. Experiments
demonstrate the competitiveness of our pcDEQ models against other implicit
models.
\\ ( https://arxiv.org/abs/2402.04029 ,  11314kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04030
Date: Tue, 6 Feb 2024 14:25:09 GMT   (417kb,D)

Title: Reducing the Cost of Quantum Chemical Data By Backpropagating Through
  Density Functional Theory
Authors: Alexander Mathiasen, Hatem Helal, Paul Balanca, Adam Krzywaniak, Ali
  Parviz, Frederik Hvilsh{\o}j, Blazej Banaszewski, Carlo Luschi, Andrew
  William Fitzgibbon
Categories: cs.LG
\\
  Density Functional Theory (DFT) accurately predicts the quantum chemical
properties of molecules, but scales as $O(N_{\text{electrons}}^3)$. Sch\"utt et
al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN).
Arguably, the biggest problem one faces when scaling to larger molecules is the
cost of DFT labels. For example, it took years to create the PCQ dataset
(Nakata & Shimazaki, 2017) on which subsequent NNs are trained within a week.
DFT labels molecules by minimizing energy $E(\cdot )$ as a "loss function." We
bypass dataset creation by directly training NNs with $E(\cdot )$ as a loss
function. For comparison, Sch\"utt et al. (2019) spent 626 hours creating a
dataset on which they trained their NN for 160h, for a total of 786h; our
method achieves comparable performance within 31h.
\\ ( https://arxiv.org/abs/2402.04030 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04033
Date: Tue, 6 Feb 2024 14:26:22 GMT   (17745kb,D)

Title: On provable privacy vulnerabilities of graph representations
Authors: Ruofan Wu, Guanhua Fang, Qiying Pan, Mingyang Zhang, Tengfei Liu,
  Weiqiang Wang, Wenbiao Zhao
Categories: cs.LG
\\
  Graph representation learning (GRL) is critical for extracting insights from
complex network structures, but it also raises security concerns due to
potential privacy vulnerabilities in these representations. This paper
investigates the structural vulnerabilities in graph neural models where
sensitive topological information can be inferred through edge reconstruction
attacks. Our research primarily addresses the theoretical underpinnings of
cosine-similarity-based edge reconstruction attacks (COSERA), providing
theoretical and empirical evidence that such attacks can perfectly reconstruct
sparse Erdos Renyi graphs with independent random features as graph size
increases. Conversely, we establish that sparsity is a critical factor for
COSERA's effectiveness, as demonstrated through analysis and experiments on
stochastic block models. Finally, we explore the resilience of (provably)
private graph representations produced via noisy aggregation (NAG) mechanism
against COSERA. We empirically delineate instances wherein COSERA demonstrates
both efficacy and deficiency in its capacity to function as an instrument for
elucidating the trade-off between privacy and utility.
\\ ( https://arxiv.org/abs/2402.04033 ,  17745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04050
Date: Tue, 6 Feb 2024 14:53:19 GMT   (223kb,D)

Title: Connecting the Dots: Collaborative Fine-tuning for Black-Box
  Vision-Language Models
Authors: Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, Tieniu Tan
Categories: cs.LG cs.AI cs.CV
\\
  With the emergence of pretrained vision-language models (VLMs), considerable
efforts have been devoted to fine-tuning them for downstream tasks. Despite the
progress made in designing efficient fine-tuning methods, such methods require
access to the model's parameters, which can be challenging as model owners
often opt to provide their models as a black box to safeguard model ownership.
This paper proposes a \textbf{C}ollabo\textbf{ra}tive
\textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning
black-box VLMs to downstream tasks, where one only has access to the input
prompts and the output predictions of the model. CraFT comprises two modules, a
prompt generation module for learning text prompts and a prediction refinement
module for enhancing output predictions in residual style. Additionally, we
introduce an auxiliary prediction-consistent loss to promote consistent
optimization across these modules. These modules are optimized by a novel
collaborative training algorithm. Extensive experiments on few-shot
classification over 15 datasets demonstrate the superiority of CraFT. The
results show that CraFT achieves a decent gain of about 12\% with 16-shot
datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only
about 1/80 of the memory footprint for deployment, while sacrificing only
1.62\% compared to the white-box method.
\\ ( https://arxiv.org/abs/2402.04050 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04051
Date: Tue, 6 Feb 2024 14:53:28 GMT   (1081kb,D)

Title: Analysis of Linear Mode Connectivity via Permutation-Based Weight
  Matching
Authors: Akira Ito, Masanori Yamada, Atsutoshi Kumagai
Categories: cs.LG
Comments: 20 pages
\\
  Recently, Ainsworth et al. showed that using weight matching (WM) to minimize
the $L_2$ distance in a permutation search of model parameters effectively
identifies permutations that satisfy linear mode connectivity (LMC), in which
the loss along a linear path between two independently trained models with
different seeds remains nearly constant. This paper provides a theoretical
analysis of LMC using WM, which is crucial for understanding stochastic
gradient descent's effectiveness and its application in areas like model
merging. We first experimentally and theoretically show that permutations found
by WM do not significantly reduce the $L_2$ distance between two models and the
occurrence of LMC is not merely due to distance reduction by WM in itself. We
then provide theoretical insights showing that permutations can change the
directions of the singular vectors, but not the singular values, of the weight
matrices in each layer. This finding shows that permutations found by WM mainly
align the directions of singular vectors associated with large singular values
across models. This alignment brings the singular vectors with large singular
values, which determine the model functionality, closer between pre-merged and
post-merged models, so that the post-merged model retains functionality similar
to the pre-merged models, making it easy to satisfy LMC. Finally, we analyze
the difference between WM and straight-through estimator (STE), a
dataset-dependent permutation search method, and show that WM outperforms STE,
especially when merging three or more models.
\\ ( https://arxiv.org/abs/2402.04051 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04054
Date: Tue, 6 Feb 2024 15:00:08 GMT   (64kb,D)

Title: More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms
Authors: Hossein Zakerinia, Amin Behjati, Christoph H. Lampert
Categories: cs.LG stat.ML
\\
  We introduce a new framework for studying meta-learning methods using
PAC-Bayesian theory. Its main advantage over previous work is that it allows
for more flexibility in how the transfer of knowledge between tasks is
realized. For previous approaches, this could only happen indirectly, by means
of learning prior distributions over models. In contrast, the new
generalization bounds that we prove express the process of meta-learning much
more directly as learning the learning algorithm that should be used for future
tasks. The flexibility of our framework makes it suitable to analyze a wide
range of meta-learning mechanisms and even design new mechanisms. Other than
our theoretical contributions we also show empirically that our framework
improves the prediction quality in practical meta-learning mechanisms.
\\ ( https://arxiv.org/abs/2402.04054 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04059
Date: Tue, 6 Feb 2024 15:03:53 GMT   (62kb,D)

Title: Deep Learning for Multivariate Time Series Imputation: A Survey
Authors: Jun Wang, Wenjie Du, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang,
  Qingsong Wen
Categories: cs.LG cs.AI
Comments: 9 pages, 1 figure, 5 tables, 58 referred papers
\\
  The ubiquitous missing values cause the multivariate time series data to be
partially observed, destroying the integrity of time series and hindering the
effective time series data analysis. Recently deep learning imputation methods
have demonstrated remarkable success in elevating the quality of corrupted time
series data, subsequently enhancing performance in downstream tasks. In this
paper, we conduct a comprehensive survey on the recently proposed deep learning
imputation methods. First, we propose a taxonomy for the reviewed methods, and
then provide a structured review of these methods by highlighting their
strengths and limitations. We also conduct empirical experiments to study
different methods and compare their enhancement for downstream tasks. Finally,
the open issues for future research on multivariate time series imputation are
pointed out. All code and configurations of this work, including a regularly
maintained multivariate time series imputation paper list, can be found in the
GitHub repository~\url{https://github.com/WenjieDu/Awesome\_Imputation}.
\\ ( https://arxiv.org/abs/2402.04059 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04062
Date: Tue, 6 Feb 2024 15:05:40 GMT   (579kb,D)

Title: Link Prediction with Relational Hypergraphs
Authors: Xingyue Huang, Miguel Romero Orth, Pablo Barcel\'o, Michael M.
  Bronstein, \.Ismail \.Ilkan Ceylan
Categories: cs.LG cs.AI
\\
  Link prediction with knowledge graphs has been thoroughly studied in graph
machine learning, leading to a rich landscape of graph neural network
architectures with successful applications. Nonetheless, it remains challenging
to transfer the success of these architectures to link prediction with
relational hypergraphs. The presence of relational hyperedges makes link
prediction a task between $k$ nodes for varying choices of $k$, which is
substantially harder than link prediction with knowledge graphs, where every
relation is binary ($k=2$). In this paper, we propose two frameworks for link
prediction with relational hypergraphs and conduct a thorough analysis of the
expressive power of the resulting model architectures via corresponding
relational Weisfeiler-Leman algorithms, and also via some natural logical
formalisms. Through extensive empirical analysis, we validate the power of the
proposed model architectures on various relational hypergraph benchmarks. The
resulting model architectures substantially outperform every baseline for
inductive link prediction, and lead to state-of-the-art results for
transductive link prediction. Our study therefore unlocks applications of graph
neural networks to fully relational structures.
\\ ( https://arxiv.org/abs/2402.04062 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04068
Date: Tue, 6 Feb 2024 15:13:17 GMT   (905kb,D)

Title: Retrieve to Explain: Evidence-driven Predictions with Language Models
Authors: Ravi Patel (1), Angus Brayne (1), Rogier Hintzen (1), Daniel
  Jaroslawicz (1), Georgiana Neculae (1), Dane Corneil (1) ((1) BenevolentAI)
Categories: cs.LG cs.CL
\\
  Machine learning models, particularly language models, are notoriously
difficult to introspect. Black-box models can mask both issues in model
training and harmful biases. For human-in-the-loop processes, opaque
predictions can drive lack of trust, limiting a model's impact even when it
performs effectively. To address these issues, we introduce Retrieve to Explain
(R2E). R2E is a retrieval-based language model that prioritizes amongst a
pre-defined set of possible answers to a research question based on the
evidence in a document corpus, using Shapley values to identify the relative
importance of pieces of evidence to the final prediction. R2E can adapt to new
evidence without retraining, and incorporate structured data through templating
into natural language. We assess on the use case of drug target identification
from published scientific literature, where we show that the model outperforms
an industry-standard genetics-based approach on predicting clinical trial
outcomes.
\\ ( https://arxiv.org/abs/2402.04068 ,  905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04080
Date: Tue, 6 Feb 2024 15:34:30 GMT   (711kb,D)

Title: Entropy-regularized Diffusion Policy with Q-Ensembles for Offline
  Reinforcement Learning
Authors: Ruoqi Zhang, Ziwei Luo, Jens Sj\"olund, Thomas B. Sch\"on, Per
  Mattsson
Categories: cs.LG cs.SY eess.SY
\\
  This paper presents advanced techniques of training diffusion policies for
offline reinforcement learning (RL). At the core is a mean-reverting stochastic
differential equation (SDE) that transfers a complex action distribution into a
standard Gaussian and then samples actions conditioned on the environment state
with a corresponding reverse-time SDE, like a typical diffusion policy. We show
that such an SDE has a solution that we can use to calculate the log
probability of the policy, yielding an entropy regularizer that improves the
exploration of offline datasets. To mitigate the impact of inaccurate value
functions from out-of-distribution data points, we further propose to learn the
lower confidence bound of Q-ensembles for more robust policy improvement. By
combining the entropy-regularized diffusion policy with Q-ensembles in offline
RL, our method achieves state-of-the-art performance on most tasks in D4RL
benchmarks. Code is available at
\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.
\\ ( https://arxiv.org/abs/2402.04080 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04081
Date: Tue, 6 Feb 2024 15:34:44 GMT   (6141kb,D)

Title: Improved Generalization of Weight Space Networks via Augmentations
Authors: Aviv Shamsian, Aviv Navon, David W. Zhang, Yan Zhang, Ethan Fetaya,
  Gal Chechik, Haggai Maron
Categories: cs.LG cs.AI
Comments: Under Review
\\
  Learning in deep weight spaces (DWS), where neural networks process the
weights of other neural networks, is an emerging research direction, with
applications to 2D and 3D neural fields (INRs, NeRFs), as well as making
inferences about other types of neural networks. Unfortunately, weight space
models tend to suffer from substantial overfitting. We empirically analyze the
reasons for this overfitting and find that a key reason is the lack of
diversity in DWS datasets. While a given object can be represented by many
different weight configurations, typical INR training sets fail to capture
variability across INRs that represent the same object. To address this, we
explore strategies for data augmentation in weight spaces and propose a MixUp
method adapted for weight spaces. We demonstrate the effectiveness of these
methods in two setups. In classification, they improve performance similarly to
having up to 10 times more data. In self-supervised contrastive learning, they
yield substantial 5-10% gains in downstream classification.
\\ ( https://arxiv.org/abs/2402.04081 ,  6141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04082
Date: Tue, 6 Feb 2024 15:36:06 GMT   (749kb)

Title: An Optimal House Price Prediction Algorithm: XGBoost
Authors: Hemlata Sharma, Hitesh Harsora, Bayode Ogunleye
Categories: cs.LG cs.AI stat.AP stat.ME
Comments: 16 pages, Journal of Analytics
ACM-class: H.3.3
Journal-ref: Analytics, 3(1), 30-45 (2024)
DOI: 10.3390/analytics3010003
\\
  An accurate prediction of house prices is a fundamental requirement for
various sectors including real estate and mortgage lending. It is widely
recognized that a property value is not solely determined by its physical
attributes but is significantly influenced by its surrounding neighbourhood.
Meeting the diverse housing needs of individuals while balancing budget
constraints is a primary concern for real estate developers. To this end, we
addressed the house price prediction problem as a regression task and thus
employed various machine learning techniques capable of expressing the
significance of independent variables. We made use of the housing dataset of
Ames City in Iowa, USA to compare support vector regressor, random forest
regressor, XGBoost, multilayer perceptron and multiple linear regression
algorithms for house price prediction. Afterwards, we identified the key
factors that influence housing costs. Our results show that XGBoost is the best
performing model for house price prediction.
\\ ( https://arxiv.org/abs/2402.04082 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04084
Date: Tue, 6 Feb 2024 15:39:09 GMT   (421kb)

Title: Provably learning a multi-head attention layer
Authors: Sitan Chen, Yuanzhi Li
Categories: cs.LG cs.DS stat.ML
Comments: 105 pages, comments welcome
\\
  The multi-head attention layer is one of the key components of the
transformer architecture that sets it apart from traditional feed-forward
models. Given a sequence length $k$, attention matrices
$\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$, and
projection matrices $\mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times
d}$, the corresponding multi-head attention layer $F: \mathbb{R}^{k\times d}\to
\mathbb{R}^{k\times d}$ transforms length-$k$ sequences of $d$-dimensional
tokens $\mathbf{X}\in\mathbb{R}^{k\times d}$ via $F(\mathbf{X}) \triangleq
\sum^m_{i=1}
\mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$.
In this work, we initiate the study of provably learning a multi-head attention
layer from random examples and give the first nontrivial upper and lower bounds
for this problem:
  - Provided $\{\mathbf{W}_i, \mathbf{\Theta}_i\}$ satisfy certain
non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns
$F$ to small error given random labeled examples drawn uniformly from $\{\pm
1\}^{k\times d}$.
  - We prove computational lower bounds showing that in the worst case,
exponential dependence on $m$ is unavoidable.
  We focus on Boolean $\mathbf{X}$ to mimic the discrete nature of tokens in
large language models, though our techniques naturally extend to standard
continuous settings, e.g. Gaussian. Our algorithm, which is centered around
using examples to sculpt a convex body containing the unknown parameters, is a
significant departure from existing provable algorithms for learning
feedforward networks, which predominantly exploit algebraic and rotation
invariance properties of the Gaussian distribution. In contrast, our analysis
is more flexible as it primarily relies on various upper and lower tail bounds
for the input distribution and "slices" thereof.
\\ ( https://arxiv.org/abs/2402.04084 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04103
Date: Tue, 6 Feb 2024 15:58:14 GMT   (759kb)

Title: An Exploration of Clustering Algorithms for Customer Segmentation in the
  UK Retail Market
Authors: Jeen Mary John, Olamilekan Shobayo, Bayode Ogunleye
Categories: cs.LG cs.AI stat.AP stat.CO
Comments: 15 pages, Journal of Analytics
ACM-class: H.3.3
Journal-ref: Analytics, 2(4), 809-823 (2023)
DOI: 10.3390/analytics2040042
\\
  Recently, peoples awareness of online purchases has significantly risen. This
has given rise to online retail platforms and the need for a better
understanding of customer purchasing behaviour. Retail companies are pressed
with the need to deal with a high volume of customer purchases, which requires
sophisticated approaches to perform more accurate and efficient customer
segmentation. Customer segmentation is a marketing analytical tool that aids
customer-centric service and thus enhances profitability. In this paper, we aim
to develop a customer segmentation model to improve decision-making processes
in the retail market industry. To achieve this, we employed a UK-based online
retail dataset obtained from the UCI machine learning repository. The retail
dataset consists of 541,909 customer records and eight features. Our study
adopted the RFM (recency, frequency, and monetary) framework to quantify
customer values. Thereafter, we compared several state-of-the-art (SOTA)
clustering algorithms, namely, K-means clustering, the Gaussian mixture model
(GMM), density-based spatial clustering of applications with noise (DBSCAN),
agglomerative clustering, and balanced iterative reducing and clustering using
hierarchies (BIRCH). The results showed the GMM outperformed other approaches,
with a Silhouette Score of 0.80.
\\ ( https://arxiv.org/abs/2402.04103 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04108
Date: Tue, 6 Feb 2024 16:02:17 GMT   (744kb,D)

Title: Hierarchical Delay Attribution Classification using Unstructured Text in
  Train Management Systems
Authors: Anton Borg, Per Lingvall, Martin Svensson
Categories: cs.LG cs.AI
Comments: 22 pages, 7 figures
\\
  EU directives stipulate a systematic follow-up of train delays. In Sweden,
the Swedish Transport Administration registers and assigns an appropriate delay
attribution code. However, this delay attribution code is assigned manually,
which is a complex task. In this paper, a machine learning-based decision
support for assigning delay attribution codes based on event descriptions is
investigated. The text is transformed using TF-IDF, and two models, Random
Forest and Support Vector Machine, are evaluated against a random uniform
classifier and the classification performance of the Swedish Transport
Administration. Further, the problem is modeled as both a hierarchical and flat
approach. The results indicate that a hierarchical approach performs better
than a flat approach. Both approaches perform better than the random uniform
classifier but perform worse than the manual classification.
\\ ( https://arxiv.org/abs/2402.04108 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04119
Date: Tue, 6 Feb 2024 16:12:36 GMT   (5717kb,D)

Title: Scientific Language Modeling: A Quantitative Review of Large Language
  Models in Molecular Science
Authors: Pengfei Liu, Jun Tao, Zhixiang Ren
Categories: cs.LG cs.CE
\\
  Efficient molecular modeling and design are crucial for the discovery and
exploration of novel molecules, and the incorporation of deep learning methods
has revolutionized this field. In particular, large language models (LLMs)
offer a fresh approach to tackle scientific problems from a natural language
processing (NLP) perspective, introducing a research paradigm called scientific
language modeling (SLM). However, two key issues remain: how to quantify the
match between model and data modalities and how to identify the
knowledge-learning preferences of models. To address these challenges, we
propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263
experiments to assess the model's compatibility with data modalities and
knowledge acquisition. Through the modal transition probability matrix, we
provide insights into the most suitable modalities for tasks. Furthermore, we
introduce a statistically interpretable approach to discover context-specific
knowledge mapping by localized feature filtering. Our pioneering analysis
offers an exploration of the learning mechanism and paves the way for advancing
SLM in molecular science.
\\ ( https://arxiv.org/abs/2402.04119 ,  5717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04129
Date: Tue, 6 Feb 2024 16:31:11 GMT   (2499kb,D)

Title: OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free
  Class-Incremental Learning
Authors: Wei-Cheng Huang, Chun-Fu Chen, Hsiang Hsu
Categories: cs.LG cs.CV
Comments: Accepted by ICLR 2024
\\
  Recent works have shown that by using large pre-trained models along with
learnable prompts, rehearsal-free methods for class-incremental learning (CIL)
settings can achieve superior performance to prominent rehearsal-based ones.
Rehearsal-free CIL methods struggle with distinguishing classes from different
tasks, as those are not trained together. In this work we propose a
regularization method based on virtual outliers to tighten decision boundaries
of the classifier, such that confusion of classes among different tasks is
mitigated. Recent prompt-based methods often require a pool of task-specific
prompts, in order to prevent overwriting knowledge of previous tasks with that
of the new task, leading to extra computation in querying and composing an
appropriate prompt from the pool. This additional cost can be eliminated,
without sacrificing accuracy, as we reveal in the paper. We illustrate that a
simplified prompt-based method can achieve results comparable to previous
state-of-the-art (SOTA) methods equipped with a prompt pool, using much less
learnable parameters and lower inference cost. Our regularization method has
demonstrated its compatibility with different prompt-based methods, boosting
those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and
CIFAR-100 benchmarks. Our source code is available at
https://github.com/jpmorganchase/ovor.
\\ ( https://arxiv.org/abs/2402.04129 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04161
Date: Tue, 6 Feb 2024 17:18:59 GMT   (101kb,D)

Title: Attention with Markov: A Framework for Principled Analysis of
  Transformers via Markov Chains
Authors: Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle,
  Martin Jaggi, Hyeji Kim, Michael Gastpar
Categories: cs.LG cs.CL cs.IT math.IT stat.ML
\\
  In recent years, attention-based transformers have achieved tremendous
success across a variety of disciplines including natural languages. A key
ingredient behind their success is the generative pretraining procedure, during
which these models are trained on a large text corpus in an auto-regressive
manner. To shed light on this phenomenon, we propose a new framework that
allows both theory and systematic experiments to study the sequential modeling
capabilities of transformers through the lens of Markov chains. Inspired by the
Markovianity of natural languages, we model the data as a Markovian source and
utilize this framework to systematically study the interplay between the
data-distributional properties, the transformer architecture, the learnt
distribution, and the final model performance. In particular, we theoretically
characterize the loss landscape of single-layer transformers and show the
existence of global minima and bad local minima contingent upon the specific
data characteristics and the transformer architecture. Backed by experiments,
we demonstrate that our theoretical findings are in congruence with the
empirical results. We further investigate these findings in the broader context
of higher order Markov chains and deeper architectures, and outline open
problems in this arena. Code is available at
\url{https://github.com/Bond1995/Markov}.
\\ ( https://arxiv.org/abs/2402.04161 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04163
Date: Tue, 6 Feb 2024 17:21:06 GMT   (13977kb)

Title: Tempered Calculus for ML: Application to Hyperbolic Model Embedding
Authors: Richard Nock and Ehsan Amid and Frank Nielsen and Alexander Soen and
  Manfred K. Warmuth
Categories: cs.LG
ACM-class: I.2.6
\\
  Most mathematical distortions used in ML are fundamentally integral in
nature: $f$-divergences, Bregman divergences, (regularized) optimal transport
distances, integral probability metrics, geodesic distances, etc. In this
paper, we unveil a grounded theory and tools which can help improve these
distortions to better cope with ML requirements. We start with a generalization
of Riemann integration that also encapsulates functions that are not strictly
additive but are, more generally, $t$-additive, as in nonextensive statistical
mechanics. Notably, this recovers Volterra's product integral as a special
case. We then generalize the Fundamental Theorem of calculus using an extension
of the (Euclidean) derivative. This, along with a series of more specific
Theorems, serves as a basis for results showing how one can specifically
design, alter, or change fundamental properties of distortion measures in a
simple way, with a special emphasis on geometric- and ML-related properties
that are the metricity, hyperbolicity, and encoding. We show how to apply it to
a problem that has recently gained traction in ML: hyperbolic embeddings with a
"cheap" and accurate encoding along the hyperbolic vs Euclidean scale. We
unveil a new application for which the Poincar\'e disk model has very appealing
features, and our theory comes in handy: \textit{model} embeddings for boosted
combinations of decision trees, trained using the log-loss (trees) and logistic
loss (combinations).
\\ ( https://arxiv.org/abs/2402.04163 ,  13977kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04168
Date: Tue, 6 Feb 2024 17:24:06 GMT   (13279kb,D)

Title: Informed Reinforcement Learning for Situation-Aware Traffic Rule
  Exceptions
Authors: Daniel Bogdoll, Jing Qin, Moritz Nekolla, Ahmed Abouelazm, Tim Joseph,
  J. Marius Z\"ollner
Categories: cs.LG cs.CV cs.RO
Comments: Daniel Bogdoll and Jing Qin contributed equally. Accepted for
  publication at ICRA 2024
\\
  Reinforcement Learning is a highly active research field with promising
advancements. In the field of autonomous driving, however, often very simple
scenarios are being examined. Common approaches use non-interpretable control
commands as the action space and unstructured reward designs which lack
structure. In this work, we introduce Informed Reinforcement Learning, where a
structured rulebook is integrated as a knowledge source. We learn trajectories
and asses them with a situation-aware reward design, leading to a dynamic
reward which allows the agent to learn situations which require controlled
traffic rule exceptions. Our method is applicable to arbitrary RL models. We
successfully demonstrate high completion rates of complex scenarios with recent
model-based agents.
\\ ( https://arxiv.org/abs/2402.04168 ,  13279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04182
Date: Tue, 6 Feb 2024 17:42:39 GMT   (1784kb,D)

Title: Reinforcement Learning with Ensemble Model Predictive Safety
  Certification
Authors: Sven Gronauer, Tom Haider, Felippe Schmoeller da Roza, Klaus Diepold
Categories: cs.LG cs.RO
Comments: Published in: Proc. of the 23rd International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2024)
\\
  Reinforcement learning algorithms need exploration to learn. However,
unsupervised exploration prevents the deployment of such algorithms on
safety-critical tasks and limits real-world deployment. In this paper, we
propose a new algorithm called Ensemble Model Predictive Safety Certification
that combines model-based deep reinforcement learning with tube-based model
predictive control to correct the actions taken by a learning agent, keeping
safety constraint violations at a minimum through planning. Our approach aims
to reduce the amount of prior knowledge about the actual system by requiring
only offline data generated by a safe controller. Our results show that we can
achieve significantly fewer constraint violations than comparable reinforcement
learning methods.
\\ ( https://arxiv.org/abs/2402.04182 ,  1784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04193
Date: Tue, 6 Feb 2024 17:49:02 GMT   (1306kb,D)

Title: Gradient Coding in Decentralized Learning for Evading Stragglers
Authors: Chengxi Li and Mikael Skoglund
Categories: cs.LG eess.SP
\\
  In this paper, we consider a decentralized learning problem in the presence
of stragglers. Although gradient coding techniques have been developed for
distributed learning to evade stragglers, where the devices send encoded
gradients with redundant training data, it is difficult to apply those
techniques directly to decentralized learning scenarios. To deal with this
problem, we propose a new gossip-based decentralized learning method with
gradient coding (GOCO). In the proposed method, to avoid the negative impact of
stragglers, the parameter vectors are updated locally using encoded gradients
based on the framework of stochastic gradient coding and then averaged in a
gossip-based manner. We analyze the convergence performance of GOCO for
strongly convex loss functions. And we also provide simulation results to
demonstrate the superiority of the proposed method in terms of learning
performance compared with the baseline methods.
\\ ( https://arxiv.org/abs/2402.04193 ,  1306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04209
Date: Tue, 6 Feb 2024 18:05:30 GMT   (1969kb)

Title: Acute kidney injury prediction for non-critical care patients: a
  retrospective external and internal validation study
Authors: Esra Adiyeke, Yuanfang Ren, Benjamin Shickel, Matthew M. Ruppert,
  Ziyuan Guan, Sandra L. Kane-Gill, Raghavan Murugan, Nabihah Amatullah,
  Britney A. Stottlemyer, Tiffany L. Tran, Dan Ricketts, Christopher M Horvat,
  Parisa Rashidi, Azra Bihorac, Tezcan Ozrazgat-Baslanti
Categories: cs.LG cs.AI
\\
  Background: Acute kidney injury (AKI), the decline of kidney excretory
function, occurs in up to 18% of hospitalized admissions. Progression of AKI
may lead to irreversible kidney damage. Methods: This retrospective cohort
study includes adult patients admitted to a non-intensive care unit at the
University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of
Florida Health (UFH) (n = 127,202). We developed and compared deep learning and
conventional machine learning models to predict progression to Stage 2 or
higher AKI within the next 48 hours. We trained local models for each site (UFH
Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a
development cohort of patients from both sites (UFH-UPMC Model). We internally
and externally validated the models on each site and performed subgroup
analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3%
(n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under
the receiver operating curve values (AUROC) for the UFH test cohort ranged
between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged
between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort.
UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval [CI] [0.80,
0.83]) for UFH and 0.82 (95% CI [0.81,0.84]) for UPMC test cohorts; an area
under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06])
for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts. Kinetic estimated
glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen
remained the top three features with the highest influence across the models
and health centers. Conclusion: Locally developed models displayed marginally
reduced discrimination when tested on another institution, while the top set of
influencing features remained the same across the models and sites.
\\ ( https://arxiv.org/abs/2402.04209 ,  1969kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04211
Date: Tue, 6 Feb 2024 18:09:05 GMT   (6621kb,D)

Title: Variational Shapley Network: A Probabilistic Approach to Self-Explaining
  Shapley values with Uncertainty Quantification
Authors: Mert Ketenci, I\~nigo Urteaga, Victor Alfonso Rodriguez, No\'emie
  Elhadad, Adler Perotte
Categories: cs.LG stat.ML
\\
  Shapley values have emerged as a foundational tool in machine learning (ML)
for elucidating model decision-making processes. Despite their widespread
adoption and unique ability to satisfy essential explainability axioms,
computational challenges persist in their estimation when ($i$) evaluating a
model over all possible subset of input feature combinations, ($ii$) estimating
model marginals, and ($iii$) addressing variability in explanations. We
introduce a novel, self-explaining method that simplifies the computation of
Shapley values significantly, requiring only a single forward pass. Recognizing
the deterministic treatment of Shapley values as a limitation, we explore
incorporating a probabilistic framework to capture the inherent uncertainty in
explanations. Unlike alternatives, our technique does not rely directly on the
observed data space to estimate marginals; instead, it uses adaptable baseline
values derived from a latent, feature-specific embedding space, generated by a
novel masked neural network architecture. Evaluations on simulated and real
datasets underscore our technique's robust predictive and explanatory
performance.
\\ ( https://arxiv.org/abs/2402.04211 ,  6621kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04229
Date: Tue, 6 Feb 2024 18:36:52 GMT   (586kb,D)

Title: MusicRL: Aligning Music Generation to Human Preferences
Authors: Geoffrey Cideron, Sertan Girgin, Mauro Verzetti, Damien Vincent, Matej
  Kastelic, Zal\'an Borsos, Brian McWilliams, Victor Ungureanu, Olivier Bachem,
  Olivier Pietquin, Matthieu Geist, L\'eonard Hussenot, Neil Zeghidour and
  Andrea Agostinelli
Categories: cs.LG cs.SD eess.AS
\\
  We propose MusicRL, the first music generation system finetuned from human
feedback. Appreciation of text-to-music models is particularly subjective since
the concept of musicality as well as the specific intention behind a caption
are user-dependent (e.g. a caption such as "upbeat work-out music" can map to a
retro guitar solo or a techno pop beat). Not only this makes supervised
training of such models challenging, but it also calls for integrating
continuous human feedback in their post-deployment finetuning. MusicRL is a
pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete
audio tokens finetuned with reinforcement learning to maximise sequence-level
rewards. We design reward functions related specifically to text-adherence and
audio quality with the help from selected raters, and use those to finetune
MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial
dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning
from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model
that incorporates human feedback at scale. Human evaluations show that both
MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU
combines the two approaches and results in the best model according to human
raters. Ablation studies shed light on the musical attributes influencing human
preferences, indicating that text adherence and quality only account for a part
of it. This underscores the prevalence of subjectivity in musical appreciation
and calls for further involvement of human listeners in the finetuning of music
generation models.
\\ ( https://arxiv.org/abs/2402.04229 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04239
Date: Tue, 6 Feb 2024 18:47:52 GMT   (2296kb,D)

Title: CAST: Clustering Self-Attention using Surrogate Tokens for Efficient
  Transformers
Authors: Adjorn van Engelenhoven, Nicola Strisciuglio, Estefan\'ia Talavera
Categories: cs.LG
\\
  The Transformer architecture has shown to be a powerful tool for a wide range
of tasks. It is based on the self-attention mechanism, which is an inherently
computationally expensive operation with quadratic computational complexity:
memory usage and compute time increase quadratically with the length of the
input sequences, thus limiting the application of Transformers. In this work,
we propose a novel Clustering self-Attention mechanism using Surrogate Tokens
(CAST), to optimize the attention computation and achieve efficient
transformers. CAST utilizes learnable surrogate tokens to construct a cluster
affinity matrix, used to cluster the input sequence and generate novel cluster
summaries. The self-attention from within each cluster is then combined with
the cluster summaries of other clusters, enabling information flow across the
entire input sequence. CAST improves efficiency by reducing the complexity from
$O(N^2)$ to $O(\alpha N)$ where N is the sequence length, and {\alpha} is
constant according to the number of clusters and samples per cluster. We show
that CAST performs better than or comparable to the baseline Transformers on
long-range sequence modeling tasks, while also achieving higher results on time
and memory efficiency than other efficient transformers.
\\ ( https://arxiv.org/abs/2402.04239 ,  2296kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04248
Date: Tue, 6 Feb 2024 18:56:35 GMT   (3122kb,D)

Title: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning
  Tasks
Authors: Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho,
  Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos
Categories: cs.LG
Comments: 17 pages, 6 figures
\\
  State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed
as alternatives to Transformer networks in language modeling, by incorporating
gating, convolutions, and input-dependent token selection to mitigate the
quadratic cost of multi-head attention. Although SSMs exhibit competitive
performance, their in-context learning (ICL) capabilities, a remarkable
emergent property of modern language models that enables task execution without
parameter optimization, remain underexplored compared to Transformers. In this
study, we evaluate the ICL performance of SSMs, focusing on Mamba, against
Transformer models across various tasks. Our results show that SSMs perform
comparably to Transformers in standard regression ICL tasks, while
outperforming them in tasks like sparse parity learning. However, SSMs fall
short in tasks involving non-standard retrieval functionality. To address these
limitations, we introduce a hybrid model, \variant, that combines Mamba with
attention blocks, surpassing individual models in tasks where they struggle
independently. Our findings suggest that hybrid architectures offer promising
avenues for enhancing ICL in language models.
\\ ( https://arxiv.org/abs/2402.04248 ,  3122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04249
Date: Tue, 6 Feb 2024 18:59:08 GMT   (1566kb,D)

Title: HarmBench: A Standardized Evaluation Framework for Automated Red Teaming
  and Robust Refusal
Authors: Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman
  Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan
  Hendrycks
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: Website: https://www.harmbench.org
\\
  Automated red teaming holds substantial promise for uncovering and mitigating
the risks associated with the malicious use of large language models (LLMs),
yet the field lacks a standardized evaluation framework to rigorously assess
new methods. To address this issue, we introduce HarmBench, a standardized
evaluation framework for automated red teaming. We identify several desirable
properties previously unaccounted for in red teaming evaluations and
systematically design HarmBench to meet these criteria. Using HarmBench, we
conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs
and defenses, yielding novel insights. We also introduce a highly efficient
adversarial training method that greatly enhances LLM robustness across a wide
range of attacks, demonstrating how HarmBench enables codevelopment of attacks
and defenses. We open source HarmBench at
https://github.com/centerforaisafety/HarmBench.
\\ ( https://arxiv.org/abs/2402.04249 ,  1566kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.03316 (*cross-listing*)
Date: Tue, 3 Oct 2023 14:58:23 GMT   (19843kb)

Title: Artificial Intelligence for EEG Prediction: Applied Chaos Theory
Authors: Soul Syrup
Categories: q-bio.NC cs.AI math.DS nlin.CD
Comments: 70 pages, 14 figures, project report
\\
  In the present research, we delve into the intricate realm of
electroencephalogram (EEG) data analysis, focusing on sequence-to-sequence
prediction of data across 32 EEG channels. The study harmoniously fuses the
principles of applied chaos theory and dynamical systems theory to engender a
novel feature set, enriching the representational capacity of our deep learning
model. The endeavour's cornerstone is a transformer-based sequence-to-sequence
architecture, calibrated meticulously to capture the non-linear and
high-dimensional temporal dependencies inherent in EEG sequences. Through
judicious architecture design, parameter initialisation strategies, and
optimisation techniques, we have navigated the intricate balance between
computational expediency and predictive performance. Our model stands as a
vanguard in EEG data sequence prediction, demonstrating remarkable
generalisability and robustness. The findings not only extend our understanding
of EEG data dynamics but also unveil a potent analytical framework that can be
adapted to diverse temporal sequence prediction tasks in neuroscience and
beyond.
\\ ( https://arxiv.org/abs/2402.03316 ,  19843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03319 (*cross-listing*)
Date: Wed, 3 Jan 2024 06:22:36 GMT   (12581kb,D)

Title: Physical Reservoir Computing Enabled by Solitary Waves and
  Biologically-Inspired Nonlinear Transformation of Input Data
Authors: Ivan S. Maksymov
Categories: cs.NE cs.AI nlin.CD nlin.PS physics.flu-dyn
Comments: The Supplementary Video can be found here:
  https://youtu.be/Zwu3KEo8f00
\\
  Reservoir computing (RC) systems can efficiently forecast chaotic time series
using nonlinear dynamical properties of an artificial neural network of random
connections. The versatility of RC systems has motivated further research on
both hardware counterparts of traditional RC algorithms and more efficient
RC-like schemes. Inspired by the nonlinear processes in a living biological
brain and using solitary waves excited on the surface of a flowing liquid film,
in this paper we experimentally validate a physical RC system that substitutes
the effect of randomness for a nonlinear transformation of input data. Carrying
out all operations using a microcontroller with a minimal computational power,
we demonstrate that the so-designed RC system serves as a technically simple
hardware counterpart to the `next-generation' improvement of the traditional RC
algorithm.
\\ ( https://arxiv.org/abs/2402.03319 ,  12581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03327 (*cross-listing*)
Date: Tue, 9 Jan 2024 06:20:23 GMT   (693kb,D)

Title: Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with
  Large Language Models
Authors: Dingning Liu, Xiaoshui Huang, Yuenan Hou, Zhihui Wang, Zhenfei Yin,
  Yongshun Gong, Peng Gao, Wanli Ouyang
Categories: cs.CV cs.AI cs.CL
Comments: 10 pages, 6 figures
\\
  In this paper, we introduce Uni3D-LLM, a unified framework that leverages a
Large Language Model (LLM) to integrate tasks of 3D perception, generation, and
editing within point cloud scenes. This framework empowers users to
effortlessly generate and modify objects at specified locations within a scene,
guided by the versatility of natural language descriptions. Uni3D-LLM harnesses
the expressive power of natural language to allow for precise command over the
generation and editing of 3D objects, thereby significantly enhancing
operational flexibility and controllability. By mapping point cloud into the
unified representation space, Uni3D-LLM achieves cross-application
functionality, enabling the seamless execution of a wide array of tasks,
ranging from the accurate instantiation of 3D objects to the diverse
requirements of interactive design. Through a comprehensive suite of rigorous
experiments, the efficacy of Uni3D-LLM in the comprehension, generation, and
editing of point cloud has been validated. Additionally, we have assessed the
impact of integrating a point cloud perception module on the generation and
editing processes, confirming the substantial potential of our approach for
practical applications.
\\ ( https://arxiv.org/abs/2402.03327 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03328 (*cross-listing*)
Date: Tue, 9 Jan 2024 18:18:32 GMT   (5495kb,D)

Title: Large-scale Generative AI Models Lack Visual Number Sense
Authors: Alberto Testolin, Kuinan Hou, Marco Zorzi
Categories: cs.CV cs.AI cs.NE
\\
  Humans can readily judge the number of objects in a visual scene, even
without counting, and such a skill has been documented in a variety of animal
species and in babies prior to language development and formal schooling.
Numerical judgments are error-free for small sets, while for larger collections
responses become approximate, with variability increasing proportionally to the
target number. This response pattern is observed for items of all kinds,
despite variation in object features (such as color or shape), suggesting that
our visual number sense relies on abstract representations of numerosity. Here,
we investigated whether generative Artificial Intelligence (AI) models based on
large-scale transformer architectures can reliably name the number of objects
in simple visual stimuli or generate images containing a target number of items
in the 1-10 range. Surprisingly, none of the foundation models considered
performed in a human-like way: They all made striking errors even with small
numbers, the response variability often did not increase in a systematic way,
and the pattern of errors varied with object category. Our findings demonstrate
that advanced AI systems still lack a basic ability that supports an intuitive
understanding of numbers, which in humans is foundational for numeracy and
mathematical development.
\\ ( https://arxiv.org/abs/2402.03328 ,  5495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03329 (*cross-listing*)
Date: Wed, 10 Jan 2024 11:46:49 GMT   (7875kb,D)

Title: Unsupervised Salient Patch Selection for Data-Efficient Reinforcement
  Learning
Authors: Zhaohui Jiang, Paul Weng
Categories: cs.CV cs.AI
DOI: 10.1007/978-3-031-43421-1_33
\\
  To improve the sample efficiency of vision-based deep reinforcement learning
(RL), we propose a novel method, called SPIRL, to automatically extract
important patches from input images. Following Masked Auto-Encoders, SPIRL is
based on Vision Transformer models pre-trained in a self-supervised fashion to
reconstruct images from randomly-sampled patches. These pre-trained models can
then be exploited to detect and select salient patches, defined as hard to
reconstruct from neighboring patches. In RL, the SPIRL agent processes selected
salient patches via an attention module. We empirically validate SPIRL on Atari
games to test its data-efficiency against relevant state-of-the-art methods,
including some traditional model-based methods and keypoint-based models. In
addition, we analyze our model's interpretability capabilities.
\\ ( https://arxiv.org/abs/2402.03329 ,  7875kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03337 (*cross-listing*)
Date: Tue, 16 Jan 2024 09:04:05 GMT   (3628kb,D)

Title: Reinforcement-learning robotic sailboats: simulator and preliminary
  results
Authors: Eduardo Charles Vasconcellos (UFF), Ronald M Sampaio, Andr\'e P D
  Ara\'ujo (UFF), Esteban Walter Gonzales Clua, Philippe Preux (SEQUEL, GRAppA
  - LIFL), Raphael Guerra, Luiz M G Gon\c{c}alves (UFRN), Luis Mart\'i, Hernan
  Lira, Nayat Sanchez-Pi
Categories: cs.RO cs.AI cs.LG
Journal-ref: NeurIPS 2023 Workshop on Robot Learning Workshop: Pretraining,
  Fine-Tuning, and Generalization with Large Scale Models, Dec 2023, New
  Orelans, United States
\\
  This work focuses on the main challenges and problems in developing a virtual
oceanic environment reproducing real experiments using Unmanned Surface
Vehicles (USV) digital twins. We introduce the key features for building
virtual worlds, considering using Reinforcement Learning (RL) agents for
autonomous navigation and control. With this in mind, the main problems concern
the definition of the simulation equations (physics and mathematics), their
effective implementation, and how to include strategies for simulated control
and perception (sensors) to be used with RL. We present the modeling,
implementation steps, and challenges required to create a functional digital
twin based on a real robotic sailing vessel. The application is immediate for
developing navigation algorithms based on RL to be applied on real boats.
\\ ( https://arxiv.org/abs/2402.03337 ,  3628kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03347 (*cross-listing*)
Date: Thu, 25 Jan 2024 03:58:40 GMT   (304kb)

Title: Transfer Learning With Densenet201 Architecture Model For Potato Leaf
  Disease Classification
Authors: Rifqi Alfinnur Charisma and Faisal Dharma Adhinata
Categories: cs.CV cs.AI
\\
  Potato plants are plants that are beneficial to humans. Like other plants in
general, potato plants also have diseases; if this disease is not treated
immediately, there will be a significant decrease in food production.
Therefore, it is necessary to detect diseases quickly and precisely so that
disease control can be carried out effectively and efficiently. Classification
of potato leaf disease can be done directly. Still, the symptoms cannot always
explain the type of disease that attacks potato leaves because there are many
types of diseases with symptoms that look the same. Humans also have
deficiencies in determining the results of identification of potato leaf
disease, so sometimes the results of identification between individuals can be
different. Therefore, the use of Deep Learning for the classification process
of potato leaf disease is expected to shorten the time and have a high
classification accuracy. This study uses a deep learning method with the
DenseNet201 architecture. The choice to use the DenseNet201 algorithm in this
study is because the model can identify important features of potato leaves and
recognize early signs of emerging diseases. This study aimed to evaluate the
effectiveness of the transfer learning method with the DenseNet201 architecture
in increasing the classification accuracy of potato leaf disease compared to
traditional classification methods. This study uses two types of scenarios,
namely, comparing the number of dropouts and comparing the three optimizers.
This test produces the best model using dropout 0.1 and Adam optimizer with an
accuracy of 99.5% for training, 95.2% for validation, and 96% for the confusion
matrix. In this study, using data testing, as many as 40 images were tested
into the model that has been built. The test results on this model resulted in
a new accuracy for classifying potato leaf disease, namely 92.5%.
\\ ( https://arxiv.org/abs/2402.03347 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03348 (*cross-listing*)
Date: Thu, 25 Jan 2024 07:20:23 GMT   (25125kb,D)

Title: Respect the model: Fine-grained and Robust Explanation with Sharing
  Ratio Decomposition
Authors: Sangyu Han, Yearim Kim, Nojun Kwak
Categories: cs.CV cs.AI
Comments: To be published in ICLR 2024
\\
  The truthfulness of existing explanation methods in authentically elucidating
the underlying model's decision-making process has been questioned. Existing
methods have deviated from faithfully representing the model, thus susceptible
to adversarial attacks. To address this, we propose a novel eXplainable AI
(XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects
the model's inference process, resulting in significantly enhanced robustness
in our explanations. Different from the conventional emphasis on the neuronal
level, we adopt a vector perspective to consider the intricate nonlinear
interactions between filters. We also introduce an interesting observation
termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the
importance of inactive neurons and redefine relevance encapsulating all
relevant information including both active and inactive neurons. Our method,
SRD, allows for the recursive decomposition of a Pointwise Feature Vector
(PFV), providing a high-resolution Effective Receptive Field (ERF) at any
layer.
\\ ( https://arxiv.org/abs/2402.03348 ,  25125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03349 (*cross-listing*)
Date: Thu, 25 Jan 2024 12:03:50 GMT   (2067kb,D)

Title: When Geoscience Meets Generative AI and Large Language Models:
  Foundations, Trends, and Future Challenges
Authors: Abdenour Hadid, Tanujit Chakraborty, Daniel Busby
Categories: physics.geo-ph cs.AI cs.LG physics.ao-ph
\\
  Generative Artificial Intelligence (GAI) represents an emerging field that
promises the creation of synthetic data and outputs in different modalities.
GAI has recently shown impressive results across a large spectrum of
applications ranging from biology, medicine, education, legislation, computer
science, and finance. As one strives for enhanced safety, efficiency, and
sustainability, generative AI indeed emerges as a key differentiator and
promises a paradigm shift in the field. This paper explores the potential
applications of generative AI and large language models in geoscience. The
recent developments in the field of machine learning and deep learning have
enabled the generative model's utility for tackling diverse prediction
problems, simulation, and multi-criteria decision-making challenges related to
geoscience and Earth system dynamics. This survey discusses several GAI models
that have been used in geoscience comprising generative adversarial networks
(GANs), physics-informed neural networks (PINNs), and generative pre-trained
transformer (GPT)-based structures. These tools have helped the geoscience
community in several applications, including (but not limited to) data
generation/augmentation, super-resolution, panchromatic sharpening, haze
removal, restoration, and land surface changing. Some challenges still remain
such as ensuring physical interpretation, nefarious use cases, and
trustworthiness. Beyond that, GAI models show promises to the geoscience
community, especially with the support to climate change, urban science,
atmospheric science, marine science, and planetary science through their
extraordinary ability to data-driven modeling and uncertainty quantification.
\\ ( https://arxiv.org/abs/2402.03349 ,  2067kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03355 (*cross-listing*)
Date: Fri, 26 Jan 2024 20:09:31 GMT   (7270kb)

Title: Techniques to Detect Crime Leaders within a Criminal Network: A Survey,
  Experimental, and Comparative Evaluations
Authors: Kamal Taha and Abdulhadi Shoufan
Categories: cs.SI cs.AI
\\
  This survey paper offers a thorough analysis of techniques and algorithms
used in the identification of crime leaders within criminal networks. For each
technique, the paper examines its effectiveness, limitations, potential for
improvement, and future prospects. The main challenge faced by existing survey
papers focusing on algorithms for identifying crime leaders and predicting
crimes is effectively categorizing these algorithms. To address this
limitation, this paper proposes a new methodological taxonomy that
hierarchically classifies algorithms into more detailed categories and specific
techniques. The paper includes empirical and experimental evaluations to rank
the different techniques. The combination of the methodological taxonomy,
empirical evaluations, and experimental comparisons allows for a nuanced and
comprehensive understanding of the techniques and algorithms for identifying
crime leaders, assisting researchers in making informed decisions. Moreover,
the paper offers valuable insights into the future prospects of techniques for
identifying crime leaders, emphasizing potential advancements and opportunities
for further research. Here's an overview of our empirical analysis findings and
experimental insights, along with the solution we've devised: (1) PageRank and
Eigenvector centrality are reliable for mapping network connections, (2) Katz
Centrality can effectively identify influential criminals through indirect
links, stressing their significance in criminal networks, (3) current models
fail to account for the specific impacts of criminal influence levels, the
importance of socio-economic context, and the dynamic nature of criminal
networks and hierarchies, and (4) we propose enhancements, such as
incorporating temporal dynamics and sentiment analysis to reflect the fluidity
of criminal activities and relationships, which could improve the detection of
key criminals .
\\ ( https://arxiv.org/abs/2402.03355 ,  7270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03357 (*cross-listing*)
Date: Sun, 28 Jan 2024 06:05:01 GMT   (874kb,D)

Title: Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers
  via Self-Imitation Learning
Authors: Xiaofei Xu, Ke Deng, Michael Dann, Xiuzhen Zhang
Categories: cs.SI cs.AI cs.LG
Comments: 10 pages, full version of this paper is accepted by AAAI'24
\\
  This study aims to minimize the influence of fake news on social networks by
deploying debunkers to propagate true news. This is framed as a reinforcement
learning problem, where, at each stage, one user is selected to propagate true
news. A challenging issue is episodic reward where the "net" effect of
selecting individual debunkers cannot be discerned from the interleaving
information propagation on social networks, and only the collective effect from
mitigation efforts can be observed. Existing Self-Imitation Learning (SIL)
methods have shown promise in learning from episodic rewards, but are
ill-suited to the real-world application of fake news mitigation because of
their poor sample efficiency. To learn a more effective debunker selection
policy for fake news mitigation, this study proposes NAGASIL - Negative
sampling and state Augmented Generative Adversarial Self-Imitation Learning,
which consists of two improvements geared towards fake news mitigation:
learning from negative samples, and an augmented state representation to
capture the "real" environment state by integrating the current observed state
with the previous state-action pairs from the same campaign. Experiments on two
social networks show that NAGASIL yields superior performance to standard GASIL
and state-of-the-art fake news mitigation models.
\\ ( https://arxiv.org/abs/2402.03357 ,  874kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03358 (*cross-listing*)
Date: Mon, 29 Jan 2024 01:19:09 GMT   (241kb,D)

Title: A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening,
  and Condensation
Authors: Mohammad Hashemi, Shengbo Gong, Juntong Ni, Wenqi Fan, B. Aditya
  Prakash, Wei Jin
Categories: cs.SI cs.AI cs.DS cs.LG
Comments: 15 pages, 3 tables, 2 figures
\\
  Many real-world datasets can be naturally represented as graphs, spanning a
wide range of domains. However, the increasing complexity and size of graph
datasets present significant challenges for analysis and computation. In
response, graph reduction techniques have gained prominence for simplifying
large graphs while preserving essential properties. In this survey, we aim to
provide a comprehensive understanding of graph reduction methods, including
graph sparsification, graph coarsening, and graph condensation. Specifically,
we establish a unified definition for these methods and introduce a
hierarchical taxonomy to categorize the challenges they address. Our survey
then systematically reviews the technical details of these methods and
emphasizes their practical applications across diverse scenarios. Furthermore,
we outline critical research directions to ensure the continued effectiveness
of graph reduction techniques, as well as provide a comprehensive paper list at
https://github.com/ChandlerBang/awesome-graph-reduction. We hope this survey
will bridge literature gaps and propel the advancement of this promising field.
\\ ( https://arxiv.org/abs/2402.03358 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03362 (*cross-listing*)
Date: Tue, 30 Jan 2024 09:10:53 GMT   (69kb,D)

Title: NanoNER: Named Entity Recognition for nanobiology using experts'
  knowledge and distant supervision
Authors: Martin Lentschat (SIGMA, GETALP), Cyril Labb\'e (LIG, SIGMA), Ran
  Cheng (LIG, SIGMA)
Categories: cs.IR cs.AI cs.CL
\\
  Here we present the training and evaluation of NanoNER, a Named Entity
Recognition (NER) model for Nanobiology. NER consists in the identification of
specific entities in spans of unstructured texts and is often a primary task in
Natural Language Processing (NLP) and Information Extraction. The aim of our
model is to recognise entities previously identified by domain experts as
constituting the essential knowledge of the domain. Relying on ontologies,
which provide us with a domain vocabulary and taxonomy, we implemented an
iterative process enabling experts to determine the entities relevant to the
domain at hand. We then delve into the potential of distant supervision
learning in NER, supporting how this method can increase the quantity of
annotated data with minimal additional manpower. On our full corpus of 728
full-text nanobiology articles, containing more than 120k entity occurrences,
NanoNER obtained a F1-score of 0.98 on the recognition of previously known
entities. Our model also demonstrated its ability to discover new entities in
the text, with precision scores ranging from 0.77 to 0.81. Ablation experiments
further confirmed this and allowed us to assess the dependency of our approach
on the external resources. It highlighted the dependency of the approach to the
resource, while also confirming its ability to rediscover up to 30% of the
ablated terms. This paper details the methodology employed, experimental
design, and key findings, providing valuable insights and directions for future
related researches on NER in specialized domain. Furthermore, since our
approach require minimal manpower , we believe that it can be generalized to
other specialized fields.
\\ ( https://arxiv.org/abs/2402.03362 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03366 (*cross-listing*)
Date: Wed, 31 Jan 2024 14:06:26 GMT   (1227kb,D)

Title: Uncertainty-Aware Explainable Recommendation with Large Language Models
Authors: Yicui Peng, Hao Chen, Chingsheng Lin, Guo Huang, Jinrong Hu, Hui Guo,
  Bin Kong, Shu Hu, Xi Wu, and Xin Wang
Categories: cs.IR cs.AI cs.CL cs.LG
\\
  Providing explanations within the recommendation system would boost user
satisfaction and foster trust, especially by elaborating on the reasons for
selecting recommended items tailored to the user. The predominant approach in
this domain revolves around generating text-based explanations, with a notable
emphasis on applying large language models (LLMs). However, refining LLMs for
explainable recommendations proves impractical due to time constraints and
computing resource limitations. As an alternative, the current approach
involves training the prompt rather than the LLM. In this study, we developed a
model that utilizes the ID vectors of user and item inputs as prompts for
GPT-2. We employed a joint training mechanism within a multi-task learning
framework to optimize both the recommendation task and explanation task. This
strategy enables a more effective exploration of users' interests, improving
recommendation effectiveness and user satisfaction. Through the experiments,
our method achieving 1.59 DIV, 0.57 USR and 0.41 FCR on the Yelp, TripAdvisor
and Amazon dataset respectively, demonstrates superior performance over four
SOTA methods in terms of explainability evaluation metric. In addition, we
identified that the proposed model is able to ensure stable textual quality on
the three public datasets.
\\ ( https://arxiv.org/abs/2402.03366 ,  1227kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03368 (*cross-listing*)
Date: Thu, 1 Feb 2024 23:51:29 GMT   (6578kb)

Title: Empirical and Experimental Perspectives on Big Data in Recommendation
  Systems: A Comprehensive Survey
Authors: Kamal Taha, Paul D. Yoo, Aya Taha
Categories: cs.IR cs.AI
\\
  This survey paper provides a comprehensive analysis of big data algorithms in
recommendation systems, addressing the lack of depth and precision in existing
literature. It proposes a two-pronged approach: a thorough analysis of current
algorithms and a novel, hierarchical taxonomy for precise categorization. The
taxonomy is based on a tri-level hierarchy, starting with the methodology
category and narrowing down to specific techniques. Such a framework allows for
a structured and comprehensive classification of algorithms, assisting
researchers in understanding the interrelationships among diverse algorithms
and techniques. Covering a wide range of algorithms, this taxonomy first
categorizes algorithms into four main analysis types: User and Item
Similarity-Based Methods, Hybrid and Combined Approaches, Deep Learning and
Algorithmic Methods, and Mathematical Modeling Methods, with further
subdivisions into sub-categories and techniques. The paper incorporates both
empirical and experimental evaluations to differentiate between the techniques.
The empirical evaluation ranks the techniques based on four criteria. The
experimental assessments rank the algorithms that belong to the same category,
sub-category, technique, and sub-technique. Also, the paper illuminates the
future prospects of big data techniques in recommendation systems, underscoring
potential advancements and opportunities for further research in this field
\\ ( https://arxiv.org/abs/2402.03368 ,  6578kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03370 (*cross-listing*)
Date: Fri, 2 Feb 2024 08:15:43 GMT   (49kb,D)

Title: Detection of tortured phrases in scientific literature
Authors: El\'ena Martel (SIGMA, LIG), Martin Lentschat (SIGMA, GETALP), Cyril
  Labb\'e (LIG, SIGMA )
Categories: cs.IR cs.AI cs.CL cs.DL
Journal-ref: Proceedings of the 2nd Workshop on Information Extraction from
  Scientific Publications, Nov 2023, Bali, Indonesia
\\
  This paper presents various automatic detection methods to extract so called
tortured phrases from scientific papers. These tortured phrases, e.g. flag to
clamor instead of signal to noise, are the results of paraphrasing tools used
to escape plagiarism detection. We built a dataset and evaluated several
strategies to flag previously undocumented tortured phrases. The proposed and
tested methods are based on language models and either on embeddings
similarities or on predictions of masked token. We found that an approach using
token prediction and that propagates the scores to the chunk level gives the
best results. With a recall value of .87 and a precision value of .61, it could
retrieve new tortured phrases to be submitted to domain experts for validation.
\\ ( https://arxiv.org/abs/2402.03370 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03379 (*cross-listing*)
Date: Sun, 4 Feb 2024 03:30:25 GMT   (3149kb,D)

Title: Entire Chain Uplift Modeling with Context-Enhanced Learning for
  Intelligent Marketing
Authors: Yinqiu Huang, Shuli Wang, Min Gao, Xue Wei, Changhao Li, Chuan Luo,
  Yinhua Zhu, Xiong Xiao, Yi Luo
Categories: cs.IR cs.AI cs.LG
Comments: Accepted by WWW2024
\\
  Uplift modeling, vital in online marketing, seeks to accurately measure the
impact of various strategies, such as coupons or discounts, on different users
by predicting the Individual Treatment Effect (ITE). In an e-commerce setting,
user behavior follows a defined sequential chain, including impression, click,
and conversion. Marketing strategies exert varied uplift effects at each stage
within this chain, impacting metrics like click-through and conversion rate.
Despite its utility, existing research has neglected to consider the inter-task
across all stages impacts within a specific treatment and has insufficiently
utilized the treatment information, potentially introducing substantial bias
into subsequent marketing decisions. We identify these two issues as the
chain-bias problem and the treatment-unadaptive problem. This paper introduces
the Entire Chain UPlift method with context-enhanced learning (ECUP), devised
to tackle these issues. ECUP consists of two primary components: 1) the Entire
Chain-Enhanced Network, which utilizes user behavior patterns to estimate ITE
throughout the entire chain space, models the various impacts of treatments on
each task, and integrates task prior information to enhance context awareness
across all stages, capturing the impact of treatment on different tasks, and 2)
the Treatment-Enhanced Network, which facilitates fine-grained treatment
modeling through bit-level feature interactions, thereby enabling adaptive
feature adjustment. Extensive experiments on public and industrial datasets
validate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan
food delivery platform, serving millions of daily active users, with the
related dataset released for future research.
\\ ( https://arxiv.org/abs/2402.03379 ,  3149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03384 (*cross-listing*)
Date: Sun, 4 Feb 2024 09:07:07 GMT   (6621kb)

Title: Survival and grade of the glioma prediction using transfer learning
Authors: Santiago Valbuena Rubio, Mar\'ia Teresa Garc\'ia-Ord\'as, Oscar
  Garc\'ia-Olalla Olivera, H\'ector Alaiz-Moret\'on, Maria-Inmaculada
  Gonz\'alez-Alonso and Jos\'e Alberto Ben\'itez-Andrades
Categories: cs.CV cs.AI cs.LG
Journal-ref: PeerJ Computer Science, Volume 9, December 2023, ID e1723
DOI: 10.7717/peerj-cs.1723
\\
  Glioblastoma is a highly malignant brain tumor with a life expectancy of only
3 to 6 months without treatment. Detecting and predicting its survival and
grade accurately are crucial. This study introduces a novel approach using
transfer learning techniques. Various pre-trained networks, including
EfficientNet, ResNet, VGG16, and Inception, were tested through exhaustive
optimization to identify the most suitable architecture. Transfer learning was
applied to fine-tune these models on a glioblastoma image dataset, aiming to
achieve two objectives: survival and tumor grade prediction.The experimental
results show 65% accuracy in survival prediction, classifying patients into
short, medium, or long survival categories. Additionally, the prediction of
tumor grade achieved an accuracy of 97%, accurately differentiating low-grade
gliomas (LGG) and high-grade gliomas (HGG). The success of the approach is
attributed to the effectiveness of transfer learning, surpassing the current
state-of-the-art methods. In conclusion, this study presents a promising method
for predicting the survival and grade of glioblastoma. Transfer learning
demonstrates its potential in enhancing prediction models, particularly in
scenarios with limited large datasets. These findings hold promise for
improving diagnostic and treatment approaches for glioblastoma patients.
\\ ( https://arxiv.org/abs/2402.03384 ,  6621kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03390 (*cross-listing*)
Date: Sun, 4 Feb 2024 14:41:56 GMT   (27603kb,D)

Title: PixelGen: Rethinking Embedded Camera Systems
Authors: Kunjun Li, Manoj Gulati, Steven Waskito, Dhairya Shah, Shantanu
  Chakrabarty, Ambuj Varshney
Categories: eess.IV cs.AI cs.CV cs.NI
\\
  Embedded camera systems are ubiquitous, representing the most widely deployed
example of a wireless embedded system. They capture a representation of the
world - the surroundings illuminated by visible or infrared light. Despite
their widespread usage, the architecture of embedded camera systems has
remained unchanged, which leads to limitations. They visualize only a tiny
portion of the world. Additionally, they are energy-intensive, leading to
limited battery lifespan. We present PixelGen, which re-imagines embedded
camera systems. Specifically, PixelGen combines sensors, transceivers, and
low-resolution image and infrared vision sensors to capture a broader world
representation. They are deliberately chosen for their simplicity, low bitrate,
and power consumption, culminating in an energy-efficient platform. We show
that despite the simplicity, the captured data can be processed using
transformer-based image and language models to generate novel representations
of the environment. For example, we demonstrate that it can allow the
generation of high-definition images, while the camera utilises low-power,
low-resolution monochrome cameras. Furthermore, the capabilities of PixelGen
extend beyond traditional photography, enabling visualization of phenomena
invisible to conventional cameras, such as sound waves. PixelGen can enable
numerous novel applications, and we demonstrate that it enables unique
visualization of the surroundings that are then projected on extended reality
headsets. We believe, PixelGen goes beyond conventional cameras and opens new
avenues for research and photography.
\\ ( https://arxiv.org/abs/2402.03390 ,  27603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03396 (*cross-listing*)
Date: Sun, 4 Feb 2024 22:48:05 GMT   (443kb,D)

Title: UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large
  Language Models for Program Testing
Authors: Yifeng He, Jiabo Huang, Yuyang Rong, Yiwen Guo, Ethan Wang, Hao Chen
Categories: cs.SE cs.AI cs.CL cs.CR cs.LG
Comments: 8 pages, 5 figures
\\
  The remarkable capability of large language models (LLMs) in generating
high-quality code has drawn increasing attention in the software testing
community. However, existing code LLMs often demonstrate unsatisfactory
capabilities in generating accurate and complete tests since they were trained
on code snippets collected without differentiating between code for testing
purposes and other code. In this paper, we present a large-scale dataset
UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test
Synthesis. Associating tests with the tested functions is crucial for LLMs to
infer the expected behavior and the logic paths to be verified. By leveraging
Language Server Protocol, UniTSyn achieves the challenging goal of collecting
focal-test pairs without per-project execution setups or per-language
heuristics that tend to be fragile and difficult to scale. It contains 2.7
million focal-test pairs across five mainstream programming languages, making
it possible to be utilized for enhancing the test generation ability of LLMs.
The details of UniTSyn can be found in Table 1. Our experiments demonstrate
that, by building an autoregressive model based on UniTSyn, we can achieve
significant benefits in learning and understanding unit test representations,
resulting in improved generation accuracy and code coverage across all
evaluated programming languages. Code and data will be publicly available.
\\ ( https://arxiv.org/abs/2402.03396 ,  443kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03500 (*cross-listing*)
Date: Mon, 5 Feb 2024 20:33:00 GMT   (7861kb,D)

Title: Curriculum reinforcement learning for quantum architecture search under
  hardware errors
Authors: Yash J. Patel, Akash Kundu, Mateusz Ostaszewski, Xavier Bonet-Monroig,
  Vedran Dunjko, and Onur Danaci
Categories: quant-ph cs.AI cs.LG
Comments: 32 pages, 11 figures, 6 tables. Accepted at ICLR 2024
\\
  The key challenge in the noisy intermediate-scale quantum era is finding
useful circuits compatible with current device limitations. Variational quantum
algorithms (VQAs) offer a potential solution by fixing the circuit architecture
and optimizing individual gate parameters in an external loop. However,
parameter optimization can become intractable, and the overall performance of
the algorithm depends heavily on the initially chosen circuit architecture.
Several quantum architecture search (QAS) algorithms have been developed to
design useful circuit architectures automatically. In the case of parameter
optimization alone, noise effects have been observed to dramatically influence
the performance of the optimizer and final outcomes, which is a key line of
study. However, the effects of noise on the architecture search, which could be
just as critical, are poorly understood. This work addresses this gap by
introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm
designed to tackle challenges in realistic VQA deployment. The algorithm
incorporates (i) a 3D architecture encoding and restrictions on environment
dynamics to explore the search space of possible circuits efficiently, (ii) an
episode halting scheme to steer the agent to find shorter circuits, and (iii) a
novel variant of simultaneous perturbation stochastic approximation as an
optimizer for faster convergence. To facilitate studies, we developed an
optimized simulator for our algorithm, significantly improving computational
efficiency in simulating noisy quantum circuits by employing the Pauli-transfer
matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing
on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS
algorithms across several metrics in both noiseless and noisy environments.
\\ ( https://arxiv.org/abs/2402.03500 ,  7861kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03501 (*cross-listing*)
Date: Mon, 5 Feb 2024 20:34:32 GMT   (347kb,D)

Title: An Inpainting-Infused Pipeline for Attire and Background Replacement
Authors: Felipe Rodrigues Perche-Mahlow and Andr\'e Felipe-Zanella and William
  Alberto Cruz-Casta\~neda and Marcellus Amadeus
Categories: cs.CV cs.AI cs.CL
\\
  In recent years, groundbreaking advancements in Generative Artificial
Intelligence (GenAI) have triggered a transformative paradigm shift,
significantly influencing various domains. In this work, we specifically
explore an integrated approach, leveraging advanced techniques in GenAI and
computer vision emphasizing image manipulation. The methodology unfolds through
several stages, including depth estimation, the creation of inpaint masks based
on depth information, the generation and replacement of backgrounds utilizing
Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the
subsequent replacement of clothes and application of aesthetic changes through
an inpainting pipeline. Experiments conducted in this study underscore the
methodology's efficacy, highlighting its potential to produce visually
captivating content. The convergence of these advanced techniques allows users
to input photographs of individuals and manipulate them to modify clothing and
background based on specific prompts without manually input inpainting masks,
effectively placing the subjects within the vast landscape of creative
imagination.
\\ ( https://arxiv.org/abs/2402.03501 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03535 (*cross-listing*)
Date: Mon, 5 Feb 2024 21:44:19 GMT   (18735kb,D)

Title: Preliminary Report on Mantis Shrimp: a Multi-Survey Computer Vision
  Photometric Redshift Model
Authors: Andrew Engel, Gautham Narayan, Nell Byler
Categories: astro-ph.IM cs.AI
Comments: 4 pages, 1 figure, 1 table. Submitted to AI4Differential Equations in
  Science Workshop at ICLR24. Public repository unavailable while under
  institutional review
\\
  The availability of large, public, multi-modal astronomical datasets presents
an opportunity to execute novel research that straddles the line between
science of AI and science of astronomy. Photometric redshift estimation is a
well-established subfield of astronomy. Prior works show that computer vision
models typically outperform catalog-based models, but these models face
additional complexities when incorporating images from more than one instrument
or sensor. In this report, we detail our progress creating Mantis Shrimp, a
multi-survey computer vision model for photometric redshift estimation that
fuses ultra-violet (GALEX), optical (PanSTARRS), and infrared (UnWISE) imagery.
We use deep learning interpretability diagnostics to measure how the model
leverages information from the different inputs. We reason about the behavior
of the CNNs from the interpretability metrics, specifically framing the result
in terms of physically-grounded knowledge of galaxy properties.
\\ ( https://arxiv.org/abs/2402.03535 ,  18735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03561 (*cross-listing*)
Date: Mon, 5 Feb 2024 22:20:19 GMT   (9474kb,D)

Title: VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language
  Navigation
Authors: Jialu Li, Aishwarya Padmakumar, Gaurav Sukhatme, Mohit Bansal
Categories: cs.CV cs.AI cs.CL
Comments: AAAI 2024
\\
  Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate
through realistic 3D outdoor environments based on natural language
instructions. The performance of existing VLN methods is limited by
insufficient diversity in navigation environments and limited training data. To
address these issues, we propose VLN-Video, which utilizes the diverse outdoor
environments present in driving videos in multiple cities in the U.S. augmented
with automatically generated navigation instructions and actions to improve
outdoor VLN performance. VLN-Video combines the best of intuitive classical
approaches and modern deep learning techniques, using template infilling to
generate grounded navigation instructions, combined with an image rotation
similarity-based navigation action predictor to obtain VLN style data from
driving videos for pretraining deep learning VLN models. We pre-train the model
on the Touchdown dataset and our video-augmented dataset created from driving
videos with three proxy tasks: Masked Language Modeling, Instruction and
Trajectory Matching, and Next Action Prediction, so as to learn
temporally-aware and visually-aligned instruction representations. The learned
instruction representation is adapted to the state-of-the-art navigator when
fine-tuning on the Touchdown dataset. Empirical results demonstrate that
VLN-Video significantly outperforms previous state-of-the-art models by 2.1% in
task completion rate, achieving a new state-of-the-art on the Touchdown
dataset.
\\ ( https://arxiv.org/abs/2402.03561 ,  9474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03578 (*cross-listing*)
Date: Mon, 5 Feb 2024 23:06:42 GMT   (102kb,D)

Title: LLM Multi-Agent Systems: Challenges and Open Problems
Authors: Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu,
  Chaoyang He
Categories: cs.MA cs.AI
\\
  This paper explores existing works of multi-agent systems and identifies
challenges that remain inadequately addressed. By leveraging the diverse
capabilities and roles of individual agents within a multi-agent system, these
systems can tackle complex tasks through collaboration. We discuss optimizing
task allocation, fostering robust reasoning through iterative debates, managing
complex and layered context information, and enhancing memory management to
support the intricate interactions within multi-agent systems. We also explore
the potential application of multi-agent systems in blockchain systems to shed
light on their future development and application in real-world distributed
systems.
\\ ( https://arxiv.org/abs/2402.03578 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03583 (*cross-listing*)
Date: Mon, 5 Feb 2024 23:20:05 GMT   (360kb,D)

Title: MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models
Authors: Yang Liu, Huang Fang, Yunfeng Cai, Mingming Sun
Categories: cs.SI cs.AI cs.LG
Comments: 18pages, 1 figure
\\
  Knowledge graph embedding (KGE) models achieved state-of-the-art results on
many knowledge graph tasks including link prediction and information retrieval.
Despite the superior performance of KGE models in practice, we discover a
deficiency in the expressiveness of some popular existing KGE models called
\emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE
model called \emph{MQuinE} that does not suffer from Z-paradox while preserves
strong expressiveness to model various relation patterns including
symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with
theoretical justification. Experiments on real-world knowledge bases indicate
that Z-paradox indeed degrades the performance of existing KGE models, and can
cause more than 20\% accuracy drop on some challenging test samples. Our
experiments further demonstrate that MQuinE can mitigate the negative impact of
Z-paradox and outperform existing KGE models by a visible margin on link
prediction tasks.
\\ ( https://arxiv.org/abs/2402.03583 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03630 (*cross-listing*)
Date: Tue, 6 Feb 2024 01:59:41 GMT   (757kb,D)

Title: Enhancing LLM-Based Coding Tools through Native Integration of
  IDE-Derived Static Context
Authors: Yichen Li and Yun Peng and Yintong Huo and Michael R. Lyu
Categories: cs.SE cs.AI
\\
  Large Language Models (LLMs) have achieved remarkable success in code
completion, as evidenced by their essential roles in developing code assistant
services such as Copilot. Being trained on in-file contexts, current LLMs are
quite effective in completing code for single source files. However, it is
challenging for them to conduct repository-level code completion for large
software projects that require cross-file information. Existing research on
LLM-based repository-level code completion identifies and integrates cross-file
contexts, but it suffers from low accuracy and limited context length of LLMs.
In this paper, we argue that Integrated Development Environments (IDEs) can
provide direct, accurate and real-time cross-file information for
repository-level code completion. We propose IDECoder, a practical framework
that leverages IDE native static contexts for cross-context construction and
diagnosis results for self-refinement. IDECoder utilizes the rich cross-context
information available in IDEs to enhance the capabilities of LLMs of
repository-level code completion. We conducted preliminary experiments to
validate the performance of IDECoder and observed that this synergy represents
a promising trend for future exploration.
\\ ( https://arxiv.org/abs/2402.03630 ,  757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03675 (*cross-listing*)
Date: Tue, 6 Feb 2024 03:57:06 GMT   (27181kb,D)

Title: Effective Protein-Protein Interaction Exploration with PPIretrieval
Authors: Chenqing Hua, Connor Coley, Guy Wolf, Doina Precup, Shuangjia Zheng
Categories: q-bio.BM cs.AI cs.CE cs.LG
\\
  Protein-protein interactions (PPIs) are crucial in regulating numerous
cellular functions, including signal transduction, transportation, and immune
defense. As the accuracy of multi-chain protein complex structure prediction
improves, the challenge has shifted towards effectively navigating the vast
complex universe to identify potential PPIs. Herein, we propose PPIretrieval,
the first deep learning-based model for protein-protein interaction
exploration, which leverages existing PPI data to effectively search for
potential PPIs in an embedding space, capturing rich geometric and chemical
information of protein surfaces. When provided with an unseen query protein
with its associated binding site, PPIretrieval effectively identifies a
potential binding partner along with its corresponding binding site in an
embedding space, facilitating the formation of protein-protein complexes.
\\ ( https://arxiv.org/abs/2402.03675 ,  27181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03681 (*cross-listing*)
Date: Tue, 6 Feb 2024 04:06:06 GMT   (4569kb,D)

Title: RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model
  Feedback
Authors: Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David
  Held, Zackory Erickson
Categories: cs.RO cs.AI cs.LG
\\
  Reward engineering has long been a challenge in Reinforcement Learning (RL)
research, as it often requires extensive human effort and iterative processes
of trial-and-error to design effective reward functions. In this paper, we
propose RL-VLM-F, a method that automatically generates reward functions for
agents to learn new tasks, using only a text description of the task goal and
the agent's visual observations, by leveraging feedbacks from vision language
foundation models (VLMs). The key to our approach is to query these models to
give preferences over pairs of the agent's image observations based on the text
description of the task goal, and then learn a reward function from the
preference labels, rather than directly prompting these models to output a raw
reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F
successfully produces effective rewards and policies across various domains -
including classic control, as well as manipulation of rigid, articulated, and
deformable objects - without the need for human supervision, outperforming
prior methods that use large pretrained models for reward generation under the
same assumptions.
\\ ( https://arxiv.org/abs/2402.03681 ,  4569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03688 (*cross-listing*)
Date: Tue, 6 Feb 2024 04:22:44 GMT   (2901kb,D)

Title: A Survey of Privacy Threats and Defense in Vertical Federated Learning:
  From Model Life Cycle Perspective
Authors: Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang,
  Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, Stacy Patterson
Categories: cs.CR cs.AI cs.LG
\\
  Vertical Federated Learning (VFL) is a federated learning paradigm where
multiple participants, who share the same set of samples but hold different
features, jointly train machine learning models. Although VFL enables
collaborative machine learning without sharing raw data, it is still
susceptible to various privacy threats. In this paper, we conduct the first
comprehensive survey of the state-of-the-art in privacy attacks and defenses in
VFL. We provide taxonomies for both attacks and defenses, based on their
characterizations, and discuss open challenges and future research directions.
Specifically, our discussion is structured around the model's life cycle, by
delving into the privacy threats encountered during different stages of machine
learning and their corresponding countermeasures. This survey not only serves
as a resource for the research community but also offers clear guidance and
actionable insights for practitioners to safeguard data privacy throughout the
model's life cycle.
\\ ( https://arxiv.org/abs/2402.03688 ,  2901kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03694 (*cross-listing*)
Date: Tue, 6 Feb 2024 04:28:33 GMT   (2552kb,D)

Title: ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis
Authors: Shinan Liu, Ted Shaowang, Gerry Wan, Jeewon Chae, Jonatas Marques,
  Sanjay Krishnan, Nick Feamster
Categories: cs.NI cs.AI
\\
  Network traffic analysis increasingly uses complex machine learning models as
the internet consolidates and traffic gets more encrypted. However, over
high-bandwidth networks, flows can easily arrive faster than model inference
rates. The temporal nature of network flows limits simple scale-out approaches
leveraged in other high-traffic machine learning applications. Accordingly,
this paper presents ServeFlow, a solution for machine-learning model serving
aimed at network traffic analysis tasks, which carefully selects the number of
packets to collect and the models to apply for individual flows to achieve a
balance between minimal latency, high service rate, and high accuracy. We
identify that on the same task, inference time across models can differ by
2.7x-136.3x, while the median inter-packet waiting time is often 6-8 orders of
magnitude higher than the inference time! ServeFlow is able to make inferences
on 76.3% flows in under 16ms, which is a speed-up of 40.5x on the median
end-to-end serving latency while increasing the service rate and maintaining
similar accuracy. Even with thousands of features per flow, it achieves a
service rate of over 48.5k new flows per second on a 16-core CPU commodity
server, which matches the order of magnitude of flow rates observed on
city-level network backbones.
\\ ( https://arxiv.org/abs/2402.03694 ,  2552kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03700 (*cross-listing*)
Date: Tue, 6 Feb 2024 04:41:06 GMT   (11760kb,D)

Title: GenLens: A Systematic Evaluation of Visual GenAI Model Outputs
Authors: Tica Lin, Hanspeter Pfister, Jui-Hsien Wang
Categories: cs.HC cs.AI
Comments: To Appear in IEEE PacificVis 2024
\\
  The rapid development of generative AI (GenAI) models in computer vision
necessitates effective evaluation methods to ensure their quality and fairness.
Existing tools primarily focus on dataset quality assurance and model
explainability, leaving a significant gap in GenAI output evaluation during
model development. Current practices often depend on developers' subjective
visual assessments, which may lack scalability and generalizability. This paper
bridges this gap by conducting a formative study with GenAI model developers in
an industrial setting. Our findings led to the development of GenLens, a visual
analytic interface designed for the systematic evaluation of GenAI model
outputs during the early stages of model development. GenLens offers a
quantifiable approach for overviewing and annotating failure cases, customizing
issue tags and classifications, and aggregating annotations from multiple users
to enhance collaboration. A user study with model developers reveals that
GenLens effectively enhances their workflow, evidenced by high satisfaction
rates and a strong intent to integrate it into their practices. This research
underscores the importance of robust early-stage evaluation tools in GenAI
development, contributing to the advancement of fair and high-quality GenAI
models.
\\ ( https://arxiv.org/abs/2402.03700 ,  11760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03706 (*cross-listing*)
Date: Tue, 6 Feb 2024 04:57:07 GMT   (37816kb,D)

Title: MMAUD: A Comprehensive Multi-Modal Anti-UAV Dataset for Modern Miniature
  Drone Threats
Authors: Shenghai Yuan, Yizhuo Yang, Thien Hoang Nguyen, Thien-Minh Nguyen,
  Jianfei Yang, Fen Liu, Jianping Li, Han Wang, Lihua Xie
Categories: cs.RO cs.AI cs.CV
Comments: Accepted by ICRA 2024
\\
  In response to the evolving challenges posed by small unmanned aerial
vehicles (UAVs), which possess the potential to transport harmful payloads or
independently cause damage, we introduce MMAUD: a comprehensive Multi-Modal
Anti-UAV Dataset. MMAUD addresses a critical gap in contemporary threat
detection methodologies by focusing on drone detection, UAV-type
classification, and trajectory estimation. MMAUD stands out by combining
diverse sensory inputs, including stereo vision, various Lidars, Radars, and
audio arrays. It offers a unique overhead aerial detection vital for addressing
real-world scenarios with higher fidelity than datasets captured on specific
vantage points using thermal and RGB. Additionally, MMAUD provides accurate
Leica-generated ground truth data, enhancing credibility and enabling confident
refinement of algorithms and models, which has never been seen in other
datasets. Most existing works do not disclose their datasets, making MMAUD an
invaluable resource for developing accurate and efficient solutions. Our
proposed modalities are cost-effective and highly adaptable, allowing users to
experiment and implement new UAV threat detection tools. Our dataset closely
simulates real-world scenarios by incorporating ambient heavy machinery sounds.
This approach enhances the dataset's applicability, capturing the exact
challenges faced during proximate vehicular operations. It is expected that
MMAUD can play a pivotal role in advancing UAV threat detection,
classification, trajectory estimation capabilities, and beyond. Our dataset,
codes, and designs will be available in https://github.com/ntu-aris/MMAUD.
\\ ( https://arxiv.org/abs/2402.03706 ,  37816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03766 (*cross-listing*)
Date: Tue, 6 Feb 2024 07:16:36 GMT   (1218kb,D)

Title: MobileVLM V2: Faster and Stronger Baseline for Vision Language Model
Authors: Xiangxiang Chu and Limeng Qiao and Xinyu Zhang and Shuang Xu and Fei
  Wei and Yang Yang and Xiaofei Sun and Yiming Hu and Xinyang Lin and Bo Zhang
  and Chunhua Shen
Categories: cs.CV cs.AI
\\
  We introduce MobileVLM V2, a family of significantly improved vision language
models upon MobileVLM, which proves that a delicate orchestration of novel
architectural design, an improved training scheme tailored for mobile VLMs, and
rich high-quality dataset curation can substantially benefit VLMs' performance.
Specifically, MobileVLM V2 1.7B achieves better or on-par performance on
standard VLM benchmarks compared with much larger VLMs at the 3B scale.
Notably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our
models will be released at https://github.com/Meituan-AutoML/MobileVLM .
\\ ( https://arxiv.org/abs/2402.03766 ,  1218kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03781 (*cross-listing*)
Date: Tue, 6 Feb 2024 07:51:56 GMT   (7820kb,D)

Title: MolTC: Towards Molecular Relational Modeling In Language Models
Authors: Junfeng Fang, Shuai Zhang, Chang Wu, Zhiyuan Liu, Sihang Li, Kun Wang,
  Wenjie Du, Xiang Wang, Xiangnan He
Categories: q-bio.QM cs.AI cs.LG
\\
  Molecular Relational Learning (MRL), aiming to understand interactions
between molecular pairs, plays a pivotal role in advancing biochemical
research. Recently, the adoption of large language models (LLMs), known for
their vast knowledge repositories and advanced logical inference capabilities,
has emerged as a promising way for efficient and effective MRL. Despite their
potential, these methods predominantly rely on the textual data, thus not fully
harnessing the wealth of structural information inherent in molecular graphs.
Moreover, the absence of a unified framework exacerbates the information
underutilization, as it hinders the sharing of interaction rationale learned
across diverse datasets. To address these challenges, this work proposes a
novel LLM-based multi-modal framework for Molecular inTeraction prediction
following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently
integrate rich graphical information of molecular pairs. For achieving a
unified MRL, MolTC innovatively develops a dynamic parameter-sharing strategy
for cross-dataset information exchange, and introduces a Multi-hierarchical CoT
principle to refine training paradigm. Our experiments, conducted across twelve
varied datasets involving over 4,000,000 molecular pairs, demonstrate the
superiority of our method over current GNN and LLM-based baselines. On the top
of that, a comprehensive Molecular Interactive Instructions dataset is
constructed for the development of biochemical LLM, including our MolTC. Code
is available at https://github.com/MangoKiller/MolTC.
\\ ( https://arxiv.org/abs/2402.03781 ,  7820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03796 (*cross-listing*)
Date: Tue, 6 Feb 2024 08:29:39 GMT   (113kb,D)

Title: Face Detection: Present State and Research Directions
Authors: Purnendu Prabhat, Himanshu Gupta and Ajeet Kumar Vishwakarma
Categories: cs.CV cs.AI cs.LG
\\
  The majority of computer vision applications that handle images featuring
humans use face detection as a core component. Face detection still has issues,
despite much research on the topic. Face detection's accuracy and speed might
yet be increased. This review paper shows the progress made in this area as
well as the substantial issues that still need to be tackled. The paper
provides research directions that can be taken up as research projects in the
field of face detection.
\\ ( https://arxiv.org/abs/2402.03796 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03843 (*cross-listing*)
Date: Tue, 6 Feb 2024 09:39:05 GMT   (3574kb,D)

Title: A new method for optical steel rope non-destructive damage detection
Authors: Yunqing Bao, Bin Hu
Categories: cs.CV cs.AI
\\
  This paper presents a novel algorithm for non-destructive damage detection
for steel ropes in high-altitude environments (aerial ropeway). The algorithm
comprises two key components: First, a segmentation model named RGBD-UNet is
designed to accurately extract steel ropes from complex backgrounds. This model
is equipped with the capability to process and combine color and depth
information through the proposed CMA module. Second, a detection model named
VovNetV3.5 is developed to differentiate between normal and abnormal steel
ropes. It integrates the VovNet architecture with a DBB module to enhance
performance. Besides, a novel background augmentation method is proposed to
enhance the generalization ability of the segmentation model. Datasets
containing images of steel ropes in different scenarios are created for the
training and testing of both the segmentation and detection models. Experiments
demonstrate a significant improvement over baseline models. On the proposed
dataset, the highest accuracy achieved by the detection model reached 0.975,
and the maximum F-measure achieved by the segmentation model reached 0.948.
\\ ( https://arxiv.org/abs/2402.03843 ,  3574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03907 (*cross-listing*)
Date: Tue, 6 Feb 2024 11:19:40 GMT   (163kb,D)

Title: Embedding Large Language Models into Extended Reality: Opportunities and
  Challenges for Inclusion, Engagement, and Privacy
Authors: Efe Bozkir and S\"uleyman \"Ozdel and Ka Hei Carrie Lau and Mengdi
  Wang and Hong Gao and Enkelejda Kasneci
Categories: cs.HC cs.AI
\\
  Recent developments in computer graphics, hardware, artificial intelligence
(AI), and human-computer interaction likely lead to extended reality (XR)
devices and setups being more pervasive. While these devices and setups provide
users with interactive, engaging, and immersive experiences with different
sensing modalities, such as eye and hand trackers, many non-player characters
are utilized in a pre-scripted way or by conventional AI techniques. In this
paper, we argue for using large language models (LLMs) in XR by embedding them
in virtual avatars or as narratives to facilitate more inclusive experiences
through prompt engineering according to user profiles and fine-tuning the LLMs
for particular purposes. We argue that such inclusion will facilitate diversity
for XR use. In addition, we believe that with the versatile conversational
capabilities of LLMs, users will engage more with XR environments, which might
help XR be more used in everyday life. Lastly, we speculate that combining the
information provided to LLM-powered environments by the users and the biometric
data obtained through the sensors might lead to novel privacy invasions. While
studying such possible privacy invasions, user privacy concerns and preferences
should also be investigated. In summary, despite some challenges, embedding
LLMs into XR is a promising and novel research area with several opportunities.
\\ ( https://arxiv.org/abs/2402.03907 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03948 (*cross-listing*)
Date: Mon, 29 Jan 2024 12:11:30 GMT   (4385kb,D)

Title: Identifying Student Profiles Within Online Judge Systems Using
  Explainable Artificial Intelligence
Authors: Juan Ram\'on Rico-Juan, V\'ictor M. S\'anchez-Cartagena, Jose J.
  Valero-Mas, Antonio Javier Gallego
Categories: cs.CY cs.AI
Journal-ref: IEEE Transactions on Learning Technologies ( Volume: 16, Issue: 6,
  December 2023)
DOI: 10.1109/TLT.2023.3239110
\\
  Online Judge (OJ) systems are typically considered within programming-related
courses as they yield fast and objective assessments of the code developed by
the students. Such an evaluation generally provides a single decision based on
a rubric, most commonly whether the submission successfully accomplished the
assignment. Nevertheless, since in an educational context such information may
be deemed insufficient, it would be beneficial for both the student and the
instructor to receive additional feedback about the overall development of the
task. This work aims to tackle this limitation by considering the further
exploitation of the information gathered by the OJ and automatically inferring
feedback for both the student and the instructor. More precisely, we consider
the use of learning-based schemes -- particularly, multi-instance learning
(MIL) and classical machine learning formulations -- to model student behavior.
Besides, explainable artificial intelligence (XAI) is contemplated to provide
human-understandable feedback. The proposal has been evaluated considering a
case of study comprising 2500 submissions from roughly 90 different students
from a programming-related course in a computer science degree. The results
obtained validate the proposal: The model is capable of significantly
predicting the user outcome (either passing or failing the assignment) solely
based on the behavioral pattern inferred by the submissions provided to the OJ.
Moreover, the proposal is able to identify prone-to-fail student groups and
profiles as well as other relevant information, which eventually serves as
feedback to both the student and the instructor.
\\ ( https://arxiv.org/abs/2402.03948 ,  4385kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03951 (*cross-listing*)
Date: Tue, 6 Feb 2024 12:23:14 GMT   (1502kb,D)

Title: Boosting Adversarial Transferability across Model Genus by
  Deformation-Constrained Warping
Authors: Qinliang Lin, Cheng Luo, Zenghao Niu, Xilin He, Weicheng Xie, Yuanbo
  Hou, Linlin Shen, Siyang Song
Categories: cs.CV cs.AI
Comments: AAAI 2024
\\
  Adversarial examples generated by a surrogate model typically exhibit limited
transferability to unknown target systems. To address this problem, many
transferability enhancement approaches (e.g., input transformation and model
augmentation) have been proposed. However, they show poor performances in
attacking systems having different model genera from the surrogate model. In
this paper, we propose a novel and generic attacking strategy, called
Deformation-Constrained Warping Attack (DeCoWA), that can be effectively
applied to cross model genus attack. Specifically, DeCoWA firstly augments
input examples via an elastic deformation, namely Deformation-Constrained
Warping (DeCoW), to obtain rich local details of the augmented input. To avoid
severe distortion of global semantics led by random deformation, DeCoW further
constrains the strength and direction of the warping transformation by a novel
adaptive control strategy. Extensive experiments demonstrate that the
transferable examples crafted by our DeCoWA on CNN surrogates can significantly
hinder the performance of Transformers (and vice versa) on various tasks,
including image classification, video action recognition, and audio
recognition. Code is made available at https://github.com/LinQinLiang/DeCoWA.
\\ ( https://arxiv.org/abs/2402.03951 ,  1502kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03972 (*cross-listing*)
Date: Tue, 6 Feb 2024 13:02:00 GMT   (3577kb,D)

Title: Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent
  Deep Reinforcement Learning
Authors: Maxime Toquebiau, Nicolas Bredeche, Fa\"iz Benamar, Jae-Yun Jun
Categories: cs.MA cs.AI
Comments: 13 pages, 13 figures. Published as an extended abstract at AAMAS 2024
\\
  Multi-agent deep reinforcement learning (MADRL) problems often encounter the
challenge of sparse rewards. This challenge becomes even more pronounced when
coordination among agents is necessary. As performance depends not only on one
agent's behavior but rather on the joint behavior of multiple agents, finding
an adequate solution becomes significantly harder. In this context, a group of
agents can benefit from actively exploring different joint strategies in order
to determine the most efficient one. In this paper, we propose an approach for
rewarding strategies where agents collectively exhibit novel behaviors. We
present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation
method that follows the centralized learning with decentralized execution
paradigm. JIM rewards joint trajectories based on a centralized measure of
novelty designed to function in continuous environments. We demonstrate the
strengths of this approach both in a synthetic environment designed to reveal
shortcomings of state-of-the-art MADRL methods, and in simulated robotic tasks.
Results show that joint exploration is crucial for solving tasks where the
optimal strategy requires a high level of coordination.
\\ ( https://arxiv.org/abs/2402.03972 ,  3577kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04009 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:03:15 GMT   (2095kb,D)

Title: Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning
Authors: Ningyuan Tang, Minghao Fu, Ke Zhu, Jianxin Wu
Categories: cs.CV cs.AI
\\
  In finetuning a large pretrained model to downstream tasks,
parameter-efficient fine-tuning (PEFT) methods can effectively finetune
pretrained models with few trainable parameters, but suffer from high GPU
memory consumption and slow training speed. Because learnable parameters from
these methods are entangled with the pretrained model, gradients related to the
frozen pretrained model's parameters have to be computed and stored during
finetuning. We propose Low-rank Attention Side-Tuning (LAST), which
disentangles the trainable module from the pretrained model by freezing not
only parameters but also outputs of the pretrained network. LAST trains a
side-network composed of only low-rank self-attention modules. By viewing the
pretrained model as a frozen feature extractor, the side-network takes
intermediate output from the pretrained model and focus on learning
task-specific knowledge. We also show that LAST can be highly parallel across
multiple optimization objectives, making it very efficient in downstream task
adaptation, for example, in finding optimal hyperparameters. LAST outperforms
previous state-of-the-art methods on VTAB-1K and other visual adaptation tasks
with roughly only 30\% of GPU memory footprint and 60\% of training time
compared to existing PEFT methods, but achieves significantly higher accuracy.
\\ ( https://arxiv.org/abs/2402.04009 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04032 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:26:22 GMT   (1025kb,D)

Title: HEAM : Hashed Embedding Acceleration using Processing-In-Memory
Authors: Youngsuk Kim, Hyuk-Jae Lee, Chae Eun Rhee
Categories: cs.AR cs.AI
Comments: 10 pages, 12 figures
\\
  In today's data centers, personalized recommendation systems face challenges
such as the need for large memory capacity and high bandwidth, especially when
performing embedding operations. Previous approaches have relied on DIMM-based
near-memory processing techniques or introduced 3D-stacked DRAM to address
memory-bound issues and expand memory bandwidth. However, these solutions fall
short when dealing with the expanding size of personalized recommendation
systems. Recommendation models have grown to sizes exceeding tens of terabytes,
making them challenging to run efficiently on traditional single-node inference
servers. Although various algorithmic methods have been proposed to reduce
embedding table capacity, they often result in increased memory access or
inefficient utilization of memory resources. This paper introduces HEAM, a
heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to
accelerate recommendation systems in which compositional embedding is
utilized-a technique aimed at reducing the size of embedding tables. The
architecture is organized into a three-tier memory hierarchy consisting of
conventional DIMM, 3D-stacked DRAM with a base die-level Processing-In-Memory
(PIM), and a bank group-level PIM incorporating a Look-Up-Table. This setup is
specifically designed to accommodate the unique aspects of compositional
embedding, such as temporal locality and embedding table capacity. This design
effectively reduces bank access, improves access efficiency, and enhances
overall throughput, resulting in a 6.3 times speedup and 58.9% energy savings
compared to the baseline.
\\ ( https://arxiv.org/abs/2402.04032 ,  1025kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04046 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:48:34 GMT   (2094kb,D)

Title: Generative Modeling of Graphs via Joint Diffusion of Node and Edge
  Attributes
Authors: Nimrod Berman, Eitan Kosman, Dotan Di Castro, Omri Azencot
Categories: cs.SI cs.AI cs.LG
\\
  Graph generation is integral to various engineering and scientific
disciplines. Nevertheless, existing methodologies tend to overlook the
generation of edge attributes. However, we identify critical applications where
edge attributes are essential, making prior methods potentially unsuitable in
such contexts. Moreover, while trivial adaptations are available, empirical
investigations reveal their limited efficacy as they do not properly model the
interplay among graph components. To address this, we propose a joint
score-based model of nodes and edges for graph generation that considers all
graph components. Our approach offers two key novelties: (i) node and edge
attributes are combined in an attention module that generates samples based on
the two ingredients; and (ii) node, edge and adjacency information are mutually
dependent during the graph diffusion process. We evaluate our method on
challenging benchmarks involving real-world and synthetic datasets in which
edge features are crucial. Additionally, we introduce a new synthetic dataset
that incorporates edge values. Furthermore, we propose a novel application that
greatly benefits from the method due to its nature: the generation of traffic
scenes represented as graphs. Our method outperforms other graph generation
methods, demonstrating a significant advantage in edge-related measures.
\\ ( https://arxiv.org/abs/2402.04046 ,  2094kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04064 (*cross-listing*)
Date: Tue, 6 Feb 2024 15:09:50 GMT   (705kb,D)

Title: Multi-class Road Defect Detection and Segmentation using Spatial and
  Channel-wise Attention for Autonomous Road Repairing
Authors: Jongmin Yu, Chen Bene Chi, Sebastiano Fichera, Paolo Paoletti, Devansh
  Mehta, and Shan Luo
Categories: cs.CV cs.AI
Comments: Accepted to the ICRA 2024
\\
  Road pavement detection and segmentation are critical for developing
autonomous road repair systems. However, developing an instance segmentation
method that simultaneously performs multi-class defect detection and
segmentation is challenging due to the textural simplicity of road pavement
image, the diversity of defect geometries, and the morphological ambiguity
between classes. We propose a novel end-to-end method for multi-class road
defect detection and segmentation. The proposed method comprises multiple
spatial and channel-wise attention blocks available to learn global
representations across spatial and channel-wise dimensions. Through these
attention blocks, more globally generalised representations of morphological
information (spatial characteristics) of road defects and colour and depth
information of images can be learned. To demonstrate the effectiveness of our
framework, we conducted various ablation studies and comparisons with prior
methods on a newly collected dataset annotated with nine road defect classes.
The experiments show that our proposed method outperforms existing
state-of-the-art methods for multi-class road defect detection and segmentation
methods.
\\ ( https://arxiv.org/abs/2402.04064 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04087 (*cross-listing*)
Date: Tue, 6 Feb 2024 15:45:27 GMT   (228kb,D)

Title: A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation
Authors: Zhengbo Wang, Jian Liang, Lijun Sheng, Ran He, Zilei Wang, Tieniu Tan
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by ICLR 2024
\\
  Contrastive Language-Image Pretraining (CLIP) has gained popularity for its
remarkable zero-shot capacity. Recent research has focused on developing
efficient fine-tuning methods, such as prompt learning and adapter, to enhance
CLIP's performance in downstream tasks. However, these methods still require
additional training time and computational resources, which is undesirable for
devices with limited resources. In this paper, we revisit a classical
algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream
classification of CLIP. Typically, GDA assumes that features of each class
follow Gaussian distributions with identical covariance. By leveraging Bayes'
formula, the classifier can be expressed in terms of the class means and
covariance, which can be estimated from the data without the need for training.
To integrate knowledge from both visual and textual modalities, we ensemble it
with the original zero-shot classifier within CLIP. Extensive results on 17
datasets validate that our method surpasses or achieves comparable results with
state-of-the-art methods on few-shot classification, imbalanced learning, and
out-of-distribution generalization. In addition, we extend our method to
base-to-new generalization and unsupervised learning, once again demonstrating
its superiority over competing approaches. Our code is publicly available at
\url{https://github.com/mrflogs/ICLR24}.
\\ ( https://arxiv.org/abs/2402.04087 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04102 (*cross-listing*)
Date: Tue, 6 Feb 2024 15:57:08 GMT   (1175kb,D)

Title: Use of Multi-CNNs for Section Analysis in Static Malware Detection
Authors: Tony Quertier, Gr\'egoire Barru\'e
Categories: cs.CR cs.AI
Comments: arXiv admin note: text overlap with arXiv:2312.12161
\\
  Existing research on malware detection focuses almost exclusively on the
detection rate. However, in some cases, it is also important to understand the
results of our algorithm, or to obtain more information, such as where to
investigate in the file for an analyst. In this aim, we propose a new model to
analyze Portable Executable files. Our method consists in splitting the files
in different sections, then transform each section into an image, in order to
train convolutional neural networks to treat specifically each identified
section. Then we use all these scores returned by CNNs to compute a final
detection score, using models that enable us to improve our analysis of the
importance of each section in the final score.
\\ ( https://arxiv.org/abs/2402.04102 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04141 (*cross-listing*)
Date: Tue, 6 Feb 2024 16:48:50 GMT   (5156kb,D)

Title: Multi-line AI-assisted Code Authoring
Authors: Omer Dunay and Daniel Cheng and Adam Tait and Parth Thakkar and Peter
  C Rigby and Andy Chiu and Imad Ahmad and Arun Ganesan and Chandra Maddila and
  Vijayaraghavan Murali and Ali Tayyebi and Nachiappan Nagappan
Categories: cs.SE cs.AI
\\
  CodeCompose is an AI-assisted code authoring tool powered by large language
models (LLMs) that provides inline suggestions to 10's of thousands of
developers at Meta. In this paper, we present how we scaled the product from
displaying single-line suggestions to multi-line suggestions. This evolution
required us to overcome several unique challenges in improving the usability of
these suggestions for developers.
  First, we discuss how multi-line suggestions can have a 'jarring' effect, as
the LLM's suggestions constantly move around the developer's existing code,
which would otherwise result in decreased productivity and satisfaction.
  Second, multi-line suggestions take significantly longer to generate; hence
we present several innovative investments we made to reduce the perceived
latency for users. These model-hosting optimizations sped up multi-line
suggestion latency by 2.5x.
  Finally, we conduct experiments on 10's of thousands of engineers to
understand how multi-line suggestions impact the user experience and contrast
this with single-line suggestions. Our experiments reveal that (i) multi-line
suggestions account for 42% of total characters accepted (despite only
accounting for 16% for displayed suggestions) (ii) multi-line suggestions
almost doubled the percentage of keystrokes saved for users from 9% to 17%.
Multi-line CodeCompose has been rolled out to all engineers at Meta, and less
than 1% of engineers have opted out of multi-line suggestions.
\\ ( https://arxiv.org/abs/2402.04141 ,  5156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04173 (*cross-listing*)
Date: Tue, 6 Feb 2024 17:27:12 GMT   (8059kb,D)

Title: COPS: A Compact On-device Pipeline for real-time Smishing detection
Authors: Harichandana B S S, Sumit Kumar, Manjunath Bhimappa Ujjinakoppa,
  Barath Raj Kandur Raja
Categories: cs.CR cs.AI
Comments: Published at IEEE Consumer Communications & Networking Conference
  (CCNC) 2024
\\
  Smartphones have become indispensable in our daily lives and can do almost
everything, from communication to online shopping. However, with the increased
usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in
particular, have observed a significant upsurge in recent years. This problem
is further exacerbated by the perpetrator creating new deceptive websites
daily, with an average life cycle of under 15 hours. This renders the standard
practice of keeping a database of malicious URLs ineffective. To this end, we
propose a novel on-device pipeline: COPS that intelligently identifies features
of fraudulent messages and URLs to alert the user in real-time. COPS is a
lightweight pipeline with a detection module based on the Disentangled
Variational Autoencoder of size 3.46MB for smishing and URL phishing detection,
and we benchmark it on open datasets. We achieve an accuracy of 98.15% and
99.5%, respectively, for both tasks, with a false negative and false positive
rate of a mere 0.037 and 0.015, outperforming previous works with the added
advantage of ensuring real-time alerts on resource-constrained devices.
\\ ( https://arxiv.org/abs/2402.04173 ,  8059kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04228 (*cross-listing*)
Date: Tue, 6 Feb 2024 18:36:44 GMT   (3591kb,D)

Title: Intelligent Collective Escape of Swarm Robots Based on a Novel
  Fish-inspired Self-adaptive Approach with Neurodynamic Models
Authors: Junfei Li, Simon X. Yang
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: This article is accepted for publication in a future issue of IEEE
  Transactions on Industrial Electronics
DOI: 10.1109/TIE.2024.3363723
\\
  Fish schools present high-efficiency group behaviors through simple
individual interactions to collective migration and dynamic escape from the
predator. The school behavior of fish is usually a good inspiration to design
control architecture for swarm robots. In this paper, a novel fish-inspired
self-adaptive approach is proposed for collective escape for the swarm robots.
In addition, a bio-inspired neural network (BINN) is introduced to generate
collision-free escape robot trajectories through the combination of attractive
and repulsive forces. Furthermore, to cope with dynamic environments, a
neurodynamics-based self-adaptive mechanism is proposed to improve the
self-adaptive performance of the swarm robots in the changing environment.
Similar to fish escape maneuvers, simulation and experimental results show that
the swarm robots are capable of collectively leaving away from the threats.
Several comparison studies demonstrated that the proposed approach can
significantly improve the effectiveness and efficiency of system performance,
and the flexibility and robustness in complex environments.
\\ ( https://arxiv.org/abs/2402.04228 ,  3591kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04247 (*cross-listing*)
Date: Tue, 6 Feb 2024 18:54:07 GMT   (12836kb,D)

Title: Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
Authors: Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang,
  Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman
  Cohan, Zhiyong Lu, Mark Gerstein
Categories: cs.CY cs.AI cs.CL cs.LG
\\
  Intelligent agents powered by large language models (LLMs) have demonstrated
substantial promise in autonomously conducting experiments and facilitating
scientific discoveries across various disciplines. While their capabilities are
promising, they also introduce novel vulnerabilities that demand careful
consideration for safety. However, there exists a notable gap in the
literature, as there has been no comprehensive exploration of these
vulnerabilities. This position paper fills this gap by conducting a thorough
examination of vulnerabilities in LLM-based agents within scientific domains,
shedding light on potential risks associated with their misuse and emphasizing
the need for safety measures. We begin by providing a comprehensive overview of
the potential risks inherent to scientific LLM agents, taking into account user
intent, the specific scientific domain, and their potential impact on the
external environment. Then, we delve into the origins of these vulnerabilities
and provide a scoping review of the limited existing works. Based on our
analysis, we propose a triadic framework involving human regulation, agent
alignment, and an understanding of environmental feedback (agent regulation) to
mitigate these identified risks. Furthermore, we highlight the limitations and
challenges associated with safeguarding scientific agents and advocate for the
development of improved models, robust benchmarks, and comprehensive
regulations to address these issues effectively.
\\ ( https://arxiv.org/abs/2402.04247 ,  12836kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03369 (*cross-listing*)
Date: Fri, 2 Feb 2024 03:13:09 GMT   (468kb)

Title: Evaluation of Google's Voice Recognition and Sentence Classification for
  Health Care Applications
Authors: Majbah Uddin, Nathan Huynh, Jose M Vidal, Kevin M Taaffe, Lawrence D
  Fredendall, and Joel S Greenstein
Categories: eess.AS cs.CL cs.LG cs.SD
Journal-ref: Engineering Management Journal, 27:3, 152-162, 2015
DOI: 10.1080/10429247.2015.1054752
\\
  This study examined the use of voice recognition technology in perioperative
services (Periop) to enable Periop staff to record workflow milestones using
mobile technology. The use of mobile technology to improve patient flow and
quality of care could be facilitated if such voice recognition technology could
be made robust. The goal of this experiment was to allow the Periop staff to
provide care without being interrupted with data entry and querying tasks.
However, the results are generalizable to other situations where an engineering
manager attempts to improve communication performance using mobile technology.
This study enhanced Google's voice recognition capability by using
post-processing classifiers (i.e., bag-of-sentences, support vector machine,
and maximum entropy). The experiments investigated three factors (original
phrasing, reduced phrasing, and personalized phrasing) at three levels (zero
training repetition, 5 training repetitions, and 10 training repetitions).
Results indicated that personal phrasing yielded the highest correctness and
that training the device to recognize an individual's voice improved
correctness as well. Although simplistic, the bag-of-sentences classifier
significantly improved voice recognition correctness. The classification
efficiency of the maximum entropy and support vector machine algorithms was
found to be nearly identical. These results suggest that engineering managers
could significantly enhance Google's voice recognition technology by using
post-processing techniques, which would facilitate its use in health care and
other applications.
\\ ( https://arxiv.org/abs/2402.03369 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03407 (*cross-listing*)
Date: Mon, 5 Feb 2024 15:08:19 GMT   (86kb,D)

Title: Enhancing the Stability of LLM-based Speech Generation Systems through
  Self-Supervised Representations
Authors: \'Alvaro Mart\'in-Cortinas, Daniel S\'aez-Trigueros, Iv\'an
  Vall\'es-P\'erez, Biel Tura-Vecino, Piotr Bili\'nski, Mateusz Lajszczak,
  Grzegorz Beringer, Roberto Barra-Chicote, Jaime Lorenzo-Trueba
Categories: eess.AS cs.CL cs.LG
Comments: 10 pages, 1 figure, 3 tables
\\
  Large Language Models (LLMs) are one of the most promising technologies for
the next era of speech generation systems, due to their scalability and
in-context learning capabilities. Nevertheless, they suffer from multiple
stability issues at inference time, such as hallucinations, content skipping or
speech repetitions. In this work, we introduce a new self-supervised Voice
Conversion (VC) architecture which can be used to learn to encode transitory
features, such as content, separately from stationary ones, such as speaker ID
or recording conditions, creating speaker-disentangled representations. Using
speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the
LLM to generate the content and the style of the speech only from the text,
similarly to humans, while the speaker identity is provided by the decoder of
the VC model. Results show that LLMs trained over speaker-disentangled
self-supervised representations provide an improvement of 4.7pp in speaker
similarity over SOTA entangled representations, and a word error rate (WER)
5.4pp lower. Furthermore, they achieve higher naturalness than human recordings
of the LibriTTS test-other dataset. Finally, we show that using explicit
reference embedding negatively impacts intelligibility (stability), with WER
increasing by 14pp compared to the model that only uses text to infer the
style.
\\ ( https://arxiv.org/abs/2402.03407 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03484 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:56:27 GMT   (863kb,D)

Title: Harnessing PubMed User Query Logs for Post Hoc Explanations of
  Recommended Similar Articles
Authors: Ashley Shin, Qiao Jin, James Anibal, Zhiyong Lu
Categories: cs.IR cs.CL
\\
  Searching for a related article based on a reference article is an integral
part of scientific research. PubMed, like many academic search engines, has a
"similar articles" feature that recommends articles relevant to the current
article viewed by a user. Explaining recommended items can be of great utility
to users, particularly in the literature search process. With more than a
million biomedical papers being published each year, explaining the recommended
similar articles would facilitate researchers and clinicians in searching for
related articles. Nonetheless, the majority of current literature
recommendation systems lack explanations for their suggestions. We employ a
post hoc approach to explaining recommendations by identifying relevant tokens
in the titles of similar articles. Our major contribution is building PubCLogs
by repurposing 5.6 million pairs of coclicked articles from PubMed's user query
logs. Using our PubCLogs dataset, we train the Highlight Similar Article Title
(HSAT), a transformer-based model designed to select the most relevant parts of
the title of a similar article, based on the title and abstract of a seed
article. HSAT demonstrates strong performance in our empirical evaluations,
achieving an F1 score of 91.72 percent on the PubCLogs test set, considerably
outperforming several baselines including BM25 (70.62), MPNet (67.11), MedCPT
(62.22), GPT-3.5 (46.00), and GPT-4 (64.89). Additional evaluations on a
separate, manually annotated test set further verifies HSAT's performance.
Moreover, participants of our user study indicate a preference for HSAT, due to
its superior balance between conciseness and comprehensiveness. Our study
suggests that repurposing user query logs of academic search engines can be a
promising way to train state-of-the-art models for explaining literature
recommendation.
\\ ( https://arxiv.org/abs/2402.03484 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03485 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:56:56 GMT   (1660kb,D)

Title: Attention Meets Post-hoc Interpretability: A Mathematical Perspective
Authors: Gianluigi Lopardo, Frederic Precioso, Damien Garreau
Categories: stat.ML cs.CL cs.LG
\\
  Attention-based architectures, in particular transformers, are at the heart
of a technological revolution. Interestingly, in addition to helping obtain
state-of-the-art results on a wide range of applications, the attention
mechanism intrinsically provides meaningful insights on the internal behavior
of the model. Can these insights be used as explanations? Debate rages on. In
this paper, we mathematically study a simple attention-based architecture and
pinpoint the differences between post-hoc and attention-based explanations. We
show that they provide quite different results, and that, despite their
limitations, post-hoc methods are capable of capturing more useful insights
than merely examining the attention weights.
\\ ( https://arxiv.org/abs/2402.03485 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03710 (*cross-listing*)
Date: Tue, 6 Feb 2024 05:05:38 GMT   (32413kb,D)

Title: Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced
  Auditory Experience
Authors: Xilin Jiang, Cong Han, Yinghao Aaron Li, and Nima Mesgarani
Categories: eess.AS cs.CL cs.SD
Comments: preprint
\\
  In daily life, we encounter a variety of sounds, both desirable and
undesirable, with limited control over their presence and volume. Our work
introduces "Listen, Chat, and Edit" (LCE), a novel multimodal sound mixture
editor that modifies each sound source in a mixture based on user-provided text
instructions. LCE distinguishes itself with a user-friendly chat interface and
its unique ability to edit multiple sound sources simultaneously within a
mixture, without needing to separate them. Users input open-vocabulary text
prompts, which are interpreted by a large language model to create a semantic
filter for editing the sound mixture. The system then decomposes the mixture
into its components, applies the semantic filter, and reassembles it into the
desired output. We developed a 160-hour dataset with over 100k mixtures,
including speech and various audio sources, along with text prompts for diverse
editing tasks like extraction, removal, and volume control. Our experiments
demonstrate significant improvements in signal quality across all editing tasks
and robust performance in zero-shot scenarios with varying numbers and types of
sound sources.
\\ ( https://arxiv.org/abs/2402.03710 ,  32413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03757 (*cross-listing*)
Date: Tue, 6 Feb 2024 06:48:46 GMT   (1146kb,D)

Title: The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
Authors: Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe
  Diao, Yong Lin, Tong Zhang
Categories: cs.CV cs.CL cs.LG
\\
  Large language models (LLMs) have recently experienced remarkable progress,
where the advent of multi-modal large language models (MLLMs) has endowed LLMs
with visual capabilities, leading to impressive performances in various
multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail
spectacularly when presented with certain image and text inputs. In this paper,
we identify a typical class of inputs that baffles MLLMs, which consist of
images that are highly relevant but inconsistent with answers, causing MLLMs to
suffer from hallucination. To quantify the effect, we propose CorrelationQA,
the first benchmark that assesses the hallucination level given spurious
images. This benchmark contains 7,308 text-image pairs across 13 categories.
Based on the proposed CorrelationQA, we conduct a thorough analysis on 9
mainstream MLLMs, illustrating that they universally suffer from this
instinctive bias to varying degrees. We hope that our curated benchmark and
evaluation results aid in better assessments of the MLLMs' robustness in the
presence of misleading images. The resource is available in
https://github.com/MasaiahHan/CorrelationQA.
\\ ( https://arxiv.org/abs/2402.03757 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03916 (*cross-listing*)
Date: Tue, 6 Feb 2024 11:33:57 GMT   (364kb,D)

Title: Can Large Language Models Detect Rumors on Social Media?
Authors: Qiang Liu, Xiang Tao, Junfei Wu, Shu Wu, Liang Wang
Categories: cs.IR cs.CL
\\
  In this work, we investigate to use Large Language Models (LLMs) for rumor
detection on social media. However, it is challenging for LLMs to reason over
the entire propagation information on social media, which contains news
contents and numerous comments, due to LLMs may not concentrate on key clues in
the complex propagation information, and have trouble in reasoning when facing
massive and redundant information. Accordingly, we propose an LLM-empowered
Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to
reason over important clues in news and comments, and divide the entire
propagation information into a Chain-of-Propagation for reducing LLMs' burden.
We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD
outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%.
Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus
shows more promising rumor detection ability in few-shot or zero-shot
scenarios.
\\ ( https://arxiv.org/abs/2402.03916 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03988 (*cross-listing*)
Date: Tue, 6 Feb 2024 13:26:19 GMT   (1181kb,D)

Title: REBORN: Reinforcement-Learned Boundary Segmentation with Iterative
  Training for Unsupervised ASR
Authors: Liang-Hsuan Tseng, En-Pei Hu, Cheng-Han Chiang, Yuan Tseng, Hung-yi
  Lee, Lin-shan Lee, Shao-Hua Sun
Categories: eess.AS cs.CL cs.SD
\\
  Unsupervised automatic speech recognition (ASR) aims to learn the mapping
between the speech signal and its corresponding textual transcription without
the supervision of paired speech-text data. A word/phoneme in the speech signal
is represented by a segment of speech signal with variable length and unknown
boundary, and this segmental structure makes learning the mapping between
speech and text challenging, especially without paired data. In this paper, we
propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative
Training for Unsupervised ASR. REBORN alternates between (1) training a
segmentation model that predicts the boundaries of the segmental structures in
speech signals and (2) training the phoneme prediction model, whose input is a
segmental structure segmented by the segmentation model, to predict a phoneme
transcription. Since supervised data for training the segmentation model is not
available, we use reinforcement learning to train the segmentation model to
favor segmentations that yield phoneme sequence predictions with a lower
perplexity. We conduct extensive experiments and find that under the same
setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech,
TIMIT, and five non-English languages in Multilingual LibriSpeech. We
comprehensively analyze why the boundaries learned by REBORN improve the
unsupervised ASR performance.
\\ ( https://arxiv.org/abs/2402.03988 ,  1181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04105 (*cross-listing*)
Date: Tue, 6 Feb 2024 15:59:23 GMT   (204kb,D)

Title: Measuring Implicit Bias in Explicitly Unbiased Large Language Models
Authors: Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, Thomas L. Griffiths
Categories: cs.CY cs.CL
\\
  Large language models (LLMs) can pass explicit bias tests but still harbor
implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit
subtle biases. Measuring such implicit biases can be a challenge: as LLMs
become increasingly proprietary, it may not be possible to access their
embeddings and apply existing bias measures; furthermore, implicit biases are
primarily a concern if they affect the actual decisions that these systems
make. We address both of these challenges by introducing two measures of bias
inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a
prompt-based method for revealing implicit bias; and LLM Decision Bias for
detecting subtle discrimination in decision-making tasks. Using these measures,
we found pervasive human-like stereotype biases in 6 LLMs across 4 social
domains (race, gender, religion, health) and 21 categories (weapons, guilt,
science, career among others). Our prompt-based measure of implicit bias
correlates with embedding-based methods but better predicts downstream
behaviors measured by LLM Decision Bias. This measure is based on asking the
LLM to decide between individuals, motivated by psychological results
indicating that relative not absolute evaluations are more related to implicit
biases. Using prompt-based measures informed by psychology allows us to
effectively expose nuanced biases and subtle discrimination in proprietary LLMs
that do not show explicit bias on standard benchmarks.
\\ ( https://arxiv.org/abs/2402.04105 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04236 (*cross-listing*)
Date: Tue, 6 Feb 2024 18:43:48 GMT   (20764kb,D)

Title: CogCoM: Train Large Vision-Language Models Diving into Details through
  Chain of Manipulations
Authors: Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin
  Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang
Categories: cs.CV cs.CL
Comments: 17 pages, 7 figures
\\
  Vision-Language Models (VLMs) have demonstrated their widespread viability
thanks to extensive training in aligning visual instructions to answers.
However, this conclusive alignment leads models to ignore critical visual
reasoning, and further result in failures on meticulous visual problems and
unfaithful responses. In this paper, we propose Chain of Manipulations, a
mechanism that enables VLMs to solve problems with a series of manipulations,
where each manipulation refers to an operation on the visual input, either from
intrinsic abilities (e.g., grounding) acquired through prior training or from
imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs
to generate faithful responses with evidential visual reasoning, and permits
users to trace error causes in the interpretable paths. We thus train CogCoM, a
general 17B VLM with a memory-based compatible architecture endowed this
reasoning mechanism. Experiments show that our model achieves the
state-of-the-art performance across 8 benchmarks from 3 categories, and a
limited number of training steps with the data swiftly gains a competitive
performance. The code and data are publicly available at
https://github.com/THUDM/CogCoM.
\\ ( https://arxiv.org/abs/2402.04236 ,  20764kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03317 (*cross-listing*)
Date: Tue, 2 Jan 2024 14:27:24 GMT   (334kb,D)

Title: SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular
  Value Penalization
Authors: Xixu Hu, Runkai Zheng, Jindong Wang, Cheuk Hang Leung, Qi Wu, Xing Xie
Categories: cs.CV cs.LG
Comments: Technical report; 21 pages
\\
  Vision Transformers (ViTs) have gained prominence as a preferred choice for a
wide range of computer vision tasks due to their exceptional performance.
However, their widespread adoption has raised concerns about security in the
face of malicious attacks. Most existing methods rely on empirical adjustments
during the training process, lacking a clear theoretical foundation. In this
study, we address this gap by introducing SpecFormer, specifically designed to
enhance ViTs' resilience against adversarial attacks, with support from
carefully derived theoretical guarantees. We establish local Lipschitz bounds
for the self-attention layer and introduce a novel approach, Maximum Singular
Value Penalization (MSVP), to attain precise control over these bounds. We
seamlessly integrate MSVP into ViTs' attention layers, using the power
iteration method for enhanced computational efficiency. The modified model,
SpecFormer, effectively reduces the spectral norms of attention weight
matrices, thereby enhancing network local Lipschitzness. This, in turn, leads
to improved training efficiency and robustness. Extensive experiments on CIFAR
and ImageNet datasets confirm SpecFormer's superior performance in defending
against adversarial attacks.
\\ ( https://arxiv.org/abs/2402.03317 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03325 (*cross-listing*)
Date: Mon, 8 Jan 2024 18:59:52 GMT   (2930kb,D)

Title: Connect Later: Improving Fine-tuning for Robustness with Targeted
  Augmentations
Authors: Helen Qu, Sang Michael Xie
Categories: cs.CV cs.LG
\\
  Models trained on a labeled source domain (e.g., labeled images from wildlife
camera traps) often generalize poorly when deployed on an out-of-distribution
(OOD) target domain (e.g., images from new camera trap locations). In the
domain adaptation setting where unlabeled target data is available,
self-supervised pretraining (e.g., masked autoencoding or contrastive learning)
is a promising method to mitigate this performance drop. Pretraining improves
OOD error when the generic data augmentations used (e.g., masking or cropping)
connect the source and target domains, which may be far apart in the input
space. In this paper, we show on real-world tasks that standard fine-tuning
after pretraining does not consistently improve OOD error over simply training
from scratch on labeled source data. To better leverage pretraining for
distribution shifts, we propose Connect Later: after pretraining with generic
augmentations, fine-tune with targeted augmentations designed with knowledge of
the distribution shift. Pretraining learns good representations within the
source and target domains, while targeted augmentations connect the domains
better during fine-tuning. Connect Later improves average OOD error over
standard fine-tuning and supervised learning with targeted augmentations on 4
real-world datasets: Connect Later achieves the state-of-the-art on
astronomical time-series classification (AstroClassification) by 2.5%, wildlife
species identification (iWildCam-WILDS) with ResNet-50 by 0.9%, and tumor
identification (Camelyon17-WILDS) with DenseNet121 by 1.1%; as well as best
performance on a new dataset for astronomical time-series redshift prediction
(Redshifts) by 0.03 RMSE (11% relative). Code and datasets are available at
https://github.com/helenqu/connect-later.
\\ ( https://arxiv.org/abs/2402.03325 ,  2930kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03326 (*cross-listing*)
Date: Mon, 8 Jan 2024 21:19:30 GMT   (466kb,D)

Title: Slot Structured World Models
Authors: Jonathan Collu, Riccardo Majellaro, Aske Plaat, Thomas M. Moerland
Categories: cs.CV cs.LG
\\
  The ability to perceive and reason about individual objects and their
interactions is a goal to be achieved for building intelligent artificial
systems. State-of-the-art approaches use a feedforward encoder to extract
object embeddings and a latent graph neural network to model the interaction
between these object embeddings. However, the feedforward encoder can not
extract {\it object-centric} representations, nor can it disentangle multiple
objects with similar appearance. To solve these issues, we introduce {\it Slot
Structured World Models} (SSWM), a class of world models that combines an {\it
object-centric} encoder (based on Slot Attention) with a latent graph-based
dynamics model. We evaluate our method in the Spriteworld benchmark with simple
rules of physical interaction, where Slot Structured World Models consistently
outperform baselines on a range of (multi-step) prediction tasks with
action-conditional object interactions. All code to reproduce paper experiments
is available from
\url{https://github.com/JonathanCollu/Slot-Structured-World-Models}.
\\ ( https://arxiv.org/abs/2402.03326 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03332 (*cross-listing*)
Date: Thu, 11 Jan 2024 07:31:53 GMT   (1227kb,D)

Title: Cyclic Neural Network
Authors: Liangwei Yang, Hengrui Zhang, Zihe Song, Jiawei Zhang, Weizhi Zhang,
  Jing Ma, Philip S. Yu
Categories: cs.NE cs.LG
Comments: 9 pages
\\
  This paper answers a fundamental question in artificial neural network (ANN)
design: We do not need to build ANNs layer-by-layer sequentially to guarantee
the Directed Acyclic Graph (DAG) property. Drawing inspiration from biological
intelligence (BI), where neurons form a complex, graph-structured network, we
introduce the groundbreaking Cyclic Neural Networks (Cyclic NNs). It emulates
the flexible and dynamic graph nature of biological neural systems, allowing
neuron connections in any graph-like structure, including cycles. This offers
greater adaptability compared to the DAG structure of current ANNs. We further
develop the Graph Over Multi-layer Perceptron, which is the first detailed
model based on this new design paradigm. Experimental validation of the Cyclic
NN's advantages on widely tested datasets in most generalized cases,
demonstrating its superiority over current BP training methods through the use
of a forward-forward (FF) training algorithm. This research illustrates a
totally new ANN design paradigm, which is a significant departure from current
ANN designs, potentially leading to more biologically plausible AI systems.
\\ ( https://arxiv.org/abs/2402.03332 ,  1227kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03338 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:46:28 GMT   (1578kb,D)

Title: CNN-DRL with Shuffled Features in Finance
Authors: Sina Montazeri, Akram Mirzaeinia, Amir Mirzaeinia
Categories: q-fin.CP cs.LG
Comments: 10th Annual Conf. on Computational Science & Computational
  Intelligence (CSCI'23). arXiv admin note: substantial text overlap with
  arXiv:2401.06179
\\
  In prior methods, it was observed that the application of Convolutional
Neural Networks agent in Deep Reinforcement Learning to financial data resulted
in an enhanced reward. In this study, a specific permutation was applied to the
feature vector, thereby generating a CNN matrix that strategically positions
more pertinent features in close proximity. Our comprehensive experimental
evaluations unequivocally demonstrate a substantial enhancement in reward
attainment.
\\ ( https://arxiv.org/abs/2402.03338 ,  1578kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03342 (*cross-listing*)
Date: Sun, 21 Jan 2024 20:08:32 GMT   (943kb,D)

Title: MADRL-based UAVs Trajectory Design with Anti-Collision Mechanism in
  Vehicular Networks
Authors: Leonardo Spampinato, Enrico Testi, Chiara Buratti, Riccardo Marini
Categories: cs.RO cs.LG cs.MA
Comments: Accepted for the 2024 IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2024)
\\
  In upcoming 6G networks, unmanned aerial vehicles (UAVs) are expected to play
a fundamental role by acting as mobile base stations, particularly for
demanding vehicle-to-everything (V2X) applications. In this scenario, one of
the most challenging problems is the design of trajectories for multiple UAVs,
cooperatively serving the same area. Such joint trajectory design can be
performed using multi-agent deep reinforcement learning (MADRL) algorithms, but
ensuring collision-free paths among UAVs becomes a critical challenge.
Traditional methods involve imposing high penalties during training to
discourage unsafe conditions, but these can be proven to be ineffective,
whereas binary masks can be used to restrict unsafe actions, but naively
applying them to all agents can lead to suboptimal solutions and
inefficiencies. To address these issues, we propose a rank-based binary masking
approach. Higher-ranked UAVs move optimally, while lower-ranked UAVs use this
information to define improved binary masks, reducing the number of unsafe
actions. This approach allows to obtain a good trade-off between exploration
and exploitation, resulting in enhanced training performance, while maintaining
safety constraints.
\\ ( https://arxiv.org/abs/2402.03342 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03345 (*cross-listing*)
Date: Wed, 24 Jan 2024 19:04:49 GMT   (798kb,D)

Title: Weakly supervised covariance matrices alignment through Stiefel matrices
  estimation for MEG applications
Authors: Antoine Collas, R\'emi Flamary, Alexandre Gramfort
Categories: eess.SP cs.LG stat.ML
\\
  This paper introduces a novel domain adaptation technique for time series
data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the
challenge of limited labeled signals in the target dataset. Leveraging a
domain-dependent mixing model and the optimal transport domain adaptation
assumption, we exploit abundant unlabeled data in the target domain to ensure
effective prediction by establishing pairwise correspondence with equivalent
signal variances between domains. Theoretical foundations are laid for
identifying crucial Stiefel matrices, essential for recovering underlying
signal variances from a Riemannian representation of observed signal
covariances. We propose an integrated cost function that simultaneously learns
these matrices, pairwise domain relationships, and a predictor, classifier, or
regressor, depending on the task. Applied to neuroscience problems, MSA
outperforms recent methods in brain-age regression with task variations using
magnetoencephalography (MEG) signals from the Cam-CAN dataset.
\\ ( https://arxiv.org/abs/2402.03345 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03352 (*cross-listing*)
Date: Fri, 26 Jan 2024 11:22:13 GMT   (211kb,D)

Title: Zeroth-Order primal-dual Alternating Projection Gradient Algorithms for
  Nonconvex Minimax Problems with Coupled linear Constraints
Authors: Huiling Zhang, Zi Xu, Yuhong Dai
Categories: math.OC cs.LG stat.ML
Comments: arXiv admin note: text overlap with arXiv:2212.04672
\\
  In this paper, we study zeroth-order algorithms for nonconvex minimax
problems with coupled linear constraints under the deterministic and stochastic
settings, which have attracted wide attention in machine learning, signal
processing and many other fields in recent years, e.g., adversarial attacks in
resource allocation problems and network flow problems etc. We propose two
single-loop algorithms, namely the zero-order primal-dual alternating projected
gradient (ZO-PDAPG) algorithm and the zero-order regularized momentum
primal-dual projected gradient algorithm (ZO-RMPDPG), for solving deterministic
and stochastic nonconvex-(strongly) concave minimax problems with coupled
linear constraints. The iteration complexity of the two proposed algorithms to
obtain an $\varepsilon$-stationary point are proved to be
$\mathcal{O}(\varepsilon ^{-2})$ (resp. $\mathcal{O}(\varepsilon ^{-4})$) for
solving nonconvex-strongly concave (resp. nonconvex-concave) minimax problems
with coupled linear constraints under deterministic settings and
$\tilde{\mathcal{O}}(\varepsilon ^{-3})$ (resp.
$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$) under stochastic settings
respectively. To the best of our knowledge, they are the first two zeroth-order
algorithms with iterative complexity guarantees for solving
nonconvex-(strongly) concave minimax problems with coupled linear constraints
under the deterministic and stochastic settings.
\\ ( https://arxiv.org/abs/2402.03352 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03353 (*cross-listing*)
Date: Fri, 26 Jan 2024 15:43:27 GMT   (11446kb)

Title: Tweet Influence on Market Trends: Analyzing the Impact of Social Media
  Sentiment on Biotech Stocks
Authors: C. Sarai R. Avila
Categories: q-fin.ST cs.LG cs.NA math.FA math.NA
Comments: This submission includes 51 pages and 24 figures
MSC-class: 62P05, 91G70, 62H30, 91B84, 68T05
ACM-class: I.2.7; I.2.6; K.4.1; A.0; J.1
\\
  This study investigates the relationship between tweet sentiment across
diverse categories: news, company opinions, CEO opinions, competitor opinions,
and stock market behavior in the biotechnology sector, with a focus on
understanding the impact of social media discourse on investor sentiment and
decision-making processes. We analyzed historical stock market data for ten of
the largest and most influential pharmaceutical companies alongside Twitter
data related to COVID-19, vaccines, the companies, and their respective CEOs.
Using VADER sentiment analysis, we examined the sentiment scores of tweets and
assessed their relationships with stock market performance. We employed ARIMA
(AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression)
models to forecast stock market performance, incorporating sentiment covariates
to improve predictions. Our findings revealed a complex interplay between tweet
sentiment, news, biotech companies, their CEOs, and stock market performance,
emphasizing the importance of considering diverse factors when modeling and
predicting stock prices. This study provides valuable insights into the
influence of social media on the financial sector and lays a foundation for
future research aimed at refining stock price prediction models.
\\ ( https://arxiv.org/abs/2402.03353 ,  11446kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03363 (*cross-listing*)
Date: Tue, 30 Jan 2024 16:44:52 GMT   (1183kb,D)

Title: Exploring Prime Number Classification: Achieving High Recall Rate and
  Rapid Convergence with Sparse Encoding
Authors: Serin Lee and S. Kim
Categories: math.NT cs.LG
MSC-class: 11, 68
ACM-class: G.0; G.1.0; G.1.10; G.1.m; I.0; I.m; I.1.1; I.2.0; I.2.6; I.2.m; J.2
\\
  This paper presents a novel approach at the intersection of machine learning
and number theory, focusing on the classification of prime and non-prime
numbers. At the core of our research is the development of a highly sparse
encoding method, integrated with conventional neural network architectures.
This combination has shown promising results, achieving a recall of over 99\%
in identifying prime numbers and 79\% for non-prime numbers from an inherently
imbalanced sequential series of integers, while exhibiting rapid model
convergence before the completion of a single training epoch. We performed
training using $10^6$ integers starting from a specified integer and tested on
a different range of $2 \times 10^6$ integers extending from $10^6$ to $3
\times 10^6$, offset by the same starting integer. While constrained by the
memory capacity of our resources, which limited our analysis to a span of
$3\times10^6$, we believe that our study contribute to the application of
machine learning in prime number analysis. This work aims to demonstrate the
potential of such applications and hopes to inspire further exploration and
possibilities in diverse fields.
\\ ( https://arxiv.org/abs/2402.03363 ,  1183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03365 (*cross-listing*)
Date: Wed, 31 Jan 2024 11:03:58 GMT   (2348kb,D)

Title: Heterophily-Aware Fair Recommendation using Graph Convolutional Networks
Authors: Nemat Gholinejad and Mostafa Haghir Chehreghani
Categories: cs.IR cs.LG cs.SI
\\
  In recent years, graph neural networks (GNNs) have become a popular tool to
improve the accuracy and performance of recommender systems. Modern recommender
systems are not only designed to serve the end users, but also to benefit other
participants, such as items and items providers. These participants may have
different or conflicting goals and interests, which raise the need for fairness
and popularity bias considerations. GNN-based recommendation methods also face
the challenges of unfairness and popularity bias and their normalization and
aggregation processes suffer from these challenges. In this paper, we propose a
fair GNN-based recommender system, called HetroFair, to improve items' side
fairness. HetroFair uses two separate components to generate fairness-aware
embeddings: i) fairness-aware attention which incorporates dot product in the
normalization process of GNNs, to decrease the effect of nodes' degrees, and
ii) heterophily feature weighting to assign distinct weights to different
features during the aggregation process. In order to evaluate the effectiveness
of HetroFair, we conduct extensive experiments over six real-world datasets.
Our experimental results reveal that HetroFair not only alleviates the
unfairness and popularity bias on the items' side, but also achieves superior
accuracy on the users' side. Our implementation is publicly available at
https://github.com/NematGH/HetroFair
\\ ( https://arxiv.org/abs/2402.03365 ,  2348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03367 (*cross-listing*)
Date: Wed, 31 Jan 2024 22:06:07 GMT   (250kb,D)

Title: RAG-Fusion: a New Take on Retrieval-Augmented Generation
Authors: Zackary Rackauckas
Categories: cs.IR cs.LG
Comments: 8 pages, 2 figures
\\
  Infineon has identified a need for engineers, account managers, and customers
to rapidly obtain product information. This problem is traditionally addressed
with retrieval-augmented generation (RAG) chatbots, but in this study, I
evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion
combines RAG and reciprocal rank fusion (RRF) by generating multiple queries,
reranking them with reciprocal scores and fusing the documents and scores.
Through manually evaluating answers on accuracy, relevance, and
comprehensiveness, I found that RAG-Fusion was able to provide accurate and
comprehensive answers due to the generated queries contextualizing the original
query from various perspectives. However, some answers strayed off topic when
the generated queries' relevance to the original query is insufficient. This
research marks significant progress in artificial intelligence (AI) and natural
language processing (NLP) applications and demonstrates transformations in a
global and multi-industry context.
\\ ( https://arxiv.org/abs/2402.03367 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03385 (*cross-listing*)
Date: Sun, 4 Feb 2024 09:19:56 GMT   (859kb)

Title: Adolescent relational behaviour and the obesity pandemic: A descriptive
  study applying social network analysis and machine learning techniques
Authors: Pilar Marqu\'es-S\'anchez, Mar\'ia Cristina Mart\'inez-Fern\'andez,
  Jos\'e Alberto Ben\'itez-Andrades, Enedina Quiroga-S\'anchez, Mar\'ia Teresa
  Garc\'ia-Ord\'as and Natalia Arias-Ramos
Categories: cs.SI cs.LG
Journal-ref: Plos One, Volume 18, Issue 8, August 2023, e0289553
DOI: 10.1371/journal.pone.0289553
\\
  Aim: To study the existence of subgroups by exploring the similarities
between the attributes of the nodes of the groups, in relation to diet and
gender and, to analyse the connectivity between groups based on aspects of
similarities between them through SNA and artificial intelligence techniques.
  Methods: 235 students from 5 different educational centres participate in
this study between March and December 2015. Data analysis carried out is
divided into two blocks: social network analysis and unsupervised machine
learning techniques. As for the social network analysis, the Girvan-Newman
technique was applied to find the best number of cohesive groups within each of
the friendship networks of the different classes analysed.
  Results: After applying Girvan-Newman in the three classes, the best division
into clusters was respectively 2 for classroom A, 7 for classroom B and 6 for
classroom C. There are significant differences between the groups and the
gender and diet variables. After applying K-means using population diet as an
input variable, a K-means clustering of 2 clusters for class A, 3 clusters for
class B and 3 clusters for class C is obtained.
  Conclusion: Adolescents form subgroups within their classrooms. Subgroup
cohesion is defined by the fact that nodes share similarities in aspects that
influence obesity, they share attributes related to food quality and gender.
The concept of homophily, related to SNA, justifies our results. Artificial
intelligence techniques together with the application of the Girvan-Newman
provide robustness to the structural analysis of similarities and cohesion
between subgroups.
\\ ( https://arxiv.org/abs/2402.03385 ,  859kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03387 (*cross-listing*)
Date: Sun, 4 Feb 2024 09:58:22 GMT   (189kb,D)

Title: Overcoming Order in Autoregressive Graph Generation
Authors: Edo Cohen-Karlik, Eyal Rozenberg and Daniel Freedman
Categories: cs.SI cs.LG
Comments: 16 pages, 3 figures
\\
  Graph generation is a fundamental problem in various domains, including
chemistry and social networks. Recent work has shown that molecular graph
generation using recurrent neural networks (RNNs) is advantageous compared to
traditional generative approaches which require converting continuous latent
representations into graphs. One issue which arises when treating graph
generation as sequential generation is the arbitrary order of the sequence
which results from a particular choice of graph flattening method. In this work
we propose using RNNs, taking into account the non-sequential nature of graphs
by adding an Orderless Regularization (OLR) term that encourages the hidden
state of the recurrent model to be invariant to different valid orderings
present under the training distribution. We demonstrate that sequential graph
generation models benefit from our proposed regularization scheme, especially
when data is scarce. Our findings contribute to the growing body of research on
graph generation and provide a valuable tool for various applications requiring
the synthesis of realistic and diverse graph structures.
\\ ( https://arxiv.org/abs/2402.03387 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03398 (*cross-listing*)
Date: Mon, 5 Feb 2024 02:52:25 GMT   (3086kb)

Title: Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning
Authors: Saeid Mehrdad, Seyed AmirHossein Janani
Categories: eess.IV cs.CV cs.LG cs.NE
\\
  Nonlinear hyperspectral unmixing has recently received considerable
attention, as linear mixture models do not lead to an acceptable resolution in
some problems. In fact, most nonlinear unmixing methods are designed by
assuming specific assumptions on the nonlinearity model which subsequently
limits the unmixing performance. In this paper, we propose an unsupervised
nonlinear unmixing approach based on deep learning by incorporating a general
nonlinear model with no special assumptions. This model consists of two
branches. In the first branch, endmembers are learned by reconstructing the
rows of hyperspectral images using some hidden layers, and in the second
branch, abundance values are learned based on the columns of respective images.
Then, using multi-task learning, we introduce an auxiliary task to enforce the
two branches to work together. This technique can be considered as a
regularizer mitigating overfitting, which improves the performance of the total
network. Extensive experiments on synthetic and real data verify the
effectiveness of the proposed method compared to some state-of-the-art
hyperspectral unmixing methods.
\\ ( https://arxiv.org/abs/2402.03398 ,  3086kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03414 (*cross-listing*)
Date: Mon, 5 Feb 2024 17:02:30 GMT   (1906kb,D)

Title: An end-to-end deep learning pipeline to derive blood input with partial
  volume corrections for automated parametric brain PET mapping
Authors: Rugved Chavan, Gabriel Hyman, Zoraiz Qureshi, Nivetha Jayakumar,
  William Terrell, Stuart Berr, David Schiff, Megan Wardius, Nathan Fountain,
  Thomas Muttikkal, Mark Quigg, Miaomiao Zhang, Bijoy Kundu
Categories: eess.IV cs.CV cs.LG
\\
  Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography
(dFDG-PET) for human brain imaging has considerable clinical potential, yet its
utilization remains limited. A key challenge in the quantitative analysis of
dFDG-PET is characterizing a patient-specific blood input function,
traditionally reliant on invasive arterial blood sampling. This research
introduces a novel approach employing non-invasive deep learning model-based
computations from the internal carotid arteries (ICA) with partial volume (PV)
corrections, thereby eliminating the need for invasive arterial sampling. We
present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA
segmentation, alongside a Recurrent Neural Network (RNN) based MCIF-net for the
derivation of a model-corrected blood input function (MCIF) with PV
corrections. The developed 3D U-Net and RNN was trained and validated using a
5-fold cross-validation approach on 50 human brain FDG PET datasets. The
ICA-net achieved an average Dice score of 82.18% and an Intersection over Union
of 68.54% across all tested scans. Furthermore, the MCIF-net exhibited a
minimal root mean squared error of 0.0052. The application of this pipeline to
ground truth data for dFDG-PET brain scans resulted in the precise localization
of seizure onset regions, which contributed to a successful clinical outcome,
with the patient achieving a seizure-free state after treatment. These results
underscore the efficacy of the ICA-net and MCIF-net deep learning pipeline in
learning the ICA structure's distribution and automating MCIF computation with
PV corrections. This advancement marks a significant leap in non-invasive
neuroimaging.
\\ ( https://arxiv.org/abs/2402.03414 ,  1906kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03445 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:00:45 GMT   (7745kb,D)

Title: Denoising Diffusion via Image-Based Rendering
Authors: Titas Anciukevicius, Fabian Manhardt, Federico Tombari, Paul Henderson
Categories: cs.CV cs.GR cs.LG
Comments: Accepted at ICLR 2024. Project page:
  https://anciukevicius.github.io/generative-image-based-rendering
\\
  Generating 3D scenes is a challenging open problem, which requires
synthesizing plausible content that is fully consistent in 3D space. While
recent methods such as neural radiance fields excel at view synthesis and 3D
reconstruction, they cannot synthesize plausible details in unobserved regions
since they lack a generative capability. Conversely, existing generative
methods are typically not capable of reconstructing detailed, large-scale
scenes in the wild, as they use limited-capacity 3D scene representations,
require aligned camera poses, or rely on additional regularizers. In this work,
we introduce the first diffusion model able to perform fast, detailed
reconstruction and generation of real-world 3D scenes. To achieve this, we make
three contributions. First, we introduce a new neural scene representation,
IB-planes, that can efficiently and accurately represent large 3D scenes,
dynamically allocating more capacity as needed to capture details visible in
each image. Second, we propose a denoising-diffusion framework to learn a prior
over this novel 3D scene representation, using only 2D images without the need
for any additional supervision signal such as masks or depths. This supports 3D
reconstruction and generation in a unified architecture. Third, we develop a
principled approach to avoid trivial 3D solutions when integrating the
image-based rendering with the diffusion model, by dropping out representations
of some images. We evaluate the model on several challenging datasets of real
and synthetic images, and demonstrate superior results on generation, novel
view synthesis and 3D reconstruction.
\\ ( https://arxiv.org/abs/2402.03445 ,  7745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03447 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:02:13 GMT   (40kb,D)

Title: Challenges in Variable Importance Ranking Under Correlation
Authors: Annie Liang and Thomas Jemielita and Andy Liaw and Vladimir Svetnik
  and Lingkang Huang and Richard Baumgartner and Jason M. Klusowski
Categories: stat.ML cs.LG stat.ME
\\
  Variable importance plays a pivotal role in interpretable machine learning as
it helps measure the impact of factors on the output of the prediction model.
Model agnostic methods based on the generation of "null" features via
permutation (or related approaches) can be applied. Such analysis is often
utilized in pharmaceutical applications due to its ability to interpret
black-box models, including tree-based ensembles. A major challenge and
significant confounder in variable importance estimation however is the
presence of between-feature correlation. Recently, several adjustments to
marginal permutation utilizing feature knockoffs were proposed to address this
issue, such as the variable importance measure known as conditional predictive
impact (CPI). Assessment and evaluation of such approaches is the focus of our
work. We first present a comprehensive simulation study investigating the
impact of feature correlation on the assessment of variable importance. We then
theoretically prove the limitation that highly correlated features pose for the
CPI through the knockoff construction. While we expect that there is always no
correlation between knockoff variables and its corresponding predictor
variables, we prove that the correlation increases linearly beyond a certain
correlation threshold between the predictor variables. Our findings emphasize
the absence of free lunch when dealing with high feature correlation, as well
as the necessity of understanding the utility and limitations behind methods in
variable importance estimation.
\\ ( https://arxiv.org/abs/2402.03447 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03460 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:11:57 GMT   (2736kb,D)

Title: Breaking the Curse of Dimensionality with Distributed Neural Computation
Authors: Haitz S\'aez de Oc\'ariz Borde and Takashi Furuya and Anastasis
  Kratsios and Marc T. Law
Categories: stat.ML cs.LG cs.NA cs.NE math.CO math.NA
\\
  We present a theoretical approach to overcome the curse of dimensionality
using a neural computation algorithm which can be distributed across several
machines. Our modular distributed deep learning paradigm, termed \textit{neural
pathways}, can achieve arbitrary accuracy while only loading a small number of
parameters into GPU VRAM. Formally, we prove that for every error level
$\varepsilon>0$ and every Lipschitz function $f:[0,1]^n\to \mathbb{R}$, one can
construct a neural pathways model which uniformly approximates $f$ to
$\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of
$\mathcal{O}(\varepsilon^{-1})$ parameters to be loaded in memory and
$\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the
forward pass. This improves the optimal bounds for traditional non-distributed
deep learning models, namely ReLU MLPs, which need
$\mathcal{O}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The
only other available deep learning model that breaks the curse of
dimensionality is MLPs with super-expressive activation functions. However, we
demonstrate that these models have an infinite VC dimension, even with bounded
depth and width restrictions, unlike the neural pathways model. This implies
that only the latter generalizes. Our analysis is validated experimentally in
both regression and classification tasks, demonstrating that our model exhibits
superior performance compared to larger centralized benchmarks.
\\ ( https://arxiv.org/abs/2402.03460 ,  2736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03474 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:34:12 GMT   (895kb,D)

Title: Active Region-based Flare Forecasting with Sliding Window Multivariate
  Time Series Forest Classifiers
Authors: Anli Ji and Berkay Aydin
Categories: astro-ph.SR cs.LG stat.AP
\\
  Over the past few decades, many applications of physics-based simulations and
data-driven techniques (including machine learning and deep learning) have
emerged to analyze and predict solar flares. These approaches are pivotal in
understanding the dynamics of solar flares, primarily aiming to forecast these
events and minimize potential risks they may pose to Earth. Although current
methods have made significant progress, there are still limitations to these
data-driven approaches. One prominent drawback is the lack of consideration for
the temporal evolution characteristics in the active regions from which these
flares originate. This oversight hinders the ability of these methods to grasp
the relationships between high-dimensional active region features, thereby
limiting their usability in operations. This study centers on the development
of interpretable classifiers for multivariate time series and the demonstration
of a novel feature ranking method with sliding window-based sub-interval
ranking. The primary contribution of our work is to bridge the gap between
complex, less understandable black-box models used for high-dimensional data
and the exploration of relevant sub-intervals from multivariate time series,
specifically in the context of solar flare forecasting. Our findings
demonstrate that our sliding-window time series forest classifier performs
effectively in solar flare prediction (with a True Skill Statistic of over
85\%) while also pinpointing the most crucial features and sub-intervals for a
given learning task.
\\ ( https://arxiv.org/abs/2402.03474 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03476 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:35:57 GMT   (3607kb,D)

Title: CT Material Decomposition using Spectral Diffusion Posterior Sampling
Authors: Xiao Jiang, Grace J. Gang, J. Webster Stayman
Categories: eess.IV cs.LG physics.med-ph
Comments: 5 pages, 4 figures
\\
  In this work, we introduce a new deep learning approach based on diffusion
posterior sampling (DPS) to perform material decomposition from spectral CT
measurements. This approach combines sophisticated prior knowledge from
unsupervised training with a rigorous physical model of the measurements. A
faster and more stable variant is proposed that uses a jumpstarted process to
reduce the number of time steps required in the reverse process and a gradient
approximation to reduce the computational cost. Performance is investigated for
two spectral CT systems: dual-kVp and dual-layer detector CT. On both systems,
DPS achieves high Structure Similarity Index Metric Measure(SSIM) with only 10%
of iterations as used in the model-based material decomposition(MBMD).
Jumpstarted DPS (JSDPS) further reduces computational time by over 85% and
achieves the highest accuracy, the lowest uncertainty, and the lowest
computational costs compared to classic DPS and MBMD. The results demonstrate
the potential of JSDPS for providing relatively fast and accurate material
decomposition based on spectral CT data.
\\ ( https://arxiv.org/abs/2402.03476 ,  3607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03481 (*cross-listing*)
Date: Mon, 5 Feb 2024 19:53:34 GMT   (7959kb,D)

Title: FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning
Authors: Sejoon Oh, Berk Ustun, Julian McAuley, Srijan Kumar
Categories: cs.IR cs.LG cs.SI
Comments: Accepted at the 6th FAccTRec Workshop on Responsible Recommendation @
  ACM RecSys 2023
\\
  Modern recommender systems may output considerably different recommendations
due to small perturbations in the training data. Changes in the data from a
single user will alter the recommendations as well as the recommendations of
other users. In applications like healthcare, housing, and finance, this
sensitivity can have adverse effects on user experience. We propose a method to
stabilize a given recommender system against such perturbations. This is a
challenging task due to (1) the lack of a ``reference'' rank list that can be
used to anchor the outputs; and (2) the computational challenges in ensuring
the stability of rank lists with respect to all possible perturbations of
training data. Our method, FINEST, overcomes these challenges by obtaining
reference rank lists from a given recommendation model and then fine-tuning the
model under simulated perturbation scenarios with rank-preserving
regularization on sampled items. Our experiments on real-world datasets
demonstrate that FINEST can ensure that recommender models output stable
recommendations under a wide range of different perturbations without
compromising next-item prediction accuracy.
\\ ( https://arxiv.org/abs/2402.03481 ,  7959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03527 (*cross-listing*)
Date: Mon, 5 Feb 2024 21:33:22 GMT   (8725kb,D)

Title: Consistent Validation for Predictive Methods in Spatial Settings
Authors: David R. Burt, Yunyi Shen and Tamara Broderick
Categories: stat.ML cs.LG stat.ME
Comments: 35 pages, 10 figures
\\
  Spatial prediction tasks are key to weather forecasting, studying air
pollution, and other scientific endeavors. Determining how much to trust
predictions made by statistical or physical methods is essential for the
credibility of scientific conclusions. Unfortunately, classical approaches for
validation fail to handle mismatch between locations available for validation
and (test) locations where we want to make predictions. This mismatch is often
not an instance of covariate shift (as commonly formalized) because the
validation and test locations are fixed (e.g., on a grid or at select points)
rather than i.i.d. from two distributions. In the present work, we formalize a
check on validation methods: that they become arbitrarily accurate as
validation data becomes arbitrarily dense. We show that classical and
covariate-shift methods can fail this check. We instead propose a method that
builds from existing ideas in the covariate-shift literature, but adapts them
to the validation data at hand. We prove that our proposal passes our check.
And we demonstrate its advantages empirically on simulated and real data.
\\ ( https://arxiv.org/abs/2402.03527 ,  8725kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03534 (*cross-listing*)
Date: Mon, 5 Feb 2024 21:43:40 GMT   (5173kb)

Title: ANN-based position and speed sensorless estimation for BLDC motors
Authors: Jose-Carlos Gamazo-Real, Victor Martinez-Martinez, Jaime Gomez-Gil
Categories: eess.SY cs.AR cs.LG cs.SY
Journal-ref: Measurement, vol. 188, no. 110602, pp. 1-15, 2022, ISSN 0263-2241
DOI: 10.1016/j.measurement.2021.110602
\\
  BLDC motor applications require precise position and speed measurements,
traditionally obtained with sensors. This article presents a method for
estimating those measurements without position sensors using terminal phase
voltages with attenuated spurious, acquired with a FPGA that also operates a
PWM-controlled inverter. Voltages are labelled with electrical and virtual
rotor states using an encoder that provides training and testing data for two
three-layer ANNs with perceptron-based cascade topology. The first ANN
estimates the position from features of voltages with incremental timestamps,
and the second ANN estimates the speed from features of position differentials
considering timestamps in an acquisition window. Sensor-based training and
sensorless testing at 125 to 1,500 rpm with a loaded 8-pole-pair motor obtained
absolute errors of 0.8 electrical degrees and 22 rpm. Results conclude that the
overall position estimation significantly improved conventional and advanced
methods, and the speed estimation slightly improved conventional methods, but
was worse than in advanced ones.
\\ ( https://arxiv.org/abs/2402.03534 ,  5173kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03648 (*cross-listing*)
Date: Tue, 6 Feb 2024 02:50:42 GMT   (1875kb,D)

Title: Multilinear Kernel Regression and Imputation via Manifold Learning
Authors: Duc Thien Nguyen and Konstantinos Slavakis
Categories: eess.SP cs.LG
\\
  This paper introduces a novel nonparametric framework for data imputation,
coined multilinear kernel regression and imputation via the manifold assumption
(MultiL-KRIM). Motivated by manifold learning, MultiL-KRIM models data features
as a point cloud located in or close to a user-unknown smooth manifold embedded
in a reproducing kernel Hilbert space. Unlike typical manifold-learning routes,
which seek low-dimensional patterns via regularizers based on graph-Laplacian
matrices, MultiL-KRIM builds instead on the intuitive concept of tangent spaces
to manifolds and incorporates collaboration among point-cloud neighbors
(regressors) directly into the data-modeling term of the loss function.
Multiple kernel functions are allowed to offer robustness and rich
approximation properties, while multiple matrix factors offer low-rank
modeling, integrate dimensionality reduction, and streamline computations with
no need of training data. Two important application domains showcase the
functionality of MultiL-KRIM: time-varying-graph-signal (TVGS) recovery, and
reconstruction of highly accelerated dynamic-magnetic-resonance-imaging (dMRI)
data. Extensive numerical tests on real and synthetic data demonstrate
MultiL-KRIM's remarkable speedups over its predecessors, and outperformance
over prevalent "shallow" data-imputation techniques, with a more intuitive and
explainable pipeline than deep-image-prior methods.
\\ ( https://arxiv.org/abs/2402.03648 ,  1875kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03651 (*cross-listing*)
Date: Tue, 6 Feb 2024 02:56:07 GMT   (929kb,D)

Title: Temporal Graph Analysis with TGX
Authors: Razieh Shirzadkhani, Shenyang Huang, Elahe Kooshafar, Reihaneh
  Rabbany, Farimah Poursafaei
Categories: cs.SI cs.LG
\\
  Real-world networks, with their evolving relations, are best captured as
temporal graphs. However, existing software libraries are largely designed for
static graphs where the dynamic nature of temporal graphs is ignored. Bridging
this gap, we introduce TGX, a Python package specially designed for analysis of
temporal networks that encompasses an automated pipeline for data loading, data
processing, and analysis of evolving graphs. TGX provides access to eleven
built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as
well as any novel datasets in the .csv format. Beyond data loading, TGX
facilitates data processing functionalities such as discretization of temporal
graphs and node subsampling to accelerate working with larger datasets. For
comprehensive investigation, TGX offers network analysis by providing a diverse
set of measures, including average node degree and the evolving number of nodes
and edges per timestamp. Additionally, the package consolidates meaningful
visualization plots indicating the evolution of temporal patterns, such as
Temporal Edge Appearance (TEA) and Temporal Edge Trafficc (TET) plots. The TGX
package is a robust tool for examining the features of temporal graphs and can
be used in various areas like studying social networks, citation networks, and
tracking user interactions. We plan to continuously support and update TGX
based on community feedback. TGX is publicly available on:
https://github.com/ComplexData-MILA/TGX.
\\ ( https://arxiv.org/abs/2402.03651 ,  929kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03714 (*cross-listing*)
Date: Tue, 6 Feb 2024 05:10:00 GMT   (32911kb,D)

Title: Advancing Location-Invariant and Device-Agnostic Motion Activity
  Recognition on Wearable Devices
Authors: Rebecca Adaimi, Abdelkareem Bedri, Jun Gong, Richard Kang, Joanna
  Arreaza-Taylor, Gerri-Michelle Pascual, Michael Ralph, and Gierad Laput
Categories: cs.HC cs.LG
\\
  Wearable sensors have permeated into people's lives, ushering impactful
applications in interactive systems and activity recognition. However,
practitioners face significant obstacles when dealing with sensing
heterogeneities, requiring custom models for different platforms. In this
paper, we conduct a comprehensive evaluation of the generalizability of motion
models across sensor locations. Our analysis highlights this challenge and
identifies key on-body locations for building location-invariant models that
can be integrated on any device. For this, we introduce the largest
multi-location activity dataset (N=50, 200 cumulative hours), which we make
publicly available. We also present deployable on-device motion models reaching
91.41% frame-level F1-score from a single model irrespective of sensor
placements. Lastly, we investigate cross-location data synthesis, aiming to
alleviate the laborious data collection tasks by synthesizing data in one
location given data from another. These contributions advance our vision of
low-barrier, location-invariant activity recognition systems, catalyzing
research in HCI and ubiquitous computing.
\\ ( https://arxiv.org/abs/2402.03714 ,  32911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03724 (*cross-listing*)
Date: Tue, 6 Feb 2024 05:42:27 GMT   (1175kb,D)

Title: Statistical Test for Anomaly Detections by Variational Auto-Encoders
Authors: Daiki Miwa, Tomohiro Shiraishi, Vo Nguyen Le Duy, Teruyuki Katsuoka,
  Ichiro Takeuchi
Categories: stat.ML cs.LG
\\
  In this study, we consider the reliability assessment of anomaly detection
(AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD
has been actively studied in various perspective, from method development to
applied research. However, when the results of ADs are used in high-stakes
decision-making, such as in medical diagnosis, it is necessary to ensure the
reliability of the detected anomalies. In this study, we propose the VAE-AD
Test as a method for quantifying the statistical reliability of VAE-based AD
within the framework of statistical testing. Using the VAE-AD Test, the
reliability of the anomaly regions detected by a VAE can be quantified in the
form of p-values. This means that if an anomaly is declared when the p-value is
below a certain threshold, it is possible to control the probability of false
detection to a desired level. Since the VAE-AD Test is constructed based on a
new statistical inference framework called selective inference, its validity is
theoretically guaranteed in finite samples. To demonstrate the validity and
effectiveness of the proposed VAE-AD Test, numerical experiments on artificial
data and applications to brain image analysis are conducted.
\\ ( https://arxiv.org/abs/2402.03724 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03736 (*cross-listing*)
Date: Tue, 6 Feb 2024 06:05:11 GMT   (840kb,D)

Title: An Effective Branch-and-Bound Algorithm with New Bounding Methods for
  the Maximum $s$-Bundle Problem
Authors: Jinghui Xue, Jiongzhi Zheng, Mingming Jin and Kun He
Categories: cs.DS cs.DM cs.LG
Comments: 10 pages, 2 figures, 5 tables
\\
  The Maximum s-Bundle Problem (MBP) addresses the task of identifying a
maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if
its vertex connectivity is at least |V|-s, where the vertex connectivity equals
the minimum number of vertices whose deletion yields a disconnected or trivial
graph. MBP is NP-hard and holds relevance in numerous realworld scenarios
emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the
branch-and-bound (BnB) framework, whose performance heavily depends on the
quality of the upper bound on the cardinality of a maximum s-bundle and the
initial lower bound with graph reduction. In this work, we introduce a novel
Partition-based Upper Bound (PUB) that leverages the graph partitioning
technique to achieve a tighter upper bound compared to existing ones. To
increase the lower bound, we propose to do short random walks on a clique to
generate larger initial solutions. Then, we propose a new BnB algorithm that
uses the initial lower bound and PUB in preprocessing for graph reduction, and
uses PUB in the BnB search process for branch pruning. Extensive experiments
with diverse s values demonstrate the significant progress of our algorithm
over state-of-the-art BnB MBP algorithms. Moreover, our initial lower bound can
also be generalized to other relaxation clique problems.
\\ ( https://arxiv.org/abs/2402.03736 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03740 (*cross-listing*)
Date: Tue, 6 Feb 2024 06:13:13 GMT   (7452kb,D)

Title: BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning
Authors: Mohammad Majid Akhtar, Navid Shadman Bhuiyan, Rahat Masood, Muhammad
  Ikram, Salil S. Kanhere
Categories: cs.SI cs.CY cs.LG
\\
  The detection of automated accounts, also known as "social bots", has been an
increasingly important concern for online social networks (OSNs). While several
methods have been proposed for detecting social bots, significant research gaps
remain. First, current models exhibit limitations in detecting sophisticated
bots that aim to mimic genuine OSN users. Second, these methods often rely on
simplistic profile features, which are susceptible to manipulation. In addition
to their vulnerability to adversarial manipulations, these models lack
generalizability, resulting in subpar performance when trained on one dataset
and tested on another.
  To address these challenges, we propose a novel framework for social Bot
detection with Self-Supervised Contrastive Learning (BotSSCL). Our framework
leverages contrastive learning to distinguish between social bots and humans in
the embedding space to improve linear separability. The high-level
representations derived by BotSSCL enhance its resilience to variations in data
distribution and ensure generalizability. We evaluate BotSSCL's robustness
against adversarial attempts to manipulate bot accounts to evade detection.
Experiments on two datasets featuring sophisticated bots demonstrate that
BotSSCL outperforms other supervised, unsupervised, and self-supervised
baseline methods. We achieve approx. 6% and approx. 8% higher (F1) performance
than SOTA on both datasets. In addition, BotSSCL also achieves 67% F1 when
trained on one dataset and tested with another, demonstrating its
generalizability. Lastly, BotSSCL increases adversarial complexity and only
allows 4% success to the adversary in evading detection.
\\ ( https://arxiv.org/abs/2402.03740 ,  7452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03752 (*cross-listing*)
Date: Tue, 6 Feb 2024 06:41:24 GMT   (1499kb,D)

Title: Pre-training of Lightweight Vision Transformers on Small Datasets with
  Minimally Scaled Images
Authors: Jen Hong Tan
Categories: cs.CV cs.LG
Comments: 7 pages, 6 figures
\\
  Can a lightweight Vision Transformer (ViT) match or exceed the performance of
Convolutional Neural Networks (CNNs) like ResNet on small datasets with small
image resolutions? This report demonstrates that a pure ViT can indeed achieve
superior performance through pre-training, using a masked auto-encoder
technique with minimal image scaling. Our experiments on the CIFAR-10 and
CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters
and a multiply-accumulate (MAC) count below 0.27G, qualifying them as
'lightweight' models. Unlike previous approaches, our method attains
state-of-the-art performance among similar lightweight transformer-based
architectures without significantly scaling up images from CIFAR-10 and
CIFAR-100. This achievement underscores the efficiency of our model, not only
in handling small datasets but also in effectively processing images close to
their original scale.
\\ ( https://arxiv.org/abs/2402.03752 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03761 (*cross-listing*)
Date: Tue, 6 Feb 2024 07:04:35 GMT   (3004kb)

Title: Deep Learning-Based Correction and Unmixing of Hyperspectral Images for
  Brain Tumor Surgery
Authors: David Black, Jaidev Gill, Andrew Xie, Benoit Liquet, Antonio Di leva,
  Walter Stummer, Eric Suero Molina
Categories: eess.IV cs.LG q-bio.TO
Comments: 20 pages, 8 figures, 3 tables - Under Review
\\
  Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection
enables visualization of differences between tissues that are not
distinguishable to humans. This augmentation can maximize brain tumor
resection, improving patient outcomes. However, much of the processing in HSI
uses simplified linear methods that are unable to capture the non-linear,
wavelength-dependent phenomena that must be modeled for accurate recovery of
fluorophore abundances. We therefore propose two deep learning models for
correction and unmixing, which can account for the nonlinear effects and
produce more accurate estimates of abundances. Both models use an
autoencoder-like architecture to process the captured spectra. One is trained
with protoporphyrin IX (PpIX) concentration labels. The other undergoes
semi-supervised training, first learning hyperspectral unmixing self-supervised
and then learning to correct fluorescence emission spectra for heterogeneous
optical and geometric properties using a reference white-light reflectance
spectrum in a few-shot manner. The models were evaluated against phantom and
pig brain data with known PpIX concentration; the supervised model achieved
Pearson correlation coefficients (R values) between the known and computed PpIX
concentrations of 0.997 and 0.990, respectively, whereas the classical approach
achieved only 0.93 and 0.82. The semi-supervised approach's R values were 0.98
and 0.91, respectively. On human data, the semi-supervised model gives
qualitatively more realistic results than the classical method, better removing
bright spots of specular reflectance and reducing the variance in PpIX
abundance over biopsies that should be relatively homogeneous. These results
show promise for using deep learning to improve HSI in fluorescence-guided
neurosurgery.
\\ ( https://arxiv.org/abs/2402.03761 ,  3004kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03779 (*cross-listing*)
Date: Tue, 6 Feb 2024 07:50:27 GMT   (696kb,D)

Title: EERO: Early Exit with Reject Option for Efficient Classification with
  limited budget
Authors: Florian Valade (LAMA), Mohamed Hebiri (LAMA), Paul Gay
Categories: stat.ML cs.LG
\\
  The increasing complexity of advanced machine learning models requires
innovative approaches to manage computational resources effectively. One such
method is the Early Exit strategy, which allows for adaptive computation by
providing a mechanism to shorten the processing path for simpler data
instances. In this paper, we propose EERO, a new methodology to translate the
problem of early exiting to a problem of using multiple classifiers with reject
option in order to better select the exiting head for each instance. We
calibrate the probabilities of exiting at the different heads using aggregation
with exponential weights to guarantee a fixed budget .We consider factors such
as Bayesian risk, budget constraints, and head-specific budget consumption.
Experimental results, conducted using a ResNet-18 model and a ConvNext
architecture on Cifar and ImageNet datasets, demonstrate that our method not
only effectively manages budget allocation but also enhances accuracy in
overthinking scenarios.
\\ ( https://arxiv.org/abs/2402.03779 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03806 (*cross-listing*)
Date: Tue, 6 Feb 2024 08:47:16 GMT   (598kb)

Title: Explainable Automated Machine Learning for Credit Decisions: Enhancing
  Human Artificial Intelligence Collaboration in Financial Engineering
Authors: Marc Schmitt
Categories: q-fin.RM cs.LG q-fin.CP
\\
  This paper explores the integration of Explainable Automated Machine Learning
(AutoML) in the realm of financial engineering, specifically focusing on its
application in credit decision-making. The rapid evolution of Artificial
Intelligence (AI) in finance has necessitated a balance between sophisticated
algorithmic decision-making and the need for transparency in these systems. The
focus is on how AutoML can streamline the development of robust machine
learning models for credit scoring, while Explainable AI (XAI) methods,
particularly SHapley Additive exPlanations (SHAP), provide insights into the
models' decision-making processes. This study demonstrates how the combination
of AutoML and XAI not only enhances the efficiency and accuracy of credit
decisions but also fosters trust and collaboration between humans and AI
systems. The findings underscore the potential of explainable AutoML in
improving the transparency and accountability of AI-driven financial decisions,
aligning with regulatory requirements and ethical considerations.
\\ ( https://arxiv.org/abs/2402.03806 ,  598kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03808 (*cross-listing*)
Date: Tue, 6 Feb 2024 08:48:39 GMT   (1877kb,D)

Title: SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal
  Denoising
Authors: Yu-Tung Liu, Kuan-Chen Wang, Kai-Chun Liu, Sheng-Yu Peng, Yu Tsao
Categories: eess.SP cs.LG
Comments: 5 pages, 4 figures
\\
  Surface electromyography (sEMG) recordings can be influenced by
electrocardiogram (ECG) signals when the muscle being monitored is close to the
heart. Several existing methods use signal-processing-based approaches, such as
high-pass filter and template subtraction, while some derive mapping functions
to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference).
Recently, the score-based diffusion model, a renowned generative model, has
been introduced to generate high-quality and accurate samples with noisy input
data. In this study, we proposed a novel approach, termed SDEMG, as a
score-based diffusion model for sEMG signal denoising. To evaluate the proposed
SDEMG approach, we conduct experiments to reduce noise in sEMG signals,
employing data from an openly accessible source, the Non-Invasive Adaptive
Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus
Rhythm Database. The experiment result indicates that SDEMG outperformed
comparative methods and produced high-quality sEMG samples. The source code of
SDEMG the framework is available at: https://github.com/tonyliu0910/SDEMG
\\ ( https://arxiv.org/abs/2402.03808 ,  1877kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03813 (*cross-listing*)
Date: Tue, 6 Feb 2024 08:57:49 GMT   (4893kb)

Title: NK Hybrid Genetic Algorithm for Clustering
Authors: Renato Tin\'os, Liang Zhao, Francisco Chicano, Darrell Whitley
Categories: cs.NE cs.LG
Journal-ref: IEEE Trans. Evol. Comput. 22(5): 748-761 (2018)
DOI: 10.1109/TEVC.2018.2828643
\\
  The NK hybrid genetic algorithm for clustering is proposed in this paper. In
order to evaluate the solutions, the hybrid algorithm uses the NK clustering
validation criterion 2 (NKCV2). NKCV2 uses information about the disposition of
$N$ small groups of objects. Each group is composed of $K+1$ objects of the
dataset. Experimental results show that density-based regions can be identified
by using NKCV2 with fixed small $K$. In NKCV2, the relationship between
decision variables is known, which in turn allows us to apply gray box
optimization. Mutation operators, a partition crossover, and a local search
strategy are proposed, all using information about the relationship between
decision variables. In partition crossover, the evaluation function is
decomposed into $q$ independent components; partition crossover then
deterministically returns the best among $2^q$ possible offspring with
computational complexity $O(N)$. The NK hybrid genetic algorithm allows the
detection of clusters with arbitrary shapes and the automatic estimation of the
number of clusters. In the experiments, the NK hybrid genetic algorithm
produced very good results when compared to another genetic algorithm approach
and to state-of-art clustering algorithms.
\\ ( https://arxiv.org/abs/2402.03813 ,  4893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03819 (*cross-listing*)
Date: Tue, 6 Feb 2024 09:07:41 GMT   (133kb,D)

Title: Theoretical and experimental study of SMOTE: limitations and comparisons
  of rebalancing strategies
Authors: Abdoulaye Sakho (LPSM), Erwan Scornet (LPSM), Emmanuel Malherbe
Categories: stat.ML cs.LG
\\
  Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing
strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE
(with default parameter) regenerates the original distribution by simply
copying the original minority samples. We also prove that SMOTE density
vanishes near the boundary of the support of the minority distribution,
therefore justifying the common BorderLine SMOTE strategy. Then we introduce
two new SMOTE-related strategies, and compare them with state-of-the-art
rebalancing procedures. We show that rebalancing strategies are only required
when the data set is highly imbalanced. For such data sets, SMOTE, our
proposals, or undersampling procedures are the best strategies.
\\ ( https://arxiv.org/abs/2402.03819 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03838 (*cross-listing*)
Date: Tue, 6 Feb 2024 09:35:40 GMT   (4860kb,D)

Title: Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman
  graph kernels
Authors: Rapha\"el Carpintero Perez (CMAP), S\'ebastien da Veiga (ENSAI,
  CREST), Josselin Garnier (CMAP), Brian Staber
Categories: stat.ML cs.LG
\\
  Supervised learning has recently garnered significant attention in the field
of computational physics due to its ability to effectively extract complex
patterns for tasks like solving partial differential equations, or predicting
material properties. Traditionally, such datasets consist of inputs given as
meshes with a large number of nodes representing the problem geometry (seen as
graphs), and corresponding outputs obtained with a numerical solver. This means
the supervised learning model must be able to handle large and sparse graphs
with continuous node attributes. In this work, we focus on Gaussian process
regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman
(SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL
kernel enjoys positive definiteness and a drastic complexity reduction, which
makes it possible to process datasets that were previously impossible to
handle. The new kernel is first validated on graph classification for molecular
datasets, where the input graphs have a few tens of nodes. The efficiency of
the SWWL kernel is then illustrated on graph regression in computational fluid
dynamics and solid mechanics, where the input graphs are made up of tens of
thousands of nodes.
\\ ( https://arxiv.org/abs/2402.03838 ,  4860kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03871 (*cross-listing*)
Date: Tue, 6 Feb 2024 10:32:39 GMT   (479kb,D)

Title: Geometric quantum machine learning of BQP$^A$ protocols and latent graph
  classifiers
Authors: Chukwudubem Umeano, Vincent E. Elfving, Oleksandr Kyriienko
Categories: quant-ph cond-mat.dis-nn cs.LG
Comments: 5 pages, 4 figures
\\
  Geometric quantum machine learning (GQML) aims to embed problem symmetries
for learning efficient solving protocols. However, the question remains if
(G)QML can be routinely used for constructing protocols with an exponential
separation from classical analogs. In this Letter we consider Simon's problem
for learning properties of Boolean functions, and show that this can be related
to an unsupervised circuit classification problem. Using the workflow of
geometric QML, we learn from first principles Simon's algorithm, thus
discovering an example of BQP$^A\neq$BPP protocol with respect to some dataset
(oracle $A$). Our key findings include the development of an equivariant
feature map for embedding Boolean functions, based on twirling with respect to
identified bitflip and permutational symmetries, and measurement based on
invariant observables with a sampling advantage. The proposed workflow points
to the importance of data embeddings and classical post-processing, while
keeping the variational circuit as a trivial identity operator. Next,
developing the intuition for the function learning, we visualize instances as
directed computational hypergraphs, and observe that the GQML protocol can
access their global topological features for distinguishing bijective and
surjective functions. Finally, we discuss the prospects for learning other
BQP$^A$-type protocols, and conjecture that this depends on the ability of
simplifying embeddings-based oracles $A$ applied as a linear combination of
unitaries.
\\ ( https://arxiv.org/abs/2402.03871 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03883 (*cross-listing*)
Date: Tue, 6 Feb 2024 10:45:51 GMT   (229kb,D)

Title: A Framework for Bilevel Optimization on Riemannian Manifolds
Authors: Andi Han, Bamdev Mishra, Pratik Jawanpuria, Akiko Takeda
Categories: math.OC cs.LG stat.ML
\\
  Bilevel optimization has seen an increasing presence in various domains of
applications. In this work, we propose a framework for solving bilevel
optimization problems where variables of both lower and upper level problems
are constrained on Riemannian manifolds. We provide several hypergradient
estimation strategies on manifolds and study their estimation error. We provide
convergence and complexity analysis for the proposed hypergradient descent
algorithm on manifolds. We also extend the developments to stochastic bilevel
optimization and to the use of general retraction. We showcase the utility of
the proposed framework on various applications.
\\ ( https://arxiv.org/abs/2402.03883 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03893 (*cross-listing*)
Date: Tue, 6 Feb 2024 10:58:13 GMT   (15227kb,D)

Title: Prediction Horizon Requirements for Automated Driving: Optimizing
  Safety, Comfort, and Efficiency
Authors: Manuel Mu\~noz S\'anchez, Chris van der Ploeg, Robin Smit, Jos
  Elfring, Emilia Silvas, Ren\'e van de Molengraft
Categories: cs.RO cs.LG
Comments: Submitted to IEEE Intelligent Vehicles Symposium. 9 pages. 10
  figures. 6 tables
\\
  Predicting the movement of other road users is beneficial for improving
automated vehicle (AV) performance. However, the relationship between the time
horizon associated with these predictions and AV performance remains unclear.
Despite the existence of numerous trajectory prediction algorithms, no studies
have been conducted on how varying prediction lengths affect AV safety and
other vehicle performance metrics, resulting in undefined horizon requirements
for prediction methods. Our study addresses this gap by examining the effects
of different prediction horizons on AV performance, focusing on safety,
comfort, and efficiency. Through multiple experiments using a state-of-the-art,
risk-based predictive trajectory planner, we simulated predictions with
horizons up to 20 seconds. Based on our simulations, we propose a framework for
specifying the minimum required and optimal prediction horizons based on
specific AV performance criteria and application needs. Our results indicate
that a horizon of 1.6 seconds is required to prevent collisions with crossing
pedestrians, horizons of 7-8 seconds yield the best efficiency, and horizons up
to 15 seconds improve passenger comfort. We conclude that prediction horizon
requirements are application-dependent, and recommend aiming for a prediction
horizon of 11.8 seconds as a general guideline for applications involving
crossing pedestrians.
\\ ( https://arxiv.org/abs/2402.03893 ,  15227kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03901 (*cross-listing*)
Date: Tue, 6 Feb 2024 11:13:26 GMT   (16kb)

Title: Batch Universal Prediction
Authors: Marco Bondaschi, Michael Gastpar
Categories: cs.IT cs.LG math.IT stat.ML
\\
  Large language models (LLMs) have recently gained much popularity due to
their surprising ability at generating human-like English sentences. LLMs are
essentially predictors, estimating the probability of a sequence of words given
the past. Therefore, it is natural to evaluate their performance from a
universal prediction perspective. In order to do that fairly, we introduce the
notion of batch regret as a modification of the classical average regret, and
we study its asymptotical value for add-constant predictors, in the case of
memoryless sources and first-order Markov sources.
\\ ( https://arxiv.org/abs/2402.03901 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03917 (*cross-listing*)
Date: Tue, 6 Feb 2024 11:35:02 GMT   (1095kb,D)

Title: Elastic Feature Consolidation for Cold Start Exemplar-free Incremental
  Learning
Authors: Simone Magistri, Tomaso Trinci, Albin Soutif-Cormerais, Joost van de
  Weijer, Andrew D. Bagdanov
Categories: cs.CV cs.LG
Comments: Accepted at Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\
  Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a
sequence of tasks without having access to previous task data. In this paper,
we consider the challenging Cold Start scenario in which insufficient data is
available in the first task to learn a high-quality backbone. This is
especially challenging for EFCIL since it requires high plasticity, which
results in feature drift which is difficult to compensate for in the
exemplar-free setting. To address this problem, we propose a simple and
effective approach that consolidates feature representations by regularizing
drift in directions highly relevant to previous tasks and employs prototypes to
reduce task-recency bias. Our method, called Elastic Feature Consolidation
(EFC), exploits a tractable second-order approximation of feature drift based
on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in
feature space which we use to regularize feature drift in important directions
and to update Gaussian prototypes used in a novel asymmetric cross entropy loss
which effectively balances prototype rehearsal with data from new tasks.
Experimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and
ImageNet-1K demonstrate that Elastic Feature Consolidation is better able to
learn new tasks by maintaining model plasticity and significantly outperform
the state-of-the-art.
\\ ( https://arxiv.org/abs/2402.03917 ,  1095kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03931 (*cross-listing*)
Date: Tue, 6 Feb 2024 12:01:00 GMT   (7157kb,D)

Title: Fully autonomous tuning of a spin qubit
Authors: Jonas Schuff, Miguel J. Carballido, Madeleine Kotzagiannidis, Juan
  Carlos Calvo, Marco Caselli, Jacob Rawling, David L. Craig, Barnaby van
  Straaten, Brandon Severin, Federico Fedele, Simon Svab, Pierre Chevalier
  Kwon, Rafael S. Eggli, Taras Patlatiuk, Nathan Korda, Dominik Zumb\"uhl,
  Natalia Ares
Categories: cond-mat.mes-hall cs.LG quant-ph
\\
  Spanning over two decades, the study of qubits in semiconductors for quantum
computing has yielded significant breakthroughs. However, the development of
large-scale semiconductor quantum circuits is still limited by challenges in
efficiently tuning and operating these circuits. Identifying optimal operating
conditions for these qubits is complex, involving the exploration of vast
parameter spaces. This presents a real 'needle in the haystack' problem, which,
until now, has resisted complete automation due to device variability and
fabrication imperfections. In this study, we present the first fully autonomous
tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a
clear indication of successful qubit operation. We demonstrate this automation,
achieved without human intervention, in a Ge/Si core/shell nanowire device. Our
approach integrates deep learning, Bayesian optimization, and computer vision
techniques. We expect this automation algorithm to apply to a wide range of
semiconductor qubit devices, allowing for statistical studies of qubit quality
metrics. As a demonstration of the potential of full automation, we
characterise how the Rabi frequency and g-factor depend on barrier gate
voltages for one of the qubits found by the algorithm. Twenty years after the
initial demonstrations of spin qubit operation, this significant advancement is
poised to finally catalyze the operation of large, previously unexplored
quantum circuits.
\\ ( https://arxiv.org/abs/2402.03931 ,  7157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03973 (*cross-listing*)
Date: Tue, 6 Feb 2024 13:06:14 GMT   (2105kb,D)

Title: Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given
  Enough Time
Authors: Netta Ollikka, Amro Abbas, Andrea Perin, Markku Kilpel\"ainen,
  St\'ephane Deny
Categories: cs.CV cs.LG
\\
  Deep learning is closing the gap with humans on several object recognition
benchmarks. Here we investigate this gap in the context of challenging images
where objects are seen from unusual viewpoints. We find that humans excel at
recognizing objects in unusual poses, in contrast with state-of-the-art
pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are
systematically brittle in this condition. Remarkably, as we limit image
exposure time, human performance degrades to the level of deep networks,
suggesting that additional mental processes (requiring additional time) take
place when humans identify objects in unusual poses. Finally, our analysis of
error patterns of humans vs. networks reveals that even time-limited humans are
dissimilar to feed-forward deep networks. We conclude that more work is needed
to bring computer vision systems to the level of robustness of the human visual
system. Understanding the nature of the mental processes taking place during
extra viewing time may be key to attain such robustness.
\\ ( https://arxiv.org/abs/2402.03973 ,  2105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03982 (*cross-listing*)
Date: Tue, 6 Feb 2024 13:19:26 GMT   (45kb)

Title: On Convergence of Adam for Stochastic Optimization under Relaxed
  Assumptions
Authors: Yusu Hong and Junhong Lin
Categories: math.OC cs.LG stat.ML
\\
  The Adaptive Momentum Estimation (Adam) algorithm is highly effective in
training various deep learning tasks. Despite this, there's limited theoretical
understanding for Adam, especially when focusing on its vanilla form in
non-convex smooth scenarios with potential unbounded gradients and affine
variance noise. In this paper, we study vanilla Adam under these challenging
conditions. We introduce a comprehensive noise model which governs affine
variance noise, bounded noise and sub-Gaussian noise. We show that Adam can
find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate
in high probability under this general noise model where $T$ denotes total
number iterations, matching the lower rate of stochastic first-order algorithms
up to logarithm factors. More importantly, we reveal that Adam is free of
tuning step-sizes with any problem-parameters, yielding a better adaptation
property than the Stochastic Gradient Descent under the same conditions. We
also provide a probabilistic convergence result for Adam under a generalized
smooth condition which allows unbounded smoothness parameters and has been
illustrated empirically to more accurately capture the smooth property of many
practical objective functions.
\\ ( https://arxiv.org/abs/2402.03982 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03990 (*cross-listing*)
Date: Tue, 6 Feb 2024 13:43:22 GMT   (68kb,D)

Title: Subsampling is not Magic: Why Large Batch Sizes Work for Differentially
  Private Stochastic Optimisation
Authors: Ossi R\"ais\"a, Joonas J\"alk\"o and Antti Honkela
Categories: stat.ML cs.CR cs.LG
\\
  We study the effect of the batch size to the total gradient variance in
differentially private stochastic gradient descent (DP-SGD), seeking a
theoretical explanation for the usefulness of large batch sizes. As DP-SGD is
the basis of modern DP deep learning, its properties have been widely studied,
and recent works have empirically found large batch sizes to be beneficial.
However, theoretical explanations of this benefit are currently heuristic at
best. We first observe that the total gradient variance in DP-SGD can be
decomposed into subsampling-induced and noise-induced variances. We then prove
that in the limit of an infinite number of iterations, the effective
noise-induced variance is invariant to the batch size. The remaining
subsampling-induced variance decreases with larger batch sizes, so large
batches reduce the effective total gradient variance. We confirm numerically
that the asymptotic regime is relevant in practical settings when the batch
size is not small, and find that outside the asymptotic regime, the total
gradient variance decreases even more with large batch sizes. We also find a
sufficient condition that implies that large batch sizes similarly reduce
effective DP noise variance for one iteration of DP-SGD.
\\ ( https://arxiv.org/abs/2402.03990 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04012 (*cross-listing*)
Date: Mon, 5 Feb 2024 09:59:57 GMT   (3509kb,D)

Title: Quantized Approximately Orthogonal Recurrent Neural Networks
Authors: Armand Foucault (IMT), Franck Mamalet (UT), Fran\c{c}ois Malgouyres
  (IMT)
Categories: cs.NE cs.LG eess.SP math.ST stat.TH
\\
  Orthogonal recurrent neural networks (ORNNs) are an appealing option for
learning tasks involving time series with long-term dependencies, thanks to
their simplicity and computational stability. However, these networks often
require a substantial number of parameters to perform well, which can be
prohibitive in power-constrained environments, such as compact devices. One
approach to address this issue is neural network quantization. The construction
of such networks remains an open problem, acknowledged for its inherent
instability.In this paper, we explore the quantization of the recurrent and
input weight matrices in ORNNs, leading to Quantized approximately Orthogonal
RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and
three quantization-aware training (QAT) algorithms that incorporate orthogonal
constraints and quantized weights. Empirical results demonstrate the advantages
of employing QAT over PTQ. The most efficient model achieves results similar to
state-of-the-art full-precision ORNN and LSTM on a variety of standard
benchmarks, even with 3-bits quantization.
\\ ( https://arxiv.org/abs/2402.04012 ,  3509kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04022 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:12:46 GMT   (702kb,D)

Title: A General Theory for Kernel Packets: from state space model to compactly
  supported basis
Authors: Liang Ding and Tuo Rui
Categories: stat.ML cs.LG
\\
  It is well known that the state space (SS) model formulation of a Gaussian
process (GP) can lower its training and prediction time both to O(n) for n data
points. We prove that an $m$-dimensional SS model formulation of GP is
equivalent to a concept we introduce as the general right Kernel Packet (KP): a
transformation for the GP covariance function $K$ such that
$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j
\leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes
$j$-th order derivative acting on $t$. We extend this idea to the backward SS
model formulation of the GP, leading to the concept of the left KP for next $m$
consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq
t_{2m}$. By combining both left and right KPs, we can prove that a suitable
linear combination of these covariance functions yields $m$ compactly supported
KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and
$j=0,\cdots,m-1$. KPs further reduces the prediction time of GP to O(log n) or
even O(1) and can be applied to more general problems involving the derivative
of GPs.
\\ ( https://arxiv.org/abs/2402.04022 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04031 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:26:02 GMT   (531kb)

Title: Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced
  Segmentation
Authors: Zolnamar Dorjsembe, Hsing-Kuo Pao and Furen Xiao
Categories: cs.CV cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  This study introduces Polyp-DDPM, a diffusion-based method for generating
realistic images of polyps conditioned on masks, aimed at enhancing the
segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the
challenges of data limitations, high annotation costs, and privacy concerns
associated with medical images. By conditioning the diffusion model on
segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM
outperforms state-of-the-art methods in terms of image quality (achieving a
Frechet Inception Distance (FID) score of 78.47, compared to scores above
83.79) and segmentation performance (achieving an Intersection over Union (IoU)
of 0.7156, versus less than 0.6694 for synthetic images from baseline models
and 0.7067 for real data). Our method generates a high-quality, diverse
synthetic dataset for training, thereby enhancing polyp segmentation models to
be comparable with real images and offering greater data augmentation
capabilities to improve segmentation models. The source code and pretrained
weights for Polyp-DDPM are made publicly available at
https://github.com/mobaidoctor/polyp-ddpm.
\\ ( https://arxiv.org/abs/2402.04031 ,  531kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04038 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:34:17 GMT   (47kb)

Title: PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural
  Network
Authors: Tan Sun, Junhong Lin
Categories: stat.ML cs.LG
Comments: 32pages
\\
  Graph neural networks (GNNs) have gained popularity for various graph-related
tasks. However, similar to deep neural networks, GNNs are also vulnerable to
adversarial attacks. Empirical studies have shown that adversarially robust
generalization has a pivotal role in establishing effective defense algorithms
against adversarial attacks. In this paper, we contribute by providing
adversarially robust generalization bounds for two kinds of popular GNNs, graph
convolutional network (GCN) and message passing graph neural network, using the
PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion
matrix on the graph and spectral norm of the weights as well as the
perturbation factor govern the robust generalization bounds of both models. Our
bounds are nontrivial generalizations of the results developed in (Liao et al.,
2020) from the standard setting to adversarial setting while avoiding
exponential dependence of the maximum node degree. As corollaries, we derive
better PAC-Bayesian robust generalization bounds for GCN in the standard
setting, which improve the bounds in (Liao et al., 2020) by avoiding
exponential dependence on the maximum node degree.
\\ ( https://arxiv.org/abs/2402.04038 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04061 (*cross-listing*)
Date: Tue, 6 Feb 2024 15:05:25 GMT   (2670kb,D)

Title: TopoNav: Topological Navigation for Efficient Exploration in Sparse
  Reward Environments
Authors: Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Jade Freeman, Timothy
  Gregory, Theron T. Trout
Categories: cs.RO cs.LG
Comments: Paper under review
\\
  Autonomous robots exploring unknown areas face a significant challenge --
navigating effectively without prior maps and with limited external feedback.
This challenge intensifies in sparse reward environments, where traditional
exploration techniques often fail. In this paper, we introduce TopoNav, a novel
framework that empowers robots to overcome these constraints and achieve
efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental
building blocks are active topological mapping, intrinsic reward mechanisms,
and hierarchical objective prioritization. Throughout its exploration, TopoNav
constructs a dynamic topological map that captures key locations and pathways.
It utilizes intrinsic rewards to guide the robot towards designated sub-goals
within this map, fostering structured exploration even in sparse reward
settings. To ensure efficient navigation, TopoNav employs the Hierarchical
Objective-Driven Active Topologies framework, enabling the robot to prioritize
immediate tasks like obstacle avoidance while maintaining focus on the overall
goal. We demonstrate TopoNav's effectiveness in simulated environments that
replicate real-world conditions. Our results reveal significant improvements in
exploration efficiency, navigational accuracy, and adaptability to unforeseen
obstacles, showcasing its potential to revolutionize autonomous exploration in
a wide range of applications, including search and rescue, environmental
monitoring, and planetary exploration.
\\ ( https://arxiv.org/abs/2402.04061 ,  2670kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04114 (*cross-listing*)
Date: Tue, 6 Feb 2024 16:06:59 GMT   (86kb,D)

Title: SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated
  Linear Stochastic Approximation and Temporal Difference Learning
Authors: Paul Mangold, Sergey Samsonov, Safwan Labbi, Ilya Levin, Reda Alami,
  Alexey Naumov, Eric Moulines
Categories: stat.ML cs.LG math.OC
\\
  In this paper, we perform a non-asymptotic analysis of the federated linear
stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias
introduced by local training with heterogeneous agents, and investigate the
sample complexity of the algorithm. We show that the communication complexity
of FedLSA scales polynomially with the desired precision $\epsilon$, which
limits the benefits of federation. To overcome this, we propose SCAFFLSA, a
novel variant of FedLSA, that uses control variates to correct the bias of
local training, and prove its convergence without assumptions on statistical
heterogeneity. We apply the proposed methodology to federated temporal
difference learning with linear function approximation, and analyze the
corresponding complexity improvements.
\\ ( https://arxiv.org/abs/2402.04114 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04146 (*cross-listing*)
Date: Tue, 6 Feb 2024 16:54:59 GMT   (9716kb,D)

Title: Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian
  Process
Authors: Sandipp Krishnan Ravi, Yigitcan Comlek, Wei Chen, Arjun Pathak, Vipul
  Gupta, Rajnikant Umretiya, Andrew Hoffman, Ghanshyam Pilania, Piyush Pandita,
  Sayan Ghosh, Nathaniel Mckeever, Liping Wang
Categories: stat.ML cs.LG
Comments: 27 Pages,9 Figures, 3 Supplementary Figures, 2 Supplementary Tables
\\
  With the advent of artificial intelligence (AI) and machine learning (ML),
various domains of science and engineering communites has leveraged data-driven
surrogates to model complex systems from numerous sources of information
(data). The proliferation has led to significant reduction in cost and time
involved in development of superior systems designed to perform specific
functionalities. A high proposition of such surrogates are built extensively
fusing multiple sources of data, may it be published papers, patents, open
repositories, or other resources. However, not much attention has been paid to
the differences in quality and comprehensiveness of the known and unknown
underlying physical parameters of the information sources that could have
downstream implications during system optimization. Towards resolving this
issue, a multi-source data fusion framework based on Latent Variable Gaussian
Process (LVGP) is proposed. The individual data sources are tagged as a
characteristic categorical variable that are mapped into a physically
interpretable latent space, allowing the development of source-aware data
fusion modeling. Additionally, a dissimilarity metric based on the latent
variables of LVGP is introduced to study and understand the differences in the
sources of data. The proposed approach is demonstrated on and analyzed through
two mathematical (representative parabola problem, 2D Ackley function) and two
materials science (design of FeCrAl and SmCoFe alloys) case studies. From the
case studies, it is observed that compared to using single-source and source
unaware ML models, the proposed multi-source data fusion framework can provide
better predictions for sparse-data problems, interpretability regarding the
sources, and enhanced modeling capabilities by taking advantage of the
correlations and relationships among different sources.
\\ ( https://arxiv.org/abs/2402.04146 ,  9716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04216 (*cross-listing*)
Date: Tue, 6 Feb 2024 18:17:02 GMT   (2132kb,D)

Title: Resource-Aware Hierarchical Federated Learning in Wireless Video Caching
  Networks
Authors: Md Ferdous Pervej and Andreas F. Molisch
Categories: cs.NI cs.LG cs.SY eess.SY
Comments: Under review for possible publication in IEEE TWC
\\
  Backhaul traffic congestion caused by the video traffic of a few popular
files can be alleviated by storing the to-be-requested content at various
levels in wireless video caching networks. Typically, content service providers
(CSPs) own the content, and the users request their preferred content from the
CSPs using their (wireless) internet service providers (ISPs). As these parties
do not reveal their private information and business secrets, traditional
techniques may not be readily used to predict the dynamic changes in users'
future demands. Motivated by this, we propose a novel resource-aware
hierarchical federated learning (RawHFL) solution for predicting user's future
content requests. A practical data acquisition technique is used that allows
the user to update its local training dataset based on its requested content.
Besides, since networking and other computational resources are limited,
considering that only a subset of the users participate in the model training,
we derive the convergence bound of the proposed algorithm. Based on this bound,
we minimize a weighted utility function for jointly configuring the
controllable parameters to train the RawHFL energy efficiently under practical
resource constraints. Our extensive simulation results validate the proposed
algorithm's superiority, in terms of test accuracy and energy cost, over
existing baselines.
\\ ( https://arxiv.org/abs/2402.04216 ,  2132kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2211.05939
replaced with revised version Tue, 6 Feb 2024 00:25:23 GMT   (873kb,D)

Title: pyRDDLGym: From RDDL to Gym Environments
Authors: Ayal Taitler, Michael Gimelfarb, Jihwan Jeong, Sriram Gopalakrishnan,
  Martin Mladenov, Xiaotian Liu, Scott Sanner
Categories: cs.AI
\\ ( https://arxiv.org/abs/2211.05939 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2302.07200
replaced with revised version Tue, 6 Feb 2024 11:24:12 GMT   (229kb)

Title: Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey
Authors: Lauren Nicole DeLong, Ramon Fern\'andez Mir, Jacques D. Fleuriot (The
  University of Edinburgh School of Informatics, Artificial Intelligence and
  its Applications Institute)
Categories: cs.AI cs.LO stat.ML
Comments: 21 pages, 6 figures, 2 table, currently under review. Corresponding
  GitHub page here: https://github.com/NeSymGraphs. Revised in February 2024
  according to major revisions by peer reviewers
\\ ( https://arxiv.org/abs/2302.07200 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09197
replaced with revised version Tue, 6 Feb 2024 08:42:45 GMT   (46kb)

Title: Integrating Temporality and Causality into Acyclic Argumentation
  Frameworks using a Transition System
Authors: Y. Munro (1), C. Sarmiento (1), I. Bloch (1), G. Bourgne (1), M.-J.
  Lesot (1) ((1) Sorbonne Universit\'e, CNRS, LIP6, Paris, France)
Categories: cs.AI
\\ ( https://arxiv.org/abs/2303.09197 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01285
replaced with revised version Mon, 5 Feb 2024 20:46:47 GMT   (3499kb,D)

Title: Flows: Building Blocks of Reasoning and Collaborating AI
Authors: Martin Josifoski, Lars Klein, Maxime Peyrard, Nicolas Baldwin, Yifei
  Li, Saibo Geng, Julian Paul Schnitzler, Yuxing Yao, Jiheng Wei, Debjit Paul,
  Robert West
Categories: cs.AI cs.HC
\\ ( https://arxiv.org/abs/2308.01285 ,  3499kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07683
replaced with revised version Mon, 5 Feb 2024 19:01:55 GMT   (816kb)

Title: Assessing the nature of large language models: A caution against
  anthropocentrism
Authors: Ann Speed
Categories: cs.AI cs.CL cs.CY cs.HC
Comments: 31 pages, 6 figures
\\ ( https://arxiv.org/abs/2309.07683 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10852
replaced with revised version Tue, 6 Feb 2024 16:59:17 GMT   (3804kb,D)

Title: Using AI Uncertainty Quantification to Improve Human Decision-Making
Authors: Laura R. Marusich, Jonathan Z. Bakdash, Yan Zhou, Murat Kantarcioglu
Categories: cs.AI cs.HC
Comments: 12 pages and 7 figures
\\ ( https://arxiv.org/abs/2309.10852 ,  3804kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14403
replaced with revised version Mon, 5 Feb 2024 20:17:14 GMT   (5468kb,D)

Title: O3D: Offline Data-driven Discovery and Distillation for Sequential
  Decision-Making with Large Language Models
Authors: Yuchen Xiao, Yanchao Sun, Mengda Xu, Udari Madhushani, Jared Vann,
  Deepeka Garg, Sumitra Ganesh
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.14403 ,  5468kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19917
replaced with revised version Tue, 6 Feb 2024 05:04:07 GMT   (371kb)

Title: Unmasking Bias in AI: A Systematic Review of Bias Detection and
  Mitigation Strategies in Electronic Health Record-based Models
Authors: Feng Chen, Liqin Wang, Julie Hong, Jiaqi Jiang, Li Zhou
Categories: cs.AI cs.CY cs.LG q-bio.QM
Comments: 29 pages, 2 figures, 4 tables, 2 supplementary files, 69 references
\\ ( https://arxiv.org/abs/2310.19917 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00006
replaced with revised version Tue, 6 Feb 2024 16:30:55 GMT   (22814kb,D)

Title: Building Open-Ended Embodied Agent via Language-Policy Bidirectional
  Adaptation
Authors: Shaopeng Zhai, Jie Wang, Tianyi Zhang, Fuxian Huang, Qi Zhang, Ming
  Zhou, Jing Hou, Yu Qiao and Yu Liu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.00006 ,  22814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01817
replaced with revised version Tue, 6 Feb 2024 01:29:37 GMT   (4552kb,D)

Title: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
Authors: Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit
  Verma, Siddhant Bhambri, Lucas Saldyt, Anil Murthy
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.01817 ,  4552kb)
------------------------------------------------------------------------------
\\
arXiv:2106.10901
replaced with revised version Tue, 6 Feb 2024 10:22:52 GMT   (2022kb,D)

Title: Software-Based Dialogue Systems: Survey, Taxonomy and Challenges
Authors: Quim Motger, Xavier Franch and Jordi Marco
Categories: cs.CL cs.SE
DOI: 10.1145/3527450
\\ ( https://arxiv.org/abs/2106.10901 ,  2022kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03457
replaced with revised version Tue, 6 Feb 2024 07:25:15 GMT   (192kb,D)

Title: Selecting Seed Words for Wordle using Character Statistics
Authors: Nisansa de Silva
Categories: cs.CL
DOI: 10.1109/MERCon55799.2022.9906176
\\ ( https://arxiv.org/abs/2202.03457 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10750
replaced with revised version Mon, 5 Feb 2024 21:25:33 GMT   (911kb,D)

Title: Improving Grounded Language Understanding in a Collaborative Environment
  by Interacting with Agents Through Help Feedback
Authors: Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz, Xin Deng, Ahmed
  Hassan Awadallah, and Julia Kiseleva
Categories: cs.CL cs.AI
Comments: Findings of EACL 2024
\\ ( https://arxiv.org/abs/2304.10750 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11062
replaced with revised version Tue, 6 Feb 2024 10:16:54 GMT   (942kb,D)

Title: Scaling Transformer to 1M tokens and beyond with RMT
Authors: Aydar Bulatov, Yuri Kuratov, Yermek Kapushev, Mikhail S. Burtsev
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2304.11062 ,  942kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06841
replaced with revised version Tue, 6 Feb 2024 11:30:00 GMT   (7004kb,D)

Title: Think Twice: Measuring the Efficiency of Eliminating Prediction
  Shortcuts of Question Answering Models
Authors: Luk\'a\v{s} Mikula, Michal \v{S}tef\'anik, Marek Petrovi\v{c}, Petr
  Sojka
Categories: cs.CL cs.AI
Comments: Long paper in Proceedings of EACL 2024: Main track
\\ ( https://arxiv.org/abs/2305.06841 ,  7004kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14998
replaced with revised version Tue, 6 Feb 2024 04:20:15 GMT   (10373kb,D)

Title: An Examination of the Robustness of Reference-Free Image Captioning
  Evaluation Metrics
Authors: Saba Ahmadi, Aishwarya Agrawal
Categories: cs.CL cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2305.14998 ,  10373kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01015
replaced with revised version Tue, 6 Feb 2024 03:52:48 GMT   (1744kb,D)

Title: How to Estimate Model Transferability of Pre-Trained Speech Models?
Authors: Zih-Ching Chen, Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen,
  Shuo-Yiin Chang, Rohit Prabhavalkar, Hung-yi Lee, Tara N. Sainath
Categories: cs.CL cs.NE cs.SD eess.AS
Comments: Accepted to Interspeech. Code is available at:
  https://github.com/virginiakm1988/LogME-CTC. Fixed a typo
DOI: 10.21437/Interspeech.2023-1079
\\ ( https://arxiv.org/abs/2306.01015 ,  1744kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07889
replaced with revised version Tue, 6 Feb 2024 17:05:58 GMT   (8371kb,D)

Title: LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise
  Comparisons using Large Language Models
Authors: Adian Liusie, Potsawee Manakul, Mark J. F. Gales
Categories: cs.CL
Comments: To Appear at EACL 2024
\\ ( https://arxiv.org/abs/2307.07889 ,  8371kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16806
replaced with revised version Tue, 6 Feb 2024 05:01:11 GMT   (81kb,D)

Title: Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based
  on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating
  ASCII-Art Are Not Totally Lacking
Authors: David Bayani
Categories: cs.CL cs.AI cs.LG
Comments: Accepted in EACL 2024 as a long paper. See
  https://2024.eacl.org/program/findings-accepted/#long-papers . Note: this
  paper's ArXiv version includes additional discussion, analysis, and types of
  experiments compared to the EACL version. Changes introduced in V2 of ArXiv
  paper: only this comment metadata. V1 was initially submission on July 26th,
  2023 - release was delayed by ArXiv for a few days
\\ ( https://arxiv.org/abs/2307.16806 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07134
replaced with revised version Tue, 6 Feb 2024 03:08:44 GMT   (8673kb,D)

Title: Language is All a Graph Needs
Authors: Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: In EACL 2024
\\ ( https://arxiv.org/abs/2308.07134 ,  8673kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09687
replaced with revised version Tue, 6 Feb 2024 18:00:18 GMT   (866kb,D)

Title: Graph of Thoughts: Solving Elaborate Problems with Large Language Models
Authors: Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal
  Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert
  Niewiadomski, Piotr Nyczyk, Torsten Hoefler
Categories: cs.CL cs.AI cs.LG
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
\\ ( https://arxiv.org/abs/2308.09687 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04146
replaced with revised version Mon, 5 Feb 2024 23:28:15 GMT   (783kb,D)

Title: NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus
Authors: Kyoungyeon Cho, Seungkum Han, Young Rok Choi, Wonseok Hwang
Categories: cs.CL cs.AI
Comments: EACL 2024 System Demonstration Track
\\ ( https://arxiv.org/abs/2309.04146 ,  783kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11674
replaced with revised version Tue, 6 Feb 2024 08:03:27 GMT   (326kb,D)

Title: A Paradigm Shift in Machine Translation: Boosting Translation
  Performance of Large Language Models
Authors: Haoran Xu, Young Jin Kim, Amr Sharaf, Hany Hassan Awadalla
Categories: cs.CL
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2309.11674 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02031
replaced with revised version Tue, 6 Feb 2024 17:00:08 GMT   (12545kb,D)

Title: OceanGPT: A Large Language Model for Ocean Science Tasks
Authors: Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng,
  Huajun Chen
Categories: cs.CL cs.AI cs.CE cs.LG cs.RO
Comments: Work in progress. Project Website:
  https://zjunlp.github.io/project/OceanGPT/
\\ ( https://arxiv.org/abs/2310.02031 ,  12545kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07818
replaced with revised version Tue, 6 Feb 2024 02:24:53 GMT   (7826kb,D)

Title: On the Relationship between Sentence Analogy Identification and Sentence
  Structure Encoding in Large Language Models
Authors: Thilini Wijesiriwardene, Ruwan Wickramarachchi, Aishwarya Naresh
  Reganti, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das
Categories: cs.CL cs.AI
Comments: To appear in Findings of EACL 2024
\\ ( https://arxiv.org/abs/2310.07818 ,  7826kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18168
replaced with revised version Tue, 6 Feb 2024 09:04:04 GMT   (2004kb,D)

Title: Personas as a Way to Model Truthfulness in Language Models
Authors: Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, He He
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.18168 ,  2004kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04076
replaced with revised version Tue, 6 Feb 2024 04:16:17 GMT   (329kb,D)

Title: Do LLMs exhibit human-like response biases? A case study in survey
  design
Authors: Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar,
  Graham Neubig
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.04076 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08588
replaced with revised version Tue, 6 Feb 2024 01:21:50 GMT   (261kb,D)

Title: CodeScope: An Execution-based Multilingual Multitask Multidimensional
  Benchmark for Evaluating LLMs on Code Understanding and Generation
Authors: Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen
  Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Shuiguang Deng, Hari Sundaram
Categories: cs.CL cs.AI cs.SE
\\ ( https://arxiv.org/abs/2311.08588 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03523
replaced with revised version Tue, 6 Feb 2024 12:14:19 GMT   (489kb,D)

Title: Sig-Networks Toolkit: Signature Networks for Longitudinal Language
  Modelling
Authors: Talia Tseriotou, Ryan Sze-Yin Chan, Adam Tsakalidis, Iman Munire
  Bilal, Elena Kochkina, Terry Lyons, Maria Liakata
Categories: cs.CL
Comments: To appear in EACL 2024: System Demonstrations
\\ ( https://arxiv.org/abs/2312.03523 ,  489kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04511
replaced with revised version Tue, 6 Feb 2024 05:17:42 GMT   (736kb,D)

Title: An LLM Compiler for Parallel Function Calling
Authors: Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W.
  Mahoney, Kurt Keutzer, Amir Gholami
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.04511 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11509
replaced with revised version Tue, 6 Feb 2024 02:14:14 GMT   (399kb,D)

Title: Toward a Reinforcement-Learning-Based System for Adjusting Medication to
  Minimize Speech Disfluency
Authors: Pavlos Constas, Vikram Rawal, Matthew Honorio Oliveira, Andreas
  Constas, Aditya Khan, Kaison Cheung, Najma Sultani, Carrie Chen, Micol
  Altomare, Michael Akzam, Jiacheng Chen, Vhea He, Lauren Altomare, Heraa
  Murqi, Asad Khan, Nimit Amikumar Bhanshali, Youssef Rachad, Michael Guerzhoy
Categories: cs.CL cs.LG eess.AS
Comments: In Proc. Machine Learning for Cognitive and Mental Health Workshop
  (ML4CMH) at AAAI 2024
\\ ( https://arxiv.org/abs/2312.11509 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14215
replaced with revised version Tue, 6 Feb 2024 10:15:01 GMT   (7258kb,D)

Title: SimLM: Can Language Models Infer Parameters of Physical Systems?
Authors: Sean Memery, Mirella Lapata, Kartic Subr
Categories: cs.CL cs.AI
ACM-class: I.2.7; I.6
\\ ( https://arxiv.org/abs/2312.14215 ,  7258kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17080
replaced with revised version Tue, 6 Feb 2024 12:27:52 GMT   (1364kb,D)

Title: MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation
Authors: Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia
Categories: cs.CL
Comments: Code: https://github.com/dvlab-research/MR-GSM8K
\\ ( https://arxiv.org/abs/2312.17080 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07237
replaced with revised version Tue, 6 Feb 2024 14:50:32 GMT   (8614kb,D)

Title: Distilling Event Sequence Knowledge From Large Language Models
Authors: Somin Wadhwa, Oktie Hassanzadeh, Debarun Bhattacharjya, Ken Barker,
  Jian Ni
Categories: cs.CL cs.AI
Comments: Under Review
\\ ( https://arxiv.org/abs/2401.07237 ,  8614kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10463
replaced with revised version Tue, 6 Feb 2024 16:32:56 GMT   (8291kb,D)

Title: Critical Data Size of Language Models from a Grokking Perspective
Authors: Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.10463 ,  8291kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13227
replaced with revised version Tue, 6 Feb 2024 09:39:54 GMT   (7677kb,D)

Title: LPNL: Scalable Link Prediction with Large Language Models
Authors: Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei and Xueqi Cheng
Categories: cs.CL cs.AI cs.LG cs.SI
\\ ( https://arxiv.org/abs/2401.13227 ,  7677kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15656
replaced with revised version Tue, 6 Feb 2024 14:55:56 GMT   (241kb,D)

Title: LLsM: Generative Linguistic Steganography with Large Language Model
Authors: Yihao Wang and Ruiqi Song and Ru Zhang and Jianyi Liu and Lingxiao Li
Categories: cs.CL
Comments: 14 pages
\\ ( https://arxiv.org/abs/2401.15656 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01685
replaced with revised version Tue, 6 Feb 2024 06:03:13 GMT   (5751kb,D)

Title: SMUTF: Schema Matching Using Generative Tags and Hybrid Features
Authors: Yu Zhang, Mei Di, Haozheng Luo, Chenwei Xu, Richard Tzong-Han Tsai
Categories: cs.CL cs.AI cs.DB
\\ ( https://arxiv.org/abs/2402.01685 ,  5751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01704
replaced with revised version Tue, 6 Feb 2024 08:53:11 GMT   (3411kb,D)

Title: States as Strings as Strategies: Steering Language Models with
  Game-Theoretic Solvers
Authors: Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi,
  Luke Marris, Georgios Piliouras, Siqi Liu, Karl Tuyls
Categories: cs.CL cs.AI cs.GT
Comments: 32 pages, 8 figures, code available @
  https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py
\\ ( https://arxiv.org/abs/2402.01704 ,  3411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02380
replaced with revised version Tue, 6 Feb 2024 07:49:32 GMT   (1048kb)

Title: Evaluating Large Language Models in Analysing Classroom Dialogue
Authors: Yun Long, Haifeng Luo, Yu Zhang
Categories: cs.CL cs.AI cs.HC
\\ ( https://arxiv.org/abs/2402.02380 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02416
replaced with revised version Tue, 6 Feb 2024 18:02:01 GMT   (2206kb,D)

Title: Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
Authors: Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang,
  Xuehai Pan, Juntao Dai, Yaodong Yang
Categories: cs.CL cs.AI cs.LG
Comments: 34 pages
\\ ( https://arxiv.org/abs/2402.02416 ,  2206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02639
replaced with revised version Tue, 6 Feb 2024 02:50:48 GMT   (8035kb,D)

Title: "It's how you do things that matters": Attending to Process to Better
  Serve Indigenous Communities with Language Technologies
Authors: Ned Cooper, Courtney Heldreth, Ben Hutchinson
Categories: cs.CL
Journal-ref: Proceedings of the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024)
\\ ( https://arxiv.org/abs/2402.02639 ,  8035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02791
replaced with revised version Tue, 6 Feb 2024 05:38:26 GMT   (350kb,D)

Title: Rethinking Optimization and Architecture for Tiny Language Models
Authors: Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai,
  Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, Yunhe Wang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.02791 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03049
replaced with revised version Tue, 6 Feb 2024 02:51:23 GMT   (2686kb,D)

Title: EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models
Authors: Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue,
  Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
Categories: cs.CL cs.AI cs.HC cs.IR cs.LG
Comments: Ongoing work; the project website is at
  https://zjunlp.github.io/project/EasyInstruct, code is at
  https://github.com/zjunlp/EasyInstruct, demo is at
  https://huggingface.co/spaces/zjunlp/EasyInstruct
\\ ( https://arxiv.org/abs/2402.03049 ,  2686kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03300
replaced with revised version Tue, 6 Feb 2024 18:39:38 GMT   (3417kb,D)

Title: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
  Language Models
Authors: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song,
  Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03300 ,  3417kb)
------------------------------------------------------------------------------
\\
arXiv:2005.09218
replaced with revised version Tue, 6 Feb 2024 09:21:28 GMT   (6580kb,D)

Title: Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot
  Learning
Authors: Jia-Fong Yeh and Hsin-Ying Lee and Bing-Chen Tsai and Yi-Rong Chen and
  Ping-Chia Huang and Winston H. Hsu
Categories: cs.LG stat.ML
Comments: Full version of the CDFSL competition report (in CVPRW'20), archived
\\ ( https://arxiv.org/abs/2005.09218 ,  6580kb)
------------------------------------------------------------------------------
\\
arXiv:2010.05784
replaced with revised version Tue, 6 Feb 2024 03:53:05 GMT   (5613kb,D)

Title: Learning Calibrated Uncertainties for Domain Shift: A Distributionally
  Robust Learning Approach
Authors: Haoxuan Wang, Zhiding Yu, Yisong Yue, Anima Anandkumar, Anqi Liu,
  Junchi Yan
Categories: cs.LG cs.CV
Comments: IJCAI 2023
\\ ( https://arxiv.org/abs/2010.05784 ,  5613kb)
------------------------------------------------------------------------------
\\
arXiv:2011.05530
replaced with revised version Tue, 6 Feb 2024 08:29:25 GMT   (3211kb,D)

Title: On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU
  Networks
Authors: Ramy E. Ali, Jinhyun So, A. Salman Avestimehr
Categories: cs.LG cs.CR cs.IT math.IT
\\ ( https://arxiv.org/abs/2011.05530 ,  3211kb)
------------------------------------------------------------------------------
\\
arXiv:2011.11877
replaced with revised version Tue, 6 Feb 2024 03:14:09 GMT   (267kb,D)

Title: InstaHide's Sample Complexity When Mixing Two Private Images
Authors: Baihe Huang, Zhao Song, Runzhou Tao, Junze Yin, Ruizhe Zhang, Danyang
  Zhuo
Categories: cs.LG cs.CC cs.CR cs.DS stat.ML
\\ ( https://arxiv.org/abs/2011.11877 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2106.07106
replaced with revised version Mon, 5 Feb 2024 20:06:39 GMT   (1189kb,D)

Title: Alignment and Comparison of Directed Networks via Transition Couplings
  of Random Walks
Authors: Bongsoo Yi, Kevin O'Connor, Kevin McGoff, Andrew B. Nobel
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2106.07106 ,  1189kb)
------------------------------------------------------------------------------
\\
arXiv:2108.08677
replaced with revised version Tue, 6 Feb 2024 12:06:44 GMT   (450kb,D)

Title: Order Optimal Bounds for One-Shot Federated Learning over non-Convex
  Loss Functions
Authors: Arsalan Sharifnassab, Saber Salehkaleybar, S. Jamaloddin Golestani
Categories: cs.LG
\\ ( https://arxiv.org/abs/2108.08677 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2205.15059
replaced with revised version Tue, 6 Feb 2024 10:58:38 GMT   (11170kb,D)

Title: Hilbert Curve Projection Distance for Distribution Comparison
Authors: Tao Li, Cheng Meng, Hongteng Xu, Jun Yu
Categories: cs.LG stat.ML
Comments: 33 pages, 11 figures
\\ ( https://arxiv.org/abs/2205.15059 ,  11170kb)
------------------------------------------------------------------------------
\\
arXiv:2208.06956
replaced with revised version Tue, 6 Feb 2024 02:45:28 GMT   (498kb,D)

Title: ARIEL: Adversarial Graph Contrastive Learning
Authors: Shengyu Feng, Baoyu Jing, Yada Zhu, Hanghang Tong
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2202.06491
\\ ( https://arxiv.org/abs/2208.06956 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2208.14966
replaced with revised version Mon, 5 Feb 2024 21:27:45 GMT   (2249kb,D)

Title: Concept Gradient: Concept-based Interpretation Without Linear Assumption
Authors: Andrew Bai, Chih-Kuan Yeh, Pradeep Ravikumar, Neil Y. C. Lin, Cho-Jui
  Hsieh
Categories: cs.LG
Comments: 21 pages, 7 figures, published in ICLR 2023
\\ ( https://arxiv.org/abs/2208.14966 ,  2249kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07481
replaced with revised version Tue, 6 Feb 2024 13:35:14 GMT   (643kb,D)

Title: Variational Representations of Annealing Paths: Bregman Information
  under Monotonic Embedding
Authors: Rob Brekelmans, Frank Nielsen
Categories: cs.LG cs.IT math.IT math.ST stat.ML stat.TH
Comments: Published in Information Geometry (Info. Geo. 2024)
DOI: 10.1007/s41884-023-00129-6
\\ ( https://arxiv.org/abs/2209.07481 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00487
replaced with revised version Tue, 6 Feb 2024 09:12:09 GMT   (4885kb,D)

Title: A Comprehensive Survey of Continual Learning: Theory, Method and
  Application
Authors: Liyuan Wang, Xingxing Zhang, Hang Su, Jun Zhu
Categories: cs.LG cs.AI cs.CV
Comments: The concise version is in IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI)
\\ ( https://arxiv.org/abs/2302.00487 ,  4885kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06014
replaced with revised version Tue, 6 Feb 2024 16:08:10 GMT   (60kb)

Title: Online Recommendations for Agents with Discounted Adaptive Preferences
Authors: Arpit Agarwal, William Brown
Categories: cs.LG cs.GT cs.IR
Comments: Updates for camera-ready version (ALT 2024)
\\ ( https://arxiv.org/abs/2302.06014 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11861
replaced with revised version Tue, 6 Feb 2024 06:04:08 GMT   (2875kb,D)

Title: Out-of-Domain Robustness via Targeted Augmentations
Authors: Irena Gao, Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, Percy
  Liang
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2302.11861 ,  2875kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05099
replaced with revised version Tue, 6 Feb 2024 15:03:21 GMT   (1522kb,D)

Title: Feudal Graph Reinforcement Learning
Authors: Tommaso Marzi, Arshjot Khehra, Andrea Cini, Cesare Alippi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.05099 ,  1522kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14621
replaced with revised version Tue, 6 Feb 2024 04:05:34 GMT   (914kb,D)

Title: MUDiff: Unified Diffusion for Complete Molecule Generation
Authors: Chenqing Hua, Sitao Luan, Minkai Xu, Rex Ying, Jie Fu, Stefano Ermon,
  Doina Precup
Categories: cs.LG q-bio.BM
\\ ( https://arxiv.org/abs/2304.14621 ,  914kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14989
replaced with revised version Tue, 6 Feb 2024 06:28:00 GMT   (872kb,D)

Title: Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded
  Rewards
Authors: Hao Qin, Kwang-Sung Jun and Chicheng Zhang
Categories: cs.LG stat.ML
Comments: Accepted by NeurIPS 2023
\\ ( https://arxiv.org/abs/2304.14989 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05465
replaced with revised version Tue, 6 Feb 2024 07:30:11 GMT   (28355kb,D)

Title: The emergence of clusters in self-attention dynamics
Authors: Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet
Categories: cs.LG math.AP stat.ML
\\ ( https://arxiv.org/abs/2305.05465 ,  28355kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13865
replaced with revised version Tue, 6 Feb 2024 07:35:41 GMT   (7228kb,D)

Title: Selective Pre-training for Private Fine-tuning
Authors: Da Yu, Sivakanth Gopi, Janardhan Kulkarni, Zinan Lin, Saurabh Naik,
  Tomasz Lukasz Religa, Jian Yin, Huishuai Zhang
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2305.13865 ,  7228kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15611
replaced with revised version Tue, 6 Feb 2024 04:15:14 GMT   (23031kb,D)

Title: Size Generalization of Graph Neural Networks on Biological Data:
  Insights and Practices from the Spectral Perspective
Authors: Gaotang Li, Yujun Yan, Danai Koutra
Categories: cs.LG cs.AI
Comments: 21 pages, including appendix
\\ ( https://arxiv.org/abs/2305.15611 ,  23031kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17326
replaced with revised version Tue, 6 Feb 2024 08:17:33 GMT   (1878kb,D)

Title: Matrix Information Theory for Self-Supervised Learning
Authors: Yifan Zhang, Zhiquan Tan, Jingqin Yang, Weiran Huang, Yang Yuan
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2305.17326 ,  1878kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19190
replaced with revised version Tue, 6 Feb 2024 13:19:57 GMT   (1121kb,D)

Title: Inverse Approximation Theory for Nonlinear Recurrent Neural Networks
Authors: Shida Wang, Zhong Li and Qianxiao Li
Categories: cs.LG cs.AI math.DS
\\ ( https://arxiv.org/abs/2305.19190 ,  1121kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19600
replaced with revised version Tue, 6 Feb 2024 08:45:27 GMT   (705kb,D)

Title: Adaptive Self-Distillation for Minimizing Client Drift in Heterogeneous
  Federated Learning
Authors: M.Yashwanth, Gaurav Kumar Nayak, Arya Singh, Yogesh Simmhan, Anirban
  Chakraborty
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.19600 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01474
replaced with revised version Tue, 6 Feb 2024 02:19:39 GMT   (3945kb,D)

Title: Generalist Equivariant Transformer Towards 3D Molecular Interaction
  Learning
Authors: Xiangzhe Kong, Wenbing Huang, Yang Liu
Categories: cs.LG q-bio.BM
Comments: NeurIPS 2023 AI4D3 workshop
\\ ( https://arxiv.org/abs/2306.01474 ,  3945kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00405
replaced with revised version Tue, 6 Feb 2024 17:56:03 GMT   (46kb)

Title: Provably Efficient UCB-type Algorithms For Learning Predictive State
  Representations
Authors: Ruiquan Huang, Yingbin Liang, Jing Yang
Categories: cs.LG stat.ML
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2307.00405 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16847
replaced with revised version Tue, 6 Feb 2024 05:44:33 GMT   (2553kb,D)

Title: CroSSL: Cross-modal Self-Supervised Learning for Time-series through
  Latent Masking
Authors: Shohreh Deldari, Dimitris Spathis, Mohammad Malekzadeh, Fahim Kawsar,
  Flora Salim, Akhil Mathur
Categories: cs.LG
Comments: Accepted in WSDM24. Short version presented in ML4MHD @ICML23
\\ ( https://arxiv.org/abs/2307.16847 ,  2553kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06911
replaced with revised version Tue, 6 Feb 2024 09:03:53 GMT   (1735kb,D)

Title: GIT-Mol: A Multi-modal Large Language Model for Molecular Science with
  Graph, Image, and Text
Authors: Pengfei Liu, Yiming Ren, Jun Tao and Zhixiang Ren
Categories: cs.LG cs.CL q-bio.BM
Comments: The article has been accepted by Computers in Biology and Medicine,
  with 14 pages and 4 figures
Journal-ref: Computers in Biology and Medicine, 108073, 2024, ISSN 0010-4825
DOI: 10.1016/j.compbiomed.2024.108073
\\ ( https://arxiv.org/abs/2308.06911 ,  1735kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11129
replaced with revised version Tue, 6 Feb 2024 05:17:37 GMT   (4013kb,D)

Title: Enhancing Graph Transformers with Hierarchical Distance Structural
  Encoding
Authors: Yuankai Luo
Categories: cs.LG cs.AI cs.SI
\\ ( https://arxiv.org/abs/2308.11129 ,  4013kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13111
replaced with revised version Mon, 5 Feb 2024 21:16:52 GMT   (644kb,D)

Title: Bayesian Low-rank Adaptation for Large Language Models
Authors: Adam X. Yang, Maxime Robeyns, Xi Wang, Laurence Aitchison
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.13111 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15812
replaced with revised version Mon, 5 Feb 2024 19:59:46 GMT   (715kb,D)

Title: Peering Through Preferences: Unraveling Feedback Acquisition for
  Aligning Large Language Models
Authors: Hritik Bansal, John Dang, Aditya Grover
Categories: cs.LG cs.AI cs.CL
Comments: 31 pages, Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2308.15812 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01945
replaced with revised version Tue, 6 Feb 2024 16:07:23 GMT   (4496kb,D)

Title: OHQ: On-chip Hardware-aware Quantization
Authors: Wei Huang, Haotong Qin, Yangdong Liu, Jingzhuo Liang, Yulun Zhang,
  Ying Li, Xianglong Liu
Categories: cs.LG cs.AI cs.AR
Comments: 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2309.01945 ,  4496kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02908
replaced with revised version Tue, 6 Feb 2024 15:37:11 GMT   (912kb,D)

Title: DECODE: Data-driven Energy Consumption Prediction leveraging Historical
  Data and Environmental Factors in Buildings
Authors: Aditya Mishra, Haroon R. Lone, Aayush Mishra
Categories: cs.LG cs.AI
Comments: 10 pages, 7 figures, 7 tables
DOI: 10.1016/j.enbuild.2024.113950
\\ ( https://arxiv.org/abs/2309.02908 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08249
replaced with revised version Tue, 6 Feb 2024 14:42:41 GMT   (3437kb,D)

Title: Deep Nonnegative Matrix Factorization with Beta Divergences
Authors: Valentin Leplat, Le Thi Khanh Hien, Akwum Onwunta, Nicolas Gillis
Categories: cs.LG cs.NA eess.SP math.NA stat.ML
Comments: 32 pages. We have improved the presentation of the paper, and added
  numerical experiments for beta=3/2 with 4 layers on the CBCL data set
\\ ( https://arxiv.org/abs/2309.08249 ,  3437kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08499
replaced with revised version Mon, 5 Feb 2024 23:04:53 GMT   (477kb,D)

Title: P-ROCKET: Pruning Random Convolution Kernels for Time Series
  Classification from a Feature Selection Perspective
Authors: Shaowu Chen, Weize Sun, Lei Huang, Xiaopeng Li, Qingyuan Wang, Deepu
  John
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.08499 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16883
replaced with revised version Tue, 6 Feb 2024 14:30:26 GMT   (1200kb,D)

Title: The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing
Authors: Blaise Delattre, Alexandre Araujo, Quentin Barth\'elemy and Alexandre
  Allauzen
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2309.16883 ,  1200kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00344
replaced with revised version Tue, 6 Feb 2024 13:40:35 GMT   (1135kb,D)

Title: HarmonyDream: Task Harmonization Inside World Models
Authors: Haoyu Ma, Jialong Wu, Ningya Feng, Chenjun Xiao, Dong Li, Jianye Hao,
  Jianmin Wang, Mingsheng Long
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.00344 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00526
replaced with revised version Tue, 6 Feb 2024 01:45:10 GMT   (2007kb,D)

Title: Are Graph Neural Networks Optimal Approximation Algorithms?
Authors: Morris Yau, Eric Lu, Nikolaos Karalias, Jessica Xu, Stefanie Jegelka
Categories: cs.LG cs.AI cs.DM cs.DS
Comments: Updated references, fixed more typos and wording issues
\\ ( https://arxiv.org/abs/2310.00526 ,  2007kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01105
replaced with revised version Tue, 6 Feb 2024 06:57:02 GMT   (10994kb,D)

Title: Energy-Guided Continuous Entropic Barycenter Estimation for General
  Costs
Authors: Alexander Kolesov, Petr Mokrov, Igor Udovichenko, Milena Gazdieva,
  Gudmund Pammer, Anastasis Kratsios, Evgeny Burnaev, Alexander Korotin
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.01105 ,  10994kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02505
replaced with revised version Mon, 5 Feb 2024 23:29:03 GMT   (12771kb,D)

Title: Learning to Reach Goals via Diffusion
Authors: Vineet Jain and Siamak Ravanbakhsh
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.02505 ,  12771kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03253
replaced with revised version Mon, 5 Feb 2024 20:51:25 GMT   (613kb,D)

Title: Molecule Design by Latent Prompt Transformer
Authors: Deqian Kong, Yuhao Huang, Jianwen Xie, Ying Nian Wu
Categories: cs.LG q-bio.BM stat.ML
\\ ( https://arxiv.org/abs/2310.03253 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18948
replaced with revised version Tue, 6 Feb 2024 18:56:18 GMT   (22097kb,D)

Title: Building a Safer Maritime Environment Through Multi-Path Long-Term
  Vessel Trajectory Forecasting
Authors: Gabriel Spadon, Jay Kumar, Matthew Smith, Sarah Vela, Romina Gehrmann,
  Derek Eden, Joshua van Berkel, Amilcar Soares, Ronan Fablet, Ronald Pelot,
  Stan Matwin
Categories: cs.LG cs.AI cs.DM math.PR
\\ ( https://arxiv.org/abs/2310.18948 ,  22097kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19218
replaced with revised version Tue, 6 Feb 2024 05:33:51 GMT   (1119kb,D)

Title: A Survey of Federated Unlearning: A Taxonomy, Challenges and Future
  Directions
Authors: Yang Zhao, Jiaxi Yang, Yiling Tao, Lixu Wang, Xiaoxiao Li, Dusit
  Niyato
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.19218 ,  1119kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19391
replaced with revised version Tue, 6 Feb 2024 10:25:37 GMT   (1457kb,D)

Title: Causal Fair Metric: Bridging Causality, Individual Fairness, and
  Adversarial Robustness
Authors: Ahmad-Reza Ehyaei, Golnoosh Farnadi, Samira Samadi
Categories: cs.LG cs.CY
\\ ( https://arxiv.org/abs/2310.19391 ,  1457kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00768
replaced with revised version Tue, 6 Feb 2024 16:33:48 GMT   (1794kb,D)

Title: Language Model Training Paradigms for Clinical Feature Embeddings
Authors: Yurong Hu, Manuel Burger, Gunnar R\"atsch, Rita Kuznetsova
Categories: cs.LG cs.CL
Comments: Poster at "NeurIPS 2023 Workshop: Self-Supervised Learning - Theory
  and Practice"
\\ ( https://arxiv.org/abs/2311.00768 ,  1794kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03191
replaced with revised version Tue, 6 Feb 2024 07:24:34 GMT   (9347kb,D)

Title: DeepInception: Hypnotize Large Language Model to Be Jailbreaker
Authors: Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo
  Han
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2311.03191 ,  9347kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03615
replaced with revised version Tue, 6 Feb 2024 02:56:12 GMT   (2774kb,D)

Title: CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data
  Centers
Authors: Jieming Bian, Lei Wang, Shaolei Ren, Jie Xu
Categories: cs.LG cs.DC
Comments: Preprint, Experiments Updated
\\ ( https://arxiv.org/abs/2311.03615 ,  2774kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09386
replaced with revised version Tue, 6 Feb 2024 03:42:12 GMT   (13984kb,D)

Title: Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction
Authors: Bahram Yaghooti, Netanel Raviv, Bruno Sinopoli
Categories: cs.LG cs.IT math.IT
\\ ( https://arxiv.org/abs/2311.09386 ,  13984kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12997
replaced with revised version Mon, 5 Feb 2024 23:29:12 GMT   (8805kb,D)

Title: Compositional Capabilities of Autoregressive Transformers: A Study on
  Synthetic, Interpretable Tasks
Authors: Rahul Ramesh, Ekdeep Singh Lubana, Mikail Khona, Robert P. Dick,
  Hidenori Tanaka
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.12997 ,  8805kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01037
replaced with revised version Tue, 6 Feb 2024 08:28:04 GMT   (1724kb,D)

Title: Eliciting Latent Knowledge from Quirky Language Models
Authors: Alex Mallen and Nora Belrose
Categories: cs.LG cs.AI cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2312.01037 ,  1724kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03414
replaced with revised version Tue, 6 Feb 2024 05:53:02 GMT   (266kb,D)

Title: Compressed Context Memory For Online Language Model Interaction
Authors: Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh Song
Categories: cs.LG cs.CL
Comments: ICLR 2024. Add streaming setting results and training set analyses
\\ ( https://arxiv.org/abs/2312.03414 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07335
replaced with revised version Tue, 6 Feb 2024 12:39:18 GMT   (4505kb,D)

Title: Momentum Particle Maximum Likelihood
Authors: Jen Ning Lim, Juan Kuntz, Samuel Power, Adam M. Johansen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.07335 ,  4505kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08846
replaced with revised version Tue, 6 Feb 2024 02:21:10 GMT   (8358kb,D)

Title: TiMix: Text-aware Image Mixing for Effective Vision-Language
  Pre-training
Authors: Chaoya Jiang, Wei ye, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang,
  Shikun Zhang
Categories: cs.LG cs.CL cs.CV
Comments: Accepted on AAAI2024
\\ ( https://arxiv.org/abs/2312.08846 ,  8358kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10794
replaced with revised version Tue, 6 Feb 2024 08:06:11 GMT   (6761kb,D)

Title: A mathematical perspective on Transformers
Authors: Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, Philippe Rigollet
Categories: cs.LG math.AP math.DS
\\ ( https://arxiv.org/abs/2312.10794 ,  6761kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12044
replaced with revised version Tue, 6 Feb 2024 09:32:36 GMT   (895kb,D)

Title: XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX
Authors: Alexander Nikulin, Vladislav Kurenkov, Ilya Zisman, Artem Agarkov,
  Viacheslav Sinii, Sergey Kolesnikov
Categories: cs.LG
Comments: NeurIPS 2023, Workshop, Source code:
  https://github.com/corl-team/xland-minigrid
\\ ( https://arxiv.org/abs/2312.12044 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15122
replaced with revised version Tue, 6 Feb 2024 00:07:19 GMT   (822kb,D)

Title: Scaling Is All You Need: Autonomous Driving with JAX-Accelerated
  Reinforcement Learning
Authors: Moritz Harmel, Anubhav Paras, Andreas Pasternak, Gary Linscott
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2312.15122 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02225
replaced with revised version Tue, 6 Feb 2024 03:13:43 GMT   (1694kb,D)

Title: Trajectory-Oriented Policy Optimization with Sparse Rewards
Authors: Guojian Wang, Faguo Wu, Xiao Zhang
Categories: cs.LG
Comments: 6 pages, 7 figures
\\ ( https://arxiv.org/abs/2401.02225 ,  1694kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03301
replaced with revised version Tue, 6 Feb 2024 18:08:40 GMT   (89kb,D)

Title: On Sample-Efficient Offline Reinforcement Learning: Data Diversity,
  Posterior Sampling, and Beyond
Authors: Thanh Nguyen-Tang and Raman Arora
Categories: cs.LG cs.AI stat.ML
Comments: NeurIPS'23; Arxiv is the authors' preferred version; v2: add a
  missing related work
\\ ( https://arxiv.org/abs/2401.03301 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06118
replaced with revised version Tue, 6 Feb 2024 18:55:25 GMT   (2993kb,D)

Title: Extreme Compression of Large Language Models via Additive Quantization
Authors: Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar,
  Artem Babenko, Dan Alistarh
Categories: cs.LG cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2401.06118 ,  2993kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07844
replaced with revised version Tue, 6 Feb 2024 01:41:41 GMT   (69kb)

Title: The ODE Method for Stochastic Approximation and Reinforcement Learning
  with Markovian Noise
Authors: Shuze Liu, Shuhang Chen, Shangtong Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.07844 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10119
replaced with revised version Tue, 6 Feb 2024 16:36:40 GMT   (477kb,D)

Title: Towards Principled Graph Transformers
Authors: Luis M\"uller and Daniel Kusuma and Christopher Morris
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.10119 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14199
replaced with revised version Tue, 6 Feb 2024 01:43:58 GMT   (551kb,D)

Title: MTRGL:Effective Temporal Correlation Discerning through Multi-modal
  Temporal Relational Graph Learning
Authors: Junwei Su, Shan Wu, Jinhui Li
Categories: cs.LG econ.GN q-fin.EC q-fin.TR
\\ ( https://arxiv.org/abs/2401.14199 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14422
replaced with revised version Tue, 6 Feb 2024 08:31:40 GMT   (1107kb,D)

Title: Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar
  Power Generation
Authors: Md Shazid Islam, A S M Jahid Hasan, Md Saydur Rahman, Jubair Yusuf, Md
  Saiful Islam Sajol, Farhana Akter Tumpa
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.14422 ,  1107kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15270
replaced with revised version Mon, 5 Feb 2024 22:22:49 GMT   (1741kb,D)

Title: SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models
Authors: Zhihao Wang, Yiqun Xie, Zhili Li, Xiaowei Jia, Zhe Jiang, Aolin Jia,
  Shuo Xu
Categories: cs.LG cs.AI cs.CY
Comments: Accepted to AAAI 2024 (preprint)
\\ ( https://arxiv.org/abs/2401.15270 ,  1741kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00326
replaced with revised version Mon, 5 Feb 2024 22:07:14 GMT   (2785kb,D)

Title: PirateNets: Physics-informed Deep Learning with Residual Adaptive
  Networks
Authors: Sifan Wang, Bowen Li, Yuhan Chen, Paris Perdikaris
Categories: cs.LG cs.NA math.NA
Comments: 29 Pages, 15 Figures, 8 Tables
\\ ( https://arxiv.org/abs/2402.00326 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00522
replaced with revised version Tue, 6 Feb 2024 07:34:41 GMT   (95kb)

Title: Understanding the Expressive Power and Mechanisms of Transformer for
  Sequence Modeling
Authors: Mingze Wang, Weinan E
Categories: cs.LG stat.ML
Comments: 65 pages
\\ ( https://arxiv.org/abs/2402.00522 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00809
replaced with revised version Tue, 6 Feb 2024 17:48:56 GMT   (134kb,D)

Title: Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
Authors: Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence
  Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin,
  Philipp Hennig, Jose Miguel Hernandez Lobato, Aliaksandr Hubin, Alexander
  Immer, Theofanis Karaletsos, Mohammad Emtiyaz Khan, Agustinus Kristiadi,
  Yingzhen Li, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim G. J.
  Rudner, David R\"ugamer, Yee Whye Teh, Max Welling, Andrew Gordon Wilson,
  Ruqi Zhang
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.00809 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00854
replaced with revised version Mon, 5 Feb 2024 21:22:18 GMT   (4816kb,D)

Title: SymbolicAI: A framework for logic-based approaches combining generative
  models and solvers
Authors: Marius-Constantin Dinu and Claudiu Leoveanu-Condrei and Markus
  Holzleitner and Werner Zellinger and Sepp Hochreiter
Categories: cs.LG cs.AI cs.SC cs.SE
Comments: 39 pages, 12 figures, external resources: framework is available at
  https://github.com/ExtensityAI/symbolicai and benchmark at
  https://github.com/ExtensityAI/benchmark
\\ ( https://arxiv.org/abs/2402.00854 ,  4816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00899
replaced with revised version Tue, 6 Feb 2024 15:27:03 GMT   (529kb,D)

Title: Weakly Supervised Learners for Correction of AI Errors with Provable
  Performance Guarantees
Authors: Ivan Y. Tyukin, Tatiana Tyukina, Daniel van Helden, Zedong Zheng,
  Evgeny M. Mirkes, Oliver J. Sutton, Qinghua Zhou, Alexander N. Gorban,
  Penelope Allison
Categories: cs.LG cs.AI stat.ML
MSC-class: 68T05, 68T37
\\ ( https://arxiv.org/abs/2402.00899 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01801
replaced with revised version Tue, 6 Feb 2024 05:34:17 GMT   (194kb,D)

Title: Large Language Models for Time Series: A Survey
Authors: Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh K. Gupta, Jingbo Shang
Categories: cs.LG cs.AI cs.CL
Comments: GitHub repository:
  https://github.com/xiyuanzh/awesome-llm-time-series
\\ ( https://arxiv.org/abs/2402.01801 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01881
replaced with revised version Tue, 6 Feb 2024 15:03:09 GMT   (2099kb,D)

Title: Large Language Model Agent for Hyper-Parameter Optimization
Authors: Siyi Liu, Chen Gao, Yong Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.01881 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01960
replaced with revised version Tue, 6 Feb 2024 04:34:47 GMT   (2751kb,D)

Title: Calibrated Uncertainty Quantification for Operator Learning via
  Conformal Prediction
Authors: Ziqi Ma, Kamyar Azizzadenesheli, Anima Anandkumar
Categories: cs.LG
Comments: 14 pages, 7 figures
\\ ( https://arxiv.org/abs/2402.01960 ,  2751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01965
replaced with revised version Tue, 6 Feb 2024 01:37:23 GMT   (1429kb,D)

Title: Analyzing Neural Network-Based Generative Diffusion Models through
  Convex Optimization
Authors: Fangzhao Zhang, Mert Pilanci
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2402.01965 ,  1429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01969
replaced with revised version Tue, 6 Feb 2024 03:22:32 GMT   (4653kb,D)

Title: Simulation-Enhanced Data Augmentation for Machine Learning Pathloss
  Prediction
Authors: Ahmed P. Mohamed, Byunghyun Lee, Yaguang Zhang, Max Hollingsworth, C.
  Robert Anderson, James V. Krogmeier, David J. Love
Categories: cs.LG eess.SP
Comments: 6 pages, 5 figures, Accepted at ICC 2024
\\ ( https://arxiv.org/abs/2402.01969 ,  4653kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02018
replaced with revised version Tue, 6 Feb 2024 15:47:26 GMT   (371kb,D)

Title: The Landscape and Challenges of HPC Research and LLMs
Authors: Le Chen, Nesreen K. Ahmed, Akash Dutta, Arijit Bhattacharjee, Sixing
  Yu, Quazi Ishtiaque Mahmud, Waqwoya Abebe, Hung Phan, Aishwarya Sarkar,
  Branden Butler, Niranjan Hasabnis, Gal Oren, Vy A. Vo, Juan Pablo Munoz,
  Theodore L. Willke, Tim Mattson, Ali Jannesari
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.02018 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02229
replaced with revised version Tue, 6 Feb 2024 09:51:09 GMT   (7705kb,D)

Title: Vanilla Bayesian Optimization Performs Great in High Dimensions
Authors: Carl Hvarfner and Erik Orm Hellsten and Luigi Nardi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.02229 ,  7705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02277
replaced with revised version Tue, 6 Feb 2024 01:54:52 GMT   (1747kb,D)

Title: Causal Bayesian Optimization via Exogenous Distribution Learning
Authors: Shaogang Ren, Xiaoning Qian
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.02277 ,  1747kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02322
replaced with revised version Tue, 6 Feb 2024 01:49:08 GMT   (1359kb,D)

Title: Dynamic Incremental Optimization for Best Subset Selection
Authors: Shaogang Ren, Xiaoning Qian
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2207.02058
\\ ( https://arxiv.org/abs/2402.02322 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02441
replaced with revised version Tue, 6 Feb 2024 17:53:31 GMT   (33kb,D)

Title: TopoX: A Suite of Python Packages for Machine Learning on Topological
  Domains
Authors: Mustafa Hajij, Mathilde Papillon, Florian Frantzen, Jens Agerberg,
  Ibrahem AlJabea, Ruben Ballester, Claudio Battiloro, Guillermo Bern\'ardez,
  Tolga Birdal, Aiden Brent, Peter Chin, Sergio Escalera, Odin Hoff Gardaa,
  Gurusankar Gopalakrishnan, Devendra Govil, Josef Hoppe, Maneel Reddy Karri,
  Jude Khouja, Manuel Lecha, Neal Livesay, Jan Mei{\ss}ner, Soham Mukherjee,
  Alexander Nikitin, Theodore Papamarkou, Jaro Pr\'ilepok, Karthikeyan Natesan
  Ramamurthy, Paul Rosen, Aldo Guzm\'an-S\'aenz, Alessandro Salatiello, Shreyas
  N. Samaga, Michael T. Schaub, Luca Scofano, Indro Spinelli, Lev Telyatnikov,
  Quang Truong, Robin Walters, Maosheng Yang, Olga Zaghen, Ghada Zamzmi, Ali
  Zia, Nina Miolane
Categories: cs.LG cs.AI stat.CO
\\ ( https://arxiv.org/abs/2402.02441 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02772
replaced with revised version Tue, 6 Feb 2024 17:49:44 GMT   (15134kb,D)

Title: Contrastive Diffuser: Planning Towards High Return States via
  Contrastive Learning
Authors: Yixiang Shan, Zhengbang Zhu, Ting Long, Qifan Liang, Yi Chang, Weinan
  Zhang, Liang Yin
Categories: cs.LG
Comments: 13 pages with appendix and references, 10 figures, 3 tables
\\ ( https://arxiv.org/abs/2402.02772 ,  15134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03021
replaced with revised version Tue, 6 Feb 2024 12:50:12 GMT   (1395kb,D)

Title: Data-induced multiscale losses and efficient multirate gradient descent
  schemes
Authors: Juncai He, Liangchen Liu, and Yen-Hsi Richard Tsai
Categories: cs.LG cs.NA math.NA
Comments: 28 pages, 4 figures, submitted under review
MSC-class: 65F10, 65F45, 68T07
ACM-class: G.1.6; I.2.6
\\ ( https://arxiv.org/abs/2402.03021 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2106.02926
replaced with revised version Tue, 6 Feb 2024 13:38:40 GMT   (2516kb)

Title: IM-META: Influence Maximization Using Node Metadata in Networks With
  Unknown Topology
Authors: Cong Tran, Won-Yong Shin, Andreas Spitz
Categories: cs.SI cs.AI cs.IT cs.LG cs.NE math.IT
Comments: 14 pages, 11 figures, 4 tables, to appear in the IEEE Transactions on
  Network Science and Engineering (Please cite our journal version that will
  appear in an upcoming issue.)
\\ ( https://arxiv.org/abs/2106.02926 ,  2516kb)
------------------------------------------------------------------------------
\\
arXiv:2207.06294 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 22:04:42 GMT   (1066kb,D)

Title: Reinforcement Learning Assisted Recursive QAOA
Authors: Yash J. Patel, Sofiene Jerbi, Thomas B\"ack, Vedran Dunjko
Categories: quant-ph cs.AI cs.LG
Comments: 17 pages, 6 figures. EPJ Quantum Technology journal version
Journal-ref: EPJ Quantum Technol. 11, 6 (2024)
DOI: 10.1140/epjqt/s40507-023-00214-w
\\ ( https://arxiv.org/abs/2207.06294 ,  1066kb)
------------------------------------------------------------------------------
\\
arXiv:2210.10718
replaced with revised version Tue, 6 Feb 2024 06:01:27 GMT   (1133kb,D)

Title: Whole Page Unbiased Learning to Rank
Authors: Haitao Mao, Lixin Zou, Yujia Zheng, Jiliang Tang, Xiaokai Chu, Jiashu
  Zhao, Qian Wang, Dawei Yin
Categories: cs.IR cs.AI
Comments: 12 pages, 5 figures
\\ ( https://arxiv.org/abs/2210.10718 ,  1133kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06108
replaced with revised version Tue, 6 Feb 2024 12:41:20 GMT   (11158kb,D)

Title: RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object
  Detection Systems
Authors: Yanlong Yang, Jianan Liu, Tao Huang, Qing-Long Han, Gang Ma and Bing
  Zhu
Categories: cs.CV cs.AI cs.LG
Comments: 14 pages, 6 figures
\\ ( https://arxiv.org/abs/2211.06108 ,  11158kb)
------------------------------------------------------------------------------
\\
arXiv:2303.18013
replaced with revised version Mon, 5 Feb 2024 22:46:06 GMT   (8793kb,D)

Title: LaCViT: A Label-aware Contrastive Fine-tuning Framework for Vision
  Transformers
Authors: Zijun Long, Zaiqiao Meng, Gerardo Aragon Camarasa, Richard McCreadie
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2303.18013 ,  8793kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14330
replaced with revised version Tue, 6 Feb 2024 18:44:30 GMT   (24926kb,D)

Title: DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot
  Text-to-Video Generation
Authors: Susung Hong, Junyoung Seo, Heeseong Shin, Sunghwan Hong, Seungryong
  Kim
Categories: cs.CV cs.AI cs.CL
Comments: The code and demo will be available at
  https://github.com/KU-CVLAB/DirecT2V
\\ ( https://arxiv.org/abs/2305.14330 ,  24926kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03933 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 18:05:54 GMT   (427kb,D)

Title: High-dimensional and Permutation Invariant Anomaly Detection
Authors: Vinicius Mikuni, Benjamin Nachman
Categories: hep-ph cs.AI cs.LG hep-ex
Comments: 7 pages, 5 figures
\\ ( https://arxiv.org/abs/2306.03933 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07286
replaced with revised version Tue, 6 Feb 2024 12:02:40 GMT   (2919kb,D)

Title: One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton
  Matching
Authors: Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot
Categories: cs.CV cs.AI
Comments: 8 pages, 4 figures, 6 tables. Accepted by IEEE Transactions on
  Pattern Analysis and Machine Intelligence
\\ ( https://arxiv.org/abs/2307.07286 ,  2919kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11776
replaced with revised version Tue, 6 Feb 2024 04:31:11 GMT   (715kb)

Title: WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation
  on Surgical Videos with Unknown Camera Parameters
Authors: Ange Lou and Jack Noble
Categories: cs.CV cs.AI eess.IV
Comments: Accepted by SPIE 2024
\\ ( https://arxiv.org/abs/2308.11776 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01516
replaced with revised version Mon, 5 Feb 2024 22:43:45 GMT   (2470kb,D)

Title: MultiWay-Adapater: Adapting large-scale multi-modal models for scalable
  image-text retrieval
Authors: Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa
Categories: cs.CV cs.AI cs.LG cs.MM
\\ ( https://arxiv.org/abs/2309.01516 ,  2470kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09844
replaced with revised version Tue, 6 Feb 2024 17:53:02 GMT   (24482kb,D)

Title: CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs
Authors: George Drayson, Efimia Panagiotaki, Daniel Omeiza, Lars Kunze
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: The first two authors contributed equally to this work
ACM-class: I.2.4; I.2.6; I.2.9; I.2.10; I.6.3; I.6.4; I.4.8
\\ ( https://arxiv.org/abs/2309.09844 ,  24482kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00793
replaced with revised version Tue, 6 Feb 2024 06:00:53 GMT   (261kb,D)

Title: Revisiting Link Prediction: A Data Perspective
Authors: Haitao Mao, Juanhui Li, Harry Shomer, Bingheng Li, Wenqi Fan, Yao Ma,
  Tong Zhao, Neil Shah, Jiliang Tang
Categories: cs.SI cs.AI
Comments: 36 pages, 12 figures
\\ ( https://arxiv.org/abs/2310.00793 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04055
replaced with revised version Mon, 5 Feb 2024 23:27:51 GMT   (5446kb,D)

Title: Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in
  Federated Learning
Authors: Shanshan Han, Wenxuan Wu, Baturalp Buyukates, Weizhao Jin, Qifan
  Zhang, Yuhang Yao, Salman Avestimehr, Chaoyang He
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2310.04055 ,  5446kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07975
replaced with revised version Tue, 6 Feb 2024 14:40:09 GMT   (363kb,D)

Title: Self-supervised visual learning for analyzing firearms trafficking
  activities on the Web
Authors: Sotirios Konstantakos, Despina Ioanna Chalkiadaki, Ioannis Mademlis,
  Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos
Categories: cs.CV cs.AI
DOI: 10.1109/BigData59044.2023.10386795
\\ ( https://arxiv.org/abs/2310.07975 ,  363kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13033
replaced with revised version Tue, 6 Feb 2024 15:22:45 GMT   (151kb,D)

Title: LASER: Linear Compression in Wireless Distributed Optimization
Authors: Ashok Vardhan Makkuva, Marco Bondaschi, Thijs Vogels, Martin Jaggi,
  Hyeji Kim, Michael C. Gastpar
Categories: cs.NE cs.AI cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2310.13033 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01344
replaced with revised version Tue, 6 Feb 2024 13:10:23 GMT   (4810kb,D)

Title: Like an Open Book? Read Neural Network Architecture with Simple Power
  Analysis on 32-bit Microcontrollers
Authors: Raphael Joud, Pierre-Alain Moellic, Simon Pontie, Jean-Baptiste Rigaud
Categories: cs.CR cs.AI cs.LG
Comments: Accepted CARDIS 2023; ANR PICTURE PROJECT (ANR-20-CE39-0013)
\\ ( https://arxiv.org/abs/2311.01344 ,  4810kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13871
replaced with revised version Tue, 6 Feb 2024 17:15:40 GMT   (1354kb,D)

Title: Legal Requirements Analysis: A Regulatory Compliance Perspective
Authors: Sallam Abualhaija and Marcello Ceci and Lionel Briand
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2311.13871 ,  1354kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15607 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 01:31:24 GMT   (2291kb,D)

Title: Spatially Covariant Image Registration with Text Prompts
Authors: Xiang Chen, Min Liu, Rongguang Wang, Renjiu Hu, Dongdong Liu, Gaolei
  Li, and Hang Zhang
Categories: eess.IV cs.AI cs.CV
Comments: 13 pages, 8 figures, 6 tables
\\ ( https://arxiv.org/abs/2311.15607 ,  2291kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00736
replaced with revised version Tue, 6 Feb 2024 12:20:06 GMT   (13728kb,D)

Title: Diffusion Models, Image Super-Resolution And Everything: A Survey
Authors: Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav
  Frolov, Sebastian Palacio and Andreas Dengel
Categories: cs.CV cs.AI cs.LG cs.MM
\\ ( https://arxiv.org/abs/2401.00736 ,  13728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01345
replaced with revised version Tue, 6 Feb 2024 05:10:33 GMT   (388kb,D)

Title: Skip \n: A simple method to reduce hallucination in Large
  Vision-Language Models
Authors: Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, Mike
  Zheng Shou
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Technical Report
\\ ( https://arxiv.org/abs/2402.01345 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01763
replaced with revised version Tue, 6 Feb 2024 01:25:18 GMT   (681kb,D)

Title: When Large Language Models Meet Vector Databases: A Survey
Authors: Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu,
  Kehai Chen, Min Zhang
Categories: cs.DB cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.01763 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02085
replaced with revised version Tue, 6 Feb 2024 02:51:00 GMT   (46587kb,D)

Title: DeCoF: Generated Video Detection via Frame Consistency
Authors: Long Ma, Jiajia Zhang, Hongping Deng, Ningyu Zhang, Yong Liao, Haiyang
  Yu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.02085 ,  46587kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05591
replaced with revised version Tue, 6 Feb 2024 09:33:48 GMT   (11478kb,D)

Title: Linear Alignment of Vision-language Models for Image Captioning
Authors: Fabian Paischer, Markus Hofmarcher, Sepp Hochreiter, Thomas Adler
Categories: cs.CV cs.CL cs.LG
Comments: 8 pages (+ references and appendix)
\\ ( https://arxiv.org/abs/2307.05591 ,  11478kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16839
replaced with revised version Tue, 6 Feb 2024 16:43:31 GMT   (4061kb,D)

Title: Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware
  Direct Preference Optimization
Authors: Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, Conghui
  He
Categories: cs.CV cs.CL
Comments: Project Website: https://opendatalab.github.io/HA-DPO, Code:
  https://github.com/opendatalab/HA-DPO
\\ ( https://arxiv.org/abs/2311.16839 ,  4061kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03161
replaced with revised version Tue, 6 Feb 2024 06:35:36 GMT   (31035kb,D)

Title: Video-LaVIT: Unified Video-Language Pre-training with Decoupled
  Visual-Motional Tokenization
Authors: Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe
  Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2402.03161 ,  31035kb)
------------------------------------------------------------------------------
\\
arXiv:1908.06486 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 20:16:15 GMT   (187kb,D)

Title: Independence Testing for Temporal Data
Authors: Cencheng Shen, Jaewon Chung, Ronak Mehta, Ting Xu, Joshua T.
  Vogelstein
Categories: stat.ML cs.LG stat.ME
Comments: 21 pages main + 10 pages appendix
\\ ( https://arxiv.org/abs/1908.06486 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2001.01095 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 20:24:50 GMT   (78kb,D)

Title: High-Dimensional Independence Testing via Maximum and Average Distance
  Correlations
Authors: Cencheng Shen, Yuexiao Dong
Categories: stat.ML cs.LG stat.ME
Comments: 21 pages main + 5 pages appendix
\\ ( https://arxiv.org/abs/2001.01095 ,  78kb)
------------------------------------------------------------------------------
\\
arXiv:2202.09724 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 08:38:09 GMT   (509kb,D)

Title: Bayes-Optimal Classifiers under Group Fairness
Authors: Xianli Zeng and Edgar Dobriban and Guang Cheng
Categories: stat.ML cs.LG
Comments: This technical report has been largely superseded by our later paper:
  "Bayes-Optimal Fair Classification with Linear Disparity Constraints via
  Pre-, In-, and Post-processing'' (arXiv:2402.02817). Please cite that one
  instead of this technical report
\\ ( https://arxiv.org/abs/2202.09724 ,  509kb)
------------------------------------------------------------------------------
\\
arXiv:2202.13059 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 14:11:06 GMT   (1323kb,D)

Title: Theoretical Error Analysis of Entropy Approximation for Gaussian Mixture
Authors: Takashi Furuya, Hiroyuki Kusumoto, Koichi Taniguchi, Naoya Kanno,
  Kazuma Suetake
Categories: stat.ML cs.LG
Comments: 34 pages, 4 figures
\\ ( https://arxiv.org/abs/2202.13059 ,  1323kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10931 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 14:05:49 GMT   (23111kb,D)

Title: Approaching an unknown communication system by latent space exploration
  and causal inference
Authors: Ga\v{s}per Begu\v{s} and Andrej Leban, Shane Gero
Categories: stat.ML cs.LG cs.SD eess.AS
Comments: 25 pages, 23 figures; new format and section layout (moved some
  sections to the appendix), added replication experiments, updated references:
  to a subsequent experimental validation of the work, as well as to related
  methodological work
\\ ( https://arxiv.org/abs/2303.10931 ,  23111kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11860 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 06:28:18 GMT   (2154kb,D)

Title: On the lifting and reconstruction of nonlinear systems with multiple
  invariant sets
Authors: Shaowu Pan and Karthik Duraisamy
Categories: math.DS cs.LG
Comments: 14 pages
MSC-class: 37M10, 37M25, 47B33, 62F15
\\ ( https://arxiv.org/abs/2304.11860 ,  2154kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14777
replaced with revised version Tue, 6 Feb 2024 09:41:44 GMT   (13731kb,D)

Title: Generative Modeling through the Semi-dual Formulation of Unbalanced
  Optimal Transport
Authors: Jaemoo Choi, Jaewoong Choi, Myungjoo Kang
Categories: cs.CV cs.LG
Comments: 23 pages, 15 figures
\\ ( https://arxiv.org/abs/2305.14777 ,  13731kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17028 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 16:49:03 GMT   (288kb,D)

Title: Better Batch for Deep Probabilistic Time Series Forecasting
Authors: Vincent Zhihao Zheng, Seongjin Choi, Lijun Sun
Categories: stat.ML cs.LG
Comments: 10 pages, 3 figures, modified peer-review version, accepted to The
  27th International Conference on Artificial Intelligence and Statistics
  (AISTATS 2024)
\\ ( https://arxiv.org/abs/2305.17028 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19302
replaced with revised version Tue, 6 Feb 2024 13:14:35 GMT   (1353kb,D)

Title: Smooth, exact rotational symmetrization for deep learning on point
  clouds
Authors: Sergey N. Pozdnyakov and Michele Ceriotti
Categories: cs.CV cond-mat.mtrl-sci cs.LG physics.chem-ph
Comments: Enhancing figures; minor polishing
\\ ( https://arxiv.org/abs/2305.19302 ,  1353kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04877
replaced with revised version Tue, 6 Feb 2024 02:46:28 GMT   (4031kb,D)

Title: Trojan Model Detection Using Activation Optimization
Authors: Mohamed E. Hussein, Sudharshan Subramaniam Janakiraman, Wael
  AbdAlmageed
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2306.04877 ,  4031kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14670
replaced with revised version Tue, 6 Feb 2024 08:42:12 GMT   (1653kb,D)

Title: Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition
Authors: Meena Jagadeesan, Michael I. Jordan, Jacob Steinhardt, Nika Haghtalab
Categories: cs.GT cs.CY cs.LG stat.ML
Comments: Appeared at NeurIPS 2023; this is the full version
\\ ( https://arxiv.org/abs/2306.14670 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07852 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 21:18:13 GMT   (12918kb,D)

Title: On the Computational Complexity of Private High-dimensional Model
  Selection
Authors: Saptarshi Roy, Zehua Wang, Ambuj Tewari
Categories: stat.ML cs.LG stat.CO stat.ME
Comments: 27 pages, 2 figures
\\ ( https://arxiv.org/abs/2310.07852 ,  12918kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10143 (*cross-listing*)
replaced with revised version Mon, 5 Feb 2024 21:20:28 GMT   (1176kb,D)

Title: An Empirical Study of Self-supervised Learning with Wasserstein Distance
Authors: Makoto Yamada and Yuki Takezawa and Guillaume Houry and Kira Michaela
  Dusterwald and Deborah Sulem and Han Zhao and Yao-Hung Hubert Tsai
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.10143 ,  1176kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15574 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 18:36:42 GMT   (134kb,D)

Title: Clustered Switchback Experiments: Near-Optimal Rates Under
  Spatiotemporal Interference
Authors: Su Jia, Nathan Kallus, Christina Lee Yu
Categories: math.ST cs.LG stat.TH
\\ ( https://arxiv.org/abs/2312.15574 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16624 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 14:57:31 GMT   (16638kb,D)

Title: Dual-stage optimizer for systematic overestimation adjustment applied to
  multi-objective genetic algorithms for biomarker selection
Authors: Luca Cattelani and Vittorio Fortino
Categories: q-bio.QM cs.LG
Comments: Added a picture with the algorithm steps and a supplementary section
  with disambiguation of the technical terms. Moved sections in the
  supplementary to shorten the main text. Fixed typos
\\ ( https://arxiv.org/abs/2312.16624 ,  16638kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01148 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 10:06:22 GMT   (56kb)

Title: PAC-Bayes-Chernoff bounds for unbounded losses
Authors: Ioar Casado, Luis A. Ortega, Andr\'es R. Masegosa and Aritz P\'erez
Categories: stat.ML cs.LG
Comments: Updated Section 5
\\ ( https://arxiv.org/abs/2401.01148 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03892 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 14:12:49 GMT   (1861kb,D)

Title: Sampling in Unit Time with Kernel Fisher-Rao Flow
Authors: Aimee Maurais and Youssef Marzouk
Categories: stat.CO cs.LG stat.ML
Comments: Updated with additional numerical examples and a stochastic variant
  of the approach
\\ ( https://arxiv.org/abs/2401.03892 ,  1861kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04795
replaced with revised version Tue, 6 Feb 2024 00:18:09 GMT   (33843kb,D)

Title: First 100 days of pandemic; an interplay of pharmaceutical, behavioral
  and digital interventions -- A study using agent based modeling
Authors: Gauri Gupta, Ritvik Kapila, Ayush Chopra, Ramesh Raskar
Categories: cs.MA cs.LG cs.SI physics.soc-ph
Comments: 12 pages, 12 figures, In Proc. of the 23rd International Conference
  on Autonomous Agents and Multiagent Systems (AAMAS 2024), Auckland, New
  Zealand, 2024
\\ ( https://arxiv.org/abs/2401.04795 ,  33843kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09902 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 17:05:48 GMT   (4242kb,D)

Title: Interplay between depth and width for interpolation in neural ODEs
Authors: Antonio \'Alvarez-L\'opez, Arselane Hadj Slimane, Enrique Zuazua
Categories: math.OC cs.LG
Comments: 16 pages, 10 figures, double column
MSC-class: 34H05, 68T07, 93B05 (Primary) 35Q49 (Secondary)
\\ ( https://arxiv.org/abs/2401.09902 ,  4242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02430
replaced with revised version Tue, 6 Feb 2024 09:18:44 GMT   (9991kb,D)

Title: Exploiting Low-level Representations for Ultra-Fast Road Segmentation
Authors: Huan Zhou, Feng Xue, Yucong Li, Shi Gong, Yiqun Li, Yu Zhou
Categories: cs.CV cs.LG
Comments: 11 pages, 7 figures, IEEE TITS
\\ ( https://arxiv.org/abs/2402.02430 ,  9991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02551
replaced with revised version Tue, 6 Feb 2024 06:56:07 GMT   (2104kb,D)

Title: Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner
  with Robust Low-Level Control for Robotic Manipulators
Authors: Mehdi Heydari Shahna, Seyed Adel Alizadeh Kolagar, Jouni Mattila
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: This work has been submitted for possible publication in the IEEE
\\ ( https://arxiv.org/abs/2402.02551 ,  2104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02817 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 07:02:16 GMT   (825kb,D)

Title: Bayes-Optimal Fair Classification with Linear Disparity Constraints via
  Pre-, In-, and Post-processing
Authors: Xianli Zeng, Guang Cheng and Edgar Dobriban
Categories: stat.ML cs.LG
Comments: This paper replaces the preprint "Bayes-optimal classifiers under
  group fairness" by Xianli Zeng, Edgar Dobriban, and Guang Cheng
  (arXiv:2202.09724)
\\ ( https://arxiv.org/abs/2402.02817 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03179 (*cross-listing*)
replaced with revised version Tue, 6 Feb 2024 10:26:43 GMT   (25807kb,D)

Title: Cool-chic video: Learned video coding with 800 parameters
Authors: Thomas Leguay, Th\'eo Ladune, Pierrick Philippe, Olivier D\'eforges
Categories: eess.IV cs.LG
Comments: 10 pages, published in Data Compression Conference 2024
\\ ( https://arxiv.org/abs/2402.03179 ,  25807kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
