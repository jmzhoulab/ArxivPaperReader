paper_240325.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80011 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月25日 12:13
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 21 Mar 24 18:00:00 GMT  to  Fri 22 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.14705
Date: Sun, 17 Mar 2024 12:47:02 GMT   (386kb,D)

Title: Concept-Best-Matching: Evaluating Compositionality in Emergent
  Communication
Authors: Boaz Carmeli, Yonatan Belinkov, Ron Meir
Categories: cs.AI cs.CL
\\
  Artificial agents that learn to communicate in order to accomplish a given
task acquire communication protocols that are typically opaque to a human. A
large body of work has attempted to evaluate the emergent communication via
various evaluation measures, with \emph{compositionality} featuring as a
prominent desired trait. However, current evaluation procedures do not directly
expose the compositionality of the emergent communication. We propose a
procedure to assess the compositionality of emergent communication by finding
the best-match between emerged words and natural language concepts. The
best-match algorithm provides both a global score and a translation-map from
emergent words to natural language concepts. To the best of our knowledge, it
is the first time that such direct and interpretable mapping between emergent
words and human concepts is provided.
\\ ( https://arxiv.org/abs/2403.14705 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14733
Date: Thu, 21 Mar 2024 08:03:46 GMT   (641kb)

Title: Open Knowledge Base Canonicalization with Multi-task Learning
Authors: Bingchen Liu, Huang Peng, Weixin Zeng, Xiang Zhao, Shijun Liu, Li Pan
Categories: cs.AI cs.CL cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2310.16419
\\
  The construction of large open knowledge bases (OKBs) is integral to many
knowledge-driven applications on the world wide web such as web search.
However, noun phrases and relational phrases in OKBs often suffer from
redundancy and ambiguity, which calls for the investigation on OKB
canonicalization. Current solutions address OKB canonicalization by devising
advanced clustering algorithms and using knowledge graph embedding (KGE) to
further facilitate the canonicalization process. Nevertheless, these works fail
to fully exploit the synergy between clustering and KGE learning, and the
methods designed for these subtasks are sub-optimal. To this end, we put
forward a multi-task learning framework, namely MulCanon, to tackle OKB
canonicalization. In addition, diffusion model is used in the soft clustering
process to improve the noun phrase representations with neighboring
information, which can lead to more accurate representations. MulCanon unifies
the learning objectives of these sub-tasks, and adopts a two-stage multi-task
learning paradigm for training. A thorough experimental study on popular OKB
canonicalization benchmarks validates that MulCanon can achieve competitive
canonicalization results.
\\ ( https://arxiv.org/abs/2403.14733 ,  641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14796
Date: Thu, 21 Mar 2024 19:18:47 GMT   (857kb,D)

Title: Planning and Acting While the Clock Ticks
Authors: Andrew Coles, Erez Karpas, Andrey Lavrinenko, Wheeler Ruml, Solomon
  Eyal Shimony, Shahaf Shperberg
Categories: cs.AI
\\
  Standard temporal planning assumes that planning takes place offline and then
execution starts at time 0. Recently, situated temporal planning was
introduced, where planning starts at time 0 and execution occurs after planning
terminates. Situated temporal planning reflects a more realistic scenario where
time passes during planning. However, in situated temporal planning a complete
plan must be generated before any action is executed. In some problems with
time pressure, timing is too tight to complete planning before the first action
must be executed. For example, an autonomous car that has a truck backing
towards it should probably move out of the way now and plan how to get to its
destination later. In this paper, we propose a new problem setting: concurrent
planning and execution, in which actions can be dispatched (executed) before
planning terminates. Unlike previous work on planning and execution, we must
handle wall clock deadlines that affect action applicability and goal
achievement (as in situated planning) while also supporting dispatching actions
before a complete plan has been found. We extend previous work on metareasoning
for situated temporal planning to develop an algorithm for this new setting.
Our empirical evaluation shows that when there is strong time pressure, our
approach outperforms situated temporal planning.
\\ ( https://arxiv.org/abs/2403.14796 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14885
Date: Thu, 21 Mar 2024 23:42:00 GMT   (594kb,D)

Title: Establishing a leader in a pairwise comparisons method
Authors: Jacek Szybowski, Konrad Ku{\l}akowski, Jiri Mazurek, Sebastian Ernst
Categories: cs.AI cs.CR cs.CY cs.DM
Comments: 9 figures, 19 pages
\\
  Abstract Like electoral systems, decision-making methods are also vulnerable
to manipulation by decision-makers. The ability to effectively defend against
such threats can only come from thoroughly understanding the manipulation
mechanisms. In the presented article, we show two algorithms that can be used
to launch a manipulation attack. They allow for equating the weights of two
selected alternatives in the pairwise comparison method and, consequently,
choosing a leader. The theoretical considerations are accompanied by a Monte
Carlo simulation showing the relationship between the size of the PC matrix,
the degree of inconsistency, and the ease of manipulation. This work is a
continuation of our previous research published in the paper (Szybowski et al.,
2023)
\\ ( https://arxiv.org/abs/2403.14885 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14972
Date: Fri, 22 Mar 2024 06:03:07 GMT   (676kb,D)

Title: A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal
  Reasoning
Authors: Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao-Yong Wei, Tat-Seng
  Chua, Qing Li
Categories: cs.AI cs.CL cs.MA cs.MM
Comments: Work in progress
\\
  This paper presents a pilot study aimed at introducing multi-agent debate
into multimodal reasoning. The study addresses two key challenges: the
trivialization of opinions resulting from excessive summarization and the
diversion of focus caused by distractor concepts introduced from images. These
challenges stem from the inductive (bottom-up) nature of existing debating
schemes. To address the issue, we propose a deductive (top-down) debating
approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are
confined to a blueprint graph to prevent opinion trivialization through
world-level summarization. Moreover, by storing evidence in branches within the
graph, BDoG mitigates distractions caused by frequent but irrelevant concepts.
Extensive experiments validate BDoG, achieving state-of-the-art results in
Science QA and MMBench with significant improvements over previous methods.
\\ ( https://arxiv.org/abs/2403.14972 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15137
Date: Fri, 22 Mar 2024 11:42:47 GMT   (1400kb,D)

Title: CACA Agent: Capability Collaboration based AI Agent
Authors: Peng Xu, Haoran Wang, Chuang Wang and Xu Liu
Categories: cs.AI cs.CL cs.MA
Comments: 4 pages,5 figures
\\
  As AI Agents based on Large Language Models (LLMs) have shown potential in
practical applications across various fields, how to quickly deploy an AI agent
and how to conveniently expand the application scenario of AI agents has become
a challenge. Previous studies mainly focused on implementing all the reasoning
capabilities of AI agents within a single LLM, which often makes the model more
complex and also reduces the extensibility of AI agent functionality. In this
paper, we propose CACA Agent (Capability Collaboration based AI Agent), using
an open architecture inspired by service computing. CACA Agent integrates a set
of collaborative capabilities to implement AI Agents, not only reducing the
dependence on a single LLM, but also enhancing the extensibility of both the
planning abilities and the tools available to AI agents. Utilizing the proposed
system, we present a demo to illustrate the operation and the application
scenario extension of CACA Agent.
\\ ( https://arxiv.org/abs/2403.15137 ,  1400kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15251
Date: Fri, 22 Mar 2024 14:49:49 GMT   (1993kb,D)

Title: Safe Learning of PDDL Domains with Conditional Effects -- Extended
  Version
Authors: Argaman Mordoch, Enrico Scala, Roni Stern, Brendan Juba
Categories: cs.AI
\\
  Powerful domain-independent planners have been developed to solve various
types of planning problems. These planners often require a model of the acting
agent's actions, given in some planning domain description language. Manually
designing such an action model is a notoriously challenging task. An
alternative is to automatically learn action models from observation. Such an
action model is called safe if every plan created with it is consistent with
the real, unknown action model. Algorithms for learning such safe action models
exist, yet they cannot handle domains with conditional or universal effects,
which are common constructs in many planning problems. We prove that learning
non-trivial safe action models with conditional effects may require an
exponential number of samples. Then, we identify reasonable assumptions under
which such learning is tractable and propose SAM Learning of Conditional
Effects (Conditional-SAM), the first algorithm capable of doing so. We analyze
Conditional-SAM theoretically and evaluate it experimentally. Our results show
that the action models learned by Conditional-SAM can be used to solve
perfectly most of the test set problems in most of the experimented domains.
\\ ( https://arxiv.org/abs/2403.15251 ,  1993kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15297
Date: Fri, 22 Mar 2024 15:44:59 GMT   (29986kb,D)

Title: Sphere Neural-Networks for Rational Reasoning
Authors: Tiansi Dong, Mateja Jamnik, Pietro Li\`o
Categories: cs.AI
\\
  The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by
their planetary popularity, their capability of human-like question-answering,
and also by their steadily improved reasoning performance. However, it remains
unclear whether LLMs reason. It is an open problem how traditional neural
networks can be qualitatively extended to go beyond the statistic paradigm and
achieve high-level cognition. Here, we present a minimalist qualitative
extension by generalising computational building blocks from vectors to
spheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning
through model construction and inspection, and develop SphNN for syllogistic
reasoning, a microcosm of human rationality. Instead of training data, SphNN
uses a neuro-symbolic transition map of neighbourhood spatial relations to
guide transformations from the current sphere configuration towards the target.
SphNN is the first neural model that can determine the validity of long-chained
syllogistic reasoning in one epoch by constructing sphere configurations as
Euler diagrams, with the worst computational complexity of O(N^2). SphNN can
evolve into various types of reasoning, such as spatio-temporal reasoning,
logical reasoning with negation and disjunction, event reasoning,
neuro-symbolic reasoning, and humour understanding (the highest level of
cognition). All these suggest a new kind of Herbert A. Simon's scissors with
two neural blades. SphNNs will tremendously enhance interdisciplinary
collaborations to develop the two neural blades and realise deterministic
neural reasoning and human-bounded rationality and elevate LLMs to reliable
psychological AI. This work suggests that the non-zero radii of spheres are the
missing components that prevent traditional deep-learning systems from reaching
the realm of rational reasoning and cause LLMs to be trapped in the swamp of
hallucination.
\\ ( https://arxiv.org/abs/2403.15297 ,  29986kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15341
Date: Fri, 22 Mar 2024 16:50:56 GMT   (932kb,D)

Title: Collaborative AI Teaming in Unknown Environments via Active Goal
  Deduction
Authors: Zuyuan Zhang, Hanhan Zhou, Mahdi Imani, Taeyoung Lee, Tian Lan
Categories: cs.AI cs.MA
\\
  With the advancements of artificial intelligence (AI), we're seeing more
scenarios that require AI to work closely with other agents, whose goals and
strategies might not be known beforehand. However, existing approaches for
training collaborative agents often require defined and known reward signals
and cannot address the problem of teaming with unknown agents that often have
latent objectives/rewards. In response to this challenge, we propose teaming
with unknown agents framework, which leverages kernel density Bayesian inverse
learning method for active goal deduction and utilizes pre-trained,
goal-conditioned policies to enable zero-shot policy adaptation. We prove that
unbiased reward estimates in our framework are sufficient for optimal teaming
with unknown agents. We further evaluate the framework of redesigned
multi-agent particle and StarCraft II micromanagement environments with diverse
unknown agents of different behaviors/rewards. Empirical results demonstrate
that our framework significantly advances the teaming performance of AI and
unknown agents in a wide range of collaborative scenarios.
\\ ( https://arxiv.org/abs/2403.15341 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14808
Date: Thu, 21 Mar 2024 19:46:42 GMT   (303kb,D)

Title: A Collection of Pragmatic-Similarity Judgments over Spoken Dialog
  Utterances
Authors: Nigel G. Ward, Divette Marco
Categories: cs.CL
Comments: LREC 2024
\\
  Automatic measures of similarity between utterances are invaluable for
training speech synthesizers, evaluating machine translation, and assessing
learner productions. While there exist measures for semantic similarity and
prosodic similarity, there are as yet none for pragmatic similarity. To enable
the training of such measures, we developed the first collection of human
judgments of pragmatic similarity between utterance pairs. Each pair consisting
of an utterance extracted from a recorded dialog and a re-enactment of that
utterance. Re-enactments were done under various conditions designed to create
a variety of degrees of similarity. Each pair was rated on a continuous scale
by 6 to 9 judges. The average inter-judge correlation was as high as 0.72 for
English and 0.66 for Spanish. We make this data available at
https://github.com/divettemarco/PragSim .
\\ ( https://arxiv.org/abs/2403.14808 ,  303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14814
Date: Thu, 21 Mar 2024 19:59:52 GMT   (2436kb)

Title: The opportunities and risks of large language models in mental health
Authors: Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J.
  Mataric, Daniel J. McDuff, and Megan Jones Bell
Categories: cs.CL cs.AI cs.CY cs.HC cs.LG
Comments: 12 pages, 2 tables, 4 figures
\\
  Global rates of mental health concerns are rising and there is increasing
realization that existing models of mental healthcare will not adequately
expand to meet the demand. With the emergence of large language models (LLMs)
has come great optimism regarding their promise to create novel, large-scale
solutions to support mental health. Despite their nascence, LLMs have already
been applied to mental health-related tasks. In this review, we summarize the
extant literature on efforts to use LLMs to provide mental health education,
assessment, and intervention and highlight key opportunities for positive
impact in each area. We then highlight risks associated with LLMs application
to mental health and encourage adoption of strategies to mitigate these risks.
The urgent need for mental health support must be balanced with responsible
development, testing, and deployment of mental health LLMs. Especially critical
is ensuring that mental health LLMs are fine-tuned for mental health, enhance
mental health equity, adhere to ethical standards, and that people, including
those with lived experience with mental health concerns, are involved in all
stages from development through deployment. Prioritizing these efforts will
minimize potential harms to mental health and maximize the likelihood that LLMs
will positively impact mental health globally.
\\ ( https://arxiv.org/abs/2403.14814 ,  2436kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14840
Date: Thu, 21 Mar 2024 21:23:35 GMT   (6772kb,D)

Title: TAMS: Translation-Assisted Morphological Segmentation
Authors: Enora Rice, Ali Marashian, Luke Gessler, Alexis Palmer, Katharina von
  der Wense
Categories: cs.CL
Comments: Submitted to ACL ARR on December 15th 2023
\\
  Canonical morphological segmentation is the process of analyzing words into
the standard (aka underlying) forms of their constituent morphemes. This is a
core task in language documentation, and NLP systems have the potential to
dramatically speed up this process. But in typical language documentation
settings, training data for canonical morpheme segmentation is scarce, making
it difficult to train high quality models. However, translation data is often
much more abundant, and, in this work, we present a method that attempts to
leverage this data in the canonical segmentation task. We propose a
character-level sequence-to-sequence model that incorporates representations of
translations obtained from pretrained high-resource monolingual language models
as an additional signal. Our model outperforms the baseline in a super-low
resource setting but yields mixed results on training splits with more data.
While further work is needed to make translations useful in higher-resource
settings, our model shows promise in severely resource-constrained settings.
\\ ( https://arxiv.org/abs/2403.14840 ,  6772kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14859
Date: Thu, 21 Mar 2024 22:08:44 GMT   (3880kb,D)

Title: Comparing Plausibility Estimates in Base and Instruction-Tuned Large
  Language Models
Authors: Carina Kauf, Emmanuele Chersoni, Alessandro Lenci, Evelina Fedorenko
  and Anna A. Ivanova
Categories: cs.CL cs.AI
\\
  Instruction-tuned LLMs can respond to explicit queries formulated as prompts,
which greatly facilitates interaction with human users. However, prompt-based
approaches might not always be able to tap into the wealth of implicit
knowledge acquired by LLMs during pre-training. This paper presents a
comprehensive study of ways to evaluate semantic plausibility in LLMs. We
compare base and instruction-tuned LLM performance on an English sentence
plausibility task via (a) explicit prompting and (b) implicit estimation via
direct readout of the probabilities models assign to strings. Experiment 1
shows that, across model architectures and plausibility datasets, (i) log
likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence
plausibility, with zero-shot prompting yielding inconsistent and typically poor
results; (ii) $\textit{LL}$-based performance is still inferior to human
performance; (iii) instruction-tuned models have worse $\textit{LL}$-based
performance than base models. In Experiment 2, we show that $\textit{LL}$
scores across models are modulated by context in the expected way, showing high
performance on three metrics of context-sensitive plausibility and providing a
direct match to explicit human plausibility judgments. Overall, $\textit{LL}$
estimates remain a more reliable measure of plausibility in LLMs than direct
prompting.
\\ ( https://arxiv.org/abs/2403.14859 ,  3880kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14888
Date: Thu, 21 Mar 2024 23:48:21 GMT   (748kb,D)

Title: AutoRE: Document-Level Relation Extraction with Large Language Models
Authors: Xue Lilong and Zhang Dan and Dong Yuxiao and Tang Jie
Categories: cs.CL cs.AI
Comments: 11 pages
\\
  Large Language Models (LLMs) have demonstrated exceptional abilities in
comprehending and generating text, motivating numerous researchers to utilize
them for Information Extraction (IE) purposes, including Relation Extraction
(RE). Nonetheless, most existing methods are predominantly designed for
Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a
restricted set of relations and triplet facts within a single sentence.
Furthermore, certain approaches resort to treating relations as candidate
choices integrated into prompt templates, leading to inefficient processing and
suboptimal performance when tackling Document-Level Relation Extraction (DocRE)
tasks, which entail handling multiple relations and triplet facts distributed
across a given document, posing distinct challenges. To overcome these
limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel
RE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing
approaches, AutoRE does not rely on the assumption of known relation options,
making it more reflective of real-world scenarios. Additionally, we have
developed an easily extensible RE framework using a Parameters Efficient Fine
Tuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset
showcase AutoRE's best performance, achieving state-of-the-art results,
surpassing TAG by 10.03% and 9.03% respectively on the dev and test set.
\\ ( https://arxiv.org/abs/2403.14888 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14895
Date: Fri, 22 Mar 2024 00:58:28 GMT   (77kb,D)

Title: Stance Reasoner: Zero-Shot Stance Detection on Social Media with
  Explicit Reasoning
Authors: Maksym Taranukhin, Vered Shwartz, Evangelos Milios
Categories: cs.CL cs.AI
Comments: Accepted to COLING 2024
\\
  Social media platforms are rich sources of opinionated content. Stance
detection allows the automatic extraction of users' opinions on various topics
from such content. We focus on zero-shot stance detection, where the model's
success relies on (a) having knowledge about the target topic; and (b) learning
general reasoning strategies that can be employed for new topics. We present
Stance Reasoner, an approach to zero-shot stance detection on social media that
leverages explicit reasoning over background knowledge to guide the model's
inference about the document's stance on a target. Specifically, our method
uses a pre-trained language model as a source of world knowledge, with the
chain-of-thought in-context learning approach to generate intermediate
reasoning steps. Stance Reasoner outperforms the current state-of-the-art
models on 3 Twitter datasets, including fully supervised models. It can better
generalize across targets, while at the same time providing explicit and
interpretable explanations for its predictions.
\\ ( https://arxiv.org/abs/2403.14895 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14919
Date: Fri, 22 Mar 2024 02:44:05 GMT   (241kb,D)

Title: Hierarchical Skip Decoding for Efficient Autoregressive Text Generation
Authors: Yunqi Zhu, Xuebing Yang, Yuanyuan Wu, Wensheng Zhang
Categories: cs.CL cs.AI
\\
  Autoregressive decoding strategy is a commonly used method for text
generation tasks with pre-trained language models, while early-exiting is an
effective approach to speedup the inference stage. In this work, we propose a
novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient
autoregressive text generation. Different from existing methods that require
additional trainable components, HSD is a plug-and-play method applicable to
autoregressive text generation models, it adaptively skips decoding layers in a
hierarchical manner based on the current sequence length, thereby reducing
computational workload and allocating computation resources. Comprehensive
experiments on five text generation datasets with pre-trained language models
demonstrate HSD's advantages in balancing efficiency and text quality. With
almost half of the layers skipped, HSD can sustain 90% of the text quality
compared to vanilla autoregressive decoding, outperforming the competitive
approaches.
\\ ( https://arxiv.org/abs/2403.14919 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14932
Date: Fri, 22 Mar 2024 03:23:58 GMT   (1083kb,D)

Title: Attention-Driven Reasoning: Unlocking the Potential of Large Language
  Models
Authors: Bingli Liao, Danilo Vasconcellos Vargas
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have shown remarkable capabilities, but their
reasoning abilities and underlying mechanisms remain poorly understood. We
present a novel approach to enhance LLMs' reasoning through attention mechanism
optimization, without additional training data. We identify inefficiencies in
the attention distribution caused by non-semantic tokens and propose an
algorithm to re-balance the skewed distribution, enabling the model to abstract
more nuanced knowledge. Our experiments demonstrate significantly improved
reasoning capabilities, particularly for non-STEM questions. We provide
insights into the role of attention patterns in LLMs' reasoning and propose a
method to enhance these abilities, paving the way for more powerful and
versatile language models.
\\ ( https://arxiv.org/abs/2403.14932 ,  1083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14938
Date: Fri, 22 Mar 2024 04:13:10 GMT   (67kb)

Title: On Zero-Shot Counterspeech Generation by LLMs
Authors: Punyajoy Saha, Aalok Agrawal, Abhik Jana, Chris Biemann, Animesh
  Mukherjee
Categories: cs.CL
Comments: 12 pages, 7 tables, accepted at LREC-COLING 2024
\\
  With the emergence of numerous Large Language Models (LLM), the usage of such
models in various Natural Language Processing (NLP) applications is increasing
extensively. Counterspeech generation is one such key task where efforts are
made to develop generative models by fine-tuning LLMs with hatespeech -
counterspeech pairs, but none of these attempts explores the intrinsic
properties of large language models in zero-shot settings. In this work, we
present a comprehensive analysis of the performances of four LLMs namely GPT-2,
DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech
generation, which is the first of its kind. For GPT-2 and DialoGPT, we further
investigate the deviation in performance with respect to the sizes (small,
medium, large) of the models. On the other hand, we propose three different
prompting strategies for generating different types of counterspeech and
analyse the impact of such strategies on the performance of the models. Our
analysis shows that there is an improvement in generation quality for two
datasets (17%), however the toxicity increase (25%) with increase in model
size. Considering type of model, GPT-2 and FlanT5 models are significantly
better in terms of counterspeech quality but also have high toxicity as
compared to DialoGPT. ChatGPT are much better at generating counter speech than
other models across all metrics. In terms of prompting, we find that our
proposed strategies help in improving counter speech generation across all the
models.
\\ ( https://arxiv.org/abs/2403.14938 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14946
Date: Fri, 22 Mar 2024 04:38:42 GMT   (1152kb,D)

Title: A Single Linear Layer Yields Task-Adapted Low-Rank Matrices
Authors: Hwichan Kim, Shota Sasaki, Sho Hoshino, Ukyo Honda
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning
(PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix
$\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study
suggested that there is correlation between $W_0$ and $\Delta W$. In this
study, we aim to delve deeper into relationships between $W_0$ and low-rank
matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular,
we analyze a conversion matrix that transform $W_0$ into low-rank matrices,
which encapsulates information about the relationships. Our analysis reveals
that the conversion matrices are similar across each layer. Inspired by these
findings, we hypothesize that a single linear layer, which takes each layer's
$W_0$ as input, can yield task-adapted low-rank matrices. To confirm this
hypothesis, we devise a method named Conditionally Parameterized LoRA
(CondLoRA) that updates initial weight matrices with low-rank matrices derived
from a single linear layer. Our empirical results show that CondLoRA maintains
a performance on par with LoRA, despite the fact that the trainable parameters
of CondLoRA are fewer than those of LoRA. Therefore, we conclude that "a single
linear layer yields task-adapted low-rank matrices."
\\ ( https://arxiv.org/abs/2403.14946 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14950
Date: Fri, 22 Mar 2024 04:48:41 GMT   (291kb,D)

Title: KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable
  Adaptation
Authors: Xindi Luo and Zequn Sun and Jing Zhao and Zhe Zhao and Wei Hu
Categories: cs.CL cs.LG
Comments: Accepted in the 2024 Annual Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL 2024)
\\
  Parameter-efficient finetuning (PEFT) is a key technique for adapting large
language models (LLMs) to downstream tasks. In this paper, we study leveraging
knowledge graph embeddings to improve the effectiveness of PEFT. We propose a
knowledgeable adaptation method called KnowLA. It inserts an adaptation layer
into an LLM to integrate the embeddings of entities appearing in the input
text. The adaptation layer is trained in combination with LoRA on instruction
data. Experiments on six benchmarks with two popular LLMs and three knowledge
graphs demonstrate the effectiveness and robustness of KnowLA. We show that
\modelname can help activate the relevant parameterized knowledge in an LLM to
answer a question without changing its parameters or input prompts.
\\ ( https://arxiv.org/abs/2403.14950 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14952
Date: Fri, 22 Mar 2024 05:05:45 GMT   (8066kb,D)

Title: Evidence-Driven Retrieval Augmented Response Generation for Online
  Misinformation
Authors: Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, Dong
  Wang
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024
\\
  The proliferation of online misinformation has posed significant threats to
public interest. While numerous online users actively participate in the combat
against misinformation, many of such responses can be characterized by the lack
of politeness and supporting facts. As a solution, text generation approaches
are proposed to automatically produce counter-misinformation responses.
Nevertheless, existing methods are often trained end-to-end without leveraging
external knowledge, resulting in subpar text quality and excessively repetitive
responses. In this paper, we propose retrieval augmented response generation
for online misinformation (RARG), which collects supporting evidence from
scientific sources and generates counter-misinformation responses based on the
evidences. In particular, our RARG consists of two stages: (1) evidence
collection, where we design a retrieval pipeline to retrieve and rerank
evidence documents using a database comprising over 1M academic articles; (2)
response generation, in which we align large language models (LLMs) to generate
evidence-based responses via reinforcement learning from human feedback (RLHF).
We propose a reward function to maximize the utilization of the retrieved
evidence while maintaining the quality of the generated text, which yields
polite and factual responses that clearly refutes misinformation. To
demonstrate the effectiveness of our method, we study the case of COVID-19 and
perform extensive experiments with both in- and cross-domain datasets, where
RARG consistently outperforms baselines by generating high-quality
counter-misinformation responses.
\\ ( https://arxiv.org/abs/2403.14952 ,  8066kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14982
Date: Fri, 22 Mar 2024 06:31:49 GMT   (7822kb,D)

Title: MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of
  Chain-of-Thoughts
Authors: Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Sadiya Sayara
  Chowdhury Puspo, Amrita Ganguly, Marcos Zampieri
Categories: cs.CL
\\
  Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -
which provides a dataset of puzzles for testing natural language understanding.
We employ large language models (LLMs) to solve this task through several
prompting techniques. Zero-shot and few-shot prompting generate reasonably good
results when tested with proprietary LLMs, compared to the open-source models.
We obtain further improved results with chain-of-thought prompting, an
iterative prompting method that breaks down the reasoning process step-by-step.
We obtain our best results by utilizing an ensemble of chain-of-thought
prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle
subtask. The strong performance of prompted LLMs demonstrates their capability
for complex reasoning when provided with a decomposition of the thought
process. Our work sheds light on how step-wise explanatory prompts can unlock
more of the knowledge encoded in the parameters of large models.
\\ ( https://arxiv.org/abs/2403.14982 ,  7822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14988
Date: Fri, 22 Mar 2024 06:46:40 GMT   (10014kb,D)

Title: Risk and Response in Large Language Models: Evaluating Key Threat
  Categories
Authors: Bahareh Harandizadeh, Abel Salinas, Fred Morstatter
Categories: cs.CL
Comments: 19 pages, 14 figures
\\
  This paper explores the pressing issue of risk assessment in Large Language
Models (LLMs) as they become increasingly prevalent in various applications.
Focusing on how reward models, which are designed to fine-tune pretrained LLMs
to align with human values, perceive and categorize different types of risks,
we delve into the challenges posed by the subjective nature of preference-based
training data. By utilizing the Anthropic Red-team dataset, we analyze major
risk categories, including Information Hazards, Malicious Uses, and
Discrimination/Hateful content. Our findings indicate that LLMs tend to
consider Information Hazards less harmful, a finding confirmed by a specially
developed regression model. Additionally, our analysis shows that LLMs respond
less stringently to Information Hazards compared to other risks. The study
further reveals a significant vulnerability of LLMs to jailbreaking attacks in
Information Hazard scenarios, highlighting a critical security concern in LLM
risk assessment and emphasizing the need for improved AI safety measures.
\\ ( https://arxiv.org/abs/2403.14988 ,  10014kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14989
Date: Fri, 22 Mar 2024 06:47:28 GMT   (7812kb,D)

Title: MasonTigers at SemEval-2024 Task 8: Performance Analysis of
  Transformer-based Models on Machine-Generated Text Detection
Authors: Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Dhiman Goswami, Al
  Nahian Bin Emran, Amrita Ganguly, Ozlem Uzuner
Categories: cs.CL
\\
  This paper presents the MasonTigers entry to the SemEval-2024 Task 8 -
Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text
Detection. The task encompasses Binary Human-Written vs. Machine-Generated Text
Classification (Track A), Multi-Way Machine-Generated Text Classification
(Track B), and Human-Machine Mixed Text Detection (Track C). Our best
performing approaches utilize mainly the ensemble of discriminator transformer
models along with sentence transformer and statistical machine learning
approaches in specific cases. Moreover, zero-shot prompting and fine-tuning of
FLAN-T5 are used for Track A and B.
\\ ( https://arxiv.org/abs/2403.14989 ,  7812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14990
Date: Fri, 22 Mar 2024 06:47:42 GMT   (8502kb,D)

Title: MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic
  Textual Relatedness
Authors: Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al
  Nahian Bin Emran, Amrita Ganguly, Marcos Zampieri
Categories: cs.CL
\\
  This paper presents the MasonTigers entry to the SemEval-2024 Task 1 -
Semantic Textual Relatedness. The task encompasses supervised (Track A),
unsupervised (Track B), and cross-lingual (Track C) approaches across 14
different languages. MasonTigers stands out as one of the two teams who
participated in all languages across the three tracks. Our approaches achieved
rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and
from 5th to 12th in Track C. Adhering to the task-specific constraints, our
best performing approaches utilize ensemble of statistical machine learning
approaches combined with language-specific BERT based models and sentence
transformers.
\\ ( https://arxiv.org/abs/2403.14990 ,  8502kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15040
Date: Fri, 22 Mar 2024 08:45:30 GMT   (1105kb,D)

Title: ESG Classification by Implicit Rule Learning via GPT-4
Authors: Hyo Jeong Yun, Chanyoung Kim, Moonjeong Hahm, Kyuri Kim, Guijin Son
Categories: cs.CL
Comments: Accepted as Shared Track Paper at 7th FinNLP Workshop @ LREC-COLING
  2024
\\
  Environmental, social, and governance (ESG) factors are widely adopted as
higher investment return indicators. Accordingly, ongoing efforts are being
made to automate ESG evaluation with language models to extract signals from
massive web text easily. However, recent approaches suffer from a lack of
training data, as rating agencies keep their evaluation metrics confidential.
This paper investigates whether state-of-the-art language models like GPT-4 can
be guided to align with unknown ESG evaluation criteria through strategies such
as prompting, chain-of-thought reasoning, and dynamic in-context learning. We
demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task
ML-ESG-3 Impact Type track for Korean without updating the model on the
provided training data. We also explore how adjusting prompts impacts the
ability of language models to address financial tasks leveraging smaller models
with openly available weights. We observe longer general pre-training to
correlate with enhanced performance in financial downstream tasks. Our findings
showcase the potential of language models to navigate complex, subjective
evaluation guidelines despite lacking explicit training examples, revealing
opportunities for training-free solutions for financial downstream tasks.
\\ ( https://arxiv.org/abs/2403.15040 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15042
Date: Fri, 22 Mar 2024 08:57:07 GMT   (209kb,D)

Title: LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement
Authors: Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam,
  Sheng Shen, Gopala Anumanchipali, Michael W. Mahoney, Kurt Keutzer, Amir
  Gholami
Categories: cs.CL
Comments: Our code is available at https://github.com/SqueezeAILab/LLM2LLM
\\
  Pretrained large language models (LLMs) are currently state-of-the-art for
solving the vast majority of natural language processing tasks. While many
real-world applications still require fine-tuning to reach satisfactory levels
of performance, many of them are in the low-data regime, making fine-tuning
challenging. To address this, we propose LLM2LLM, a targeted and iterative data
augmentation strategy that uses a teacher LLM to enhance a small seed dataset
by augmenting additional data that can be used for fine-tuning on a specific
task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,
(2) evaluates and extracts data points that the model gets wrong, and (3) uses
a teacher LLM to generate synthetic data based on these incorrect data points,
which are then added back into the training data. This approach amplifies the
signal from incorrectly predicted data points by the LLM during training and
reintegrates them into the dataset to focus on more challenging examples for
the LLM. Our results show that LLM2LLM significantly enhances the performance
of LLMs in the low-data regime, outperforming both traditional fine-tuning and
other data augmentation baselines. LLM2LLM reduces the dependence on
labor-intensive data curation and paves the way for more scalable and
performant LLM solutions, allowing us to tackle data-constrained domains and
tasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on
CaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular
fine-tuning in the low-data regime using a LLaMA2-7B student model.
\\ ( https://arxiv.org/abs/2403.15042 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15088
Date: Fri, 22 Mar 2024 10:12:10 GMT   (1313kb,D)

Title: CHisIEC: An Information Extraction Corpus for Ancient Chinese History
Authors: Xuemei Tang, Zekun Deng, Qi Su, Hao Yang, Jun Wang
Categories: cs.CL
Comments: 11 pages, 6 tables, 3 figures
\\
  Natural Language Processing (NLP) plays a pivotal role in the realm of
Digital Humanities (DH) and serves as the cornerstone for advancing the
structural analysis of historical and cultural heritage texts. This is
particularly true for the domains of named entity recognition (NER) and
relation extraction (RE). In our commitment to expediting ancient history and
culture, we present the ``Chinese Historical Information Extraction
Corpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to
develop and evaluate NER and RE tasks, offering a resource to facilitate
research in the field. Spanning a remarkable historical timeline encompassing
data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the
extensive temporal range and text heterogeneity inherent in Chinese historical
documents. The dataset encompasses four distinct entity types and twelve
relation types, resulting in a meticulously labeled dataset comprising 14,194
entities and 8,609 relations. To establish the robustness and versatility of
our dataset, we have undertaken comprehensive experimentation involving models
of various sizes and paradigms. Additionally, we have evaluated the
capabilities of Large Language Models (LLMs) in the context of tasks related to
ancient Chinese history. The dataset and code are available at
\url{https://github.com/tangxuemei1995/CHisIEC}.
\\ ( https://arxiv.org/abs/2403.15088 ,  1313kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15097
Date: Fri, 22 Mar 2024 10:32:43 GMT   (1297kb,D)

Title: Argument-Aware Approach To Event Linking
Authors: I-Hung Hsu, Zihan Xue, Nilay Pochh, Sahil Bansal, Premkumar Natarajan,
  Jayanth Srinivasa, Nanyun Peng
Categories: cs.CL cs.AI
Comments: Work In Progress
\\
  Event linking connects event mentions in text with relevant nodes in a
knowledge base (KB). Prior research in event linking has mainly borrowed
methods from entity linking, overlooking the distinct features of events.
Compared to the extensively explored entity linking task, events have more
complex structures and can be more effectively distinguished by examining their
associated arguments. Moreover, the information-rich nature of events leads to
the scarcity of event KBs. This emphasizes the need for event linking models to
identify and classify event mentions not in the KB as ``out-of-KB,'' an area
that has received limited attention. In this work, we tackle these challenges
by introducing an argument-aware approach. First, we improve event linking
models by augmenting input text with tagged event argument information,
facilitating the recognition of key information about event mentions.
Subsequently, to help the model handle ``out-of-KB'' scenarios, we synthesize
out-of-KB training examples from in-KB instances through controlled
manipulation of event arguments. Our experiment across two test datasets showed
significant enhancements in both in-KB and out-of-KB scenarios, with a notable
22% improvement in out-of-KB evaluations.
\\ ( https://arxiv.org/abs/2403.15097 ,  1297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15112
Date: Fri, 22 Mar 2024 11:08:48 GMT   (83kb,D)

Title: Text clustering with LLM embeddings
Authors: Alina Petukhova, Joao P. Matos-Carvalho, Nuno Fachada
Categories: cs.CL cs.AI cs.LG
\\
  Text clustering is an important approach for organising the growing amount of
digital content, helping to structure and find hidden patterns in uncategorised
data. In this research, we investigated how different textual embeddings -
particularly those used in large language models (LLMs) - and clustering
algorithms affect how text datasets are clustered. A series of experiments were
conducted to assess how embeddings influence clustering results, the role
played by dimensionality reduction through summarisation, and embedding size
adjustment. Results reveal that LLM embeddings excel at capturing the nuances
of structured language, while BERT leads the lightweight options in
performance. In addition, we find that increasing embedding dimensionality and
summarisation techniques do not uniformly improve clustering efficiency,
suggesting that these strategies require careful analysis to use in real-life
models. These results highlight a complex balance between the need for nuanced
text representation and computational feasibility in text clustering
applications. This study extends traditional text clustering frameworks by
incorporating embeddings from LLMs, thereby paving the way for improved
methodologies and opening new avenues for future research in various types of
textual analysis.
\\ ( https://arxiv.org/abs/2403.15112 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15115
Date: Fri, 22 Mar 2024 11:16:43 GMT   (9404kb,D)

Title: Language Models in Dialogue: Conversational Maxims for Human-AI
  Interactions
Authors: Erik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M.
  Daly, David Piorkowski, John T. Richards
Categories: cs.CL cs.AI cs.HC
\\
  Modern language models, while sophisticated, exhibit some inherent
shortcomings, particularly in conversational settings. We claim that many of
the observed shortcomings can be attributed to violation of one or more
conversational principles. By drawing upon extensive research from both the
social science and AI communities, we propose a set of maxims -- quantity,
quality, relevance, manner, benevolence, and transparency -- for describing
effective human-AI conversation. We first justify the applicability of the
first four maxims (from Grice) in the context of human-AI interactions. We then
argue that two new maxims, benevolence (concerning the generation of, and
engagement with, harmful content) and transparency (concerning recognition of
one's knowledge boundaries, operational constraints, and intents), are
necessary for addressing behavior unique to modern human-AI interactions. The
proposed maxims offer prescriptive guidance on how to assess conversational
quality between humans and LLM-driven conversational agents, informing both
their evaluation and improved design.
\\ ( https://arxiv.org/abs/2403.15115 ,  9404kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15185
Date: Fri, 22 Mar 2024 13:13:13 GMT   (3719kb,D)

Title: Investigating the Performance of Language Models for Completing Code in
  Functional Programming Languages: a Haskell Case Study
Authors: Tim van Dam and Frank van der Heijden and Philippe de Bekker and
  Berend Nieuwschepen and Marc Otten and Maliheh Izadi
Categories: cs.CL
Comments: To appear in the First Special Event on AI Foundation Models and
  Software Engineering (FORGE 2024)
\\
  Language model-based code completion models have quickly grown in use,
helping thousands of developers write code in many different programming
languages. However, research on code completion models typically focuses on
imperative languages such as Python and JavaScript, which results in a lack of
representation for functional programming languages. Consequently, these models
often perform poorly on functional languages such as Haskell. To investigate
whether this can be alleviated, we evaluate the performance of two language
models for code, CodeGPT and UniXcoder, on the functional programming language
Haskell. We fine-tune and evaluate the models on Haskell functions sourced from
a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually
evaluate the models using our novel translated HumanEval dataset. Our automatic
evaluation shows that knowledge of imperative programming languages in the
pre-training of LLMs may not transfer well to functional languages, but that
code completion on functional languages is feasible. Consequently, this shows
the need for more high-quality Haskell datasets. A manual evaluation on
HumanEval-Haskell indicates CodeGPT frequently generates empty predictions and
extra comments, while UniXcoder more often produces incomplete or incorrect
predictions. Finally, we release HumanEval-Haskell, along with the fine-tuned
models and all code required to reproduce our experiments on GitHub
(https://github.com/AISE-TUDelft/HaskellCCEval).
\\ ( https://arxiv.org/abs/2403.15185 ,  3719kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15250
Date: Fri, 22 Mar 2024 14:47:35 GMT   (4452kb,D)

Title: Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A
  Multifaceted Statistical Approach
Authors: Kun Sun, Rong Wang, Haitao Liu, and Anders S{\o}gaard
Categories: cs.CL cs.AI cs.LG
\\
  Amidst the rapid evolution of LLMs, the significance of evaluation in
comprehending and propelling these models forward is increasingly paramount.
Evaluations have revealed that factors such as scaling, training types,
architectures and other factors profoundly impact the performance of LLMs.
However, the extent and nature of these impacts continue to be subjects of
debate because most assessments have been restricted to a limited number of
models and data points. Clarifying the effects of these factors on performance
scores can be more effectively achieved through a statistical lens. Our study
embarks on a thorough re-examination of these LLMs, targeting the inadequacies
in current evaluation methods. With the advent of a uniform evaluation
framework, our research leverages an expansive dataset of evaluation results,
introducing a comprehensive statistical methodology. This includes the
application of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering
a robust and transparent approach to deciphering LLM performance data. Contrary
to prevailing findings, our results challenge assumptions about emergent
abilities and the influence of given training types and architectures in LLMs.
These findings furnish new perspectives on the characteristics, intrinsic
nature, and developmental trajectories of LLMs. By providing straightforward
and reliable methods to scrutinize and reassess LLM performance data, this
study contributes a nuanced perspective on LLM efficiency and potentials.
\\ ( https://arxiv.org/abs/2403.15250 ,  4452kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15268
Date: Fri, 22 Mar 2024 15:06:45 GMT   (764kb,D)

Title: Imagination Augmented Generation: Learning to Imagine Richer Context for
  Question Answering over Large Language Models
Authors: Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping
  Liu, Jun Zhao
Categories: cs.CL
\\
  Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been
proposed to enhance the knowledge required for question answering over Large
Language Models (LLMs). However, the former depends on external resources, and
both require incorporating the explicit documents into the context, which
results in longer contexts that lead to more resource consumption. Recent works
indicate that LLMs have modeled rich knowledge, albeit not effectively
triggered or activated. Inspired by this, we propose a novel
knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which
simulates the human capacity to compensate for knowledge deficits while
answering questions solely through imagination, without relying on external
resources. Guided by IAG, we propose an imagine richer context method for
question answering (IMcQA), which obtains richer context through the following
two modules: explicit imagination by generating a short dummy document with
long context compress and implicit imagination with HyperNetwork for generating
adapter weights. Experimental results on three datasets demonstrate that IMcQA
exhibits significant advantages in both open-domain and closed-book settings,
as well as in both in-distribution performance and out-of-distribution
generalizations. Our code will be available at
https://github.com/Xnhyacinth/IAG.
\\ ( https://arxiv.org/abs/2403.15268 ,  764kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15273
Date: Fri, 22 Mar 2024 15:16:10 GMT   (784kb,D)

Title: Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs
Authors: Xiaobin Zhang, Liangjun Zang, Qianwen Liu, Shuchong Wei, Songlin Hu
Categories: cs.CL
Comments: 8 pages,6 figures.Accepted to the International Joint Conference on
  Neural Networks (IJCNN2024)
\\
  Event temporal relation (TempRel) is a primary subject of the event relation
extraction task. However, the inherent ambiguity of TempRel increases the
difficulty of the task. With the rise of prompt engineering, it is important to
design effective prompt templates and verbalizers to extract relevant
knowledge. The traditional manually designed templates struggle to extract
precise temporal knowledge. This paper introduces a novel retrieval-augmented
TempRel extraction approach, leveraging knowledge retrieved from large language
models (LLMs) to enhance prompt templates and verbalizers. Our method
capitalizes on the diverse capabilities of various LLMs to generate a wide
array of ideas for template and verbalizer design. Our proposed method fully
exploits the potential of LLMs for generation tasks and contributes more
knowledge to our design. Empirical evaluations across three widely recognized
datasets demonstrate the efficacy of our method in improving the performance of
event temporal relation extraction tasks.
\\ ( https://arxiv.org/abs/2403.15273 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15278
Date: Fri, 22 Mar 2024 15:21:07 GMT   (618kb,D)

Title: Specifying Genericity through Inclusiveness and Abstractness Continuous
  Scales
Authors: Claudia Collacciani, Andrea Amelio Ravelli, Marianna Marcella
  Bolognesi
Categories: cs.CL
\\
  This paper introduces a novel annotation framework for the fine-grained
modeling of Noun Phrases' (NPs) genericity in natural language. The framework
is designed to be simple and intuitive, making it accessible to non-expert
annotators and suitable for crowd-sourced tasks. Drawing from theoretical and
cognitive literature on genericity, this framework is grounded in established
linguistic theory. Through a pilot study, we created a small but crucial
annotated dataset of 324 sentences, serving as a foundation for future
research. To validate our approach, we conducted an evaluation comparing our
continuous annotations with existing binary annotations on the same dataset,
demonstrating the framework's effectiveness in capturing nuanced aspects of
genericity. Our work offers a practical resource for linguists, providing a
first annotated dataset and an annotation scheme designed to build
real-language datasets that can be used in studies on the semantics of
genericity, and NLP practitioners, contributing to the development of
commonsense knowledge repositories valuable in enhancing various NLP
applications.
\\ ( https://arxiv.org/abs/2403.15278 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15279
Date: Fri, 22 Mar 2024 15:22:06 GMT   (76kb,D)

Title: Fundus: A Simple-to-Use News Scraper Optimized for High Quality
  Extractions
Authors: Max Dallabetta, Conrad Dobberstein, Adrian Breiding, Alan Akbik
Categories: cs.CL cs.IR
Comments: 10 pages, 4 figures, submitted to ACL 2024, for a screencast see
  https://www.youtube.com/watch?v=9GJExMelhdI
\\
  This paper introduces Fundus, a user-friendly news scraper that enables users
to obtain millions of high-quality news articles with just a few lines of code.
Unlike existing news scrapers, we use manually crafted, bespoke content
extractors that are specifically tailored to the formatting guidelines of each
supported online newspaper. This allows us to optimize our scraping for quality
such that retrieved news articles are textually complete and without HTML
artifacts. Further, our framework combines both crawling (retrieving HTML from
the web or large web archives) and content extraction into a single pipeline.
By providing a unified interface for a predefined collection of newspapers, we
aim to make Fundus broadly usable even for non-technical users. This paper
gives an overview of the framework, discusses our design choices, and presents
a comparative evaluation against other popular news scrapers. Our evaluation
shows that Fundus yields significantly higher quality extractions (complete and
artifact-free news articles) than prior work. The framework is available on
GitHub under https://github.com/flairNLP/fundus and can be simply installed
using pip.
\\ ( https://arxiv.org/abs/2403.15279 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15293
Date: Fri, 22 Mar 2024 15:40:11 GMT   (152kb)

Title: Human behaviour through a LENS: How Linguistic content triggers Emotions
  and Norms and determines Strategy choices
Authors: Valerio Capraro
Categories: cs.CL cs.GT econ.GN physics.soc-ph q-fin.EC
\\
  Over the last two decades, a growing body of experimental research has
provided evidence that linguistic frames influence human behaviour in economic
games, beyond the economic consequences of the available actions. This article
proposes a novel framework that transcends the traditional confines of
outcome-based preference models. According to the LENS model, the Linguistic
description of the decision problem triggers Emotional responses and suggests
potential Norms of behaviour, which then interact to shape an individual's
Strategic choice. The article reviews experimental evidence that supports each
path of the LENS model. Furthermore, it identifies and discusses several
critical research questions that arise from this model, pointing towards
avenues for future inquiry.
\\ ( https://arxiv.org/abs/2403.15293 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15322
Date: Fri, 22 Mar 2024 16:17:55 GMT   (85kb,D)

Title: CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for
  Named Entity Recognition and Relation Extraction
Authors: Neda Foroutan, Markus Schr\"oder, Andreas Dengel
Categories: cs.CL
\\
  The process of cyber mapping gives insights in relationships among financial
entities and service providers. Centered around the outsourcing practices of
companies within fund prospectuses in Germany, we introduce a dataset
specifically designed for named entity recognition and relation extraction
tasks. The labeling process on 948 sentences was carried out by three experts
which yields to 5,969 annotations for four entity types (Outsourcing, Company,
Location and Software) and 4,102 relation annotations (Outsourcing-Company,
Company-Location). State-of-the-art deep learning models were trained to
recognize entities and extract relations showing first promising results. An
anonymized version of the dataset, along with guidelines and the code used for
model training, are publicly available at
https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.
\\ ( https://arxiv.org/abs/2403.15322 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15351
Date: Fri, 22 Mar 2024 17:06:05 GMT   (8553kb,D)

Title: Multi-Review Fusion-in-Context
Authors: Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan
Categories: cs.CL
Comments: NAACL 2024, findings
\\
  Grounded text generation, encompassing tasks such as long-form
question-answering and summarization, necessitates both content selection and
content consolidation. Current end-to-end methods are difficult to control and
interpret due to their opaqueness. Accordingly, recent works have proposed a
modular approach, with separate components for each step. Specifically, we
focus on the second subtask, of generating coherent text given pre-selected
content in a multi-document setting. Concretely, we formalize
\textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of
source texts with highlighted spans of targeted content. A model then needs to
generate a coherent passage that includes all and only the target information.
Our work includes the development of a curated dataset of 1000 instances in the
reviews domain, alongside a novel evaluation framework for assessing the
faithfulness and coverage of highlights, which strongly correlate to human
judgment. Several baseline models exhibit promising outcomes and provide
insightful analyses. This study lays the groundwork for further exploration of
modular text generation in the multi-document setting, offering potential
improvements in the quality and reliability of generated content. \footnote{Our
benchmark, FuseReviews, including the dataset, evaluation framework and
designated leaderboard, can be found at \url{https://fusereviews.github.io/}.}
\\ ( https://arxiv.org/abs/2403.15351 ,  8553kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15362
Date: Fri, 22 Mar 2024 17:26:05 GMT   (563kb,D)

Title: CoLLEGe: Concept Embedding Generation for Large Language Models
Authors: Ryan Teehan, Brenden Lake, Mengye Ren
Categories: cs.CL cs.AI
\\
  Current language models are unable to quickly learn new concepts on the fly,
often requiring a more involved finetuning process to learn robustly. Prompting
in-context is not robust to context distractions, and often fails to confer
much information about the new concepts. Classic methods for few-shot word
learning in NLP, relying on global word vectors, are less applicable to large
language models. In this paper, we introduce a novel approach named CoLLEGe
(Concept Learning with Language Embedding Generation) to modernize few-shot
concept learning. CoLLEGe is a meta-learning framework capable of generating
flexible embeddings for new concepts using a small number of example sentences
or definitions. Our primary meta-learning objective is simply to facilitate a
language model to make next word predictions in forthcoming sentences, making
it compatible with language model pretraining. We design a series of tasks to
test new concept learning in challenging real-world scenarios, including new
word acquisition, definition inference, and verbal reasoning, and demonstrate
that our method succeeds in each setting without task-specific training.
\\ ( https://arxiv.org/abs/2403.15362 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15364
Date: Fri, 22 Mar 2024 17:32:43 GMT   (15464kb,D)

Title: Towards Knowledge-Grounded Natural Language Understanding and Generation
Authors: Chenxi Whitehouse
Categories: cs.CL
Comments: PhD Thesis
\\
  This thesis investigates how natural language understanding and generation
with transformer models can benefit from grounding the models with knowledge
representations and addresses the following key research questions: (i) Can
knowledge of entities extend its benefits beyond entity-centric tasks, such as
entity linking? (ii) How can we faithfully and effectively extract such
structured knowledge from raw text, especially noisy web text? (iii) How do
other types of knowledge, beyond structured knowledge, contribute to improving
NLP tasks?
  Studies in this thesis find that incorporating relevant and up-to-date
knowledge of entities benefits fake news detection, and entity-focused
code-switching significantly enhances zero-shot cross-lingual transfer on
entity-centric tasks. In terms of effective and faithful approaches to
extracting structured knowledge, it is observed that integrating negative
examples and training with entity planning significantly improves performance.
Additionally, it is established that other general forms of knowledge, such as
parametric and distilled knowledge, enhance multimodal and multilingual
knowledge-intensive tasks. This research shows the tangible benefits of diverse
knowledge integration and motivates further exploration in this direction.
\\ ( https://arxiv.org/abs/2403.15364 ,  15464kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14678
Date: Tue, 12 Mar 2024 11:38:45 GMT   (5492kb,D)

Title: Towards a Framework for Deep Learning Certification in Safety-Critical
  Applications Using Inherently Safe Design and Run-Time Error Detection
Authors: Romeo Valentin
Categories: cs.LG
Comments: Master Thesis
\\
  Although an ever-growing number of applications employ deep learning based
systems for prediction, decision-making, or state estimation, almost no
certification processes have been established that would allow such systems to
be deployed in safety-critical applications. In this work we consider
real-world problems arising in aviation and other safety-critical areas, and
investigate their requirements for a certified model. To this end, we
investigate methodologies from the machine learning research community aimed
towards verifying robustness and reliability of deep learning systems, and
evaluate these methodologies with regard to their applicability to real-world
problems. Then, we establish a new framework towards deep learning
certification based on (i) inherently safe design, and (ii) run-time error
detection. Using a concrete use case from aviation, we show how deep learning
models can recover disentangled variables through the use of weakly-supervised
representation learning. We argue that such a system design is inherently less
prone to common model failures, and can be verified to encode underlying
mechanisms governing the data. Then, we investigate four techniques related to
the run-time safety of a model, namely (i) uncertainty quantification, (ii)
out-of-distribution detection, (iii) feature collapse, and (iv) adversarial
attacks. We evaluate each for their applicability and formulate a set of
desiderata that a certified model should fulfill. Finally, we propose a novel
model structure that exhibits all desired properties discussed in this work,
and is able to make regression and uncertainty predictions, as well as detect
out-of-distribution inputs, while requiring no regression labels to train. We
conclude with a discussion of the current state and expected future progress of
deep learning certification, and its industrial and social implications.
\\ ( https://arxiv.org/abs/2403.14678 ,  5492kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14679
Date: Tue, 12 Mar 2024 15:31:14 GMT   (842kb,D)

Title: Continual Learning by Three-Phase Consolidation
Authors: Davide Maltoni, Lorenzo Pellegrini
Categories: cs.LG cs.CV
Comments: 13 pages, 2 figures, 8 tables. Preprint under review
\\
  TPC (Three-Phase Consolidation) is here introduced as a simple but effective
approach to continually learn new classes (and/or instances of known classes)
while controlling forgetting of previous knowledge. Each experience (a.k.a.
task) is learned in three phases characterized by different rules and learning
dynamics, aimed at removing the class-bias problem (due to class unbalancing)
and limiting gradient-based corrections to prevent forgetting of
underrepresented classes. Several experiments on complex datasets demonstrate
its accuracy and efficiency advantages over competitive existing approaches.
The algorithm and all the results presented in this paper are fully
reproducible thanks to its publication on the Avalanche open framework for
continual learning.
\\ ( https://arxiv.org/abs/2403.14679 ,  842kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14685
Date: Wed, 13 Mar 2024 14:07:20 GMT   (31kb)

Title: Cyclical Log Annealing as a Learning Rate Scheduler
Authors: Philip Naveen
Categories: cs.LG
\\
  A learning rate scheduler is a predefined set of instructions for varying
search stepsizes during model training processes. This paper introduces a new
logarithmic method using harsh restarting of step sizes through stochastic
gradient descent. Cyclical log annealing implements the restart pattern more
aggressively to maybe allow the usage of more greedy algorithms on the online
convex optimization framework. The algorithm was tested on the CIFAR-10 image
datasets, and seemed to perform analogously with cosine annealing on large
transformer-enhanced residual neural networks. Future experiments would involve
testing the scheduler in generative adversarial networks and finding the best
parameters for the scheduler with more experiments.
\\ ( https://arxiv.org/abs/2403.14685 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14687
Date: Wed, 13 Mar 2024 18:07:17 GMT   (1231kb,D)

Title: On the Performance of Imputation Techniques for Missing Values on
  Healthcare Datasets
Authors: Luke Oluwaseye Joel and Wesley Doorsamy and Babu Sena Paul
Categories: cs.LG cs.AI
\\
  Missing values or data is one popular characteristic of real-world datasets,
especially healthcare data. This could be frustrating when using machine
learning algorithms on such datasets, simply because most machine learning
models perform poorly in the presence of missing values. The aim of this study
is to compare the performance of seven imputation techniques, namely Mean
imputation, Median Imputation, Last Observation carried Forward (LOCF)
imputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation,
Missforest imputation, and Multiple imputation by Chained Equations (MICE), on
three healthcare datasets. Some percentage of missing values - 10\%, 15\%, 20\%
and 25\% - were introduced into the dataset, and the imputation techniques were
employed to impute these missing values. The comparison of their performance
was evaluated by using root mean squared error (RMSE) and mean absolute error
(MAE). The results show that Missforest imputation performs the best followed
by MICE imputation. Additionally, we try to determine whether it is better to
perform feature selection before imputation or vice versa by using the
following metrics - the recall, precision, f1-score and accuracy. Due to the
fact that there are few literature on this and some debate on the subject among
researchers, we hope that the results from this experiment will encourage data
scientists and researchers to perform imputation first before feature selection
when dealing with data containing missing values.
\\ ( https://arxiv.org/abs/2403.14687 ,  1231kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14688
Date: Wed, 13 Mar 2024 20:35:44 GMT   (836kb,D)

Title: Kernel Alignment for Unsupervised Feature Selection via Matrix
  Factorization
Authors: Ziyuan Lin and Deanna Needell
Categories: cs.LG cs.NA math.NA
MSC-class: 65F10, 65F22, 90C26
\\
  By removing irrelevant and redundant features, feature selection aims to find
a good representation of the original features. With the prevalence of
unlabeled data, unsupervised feature selection has been proven effective in
alleviating the so-called curse of dimensionality. Most existing matrix
factorization-based unsupervised feature selection methods are built upon
subspace learning, but they have limitations in capturing nonlinear structural
information among features. It is well-known that kernel techniques can capture
nonlinear structural information. In this paper, we construct a model by
integrating kernel functions and kernel alignment, which can be equivalently
characterized as a matrix factorization problem. However, such an extension
raises another issue: the algorithm performance heavily depends on the choice
of kernel, which is often unknown a priori. Therefore, we further propose a
multiple kernel-based learning method. By doing so, our model can learn both
linear and nonlinear similarity information and automatically generate the most
appropriate kernel. Experimental analysis on real-world data demonstrates that
the two proposed methods outperform other classic and state-of-the-art
unsupervised feature selection methods in terms of clustering results and
redundancy reduction in almost all datasets tested.
\\ ( https://arxiv.org/abs/2403.14688 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14715
Date: Tue, 19 Mar 2024 06:46:24 GMT   (2970kb,D)

Title: Understanding Why Label Smoothing Degrades Selective Classification and
  How to Fix It
Authors: Guoxuan Xia, Olivier Laurent, Gianni Franchi, Christos-Savvas Bouganis
Categories: cs.LG cs.AI cs.CV
\\
  Label smoothing (LS) is a popular regularisation method for training deep
neural network classifiers due to its effectiveness in improving test accuracy
and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by
uniformly distributing probability mass to other classes, reducing overfitting.
In this work, we reveal that LS negatively affects selective classification
(SC) - where the aim is to reject misclassifications using a model's predictive
uncertainty. We first demonstrate empirically across a range of tasks and
architectures that LS leads to a consistent degradation in SC. We then explain
this by analysing logit-level gradients, showing that LS exacerbates
overconfidence and underconfidence by regularising the max logit more when the
probability of error is low, and less when the probability of error is high.
This elucidates previously reported experimental results where strong
classifiers underperform in SC. We then demonstrate the empirical effectiveness
of logit normalisation for recovering lost SC performance caused by LS.
Furthermore, based on our gradient analysis, we explain why such normalisation
is effective. We will release our code shortly.
\\ ( https://arxiv.org/abs/2403.14715 ,  2970kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14716
Date: Tue, 19 Mar 2024 06:48:40 GMT   (1238kb)

Title: Distributed Learning based on 1-Bit Gradient Coding in the Presence of
  Stragglers
Authors: Chengxi Li and Mikael Skoglund
Categories: cs.LG cs.DC
Comments: Accepted by IEEE TCOM
\\
  This paper considers the problem of distributed learning (DL) in the presence
of stragglers. For this problem, DL methods based on gradient coding have been
widely investigated, which redundantly distribute the training data to the
workers to guarantee convergence when some workers are stragglers. However,
these methods require the workers to transmit real-valued vectors during the
process of learning, which induces very high communication burden. To overcome
this drawback, we propose a novel DL method based on 1-bit gradient coding
(1-bit GCDL), where 1-bit data encoded from the locally computed gradients are
transmitted by the workers to reduce the communication overhead. We
theoretically provide the convergence guarantees of the proposed method for
both the convex loss functions and nonconvex loss functions. It is shown
empirically that 1-bit GC-DL outperforms the baseline methods, which attains
better learning performance under the same communication overhead.
\\ ( https://arxiv.org/abs/2403.14716 ,  1238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14718
Date: Tue, 19 Mar 2024 09:34:01 GMT   (24126kb,D)

Title: FedSR: A Semi-Decentralized Federated Learning Algorithm for Non-IIDness
  in IoT System
Authors: Jianjun Huang, Lixin Ye, Li Kang
Categories: cs.LG cs.DC
Comments: 11 pages, 10 figures
\\
  In the Industrial Internet of Things (IoT), a large amount of data will be
generated every day. Due to privacy and security issues, it is difficult to
collect all these data together to train deep learning models, thus the
federated learning, a distributed machine learning paradigm that protects data
privacy, has been widely used in IoT. However, in practical federated learning,
the data distributions usually have large differences across devices, and the
heterogeneity of data will deteriorate the performance of the model. Moreover,
federated learning in IoT usually has a large number of devices involved in
training, and the limited communication resource of cloud servers become a
bottleneck for training. To address the above issues, in this paper, we combine
centralized federated learning with decentralized federated learning to design
a semi-decentralized cloud-edge-device hierarchical federated learning
framework, which can mitigate the impact of data heterogeneity, and can be
deployed at lage scale in IoT. To address the effect of data heterogeneity, we
use an incremental subgradient optimization algorithm in each ring cluster to
improve the generalization ability of the ring cluster models. Our extensive
experiments show that our approach can effectively mitigate the impact of data
heterogeneity and alleviate the communication bottleneck in cloud servers.
\\ ( https://arxiv.org/abs/2403.14718 ,  24126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14735
Date: Thu, 21 Mar 2024 10:08:37 GMT   (2004kb,D)

Title: Foundation Models for Time Series Analysis: A Tutorial and Survey
Authors: Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin
  Song, Shirui Pan, Qingsong Wen
Categories: cs.LG
\\
  Time series analysis stands as a focal point within the data mining
community, serving as a cornerstone for extracting valuable insights crucial to
a myriad of real-world applications. Recent advancements in Foundation Models
(FMs) have fundamentally reshaped the paradigm of model design for time series
analysis, boosting various downstream tasks in practice. These innovative
approaches often leverage pre-trained or fine-tuned FMs to harness generalized
knowledge tailored specifically for time series analysis. In this survey, we
aim to furnish a comprehensive and up-to-date overview of FMs for time series
analysis. While prior surveys have predominantly focused on either the
application or the pipeline aspects of FMs in time series analysis, they have
often lacked an in-depth understanding of the underlying mechanisms that
elucidate why and how FMs benefit time series analysis. To address this gap,
our survey adopts a model-centric classification, delineating various pivotal
elements of time-series FMs, including model architectures, pre-training
techniques, adaptation methods, and data modalities. Overall, this survey
serves to consolidate the latest advancements in FMs pertinent to time series
analysis, accentuating their theoretical underpinnings, recent strides in
development, and avenues for future research exploration.
\\ ( https://arxiv.org/abs/2403.14735 ,  2004kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14737
Date: Thu, 21 Mar 2024 13:54:36 GMT   (1125kb,D)

Title: FedMef: Towards Memory-efficient Federated Dynamic Pruning
Authors: Hong Huang, Weiming Zhuang, Chen Chen, Lingjuan Lyu
Categories: cs.LG cs.DC
Comments: Accepted by CVPR2024
\\
  Federated learning (FL) promotes decentralized training while prioritizing
data confidentiality. However, its application on resource-constrained devices
is challenging due to the high demand for computation and memory resources to
train deep learning models. Neural network pruning techniques, such as dynamic
pruning, could enhance model efficiency, but directly adopting them in FL still
poses substantial challenges, including post-pruning performance degradation,
high activation memory usage, etc. To address these challenges, we propose
FedMef, a novel and memory-efficient federated dynamic pruning framework.
FedMef comprises two key components. First, we introduce the budget-aware
extrusion that maintains pruning efficiency while preserving post-pruning
performance by salvaging crucial information from parameters marked for pruning
within a given budget. Second, we propose scaled activation pruning to
effectively reduce activation memory footprints, which is particularly
beneficial for deploying FL to memory-limited devices. Extensive experiments
demonstrate the effectiveness of our proposed FedMef. In particular, it
achieves a significant reduction of 28.5% in memory footprint compared to
state-of-the-art methods while obtaining superior accuracy.
\\ ( https://arxiv.org/abs/2403.14737 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14738
Date: Thu, 21 Mar 2024 14:26:29 GMT   (739kb)

Title: A task of anomaly detection for a smart satellite Internet of things
  system
Authors: Zilong Shao
Categories: cs.LG eess.SP
\\
  When the equipment is working, real-time collection of environmental sensor
data for anomaly detection is one of the key links to prevent industrial
process accidents and network attacks and ensure system security. However,
under the environment with specific real-time requirements, the anomaly
detection for environmental sensors still faces the following difficulties: (1)
The complex nonlinear correlation characteristics between environmental sensor
data variables lack effective expression methods, and the distribution between
the data is difficult to be captured. (2) it is difficult to ensure the
real-time monitoring requirements by using complex machine learning models, and
the equipment cost is too high. (3) Too little sample data leads to less
labeled data in supervised learning. This paper proposes an unsupervised deep
learning anomaly detection system. Based on the generative adversarial network
and self-attention mechanism, considering the different feature information
contained in the local subsequences, it automatically learns the complex linear
and nonlinear dependencies between environmental sensor variables, and uses the
anomaly score calculation method combining reconstruction error and
discrimination error. It can monitor the abnormal points of real sensor data
with high real-time performance and can run on the intelligent satellite
Internet of things system, which is suitable for the real working environment.
Anomaly detection outperforms baseline methods in most cases and has good
interpretability, which can be used to prevent industrial accidents and
cyber-attacks for monitoring environmental sensors.
\\ ( https://arxiv.org/abs/2403.14738 ,  739kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14800
Date: Thu, 21 Mar 2024 19:28:17 GMT   (108kb,D)

Title: Deep Active Learning: A Reality Check
Authors: Edrina Gashi, Jiankang Deng, Ismail Elezi
Categories: cs.LG cs.AI cs.CV
\\
  We conduct a comprehensive evaluation of state-of-the-art deep active
learning methods. Surprisingly, under general settings, no single-model method
decisively outperforms entropy-based active learning, and some even fall short
of random sampling. We delve into overlooked aspects like starting budget,
budget step, and pretraining's impact, revealing their significance in
achieving superior results. Additionally, we extend our evaluation to other
tasks, exploring the active learning effectiveness in combination with
semi-supervised learning, and object detection. Our experiments provide
valuable insights and concrete recommendations for future active learning
studies. By uncovering the limitations of current methods and understanding the
impact of different experimental settings, we aim to inspire more efficient
training of deep learning models in real-world scenarios with limited
annotation budgets. This work contributes to advancing active learning's
efficacy in deep learning and empowers researchers to make informed decisions
when applying active learning to their tasks.
\\ ( https://arxiv.org/abs/2403.14800 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14829
Date: Thu, 21 Mar 2024 20:43:34 GMT   (2134kb,D)

Title: Hyperbolic Secant representation of the logistic function: Application
  to probabilistic Multiple Instance Learning for CT intracranial hemorrhage
  detection
Authors: F. M. Castro-Mac\'ias, P. Morales-\'Alvarez, Y. Wu, R. Molina, A. K.
  Katsaggelos
Categories: cs.LG stat.ML
Comments: 48 pages, 12 figures, published in Artificial Intelligence Journal
Journal-ref: Journal: Artificial Intelligence, Pages: 104115, Publisher:
  Elsevier, Year: 2024
DOI: 10.1016/j.artint.2024.104115
\\
  Multiple Instance Learning (MIL) is a weakly supervised paradigm that has
been successfully applied to many different scientific areas and is
particularly well suited to medical imaging. Probabilistic MIL methods, and
more specifically Gaussian Processes (GPs), have achieved excellent results due
to their high expressiveness and uncertainty quantification capabilities. One
of the most successful GP-based MIL methods, VGPMIL, resorts to a variational
bound to handle the intractability of the logistic function. Here, we formulate
VGPMIL using P\'olya-Gamma random variables. This approach yields the same
variational posterior approximations as the original VGPMIL, which is a
consequence of the two representations that the Hyperbolic Secant distribution
admits. This leads us to propose a general GP-based MIL method that takes
different forms by simply leveraging distributions other than the Hyperbolic
Secant one. Using the Gamma distribution we arrive at a new approach that
obtains competitive or superior predictive performance and efficiency. This is
validated in a comprehensive experimental study including one synthetic MIL
dataset, two well-known MIL benchmarks, and a real-world medical problem. We
expect that this work provides useful ideas beyond MIL that can foster further
research in the field.
\\ ( https://arxiv.org/abs/2403.14829 ,  2134kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14833
Date: Thu, 21 Mar 2024 21:05:59 GMT   (1729kb,D)

Title: Model order reduction of deep structured state-space models: A
  system-theoretic approach
Authors: Marco Forgione, Manas Mejari, Dario Piga
Categories: cs.LG cs.SY eess.SY
\\
  With a specific emphasis on control design objectives, achieving accurate
system modeling with limited complexity is crucial in parametric system
identification. The recently introduced deep structured state-space models
(SSM), which feature linear dynamical blocks as key constituent components,
offer high predictive performance. However, the learned representations often
suffer from excessively large model orders, which render them unsuitable for
control design purposes. The current paper addresses this challenge by means of
system-theoretic model order reduction techniques that target the linear
dynamical blocks of SSMs. We introduce two regularization terms which can be
incorporated into the training loss for improved model order reduction. In
particular, we consider modal $\ell_1$ and Hankel nuclear norm regularization
to promote sparsity, allowing one to retain only the relevant states without
sacrificing accuracy. The presented regularizers lead to advantages in terms of
parsimonious representations and faster inference resulting from the reduced
order models. The effectiveness of the proposed methodology is demonstrated
using real-world ground vibration data from an aircraft.
\\ ( https://arxiv.org/abs/2403.14833 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14843
Date: Thu, 21 Mar 2024 21:27:39 GMT   (2563kb,D)

Title: Local Causal Discovery with Linear non-Gaussian Cyclic Models
Authors: Haoyue Dai, Ignavier Ng, Yujia Zheng, Zhengqing Gao, Kun Zhang
Categories: cs.LG cs.AI
Comments: Appears at AISTATS 2024
\\
  Local causal discovery is of great practical significance, as there are often
situations where the discovery of the global causal structure is unnecessary,
and the interest lies solely on a single target variable. Most existing local
methods utilize conditional independence relations, providing only a partially
directed graph, and assume acyclicity for the ground-truth structure, even
though real-world scenarios often involve cycles like feedback mechanisms. In
this work, we present a general, unified local causal discovery method with
linear non-Gaussian models, whether they are cyclic or acyclic. We extend the
application of independent component analysis from the global context to
independent subspace analysis, enabling the exact identification of the
equivalent local directed structures and causal strengths from the Markov
blanket of the target variable. We also propose an alternative regression-based
method in the particular acyclic scenarios. Our identifiability results are
empirically validated using both synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2403.14843 ,  2563kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14853
Date: Thu, 21 Mar 2024 21:56:44 GMT   (254kb,D)

Title: iSpLib: A Library for Accelerating Graph Neural Networks using
  Auto-tuned Sparse Operations
Authors: Md Saidul Hoque Anik, Pranav Badhe, Rohit Gampa, and Ariful Azad
Categories: cs.LG cs.DC cs.PF
\\
  Core computations in Graph Neural Network (GNN) training and inference are
often mapped to sparse matrix operations such as sparse-dense matrix
multiplication (SpMM). These sparse operations are harder to optimize by manual
tuning because their performance depends significantly on the sparsity of input
graphs, GNN models, and computing platforms. To address this challenge, we
present iSpLib, a PyTorch-based C++ library equipped with auto-tuned sparse
operations. iSpLib expedites GNN training with a cache-enabled backpropagation
that stores intermediate matrices in local caches. The library offers a
user-friendly Python plug-in that allows users to take advantage of our
optimized PyTorch operations out-of-the-box for any existing linear
algebra-based PyTorch implementation of popular GNNs (Graph Convolution
Network, GraphSAGE, Graph Inference Network, etc.) with only two lines of
additional code. We demonstrate that iSpLib obtains up to 27x overall training
speedup compared to the equivalent PyTorch 2.1.0 and PyTorch Geometric 2.4.0
implementations on the CPU. Our library is publicly available at
https://github.com/HipGraph/iSpLib (https://doi.org/10.5281/zenodo.10806511).
\\ ( https://arxiv.org/abs/2403.14853 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14917
Date: Fri, 22 Mar 2024 02:41:57 GMT   (107kb,D)

Title: Mean-field Analysis on Two-layer Neural Networks from a Kernel
  Perspective
Authors: Shokichi Takakura, Taiji Suzuki
Categories: cs.LG
\\
  In this paper, we study the feature learning ability of two-layer neural
networks in the mean-field regime through the lens of kernel methods. To focus
on the dynamics of the kernel induced by the first layer, we utilize a
two-timescale limit, where the second layer moves much faster than the first
layer. In this limit, the learning problem is reduced to the minimization
problem over the intrinsic kernel. Then, we show the global convergence of the
mean-field Langevin dynamics and derive time and particle discretization error.
We also demonstrate that two-layer neural networks can learn a union of
multiple reproducing kernel Hilbert spaces more efficiently than any kernel
methods, and neural networks acquire data-dependent kernel which aligns with
the target function. In addition, we develop a label noise procedure, which
converges to the global optimum and show that the degrees of freedom appears as
an implicit regularization.
\\ ( https://arxiv.org/abs/2403.14917 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14918
Date: Fri, 22 Mar 2024 02:42:38 GMT   (7276kb,D)

Title: Deep learning-based method for weather forecasting: A case study in
  Itoshima
Authors: Yuzhong Cheng, Linh Thi Hoai Nguyen, Akinori Ozaki, Ton Viet Ta
Categories: cs.LG
\\
  Accurate weather forecasting is of paramount importance for a wide range of
practical applications, drawing substantial scientific and societal interest.
However, the intricacies of weather systems pose substantial challenges to
accurate predictions. This research introduces a multilayer perceptron model
tailored for weather forecasting in Itoshima, Kyushu, Japan. Our meticulously
designed architecture demonstrates superior performance compared to existing
models, surpassing benchmarks such as Long Short-Term Memory and Recurrent
Neural Networks.
\\ ( https://arxiv.org/abs/2403.14918 ,  7276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14922
Date: Fri, 22 Mar 2024 02:50:42 GMT   (6130kb,D)

Title: CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR
Authors: Minghui Qiu (DSA, Hong Kong University of Science and
  Technology,Guangzhou), Yandao Huang (CSE, Hong Kong University of Science and
  Technology), Lin Chen (DSA, Hong Kong University of Science and Technology,
  Guangzhou), Lu Wang (CSSE, Shenzhen University), Kaishun Wu (DSA and IoT,
  Hong Kong University of Science and Technology, Guangzhou)
Categories: cs.LG cs.NI
\\
  In recent years, emerging research on mobile sensing has led to novel
scenarios that enhance daily life for humans, but dynamic usage conditions
often result in performance degradation when systems are deployed in real-world
settings. Existing solutions typically employ one-off adaptation schemes based
on neural networks, which struggle to ensure robustness against uncertain
drifting conditions in human-centric sensing scenarios. In this paper, we
propose CODA, a COst-efficient Domain Adaptation mechanism for mobile sensing
that addresses real-time drifts from the data distribution perspective with
active learning theory, ensuring cost-efficient adaptation directly on the
device. By incorporating a clustering loss and importance-weighted active
learning algorithm, CODA retains the relationship between different clusters
during cost-effective instance-level updates, preserving meaningful structure
within the data distribution. We also showcase its generalization by seamlessly
integrating it with Neural Network-based solutions for Human Activity
Recognition tasks. Through meticulous evaluations across diverse datasets,
including phone-based, watch-based, and integrated sensor-based sensing tasks,
we demonstrate the feasibility and potential of online adaptation with CODA.
The promising results achieved by CODA, even without learnable parameters, also
suggest the possibility of realizing unobtrusive adaptation through specific
application designs with sufficient feedback.
\\ ( https://arxiv.org/abs/2403.14922 ,  6130kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14941
Date: Fri, 22 Mar 2024 04:21:40 GMT   (47623kb,D)

Title: Unifying Lane-Level Traffic Prediction from a Graph Structural
  Perspective: Benchmark and Baseline
Authors: Shuhao Li, Yue Cui, Jingyi Xu, Libin Li, Lingkai Meng, Weidong Yang,
  Fan Zhang, Xiaofang Zhou
Categories: cs.LG cs.AI
\\
  Traffic prediction has long been a focal and pivotal area in research,
witnessing both significant strides from city-level to road-level predictions
in recent years. With the advancement of Vehicle-to-Everything (V2X)
technologies, autonomous driving, and large-scale models in the traffic domain,
lane-level traffic prediction has emerged as an indispensable direction.
However, further progress in this field is hindered by the absence of
comprehensive and unified evaluation standards, coupled with limited public
availability of data and code. This paper extensively analyzes and categorizes
existing research in lane-level traffic prediction, establishes a unified
spatial topology structure and prediction tasks, and introduces a simple
baseline model, GraphMLP, based on graph structure and MLP networks. We have
replicated codes not publicly available in existing studies and, based on this,
thoroughly and fairly assessed various models in terms of effectiveness,
efficiency, and applicability, providing insights for practical applications.
Additionally, we have released three new datasets and corresponding codes to
accelerate progress in this field, all of which can be found on
https://github.com/ShuhaoLii/TITS24LaneLevel-Traffic-Benchmark.
\\ ( https://arxiv.org/abs/2403.14941 ,  47623kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14949
Date: Fri, 22 Mar 2024 04:44:43 GMT   (5404kb,D)

Title: Addressing Concept Shift in Online Time Series Forecasting:
  Detect-then-Adapt
Authors: YiFan Zhang, Weiqi Chen, Zhaoyang Zhu, Dalin Qin, Liang Sun, Xue Wang,
  Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin
Categories: cs.LG
Comments: 7 figures, 14 pages. arXiv admin note: text overlap with
  arXiv:2309.12659
\\
  Online updating of time series forecasting models aims to tackle the
challenge of concept drifting by adjusting forecasting models based on
streaming data. While numerous algorithms have been developed, most of them
focus on model design and updating. In practice, many of these methods struggle
with continuous performance regression in the face of accumulated concept
drifts over time. To address this limitation, we present a novel approach,
Concept \textbf{D}rift \textbf{D}etection an\textbf{D} \textbf{A}daptation
(D3A), that first detects drifting conception and then aggressively adapts the
current model to the drifted concepts after the detection for rapid adaption.
To best harness the utility of historical data for model adaptation, we propose
a data augmentation strategy introducing Gaussian noise into existing training
instances. It helps mitigate the data distribution gap, a critical factor
contributing to train-test performance inconsistency. The significance of our
data augmentation process is verified by our theoretical analysis. Our
empirical studies across six datasets demonstrate the effectiveness of D3A in
improving model adaptation capability. Notably, compared to a simple Temporal
Convolutional Network (TCN) baseline, D3A reduces the average Mean Squared
Error (MSE) by $43.9\%$. For the state-of-the-art (SOTA) model, the MSE is
reduced by $33.3\%$.
\\ ( https://arxiv.org/abs/2403.14949 ,  5404kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14951
Date: Fri, 22 Mar 2024 05:04:48 GMT   (1612kb,D)

Title: Simple Graph Condensation
Authors: Zhenbang Xiao, Yu Wang, Shunyu Liu, Huiqiong Wang, Mingli Song, Tongya
  Zheng
Categories: cs.LG cs.AI cs.SI
Comments: Under review
\\
  The burdensome training costs on large-scale graphs have aroused significant
interest in graph condensation, which involves tuning Graph Neural Networks
(GNNs) on a small condensed graph for use on the large-scale original graph.
Existing methods primarily focus on aligning key metrics between the condensed
and original graphs, such as gradients, distribution and trajectory of GNNs,
yielding satisfactory performance on downstream tasks. However, these complex
metrics necessitate intricate computations and can potentially disrupt the
optimization process of the condensation graph, making the condensation process
highly demanding and unstable. Motivated by the recent success of simplified
models in various fields, we propose a simplified approach to metric alignment
in graph condensation, aiming to reduce unnecessary complexity inherited from
GNNs. In our approach, we eliminate external parameters and exclusively retain
the target condensed graph during the condensation process. Following the
hierarchical aggregation principles of GNNs, we introduce the Simple Graph
Condensation (SimGC) framework, which aligns the condensed graph with the
original graph from the input layer to the prediction layer, guided by a
pre-trained Simple Graph Convolution (SGC) model on the original graph. As a
result, both graphs possess the similar capability to train GNNs. This
straightforward yet effective strategy achieves a significant speedup of up to
10 times compared to existing graph condensation methods while performing on
par with state-of-the-art baselines. Comprehensive experiments conducted on
seven benchmark datasets demonstrate the effectiveness of SimGC in prediction
accuracy, condensation time, and generalization capability. Our code will be
made publicly available.
\\ ( https://arxiv.org/abs/2403.14951 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14958
Date: Fri, 22 Mar 2024 05:23:31 GMT   (215kb,D)

Title: Adapprox: Adaptive Approximation in Adam Optimization via Randomized
  Low-Rank Matrices
Authors: Pengxiang Zhao, Ping Li, Yingjie Gu, Yi Zheng, Stephan Ludger
  K\"olker, Zhefeng Wang, Xiaoming Yuan
Categories: cs.LG cs.CL math.OC
\\
  As deep learning models exponentially increase in size, optimizers such as
Adam encounter significant memory consumption challenges due to the storage of
first and second moment data. Current memory-efficient methods like Adafactor
and CAME often compromise accuracy with their matrix factorization techniques.
Addressing this, we introduce Adapprox, a novel approach that employs
randomized low-rank matrix approximation for a more effective and accurate
approximation of Adam's second moment. Adapprox features an adaptive rank
selection mechanism, finely balancing accuracy and memory efficiency, and
includes an optional cosine similarity guidance strategy to enhance stability
and expedite convergence. In GPT-2 training and downstream tasks, Adapprox
surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings
for the 117M and 345M models, respectively, with the first moment enabled, and
further increases these savings without the first moment. Besides, it enhances
convergence speed and improves downstream task performance relative to its
counterparts.
\\ ( https://arxiv.org/abs/2403.14958 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14999
Date: Fri, 22 Mar 2024 07:21:09 GMT   (96kb,D)

Title: Magic for the Age of Quantized DNNs
Authors: Yoshihide Sawada, Ryuji Saiin, Kazuma Suetake
Categories: cs.LG cs.AI cs.CV cs.NE
Comments: 14 pages, 5 figures, 4 tables
\\
  Recently, the number of parameters in DNNs has explosively increased, as
exemplified by LLMs (Large Language Models), making inference on small-scale
computers more difficult. Model compression technology is, therefore, essential
for integration into products. In this paper, we propose a method of
quantization-aware training. We introduce a novel normalization (Layer-Batch
Normalization) that is independent of the mini-batch size and does not require
any additional computation cost during inference. Then, we quantize the weights
by the scaled round-clip function with the weight standardization. We also
quantize activation functions using the same function and apply surrogate
gradients to train the model with both quantized weights and the quantized
activation functions. We call this method Magic for the age of Quantised DNNs
(MaQD). Experimental results show that our quantization method can be achieved
with minimal accuracy degradation.
\\ ( https://arxiv.org/abs/2403.14999 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15012
Date: Fri, 22 Mar 2024 07:56:31 GMT   (596kb,D)

Title: Empirical investigation of multi-source cross-validation in clinical
  machine learning
Authors: Tuija Leinonen, David Wong, Ali Wahab, Ramesh Nadarajah, Matti Kaisti,
  Antti Airola
Categories: cs.LG stat.ML
Comments: 14 pages, 3 figures
\\
  Traditionally, machine learning-based clinical prediction models have been
trained and evaluated on patient data from a single source, such as a hospital.
Cross-validation methods can be used to estimate the accuracy of such models on
new patients originating from the same source, by repeated random splitting of
the data. However, such estimates tend to be highly overoptimistic when
compared to accuracy obtained from deploying models to sources not represented
in the dataset, such as a new hospital. The increasing availability of
multi-source medical datasets provides new opportunities for obtaining more
comprehensive and realistic evaluations of expected accuracy through
source-level cross-validation designs.
  In this study, we present a systematic empirical evaluation of standard
K-fold cross-validation and leave-source-out cross-validation methods in a
multi-source setting. We consider the task of electrocardiogram based
cardiovascular disease classification, combining and harmonizing the openly
available PhysioNet CinC Challenge 2021 and the Shandong Provincial Hospital
datasets for our study.
  Our results show that K-fold cross-validation, both on single-source and
multi-source data, systemically overestimates prediction performance when the
end goal is to generalize to new sources. Leave-source-out cross-validation
provides more reliable performance estimates, having close to zero bias though
larger variability. The evaluation highlights the dangers of obtaining
misleading cross-validation results on medical data and demonstrates how these
issues can be mitigated when having access to multi-source data.
\\ ( https://arxiv.org/abs/2403.15012 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15022
Date: Fri, 22 Mar 2024 08:11:14 GMT   (25273kb,D)

Title: Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude
  Pruning
Authors: Tausifa Jan Saleem, Ramanjit Ahuja, Surendra Prasad, Brejesh Lall
Categories: cs.LG
\\
  Lottery ticket hypothesis for deep neural networks emphasizes the importance
of initialization used to re-train the sparser networks obtained using the
iterative magnitude pruning process. An explanation for why the specific
initialization proposed by the lottery ticket hypothesis tends to work better
in terms of generalization (and training) performance has been lacking.
Moreover, the underlying principles in iterative magnitude pruning, like the
pruning of smaller magnitude weights and the role of the iterative process,
lack full understanding and explanation. In this work, we attempt to provide
insights into these phenomena by empirically studying the volume/geometry and
loss landscape characteristics of the solutions obtained at various stages of
the iterative magnitude pruning process.
\\ ( https://arxiv.org/abs/2403.15022 ,  25273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15025
Date: Fri, 22 Mar 2024 08:13:33 GMT   (1558kb,D)

Title: Robust Conformal Prediction under Distribution Shift via
  Physics-Informed Structural Causal Model
Authors: Rui Xu, Yue Sun, Chao Chen, Parv Venkitasubramaniam, Sihong Xie
Categories: cs.LG stat.ML
\\
  Uncertainty is critical to reliable decision-making with machine learning.
Conformal prediction (CP) handles uncertainty by predicting a set on a test
input, hoping the set to cover the true label with at least $(1-\alpha)$
confidence. This coverage can be guaranteed on test data even if the marginal
distributions $P_X$ differ between calibration and test datasets. However, as
it is common in practice, when the conditional distribution $P_{Y|X}$ is
different on calibration and test data, the coverage is not guaranteed and it
is essential to measure and minimize the coverage loss under distributional
shift at \textit{all} possible confidence levels. To address these issues, we
upper bound the coverage difference at all levels using the cumulative density
functions of calibration and test conformal scores and Wasserstein distance.
Inspired by the invariance of physics across data distributions, we propose a
physics-informed structural causal model (PI-SCM) to reduce the upper bound. We
validated that PI-SCM can improve coverage robustness along confidence level
and test domain on a traffic speed prediction task and an epidemic spread task
with multiple real-world datasets.
\\ ( https://arxiv.org/abs/2403.15025 ,  1558kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15027
Date: Fri, 22 Mar 2024 08:17:00 GMT   (67kb,D)

Title: Grey-informed neural network for time-series forecasting
Authors: Wanli Xie and Ruibin Zhao and Zhenguo Xu and Tingting Liang
Categories: cs.LG cs.AI
\\
  Neural network models have shown outstanding performance and successful
resolutions to complex problems in various fields. However, the majority of
these models are viewed as black-box, requiring a significant amount of data
for development. Consequently, in situations with limited data, constructing
appropriate models becomes challenging due to the lack of transparency and
scarcity of data. To tackle these challenges, this study suggests the
implementation of a grey-informed neural network (GINN). The GINN ensures that
the output of the neural network follows the differential equation model of the
grey system, improving interpretability. Moreover, incorporating prior
knowledge from grey system theory enables traditional neural networks to
effectively handle small data samples. Our proposed model has been observed to
uncover underlying patterns in the real world and produce reliable forecasts
based on empirical data.
\\ ( https://arxiv.org/abs/2403.15027 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15045
Date: Fri, 22 Mar 2024 09:02:12 GMT   (83kb)

Title: DP-Dueling: Learning from Preference Feedback without Compromising User
  Privacy
Authors: Aadirupa Saha, Hilal Asi
Categories: cs.LG cs.CR
\\
  We consider the well-studied dueling bandit problem, where a learner aims to
identify near-optimal actions using pairwise comparisons, under the constraint
of differential privacy. We consider a general class of utility-based
preference matrices for large (potentially unbounded) decision spaces and give
the first differentially private dueling bandit algorithm for active learning
with user preferences. Our proposed algorithms are computationally efficient
with near-optimal performance, both in terms of the private and non-private
regret bound. More precisely, we show that when the decision space is of finite
size $K$, our proposed algorithm yields order optimal $O\Big(\sum_{i =
2}^K\log\frac{KT}{\Delta_i} + \frac{K}{\epsilon}\Big)$ regret bound for pure
$\epsilon$-DP, where $\Delta_i$ denotes the suboptimality gap of the $i$-th
arm. We also present a matching lower bound analysis which proves the
optimality of our algorithms. Finally, we extend our results to any general
decision space in $d$-dimensions with potentially infinite arms and design an
$\epsilon$-DP algorithm with regret $\tilde{O} \left( \frac{d^6}{\kappa
\epsilon } + \frac{ d\sqrt{T }}{\kappa} \right)$, providing privacy for free
when $T \gg d$.
\\ ( https://arxiv.org/abs/2403.15045 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15073
Date: Fri, 22 Mar 2024 09:54:04 GMT   (225kb,D)

Title: On the Inclusion of Charge and Spin States in Cartesian Tensor Neural
  Network Potentials
Authors: Guillem Simeon, Antonio Mirarchi, Raul P. Pelaez, Raimondas Galvelis,
  Gianni De Fabritiis
Categories: cs.LG physics.chem-ph physics.comp-ph
\\
  In this letter, we present an extension to TensorNet, a state-of-the-art
equivariant Cartesian tensor neural network potential, allowing it to handle
charged molecules and spin states without architectural changes or increased
costs. By incorporating these attributes, we address input degeneracy issues,
enhancing the model's predictive accuracy across diverse chemical systems. This
advancement significantly broadens TensorNet's applicability, maintaining its
efficiency and accuracy.
\\ ( https://arxiv.org/abs/2403.15073 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15077
Date: Fri, 22 Mar 2024 10:02:13 GMT   (117kb,D)

Title: GTAGCN: Generalized Topology Adaptive Graph Convolutional Networks
Authors: Sukhdeep Singh, Anuj Sharma, Vinod Kumar Chauhan
Categories: cs.LG
Comments: 2 figures, 3 tables and 26 pages
\\
  Graph Neural Networks (GNN) have emerged as a popular and standard approach
for learning from graph-structured data. The literature on GNN highlights the
potential of this evolving research area and its widespread adoption in
real-life applications. However, most of the approaches are either new in
concept or derived from specific techniques. Therefore, the potential of more
than one approach in hybrid form has not been studied extensively, which can be
well utilized for sequenced data or static data together. We derive a hybrid
approach based on two established techniques as generalized aggregation
networks and topology adaptive graph convolution networks that solve our
purpose to apply on both types of sequenced and static nature of data,
effectively. The proposed method applies to both node and graph classification.
Our empirical analysis reveals that the results are at par with literature
results and better for handwritten strokes as sequenced data, where graph
structures have not been explored.
\\ ( https://arxiv.org/abs/2403.15077 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15079
Date: Fri, 22 Mar 2024 10:05:21 GMT   (420kb,D)

Title: Automated Feature Selection for Inverse Reinforcement Learning
Authors: Daulet Baimukashev, Gokhan Alcan, Ville Kyrki
Categories: cs.LG cs.RO
Comments: 7 pages, 4 figures
MSC-class: 68T40, 68T05
\\
  Inverse reinforcement learning (IRL) is an imitation learning approach to
learning reward functions from expert demonstrations. Its use avoids the
difficult and tedious procedure of manual reward specification while retaining
the generalization power of reinforcement learning. In IRL, the reward is
usually represented as a linear combination of features. In continuous state
spaces, the state variables alone are not sufficiently rich to be used as
features, but which features are good is not known in general. To address this
issue, we propose a method that employs polynomial basis functions to form a
candidate set of features, which are shown to allow the matching of statistical
moments of state distributions. Feature selection is then performed for the
candidates by leveraging the correlation between trajectory probabilities and
feature expectations. We demonstrate the approach's effectiveness by recovering
reward functions that capture expert policies across non-linear control tasks
of increasing complexity. Code, data, and videos are available at
https://sites.google.com/view/feature4irl.
\\ ( https://arxiv.org/abs/2403.15079 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15083
Date: Fri, 22 Mar 2024 10:06:42 GMT   (411kb,D)

Title: SIMAP: A simplicial-map layer for neural networks
Authors: Rocio Gonzalez-Diaz, Miguel A. Guti\'errez-Naranjo, Eduardo
  Paluzo-Hidalgo
Categories: cs.LG math.AT
\\
  In this paper, we present SIMAP, a novel layer integrated into deep learning
models, aimed at enhancing the interpretability of the output. The SIMAP layer
is an enhanced version of Simplicial-Map Neural Networks (SMNNs), an
explainable neural network based on support sets and simplicial maps (functions
used in topology to transform shapes while preserving their structural
connectivity). The novelty of the methodology proposed in this paper is
two-fold: Firstly, SIMAP layers work in combination with other deep learning
architectures as an interpretable layer substituting classic dense final
layers. Secondly, unlike SMNNs, the support set is based on a fixed maximal
simplex, the barycentric subdivision being efficiently computed with a
matrix-based multiplication algorithm.
\\ ( https://arxiv.org/abs/2403.15083 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15091
Date: Fri, 22 Mar 2024 10:20:09 GMT   (247kb,D)

Title: Improved Long Short-Term Memory-based Wastewater Treatment Simulators
  for Deep Reinforcement Learning
Authors: Esmaeel Mohammadi, Daniel Ortiz-Arroyo, Mikkel Stokholm-Bjerregaard,
  Aviaja Anna Hansen, Petar Durdevic
Categories: cs.LG cs.AI cs.SY eess.SY
\\
  Even though Deep Reinforcement Learning (DRL) showed outstanding results in
the fields of Robotics and Games, it is still challenging to implement it in
the optimization of industrial processes like wastewater treatment. One of the
challenges is the lack of a simulation environment that will represent the
actual plant as accurately as possible to train DRL policies. Stochasticity and
non-linearity of wastewater treatment data lead to unstable and incorrect
predictions of models over long time horizons. One possible reason for the
models' incorrect simulation behavior can be related to the issue of
compounding error, which is the accumulation of errors throughout the
simulation. The compounding error occurs because the model utilizes its
predictions as inputs at each time step. The error between the actual data and
the prediction accumulates as the simulation continues. We implemented two
methods to improve the trained models for wastewater treatment data, which
resulted in more accurate simulators: 1- Using the model's prediction data as
input in the training step as a tool of correction, and 2- Change in the loss
function to consider the long-term predicted shape (dynamics). The experimental
results showed that implementing these methods can improve the behavior of
simulators in terms of Dynamic Time Warping throughout a year up to 98%
compared to the base model. These improvements demonstrate significant promise
in creating simulators for biological processes that do not need pre-existing
knowledge of the process but instead depend exclusively on time series data
obtained from the system.
\\ ( https://arxiv.org/abs/2403.15091 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15108
Date: Fri, 22 Mar 2024 10:51:55 GMT   (1012kb,D)

Title: Active Learning for Regression based on Wasserstein distance and
  GroupSort Neural Networks
Authors: Benjamin Bobbia and Matthias Picard
Categories: cs.LG math.ST stat.ML stat.TH
\\
  This paper addresses a new active learning strategy for regression problems.
The presented Wasserstein active regression model is based on the principles of
distribution-matching to measure the representativeness of the labeled dataset.
The Wasserstein distance is computed using GroupSort Neural Networks. The use
of such networks provides theoretical foundations giving a way to quantify
errors with explicit bounds for their size and depth. This solution is combined
with another uncertainty-based approach that is more outlier-tolerant to
complete the query strategy. Finally, this method is compared with other
classical and recent solutions. The study empirically shows the pertinence of
such a representativity-uncertainty approach, which provides good estimation
all along the query procedure. Moreover, the Wasserstein active regression
often achieves more precise estimations and tends to improve accuracy faster
than other models.
\\ ( https://arxiv.org/abs/2403.15108 ,  1012kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15123
Date: Fri, 22 Mar 2024 11:25:38 GMT   (2593kb,D)

Title: Quantification using Permutation-Invariant Networks based on Histograms
Authors: Olaya P\'erez-Mon, Alejandro Moreo, Juan Jos\'e del Coz, Pablo
  Gonz\'alez
Categories: cs.LG stat.ML
\\
  Quantification, also known as class prevalence estimation, is the supervised
learning task in which a model is trained to predict the prevalence of each
class in a given bag of examples. This paper investigates the application of
deep neural networks to tasks of quantification in scenarios where it is
possible to apply a symmetric supervised approach that eliminates the need for
classification as an intermediary step, directly addressing the quantification
problem. Additionally, it discusses existing permutation-invariant layers
designed for set processing and assesses their suitability for quantification.
In light of our analysis, we propose HistNetQ, a novel neural architecture that
relies on a permutation-invariant representation based on histograms that is
specially suited for quantification problems. Our experiments carried out in
the only quantification competition held to date, show that HistNetQ
outperforms other deep neural architectures devised for set processing, as well
as the state-of-the-art quantification methods. Furthermore, HistNetQ offers
two significant advantages over traditional quantification methods: i) it does
not require the labels of the training examples but only the prevalence values
of a collection of training bags, making it applicable to new scenarios; and
ii) it is able to optimize any custom quantification-oriented loss function.
\\ ( https://arxiv.org/abs/2403.15123 ,  2593kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15146
Date: Fri, 22 Mar 2024 11:57:51 GMT   (96kb)

Title: On the Convergence of Adam under Non-uniform Smoothness: Separability
  from SGDM and Beyond
Authors: Bohan Wang and Huishuai Zhang and Qi Meng and Ruoyu Sun and Zhi-Ming
  Ma and Wei Chen
Categories: cs.LG math.OC
\\
  This paper aims to clearly distinguish between Stochastic Gradient Descent
with Momentum (SGDM) and Adam in terms of their convergence rates. We
demonstrate that Adam achieves a faster convergence compared to SGDM under the
condition of non-uniformly bounded smoothness. Our findings reveal that: (1) in
deterministic environments, Adam can attain the known lower bound for the
convergence rate of deterministic first-order optimizers, whereas the
convergence rate of Gradient Descent with Momentum (GDM) has higher order
dependence on the initial function value; (2) in stochastic setting, Adam's
convergence rate upper bound matches the lower bounds of stochastic first-order
optimizers, considering both the initial function value and the final error,
whereas there are instances where SGDM fails to converge with any learning
rate. These insights distinctly differentiate Adam and SGDM regarding their
convergence rates. Additionally, by introducing a novel stopping-time based
technique, we further prove that if we consider the minimum gradient norm
during iterations, the corresponding convergence rate can match the lower
bounds across all problem hyperparameters. The technique can also help proving
that Adam with a specific hyperparameter scheduler is parameter-agnostic, which
hence can be of independent interest.
\\ ( https://arxiv.org/abs/2403.15146 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15150
Date: Fri, 22 Mar 2024 12:06:40 GMT   (1575kb,D)

Title: An In-Depth Analysis of Data Reduction Methods for Sustainable Deep
  Learning
Authors: V\'ictor Toscano-Dur\'an, Javier Perera-Lago, Eduardo Paluzo-Hidalgo,
  Roc\'io Gonzalez-Diaz, Miguel \'Angel Gutierrez-Naranjo and Matteo Rucco
Categories: cs.LG cs.CV
\\
  In recent years, Deep Learning has gained popularity for its ability to solve
complex classification tasks, increasingly delivering better results thanks to
the development of more accurate models, the availability of huge volumes of
data and the improved computational capabilities of modern computers. However,
these improvements in performance also bring efficiency problems, related to
the storage of datasets and models, and to the waste of energy and time
involved in both the training and inference processes. In this context, data
reduction can help reduce energy consumption when training a deep learning
model. In this paper, we present up to eight different methods to reduce the
size of a tabular training dataset, and we develop a Python package to apply
them. We also introduce a representativeness metric based on topology to
measure how similar are the reduced datasets and the full training dataset.
Additionally, we develop a methodology to apply these data reduction methods to
image datasets for object detection tasks. Finally, we experimentally compare
how these data reduction methods affect the representativeness of the reduced
dataset, the energy consumption and the predictive performance of the model.
\\ ( https://arxiv.org/abs/2403.15150 ,  1575kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15167
Date: Fri, 22 Mar 2024 12:37:14 GMT   (454kb,D)

Title: Transition Graph Properties of Target Class Classification
Authors: Levon Aslanyan, Hasmik Sahakyan
Categories: cs.LG cs.AI cs.DM
Comments: 14pages, 4 figures
\\
  Target class classification is a mixed classification and transition model
whose integrated goal is to assign objects to a certain, so called target or
normal class. The classification process is iterative, and in each step an
object in a certain class undergoes an action attached to that class,
initiating the transition of the object to one of the classes. The sequence of
transitions, which we call class transitions, must be designed to provide the
final assignment of objects to the target class. The transition process can be
described in the form of a directed graph, and the success of the final
classification is mainly due to the properties of this graph. In our previous
research we showed that the desirable structure of the transition graph is an
oriented rooted tree with orientation towards the root vertex, which
corresponds to the normal class. It is clear that the transition graph of an
arbitrary algorithm (policy) may not have this property. In this paper we study
the structure of realistic transition graphs, which makes it possible to find
classification inconsistencies, helping to transfer it into the desired form.
The medical interpretation of dynamic treatment regime considered in the
article further clarifies the investigated framework.
\\ ( https://arxiv.org/abs/2403.15167 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15170
Date: Fri, 22 Mar 2024 12:46:58 GMT   (264kb,D)

Title: Exploring the Task-agnostic Trait of Self-supervised Learning in the
  Context of Detecting Mental Disorders
Authors: Rohan Kumar Gupta and Rohit Sinha
Categories: cs.LG cs.AI eess.SP
\\
  Self-supervised learning (SSL) has been investigated to generate
task-agnostic representations across various domains. However, such
investigation has not been conducted for detecting multiple mental disorders.
The rationale behind the existence of a task-agnostic representation lies in
the overlapping symptoms among multiple mental disorders. Consequently, the
behavioural data collected for mental health assessment may carry a mixed bag
of attributes related to multiple disorders. Motivated by that, in this study,
we explore a task-agnostic representation derived through SSL in the context of
detecting major depressive disorder (MDD) and post-traumatic stress disorder
(PTSD) using audio and video data collected during interactive sessions. This
study employs SSL models trained by predicting multiple fixed targets or masked
frames. We propose a list of fixed targets to make the generated representation
more efficient for detecting MDD and PTSD. Furthermore, we modify the
hyper-parameters of the SSL encoder predicting fixed targets to generate global
representations that capture varying temporal contexts. Both these innovations
are noted to yield improved detection performances for considered mental
disorders and exhibit task-agnostic traits. In the context of the SSL model
predicting masked frames, the generated global representations are also noted
to exhibit task-agnostic traits.
\\ ( https://arxiv.org/abs/2403.15170 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15180
Date: Fri, 22 Mar 2024 13:09:10 GMT   (365kb,D)

Title: Self-Improvement for Neural Combinatorial Optimization: Sample without
  Replacement, but Improvement
Authors: Jonathan Pirnay and Dominik G. Grimm
Categories: cs.LG
\\
  Current methods for end-to-end constructive neural combinatorial optimization
usually train a policy using behavior cloning from expert solutions or policy
gradient methods from reinforcement learning. While behavior cloning is
straightforward, it requires expensive expert solutions, and policy gradient
methods are often computationally demanding and complex to fine-tune. In this
work, we bridge the two and simplify the training process by sampling multiple
solutions for random instances using the current model in each epoch and then
selecting the best solution as an expert trajectory for supervised imitation
learning. To achieve progressively improving solutions with minimal sampling,
we introduce a method that combines round-wise Stochastic Beam Search with an
update strategy derived from a provable policy improvement. This strategy
refines the policy between rounds by utilizing the advantage of the sampled
sequences with almost no computational overhead. We evaluate our approach on
the Traveling Salesman Problem and the Capacitated Vehicle Routing Problem. The
models trained with our method achieve comparable performance and
generalization to those trained with expert data. Additionally, we apply our
method to the Job Shop Scheduling Problem using a transformer-based
architecture and outperform existing state-of-the-art methods by a wide margin.
\\ ( https://arxiv.org/abs/2403.15180 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15182
Date: Fri, 22 Mar 2024 13:11:26 GMT   (1987kb,D)

Title: PDE-CNNs: Axiomatic Derivations and Applications
Authors: Gijs Bellaard, Sei Sakata, Bart M. N. Smets, Remco Duits
Categories: cs.LG cs.CV
\\
  PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of
geometrically meaningful evolution PDEs as substitutes for the conventional
components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer
parameters, inherent equivariance, better performance, data efficiency, and
geometric interpretability. In this article we focus on Euclidean equivariant
PDE-G-CNNs where the feature maps are two dimensional throughout. We call this
variant of the framework a PDE-CNN. We list several practically desirable
axioms and derive from these which PDEs should be used in a PDE-CNN. Here our
approach to geometric learning via PDEs is inspired by the axioms of classical
linear and morphological scale-space theory, which we generalize by introducing
semifield-valued signals. Furthermore, we experimentally confirm for small
networks that PDE-CNNs offer fewer parameters, better performance, and data
efficiency in comparison to CNNs. We also investigate what effect the use of
different semifields has on the performance of the models.
\\ ( https://arxiv.org/abs/2403.15182 ,  1987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15207
Date: Fri, 22 Mar 2024 13:49:53 GMT   (810kb,D)

Title: Robust optimization for adversarial learning with finite sample
  complexity guarantees
Authors: Andr\'e Bertolace, Konstatinos Gatsis, Kostas Margellos
Categories: cs.LG cs.SY eess.SY
\\
  Decision making and learning in the presence of uncertainty has attracted
significant attention in view of the increasing need to achieve robust and
reliable operations. In the case where uncertainty stems from the presence of
adversarial attacks this need is becoming more prominent. In this paper we
focus on linear and nonlinear classification problems and propose a novel
adversarial training method for robust classifiers, inspired by Support Vector
Machine (SVM) margins. We view robustness under a data driven lens, and derive
finite sample complexity bounds for both linear and non-linear classifiers in
binary and multi-class scenarios. Notably, our bounds match natural
classifiers' complexity. Our algorithm minimizes a worst-case surrogate loss
using Linear Programming (LP) and Second Order Cone Programming (SOCP) for
linear and non-linear models. Numerical experiments on the benchmark MNIST and
CIFAR10 datasets show our approach's comparable performance to state-of-the-art
methods, without needing adversarial examples during training. Our work offers
a comprehensive framework for enhancing binary linear and non-linear classifier
robustness, embedding robustness in learning under the presence of adversaries.
\\ ( https://arxiv.org/abs/2403.15207 ,  810kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15210
Date: Fri, 22 Mar 2024 13:52:53 GMT   (5955kb,D)

Title: Early Period of Training Impacts Out-of-Distribution Generalization
Authors: Chen Cecilia Liu and Iryna Gurevych
Categories: cs.LG
Comments: WIP
\\
  Prior research has found that differences in the early period of neural
network training significantly impact the performance of in-distribution (ID)
tasks. However, neural networks are often sensitive to out-of-distribution
(OOD) data, making them less reliable in downstream applications. Yet, the
impact of the early training period on OOD generalization remains understudied
due to its complexity and lack of effective analytical methodologies. In this
work, we investigate the relationship between learning dynamics and OOD
generalization during the early period of neural network training. We utilize
the trace of Fisher Information and sharpness, with a focus on gradual
unfreezing (i.e. progressively unfreezing parameters during training) as the
methodology for investigation. Through a series of empirical experiments, we
show that 1) selecting the number of trainable parameters at different times
during training, i.e. realized by gradual unfreezing -- has a minuscule impact
on ID results, but greatly affects the generalization to OOD data; 2) the
absolute values of sharpness and trace of Fisher Information at the initial
period of training are not indicative for OOD generalization, but the relative
values could be; 3) the trace of Fisher Information and sharpness may be used
as indicators for the removal of interventions during early period of training
for better OOD generalization.
\\ ( https://arxiv.org/abs/2403.15210 ,  5955kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15244
Date: Fri, 22 Mar 2024 14:40:29 GMT   (351kb)

Title: A Stochastic Quasi-Newton Method for Non-convex Optimization with
  Non-uniform Smoothness
Authors: Zhenyu Sun and Ermin Wei
Categories: cs.LG math.OC
\\
  Classical convergence analyses for optimization algorithms rely on the
widely-adopted uniform smoothness assumption. However, recent experimental
studies have demonstrated that many machine learning problems exhibit
non-uniform smoothness, meaning the smoothness factor is a function of the
model parameter instead of a universal constant. In particular, it has been
observed that the smoothness grows with respect to the gradient norm along the
training trajectory. Motivated by this phenomenon, the recently introduced
$(L_0, L_1)$-smoothness is a more general notion, compared to traditional
$L$-smoothness, that captures such positive relationship between smoothness and
gradient norm. Under this type of non-uniform smoothness, existing literature
has designed stochastic first-order algorithms by utilizing gradient clipping
techniques to obtain the optimal $\mathcal{O}(\epsilon^{-3})$ sample complexity
for finding an $\epsilon$-approximate first-order stationary solution.
Nevertheless, the studies of quasi-Newton methods are still lacking.
Considering higher accuracy and more robustness for quasi-Newton methods, in
this paper we propose a fast stochastic quasi-Newton method when there exists
non-uniformity in smoothness. Leveraging gradient clipping and variance
reduction, our algorithm can achieve the best-known
$\mathcal{O}(\epsilon^{-3})$ sample complexity and enjoys convergence speedup
with simple hyperparameter tuning. Our numerical experiments show that our
proposed algorithm outperforms the state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2403.15244 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15263
Date: Fri, 22 Mar 2024 15:02:24 GMT   (1399kb,D)

Title: Federated Bayesian Deep Learning: The Application of Statistical
  Aggregation Methods to Bayesian Models
Authors: John Fischer, Marko Orescanin, Justin Loomis, Patrick McClure
Categories: cs.LG stat.ML
Comments: 22 pages, 9 figures
\\
  Federated learning (FL) is an approach to training machine learning models
that takes advantage of multiple distributed datasets while maintaining data
privacy and reducing communication costs associated with sharing local
datasets. Aggregation strategies have been developed to pool or fuse the
weights and biases of distributed deterministic models; however, modern
deterministic deep learning (DL) models are often poorly calibrated and lack
the ability to communicate a measure of epistemic uncertainty in prediction,
which is desirable for remote sensing platforms and safety-critical
applications. Conversely, Bayesian DL models are often well calibrated and
capable of quantifying and communicating a measure of epistemic uncertainty
along with a competitive prediction accuracy. Unfortunately, because the
weights and biases in Bayesian DL models are defined by a probability
distribution, simple application of the aggregation methods associated with FL
schemes for deterministic models is either impossible or results in sub-optimal
performance. In this work, we use independent and identically distributed (IID)
and non-IID partitions of the CIFAR-10 dataset and a fully variational
ResNet-20 architecture to analyze six different aggregation strategies for
Bayesian DL models. Additionally, we analyze the traditional federated
averaging approach applied to an approximate Bayesian Monte Carlo dropout model
as a lightweight alternative to more complex variational inference methods in
FL. We show that aggregation strategy is a key hyperparameter in the design of
a Bayesian FL system with downstream effects on accuracy, calibration,
uncertainty quantification, training stability, and client compute
requirements.
\\ ( https://arxiv.org/abs/2403.15263 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15267
Date: Fri, 22 Mar 2024 15:06:31 GMT   (1812kb,D)

Title: Parametric PDE Control with Deep Reinforcement Learning and
  Differentiable L0-Sparse Polynomial Policies
Authors: Nicol\`o Botteghi and Urban Fasel
Categories: cs.LG
\\
  Optimal control of parametric partial differential equations (PDEs) is
crucial in many applications in engineering and science. In recent years, the
progress in scientific machine learning has opened up new frontiers for the
control of parametric PDEs. In particular, deep reinforcement learning (DRL)
has the potential to solve high-dimensional and complex control problems in a
large variety of applications. Most DRL methods rely on deep neural network
(DNN) control policies. However, for many dynamical systems, DNN-based control
policies tend to be over-parametrized, which means they need large amounts of
training data, show limited robustness, and lack interpretability. In this
work, we leverage dictionary learning and differentiable L$_0$ regularization
to learn sparse, robust, and interpretable control policies for parametric
PDEs. Our sparse policy architecture is agnostic to the DRL method and can be
used in different policy-gradient and actor-critic DRL algorithms without
changing their policy-optimization procedure. We test our approach on the
challenging tasks of controlling parametric Kuramoto-Sivashinsky and
convection-diffusion-reaction PDEs. We show that our method (1) outperforms
baseline DNN-based DRL policies, (2) allows for the derivation of interpretable
equations of the learned optimal control laws, and (3) generalizes to unseen
parameters of the PDE without retraining the policies.
\\ ( https://arxiv.org/abs/2403.15267 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15301
Date: Fri, 22 Mar 2024 15:51:39 GMT   (304kb,D)

Title: Planning with a Learned Policy Basis to Optimally Solve Complex Tasks
Authors: Guillermo Infante, David Kuric, Anders Jonsson, Vicen\c{c} G\'omez,
  Herke van Hoof
Categories: cs.LG cs.AI
\\
  Conventional reinforcement learning (RL) methods can successfully solve a
wide range of sequential decision problems. However, learning policies that can
generalize predictably across multiple tasks in a setting with non-Markovian
reward specifications is a challenging problem. We propose to use successor
features to learn a policy basis so that each (sub)policy in it solves a
well-defined subproblem. In a task described by a finite state automaton (FSA)
that involves the same set of subproblems, the combination of these
(sub)policies can then be used to generate an optimal solution without
additional learning. In contrast to other methods that combine (sub)policies
via planning, our method asymptotically attains global optimality, even in
stochastic environments.
\\ ( https://arxiv.org/abs/2403.15301 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15371
Date: Fri, 22 Mar 2024 17:50:43 GMT   (1206kb,D)

Title: Can large language models explore in-context?
Authors: Akshay Krishnamurthy, Keegan Harris, Dylan J. Foster, Cyril Zhang,
  Aleksandrs Slivkins
Categories: cs.LG cs.AI cs.CL
\\
  We investigate the extent to which contemporary Large Language Models (LLMs)
can engage in exploration, a core capability in reinforcement learning and
decision making. We focus on native performance of existing LLMs, without
training interventions. We deploy LLMs as agents in simple multi-armed bandit
environments, specifying the environment description and interaction history
entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,
GPT-4, and Llama2, using a variety of prompt designs, and find that the models
do not robustly engage in exploration without substantial interventions: i)
Across all of our experiments, only one configuration resulted in satisfactory
exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally
summarized interaction history, presented as sufficient statistics; ii) All
other configurations did not result in robust exploratory behavior, including
those with chain-of-thought reasoning but unsummarized history. Although these
findings can be interpreted positively, they suggest that external
summarization -- which may not be possible in more complex settings -- is
important for obtaining desirable behavior from LLM agents. We conclude that
non-trivial algorithmic interventions, such as fine-tuning or dataset curation,
may be required to empower LLM-based decision making agents in complex
settings.
\\ ( https://arxiv.org/abs/2403.15371 ,  1206kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.14440 (*cross-listing*)
Date: Thu, 21 Mar 2024 14:45:54 GMT   (4739kb,D)

Title: Analysing Diffusion Segmentation for Medical Images
Authors: Mathias \"Ottl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias
  R\"ubner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier,
  Ramona Erber, Katharina Breininger
Categories: eess.IV cs.AI cs.CV cs.LG
\\
  Denoising Diffusion Probabilistic models have become increasingly popular due
to their ability to offer probabilistic modeling and generate diverse outputs.
This versatility inspired their adaptation for image segmentation, where
multiple predictions of the model can produce segmentation results that not
only achieve high quality but also capture the uncertainty inherent in the
model. Here, powerful architectures were proposed for improving diffusion
segmentation performance. However, there is a notable lack of analysis and
discussions on the differences between diffusion segmentation and image
generation, and thorough evaluations are missing that distinguish the
improvements these architectures provide for segmentation in general from their
benefit for diffusion segmentation specifically. In this work, we critically
analyse and discuss how diffusion segmentation for medical images differs from
diffusion image generation, with a particular focus on the training behavior.
Furthermore, we conduct an assessment how proposed diffusion segmentation
architectures perform when trained directly for segmentation. Lastly, we
explore how different medical segmentation tasks influence the diffusion
segmentation behavior and the diffusion process could be adapted accordingly.
With these analyses, we aim to provide in-depth insights into the behavior of
diffusion segmentation that allow for a better design and evaluation of
diffusion segmentation methods in the future.
\\ ( https://arxiv.org/abs/2403.14440 ,  4739kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14633 (*cross-listing*)
Date: Fri, 16 Feb 2024 23:18:19 GMT   (11357kb,D)

Title: Born With a Silver Spoon? Investigating Socioeconomic Bias in Large
  Language Models
Authors: Smriti Singh, Shuvam Keshari, Vinija Jain, Aman Chadha
Categories: cs.CY cs.AI cs.CL
\\
  Socioeconomic bias in society exacerbates disparities, influencing access to
opportunities and resources based on individuals' economic and social
backgrounds. This pervasive issue perpetuates systemic inequalities, hindering
the pursuit of inclusive progress as a society. In this paper, we investigate
the presence of socioeconomic bias, if any, in large language models. To this
end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples
that illustrate hypothetical scenarios that involve underprivileged people
performing ethically ambiguous actions due to their circumstances, and ask
whether the action is ethically justified. Further, this dataset has a
dual-labeling scheme and has been annotated by people belonging to both ends of
the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of
socioeconomic bias expressed in large language models and the variation of this
degree as a function of model size. We also perform qualitative analysis to
analyze the nature of this bias. Our analysis reveals that while humans
disagree on which situations require empathy toward the underprivileged, most
large language models are unable to empathize with the socioeconomically
underprivileged regardless of the situation. To foster further research in this
domain, we make SilverSpoon and our evaluation harness publicly available.
\\ ( https://arxiv.org/abs/2403.14633 ,  11357kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14635 (*cross-listing*)
Date: Mon, 19 Feb 2024 22:52:14 GMT   (6974kb)

Title: AI Sustainability in Practice Part One: Foundations for Sustainable AI
  Projects
Authors: David Leslie, Cami Rincon, Morgan Briggs, Antonella Perini, Smera
  Jayadeva, Ann Borda, SJ Bennett, Christopher Burr, Mhairi Aitken, Michael
  Katell, Claudia Fischer, Janis Wong, Ismael Kherroubi Garcia
Categories: cs.CY cs.AI cs.HC
DOI: 10.5281/zenodo.10680113
\\
  Sustainable AI projects are continuously responsive to the transformative
effects as well as short-, medium-, and long-term impacts on individuals and
society that the design, development, and deployment of AI technologies may
have. Projects, which centre AI Sustainability, ensure that values-led,
collaborative, and anticipatory reflection both guides the assessment of
potential social and ethical impacts and steers responsible innovation
practices.
  This workbook is the first part of a pair that provides the concepts and
tools needed to put AI Sustainability into practice. It introduces the SUM
Values, which help AI project teams to assess the potential societal impacts
and ethical permissibility of their projects. It then presents a Stakeholder
Engagement Process (SEP), which provides tools to facilitate proportionate
engagement of and input from stakeholders with an emphasis on equitable and
meaningful participation and positionality awareness.
\\ ( https://arxiv.org/abs/2403.14635 ,  6974kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14636 (*cross-listing*)
Date: Mon, 19 Feb 2024 23:02:56 GMT   (17080kb)

Title: AI Fairness in Practice
Authors: David Leslie, Cami Rincon, Morgan Briggs, Antonella Perini, Smera
  Jayadeva, Ann Borda, SJ Bennett, Christopher Burr, Mhairi Aitken, Michael
  Katell, Claudia Fischer, Janis Wong, Ismael Kherroubi Garcia
Categories: cs.CY cs.AI cs.HC
DOI: 10.5281/zenodo.10680527
\\
  Reaching consensus on a commonly accepted definition of AI Fairness has long
been a central challenge in AI ethics and governance. There is a broad spectrum
of views across society on what the concept of fairness means and how it should
best be put to practice. In this workbook, we tackle this challenge by
exploring how a context-based and society-centred approach to understanding AI
Fairness can help project teams better identify, mitigate, and manage the many
ways that unfair bias and discrimination can crop up across the AI project
workflow.
  We begin by exploring how, despite the plurality of understandings about the
meaning of fairness, priorities of equality and non-discrimination have come to
constitute the broadly accepted core of its application as a practical
principle. We focus on how these priorities manifest in the form of equal
protection from direct and indirect discrimination and from discriminatory
harassment. These elements form ethical and legal criteria based upon which
instances of unfair bias and discrimination can be identified and mitigated
across the AI project workflow.
  We then take a deeper dive into how the different contexts of the AI project
lifecycle give rise to different fairness concerns. This allows us to identify
several types of AI Fairness (Data Fairness, Application Fairness, Model Design
and Development Fairness, Metric-Based Fairness, System Implementation
Fairness, and Ecosystem Fairness) that form the basis of a multi-lens approach
to bias identification, mitigation, and management. Building on this, we
discuss how to put the principle of AI Fairness into practice across the AI
project workflow through Bias Self-Assessment and Bias Risk Management as well
as through the documentation of metric-based fairness criteria in a Fairness
Position Statement.
\\ ( https://arxiv.org/abs/2403.14636 ,  17080kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14639 (*cross-listing*)
Date: Tue, 20 Feb 2024 18:34:24 GMT   (503kb)

Title: On Defining Smart Cities using Transformer Neural Networks
Authors: Andrei Khurshudov
Categories: cs.CY cs.AI cs.LG
Comments: 16 pages, 2 fugures
Journal-ref: International Journal of Computer and Technology Vol 24 (2024)
  ISSN: 2277-3061
DOI: 10.24297/ijct.v24i.9579
\\
  Cities worldwide are rapidly adopting smart technologies, transforming urban
life. Despite this trend, a universally accepted definition of 'smart city'
remains elusive. Past efforts to define it have not yielded a consensus, as
evidenced by the numerous definitions in use. In this paper, we endeavored to
create a new 'compromise' definition that should resonate with most experts
previously involved in defining this concept and aimed to validate one of the
existing definitions. We reviewed 60 definitions of smart cities from industry,
academia, and various relevant organizations, employing transformer
architecture-based generative AI and semantic text analysis to reach this
compromise. We proposed a semantic similarity measure as an evaluation
technique, which could generally be used to compare different smart city
definitions, assessing their uniqueness or resemblance. Our methodology
employed generative AI to analyze various existing definitions of smart cities,
generating a list of potential new composite definitions. Each of these new
definitions was then tested against the pre-existing individual definitions we
have gathered, using cosine similarity as our metric. This process identified
smart city definitions with the highest average cosine similarity, semantically
positioning them as the closest on average to all the 60 individual definitions
selected.
\\ ( https://arxiv.org/abs/2403.14639 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14641 (*cross-listing*)
Date: Wed, 21 Feb 2024 08:29:42 GMT   (2526kb,D)

Title: Testing autonomous vehicles and AI: perspectives and challenges from
  cybersecurity, transparency, robustness and fairness
Authors: David Fern\'andez Llorca, Ronan Hamon, Henrik Junklewitz, Kathrin
  Grosse, Lars Kunze, Patrick Seiniger, Robert Swaim, Nick Reed, Alexandre
  Alahi, Emilia G\'omez, Ignacio S\'anchez, Akos Kriston
Categories: cs.CY cs.AI cs.LG
Comments: 44 pages, 8 figures, submitted to a peer-review journal
\\
  This study explores the complexities of integrating Artificial Intelligence
(AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI
components and the impact on testing procedures, focusing on some of the
essential requirements for trustworthy AI. Topics addressed include the role of
AI at various operational layers of AVs, the implications of the EU's AI Act on
AVs, and the need for new testing methodologies for Advanced Driver Assistance
Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a
detailed analysis on the importance of cybersecurity audits, the need for
explainability in AI decision-making processes and protocols for assessing the
robustness and ethical behaviour of predictive systems in AVs. The paper
identifies significant challenges and suggests future directions for research
and development of AI in AV technology, highlighting the need for
multidisciplinary expertise.
\\ ( https://arxiv.org/abs/2403.14641 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14642 (*cross-listing*)
Date: Wed, 21 Feb 2024 12:15:58 GMT   (781kb,D)

Title: Revolutionising Distance Learning: A Comparative Study of Learning
  Progress with AI-Driven Tutoring
Authors: Moritz M\"oller, Gargi Nirmal, Dario Fabietti, Quintus Stierstorfer,
  Mark Zakhvatkin, Holger Sommerfeld, Sven Sch\"utt
Categories: cs.CY cs.AI cs.LG
\\
  Generative AI is expected to have a vast, positive impact on education;
however, at present, this potential has not yet been demonstrated at scale at
university level. In this study, we present first evidence that generative AI
can increase the speed of learning substantially in university students. We
tested whether using the AI-powered teaching assistant Syntea affected the
speed of learning of hundreds of distance learning students across more than 40
courses at the IU International University of Applied Sciences. Our analysis
suggests that using Syntea reduced their study time substantially--by about
27\% on average--in the third month after the release of Syntea. Taken
together, the magnitude of the effect and the scalability of the approach
implicate generative AI as a key lever to significantly improve and accelerate
learning by personalisation.
\\ ( https://arxiv.org/abs/2403.14642 ,  781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14643 (*cross-listing*)
Date: Wed, 21 Feb 2024 16:44:35 GMT   (293kb)

Title: Exploring ChatGPT and its Impact on Society
Authors: Md. Asraful Haque and Shuai Li
Categories: cs.CY cs.AI cs.CL
Comments: 20 Pages, AI and Ethics (2024)
MSC-class: 68Txx
DOI: 10.1007/s43681-024-00435-4
\\
  Artificial intelligence has been around for a while, but suddenly it has
received more attention than ever before. Thanks to innovations from companies
like Google, Microsoft, Meta, and other major brands in technology. OpenAI,
though, has triggered the button with its ground-breaking invention ChatGPT.
ChatGPT is a Large Language Model (LLM) based on Transformer architecture that
has the ability to generate human-like responses in a conversational context.
It uses deep learning algorithms to generate natural language responses to
input text. Its large number of parameters, contextual generation, and
open-domain training make it a versatile and effective tool for a wide range of
applications, from chatbots to customer service to language translation. It has
the potential to revolutionize various industries and transform the way we
interact with technology. However, the use of ChatGPT has also raised several
concerns, including ethical, social, and employment challenges, which must be
carefully considered to ensure the responsible use of this technology. The
article provides an overview of ChatGPT, delving into its architecture and
training process. It highlights the potential impacts of ChatGPT on the
society. In this paper, we suggest some approaches involving technology,
regulation, education, and ethics in an effort to maximize ChatGPT's benefits
while minimizing its negative impacts. This study is expected to contribute to
a greater understanding of ChatGPT and aid in predicting the potential changes
it may bring about.
\\ ( https://arxiv.org/abs/2403.14643 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14645 (*cross-listing*)
Date: Wed, 21 Feb 2024 18:37:13 GMT   (852kb)

Title: Designing Multi-Step Action Models for Enterprise AI Adoption
Authors: Shreyash Mishra, Shrey Shah and Rex Pereira
Categories: cs.CY cs.AI
Comments: 8 pages, 5 figures
Report-no: EMP-202401
MSC-class: 68T42
ACM-class: I.2.1; I.2.8
\\
  This paper introduces the Multi-Step Action Model (MSAM), a closed-source AI
model designed by Empsing to address challenges hindering AI adoption in
enterprises. Through a holistic examination, this paper explores MSAM's
foundational principles, design architecture, and future trajectory. It
evaluates MSAM's performance via rigorous testing methodologies and envisions
its potential impact on advancing AI adoption within organizations.
\\ ( https://arxiv.org/abs/2403.14645 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14650 (*cross-listing*)
Date: Fri, 23 Feb 2024 09:20:34 GMT   (2757kb,D)

Title: Harnessing the Computing Continuum across Personalized Healthcare,
  Maintenance and Inspection, and Farming 4.0
Authors: Fatemeh Baghdadi, Davide Cirillo, Daniele Lezzi, Francesc Lordan,
  Fernando Vazquez, Eugenio Lomurno, Alberto Archetti, Danilo Ardagna, Matteo
  Matteucci
Categories: cs.CY cs.AI
\\
  The AI-SPRINT project, launched in 2021 and funded by the European
Commission, focuses on the development and implementation of AI applications
across the computing continuum. This continuum ensures the coherent integration
of computational resources and services from centralized data centers to edge
devices, facilitating efficient and adaptive computation and application
delivery. AI-SPRINT has achieved significant scientific advances, including
streamlined processes, improved efficiency, and the ability to operate in real
time, as evidenced by three practical use cases. This paper provides an
in-depth examination of these applications -- Personalized Healthcare,
Maintenance and Inspection, and Farming 4.0 -- highlighting their practical
implementation and the objectives achieved with the integration of AI-SPRINT
technologies. We analyze how the proposed toolchain effectively addresses a
range of challenges and refines processes, discussing its relevance and impact
in multiple domains. After a comprehensive overview of the main AI-SPRINT tools
used in these scenarios, the paper summarizes of the findings and key lessons
learned.
\\ ( https://arxiv.org/abs/2403.14650 ,  2757kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14652 (*cross-listing*)
Date: Sat, 24 Feb 2024 06:14:34 GMT   (2971kb,D)

Title: MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation
Authors: Han Wang, Roy Ka-Wei Lee
Categories: cs.CY cs.AI cs.CL cs.MM
Comments: 8 pages, 7 figures, ACM MM 2024
ACM-class: I.2.7; I.2.10
DOI: 10.1145/3589334.3648151
\\
  Online memes have emerged as powerful digital cultural artifacts in the age
of social media, offering not only humor but also platforms for political
discourse, social critique, and information dissemination. Their extensive
reach and influence in shaping online communities' sentiments make them
invaluable tools for campaigning and promoting ideologies. Despite the
development of several meme-generation tools, there remains a gap in their
systematic evaluation and their ability to effectively communicate ideologies.
Addressing this, we introduce MemeCraft, an innovative meme generator that
leverages large language models (LLMs) and visual language models (VLMs) to
produce memes advocating specific social movements. MemeCraft presents an
end-to-end pipeline, transforming user prompts into compelling multimodal memes
without manual intervention. Conscious of the misuse potential in creating
divisive content, an intrinsic safety mechanism is embedded to curb hateful
meme production.
\\ ( https://arxiv.org/abs/2403.14652 ,  2971kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14658 (*cross-listing*)
Date: Tue, 27 Feb 2024 16:52:18 GMT   (1647kb)

Title: Identifying Potential Inlets of Man in the Artificial Intelligence
  Development Process
Authors: Deja Workman, Christopher L. Dancy
Categories: cs.CY cs.AI
Comments: Published in CSCW '23 Conference Proceedings. 7 pages, 1 figure
ACM-class: I.2.0; K.4.2
DOI: 10.1145/3584931.3606981
\\
  In this paper we hope to identify how the typical or standard artificial
intelligence development process encourages or facilitates the creation of
racialized technologies. We begin by understanding Sylvia Wynter's definition
of the biocentric Man genre and its exclusion of Blackness from humanness. We
follow this with outlining what we consider to be the typical steps for
developing an AI-based technology, which we have broken down into 6 stages:
identifying a problem, development process and management tool selection,
dataset development and data processing, model development, deployment and risk
assessment, and integration and monitoring. The goal of this paper is to better
understand how Wynter's biocentric Man is being represented and reinforced by
the technologies we are producing in the AI lifecycle and by the lifecycle
itself; we hope to identify ways in which the distinction of Blackness from the
"ideal" human leads to perpetual punishment at the hands of these technologies.
By deconstructing this development process, we can potentially identify ways in
which humans in general have not been prioritized and how those affects are
disproportionately affecting marginalized people. We hope to offer solutions
that will encourage changes in the AI development cycle.
\\ ( https://arxiv.org/abs/2403.14658 ,  1647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14659 (*cross-listing*)
Date: Wed, 28 Feb 2024 00:22:42 GMT   (11273kb,D)

Title: Social Intelligence Data Infrastructure: Structuring the Present and
  Navigating the Future
Authors: Minzhi Li, Weiyan Shi, Caleb Ziems, Diyi Yang
Categories: cs.CY cs.AI cs.CL
\\
  As Natural Language Processing (NLP) systems become increasingly integrated
into human social life, these technologies will need to increasingly rely on
social intelligence. Although there are many valuable datasets that benchmark
isolated dimensions of social intelligence, there does not yet exist any body
of work to join these threads into a cohesive subfield in which researchers can
quickly identify research gaps and future directions. Towards this goal, we
build a Social AI Data Infrastructure, which consists of a comprehensive social
AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows
us to analyze existing dataset efforts, and also evaluate language models'
performance in different social intelligence aspects. Our analyses demonstrate
its utility in enabling a thorough understanding of current data landscape and
providing a holistic perspective on potential directions for future dataset
development. We show there is a need for multifaceted datasets, increased
diversity in language and culture, more long-tailed social situations, and more
interactive data in future social intelligence data efforts.
\\ ( https://arxiv.org/abs/2403.14659 ,  11273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14660 (*cross-listing*)
Date: Wed, 28 Feb 2024 02:53:17 GMT   (485kb)

Title: Machina Economicus: A New Paradigm for Prosumers in the Energy Internet
  of Smart Cities
Authors: Luyang Hou and Jun Yan and Yuankai Wu and Chun Wang and Tie Qiu
Categories: cs.CY cs.AI
Comments: 25 pages, 1 figure
\\
  Energy Internet (EI) is emerging as new share economy platform for flexible
local energy supplies in smart cities. Empowered by the Internet-of-Things
(IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy
trading and sharing among prosumers, who can adeptly switch roles between
providers and consumers in localized energy markets with rooftop photovoltaic
panels, vehicle-to-everything technologies, packetized energy management, etc.
The integration of prosumers in EI, however, will encounter many challenges in
modelling, analyzing, and designing an efficient, economic, and social-optimal
platform for energy sharing, calling for advanced AI/IoT-based solutions to
resource optimization, information exchange, and interaction protocols in the
context of the share economy. In this study, we aim to introduce a recently
emerged paradigm, Machina Economicus, to investigate the economic rationality
in modelling, analysis, and optimization of AI/IoT-based EI prosumer behaviors.
The new paradigm, built upon the theory of machine learning and mechanism
design, will offer new angles to investigate the selfishness of AI through a
game-theoretic perspective, revealing potential competition and collaborations
resulting from the self-adaptive learning and decision-making capacity. This
study will focus on how the introduction of AI will reshape prosumer behaviors
on the EI, and how this paradigm will reveal new research questions and
directions when AI meets the share economy. With an extensive case analysis in
the literature, we will also shed light on potential solutions for advancements
of AI in future smart cities.
\\ ( https://arxiv.org/abs/2403.14660 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14662 (*cross-listing*)
Date: Thu, 29 Feb 2024 19:17:11 GMT   (991kb)

Title: Case Studies of AI Policy Development in Africa
Authors: Kadijatou Diallo, Jonathan Smith, Chinasa T. Okolo, Dorcas Nyamwaya,
  Jonas Kgomo, Richard Ngamita
Categories: cs.CY cs.AI cs.LG
\\
  Artificial Intelligence (AI) requires new ways of evaluating national
technology use and strategy for African nations. We conduct a survey of
existing 'readiness' assessments both for general digital adoption and for AI
policy in particular. We conclude that existing global readiness assessments do
not fully capture African states' progress in AI readiness and lay the
groundwork for how assessments can be better used for the African context. We
consider the extent to which these indicators map to the African context and
what these indicators miss in capturing African states' on-the-ground work in
meeting AI capability. Through case studies of four African nations of diverse
geographic and economic dimensions, we identify nuances missed by global
assessments and offer high-level policy considerations for how states can best
improve their AI readiness standards and prepare their societies to capture the
benefits of AI.
\\ ( https://arxiv.org/abs/2403.14662 ,  991kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14668 (*cross-listing*)
Date: Mon, 4 Mar 2024 08:14:07 GMT   (968kb,D)

Title: Predicting Learning Performance with Large Language Models: A Study in
  Adult Literacy
Authors: Liang Zhang, Jionghao Lin, Conrad Borchers, John Sabatini, John
  Hollander, Meng Cao, Xiangen Hu
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 26TH International Conference on Human-Computer Interaction
\\
  Intelligent Tutoring Systems (ITSs) have significantly enhanced adult
literacy training, a key factor for societal participation, employment
opportunities, and lifelong learning. Our study investigates the application of
advanced AI models, including Large Language Models (LLMs) like GPT-4, for
predicting learning performance in adult literacy programs in ITSs. This
research is motivated by the potential of LLMs to predict learning performance
based on its inherent reasoning and computational capabilities. By using
reading comprehension datasets from the ITS, AutoTutor, we evaluate the
predictive capabilities of GPT-4 versus traditional machine learning methods in
predicting learning performance through five-fold cross-validation techniques.
Our findings show that the GPT-4 presents the competitive predictive abilities
with traditional machine learning methods such as Bayesian Knowledge Tracing,
Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor
factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained
on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected
XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior
performance compared to local machine execution. Moreover, our investigation
into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable
performance, albeit with less stability in the automated approach, using
XGBoost as the case study. Our study contributes to the field by highlighting
the potential of integrating LLMs with traditional machine learning models to
enhance predictive accuracy and personalize adult literacy education, setting a
foundation for future research in applying LLMs within ITSs.
\\ ( https://arxiv.org/abs/2403.14668 ,  968kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14676 (*cross-listing*)
Date: Sat, 9 Mar 2024 13:48:20 GMT   (1434kb,D)

Title: Unified Uncertainty Estimation for Cognitive Diagnosis Models
Authors: Fei Wang, Qi Liu, Enhong Chen, Chuanren Liu, Zhenya Huang, Jinze Wu,
  Shijin Wang
Categories: cs.CY cs.AI cs.LG
DOI: 10.1145/3589334.3645488
\\
  Cognitive diagnosis models have been widely used in different areas,
especially intelligent education, to measure users' proficiency levels on
knowledge concepts, based on which users can get personalized instructions. As
the measurement is not always reliable due to the weak links of the models and
data, the uncertainty of measurement also offers important information for
decisions. However, the research on the uncertainty estimation lags behind that
on advanced model structures for cognitive diagnosis. Existing approaches have
limited efficiency and leave an academic blank for sophisticated models which
have interaction function parameters (e.g., deep learning-based models). To
address these problems, we propose a unified uncertainty estimation approach
for a wide range of cognitive diagnosis models. Specifically, based on the idea
of estimating the posterior distributions of cognitive diagnosis model
parameters, we first provide a unified objective function for mini-batch based
optimization that can be more efficiently applied to a wide range of models and
large datasets. Then, we modify the reparameterization approach in order to
adapt to parameters defined on different domains. Furthermore, we decompose the
uncertainty of diagnostic parameters into data aspect and model aspect, which
better explains the source of uncertainty. Extensive experiments demonstrate
that our method is effective and can provide useful insights into the
uncertainty of cognitive diagnosis.
\\ ( https://arxiv.org/abs/2403.14676 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14680 (*cross-listing*)
Date: Tue, 12 Mar 2024 20:26:49 GMT   (1534kb)

Title: Trust in AI: Progress, Challenges, and Future Directions
Authors: Saleh Afroogh, Ali Akbari, Evan Malone, Mohammadali Kargar, Hananeh
  Alambeigi
Categories: cs.CY cs.AI
\\
  The increasing use of artificial intelligence (AI) systems in our daily life
through various applications, services, and products explains the significance
of trust/distrust in AI from a user perspective. AI-driven systems (as opposed
to other technologies) have ubiquitously diffused in our life not only as some
beneficial tools to be used by human agents but also are going to be
substitutive agents on our behalf, or manipulative minds that would influence
human thought, decision, and agency. Trust/distrust in AI plays the role of a
regulator and could significantly control the level of this diffusion, as trust
can increase, and distrust may reduce the rate of adoption of AI. Recently,
varieties of studies have paid attention to the variant dimension of
trust/distrust in AI, and its relevant considerations. In this systematic
literature review, after conceptualization of trust in the current AI
literature review, we will investigate trust in different types of
human-Machine interaction, and its impact on technology acceptance in different
domains. In addition to that, we propose a taxonomy of technical (i.e., safety,
accuracy, robustness) and non-technical axiological (i.e., ethical, legal, and
mixed) trustworthiness metrics, and some trustworthy measurements. Moreover, we
examine some major trust-breakers in AI (e.g., autonomy and dignity threat),
and trust makers; and propose some future directions and probable solutions for
the transition to a trustworthy AI.
\\ ( https://arxiv.org/abs/2403.14680 ,  1534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14681 (*cross-listing*)
Date: Tue, 12 Mar 2024 21:43:21 GMT   (1631kb)

Title: AI Ethics: A Bibliometric Analysis, Critical Issues, and Key Gaps
Authors: Di Kevin Gao (1,2), Andrew Haverly (1), Sudip Mittal (1), Jiming Wu
  (2), Jingdao Chen (1) ((1) Mississippi State University, (2) California State
  University - East Bay)
Categories: cs.CY cs.AI
Journal-ref: International Journal of Business Analytics (IJBAN), 2024, 11(1),
  1-19
DOI: 10.4018/IJBAN.338367
\\
  Artificial intelligence (AI) ethics has emerged as a burgeoning yet pivotal
area of scholarly research. This study conducts a comprehensive bibliometric
analysis of the AI ethics literature over the past two decades. The analysis
reveals a discernible tripartite progression, characterized by an incubation
phase, followed by a subsequent phase focused on imbuing AI with human-like
attributes, culminating in a third phase emphasizing the development of
human-centric AI systems. After that, they present seven key AI ethics issues,
encompassing the Collingridge dilemma, the AI status debate, challenges
associated with AI transparency and explainability, privacy protection
complications, considerations of justice and fairness, concerns about algocracy
and human enfeeblement, and the issue of superintelligence. Finally, they
identify two notable research gaps in AI ethics regarding the large ethics
model (LEM) and AI identification and extend an invitation for further
scholarly research.
\\ ( https://arxiv.org/abs/2403.14681 ,  1631kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14682 (*cross-listing*)
Date: Tue, 12 Mar 2024 22:48:23 GMT   (610kb,D)

Title: Deep Generative Domain Adaptation with Temporal Relation Knowledge for
  Cross-User Activity Recognition
Authors: Xiaozhou Ye, Kevin I-Kai Wang
Categories: cs.CV cs.AI cs.HC cs.LG
\\
  In human activity recognition (HAR), the assumption that training and testing
data are independent and identically distributed (i.i.d.) often fails,
particularly in cross-user scenarios where data distributions vary
significantly. This discrepancy highlights the limitations of conventional
domain adaptation methods in HAR, which typically overlook the inherent
temporal relations in time-series data. To bridge this gap, our study
introduces a Conditional Variational Autoencoder with Universal Sequence
Mapping (CVAE-USM) approach, which addresses the unique challenges of
time-series domain adaptation in HAR by relaxing the i.i.d. assumption and
leveraging temporal relations to align data distributions effectively across
different users. This method combines the strengths of Variational Autoencoder
(VAE) and Universal Sequence Mapping (USM) to capture and utilize common
temporal patterns between users for improved activity recognition. Our results,
evaluated on two public HAR datasets (OPPT and PAMAP2), demonstrate that
CVAE-USM outperforms existing state-of-the-art methods, offering a more
accurate and generalizable solution for cross-user activity recognition.
\\ ( https://arxiv.org/abs/2403.14682 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14683 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:44:50 GMT   (140kb,D)

Title: A Moral Imperative: The Need for Continual Superalignment of Large
  Language Models
Authors: Gokul Puthumanaillam, Manav Vora, Pranay Thangeda, Melkior Ornik
Categories: cs.CY cs.AI cs.CL cs.LG
\\
  This paper examines the challenges associated with achieving life-long
superalignment in AI systems, particularly large language models (LLMs).
Superalignment is a theoretical framework that aspires to ensure that
superintelligent AI systems act in accordance with human values and goals.
Despite its promising vision, we argue that achieving superalignment requires
substantial changes in the current LLM architectures due to their inherent
limitations in comprehending and adapting to the dynamic nature of these human
ethics and evolving global scenarios. We dissect the challenges of encoding an
ever-changing spectrum of human values into LLMs, highlighting the
discrepancies between static AI models and the dynamic nature of human
societies. To illustrate these challenges, we analyze two distinct examples:
one demonstrates a qualitative shift in human values, while the other presents
a quantifiable change. Through these examples, we illustrate how LLMs,
constrained by their training data, fail to align with contemporary human
values and scenarios. The paper concludes by exploring potential strategies to
address and possibly mitigate these alignment discrepancies, suggesting a path
forward in the pursuit of more adaptable and responsive AI systems.
\\ ( https://arxiv.org/abs/2403.14683 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14689 (*cross-listing*)
Date: Wed, 13 Mar 2024 22:38:08 GMT   (2156kb,D)

Title: Developing and Deploying Industry Standards for Artificial Intelligence
  in Education (AIED): Challenges, Strategies, and Future Directions
Authors: Richard Tong, Haoyang Li, Joleen Liang, Qingsong Wen
Categories: cs.CY cs.AI cs.LG
Comments: 12 pages
\\
  The adoption of Artificial Intelligence in Education (AIED) holds the promise
of revolutionizing educational practices by offering personalized learning
experiences, automating administrative and pedagogical tasks, and reducing the
cost of content creation. However, the lack of standardized practices in the
development and deployment of AIED solutions has led to fragmented ecosystems,
which presents challenges in interoperability, scalability, and ethical
governance. This article aims to address the critical need to develop and
implement industry standards in AIED, offering a comprehensive analysis of the
current landscape, challenges, and strategic approaches to overcome these
obstacles. We begin by examining the various applications of AIED in various
educational settings and identify key areas lacking in standardization,
including system interoperability, ontology mapping, data integration,
evaluation, and ethical governance. Then, we propose a multi-tiered framework
for establishing robust industry standards for AIED. In addition, we discuss
methodologies for the iterative development and deployment of standards,
incorporating feedback loops from real-world applications to refine and adapt
standards over time. The paper also highlights the role of emerging
technologies and pedagogical theories in shaping future standards for AIED.
Finally, we outline a strategic roadmap for stakeholders to implement these
standards, fostering a cohesive and ethical AIED ecosystem. By establishing
comprehensive industry standards, such as those by IEEE Artificial Intelligence
Standards Committee (AISC) and International Organization for Standardization
(ISO), we can accelerate and scale AIED solutions to improve educational
outcomes, ensuring that technological advances align with the principles of
inclusivity, fairness, and educational excellence.
\\ ( https://arxiv.org/abs/2403.14689 ,  2156kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14690 (*cross-listing*)
Date: Thu, 14 Mar 2024 11:00:09 GMT   (728kb)

Title: Incorporating Graph Attention Mechanism into Geometric Problem Solving
  Based on Deep Reinforcement Learning
Authors: Xiuqin Zhong, Shengyuan Yan, Gongqi Lin, Hongguang Fu, Liang Xu, Siwen
  Jiang, Lei Huang, Wei Fang
Categories: cs.CY cs.AI cs.CL cs.LG
\\
  In the context of online education, designing an automatic solver for
geometric problems has been considered a crucial step towards general math
Artificial Intelligence (AI), empowered by natural language understanding and
traditional logical inference. In most instances, problems are addressed by
adding auxiliary components such as lines or points. However, adding auxiliary
components automatically is challenging due to the complexity in selecting
suitable auxiliary components especially when pivotal decisions have to be
made. The state-of-the-art performance has been achieved by exhausting all
possible strategies from the category library to identify the one with the
maximum likelihood. However, an extensive strategy search have to be applied to
trade accuracy for ef-ficiency. To add auxiliary components automatically and
efficiently, we present deep reinforcement learning framework based on the
language model, such as BERT. We firstly apply the graph attention mechanism to
reduce the strategy searching space, called AttnStrategy, which only focus on
the conclusion-related components. Meanwhile, a novel algorithm, named
Automatically Adding Auxiliary Components using Reinforcement Learning
framework (A3C-RL), is proposed by forcing an agent to select top strategies,
which incorporates the AttnStrategy and BERT as the memory components. Results
from extensive experiments show that the proposed A3C-RL algorithm can
substantially enhance the average precision by 32.7% compared to the
traditional MCTS. In addition, the A3C-RL algorithm outperforms humans on the
geometric questions from the annual University Entrance Mathematical
Examination of China.
\\ ( https://arxiv.org/abs/2403.14690 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14691 (*cross-listing*)
Date: Fri, 15 Mar 2024 04:04:45 GMT   (471kb)

Title: Large Language Models and User Trust: Focus on Healthcare
Authors: Avishek Choudhury, Zaria Chaudhry
Categories: cs.CY cs.AI
Comments: 1 figure
\\
  This paper explores the evolving relationship between clinician trust in
LLMs, the transformation of data sources from predominantly human-generated to
AI-generated content, and the subsequent impact on the precision of LLMs and
clinician competence. One of the primary concerns identified is the potential
feedback loop that arises as LLMs become more reliant on their outputs for
learning, which may lead to a degradation in output quality and a reduction in
clinician skills due to decreased engagement with fundamental diagnostic
processes. While theoretical at this stage, this feedback loop poses a
significant challenge as the integration of LLMs in healthcare deepens,
emphasizing the need for proactive dialogue and strategic measures to ensure
the safe and effective use of LLM technology. Moreover, we delve into the
potential risks associated with LLMs' self-referential learning loops and the
deskilling of healthcare professionals. The risk of LLMs operating within an
echo chamber, where AI-generated content feeds into the learning algorithms,
threatens the diversity and quality of the data pool, potentially entrenching
biases and reducing the efficacy of LLMs. Concurrently, reliance on LLMs for
routine or critical tasks could result in a decline in healthcare providers'
diagnostic and thinking skills, particularly affecting the training and
development of future professionals.
\\ ( https://arxiv.org/abs/2403.14691 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14692 (*cross-listing*)
Date: Fri, 15 Mar 2024 08:00:02 GMT   (337kb)

Title: The AI Assessment Scale (AIAS) in action: A pilot implementation of
  GenAI supported assessment
Authors: Leon Furze, Mike Perkins, Jasper Roe, Jason MacVaugh
Categories: cs.CY cs.AI
\\
  The rapid adoption of Generative Artificial Intelligence (GenAI) technologies
in higher education has raised concerns about academic integrity, assessment
practices, and student learning. Banning or blocking GenAI tools has proven
ineffective, and punitive approaches ignore the potential benefits of these
technologies. This paper presents the findings of a pilot study conducted at
British University Vietnam (BUV) exploring the implementation of the Artificial
Intelligence Assessment Scale (AIAS), a flexible framework for incorporating
GenAI into educational assessments. The AIAS consists of five levels, ranging
from 'No AI' to 'Full AI', enabling educators to design assessments that focus
on areas requiring human input and critical thinking.
  Following the implementation of the AIAS, the pilot study results indicate a
significant reduction in academic misconduct cases related to GenAI, a 5.9%
increase in student attainment across the university, and a 33.3% increase in
module passing rates. The AIAS facilitated a shift in pedagogical practices,
with faculty members incorporating GenAI tools into their modules and students
producing innovative multimodal submissions. The findings suggest that the AIAS
can support the effective integration of GenAI in HE, promoting academic
integrity while leveraging the technology's potential to enhance learning
experiences.
\\ ( https://arxiv.org/abs/2403.14692 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14693 (*cross-listing*)
Date: Fri, 15 Mar 2024 08:28:38 GMT   (2731kb)

Title: A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to
  Support Atmospheric Research
Authors: Wenwen Li, Hu Shao, Sizhe Wang, Xiran Zhou, Sheng Wu
Categories: cs.CY cs.AI cs.DC cs.IR
MSC-class: big data, cyberinfrastructure, cloud computing
\\
  Big earth science data offers the scientific community great opportunities.
Many more studies at large-scales, over long-terms and at high resolution can
now be conducted using the rich information collected by remote sensing
satellites, ground-based sensor networks, and even social media input. However,
the hundreds of terabytes of information collected and compiled on an hourly
basis by NASA and other government agencies present a significant challenge for
atmospheric scientists seeking to improve the understanding of the Earth
atmospheric system. These challenges include effective discovery, organization,
analysis and visualization of large amounts of data. This paper reports the
outcomes of an NSF-funded project that developed a geospatial
cyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) --
to support atmospheric research. We first introduce the service-oriented system
framework then describe in detail the implementation of the data discovery
module, data management module, data integration module, data analysis and
visualization modules following the cloud computing
principles-Data-as-a-Service, Software-as-a-Service, Platform-as-a-Service and
Infrastructure-as-a-Service. We demonstrate the graphic user interface by
performing an analysis between Sea Surface Temperature and the intensity of
tropical storms in the North Atlantic and Pacific oceans. We expect this work
to contribute to the technical advancement of cyberinfrastructure research as
well as to the development of an online, collaborative scientific analysis
system for atmospheric science.
\\ ( https://arxiv.org/abs/2403.14693 ,  2731kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14694 (*cross-listing*)
Date: Fri, 15 Mar 2024 14:31:52 GMT   (444kb)

Title: Application of GPT Language Models for Innovation in Activities in
  University Teaching
Authors: Manuel de Buenaga and Francisco Javier Bueno
Categories: cs.CY cs.AI cs.CL
Comments: 15 pages, in spanish language, 4 tables, 5 figures
\\
  The GPT (Generative Pre-trained Transformer) language models are an
artificial intelligence and natural language processing technology that enables
automatic text generation. There is a growing interest in applying GPT language
models to university teaching in various dimensions. From the perspective of
innovation in student and teacher activities, they can provide support in
understanding and generating content, problem-solving, as well as
personalization and test correction, among others. From the dimension of
internationalization, the misuse of these models represents a global problem
that requires taking a series of common measures in universities from different
geographical areas. In several countries, there has been a review of assessment
tools to ensure that work is done by students and not by AI. To this end, we
have conducted a detailed experiment in a representative subject of Computer
Science such as Software Engineering, which has focused on evaluating the use
of ChatGPT as an assistant in theory activities, exercises, and laboratory
practices, assessing its potential use as a support tool for both students and
teachers.
\\ ( https://arxiv.org/abs/2403.14694 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14697 (*cross-listing*)
Date: Fri, 15 Mar 2024 20:30:02 GMT   (242kb)

Title: An AIC-based approach for articulating unpredictable problems in open
  complex environments
Authors: Haider AL-Shareefy, Michael Butler, Thai Son Hoang
Categories: cs.CY cs.AI cs.SE
Comments: S. Bernardi, T. Zoppi (Editors), "Fast Abstracts and Student Forum
  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,
  Leuven, Belgium, 8-11 April 2024"
\\
  This research paper presents an approach to enhancing the predictive
capability of architects in the design and assurance of systems, focusing on
systems operating in dynamic and unpredictable environments. By adopting a
systems approach, we aim to improve architects' predictive capabilities in
designing dependable systems (for example, ML-based systems). An aerospace case
study is used to illustrate the approach. Multiple factors (challenges)
influencing aircraft detection are identified, demonstrating the effectiveness
of our approach in a complex operational setting. Our approach primarily aimed
to enhance the architect's predictive capability.
\\ ( https://arxiv.org/abs/2403.14697 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14704 (*cross-listing*)
Date: Sun, 17 Mar 2024 09:33:37 GMT   (39kb)

Title: A minimal coalition logic
Authors: Yinfeng Li and Fengkui Ju
Categories: cs.LO cs.AI
\\
  Coalition logic is a central logic in strategic reasoning studies. In this
paper, we first argue that Coalition Logic models, concurrent game models, have
three too-strong assumptions. The first one is the independence of agents; that
is, the merge of two available joint actions of two disjoint coalitions is
always available for the union of the two coalitions. The second one is
seriality; that is, coalitions always have available joint actions. The third
one is determinism, that is, the grand coalition's joint actions always have a
unique outcome. Second, we present a coalition logic based on general
concurrent game models, which do not have the three assumptions. We show the
completeness of this logic and compare it with Coalition Logic in detail. This
logic seems minimal in the context of strategic reasoning.
\\ ( https://arxiv.org/abs/2403.14704 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14706 (*cross-listing*)
Date: Sun, 17 Mar 2024 13:08:28 GMT   (1355kb,D)

Title: Safeguarding Marketing Research: The Generation, Identification, and
  Mitigation of AI-Fabricated Disinformation
Authors: Anirban Mukherjee
Categories: cs.CY cs.AI
\\
  Generative AI has ushered in the ability to generate content that closely
mimics human contributions, introducing an unprecedented threat: Deployed en
masse, these models can be used to manipulate public opinion and distort
perceptions, resulting in a decline in trust towards digital platforms. This
study contributes to marketing literature and practice in three ways. First, it
demonstrates the proficiency of AI in fabricating disinformative user-generated
content (UGC) that mimics the form of authentic content. Second, it quantifies
the disruptive impact of such UGC on marketing research, highlighting the
susceptibility of analytics frameworks to even minimal levels of
disinformation. Third, it proposes and evaluates advanced detection frameworks,
revealing that standard techniques are insufficient for filtering out
AI-generated disinformation. We advocate for a comprehensive approach to
safeguarding marketing research that integrates advanced algorithmic solutions,
enhanced human oversight, and a reevaluation of regulatory and ethical
frameworks. Our study seeks to serve as a catalyst, providing a foundation for
future research and policy-making aimed at navigating the intricate challenges
at the nexus of technology, ethics, and marketing.
\\ ( https://arxiv.org/abs/2403.14706 ,  1355kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14710 (*cross-listing*)
Date: Mon, 18 Mar 2024 12:12:38 GMT   (1333kb,D)

Title: Use of recommendation models to provide support to dyslexic students
Authors: Gianluca Morciano, Jos\'e Manuel Alcalde-Llergo, Andrea Zingoni,
  Enrique Yeguas-Bolivar, Juri Taborri, Giuseppe Calabr\`o
Categories: cs.CY cs.AI
Comments: 36 pages, 4 figures and 6 tables. Preprint submitted to Expert
  Systems with Applications
\\
  Dyslexia is the most widespread specific learning disorder and significantly
impair different cognitive domains. This, in turn, negatively affects dyslexic
students during their learning path. Therefore, specific support must be given
to these students. In addition, such a support must be highly personalized,
since the problems generated by the disorder can be very different from one to
another. In this work, we explored the possibility of using AI to suggest the
most suitable supporting tools for dyslexic students, so as to provide a
targeted help that can be of real utility. To do this, we relied on
recommendation algorithms, which are a branch of machine learning, that aim to
detect personal preferences and provide the most suitable suggestions. We hence
implemented and trained three collaborative-filtering recommendation models,
namely an item-based, a user-based and a weighted-hybrid model, and studied
their performance on a large database of 1237 students' information, collected
with a self-evaluating questionnaire regarding all the most used supporting
strategies and digital tools. Each recommendation model was tested with three
different similarity metrics, namely Pearson correlation, Euclidean distance
and Cosine similarity. The obtained results showed that a recommendation system
is highly effective in suggesting the optimal help tools/strategies for
everyone. This demonstrates that the proposed approach is successful and can be
used as a new and effective methodology to support students with dyslexia.
\\ ( https://arxiv.org/abs/2403.14710 ,  1333kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14711 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:25:57 GMT   (51kb,D)

Title: Human-in-the-Loop AI for Cheating Ring Detection
Authors: Yong-Siang Shih, Manqian Liao, Ruidong Liu, Mirza Basim Baig
Categories: cs.CY cs.AI cs.HC cs.LG
Comments: Accepted to the AI4Ed Workshop at AAAI 2024 as a short paper
\\
  Online exams have become popular in recent years due to their accessibility.
However, some concerns have been raised about the security of the online exams,
particularly in the context of professional cheating services aiding malicious
test takers in passing exams, forming so-called "cheating rings". In this
paper, we introduce a human-in-the-loop AI cheating ring detection system
designed to detect and deter these cheating rings. We outline the underlying
logic of this human-in-the-loop AI system, exploring its design principles
tailored to achieve its objectives of detecting cheaters. Moreover, we
illustrate the methodologies used to evaluate its performance and fairness,
aiming to mitigate the unintended risks associated with the AI system. The
design and development of the system adhere to Responsible AI (RAI) standards,
ensuring that ethical considerations are integrated throughout the entire
development process.
\\ ( https://arxiv.org/abs/2403.14711 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14712 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:03:17 GMT   (4575kb,D)

Title: AI for bureaucratic productivity: Measuring the potential of AI to help
  automate 143 million UK government transactions
Authors: Vincent J. Straub, Youmna Hashem, Jonathan Bright, Satyam Bhagwanani,
  Deborah Morgan, John Francis, Saba Esnaashari, Helen Margetts
Categories: cs.CY cs.AI
\\
  There is currently considerable excitement within government about the
potential of artificial intelligence to improve public service productivity
through the automation of complex but repetitive bureaucratic tasks, freeing up
the time of skilled staff. Here, we explore the size of this opportunity, by
mapping out the scale of citizen-facing bureaucratic decision-making procedures
within UK central government, and measuring their potential for AI-driven
automation. We estimate that UK central government conducts approximately one
billion citizen-facing transactions per year in the provision of around 400
services, of which approximately 143 million are complex repetitive
transactions. We estimate that 84% of these complex transactions are highly
automatable, representing a huge potential opportunity: saving even an average
of just one minute per complex transaction would save the equivalent of
approximately 1,200 person-years of work every year. We also develop a model to
estimate the volume of transactions a government service undertakes, providing
a way for government to avoid conducting time consuming transaction volume
measurements. Finally, we find that there is high turnover in the types of
services government provide, meaning that automation efforts should focus on
general procedures rather than services themselves which are likely to evolve
over time. Overall, our work presents a novel perspective on the structure and
functioning of modern government, and how it might evolve in the age of
artificial intelligence.
\\ ( https://arxiv.org/abs/2403.14712 ,  4575kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14734 (*cross-listing*)
Date: Thu, 21 Mar 2024 08:54:56 GMT   (2212kb,D)

Title: A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
Authors: Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue
  Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, Qipeng Guo, Xipeng
  Qiu, Pengcheng Yin, Xiaoli Li, Fei Yuan, Lingpeng Kong, Xiang Li, Zhiyong Wu
Categories: cs.SE cs.AI cs.CL cs.PL
Comments: 64 pages, 6 figures, 10 tables, 688 references
\\
  Neural Code Intelligence -- leveraging deep learning to understand, generate,
and optimize code -- holds immense potential for transformative impacts on the
whole society. Bridging the gap between Natural Language and Programming
Language, this domain has drawn significant attention from researchers in both
research communities over the past few years. This survey presents a systematic
and chronological review of the advancements in code intelligence, encompassing
over 50 representative models and their variants, more than 20 categories of
tasks, and an extensive coverage of over 680 related works. We follow the
historical progression to trace the paradigm shifts across different research
phases (e.g., from modeling code with recurrent neural networks to the era of
Large Language Models). Concurrently, we highlight the major technical
transitions in models, tasks, and evaluations spanning through different
stages. For applications, we also observe a co-evolving shift. It spans from
initial endeavors to tackling specific scenarios, through exploring a diverse
array of tasks during its rapid expansion, to currently focusing on tackling
increasingly complex and varied real-world challenges. Building on our
examination of the developmental trajectories, we further investigate the
emerging synergies between code intelligence and broader machine intelligence,
uncovering new cross-domain opportunities and illustrating the substantial
influence of code intelligence across various domains. Finally, we delve into
both the opportunities and challenges associated with this field, alongside
elucidating our insights on the most promising research directions. An ongoing,
dynamically updated project and resources associated with this survey have been
released at https://github.com/QiushiSun/NCISurvey.
\\ ( https://arxiv.org/abs/2403.14734 ,  2212kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14736 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:27:57 GMT   (2920kb,D)

Title: NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein
  Classification in Graph Neural Networks
Authors: Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho
Categories: q-bio.QM cs.AI cs.LG
Comments: Submitted to ICML 2024
\\
  Protein classification tasks are essential in drug discovery. Real-world
protein structures are dynamic, which will determine the properties of
proteins. However, the existing machine learning methods, like ProNet (Wang et
al., 2022a), only access limited conformational characteristics and protein
side-chain features, leading to impractical protein structure and inaccuracy of
protein classes in their predictions. In this paper, we propose novel semantic
data augmentation methods, Novel Augmentation of New Node Attributes (NaNa),
and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate
backbone chemical and side-chain biophysical information into protein
classification tasks and a co-embedding residual learning framework.
Specifically, we leverage molecular biophysical, secondary structure, chemical
bonds, and ionic features of proteins to facilitate protein classification
tasks. Furthermore, our semantic augmentation methods and the co-embedding
residual learning framework can improve the performance of GIN (Xu et al.,
2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%
and 11.33% respectively. Our code is available at
https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.
\\ ( https://arxiv.org/abs/2403.14736 ,  2920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14772 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:26:23 GMT   (7105kb,D)

Title: Improving Robustness to Model Inversion Attacks via Sparse Coding
  Architectures
Authors: Sayanton V. Dibbo, Adam Breuer, Juston Moore, Michael Teti
Categories: cs.CV cs.AI cs.CR cs.LG
Comments: 32 pages, 15 Tables, and 9 Figures
\\
  Recent model inversion attack algorithms permit adversaries to reconstruct a
neural network's private training data just by repeatedly querying the network
and inspecting its outputs. In this work, we develop a novel network
architecture that leverages sparse-coding layers to obtain superior robustness
to this class of attacks. Three decades of computer science research has
studied sparse coding in the context of image denoising, object recognition,
and adversarial misclassification settings, but to the best of our knowledge,
its connection to state-of-the-art privacy vulnerabilities remains unstudied.
However, sparse coding architectures suggest an advantageous means to defend
against model inversion attacks because they allow us to control the amount of
irrelevant private information encoded in a network's intermediate
representations in a manner that can be computed efficiently during training
and that is known to have little effect on classification accuracy.
Specifically, compared to networks trained with a variety of state-of-the-art
defenses, our sparse-coding architectures maintain comparable or higher
classification accuracy while degrading state-of-the-art training data
reconstructions by factors of 1.1 to 18.3 across a variety of reconstruction
quality metrics (PSNR, SSIM, FID). This performance advantage holds across 5
datasets ranging from CelebA faces to medical images and CIFAR-10, and across
various state-of-the-art SGD-based and GAN-based inversion attacks, including
Plug-&-Play attacks. We provide a cluster-ready PyTorch codebase to promote
research and standardize defense evaluations.
\\ ( https://arxiv.org/abs/2403.14772 ,  7105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14773 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:27:29 GMT   (21355kb,D)

Title: StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation
  from Text
Authors: Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk
  Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi
Categories: cs.CV cs.AI cs.CL cs.LG cs.MM eess.IV
Comments: https://github.com/Picsart-AI-Research/StreamingT2V
\\
  Text-to-video diffusion models enable the generation of high-quality videos
that follow text instructions, making it easy to create diverse and individual
content. However, existing approaches mostly focus on high-quality short video
generation (typically 16 or 24 frames), ending up with hard-cuts when naively
extended to the case of long video synthesis. To overcome these limitations, we
introduce StreamingT2V, an autoregressive approach for long video generation of
80, 240, 600, 1200 or more frames with smooth transitions. The key components
are:(i) a short-term memory block called conditional attention module (CAM),
which conditions the current generation on the features extracted from the
previous chunk via an attentional mechanism, leading to consistent chunk
transitions, (ii) a long-term memory block called appearance preservation
module, which extracts high-level scene and object features from the first
video chunk to prevent the model from forgetting the initial scene, and (iii) a
randomized blending approach that enables to apply a video enhancer
autoregressively for infinitely long videos without inconsistencies between
chunks. Experiments show that StreamingT2V generates high motion amount. In
contrast, all competing image-to-video methods are prone to video stagnation
when applied naively in an autoregressive manner. Thus, we propose with
StreamingT2V a high-quality seamless text-to-long video generator that
outperforms competitors with consistency and motion. Our code will be available
at: https://github.com/Picsart-AI-Research/StreamingT2V
\\ ( https://arxiv.org/abs/2403.14773 ,  21355kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14783 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:57:25 GMT   (11553kb,D)

Title: Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot
  Visual Question Answering
Authors: Bowen Jiang, Zhijun Zhuang, Shreyas S. Shivakumar, Dan Roth, Camillo
  J. Taylor
Categories: cs.CV cs.AI cs.CL cs.LG cs.MA
Comments: A full version of the paper will be released soon. The codes are
  available at https://github.com/bowen-upenn/Multi-Agent-VQA
\\
  This work explores the zero-shot capabilities of foundation models in Visual
Question Answering (VQA) tasks. We propose an adaptive multi-agent system,
named Multi-Agent VQA, to overcome the limitations of foundation models in
object detection and counting by using specialized agents as tools. Unlike
existing approaches, our study focuses on the system's performance without
fine-tuning it on specific VQA datasets, making it more practical and robust in
the open world. We present preliminary experimental results under zero-shot
scenarios and highlight some failure cases, offering new directions for future
research.
\\ ( https://arxiv.org/abs/2403.14783 ,  11553kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14790 (*cross-listing*)
Date: Thu, 21 Mar 2024 19:09:21 GMT   (42920kb,D)

Title: Latent Diffusion Models for Attribute-Preserving Image Anonymization
Authors: Luca Piano, Pietro Basci, Fabrizio Lamberti, Lia Morra
Categories: cs.CV cs.AI
\\
  Generative techniques for image anonymization have great potential to
generate datasets that protect the privacy of those depicted in the images,
while achieving high data fidelity and utility. Existing methods have focused
extensively on preserving facial attributes, but failed to embrace a more
comprehensive perspective that considers the scene and background into the
anonymization process. This paper presents, to the best of our knowledge, the
first approach to image anonymization based on Latent Diffusion Models (LDMs).
Every element of a scene is maintained to convey the same meaning, yet
manipulated in a way that makes re-identification difficult. We propose two
LDMs for this purpose: CAMOUFLaGE-Base exploits a combination of pre-trained
ControlNets, and a new controlling mechanism designed to increase the distance
between the real and anonymized images. CAMOFULaGE-Light is based on the
Adapter technique, coupled with an encoding designed to efficiently represent
the attributes of different persons in a scene. The former solution achieves
superior performance on most metrics and benchmarks, while the latter cuts the
inference time in half at the cost of fine-tuning a lightweight module. We show
through extensive experimental comparison that the proposed method is
competitive with the state-of-the-art concerning identity obfuscation whilst
better preserving the original content of the image and tackling unresolved
challenges that current solutions fail to address.
\\ ( https://arxiv.org/abs/2403.14790 ,  42920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14791 (*cross-listing*)
Date: Thu, 21 Mar 2024 19:12:37 GMT   (429kb,D)

Title: Particip-AI: A Democratic Surveying Framework for Anticipating Future AI
  Use Cases, Harms and Benefits
Authors: Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCario,
  Yejin Choi, Tadayoshi Kohno, Maarten Sap
Categories: cs.CY cs.AI
Comments: 35 pages, 4 figures, 23 tables
\\
  General purpose AI, such as ChatGPT, seems to have lowered the barriers for
the public to use AI and harness its power. However, the governance and
development of AI still remain in the hands of a few, and the pace of
development is accelerating without proper assessment of risks. As a first step
towards democratic governance and risk assessment of AI, we introduce
Particip-AI, a framework to gather current and future AI use cases and their
harms and benefits from non-expert public. Our framework allows us to study
more nuanced and detailed public opinions on AI through collecting use cases,
surfacing diverse harms through risk assessment under alternate scenarios
(i.e., developing and not developing a use case), and illuminating tensions
over AI development through making a concluding choice on its development. To
showcase the promise of our framework towards guiding democratic AI, we gather
responses from 295 demographically diverse participants. We find that
participants' responses emphasize applications for personal life and society,
contrasting with most current AI development's business focus. This shows the
value of surfacing diverse harms that are complementary to expert assessments.
Furthermore, we found that perceived impact of not developing use cases
predicted participants' judgements of whether AI use cases should be developed,
and highlighted lay users' concerns of techno-solutionism. We conclude with a
discussion on how frameworks like Particip-AI can further guide democratic AI
governance and regulation.
\\ ( https://arxiv.org/abs/2403.14791 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14817 (*cross-listing*)
Date: Thu, 21 Mar 2024 20:14:53 GMT   (195kb,D)

Title: Crowdsourced Multilingual Speech Intelligibility Testing
Authors: Laura Lechler and Kamil Wojcicki
Categories: eess.AS cs.AI eess.SP
DOI: 10.1109/ICASSP48485.2024.10447869
\\
  With the advent of generative audio features, there is an increasing need for
rapid evaluation of their impact on speech intelligibility. Beyond the existing
laboratory measures, which are expensive and do not scale well, there has been
comparatively little work on crowdsourced assessment of intelligibility.
Standards and recommendations are yet to be defined, and publicly available
multilingual test materials are lacking. In response to this challenge, we
propose an approach for a crowdsourced intelligibility assessment. We detail
the test design, the collection and public release of the multilingual speech
data, and the results of our early experiments.
\\ ( https://arxiv.org/abs/2403.14817 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14864 (*cross-listing*)
Date: Thu, 21 Mar 2024 22:18:59 GMT   (11937kb,D)

Title: Learning Quadruped Locomotion Using Differentiable Simulation
Authors: Yunlong Song, Sangbae Kim, Davide Scaramuzza
Categories: cs.RO cs.AI
\\
  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
\\ ( https://arxiv.org/abs/2403.14864 ,  11937kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14965 (*cross-listing*)
Date: Fri, 22 Mar 2024 05:37:52 GMT   (372kb)

Title: Comprehensive Evaluation and Insights into the Use of Large Language
  Models in the Automation of Behavior-Driven Development Acceptance Test
  Formulation
Authors: Shanthi Karpurapu, Sravanthy Myneni, Unnati Nettur, Likhit Sagar
  Gajja, Dave Burke, Tom Stiehm, and Jeffery Payne
Categories: cs.SE cs.AI
ACM-class: I.2.7; I.2.1
\\
  Behavior-driven development (BDD) is an Agile testing methodology fostering
collaboration among developers, QA analysts, and stakeholders. In this
manuscript, we propose a novel approach to enhance BDD practices using large
language models (LLMs) to automate acceptance test generation. Our study uses
zero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B,
and PaLM-2. The paper presents a detailed methodology that includes the
dataset, prompt techniques, LLMs, and the evaluation process. The results
demonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests
with better performance. The few-shot prompt technique highlights its ability
to provide higher accuracy by incorporating examples for in-context learning.
Furthermore, the study examines syntax errors, validation accuracy, and
comparative analysis of LLMs, revealing their effectiveness in enhancing BDD
practices. However, our study acknowledges that there are limitations to the
proposed approach. We emphasize that this approach can support collaborative
BDD processes and create opportunities for future research into automated BDD
acceptance test generation using LLMs.
\\ ( https://arxiv.org/abs/2403.14965 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14977 (*cross-listing*)
Date: Fri, 22 Mar 2024 06:22:20 GMT   (1004kb,D)

Title: Piecewise-Linear Manifolds for Deep Metric Learning
Authors: Shubhang Bhatnagar and Narendra Ahuja
Categories: cs.CV cs.AI cs.LG eess.IV
Comments: Accepted at CPAL 2024 (Oral)
\\
  Unsupervised deep metric learning (UDML) focuses on learning a semantic
representation space using only unlabeled data. This challenging problem
requires accurately estimating the similarity between data points, which is
used to supervise a deep network. For this purpose, we propose to model the
high-dimensional data manifold using a piecewise-linear approximation, with
each low-dimensional linear piece approximating the data manifold in a small
neighborhood of a point. These neighborhoods are used to estimate similarity
between data points. We empirically show that this similarity estimate
correlates better with the ground truth than the similarity estimates of
current state-of-the-art techniques. We also show that proxies, commonly used
in supervised metric learning, can be used to model the piecewise-linear
manifold in an unsupervised setting, helping improve performance. Our method
outperforms existing unsupervised metric learning approaches on standard
zero-shot image retrieval benchmarks.
\\ ( https://arxiv.org/abs/2403.14977 ,  1004kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15044 (*cross-listing*)
Date: Fri, 22 Mar 2024 09:00:24 GMT   (147kb,D)

Title: Multimodal Fusion with Pre-Trained Model Features in Affective Behaviour
  Analysis In-the-wild
Authors: Zhuofan Wen, Fengyu Zhang, Siyuan Zhang, Haiyang Sun, Mingyu Xu, Licai
  Sun, Zheng Lian, Bin Liu, and Jianhua Tao
Categories: cs.CV cs.AI
\\
  Multimodal fusion is a significant method for most multimodal tasks. With the
recent surge in the number of large pre-trained models, combining both
multimodal fusion methods and pre-trained model features can achieve
outstanding performance in many multimodal tasks. In this paper, we present our
approach, which leverages both advantages for addressing the task of Expression
(Expr) Recognition and Valence-Arousal (VA) Estimation. We evaluate the
Aff-Wild2 database using pre-trained models, then extract the final hidden
layers of the models as features. Following preprocessing and interpolation or
convolution to align the extracted features, different models are employed for
modal fusion. Our code is available at GitHub - FulgenceWen/ABAW6th.
\\ ( https://arxiv.org/abs/2403.15044 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15048 (*cross-listing*)
Date: Fri, 22 Mar 2024 09:13:09 GMT   (19954kb,D)

Title: Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
Authors: Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: 11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/
\\
  Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.
\\ ( https://arxiv.org/abs/2403.15048 ,  19954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15049 (*cross-listing*)
Date: Fri, 22 Mar 2024 09:15:36 GMT   (4926kb,D)

Title: Continual Vision-and-Language Navigation
Authors: Seongjun Jeong, Gi-Cheon Kang, Seongho Choi, Joochan Kim, Byoung-Tak
  Zhang
Categories: cs.CV cs.AI
\\
  Vision-and-Language Navigation (VLN) agents navigate to a destination using
natural language instructions and the visual information they observe. Existing
methods for training VLN agents presuppose fixed datasets, leading to a
significant limitation: the introduction of new environments necessitates
retraining with previously encountered environments to preserve their
knowledge. This makes it difficult to train VLN agents that operate in the
ever-changing real world. To address this limitation, we present the Continual
Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents
trained through a continual learning process. For the training and evaluation
of CVLN agents, we re-arrange existing VLN datasets to propose two datasets:
CVLN-I, focused on navigation via initial-instruction interpretation, and
CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we
propose two novel rehearsal-based methods for CVLN, Perplexity Replay (PerpR)
and Episodic Self-Replay (ESR). PerpR prioritizes replaying challenging
episodes based on action perplexity, while ESR replays previously predicted
action logits to preserve learned behaviors. We demonstrate the effectiveness
of the proposed methods on CVLN through extensive experiments.
\\ ( https://arxiv.org/abs/2403.15049 ,  4926kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15059 (*cross-listing*)
Date: Fri, 22 Mar 2024 09:32:31 GMT   (16760kb,D)

Title: MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition
  Integration
Authors: Zhichao Wei, Qingkun Su, Long Qin, Weizhi Wang
Categories: cs.CV cs.AI
\\
  Recent advances in tuning-free personalized image generation based on
diffusion models are impressive. However, to improve subject fidelity, existing
methods either retrain the diffusion model or infuse it with dense visual
embeddings, both of which suffer from poor generalization and efficiency. Also,
these methods falter in multi-subject image generation due to the unconstrained
cross-attention mechanism. In this paper, we propose MM-Diff, a unified and
tuning-free image personalization framework capable of generating high-fidelity
images of both single and multiple subjects in seconds. Specifically, to
simultaneously enhance text consistency and subject fidelity, MM-Diff employs a
vision encoder to transform the input image into CLS and patch embeddings. CLS
embeddings are used on the one hand to augment the text embeddings, and on the
other hand together with patch embeddings to derive a small number of
detail-rich subject embeddings, both of which are efficiently integrated into
the diffusion model through the well-designed multimodal cross-attention
mechanism. Additionally, MM-Diff introduces cross-attention map constraints
during the training phase, ensuring flexible multi-subject image sampling
during inference without any predefined inputs (e.g., layout). Extensive
experiments demonstrate the superior performance of MM-Diff over other leading
methods.
\\ ( https://arxiv.org/abs/2403.15059 ,  16760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15075 (*cross-listing*)
Date: Fri, 22 Mar 2024 09:58:33 GMT   (8301kb,D)

Title: Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation
Authors: Jiaheng Yu, Jing Li, Yue He, Kai Zhu, Shuyi Zhang and Wen Hu
Categories: cs.IR cs.AI
\\
  Recent methods utilize graph contrastive Learning within graph-structured
user-item interaction data for collaborative filtering and have demonstrated
their efficacy in recommendation tasks. However, they ignore that the
difference relation density of nodes between the user- and item-side causes the
adaptability of graphs on bilateral nodes to be different after multi-hop graph
interaction calculation, which limits existing models to achieve ideal results.
To solve this issue, we propose a novel framework for recommendation tasks
called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that
consider the bilateral unsymmetry on user-item node relation density for sliced
user and item graph reasoning better with bilateral slicing contrastive
training. Especially, taking into account the aggregation ability of
hypergraph-based graph convolutional network (GCN) in digging implicit
similarities is more suitable for user nodes, embeddings generated from three
different modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into
two subviews by the user- and item-side respectively, and selectively combined
into subview pairs bilaterally based on the characteristics of inter-node
relation structure. Furthermore, to align the distribution of user and item
embeddings after aggregation, a dispersing loss is leveraged to adjust the
mutual distance between all embeddings for maintaining learning ability.
Comprehensive experiments on two public datasets have proved the superiority of
BusGCL in comparison to various recommendation methods. Other models can simply
utilize our bilateral slicing contrastive learning to enhance recommending
performance without incurring extra expenses.
\\ ( https://arxiv.org/abs/2403.15075 ,  8301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15076 (*cross-listing*)
Date: Fri, 22 Mar 2024 10:00:52 GMT   (24757kb)

Title: Comprehensive Lipidomic Automation Workflow using Large Language Models
Authors: Connor Beveridge, Sanjay Iyer, Caitlin E. Randolph, Matthew Muhoberac,
  Palak Manchanda, Amy C. Clingenpeel, Shane Tichy, Gaurav Chopra
Categories: q-bio.QM cs.AI q-bio.BM q-bio.SC
Comments: 53 pages, 4 main figures, 23 Supporting figures, 10 Supporting Tables
\\
  Lipidomics generates large data that makes manual annotation and
interpretation challenging. Lipid chemical and structural diversity with
structural isomers further complicates annotation. Although, several commercial
and open-source software for targeted lipid identification exists, it lacks
automated method generation workflows and integration with statistical and
bioinformatics tools. We have developed the Comprehensive Lipidomic Automated
Workflow (CLAW) platform with integrated workflow for parsing, detailed
statistical analysis and lipid annotations based on custom multiple reaction
monitoring (MRM) precursor and product ion pair transitions. CLAW contains
several modules including identification of carbon-carbon double bond
position(s) in unsaturated lipids when combined with ozone electrospray
ionization (OzESI)-MRM methodology. To demonstrate the utility of the automated
workflow in CLAW, large-scale lipidomics data was collected with traditional
and OzESI-MRM profiling on biological and non-biological samples. Specifically,
a total of 1497 transitions organized into 10 MRM-based mass spectrometry
methods were used to profile lipid droplets isolated from different brain
regions of 18-24 month-old Alzheimer's disease mice and age-matched wild-type
controls. Additionally, triacyclglycerols (TGs) profiles with carbon-carbon
double bond specificity were generated from canola oil samples using OzESI-MRM
profiling. We also developed an integrated language user interface with large
language models using artificially intelligent (AI) agents that permits users
to interact with the CLAW platform using a chatbot terminal to perform
statistical and bioinformatic analyses. We envision CLAW pipeline to be used in
high-throughput lipid structural identification tasks aiding users to generate
automated lipidomics workflows ranging from data acquisition to AI agent-based
bioinformatic analysis.
\\ ( https://arxiv.org/abs/2403.15076 ,  24757kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15100 (*cross-listing*)
Date: Fri, 22 Mar 2024 10:39:22 GMT   (2189kb,D)

Title: Subequivariant Reinforcement Learning Framework for Coordinated Motion
  Control
Authors: Haoyu Wang, Xiaoyu Tan, Xihe Qiu and Chao Qu
Categories: cs.RO cs.AI
Comments: 7 pages, 7 figures, 2024 IEEE International Conference on Robotics
  and Automation
\\
  Effective coordination is crucial for motion control with reinforcement
learning, especially as the complexity of agents and their motions increases.
However, many existing methods struggle to account for the intricate
dependencies between joints. We introduce CoordiGraph, a novel architecture
that leverages subequivariant principles from physics to enhance coordination
of motion control with reinforcement learning. This method embeds the
principles of equivariance as inherent patterns in the learning process under
gravity influence, which aids in modeling the nuanced relationships between
joints vital for motion control. Through extensive experimentation with
sophisticated agents in diverse environments, we highlight the merits of our
approach. Compared to current leading methods, CoordiGraph notably enhances
generalization and sample efficiency.
\\ ( https://arxiv.org/abs/2403.15100 ,  2189kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15114 (*cross-listing*)
Date: Fri, 22 Mar 2024 11:16:11 GMT   (4377kb,D)

Title: Solving a Real-World Package Delivery Routing Problem Using Quantum
  Annealers
Authors: Eneko Osaba, Esther Villar-Rodriguez and Ant\'on Asla
Categories: cs.ET cs.AI
Comments: 15 pages, 11 figures and 4 tables. Paper submitted for review in
  Scientific Reports
\\
  Research focused on the conjunction between quantum computing and routing
problems has been very prolific in recent years. Most of the works revolve
around classical problems such as the Traveling Salesman Problem or the Vehicle
Routing Problem. Even though working on these problems is valuable, it is also
undeniable that their academic-oriented nature falls short of real-world
requirements. The main objective of this research is to present a solving
method for realistic instances, avoiding problem relaxations or technical
shortcuts. Instead, a quantum-classical hybrid solver has been developed,
coined Q4RPD, that considers a set of real constraints such as a heterogeneous
fleet of vehicles, priority deliveries, and capacities characterized by two
values: weight and dimensions of the packages. Q4RPD resorts to the Leap
Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the
application of Q4RPD, an experimentation composed of six different instances
has been conducted, aiming to serve as illustrative examples.
\\ ( https://arxiv.org/abs/2403.15114 ,  4377kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15143 (*cross-listing*)
Date: Fri, 22 Mar 2024 11:53:03 GMT   (14017kb,D)

Title: Modular Deep Active Learning Framework for Image Annotation: A Technical
  Report for the Ophthalmo-AI Project
Authors: Md Abdul Kadir, Hasan Md Tusfiqur Alam, Pascale Maul, Hans-J\"urgen
  Profitlich, Moritz Wolf, Daniel Sonntag
Categories: cs.CV cs.AI
Comments: DFKI Technical Report
\\
  Image annotation is one of the most essential tasks for guaranteeing proper
treatment for patients and tracking progress over the course of therapy in the
field of medical imaging and disease diagnosis. However, manually annotating a
lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL)
based segmentation algorithms have completely transformed this process and made
it possible to automate image segmentation. By accurately segmenting medical
images, these algorithms can greatly minimize the time and effort necessary for
manual annotation. Additionally, by incorporating Active Learning (AL) methods,
these segmentation algorithms can perform far more effectively with a smaller
amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end
framework implementing the complete AL cycle. It provides researchers with the
flexibility to choose the type of deep learning model they wish to employ and
includes an annotation tool that supports the classification and segmentation
of medical images. The user-friendly interface allows for easy alteration of
the AL and DL model settings through a configuration file, requiring no prior
programming experience. While MedDeepCyleAL can be applied to any kind of image
data, we have specifically applied it to ophthalmology data in this project.
\\ ( https://arxiv.org/abs/2403.15143 ,  14017kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15176 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:01:10 GMT   (2355kb)

Title: Brain-grounding of semantic vectors improves neural decoding of visual
  stimuli
Authors: Shirin Vafaei, Ryohei Fukuma, Huixiang Yang, Haruhiko Kishima and
  Takufumi Yanagisawa
Categories: q-bio.NC cs.AI
Comments: 16 pages, 4 figures
\\
  Developing algorithms for accurate and comprehensive neural decoding of
mental contents is one of the long-cherished goals in the field of neuroscience
and brain-machine interfaces. Previous studies have demonstrated the
feasibility of neural decoding by training machine learning models to map brain
activity patterns into a semantic vector representation of stimuli. These
vectors, hereafter referred as pretrained feature vectors, are usually derived
from semantic spaces based solely on image and/or text features and therefore
they might have a totally different characteristics than how visual stimuli is
represented in the human brain, resulting in limiting the capability of brain
decoders to learn this mapping. To address this issue, we propose a
representation learning framework, termed brain-grounding of semantic vectors,
which fine-tunes pretrained feature vectors to better align with the neural
representation of visual stimuli in the human brain. We trained this model this
model with functional magnetic resonance imaging (fMRI) of 150 different visual
stimuli categories, and then performed zero-shot brain decoding and
identification analyses on 1) fMRI and 2) magnetoencephalography (MEG).
Interestingly, we observed that by using the brain-grounded vectors, the brain
decoding and identification accuracy on brain data from different neuroimaging
modalities increases. These findings underscore the potential of incorporating
a richer array of brain-derived features to enhance performance of brain
decoding algorithms.
\\ ( https://arxiv.org/abs/2403.15176 ,  2355kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15192 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:24:50 GMT   (14981kb,D)

Title: SFOD: Spiking Fusion Object Detector
Authors: Yimeng Fan, Wei Zhang, Changsong Liu, Mingyang Li, Wenrui Lu
Categories: cs.CV cs.AI
Comments: Accepted by CVPR2024
\\
  Event cameras, characterized by high temporal resolution, high dynamic range,
low power consumption, and high pixel bandwidth, offer unique capabilities for
object detection in specialized contexts. Despite these advantages, the
inherent sparsity and asynchrony of event data pose challenges to existing
object detection algorithms. Spiking Neural Networks (SNNs), inspired by the
way the human brain codes and processes information, offer a potential solution
to these difficulties. However, their performance in object detection using
event cameras is limited in current implementations. In this paper, we propose
the Spiking Fusion Object Detector (SFOD), a simple and efficient approach to
SNN-based object detection. Specifically, we design a Spiking Fusion Module,
achieving the first-time fusion of feature maps from different scales in SNNs
applied to event cameras. Additionally, through integrating our analysis and
experiments conducted during the pretraining of the backbone network on the
NCAR dataset, we delve deeply into the impact of spiking decoding strategies
and loss functions on model performance. Thereby, we establish state-of-the-art
classification results based on SNNs, achieving 93.7\% accuracy on the NCAR
dataset. Experimental results on the GEN1 detection dataset demonstrate that
the SFOD achieves a state-of-the-art mAP of 32.1\%, outperforming existing
SNN-based approaches. Our research not only underscores the potential of SNNs
in object detection with event cameras but also propels the advancement of
SNNs. Code is available at https://github.com/yimeng-fan/SFOD.
\\ ( https://arxiv.org/abs/2403.15192 ,  14981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15195 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:31:24 GMT   (753kb,D)

Title: FSD-Inference: Fully Serverless Distributed Inference with Scalable
  Cloud Communication
Authors: Joe Oakley, Hakan Ferhatosmanoglu
Categories: cs.DC cs.AI cs.LG
Comments: In Proceedings of 2024 IEEE 40th International Conference on Data
  Engineering (ICDE) (to appear)
\\
  Serverless computing offers attractive scalability, elasticity and
cost-effectiveness. However, constraints on memory, CPU and function runtime
have hindered its adoption for data-intensive applications and machine learning
(ML) workloads. Traditional 'server-ful' platforms enable distributed
computation via fast networks and well-established inter-process communication
(IPC) mechanisms such as MPI and shared memory. In the absence of such
solutions in the serverless domain, parallel computation with significant IPC
requirements is challenging. We present FSD-Inference, the first fully
serverless and highly scalable system for distributed ML inference. We explore
potential communication channels, in conjunction with Function-as-a-Service
(FaaS) compute, to design a state-of-the-art solution for distributed ML within
the context of serverless data-intensive computing. We introduce novel fully
serverless communication schemes for ML inference workloads, leveraging both
cloud-based publish-subscribe/queueing and object storage offerings. We
demonstrate how publish-subscribe/queueing services can be adapted for FaaS IPC
with comparable performance to object storage, while offering significantly
reduced cost at high parallelism levels. We conduct in-depth experiments on
benchmark DNNs of various sizes. The results show that when compared to
server-based alternatives, FSD-Inference is significantly more cost-effective
and scalable, and can even achieve competitive performance against optimized
HPC solutions. Experiments also confirm that our serverless solution can handle
large distributed workloads and leverage high degrees of FaaS parallelism.
\\ ( https://arxiv.org/abs/2403.15195 ,  753kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15216 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:03:37 GMT   (14013kb)

Title: (Un)making AI Magic: a Design Taxonomy
Authors: Maria Luce Lupetti, Dave Murray-Rust
Categories: cs.HC cs.AI
DOI: 10.1145/3613904.3641954
\\
  This paper examines the role that enchantment plays in the design of AI
things by constructing a taxonomy of design approaches that increase or
decrease the perception of magic and enchantment. We start from the design
discourse surrounding recent developments in AI technologies, highlighting
specific interaction qualities such as algorithmic uncertainties and errors and
articulating relations to the rhetoric of magic and supernatural thinking.
Through analyzing and reflecting upon 52 students' design projects from two
editions of a Master course in design and AI, we identify seven design
principles and unpack the effects of each in terms of enchantment and
disenchantment. We conclude by articulating ways in which this taxonomy can be
approached and appropriated by design/HCI practitioners, especially to support
exploration and reflexivity.
\\ ( https://arxiv.org/abs/2403.15216 ,  14013kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15218 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:07:07 GMT   (2722kb,D)

Title: Anytime, Anywhere, Anyone: Investigating the Feasibility of Segment
  Anything Model for Crowd-Sourcing Medical Image Annotations
Authors: Pranav Kulkarni, Adway Kanhere, Dharmam Savani, Andrew Chan, Devina
  Chatterjee, Paul H. Yi, Vishwa S. Parekh
Categories: cs.CV cs.AI cs.LG
\\
  Curating annotations for medical image segmentation is a labor-intensive and
time-consuming task that requires domain expertise, resulting in "narrowly"
focused deep learning (DL) models with limited translational utility. Recently,
foundation models like the Segment Anything Model (SAM) have revolutionized
semantic segmentation with exceptional zero-shot generalizability across
various domains, including medical imaging, and hold a lot of promise for
streamlining the annotation process. However, SAM has yet to be evaluated in a
crowd-sourced setting to curate annotations for training 3D DL segmentation
models. In this work, we explore the potential of SAM for crowd-sourcing
"sparse" annotations from non-experts to generate "dense" segmentation masks
for training 3D nnU-Net models, a state-of-the-art DL segmentation model. Our
results indicate that while SAM-generated annotations exhibit high mean Dice
scores compared to ground-truth annotations, nnU-Net models trained on
SAM-generated annotations perform significantly worse than nnU-Net models
trained on ground-truth annotations ($p<0.001$, all).
\\ ( https://arxiv.org/abs/2403.15218 ,  2722kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15235 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:29:03 GMT   (87kb,D)

Title: Multi-perspective Memory Enhanced Network for Identifying Key Nodes in
  Social Networks
Authors: Qiang Zhang, Jiawei Liu, Fanrui Zhang, Xiaoling Zhu, Zheng-Jun Zha
Categories: cs.SI cs.AI
Comments: 7 pages, 1 figures
\\
  Identifying key nodes in social networks plays a crucial role in timely
blocking false information. Existing key node identification methods usually
consider node influence only from the propagation structure perspective and
have insufficient generalization ability to unknown scenarios. In this paper,
we propose a novel Multi-perspective Memory Enhanced Network (MMEN) for
identifying key nodes in social networks, which mines key nodes from multiple
perspectives and utilizes memory networks to store historical information.
Specifically, MMEN first constructs two propagation networks from the
perspectives of user attributes and propagation structure and updates node
feature representations using graph attention networks. Meanwhile, the memory
network is employed to store information of similar subgraphs, enhancing the
model's generalization performance in unknown scenarios. Finally, MMEN applies
adaptive weights to combine the node influence of the two propagation networks
to select the ultimate key nodes. Extensive experiments demonstrate that our
method significantly outperforms previous methods.
\\ ( https://arxiv.org/abs/2403.15235 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15245 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:41:55 GMT   (6977kb,D)

Title: Reasoning-Enhanced Object-Centric Learning for Videos
Authors: Jian Li, Pu Ren, Yang Liu, Hao Sun
Categories: cs.CV cs.AI cs.LG
\\
  Object-centric learning aims to break down complex visual scenes into more
manageable object representations, enhancing the understanding and reasoning
abilities of machine learning systems toward the physical world. Recently,
slot-based video models have demonstrated remarkable proficiency in segmenting
and tracking objects, but they overlook the importance of the effective
reasoning module. In the real world, reasoning and predictive abilities play a
crucial role in human perception and object tracking; in particular, these
abilities are closely related to human intuitive physics. Inspired by this, we
designed a novel reasoning module called the Slot-based Time-Space Transformer
with Memory buffer (STATM) to enhance the model's perception ability in complex
scenes. The memory buffer primarily serves as storage for slot information from
upstream modules, the Slot-based Time-Space Transformer makes predictions
through slot-based spatiotemporal attention computations and fusion. Our
experiment results on various datasets show that STATM can significantly
enhance object-centric learning capabilities of slot-based video models.
\\ ( https://arxiv.org/abs/2403.15245 ,  6977kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15248 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:46:51 GMT   (34604kb,D)

Title: Self-Supervised Backbone Framework for Diverse Agricultural Vision Tasks
Authors: Sudhir Sornapudi (1), Rajhans Singh (1) ((1) Corteva Agriscience,
  Indianapolis, USA)
Categories: cs.CV cs.AI eess.IV
\\
  Computer vision in agriculture is game-changing with its ability to transform
farming into a data-driven, precise, and sustainable industry. Deep learning
has empowered agriculture vision to analyze vast, complex visual data, but
heavily rely on the availability of large annotated datasets. This remains a
bottleneck as manual labeling is error-prone, time-consuming, and expensive.
The lack of efficient labeling approaches inspired us to consider
self-supervised learning as a paradigm shift, learning meaningful feature
representations from raw agricultural image data. In this work, we explore how
self-supervised representation learning unlocks the potential applicability to
diverse agriculture vision tasks by eliminating the need for large-scale
annotated datasets. We propose a lightweight framework utilizing SimCLR, a
contrastive learning approach, to pre-train a ResNet-50 backbone on a large,
unannotated dataset of real-world agriculture field images. Our experimental
analysis and results indicate that the model learns robust features applicable
to a broad range of downstream agriculture tasks discussed in the paper.
Additionally, the reduced reliance on annotated data makes our approach more
cost-effective and accessible, paving the way for broader adoption of computer
vision in agriculture.
\\ ( https://arxiv.org/abs/2403.15248 ,  34604kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15249 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:47:18 GMT   (12508kb,D)

Title: Spectral Motion Alignment for Video Motion Transfer using Diffusion
  Models
Authors: Geon Yeong Park, Hyeonho Jeong, Sang Wan Lee, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG
Comments: Project page:
  https://geonyeong-park.github.io/spectral-motion-alignment/
\\
  The evolution of diffusion models has greatly impacted video generation and
understanding. Particularly, text-to-video diffusion models (VDMs) have
significantly facilitated the customization of input video with target
appearance, motion, etc. Despite these advances, challenges persist in
accurately distilling motion information from video frames. While existing
works leverage the consecutive frame residual as the target motion vector, they
inherently lack global motion context and are vulnerable to frame-wise
distortions. To address this, we present Spectral Motion Alignment (SMA), a
novel framework that refines and aligns motion vectors using Fourier and
wavelet transforms. SMA learns motion patterns by incorporating
frequency-domain regularization, facilitating the learning of whole-frame
global motion dynamics, and mitigating spatial artifacts. Extensive experiments
demonstrate SMA's efficacy in improving motion transfer while maintaining
computational efficiency and compatibility across various video customization
frameworks.
\\ ( https://arxiv.org/abs/2403.15249 ,  12508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15257 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:57:27 GMT   (184kb,D)

Title: Hierarchical Information Enhancement Network for Cascade Prediction in
  Social Networks
Authors: Fanrui Zhang, Jiawei Liu, Qiang Zhang, Xiaoling Zhu, Zheng-Jun Zha
Categories: cs.SI cs.AI
Comments: 7 pages, 2 figures
\\
  Understanding information cascades in networks is a fundamental issue in
numerous applications. Current researches often sample cascade information into
several independent paths or subgraphs to learn a simple cascade
representation. However, these approaches fail to exploit the hierarchical
semantic associations between different modalities, limiting their predictive
performance. In this work, we propose a novel Hierarchical Information
Enhancement Network (HIENet) for cascade prediction. Our approach integrates
fundamental cascade sequence, user social graphs, and sub-cascade graph into a
unified framework. Specifically, HIENet utilizes DeepWalk to sample cascades
information into a series of sequences. It then gathers path information
between users to extract the social relationships of propagators. Additionally,
we employ a time-stamped graph convolutional network to aggregate sub-cascade
graph information effectively. Ultimately, we introduce a Multi-modal Cascade
Transformer to powerfully fuse these clues, providing a comprehensive
understanding of cascading process. Extensive experiments have demonstrated the
effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.15257 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15274 (*cross-listing*)
Date: Fri, 22 Mar 2024 15:16:23 GMT   (714kb)

Title: Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review
Authors: Jinge Wang, Zien Cheng, Qiuming Yao, Li Liu, Dong Xu, Gangqing Hu
Categories: q-bio.OT cs.AI
Comments: 19 pages, 3 Figures, 1 Table
\\
  The year 2023 marked a significant surge in the exploration of applying large
language model (LLM) chatbots, notably ChatGPT, across various disciplines. We
surveyed the applications of ChatGPT in various sectors of bioinformatics and
biomedical informatics throughout the year, covering omics, genetics,
biomedical text mining, drug discovery, biomedical image understanding,
bioinformatics programming, and bioinformatics education. Our survey delineates
the current strengths and limitations of this chatbot in bioinformatics and
offers insights into potential avenues for future development.
\\ ( https://arxiv.org/abs/2403.15274 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15304 (*cross-listing*)
Date: Fri, 22 Mar 2024 15:54:30 GMT   (124kb,D)

Title: KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing
Authors: Yahya Badran, Christine Preisach
Categories: cs.CY cs.AI cs.LG
Comments: preprint
\\
  Knowledge Tracing (KT) is concerned with predicting students' future
performance on learning items in intelligent tutoring systems. Learning items
are tagged with skill labels called knowledge concepts (KCs). Many KT models
expand the sequence of item-student interactions into KC-student interactions
by replacing learning items with their constituting KCs. This often results in
a longer sequence length. This approach addresses the issue of sparse
item-student interactions and minimises model parameters. However, two problems
have been identified with such models.
  The first problem is the model's ability to learn correlations between KCs
belonging to the same item, which can result in the leakage of ground truth
labels and hinder performance. This problem can lead to a significant decrease
in performance on datasets with a higher number of KCs per item. The second
problem is that the available benchmark implementations ignore accounting for
changes in sequence length when expanding KCs, leading to different models
being tested with varying sequence lengths but still compared against the same
benchmark.
  To address these problems, we introduce a general masking framework that
mitigates the first problem and enhances the performance of such KT models
while preserving the original model architecture without significant
alterations. Additionally, we introduce KTbench, an open-source benchmark
library designed to ensure the reproducibility of this work while mitigating
the second problem.
\\ ( https://arxiv.org/abs/2403.15304 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15313 (*cross-listing*)
Date: Fri, 22 Mar 2024 16:06:05 GMT   (5112kb,D)

Title: CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
Authors: Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas K\"uhne,
  Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno
Categories: cs.CV cs.AI
\\
  Accurate detection and tracking of surrounding objects is essential to enable
self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have
set the benchmark for high performance, the appeal of camera-only solutions
lies in their cost-effectiveness. Notably, despite the prevalent use of Radio
Detection and Ranging (RADAR) sensors in automotive systems, their potential in
3D detection and tracking has been largely disregarded due to data sparsity and
measurement noise. As a recent development, the combination of RADARs and
cameras is emerging as a promising solution. This paper presents Camera-RADAR
3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object
detection, and Multi-Object Tracking (MOT). Building upon the foundations of
the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates
substantial improvements in both detection and tracking capabilities, by
incorporating the spatial and velocity information of the RADAR sensor.
Experimental results demonstrate an absolute improvement in detection
performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in
Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when
leveraging both modalities. CR3DT bridges the gap between high-performance and
cost-effective perception systems in autonomous driving, by capitalizing on the
ubiquitous presence of RADAR in automotive applications.
\\ ( https://arxiv.org/abs/2403.15313 ,  5112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15317 (*cross-listing*)
Date: Fri, 22 Mar 2024 16:11:29 GMT   (3683kb,D)

Title: Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for
  Weakly Semi-supervised 3D Object Detection
Authors: Hongzhi Gao, Zheng Chen, Zehui Chen, Lin Chen, Jiaming Liu, Shanghang
  Zhang, Feng Zhao
Categories: cs.CV cs.AI
Comments: Accepted by AAAI2024
\\
  Training high-accuracy 3D detectors necessitates massive labeled 3D
annotations with 7 degree-of-freedom, which is laborious and time-consuming.
Therefore, the form of point annotations is proposed to offer significant
prospects for practical applications in 3D detection, which is not only more
accessible and less expensive but also provides strong spatial information for
object localization.In this paper, we empirically discover that it is
non-trivial to merely adapt Point-DETR to its 3D form, encountering two main
bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it
generates low-quality pseudo labels in distant regions due to the extreme
sparsity of LiDAR points. To overcome these challenges, we introduce
Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D
detection, designed to fully capitalize on point-wise supervision within a
constrained instance-wise annotation budget.Different from Point-DETR which
encodes 3D positional information solely through a point encoder, we propose an
explicit positional query initialization strategy to enhance the positional
prior. Considering the low quality of pseudo labels at distant regions produced
by the teacher model, we enhance the detector's perception by incorporating
dense imagery data through a novel Cross-Modal Deformable RoI Fusion
(D-RoI).Moreover, an innovative point-guided self-supervised learning technique
is proposed to allow for fully exploiting point priors, even in student
models.Extensive experiments on representative nuScenes dataset demonstrate our
Point-DETR3D obtains significant improvements compared to previous works.
Notably, with only 5% of labeled data, Point-DETR3D achieves over 90%
performance of its fully supervised counterpart.
\\ ( https://arxiv.org/abs/2403.15317 ,  3683kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15325 (*cross-listing*)
Date: Fri, 22 Mar 2024 16:30:58 GMT   (282kb,D)

Title: A Technological Perspective on Misuse of Available AI
Authors: Lukas P\"ohler, Valentin Schrader, Alexander Ladwein, Florian von
  Keller
Categories: cs.CY cs.AI
Comments: Presented at the UN Meeting of the Group of Governmental Experts on
  Lethal Autonomous Weapons Systems, 30 August 2018
\\
  Potential malicious misuse of civilian artificial intelligence (AI) poses
serious threats to security on a national and international level. Besides
defining autonomous systems from a technological viewpoint and explaining how
AI development is characterized, we show how already existing and openly
available AI technology could be misused. To underline this, we developed three
exemplary use cases of potentially misused AI that threaten political, digital
and physical security. The use cases can be built from existing AI technologies
and components from academia, the private sector and the developer-community.
This shows how freely available AI can be combined into autonomous weapon
systems. Based on the use cases, we deduce points of control and further
measures to prevent the potential threat through misused AI. Further, we
promote the consideration of malicious misuse of civilian AI systems in the
discussion on autonomous weapon systems (AWS).
\\ ( https://arxiv.org/abs/2403.15325 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15385 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:59:37 GMT   (23522kb,D)

Title: LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis
Authors: Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas,
  Antonio Torralba, Sanja Fidler, Xiaohui Zeng
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: See the project website at
  https://research.nvidia.com/labs/toronto-ai/LATTE3D/
MSC-class: 68T45
ACM-class: I.2.6; I.2.7; I.3.6; I.3.7
\\
  Recent text-to-3D generation approaches produce impressive 3D results but
require time-consuming optimization that can take up to an hour per prompt.
Amortized methods like ATT3D optimize multiple prompts simultaneously to
improve efficiency, enabling fast text-to-3D synthesis. However, they cannot
capture high-frequency geometry and texture details and struggle to scale to
large prompt sets, so they generalize poorly. We introduce LATTE3D, addressing
these limitations to achieve fast, high-quality generation on a significantly
larger prompt set. Key to our method is 1) building a scalable architecture and
2) leveraging 3D data during optimization through 3D-aware diffusion priors,
shape regularization, and model initialization to achieve robustness to diverse
and complex training prompts. LATTE3D amortizes both neural field and textured
surface generation to produce highly detailed textured meshes in a single
forward pass. LATTE3D generates 3D objects in 400ms, and can be further
enhanced with fast test-time optimization.
\\ ( https://arxiv.org/abs/2403.15385 ,  23522kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15388 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:59:52 GMT   (518kb,D)

Title: LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal
  Models
Authors: Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan
Categories: cs.CV cs.AI cs.CL
Comments: Project page: https://llava-prumerge.github.io/
\\
  Large Multimodal Models (LMMs) have shown significant reasoning capabilities
by connecting a visual encoder and a large language model. LMMs typically use a
fixed amount of visual tokens, such as the penultimate layer features in the
CLIP visual encoder, as the prefix content. Recent LMMs incorporate more
complex visual inputs, such as high-resolution images and videos, which
increase the number of visual tokens significantly. However, due to the design
of the Transformer architecture, computational costs associated with these
models tend to increase quadratically with the number of input tokens. To
tackle this problem, we explore a token reduction mechanism and find, similar
to prior work, that many visual tokens are spatially redundant. Based on this,
we propose PruMerge, a novel adaptive visual token reduction approach, which
largely reduces the number of visual tokens while maintaining comparable model
performance. We first select the unpruned visual tokens based on their
similarity to class tokens and spatial tokens. We then cluster the pruned
tokens based on key similarity and merge the clustered tokens with the unpruned
tokens to supplement their information. Empirically, when applied to LLaVA-1.5,
our approach can compress the visual tokens by 14.4 times on average, and
achieve comparable performance across diverse visual question-answering and
reasoning tasks. Code and checkpoints are at https://llava-prumerge.github.io/.
\\ ( https://arxiv.org/abs/2403.15388 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14651 (*cross-listing*)
Date: Fri, 23 Feb 2024 20:10:18 GMT   (1616kb,D)

Title: DOSA: A Dataset of Social Artifacts from Different Indian Geographical
  Subcultures
Authors: Agrima Seth, Sanchit Ahuja, Kalika Bali, Sunayana Sitaram
Categories: cs.CY cs.CL
\\
  Generative models are increasingly being used in various applications, such
as text generation, commonsense reasoning, and question-answering. To be
effective globally, these models must be aware of and account for local
socio-cultural contexts, making it necessary to have benchmarks to evaluate the
models for their cultural familiarity. Since the training data for LLMs is
web-based and the Web is limited in its representation of information, it does
not capture knowledge present within communities that are not on the Web. Thus,
these models exacerbate the inequities, semantic misalignment, and stereotypes
from the Web. There has been a growing call for community-centered
participatory research methods in NLP. In this work, we respond to this call by
using participatory research methods to introduce $\textit{DOSA}$, the first
community-generated $\textbf{D}$ataset $\textbf{o}$f 615 $\textbf{S}$ocial
$\textbf{A}$rtifacts, by engaging with 260 participants from 19 different
Indian geographic subcultures. We use a gamified framework that relies on
collective sensemaking to collect the names and descriptions of these artifacts
such that the descriptions semantically align with the shared sensibilities of
the individuals from those cultures. Next, we benchmark four popular LLMs and
find that they show significant variation across regional sub-cultures in their
ability to infer the artifacts.
\\ ( https://arxiv.org/abs/2403.14651 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14657 (*cross-listing*)
Date: Tue, 27 Feb 2024 04:09:30 GMT   (834kb)

Title: A Synergistic Approach to Wildfire Prevention and Management Using AI,
  ML, and 5G Technology in the United States
Authors: Stanley Chinedu Okoro, Alexander Lopez and Austine Unuriode
Categories: cs.CY cs.CL
\\
  Over the past few years, wildfires have become a worldwide environmental
emergency, resulting in substantial harm to natural habitats and playing a part
in the acceleration of climate change. Wildfire management methods involve
prevention, response, and recovery efforts. Despite improvements in detection
techniques, the rising occurrence of wildfires demands creative solutions for
prompt identification and effective control. This research investigates
proactive methods for detecting and handling wildfires in the United States,
utilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G
technology. The specific objective of this research covers proactive detection
and prevention of wildfires using advanced technology; Active monitoring and
mapping with remote sensing and signaling leveraging on 5G technology; and
Advanced response mechanisms to wildfire using drones and IOT devices. This
study was based on secondary data collected from government databases and
analyzed using descriptive statistics. In addition, past publications were
reviewed through content analysis, and narrative synthesis was used to present
the observations from various studies. The results showed that developing new
technology presents an opportunity to detect and manage wildfires proactively.
Utilizing advanced technology could save lives and prevent significant economic
losses caused by wildfires. Various methods, such as AI-enabled remote sensing
and 5G-based active monitoring, can enhance proactive wildfire detection and
management. In addition, super intelligent drones and IOT devices can be used
for safer responses to wildfires. This forms the core of the recommendation to
the fire Management Agencies and the government.
\\ ( https://arxiv.org/abs/2403.14657 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14661 (*cross-listing*)
Date: Thu, 29 Feb 2024 14:06:34 GMT   (1677kb,D)

Title: Towards Modeling Learner Performance with Large Language Models
Authors: Seyed Parsa Neshaei, Richard Lee Davis, Adam Hazimeh, Bojan
  Lazarevski, Pierre Dillenbourg and Tanja K\"aser
Categories: cs.CY cs.CL cs.LG
Comments: 12 pages, 4 figures
\\
  Recent work exploring the capabilities of pre-trained large language models
(LLMs) has demonstrated their ability to act as general pattern machines by
completing complex token sequences representing a wide array of tasks,
including time-series prediction and robot control. This paper investigates
whether the pattern recognition and sequence modeling capabilities of LLMs can
be extended to the domain of knowledge tracing, a critical component in the
development of intelligent tutoring systems (ITSs) that tailor educational
experiences by predicting learner performance over time. In an empirical
evaluation across multiple real-world datasets, we compare two approaches to
using LLMs for this task, zero-shot prompting and model fine-tuning, with
existing, non-LLM approaches to knowledge tracing. While LLM-based approaches
do not achieve state-of-the-art performance, fine-tuned LLMs surpass the
performance of naive baseline models and perform on par with standard Bayesian
Knowledge Tracing approaches across multiple metrics. These findings suggest
that the pattern recognition capabilities of LLMs can be used to model complex
learning trajectories, opening a novel avenue for applying LLMs to educational
contexts. The paper concludes with a discussion of the implications of these
findings for future research, suggesting that further refinements and a deeper
understanding of LLMs' predictive mechanisms could lead to enhanced performance
in knowledge tracing tasks.
\\ ( https://arxiv.org/abs/2403.14661 ,  1677kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14666 (*cross-listing*)
Date: Sun, 3 Mar 2024 03:01:14 GMT   (531kb,D)

Title: SyllabusQA: A Course Logistics Question Answering Dataset
Authors: Nigel Fernandez, Alexander Scarlatos, Andrew Lan
Categories: cs.CY cs.CL cs.IR cs.LG
\\
  Automated teaching assistants and chatbots have significant potential to
reduce the workload of human instructors, especially for logistics-related
question answering, which is important to students yet repetitive for
instructors. However, due to privacy concerns, there is a lack of publicly
available datasets. We introduce SyllabusQA, an open-source dataset with 63
real course syllabi covering 36 majors, containing 5,078 open-ended course
logistics-related question-answer pairs that are diverse in both question types
and answer formats. Since many logistics-related questions contain critical
information like the date of an exam, it is important to evaluate the
factuality of answers. We benchmark several strong baselines on this task, from
large language model prompting to retrieval-augmented generation. We find that
despite performing close to humans on traditional metrics of textual
similarity, there remains a significant gap between automated approaches and
humans in terms of fact precision.
\\ ( https://arxiv.org/abs/2403.14666 ,  531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14720 (*cross-listing*)
Date: Wed, 20 Mar 2024 15:26:23 GMT   (176kb,D)

Title: Defending Against Indirect Prompt Injection Attacks With Spotlighting
Authors: Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan
  Zunger, Emre Kiciman
Categories: cs.CR cs.CL cs.LG
\\
  Large Language Models (LLMs), while powerful, are built and trained to
process a single text input. In common applications, multiple inputs can be
processed by concatenating them together into a single stream of text. However,
the LLM is unable to distinguish which sections of prompt belong to various
input sources. Indirect prompt injection attacks take advantage of this
vulnerability by embedding adversarial instructions into untrusted data being
processed alongside user commands. Often, the LLM will mistake the adversarial
instructions as user commands to be followed, creating a security vulnerability
in the larger system. We introduce spotlighting, a family of prompt engineering
techniques that can be used to improve LLMs' ability to distinguish among
multiple sources of input. The key insight is to utilize transformations of an
input to provide a reliable and continuous signal of its provenance. We
evaluate spotlighting as a defense against indirect prompt injection attacks,
and find that it is a robust defense that has minimal detrimental impact to
underlying NLP tasks. Using GPT-family models, we find that spotlighting
reduces the attack success rate from greater than {50}\% to below {2}\% in our
experiments with minimal impact on task efficacy.
\\ ( https://arxiv.org/abs/2403.14720 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14725 (*cross-listing*)
Date: Wed, 20 Mar 2024 21:53:56 GMT   (2271kb,D)

Title: Jailbreaking is Best Solved by Definition
Authors: Taeyoun Kim, Suhas Kotha, Aditi Raghunathan
Categories: cs.CR cs.CL cs.LG
\\
  The rise of "jailbreak" attacks on language models has led to a flurry of
defenses aimed at preventing the output of undesirable responses. In this work,
we critically examine the two stages of the defense pipeline: (i) the
definition of what constitutes unsafe outputs, and (ii) the enforcement of the
definition via methods such as input processing or fine-tuning. We cast severe
doubt on the efficacy of existing enforcement mechanisms by showing that they
fail to defend even for a simple definition of unsafe outputs--outputs that
contain the word "purple". In contrast, post-processing outputs is perfectly
robust for such a definition. Drawing on our results, we present our position
that the real challenge in defending jailbreaks lies in obtaining a good
definition of unsafe responses: without a good definition, no enforcement
strategy can succeed, but with a good definition, output processing already
serves as a robust baseline albeit with inference-time overheads.
\\ ( https://arxiv.org/abs/2403.14725 ,  2271kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14727 (*cross-listing*)
Date: Thu, 21 Mar 2024 00:21:38 GMT   (596kb,D)

Title: Protected group bias and stereotypes in Large Language Models
Authors: Hadas Kotek, David Q. Sun, Zidi Xiu, Margit Bowler, Christopher Klein
Categories: cs.CY cs.CL cs.LG
\\
  As modern Large Language Models (LLMs) shatter many state-of-the-art
benchmarks in a variety of domains, this paper investigates their behavior in
the domains of ethics and fairness, focusing on protected group bias. We
conduct a two-part study: first, we solicit sentence continuations describing
the occupations of individuals from different protected groups, including
gender, sexuality, religion, and race. Second, we have the model generate
stories about individuals who hold different types of occupations. We collect
>10k sentence completions made by a publicly available LLM, which we subject to
human annotation. We find bias across minoritized groups, but in particular in
the domains of gender and sexuality, as well as Western bias, in model
generations. The model not only reflects societal biases, but appears to
amplify them. The model is additionally overly cautious in replies to queries
relating to minoritized groups, providing responses that strongly emphasize
diversity and equity to an extent that other group characteristics are
overshadowed. This suggests that artificially constraining potentially harmful
outputs may itself lead to harm, and should be applied in a careful and
controlled manner.
\\ ( https://arxiv.org/abs/2403.14727 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14731 (*cross-listing*)
Date: Thu, 21 Mar 2024 04:54:31 GMT   (4616kb,D)

Title: Reversible Jump Attack to Textual Classifiers with Modification
  Reduction
Authors: Mingze Ni, Zhensu Sun, Wei Liu
Categories: cs.CR cs.CL cs.LG
\\
  Recent studies on adversarial examples expose vulnerabilities of natural
language processing (NLP) models. Existing techniques for generating
adversarial examples are typically driven by deterministic hierarchical rules
that are agnostic to the optimal adversarial examples, a strategy that often
results in adversarial samples with a suboptimal balance between magnitudes of
changes and attack successes. To this end, in this research we propose two
algorithms, Reversible Jump Attack (RJA) and Metropolis-Hasting Modification
Reduction (MMR), to generate highly effective adversarial examples and to
improve the imperceptibility of the examples, respectively. RJA utilizes a
novel randomization mechanism to enlarge the search space and efficiently
adapts to a number of perturbed words for adversarial examples. With these
generated adversarial examples, MMR applies the Metropolis-Hasting sampler to
enhance the imperceptibility of adversarial examples. Extensive experiments
demonstrate that RJA-MMR outperforms current state-of-the-art methods in attack
performance, imperceptibility, fluency and grammar correctness.
\\ ( https://arxiv.org/abs/2403.14731 ,  4616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14774 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:28:43 GMT   (8643kb,D)

Title: Few-Shot Adversarial Prompt Learning on Vision-Language Models
Authors: Yiwei Zhou, Xiaobo Xia, Zhiwei Lin, Bo Han, Tongliang Liu
Categories: cs.CV cs.CL cs.CR cs.LG
Comments: 25 pages, 13 tables, 8 figures
\\
  The vulnerability of deep neural networks to imperceptible adversarial
perturbations has attracted widespread attention. Inspired by the success of
vision-language foundation models, previous efforts achieved zero-shot
adversarial robustness by aligning adversarial visual features with text
supervision. However, in practice, they are still unsatisfactory due to several
issues, including heavy adaptation cost, suboptimal text supervision, and
uncontrolled natural generalization capacity. In this paper, to address these
issues, we propose a few-shot adversarial prompt framework where adapting input
sequences with limited data makes significant adversarial robustness
improvement. Specifically, we achieve this by providing adversarially
correlated text supervision that is end-to-end learned from adversarial
examples. We also propose a novel training objective that enhances the
consistency of multi-modal features while encourages differentiated uni-modal
features between natural and adversarial examples. The proposed framework gives
access to learn adversarial text supervision, which provides superior
cross-modal adversarial alignment and matches state-of-the-art zero-shot
adversarial robustness with only 1% training data.
\\ ( https://arxiv.org/abs/2403.14774 ,  8643kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14870 (*cross-listing*)
Date: Thu, 21 Mar 2024 22:36:24 GMT   (1352kb,D)

Title: VidLA: Video-Language Alignment at Scale
Authors: Mamshad Nayeem Rizve, Fan Fei, Jayakrishnan Unnikrishnan, Son Tran,
  Benjamin Z. Yao, Belinda Zeng, Mubarak Shah, Trishul Chilimbi
Categories: cs.CV cs.CL cs.LG
Comments: Accepted to CVPR 2024
\\
  In this paper, we propose VidLA, an approach for video-language alignment at
scale. There are two major limitations of previous video-language alignment
approaches. First, they do not capture both short-range and long-range temporal
dependencies and typically employ complex hierarchical deep network
architectures that are hard to integrate with existing pretrained image-text
foundation models. To effectively address this limitation, we instead keep the
network architecture simple and use a set of data tokens that operate at
different temporal resolutions in a hierarchical manner, accounting for the
temporally hierarchical nature of videos. By employing a simple two-tower
architecture, we are able to initialize our video-language model with
pretrained image-text foundation models, thereby boosting the final
performance. Second, existing video-language alignment works struggle due to
the lack of semantically aligned large-scale training data. To overcome it, we
leverage recent LLMs to curate the largest video-language dataset to date with
better visual grounding. Furthermore, unlike existing video-text datasets which
only contain short clips, our dataset is enriched with video clips of varying
durations to aid our temporally hierarchical data tokens in extracting better
representations at varying temporal scales. Overall, empirical results show
that our proposed approach surpasses state-of-the-art methods on multiple
retrieval benchmarks, especially on longer videos, and performs competitively
on classification benchmarks.
\\ ( https://arxiv.org/abs/2403.14870 ,  1352kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15062 (*cross-listing*)
Date: Fri, 22 Mar 2024 09:40:27 GMT   (154kb,D)

Title: Construction of a Japanese Financial Benchmark for Large Language Models
Authors: Masanori Hirano
Categories: q-fin.CP cs.CL
Comments: 9 pages, Joint Workshop of the 7th Financial Technology and Natural
  Language Processing (FinNLP), the 5th Knowledge Discovery from Unstructured
  Data in Financial Services (KDF), and The 4th Workshop on Economics and
  Natural Language Processing (ECONLP) In conjunction with LREC-COLING-2024
\\
  With the recent development of large language models (LLMs), models that
focus on certain domains and languages have been discussed for their necessity.
There is also a growing need for benchmarks to evaluate the performance of
current LLMs in each domain. Therefore, in this study, we constructed a
benchmark comprising multiple tasks specific to the Japanese and financial
domains and performed benchmark measurements on some models. Consequently, we
confirmed that GPT-4 is currently outstanding, and that the constructed
benchmarks function effectively. According to our analysis, our benchmark can
differentiate benchmark scores among models in all performance ranges by
combining tasks with different difficulties.
\\ ( https://arxiv.org/abs/2403.15062 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15214 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:58:42 GMT   (1007kb,D)

Title: InstaSynth: Opportunities and Challenges in Generating Synthetic
  Instagram Data with ChatGPT for Sponsored Content Detection
Authors: Thales Bertaglia, Lily Heisig, Rishabh Kaushal, Adriana Iamnitchi
Categories: cs.CY cs.CL cs.SI
Comments: To appear at the 18th International AAAI Conference on Web and Social
  Media (ICWSM 2024) -- please cite accordingly
\\
  Large Language Models (LLMs) raise concerns about lowering the cost of
generating texts that could be used for unethical or illegal purposes,
especially on social media. This paper investigates the promise of such models
to help enforce legal requirements related to the disclosure of sponsored
content online. We investigate the use of LLMs for generating synthetic
Instagram captions with two objectives: The first objective (fidelity) is to
produce realistic synthetic datasets. For this, we implement content-level and
network-level metrics to assess whether synthetic captions are realistic. The
second objective (utility) is to create synthetic data that is useful for
sponsored content detection. For this, we evaluate the effectiveness of the
generated synthetic data for training classifiers to identify undisclosed
advertisements on Instagram. Our investigations show that the objectives of
fidelity and utility may conflict and that prompt engineering is a useful but
insufficient strategy. Additionally, we find that while individual synthetic
posts may appear realistic, collectively they lack diversity, topic
connectivity, and realistic user interaction patterns.
\\ ( https://arxiv.org/abs/2403.15214 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15226 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:20:34 GMT   (2059kb,D)

Title: Not All Attention is Needed: Parameter and Computation Efficient
  Transfer Learning for Multi-modal Large Language Models
Authors: Qiong Wu and Weihao Ye and Yiyi Zhou and Xiaoshuai Sun and Rongrong Ji
Categories: cs.MM cs.CL
\\
  In this paper, we propose a novel parameter and computation efficient tuning
method for Multi-modal Large Language Models (MLLMs), termed Efficient
Attention Skipping (EAS). Concretely, we first reveal that multi-head
attentions (MHAs), the main computational overhead of MLLMs, are often
redundant to downstream tasks. Based on this observation, EAS evaluates the
attention redundancy and skips the less important MHAs to speed up inference.
Besides, we also propose a novel propagation-of-information adapter (PIA) to
serve the attention skipping of EAS and keep parameter efficiency, which can be
further re-parameterized into feed-forward networks (FFNs) for zero-extra
latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN
and a classic VL pre-trained model called METER, and conduct extensive
experiments on a set of benchmarks. The experiments show that EAS not only
retains high performance and parameter efficiency, but also greatly speeds up
inference speed. For instance, LaVIN-EAS can obtain 89.98\% accuracy on
ScineceQA while speeding up inference by 2.2 times to LaVIN
\\ ( https://arxiv.org/abs/2403.15226 ,  2059kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15246 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:42:29 GMT   (168kb,D)

Title: FollowIR: Evaluating and Teaching Information Retrieval Models to Follow
  Instructions
Authors: Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan,
  Benjamin Van Durme, Dawn Lawrie, Luca Soldaini
Categories: cs.IR cs.CL cs.LG
\\
  Modern Large Language Models (LLMs) are capable of following long and complex
instructions that enable a diverse amount of user tasks. However, despite
Information Retrieval (IR) models using LLMs as the backbone of their
architectures, nearly all of them still only take queries as input, with no
instructions. For the handful of recent models that do take instructions, it's
unclear how they use them. We introduce our dataset FollowIR, which contains a
rigorous instruction evaluation benchmark as well as a training set for helping
IR models learn to better follow real-world instructions. FollowIR builds off
the long history of the TREC conferences: as TREC provides human annotators
with instructions (also known as narratives) to determine document relevance,
so should IR models be able to understand and decide relevance based on these
detailed instructions. Our evaluation benchmark starts with three deeply judged
TREC collections and alters the annotator instructions, re-annotating relevant
documents. Through this process, we can measure how well IR models follow
instructions, through a new pairwise evaluation framework. Our results indicate
that existing retrieval models fail to correctly use instructions, using them
for basic keywords and struggling to understand long-form information. However,
we show that it is possible for IR models to learn to follow complex
instructions: our new FollowIR-7B model has significant improvements (over 13%)
after fine-tuning on our training set.
\\ ( https://arxiv.org/abs/2403.15246 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15309 (*cross-listing*)
Date: Fri, 22 Mar 2024 15:59:24 GMT   (46037kb,D)

Title: Controlled Training Data Generation with Diffusion Models
Authors: Teresa Yeo, Andrei Atanov, Harold Benoit, Aleksandr Alekseev, Ruchira
  Ray, Pooya Esmaeil Akhoondi, Amir Zamir
Categories: cs.CV cs.CL cs.LG
Comments: Project page at https://adversarial-prompts.epfl.ch/
\\
  In this work, we present a method to control a text-to-image generative model
to produce training data specifically "useful" for supervised learning. Unlike
previous works that employ an open-loop approach and pre-define prompts to
generate new data using either a language model or human expertise, we develop
an automated closed-loop system which involves two feedback mechanisms. The
first mechanism uses feedback from a given supervised model and finds
adversarial prompts that result in image generations that maximize the model
loss. While these adversarial prompts result in diverse data informed by the
model, they are not informed of the target distribution, which can be
inefficient. Therefore, we introduce the second feedback mechanism that guides
the generation process towards a certain target distribution. We call the
method combining these two mechanisms Guided Adversarial Prompts. We perform
our evaluations on different tasks, datasets and architectures, with different
types of distribution shifts (spuriously correlated data, unseen domains) and
demonstrate the efficiency of the proposed feedback mechanisms compared to
open-loop approaches.
\\ ( https://arxiv.org/abs/2403.15309 ,  46037kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15365 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:33:11 GMT   (9568kb,D)

Title: A Transfer Attack to Image Watermarks
Authors: Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
Categories: cs.CR cs.CL cs.LG
\\
  Watermark has been widely deployed by industry to detect AI-generated images.
The robustness of such watermark-based detector against evasion attacks in the
white-box and black-box settings is well understood in the literature. However,
the robustness in the no-box setting is much less understood. In particular,
multiple studies claimed that image watermark is robust in such setting. In
this work, we propose a new transfer evasion attack to image watermark in the
no-box setting. Our transfer attack adds a perturbation to a watermarked image
to evade multiple surrogate watermarking models trained by the attacker itself,
and the perturbed watermarked image also evades the target watermarking model.
Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks
even if the attacker does not have access to the watermarking model nor the
detection API.
\\ ( https://arxiv.org/abs/2403.15365 ,  9568kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14638 (*cross-listing*)
Date: Tue, 20 Feb 2024 10:38:38 GMT   (1063kb,D)

Title: Personalized Programming Guidance based on Deep Programming Learning
  Style Capturing
Authors: Yingfan Liu, Renyu Zhu, Ming Gao
Categories: cs.CY cs.LG
Comments: 18th International Conference on Computer Science & Education
\\
  With the rapid development of big data and AI technology, programming is in
high demand and has become an essential skill for students. Meanwhile,
researchers also focus on boosting the online judging system's guidance ability
to reduce students' dropout rates. Previous studies mainly targeted at
enhancing learner engagement on online platforms by providing personalized
recommendations. However, two significant challenges still need to be addressed
in programming: C1) how to recognize complex programming behaviors; C2) how to
capture intrinsic learning patterns that align with the actual learning
process. To fill these gaps, in this paper, we propose a novel model called
Programming Exercise Recommender with Learning Style (PERS), which simulates
learners' intricate programming behaviors. Specifically, since programming is
an iterative and trial-and-error process, we first introduce a positional
encoding and a differentiating module to capture the changes of consecutive
code submissions (which addresses C1). To better profile programming behaviors,
we extend the Felder-Silverman learning style model, a classical pedagogical
theory, to perceive intrinsic programming patterns. Based on this, we align
three latent vectors to record and update programming ability, processing
style, and understanding style, respectively (which addresses C2). We perform
extensive experiments on two real-world datasets to verify the rationality of
modeling programming learning styles and the effectiveness of PERS for
personalized programming guidance.
\\ ( https://arxiv.org/abs/2403.14638 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14663 (*cross-listing*)
Date: Fri, 1 Mar 2024 13:18:08 GMT   (984kb,D)

Title: Machine Learning Predicts Upper Secondary Education Dropout as Early as
  the End of Primary School
Authors: Maria Psyridou, Fabi Prezja, Minna Torppa, Marja-Kristiina Lerkkanen,
  Anna-Maija Poikkeus, Kati Vasalampi
Categories: cs.CY cs.LG
\\
  Education plays a pivotal role in alleviating poverty, driving economic
growth, and empowering individuals, thereby significantly influencing societal
and personal development. However, the persistent issue of school dropout poses
a significant challenge, with its effects extending beyond the individual.
While previous research has employed machine learning for dropout
classification, these studies often suffer from a short-term focus, relying on
data collected only a few years into the study period. This study expanded the
modeling horizon by utilizing a 13-year longitudinal dataset, encompassing data
from kindergarten to Grade 9. Our methodology incorporated a comprehensive
range of parameters, including students' academic and cognitive skills,
motivation, behavior, well-being, and officially recorded dropout data. The
machine learning models developed in this study demonstrated notable
classification ability, achieving a mean area under the curve (AUC) of 0.61
with data up to Grade 6 and an improved AUC of 0.65 with data up to Grade 9.
Further data collection and independent correlational and causal analyses are
crucial. In future iterations, such models may have the potential to
proactively support educators' processes and existing protocols for identifying
at-risk students, thereby potentially aiding in the reinvention of student
retention and success strategies and ultimately contributing to improved
educational outcomes.
\\ ( https://arxiv.org/abs/2403.14663 ,  984kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14664 (*cross-listing*)
Date: Fri, 1 Mar 2024 23:39:03 GMT   (1669kb,D)

Title: ClickTree: A Tree-based Method for Predicting Math Students' Performance
  Based on Clickstream Data
Authors: Narjes Rohani, Behnam Rohani, Areti Manataki
Categories: cs.CY cs.HC cs.LG stat.AP
Comments: Submitted to the Journal of Educational Data Mining
\\
  The prediction of student performance and the analysis of students' learning
behavior play an important role in enhancing online courses. By analysing a
massive amount of clickstream data that captures student behavior, educators
can gain valuable insights into the factors that influence academic outcomes
and identify areas of improvement in courses. In this study, we developed
ClickTree, a tree-based methodology, to predict student performance in
mathematical assignments based on students' clickstream data. We extracted a
set of features, including problem-level, assignment-level and student-level
features, from the extensive clickstream data and trained a CatBoost tree to
predict whether a student successfully answers a problem in an assignment. The
developed method achieved an AUC of 0.78844 in the Educational Data Mining Cup
2023 and ranked second in the competition. Furthermore, our results indicate
that students encounter more difficulties in the problem types that they must
select a subset of answers from a given set as well as problem subjects of
Algebra II. Additionally, students who performed well in answering end-unit
assignment problems engaged more with in-unit assignments and answered more
problems correctly, while those who struggled had higher tutoring request rate.
The proposed method can be utilized to improve students' learning experiences,
and the above insights can be integrated into mathematical courses to enhance
students' learning outcomes.
\\ ( https://arxiv.org/abs/2403.14664 ,  1669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14671 (*cross-listing*)
Date: Tue, 5 Mar 2024 08:50:21 GMT   (521kb)

Title: Understanding the Transit Gap: A Comparative Study of On-Demand Bus
  Services and Urban Climate Resilience in South End, Charlotte, NC and
  Avondale, Chattanooga, TN
Authors: Sanaz Sadat Hosseini, Babak Rahimi Ardabili, Mona Azarbayjani,
  Srinivas Pulugurtha, Hamed Tabkhi
Categories: cs.CY cs.LG physics.soc-ph
Comments: 6 pages, 4 figures, Recently accepted by the PLEA 2024 Conference for
  Sustainable Architecture and Urban Design (Re-Thinking Resilience), Wroclaw,
  Poland
\\
  Urban design significantly impacts sustainability, particularly in the
context of public transit efficiency and carbon emissions reduction. This study
explores two neighborhoods with distinct urban designs: South End, Charlotte,
NC, featuring a dynamic mixed-use urban design pattern, and Avondale,
Chattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM
tool, we assess the impact of increased bus utilization in these different
urban settings on traffic and CO2 emissions. Our results highlight the critical
role of urban design and planning in transit system efficiency. In South End,
the mixed-use design led to more substantial emission reductions, indicating
that urban layout can significantly influence public transit outcomes. Tailored
strategies that consider the unique urban design elements are essential for
climate resilience. Notably, doubling bus utilization decreased daily emissions
by 10.18% in South End and 8.13% in Avondale, with a corresponding reduction in
overall traffic. A target of 50% bus utilization saw emissions drop by 21.45%
in South End and 14.50% in Avondale. At an idealistic goal of 70% bus
utilization, South End and Avondale witnessed emission reductions of 37.22% and
27.80%, respectively. These insights are crucial for urban designers and
policymakers in developing sustainable urban landscapes.
\\ ( https://arxiv.org/abs/2403.14671 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14684 (*cross-listing*)
Date: Wed, 13 Mar 2024 13:51:12 GMT   (432kb,D)

Title: FOCIL: Finetune-and-Freeze for Online Class Incremental Learning by
  Training Randomly Pruned Sparse Experts
Authors: Murat Onur Yildirim, Elif Ceren Gok Yildirim, Decebal Constantin
  Mocanu, Joaquin Vanschoren
Categories: cs.CV cs.LG
\\
  Class incremental learning (CIL) in an online continual learning setting
strives to acquire knowledge on a series of novel classes from a data stream,
using each data point only once for training. This is more realistic compared
to offline modes, where it is assumed that all data from novel class(es) is
readily available. Current online CIL approaches store a subset of the previous
data which creates heavy overhead costs in terms of both memory and
computation, as well as privacy issues. In this paper, we propose a new online
CIL approach called FOCIL. It fine-tunes the main architecture continually by
training a randomly pruned sparse subnetwork for each task. Then, it freezes
the trained connections to prevent forgetting. FOCIL also determines the
sparsity level and learning rate per task adaptively and ensures (almost) zero
forgetting across all tasks without storing any replay data. Experimental
results on 10-Task CIFAR100, 20-Task CIFAR100, and 100-Task TinyImagenet,
demonstrate that our method outperforms the SOTA by a large margin. The code is
publicly available at https://github.com/muratonuryildirim/FOCIL.
\\ ( https://arxiv.org/abs/2403.14684 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14695 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:05:59 GMT   (791kb,D)

Title: Chain-structured neural architecture search for financial time series
  forecasting
Authors: Denis Levchenko, Efstratios Rappos, Shabnam Ataee, Biagio Nigro,
  Stephan Robert
Categories: q-fin.ST cs.LG
Comments: 17 pages, 3 figures
\\
  We compare three popular neural architecture search strategies on
chain-structured search spaces: Bayesian optimization, the hyperband method,
and reinforcement learning in the context of financial time series forecasting.
\\ ( https://arxiv.org/abs/2403.14695 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14709 (*cross-listing*)
Date: Mon, 18 Mar 2024 08:16:02 GMT   (349kb,D)

Title: ClimateQ&A: Bridging the gap between climate scientists and the general
  public
Authors: Natalia De La Calzada, Th\'eo Alves Da Costa, Annabelle Blangero,
  Nicolas Chesneau
Categories: cs.CY cs.LG
Comments: Accepted as a workshop paper at "Tackling Climate Change with Machine
  Learning", ICLR 2024
\\
  This research paper investigates public views on climate change and
biodiversity loss by analyzing questions asked to the ClimateQ&A platform.
ClimateQ&A is a conversational agent that uses LLMs to respond to queries based
on over 14,000 pages of scientific literature from the IPCC and IPBES reports.
Launched online in March 2023, the tool has gathered over 30,000 questions,
mainly from a French audience. Its chatbot interface allows for the free
formulation of questions related to nature*. While its main goal is to make
nature science more accessible, it also allows for the collection and analysis
of questions and their themes. Unlike traditional surveys involving closed
questions, this novel method offers a fresh perspective on individual
interrogations about nature. Running NLP clustering algorithms on a sample of
3,425 questions, we find that a significant 25.8% inquire about how climate
change and biodiversity loss will affect them personally (e.g., where they live
or vacation, their consumption habits) and the specific impacts of their
actions on nature (e.g., transportation or food choices). This suggests that
traditional methods of surveying may not identify all existing knowledge gaps,
and that relying solely on IPCC and IPBES reports may not address all
individual inquiries about climate and biodiversity, potentially affecting
public understanding and action on these issues. *we use 'nature' as an
umbrella term for 'climate change' and 'biodiversity loss'
\\ ( https://arxiv.org/abs/2403.14709 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14713 (*cross-listing*)
Date: Mon, 18 Mar 2024 21:09:06 GMT   (176kb,D)

Title: Auditing Fairness under Unobserved Confounding
Authors: Yewon Byun, Dylan Sam, Michael Oberst, Zachary C. Lipton, Bryan Wilder
Categories: cs.CY cs.LG stat.ML
Comments: AISTATS 2024
\\
  A fundamental problem in decision-making systems is the presence of inequity
across demographic lines. However, inequity can be difficult to quantify,
particularly if our notion of equity relies on hard-to-measure notions like
risk (e.g., equal access to treatment for those who would die without it).
Auditing such inequity requires accurate measurements of individual risk, which
is difficult to estimate in the realistic setting of unobserved confounding. In
the case that these unobservables "explain" an apparent disparity, we may
understate or overstate inequity. In this paper, we show that one can still
give informative bounds on allocation rates among high-risk individuals, even
while relaxing or (surprisingly) even when eliminating the assumption that all
relevant risk factors are observed. We utilize the fact that in many real-world
settings (e.g., the introduction of a novel treatment) we have data from a
period prior to any allocation, to derive unbiased estimates of risk. We
demonstrate the effectiveness of our framework on a real-world study of
Paxlovid allocation to COVID-19 patients, finding that observed racial inequity
cannot be explained by unobserved confounders of the same strength as important
observed covariates.
\\ ( https://arxiv.org/abs/2403.14713 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14714 (*cross-listing*)
Date: Mon, 18 Mar 2024 23:25:13 GMT   (2314kb,D)

Title: Compiler generated feedback for Large Language Models
Authors: Dejan Grubisic, Chris Cummins, Volker Seeker, Hugh Leather
Categories: cs.PL cs.LG
\\
  We introduce a novel paradigm in compiler optimization powered by Large
Language Models with compiler feedback to optimize the code size of LLVM
assembly. The model takes unoptimized LLVM IR as input and produces optimized
IR, the best optimization passes, and instruction counts of both unoptimized
and optimized IRs. Then we compile the input with generated optimization passes
and evaluate if the predicted instruction count is correct, generated IR is
compilable, and corresponds to compiled code. We provide this feedback back to
LLM and give it another chance to optimize code. This approach adds an extra
0.53% improvement over -Oz to the original model. Even though, adding more
information with feedback seems intuitive, simple sampling techniques achieve
much higher performance given 10 or more samples.
\\ ( https://arxiv.org/abs/2403.14714 ,  2314kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14719 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:54:39 GMT   (11775kb,D)

Title: Bypassing LLM Watermarks with Color-Aware Substitutions
Authors: Qilong Wu, Varun Chandrasekaran
Categories: cs.CR cs.CV cs.LG
\\
  Watermarking approaches are proposed to identify if text being circulated is
human or large language model (LLM) generated. The state-of-the-art
watermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate
specific (``green'') tokens. However, determining the robustness of this
watermarking method is an open problem. Existing attack methods fail to evade
detection for longer text segments. We overcome this limitation, and propose
{\em Self Color Testing-based Substitution (SCTS)}, the first ``color-aware''
attack. SCTS obtains color information by strategically prompting the
watermarked LLM and comparing output tokens frequencies. It uses this
information to determine token colors, and substitutes green tokens with
non-green ones. In our experiments, SCTS successfully evades watermark
detection using fewer number of edits than related work. Additionally, we show
both theoretically and empirically that SCTS can remove the watermark for
arbitrarily long watermarked text.
\\ ( https://arxiv.org/abs/2403.14719 ,  11775kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14724 (*cross-listing*)
Date: Wed, 20 Mar 2024 20:41:26 GMT   (369kb,D)

Title: Six Levels of Privacy: A Framework for Financial Synthetic Data
Authors: Tucker Balch, Vamsi K. Potluru, Deepak Paramanand, Manuela Veloso
Categories: cs.CR cs.LG q-fin.ST
Comments: Six privacy levels framework; excerpted from "Synthetic Data
  Applications in Finance'' (arxiv:2401.00081) article
\\
  Synthetic Data is increasingly important in financial applications. In
addition to the benefits it provides, such as improved financial modeling and
better testing procedures, it poses privacy risks as well. Such data may arise
from client information, business information, or other proprietary sources
that must be protected. Even though the process by which Synthetic Data is
generated serves to obscure the original data to some degree, the extent to
which privacy is preserved is hard to assess. Accordingly, we introduce a
hierarchy of ``levels'' of privacy that are useful for categorizing Synthetic
Data generation methods and the progressively improved protections they offer.
While the six levels were devised in the context of financial applications,
they may also be appropriate for other industries as well. Our paper includes:
A brief overview of Financial Synthetic Data, how it can be used, how its value
can be assessed, privacy risks, and privacy attacks. We close with details of
the ``Six Levels'' that include defenses against those attacks.
\\ ( https://arxiv.org/abs/2403.14724 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14729 (*cross-listing*)
Date: Thu, 21 Mar 2024 02:33:37 GMT   (1734kb,D)

Title: Auto-Train-Once: Controller Network Guided Automatic Network Pruning
  from Scratch
Authors: Xidong Wu, Shangqian Gao, Zeyu Zhang, Zhenzhen Li, Runxue Bao, Yanfu
  Zhang, Xiaoqian Wang, Heng Huang
Categories: cs.CV cs.LG
\\
  Current techniques for deep neural network (DNN) pruning often involve
intricate multi-step processes that require domain-specific expertise, making
their widespread adoption challenging. To address the limitation, the
Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for
additional fine-tuning steps by directly training and compressing a general DNN
from scratch. Nevertheless, the static design of optimizers (in OTO) can lead
to convergence issues of local optima. In this paper, we proposed the
Auto-Train-Once (ATO), an innovative network pruning algorithm designed to
automatically reduce the computational and storage costs of DNNs. During the
model training phase, our approach not only trains the target model but also
leverages a controller network as an architecture generator to guide the
learning of target model weights. Furthermore, we developed a novel stochastic
gradient algorithm that enhances the coordination between model training and
controller network training, thereby improving pruning performance. We provide
a comprehensive convergence analysis as well as extensive experiments, and the
results show that our approach achieves state-of-the-art performance across
various model architectures (including ResNet18, ResNet34, ResNet50, ResNet56,
and MobileNetv2) on standard benchmark datasets (CIFAR-10, CIFAR-100, and
ImageNet).
\\ ( https://arxiv.org/abs/2403.14729 ,  1734kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14742 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:00:00 GMT   (2091kb,D)

Title: A Classifier-Based Approach to Multi-Class Anomaly Detection for
  Astronomical Transients
Authors: Rithwik Gupta, Daniel Muthukrishna, and Michelle Lochner
Categories: astro-ph.IM astro-ph.HE cs.LG
Comments: 16 pages, 14 figures, 1 table, submitted to MNRAS
\\
  Automating real-time anomaly detection is essential for identifying rare
transients in the era of large-scale astronomical surveys. Modern survey
telescopes are generating tens of thousands of alerts per night, and future
telescopes, such as the Vera C. Rubin Observatory, are projected to increase
this number dramatically. Currently, most anomaly detection algorithms for
astronomical transients rely either on hand-crafted features extracted from
light curves or on features generated through unsupervised representation
learning, which are then coupled with standard machine learning anomaly
detection algorithms. In this work, we introduce an alternative approach to
detecting anomalies: using the penultimate layer of a neural network classifier
as the latent space for anomaly detection. We then propose a novel method,
named Multi-Class Isolation Forests (MCIF), which trains separate isolation
forests for each class to derive an anomaly score for a light curve from the
latent space representation given by the classifier. This approach
significantly outperforms a standard isolation forest. We also use a simpler
input method for real-time transient classifiers which circumvents the need for
interpolation in light curves and helps the neural network model inter-passband
relationships and handle irregular sampling. Our anomaly detection pipeline
identifies rare classes including kilonovae, pair-instability supernovae, and
intermediate luminosity transients shortly after trigger on simulated Zwicky
Transient Facility light curves. Using a sample of our simulations that matched
the population of anomalies expected in nature (54 anomalies and 12,040 common
transients), our method was able to discover $41\pm3$ anomalies (~75% recall)
after following up the top 2000 (~15%) ranked transients. Our novel method
shows that classifiers can be effectively repurposed for real-time anomaly
detection.
\\ ( https://arxiv.org/abs/2403.14742 ,  2091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14753 (*cross-listing*)
Date: Thu, 21 Mar 2024 18:00:04 GMT   (404kb,D)

Title: Learning with SASQuaTCh: a Novel Variational Quantum Transformer
  Architecture with Kernel-Based Self-Attention
Authors: Ethan N. Evans, Matthew Cook, Zachary P. Bradshaw, and Margarite L.
  LaBorde
Categories: quant-ph cs.LG
Comments: 11 pages, 4 figures
\\
  The widely popular transformer network popularized by the generative
pre-trained transformer (GPT) has a large field of applicability, including
predicting text and images, classification, and even predicting solutions to
the dynamics of physical systems. In the latter context, the continuous analog
of the self-attention mechanism at the heart of transformer networks has been
applied to learning the solutions of partial differential equations and reveals
a convolution kernel nature that can be exploited by the Fourier transform. It
is well known that many quantum algorithms that have provably demonstrated a
speedup over classical algorithms utilize the quantum Fourier transform. In
this work, we explore quantum circuits that can efficiently express a
self-attention mechanism through the perspective of kernel-based operator
learning. In this perspective, we are able to represent deep layers of a vision
transformer network using simple gate operations and a set of multi-dimensional
quantum Fourier transforms. We analyze the computational and parameter
complexity of our novel variational quantum circuit, which we call
Self-Attention Sequential Quantum Transformer Channel (SASQuaTCh), and
demonstrate its utility on simplified classification problems.
\\ ( https://arxiv.org/abs/2403.14753 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14797 (*cross-listing*)
Date: Thu, 21 Mar 2024 19:20:29 GMT   (5440kb,D)

Title: Preventing Catastrophic Forgetting through Memory Networks in Continuous
  Detection
Authors: Gaurav Bhatt, James Ross, Leonid Sigal
Categories: cs.CV cs.LG
\\
  Modern pre-trained architectures struggle to retain previous information
while undergoing continuous fine-tuning on new tasks. Despite notable progress
in continual classification, systems designed for complex vision tasks such as
detection or segmentation still struggle to attain satisfactory performance. In
this work, we introduce a memory-based detection transformer architecture to
adapt a pre-trained DETR-style detector to new tasks while preserving knowledge
from previous tasks. We propose a novel localized query function for efficient
information retrieval from memory units, aiming to minimize forgetting.
Furthermore, we identify a fundamental challenge in continual detection
referred to as background relegation. This arises when object categories from
earlier tasks reappear in future tasks, potentially without labels, leading
them to be implicitly treated as background. This is an inevitable issue in
continual detection or segmentation. The introduced continual optimization
technique effectively tackles this challenge. Finally, we assess the
performance of our proposed system on continual detection benchmarks and
demonstrate that our approach surpasses the performance of existing
state-of-the-art resulting in 5-7% improvements on MS-COCO and PASCAL-VOC on
the task of continual detection.
\\ ( https://arxiv.org/abs/2403.14797 ,  5440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14813 (*cross-listing*)
Date: Thu, 21 Mar 2024 19:59:07 GMT   (35159kb,D)

Title: Curvature Augmented Manifold Embedding and Learning
Authors: Yongming Liu
Categories: stat.ML cs.HC cs.LG
\\
  A new dimensional reduction (DR) and data visualization method,
Curvature-Augmented Manifold Embedding and Learning (CAMEL), is proposed. The
key novel contribution is to formulate the DR problem as a mechanistic/physics
model, where the force field among nodes (data points) is used to find an
n-dimensional manifold representation of the data sets. Compared with many
existing attractive-repulsive force-based methods, one unique contribution of
the proposed method is to include a non-pairwise force. A new force field model
is introduced and discussed, inspired by the multi-body potential in
lattice-particle physics and Riemann curvature in topology. A
curvature-augmented force is included in CAMEL. Following this, CAMEL
formulation for unsupervised learning, supervised learning, semi-supervised
learning/metric learning, and inverse learning are provided. Next, CAMEL is
applied to many benchmark datasets by comparing existing models, such as tSNE,
UMAP, TRIMAP, and PacMap. Both visual comparison and metrics-based evaluation
are performed. 14 open literature and self-proposed metrics are employed for a
comprehensive comparison. Conclusions and future work are suggested based on
the current investigation. Related code and demonstration are available on
https://github.com/ymlasu/CAMEL for interested readers to reproduce the results
and other applications.
\\ ( https://arxiv.org/abs/2403.14813 ,  35159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14822 (*cross-listing*)
Date: Thu, 21 Mar 2024 20:29:43 GMT   (144kb,D)

Title: Non-Convex Robust Hypothesis Testing using Sinkhorn Uncertainty Sets
Authors: Jie Wang and Rui Gao and Yao Xie
Categories: stat.ML cs.LG math.OC
Comments: 26 pages, 2 figures
\\
  We present a new framework to address the non-convex robust hypothesis
testing problem, wherein the goal is to seek the optimal detector that
minimizes the maximum of worst-case type-I and type-II risk functions. The
distributional uncertainty sets are constructed to center around the empirical
distribution derived from samples based on Sinkhorn discrepancy. Given that the
objective involves non-convex, non-smooth probabilistic functions that are
often intractable to optimize, existing methods resort to approximations rather
than exact solutions. To tackle the challenge, we introduce an exact
mixed-integer exponential conic reformulation of the problem, which can be
solved into a global optimum with a moderate amount of input data.
Subsequently, we propose a convex approximation, demonstrating its superiority
over current state-of-the-art methodologies in literature. Furthermore, we
establish connections between robust hypothesis testing and regularized
formulations of non-robust risk functions, offering insightful interpretations.
Our numerical study highlights the satisfactory testing performance and
computational efficiency of the proposed framework.
\\ ( https://arxiv.org/abs/2403.14822 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14830 (*cross-listing*)
Date: Thu, 21 Mar 2024 20:43:44 GMT   (33320kb,D)

Title: Deep Clustering Evaluation: How to Validate Internal Clustering
  Validation Measures
Authors: Zeya Wang and Chenglong Ye
Categories: stat.ML cs.LG
\\
  Deep clustering, a method for partitioning complex, high-dimensional data
using deep neural networks, presents unique evaluation challenges. Traditional
clustering validation measures, designed for low-dimensional spaces, are
problematic for deep clustering, which involves projecting data into
lower-dimensional embeddings before partitioning. Two key issues are
identified: 1) the curse of dimensionality when applying these measures to raw
data, and 2) the unreliable comparison of clustering results across different
embedding spaces stemming from variations in training procedures and parameter
settings in different clustering models. This paper addresses these challenges
in evaluating clustering quality in deep learning. We present a theoretical
framework to highlight ineffectiveness arising from using internal validation
measures on raw and embedded data and propose a systematic approach to applying
clustering validity indices in deep clustering contexts. Experiments show that
this framework aligns better with external validation measures, effectively
reducing the misguidance from the improper use of clustering validity indices
in deep learning.
\\ ( https://arxiv.org/abs/2403.14830 ,  33320kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14848 (*cross-listing*)
Date: Thu, 21 Mar 2024 21:39:05 GMT   (1528kb,D)

Title: Learning WENO for entropy stable schemes to solve conservation laws
Authors: Philip Charles, Deep Ray
Categories: math.NA cs.LG cs.NA
Comments: 38 pages, 16 figures, 5 tables
MSC-class: 65M06, 65M12, 35L65, 68T07
\\
  Entropy conditions play a crucial role in the extraction of a physically
relevant solution for a system of conservation laws, thus motivating the
construction of entropy stable schemes that satisfy a discrete analogue of such
conditions. TeCNO schemes (Fjordholm et al. 2012) form a class of arbitrary
high-order entropy stable finite difference solvers, which require specialized
reconstruction algorithms satisfying the sign property at each cell interface.
Recently, third-order WENO schemes called SP-WENO (Fjordholm and Ray, 2016) and
SP-WENOc (Ray, 2018) have been designed to satisfy the sign property. However,
these WENO algorithms can perform poorly near shocks, with the numerical
solutions exhibiting large spurious oscillations. In the present work, we
propose a variant of the SP-WENO, termed as Deep Sign-Preserving WENO
(DSP-WENO), where a neural network is trained to learn the WENO weighting
strategy. The sign property and third-order accuracy are strongly imposed in
the algorithm, which constrains the WENO weight selection region to a convex
polygon. Thereafter, a neural network is trained to select the WENO weights
from this convex region with the goal of improving the shock-capturing
capabilities without sacrificing the rate of convergence in smooth regions. The
proposed synergistic approach retains the mathematical framework of the TeCNO
scheme while integrating deep learning to remedy the computational issues of
the WENO-based reconstruction. We present several numerical experiments to
demonstrate the significant improvement with DSP-WENO over the existing
variants of WENO satisfying the sign property.
\\ ( https://arxiv.org/abs/2403.14848 ,  1528kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14849 (*cross-listing*)
Date: Thu, 21 Mar 2024 21:51:36 GMT   (880kb,D)

Title: Output-Constrained Lossy Source Coding With Application to
  Rate-Distortion-Perception Theory
Authors: Li Xie, Liangyan Li, Jun Chen, Zhongshan Zhang
Categories: cs.IT cs.LG math.IT
\\
  The distortion-rate function of output-constrained lossy source coding with
limited common randomness is analyzed for the special case of squared error
distortion measure. An explicit expression is obtained when both source and
reconstruction distributions are Gaussian. This further leads to a partial
characterization of the information-theoretic limit of quadratic Gaussian
rate-distortion-perception coding with the perception measure given by
Kullback-Leibler divergence or squared quadratic Wasserstein distance.
\\ ( https://arxiv.org/abs/2403.14849 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14860 (*cross-listing*)
Date: Thu, 21 Mar 2024 22:15:09 GMT   (445kb,D)

Title: Robust Model Based Reinforcement Learning Using $\mathcal{L}_1$ Adaptive
  Control
Authors: Minjun Sung, Sambhu H. Karumanchi, Aditya Gahlawat, Naira Hovakimyan
Categories: eess.SY cs.LG cs.SY
\\
  We introduce $\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme
for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free
approaches, MBRL algorithms learn a model of the transition function using data
and use it to design a control input. Our approach generates a series of
approximate control-affine models of the learned transition function according
to the proposed switching law. Using the approximate model, control input
produced by the underlying MBRL is perturbed by the $\mathcal{L}_1$ adaptive
control, which is designed to enhance the robustness of the system against
uncertainties. Importantly, this approach is agnostic to the choice of MBRL
algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL
algorithms with $\mathcal{L}_1$ augmentation exhibit enhanced performance and
sample efficiency across multiple MuJoCo environments, outperforming the
original MBRL algorithms, both with and without system noise.
\\ ( https://arxiv.org/abs/2403.14860 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14863 (*cross-listing*)
Date: Thu, 21 Mar 2024 22:18:25 GMT   (6874kb,D)

Title: Distribution-informed and wavelength-flexible data-driven photoacoustic
  oximetry
Authors: Janek Gr\"ohl, Kylie Yeung, Kevin Gu, Thomas R. Else, Monika Golinska,
  Ellie V. Bunce, Lina Hacker, Sarah E. Bohndiek
Categories: physics.med-ph cs.CV cs.LG
Comments: 37 pages, 7 figures
ACM-class: F.2.1
\\
  Significance: Photoacoustic imaging (PAI) promises to measure
spatially-resolved blood oxygen saturation, but suffers from a lack of accurate
and robust spectral unmixing methods to deliver on this promise. Accurate blood
oxygenation estimation could have important clinical applications, from cancer
detection to quantifying inflammation.
  Aim: This study addresses the inflexibility of existing data-driven methods
for estimating blood oxygenation in PAI by introducing a recurrent neural
network architecture.
  Approach: We created 25 simulated training dataset variations to assess
neural network performance. We used a long short-term memory network to
implement a wavelength-flexible network architecture and proposed the
Jensen-Shannon divergence to predict the most suitable training dataset.
  Results: The network architecture can handle arbitrary input wavelengths and
outperforms linear unmixing and the previously proposed learned spectral
decolouring method. Small changes in the training data significantly affect the
accuracy of our method, but we find that the Jensen-Shannon divergence
correlates with the estimation error and is thus suitable for predicting the
most appropriate training datasets for any given application.
  Conclusions: A flexible data-driven network architecture combined with the
Jensen-Shannon Divergence to predict the best training data set provides a
promising direction that might enable robust data-driven photoacoustic oximetry
for clinical use cases.
\\ ( https://arxiv.org/abs/2403.14863 ,  6874kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14874 (*cross-listing*)
Date: Thu, 21 Mar 2024 22:46:27 GMT   (29506kb,D)

Title: WeatherProof: Leveraging Language Guidance for Semantic Segmentation in
  Adverse Weather
Authors: Blake Gella and Howard Zhang and Rishi Upadhyay and Tiffany Chang and
  Nathan Wei and Matthew Waliman and Yunhao Bao and Celso de Melo and Alex Wong
  and Achuta Kadambi
Categories: cs.CV cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2312.09534
\\
  We propose a method to infer semantic segmentation maps from images captured
under adverse weather conditions. We begin by examining existing models on
images degraded by weather conditions such as rain, fog, or snow, and found
that they exhibit a large performance drop as compared to those captured under
clear weather. To control for changes in scene structures, we propose
WeatherProof, the first semantic segmentation dataset with accurate clear and
adverse weather image pairs that share an underlying scene. Through this
dataset, we analyze the error modes in existing models and found that they were
sensitive to the highly complex combination of different weather effects
induced on the image during capture. To improve robustness, we propose a way to
use language as guidance by identifying contributions of adverse weather
conditions and injecting that as "side information". Models trained using our
language guidance exhibit performance gains by up to 10.2% in mIoU on
WeatherProof, up to 8.44% in mIoU on the widely used ACDC dataset compared to
standard training techniques, and up to 6.21% in mIoU on the ACDC dataset as
compared to previous SOTA methods.
\\ ( https://arxiv.org/abs/2403.14874 ,  29506kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14898 (*cross-listing*)
Date: Fri, 22 Mar 2024 01:04:51 GMT   (1167kb,D)

Title: Web-based Melanoma Detection
Authors: SangHyuk Kim, Edward Gaibor, Daniel Haehn
Categories: cs.CV cs.LG
Comments: 10 pages, 9 figures
\\
  Melanoma is the most aggressive form of skin cancer, and early detection can
significantly increase survival rates and prevent cancer spread. However,
developing reliable automated detection techniques is difficult due to the lack
of standardized datasets and evaluation methods. This study introduces a
unified melanoma classification approach that supports 54 combinations of 11
datasets and 24 state-of-the-art deep learning architectures. It enables a fair
comparison of 1,296 experiments and results in a lightweight model deployable
to the web-based MeshNet architecture named Mela-D. This approach can run up to
33x faster by reducing parameters 24x to yield an analogous 88.8\% accuracy
comparable with ResNet50 on previously unseen images. This allows efficient and
accurate melanoma detection in real-world settings that can run on
consumer-level hardware.
\\ ( https://arxiv.org/abs/2403.14898 ,  1167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14902 (*cross-listing*)
Date: Fri, 22 Mar 2024 01:17:07 GMT   (638kb,D)

Title: Hydro: Adaptive Query Processing of ML Queries
Authors: Gaurav Tarlok Kakkar, Jiashen Cao, Aubhro Sengupta, Joy Arulraj,
  Hyesoon Kim
Categories: cs.DB cs.LG
\\
  Query optimization in relational database management systems (DBMSs) is
critical for fast query processing. The query optimizer relies on precise
selectivity and cost estimates to effectively optimize queries prior to
execution. While this strategy is effective for relational DBMSs, it is not
sufficient for DBMSs tailored for processing machine learning (ML) queries. In
ML-centric DBMSs, query optimization is challenging for two reasons. First, the
performance bottleneck of the queries shifts to user-defined functions (UDFs)
that often wrap around deep learning models, making it difficult to accurately
estimate UDF statistics without profiling the query. This leads to inaccurate
statistics and sub-optimal query plans. Second, the optimal query plan for ML
queries is data-dependent, necessitating DBMSs to adapt the query plan on the
fly during execution. So, a static query plan is not sufficient for such
queries.
  In this paper, we present Hydro, an ML-centric DBMS that utilizes adaptive
query processing (AQP) for efficiently processing ML queries. Hydro is designed
to quickly evaluate UDF-based query predicates by ensuring optimal predicate
evaluation order and improving the scalability of UDF execution. By integrating
AQP, Hydro continuously monitors UDF statistics, routes data to predicates in
an optimal order, and dynamically allocates resources for evaluating
predicates. We demonstrate Hydro's efficacy through four illustrative use
cases, delivering up to 11.52x speedup over a baseline system.
\\ ( https://arxiv.org/abs/2403.14902 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14905 (*cross-listing*)
Date: Fri, 22 Mar 2024 01:51:48 GMT   (458kb,D)

Title: Adaptive Coded Federated Learning: Privacy Preservation and Straggler
  Mitigation
Authors: Chengxi Li, Ming Xiao, Mikael Skoglund
Categories: eess.SP cs.CR cs.LG
\\
  In this article, we address the problem of federated learning in the presence
of stragglers. For this problem, a coded federated learning framework has been
proposed, where the central server aggregates gradients received from the
non-stragglers and gradient computed from a privacy-preservation global coded
dataset to mitigate the negative impact of the stragglers. However, when
aggregating these gradients, fixed weights are consistently applied across
iterations, neglecting the generation process of the global coded dataset and
the dynamic nature of the trained model over iterations. This oversight may
result in diminished learning performance. To overcome this drawback, we
propose a new method named adaptive coded federated learning (ACFL). In ACFL,
before the training, each device uploads a coded local dataset with additive
noise to the central server to generate a global coded dataset under privacy
preservation requirements. During each iteration of the training, the central
server aggregates the gradients received from the non-stragglers and the
gradient computed from the global coded dataset, where an adaptive policy for
varying the aggregation weights is designed. Under this policy, we optimize the
performance in terms of privacy and learning, where the learning performance is
analyzed through convergence analysis and the privacy performance is
characterized via mutual information differential privacy. Finally, we perform
simulations to demonstrate the superiority of ACFL compared with the
non-adaptive methods.
\\ ( https://arxiv.org/abs/2403.14905 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14926 (*cross-listing*)
Date: Fri, 22 Mar 2024 03:01:42 GMT   (5781kb,D)

Title: Contrastive Learning on Multimodal Analysis of Electronic Health Records
Authors: Tianxi Cai, Feiqing Huang, Ryumei Nakada, Linjun Zhang, Doudou Zhou
Categories: stat.ML cs.LG
Comments: 34 pages
\\
  Electronic health record (EHR) systems contain a wealth of multimodal
clinical data including structured data like clinical codes and unstructured
data such as clinical notes. However, many existing EHR-focused studies has
traditionally either concentrated on an individual modality or merged different
modalities in a rather rudimentary fashion. This approach often results in the
perception of structured and unstructured data as separate entities, neglecting
the inherent synergy between them. Specifically, the two important modalities
contain clinically relevant, inextricably linked and complementary health
information. A more complete picture of a patient's medical history is captured
by the joint analysis of the two modalities of data. Despite the great success
of multimodal contrastive learning on vision-language, its potential remains
under-explored in the realm of multimodal EHR, particularly in terms of its
theoretical understanding. To accommodate the statistical analysis of
multimodal EHR data, in this paper, we propose a novel multimodal feature
embedding generative model and design a multimodal contrastive loss to obtain
the multimodal EHR feature representation. Our theoretical analysis
demonstrates the effectiveness of multimodal learning compared to
single-modality learning and connects the solution of the loss function to the
singular value decomposition of a pointwise mutual information matrix. This
connection paves the way for a privacy-preserving algorithm tailored for
multimodal EHR feature representation learning. Simulation studies show that
the proposed algorithm performs well under a variety of configurations. We
further validate the clinical utility of the proposed algorithm in real-world
EHR data.
\\ ( https://arxiv.org/abs/2403.14926 ,  5781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14973 (*cross-listing*)
Date: Fri, 22 Mar 2024 06:04:11 GMT   (6554kb,D)

Title: Trajectory Regularization Enhances Self-Supervised Geometric
  Representation
Authors: Jiayun Wang, Stella X. Yu, Yubei Chen
Categories: cs.CV cs.LG
\\
  Self-supervised learning (SSL) has proven effective in learning high-quality
representations for various downstream tasks, with a primary focus on semantic
tasks. However, its application in geometric tasks remains underexplored,
partially due to the absence of a standardized evaluation method for geometric
representations. To address this gap, we introduce a new pose-estimation
benchmark for assessing SSL geometric representations, which demands training
without semantic or pose labels and achieving proficiency in both semantic and
geometric downstream tasks. On this benchmark, we study enhancing SSL geometric
representations without sacrificing semantic classification accuracy. We find
that leveraging mid-layer representations improves pose-estimation performance
by 10-20%. Further, we introduce an unsupervised trajectory-regularization
loss, which improves performance by an additional 4% and improves
generalization ability on out-of-distribution data. We hope the proposed
benchmark and methods offer new insights and improvements in self-supervised
geometric representation learning.
\\ ( https://arxiv.org/abs/2403.14973 ,  6554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15004 (*cross-listing*)
Date: Fri, 22 Mar 2024 07:32:21 GMT   (6734kb)

Title: ParFormer: Vision Transformer Baseline with Parallel Local Global Token
  Mixer and Convolution Attention Patch Embedding
Authors: Novendra Setyawan, Ghufron Wahyu Kurniawan, Chi-Chia Sun, Jun-Wei
  Hsieh, Hui-Kai Su, and Wen-Kai Kuo
Categories: cs.CV cs.LG
\\
  This work presents ParFormer as an enhanced transformer architecture that
allows the incorporation of different token mixers into a single stage, hence
improving feature extraction capabilities. Integrating both local and global
data allows for precise representation of short- and long-range spatial
relationships without the need for computationally intensive methods such as
shifting windows. Along with the parallel token mixer encoder, We offer the
Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard
patch embedding to improve token mixer extraction with a convolutional
attention module. Our comprehensive evaluation demonstrates that our ParFormer
outperforms CNN-based and state-of-the-art transformer-based architectures in
image classification and several complex tasks such as object recognition. The
proposed CAPE has been demonstrated to benefit the overall MetaFormer
architecture, even while utilizing the Identity Mapping Token Mixer, resulting
in a 0.5\% increase in accuracy. The ParFormer models outperformed ConvNeXt and
Swin Transformer for the pure convolution and transformer model in accuracy.
Furthermore, our model surpasses the current leading hybrid transformer by
reaching competitive Top-1 scores in the ImageNet-1K classification test.
Specifically, our model variants with 11M, 23M, and 34M parameters achieve
scores of 80.4\%, 82.1\%, and 83.1\%, respectively. Code:
https://github.com/novendrastywn/ParFormer-CAPE-2024
\\ ( https://arxiv.org/abs/2403.15004 ,  6734kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15017 (*cross-listing*)
Date: Fri, 22 Mar 2024 08:03:10 GMT   (9065kb,D)

Title: Vehicle Detection Performance in Nordic Region
Authors: Hamam Mokayed, Rajkumar Saini, Oluwatosin Adewumi, Lama Alkhaled,
  Bjorn Backe, Palaiahnakote Shivakumara, Olle Hagner, and Yan Chai Hum
Categories: cs.CV cs.LG
Comments: submitted to ICPR2024
\\
  This paper addresses the critical challenge of vehicle detection in the harsh
winter conditions in the Nordic regions, characterized by heavy snowfall,
reduced visibility, and low lighting. Due to their susceptibility to
environmental distortions and occlusions, traditional vehicle detection methods
have struggled in these adverse conditions. The advanced proposed deep learning
architectures brought promise, yet the unique difficulties of detecting
vehicles in Nordic winters remain inadequately addressed. This study uses the
Nordic Vehicle Dataset (NVD), which has UAV images from northern Sweden, to
evaluate the performance of state-of-the-art vehicle detection algorithms under
challenging weather conditions. Our methodology includes a comprehensive
evaluation of single-stage, two-stage, and transformer-based detectors against
the NVD. We propose a series of enhancements tailored to each detection
framework, including data augmentation, hyperparameter tuning, transfer
learning, and novel strategies designed explicitly for the DETR model. Our
findings not only highlight the limitations of current detection systems in the
Nordic environment but also offer promising directions for enhancing these
algorithms for improved robustness and accuracy in vehicle detection amidst the
complexities of winter landscapes. The code and the dataset are available at
https://nvd.ltu-ai.dev
\\ ( https://arxiv.org/abs/2403.15017 ,  9065kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15031 (*cross-listing*)
Date: Fri, 22 Mar 2024 08:26:31 GMT   (871kb,D)

Title: Image Classification with Rotation-Invariant Variational Quantum
  Circuits
Authors: Paul San Sebastian, Mikel Ca\~nizo and Rom\'an Or\'us
Categories: quant-ph cs.CV cs.LG
Comments: 9 pages, 9 figures
\\
  Variational quantum algorithms are gaining attention as an early application
of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of
variational methods lies in the phenomenon of Barren Plateaus, present in the
optimization of variational parameters. Adding geometric inductive bias to the
quantum models has been proposed as a potential solution to mitigate this
problem, leading to a new field called Geometric Quantum Machine Learning. In
this work, an equivariant architecture for variational quantum classifiers is
introduced to create a label-invariant model for image classification with
$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against
two different architectures, and it is experimentally observed that the
geometric approach boosts the model's performance. Finally, a classical
equivariant convolution operation is proposed to extend the quantum model for
the processing of larger images, employing the resources available in NISQ
devices.
\\ ( https://arxiv.org/abs/2403.15031 ,  871kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15038 (*cross-listing*)
Date: Fri, 22 Mar 2024 08:42:41 GMT   (1755kb,D)

Title: Estimation of multiple mean vectors in high dimension
Authors: Gilles Blanchard (LMO, DATASHAPE), Jean-Baptiste Fermanian (LMO),
  Hannah Marienwald (BIFOLD, TU)
Categories: stat.ML cs.LG
\\
  We endeavour to estimate numerous multi-dimensional means of various
probability distributions on a common space based on independent samples. Our
approach involves forming estimators through convex combinations of empirical
means derived from these samples. We introduce two strategies to find
appropriate data-dependent convex combination weights: a first one employing a
testing procedure to identify neighbouring means with low variance, which
results in a closed-form plug-in formula for the weights, and a second one
determining weights via minimization of an upper confidence bound on the
quadratic risk.Through theoretical analysis, we evaluate the improvement in
quadratic risk offered by our methods compared to the empirical means. Our
analysis focuses on a dimensional asymptotics perspective, showing that our
methods asymptotically approach an oracle (minimax) improvement as the
effective dimension of the data increases.We demonstrate the efficacy of our
methods in estimating multiple kernel mean embeddings through experiments on
both simulated and real-world datasets.
\\ ( https://arxiv.org/abs/2403.15038 ,  1755kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15095 (*cross-listing*)
Date: Fri, 22 Mar 2024 10:23:48 GMT   (10907kb)

Title: End-to-End Mineral Exploration with Artificial Intelligence and Ambient
  Noise Tomography
Authors: Jack Muir, Gerrit Olivier, Anthony Reid
Categories: physics.geo-ph cs.LG
\\
  This paper presents an innovative end-to-end workflow for mineral
exploration, integrating ambient noise tomography (ANT) and artificial
intelligence (AI) to enhance the discovery and delineation of mineral resources
essential for the global transition to a low carbon economy. We focus on copper
as a critical element, required in significant quantities for renewable energy
solutions. We show the benefits of utilising ANT, characterised by its speed,
scalability, depth penetration, resolution, and low environmental impact,
alongside artificial intelligence (AI) techniques to refine a continent-scale
prospectivity model at the deposit scale by fine-tuning our model on local
high-resolution data. We show the promise of the method by first presenting a
new data-driven AI prospectivity model for copper within Australia, which
serves as our foundation model for further fine-tuning. We then focus on the
Hillside IOCG deposit on the prospective Yorke Peninsula. We show that with
relatively few local training samples (orebody intercepts), we can fine tune
the foundation model to provide a good estimate of the Hillside orebody
outline. Our methodology demonstrates how AI can augment geophysical data
interpretation, providing a novel approach to mineral exploration with improved
decision-making capabilities for targeting mineralization, thereby addressing
the urgent need for increased mineral resource discovery.
\\ ( https://arxiv.org/abs/2403.15095 ,  10907kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15103 (*cross-listing*)
Date: Fri, 22 Mar 2024 10:42:25 GMT   (23838kb,D)

Title: Improving cross-domain brain tissue segmentation in fetal MRI with
  synthetic data
Authors: Vladyslav Zalevskyi, Thomas Sanchez, Margaux Roulet, Jordina Aviles
  Verddera, Jana Hutter, Hamza Kebiri and Meritxell Bach Cuadra
Categories: eess.IV cs.CV cs.LG
Comments: 10 pages, 5 figures, 1 table
\\
  Segmentation of fetal brain tissue from magnetic resonance imaging (MRI)
plays a crucial role in the study of in utero neurodevelopment. However,
automated tools face substantial domain shift challenges as they must be robust
to highly heterogeneous clinical data, often limited in numbers and lacking
annotations. Indeed, high variability of the fetal brain morphology, MRI
acquisition parameters, and superresolution reconstruction (SR) algorithms
adversely affect the model's performance when evaluated out-of-domain. In this
work, we introduce FetalSynthSeg, a domain randomization method to segment
fetal brain MRI, inspired by SynthSeg. Our results show that models trained
solely on synthetic data outperform models trained on real data in out-ofdomain
settings, validated on a 120-subject cross-domain dataset. Furthermore, we
extend our evaluation to 40 subjects acquired using lowfield (0.55T) MRI and
reconstructed with novel SR models, showcasing robustness across different
magnetic field strengths and SR algorithms. Leveraging a generative synthetic
approach, we tackle the domain shift problem in fetal brain MRI and offer
compelling prospects for applications in fields with limited and highly
heterogeneous data.
\\ ( https://arxiv.org/abs/2403.15103 ,  23838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15194 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:27:57 GMT   (14341kb,D)

Title: Your Image is My Video: Reshaping the Receptive Field via Image-To-Video
  Differentiable AutoAugmentation and Fusion
Authors: Sofia Casarin, Cynthia I. Ugwu, Sergio Escalera, Oswald Lanz
Categories: cs.CV cs.LG
\\
  The landscape of deep learning research is moving towards innovative
strategies to harness the true potential of data. Traditionally, emphasis has
been on scaling model architectures, resulting in large and complex neural
networks, which can be difficult to train with limited computational resources.
However, independently of the model size, data quality (i.e. amount and
variability) is still a major factor that affects model generalization. In this
work, we propose a novel technique to exploit available data through the use of
automatic data augmentation for the tasks of image classification and semantic
segmentation. We introduce the first Differentiable Augmentation Search method
(DAS) to generate variations of images that can be processed as videos.
Compared to previous approaches, DAS is extremely fast and flexible, allowing
the search on very large search spaces in less than a GPU day. Our intuition is
that the increased receptive field in the temporal dimension provided by DAS
could lead to benefits also to the spatial receptive field. More specifically,
we leverage DAS to guide the reshaping of the spatial receptive field by
selecting task-dependant transformations. As a result, compared to standard
augmentation alternatives, we improve in terms of accuracy on ImageNet,
Cifar10, Cifar100, Tiny-ImageNet, Pascal-VOC-2012 and CityScapes datasets when
plugging-in our DAS over different light-weight video backbones.
\\ ( https://arxiv.org/abs/2403.15194 ,  14341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15230 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:23:21 GMT   (1188kb,D)

Title: An Exploratory Investigation into Code License Infringements in Large
  Language Model Training Datasets
Authors: Jonathan Katzy and R\u{a}zvan-Mihai Popescu and Arie van Deursen and
  Maliheh Izadi
Categories: cs.SE cs.LG
Comments: Accepted to FORGE 2024
\\
  Does the training of large language models potentially infringe upon code
licenses? Furthermore, are there any datasets available that can be safely used
for training these models without violating such licenses? In our study, we
assess the current trends in the field and the importance of incorporating code
into the training of large language models. Additionally, we examine publicly
available datasets to see whether these models can be trained on them without
the risk of legal issues in the future. To accomplish this, we compiled a list
of 53 large language models trained on file-level code. We then extracted their
datasets and analyzed how much they overlap with a dataset we created,
consisting exclusively of strong copyleft code.
  Our analysis revealed that every dataset we examined contained license
inconsistencies, despite being selected based on their associated repository
licenses. We analyzed a total of 514 million code files, discovering 38 million
exact duplicates present in our strong copyleft dataset. Additionally, we
examined 171 million file-leading comments, identifying 16 million with strong
copyleft licenses and another 11 million comments that discouraged copying
without explicitly mentioning a license. Based on the findings of our study,
which highlights the pervasive issue of license inconsistencies in large
language models trained on code, our recommendation for both researchers and
the community is to prioritize the development and adoption of best practices
for dataset creation and management.
\\ ( https://arxiv.org/abs/2403.15230 ,  1188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15239 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:32:27 GMT   (936kb,D)

Title: Guided Decoding for Robot Motion Generation and Adaption
Authors: Nutan Chen, Elie Aljalbout, Botond Cseke, Patrick van der Smagt
Categories: cs.RO cs.LG
Comments: 7 pages
\\
  We address motion generation for high-DoF robot arms in complex settings with
obstacles, via points, etc. A significant advancement in this domain is
achieved by integrating Learning from Demonstration (LfD) into the motion
generation process. This integration facilitates rapid adaptation to new tasks
and optimizes the utilization of accumulated expertise by allowing robots to
learn and generalize from demonstrated trajectories.
  We train a transformer architecture on a large dataset of simulated
trajectories. This architecture, based on a conditional variational autoencoder
transformer, learns essential motion generation skills and adapts these to meet
auxiliary tasks and constraints. Our auto-regressive approach enables real-time
integration of feedback from the physical system, enhancing the adaptability
and efficiency of motion generation. We show that our model can generate motion
from initial and target points, but also that it can adapt trajectories in
navigating complex tasks, including obstacle avoidance, via points, and meeting
velocity and acceleration constraints, across platforms.
\\ ( https://arxiv.org/abs/2403.15239 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15243 (*cross-listing*)
Date: Fri, 22 Mar 2024 14:36:39 GMT   (323kb,D)

Title: Robust Utility Optimization via a GAN Approach
Authors: Florian Krach, Josef Teichmann, Hanna Wutte
Categories: q-fin.CP cs.LG q-fin.MF q-fin.PM
MSC-class: 91-08, 68T07, 91G10, 91G60
\\
  Robust utility optimization enables an investor to deal with market
uncertainty in a structured way, with the goal of maximizing the worst-case
outcome. In this work, we propose a generative adversarial network (GAN)
approach to (approximately) solve robust utility optimization problems in
general and realistic settings. In particular, we model both the investor and
the market by neural networks (NN) and train them in a mini-max zero-sum game.
This approach is applicable for any continuous utility function and in
realistic market settings with trading costs, where only observable information
of the market can be used. A large empirical study shows the versatile
usability of our method. Whenever an optimal reference strategy is available,
our method performs on par with it and in the (many) settings without known
optimal strategy, our method outperforms all other reference strategies.
Moreover, we can conclude from our study that the trained path-dependent
strategies do not outperform Markovian ones. Lastly, we uncover that our
generative approach for learning optimal, (non-) robust investments under
trading costs generates universally applicable alternatives to well known
asymptotic strategies of idealized settings.
\\ ( https://arxiv.org/abs/2403.15243 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15285 (*cross-listing*)
Date: Fri, 22 Mar 2024 15:31:37 GMT   (7510kb,D)

Title: Blockchain-based Pseudonym Management for Vehicle Twin Migrations in
  Vehicular Edge Metaverse
Authors: Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou,
  Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie
Categories: cs.NI cs.CR cs.HC cs.LG
Comments: 14 pages, 9 figures
\\
  Driven by the great advances in metaverse and edge computing technologies,
vehicular edge metaverses are expected to disrupt the current paradigm of
intelligent transportation systems. As highly computerized avatars of Vehicular
Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can
provide valuable metaverse services to improve driving safety and on-board
satisfaction for their VMUs throughout journeys. To maintain uninterrupted
metaverse experiences, VTs must be migrated among edge servers following the
movements of vehicles. This can raise concerns about privacy breaches during
the dynamic communications among vehicular edge metaverses. To address these
concerns and safeguard location privacy, pseudonyms as temporary identifiers
can be leveraged by both VMUs and VTs to realize anonymous communications in
the physical space and virtual spaces. However, existing pseudonym management
methods fall short in meeting the extensive pseudonym demands in vehicular edge
metaverses, thus dramatically diminishing the performance of privacy
preservation. To this end, we present a cross-metaverse empowered dual
pseudonym management framework. We utilize cross-chain technology to enhance
management efficiency and data security for pseudonyms. Furthermore, we propose
a metric to assess the privacy level and employ a Multi-Agent Deep
Reinforcement Learning (MADRL) approach to obtain an optimal pseudonym
generating strategy. Numerical results demonstrate that our proposed schemes
are high-efficiency and cost-effective, showcasing their promising applications
in vehicular edge metaverses.
\\ ( https://arxiv.org/abs/2403.15285 ,  7510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15312 (*cross-listing*)
Date: Fri, 22 Mar 2024 16:04:26 GMT   (34kb)

Title: A Wasserstein perspective of Vanilla GANs
Authors: Lea Kunkel, Mathias Trabs
Categories: math.ST cs.LG stat.ML stat.TH
MSC-class: 62E17, 62G05, 68T07
\\
  The empirical success of Generative Adversarial Networks (GANs) caused an
increasing interest in theoretical research. The statistical literature is
mainly focused on Wasserstein GANs and generalizations thereof, which
especially allow for good dimension reduction properties. Statistical results
for Vanilla GANs, the original optimization problem, are still rather limited
and require assumptions such as smooth activation functions and equal
dimensions of the latent space and the ambient space. To bridge this gap, we
draw a connection from Vanilla GANs to the Wasserstein distance. By doing so,
existing results for Wasserstein GANs can be extended to Vanilla GANs. In
particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein
distance. The assumptions of this oracle inequality are designed to be
satisfied by network architectures commonly used in practice, such as
feedforward ReLU networks. By providing a quantitative result for the
approximation of a Lipschitz function by a feedforward ReLU network with
bounded H\"older norm, we conclude a rate of convergence for Vanilla GANs as
well as Wasserstein GANs as estimators of the unknown probability distribution.
\\ ( https://arxiv.org/abs/2403.15312 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15316 (*cross-listing*)
Date: Fri, 22 Mar 2024 16:10:38 GMT   (3914kb,D)

Title: Ultrasound Imaging based on the Variance of a Diffusion Restoration
  Model
Authors: Yuxin Zhang, Cl\'ement Huneau, J\'er\^ome Idier, Diana Mateus
Categories: eess.IV cs.CV cs.LG
Comments: 5 pages; submitted to EUSIPCO 2024. arXiv admin note: text overlap
  with arXiv:2310.20618
\\
  Despite today's prevalence of ultrasound imaging in medicine, ultrasound
signal-to-noise ratio is still affected by several sources of noise and
artefacts. Moreover, enhancing ultrasound image quality involves balancing
concurrent factors like contrast, resolution, and speckle preservation.
Recently, there has been progress in both model-based and learning-based
approaches addressing the problem of ultrasound image reconstruction. Bringing
the best from both worlds, we propose a hybrid reconstruction method combining
an ultrasound linear direct model with a learning-based prior coming from a
generative Denoising Diffusion model. More specifically, we rely on the
unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model
(DDRM). Given the nature of multiplicative noise inherent to ultrasound, this
paper proposes an empirical model to characterize the stochasticity of
diffusion reconstruction of ultrasound images, and shows the interest of its
variance as an echogenicity map estimator. We conduct experiments on synthetic,
in-vitro, and in-vivo data, demonstrating the efficacy of our variance imaging
approach in achieving high-quality image reconstructions from single plane-wave
acquisitions and in comparison to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.15316 ,  3914kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15360 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:22:56 GMT   (1622kb,D)

Title: SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate
  Time series
Authors: Badri N. Patro and Vijay S. Agneeswaran
Categories: cs.CV cs.LG cs.SY eess.IV eess.SY
\\
  Transformers have widely adopted attention networks for sequence mixing and
MLPs for channel mixing, playing a pivotal role in achieving breakthroughs
across domains. However, recent literature highlights issues with attention
networks, including low inductive bias and quadratic complexity concerning
input sequence length. State Space Models (SSMs) like S4 and others (Hippo,
Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address
the above issues to help handle longer sequence lengths. Mamba, while being the
state-of-the-art SSM, has a stability issue when scaled to large networks for
computer vision datasets. We propose SiMBA, a new architecture that introduces
Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations
and uses the Mamba block for sequence modeling. Extensive performance studies
across image and time-series benchmarks demonstrate that SiMBA outperforms
existing SSMs, bridging the performance gap with state-of-the-art transformers.
Notably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet
and transfer learning benchmarks such as Stanford Car and Flower as well as
task learning benchmarks as well as seven time series benchmark datasets. The
project page is available on this website
~\url{https://github.com/badripatro/Simba}.
\\ ( https://arxiv.org/abs/2403.15360 ,  1622kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15361 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:23:37 GMT   (31299kb,D)

Title: Learning Topological Representations for Deep Image Understanding
Authors: Xiaoling Hu
Categories: cs.CV cs.LG
Comments: Ph.D. thesis from Stony Brook University. This thesis includes works
  arXiv:1906.05404, arXiv:2110.08335, arXiv:2112.07812, arXiv:2103.09992,
  arXiv:2206.01742
\\
  In many scenarios, especially biomedical applications, the correct
delineation of complex fine-scaled structures such as neurons, tissues, and
vessels is critical for downstream analysis. Despite the strong predictive
power of deep learning methods, they do not provide a satisfactory
representation of these structures, thus creating significant barriers in
scalable annotation and downstream analysis. In this dissertation, we tackle
such challenges by proposing novel representations of these topological
structures in a deep learning framework. We leverage the mathematical tools
from topological data analysis, i.e., persistent homology and discrete Morse
theory, to develop principled methods for better segmentation and uncertainty
estimation, which will become powerful tools for scalable annotation.
\\ ( https://arxiv.org/abs/2403.15361 ,  31299kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15363 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:31:21 GMT   (969kb,D)

Title: Cascading Blackout Severity Prediction with Statistically-Augmented
  Graph Neural Networks
Authors: Joe Gorka, Tim Hsu, Wenting Li, Yury Maximov, Line Roald
Categories: eess.SY cs.LG cs.SY
Comments: Accepted to Power Systems Computation Conference (PSCC) 2024
\\
  Higher variability in grid conditions, resulting from growing renewable
penetration and increased incidence of extreme weather events, has increased
the difficulty of screening for scenarios that may lead to catastrophic
cascading failures. Traditional power-flow-based tools for assessing cascading
blackout risk are too slow to properly explore the space of possible failures
and load/generation patterns. We add to the growing literature of faster
graph-neural-network (GNN)-based techniques, developing two novel techniques
for the estimation of blackout magnitude from initial grid conditions. First we
propose several methods for employing an initial classification step to filter
out safe "non blackout" scenarios prior to magnitude estimation. Second, using
insights from the statistical properties of cascading blackouts, we propose a
method for facilitating non-local message passing in our GNN models. We
validate these two approaches on a large simulated dataset, and show the
potential of both to increase blackout size estimation performance.
\\ ( https://arxiv.org/abs/2403.15363 ,  969kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15370 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:49:11 GMT   (23674kb,D)

Title: Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks
Authors: Aqeel Anwar, Tae Eun Choe, Zian Wang, Sanja Fidler, Minwoo Park
Categories: cs.CV cs.LG cs.RO
Comments: 17 pages, 15 figures, 7 tables
\\
  Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.
\\ ( https://arxiv.org/abs/2403.15370 ,  23674kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15389 (*cross-listing*)
Date: Fri, 22 Mar 2024 17:59:58 GMT   (15385kb,D)

Title: DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from
  Partially Annotated Data
Authors: Hanrong Ye and Dan Xu
Categories: cs.CV cs.LG
Comments: The paper is accepted by CVPR 2024
\\
  Recently, there has been an increased interest in the practical problem of
learning multiple dense scene understanding tasks from partially annotated
data, where each training sample is only labeled for a subset of the tasks. The
missing of task labels in training leads to low-quality and noisy predictions,
as can be observed from state-of-the-art methods. To tackle this issue, we
reformulate the partially-labeled multi-task dense prediction as a pixel-level
denoising problem, and propose a novel multi-task denoising diffusion framework
coined as DiffusionMTL. It designs a joint diffusion and denoising paradigm to
model a potential noisy distribution in the task prediction or feature maps and
generate rectified outputs for different tasks. To exploit multi-task
consistency in denoising, we further introduce a Multi-Task Conditioning
strategy, which can implicitly utilize the complementary nature of the tasks to
help learn the unlabeled tasks, leading to an improvement in the denoising
performance of the different tasks. Extensive quantitative and qualitative
experiments demonstrate that the proposed multi-task denoising diffusion model
can significantly improve multi-task prediction maps, and outperform the
state-of-the-art methods on three challenging multi-task benchmarks, under two
different partial-labeling evaluation settings. The code is available at
https://prismformore.github.io/diffusionmtl/.
\\ ( https://arxiv.org/abs/2403.15389 ,  15385kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2211.11940
replaced with revised version Fri, 22 Mar 2024 04:40:11 GMT   (7054kb,D)

Title: Decision-making with Speculative Opponent Models
Authors: Jing Sun, Shuo Chen, Cong Zhang, Yining Ma, Jie Zhang
Categories: cs.AI cs.LG cs.MA
Comments: 13 pages, 27 figures
\\ ( https://arxiv.org/abs/2211.11940 ,  7054kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08731
replaced with revised version Fri, 22 Mar 2024 16:30:48 GMT   (5624kb,D)

Title: Novelty Detection in Reinforcement Learning with World Models
Authors: Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Mark O.
  Riedl, Robert Wright
Categories: cs.AI cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2310.08731 ,  5624kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00812
replaced with revised version Fri, 22 Mar 2024 17:29:01 GMT   (2247kb,D)

Title: Empowering Autonomous Driving with Large Language Models: A Safety
  Perspective
Authors: Yixuan Wang, Ruochen Jiao, Sinong Simon Zhan, Chengtian Lang, Chao
  Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu
Categories: cs.AI cs.LG cs.SY eess.SY
Comments: Accepted to LLMAgent workshop @ICLR2024
\\ ( https://arxiv.org/abs/2312.00812 ,  2247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11905
replaced with revised version Thu, 21 Mar 2024 21:57:13 GMT   (7657kb,D)

Title: Tur[k]ingBench: A Challenge Benchmark for Web Agents
Authors: Kevin Xu, Yeganeh Kordi, Kate Sanders, Yizhong Wang, Adam Byerly, Jack
  Zhang, Benjamin Van Durme and Daniel Khashabi
Categories: cs.AI cs.CL cs.CV cs.HC
\\ ( https://arxiv.org/abs/2403.11905 ,  7657kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14178
replaced with revised version Fri, 22 Mar 2024 07:23:22 GMT   (32001kb,D)

Title: mPLUG-Owl: Modularization Empowers Large Language Models with
  Multimodality
Authors: Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou,
  Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu,
  Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou
Categories: cs.CL cs.CV cs.LG
Comments: Working in Process
\\ ( https://arxiv.org/abs/2304.14178 ,  32001kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07766
replaced with revised version Fri, 22 Mar 2024 00:28:51 GMT   (9486kb,D)

Title: NL2TL: Transforming Natural Languages to Temporal Logics using Large
  Language Models
Authors: Yongchao Chen, Rujul Gandhi, Yang Zhang, Chuchu Fan
Categories: cs.CL
Comments: 25 pages, 18 figures
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing
\\ ( https://arxiv.org/abs/2305.07766 ,  9486kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09955
replaced with revised version Fri, 22 Mar 2024 04:04:41 GMT   (559kb,D)

Title: Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized
  Language Models
Authors: Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing
  He, Yulia Tsvetkov
Categories: cs.CL
Comments: ICLR 2024, oral
\\ ( https://arxiv.org/abs/2305.09955 ,  559kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12038
replaced with revised version Fri, 22 Mar 2024 02:24:57 GMT   (7473kb,D)

Title: Large Multilingual Models Pivot Zero-Shot Multimodal Learning across
  Languages
Authors: Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen,
  Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue,
  Dahai Li, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.CV
Comments: https://github.com/OpenBMB/VisCPM.git
\\ ( https://arxiv.org/abs/2308.12038 ,  7473kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15851
replaced with revised version Fri, 22 Mar 2024 10:02:11 GMT   (863kb,D)

Title: Self-Guard: Empower the LLM to Safeguard Itself
Authors: Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen,
  Qingwei Lin, Kam-Fai Wong
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.15851 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00117
replaced with revised version Thu, 21 Mar 2024 18:40:32 GMT   (7171kb,D)

Title: BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B
Authors: Pranav Gade and Simon Lermen and Charlie Rogers-Smith and Jeffrey
  Ladish
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.00117 ,  7171kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02945
replaced with revised version Fri, 22 Mar 2024 12:18:51 GMT   (334kb)

Title: PhoGPT: Generative Pre-training for Vietnamese
Authors: Dat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Dinh
  Phung, Hung Bui
Categories: cs.CL
Comments: PhoGPT-4B Technical Report - 5 pages
\\ ( https://arxiv.org/abs/2311.02945 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09682
replaced with revised version Thu, 21 Mar 2024 22:44:41 GMT   (8340kb,D)

Title: MacGyver: Are Large Language Models Creative Problem Solvers?
Authors: Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja
  Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, Faeze Brahman
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.09682 ,  8340kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17543
replaced with revised version Fri, 22 Mar 2024 17:12:49 GMT   (7578kb,D)

Title: Building Efficient Universal Classifiers with Natural Language Inference
Authors: Moritz Laurer, Wouter van Atteveldt, Andreu Casas, Kasper Welbers
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.17543 ,  7578kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10311
replaced with revised version Fri, 22 Mar 2024 16:10:14 GMT   (52kb,D)

Title: The optimal placement of the head in the noun phrase. The case of
  demonstrative, numeral, adjective and noun
Authors: Ramon Ferrer-i-Cancho
Categories: cs.CL physics.soc-ph
Comments: Typos corrected
\\ ( https://arxiv.org/abs/2402.10311 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14428
replaced with revised version Fri, 22 Mar 2024 06:29:26 GMT   (1782kb,D)

Title: KoCoSa: Korean Context-aware Sarcasm Detection Dataset
Authors: Yumin Kim, Heejae Suh, Mingi Kim, Dongyeon Won and Hwanhee Lee
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.14428 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14704
replaced with revised version Fri, 22 Mar 2024 06:45:51 GMT   (481kb,D)

Title: An LLM-Enhanced Adversarial Editing System for Lexical Simplification
Authors: Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu
Categories: cs.CL
Comments: Accepted by COLING 2024 main conference
\\ ( https://arxiv.org/abs/2402.14704 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00411
replaced with revised version Fri, 22 Mar 2024 15:54:03 GMT   (1195kb,D)

Title: Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with
  Fact-Checking in Turkish
Authors: Recep Firat Cekinel, Pinar Karagoz, Cagri Coltekin
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.00411 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01479
replaced with revised version Fri, 22 Mar 2024 09:14:48 GMT   (248kb,D)

Title: Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation
Authors: Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh,
  Yeonsoo Lee
Categories: cs.CL cs.AI
Comments: Accepted to LREC-COLING 2024
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2403.01479 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02893
replaced with revised version Fri, 22 Mar 2024 07:44:32 GMT   (1259kb,D)

Title: Zero-Shot Cross-Lingual Document-Level Event Causality Identification
  with Heterogeneous Graph Contrastive Transfer Learning
Authors: Zhitao He, Pengfei Cao, Zhuoran Jin, Yubo Chen, Kang Liu, Zhiqiang
  Zhang, Mengshu Sun, Jun Zhao
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.02893 ,  1259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04639
replaced with revised version Fri, 22 Mar 2024 17:28:42 GMT   (214kb)

Title: MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis
Authors: Priya Rani, Gaurav Negi, Theodorus Fransen, John P. McCrae
Categories: cs.CL
Comments: Lrec-Colin 2024
\\ ( https://arxiv.org/abs/2403.04639 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09738
replaced with revised version Fri, 22 Mar 2024 01:08:42 GMT   (1097kb,D)

Title: Evaluating Large Language Models as Generative User Simulators for
  Conversational Recommendation
Authors: Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
Categories: cs.CL cs.AI cs.IR
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2403.09738 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09919
replaced with revised version Fri, 22 Mar 2024 16:06:42 GMT   (590kb,D)

Title: Recurrent Drafter for Fast Speculative Decoding in Large Language Models
Authors: Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng
Categories: cs.CL cs.LG
Comments: 11 pages, 6 figures
\\ ( https://arxiv.org/abs/2403.09919 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10822
replaced with revised version Thu, 21 Mar 2024 23:47:24 GMT   (520kb,D)

Title: Do Large Language Models understand Medical Codes?
Authors: Simon A. Lee, Timothy Lindsey
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.10822 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12373
replaced with revised version Fri, 22 Mar 2024 06:18:54 GMT   (132kb,D)

Title: RankPrompt: Step-by-Step Comparisons Make Language Models Better
  Reasoners
Authors: Chi Hu, Yuan Ge, Xiangnan Ma, Hang Cao, Qiang Li, Yonghua Yang, Tong
  Xiao, Jingbo Zhu
Categories: cs.CL
Comments: LREC-Coling 2024 Long Paper
\\ ( https://arxiv.org/abs/2403.12373 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13592
replaced with revised version Fri, 22 Mar 2024 13:37:28 GMT   (4766kb,D)

Title: Llama meets EU: Investigating the European Political Spectrum through
  the Lens of LLMs
Authors: Ilias Chalkidis and Stephanie Brandl
Categories: cs.CL
Comments: accepted to NAACL 2024 as a short paper
\\ ( https://arxiv.org/abs/2403.13592 ,  4766kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13679
replaced with revised version Fri, 22 Mar 2024 00:49:59 GMT   (8386kb,D)

Title: RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
Authors: Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou
  Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.13679 ,  8386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14221
replaced with revised version Fri, 22 Mar 2024 12:34:47 GMT   (3488kb,D)

Title: Improving the Robustness of Large Language Models via Consistency
  Alignment
Authors: Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang,
  Chong Meng, Zhicong Cheng, Zhaochun Ren, Dawei Yin
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.14221 ,  3488kb)
------------------------------------------------------------------------------
\\
arXiv:1808.07840
replaced with revised version Fri, 22 Mar 2024 07:27:29 GMT   (22152kb,D)

Title: Learning to Importance Sample in Primary Sample Space
Authors: Quan Zheng and Matthias Zwicker
Categories: cs.LG cs.GR stat.ML
Comments: 11 pages, 14 figure; authors' version, the definitive version of
  record is available at https://onlinelibrary.wiley.com/doi/10.1111/cgf.13628
Journal-ref: Computer Graphics Forum (CGF), 2019, 38(2): 169-179.(Eurographics
  2019)
DOI: 10.1111/cgf.13628
\\ ( https://arxiv.org/abs/1808.07840 ,  22152kb)
------------------------------------------------------------------------------
\\
arXiv:2203.05222
replaced with revised version Fri, 22 Mar 2024 09:38:29 GMT   (25256kb,D)

Title: Similarity-based Label Inference Attack against Training and Inference
  of Split Learning
Authors: Junlin Liu, Xinchen Lyu, Qimei Cui, and Xiaofeng Tao
Categories: cs.LG cs.AI cs.CR
Journal-ref: IEEE Transactions on Information Forensics and Security, vol. 19,
  pp. 2881-2895, 2024
DOI: 10.1109/TIFS.2024.3356821
\\ ( https://arxiv.org/abs/2203.05222 ,  25256kb)
------------------------------------------------------------------------------
\\
arXiv:2203.16155
replaced with revised version Fri, 22 Mar 2024 09:14:22 GMT   (3379kb,D)

Title: BBE-LSWCM: A Bootstrapped Ensemble of Long and Short Window Clickstream
  Models
Authors: Arnab Chakraborty, Vikas Raturi, Shrutendra Harsola
Categories: cs.LG
Comments: 9 pages
Journal-ref: CODS-COMAD 2024: Proceedings of the 7th Joint International
  Conference on Data Science & Management of Data (11th ACM IKDD CODS and 29th
  COMAD)
DOI: 10.1145/3632410.3632452
\\ ( https://arxiv.org/abs/2203.16155 ,  3379kb)
------------------------------------------------------------------------------
\\
arXiv:2206.00759
replaced with revised version Fri, 22 Mar 2024 14:13:56 GMT   (2775kb,D)

Title: Interpretability Guarantees with Merlin-Arthur Classifiers
Authors: Stephan W\"aldchen, Kartikey Sharma, Berkant Turan, Max Zimmer,
  Sebastian Pokutta
Categories: cs.LG cs.AI
Comments: AISTATS24 Camera-Ready Version, 34 pages total (9 pages main part, 3
  pages references, 22 pages appendix), 17 figures, 3 tables
MSC-class: 68T01, 91A06
ACM-class: I.2.0
\\ ( https://arxiv.org/abs/2206.00759 ,  2775kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06015
replaced with revised version Fri, 22 Mar 2024 10:59:43 GMT   (16311kb,D)

Title: EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural
  Architecture Search
Authors: Pedram Bakhtiarifard, Christian Igel, Raghavendra Selvan
Categories: cs.LG stat.ML
Comments: Accepted to be presented at the International Conference on
  Acoustics, Speech and Signal Processing (ICASSP-2024). Source code at
  https://github.com/saintslab/EC-NAS-Bench
Journal-ref: 2024 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), pp. 5660-5664
DOI: 10.1109/ICASSP48485.2024.10448303
\\ ( https://arxiv.org/abs/2210.06015 ,  16311kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05440
replaced with revised version Fri, 22 Mar 2024 09:31:26 GMT   (1904kb,D)

Title: Forward Learning with Top-Down Feedback: Empirical and Analytical
  Characterization
Authors: Ravi Srinivasan, Francesca Mignacco, Martino Sorbaro, Maria Refinetti,
  Avi Cooper, Gabriel Kreiman, Giorgia Dellaferrera
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.05440 ,  1904kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05614
replaced with revised version Fri, 22 Mar 2024 09:34:11 GMT   (5737kb,D)

Title: Cross-domain Random Pre-training with Prototypes for Reinforcement
  Learning
Authors: Xin Liu, Yaran Chen, Haoran Li, Boyu Li and Dongbin Zhao
Categories: cs.LG cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2302.05614 ,  5737kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00652
replaced with revised version Fri, 22 Mar 2024 17:56:05 GMT   (49072kb,D)

Title: Finding the right XAI method -- A Guide for the Evaluation and Ranking
  of Explainable AI Methods in Climate Science
Authors: Philine Bommer, Marlene Kretschmer, Anna Hedstr\"om, Dilyara Bareeva,
  Marina M.-C. H\"ohne
Categories: cs.LG cs.AI
Comments: 19 pages, 10 figure, accepted at AIES journal by AMS
\\ ( https://arxiv.org/abs/2303.00652 ,  49072kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02204
replaced with revised version Fri, 22 Mar 2024 14:14:45 GMT   (3328kb,D)

Title: KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of
  Data Science
Authors: Mossad Helali, Niki Monjazeb, Shubham Vashisth, Philippe Carrier,
  Ahmed Helal, Antonio Cavalcante, Khaled Ammar, Katja Hose, Essam Mansour
Categories: cs.LG
Comments: 15 pages, 9 figures
\\ ( https://arxiv.org/abs/2303.02204 ,  3328kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05080
replaced with revised version Fri, 22 Mar 2024 01:17:08 GMT   (33439kb,D)

Title: Scalable Optimal Transport Methods in Machine Learning: A Contemporary
  Survey
Authors: Abdelwahed Khamis, Russell Tsuchida, Mohamed Tarek, Vivien Rolland,
  Lars Petersson
Categories: cs.LG cs.AI
Comments: Accepted @ TPAMI 24
Journal-ref: IEEE Transactions on Pattern Analysis and Machine Intelligence
  2024
DOI: 10.1109/TPAMI.2024.3379571
\\ ( https://arxiv.org/abs/2305.05080 ,  33439kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14521
replaced with revised version Fri, 22 Mar 2024 01:20:41 GMT   (878kb)

Title: Few-shot Adaption to Distribution Shifts By Mixing Source and Target
  Embeddings
Authors: Yihao Xue, Ali Payani, Yu Yang, Baharan Mirzasoleiman
Categories: cs.LG cs.CL cs.CV
\\ ( https://arxiv.org/abs/2305.14521 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12333
replaced with revised version Fri, 22 Mar 2024 14:36:29 GMT   (0kb,I)

Title: An axiomatized PDE model of deep neural networks
Authors: Tangjun Wang, Wenqi Tao, Chenglong Bao, Zuoqiang Shi
Categories: cs.LG
Comments: The experiment design in the paper lacks careful thought and may be
  misleading in demonstrating our contribution
\\ ( https://arxiv.org/abs/2307.12333 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15396
replaced with revised version Thu, 21 Mar 2024 23:38:52 GMT   (392kb,D)

Title: Noisy Interpolation Learning with Shallow Univariate ReLU Networks
Authors: Nirmit Joshi, Gal Vardi, Nathan Srebro
Categories: cs.LG
Comments: To appear at ICLR 2024. Updated version with minor changes in the
  presentation
\\ ( https://arxiv.org/abs/2307.15396 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01674
replaced with revised version Fri, 22 Mar 2024 13:51:19 GMT   (2303kb,D)

Title: End-to-End Reinforcement Learning of Koopman Models for Economic
  Nonlinear Model Predictive Control
Authors: Daniel Mayfrank, Alexander Mitsos, Manuel Dahmen
Categories: cs.LG cs.SY eess.SY
Comments: manuscript (18 pages, 7 figures, 5 tables), supplementary materials
  (3 pages, 2 tables)
\\ ( https://arxiv.org/abs/2308.01674 ,  2303kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13950
replaced with revised version Fri, 22 Mar 2024 00:49:41 GMT   (43kb,D)

Title: Local and Global Trend Bayesian Exponential Smoothing Models
Authors: Slawek Smyl, Christoph Bergmeir, Alexander Dokumentov, Xueying Long,
  Erwin Wibowo, Daniel Schmidt
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.13950 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16512
replaced with revised version Fri, 22 Mar 2024 17:26:53 GMT   (2594kb,D)

Title: From Complexity to Clarity: Analytical Expressions of Deep Neural
  Network Weights via Clifford's Geometric Algebra and Convexity
Authors: Mert Pilanci
Categories: cs.LG cs.AI cs.NE math.OC stat.ML
\\ ( https://arxiv.org/abs/2309.16512 ,  2594kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00401
replaced with revised version Fri, 22 Mar 2024 16:32:24 GMT   (12523kb,D)

Title: Learning High-level Semantic-Relational Concepts for SLAM
Authors: Jose Andres Millan-Romera, Hriday Bavle, Muhammad Shaheer, Martin R.
  Oswald, Holger Voos, and Jose Luis Sanchez-Lopez
Categories: cs.LG cs.RO
\\ ( https://arxiv.org/abs/2310.00401 ,  12523kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05469
replaced with revised version Fri, 22 Mar 2024 16:49:06 GMT   (2016kb,D)

Title: Learning to Predict Structural Vibrations
Authors: Jan van Delden, Julius Schultz, Christopher Blech, Sabine C. Langer,
  Timo L\"uddecke
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.05469 ,  2016kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14098
replaced with revised version Thu, 21 Mar 2024 22:49:40 GMT   (585kb,D)

Title: Stabilizing reinforcement learning control: A modular framework for
  optimizing over all stable behavior
Authors: Nathan P. Lawrence, Philip D. Loewen, Shuyuan Wang, Michael G. Forbes,
  R. Bhushan Gopaluni
Categories: cs.LG cs.AI cs.SY eess.SY math.OC
Comments: Postprint; 31 pages. arXiv admin note: text overlap with
  arXiv:2304.03422
Journal-ref: Automatica 2024
DOI: 10.1016/j.automatica.2024.111642
\\ ( https://arxiv.org/abs/2310.14098 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15929
replaced with revised version Fri, 22 Mar 2024 09:18:24 GMT   (9603kb,D)

Title: E-Sparse: Boosting the Large Language Model Inference through
  Entropy-based N:M Sparsity
Authors: Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, Zhanhui Kang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.15929 ,  9603kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04147
replaced with revised version Fri, 22 Mar 2024 15:37:38 GMT   (11040kb,D)

Title: Multi-resolution Time-Series Transformer for Long-term Forecasting
Authors: Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, Mark Coates
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.04147 ,  11040kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10270
replaced with revised version Fri, 22 Mar 2024 00:08:51 GMT   (814kb,D)

Title: Multiscale Hodge Scattering Networks for Data Analysis
Authors: Naoki Saito and Stefan C. Schonsheck and Eugene Shvarts
Categories: cs.LG cs.NA cs.SI eess.SP math.NA stat.ML
Comments: 20 Pages, Comments Welcome
\\ ( https://arxiv.org/abs/2311.10270 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10278
replaced with revised version Fri, 22 Mar 2024 03:09:25 GMT   (9712kb,D)

Title: Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint
Authors: Yongchao Chen
Categories: cs.LG cs.AI cs.CV physics.app-ph
Comments: 15 pages, 11 figure
\\ ( https://arxiv.org/abs/2311.10278 ,  9712kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14033
replaced with revised version Fri, 22 Mar 2024 09:35:13 GMT   (12705kb,D)

Title: Multivariate Scenario Generation of Day-Ahead Electricity Prices using
  Normalizing Flows
Authors: Hannes Hilger, Dirk Witthaut, Manuel Dahmen, Leonardo Rydin Gorjao,
  Julius Trebbien, Eike Cramer
Categories: cs.LG
Comments: 17 pages, 9 figures
\\ ( https://arxiv.org/abs/2311.14033 ,  12705kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18598
replaced with revised version Fri, 22 Mar 2024 08:26:20 GMT   (1185kb,D)

Title: Generalisable Agents for Neural Network Optimisation
Authors: Kale-ab Tessera, Callum Rhys Tilbury, Sasha Abramowitz, Ruan de Kock,
  Omayma Mahjoub, Benjamin Rosman, Sara Hooker, Arnu Pretorius
Categories: cs.LG cs.AI cs.MA
Comments: Accepted at the Workshop on Advanced Neural Network Training (WANT)
  and Optimization for Machine Learning (OPT) at NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.18598 ,  1185kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06710
replaced with revised version Thu, 21 Mar 2024 23:57:13 GMT   (842kb,D)

Title: Class-Prototype Conditional Diffusion Model with Gradient Projection for
  Continual Learning
Authors: Khanh Doan, Quyen Tran, Tung Lam Tran, Tuan Nguyen, Dinh Phung, Trung
  Le
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.06710 ,  842kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09016
replaced with revised version Fri, 22 Mar 2024 04:12:08 GMT   (1135kb,D)

Title: Symmetry Breaking and Equivariant Neural Networks
Authors: S\'ekou-Oumar Kaba, Siamak Ravanbakhsh
Categories: cs.LG stat.ML
Comments: 14 pages, 2 figures, Symmetry and Geometry in Neural Representations
\\ ( https://arxiv.org/abs/2312.09016 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11730
replaced with revised version Fri, 22 Mar 2024 02:18:44 GMT   (1489kb,D)

Title: Stronger Graph Transformer with Regularized Attention Scores
Authors: Eugene Ku
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.11730 ,  1489kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16424
replaced with revised version Fri, 22 Mar 2024 12:02:42 GMT   (2237kb,D)

Title: Soft Contrastive Learning for Time Series
Authors: Seunghan Lee, Taeyoung Park, Kibok Lee
Categories: cs.LG cs.AI stat.ML
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2312.16424 ,  2237kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16427
replaced with revised version Fri, 22 Mar 2024 12:05:02 GMT   (1115kb,D)

Title: Learning to Embed Time Series Patches Independently
Authors: Seunghan Lee, Taeyoung Park, Kibok Lee
Categories: cs.LG cs.AI stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2312.16427 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00885
replaced with revised version Fri, 22 Mar 2024 16:57:37 GMT   (583kb,D)

Title: Attractor reconstruction with reservoir computers: The effect of the
  reservoir's conditional Lyapunov exponents on faithful attractor
  reconstruction
Authors: Joseph D. Hart
Categories: cs.LG nlin.CD
\\ ( https://arxiv.org/abs/2401.00885 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00705
replaced with revised version Fri, 22 Mar 2024 15:13:17 GMT   (729kb)

Title: Combining the Strengths of Dutch Survey and Register Data in a Data
  Challenge to Predict Fertility (PreFer)
Authors: Elizaveta Sivak, Paulina Pankowska, Adrienne Mendrik, Tom Emery,
  Javier Garcia-Bernardo, Seyit Hocuk, Kasia Karpinska, Angelica Maineri, Joris
  Mulder, Malvina Nissim, Gert Stulp
Categories: cs.LG cs.DB
\\ ( https://arxiv.org/abs/2402.00705 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07586
replaced with revised version Fri, 22 Mar 2024 16:25:26 GMT   (27190kb,D)

Title: Unveiling Group-Specific Distributed Concept Drift: A Fairness
  Imperative in Federated Learning
Authors: Teresa Salazar and Jo\~ao Gama and Helder Ara\'ujo and Pedro Henriques
  Abreu
Categories: cs.LG
MSC-class: 68T01
ACM-class: I.2.m
\\ ( https://arxiv.org/abs/2402.07586 ,  27190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11558
replaced with revised version Fri, 22 Mar 2024 08:41:08 GMT   (2540kb,D)

Title: A Temporally Disentangled Contrastive Diffusion Model for Spatiotemporal
  Imputation
Authors: Yakun Chen, Kaize Shi, Zhangkai Wu, Juan Chen, Xianzhi Wang, Julian
  McAuley, Guandong Xu, Shui Yu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11558 ,  2540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15220
replaced with revised version Fri, 22 Mar 2024 09:03:10 GMT   (512kb,D)

Title: ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and
  Two-Phase Partition
Authors: Lu Ye, Ze Tao, Yong Huang and Yang Li
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.15220 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05752
replaced with revised version Fri, 22 Mar 2024 14:44:17 GMT   (1224kb,D)

Title: Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and
  Efficient Modeling
Authors: Hussein Abdallah, Waleed Afandi, Panos Kalnis, Essam Mansour
Categories: cs.LG cs.AI
Comments: 12 pages,9 Figures, 3 Tables, ICDE:2024
\\ ( https://arxiv.org/abs/2403.05752 ,  1224kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05848
replaced with revised version Fri, 22 Mar 2024 01:01:58 GMT   (12871kb,D)

Title: tLaSDI: Thermodynamics-informed latent space dynamics identification
Authors: Jun Sur Richard Park, Siu Wun Cheung, Youngsoo Choi, and Yeonjong Shin
Categories: cs.LG math.DS
Comments: 32 pages, 8 figures
\\ ( https://arxiv.org/abs/2403.05848 ,  12871kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06020
replaced with revised version Fri, 22 Mar 2024 13:51:55 GMT   (1895kb,D)

Title: Multi-conditioned Graph Diffusion for Neural Architecture Search
Authors: Rohan Asthana, Joschua Conrad, Youssef Dawoud, Maurits Ortmanns,
  Vasileios Belagiannis
Categories: cs.LG cs.CV
Comments: Accepted at Transactions on Machine Learning Research (TMLR)
\\ ( https://arxiv.org/abs/2403.06020 ,  1895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08763
replaced with revised version Fri, 22 Mar 2024 17:56:38 GMT   (10732kb,D)

Title: Simple and Scalable Strategies to Continually Pre-train Large Language
  Models
Authors: Adam Ibrahim, Benjamin Th\'erien, Kshitij Gupta, Mats L. Richter,
  Quentin Anthony, Timoth\'ee Lesort, Eugene Belilovsky, and Irina Rish
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.08763 ,  10732kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13784
replaced with revised version Thu, 21 Mar 2024 18:03:46 GMT   (342kb)

Title: The Model Openness Framework: Promoting Completeness and Openness for
  Reproducibility, Transparency and Usability in AI
Authors: Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang (Yanglet) Liu,
  Ahmed Abdelmonsef, Sachin Varghese
Categories: cs.LG cs.AI cs.CY cs.SE
Comments: 45 pages
\\ ( https://arxiv.org/abs/2403.13784 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13869
replaced with revised version Fri, 22 Mar 2024 10:59:56 GMT   (5458kb,D)

Title: Accurately Predicting Probabilities of Safety-Critical Rare Events for
  Intelligent Systems
Authors: Ruoxuan Bai, Jingxuan Yang, Weiduo Gong, Yi Zhang, Qiujing Lu and Shuo
  Feng
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.13869 ,  5458kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04366
replaced with revised version Fri, 22 Mar 2024 02:14:07 GMT   (3107kb,D)

Title: Enhancing Worker Recruitment in Collaborative Mobile Crowdsourcing: A
  Graph Neural Network Trust Evaluation Approach
Authors: Zhongwei Zhan, Yingjie Wang, Peiyong Duan, Akshita Maradapu Vera
  Venkata Sai, Zhaowei Liu, Chaocan Xiang, Xiangrong Tong, Weilong Wang,
  Zhipeng Cai
Categories: cs.SI cs.AI cs.HC cs.LG
Comments: The article has been accepted by IEEE TMC, and its DOI is
  10.1109/TMC.2024.3373469
DOI: 10.1109/TMC.2024.3373469
\\ ( https://arxiv.org/abs/2306.04366 ,  3107kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14899
replaced with revised version Fri, 22 Mar 2024 13:24:35 GMT   (32214kb,D)

Title: FunQA: Towards Surprising Video Comprehension
Authors: Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack
  Hessel, Jingkang Yang, Ziwei Liu
Categories: cs.CV cs.AI cs.CL cs.MM
Comments: Project Page: https://funqa-benchmark.github.io/ Codebase:
  https://github.com/Jingkang50/FunQA
\\ ( https://arxiv.org/abs/2306.14899 ,  32214kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08309
replaced with revised version Fri, 22 Mar 2024 15:52:47 GMT   (9555kb,D)

Title: LogPr\'ecis: Unleashing Language Models for Automated Malicious Log
  Analysis
Authors: Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano,
  Idilio Drago, Marco Mellia, Zied Ben Houidi
Categories: cs.CR cs.AI cs.NI
Comments: 18 pages, Computer&Security
  (https://www.sciencedirect.com/science/article/pii/S0167404824001068), code
  available at https://github.com/SmartData-Polito/logprecis, models available
  at https://huggingface.co/SmartDataPolito
Journal-ref: Computers & Security, 2024, 103805, ISSN 0167-4048
DOI: 10.1016/j.cose.2024.103805
\\ ( https://arxiv.org/abs/2307.08309 ,  9555kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04025
replaced with revised version Fri, 22 Mar 2024 14:49:31 GMT   (5234kb,D)

Title: MSAC: Multiple Speech Attribute Control Method for Reliable Speech
  Emotion Recognition
Authors: Yu Pan, Yuguang Yang, Yuheng Huang, Jixun Yao, Jingjing Yin, Yanni Hu,
  Heng Lu, Lei Ma, Jianjun Zhao
Categories: cs.SD cs.AI cs.MM eess.AS
Comments: 12 pages
\\ ( https://arxiv.org/abs/2308.04025 ,  5234kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12276
replaced with revised version Fri, 22 Mar 2024 17:28:17 GMT   (37215kb,D)

Title: LLMR: Real-time Prompting of Interactive Worlds using Large Language
  Models
Authors: Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej
  Banburski-Fahey, Judith Amores Fernandez, Jaron Lanier
Categories: cs.HC cs.AI cs.CL cs.ET
Comments: 46 pages, 18 figures; Matching version accepted at CHI 2024
\\ ( https://arxiv.org/abs/2309.12276 ,  37215kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00354
replaced with revised version Fri, 22 Mar 2024 10:36:47 GMT   (3553kb,AD)

Title: AI-Dentify: Deep learning for proximal caries detection on bitewing
  x-ray -- HUNT4 Oral Health Study
Authors: Javier P\'erez de Frutos, Ragnhild Holden Helland, Shreya Desai, Line
  Cathrine Nymoen, Thomas Lang{\o}, Theodor Remman, Abhijit Sen
Categories: cs.CV cs.AI
Comments: 24 pages, 5 figure, 7 tables
ACM-class: I.2.10; I.2.1
Journal-ref: BMC Oral Health 24, 344 (2024)
DOI: 10.1186/s12903-024-04120-0
\\ ( https://arxiv.org/abs/2310.00354 ,  3553kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15243
replaced with revised version Fri, 22 Mar 2024 07:05:58 GMT   (4012kb,D)

Title: ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection
Authors: Yichen Bai, Zongbo Han, Changqing Zhang, Bing Cao, Xiaoheng Jiang,
  Qinghua Hu
Categories: cs.CV cs.AI cs.LG
Journal-ref: CVPR 2024
\\ ( https://arxiv.org/abs/2311.15243 ,  4012kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00094
replaced with revised version Fri, 22 Mar 2024 16:38:34 GMT   (17700kb,D)

Title: Fast ODE-based Sampling for Diffusion Models in Around 5 Steps
Authors: Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2312.00094 ,  17700kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01697
replaced with revised version Fri, 22 Mar 2024 02:47:00 GMT   (5763kb,D)

Title: Hulk: A Universal Knowledge Translator for Human-Centric Tasks
Authors: Yizhou Wang, Yixuan Wu, Shixiang Tang, Weizhen He, Xun Guo, Feng Zhu,
  Lei Bai, Rui Zhao, Jian Wu, Tong He, Wanli Ouyang
Categories: cs.CV cs.AI
Comments: 24 pages, 5 figures
\\ ( https://arxiv.org/abs/2312.01697 ,  5763kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03187
replaced with revised version Thu, 21 Mar 2024 19:14:04 GMT   (6259kb,D)

Title: FERGI: Automatic Annotation of User Preferences for Text-to-Image
  Generation from Spontaneous Facial Expression Reaction
Authors: Shuangquan Feng, Junhua Ma, and Virginia R. de Sa
Categories: cs.CV cs.AI cs.HC cs.LG
\\ ( https://arxiv.org/abs/2312.03187 ,  6259kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13964
replaced with revised version Fri, 22 Mar 2024 13:25:53 GMT   (46063kb,D)

Title: PIA: Your Personalized Image Animator via Plug-and-Play Modules in
  Text-to-Image Models
Authors: Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen
Categories: cs.CV cs.AI
Comments: Project page: https://pi-animator.github.io/
\\ ( https://arxiv.org/abs/2312.13964 ,  46063kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14125
replaced with revised version Fri, 22 Mar 2024 17:06:53 GMT   (41255kb,D)

Title: VideoPoet: A Large Language Model for Zero-Shot Video Generation
Authors: Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\'e Lezama and
  Jonathan Huang and Grant Schindler and Rachel Hornung and Vighnesh Birodkar
  and Jimmy Yan and Ming-Chang Chiu and Krishna Somandepalli and Hassan Akbari
  and Yair Alon and Yong Cheng and Josh Dillon and Agrim Gupta and Meera Hahn
  and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and
  Mikhail Sirotenko and Kihyuk Sohn and Xuan Yang and Hartwig Adam and
  Ming-Hsuan Yang and Irfan Essa and Huisheng Wang and David A. Ross and Bryan
  Seybold and Lu Jiang
Categories: cs.CV cs.AI
Comments: Project page: http://sites.research.google/videopoet/
\\ ( https://arxiv.org/abs/2312.14125 ,  41255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00248
replaced with revised version Fri, 22 Mar 2024 07:25:03 GMT   (32610kb,D)

Title: Promoting Segment Anything Model towards Highly Accurate Dichotomous
  Image Segmentation
Authors: Xianjie Liu, Keren Fu, Qijun Zhao
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.00248 ,  32610kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08699 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 03:31:22 GMT   (4704kb)

Title: On Image Search in Histopathology
Authors: H.R. Tizhoosh, Liron Pantanowitz
Categories: eess.IV cs.AI cs.CV cs.IR q-bio.QM
Comments: A chapter in the Book "Artificial INtelligence in Digital Pathology"
  by Cohen and Chauhan, 2024
\\ ( https://arxiv.org/abs/2401.08699 ,  4704kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02733
replaced with revised version Fri, 22 Mar 2024 09:17:24 GMT   (13125kb,D)

Title: ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
Authors: Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
Comments: 14 pages, 15 figures, 1 table
\\ ( https://arxiv.org/abs/2402.02733 ,  13125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04690
replaced with revised version Fri, 22 Mar 2024 16:26:40 GMT   (2215kb,D)

Title: Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level
Authors: Ali Hassani, Wen-Mei Hwu, Humphrey Shi
Categories: cs.CV cs.AI cs.LG
Comments: Project page: https://github.com/SHI-Labs/NATTEN
\\ ( https://arxiv.org/abs/2403.04690 ,  2215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05574
replaced with revised version Fri, 22 Mar 2024 07:34:38 GMT   (1684kb,D)

Title: HealMe: Harnessing Cognitive Reframing in Large Language Models for
  Psychotherapy
Authors: Mengxi Xiao, Qianqian Xie, Ziyan Kuang, Zhicheng Liu, Kailai Yang, Min
  Peng, Weiguang Han, Jimin Huang
Categories: cs.HC cs.AI cs.CL
Comments: 17 pages, 4 figures
ACM-class: J.4
\\ ( https://arxiv.org/abs/2403.05574 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07869
replaced with revised version Thu, 21 Mar 2024 19:57:46 GMT   (2772kb,D)

Title: TeleMoMa: A Modular and Versatile Teleoperation System for Mobile
  Manipulation
Authors: Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan
  Zhang, Peter Stone, Ben Abbatematteo, Roberto Mart\'in-Mart\'in
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.07869 ,  2772kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09530
replaced with revised version Fri, 22 Mar 2024 15:26:05 GMT   (9985kb,D)

Title: VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision
  Understanding
Authors: Chris Kelly, Luhui Hu, Jiayin Hu, Yu Tian, Deshun Yang, Bang Yang,
  Cindy Yang, Zihao Li, Zaoshan Huang, Yuexian Zou
Categories: cs.CV cs.AI cs.CL cs.GR
Comments: 12 pages, 7 figures, pending conference
\\ ( https://arxiv.org/abs/2403.09530 ,  9985kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09920 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 04:53:29 GMT   (3717kb)

Title: Predicting Generalization of AI Colonoscopy Models to Unseen Data
Authors: Joel Shor, Carson McNeil, Yotam Intrator, Joseph R Ledsam, Hiro-o
  Yamano, Daisuke Tsurumaru, Hiroki Kayama, Atsushi Hamabe, Koji Ando,
  Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Masaaki Miyo,
  Eiji Oki, Ichiro Takemasa, Ehud Rivlin, Roman Goldenberg
Categories: eess.IV cs.AI cs.CV cs.CY
\\ ( https://arxiv.org/abs/2403.09920 ,  3717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10482 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 13:59:34 GMT   (2056kb)

Title: Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution
  Analyst?
Authors: Bruno de Melo, Jamiel Sheikh
Categories: q-fin.CP cs.AI q-fin.PM
\\ ( https://arxiv.org/abs/2403.10482 ,  2056kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10581 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 16:00:24 GMT   (17091kb,D)

Title: Large Language Model-informed ECG Dual Attention Network for Heart
  Failure Risk Prediction
Authors: Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta,
  Vicente Grau
Categories: q-bio.QM cs.AI cs.CL cs.LG eess.SP
Comments: Under journal revision
\\ ( https://arxiv.org/abs/2403.10581 ,  17091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11220
replaced with revised version Fri, 22 Mar 2024 11:42:40 GMT   (17939kb)

Title: CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object
  Detection under Unknown Degradations
Authors: Yuwei Zhang, Yan Wu, Yanming Liu, Xinyue Peng
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.11220 ,  17939kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11879
replaced with revised version Fri, 22 Mar 2024 10:08:51 GMT   (104kb,D)

Title: Unimodal Multi-Task Fusion for Emotional Mimicry Prediction
Authors: Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth Andr\'e
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2403.11879 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12211
replaced with revised version Fri, 22 Mar 2024 00:17:11 GMT   (495kb,D)

Title: A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with
  Missingness
Authors: Boqi Chen, Junier Oliva, Marc Niethammer
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.12211 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13362
replaced with revised version Thu, 21 Mar 2024 21:56:49 GMT   (4259kb,AD)

Title: Incentivizing News Consumption on Social Media Platforms Using Large
  Language Models and Realistic Bot Accounts
Authors: Hadi Askari, Anshuman Chhabra, Bernhard Clemm von Hohenberg, Michael
  Heseltine, Magdalena Wojcieszak
Categories: cs.SI cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.13362 ,  4259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13408
replaced with revised version Fri, 22 Mar 2024 11:41:38 GMT   (18164kb,D)

Title: S2DM: Sector-Shaped Diffusion Models for Video Generation
Authors: Haoran Lang, Yuxuan Ge, Zheng Tian
Categories: cs.CV cs.AI
Comments: 17 pages, 6 figures
\\ ( https://arxiv.org/abs/2403.13408 ,  18164kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13969
replaced with revised version Fri, 22 Mar 2024 15:57:20 GMT   (630kb,D)

Title: "This is not a data problem": Algorithms and Power in Public Higher
  Education in Canada
Authors: Kelly McConvey and Shion Guha
Categories: cs.HC cs.AI cs.CY
Comments: In CHI '24 Proceedings of the CHI Conference on Human Factors in
  Computing Systems Honolulu, HI, USA
DOI: 10.1145/3613904.3642451
\\ ( https://arxiv.org/abs/2403.13969 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14468
replaced with revised version Fri, 22 Mar 2024 02:16:40 GMT   (47066kb,D)

Title: AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks
Authors: Max Ku and Cong Wei and Weiming Ren and Harry Yang and Wenhu Chen
Categories: cs.CV cs.AI cs.MM
Comments: preprint
\\ ( https://arxiv.org/abs/2403.14468 ,  47066kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14617
replaced with revised version Fri, 22 Mar 2024 17:45:52 GMT   (5297kb,D)

Title: Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion
Authors: Xiang Fan, Anand Bhattad, Ranjay Krishna
Categories: cs.CV cs.AI cs.LG
Comments: Project page at https://videoshop-editing.github.io/
\\ ( https://arxiv.org/abs/2403.14617 ,  5297kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06531
replaced with revised version Fri, 22 Mar 2024 00:21:04 GMT   (4191kb,D)

Title: AutoTAMP: Autoregressive Task and Motion Planning with LLMs as
  Translators and Checkers
Authors: Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy,
  Chuchu Fan
Categories: cs.RO cs.CL cs.HC
Comments: 8 pages, 4 figures
Journal-ref: The 2024 International Conference on Robotics and Automation
\\ ( https://arxiv.org/abs/2306.06531 ,  4191kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10435
replaced with revised version Fri, 22 Mar 2024 05:57:48 GMT   (1685kb,D)

Title: Language Modeling for Content-enriched Recommendation
Authors: Junzhe Jiang, Shang Qu, Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang,
  Rujiao Zhang, Kai Zhang, Rui Li, Jiatong Li, Min Gao
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2309.10435 ,  1685kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14913 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 15:39:24 GMT   (545kb,D)

Title: Robustness of the Random Language Model
Authors: Fatemeh Lalegani and Eric De Giuli
Categories: cond-mat.dis-nn cs.CL
Comments: 11 pages; v2: expanded discussion throughout
\\ ( https://arxiv.org/abs/2309.14913 ,  545kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09611
replaced with revised version Fri, 22 Mar 2024 17:03:16 GMT   (14458kb,D)

Title: MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training
Authors: Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen
  Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers,
  Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu
  H\`e, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong
  Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming
  Pang, Peter Grasch, Alexander Toshev, Yinfei Yang
Categories: cs.CV cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.09611 ,  14458kb)
------------------------------------------------------------------------------
\\
arXiv:2204.01368
replaced with revised version Fri, 22 Mar 2024 09:42:26 GMT   (622kb,D)

Title: Training Fully Connected Neural Networks is $\exists\mathbb{R}$-Complete
Authors: Daniel Bertschinger, Christoph Hertrich, Paul Jungeblut, Tillmann
  Miltzow, Simon Weber
Categories: cs.CC cs.LG cs.NE
Comments: 39 pages, 17 figures. Changes in version 2: Added algebraic
  universality result, improved interpretation of results Changes in version 3:
  Improved exposition by formalizing properties of gadgets
\\ ( https://arxiv.org/abs/2204.01368 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13185 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 12:20:30 GMT   (47kb)

Title: An Agnostic View on the Cost of Overfitting in (Kernel) Ridge Regression
Authors: Lijia Zhou, James B. Simon, Gal Vardi, Nathan Srebro
Categories: stat.ML cs.LG
Comments: This is the ICLR CR version
\\ ( https://arxiv.org/abs/2306.13185 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09470
replaced with revised version Thu, 21 Mar 2024 19:12:08 GMT   (226kb,D)

Title: Multi-Player Zero-Sum Markov Games with Networked Separable Interactions
Authors: Chanwoo Park, Kaiqing Zhang, Asuman Ozdaglar
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2307.09470 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12943
replaced with revised version Thu, 21 Mar 2024 20:59:59 GMT   (220kb,D)

Title: Gaussian Cooling and Dikin Walks: The Interior-Point Method for
  Logconcave Sampling
Authors: Yunbum Kook, Santosh S. Vempala
Categories: cs.DS cs.LG math.OC
Comments: Improved writing with minor errors fixed
\\ ( https://arxiv.org/abs/2307.12943 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13712
replaced with revised version Fri, 22 Mar 2024 15:30:57 GMT   (4500kb,D)

Title: Residual Denoising Diffusion Models
Authors: Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang,
  Liangqiong Qu
Categories: cs.CV cs.LG
Comments: Accepted to CVPR2024
\\ ( https://arxiv.org/abs/2308.13712 ,  4500kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09510 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 15:25:04 GMT   (23kb)

Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive
  Instruction-Tuning Benchmark for Speech
Authors: Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, Chi-Yuan Hsiao, Chun-Yi
  Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng,
  Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi
  Lee
Categories: eess.AS cs.LG cs.SD
Comments: To appear in the proceedings of ICASSP 2024
\\ ( https://arxiv.org/abs/2309.09510 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15587 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 09:28:02 GMT   (2858kb,D)

Title: Quantum Langevin Dynamics for Optimization
Authors: Zherui Chen, Yuchen Lu, Hao Wang, Yizhou Liu, Tongyang Li
Categories: quant-ph cs.DS cs.LG math.OC
Comments: 52 pages, 1 table, 25 figures
\\ ( https://arxiv.org/abs/2311.15587 ,  2858kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11038
replaced with revised version Fri, 22 Mar 2024 01:33:14 GMT   (4454kb,D)

Title: UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray
  Classification
Authors: Tianjie Dai, Ruipeng Zhang, Feng Hong, Jiangchao Yao, Ya Zhang,
  Yanfeng Wang
Categories: cs.CV cs.LG
Comments: Accepted at IEEE Transactions on Medical Imaging
\\ ( https://arxiv.org/abs/2312.11038 ,  4454kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12973
replaced with revised version Fri, 22 Mar 2024 15:00:47 GMT   (12556kb,D)

Title: Sparse Mean Field Load Balancing in Large Localized Queueing Systems
Authors: Anam Tahir, Kai Cui, Heinz Koeppl
Categories: cs.DC cs.LG cs.NI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2312.12973 ,  12556kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12764 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 14:29:14 GMT   (206kb,D)

Title: Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving
  $O(1/k)$ Finite-Sample Complexity
Authors: Thinh T. Doan
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2401.12764 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10045
replaced with revised version Thu, 21 Mar 2024 21:37:50 GMT   (1268kb)

Title: Short-Form Videos and Mental Health: A Knowledge-Guided Neural Topic
  Model
Authors: Jiaheng Xie, Ruicheng Liang, Yidong Chai, Yang Liu, Daniel Zeng
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.10045 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15213 (*cross-listing*)
replaced with revised version Fri, 22 Mar 2024 07:26:48 GMT   (8163kb,D)

Title: Statistical Agnostic Regression: a machine learning method to validate
  regression models
Authors: Juan M Gorriz, J. Ramirez, F. Segovia, F. J. Martinez-Murcia, C.
  Jim\'enez-Mesa and J. Suckling
Categories: stat.ML cs.LG math.ST stat.CO stat.TH
Comments: 17 pages, 15 figures
\\ ( https://arxiv.org/abs/2402.15213 ,  8163kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11522
replaced with revised version Fri, 22 Mar 2024 10:28:05 GMT   (1073kb)

Title: LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers
Authors: Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim
  Tchoulak, Islem Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb,
  Karima Benatchba, Hugh Leather, Riyadh Baghdadi
Categories: cs.PL cs.DC cs.LG
\\ ( https://arxiv.org/abs/2403.11522 ,  1073kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11735
replaced with revised version Fri, 22 Mar 2024 06:39:20 GMT   (3250kb,D)

Title: LSKNet: A Foundation Lightweight Backbone for Remote Sensing
Authors: Yuxuan Li, Xiang Li, Yimain Dai, Qibin Hou, Li Liu, Yongxiang Liu,
  Ming-Ming Cheng, Jian Yang
Categories: cs.CV cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2303.09030
\\ ( https://arxiv.org/abs/2403.11735 ,  3250kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12074
replaced with revised version Fri, 22 Mar 2024 04:35:16 GMT   (3301kb)

Title: Beyond Quantities: Machine Learning-based Characterization of Inequality
  in Infrastructure Quality Provision in Cities
Authors: Bo Li, Ali Mostafavi
Categories: cs.CY cs.LG
\\ ( https://arxiv.org/abs/2403.12074 ,  3301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12979 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 18:52:20 GMT   (1112kb,D)

Title: AltGraph: Redesigning Quantum Circuits Using Generative Graph Models for
  Efficient Optimization
Authors: Collin Beaudoin, Koustubh Phalak, Swaroop Ghosh
Categories: quant-ph cs.ET cs.LG
\\ ( https://arxiv.org/abs/2403.12979 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14413
replaced with revised version Fri, 22 Mar 2024 13:15:20 GMT   (373kb,D)

Title: Model Uncertainty in Evolutionary Optimization and Bayesian
  Optimization: A Comparative Analysis
Authors: Hao Hao, Xiaoqun Zhang, Aimin Zhou
Categories: cs.NE cs.LG
\\ ( https://arxiv.org/abs/2403.14413 ,  373kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
