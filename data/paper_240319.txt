paper_240319.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月19日 14:41
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri 15 Mar 24 18:00:00 GMT  to  Mon 18 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.10720
Date: Fri, 15 Mar 2024 22:43:37 GMT   (642kb,D)

Title: Development and Application of a Monte Carlo Tree Search Algorithm for
  Simulating Da Vinci Code Game Strategies
Authors: Ye Zhang, Mengran Zhu, Kailin Gui, Jiayue Yu, Yong Hao, Haozhan Sun
Categories: cs.AI
Comments: This paper has been accepted by CVIDL2024
\\
  In this study, we explore the efficiency of the Monte Carlo Tree Search
(MCTS), a prominent decision-making algorithm renowned for its effectiveness in
complex decision environments, contingent upon the volume of simulations
conducted. Notwithstanding its broad applicability, the algorithm's performance
can be adversely impacted in certain scenarios, particularly within the domain
of game strategy development. This research posits that the inherent branch
divergence within the Da Vinci Code board game significantly impedes
parallelism when executed on Graphics Processing Units (GPUs). To investigate
this hypothesis, we implemented and meticulously evaluated two variants of the
MCTS algorithm, specifically designed to assess the impact of branch divergence
on computational performance. Our comparative analysis reveals a linear
improvement in performance with the CPU-based implementation, in stark contrast
to the GPU implementation, which exhibits a non-linear enhancement pattern and
discernible performance troughs. These findings contribute to a deeper
understanding of the MCTS algorithm's behavior in divergent branch scenarios,
highlighting critical considerations for optimizing game strategy algorithms on
parallel computing architectures.
\\ ( https://arxiv.org/abs/2403.10720 ,  642kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10744
Date: Sat, 16 Mar 2024 00:26:59 GMT   (2848kb)

Title: Game and Reference: Policy Combination Synthesis for Epidemic Prevention
  and Control
Authors: Zhiyi Tan, Bingkun Bao
Categories: cs.AI
Comments: 16 pages, single line, 7 figures, written with Springer conference
  template
\\
  In recent years, epidemic policy-making models are increasingly being used to
provide reference for governors on prevention and control policies against
catastrophic epidemics such as SARS, H1N1 and COVID-19. Existing studies are
currently constrained by two issues: First, previous methods develop policies
based on effect evaluation, since few of factors in real-world decision-making
can be modeled, the output policies will then easily become extreme. Second,
the subjectivity and cognitive limitation of human make the historical policies
not always optimal for the training of decision models. To these ends, we
present a novel Policy Combination Synthesis (PCS) model for epidemic
policy-making. Specially, to prevent extreme decisions, we introduce
adversarial learning between the model-made policies and the real policies to
force the output policies to be more human-liked. On the other hand, to
minimize the impact of sub-optimal historical policies, we employ contrastive
learning to let the model draw on experience from the best historical policies
under similar scenarios. Both adversarial and contrastive learning are adaptive
based on the comprehensive effects of real policies to ensure the model always
learns useful information. Extensive experiments on real-world data prove the
effectiveness of the proposed model.
\\ ( https://arxiv.org/abs/2403.10744 ,  2848kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10761
Date: Sat, 16 Mar 2024 01:51:42 GMT   (9712kb,D)

Title: Scheduling Drone and Mobile Charger via Hybrid-Action Deep Reinforcement
  Learning
Authors: Jizhe Dou and Haotian Zhang and Guodong Sun
Categories: cs.AI cs.LG cs.RO
\\
  Recently there has been a growing interest in industry and academia,
regarding the use of wireless chargers to prolong the operational longevity of
unmanned aerial vehicles (commonly knowns as drones). In this paper we consider
a charger-assisted drone application: a drone is deployed to observe a set
points of interest, while a charger can move to recharge the drone's battery.
We focus on the route and charging schedule of the drone and the mobile
charger, to obtain high observation utility with the shortest possible time,
while ensuring the drone remains operational during task execution.
Essentially, this proposed drone-charger scheduling problem is a multi-stage
decision-making process, in which the drone and the mobile charger act as two
agents who cooperate to finish a task. The discrete-continuous hybrid action
space of the two agents poses a significant challenge in our problem. To
address this issue, we present a hybrid-action deep reinforcement learning
framework, called HaDMC, which uses a standard policy learning algorithm to
generate latent continuous actions. Motivated by representation learning, we
specifically design and train an action decoder. It involves two pipelines to
convert the latent continuous actions into original discrete and continuous
actions, by which the drone and the charger can directly interact with
environment. We embed a mutual learning scheme in model training, emphasizing
the collaborative rather than individual actions. We conduct extensive
numerical experiments to evaluate HaDMC and compare it with state-of-the-art
deep reinforcement learning approaches. The experimental results show the
effectiveness and efficiency of our solution.
\\ ( https://arxiv.org/abs/2403.10761 ,  9712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10930
Date: Sat, 16 Mar 2024 14:06:29 GMT   (459kb,D)

Title: Inducing Individual Students' Learning Strategies through Homomorphic
  POMDPs
Authors: Huifan Gao, Yifeng Zeng and Yinghui Pan
Categories: cs.AI
Comments: 11pages, 3figures
\\
  Optimizing students' learning strategies is a crucial component in
intelligent tutoring systems. Previous research has demonstrated the
effectiveness of devising personalized learning strategies for students by
modelling their learning processes through partially observable Markov decision
process (POMDP). However, the research holds the assumption that the student
population adheres to a uniform cognitive pattern. While this assumption
simplifies the POMDP modelling process, it evidently deviates from a real-world
scenario, thus reducing the precision of inducing individual students' learning
strategies. In this article, we propose the homomorphic POMDP (H-POMDP) model
to accommodate multiple cognitive patterns and present the parameter learning
approach to automatically construct the H-POMDP model. Based on the H-POMDP
model, we are able to represent different cognitive patterns from the data and
induce more personalized learning strategies for individual students. We
conduct experiments to show that, in comparison to the general POMDP approach,
the H-POMDP model demonstrates better precision when modelling mixed data from
multiple cognitive patterns. Moreover, the learning strategies derived from
H-POMDPs exhibit better personalization in the performance evaluation.
\\ ( https://arxiv.org/abs/2403.10930 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11217
Date: Sun, 17 Mar 2024 13:34:45 GMT   (449kb)

Title: Research on Personal Credit Risk Assessment Methods Based on Causal
  Inference
Authors: Jiaxin Wang, YiLong Ma
Categories: cs.AI math.CT
\\
  The discussion on causality in human history dates back to ancient Greece,
yet to this day, there is still no consensus. Fundamentally, this stems from
the nature of human cognition, as understanding causality requires abstract
tools to transcend the limitations of human cognition. In recent decades, the
rapid development of mathematical and computational tools has provided new
theoretical and technical means for exploring causality, creating more avenues
for investigation.
  Based on this, this paper introduces a new definition of causality using
category theory, proposed by Samuel Eilenberg and Saunders Mac Lane in 1945 to
avoid the self-referential contradictions in set theory, notably the Russell
paradox. Within this framework, the feasibility of indicator synthesis in
causal inference is demonstrated. Due to the limitations in the development of
category theory-related technical tools, this paper adopts the widely-used
probabilistic causal graph tool proposed by Judea Pearl in 1995 to study the
application of causal inference in personal credit risk management. The
specific work includes: research on the construction method of causal inference
index system, definition of causality and feasibility proof of indicator
synthesis causal inference within this framework, application methods of causal
graph model and intervention alternative criteria in personal credit risk
management, and so on.
\\ ( https://arxiv.org/abs/2403.11217 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11219
Date: Sun, 17 Mar 2024 13:39:43 GMT   (201kb,D)

Title: Causality from Bottom to Top: A Survey
Authors: Abraham Itzhak Weinberg, Cristiano Premebida, Diego Resende Faria
Categories: cs.AI
\\
  Causality has become a fundamental approach for explaining the relationships
between events, phenomena, and outcomes in various fields of study. It has
invaded various fields and applications, such as medicine, healthcare,
economics, finance, fraud detection, cybersecurity, education, public policy,
recommender systems, anomaly detection, robotics, control, sociology,
marketing, and advertising. In this paper, we survey its development over the
past five decades, shedding light on the differences between causality and
other approaches, as well as the preconditions for using it. Furthermore, the
paper illustrates how causality interacts with new approaches such as
Artificial Intelligence (AI), Generative AI (GAI), Machine and Deep Learning,
Reinforcement Learning (RL), and Fuzzy Logic. We study the impact of causality
on various fields, its contribution, and its interaction with state-of-the-art
approaches. Additionally, the paper exemplifies the trustworthiness and
explainability of causality models. We offer several ways to evaluate causality
models and discuss future directions.
\\ ( https://arxiv.org/abs/2403.11219 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11381
Date: Mon, 18 Mar 2024 00:13:43 GMT   (1147kb,D)

Title: Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
  cooperative capabilities through Melting Pot
Authors: Manuel Mosquera, Juan Sebastian Pinzon, Manuel Rios, Yesid Fonseca,
  Luis Felipe Giraldo, Nicanor Quijano, Ruben Manrique
Categories: cs.AI cs.CL
\\
  As the field of AI continues to evolve, a significant dimension of this
progression is the development of Large Language Models and their potential to
enhance multi-agent artificial intelligence systems. This paper explores the
cooperative capabilities of Large Language Model-augmented Autonomous Agents
(LAAs) using the well-known Meltin Pot environments along with reference models
such as GPT4 and GPT3.5. Preliminary results suggest that while these agents
demonstrate a propensity for cooperation, they still struggle with effective
collaboration in given environments, emphasizing the need for more robust
architectures. The study's contributions include an abstraction layer to adapt
Melting Pot game scenarios for LLMs, the implementation of a reusable
architecture for LLM-mediated agent development - which includes short and
long-term memories and different cognitive modules, and the evaluation of
cooperation capabilities using a set of metrics tied to the Melting Pot's
"Commons Harvest" game. The paper closes, by discussing the limitations of the
current architectural framework and the potential of a new set of modules that
fosters better cooperation among LAAs.
\\ ( https://arxiv.org/abs/2403.11381 ,  1147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11642
Date: Mon, 18 Mar 2024 10:34:40 GMT   (6528kb,D)

Title: Guiding the generation of counterfactual explanations through temporal
  background knowledge for Predictive Process Monitoring
Authors: Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Ivan
  Donadello, Fabrizio Maria Maggi
Categories: cs.AI cs.LG
\\
  Counterfactual explanations suggest what should be different in the input
instance to change the outcome of an AI system. When dealing with
counterfactual explanations in the field of Predictive Process Monitoring,
however, control flow relationships among events have to be carefully
considered. A counterfactual, indeed, should not violate control flow
relationships among activities (temporal background knowledege). Within the
field of Explainability in Predictive Process Monitoring, there have been a
series of works regarding counterfactual explanations for outcome-based
predictions. However, none of them consider the inclusion of temporal
background knowledge when generating these counterfactuals. In this work, we
adapt state-of-the-art techniques for counterfactual generation in the domain
of XAI that are based on genetic algorithms to consider a series of temporal
constraints at runtime. We assume that this temporal background knowledge is
given, and we adapt the fitness function, as well as the crossover and mutation
operators, to maintain the satisfaction of the constraints. The proposed
methods are evaluated with respect to state-of-the-art genetic algorithms for
counterfactual generation and the results are presented. We showcase that the
inclusion of temporal background knowledge allows the generation of
counterfactuals more conformant to the temporal background knowledge, without
however losing in terms of the counterfactual traditional quality metrics.
\\ ( https://arxiv.org/abs/2403.11642 ,  6528kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11734
Date: Mon, 18 Mar 2024 12:42:53 GMT   (247kb,D)

Title: Learning General Policies for Classical Planning Domains: Getting Beyond
  C$_2$
Authors: Simon St\r{a}hlberg, Blai Bonet, Hector Geffner
Categories: cs.AI cs.LG
Comments: Submitted to IJCAI 2024
\\
  GNN-based approaches for learning general policies across planning domains
are limited by the expressive power of $C_2$, namely; first-order logic with
two variables and counting. This limitation can be overcomed by transitioning
to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet
embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$-
and $2$-GNNs that are confined to $C_2$, they require quartic time for message
exchange and cubic space for embeddings, rendering them impractical. In this
work, we introduce a parameterized version of relational GNNs. When $t$ is
infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for
embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$]
achieves a weaker approximation by exchanging fewer messages, yet
interestingly, often yield the $C_3$ features required in several planning
domains. Furthermore, the new R-GNN[$t$] architecture is the original R-GNN
architecture with a suitable transformation applied to the input states only.
Experimental results illustrate the clear performance gains of R-GNN[$1$] and
R-GNN[$2$] over plain R-GNNs, and also over edge transformers that also
approximate $3$-GNNs.
\\ ( https://arxiv.org/abs/2403.11734 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11807
Date: Mon, 18 Mar 2024 14:04:47 GMT   (2223kb,D)

Title: How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming
  Ability in Multi-Agent Environments
Authors: Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang,
  Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu
Categories: cs.AI cs.CL
Comments: 16 pages, 15 figures, 9 tables. Working in Progress
\\
  Decision-making, a complicated task requiring various types of abilities,
presents an excellent framework for assessing Large Language Models (LLMs). Our
research investigates LLMs' decision-making capabilities through the lens of a
well-established field, Game Theory. We focus specifically on games that
support the participation of more than two agents simultaneously. Subsequently,
we introduce our framework, GAMA-Bench, including eight classical multi-agent
games. We design a scoring scheme to assess a model's performance in these
games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness,
generalizability, and enhancement strategies. Results reveal that while GPT-3.5
shows satisfying robustness, its generalizability is relatively limited.
However, its performance can be improved through approaches such as
Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and
find that GPT-4 outperforms other models on GAMA-Bench, achieving a score of
72.5. Moreover, the increasingly higher scores across the three iterations of
GPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model's
intelligence with each update. The code and experimental results are made
publicly available via https://github.com/CUHK-ARISE/GAMABench.
\\ ( https://arxiv.org/abs/2403.11807 ,  2223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11905
Date: Mon, 18 Mar 2024 16:06:30 GMT   (7331kb,D)

Title: Tur[k]ingBench: A Challenge Benchmark for Web Agents
Authors: Kevin Xu, Yeganeh Kordi, Kate Sanders, Yizhong Wang, Adam Byerly, Jack
  Zhang, Benjamin Van Durme and Daniel Khashabi
Categories: cs.AI cs.CL cs.CV cs.HC
\\
  Recent chatbots have demonstrated impressive ability to understand and
communicate in raw-text form. However, there is more to the world than raw
text. For example, humans spend long hours of their time on web pages, where
text is intertwined with other modalities and tasks are accomplished in the
form of various complex interactions. Can state-of-the-art multi-modal models
generalize to such complex domains?
  To address this question, we introduce TurkingBench, a benchmark of tasks
formulated as web pages containing textual instructions with multi-modal
context. Unlike existing work which employs artificially synthesized web pages,
here we use natural HTML pages that were originally designed for crowdsourcing
workers for various annotation purposes. The HTML instructions of each task are
also instantiated with various values (obtained from the crowdsourcing tasks)
to form new instances of the task. This benchmark contains 32.2K instances
distributed across 158 tasks.
  Additionally, to facilitate the evaluation on TurkingBench, we develop an
evaluation framework that connects the responses of chatbots to modifications
on web pages (modifying a text box, checking a radio, etc.). We evaluate the
performance of state-of-the-art models, including language-only, vision-only,
and layout-only models, and their combinations, on this benchmark. Our findings
reveal that these models perform significantly better than random chance, yet
considerable room exists for improvement. We hope this benchmark will help
facilitate the evaluation and development of web-based agents.
\\ ( https://arxiv.org/abs/2403.11905 ,  7331kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10596
Date: Fri, 15 Mar 2024 18:00:00 GMT   (1843kb,D)

Title: Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI
  Systems
Authors: Antonios Alexos, Yu-Dai Tsai, Ian Domingo, Maryam Pishgar, Pierre
  Baldi
Categories: cs.CL cs.AI q-bio.NC
Comments: 19 pages, 6 figures in the main text, 5 figures in the Appendix
\\
  Creating controlled methods to simulate neurodegeneration in artificial
intelligence (AI) is crucial for applications that emulate brain function
decline and cognitive disorders. We use IQ tests performed by Large Language
Models (LLMs) and, more specifically, the LLaMA 2 to introduce the concept of
``neural erosion." This deliberate erosion involves ablating synapses or
neurons, or adding Gaussian noise during or after training, resulting in a
controlled progressive decline in the LLMs' performance. We are able to
describe the neurodegeneration in the IQ tests and show that the LLM first
loses its mathematical abilities and then its linguistic abilities, while
further losing its ability to understand the questions. To the best of our
knowledge, this is the first work that models neurodegeneration with text data,
compared to other works that operate in the computer vision domain. Finally, we
draw similarities between our study and cognitive decline clinical studies
involving test subjects. We find that with the application of neurodegenerative
methods, LLMs lose abstract thinking abilities, followed by mathematical
degradation, and ultimately, a loss in linguistic ability, responding to
prompts incoherently. These findings are in accordance with human studies.
\\ ( https://arxiv.org/abs/2403.10596 ,  1843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10691
Date: Fri, 15 Mar 2024 21:21:11 GMT   (11044kb,D)

Title: MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual
  Language Modeling
Authors: Tomasz Limisiewicz and Terra Blevins and Hila Gonen and Orevaoghene
  Ahia and Luke Zettlemoyer
Categories: cs.CL cs.AI cs.LG
\\
  A major consideration in multilingual language modeling is how to best
represent languages with diverse vocabularies and scripts. Although
contemporary text encoding methods cover most of the world's writing systems,
they exhibit bias towards the high-resource languages of the Global West. As a
result, texts of underrepresented languages tend to be segmented into long
sequences of linguistically meaningless units. To address the disparities, we
introduce a new paradigm that encodes the same information with segments of
consistent size across diverse languages. Our encoding convention (MYTE) is
based on morphemes, as their inventories are more balanced across languages
than characters, which are used in previous methods. We show that MYTE produces
shorter encodings for all 99 analyzed languages, with the most notable
improvements for non-European languages and non-Latin scripts. This, in turn,
improves multilingual LM performance and diminishes the perplexity gap
throughout diverse languages.
\\ ( https://arxiv.org/abs/2403.10691 ,  11044kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10692
Date: Fri, 15 Mar 2024 21:22:37 GMT   (8253kb,D)

Title: EXPLORER: Exploration-guided Reasoning for Textual Reinforcement
  Learning
Authors: Kinjal Basu, Keerthiram Murugesan, Subhajit Chaudhury, Murray
  Campbell, Kartik Talamadupula, and Tim Klinger
Categories: cs.CL cs.AI cs.LO
\\
  Text-based games (TBGs) have emerged as an important collection of NLP tasks,
requiring reinforcement learning (RL) agents to combine natural language
understanding with reasoning. A key challenge for agents attempting to solve
such tasks is to generalize across multiple games and demonstrate good
performance on both seen and unseen objects. Purely deep-RL-based approaches
may perform well on seen objects; however, they fail to showcase the same
performance on unseen objects. Commonsense-infused deep-RL agents may work
better on unseen data; unfortunately, their policies are often not
interpretable or easily transferable. To tackle these issues, in this paper, we
present EXPLORER which is an exploration-guided reasoning agent for textual
reinforcement learning. EXPLORER is neurosymbolic in nature, as it relies on a
neural module for exploration and a symbolic module for exploitation. It can
also learn generalized symbolic policies and perform well over unseen data. Our
experiments show that EXPLORER outperforms the baseline agents on Text-World
cooking (TW-Cooking) and Text-World Commonsense (TWC) games.
\\ ( https://arxiv.org/abs/2403.10692 ,  8253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10699
Date: Fri, 15 Mar 2024 21:35:21 GMT   (13510kb,D)

Title: A Multilingual Perspective on Probing Gender Bias
Authors: Karolina Sta\'nczak
Categories: cs.CL
Comments: Ph.D. Thesis
\\
  Gender bias represents a form of systematic negative treatment that targets
individuals based on their gender. This discrimination can range from subtle
sexist remarks and gendered stereotypes to outright hate speech. Prior research
has revealed that ignoring online abuse not only affects the individuals
targeted but also has broader societal implications. These consequences extend
to the discouragement of women's engagement and visibility within public
spheres, thereby reinforcing gender inequality. This thesis investigates the
nuances of how gender bias is expressed through language and within language
technologies. Significantly, this thesis expands research on gender bias to
multilingual contexts, emphasising the importance of a multilingual and
multicultural perspective in understanding societal biases. In this thesis, I
adopt an interdisciplinary approach, bridging natural language processing with
other disciplines such as political science and history, to probe gender bias
in natural language and language models.
\\ ( https://arxiv.org/abs/2403.10699 ,  13510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10707
Date: Fri, 15 Mar 2024 21:54:00 GMT   (7442kb,D)

Title: Uncovering Latent Themes of Messaging on Social Media by Integrating
  LLMs: A Case Study on Climate Campaigns
Authors: Tunazzina Islam, Dan Goldwasser
Categories: cs.CL cs.AI cs.CY cs.LG cs.SI
\\
  This paper introduces a novel approach to uncovering and analyzing themes in
social media messaging. Recognizing the limitations of traditional topic-level
analysis, which tends to capture only the overarching patterns, this study
emphasizes the need for a finer-grained, theme-focused exploration.
Conventional methods of theme discovery, involving manual processes and a
human-in-the-loop approach, are valuable but face challenges in scalability,
consistency, and resource intensity in terms of time and cost. To address these
challenges, we propose a machine-in-the-loop approach that leverages the
advanced capabilities of Large Language Models (LLMs). This approach allows for
a deeper investigation into the thematic aspects of social media discourse,
enabling us to uncover a diverse array of themes, each with unique
characteristics and relevance, thereby offering a comprehensive understanding
of the nuances present within broader topics. Furthermore, this method
efficiently maps the text and the newly discovered themes, enhancing our
understanding of the thematic nuances in social media messaging. We employ
climate campaigns as a case study and demonstrate that our methodology yields
more accurate and interpretable results compared to traditional topic models.
Our results not only demonstrate the effectiveness of our approach in
uncovering latent themes but also illuminate how these themes are tailored for
demographic targeting in social media contexts. Additionally, our work sheds
light on the dynamic nature of social media, revealing the shifts in the
thematic focus of messaging in response to real-world events.
\\ ( https://arxiv.org/abs/2403.10707 ,  7442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10750
Date: Sat, 16 Mar 2024 01:01:16 GMT   (250kb,D)

Title: Depression Detection on Social Media with Large Language Models
Authors: Xiaochong Lan, Yiming Cheng, Li Sheng, Chen Gao, Yong Li
Categories: cs.CL cs.AI
\\
  Depression harms. However, due to a lack of mental health awareness and fear
of stigma, many patients do not actively seek diagnosis and treatment, leading
to detrimental outcomes. Depression detection aims to determine whether an
individual suffers from depression by analyzing their history of posts on
social media, which can significantly aid in early detection and intervention.
It mainly faces two key challenges: 1) it requires professional medical
knowledge, and 2) it necessitates both high accuracy and explainability. To
address it, we propose a novel depression detection system called DORIS,
combining medical knowledge and the recent advances in large language models
(LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based
solution to first annotate whether high-risk texts meet medical diagnostic
criteria. Further, we retrieve texts with high emotional intensity and
summarize critical information from the historical mood records of users,
so-called mood courses. To tackle the second challenge, we combine LLM and
traditional classifiers to integrate medical knowledge-guided features, for
which the model can also explain its prediction results, achieving both high
accuracy and explainability. Extensive experimental results on benchmarking
datasets show that, compared to the current best baseline, our approach
improves by 0.036 in AUPRC, which can be considered significant, demonstrating
the effectiveness of our approach and its high value as an NLP application.
\\ ( https://arxiv.org/abs/2403.10750 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10758
Date: Sat, 16 Mar 2024 01:40:36 GMT   (1098kb)

Title: Rules still work for Open Information Extraction
Authors: Jialin Hua, Liangqing Luo, Weiying Ping, Yan Liao, Chunhai Tao, Xuewen
  Lub
Categories: cs.CL
\\
  Open information extraction (OIE) aims to extract surface relations and their
corresponding arguments from natural language text, irrespective of domain.
This paper presents an innovative OIE model, APRCOIE, tailored for Chinese
text. Diverging from previous models, our model generates extraction patterns
autonomously. The model defines a new pattern form for Chinese OIE and proposes
an automated pattern generation methodology. In that way, the model can handle
a wide array of complex and diverse Chinese grammatical phenomena. We design a
preliminary filter based on tensor computing to conduct the extraction
procedure efficiently. To train the model, we manually annotated a large-scale
Chinese OIE dataset. In the comparative evaluation, we demonstrate that APRCOIE
outperforms state-of-the-art Chinese OIE models and significantly expands the
boundaries of achievable OIE performance. The code of APRCOIE and the annotated
dataset are released on GitHub (https://github.com/jialin666/APRCOIE_v1)
\\ ( https://arxiv.org/abs/2403.10758 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10764
Date: Sat, 16 Mar 2024 02:07:31 GMT   (765kb,D)

Title: ECRC: Emotion-Causality Recognition in Korean Conversation for GCN
Authors: J. K. Lee, T. M. Chung
Categories: cs.CL cs.AI
Comments: 10 pages, 5 figures
\\
  In this multi-task learning study on simultaneous analysis of emotions and
their underlying causes in conversational contexts, deep neural network methods
were employed to effectively process and train large labeled datasets. However,
these approaches are typically limited to conducting context analyses across
the entire corpus because they rely on one of the two methods: word- or
sentence-level embedding. The former struggles with polysemy and homonyms,
whereas the latter causes information loss when processing long sentences. In
this study, we overcome the limitations of previous embeddings by utilizing
both word- and sentence-level embeddings. Furthermore, we propose the
emotion-causality recognition in conversation (ECRC) model, which is based on a
novel graph structure, thereby leveraging the strengths of both embedding
methods. This model uniquely integrates the bidirectional long short-term
memory (Bi-LSTM) and graph neural network (GCN) models for Korean conversation
analysis. Compared with models that rely solely on one embedding method, the
proposed model effectively structures abstract concepts, such as language
features and relationships, thereby minimizing information loss. To assess
model performance, we compared the multi-task learning results of three deep
neural network models with varying graph structures. Additionally, we evaluated
the proposed model using Korean and English datasets. The experimental results
show that the proposed model performs better in emotion and causality
multi-task learning (74.62% and 75.30%, respectively) when node and edge
characteristics are incorporated into the graph structure. Similar results were
recorded for the Korean ECC and Wellness datasets (74.62% and 73.44%,
respectively) with 71.35% on the IEMOCAP English dataset.
\\ ( https://arxiv.org/abs/2403.10764 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10774
Date: Sat, 16 Mar 2024 02:27:19 GMT   (1056kb,D)

Title: Detecting Bias in Large Language Models: Fine-tuned KcBERT
Authors: J. K. Lee, T. M. Chung
Categories: cs.CL
Comments: 14 pages, 5 figures
\\
  The rapid advancement of large language models (LLMs) has enabled natural
language processing capabilities similar to those of humans, and LLMs are being
widely utilized across various societal domains such as education and
healthcare. While the versatility of these models has increased, they have the
potential to generate subjective and normative language, leading to
discriminatory treatment or outcomes among social groups, especially due to
online offensive language. In this paper, we define such harm as societal bias
and assess ethnic, gender, and racial biases in a model fine-tuned with Korean
comments using Bidirectional Encoder Representations from Transformers (KcBERT)
and KOLD data through template-based Masked Language Modeling (MLM). To
quantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to
KcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates
significant changes in gender and racial biases. Based on these results, we
propose two methods to mitigate societal bias. Firstly, a data balancing
approach during the pre-training phase adjusts the uniformity of data by
aligning the distribution of the occurrences of specific words and converting
surrounding harmful words into non-harmful words. Secondly, during the
in-training phase, we apply Debiasing Regularization by adjusting dropout and
regularization, confirming a decrease in training loss. Our contribution lies
in demonstrating that societal bias exists in Korean language models due to
language-dependent characteristics.
\\ ( https://arxiv.org/abs/2403.10774 ,  1056kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10779
Date: Sat, 16 Mar 2024 02:48:50 GMT   (11474kb,D)

Title: LLM-based Conversational AI Therapist for Daily Functioning Screening
  and Psychotherapeutic Intervention via Everyday Smart Devices
Authors: Jingping Nie, Hanya Shao, Yuang Fan, Qijia Shao, Haoxuan You, Matthias
  Preindl, Xiaofan Jiang
Categories: cs.CL
\\
  Despite the global mental health crisis, access to screenings, professionals,
and treatments remains high. In collaboration with licensed psychotherapists,
we propose a Conversational AI Therapist with psychotherapeutic Interventions
(CaiTI), a platform that leverages large language models (LLM)s and smart
devices to enable better mental health self-care. CaiTI can screen the
day-to-day functioning using natural and psychotherapeutic conversations. CaiTI
leverages reinforcement learning to provide personalized conversation flow.
CaiTI can accurately understand and interpret user responses. When the user
needs further attention during the conversation, CaiTI can provide
conversational psychotherapeutic interventions, including cognitive behavioral
therapy (CBT) and motivational interviewing (MI). Leveraging the datasets
prepared by the licensed psychotherapists, we experiment and microbenchmark
various LLMs' performance in tasks along CaiTI's conversation flow and discuss
their strengths and weaknesses. With the psychotherapists, we implement CaiTI
and conduct 14-day and 24-week studies. The study results, validated by
therapists, demonstrate that CaiTI can converse with users naturally,
accurately understand and interpret user responses, and provide
psychotherapeutic interventions appropriately and effectively. We showcase the
potential of CaiTI LLMs to assist the mental therapy diagnosis and treatment
and improve day-to-day functioning screening and precautionary
psychotherapeutic intervention systems.
\\ ( https://arxiv.org/abs/2403.10779 ,  11474kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10781
Date: Sat, 16 Mar 2024 02:58:57 GMT   (398kb,D)

Title: Exploring Chinese Humor Generation: A Study on Two-Part Allegorical
  Sayings
Authors: Rongwu Xu
Categories: cs.CL cs.AI
\\
  Humor, a culturally nuanced aspect of human language, poses challenges for
computational understanding and generation, especially in Chinese humor, which
remains relatively unexplored in the NLP community. This paper investigates the
capability of state-of-the-art language models to comprehend and generate
Chinese humor, specifically focusing on training them to create allegorical
sayings. We employ two prominent training methods: fine-tuning a medium-sized
language model and prompting a large one. Our novel fine-tuning approach
incorporates fused Pinyin embeddings to consider homophones and employs
contrastive learning with synthetic hard negatives to distinguish humor
elements. Human-annotated results show that these models can generate humorous
allegorical sayings, with prompting proving to be a practical and effective
method. However, there is still room for improvement in generating allegorical
sayings that match human creativity.
\\ ( https://arxiv.org/abs/2403.10781 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10795
Date: Sat, 16 Mar 2024 03:54:38 GMT   (6256kb,D)

Title: From Words to Routes: Applying Large Language Models to Vehicle Routing
Authors: Zhehui Huang, Guangyao Shi, Gaurav S. Sukhatme
Categories: cs.CL cs.AI cs.LG cs.RO
Comments: Submitted to IEEE Robotics and Automation Society (IROS 2024)
\\
  LLMs have shown impressive progress in robotics (e.g., manipulation and
navigation) with natural language task descriptions. The success of LLMs in
these tasks leads us to wonder: What is the ability of LLMs to solve vehicle
routing problems (VRPs) with natural language task descriptions? In this work,
we study this question in three steps. First, we construct a dataset with 21
types of single- or multi-vehicle routing problems. Second, we evaluate the
performance of LLMs across four basic prompt paradigms of text-to-code
generation, each involving different types of text input. We find that the
basic prompt paradigm, which generates code directly from natural language task
descriptions, performs the best for GPT-4, achieving 56% feasibility, 40%
optimality, and 53% efficiency. Third, based on the observation that LLMs may
not be able to provide correct solutions at the initial attempt, we propose a
framework that enables LLMs to refine solutions through self-reflection,
including self-debugging and self-verification. With GPT-4, our proposed
framework achieves a 16% increase in feasibility, a 7% increase in optimality,
and a 15% increase in efficiency. Moreover, we examine the sensitivity of GPT-4
to task descriptions, specifically focusing on how its performance changes when
certain details are omitted from the task descriptions, yet the core meaning is
preserved. Our findings reveal that such omissions lead to a notable decrease
in performance: 4% in feasibility, 4% in optimality, and 5% in efficiency.
Website: https://sites.google.com/view/words-to-routes/
\\ ( https://arxiv.org/abs/2403.10795 ,  6256kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10799
Date: Sat, 16 Mar 2024 04:12:50 GMT   (533kb,D)

Title: Efficient Pruning of Large Language Model with Adaptive Estimation
  Fusion
Authors: Jun Liu, Chao Wu, Changdi Yang, Hao Tang, Haoye Dong, Zhenglun Kong,
  Geng Yuan, Wei Niu, Dong Huang, Yanzhi Wang
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have become crucial for many generative
downstream tasks, leading to an inevitable trend and significant challenge to
deploy them efficiently on resource-constrained devices. Structured pruning is
a widely used method to address this challenge. However, when dealing with the
complex structure of the multiple decoder layers, general methods often employ
common estimation approaches for pruning. These approaches lead to a decline in
accuracy for specific downstream tasks. In this paper, we introduce a simple
yet efficient method that adaptively models the importance of each
substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained
estimations based on the results from complex and multilayer structures. All
aspects of our design seamlessly integrate into the endto-end pruning
framework. Our experimental results, compared with state-of-the-art methods on
mainstream datasets, demonstrate average accuracy improvements of 1.1%, 1.02%,
2.0%, and 1.2% for LLaMa-7B,Vicuna-7B, Baichuan-7B, and Bloom-7b1,
respectively.
\\ ( https://arxiv.org/abs/2403.10799 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10822
Date: Sat, 16 Mar 2024 06:18:15 GMT   (281kb,D)

Title: Do Large Language Models understand Medical Codes?
Authors: Simon A. Lee, Timothy Lindsey
Categories: cs.CL
\\
  The overarching goal of recent AI research has been to make steady progress
towards achieving Artificial General Intelligence (AGI), prompting the
evaluation of Large Language Models (LLMs) across a variety of tasks and
domains. One such domain is healthcare, where LLMs can greatly benefit clinical
practice by assisting with a wide range of tasks. However, these models are
also prone to producing "hallucinations" or incorrect responses when faced with
queries they cannot adequately address, raising concerns and skepticism,
especially within the healthcare community. Therefore, in this work, we
investigate whether LLMs understand the inherent meaning of medical codes,
which are widely used in healthcare practice. We evaluate various off-the-shelf
LLMs (e.g., GPT, LLaMA, etc.) and LLMs specifically designed for biomedical
applications to assess their awareness and understanding of these
domain-specific terminologies. Our results indicate that these models do not
comprehend the meaning of the medical codes, highlighting the need for better
representation of these alphanumeric codes extensively used in healthcare. We
call for improved strategies to effectively capture and represent the nuances
of medical codes and terminologies within LLMs, enabling them to become more
reliable and trustworthy tools for healthcare professionals.
\\ ( https://arxiv.org/abs/2403.10822 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10827
Date: Sat, 16 Mar 2024 06:33:44 GMT   (2229kb,D)

Title: Multi-party Response Generation with Relation Disentanglement
Authors: Tianhao Dai, Chengyu Huang, and Lizi Liao
Categories: cs.CL
\\
  Existing neural response generation models have achieved impressive
improvements for two-party conversations, which assume that utterances are
sequentially organized. However, many real-world dialogues involve multiple
interlocutors and the structure of conversational context is much more complex,
e.g. utterances from different interlocutors can occur "in parallel". Facing
this challenge, there are works trying to model the relations among utterances
or interlocutors to facilitate response generation with clearer context.
Nonetheless, these methods rely heavily on such relations and all assume that
these are given beforehand, which is impractical and hinders the generality of
such methods. In this work, we propose to automatically infer the relations via
relational thinking on subtle clues inside the conversation context without any
human label, and leverage these relations to guide the neural response
generation. Specifically, we first apply a deep graph random process to fully
consider all possible relations among utterances in the conversational context.
Then the inferred relation graphs are integrated with a variational
auto-encoder framework to train a GAN for structure-aware response generation.
Experimental results on the Ubuntu Internet Relay Chat (IRC) channel benchmark
and the most recent Movie Dialogues show that our method outperforms various
baseline models for multi-party response generation.
\\ ( https://arxiv.org/abs/2403.10827 ,  2229kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10829
Date: Sat, 16 Mar 2024 06:39:41 GMT   (4804kb,D)

Title: Deciphering Hate: Identifying Hateful Memes and Their Targets
Authors: Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque, Sarah M. Preum
Categories: cs.CL
\\
  Internet memes have become a powerful means for individuals to express
emotions, thoughts, and perspectives on social media. While often considered as
a source of humor and entertainment, memes can also disseminate hateful content
targeting individuals or communities. Most existing research focuses on the
negative aspects of memes in high-resource languages, overlooking the
distinctive challenges associated with low-resource languages like Bengali
(also known as Bangla). Furthermore, while previous work on Bengali memes has
focused on detecting hateful memes, there has been no work on detecting their
targeted entities. To bridge this gap and facilitate research in this arena, we
introduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes).
The dataset consists of 7,148 memes with Bengali as well as code-mixed
captions, tailored for two tasks: (i) detecting hateful memes, and (ii)
detecting the social entities they target (i.e., Individual, Organization,
Community, and Society). To solve these tasks, we propose DORA (Dual cO
attention fRAmework), a multimodal deep neural network that systematically
extracts the significant modality features from the memes and jointly evaluates
them with the modality-specific features to understand the context better. Our
experiments show that DORA is generalizable on other low-resource hateful meme
datasets and outperforms several state-of-the-art rivaling baselines.
\\ ( https://arxiv.org/abs/2403.10829 ,  4804kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10838
Date: Sat, 16 Mar 2024 07:18:29 GMT   (1045kb,D)

Title: Two-step Automated Cybercrime Coded Word Detection using Multi-level
  Representation Learning
Authors: Yongyeon Kim and Byung-Won On and Ingyu Lee
Categories: cs.CL
\\
  In social network service platforms, crime suspects are likely to use
cybercrime coded words for communication by adding criminal meanings to
existing words or replacing them with similar words. For instance, the word
'ice' is often used to mean methamphetamine in drug crimes. To analyze the
nature of cybercrime and the behavior of criminals, quickly detecting such
words and further understanding their meaning are critical. In the automated
cybercrime coded word detection problem, it is difficult to collect a
sufficient amount of training data for supervised learning and to directly
apply language models that utilize context information to better understand
natural language. To overcome these limitations, we propose a new two-step
approach, in which a mean latent vector is constructed for each cybercrime
through one of five different AutoEncoder models in the first step, and
cybercrime coded words are detected based on multi-level latent representations
in the second step. Moreover, to deeply understand cybercrime coded words
detected through the two-step approach, we propose three novel methods: (1)
Detection of new words recently coined, (2) Detection of words frequently
appeared in both drug and sex crimes, and (3) Automatic generation of word
taxonomy. According to our experimental results, among various AutoEncoder
models, the stacked AutoEncoder model shows the best performance. Additionally,
the F1-score of the two-step approach is 0.991, which is higher than 0.987 and
0.903 of the existing dark-GloVe and dark-BERT models. By analyzing the
experimental results of the three proposed methods, we can gain a deeper
understanding of drug and sex crimes.
\\ ( https://arxiv.org/abs/2403.10838 ,  1045kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10849
Date: Sat, 16 Mar 2024 08:08:20 GMT   (362kb,D)

Title: RETINAQA : A Knowledge Base Question Answering Model Robust to both
  Answerable and Unanswerable Questions
Authors: Prayushi Faldu, Indrajit Bhattacharya, Mausam
Categories: cs.CL
\\
  State-of-the-art KBQA models assume answerability of questions. Recent
research has shown that while these can be adapted to detect unaswerability
with suitable training and thresholding, this comes at the expense of accuracy
for answerable questions, and no single model is able to handle all categories
of unanswerability. We propose a new model for KBQA named RetinaQA that is
robust against unaswerability. It complements KB-traversal based logical form
retrieval with sketch-filling based logical form construction. This helps with
questions that have valid logical forms but no data paths in the KB leading to
an answer. Additionally, it uses discrimination instead of generation to better
identify questions that do not have valid logical forms. We demonstrate that
RetinaQA significantly outperforms adaptations of state-of-the-art KBQA models
across answerable and unanswerable questions, while showing robustness across
unanswerability categories. Remarkably, it also establishes a new state-of-the
art for answerable KBQA by surpassing existing models
\\ ( https://arxiv.org/abs/2403.10849 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10856
Date: Sat, 16 Mar 2024 08:31:25 GMT   (763kb,D)

Title: Zero-shot Generative Linguistic Steganography
Authors: Ke Lin, Yiyang Luo, Zijian Zhang and Ping Luo
Categories: cs.CL cs.CR
Comments: 15 pages, 6 figures. Accepted at NAACL 2024
\\
  Generative linguistic steganography attempts to hide secret messages into
covertext. Previous studies have generally focused on the statistical
differences between the covertext and stegotext, however, ill-formed stegotext
can readily be identified by humans. In this paper, we propose a novel
zero-shot approach based on in-context learning for linguistic steganography to
achieve better perceptual and statistical imperceptibility. We also design
several new metrics and reproducible language evaluations to measure the
imperceptibility of the stegotext. Our experimental results indicate that our
method produces $1.926\times$ more innocent and intelligible stegotext than any
other method.
\\ ( https://arxiv.org/abs/2403.10856 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10882
Date: Sat, 16 Mar 2024 10:26:38 GMT   (3729kb,D)

Title: Optimizing Language Augmentation for Multilingual Large Language Models:
  A Case Study on Korean
Authors: ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim,
  SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee,
  Younggyun Hahm, Hansaem Kim and KyungTae Lim
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) use pretraining to predict the subsequent word;
however, their expansion requires significant computing resources. Numerous big
tech companies and research institutes have developed multilingual LLMs (MLLMs)
to meet current demands, overlooking less-resourced languages (LRLs). This
study proposed three strategies to enhance the performance of LRLs based on the
publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to
enhance expressiveness. Second, bilingual data were used for pretraining to
align the high- and less-resourced languages. Third, a high-quality small-scale
instruction dataset was constructed and instruction-tuning was performed to
augment the LRL. The experiments employed the Llama2 model and Korean was used
as the LRL, which was quantitatively evaluated against other developed LLMs
across eight tasks. Furthermore, a qualitative assessment was performed based
on human evaluation and GPT4. Experimental results showed that our proposed
Bllossom model exhibited superior performance in qualitative analyses compared
to previously proposed Korean monolingual models.
\\ ( https://arxiv.org/abs/2403.10882 ,  3729kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10894
Date: Sat, 16 Mar 2024 11:09:27 GMT   (491kb,D)

Title: Towards Robustness and Diversity: Continual Learning in Dialog
  Generation with Text-Mixup and Batch Nuclear-Norm Maximization
Authors: Zihan Wang, Jiayu Xiao, Mengxiang Li, Zhongjiang He, Yongxiang Li,
  Chao Wang, Shuangyong Song
Categories: cs.CL
Comments: 12 pages, 2 figures
\\
  In our dynamic world where data arrives in a continuous stream, continual
learning enables us to incrementally add new tasks/domains without the need to
retrain from scratch. A major challenge in continual learning of language model
is catastrophic forgetting, the tendency of models to forget knowledge from
previously trained tasks/domains when training on new ones. This paper studies
dialog generation under the continual learning setting. We propose a novel
method that 1) uses \textit{Text-Mixup} as data augmentation to avoid model
overfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization
(BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain
task-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset)
demonstrate that our proposed approach outperforms the state-of-the-art in
continual learning.
\\ ( https://arxiv.org/abs/2403.10894 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10900
Date: Sat, 16 Mar 2024 11:27:42 GMT   (7990kb,D)

Title: BEnQA: A Question Answering and Reasoning Benchmark for Bengali and
  English
Authors: Sheikh Shafayat, H M Quamran Hasan, Minhajur Rahman Chowdhury Mahim,
  Rifki Afina Putri, James Thorne, Alice Oh
Categories: cs.CL
\\
  In this study, we introduce BEnQA, a dataset comprising parallel Bengali and
English exam questions for middle and high school levels in Bangladesh. Our
dataset consists of approximately 5K questions covering several subjects in
science with different types of questions, including factual, application, and
reasoning-based questions. We benchmark several Large Language Models (LLMs)
with our parallel dataset and observe a notable performance disparity between
the models in Bengali and English. We also investigate some prompting methods,
and find that Chain-of-Thought prompting is beneficial mostly on reasoning
questions, but not so much on factual ones. We also find that appending English
translation helps to answer questions in Bengali. Our findings point to
promising future research directions for improving the performance of LLMs in
Bengali and more generally in low-resource languages.
\\ ( https://arxiv.org/abs/2403.10900 ,  7990kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10949
Date: Sat, 16 Mar 2024 15:30:34 GMT   (5469kb,D)

Title: SelfIE: Self-Interpretation of Large Language Model Embeddings
Authors: Haozhe Chen, Carl Vondrick, Chengzhi Mao
Categories: cs.CL cs.AI cs.LG
\\
  How do large language models (LLMs) obtain their answers? The ability to
explain and control an LLM's reasoning process is key for reliability,
transparency, and future model developments. We propose SelfIE
(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret
their own embeddings in natural language by leveraging their ability to respond
inquiry about a given passage. Capable of interpreting open-world concepts in
the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as
making ethical decisions, internalizing prompt injection, and recalling harmful
knowledge. SelfIE's text descriptions on hidden embeddings also open up new
avenues to control LLM reasoning. We propose Supervised Control, which allows
editing open-ended concepts while only requiring gradient computation of
individual layer. We extend RLHF to hidden embeddings and propose Reinforcement
Control that erases harmful knowledge in LLM without supervision targets.
\\ ( https://arxiv.org/abs/2403.10949 ,  5469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10963
Date: Sat, 16 Mar 2024 16:17:47 GMT   (3443kb,D)

Title: Pointer-Generator Networks for Low-Resource Machine Translation: Don't
  Copy That!
Authors: Niyati Bafna and David Yarowsky
Categories: cs.CL
Comments: 4 pages
\\
  While Transformer-based neural machine translation (NMT) is very effective in
high-resource settings, many languages lack the necessary large parallel
corpora to benefit from it. In the context of low-resource (LR) MT between two
closely-related languages, a natural intuition is to seek benefits from
structural "shortcuts", such as copying subwords from the source to the target,
given that such language pairs often share a considerable number of identical
words, cognates, and borrowings. We test Pointer-Generator Networks for this
purpose for six language pairs over a variety of resource ranges, and find weak
improvements for most settings. However, analysis shows that the model does not
show greater improvements for closely-related vs. more distant language pairs,
or for lower resource ranges, and that the models do not exhibit the expected
usage of the mechanism for shared subwords. Our discussion of the reasons for
this behaviour highlights several general challenges for LR NMT, such as modern
tokenization strategies, noisy real-world conditions, and linguistic
complexities. We call for better scrutiny of linguistically motivated
improvements to NMT given the blackbox nature of Transformer models, as well as
for a focus on the above problems in the field.
\\ ( https://arxiv.org/abs/2403.10963 ,  3443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10978
Date: Sat, 16 Mar 2024 17:21:58 GMT   (1563kb,D)

Title: Entity Alignment with Unlabeled Dangling Cases
Authors: Hang Yin, Dong Ding, Liyao Xiang, Yuheng He, Yihan Wu, Xinbing Wang,
  Chenghu Zhou
Categories: cs.CL cs.IR
Comments: 14 pages
ACM-class: I.2.4; H.3.3
\\
  We investigate the entity alignment problem with unlabeled dangling cases,
meaning that there are entities in the source or target graph having no
counterparts in the other, and those entities remain unlabeled. The problem
arises when the source and target graphs are of different scales, and it is
much cheaper to label the matchable pairs than the dangling entities. To solve
the issue, we propose a novel GNN-based dangling detection and entity alignment
framework. While the two tasks share the same GNN and are trained together, the
detected dangling entities are removed in the alignment. Our framework is
featured by a designed entity and relation attention mechanism for selective
neighborhood aggregation in representation learning, as well as a
positive-unlabeled learning loss for an unbiased estimation of dangling
entities. Experimental results have shown that each component of our design
contributes to the overall alignment performance which is comparable or
superior to baselines, even if the baselines additionally have 30\% of the
dangling entities labeled as training data.
\\ ( https://arxiv.org/abs/2403.10978 ,  1563kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11009
Date: Sat, 16 Mar 2024 20:18:36 GMT   (12034kb,D)

Title: DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and
  Closely-Related Languages
Authors: Fahim Faisal, Orevaoghene Ahia, Aarohi Srivastava, Kabir Ahuja, David
  Chiang, Yulia Tsvetkov, Antonios Anastasopoulos
Categories: cs.CL cs.AI
Comments: Equal contribution: Fahim Faisal, Orevaoghene Ahia
\\
  Language technologies should be judged on their usefulness in real-world use
cases. An often overlooked aspect in natural language processing (NLP) research
and evaluation is language variation in the form of non-standard dialects or
language varieties (hereafter, varieties). Most NLP benchmarks are limited to
standard language varieties. To fill this gap, we propose DIALECTBENCH, the
first-ever large-scale benchmark for NLP on varieties, which aggregates an
extensive set of task-varied variety datasets (10 text-level tasks covering 281
varieties). This allows for a comprehensive evaluation of NLP system
performance on different language varieties. We provide substantial evidence of
performance disparities between standard and non-standard language varieties,
and we also identify language clusters with large performance divergence across
tasks. We believe DIALECTBENCH provides a comprehensive view of the current
state of NLP for language varieties and one step towards advancing it further.
Code/data: https://github.com/ffaisal93/DialectBench
\\ ( https://arxiv.org/abs/2403.11009 ,  12034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11025
Date: Sat, 16 Mar 2024 22:01:39 GMT   (7798kb,D)

Title: Pre-Trained Language Models Represent Some Geographic Populations Better
  Than Others
Authors: Jonathan Dunn and Benjamin Adams and Harish Tayyar Madabushi
Categories: cs.CL
\\
  This paper measures the skew in how well two families of LLMs represent
diverse geographic populations. A spatial probing task is used with
geo-referenced corpora to measure the degree to which pre-trained language
models from the OPT and BLOOM series represent diverse populations around the
world. Results show that these models perform much better for some populations
than others. In particular, populations across the US and the UK are
represented quite well while those in South and Southeast Asia are poorly
represented. Analysis shows that both families of models largely share the same
skew across populations. At the same time, this skew cannot be fully explained
by sociolinguistic factors, economic factors, or geographic factors. The basic
conclusion from this analysis is that pre-trained models do not equally
represent the world's population: there is a strong skew towards specific
geographic populations. This finding challenges the idea that a single model
can be used for all populations.
\\ ( https://arxiv.org/abs/2403.11025 ,  7798kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11069
Date: Sun, 17 Mar 2024 03:15:29 GMT   (474kb)

Title: Deep Learning-based Sentiment Analysis in Persian Language
Authors: Mohammad Heydari, Mohsen Khazeni, Mohammad Ali Soltanshahi
Categories: cs.CL
Comments: 7 pages, 5 figures, 4 tables
DOI: 10.1109/ICWR51868.2021.9443152
\\
  Recently, there has been a growing interest in the use of deep learning
techniques for tasks in natural language processing (NLP), with sentiment
analysis being one of the most challenging areas, particularly in the Persian
language. The vast amounts of content generated by Persian users on thousands
of websites, blogs, and social networks such as Telegram, Instagram, and
Twitter present a rich resource of information. Deep learning techniques have
become increasingly favored for extracting insights from this extensive pool of
raw data, although they face several challenges. In this study, we introduced
and implemented a hybrid deep learning-based model for sentiment analysis,
using customer review data from the Digikala Online Retailer website. We
employed a variety of deep learning networks and regularization techniques as
classifiers. Ultimately, our hybrid approach yielded an impressive performance,
achieving an F1 score of 78.3 across three sentiment categories: positive,
negative, and neutral.
\\ ( https://arxiv.org/abs/2403.11069 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11082
Date: Sun, 17 Mar 2024 04:29:45 GMT   (1268kb,D)

Title: RobustSentEmbed: Robust Sentence Embeddings Using Adversarial
  Self-Supervised Contrastive Learning
Authors: Javad Rafiei Asl, Prajwal Panzade, Eduardo Blanco, Daniel Takabi,
  Zhipeng Cai
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at the Annual Conference of the North American Chapter of
  the Association for Computational Linguistics (NAACL Findings) 2024.
  [https://openreview.net/forum?id=9dEAg4lJEA]
\\
  Pre-trained language models (PLMs) have consistently demonstrated outstanding
performance across a diverse spectrum of natural language processing tasks.
Nevertheless, despite their success with unseen data, current PLM-based
representations often exhibit poor robustness in adversarial settings. In this
paper, we introduce RobustSentEmbed, a self-supervised sentence embedding
framework designed to improve both generalization and robustness in diverse
text representation tasks and against a diverse set of adversarial attacks.
Through the generation of high-risk adversarial perturbations and their
utilization in a novel objective function, RobustSentEmbed adeptly learns
high-quality and robust sentence embeddings. Our experiments confirm the
superiority of RobustSentEmbed over state-of-the-art representations.
Specifically, Our framework achieves a significant reduction in the success
rate of various adversarial attacks, notably reducing the BERTAttack success
rate by almost half (from 75.51\% to 38.81\%). The framework also yields
improvements of 1.59\% and 0.23\% in semantic textual similarity tasks and
various transfer tasks, respectively.
\\ ( https://arxiv.org/abs/2403.11082 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11092
Date: Sun, 17 Mar 2024 05:05:11 GMT   (8487kb,D)

Title: Lost in Translation? Translation Errors and Challenges for Fair
  Assessment of Text-to-Image Models on Multilingual Concepts
Authors: Michael Saxon, Yiran Luo, Sharon Levy, Chitta Baral, Yezhou Yang,
  William Yang Wang
Categories: cs.CL cs.AI cs.CV cs.CY eess.IV
Comments: NAACL 2024 Main Conference
\\
  Benchmarks of the multilingual capabilities of text-to-image (T2I) models
compare generated images prompted in a test language to an expected image
distribution over a concept set. One such benchmark, "Conceptual Coverage
Across Languages" (CoCo-CroLa), assesses the tangible noun inventory of T2I
models by prompting them to generate pictures from a concept list translated to
seven languages and comparing the output image populations. Unfortunately, we
find that this benchmark contains translation errors of varying severity in
Spanish, Japanese, and Chinese. We provide corrections for these errors and
analyze how impactful they are on the utility and validity of CoCo-CroLa as a
benchmark. We reassess multiple baseline T2I models with the revisions, compare
the outputs elicited under the new translations to those conditioned on the
old, and show that a correction's impactfulness on the image-domain benchmark
results can be predicted in the text domain with similarity scores. Our
findings will guide the future development of T2I multilinguality metrics by
providing analytical tools for practical translation decisions.
\\ ( https://arxiv.org/abs/2403.11092 ,  8487kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11103
Date: Sun, 17 Mar 2024 06:12:43 GMT   (8954kb,D)

Title: ProgGen: Generating Named Entity Recognition Datasets Step-by-step with
  Self-Reflexive Large Language Models
Authors: Yuzhao Heng, Chunyuan Deng, Yitong Li, Yue Yu, Yinghao Li, Rongzhi
  Zhang, Chao Zhang
Categories: cs.CL cs.LG
Comments: under review
\\
  Although Large Language Models (LLMs) exhibit remarkable adaptability across
domains, these models often fall short in structured knowledge extraction tasks
such as named entity recognition (NER). This paper explores an innovative,
cost-efficient strategy to harness LLMs with modest NER capabilities for
producing superior NER datasets. Our approach diverges from the basic
class-conditional prompts by instructing LLMs to self-reflect on the specific
domain, thereby generating domain-relevant attributes (such as category and
emotions for movie reviews), which are utilized for creating attribute-rich
training data. Furthermore, we preemptively generate entity terms and then
develop NER context data around these entities, effectively bypassing the LLMs'
challenges with complex structures. Our experiments across both general and
niche domains reveal significant performance enhancements over conventional
data generation methods while being more cost-effective than existing
alternatives.
\\ ( https://arxiv.org/abs/2403.11103 ,  8954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11108
Date: Sun, 17 Mar 2024 06:23:25 GMT   (234kb)

Title: HarmPot: An Annotation Framework for Evaluating Offline Harm Potential
  of Social Media Text
Authors: Ritesh Kumar and Ojaswee Bhalla and Madhu Vanthi and Shehlat Maknoon
  Wani and Siddharth Singh
Categories: cs.CL
Comments: Accepted for: LREC COLING 2024
\\
  In this paper, we discuss the development of an annotation schema to build
datasets for evaluating the offline harm potential of social media texts. We
define "harm potential" as the potential for an online public post to cause
real-world physical harm (i.e., violence). Understanding that real-world
violence is often spurred by a web of triggers, often combining several online
tactics and pre-existing intersectional fissures in the social milieu, to
result in targeted physical violence, we do not focus on any single divisive
aspect (i.e., caste, gender, religion, or other identities of the victim and
perpetrators) nor do we focus on just hate speech or mis/dis-information.
Rather, our understanding of the intersectional causes of such triggers focuses
our attempt at measuring the harm potential of online content, irrespective of
whether it is hateful or not. In this paper, we discuss the development of a
framework/annotation schema that allows annotating the data with different
aspects of the text including its socio-political grounding and intent of the
speaker (as expressed through mood and modality) that together contribute to it
being a trigger for offline harm. We also give a comparative analysis and
mapping of our framework with some of the existing frameworks.
\\ ( https://arxiv.org/abs/2403.11108 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11123
Date: Sun, 17 Mar 2024 07:07:44 GMT   (7608kb,D)

Title: Granular Change Accuracy: A More Accurate Performance Metric for
  Dialogue State Tracking
Authors: Taha Aksu and Nancy F. Chen
Categories: cs.CL
Comments: Accepted to COLING 2024
\\
  Current metrics for evaluating Dialogue State Tracking (DST) systems exhibit
three primary limitations. They: i) erroneously presume a uniform distribution
of slots throughout the dialog, ii) neglect to assign partial scores for
individual turns, iii) frequently overestimate or underestimate performance by
repeatedly counting the models' successful or failed predictions. To address
these shortcomings, we introduce a novel metric: Granular Change Accuracy
(GCA). GCA focuses on evaluating the predicted changes in dialogue state over
the entire dialogue history. Benchmarking reveals that GCA effectively reduces
biases arising from distribution uniformity and the positioning of errors
across turns, resulting in a more precise evaluation. Notably, we find that
these biases are particularly pronounced when evaluating few-shot or zero-shot
trained models, becoming even more evident as the model's error rate increases.
Hence, GCA offers significant promise, particularly for assessing models
trained with limited resources. Our GCA implementation is a useful addition to
the pool of DST metrics.
\\ ( https://arxiv.org/abs/2403.11123 ,  7608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11124
Date: Sun, 17 Mar 2024 07:08:55 GMT   (203kb,D)

Title: Scaling Data Diversity for Fine-Tuning Language Models in Human
  Alignment
Authors: Feifan Song, Bowen Yu, Hao Lang, Haiyang Yu, Fei Huang, Houfeng Wang,
  Yongbin Li
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024
\\
  Alignment with human preference prevents large language models (LLMs) from
generating misleading or toxic content while requiring high-cost human
feedback. Assuming resources of human annotation are limited, there are two
different ways of allocating considered: more diverse PROMPTS or more diverse
RESPONSES to be labeled. Nonetheless, a straightforward comparison between
their impact is absent. In this work, we first control the diversity of both
sides according to the number of samples for fine-tuning, which can directly
reflect their influence. We find that instead of numerous prompts, more
responses but fewer prompts better trigger LLMs for human alignment.
Additionally, the concept of diversity for prompts can be more complex than
responses that are typically quantified by single digits. Consequently, a new
formulation of prompt diversity is proposed, further implying a linear
correlation with the final performance of LLMs after fine-tuning. We also
leverage it on data augmentation and conduct experiments to show its effect on
different algorithms.
\\ ( https://arxiv.org/abs/2403.11124 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11128
Date: Sun, 17 Mar 2024 07:34:12 GMT   (1512kb,D)

Title: Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'
  API Invocation Capabilities
Authors: Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou,
  Wanxiang Che
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  With the rise of Large Language Models (LLMs), AI assistants' ability to
utilize tools, especially through API calls, has advanced notably. This
progress has necessitated more accurate evaluation methods. Many existing
studies adopt static evaluation, where they assess AI assistants' API call
based on pre-defined dialogue histories. However, such evaluation method can be
misleading, as an AI assistant might fail in generating API calls from
preceding human interaction in real cases. Instead of the resource-intensive
method of direct human-machine interactions, we propose Automated Dynamic
Evaluation (AutoDE) to assess an assistant's API call capability without human
involvement. In our framework, we endeavor to closely mirror genuine human
conversation patterns in human-machine interactions, using a LLM-based user
agent, equipped with a user script to ensure human alignment. Experimental
results highlight that AutoDE uncovers errors overlooked by static evaluations,
aligning more closely with human assessment. Testing four AI assistants using
our crafted benchmark, our method mirrored human evaluation with an correlation
of 0.99, marking an 8% enhancement compared to conventional static evaluations.
\\ ( https://arxiv.org/abs/2403.11128 ,  1512kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11129
Date: Sun, 17 Mar 2024 07:41:58 GMT   (345kb,D)

Title: Enhancing Event Causality Identification with Rationale and
  Structure-Aware Causal Question Answering
Authors: Baiyan Zhang, Qin Chen, Jie Zhou, Jian Jin, Liang He
Categories: cs.CL
\\
  Document-level Event Causality Identification (DECI) aims to identify causal
relations between two events in documents. Recent research tends to use
pre-trained language models to generate the event causal relations. Whereas,
these methods are prone to the errors of sequential generation due to multiple
events in a document. Moreover, the potential structures such as event
coreference and related causal chain are neglected. In this paper, we propose a
multi-task learning framework to enhance event causality identification with
rationale and structure-aware causal question answering. Specifically, the DECI
task is transformed into multiple-choice question answering, and the causes and
effects of the questioned event are generated with large language models. In
addition, we generate the rationales to explain why these events have causal
relations. Moreover, we construct an event structure graph, which models the
multi-hop potential relations for causal reasoning of the current event.
Experiments on two benchmark datasets show the great advantages of our proposed
approach compared to the state-of-the-art methods. Moreover, we conduct both
quantitative and qualitative analyses, which shed light on why each component
of our approach can lead to great improvements.
\\ ( https://arxiv.org/abs/2403.11129 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11130
Date: Sun, 17 Mar 2024 07:44:44 GMT   (1876kb)

Title: Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced
  Arabic Language Models
Authors: Mohamed Taher Alrefaie, Nour Eldin Morsy, Nada Samir
Categories: cs.CL
\\
  This paper presents a comprehensive examination of the impact of tokenization
strategies and vocabulary sizes on the performance of Arabic language models in
downstream natural language processing tasks. Our investigation focused on the
effectiveness of four tokenizers across various tasks, including News
Classification, Hate Speech Detection, Sentiment Analysis, and Natural Language
Inference. Leveraging a diverse set of vocabulary sizes, we scrutinize the
intricate interplay between tokenization approaches and model performance. The
results reveal that Byte Pair Encoding (BPE) with Farasa outperforms other
strategies in multiple tasks, underscoring the significance of morphological
analysis in capturing the nuances of the Arabic language. However, challenges
arise in sentiment analysis, where dialect specific segmentation issues impact
model efficiency. Computational efficiency analysis demonstrates the stability
of BPE with Farasa, suggesting its practical viability. Our study uncovers
limited impacts of vocabulary size on model performance while keeping the model
size unchanged. This is challenging the established beliefs about the
relationship between vocabulary, model size, and downstream tasks, emphasizing
the need for the study of models' size and their corresponding vocabulary size
to generalize across domains and mitigate biases, particularly in dialect based
datasets. Paper's recommendations include refining tokenization strategies to
address dialect challenges, enhancing model robustness across diverse
linguistic contexts, and expanding datasets to encompass the rich dialect based
Arabic. This work not only advances our understanding of Arabic language models
but also lays the foundation for responsible and ethical developments in
natural language processing technologies tailored to the intricacies of the
Arabic language.
\\ ( https://arxiv.org/abs/2403.11130 ,  1876kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11145
Date: Sun, 17 Mar 2024 08:51:01 GMT   (1090kb,D)

Title: A Challenge Dataset and Effective Models for Conversational Stance
  Detection
Authors: Fuqiang Niu, Min Yang, Ang Li, Baoquan Zhang, Xiaojiang Peng and Bowen
  Zhang
Categories: cs.CL
Journal-ref: LREC-COLING 2024
\\
  Previous stance detection studies typically concentrate on evaluating stances
within individual instances, thereby exhibiting limitations in effectively
modeling multi-party discussions concerning the same specific topic, as
naturally transpire in authentic social media interactions. This constraint
arises primarily due to the scarcity of datasets that authentically replicate
real social media contexts, hindering the research progress of conversational
stance detection. In this paper, we introduce a new multi-turn conversation
stance detection dataset (called \textbf{MT-CSD}), which encompasses multiple
targets for conversational stance detection. To derive stances from this
challenging dataset, we propose a global-local attention network
(\textbf{GLAN}) to address both long and short-range dependencies inherent in
conversational data. Notably, even state-of-the-art stance detection methods,
exemplified by GLAN, exhibit an accuracy of only 50.47\%, highlighting the
persistent challenges in conversational stance detection. Furthermore, our
MT-CSD dataset serves as a valuable resource to catalyze advancements in
cross-domain stance detection, where a classifier is adapted from a different
yet related target. We believe that MT-CSD will contribute to advancing
real-world applications of stance detection research. Our source code, data,
and models are available at \url{https://github.com/nfq729/MT-CSD}.
\\ ( https://arxiv.org/abs/2403.11145 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11152
Date: Sun, 17 Mar 2024 09:05:13 GMT   (8046kb,D)

Title: Evaluation Ethics of LLMs in Legal Domain
Authors: Ruizhe Zhang, Haitao Li, Yueyue Wu, Qingyao Ai, Yiqun Liu, Min Zhang
  and Shaoping Ma
Categories: cs.CL cs.AI
Comments: 10 pages, in processing of ACL 2024
\\
  In recent years, the utilization of large language models for natural
language dialogue has gained momentum, leading to their widespread adoption
across various domains. However, their universal competence in addressing
challenges specific to specialized fields such as law remains a subject of
scrutiny. The incorporation of legal ethics into the model has been overlooked
by researchers. We asserts that rigorous ethic evaluation is essential to
ensure the effective integration of large language models in legal domains,
emphasizing the need to assess domain-specific proficiency and domain-specific
ethic. To address this, we propose a novelty evaluation methodology, utilizing
authentic legal cases to evaluate the fundamental language abilities,
specialized legal knowledge and legal robustness of large language models
(LLMs). The findings from our comprehensive evaluation contribute significantly
to the academic discourse surrounding the suitability and performance of large
language models in legal domains.
\\ ( https://arxiv.org/abs/2403.11152 ,  8046kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11169
Date: Sun, 17 Mar 2024 10:59:09 GMT   (28311kb,D)

Title: Correcting misinformation on social media with a large language model
Authors: Xinyi Zhou, Ashish Sharma, Amy X. Zhang, Tim Althoff
Categories: cs.CL cs.AI
Comments: 50 pages
\\
  Misinformation undermines public trust in science and democracy, particularly
on social media where inaccuracies can spread rapidly. Experts and laypeople
have shown to be effective in correcting misinformation by manually identifying
and explaining inaccuracies. Nevertheless, this approach is difficult to scale,
a concern as technologies like large language models (LLMs) make misinformation
easier to produce. LLMs also have versatile capabilities that could accelerate
misinformation correction; however, they struggle due to a lack of recent
information, a tendency to produce plausible but false content and references,
and limitations in addressing multimodal information. To address these issues,
we propose MUSE, an LLM augmented with access to and credibility evaluation of
up-to-date information. By retrieving contextual evidence and refutations, MUSE
can provide accurate and trustworthy explanations and references. It also
describes visuals and conducts multimodal searches for correcting multimodal
misinformation. We recruit fact-checking and journalism experts to evaluate
corrections to real social media posts across 13 dimensions, ranging from the
factuality of explanation to the relevance of references. The results
demonstrate MUSE's ability to correct misinformation promptly after appearing
on social media; overall, MUSE outperforms GPT-4 by 37% and even high-quality
corrections from laypeople by 29%. This work underscores the potential of LLMs
to combat real-world misinformation effectively and efficiently.
\\ ( https://arxiv.org/abs/2403.11169 ,  28311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11183
Date: Sun, 17 Mar 2024 12:12:33 GMT   (9052kb,D)

Title: Decoding Continuous Character-based Language from Non-invasive Brain
  Recordings
Authors: Cenyuan Zhang, Xiaoqing Zheng, Ruicheng Yin, Shujie Geng, Jianhan Xu,
  Xuan Gao, Changze Lv, Zixuan Ling, Xuanjing Huang, Miao Cao, Jianfeng Feng
Categories: cs.CL
\\
  Deciphering natural language from brain activity through non-invasive devices
remains a formidable challenge. Previous non-invasive decoders either require
multiple experiments with identical stimuli to pinpoint cortical regions and
enhance signal-to-noise ratios in brain activity, or they are limited to
discerning basic linguistic elements such as letters and words. We propose a
novel approach to decoding continuous language from single-trial non-invasive
fMRI recordings, in which a three-dimensional convolutional network augmented
with information bottleneck is developed to automatically identify responsive
voxels to stimuli, and a character-based decoder is designed for the semantic
reconstruction of continuous language characterized by inherent character
structures. The resulting decoder can produce intelligible textual sequences
that faithfully capture the meaning of perceived speech both within and across
subjects, while existing decoders exhibit significantly inferior performance in
cross-subject contexts. The ability to decode continuous language from single
trials across subjects demonstrates the promising applications of non-invasive
language brain-computer interfaces in both healthcare and neuroscience.
\\ ( https://arxiv.org/abs/2403.11183 ,  9052kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11203
Date: Sun, 17 Mar 2024 13:04:35 GMT   (2793kb,D)

Title: TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced
  Language Models
Authors: Junbing Yan, Chengyu Wang, Taolin Zhang, Xiaofeng He, Jun Huang,
  Longtao Huang, Hui Xue and Wei Zhang
Categories: cs.CL
\\
  KEPLMs are pre-trained models that utilize external knowledge to enhance
language understanding. Previous language models facilitated knowledge
acquisition by incorporating knowledge-related pre-training tasks learned from
relation triples in knowledge graphs. However, these models do not prioritize
learning embeddings for entity-related tokens. Moreover, updating the entire
set of parameters in KEPLMs is computationally demanding. This paper introduces
TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced
Language Models. We observe that entities in text corpora usually follow the
long-tail distribution, where the representations of some entities are
suboptimally optimized and hinder the pre-training process for KEPLMs. To
tackle this, we employ a robust approach to inject knowledge triples and employ
a knowledge-augmented memory bank to capture valuable information. Furthermore,
updating a small subset of neurons in the feed-forward networks (FFNs) that
store factual knowledge is both sufficient and efficient. Specifically, we
utilize dynamic knowledge routing to identify knowledge paths in FFNs and
selectively update parameters during pre-training. Experimental results show
that TRELM reduces pre-training time by at least 50% and outperforms other
KEPLMs in knowledge probing tasks and multiple knowledge-aware language
understanding tasks.
\\ ( https://arxiv.org/abs/2403.11203 ,  2793kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11209
Date: Sun, 17 Mar 2024 13:21:33 GMT   (10413kb,D)

Title: Creating an African American-Sounding TTS: Guidelines, Technical
  Challenges,and Surprising Evaluations
Authors: Claudio Pinhanez, Raul Fernandez, Marcelo Grave, Julio Nogima, Ron
  Hoory
Categories: cs.CL cs.HC
Comments: Full version including appendixes
\\
  Representations of AI agents in user interfaces and robotics are
predominantly White, not only in terms of facial and skin features, but also in
the synthetic voices they use. In this paper we explore some unexpected
challenges in the representation of race we found in the process of developing
an U.S. English Text-to-Speech (TTS) system aimed to sound like an educated,
professional, regional accent-free African American woman. The paper starts by
presenting the results of focus groups with African American IT professionals
where guidelines and challenges for the creation of a representative and
appropriate TTS system were discussed and gathered, followed by a discussion
about some of the technical difficulties faced by the TTS system developers. We
then describe two studies with U.S. English speakers where the participants
were not able to attribute the correct race to the African American TTS voice
while overwhelmingly correctly recognizing the race of a White TTS system of
similar quality. A focus group with African American IT workers not only
confirmed the representativeness of the African American voice we built, but
also suggested that the surprising recognition results may have been caused by
the inability or the latent prejudice from non-African Americans to associate
educated, non-vernacular, professionally-sounding voices to African American
people.
\\ ( https://arxiv.org/abs/2403.11209 ,  10413kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11227
Date: Sun, 17 Mar 2024 14:21:42 GMT   (633kb,D)

Title: Cheap Ways of Extracting Clinical Markers from Texts
Authors: Anastasia Sandu, Teodor Mihailescu, Sergiu Nisioi
Categories: cs.CL cs.LG
Comments: https://github.com/nlp-unibuc/clpsych24-task
\\
  This paper describes the work of the UniBuc Archaeology team for CLPsych's
2024 Shared Task, which involved finding evidence within the text supporting
the assigned suicide risk level. Two types of evidence were required:
highlights (extracting relevant spans within the text) and summaries
(aggregating evidence into a synthesis). Our work focuses on evaluating Large
Language Models (LLM) as opposed to an alternative method that is much more
memory and resource efficient. The first approach employs a good old-fashioned
machine learning (GOML) pipeline consisting of a tf-idf vectorizer with a
logistic regression classifier, whose representative features are used to
extract relevant highlights. The second, more resource intensive, uses an LLM
for generating the summaries and is guided by chain-of-thought to provide
sequences of text indicating clinical markers.
\\ ( https://arxiv.org/abs/2403.11227 ,  633kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11236
Date: Sun, 17 Mar 2024 14:49:09 GMT   (7562kb,D)

Title: ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart
  Summarization
Authors: Mengsha Liu, Daoyuan Chen, Yaliang Li, Guian Fang, Ying Shen
Categories: cs.CL
\\
  Data visualization serves as a critical means for presenting data and mining
its valuable insights. The task of chart summarization, through natural
language processing techniques, facilitates in-depth data analysis of charts.
However, there still are notable deficiencies in terms of visual-language
matching and reasoning ability for existing approaches. To address these
limitations, this study constructs a large-scale dataset of comprehensive
chart-caption pairs and fine-tuning instructions on each chart. Thanks to the
broad coverage of various topics and visual styles within this dataset, better
matching degree can be achieved from the view of training data. Moreover, we
propose an innovative chart summarization method, ChartThinker, which
synthesizes deep analysis based on chains of thought and strategies of context
retrieval, aiming to improve the logical coherence and accuracy of the
generated summaries. Built upon the curated datasets, our trained model
consistently exhibits superior performance in chart summarization tasks,
surpassing 8 state-of-the-art models over 7 evaluation metrics. Our dataset and
codes are publicly accessible.
\\ ( https://arxiv.org/abs/2403.11236 ,  7562kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11297
Date: Sun, 17 Mar 2024 18:39:14 GMT   (247kb)

Title: A Modified Word Saliency-Based Adversarial Attack on Text Classification
  Models
Authors: Hetvi Waghela, Sneha Rakshit, Jaydip Sen
Categories: cs.CL cs.CR cs.LG
Comments: The paper is a preprint of a version submitted in ICCIDA 2024. It
  consists of 10 pages and contains 7 tables
\\
  This paper introduces a novel adversarial attack method targeting text
classification models, termed the Modified Word Saliency-based Adversarial
At-tack (MWSAA). The technique builds upon the concept of word saliency to
strategically perturb input texts, aiming to mislead classification models
while preserving semantic coherence. By refining the traditional adversarial
attack approach, MWSAA significantly enhances its efficacy in evading detection
by classification systems. The methodology involves first identifying salient
words in the input text through a saliency estimation process, which
prioritizes words most influential to the model's decision-making process.
Subsequently, these salient words are subjected to carefully crafted
modifications, guided by semantic similarity metrics to ensure that the altered
text remains coherent and retains its original meaning. Empirical evaluations
conducted on diverse text classification datasets demonstrate the effectiveness
of the proposed method in generating adversarial examples capable of
successfully deceiving state-of-the-art classification models. Comparative
analyses with existing adversarial attack techniques further indicate the
superiority of the proposed approach in terms of both attack success rate and
preservation of text coherence.
\\ ( https://arxiv.org/abs/2403.11297 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11311
Date: Sun, 17 Mar 2024 19:12:26 GMT   (551kb,D)

Title: Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding
Authors: Zichen Wu, HsiuYuan Huang, Fanyi Qu and Yunfang Wu
Categories: cs.CL cs.MM
Comments: LREC-COLING 2024, Long Paper
\\
  Deep multimodal semantic understanding that goes beyond the mere superficial
content relation mining has received increasing attention in the realm of
artificial intelligence. The challenges of collecting and annotating
high-quality multi-modal data have underscored the significance of few-shot
learning. In this paper, we focus on two critical tasks under this context:
few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis
(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware
Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on
the unified vision-language model (VLM). Specifically, we design three experts
of soft prompts: a text prompt and an image prompt that extract
modality-specific features to enrich the single-modal representation, and a
unified prompt to assist multi-modal interaction. Additionally, we reorganize
Transformer layers into several blocks and introduce cross-modal prompt
attention between adjacent blocks, which smoothens the transition from
single-modal representation to multi-modal fusion. On both MSD and MSA datasets
in few-shot setting, our proposed model not only surpasses the 8.2B model
InstructBLIP with merely 2% parameters (150M), but also significantly
outperforms other widely-used prompt methods on VLMs or task-specific methods.
\\ ( https://arxiv.org/abs/2403.11311 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11317
Date: Sun, 17 Mar 2024 19:44:05 GMT   (1572kb,D)

Title: Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches
Authors: Igor Sterner, Weizhe Lin, Jinghong Chen, Bill Byrne
Categories: cs.CL cs.CV
\\
  Two approaches have emerged to input images into large language models
(LLMs). The first is to caption images into natural language. The second is to
map image feature embeddings into the domain of the LLM and pass the mapped
embeddings directly to the LLM. The majority of recent few-shot multimodal work
reports performance using architectures that employ variations of one of these
two approaches. But they overlook an important comparison between them. We
design a controlled and focused experiment to compare these two approaches to
few-shot visual question answering (VQA) with LLMs. Our findings indicate that
for Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to
the LLM embedding space does not guarantee improved performance over using
image captions. In the zero-shot regime, we find using textual image captions
is better. In the few-shot regimes, how the in-context examples are selected
determines which is better.
\\ ( https://arxiv.org/abs/2403.11317 ,  1572kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11322
Date: Sun, 17 Mar 2024 19:54:16 GMT   (2571kb,D)

Title: StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
Authors: Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, Qingyun Wu
Categories: cs.CL cs.AI
\\
  It is a notable trend to use Large Language Models (LLMs) to tackle complex
tasks, e.g., tasks that require a sequence of actions and dynamic interaction
with tools and environments. In this paper, we propose StateFlow, a novel
LLM-based task-solving paradigm that conceptualizes complex task-solving
processes backed by LLMs as state machines. With proper construction of states
and definition of state transitions, StateFlow grounds the progress of
task-solving, ensuring clear tracking and management of LLMs' responses
throughout the task-solving process. Within each state, StateFlow allows
execution of a series of actions, involving not only the generation of LLM's
responses guided by a specific prompt, but also the utilization of external
tools as needed. State transitions are controlled by specific rules or
decisions made by the LLM, allowing for a dynamic and adaptive progression
through the task's pre-defined StateFlow model. Evaluations on the InterCode
SQL and Bash benchmarks show that StateFlow significantly enhances LLMs'
efficiency.
\\ ( https://arxiv.org/abs/2403.11322 ,  2571kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11330
Date: Sun, 17 Mar 2024 20:21:26 GMT   (7100kb,D)

Title: Improving Dialogue Agents by Decomposing One Global Explicit Annotation
  with Local Implicit Multimodal Feedback
Authors: Dong Won Lee, Hae Won Park, Yoon Kim, Cynthia Breazeal, Louis-Philippe
  Morency
Categories: cs.CL cs.AI cs.HC cs.LG
Comments: 9 pages, 3 figures, 2 tables
\\
  We describe an approach for aligning an LLM-based dialogue agent based on
global (i.e., dialogue-level) rewards, while also taking into account
naturally-occurring multimodal signals. At a high level, our approach (dubbed
GELI) learns a local, turn-level reward model by decomposing the human-provided
Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal
reward signals to crossmodally shape the reward decomposition step. This
decomposed reward model is then used as part of the standard RHLF pipeline
improve an LLM-based dialog agent. We run quantitative and qualitative human
studies to evaluate the performance of our GELI approach, and find that it
shows consistent improvements across various conversational metrics compared to
baseline methods.
\\ ( https://arxiv.org/abs/2403.11330 ,  7100kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11346
Date: Sun, 17 Mar 2024 21:16:17 GMT   (1161kb,D)

Title: CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using
  Synthetic Back-Translation Data
Authors: Kung Yin Hong, Lifeng Han, Riza Batista-Navarro, Goran Nenadic
Categories: cs.CL cs.AI
\\
  Neural Machine Translation (NMT) for low-resource languages is still a
challenging task in front of NLP researchers. In this work, we deploy a
standard data augmentation methodology by back-translation to a new language
translation direction Cantonese-to-English. We present the models we fine-tuned
using the limited amount of real data and the synthetic data we generated using
back-translation including OpusMT, NLLB, and mBART. We carried out automatic
evaluation using a range of different metrics including lexical-based and
embedding-based. Furthermore. we create a user-friendly interface for the
models we included in this\textsc{ CantonMT} research project and make it
available to facilitate Cantonese-to-English MT research. Researchers can add
more models into this platform via our open-source\textsc{ CantonMT} toolkit
\url{https://github.com/kenrickkung/CantoneseTranslation}.
\\ ( https://arxiv.org/abs/2403.11346 ,  1161kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11369
Date: Sun, 17 Mar 2024 23:18:40 GMT   (5113kb,D)

Title: What Makes Math Word Problems Challenging for LLMs?
Authors: KV Aditya Srivatsa and Ekaterina Kochmar
Categories: cs.CL
Comments: Accepted to NAACL Findings 2024
\\
  This paper investigates the question of what makes math word problems (MWPs)
challenging for large language models (LLMs). We conduct an in-depth analysis
of the key linguistic and mathematical characteristics of MWPs. In addition, we
train feature-based classifiers to better understand the impact of each feature
on the overall difficulty of MWPs for prominent LLMs and investigate whether
this helps predict how well LLMs fare against specific categories of MWPs.
\\ ( https://arxiv.org/abs/2403.11369 ,  5113kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11399
Date: Mon, 18 Mar 2024 01:14:47 GMT   (12284kb,D)

Title: X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment
Authors: Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim,
  Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim
Categories: cs.CL
\\
  The impressive development of large language models (LLMs) is expanding into
the realm of large multimodal models (LMMs), which incorporate multiple types
of data beyond text. However, the nature of multimodal models leads to
significant expenses in the creation of training data. Furthermore,
constructing multilingual data for LMMs presents its own set of challenges due
to language diversity and complexity. Therefore, in this study, we propose two
cost-effective methods to solve this problem: (1) vocabulary expansion and
pretraining of multilingual LLM for specific languages, and (2) automatic and
elaborate construction of multimodal datasets using GPT4-V. Based on015 these
methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal
training dataset. Additionally, we developed a bilingual multimodal model that
exhibits excellent performance in both Korean and English, surpassing existing
approaches.
\\ ( https://arxiv.org/abs/2403.11399 ,  12284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11413
Date: Mon, 18 Mar 2024 02:01:58 GMT   (132kb,D)

Title: Dynamic Contexts for Generating Suggestion Questions in RAG Based
  Conversational Systems
Authors: Anuja Tayal and Aman Tyagi
Categories: cs.CL
Comments: 4 pages , 1 figure, accepted at WWW'24 workshop PromptEng
DOI: 10.1145/3589335.3651905
\\
  When interacting with Retrieval-Augmented Generation (RAG)-based
conversational agents, the users must carefully craft their queries to be
understood correctly. Yet, understanding the system's capabilities can be
challenging for the users, leading to ambiguous questions that necessitate
further clarification. This work aims to bridge the gap by developing a
suggestion question generator. To generate suggestion questions, our approach
involves utilizing dynamic context, which includes both dynamic few-shot
examples and dynamically retrieved contexts. Through experiments, we show that
the dynamic contexts approach can generate better suggestion questions as
compared to other prompting approaches.
\\ ( https://arxiv.org/abs/2403.11413 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11430
Date: Mon, 18 Mar 2024 02:53:49 GMT   (272kb,D)

Title: A Novel Paradigm Boosting Translation Capabilities of Large Language
  Models
Authors: Jiaxin Guo, Hao Yang, Zongyao Li, Daimeng Wei, Hengchao Shang, Xiaoyu
  Chen
Categories: cs.CL
Comments: Accepted in NAACL 2024
\\
  This paper presents a study on strategies to enhance the translation
capabilities of large language models (LLMs) in the context of machine
translation (MT) tasks. The paper proposes a novel paradigm consisting of three
stages: Secondary Pre-training using Extensive Monolingual Data, Continual
Pre-training with Interlinear Text Format Documents, and Leveraging
Source-Language Consistent Instruction for Supervised Fine-Tuning. Previous
research on LLMs focused on various strategies for supervised fine-tuning
(SFT), but their effectiveness has been limited. While traditional machine
translation approaches rely on vast amounts of parallel bilingual data, our
paradigm highlights the importance of using smaller sets of high-quality
bilingual data. We argue that the focus should be on augmenting LLMs'
cross-lingual alignment abilities during pre-training rather than solely
relying on extensive bilingual data during SFT. Experimental results conducted
using the Llama2 model, particularly on Chinese-Llama2 after monolingual
augmentation, demonstrate the improved translation capabilities of LLMs. A
significant contribution of our approach lies in Stage2: Continual Pre-training
with Interlinear Text Format Documents, which requires less than 1B training
data, making our method highly efficient. Additionally, in Stage3, we observed
that setting instructions consistent with the source language benefits the
supervised fine-tuning process. Experimental results demonstrate that our
approach surpasses previous work and achieves superior performance compared to
models such as NLLB-54B and GPT3.5-text-davinci-003, despite having a
significantly smaller parameter count of only 7B or 13B. This achievement
establishes our method as a pioneering strategy in the field of machine
translation.
\\ ( https://arxiv.org/abs/2403.11430 ,  272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11435
Date: Mon, 18 Mar 2024 03:10:36 GMT   (9850kb,D)

Title: InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning
  Large Language Models with Instructions
Authors: Yifan Wang, Yafei Liu, Chufan Shi, Haoling Li, Chen Chen, Haonan Lu,
  Yujiu Yang
Categories: cs.CL
Comments: Accepted by NAACL 2024
\\
  Instruction tuning effectively optimizes Large Language Models (LLMs) for
downstream tasks. Due to the changing environment in real-life applications,
LLMs necessitate continual task-specific adaptation without catastrophic
forgetting. Considering the heavy computational cost, replay-based Continual
Learning (CL) methods are the simplest and most widely used for LLMs to address
the forgetting issue. However, traditional replay-based methods do not fully
utilize instructions to customize the replay strategy. In this work, we propose
a novel paradigm called Instruction-based Continual Learning (InsCL). InsCL
dynamically replays previous data based on task similarity, calculated by
Wasserstein Distance with instructions. Moreover, we further introduce an
Instruction Information Metric (InsInfo) to quantify the complexity and
diversity of instructions. According to InsInfo, InsCL guides the replay
process more inclined to high-quality data. We conduct extensive experiments
over 16 tasks with different training orders, observing consistent performance
improvements of InsCL. When all tasks have been trained, InsCL achieves
performance gains of 3.0 Relative Gain compared with Random Replay, and 27.96
Relative Gain compared with No Replay.
\\ ( https://arxiv.org/abs/2403.11435 ,  9850kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11439
Date: Mon, 18 Mar 2024 03:26:18 GMT   (932kb,D)

Title: StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized
  Dialogue Generation
Authors: Jinpeng Li, Zekai Zhang, Quan Tu, Xin Cheng, Dongyan Zhao, Rui Yan
Categories: cs.CL
\\
  Large Language Models (LLMs) demonstrate superior performance in generative
scenarios and have attracted widespread attention. Among them, stylized
dialogue generation is essential in the context of LLMs for building
intelligent and engaging dialogue agent. However the ability of LLMs is
data-driven and limited by data bias, leading to poor performance on specific
tasks. In particular, stylized dialogue generation suffers from a severe lack
of supervised data. Furthermore, although many prompt-based methods have been
proposed to accomplish specific tasks, their performance in complex real-world
scenarios involving a wide variety of dialog styles further enhancement. In
this work, we first introduce a stylized dialogue dataset StyleEval with 38
styles by leveraging the generative power of LLMs comprehensively, which has
been carefully constructed with rigorous human-led quality control. Based on
this, we propose the stylized dialogue framework StyleChat via
recitation-augmented memory strategy and multi-task style learning strategy to
promote generalization ability. To evaluate the effectiveness of our approach,
we created a test benchmark that included both a generation task and a choice
task to comprehensively evaluate trained models and assess whether styles and
preferences are remembered and understood. Experimental results show that our
proposed framework StyleChat outperforms all the baselines and helps to break
the style boundary of LLMs.
\\ ( https://arxiv.org/abs/2403.11439 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11456
Date: Mon, 18 Mar 2024 04:12:35 GMT   (9901kb,D)

Title: HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive
  Speech Detection via Large Language Models
Authors: Huy Nghiem, Hal Daum\'e III
Categories: cs.CL cs.AI cs.SI
Comments: Preprint
\\
  The ubiquitousness of social media has led to the need for reliable and
efficient detection of offensive content to limit harmful effects. This has led
to a proliferation of datasets and models related to detecting offensive
content. While sophisticated models have attained strong performance on
individual datasets, these models often do not generalize due to differences
between how "offensive content" is conceptualized, and the resulting
differences in how these datasets are labeled. In this paper, we introduce
HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with
explanations generated by GPT-3.5-Turbo and human-curated. We show that
pre-training models for the detection of offensive content on HateCOT
significantly boots open-sourced Language Models on three benchmark datasets in
both zero and few-shot settings, despite differences in domain and task.} We
further find that HateCOT enables effective K-shot fine-tuning in the
low-resource settings.
\\ ( https://arxiv.org/abs/2403.11456 ,  9901kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11473
Date: Mon, 18 Mar 2024 04:45:44 GMT   (865kb,D)

Title: Word Order's Impacts: Insights from Reordering and Generation Analysis
Authors: Qinghua Zhao, Jiaang Li, Lei Li, Zenghui Zhou, Junfeng Liu
Categories: cs.CL cs.AI
\\
  Existing works have studied the impacts of the order of words within natural
text. They usually analyze it by destroying the original order of words to
create a scrambled sequence, and then comparing the models' performance between
the original and scrambled sequences. The experimental results demonstrate
marginal drops. Considering this findings, different hypothesis about word
order is proposed, including ``the order of words is redundant with lexical
semantics'', and ``models do not rely on word order''. In this paper, we
revisit the aforementioned hypotheses by adding a order reconstruction
perspective, and selecting datasets of different spectrum. Specifically, we
first select four different datasets, and then design order reconstruction and
continuing generation tasks. Empirical findings support that ChatGPT relies on
word order to infer, but cannot support or negate the redundancy relations
between word order lexical semantics.
\\ ( https://arxiv.org/abs/2403.11473 ,  865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11509
Date: Mon, 18 Mar 2024 06:30:41 GMT   (533kb,D)

Title: DEE: Dual-stage Explainable Evaluation Method for Text Generation
Authors: Shenyu Zhang, Yu Li, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu,
  Guilin Qi
Categories: cs.CL
Comments: Accepted by DASFAA 2024
\\
  Automatic methods for evaluating machine-generated texts hold significant
importance due to the expanding applications of generative systems.
Conventional methods tend to grapple with a lack of explainability, issuing a
solitary numerical score to signify the assessment outcome. Recent advancements
have sought to mitigate this limitation by incorporating large language models
(LLMs) to offer more detailed error analyses, yet their applicability remains
constrained, particularly in industrial contexts where comprehensive error
coverage and swift detection are paramount. To alleviate these challenges, we
introduce DEE, a Dual-stage Explainable Evaluation method for estimating the
quality of text generation. Built upon Llama 2, DEE follows a dual-stage
principle guided by stage-specific instructions to perform efficient
identification of errors in generated texts in the initial stage and
subsequently delves into providing comprehensive diagnostic reports in the
second stage. DEE is fine-tuned on our elaborately assembled dataset AntEval,
which encompasses 15K examples from 4 real-world applications of Alipay that
employ generative systems. The dataset concerns newly emerged issues like
hallucination and toxicity, thereby broadening the scope of DEE's evaluation
criteria. Experimental results affirm that DEE's superiority over existing
evaluation methods, achieving significant improvements in both human
correlation as well as efficiency.
\\ ( https://arxiv.org/abs/2403.11509 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11558
Date: Mon, 18 Mar 2024 08:18:37 GMT   (8423kb,D)

Title: Reinforcement Learning with Token-level Feedback for Controllable Text
  Generation
Authors: Wendi Li, Wei Wei, Kaihe Xu, Wenfeng Xie, Dangyang Chen, Yu Cheng
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Findings
\\
  To meet the requirements of real-world applications, it is essential to
control generations of large language models (LLMs). Prior research has tried
to introduce reinforcement learning (RL) into controllable text generation
while most existing methods suffer from overfitting issues (finetuning-based
methods) or semantic collapse (post-processing methods). However, current RL
methods are generally guided by coarse-grained (sentence/paragraph-level)
feedback, which may lead to suboptimal performance owing to semantic twists or
progressions within sentences. To tackle that, we propose a novel reinforcement
learning algorithm named TOLE which formulates TOken-LEvel rewards for
controllable text generation, and employs a "first-quantize-then-noise"
paradigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be
flexibly extended to multiple constraints with little computational expense.
Experimental results show that our algorithm can achieve superior performance
on both single-attribute and multi-attribute control tasks. We have released
our codes at https://github.com/WindyLee0822/CTG
\\ ( https://arxiv.org/abs/2403.11558 ,  8423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11621
Date: Mon, 18 Mar 2024 09:55:01 GMT   (2307kb,D)

Title: Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large
  Language Model
Authors: Haoyun Xu, Runzhe Zhan, Derek F. Wong, Lidia S. Chao
Categories: cs.CL
\\
  Large Language Models (LLMs) are composed of neurons that exhibit various
behaviors and roles, which become increasingly diversified as models scale.
Recent studies have revealed that not all neurons are active across different
datasets, and this sparsity correlates positively with the task-specific
ability, leading to advancements in model pruning and training efficiency.
Traditional fine-tuning methods engage all parameters of LLMs, which is
computationally expensive and may not be necessary. In contrast,
Parameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of
trainable parameters, yet they still operate at a relatively macro scale (e.g.,
layer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach
that refines the granularity of parameter training down to the individual
neuron, enabling more precise and computationally efficient model updates. The
experimental results show that NeFT not only exceeded the performance of
full-parameter fine-tuning and PEFT but also provided insights into the
analysis of neurons.
\\ ( https://arxiv.org/abs/2403.11621 ,  2307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11747
Date: Mon, 18 Mar 2024 12:58:16 GMT   (341kb,D)

Title: Embedded Named Entity Recognition using Probing Classifiers
Authors: Nicholas Popovi\v{c} and Michael F\"arber
Categories: cs.CL
\\
  Extracting semantic information from generated text is a useful tool for
applications such as automated fact checking or retrieval augmented generation.
Currently, this requires either separate models during inference, which
increases computational cost, or destructive fine-tuning of the language model.
Instead, we propose directly embedding information extraction capabilities into
pre-trained language models using probing classifiers, enabling efficient
simultaneous text generation and information extraction. For this, we introduce
an approach called EMBER and show that it enables named entity recognition in
decoder-only language models without fine-tuning them and while incurring
minimal additional computational cost at inference time. Specifically, our
experiments using GPT-2 show that EMBER maintains high token generation rates
during streaming text generation, with only a negligible decrease in speed of
around 1% compared to a 43.64% slowdown measured for a baseline using a
separate NER model. Code and data are available at
https://github.com/nicpopovic/EMBER.
\\ ( https://arxiv.org/abs/2403.11747 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11752
Date: Mon, 18 Mar 2024 13:02:02 GMT   (221kb,D)

Title: Revisiting The Classics: A Study on Identifying and Rectifying Gender
  Stereotypes in Rhymes and Poems
Authors: Aditya Narayan Sankaran, Vigneshwaran Shankaran, Sampath Lonka, Rajesh
  Sharma
Categories: cs.CL
Comments: Accepted to appear at LREC-COLING 2024
\\
  Rhymes and poems are a powerful medium for transmitting cultural norms and
societal roles. However, the pervasive existence of gender stereotypes in these
works perpetuates biased perceptions and limits the scope of individuals'
identities. Past works have shown that stereotyping and prejudice emerge in
early childhood, and developmental research on causal mechanisms is critical
for understanding and controlling stereotyping and prejudice. This work
contributes by gathering a dataset of rhymes and poems to identify gender
stereotypes and propose a model with 97\% accuracy to identify gender bias.
Gender stereotypes were rectified using a Large Language Model (LLM) and its
effectiveness was evaluated in a comparative survey against human educator
rectifications. To summarize, this work highlights the pervasive nature of
gender stereotypes in literary works and reveals the potential of LLMs to
rectify gender stereotypes. This study raises awareness and promotes
inclusivity within artistic expressions, making a significant contribution to
the discourse on gender equality.
\\ ( https://arxiv.org/abs/2403.11752 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11786
Date: Mon, 18 Mar 2024 13:44:48 GMT   (2212kb,D)

Title: Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained
  Large Language Models
Authors: Preetha Datta, Fedor Vitiugin, Anastasiia Chizhikova, Nitin Sawhney
Categories: cs.CL cs.AI
Comments: 5 pages + references
\\
  Extracting hyper-relations is crucial for constructing comprehensive
knowledge graphs, but there are limited supervised methods available for this
task. To address this gap, we introduce a zero-shot prompt-based method using
OpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.
Comparing our model with a baseline, we achieved promising results, with a
recall of 0.77. Although our precision is currently lower, a detailed analysis
of the model outputs has uncovered potential pathways for future research in
this area.
\\ ( https://arxiv.org/abs/2403.11786 ,  2212kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11793
Date: Mon, 18 Mar 2024 13:50:50 GMT   (8423kb,D)

Title: Reasoning Abilities of Large Language Models: In-Depth Analysis on the
  Abstraction and Reasoning Corpus
Authors: Seungpil Lee and Woochang Sim and Donghyeon Shin and Sanha Hwang and
  Wongyu Seo and Jiwon Park and Seokki Lee and Sejin Kim and Sundong Kim
Categories: cs.CL cs.AI cs.ET cs.SC
Comments: 25 pages
\\
  The existing methods for evaluating the inference abilities of Large Language
Models (LLMs) have been results-centric, making it difficult to assess the
inference process. We introduce a new approach using the Abstract and Reasoning
Corpus (ARC) dataset to evaluate the inference and contextual understanding
abilities of large language models in a process-centric manner. ARC demands
rigorous logical structures for problem-solving, making it a benchmark that
facilitates the comparison of model inference abilities with humans.
Experimental results confirm that while large language models possess weak
inference abilities, they still lag in terms of logical coherence,
compositionality, and productivity. Our experiments highlight the reasoning
capabilities of LLMs, proposing development paths for achieving human-level
reasoning.
\\ ( https://arxiv.org/abs/2403.11793 ,  8423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11802
Date: Mon, 18 Mar 2024 14:01:45 GMT   (929kb,D)

Title: Counting-Stars: A Simple, Efficient, and Reasonable Strategy for
  Evaluating Long-Context Large Language Models
Authors: Mingyang Song, Mao Zheng, Xuan Luo
Categories: cs.CL
Comments: a technical report
\\
  While recent research endeavors have concentrated on developing Large
Language Models (LLMs) with robust long-context capabilities, due to the lack
of appropriate evaluation strategies, relatively little is known about how well
the long-context processing abilities and performance of leading LLMs (e.g.,
ChatGPT and KimiChat). To address this gap, we propose a simple, efficient, and
reasonable strategy for evaluating long-context LLMs as a new benchmark, named
Counting-Stars. The Counting-Stars is designed to require LLMs to fully
understand and capture long dependencies in long contexts and be able to
collect inter-dependency across multiple pieces of evidence spanning the entire
context to finish the task. Based on the Counting-Stars, we conduct experiments
to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo and Kimi Chat.
The experimental results indicate that GPT-4 Turbo and Kimi Chat achieve
significant performance in the long context from 4K to 128K. We further present
two intriguing analyses regarding the behavior of LLMs processing long context.
\\ ( https://arxiv.org/abs/2403.11802 ,  929kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11810
Date: Mon, 18 Mar 2024 14:08:59 GMT   (8428kb,D)

Title: Metaphor Understanding Challenge Dataset for LLMs
Authors: Xiaoyu Tong and Rochelle Choenni and Martha Lewis and Ekaterina
  Shutova
Categories: cs.CL
\\
  Metaphors in natural language are a reflection of fundamental cognitive
processes such as analogical reasoning and categorisation, and are deeply
rooted in everyday communication. Metaphor understanding is therefore an
essential task for large language models (LLMs). We release the Metaphor
Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor
understanding capabilities of LLMs. The dataset provides over 10k paraphrases
for sentences containing metaphor use, as well as 1.5k instances containing
inapt paraphrases. The inapt paraphrases were carefully selected to serve as
control to determine whether the model indeed performs full metaphor
interpretation or rather resorts to lexical similarity. All apt and inapt
paraphrases were manually annotated. The metaphorical sentences cover natural
metaphor uses across 4 genres (academic, news, fiction, and conversation), and
they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5
demonstrate that MUNCH presents a challenging task for LLMs. The dataset is
freely accessible at
https://github.com/xiaoyuisrain/metaphor-understanding-challenge.
\\ ( https://arxiv.org/abs/2403.11810 ,  8428kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11833
Date: Mon, 18 Mar 2024 14:45:20 GMT   (5788kb,D)

Title: SSCAE -- Semantic, Syntactic, and Context-aware natural language
  Adversarial Examples generator
Authors: Javad Rafiei Asl, Mohammad H. Rafiei, Manar Alohaly, Daniel Takabi
Categories: cs.CL cs.CR cs.LG
Journal-ref: IEEE Transactions on Dependable and Secure Computing (2024), pp.
  1-17
DOI: 10.1109/TDSC.2024.3359817
\\
  Machine learning models are vulnerable to maliciously crafted Adversarial
Examples (AEs). Training a machine learning model with AEs improves its
robustness and stability against adversarial attacks. It is essential to
develop models that produce high-quality AEs. Developing such models has been
much slower in natural language processing (NLP) than in areas such as computer
vision. This paper introduces a practical and efficient adversarial attack
model called SSCAE for \textbf{S}emantic, \textbf{S}yntactic, and
\textbf{C}ontext-aware natural language \textbf{AE}s generator. SSCAE
identifies important words and uses a masked language model to generate an
early set of substitutions. Next, two well-known language models are employed
to evaluate the initial set in terms of semantic and syntactic characteristics.
We introduce (1) a dynamic threshold to capture more efficient perturbations
and (2) a local greedy search to generate high-quality AEs. As a black-box
method, SSCAE generates humanly imperceptible and context-aware AEs that
preserve semantic consistency and the source language's syntactical and
grammatical requirements. The effectiveness and superiority of the proposed
SSCAE model are illustrated with fifteen comparative experiments and extensive
sensitivity analysis for parameter optimization. SSCAE outperforms the existing
models in all experiments while maintaining a higher semantic consistency with
a lower query number and a comparable perturbation rate.
\\ ( https://arxiv.org/abs/2403.11833 ,  5788kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11834
Date: Mon, 18 Mar 2024 14:45:52 GMT   (7563kb,D)

Title: Towards Understanding the Relationship between In-context Learning and
  Compositional Generalization
Authors: Sungjun Han and Sebastian Pad\'o
Categories: cs.CL cs.LG
Comments: To be published in LREC-COLING 2024
\\
  According to the principle of compositional generalization, the meaning of a
complex expression can be understood as a function of the meaning of its parts
and of how they are combined. This principle is crucial for human language
processing and also, arguably, for NLP models in the face of
out-of-distribution data. However, many neural network models, including
Transformers, have been shown to struggle with compositional generalization. In
this paper, we hypothesize that forcing models to in-context learn can provide
an inductive bias to promote compositional generalization. To test this
hypothesis, we train a causal Transformer in a setting that renders ordinary
learning very difficult: we present it with different orderings of the training
instance and shuffle instance labels. This corresponds to training the model on
all possible few-shot learning problems attainable from the dataset. The model
can solve the task, however, by utilizing earlier examples to generalize to
later ones (i.e. in-context learning). In evaluations on the datasets, SCAN,
COGS, and GeoQuery, models trained in this manner indeed show improved
compositional generalization. This indicates the usefulness of in-context
learning problems as an inductive bias for generalization.
\\ ( https://arxiv.org/abs/2403.11834 ,  7563kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11838
Date: Mon, 18 Mar 2024 14:48:29 GMT   (9067kb,D)

Title: Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for
  Language Models
Authors: Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu,
  Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\
  Large Language Models (LLMs) exhibit impressive capabilities but also present
risks such as biased content generation and privacy issues. One of the current
alignment techniques includes principle-driven integration, but it faces
challenges arising from the imprecision of manually crafted rules and
inadequate risk perception in models without safety training. To address these,
we introduce Guide-Align, a two-stage approach. Initially, a safety-trained
model identifies potential risks and formulates specific guidelines for various
inputs, thereby establishing a comprehensive library of guidelines and models
for input-guidelines retrieval. Subsequently, the retrieval model correlates
new inputs with pertinent guidelines, guiding LLMs in response generation to
ensure safe and high-quality outputs, thus aligning with human values. An
additional optional stage involves fine-tuning a model with new well-aligned
datasets generated through the process implemented in the second stage. Our
method customizes guidelines to accommodate diverse inputs, thereby enhancing
the fine-grainedness and comprehensiveness of the guideline library.
Furthermore, it incorporates safety expertise from a safety-trained LLM through
a lightweight retrieval model. We evaluated our approach on three benchmarks,
demonstrating significant improvements in LLM security and quality. Notably,
our fine-tuned model, Labrador, even at 13 billion parameters, outperforms
GPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.
\\ ( https://arxiv.org/abs/2403.11838 ,  9067kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11858
Date: Mon, 18 Mar 2024 15:08:01 GMT   (146kb,D)

Title: GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management
  in Agriculture
Authors: Shanglong Yang, Zhipeng Yuan, Shunbao Li, Ruoling Peng, Kang Liu, Po
  Yang
Categories: cs.CL
\\
  In the rapidly evolving field of artificial intelligence (AI), the
application of large language models (LLMs) in agriculture, particularly in
pest management, remains nascent. We aimed to prove the feasibility by
evaluating the content of the pest management advice generated by LLMs,
including the Generative Pre-trained Transformer (GPT) series from OpenAI and
the FLAN series from Google. Considering the context-specific properties of
agricultural advice, automatically measuring or quantifying the quality of text
generated by LLMs becomes a significant challenge. We proposed an innovative
approach, using GPT-4 as an evaluator, to score the generated content on
Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and
Exhaustiveness. Additionally, we integrated an expert system based on crop
threshold data as a baseline to obtain scores for Factual Accuracy on whether
pests found in crop fields should take management action. Each model's score
was weighted by percentage to obtain a final score. The results showed that
GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories.
Furthermore, the use of instruction-based prompting containing domain-specific
knowledge proved the feasibility of LLMs as an effective tool in agriculture,
with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing
pest management suggestions.
\\ ( https://arxiv.org/abs/2403.11858 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11873
Date: Mon, 18 Mar 2024 15:26:32 GMT   (3151kb,D)

Title: CO3: Low-resource Contrastive Co-training for Generative Conversational
  Query Rewrite
Authors: Yifei Yuan, Chen Shi, Runze Wang, Liyi Chen, Renjun Hu, Zengming
  Zhang, Feijun Jiang and Wai Lam
Categories: cs.CL
Comments: Accepted to COLING 2024
\\
  Generative query rewrite generates reconstructed query rewrites using the
conversation history while rely heavily on gold rewrite pairs that are
expensive to obtain. Recently, few-shot learning is gaining increasing
popularity for this task, whereas these methods are sensitive to the inherent
noise due to limited data size. Besides, both attempts face performance
degradation when there exists language style shift between training and testing
cases. To this end, we study low-resource generative conversational query
rewrite that is robust to both noise and language style shift. The core idea is
to utilize massive unlabeled data to make further improvements via a
contrastive co-training paradigm. Specifically, we co-train two dual models
(namely Rewriter and Simplifier) such that each of them provides extra guidance
through pseudo-labeling for enhancing the other in an iterative manner. We also
leverage contrastive learning with data augmentation, which enables our model
pay more attention on the truly valuable information than the noise. Extensive
experiments demonstrate the superiority of our model under both few-shot and
zero-shot scenarios. We also verify the better generalization ability of our
model when encountering language style shift.
\\ ( https://arxiv.org/abs/2403.11873 ,  3151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11886
Date: Mon, 18 Mar 2024 15:39:14 GMT   (2691kb,D)

Title: QueryAgent: A Reliable and Efficient Reasoning Framework with
  Environmental Feedback based Self-Correction
Authors: Xiang Huang, Sitao Cheng, Shanshan Huang, Jiayu Shen, Yong Xu, Chaoyun
  Zhang, Yuzhong Qu
Categories: cs.CL cs.AI
Comments: ACL 2024 under review
\\
  Employing Large Language Models (LLMs) for semantic parsing has achieved
remarkable success. However, we find existing methods fall short in terms of
reliability and efficiency when hallucinations are encountered. In this paper,
we address these challenges with a framework called QueryAgent, which solves a
question step-by-step and performs step-wise self-correction. We introduce an
environmental feedback-based self-correction method called ERASER. Unlike
traditional approaches, ERASER leverages rich environmental feedback in the
intermediate steps to perform selective and differentiated self-correction only
when necessary. Experimental results demonstrate that QueryAgent notably
outperforms all previous few-shot methods using only one example on GrailQA and
GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms
of efficiency, including runtime, query overhead, and API invocation costs. By
leveraging ERASER, we further improve another baseline (i.e., AgentBench) by
approximately 10 points, revealing the strong transferability of our approach.
\\ ( https://arxiv.org/abs/2403.11886 ,  2691kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11894
Date: Mon, 18 Mar 2024 15:53:33 GMT   (488kb,D)

Title: From explainable to interpretable deep learning for natural language
  processing in healthcare: how far from reality?
Authors: Guangming Huang, Yunfei Long, Yingya Li, Giorgos Papanastasiou
Categories: cs.CL cs.AI cs.LG
\\
  Deep learning (DL) has substantially enhanced healthcare research by
addressing various natural language processing (NLP) tasks. Yet, the increasing
complexity of DL-based NLP methods necessitates transparent model
interpretability, or at least explainability, for reliable decision-making.
This work presents a thorough scoping review on explainable and interpretable
DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial
Intelligence) was introduced to distinguish XAI from IAI. Methods were further
categorized based on their functionality (model-, input-, output-based) and
scope (local, global). Our analysis shows that attention mechanisms were the
most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The
major challenges identified are that most XIAI do not explore "global" modeling
processes, the lack of best practices, and the unmet need for systematic
evaluation and benchmarks. Important opportunities were raised such as using
"attention" to enhance multi-modal XIAI for personalized medicine and combine
DL with causal reasoning. Our discussion encourages the integration of XIAI in
LLMs and domain-specific smaller models. Our review can stimulate further
research and benchmarks toward improving inherent IAI and engaging complex NLP
in healthcare.
\\ ( https://arxiv.org/abs/2403.11894 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11896
Date: Mon, 18 Mar 2024 15:54:46 GMT   (155kb,D)

Title: Investigating Markers and Drivers of Gender Bias in Machine Translations
Authors: Peter J Barclay and Ashkan Sami (Edinburgh Napier University)
Categories: cs.CL cs.CY cs.SE
Comments: Accepted to SANER 2024; see
  https://conf.researchr.org/home/saner-2024
\\
  Implicit gender bias in Large Language Models (LLMs) is a well-documented
problem, and implications of gender introduced into automatic translations can
perpetuate real-world biases. However, some LLMs use heuristics or
post-processing to mask such bias, making investigation difficult. Here, we
examine bias in LLMss via back-translation, using the DeepL translation API to
investigate the bias evinced when repeatedly translating a set of 56 Software
Engineering tasks used in a previous study. Each statement starts with 'she',
and is translated first into a 'genderless' intermediate language then back
into English; we then examine pronoun- choice in the back-translated texts. We
expand prior research in the following ways: (1) by comparing results across
five intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and
Hungarian; (2) by proposing a novel metric for assessing the variation in
gender implied in the repeated translations, avoiding the over-interpretation
of individual pronouns, apparent in earlier work; (3) by investigating sentence
features that drive bias; (4) and by comparing results from three time-lapsed
datasets to establish the reproducibility of the approach. We found that some
languages display similar patterns of pronoun use, falling into three loose
groups, but that patterns vary between groups; this underlines the need to work
with multiple languages. We also identify the main verb appearing in a sentence
as a likely significant driver of implied gender in the translations. Moreover,
we see a good level of replicability in the results, and establish that our
variation metric proves robust despite an obvious change in the behaviour of
the DeepL translation API during the course of the study. These results show
that the back-translation method can provide further insights into bias in
language models.
\\ ( https://arxiv.org/abs/2403.11896 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11903
Date: Mon, 18 Mar 2024 16:03:45 GMT   (194kb,D)

Title: A Closer Look at Claim Decomposition
Authors: Miriam Wanner, Seth Ebner, Zhengping Jiang, Mark Dredze, Benjamin Van
  Durme
Categories: cs.CL
\\
  As generated text becomes more commonplace, it is increasingly important to
evaluate how well-supported such text is by external knowledge sources. Many
approaches for evaluating textual support rely on some method for decomposing
text into its individual subclaims which are scored against a trusted
reference. We investigate how various methods of claim decomposition --
especially LLM-based methods -- affect the result of an evaluation approach
such as the recently proposed FActScore, finding that it is sensitive to the
decomposition method used. This sensitivity arises because such metrics
attribute overall textual support to the model that generated the text even
though error can also come from the metric's decomposition step. To measure
decomposition quality, we introduce an adaptation of FActScore, which we call
DecompScore. We then propose an LLM-based approach to generating decompositions
inspired by Bertrand Russell's theory of logical atomism and neo-Davidsonian
semantics and demonstrate its improved decomposition quality over previous
methods.
\\ ( https://arxiv.org/abs/2403.11903 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11904
Date: Mon, 18 Mar 2024 16:04:55 GMT   (8048kb,D)

Title: CICLe: Conformal In-Context Learning for Largescale Multi-Class Food
  Risk Classification
Authors: Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren
Categories: cs.CL cs.LG
\\
  Contaminated or adulterated food poses a substantial risk to human health.
Given sets of labeled web texts for training, Machine Learning and Natural
Language Processing can be applied to automatically detect such risks. We
publish a dataset of 7,546 short texts describing public food recall
announcements. Each text is manually labeled, on two granularity levels (coarse
and fine), for food products and hazards that the recall corresponds to. We
describe the dataset and benchmark naive, traditional, and Transformer models.
Based on our analysis, Logistic Regression based on a tf-idf representation
outperforms RoBERTa and XLM-R on classes with low support. Finally, we discuss
different prompting strategies and present an LLM-in-the-loop framework, based
on Conformal Prediction, which boosts the performance of the base classifier
while reducing energy consumption compared to normal prompting.
\\ ( https://arxiv.org/abs/2403.11904 ,  8048kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11921
Date: Mon, 18 Mar 2024 16:19:41 GMT   (167kb)

Title: Adaptative Bilingual Aligning Using Multilingual Sentence Embedding
Authors: Olivier Kraif
Categories: cs.CL
\\
  In this paper, we present an adaptive bitextual alignment system called
AIlign. This aligner relies on sentence embeddings to extract reliable anchor
points that can guide the alignment path, even for texts whose parallelism is
fragmentary and not strictly monotonic. In an experiment on several datasets,
we show that AIlign achieves results equivalent to the state of the art, with
quasi-linear complexity. In addition, AIlign is able to handle texts whose
parallelism and monotonicity properties are only satisfied locally, unlike
recent systems such as Vecalign or Bertalign.
\\ ( https://arxiv.org/abs/2403.11921 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11958
Date: Mon, 18 Mar 2024 16:52:54 GMT   (10823kb,D)

Title: Language Evolution with Deep Learning
Authors: Mathieu Rita, Paul Michel, Rahma Chaabouni, Olivier Pietquin, Emmanuel
  Dupoux, Florian Strub
Categories: cs.CL cs.MA
Comments: to appear in the Oxford Handbook of Approaches to Language Evolution
\\
  Computational modeling plays an essential role in the study of language
emergence. It aims to simulate the conditions and learning processes that could
trigger the emergence of a structured language within a simulated controlled
environment. Several methods have been used to investigate the origin of our
language, including agent-based systems, Bayesian agents, genetic algorithms,
and rule-based systems. This chapter explores another class of computational
models that have recently revolutionized the field of machine learning: deep
learning models. The chapter introduces the basic concepts of deep and
reinforcement learning methods and summarizes their helpfulness for simulating
language emergence. It also discusses the key findings, limitations, and recent
attempts to build realistic simulations. This chapter targets linguists and
cognitive scientists seeking an introduction to deep learning as a tool to
investigate language evolution.
\\ ( https://arxiv.org/abs/2403.11958 ,  10823kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11984
Date: Mon, 18 Mar 2024 17:21:35 GMT   (1342kb,D)

Title: Using Generative Text Models to Create Qualitative Codebooks for Student
  Evaluations of Teaching
Authors: Andrew Katz, Mitchell Gerhardt, Michelle Soledad
Categories: cs.CL cs.AI cs.HC
Comments: Natural language processing, large language models, generative AI,
  student evaluations of teaching, codebook generation, qualitative data
  analysis
\\
  Feedback is a critical aspect of improvement. Unfortunately, when there is a
lot of feedback from multiple sources, it can be difficult to distill the
information into actionable insights. Consider student evaluations of teaching
(SETs), which are important sources of feedback for educators. They can give
instructors insights into what worked during a semester. A collection of SETs
can also be useful to administrators as signals for courses or entire programs.
However, on a large scale as in high-enrollment courses or administrative
records over several years, the volume of SETs can render them difficult to
analyze. In this paper, we discuss a novel method for analyzing SETs using
natural language processing (NLP) and large language models (LLMs). We
demonstrate the method by applying it to a corpus of 5,000 SETs from a large
public university. We show that the method can be used to extract, embed,
cluster, and summarize the SETs to identify the themes they express. More
generally, this work illustrates how to use the combination of NLP techniques
and LLMs to generate a codebook for SETs. We conclude by discussing the
implications of this method for analyzing SETs and other types of student
writing in teaching and research settings.
\\ ( https://arxiv.org/abs/2403.11984 ,  1342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12014
Date: Mon, 18 Mar 2024 17:51:16 GMT   (1122kb,D)

Title: EnvGen: Generating and Adapting Environments via LLMs for Training
  Embodied Agents
Authors: Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal
Categories: cs.CL cs.AI cs.LG
Comments: First two authors contributed equally; Project website:
  https://envgen-llm.github.io/
\\
  Recent SOTA approaches for embodied learning via interaction directly employ
large language models (LLMs) as agents to determine the next steps in an
environment. Due to their world knowledge and reasoning capabilities, LLM
agents achieve stronger performance than previous smaller agents based on
reinforcement learning (RL); however, frequently calling LLMs is slow and
expensive. Instead of directly employing LLMs as agents, can we use LLMs'
reasoning capabilities to adaptively create training environments to help
smaller embodied RL agents learn useful skills that they are weak at? We
propose EnvGen, a novel framework to address this question. First, we prompt an
LLM to generate training environments that allow agents to quickly learn
different tasks in parallel. Concretely, the LLM is given the task description
and simulator objectives that the agents should learn and is then asked to
generate a set of environment configurations (e.g., different terrains, items
given to agents, etc.). Next, we train a small RL agent in a mixture of the
original and LLM-generated environments. Then, we enable the LLM to
continuously adapt the generated environments to progressively improve the
skills that the agent is weak at, by providing feedback to the LLM in the form
of the agent's performance. We demonstrate the usefulness of EnvGen with
comprehensive experiments in Crafter and Heist environments. We find that a
small RL agent trained with EnvGen can outperform SOTA methods, including a
GPT-4 agent, and learns long-horizon tasks significantly faster. We show
qualitatively how the LLM adapts training environments to help improve RL
agents' weaker skills over time. Additionally, EnvGen is substantially more
efficient as it only uses a small number of LLM calls (e.g., 4 in total),
whereas LLM agents require thousands of LLM calls. Lastly, we present detailed
ablation studies for our design choices.
\\ ( https://arxiv.org/abs/2403.12014 ,  1122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12024
Date: Mon, 18 Mar 2024 17:56:13 GMT   (6747kb,D)

Title: Enhancing Hokkien Dual Translation by Exploring and Standardizing of
  Four Writing Systems
Authors: Bo-Han Lu, Yi-Hsuan Lin, En-Shiun Annie Lee, Richard Tzong-Han Tsai
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Machine translation focuses mainly on high-resource languages (HRLs), while
low-resource languages (LRLs) like Taiwanese Hokkien are relatively
under-explored. This study aims to address this gap by developing a dual
translation model between Taiwanese Hokkien and both Traditional Mandarin
Chinese and English. We employ a pre-trained LLaMA2-7B model specialized in
Traditional Mandarin Chinese to leverage the orthographic similarities between
Taiwanese Hokkien Han and Traditional Mandarin Chinese. Our comprehensive
experiments involve translation tasks across various writing systems of
Taiwanese Hokkien and between Taiwanese Hokkien and other HRLs. We find that
the use of a limited monolingual corpus also further improve the model's
Taiwanese Hokkien capabilities. We then utilize our translation model to
standardize all Taiwanese Hokkien writing systems into Hokkien Han, resulting
in further performance improvements. Additionally, we introduce an evaluation
method incorporating back-translation and GPT-4 to ensure reliable translation
quality assessment even for LRLs. The study contributes to narrowing the
resource gap for Taiwanese Hokkien and empirically investigates the advantages
and limitations of pre-training and fine-tuning based on LLaMA 2.
\\ ( https://arxiv.org/abs/2403.12024 ,  6747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12027
Date: Mon, 18 Mar 2024 17:57:09 GMT   (415kb,D)

Title: From Pixels to Insights: A Survey on Automatic Chart Understanding in
  the Era of Large Foundation Models
Authors: Kung-Hsiang Huang, Hou Pong Chan, Yi R. Fung, Haoyi Qiu, Mingyang
  Zhou, Shafiq Joty, Shih-Fu Chang, Heng Ji
Categories: cs.CL cs.AI cs.CV
\\
  Data visualization in the form of charts plays a pivotal role in data
analysis, offering critical insights and aiding in informed decision-making.
Automatic chart understanding has witnessed significant advancements with the
rise of large foundation models in recent years. Foundation models, such as
large language models (LLMs), have revolutionized various natural language
processing (NLP) tasks and are increasingly being applied to chart
understanding tasks. This survey paper provides a comprehensive overview of the
recent developments, challenges, and future directions in chart understanding
within the context of these foundation models. The paper begins by defining
chart understanding, outlining problem formulations, and discussing fundamental
building blocks crucial for studying chart understanding tasks. In the section
on tasks and datasets, we explore various tasks within chart understanding and
discuss their evaluation metrics and sources of both charts and textual inputs.
Modeling strategies are then examined, encompassing both classification-based
and generation-based approaches, along with tool augmentation techniques that
enhance chart understanding performance. Furthermore, we discuss the
state-of-the-art performance of each task and discuss how we can improve the
performance. Challenges and future directions are addressed in a dedicated
section, highlighting issues such as domain-specific charts, lack of efforts in
evaluation, and agent-oriented settings. This survey paper serves to provide
valuable insights and directions for future research in chart understanding
leveraging large foundation models. The studies mentioned in this paper, along
with emerging new research, will be continually updated at:
https://github.com/khuangaf/Awesome-Chart-Understanding.
\\ ( https://arxiv.org/abs/2403.12027 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10550
Date: Wed, 13 Mar 2024 02:10:32 GMT   (4804kb)

Title: Semi-Supervised Learning for Anomaly Traffic Detection via Bidirectional
  Normalizing Flows
Authors: Zhangxuan Dang, Yu Zheng, Xinglin Lin, Chunlei Peng, Qiuyu Chen, Xinbo
  Gao
Categories: cs.LG cs.AI cs.CR
\\
  With the rapid development of the Internet, various types of anomaly traffic
are threatening network security. We consider the problem of anomaly network
traffic detection and propose a three-stage anomaly detection framework using
only normal traffic. Our framework can generate pseudo anomaly samples without
prior knowledge of anomalies to achieve the detection of anomaly data. Firstly,
we employ a reconstruction method to learn the deep representation of normal
samples. Secondly, these representations are normalized to a standard normal
distribution using a bidirectional flow module. To simulate anomaly samples, we
add noises to the normalized representations which are then passed through the
generation direction of the bidirectional flow module. Finally, a simple
classifier is trained to differentiate the normal samples and pseudo anomaly
samples in the latent space. During inference, our framework requires only two
modules to detect anomalous samples, leading to a considerable reduction in
model size. According to the experiments, our method achieves the state
of-the-art results on the common benchmarking datasets of anomaly network
traffic detection. The code is given in the
https://github.com/ZxuanDang/ATD-via-Flows.git
\\ ( https://arxiv.org/abs/2403.10550 ,  4804kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10552
Date: Wed, 13 Mar 2024 03:20:47 GMT   (1391kb)

Title: Training Self-localization Models for Unseen Unfamiliar Places via
  Teacher-to-Student Data-Free Knowledge Transfer
Authors: Kenta Tsukahara, Kanji Tanaka, Daiki Iwata
Categories: cs.LG cs.AI cs.CV cs.RO
Comments: 7 pages, 3 figures, technical report
\\
  A typical assumption in state-of-the-art self-localization models is that an
annotated training dataset is available in the target workspace. However, this
does not always hold when a robot travels in a general open-world. This study
introduces a novel training scheme for open-world distributed robot systems. In
our scheme, a robot ("student") can ask the other robots it meets at unfamiliar
places ("teachers") for guidance. Specifically, a pseudo-training dataset is
reconstructed from the teacher model and thereafter used for continual learning
of the student model. Unlike typical knowledge transfer schemes, our scheme
introduces only minimal assumptions on the teacher model, such that it can
handle various types of open-set teachers, including uncooperative, untrainable
(e.g., image retrieval engines), and blackbox teachers (i.e., data privacy).
Rather than relying on the availability of private data of teachers as in
existing methods, we propose to exploit an assumption that holds universally in
self-localization tasks: "The teacher model is a self-localization system" and
to reuse the self-localization system of a teacher as a sole accessible
communication channel. We particularly focus on designing an excellent
student/questioner whose interactions with teachers can yield effective
question-and-answer sequences that can be used as pseudo-training datasets for
the student self-localization model. When applied to a generic recursive
knowledge distillation scenario, our approach exhibited stable and consistent
performance improvement.
\\ ( https://arxiv.org/abs/2403.10552 ,  1391kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10553
Date: Wed, 13 Mar 2024 03:43:39 GMT   (399kb,D)

Title: Learning to Watermark LLM-generated Text via Reinforcement Learning
Authors: Xiaojun Xu, Yuanshun Yao, Yang Liu
Categories: cs.LG cs.AI cs.CR
\\
  We study how to watermark LLM outputs, i.e. embedding algorithmically
detectable signals into LLM-generated text to track misuse. Unlike the current
mainstream methods that work with a fixed LLM, we expand the watermark design
space by including the LLM tuning stage in the watermark pipeline. While prior
works focus on token-level watermark that embeds signals into the output, we
design a model-level watermark that embeds signals into the LLM weights, and
such signals can be detected by a paired detector. We propose a co-training
framework based on reinforcement learning that iteratively (1) trains a
detector to detect the generated watermarked text and (2) tunes the LLM to
generate text easily detectable by the detector while keeping its normal
utility. We empirically show that our watermarks are more accurate, robust, and
adaptable (to new attacks). It also allows watermarked model open-sourcing. In
addition, if used together with alignment, the extra overhead introduced is low
- only training an extra reward model (i.e. our detector). We hope our work can
bring more effort into studying a broader watermark design that is not limited
to working with a fixed LLM. We open-source the code:
https://github.com/xiaojunxu/learning-to-watermark-llm .
\\ ( https://arxiv.org/abs/2403.10553 ,  399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10555
Date: Wed, 13 Mar 2024 06:41:37 GMT   (8743kb,D)

Title: KARINA: An Efficient Deep Learning Model for Global Weather Forecast
Authors: Minjong Cheon, Yo-Hwan Choi, Seon-Yu Kang, Yumi Choi, Jeong-Gil Lee,
  Daehyun Kang
Categories: cs.LG cs.AI cs.CV physics.ao-ph
\\
  Deep learning-based, data-driven models are gaining prevalence in climate
research, particularly for global weather prediction. However, training the
global weather data at high resolution requires massive computational
resources. Therefore, we present a new model named KARINA to overcome the
substantial computational demands typical of this field. This model achieves
forecasting accuracy comparable to higher-resolution counterparts with
significantly less computational resources, requiring only 4 NVIDIA A100 GPUs
and less than 12 hours of training. KARINA combines ConvNext, SENet, and
Geocyclic Padding to enhance weather forecasting at a 2.5{\deg} resolution,
which could filter out high-frequency noise. Geocyclic Padding preserves pixels
at the lateral boundary of the input image, thereby maintaining atmospheric
flow continuity in the spherical Earth. SENet dynamically improves feature
response, advancing atmospheric process modeling, particularly in the vertical
column process as numerous channels. In this vein, KARINA sets new benchmarks
in weather forecasting accuracy, surpassing existing models like the ECMWF S2S
reforecasts at a lead time of up to 7 days. Remarkably, KARINA achieved
competitive performance even when compared to the recently developed models
(Pangu-Weather, GraphCast, ClimaX, and FourCastNet) trained with
high-resolution data having 100 times larger pixels. Conclusively, KARINA
significantly advances global weather forecasting by efficiently modeling
Earth's atmosphere with improved accuracy and resource efficiency.
\\ ( https://arxiv.org/abs/2403.10555 ,  8743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10557
Date: Wed, 13 Mar 2024 18:57:30 GMT   (2513kb,D)

Title: Second-Order Information Matters: Revisiting Machine Unlearning for
  Large Language Models
Authors: Kang Gu, Md Rafi Ur Rashid, Najrin Sultana, Shagufta Mehnaz
Categories: cs.LG cs.AI cs.CL
\\
  With the rapid development of Large Language Models (LLMs), we have witnessed
intense competition among the major LLM products like ChatGPT, LLaMa, and
Gemini. However, various issues (e.g. privacy leakage and copyright violation)
of the training corpus still remain underexplored. For example, the Times sued
OpenAI and Microsoft for infringing on its copyrights by using millions of its
articles for training. From the perspective of LLM practitioners, handling such
unintended privacy violations can be challenging. Previous work addressed the
``unlearning" problem of LLMs using gradient information, while they mostly
introduced significant overheads like data preprocessing or lacked robustness.
In this paper, contrasting with the methods based on first-order information,
we revisit the unlearning problem via the perspective of second-order
information (Hessian). Our unlearning algorithms, which are inspired by classic
Newton update, are not only data-agnostic/model-agnostic but also proven to be
robust in terms of utility preservation or privacy guarantee. Through a
comprehensive evaluation with four NLP datasets as well as a case study on
real-world datasets, our methods consistently show superiority over the
first-order methods.
\\ ( https://arxiv.org/abs/2403.10557 ,  2513kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10559
Date: Thu, 14 Mar 2024 06:51:26 GMT   (713kb,D)

Title: Generative Models and Connected and Automated Vehicles: A Survey in
  Exploring the Intersection of Transportation and AI
Authors: Dong Shu, Zhouyao Zhu
Categories: cs.LG cs.AI cs.RO
Comments: 9 pages, 2 figures
\\
  This report investigates the history and impact of Generative Models and
Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing
progress in technology and transportation. By focusing on the application of
generative models within the context of CAVs, the study aims to unravel how
this integration could enhance predictive modeling, simulation accuracy, and
decision-making processes in autonomous vehicles. This thesis discusses the
benefits and challenges of integrating generative models and CAV technology in
transportation. It aims to highlight the progress made, the remaining
obstacles, and the potential for advancements in safety and innovation.
\\ ( https://arxiv.org/abs/2403.10559 ,  713kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10561
Date: Thu, 14 Mar 2024 08:46:07 GMT   (1kb)

Title: A collection of the accepted papers for the Human-Centric Representation
  Learning workshop at AAAI 2024
Authors: Dimitris Spathis, Aaqib Saeed, Ali Etemad, Sana Tonekaboni, Stefanos
  Laskaridis, Shohreh Deldari, Chi Ian Tang, Patrick Schwab, Shyam Tailor
Categories: cs.LG cs.AI
\\
  This non-archival index is not complete, as some accepted papers chose to
opt-out of inclusion. The list of all accepted papers is available on the
workshop website.
\\ ( https://arxiv.org/abs/2403.10561 ,  1kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10566
Date: Thu, 14 Mar 2024 16:51:51 GMT   (7596kb,D)

Title: Cooling-Guide Diffusion Model for Battery Cell Arrangement
Authors: Nicholas Sung, Liu Zheng, Pingfeng Wang, Faez Ahmed
Categories: cs.LG cs.AI
\\
  Our study introduces a Generative AI method that employs a cooling-guided
diffusion model to optimize the layout of battery cells, a crucial step for
enhancing the cooling performance and efficiency of battery thermal management
systems. Traditional design processes, which rely heavily on iterative
optimization and extensive guesswork, are notoriously slow and inefficient,
often leading to suboptimal solutions. In contrast, our innovative method uses
a parametric denoising diffusion probabilistic model (DDPM) with classifier and
cooling guidance to generate optimized cell layouts with enhanced cooling
paths, significantly lowering the maximum temperature of the cells. By
incorporating position-based classifier guidance, we ensure the feasibility of
generated layouts. Meanwhile, cooling guidance directly optimizes
cooling-efficiency, making our approach uniquely effective. When compared to
two advanced models, the Tabular Denoising Diffusion Probabilistic Model
(TabDDPM) and the Conditional Tabular GAN (CTGAN), our cooling-guided diffusion
model notably outperforms both. It is five times more effective than TabDDPM
and sixty-six times better than CTGAN across key metrics such as feasibility,
diversity, and cooling efficiency. This research marks a significant leap
forward in the field, aiming to optimize battery cell layouts for superior
cooling efficiency, thus setting the stage for the development of more
effective and dependable battery thermal management systems.
\\ ( https://arxiv.org/abs/2403.10566 ,  7596kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10567
Date: Thu, 14 Mar 2024 17:45:56 GMT   (2629kb)

Title: Uncertainty estimation in spatial interpolation of satellite
  precipitation with ensemble learning
Authors: Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis,
  Anastasios Doulamis
Categories: cs.LG stat.ME
\\
  Predictions in the form of probability distributions are crucial for
decision-making. Quantile regression enables this within spatial interpolation
settings for merging remote sensing and gauge precipitation data. However,
ensemble learning of quantile regression algorithms remains unexplored in this
context. Here, we address this gap by introducing nine quantile-based ensemble
learners and applying them to large precipitation datasets. We employed a novel
feature engineering strategy, reducing predictors to distance-weighted
satellite precipitation at relevant locations, combined with location
elevation. Our ensemble learners include six stacking and three simple methods
(mean, median, best combiner), combining six individual algorithms: quantile
regression (QR), quantile regression forests (QRF), generalized random forests
(GRF), gradient boosting machines (GBM), light gradient boosting machines
(LightGBM), and quantile regression neural networks (QRNN). These algorithms
serve as both base learners and combiners within different stacking methods. We
evaluated performance against QR using quantile scoring functions in a large
dataset comprising 15 years of monthly gauge-measured and satellite
precipitation in contiguous US (CONUS). Stacking with QR and QRNN yielded the
best results across quantile levels of interest (0.025, 0.050, 0.075, 0.100,
0.200, 0.300, 0.400, 0.500, 0.600, 0.700, 0.800, 0.900, 0.925, 0.950, 0.975),
surpassing the reference method by 3.91% to 8.95%. This demonstrates the
potential of stacking to improve probabilistic predictions in spatial
interpolation and beyond.
\\ ( https://arxiv.org/abs/2403.10567 ,  2629kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10568
Date: Thu, 14 Mar 2024 17:47:10 GMT   (8790kb,D)

Title: MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of
  Prompt Experts
Authors: Ruixiang Jiang, Lingbo Liu, Changwen Chen
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: Extended version of arxiv:2312.03734
\\
  Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal
foundation models for multimodal tasks. However, its limited adaptivity and
expressiveness lead to suboptimal performance when compared with other tuning
methods. In this paper, we address this issue by disentangling the vanilla
prompts to adaptively capture dataset-level and instance-level features.
Building upon this disentanglement, we introduce the mixture of prompt experts
(MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing
priors to route the most effective prompt on a per-instance basis. Compared to
vanilla prompting, our MoPE-based conditional prompting exhibits greater
expressiveness for multimodal fusion, scaling better with the training data and
the overall number of trainable parameters. We also study a regularization term
for expert routing, leading to emergent expert specialization, where different
experts focus on different concepts, enabling interpretable soft prompting.
Extensive experiments across three multimodal datasets demonstrate that our
method achieves state-of-the-art results, matching or even surpassing the
performance of fine-tuning, while requiring only 0.8% of the trainable
parameters. Code will be released: https://github.com/songrise/MoPE.
\\ ( https://arxiv.org/abs/2403.10568 ,  8790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10569
Date: Thu, 14 Mar 2024 19:40:58 GMT   (24849kb,D)

Title: Achieving Pareto Optimality using Efficient Parameter Reduction for DNNs
  in Resource-Constrained Edge Environment
Authors: Atah Nuh Mih, Alireza Rahimi, Asfia Kawnine, Francis Palma, Monica
  Wachowicz, Rickey Dubay, Hung Cao
Categories: cs.LG cs.AI cs.CV
Comments: arXiv admin note: text overlap with arXiv:2401.05355
\\
  This paper proposes an optimization of an existing Deep Neural Network (DNN)
that improves its hardware utilization and facilitates on-device training for
resource-constrained edge environments. We implement efficient parameter
reduction strategies on Xception that shrink the model size without sacrificing
accuracy, thus decreasing memory utilization during training. We evaluate our
model in two experiments: Caltech-101 image classification and PCB defect
detection and compare its performance against the original Xception and
lightweight models, EfficientNetV2B1 and MobileNetV2. The results of the
Caltech-101 image classification show that our model has a better test accuracy
(76.21%) than Xception (75.89%), uses less memory on average (847.9MB) than
Xception (874.6MB), and has faster training and inference times. The
lightweight models overfit with EfficientNetV2B1 having a 30.52% test accuracy
and MobileNetV2 having a 58.11% test accuracy. Both lightweight models have
better memory usage than our model and Xception. On the PCB defect detection,
our model has the best test accuracy (90.30%), compared to Xception (88.10%),
EfficientNetV2B1 (55.25%), and MobileNetV2 (50.50%). MobileNetV2 has the least
average memory usage (849.4MB), followed by our model (865.8MB), then
EfficientNetV2B1 (874.8MB), and Xception has the highest (893.6MB). We further
experiment with pre-trained weights and observe that memory usage decreases
thereby showing the benefits of transfer learning. A Pareto analysis of the
models' performance shows that our optimized model architecture satisfies
accuracy and low memory utilization objectives.
\\ ( https://arxiv.org/abs/2403.10569 ,  24849kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10572
Date: Fri, 15 Mar 2024 02:25:45 GMT   (3935kb,D)

Title: Discovering Invariant Neighborhood Patterns for Heterophilic Graphs
Authors: Ruihao Zhang, Zhengyu Chen, Teng Xiao, Yueyang Wang, Kun Kuang
Categories: cs.LG cs.SI
Comments: 11 pages,11 figures
\\
  This paper studies the problem of distribution shifts on non-homophilous
graphs Mosting existing graph neural network methods rely on the homophilous
assumption that nodes from the same class are more likely to be linked.
However, such assumptions of homophily do not always hold in real-world graphs,
which leads to more complex distribution shifts unaccounted for in previous
methods. The distribution shifts of neighborhood patterns are much more diverse
on non-homophilous graphs. We propose a novel Invariant Neighborhood Pattern
Learning (INPL) to alleviate the distribution shifts problem on non-homophilous
graphs. Specifically, we propose the Adaptive Neighborhood Propagation (ANP)
module to capture the adaptive neighborhood information, which could alleviate
the neighborhood pattern distribution shifts problem on non-homophilous graphs.
We propose Invariant Non-Homophilous Graph Learning (INHGL) module to constrain
the ANP and learn invariant graph representation on non-homophilous graphs.
Extensive experimental results on real-world non-homophilous graphs show that
INPL could achieve state-of-the-art performance for learning on large
non-homophilous graphs.
\\ ( https://arxiv.org/abs/2403.10572 ,  3935kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10586
Date: Fri, 15 Mar 2024 17:03:45 GMT   (2037kb,D)

Title: From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive
  Bladder Cancer Recurrence Prediction
Authors: Saram Abbas, Dr Rishad Shafik, Prof Naeem Soomro, Prof Rakesh Heer, Dr
  Kabita Adhikari
Categories: cs.LG cs.AI
Comments: 16 pages, 4 Figures
\\
  Bladder cancer, the leading urinary tract cancer, is responsible for 15
deaths daily in the UK. This cancer predominantly manifests as
non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet
penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very
high recurrence rate of 70-80% and hence the costliest treatments. Current
tools for predicting recurrence use scoring systems that overestimate risk and
have poor accuracy. Inaccurate and delayed prediction of recurrence
significantly elevates the likelihood of mortality. Accurate prediction of
recurrence is hence vital for cost-effective management and treatment planning.
This is where Machine learning (ML) techniques have emerged as a promising
approach for predicting NMIBC recurrence by leveraging molecular and clinical
data. This review provides a comprehensive analysis of ML approaches for
predicting NMIBC recurrence. Our systematic evaluation demonstrates the
potential of diverse ML algorithms and markers, including radiomic, clinical,
histopathological, genomic, and biochemical data in enhancing recurrence
prediction and personalised patient management. We summarise various prediction
tasks, data modalities, and ML models used, highlighting their performance,
limitations, and future directions of incorporating cost-effectiveness.
Challenges related to generalisability and interpretability of artificial
intelligent models are discussed, emphasising the need for collaborative
efforts and robust datasets.
\\ ( https://arxiv.org/abs/2403.10586 ,  2037kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10610
Date: Fri, 15 Mar 2024 18:13:48 GMT   (4594kb,D)

Title: Sequential Monte Carlo for Inclusive KL Minimization in Amortized
  Variational Inference
Authors: Declan McNamara, Jackson Loper, Jeffrey Regier
Categories: cs.LG stat.ML
Comments: Accepted to the International Conference on Artificial Intelligence
  and Statistics (AISTATS 2024)
\\
  For training an encoder network to perform amortized variational inference,
the Kullback-Leibler (KL) divergence from the exact posterior to its
approximation, known as the inclusive or forward KL, is an increasingly popular
choice of variational objective due to the mass-covering property of its
minimizer. However, minimizing this objective is challenging. A popular
existing approach, Reweighted Wake-Sleep (RWS), suffers from heavily biased
gradients and a circular pathology that results in highly concentrated
variational distributions. As an alternative, we propose SMC-Wake, a procedure
for fitting an amortized variational approximation that uses
likelihood-tempered sequential Monte Carlo samplers to estimate the gradient of
the inclusive KL divergence. We propose three gradient estimators, all of which
are asymptotically unbiased in the number of iterations and two of which are
strongly consistent. Our method interleaves stochastic gradient updates, SMC
samplers, and iterative improvement to an estimate of the normalizing constant
to reduce bias from self-normalization. In experiments with both simulated and
real datasets, SMC-Wake fits variational distributions that approximate the
posterior more accurately than existing methods.
\\ ( https://arxiv.org/abs/2403.10610 ,  4594kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10616
Date: Fri, 15 Mar 2024 18:26:51 GMT   (8311kb,D)

Title: DiPaCo: Distributed Path Composition
Authors: Arthur Douillard, Qixuan Feng, Andrei A. Rusu, Adhiguna Kuncoro, Yani
  Donchev, Rachita Chhaparia, Ionel Gog, Marc'Aurelio Ranzato, Jiajun Shen,
  Arthur Szlam
Categories: cs.LG cs.CL
\\
  Progress in machine learning (ML) has been fueled by scaling neural network
models. This scaling has been enabled by ever more heroic feats of engineering,
necessary for accommodating ML approaches that require high bandwidth
communication between devices working in parallel. In this work, we propose a
co-designed modular architecture and training approach for ML models, dubbed
DIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes
computation by paths through a set of shared modules. Together with a Local-SGD
inspired optimization (DiLoCo) that keeps modules in sync with drastically
reduced communication, Our approach facilitates training across poorly
connected and heterogeneous workers, with a design that ensures robustness to
worker failures and preemptions. At inference time, only a single path needs to
be executed for each input, without the need for any model compression. We
consider this approach as a first prototype towards a new paradigm of
large-scale learning, one that is less synchronous and more modular. Our
experiments on the widely used C4 benchmark show that, for the same amount of
training steps but less wall-clock time, DiPaCo exceeds the performance of a 1
billion-parameter dense transformer language model by choosing one of 256
possible paths, each with a size of 150 million parameters.
\\ ( https://arxiv.org/abs/2403.10616 ,  8311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10618
Date: Fri, 15 Mar 2024 18:30:06 GMT   (21kb)

Title: Limits of Approximating the Median Treatment Effect
Authors: Raghavendra Addanki, Siddharth Bhandari
Categories: cs.LG cs.AI cs.DS econ.EM stat.ME
\\
  Average Treatment Effect (ATE) estimation is a well-studied problem in causal
inference. However, it does not necessarily capture the heterogeneity in the
data, and several approaches have been proposed to tackle the issue, including
estimating the Quantile Treatment Effects. In the finite population setting
containing $n$ individuals, with treatment and control values denoted by the
potential outcome vectors $\mathbf{a}, \mathbf{b}$, much of the prior work
focused on estimating median$(\mathbf{a}) -$ median$(\mathbf{b})$, where
median($\mathbf x$) denotes the median value in the sorted ordering of all the
values in vector $\mathbf x$. It is known that estimating the difference of
medians is easier than the desired estimand of median$(\mathbf{a-b})$, called
the Median Treatment Effect (MTE). The fundamental problem of causal inference
-- for every individual $i$, we can only observe one of the potential outcome
values, i.e., either the value $a_i$ or $b_i$, but not both, makes estimating
MTE particularly challenging. In this work, we argue that MTE is not estimable
and detail a novel notion of approximation that relies on the sorted order of
the values in $\mathbf{a-b}$. Next, we identify a quantity called variability
that exactly captures the complexity of MTE estimation. By drawing connections
to instance-optimality studied in theoretical computer science, we show that
every algorithm for estimating the MTE obtains an approximation error that is
no better than the error of an algorithm that computes variability. Finally, we
provide a simple linear time algorithm for computing the variability exactly.
Unlike much prior work, a particular highlight of our work is that we make no
assumptions about how the potential outcome vectors are generated or how they
are correlated, except that the potential outcome values are $k$-ary, i.e.,
take one of $k$ discrete values.
\\ ( https://arxiv.org/abs/2403.10618 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10638
Date: Fri, 15 Mar 2024 19:12:28 GMT   (8240kb,D)

Title: A resource-constrained stochastic scheduling algorithm for homeless
  street outreach and gleaning edible food
Authors: Conor M. Artman, Aditya Mate, Ezinne Nwankwo, Aliza Heching, Tsuyoshi
  Id\'e, Ji\v{r}\'i\, Navr\'atil, Karthikeyan Shanmugam, Wei Sun, Kush R.
  Varshney, Lauri Goldkind, Gidi Kroch, Jaclyn Sawyer, Ian Watson
Categories: cs.LG cs.CY stat.ML
\\
  We developed a common algorithmic solution addressing the problem of
resource-constrained outreach encountered by social change organizations with
different missions and operations: Breaking Ground -- an organization that
helps individuals experiencing homelessness in New York transition to permanent
housing and Leket -- the national food bank of Israel that rescues food from
farms and elsewhere to feed the hungry. Specifically, we developed an
estimation and optimization approach for partially-observed episodic restless
bandits under $k$-step transitions. The results show that our Thompson sampling
with Markov chain recovery (via Stein variational gradient descent) algorithm
significantly outperforms baselines for the problems of both organizations. We
carried out this work in a prospective manner with the express goal of devising
a flexible-enough but also useful-enough solution that can help overcome a lack
of sustainable impact in data science for social good.
\\ ( https://arxiv.org/abs/2403.10638 ,  8240kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10642
Date: Fri, 15 Mar 2024 19:21:27 GMT   (8351kb,D)

Title: Using Uncertainty Quantification to Characterize and Improve
  Out-of-Domain Learning for PDEs
Authors: S. Chandra Mouli, Danielle C. Maddix, Shima Alizadeh, Gaurav Gupta,
  Andrew Stuart, Michael W. Mahoney, Yuyang Wang
Categories: cs.LG cs.NA math.NA
\\
  Existing work in scientific machine learning (SciML) has shown that
data-driven learning of solution operators can provide a fast approximate
alternative to classical numerical partial differential equation (PDE) solvers.
Of these, Neural Operators (NOs) have emerged as particularly promising. We
observe that several uncertainty quantification (UQ) methods for NOs fail for
test inputs that are even moderately out-of-domain (OOD), even when the model
approximates the solution well for in-domain tasks. To address this limitation,
we show that ensembling several NOs can identify high-error regions and provide
good uncertainty estimates that are well-correlated with prediction errors.
Based on this, we propose a cost-effective alternative, DiverseNO, that mimics
the properties of the ensemble by encouraging diverse predictions from its
multiple heads in the last feed-forward layer. We then introduce
Operator-ProbConserv, a method that uses these well-calibrated UQ estimates
within the ProbConserv framework to update the model. Our empirical results
show that Operator-ProbConserv enhances OOD model performance for a variety of
challenging PDE problems and satisfies physical constraints such as
conservation laws.
\\ ( https://arxiv.org/abs/2403.10642 ,  8351kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10646
Date: Fri, 15 Mar 2024 19:27:48 GMT   (3088kb)

Title: A Survey of Source Code Representations for Machine Learning-Based
  Cybersecurity Tasks
Authors: Beatrice Casey, Joanna C. S. Santos, George Perry
Categories: cs.LG cs.CR
\\
  Machine learning techniques for cybersecurity-related software engineering
tasks are becoming increasingly popular. The representation of source code is a
key portion of the technique that can impact the way the model is able to learn
the features of the source code. With an increasing number of these techniques
being developed, it is valuable to see the current state of the field to better
understand what exists and what's not there yet. This paper presents a study of
these existing ML-based approaches and demonstrates what type of
representations were used for different cybersecurity tasks and programming
languages. Additionally, we study what types of models are used with different
representations. We have found that graph-based representations are the most
popular category of representation, and Tokenizers and Abstract Syntax Trees
(ASTs) are the two most popular representations overall. We also found that the
most popular cybersecurity task is vulnerability detection, and the language
that is covered by the most techniques is C. Finally, we found that
sequence-based models are the most popular category of models, and Support
Vector Machines (SVMs) are the most popular model overall.
\\ ( https://arxiv.org/abs/2403.10646 ,  3088kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10652
Date: Fri, 15 Mar 2024 19:36:56 GMT   (203kb,D)

Title: Improving Fairness in Credit Lending Models using Subgroup Threshold
  Optimization
Authors: Cecilia Ying, Stephen Thomas
Categories: cs.LG q-fin.RM
Comments: Neural Information Processing Systems (NeurIPS) Workshop in Strategic
  ML
\\
  In an effort to improve the accuracy of credit lending decisions, many
financial intuitions are now using predictions from machine learning models.
While such predictions enjoy many advantages, recent research has shown that
the predictions have the potential to be biased and unfair towards certain
subgroups of the population. To combat this, several techniques have been
introduced to help remove the bias and improve the overall fairness of the
predictions. We introduce a new fairness technique, called \textit{Subgroup
Threshold Optimizer} (\textit{STO}), that does not require any alternations to
the input training data nor does it require any changes to the underlying
machine learning algorithm, and thus can be used with any existing machine
learning pipeline. STO works by optimizing the classification thresholds for
individual subgroups in order to minimize the overall discrimination score
between them. Our experiments on a real-world credit lending dataset show that
STO can reduce gender discrimination by over 90\%.
\\ ( https://arxiv.org/abs/2403.10652 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10704
Date: Fri, 15 Mar 2024 21:43:46 GMT   (3473kb,D)

Title: PERL: Parameter Efficient Reinforcement Learning from Human Feedback
Authors: Hakim Sidahmed and Samrat Phatale and Alex Hutcheson and Zhuonan Lin
  and Zhang Chen and Zac Yu and Jarvis Jin and Roman Komarytsia and Christiane
  Ahlheim and Yonghao Zhu and Simral Chaudhary and Bowen Li and Saravanan
  Ganesh and Bill Byrne and Jessica Hoffmann and Hassan Mansoor and Wei Li and
  Abhinav Rastogi and Lucas Dixon
Categories: cs.LG cs.AI cs.CL
\\
  Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong
method to align Pretrained Large Language Models (LLMs) with human preferences.
But training models with RLHF is computationally expensive, and an overall
complex process. In this work, we study RLHF where the underlying models are
trained using the parameter efficient method of Low-Rank Adaptation (LoRA)
introduced by Hu et al. [2021]. We investigate the setup of "Parameter
Efficient Reinforcement Learning" (PERL), in which we perform reward model
training and reinforcement learning using LoRA. We compare PERL to conventional
fine-tuning (full-tuning) across various configurations for 7 benchmarks,
including 2 novel datasets, of reward modeling and reinforcement learning. We
find that PERL performs on par with the conventional RLHF setting, while
training faster, and with less memory. This enables the high performance of
RLHF, while reducing the computational burden that limits its adoption as an
alignment technique for Large Language Models. We also release 2 novel thumbs
up/down preference datasets: "Taskmaster Coffee", and "Taskmaster Ticketing" to
promote research around RLHF.
\\ ( https://arxiv.org/abs/2403.10704 ,  3473kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10717
Date: Fri, 15 Mar 2024 22:35:07 GMT   (6795kb,D)

Title: Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized
  Scaled Prediction Consistency
Authors: Soumyadeep Pal, Yuguang Yao, Ren Wang, Bingquan Shen, Sijia Liu
Categories: cs.LG cs.AI cs.CR
Comments: The Twelfth International Conference on Learning Representations
  (ICLR 2024)
\\
  Modern machine learning (ML) systems demand substantial training data, often
resorting to external sources. Nevertheless, this practice renders them
vulnerable to backdoor poisoning attacks. Prior backdoor defense strategies
have primarily focused on the identification of backdoored models or poisoned
data characteristics, typically operating under the assumption of access to
clean data. In this work, we delve into a relatively underexplored challenge:
the automatic identification of backdoor data within a poisoned dataset, all
under realistic conditions, i.e., without the need for additional clean data or
without manually defining a threshold for backdoor detection. We draw an
inspiration from the scaled prediction consistency (SPC) technique, which
exploits the prediction invariance of poisoned data to an input scaling factor.
Based on this, we pose the backdoor data identification problem as a
hierarchical data splitting optimization problem, leveraging a novel SPC-based
loss function as the primary optimization objective. Our innovation unfolds in
several key aspects. First, we revisit the vanilla SPC method, unveiling its
limitations in addressing the proposed backdoor identification problem.
Subsequently, we develop a bi-level optimization-based approach to precisely
identify backdoor data by minimizing the advanced SPC loss. Finally, we
demonstrate the efficacy of our proposal against a spectrum of backdoor
attacks, encompassing basic label-corrupted attacks as well as more
sophisticated clean-label attacks, evaluated across various benchmark datasets.
Experiment results show that our approach often surpasses the performance of
current baselines in identifying backdoor data points, resulting in about
4%-36% improvement in average AUROC. Codes are available at
https://github.com/OPTML-Group/BackdoorMSPC.
\\ ( https://arxiv.org/abs/2403.10717 ,  6795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10730
Date: Fri, 15 Mar 2024 23:29:32 GMT   (4063kb,D)

Title: Counterfactual Analysis of Neural Networks Used to Create Fertilizer
  Management Zones
Authors: Giorgio Morales and John Sheppard
Categories: cs.LG
Comments: Accepted to appear in the International Joint Conference on Neural
  Networks 2024
\\
  In Precision Agriculture, the utilization of management zones (MZs) that take
into account within-field variability facilitates effective fertilizer
management. This approach enables the optimization of nitrogen (N) rates to
maximize crop yield production and enhance agronomic use efficiency. However,
existing works often neglect the consideration of responsivity to fertilizer as
a factor influencing MZ determination. In response to this gap, we present a MZ
clustering method based on fertilizer responsivity. We build upon the statement
that the responsivity of a given site to the fertilizer rate is described by
the shape of its corresponding N fertilizer-yield response (N-response) curve.
Thus, we generate N-response curves for all sites within the field using a
convolutional neural network (CNN). The shape of the approximated N-response
curves is then characterized using functional principal component analysis.
Subsequently, a counterfactual explanation (CFE) method is applied to discern
the impact of various variables on MZ membership. The genetic algorithm-based
CFE solves a multi-objective optimization problem and aims to identify the
minimum combination of features needed to alter a site's cluster assignment.
Results from two yield prediction datasets indicate that the features with the
greatest influence on MZ membership are associated with terrain characteristics
that either facilitate or impede fertilizer runoff, such as terrain slope or
topographic aspect.
\\ ( https://arxiv.org/abs/2403.10730 ,  4063kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10732
Date: Fri, 15 Mar 2024 23:36:55 GMT   (73kb,D)

Title: Variance-Dependent Regret Bounds for Non-stationary Linear Bandits
Authors: Zhiyong Wang and Jize Xie and Yi Chen and John C.S. Lui and Dongruo
  Zhou
Categories: cs.LG cs.AI
Comments: 30 pages
\\
  We investigate the non-stationary stochastic linear bandit problem where the
reward distribution evolves each round. Existing algorithms characterize the
non-stationarity by the total variation budget $B_K$, which is the summation of
the change of the consecutive feature vectors of the linear bandits over $K$
rounds. However, such a quantity only measures the non-stationarity with
respect to the expectation of the reward distribution, which makes existing
algorithms sub-optimal under the general non-stationary distribution setting.
In this work, we propose algorithms that utilize the variance of the reward
distribution as well as the $B_K$, and show that they can achieve tighter
regret upper bounds. Specifically, we introduce two novel algorithms: Restarted
Weighted$\text{OFUL}^+$ and Restarted $\text{SAVE}^+$. These algorithms address
cases where the variance information of the rewards is known and unknown,
respectively. Notably, when the total variance $V_K$ is much smaller than $K$,
our algorithms outperform previous state-of-the-art results on non-stationary
stochastic linear bandits under different settings. Experimental evaluations
further validate the superior performance of our proposed algorithms over
existing works.
\\ ( https://arxiv.org/abs/2403.10732 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10738
Date: Fri, 15 Mar 2024 23:50:58 GMT   (104kb)

Title: Horizon-Free Regret for Linear Markov Decision Processes
Authors: Zihan Zhang, Jason D. Lee, Yuxin Chen, Simon S. Du
Categories: cs.LG
Comments: Published as a conference paper in ICLR 2024
\\
  A recent line of works showed regret bounds in reinforcement learning (RL)
can be (nearly) independent of planning horizon, a.k.a.~the horizon-free
bounds. However, these regret bounds only apply to settings where a polynomial
dependency on the size of transition model is allowed, such as tabular Markov
Decision Process (MDP) and linear mixture MDP. We give the first horizon-free
bound for the popular linear MDP setting where the size of the transition model
can be exponentially large or even uncountable. In contrast to prior works
which explicitly estimate the transition model and compute the inhomogeneous
value functions at different time steps, we directly estimate the value
functions and confidence sets. We obtain the horizon-free bound by: (1)
maintaining multiple weighted least square estimators for the value functions;
and (2) a structural lemma which shows the maximal total variation of the
inhomogeneous value functions is bounded by a polynomial factor of the feature
dimension.
\\ ( https://arxiv.org/abs/2403.10738 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10766
Date: Sat, 16 Mar 2024 02:07:45 GMT   (433kb,D)

Title: ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference
Authors: Krzysztof Kacprzyk, Samuel Holt, Jeroen Berrevoets, Zhaozhi Qian,
  Mihaela van der Schaar
Categories: cs.LG stat.ME
Comments: Published in The Twelfth International Conference on Learning
  Representations (ICLR). Copyright 2024 by the author(s)
\\
  Inferring unbiased treatment effects has received widespread attention in the
machine learning community. In recent years, our community has proposed
numerous solutions in standard settings, high-dimensional treatment settings,
and even longitudinal settings. While very diverse, the solution has mostly
relied on neural networks for inference and simultaneous correction of
assignment bias. New approaches typically build on top of previous approaches
by proposing new (or refined) architectures and learning algorithms. However,
the end result -- a neural-network-based inference machine -- remains
unchallenged. In this paper, we introduce a different type of solution in the
longitudinal setting: a closed-form ordinary differential equation (ODE). While
we still rely on continuous optimization to learn an ODE, the resulting
inference machine is no longer a neural network. Doing so yields several
advantages such as interpretability, irregular sampling, and a different set of
identification assumptions. Above all, we consider the introduction of a
completely new type of solution to be our most important contribution as it may
spark entirely new innovations in treatment effects in general. We facilitate
this by formulating our contribution as a framework that can transform any ODE
discovery method into a treatment effects method.
\\ ( https://arxiv.org/abs/2403.10766 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10771
Date: Sat, 16 Mar 2024 02:19:21 GMT   (733kb,D)

Title: A Probabilistic Approach for Alignment with Human Comparisons
Authors: Junyu Cao, Mohsen Bayati
Categories: cs.LG stat.ML
\\
  A growing trend involves integrating human knowledge into learning
frameworks, leveraging subtle human feedback to refine AI models. Despite these
advances, no comprehensive theoretical framework describing the specific
conditions under which human comparisons improve the traditional supervised
fine-tuning process has been developed. To bridge this gap, this paper studies
the effective use of human comparisons to address limitations arising from
noisy data and high-dimensional models. We propose a two-stage "Supervised Fine
Tuning+Human Comparison" (SFT+HC) framework connecting machine learning with
human feedback through a probabilistic bisection approach. The two-stage
framework first learns low-dimensional representations from noisy-labeled data
via an SFT procedure, and then uses human comparisons to improve the model
alignment. To examine the efficacy of the alignment phase, we introduce a novel
concept termed the "label-noise-to-comparison-accuracy" (LNCA) ratio. This
paper theoretically identifies the conditions under which the "SFT+HC"
framework outperforms pure SFT approach, leveraging this ratio to highlight the
advantage of incorporating human evaluators in reducing sample complexity. We
validate that the proposed conditions for the LNCA ratio are met in a case
study conducted via an Amazon Mechanical Turk experiment.
\\ ( https://arxiv.org/abs/2403.10771 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10787
Date: Sat, 16 Mar 2024 03:37:19 GMT   (3303kb,D)

Title: Time Series Representation Learning with Supervised Contrastive Temporal
  Transformer
Authors: Yuansan Liu, Sudanthi Wijewickrema, Christofer Bester, Stephen
  O'Leary, James Bailey
Categories: cs.LG cs.AI
Comments: 8 pages, 8 figures, IJCNN 2024
\\
  Finding effective representations for time series data is a useful but
challenging task. Several works utilize self-supervised or unsupervised
learning methods to address this. However, there still remains the open
question of how to leverage available label information for better
representations. To answer this question, we exploit pre-existing techniques in
time series and representation learning domains and develop a simple, yet novel
fusion model, called: \textbf{S}upervised \textbf{CO}ntrastive
\textbf{T}emporal \textbf{T}ransformer (SCOTT). We first investigate suitable
augmentation methods for various types of time series data to assist with
learning change-invariant representations. Secondly, we combine Transformer and
Temporal Convolutional Networks in a simple way to efficiently learn both
global and local features. Finally, we simplify Supervised Contrastive Loss for
representation learning of labelled time series data. We preliminarily evaluate
SCOTT on a downstream task, Time Series Classification, using 45 datasets from
the UCR archive. The results show that with the representations learnt by
SCOTT, even a weak classifier can perform similar to or better than existing
state-of-the-art models (best performance on 23/45 datasets and highest rank
against 9 baseline models). Afterwards, we investigate SCOTT's ability to
address a real-world task, online Change Point Detection (CPD), on two
datasets: a human activity dataset and a surgical patient dataset. We show that
the model performs with high reliability and efficiency on the online CPD
problem ($\sim$98\% and $\sim$97\% area under precision-recall curve
respectively). Furthermore, we demonstrate the model's potential in tackling
early detection and show it performs best compared to other candidates.
\\ ( https://arxiv.org/abs/2403.10787 ,  3303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10800
Date: Sat, 16 Mar 2024 04:19:48 GMT   (7282kb,D)

Title: Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data
  in Text-Image Encoders
Authors: Andrew Geng, Pin-Yu Chen
Categories: cs.LG
Comments: Accepted in SatML 2024
\\
  When evaluating the performance of a pre-trained model transferred to a
downstream task, it is imperative to assess not only the in-distribution (ID)
accuracy of the downstream model but also its capacity to generalize and
identify out-of-distribution (OOD) samples. In this paper, we unveil the hidden
costs associated with intrusive fine-tuning techniques. Specifically, we
demonstrate that commonly used fine-tuning methods not only distort the
representations necessary for generalizing to covariate-shifted OOD samples
(OOD generalization) but also distort the representations necessary for
detecting semantically-shifted OOD samples (OOD detection). To address these
challenges, we introduce a new model reprogramming approach for fine-tuning,
which we name Reprogrammer. Reprogrammer aims to improve the holistic
performance of the downstream model across ID, OOD generalization, and OOD
detection tasks. Our empirical evidence reveals that Reprogrammer is less
intrusive and yields superior downstream models. Furthermore, we demonstrate
that by appending an additional representation residual connection to
Reprogrammer, we can further preserve pre-training representations, resulting
in an even more safe and robust downstream model capable of excelling in many
ID classification, OOD generalization, and OOD detection settings.
\\ ( https://arxiv.org/abs/2403.10800 ,  7282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10802
Date: Sat, 16 Mar 2024 04:29:21 GMT   (3039kb,D)

Title: Anomaly Detection Based on Isolation Mechanisms: A Survey
Authors: Yang Cao, Haolong Xiang, Hang Zhang, Ye Zhu, Kai Ming Ting
Categories: cs.LG
\\
  Anomaly detection is a longstanding and active research area that has many
applications in domains such as finance, security, and manufacturing. However,
the efficiency and performance of anomaly detection algorithms are challenged
by the large-scale, high-dimensional, and heterogeneous data that are prevalent
in the era of big data. Isolation-based unsupervised anomaly detection is a
novel and effective approach for identifying anomalies in data. It relies on
the idea that anomalies are few and different from normal instances, and thus
can be easily isolated by random partitioning. Isolation-based methods have
several advantages over existing methods, such as low computational complexity,
low memory usage, high scalability, robustness to noise and irrelevant
features, and no need for prior knowledge or heavy parameter tuning. In this
survey, we review the state-of-the-art isolation-based anomaly detection
methods, including their data partitioning strategies, anomaly score functions,
and algorithmic details. We also discuss some extensions and applications of
isolation-based methods in different scenarios, such as detecting anomalies in
streaming data, time series, trajectory, and image datasets. Finally, we
identify some open challenges and future directions for isolation-based anomaly
detection research.
\\ ( https://arxiv.org/abs/2403.10802 ,  3039kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10803
Date: Sat, 16 Mar 2024 04:35:04 GMT   (34kb,D)

Title: Enhancing Out-of-Distribution Detection with Multitesting-based
  Layer-wise Feature Fusion
Authors: Jiawei Li, Sitong Li, Shanshan Wang, Yicheng Zeng, Falong Tan,
  Chuanlong Xie
Categories: cs.LG cs.AI cs.CV
\\
  Deploying machine learning in open environments presents the challenge of
encountering diverse test inputs that differ significantly from the training
data. These out-of-distribution samples may exhibit shifts in local or global
features compared to the training distribution. The machine learning (ML)
community has responded with a number of methods aimed at distinguishing
anomalous inputs from original training data. However, the majority of previous
studies have primarily focused on the output layer or penultimate layer of
pre-trained deep neural networks. In this paper, we propose a novel framework,
Multitesting-based Layer-wise Out-of-Distribution (OOD) Detection (MLOD), to
identify distributional shifts in test samples at different levels of features
through rigorous multiple testing procedure. Our approach distinguishes itself
from existing methods as it does not require modifying the structure or
fine-tuning of the pre-trained classifier. Through extensive experiments, we
demonstrate that our proposed framework can seamlessly integrate with any
existing distance-based inspection method while efficiently utilizing feature
extractors of varying depths. Our scheme effectively enhances the performance
of out-of-distribution detection when compared to baseline methods. In
particular, MLOD-Fisher achieves superior performance in general. When trained
using KNN on CIFAR10, MLOD-Fisher significantly lowers the false positive rate
(FPR) from 24.09% to 7.47% on average compared to merely utilizing the features
of the last layer.
\\ ( https://arxiv.org/abs/2403.10803 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10807
Date: Sat, 16 Mar 2024 04:43:46 GMT   (2301kb,D)

Title: FlyKD: Graph Knowledge Distillation on the Fly with Curriculum Learning
Authors: Eugene Ku
Categories: cs.LG
\\
  Knowledge Distillation (KD) aims to transfer a more capable teacher model's
knowledge to a lighter student model in order to improve the efficiency of the
model, making it faster and more deployable. However, the student model's
optimization process over the noisy pseudo labels (generated by the teacher
model) is tricky and the amount of pseudo labels one can generate is limited
due to Out of Memory (OOM) error. In this paper, we propose FlyKD (Knowledge
Distillation on the Fly) which enables the generation of virtually unlimited
number of pseudo labels, coupled with Curriculum Learning that greatly
alleviates the optimization process over the noisy pseudo labels. Empirically,
we observe that FlyKD outperforms vanilla KD and the renown Local Structure
Preserving Graph Convolutional Network (LSPGCN). Lastly, with the success of
Curriculum Learning, we shed light on a new research direction of improving
optimization over noisy pseudo labels.
\\ ( https://arxiv.org/abs/2403.10807 ,  2301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10819
Date: Sat, 16 Mar 2024 06:06:44 GMT   (986kb,D)

Title: Incentivized Exploration of Non-Stationary Stochastic Bandits
Authors: Sourav Chakraborty and Lijun Chen
Categories: cs.LG cs.AI stat.ML
\\
  We study incentivized exploration for the multi-armed bandit (MAB) problem
with non-stationary reward distributions, where players receive compensation
for exploring arms other than the greedy choice and may provide biased feedback
on the reward. We consider two different non-stationary environments:
abruptly-changing and continuously-changing, and propose respective
incentivized exploration algorithms. We show that the proposed algorithms
achieve sublinear regret and compensation over time, thus effectively
incentivizing exploration despite the nonstationarity and the biased or drifted
feedback.
\\ ( https://arxiv.org/abs/2403.10819 ,  986kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10824
Date: Sat, 16 Mar 2024 06:25:53 GMT   (283kb,D)

Title: LookALike: Human Mimicry based collaborative decision making
Authors: Rabimba Karanjai, Weidong Shi
Categories: cs.LG cs.AI cs.HC
\\
  Artificial General Intelligence falls short when communicating role specific
nuances to other systems. This is more pronounced when building autonomous LLM
agents capable and designed to communicate with each other for real world
problem solving. Humans can communicate context and domain specific nuances
along with knowledge, and that has led to refinement of skills. In this work we
propose and evaluate a novel method that leads to knowledge distillation among
LLM agents leading to realtime human role play preserving unique contexts
without relying on any stored data or pretraining. We also evaluate how our
system performs better in simulated real world tasks compared to state of the
art.
\\ ( https://arxiv.org/abs/2403.10824 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10842
Date: Sat, 16 Mar 2024 07:40:23 GMT   (693kb)

Title: Twin Transformer using Gated Dynamic Learnable Attention mechanism for
  Fault Detection and Diagnosis in the Tennessee Eastman Process
Authors: Mohammad Ali Labbaf-Khaniki, Mohammad Manthouri, Hanieh Ajami
Categories: cs.LG cs.AI
\\
  Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety
and efficiency of industrial processes. We propose a novel FDD methodology for
the Tennessee Eastman Process (TEP), a widely used benchmark for chemical
process control. The model employs two separate Transformer branches, enabling
independent processing of input data and potential extraction of diverse
information. A novel attention mechanism, Gated Dynamic Learnable Attention
(GDLAttention), is introduced which integrates a gating mechanism and dynamic
learning capabilities. The gating mechanism modulates the attention weights,
allowing the model to focus on the most relevant parts of the input. The
dynamic learning approach adapts the attention strategy during training,
potentially leading to improved performance. The attention mechanism uses a
bilinear similarity function, providing greater flexibility in capturing
complex relationships between query and key vectors. In order to assess the
effectiveness of our approach, we tested it against 21 and 18 distinct fault
scenarios in TEP, and compared its performance with several established FDD
techniques. The outcomes indicate that the method outperforms others in terms
of accuracy, false alarm rate, and misclassification rate. This underscores the
robustness and efficacy of the approach for FDD in intricate industrial
processes.
\\ ( https://arxiv.org/abs/2403.10842 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10853
Date: Sat, 16 Mar 2024 08:28:42 GMT   (39314kb,D)

Title: Just Say the Name: Online Continual Learning with Category Names Only
  via Data Generation
Authors: Minhyuk Seo, Diganta Misra, Seongwon Cho, Minjae Lee, Jonghyun Choi
Categories: cs.LG cs.AI cs.CV
\\
  In real-world scenarios, extensive manual annotation for continual learning
is impractical due to prohibitive costs. Although prior arts, influenced by
large-scale webly supervised training, suggest leveraging web-scraped data in
continual learning, this poses challenges such as data imbalance, usage
restrictions, and privacy concerns. Addressing the risks of continual webly
supervised training, we present an online continual learning framework -
Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a
set of generators G along with the learner. When encountering new concepts
(i.e., classes), G-NoCL employs the novel sample complexity-guided data
ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to
optimally sample training data from generated data. Through extensive
experimentation, we demonstrate superior performance of DISCOBER in G-NoCL
online CL benchmarks, covering both In-Distribution (ID) and
Out-of-Distribution (OOD) generalization evaluations, compared to naive
generator-ensembling, web-supervised, and manually annotated data.
\\ ( https://arxiv.org/abs/2403.10853 ,  39314kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10855
Date: Sat, 16 Mar 2024 08:30:55 GMT   (526kb,D)

Title: Reinforcement Learning with Options
Authors: Ayoub Ghriss and Masashi Sugiyama and Alessandro Lazaric
Categories: cs.LG cs.RO
Comments: Master Thesis 2018, MVA ENS Paris-Saclay, Tokyo RIKEN AIP
\\
  The current thesis aims to explore the reinforcement learning field and build
on existing methods to produce improved ones to tackle the problem of learning
in high-dimensional and complex environments. It addresses such goals by
decomposing learning tasks in a hierarchical fashion known as Hierarchical
Reinforcement Learning.
  We start in the first chapter by getting familiar with the Markov Decision
Process framework and presenting some of its recent techniques that the
following chapters use. We then proceed to build our Hierarchical Policy
learning as an answer to the limitations of a single primitive policy. The
hierarchy is composed of a manager agent at the top and employee agents at the
lower level.
  In the last chapter, which is the core of this thesis, we attempt to learn
lower-level elements of the hierarchy independently of the manager level in
what is known as the "Eigenoption". Based on the graph structure of the
environment, Eigenoptions allow us to build agents that are aware of the
geometric and dynamic properties of the environment. Their decision-making has
a special property: it is invariant to symmetric transformations of the
environment, allowing as a consequence to greatly reduce the complexity of the
learning task.
\\ ( https://arxiv.org/abs/2403.10855 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10875
Date: Sat, 16 Mar 2024 09:58:49 GMT   (778kb,D)

Title: Probabilistic World Modeling with Asymmetric Distance Measure
Authors: Meng Song
Categories: cs.LG
\\
  Representation learning is a fundamental task in machine learning, aiming at
uncovering structures from data to facilitate subsequent tasks. However, what
is a good representation for planning and reasoning in a stochastic world
remains an open problem. In this work, we posit that learning a distance
function is essential to allow planning and reasoning in the representation
space. We show that a geometric abstraction of the probabilistic world dynamics
can be embedded into the representation space through asymmetric contrastive
learning. Unlike previous approaches that focus on learning mutual similarity
or compatibility measures, we instead learn an asymmetric similarity function
that reflects the state reachability and allows multi-way probabilistic
inference. Moreover, by conditioning on a common reference state (e.g. the
observer's current state), the learned representation space allows us to
discover the geometrically salient states that only a handful of paths can lead
through. These states can naturally serve as subgoals to break down
long-horizon planning tasks. We evaluate our method in gridworld environments
with various layouts and demonstrate its effectiveness in discovering the
subgoals.
\\ ( https://arxiv.org/abs/2403.10875 ,  778kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10889
Date: Sat, 16 Mar 2024 10:49:27 GMT   (509kb,D)

Title: List Sample Compression and Uniform Convergence
Authors: Steve Hanneke, Shay Moran, and Tom Waknine
Categories: cs.LG stat.ML
\\
  List learning is a variant of supervised classification where the learner
outputs multiple plausible labels for each instance rather than just one. We
investigate classical principles related to generalization within the context
of list learning. Our primary goal is to determine whether classical principles
in the PAC setting retain their applicability in the domain of list PAC
learning. We focus on uniform convergence (which is the basis of Empirical Risk
Minimization) and on sample compression (which is a powerful manifestation of
Occam's Razor). In classical PAC learning, both uniform convergence and sample
compression satisfy a form of `completeness': whenever a class is learnable, it
can also be learned by a learning rule that adheres to these principles. We ask
whether the same completeness holds true in the list learning setting.
  We show that uniform convergence remains equivalent to learnability in the
list PAC learning setting. In contrast, our findings reveal surprising results
regarding sample compression: we prove that when the label space is
$Y=\{0,1,2\}$, then there are 2-list-learnable classes that cannot be
compressed. This refutes the list version of the sample compression conjecture
by Littlestone and Warmuth (1986). We prove an even stronger impossibility
result, showing that there are $2$-list-learnable classes that cannot be
compressed even when the reconstructed function can work with lists of
arbitrarily large size. We prove a similar result for (1-list) PAC learnable
classes when the label space is unbounded. This generalizes a recent result by
arXiv:2308.06424.
\\ ( https://arxiv.org/abs/2403.10889 ,  509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10903
Date: Sat, 16 Mar 2024 11:38:31 GMT   (238kb,D)

Title: DTOR: Decision Tree Outlier Regressor to explain anomalies
Authors: Riccardo Crupi, Alessandro Damiano Sabatino, Immacolata Marano,
  Massimiliano Brinis, Luca Albertazzi, Andrea Cirillo, Andrea Claudio
  Cosentini
Categories: cs.LG cs.AI stat.ML
\\
  Explaining outliers occurrence and mechanism of their occurrence can be
extremely important in a variety of domains. Malfunctions, frauds, threats, in
addition to being correctly identified, oftentimes need a valid explanation in
order to effectively perform actionable counteracts. The ever more widespread
use of sophisticated Machine Learning approach to identify anomalies make such
explanations more challenging. We present the Decision Tree Outlier Regressor
(DTOR), a technique for producing rule-based explanations for individual data
points by estimating anomaly scores generated by an anomaly detection model.
This is accomplished by first applying a Decision Tree Regressor, which
computes the estimation score, and then extracting the relative path associated
with the data point score. Our results demonstrate the robustness of DTOR even
in datasets with a large number of features. Additionally, in contrast to other
rule-based approaches, the generated rules are consistently satisfied by the
points to be explained. Furthermore, our evaluation metrics indicate comparable
performance to Anchors in outlier explanation tasks, with reduced execution
time.
\\ ( https://arxiv.org/abs/2403.10903 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10910
Date: Sat, 16 Mar 2024 12:10:01 GMT   (10168kb,D)

Title: Graph Regularized NMF with L20-norm for Unsupervised Feature Learning
Authors: Zhen Wang and Wenwen Min
Categories: cs.LG
Comments: Submitted to IEEE Trans journal
\\
  Nonnegative Matrix Factorization (NMF) is a widely applied technique in the
fields of machine learning and data mining. Graph Regularized Non-negative
Matrix Factorization (GNMF) is an extension of NMF that incorporates graph
regularization constraints. GNMF has demonstrated exceptional performance in
clustering and dimensionality reduction, effectively discovering inherent
low-dimensional structures embedded within high-dimensional spaces. However,
the sensitivity of GNMF to noise limits its stability and robustness in
practical applications. In order to enhance feature sparsity and mitigate the
impact of noise while mining row sparsity patterns in the data for effective
feature selection, we introduce the $\ell_{2,0}$-norm constraint as the
sparsity constraints for GNMF. We propose an unsupervised feature learning
framework based on GNMF\_$\ell_{20}$ and devise an algorithm based on PALM and
its accelerated version to address this problem. Additionally, we establish the
convergence of the proposed algorithms and validate the efficacy and
superiority of our approach through experiments conducted on both simulated and
real image data.
\\ ( https://arxiv.org/abs/2403.10910 ,  10168kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10923
Date: Sat, 16 Mar 2024 13:35:15 GMT   (591kb,D)

Title: Interpretable Machine Learning for TabPFN
Authors: David Rundel, Julius Kobialka, Constantin von Crailsheim, Matthias
  Feurer, Thomas Nagler, David R\"ugamer
Categories: cs.LG cs.AI stat.CO stat.ML
\\
  The recently developed Prior-Data Fitted Networks (PFNs) have shown very
promising results for applications in low-data regimes. The TabPFN model, a
special case of PFNs for tabular data, is able to achieve state-of-the-art
performance on a variety of classification tasks while producing posterior
predictive distributions in mere seconds by in-context learning without the
need for learning parameters or hyperparameter tuning. This makes TabPFN a very
attractive option for a wide range of domain applications. However, a major
drawback of the method is its lack of interpretability. Therefore, we propose
several adaptations of popular interpretability methods that we specifically
design for TabPFN. By taking advantage of the unique properties of the model,
our adaptations allow for more efficient computations than existing
implementations. In particular, we show how in-context learning facilitates the
estimation of Shapley values by avoiding approximate retraining and enables the
use of Leave-One-Covariate-Out (LOCO) even when working with large-scale
Transformers. In addition, we demonstrate how data valuation methods can be
used to address scalability challenges of TabPFN. Our proposed methods are
implemented in a package tabpfn_iml and made available at
https://github.com/david-rundel/tabpfn_iml.
\\ ( https://arxiv.org/abs/2403.10923 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10961
Date: Sat, 16 Mar 2024 16:16:31 GMT   (3712kb,D)

Title: Energy-Based Models with Applications to Speech and Language Processing
Authors: Zhijian Ou
Categories: cs.LG cs.CL cs.SD eess.AS
Comments: The version before publisher editing
Journal-ref: Foundations and Trends in Signal Processing: Vol. 18: No. 1-2, pp
  1-199
DOI: 10.1561/2000000117
\\
  Energy-Based Models (EBMs) are an important class of probabilistic models,
also known as random fields and undirected graphical models. EBMs are
un-normalized and thus radically different from other popular self-normalized
probabilistic models such as hidden Markov models (HMMs), autoregressive
models, generative adversarial nets (GANs) and variational auto-encoders
(VAEs). Over the past years, EBMs have attracted increasing interest not only
from the core machine learning community, but also from application domains
such as speech, vision, natural language processing (NLP) and so on, due to
significant theoretical and algorithmic progress. The sequential nature of
speech and language also presents special challenges and needs a different
treatment from processing fix-dimensional data (e.g., images). Therefore, the
purpose of this monograph is to present a systematic introduction to
energy-based models, including both algorithmic progress and applications in
speech and language processing. First, the basics of EBMs are introduced,
including classic models, recent models parameterized by neural networks,
sampling methods, and various learning methods from the classic learning
algorithms to the most advanced ones. Then, the application of EBMs in three
different scenarios is presented, i.e., for modeling marginal, conditional and
joint distributions, respectively. 1) EBMs for sequential data with
applications in language modeling, where the main focus is on the marginal
distribution of a sequence itself; 2) EBMs for modeling conditional
distributions of target sequences given observation sequences, with
applications in speech recognition, sequence labeling and text generation; 3)
EBMs for modeling joint distributions of both sequences of observations and
targets, and their applications in semi-supervised learning and calibrated
natural language understanding.
\\ ( https://arxiv.org/abs/2403.10961 ,  3712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10967
Date: Sat, 16 Mar 2024 16:29:40 GMT   (594kb,D)

Title: Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot
  Generalization
Authors: Sai Prasanna, Karim Farid, Raghu Rajan, Andr\'e Biedenkapp
Categories: cs.LG cs.AI
Comments: 33 pages
\\
  Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for
creating generally capable embodied agents. To address the broader challenge,
we start with the simpler setting of contextual reinforcement learning (cRL),
assuming observability of the context values that parameterize the variation in
the system's dynamics, such as the mass or dimensions of a robot, without
making further simplifying assumptions about the observability of the Markovian
state. Toward the goal of ZSG to unseen variation in context, we propose the
contextual recurrent state-space model (cRSSM), which introduces changes to the
world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world
model to incorporate context for inferring latent Markovian states from the
observations and modeling the latent dynamics. Our experiments show that such
systematic incorporation of the context improves the ZSG of the policies
trained on the ``dreams'' of the world model. We further find qualitatively
that our approach allows Dreamer to disentangle the latent state from context,
allowing it to extrapolate its dreams to the many worlds of unseen contexts.
The code for all our experiments is available at
\url{https://github.com/sai-prasanna/dreaming_of_many_worlds}.
\\ ( https://arxiv.org/abs/2403.10967 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10984
Date: Sat, 16 Mar 2024 17:32:59 GMT   (129kb,D)

Title: IoTCO2: Assessing the End-To-End Carbon Footprint of
  Internet-of-Things-Enabled Deep Learning
Authors: Ahmad Faiz, Shahzeen Attari, Gayle Buck, Fan Chen, Lei Jiang
Categories: cs.LG cs.AI cs.CY
Comments: 5 figures, 8 tables
\\
  To improve privacy and ensure quality-of-service (QoS), deep learning (DL)
models are increasingly deployed on Internet of Things (IoT) devices for data
processing, significantly increasing the carbon footprint associated with DL on
IoT, covering both operational and embodied aspects. Existing operational
energy predictors often overlook quantized DL models and emerging neural
processing units (NPUs), while embodied carbon footprint modeling tools neglect
non-computing hardware components common in IoT devices, creating a gap in
accurate carbon footprint modeling tools for IoT-enabled DL. This paper
introduces \textit{\carb}, an end-to-end modeling tool for precise carbon
footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21\%$
deviation in carbon footprint values compared to actual measurements across
various DL models. Additionally, practical applications of \carb are showcased
through multiple user case studies.
\\ ( https://arxiv.org/abs/2403.10984 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10995
Date: Sat, 16 Mar 2024 18:44:56 GMT   (511kb,D)

Title: Edge Private Graph Neural Networks with Singular Value Perturbation
Authors: Tingting Tang, Yue Niu, Salman Avestimehr, Murali Annavaram
Categories: cs.LG cs.AI cs.CR cs.SI
Comments: Accepted at Privacy Enhancing Technologies Symposium (PETS) 2024
\\
  Graph neural networks (GNNs) play a key role in learning representations from
graph-structured data and are demonstrated to be useful in many applications.
However, the GNN training pipeline has been shown to be vulnerable to node
feature leakage and edge extraction attacks. This paper investigates a scenario
where an attacker aims to recover private edge information from a trained GNN
model. Previous studies have employed differential privacy (DP) to add noise
directly to the adjacency matrix or a compact graph representation. The added
perturbations cause the graph structure to be substantially morphed, reducing
the model utility. We propose a new privacy-preserving GNN training algorithm,
Eclipse, that maintains good model utility while providing strong privacy
protection on edges. Eclipse is based on two key observations. First, adjacency
matrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains
GNNs with a low-rank format of the graph via singular values decomposition
(SVD), rather than the original graph. Using the low-rank format, Eclipse
preserves the primary graph topology and removes the remaining residual edges.
Eclipse adds noise to the low-rank singular values instead of the entire graph,
thereby preserving the graph privacy while still maintaining enough of the
graph structure to maintain model utility. We theoretically show Eclipse
provide formal DP guarantee on edges. Experiments on benchmark graph datasets
show that Eclipse achieves significantly better privacy-utility tradeoff
compared to existing privacy-preserving GNN training methods. In particular,
under strong privacy constraints ($\epsilon$ < 4), Eclipse shows significant
gains in the model utility by up to 46%. We further demonstrate that Eclipse
also has better resilience against common edge attacks (e.g., LPA), lowering
the attack AUC by up to 5% compared to other state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.10995 ,  511kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11004
Date: Sat, 16 Mar 2024 19:40:35 GMT   (392kb,D)

Title: Forward Learning of Graph Neural Networks
Authors: Namyong Park, Xing Wang, Antoine Simoulin, Shuai Yang, Grey Yang, Ryan
  Rossi, Puja Trivedi, Nesreen Ahmed
Categories: cs.LG cs.SI
Comments: ICLR 2024
\\
  Graph neural networks (GNNs) have achieved remarkable success across a wide
range of applications, such as recommendation, drug discovery, and question
answering. Behind the success of GNNs lies the backpropagation (BP) algorithm,
which is the de facto standard for training deep neural networks (NNs).
However, despite its effectiveness, BP imposes several constraints, which are
not only biologically implausible, but also limit the scalability, parallelism,
and flexibility in learning NNs. Examples of such constraints include storage
of neural activities computed in the forward pass for use in the subsequent
backward pass, and the dependence of parameter updates on non-local signals. To
address these limitations, the forward-forward algorithm (FF) was recently
proposed as an alternative to BP in the image classification domain, which
trains NNs by performing two forward passes over positive and negative data.
Inspired by this advance, we propose ForwardGNN in this work, a new forward
learning procedure for GNNs, which avoids the constraints imposed by BP via an
effective layer-wise local forward training. ForwardGNN extends the original FF
to deal with graph data and GNNs, and makes it possible to operate without
generating negative inputs (hence no longer forward-forward). Further,
ForwardGNN enables each layer to learn from both the bottom-up and top-down
signals without relying on the backpropagation of errors. Extensive experiments
on real-world datasets show the effectiveness and generality of the proposed
forward graph learning framework. We release our code at
https://github.com/facebookresearch/forwardgnn.
\\ ( https://arxiv.org/abs/2403.11004 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11013
Date: Sat, 16 Mar 2024 20:42:27 GMT   (622kb,D)

Title: Improved Algorithm and Bounds for Successive Projection
Authors: Jiashun Jin, Zheng Tracy Ke, Gabriel Moryoussef, Jiajun Tang, Jingming
  Wang
Categories: cs.LG math.ST stat.TH
Comments: 32 pages, 5 figures
\\
  Given a $K$-vertex simplex in a $d$-dimensional space, suppose we measure $n$
points on the simplex with noise (hence, some of the observed points fall
outside the simplex). Vertex hunting is the problem of estimating the $K$
vertices of the simplex. A popular vertex hunting algorithm is successive
projection algorithm (SPA). However, SPA is observed to perform
unsatisfactorily under strong noise or outliers. We propose pseudo-point SPA
(pp-SPA). It uses a projection step and a denoise step to generate
pseudo-points and feed them into SPA for vertex hunting. We derive error bounds
for pp-SPA, leveraging on extreme value theory of (possibly) high-dimensional
random vectors. The results suggest that pp-SPA has faster rates and better
numerical performances than SPA. Our analysis includes an improved
non-asymptotic bound for the original SPA, which is of independent interest.
\\ ( https://arxiv.org/abs/2403.11013 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11020
Date: Sat, 16 Mar 2024 21:34:24 GMT   (302kb,D)

Title: Accelerating prototype selection with spatial abstraction
Authors: Joel Lu\'is Carbonera
Categories: cs.LG
Comments: 15 pages, 12 figures
\\
  The increasing digitalization in industry and society leads to a growing
abundance of data available to be processed and exploited. However, the high
volume of data requires considerable computational resources for applying
machine learning approaches. Prototype selection techniques have been applied
to reduce the requirements of computational resources that are needed by these
techniques. In this paper, we propose an approach for speeding up existing
prototype selection techniques. It builds an abstract representation of the
dataset, using the notion of spatial partition. The second step uses this
abstract representation to prune the search space efficiently and select a set
of candidate prototypes. After, some conventional prototype selection
algorithms can be applied to the candidates selected by our approach. Our
approach was integrated with five conventional prototype selection algorithms
and tested on 14 widely recognized datasets used in classification tasks. The
performance of the modified algorithms was compared to that of their original
versions in terms of accuracy and reduction rate. The experimental results
demonstrate that, overall, our proposed approach maintains accuracy while
enhancing the reduction rate of the original prototype selection algorithms and
simultaneously reducing their execution times.
\\ ( https://arxiv.org/abs/2403.11020 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11032
Date: Sat, 16 Mar 2024 22:35:21 GMT   (1242kb,D)

Title: FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a
  Multi-Stage Tabular Deep Learning
Authors: Sadaf Khademi, Zohreh Hajiakhondi, Golnaz Vaseghi, Nizal
  Sarrafzadegan, Arash Mohammadi
Categories: cs.LG cs.CV eess.IV
\\
  Familial Hypercholesterolemia (FH) is a genetic disorder characterized by
elevated levels of Low-Density Lipoprotein (LDL) cholesterol or its associated
genes. Early-stage and accurate categorization of FH is of significance
allowing for timely interventions to mitigate the risk of life-threatening
conditions. Conventional diagnosis approach, however, is complex, costly, and a
challenging interpretation task even for experienced clinicians resulting in
high underdiagnosis rates. Although there has been a recent surge of interest
in using Machine Learning (ML) models for early FH detection, existing
solutions only consider a binary classification task solely using classical ML
models. Despite its significance, application of Deep Learning (DL) for FH
detection is in its infancy, possibly, due to categorical nature of the
underlying clinical data. The paper addresses this gap by introducing the
FH-TabNet, which is a multi-stage tabular DL network for multi-class (Definite,
Probable, Possible, and Unlikely) FH detection. The FH-TabNet initially
involves applying a deep tabular data learning architecture (TabNet) for
primary categorization into healthy (Possible/Unlikely) and patient
(Probable/Definite) classes. Subsequently, independent TabNet classifiers are
applied to each subgroup, enabling refined classification. The model's
performance is evaluated through 5-fold cross-validation illustrating superior
performance in categorizing FH patients, particularly in the challenging
low-prevalence subcategories.
\\ ( https://arxiv.org/abs/2403.11032 ,  1242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11041
Date: Sat, 16 Mar 2024 23:24:03 GMT   (613kb,D)

Title: FAGH: Accelerating Federated Learning with Approximated Global Hessian
Authors: Mrinmay Sen, A. K. Qin, Krishna Mohan C
Categories: cs.LG cs.NE
\\
  In federated learning (FL), the significant communication overhead due to the
slow convergence speed of training the global model poses a great challenge.
Specifically, a large number of communication rounds are required to achieve
the convergence in FL. One potential solution is to employ the Newton-based
optimization method for training, known for its quadratic convergence rate.
However, the existing Newton-based FL training methods suffer from either
memory inefficiency or high computational costs for local clients or the
server. To address this issue, we propose an FL with approximated global
Hessian (FAGH) method to accelerate FL training. FAGH leverages the first
moment of the approximated global Hessian and the first moment of the global
gradient to train the global model. By harnessing the approximated global
Hessian curvature, FAGH accelerates the convergence of global model training,
leading to the reduced number of communication rounds and thus the shortened
training time. Experimental results verify FAGH's effectiveness in decreasing
the number of communication rounds and the time required to achieve the
pre-specified objectives of the global model performance in terms of training
and test losses as well as test accuracy. Notably, FAGH outperforms several
state-of-the-art FL training methods.
\\ ( https://arxiv.org/abs/2403.11041 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11044
Date: Sat, 16 Mar 2024 23:52:25 GMT   (940kb,D)

Title: Advancing multivariate time series similarity assessment: an integrated
  computational approach
Authors: Franck Tonle, Henri Tonnang, Milliam Ndadji, Maurice Tchendji, Armand
  Nzeukou, Kennedy Senagi, Saliou Niassy
Categories: cs.LG
\\
  Data mining, particularly the analysis of multivariate time series data,
plays a crucial role in extracting insights from complex systems and supporting
informed decision-making across diverse domains. However, assessing the
similarity of multivariate time series data presents several challenges,
including dealing with large datasets, addressing temporal misalignments, and
the need for efficient and comprehensive analytical frameworks. To address all
these challenges, we propose a novel integrated computational approach known as
Multivariate Time series Alignment and Similarity Assessment (MTASA). MTASA is
built upon a hybrid methodology designed to optimize time series alignment,
complemented by a multiprocessing engine that enhances the utilization of
computational resources. This integrated approach comprises four key
components, each addressing essential aspects of time series similarity
assessment, thereby offering a comprehensive framework for analysis. MTASA is
implemented as an open-source Python library with a user-friendly interface,
making it accessible to researchers and practitioners. To evaluate the
effectiveness of MTASA, we conducted an empirical study focused on assessing
agroecosystem similarity using real-world environmental data. The results from
this study highlight MTASA's superiority, achieving approximately 1.5 times
greater accuracy and twice the speed compared to existing state-of-the-art
integrated frameworks for multivariate time series similarity assessment. It is
hoped that MTASA will significantly enhance the efficiency and accessibility of
multivariate time series analysis, benefitting researchers and practitioners
across various domains. Its capabilities in handling large datasets, addressing
temporal misalignments, and delivering accurate results make MTASA a valuable
tool for deriving insights and aiding decision-making processes in complex
systems.
\\ ( https://arxiv.org/abs/2403.11044 ,  940kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11062
Date: Sun, 17 Mar 2024 02:24:09 GMT   (8603kb,D)

Title: A Simple Mixture Policy Parameterization for Improving Sample Efficiency
  of CVaR Optimization
Authors: Yudong Luo, Yangchen Pan, Han Wang, Philip Torr, Pascal Poupart
Categories: cs.LG math.OC
\\
  Reinforcement learning algorithms utilizing policy gradients (PG) to optimize
Conditional Value at Risk (CVaR) face significant challenges with sample
inefficiency, hindering their practical applications. This inefficiency stems
from two main facts: a focus on tail-end performance that overlooks many
sampled trajectories, and the potential of gradient vanishing when the lower
tail of the return distribution is overly flat. To address these challenges, we
propose a simple mixture policy parameterization. This method integrates a
risk-neutral policy with an adjustable policy to form a risk-averse policy. By
employing this strategy, all collected trajectories can be utilized for policy
updating, and the issue of vanishing gradients is counteracted by stimulating
higher returns through the risk-neutral component, thus lifting the tail and
preventing flatness. Our empirical study reveals that this mixture
parameterization is uniquely effective across a variety of benchmark domains.
Specifically, it excels in identifying risk-averse CVaR policies in some Mujoco
environments where the traditional CVaR-PG fails to learn a reasonable policy.
\\ ( https://arxiv.org/abs/2403.11062 ,  8603kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11087
Date: Sun, 17 Mar 2024 04:42:41 GMT   (444kb,D)

Title: Incorporating Higher-order Structural Information for Graph Clustering
Authors: Qiankun Li, Haobing Liu, Ruobing Jiang, and Tingting Wang
Categories: cs.LG cs.SI
Journal-ref: DASFAA 2024
\\
  Clustering holds profound significance in data mining. In recent years, graph
convolutional network (GCN) has emerged as a powerful tool for deep clustering,
integrating both graph structural information and node attributes. However,
most existing methods ignore the higher-order structural information of the
graph. Evidently, nodes within the same cluster can establish distant
connections. Besides, recent deep clustering methods usually apply a
self-supervised module to monitor the training process of their model, focusing
solely on node attributes without paying attention to graph structure. In this
paper, we propose a novel graph clustering network to make full use of graph
structural information. To capture the higher-order structural information, we
design a graph mutual infomax module, effectively maximizing mutual information
between graph-level and node-level representations, and employ a trinary
self-supervised module that includes modularity as a structural constraint. Our
proposed model outperforms many state-of-the-art methods on various datasets,
demonstrating its superiority.
\\ ( https://arxiv.org/abs/2403.11087 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11100
Date: Sun, 17 Mar 2024 06:08:08 GMT   (3845kb,D)

Title: Graph Expansion in Pruned Recurrent Neural Network Layers Preserve
  Performance
Authors: Suryam Arnav Kalra, Arindam Biswas, Pabitra Mitra, Biswajit Basu
Categories: cs.LG cs.CV cs.NE
Comments: Accepted as tiny paper in ICLR 2024
MSC-class: 05C68
ACM-class: I.2.6
\\
  Expansion property of a graph refers to its strong connectivity as well as
sparseness. It has been reported that deep neural networks can be pruned to a
high degree of sparsity while maintaining their performance. Such pruning is
essential for performing real time sequence learning tasks using recurrent
neural networks in resource constrained platforms. We prune recurrent networks
such as RNNs and LSTMs, maintaining a large spectral gap of the underlying
graphs and ensuring their layerwise expansion properties. We also study the
time unfolded recurrent network graphs in terms of the properties of their
bipartite layers. Experimental results for the benchmark sequence MNIST,
CIFAR-10, and Google speech command data show that expander graph properties
are key to preserving classification accuracy of RNN and LSTM.
\\ ( https://arxiv.org/abs/2403.11100 ,  3845kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11106
Date: Sun, 17 Mar 2024 06:20:28 GMT   (2632kb,D)

Title: Self-Supervised Quantization-Aware Knowledge Distillation
Authors: Kaiqi Zhao, Ming Zhao
Categories: cs.LG cs.AI cs.CV
\\
  Quantization-aware training (QAT) and Knowledge Distillation (KD) are
combined to achieve competitive performance in creating low-bit deep learning
models. However, existing works applying KD to QAT require tedious
hyper-parameter tuning to balance the weights of different loss terms, assume
the availability of labeled training data, and require complex, computationally
intensive training procedures for good performance. To address these
limitations, this paper proposes a novel Self-Supervised Quantization-Aware
Knowledge Distillation (SQAKD) framework. SQAKD first unifies the forward and
backward dynamics of various quantization functions, making it flexible for
incorporating various QAT works. Then it formulates QAT as a co-optimization
problem that simultaneously minimizes the KL-Loss between the full-precision
and low-bit models for KD and the discretization error for quantization,
without supervision from labels. A comprehensive evaluation shows that SQAKD
substantially outperforms the state-of-the-art QAT and KD works for a variety
of model architectures. Our code is at: https://github.com/kaiqi123/SQAKD.git.
\\ ( https://arxiv.org/abs/2403.11106 ,  2632kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11114
Date: Sun, 17 Mar 2024 06:41:09 GMT   (3264kb,D)

Title: Phasic Diversity Optimization for Population-Based Reinforcement
  Learning
Authors: Jingcheng Jiang, Haiyin Piao, Yu Fu, Yihang Hao, Chuanlu Jiang, Ziqi
  Wei, Xin Yang
Categories: cs.LG cs.AI
Comments: 7 pages, 4 figures
MSC-class: 14J60 (Primary)
ACM-class: I.2.9
\\
  Reviewing the previous work of diversity Rein-forcement Learning,diversity is
often obtained via an augmented loss function,which requires a balance between
reward and diversity.Generally,diversity optimization algorithms use
Multi-armed Bandits algorithms to select the coefficient in the pre-defined
space. However, the dynamic distribution of reward signals for MABs or the
conflict between quality and diversity limits the performance of these methods.
We introduce the Phasic Diversity Optimization (PDO) algorithm, a
Population-Based Training framework that separates reward and diversity
training into distinct phases instead of optimizing a multi-objective function.
In the auxiliary phase, agents with poor performance diversified via
determinants will not replace the better agents in the archive. The decoupling
of reward and diversity allows us to use an aggressive diversity optimization
in the auxiliary phase without performance degradation. Furthermore, we
construct a dogfight scenario for aerial agents to demonstrate the practicality
of the PDO algorithm. We introduce two implementations of PDO archive and
conduct tests in the newly proposed adversarial dogfight and MuJoCo
simulations. The results show that our proposed algorithm achieves better
performance than baselines.
\\ ( https://arxiv.org/abs/2403.11114 ,  3264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11144
Date: Sun, 17 Mar 2024 08:50:44 GMT   (187kb,D)

Title: Is Mamba Effective for Time Series Forecasting?
Authors: Zihan Wang and Fanheng Kong and Shi Feng and Ming Wang and Han Zhao
  and Daling Wang and Yifei Zhang
Categories: cs.LG
\\
  In the realm of time series forecasting (TSF), the Transformer has
consistently demonstrated robust performance due to its ability to focus on the
global context and effectively capture long-range dependencies within time, as
well as discern correlations between multiple variables. However, due to the
inefficiencies of the Transformer model and questions surrounding its ability
to capture dependencies, ongoing efforts to refine the Transformer architecture
persist. Recently, state space models (SSMs), e.g. Mamba, have gained traction
due to their ability to capture complex dependencies in sequences, similar to
the Transformer, while maintaining near-linear complexity. In text and image
tasks, Mamba-based models can improve performance and cost savings, creating a
win-win situation. This has piqued our interest in exploring SSM's potential in
TSF tasks. In this paper, we introduce two straightforward SSM-based models for
TSF, S-Mamba and D-Mamba, both employing the Mamba Block to extract variate
correlations. Remarkably, S-Mamba and D-Mamba achieve superior performance
while saving GPU memory and training time. Furthermore, we conduct extensive
experiments to delve deeper into the potential of Mamba compared to the
Transformer in the TSF, aiming to explore a new research direction for this
field. Our code is available at https://github.com/wzhwzhwzh0921/S-D-Mamba.
\\ ( https://arxiv.org/abs/2403.11144 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11199
Date: Sun, 17 Mar 2024 12:55:23 GMT   (1087kb,D)

Title: Graph Unitary Message Passing
Authors: Haiquan Qiu, Yatao Bian, Quanming Yao
Categories: cs.LG cs.AI
Comments: 15 pages, 3 figures
\\
  Message passing mechanism contributes to the success of GNNs in various
applications, but also brings the oversquashing problem. Recent works combat
oversquashing by improving the graph spectrums with rewiring techniques,
disrupting the structural bias in graphs, and having limited improvement on
oversquashing in terms of oversquashing measure. Motivated by unitary RNN, we
propose Graph Unitary Message Passing (GUMP) to alleviate oversquashing in GNNs
by applying unitary adjacency matrix for message passing. To design GUMP, a
transformation is first proposed to make general graphs have unitary adjacency
matrix and keep its structural bias. Then, unitary adjacency matrix is obtained
with a unitary projection algorithm, which is implemented by utilizing the
intrinsic structure of unitary adjacency matrix and allows GUMP to be
permutation-equivariant. Experimental results show the effectiveness of GUMP in
improving the performance on various graph learning tasks.
\\ ( https://arxiv.org/abs/2403.11199 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11204
Date: Sun, 17 Mar 2024 13:06:29 GMT   (665kb,D)

Title: Partitioned Neural Network Training via Synthetic Intermediate Labels
Authors: Cevat Volkan Karada\u{g}, Nezih Topalo\u{g}lu
Categories: cs.LG cs.AI cs.DC
Comments: 12 pages, 10 figures
MSC-class: 68T01 (Primary) 68T07, 68T05 (Secondary)
ACM-class: I.2.6; I.2.11
\\
  The proliferation of extensive neural network architectures, particularly
deep learning models, presents a challenge in terms of resource-intensive
training. GPU memory constraints have become a notable bottleneck in training
such sizable models. Existing strategies, including data parallelism, model
parallelism, pipeline parallelism, and fully sharded data parallelism, offer
partial solutions. Model parallelism, in particular, enables the distribution
of the entire model across multiple GPUs, yet the ensuing data communication
between these partitions slows down training. Additionally, the substantial
memory overhead required to store auxiliary parameters on each GPU compounds
computational demands. Instead of using the entire model for training, this
study advocates partitioning the model across GPUs and generating synthetic
intermediate labels to train individual segments. These labels, produced
through a random process, mitigate memory overhead and computational load. This
approach results in a more efficient training process that minimizes data
communication while maintaining model accuracy. To validate this method, a
6-layer fully connected neural network is partitioned into two parts and its
performance is assessed on the extended MNIST dataset. Experimental results
indicate that the proposed approach achieves similar testing accuracies to
conventional training methods, while significantly reducing memory and
computational requirements. This work contributes to mitigating the
resource-intensive nature of training large neural networks, paving the way for
more efficient deep learning model development.
\\ ( https://arxiv.org/abs/2403.11204 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11206
Date: Sun, 17 Mar 2024 13:14:09 GMT   (709kb,D)

Title: CBR - Boosting Adaptive Classification By Retrieval of Encrypted Network
  Traffic with Out-of-distribution
Authors: Amir Lukach, Ran Dubin, Amit Dvir, Chen Hajaj
Categories: cs.LG cs.CR cs.NI
MSC-class: ACM-class: F.2.2, I.2.7 ACM-class: F.2.2, I.2.7 ACM-class: F.2.2,
  I.2.7 ACM-class: F.2.2, I.2.7 ACM-class: I.2.6
\\
  Encrypted network traffic Classification tackles the problem from different
approaches and with different goals. One of the common approaches is using
Machine learning or Deep Learning-based solutions on a fixed number of classes,
leading to misclassification when an unknown class is given as input. One of
the solutions for handling unknown classes is to retrain the model, however,
retraining models every time they become obsolete is both resource and
time-consuming. Therefore, there is a growing need to allow classification
models to detect and adapt to new classes dynamically, without retraining, but
instead able to detect new classes using few shots learning [1]. In this paper,
we introduce Adaptive Classification By Retrieval CBR, a novel approach for
encrypted network traffic classification. Our new approach is based on an
ANN-based method, which allows us to effectively identify new and existing
classes without retraining the model. The novel approach is simple, yet
effective and achieved similar results to RF with up to 5% difference (usually
less than that) in the classification tasks while having a slight decrease in
the case of new samples (from new classes) without retraining. To summarize,
the new method is a real-time classification, which can classify new classes
without retraining. Furthermore, our solution can be used as a complementary
solution alongside RF or any other machine/deep learning classification method,
as an aggregated solution.
\\ ( https://arxiv.org/abs/2403.11206 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11259
Date: Sun, 17 Mar 2024 16:23:00 GMT   (735kb)

Title: A learning-based solution approach to the application placement problem
  in mobile edge computing under uncertainty
Authors: Taha-Hossein Hejazi, Zahra Ghadimkhani, Arezoo Borji
Categories: cs.LG cs.AI cs.DC eess.SP
\\
  Placing applications in mobile edge computing servers presents a complex
challenge involving many servers, users, and their requests. Existing
algorithms take a long time to solve high-dimensional problems with significant
uncertainty scenarios. Therefore, an efficient approach is required to maximize
the quality of service while considering all technical constraints. One of
these approaches is machine learning, which emulates optimal solutions for
application placement in edge servers. Machine learning models are expected to
learn how to allocate user requests to servers based on the spatial positions
of users and servers. In this study, the problem is formulated as a two-stage
stochastic programming. A sufficient amount of training records is generated by
varying parameters such as user locations, their request rates, and solving the
optimization model. Then, based on the distance features of each user from the
available servers and their request rates, machine learning models generate
decision variables for the first stage of the stochastic optimization model,
which is the user-to-server request allocation, and are employed as independent
decision agents that reliably mimic the optimization model. Support Vector
Machines (SVM) and Multi-layer Perceptron (MLP) are used in this research to
achieve practical decisions from the stochastic optimization models. The
performance of each model has shown an execution effectiveness of over 80%.
This research aims to provide a more efficient approach for tackling
high-dimensional problems and scenarios with uncertainties in mobile edge
computing by leveraging machine learning models for optimal decision-making in
request allocation to edge servers. These results suggest that machine-learning
models can significantly improve solution times compared to conventional
approaches.
\\ ( https://arxiv.org/abs/2403.11259 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11261
Date: Sun, 17 Mar 2024 16:24:07 GMT   (75kb)

Title: A Lie Group Approach to Riemannian Batch Normalization
Authors: Ziheng Chen, Yue Song, Yunmei Liu, Nicu Sebe
Categories: cs.LG cs.AI cs.MS
Comments: Accepted by ICLR 2024
\\
  Manifold-valued measurements exist in numerous applications within computer
vision and machine learning. Recent studies have extended Deep Neural Networks
(DNNs) to manifolds, and concomitantly, normalization techniques have also been
adapted to several manifolds, referred to as Riemannian normalization.
Nonetheless, most of the existing Riemannian normalization methods have been
derived in an ad hoc manner and only apply to specific manifolds. This paper
establishes a unified framework for Riemannian Batch Normalization (RBN)
techniques on Lie groups. Our framework offers the theoretical guarantee of
controlling both the Riemannian mean and variance. Empirically, we focus on
Symmetric Positive Definite (SPD) manifolds, which possess three distinct types
of Lie group structures. Using the deformation concept, we generalize the
existing Lie groups on SPD manifolds into three families of parameterized Lie
groups. Specific normalization layers induced by these Lie groups are then
proposed for SPD neural networks. We demonstrate the effectiveness of our
approach through three sets of experiments: radar recognition, human action
recognition, and electroencephalography (EEG) classification. The code is
available at https://github.com/GitZH-Chen/LieBN.git.
\\ ( https://arxiv.org/abs/2403.11261 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11262
Date: Sun, 17 Mar 2024 16:24:29 GMT   (1629kb,D)

Title: Understanding Diffusion Models by Feynman's Path Integral
Authors: Yuji Hirono, Akinori Tanaka, Kenji Fukushima
Categories: cs.LG cond-mat.stat-mech cs.AI hep-th
Comments: 27 pages, 14 figures
Report-no: RIKEN-iTHEMS-Report-24
\\
  Score-based diffusion models have proven effective in image generation and
have gained widespread usage; however, the underlying factors contributing to
the performance disparity between stochastic and deterministic (i.e., the
probability flow ODEs) sampling schemes remain unclear. We introduce a novel
formulation of diffusion models using Feynman's path integral, which is a
formulation originally developed for quantum physics. We find this formulation
providing comprehensive descriptions of score-based generative models, and
demonstrate the derivation of backward stochastic differential equations and
loss functions.The formulation accommodates an interpolating parameter
connecting stochastic and deterministic sampling schemes, and we identify this
parameter as a counterpart of Planck's constant in quantum physics. This
analogy enables us to apply the Wentzel-Kramers-Brillouin (WKB) expansion, a
well-established technique in quantum physics, for evaluating the negative
log-likelihood to assess the performance disparity between stochastic and
deterministic sampling schemes.
\\ ( https://arxiv.org/abs/2403.11262 ,  1629kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11265
Date: Sun, 17 Mar 2024 16:36:26 GMT   (1571kb,D)

Title: Forging the Forger: An Attempt to Improve Authorship Verification via
  Data Augmentation
Authors: Silvia Corbara and Alejandro Moreo
Categories: cs.LG cs.AI cs.CL
\\
  Authorship Verification (AV) is a text classification task concerned with
inferring whether a candidate text has been written by one specific author or
by someone else. It has been shown that many AV systems are vulnerable to
adversarial attacks, where a malicious author actively tries to fool the
classifier by either concealing their writing style, or by imitating the style
of another author. In this paper, we investigate the potential benefits of
augmenting the classifier training set with (negative) synthetic examples.
These synthetic examples are generated to imitate the style of the author of
interest. We analyze the improvements in classifier prediction that this
augmentation brings to bear in the task of AV in an adversarial setting. In
particular, we experiment with three different generator architectures (one
based on Recurrent Neural Networks, another based on small-scale transformers,
and another based on the popular GPT model) and with two training strategies
(one inspired by standard Language Models, and another inspired by Wasserstein
Generative Adversarial Networks). We evaluate our hypothesis on five datasets
(three of which have been specifically collected to represent an adversarial
setting) and using two learning algorithms for the AV classifier (Support
Vector Machines and Convolutional Neural Networks). This experimentation has
yielded negative results, revealing that, although our methodology proves
effective in many adversarial settings, its benefits are too sporadic for a
pragmatical application.
\\ ( https://arxiv.org/abs/2403.11265 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11292
Date: Sun, 17 Mar 2024 18:08:22 GMT   (977kb,D)

Title: Multi-Relational Graph Neural Network for Out-of-Domain Link Prediction
Authors: Asma Sattar and Georgios Deligiorgis and Marco Trincavelli and Davide
  Bacciu
Categories: cs.LG cs.AI
Comments: 8 pages, 3 figures, 3 Tables, conference [accepted in IEEE WCCI 2024]
\\
  Dynamic multi-relational graphs are an expressive relational representation
for data enclosing entities and relations of different types, and where
relationships are allowed to vary in time. Addressing predictive tasks over
such data requires the ability to find structure embeddings that capture the
diversity of the relationships involved, as well as their dynamic evolution. In
this work, we establish a novel class of challenging tasks for dynamic
multi-relational graphs involving out-of-domain link prediction, where the
relationship being predicted is not available in the input graph. We then
introduce a novel Graph Neural Network model, named GOOD, designed specifically
to tackle the out-of-domain generalization problem. GOOD introduces a novel
design concept for multi-relation embedding aggregation, based on the idea that
good representations are such when it is possible to disentangle the mixing
proportions of the different relational embeddings that have produced it. We
also propose five benchmarks based on two retail domains, where we show that
GOOD can effectively generalize predictions out of known relationship types and
achieve state-of-the-art results. Most importantly, we provide insights into
problems where out-of-domain prediction might be preferred to an in-domain
formulation, that is, where the relationship to be predicted has very few
positive examples.
\\ ( https://arxiv.org/abs/2403.11292 ,  977kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11314
Date: Sun, 17 Mar 2024 19:32:12 GMT   (2142kb,D)

Title: Reasoning in Transformers - Mitigating Spurious Correlations and
  Reasoning Shortcuts
Authors: Daniel Enstr\"om, Viktor Kjellberg, Moa Johansson
Categories: cs.LG cs.CL
\\
  Transformer language models are neural networks used for a wide variety of
tasks concerning natural language, including some that also require logical
reasoning. However, a transformer model may easily learn spurious patterns in
the data, short-circuiting actual reasoning. In this paper we investigate to
what extent transformers can be trained to a) approximate reasoning in
propositional logic while b) avoiding known reasoning shortcuts via spurious
correlations in the training data. To do so, we use a dataset with known
spurious correlation between truth and e.g. the number of rules in the problem.
We augment the data with proofs, and train two models: a generative
transformer, WP-BART, trained on problems and their whole proofs, and a
neuro-symbolic model, SIP-BART, trained on individual proof steps and combining
the generative transformer model BART with a symbolic proof checker. We find
that SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not.
For SIP-BART, we then identify a few remaining reasoning errors, not previously
described in the literature, arising from using a pre-trained language model.
These are qualitatively analysed to create a taxonomy of four different types
of additional pitfalls.
\\ ( https://arxiv.org/abs/2403.11314 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11332
Date: Sun, 17 Mar 2024 20:23:42 GMT   (91kb,D)

Title: Graph Neural Network based Double Machine Learning Estimator of Network
  Causal Effects
Authors: Seyedeh Baharan Khatami, Harsh Parikh, Haowei Chen, Sudeepa Roy, Babak
  Salimi
Categories: cs.LG cs.SI stat.ME
\\
  Our paper addresses the challenge of inferring causal effects in social
network data, characterized by complex interdependencies among individuals
resulting in challenges such as non-independence of units, interference (where
a unit's outcome is affected by neighbors' treatments), and introduction of
additional confounding factors from neighboring units. We propose a novel
methodology combining graph neural networks and double machine learning,
enabling accurate and efficient estimation of direct and peer effects using a
single observational social network. Our approach utilizes graph isomorphism
networks in conjunction with double machine learning to effectively adjust for
network confounders and consistently estimate the desired causal effects. We
demonstrate that our estimator is both asymptotically normal and
semiparametrically efficient. A comprehensive evaluation against four
state-of-the-art baseline methods using three semi-synthetic social network
datasets reveals our method's on-par or superior efficacy in precise causal
effect estimation. Further, we illustrate the practical application of our
method through a case study that investigates the impact of Self-Help Group
participation on financial risk tolerance. The results indicate a significant
positive direct effect, underscoring the potential of our approach in social
network analysis. Additionally, we explore the effects of network sparsity on
estimation performance.
\\ ( https://arxiv.org/abs/2403.11332 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11343
Date: Sun, 17 Mar 2024 21:04:48 GMT   (546kb,D)

Title: Federated Transfer Learning with Differential Privacy
Authors: Mengchu Li, Ye Tian, Yang Feng, Yi Yu
Categories: cs.LG cs.CR math.ST stat.ME stat.ML stat.TH
Comments: 76 pages, 3 figures
\\
  Federated learning is gaining increasing popularity, with data heterogeneity
and privacy being two prominent challenges. In this paper, we address both
issues within a federated transfer learning framework, aiming to enhance
learning on a target data set by leveraging information from multiple
heterogeneous source data sets while adhering to privacy constraints. We
rigorously formulate the notion of \textit{federated differential privacy},
which offers privacy guarantees for each data set without assuming a trusted
central server. Under this privacy constraint, we study three classical
statistical problems, namely univariate mean estimation, low-dimensional linear
regression, and high-dimensional linear regression. By investigating the
minimax rates and identifying the costs of privacy for these problems, we show
that federated differential privacy is an intermediate privacy model between
the well-established local and central models of differential privacy. Our
analyses incorporate data heterogeneity and privacy, highlighting the
fundamental costs of both in federated learning and underscoring the benefit of
knowledge transfer across data sets.
\\ ( https://arxiv.org/abs/2403.11343 ,  546kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11345
Date: Sun, 17 Mar 2024 21:11:55 GMT   (4189kb,D)

Title: Independent RL for Cooperative-Competitive Agents: A Mean-Field
  Perspective
Authors: Muhammad Aneeq uz Zaman, Alec Koppel, Mathieu Lauri\`ere, Tamer
  Ba\c{s}ar
Categories: cs.LG cs.AI cs.GT cs.MA
\\
  We address in this paper Reinforcement Learning (RL) among agents that are
grouped into teams such that there is cooperation within each team but
general-sum (non-zero sum) competition across different teams. To develop an RL
method that provably achieves a Nash equilibrium, we focus on a
linear-quadratic structure. Moreover, to tackle the non-stationarity induced by
multi-agent interactions in the finite population setting, we consider the case
where the number of agents within each team is infinite, i.e., the mean-field
setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We
characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard
invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE
for the finite population game where $M$ is a lower bound on the number of
agents in each team. These structural results motivate an algorithm called
Multi-player Receding-horizon Natural Policy Gradient (MRPG), where each team
minimizes its cumulative cost independently in a receding-horizon manner.
Despite the non-convexity of the problem, we establish that the resulting
algorithm converges to a global NE through a novel problem decomposition into
sub-problems using backward recursive discrete-time Hamilton-Jacobi-Isaacs
(HJI) equations, in which independent natural policy gradient is shown to
exhibit linear convergence under time-independent diagonal dominance.
Experiments illuminate the merits of this approach in practice.
\\ ( https://arxiv.org/abs/2403.11345 ,  4189kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11348
Date: Sun, 17 Mar 2024 21:23:45 GMT   (671kb,D)

Title: COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via
  Probabilistic Circuits
Authors: Mintong Kang, Nezihe Merve G\"urel, Linyi Li, Bo Li
Categories: cs.LG cs.AI stat.ML
Comments: Accepted to ICLR 2024
\\
  Conformal prediction has shown spurring performance in constructing
statistically rigorous prediction sets for arbitrary black-box machine learning
models, assuming the data is exchangeable. However, even small adversarial
perturbations during the inference can violate the exchangeability assumption,
challenge the coverage guarantees, and result in a subsequent decline in
empirical coverage. In this work, we propose a certifiably robust
learning-reasoning conformal prediction framework (COLEP) via probabilistic
circuits, which comprise a data-driven learning component that trains
statistical models to learn different semantic concepts, and a reasoning
component that encodes knowledge and characterizes the relationships among the
trained models for logic reasoning. To achieve exact and efficient reasoning,
we employ probabilistic circuits (PCs) within the reasoning component.
Theoretically, we provide end-to-end certification of prediction coverage for
COLEP in the presence of bounded adversarial perturbations. We also provide
certified coverage considering the finite size of the calibration set.
Furthermore, we prove that COLEP achieves higher prediction coverage and
accuracy over a single model as long as the utilities of knowledge models are
non-trivial. Empirically, we show the validity and tightness of our certified
coverage, demonstrating the robust conformal prediction of COLEP on various
datasets, including GTSRB, CIFAR10, and AwA2. We show that COLEP achieves up to
12% improvement in certified coverage on GTSRB, 9% on CIFAR-10, and 14% on
AwA2.
\\ ( https://arxiv.org/abs/2403.11348 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11353
Date: Sun, 17 Mar 2024 21:52:51 GMT   (13070kb,D)

Title: Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and
  Iterative Self-Training Strategies
Authors: Yunrui Li, Hao Xu, Pengyu Hong
Categories: cs.LG cs.AI physics.chem-ph
\\
  Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various
scientific fields, offering insights into structural information, electronic
properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction
efficiently produces candidate molecules, enabling chemists to compare them
with actual experimental spectra. This process aids in confirming molecular
structures or pinpointing discrepancies, guiding further investigation. Machine
Learning (ML) has then emerged as a promising alternative approach for
predicting atomic NMR chemical shits of molecules given their structures.
Although significant progresses have been made in predicting one-dimensional
(1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to
the lack of annotated NMR training datasets. To address this gap, we propose an
iterative self-training (IST) approach to train a deep learning model for
predicting atomic 2DNMR shifts and assigning peaks in experimental spectra. Our
model undergoes an initial pre-training phase employing a Multi-Task Training
(MTT) approach, which simultaneously leverages annotated 1D NMR datasets of
both $^{1}\text{H}$ and $^{13}\text{C}$ spectra to enhance its understanding of
NMR spectra. Subsequently, the pre-trained model is utilized to generate
pseudo-annotations for unlabelled 2D NMR spectra, which are subsequently used
to refine the 2D NMR prediction model. Our approach iterates between annotated
unlabelled 2D NMR data and refining our 2D NMR prediction model until
convergence. Finally, our model is able to not only accurately predict 2D NMR
but also annotate peaks in experimental 2D NMR spectra. Experimental results
show that our model is capable of accurately handling medium-sized and large
molecules, including polysaccharides, underscoring its effectiveness.
\\ ( https://arxiv.org/abs/2403.11353 ,  13070kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11363
Date: Sun, 17 Mar 2024 22:44:36 GMT   (3188kb,D)

Title: IGANN Sparse: Bridging Sparsity and Interpretability with Non-linear
  Insight
Authors: Theodor Stoecker, Nico Hambauer, Patrick Zschech, Mathias Kraus
Categories: cs.LG cs.AI cs.CY
Comments: Preprint conditionally accepted for archival and presentation at the
  32nd European Conference on Information Systems (ECIS 2024)
\\
  Feature selection is a critical component in predictive analytics that
significantly affects the prediction accuracy and interpretability of models.
Intrinsic methods for feature selection are built directly into model learning,
providing a fast and attractive option for large amounts of data. Machine
learning algorithms, such as penalized regression models (e.g., lasso) are the
most common choice when it comes to in-built feature selection. However, they
fail to capture non-linear relationships, which ultimately affects their
ability to predict outcomes in intricate datasets. In this paper, we propose
IGANN Sparse, a novel machine learning model from the family of generalized
additive models, which promotes sparsity through a non-linear feature selection
process during training. This ensures interpretability through improved model
sparsity without sacrificing predictive performance. Moreover, IGANN Sparse
serves as an exploratory tool for information systems researchers to unveil
important non-linear relationships in domains that are characterized by complex
patterns. Our ongoing research is directed at a thorough evaluation of the
IGANN Sparse model, including user studies that allow to assess how well users
of the model can benefit from the reduced number of features. This will allow
for a deeper understanding of the interactions between linear vs. non-linear
modeling, number of selected features, and predictive performance.
\\ ( https://arxiv.org/abs/2403.11363 ,  3188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11366
Date: Sun, 17 Mar 2024 23:02:04 GMT   (143kb,D)

Title: JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning
Authors: Anique Tahir, Lu Cheng, Huan Liu
Categories: cs.LG cs.CL cs.DC
\\
  The scaling of Large Language Models (LLMs) for retrieval-based tasks,
particularly in Retrieval Augmented Generation (RAG), faces significant memory
constraints, especially when fine-tuning extensive prompt sequences. Current
open-source libraries support full-model inference and fine-tuning across
multiple GPUs but fall short of accommodating the efficient parameter
distribution required for retrieved context. Addressing this gap, we introduce
a novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging
distributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)
compilation and tensor-sharding for efficient resource management, thereby
enabling accelerated fine-tuning with reduced memory requirements. This
advancement significantly improves the scalability and feasibility of
fine-tuning LLMs for complex RAG applications, even on systems with limited GPU
resources. Our experiments show more than 12x improvement in runtime compared
to Hugging Face/DeepSpeed implementation with four GPUs while consuming less
than half the VRAM per GPU. Our library will be open-sourced in due course.
\\ ( https://arxiv.org/abs/2403.11366 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11391
Date: Mon, 18 Mar 2024 00:48:58 GMT   (2096kb,D)

Title: Investigating the Benefits of Projection Head for Representation
  Learning
Authors: Yihao Xue, Eric Gan, Jiayi Ni, Siddharth Joshi, Baharan Mirzasoleiman
Categories: cs.LG cs.CV
Journal-ref: ICLR 2024
\\
  An effective technique for obtaining high-quality representations is adding a
projection head on top of the encoder during training, then discarding it and
using the pre-projection representations. Despite its proven practical
effectiveness, the reason behind the success of this technique is poorly
understood. The pre-projection representations are not directly optimized by
the loss function, raising the question: what makes them better? In this work,
we provide a rigorous theoretical answer to this question. We start by
examining linear models trained with self-supervised contrastive loss. We
reveal that the implicit bias of training algorithms leads to layer-wise
progressive feature weighting, where features become increasingly unequal as we
go deeper into the layers. Consequently, lower layers tend to have more
normalized and less specialized representations. We theoretically characterize
scenarios where such representations are more beneficial, highlighting the
intricate interplay between data augmentation and input features. Additionally,
we demonstrate that introducing non-linearity into the network allows lower
layers to learn features that are completely absent in higher layers. Finally,
we show how this mechanism improves the robustness in supervised contrastive
learning and supervised learning. We empirically validate our results through
various experiments on CIFAR-10/100, UrbanCars and shifted versions of
ImageNet. We also introduce a potential alternative to projection head, which
offers a more interpretable and controllable design.
\\ ( https://arxiv.org/abs/2403.11391 ,  2096kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11395
Date: Mon, 18 Mar 2024 01:07:48 GMT   (2451kb)

Title: Automated data processing and feature engineering for deep learning and
  big data applications: a survey
Authors: Alhassan Mumuni amd Fuseini Mumuni
Categories: cs.LG cs.AI cs.DB
Comments: Journal of Information and Intelligence (2024)
DOI: 10.1016/j.jiixd.2024.01.002
\\
  Modern approach to artificial intelligence (AI) aims to design algorithms
that learn directly from data. This approach has achieved impressive results
and has contributed significantly to the progress of AI, particularly in the
sphere of supervised deep learning. It has also simplified the design of
machine learning systems as the learning process is highly automated. However,
not all data processing tasks in conventional deep learning pipelines have been
automated. In most cases data has to be manually collected, preprocessed and
further extended through data augmentation before they can be effective for
training. Recently, special techniques for automating these tasks have emerged.
The automation of data processing tasks is driven by the need to utilize large
volumes of complex, heterogeneous data for machine learning and big data
applications. Today, end-to-end automated data processing systems based on
automated machine learning (AutoML) techniques are capable of taking raw data
and transforming them into useful features for Big Data tasks by automating all
intermediate processing stages. In this work, we present a thorough review of
approaches for automating data processing tasks in deep learning pipelines,
including automated data preprocessing--e.g., data cleaning, labeling, missing
data imputation, and categorical data encoding--as well as data augmentation
(including synthetic data generation using generative AI methods) and feature
engineering--specifically, automated feature extraction, feature construction
and feature selection. In addition to automating specific data processing
tasks, we discuss the use of AutoML methods and tools to simultaneously
optimize all stages of the machine learning pipeline.
\\ ( https://arxiv.org/abs/2403.11395 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11408
Date: Mon, 18 Mar 2024 01:48:50 GMT   (9134kb,D)

Title: Layer-diverse Negative Sampling for Graph Neural Networks
Authors: Wei Duan, Jie Lu, Yu Guang Wang, Junyu Xuan
Categories: cs.LG
Comments: Published in Transactions on Machine Learning Research (03/2024)
\\
  Graph neural networks (GNNs) are a powerful solution for various structure
learning applications due to their strong representation capabilities for graph
data. However, traditional GNNs, relying on message-passing mechanisms that
gather information exclusively from first-order neighbours (known as positive
samples), can lead to issues such as over-smoothing and over-squashing. To
mitigate these issues, we propose a layer-diverse negative sampling method for
message-passing propagation. This method employs a sampling matrix within a
determinantal point process, which transforms the candidate set into a space
and selectively samples from this space to generate negative samples. To
further enhance the diversity of the negative samples during each forward pass,
we develop a space-squeezing method to achieve layer-wise diversity in
multi-layer GNNs. Experiments on various real-world graph datasets demonstrate
the effectiveness of our approach in improving the diversity of negative
samples and overall learning performance. Moreover, adding negative samples
dynamically changes the graph's topology, thus with the strong potential to
improve the expressiveness of GNNs and reduce the risk of over-squashing.
\\ ( https://arxiv.org/abs/2403.11408 ,  9134kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11418
Date: Mon, 18 Mar 2024 02:12:12 GMT   (13273kb,D)

Title: Variational Sampling of Temporal Trajectories
Authors: Jurijs Nazarovs, Zhichun Huang, Xingjian Zhen, Sourav Pal, Rudrasis
  Chakraborty, Vikas Singh
Categories: cs.LG cs.AI
\\
  A deterministic temporal process can be determined by its trajectory, an
element in the product space of (a) initial condition $z_0 \in \mathcal{Z}$ and
(b) transition function $f: (\mathcal{Z}, \mathcal{T}) \to \mathcal{Z}$ often
influenced by the control of the underlying dynamical system. Existing methods
often model the transition function as a differential equation or as a
recurrent neural network. Despite their effectiveness in predicting future
measurements, few results have successfully established a method for sampling
and statistical inference of trajectories using neural networks, partially due
to constraints in the parameterization. In this work, we introduce a mechanism
to learn the distribution of trajectories by parameterizing the transition
function $f$ explicitly as an element in a function space. Our framework allows
efficient synthesis of novel trajectories, while also directly providing a
convenient tool for inference, i.e., uncertainty estimation, likelihood
evaluations and out of distribution detection for abnormal trajectories. These
capabilities can have implications for various downstream tasks, e.g.,
simulation and evaluation for reinforcement learning.
\\ ( https://arxiv.org/abs/2403.11418 ,  13273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11425
Date: Mon, 18 Mar 2024 02:42:01 GMT   (689kb)

Title: Narrative Feature or Structured Feature? A Study of Large Language
  Models to Identify Cancer Patients at Risk of Heart Failure
Authors: Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas J.
  George, Jiang Bian, Yonghui Wu
Categories: cs.LG cs.CL
Comments: 9 pages, 2 figures, 5 tables
\\
  Cancer treatments are known to introduce cardiotoxicity, negatively impacting
outcomes and survivorship. Identifying cancer patients at risk of heart failure
(HF) is critical to improving cancer treatment outcomes and safety. This study
examined machine learning (ML) models to identify cancer patients at risk of HF
using electronic health records (EHRs), including traditional ML, Time-Aware
long short-term memory (T-LSTM), and large language models (LLMs) using novel
narrative features derived from the structured medical codes. We identified a
cancer cohort of 12,806 patients from the University of Florida Health,
diagnosed with lung, breast, and colorectal cancers, among which 1,602
individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the
best F1 scores, outperforming the traditional support vector machines by 39%,
the T-LSTM deep learning model by 7%, and a widely used transformer model,
BERT, by 5.6%. The analysis shows that the proposed narrative features
remarkably increased feature density and improved performance.
\\ ( https://arxiv.org/abs/2403.11425 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11449
Date: Mon, 18 Mar 2024 03:56:34 GMT   (8242kb,D)

Title: Graph Partial Label Learning with Potential Cause Discovering
Authors: Hang Gao, Jiaguo Yuan, Jiangmeng Li, Chengyu Yao, Fengge Wu, Junsuo
  Zhao, Changwen Zheng
Categories: cs.LG
\\
  Graph Neural Networks (GNNs) have gained considerable attention for their
potential in addressing challenges posed by complex graph-structured data in
diverse domains. However, accurately annotating graph data for training is
difficult due to the inherent complexity and interconnectedness of graphs. To
tackle this issue, we propose a novel graph representation learning method that
enables GNN models to effectively learn discriminative information even in the
presence of noisy labels within the context of Partially Labeled Learning
(PLL). PLL is a critical weakly supervised learning problem, where each
training instance is associated with a set of candidate labels, including both
the true label and additional noisy labels. Our approach leverages potential
cause extraction to obtain graph data that exhibit a higher likelihood of
possessing a causal relationship with the labels. By incorporating auxiliary
training based on the extracted graph data, our model can effectively filter
out the noise contained in the labels. We support the rationale behind our
approach with a series of theoretical analyses. Moreover, we conduct extensive
evaluations and ablation studies on multiple datasets, demonstrating the
superiority of our proposed method.
\\ ( https://arxiv.org/abs/2403.11449 ,  8242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11464
Date: Mon, 18 Mar 2024 04:31:38 GMT   (1817kb,D)

Title: FedSPU: Personalized Federated Learning for Resource-constrained Devices
  with Stochastic Parameter Update
Authors: Ziru Niu, Hai Dong, A. K. Qin
Categories: cs.LG
Comments: 14 pages including ref
MSC-class: 68U35
ACM-class: C.2.4; I.2.11
\\
  Personalized Federated Learning (PFL) is widely employed in IoT applications
to handle high-volume, non-iid client data while ensuring data privacy.
However, heterogeneous edge devices owned by clients may impose varying degrees
of resource constraints, causing computation and communication bottlenecks for
PFL. Federated Dropout has emerged as a popular strategy to address this
challenge, wherein only a subset of the global model, i.e. a
\textit{sub-model}, is trained on a client's device, thereby reducing
computation and communication overheads. Nevertheless, the dropout-based
model-pruning strategy may introduce bias, particularly towards non-iid local
data. When biased sub-models absorb highly divergent parameters from other
clients, performance degradation becomes inevitable. In response, we propose
federated learning with stochastic parameter update (FedSPU). Unlike dropout
that tailors the global model to small-size local sub-models, FedSPU maintains
the full model architecture on each device but randomly freezes a certain
percentage of neurons in the local model during training while updating the
remaining neurons. This approach ensures that a portion of the local model
remains personalized, thereby enhancing the model's robustness against biased
parameters from other clients. Experimental results demonstrate that FedSPU
outperforms federated dropout by 7.57\% on average in terms of accuracy.
Furthermore, an introduced early stopping scheme leads to a significant
reduction of the training time by \(24.8\%\sim70.4\%\) while maintaining high
accuracy.
\\ ( https://arxiv.org/abs/2403.11464 ,  1817kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11472
Date: Mon, 18 Mar 2024 04:44:00 GMT   (6667kb,D)

Title: Accelerating String-Key Learned Index Structures via Memoization-based
  Incremental Training
Authors: Minsu Kim, Jinwoo Hwang, Guseul Heo, Seiyeon Cho, Divya Mahajan,
  Jongse Park
Categories: cs.LG cs.AR cs.DB
Comments: Accepted at VLDB '24; 12 pages + 2 pages (ref), 18 figures, 2 tables
\\
  Learned indexes use machine learning models to learn the mappings between
keys and their corresponding positions in key-value indexes. These indexes use
the mapping information as training data. Learned indexes require frequent
retrainings of their models to incorporate the changes introduced by update
queries. To efficiently retrain the models, existing learned index systems
often harness a linear algebraic QR factorization technique that performs
matrix decomposition. This factorization approach processes all key-position
pairs during each retraining, resulting in compute operations that grow
linearly with the total number of keys and their lengths. Consequently, the
retrainings create a severe performance bottleneck, especially for
variable-length string keys, while the retrainings are crucial for maintaining
high prediction accuracy and in turn, ensuring low query service latency.
  To address this performance problem, we develop an algorithm-hardware
co-designed string-key learned index system, dubbed SIA. In designing SIA, we
leverage a unique algorithmic property of the matrix decomposition-based
training method. Exploiting the property, we develop a memoization-based
incremental training scheme, which only requires computation over updated keys,
while decomposition results of non-updated keys from previous computations can
be reused. We further enhance SIA to offload a portion of this training process
to an FPGA accelerator to not only relieve CPU resources for serving index
queries (i.e., inference), but also accelerate the training itself. Our
evaluation shows that compared to ALEX, LIPP, and SIndex, a state-of-the-art
learned index systems, SIA-accelerated learned indexes offer 2.6x and 3.4x
higher throughput on the two real-world benchmark suites, YCSB and Twitter
cache trace, respectively.
\\ ( https://arxiv.org/abs/2403.11472 ,  6667kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11477
Date: Mon, 18 Mar 2024 04:52:11 GMT   (38kb)

Title: Span-Based Optimal Sample Complexity for Weakly Communicating and
  General Average Reward MDPs
Authors: Matthew Zurek, Yudong Chen
Categories: cs.LG cs.IT math.IT math.OC stat.ML
Comments: 42 pages, 2 figures; this article supersedes arXiv:2311.13469
\\
  We study the sample complexity of learning an $\epsilon$-optimal policy in an
average-reward Markov decision process (MDP) under a generative model. For
weakly communicating MDPs, we establish the complexity bound
$\tilde{O}(SA\frac{H}{\epsilon^2})$, where $H$ is the span of the bias function
of the optimal policy and $SA$ is the cardinality of the state-action space.
Our result is the first that is minimax optimal (up to log factors) in all
parameters $S,A,H$ and $\epsilon$, improving on existing work that either
assumes uniformly bounded mixing times for all policies or has suboptimal
dependence on the parameters. We further investigate sample complexity in
general (non-weakly-communicating) average-reward MDPs. We argue a new
transient time parameter $B$ is necessary, establish an
$\tilde{O}(SA\frac{B+H}{\epsilon^2})$ complexity bound, and prove a matching
(up to log factors) minimax lower bound. Both results are based on reducing the
average-reward MDP to a discounted MDP, which requires new ideas in the general
setting. To establish the optimality of this reduction, we develop improved
bounds for $\gamma$-discounted MDPs, showing that
$\tilde{\Omega}\left(SA\frac{H}{(1-\gamma)^2\epsilon^2}\right)$ samples suffice
to learn an $\epsilon$-optimal policy in weakly communicating MDPs under the
regime that $\gamma\geq 1-1/H$, and
$\tilde{\Omega}\left(SA\frac{B+H}{(1-\gamma)^2\epsilon^2}\right)$ samples
suffice in general MDPs when $\gamma\geq 1-\frac{1}{B+H}$. Both these results
circumvent the well-known lower bound of
$\tilde{\Omega}\left(SA\frac{1}{(1-\gamma)^3\epsilon^2}\right)$ for arbitrary
$\gamma$-discounted MDPs. Our analysis develops upper bounds on certain
instance-dependent variance parameters in terms of the span and transient time
parameters. The weakly communicating bounds are tighter than those based on the
mixing time or diameter of the MDP and may be of broader use.
\\ ( https://arxiv.org/abs/2403.11477 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11482
Date: Mon, 18 Mar 2024 05:10:13 GMT   (35240kb,D)

Title: SeisFusion: Constrained Diffusion Model with Input Guidance for 3D
  Seismic Data Interpolation and Reconstruction
Authors: Shuang Wang, Fei Deng, Peifan Jiang, Zishan Gong, Xiaolin Wei, and
  Yuqing Wang
Categories: cs.LG physics.geo-ph
\\
  Geographical, physical, or economic constraints often result in missing
traces within seismic data, making the reconstruction of complete seismic data
a crucial step in seismic data processing. Traditional methods for seismic data
reconstruction require the selection of multiple empirical parameters and
struggle to handle large-scale continuous missing data. With the development of
deep learning, various neural networks have demonstrated powerful
reconstruction capabilities. However, these convolutional neural networks
represent a point-to-point reconstruction approach that may not cover the
entire distribution of the dataset. Consequently, when dealing with seismic
data featuring complex missing patterns, such networks may experience varying
degrees of performance degradation. In response to this challenge, we propose a
novel diffusion model reconstruction framework tailored for 3D seismic data. To
constrain the results generated by the diffusion model, we introduce
conditional supervision constraints into the diffusion model, constraining the
generated data of the diffusion model based on the input data to be
reconstructed. We introduce a 3D neural network architecture into the diffusion
model, successfully extending the 2D diffusion model to 3D space. Additionally,
we refine the model's generation process by incorporating missing data into the
generation process, resulting in reconstructions with higher consistency.
Through ablation studies determining optimal parameter values, our method
exhibits superior reconstruction accuracy when applied to both field datasets
and synthetic datasets, effectively addressing a wide range of complex missing
patterns. Our implementation is available at
https://github.com/WAL-l/SeisFusion.
\\ ( https://arxiv.org/abs/2403.11482 ,  35240kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11483
Date: Mon, 18 Mar 2024 05:12:54 GMT   (2109kb,D)

Title: Open-World Semi-Supervised Learning for Node Classification
Authors: Yanling Wang, Jing Zhang, Lingxi Zhang, Lixin Liu, Yuxiao Dong,
  Cuiping Li, Hong Chen, Hongzhi Yin
Categories: cs.LG cs.AI cs.SI
Comments: Accepted by ICDE 2024
\\
  Open-world semi-supervised learning (Open-world SSL) for node classification,
that classifies unlabeled nodes into seen classes or multiple novel classes, is
a practical but under-explored problem in the graph community. As only seen
classes have human labels, they are usually better learned than novel classes,
and thus exhibit smaller intra-class variances within the embedding space
(named as imbalance of intra-class variances between seen and novel classes).
Based on empirical and theoretical analysis, we find the variance imbalance can
negatively impact the model performance. Pre-trained feature encoders can
alleviate this issue via producing compact representations for novel classes.
However, creating general pre-trained encoders for various types of graph data
has been proven to be challenging. As such, there is a demand for an effective
method that does not rely on pre-trained graph encoders. In this paper, we
propose an IMbalance-Aware method named OpenIMA for Open-world semi-supervised
node classification, which trains the node classification model from scratch
via contrastive learning with bias-reduced pseudo labels. Extensive experiments
on seven popular graph benchmarks demonstrate the effectiveness of OpenIMA, and
the source code has been available on GitHub.
\\ ( https://arxiv.org/abs/2403.11483 ,  2109kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11491
Date: Mon, 18 Mar 2024 05:49:45 GMT   (524kb,D)

Title: Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting
Authors: Mingkui Tan, Guohao Chen, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Peilin
  Zhao, Shuaicheng Niu
Categories: cs.LG
Comments: 20 pages, 14 tables, 11 figures. arXiv admin note: substantial text
  overlap with arXiv:2204.02610
\\
  Test-time adaptation (TTA) seeks to tackle potential distribution shifts
between training and test data by adapting a given model w.r.t. any test
sample. Although recent TTA has shown promising performance, we still face two
key challenges: 1) prior methods perform backpropagation for each test sample,
resulting in unbearable optimization costs to many applications; 2) while
existing TTA can significantly improve the test performance on
out-of-distribution data, they often suffer from severe performance degradation
on in-distribution data after TTA (known as forgetting). To this end, we have
proposed an Efficient Anti-Forgetting Test-Time Adaptation (EATA) method which
develops an active sample selection criterion to identify reliable and
non-redundant samples for test-time entropy minimization. To alleviate
forgetting, EATA introduces a Fisher regularizer estimated from test samples to
constrain important model parameters from drastic changes. However, in EATA,
the adopted entropy loss consistently assigns higher confidence to predictions
even for samples that are underlying uncertain, leading to overconfident
predictions. To tackle this, we further propose EATA with Calibration (EATA-C)
to separately exploit the reducible model uncertainty and the inherent data
uncertainty for calibrated TTA. Specifically, we measure the model uncertainty
by the divergence between predictions from the full network and its
sub-networks, on which we propose a divergence loss to encourage consistent
predictions instead of overconfident ones. To further recalibrate prediction
confidence, we utilize the disagreement among predicted labels as an indicator
of the data uncertainty, and then devise a min-max entropy regularizer to
selectively increase and decrease prediction confidence for different samples.
Experiments on image classification and semantic segmentation verify the
effectiveness of our methods.
\\ ( https://arxiv.org/abs/2403.11491 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11495
Date: Mon, 18 Mar 2024 05:59:56 GMT   (2501kb,D)

Title: Semantic-Enhanced Representation Learning for Road Networks with
  Temporal Dynamics
Authors: Yile Chen, Xiucheng Li, Gao Cong, Zhifeng Bao, Cheng Long
Categories: cs.LG cs.AI
\\
  In this study, we introduce a novel framework called Toast for learning
general-purpose representations of road networks, along with its advanced
counterpart DyToast, designed to enhance the integration of temporal dynamics
to boost the performance of various time-sensitive downstream tasks.
Specifically, we propose to encode two pivotal semantic characteristics
intrinsic to road networks: traffic patterns and traveling semantics. To
achieve this, we refine the skip-gram module by incorporating auxiliary
objectives aimed at predicting the traffic context associated with a target
road segment. Moreover, we leverage trajectory data and design pre-training
strategies based on Transformer to distill traveling semantics on road
networks. DyToast further augments this framework by employing unified
trigonometric functions characterized by their beneficial properties, enabling
the capture of temporal evolution and dynamic nature of road networks more
effectively. With these proposed techniques, we can obtain representations that
encode multi-faceted aspects of knowledge within road networks, applicable
across both road segment-based applications and trajectory-based applications.
Extensive experiments on two real-world datasets across three tasks demonstrate
that our proposed framework consistently outperforms the state-of-the-art
baselines by a significant margin.
\\ ( https://arxiv.org/abs/2403.11495 ,  2501kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11520
Date: Mon, 18 Mar 2024 07:14:21 GMT   (1603kb,D)

Title: State-Separated SARSA: A Practical Sequential Decision-Making Algorithm
  with Recovering Rewards
Authors: Yuto Tanimoto, Kenji Fukumizu
Categories: cs.LG stat.ML
\\
  While many multi-armed bandit algorithms assume that rewards for all arms are
constant across rounds, this assumption does not hold in many real-world
scenarios. This paper considers the setting of recovering bandits (Pike-Burke &
Grunewalder, 2019), where the reward depends on the number of rounds elapsed
since the last time an arm was pulled. We propose a new reinforcement learning
(RL) algorithm tailored to this setting, named the State-Separate SARSA
(SS-SARSA) algorithm, which treats rounds as states. The SS-SARSA algorithm
achieves efficient learning by reducing the number of state combinations
required for Q-learning/SARSA, which often suffers from combinatorial issues
for large-scale RL problems. Additionally, it makes minimal assumptions about
the reward structure and offers lower computational complexity. Furthermore, we
prove asymptotic convergence to an optimal policy under mild assumptions.
Simulation studies demonstrate the superior performance of our algorithm across
various settings.
\\ ( https://arxiv.org/abs/2403.11520 ,  1603kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11544
Date: Mon, 18 Mar 2024 07:54:11 GMT   (63kb)

Title: RL en Markov Games with Independent Function Approximation: Improved
  Sample Complexity Bound under the Local Access Model
Authors: Junyi Fan, Yuxuan Han, Jialin Zeng, Jian-Feng Cai, Yang Wang, Yang
  Xiang, Jiheng Zhang
Categories: cs.LG
Comments: Accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2024)
\\
  Efficiently learning equilibria with large state and action spaces in
general-sum Markov games while overcoming the curse of multi-agency is a
challenging problem. Recent works have attempted to solve this problem by
employing independent linear function classes to approximate the marginal
$Q$-value for each agent. However, existing sample complexity bounds under such
a framework have a suboptimal dependency on the desired accuracy $\varepsilon$
or the action space. In this work, we introduce a new algorithm,
Lin-Confident-FTRL, for learning coarse correlated equilibria (CCE) with local
access to the simulator, i.e., one can interact with the underlying environment
on the visited states. Up to a logarithmic dependence on the size of the state
space, Lin-Confident-FTRL learns $\epsilon$-CCE with a provable optimal
accuracy bound $O(\epsilon^{-2})$ and gets rids of the linear dependency on the
action space, while scaling polynomially with relevant problem parameters (such
as the number of agents and time horizon). Moreover, our analysis of
Linear-Confident-FTRL generalizes the virtual policy iteration technique in the
single-agent local planning literature, which yields a new computationally
efficient algorithm with a tighter sample complexity bound when assuming random
access to the simulator.
\\ ( https://arxiv.org/abs/2403.11544 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11574
Date: Mon, 18 Mar 2024 08:50:30 GMT   (75kb)

Title: Offline Multitask Representation Learning for Reinforcement Learning
Authors: Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi
  Wang, Ming Yin, Doina Precup
Categories: cs.LG
\\
  We study offline multitask representation learning in reinforcement learning
(RL), where a learner is provided with an offline dataset from different tasks
that share a common representation and is asked to learn the shared
representation. We theoretically investigate offline multitask low-rank RL, and
propose a new algorithm called MORL for offline multitask representation
learning. Furthermore, we examine downstream RL in reward-free, offline and
online scenarios, where a new task is introduced to the agent that shares the
same representation as the upstream offline tasks. Our theoretical results
demonstrate the benefits of using the learned representation from the upstream
offline task instead of directly learning the representation of the low-rank
model.
\\ ( https://arxiv.org/abs/2403.11574 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11585
Date: Mon, 18 Mar 2024 08:58:47 GMT   (3698kb,D)

Title: Linguacodus: A Synergistic Framework for Transformative Code Generation
  in Machine Learning Pipelines
Authors: Ekaterina Trofimova, Emil Sataev, Andrey E. Ustyuzhanin
Categories: cs.LG cs.AI cs.CL cs.PL cs.SE
\\
  In the ever-evolving landscape of machine learning, seamless translation of
natural language descriptions into executable code remains a formidable
challenge. This paper introduces Linguacodus, an innovative framework designed
to tackle this challenge by deploying a dynamic pipeline that iteratively
transforms natural language task descriptions into code through high-level
data-shaping instructions. The core of Linguacodus is a fine-tuned large
language model (LLM), empowered to evaluate diverse solutions for various
problems and select the most fitting one for a given task. This paper details
the fine-tuning process, and sheds light on how natural language descriptions
can be translated into functional code. Linguacodus represents a substantial
leap towards automated code generation, effectively bridging the gap between
task descriptions and executable code. It holds great promise for advancing
machine learning applications across diverse domains. Additionally, we propose
an algorithm capable of transforming a natural description of an ML task into
code with minimal human interaction. In extensive experiments on a vast machine
learning code dataset originating from Kaggle, we showcase the effectiveness of
Linguacodus. The investigations highlight its potential applications across
diverse domains, emphasizing its impact on applied machine learning in various
scientific fields.
\\ ( https://arxiv.org/abs/2403.11585 ,  3698kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11637
Date: Mon, 18 Mar 2024 10:19:52 GMT   (227kb,D)

Title: The Value of Reward Lookahead in Reinforcement Learning
Authors: Nadav Merlis, Dorian Baudry, Vianney Perchet
Categories: cs.LG stat.ML
\\
  In reinforcement learning (RL), agents sequentially interact with changing
environments while aiming to maximize the obtained rewards. Usually, rewards
are observed only after acting, and so the goal is to maximize the expected
cumulative reward. Yet, in many practical settings, reward information is
observed in advance -- prices are observed before performing transactions;
nearby traffic information is partially known; and goals are oftentimes given
to agents prior to the interaction. In this work, we aim to quantifiably
analyze the value of such future reward information through the lens of
competitive analysis. In particular, we measure the ratio between the value of
standard RL agents and that of agents with partial future-reward lookahead. We
characterize the worst-case reward distribution and derive exact ratios for the
worst-case reward expectations. Surprisingly, the resulting ratios relate to
known quantities in offline RL and reward-free exploration. We further provide
tight bounds for the ratio given the worst-case dynamics. Our results cover the
full spectrum between observing the immediate rewards before acting to
observing all the rewards before the interaction starts.
\\ ( https://arxiv.org/abs/2403.11637 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11686
Date: Mon, 18 Mar 2024 11:37:42 GMT   (825kb,D)

Title: Crystalformer: Infinitely Connected Attention for Periodic Structure
  Encoding
Authors: Tatsunori Taniai, Ryo Igarashi, Yuta Suzuki, Naoya Chiba, Kotaro
  Saito, Yoshitaka Ushiku, Kanta Ono
Categories: cs.LG cond-mat.mtrl-sci physics.comp-ph
Comments: 13 main pages, 3 figures, 4 tables, 10 appendix pages. Published as a
  conference paper at ICLR 2024. For more information, see
  https://omron-sinicx.github.io/crystalformer/
\\
  Predicting physical properties of materials from their crystal structures is
a fundamental problem in materials science. In peripheral areas such as the
prediction of molecular properties, fully connected attention networks have
been shown to be successful. However, unlike these finite atom arrangements,
crystal structures are infinitely repeating, periodic arrangements of atoms,
whose fully connected attention results in infinitely connected attention. In
this work, we show that this infinitely connected attention can lead to a
computationally tractable formulation, interpreted as neural potential
summation, that performs infinite interatomic potential summations in a deeply
learned feature space. We then propose a simple yet effective Transformer-based
encoder architecture for crystal structures called Crystalformer. Compared to
an existing Transformer-based model, the proposed model requires only 29.4% of
the number of parameters, with minimal modifications to the original
Transformer architecture. Despite the architectural simplicity, the proposed
method outperforms state-of-the-art methods for various property regression
tasks on the Materials Project and JARVIS-DFT datasets.
\\ ( https://arxiv.org/abs/2403.11686 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11696
Date: Mon, 18 Mar 2024 11:52:33 GMT   (203kb,D)

Title: Generalization error of spectral algorithms
Authors: Maksim Velikanov, Maxim Panov, Dmitry Yarotsky
Categories: cs.LG stat.ML
\\
  The asymptotically precise estimation of the generalization of kernel methods
has recently received attention due to the parallels between neural networks
and their associated kernels. However, prior works derive such estimates for
training by kernel ridge regression (KRR), whereas neural networks are
typically trained with gradient descent (GD). In the present work, we consider
the training of kernels with a family of $\textit{spectral algorithms}$
specified by profile $h(\lambda)$, and including KRR and GD as special cases.
Then, we derive the generalization error as a functional of learning profile
$h(\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional
translation-invariant model. Under power-law assumptions on the spectrum of the
kernel and target, we use our framework to (i) give full loss asymptotics for
both noisy and noiseless observations (ii) show that the loss localizes on
certain spectral scales, giving a new perspective on the KRR saturation
phenomenon (iii) conjecture, and demonstrate for the considered data models,
the universality of the loss w.r.t. non-spectral details of the problem, but
only in case of noisy observation.
\\ ( https://arxiv.org/abs/2403.11696 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11722
Date: Mon, 18 Mar 2024 12:22:11 GMT   (113kb,D)

Title: Time Series Compression using Quaternion Valued Neural Networks and
  Quaternion Backpropagation
Authors: Johannes P\"oppelbaum and Andreas Schwung
Categories: cs.LG
\\
  We propose a novel quaternionic time-series compression methodology where we
divide a long time-series into segments of data, extract the min, max, mean and
standard deviation of these chunks as representative features and encapsulate
them in a quaternion, yielding a quaternion valued time-series. This
time-series is processed using quaternion valued neural network layers, where
we aim to preserve the relation between these features through the usage of the
Hamilton product. To train this quaternion neural network, we derive quaternion
backpropagation employing the GHR calculus, which is required for a valid
product and chain rule in quaternion space. Furthermore, we investigate the
connection between the derived update rules and automatic differentiation. We
apply our proposed compression method on the Tennessee Eastman Dataset, where
we perform fault classification using the compressed data in two settings: a
fully supervised one and in a semi supervised, contrastive learning setting.
Both times, we were able to outperform real valued counterparts as well as two
baseline models: one with the uncompressed time-series as the input and the
other with a regular downsampling using the mean. Further, we could improve the
classification benchmark set by SimCLR-TS from 81.43% to 83.90%.
\\ ( https://arxiv.org/abs/2403.11722 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11728
Date: Mon, 18 Mar 2024 12:37:41 GMT   (639kb)

Title: PITA: Physics-Informed Trajectory Autoencoder
Authors: Johannes Fischer, Kevin R\"osch, Martin Lauer, Christoph Stiller
Categories: cs.LG cs.RO
\\
  Validating robotic systems in safety-critical appli-cations requires testing
in many scenarios including rare edgecases that are unlikely to occur,
requiring to complement real-world testing with testing in simulation.
Generative models canbe used to augment real-world datasets with generated data
toproduce edge case scenarios by sampling in a learned latentspace.
Autoencoders can learn said latent representation for aspecific domain by
learning to reconstruct the input data froma lower-dimensional intermediate
representation. However, theresulting trajectories are not necessarily
physically plausible, butinstead typically contain noise that is not present in
the inputtrajectory. To resolve this issue, we propose the novel
Physics-Informed Trajectory Autoencoder (PITA) architecture, whichincorporates
a physical dynamics model into the loss functionof the autoencoder. This
results in smooth trajectories that notonly reconstruct the input trajectory
but also adhere to thephysical model. We evaluate PITA on a real-world dataset
ofvehicle trajectories and compare its performance to a normalautoencoder and a
state-of-the-art action-space autoencoder.
\\ ( https://arxiv.org/abs/2403.11728 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11743
Date: Mon, 18 Mar 2024 12:55:40 GMT   (14273kb,D)

Title: PARMESAN: Parameter-Free Memory Search and Transduction for Dense
  Prediction Tasks
Authors: Philip Matthias Winter, Maria Wimmer, David Major, Dimitrios Lenis,
  Astrid Berg, Theresa Neubauer, Gaia Romana De Paolis, Johannes Novotny,
  Sophia Ulonska, Katja B\"uhler
Categories: cs.LG stat.ML
Comments: preprint, 27 pages, 8 figures
\\
  In this work we address flexibility in deep learning by means of transductive
reasoning. For adaptation to new tasks or new data, existing methods typically
involve tuning of learnable parameters or even complete re-training from
scratch, rendering such approaches unflexible in practice. We argue that the
notion of separating computation from memory by the means of transduction can
act as a stepping stone for solving these issues. We therefore propose PARMESAN
(parameter-free memory search and transduction), a scalable transduction method
which leverages a memory module for solving dense prediction tasks. At
inference, hidden representations in memory are being searched to find
corresponding examples. In contrast to other methods, PARMESAN learns without
the requirement for any continuous training or fine-tuning of learnable
parameters simply by modifying the memory content. Our method is compatible
with commonly used neural architectures and canonically transfers to 1D, 2D,
and 3D grid-based data. We demonstrate the capabilities of our approach at
complex tasks such as continual and few-shot learning. PARMESAN learns up to
370 times faster than common baselines while being on par in terms of
predictive performance, knowledge retention, and data-efficiency.
\\ ( https://arxiv.org/abs/2403.11743 ,  14273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11772
Date: Mon, 18 Mar 2024 13:30:12 GMT   (334kb,D)

Title: S-JEPA: towards seamless cross-dataset transfer through dynamic spatial
  attention
Authors: Pierre Guetschel, Thomas Moreau, Michael Tangermann
Categories: cs.LG cs.AI
Comments: Submitted to 9th Graz BCI Conference 2024
\\
  Motivated by the challenge of seamless cross-dataset transfer in EEG signal
processing, this article presents an exploratory study on the use of Joint
Embedding Predictive Architectures (JEPAs). In recent years, self-supervised
learning has emerged as a promising approach for transfer learning in various
domains. However, its application to EEG signals remains largely unexplored. In
this article, we introduce Signal-JEPA for representing EEG recordings which
includes a novel domain-specific spatial block masking strategy and three novel
architectures for downstream classification. The study is conducted on a
54~subjects dataset and the downstream performance of the models is evaluated
on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study
provides preliminary evidence for the potential of JEPAs in EEG signal
encoding. Notably, our results highlight the importance of spatial filtering
for accurate downstream classification and reveal an influence of the length of
the pre-training examples but not of the mask size on the downstream
performance.
\\ ( https://arxiv.org/abs/2403.11772 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11782
Date: Mon, 18 Mar 2024 13:40:48 GMT   (2689kb,D)

Title: A tutorial on learning from preferences and choices with Gaussian
  Processes
Authors: Alessio Benavoli and Dario Azzimonti
Categories: cs.LG stat.ML
\\
  Preference modelling lies at the intersection of economics, decision theory,
machine learning and statistics. By understanding individuals' preferences and
how they make choices, we can build products that closely match their
expectations, paving the way for more efficient and personalised applications
across a wide range of domains. The objective of this tutorial is to present a
cohesive and comprehensive framework for preference learning with Gaussian
Processes (GPs), demonstrating how to seamlessly incorporate rationality
principles (from economics and decision theory) into the learning process. By
suitably tailoring the likelihood function, this framework enables the
construction of preference learning models that encompass random utility
models, limits of discernment, and scenarios with multiple conflicting
utilities for both object- and label-preference. This tutorial builds upon
established research while simultaneously introducing some novel GP-based
models to address specific gaps in the existing literature.
\\ ( https://arxiv.org/abs/2403.11782 ,  2689kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11795
Date: Mon, 18 Mar 2024 13:53:17 GMT   (263kb,D)

Title: Low-Cost Privacy-Aware Decentralized Learning
Authors: Sayan Biswas, Davide Frey, Romaric Gaudel, Anne-Marie Kermarrec,
  Dimitri Ler\'ev\'erend, Rafael Pires, Rishi Sharma, Fran\c{c}ois Ta\"iani
Categories: cs.LG cs.DC
\\
  This paper introduces ZIP-DL, a novel privacy-aware decentralized learning
(DL) algorithm that relies on adding correlated noise to each model update
during the model training process. This technique ensures that the added noise
almost neutralizes itself during the aggregation process due to its
correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL
does not require multiple communication rounds for noise cancellation,
addressing the common trade-off between privacy protection and communication
overhead. We provide theoretical guarantees for both convergence speed and
privacy guarantees, thereby making ZIP-DL applicable to practical scenarios.
Our extensive experimental study shows that ZIP-DL achieves the best trade-off
between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the
effectiveness of a linkability attack by up to 52 points compared to baseline
DL, and (ii) achieves up to 37 more accuracy points for the same vulnerability
under membership inference attacks against a privacy-preserving competitor
\\ ( https://arxiv.org/abs/2403.11795 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11840
Date: Mon, 18 Mar 2024 14:50:48 GMT   (288kb)

Title: Multi-Criteria Comparison as a Method of Advancing Knowledge-Guided
  Machine Learning
Authors: Jason L. Harman and Jaelle Scheuerman
Categories: cs.LG
Comments: Paper presented at AAAI 2022 Fall Symposium on Knowledge Guided
  Machine Learning, Washington D.C
\\
  This paper describes a generalizable model evaluation method that can be
adapted to evaluate AI/ML models across multiple criteria including core
scientific principles and more practical outcomes. Emerging from prediction
competitions in Psychology and Decision Science, the method evaluates a group
of candidate models of varying type and structure across multiple scientific,
theoretic, and practical criteria. Ordinal ranking of criteria scores are
evaluated using voting rules from the field of computational social choice and
allow the comparison of divergent measures and types of models in a holistic
evaluation. Additional advantages and applications are discussed.
\\ ( https://arxiv.org/abs/2403.11840 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11843
Date: Mon, 18 Mar 2024 14:53:48 GMT   (214kb)

Title: Fuzzy Rough Choquet Distances for Classification
Authors: Adnan Theerens and Chris Cornelis
Categories: cs.LG cs.AI
\\
  This paper introduces a novel Choquet distance using fuzzy rough set based
measures. The proposed distance measure combines the attribute information
received from fuzzy rough set theory with the flexibility of the Choquet
integral. This approach is designed to adeptly capture non-linear relationships
within the data, acknowledging the interplay of the conditional attributes
towards the decision attribute and resulting in a more flexible and accurate
distance. We explore its application in the context of machine learning, with a
specific emphasis on distance-based classification approaches (e.g. k-nearest
neighbours). The paper examines two fuzzy rough set based measures that are
based on the positive region. Moreover, we explore two procedures for
monotonizing the measures derived from fuzzy rough set theory, making them
suitable for use with the Choquet integral, and investigate their differences.
\\ ( https://arxiv.org/abs/2403.11843 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11844
Date: Mon, 18 Mar 2024 14:55:45 GMT   (334kb,D)

Title: Near-Optimal Solutions of Constrained Learning Problems
Authors: Juan Elenter, Luiz F. O. Chamon, Alejandro Ribeiro
Categories: cs.LG eess.SP math.OC
\\
  With the widespread adoption of machine learning systems, the need to curtail
their behavior has become increasingly apparent. This is evidenced by recent
advancements towards developing models that satisfy robustness, safety, and
fairness requirements. These requirements can be imposed (with generalization
guarantees) by formulating constrained learning problems that can then be
tackled by dual ascent algorithms. Yet, though these algorithms converge in
objective value, even in non-convex settings, they cannot guarantee that their
outcome is feasible. Doing so requires randomizing over all iterates, which is
impractical in virtually any modern applications. Still, final iterates have
been observed to perform well in practice. In this work, we address this gap
between theory and practice by characterizing the constraint violation of
Lagrangian minimizers associated with optimal dual variables, despite lack of
convexity. To do this, we leverage the fact that non-convex, finite-dimensional
constrained learning problems can be seen as parametrizations of convex,
functional problems. Our results show that rich parametrizations effectively
mitigate the issue of feasibility in dual methods, shedding light on prior
empirical successes of dual learning. We illustrate our findings in fair
learning tasks.
\\ ( https://arxiv.org/abs/2403.11844 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11857
Date: Mon, 18 Mar 2024 15:06:37 GMT   (5061kb,D)

Title: Complete and Efficient Graph Transformers for Crystal Material Property
  Prediction
Authors: Keqiang Yan, Cong Fu, Xiaofeng Qian, Xiaoning Qian, Shuiwang Ji
Categories: cs.LG cond-mat.mtrl-sci
Comments: This paper has been accepted by ICLR 2024
\\
  Crystal structures are characterized by atomic bases within a primitive unit
cell that repeats along a regular lattice throughout 3D space. The periodic and
infinite nature of crystals poses unique challenges for geometric graph
representation learning. Specifically, constructing graphs that effectively
capture the complete geometric information of crystals and handle chiral
crystals remains an unsolved and challenging problem. In this paper, we
introduce a novel approach that utilizes the periodic patterns of unit cells to
establish the lattice-based representation for each atom, enabling efficient
and expressive graph representations of crystals. Furthermore, we propose
ComFormer, a SE(3) transformer designed specifically for crystalline materials.
ComFormer includes two variants; namely, iComFormer that employs invariant
geometric descriptors of Euclidean distances and angles, and eComFormer that
utilizes equivariant vector representations. Experimental results demonstrate
the state-of-the-art predictive accuracy of ComFormer variants on various tasks
across three widely-used crystal benchmarks. Our code is publicly available as
part of the AIRS library (https://github.com/divelab/AIRS).
\\ ( https://arxiv.org/abs/2403.11857 ,  5061kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11877
Date: Mon, 18 Mar 2024 15:31:09 GMT   (954kb,D)

Title: Efficient Training of Learning-Based Thermal Power Flow for 4th
  Generation District Heating Grids
Authors: Andreas Bott, Mario Beykirch, Florian Steinke
Categories: cs.LG cs.CE cs.SY eess.SY
Comments: The paper is submitted to Energy, special issue on Smart Energy
  Systems SESAAU2023 and is currently under review
\\
  Thermal power flow (TPF) is an important task for various control purposes in
4 Th generation district heating grids with multiple decentral heat sources and
meshed grid structures. Computing the TPF, i.e., determining the grid state
consisting of temperatures, pressures, and mass flows for given supply and
demand values, is classically done by solving the nonlinear heat grid
equations, but can be sped up by orders of magnitude using learned models such
as neural networks. We propose a novel, efficient scheme to generate a
sufficiently large training data set covering relevant supply and demand
values. Instead of sampling supply and demand values, our approach generates
training examples from a proxy distribution over generator and consumer mass
flows, omitting the iterations needed for solving the heat grid equations. The
exact, but slightly different, training examples can be weighted to represent
the original training distribution. We show with simulations for typical grid
structures that the new approach can reduce training set generation times by
two orders of magnitude compared to sampling supply and demand values directly,
without loss of relevance for the training samples. Moreover, learning TPF with
a training data set is shown to outperform sample-free, physics-aware training
approaches significantly.
\\ ( https://arxiv.org/abs/2403.11877 ,  954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11892
Date: Mon, 18 Mar 2024 15:49:48 GMT   (855kb,D)

Title: KnFu: Effective Knowledge Fusion
Authors: S. Jamal Seyedmohammadi, S. Kawa Atapour, Jamshid Abouei, Arash
  Mohammadi
Categories: cs.LG cs.DC
\\
  Federated Learning (FL) has emerged as a prominent alternative to the
traditional centralized learning approach. Generally speaking, FL is a
decentralized approach that allows for collaborative training of Machine
Learning (ML) models across multiple local nodes, ensuring data privacy and
security while leveraging diverse datasets. Conventional FL, however, is
susceptible to gradient inversion attacks, restrictively enforces a uniform
architecture on local models, and suffers from model heterogeneity (model
drift) due to non-IID local datasets. To mitigate some of these challenges, the
new paradigm of Federated Knowledge Distillation (FKD) has emerged. FDK is
developed based on the concept of Knowledge Distillation (KD), which involves
extraction and transfer of a large and well-trained teacher model's knowledge
to lightweight student models. FKD, however, still faces the model drift issue.
Intuitively speaking, not all knowledge is universally beneficial due to the
inherent diversity of data among local nodes. This calls for innovative
mechanisms to evaluate the relevance and effectiveness of each client's
knowledge for others, to prevent propagation of adverse knowledge. In this
context, the paper proposes Effective Knowledge Fusion (KnFu) algorithm that
evaluates knowledge of local models to only fuse semantic neighbors' effective
knowledge for each client. The KnFu is a personalized effective knowledge
fusion scheme for each client, that analyzes effectiveness of different local
models' knowledge prior to the aggregation phase. Comprehensive experiments
were performed on MNIST and CIFAR10 datasets illustrating effectiveness of the
proposed KnFu in comparison to its state-of-the-art counterparts. A key
conclusion of the work is that in scenarios with large and highly heterogeneous
local datasets, local training could be preferable to knowledge fusion-based
solutions.
\\ ( https://arxiv.org/abs/2403.11892 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11901
Date: Mon, 18 Mar 2024 16:01:42 GMT   (5792kb,D)

Title: Larimar: Large Language Models with Episodic Memory Control
Authors: Payel Das and Subhajit Chaudhury and Elliot Nelson and Igor Melnyk and
  Sarath Swaminathan and Sihui Dai and Aur\'elie Lozano and Georgios Kollias
  and Vijil Chenthamarakshan and Ji\v{r}\'i, Navr\'atil and Soham Dan and
  Pin-Yu Chen
Categories: cs.LG cs.AI
\\
  Efficient and accurate updating of knowledge stored in Large Language Models
(LLMs) is one of the most pressing research challenges today. This paper
presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with
a distributed episodic memory. Larimar's memory allows for dynamic, one-shot
updates of knowledge without the need for computationally expensive re-training
or fine-tuning. Experimental results on multiple fact editing benchmarks
demonstrate that Larimar attains accuracy comparable to most competitive
baselines, even in the challenging sequential editing setup, but also excels in
speed - yielding speed-ups of 4-10x depending on the base LLM - as well as
flexibility due to the proposed architecture being simple, LLM-agnostic, and
hence general. We further provide mechanisms for selective fact forgetting and
input context length generalization with Larimar and show their effectiveness.
\\ ( https://arxiv.org/abs/2403.11901 ,  5792kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11914
Date: Mon, 18 Mar 2024 16:13:02 GMT   (441kb,D)

Title: Single-Agent Actor Critic for Decentralized Cooperative Driving
Authors: Shengchao Yan, Lukas K\"onig, Wolfram Burgard
Categories: cs.LG cs.RO
\\
  Active traffic management incorporating autonomous vehicles (AVs) promises a
future with diminished congestion and enhanced traffic flow. However,
developing algorithms for real-world application requires addressing the
challenges posed by continuous traffic flow and partial observability. To
bridge this gap and advance the field of active traffic management towards
greater decentralization, we introduce a novel asymmetric actor-critic model
aimed at learning decentralized cooperative driving policies for autonomous
vehicles using single-agent reinforcement learning. Our approach employs
attention neural networks with masking to handle the dynamic nature of
real-world traffic flow and partial observability. Through extensive
evaluations against baseline controllers across various traffic scenarios, our
model shows great potential for improving traffic flow at diverse bottleneck
locations within the road system. Additionally, we explore the challenge
associated with the conservative driving behaviors of autonomous vehicles that
adhere strictly to traffic regulations. The experiment results illustrate that
our proposed cooperative policy can mitigate potential traffic slowdowns
without compromising safety.
\\ ( https://arxiv.org/abs/2403.11914 ,  441kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11925
Date: Mon, 18 Mar 2024 16:23:47 GMT   (872kb,D)

Title: Global Optimality without Mixing Time Oracles in Average-reward RL via
  Multi-level Actor-Critic
Authors: Bhrij Patel, Wesley A. Suttle, Alec Koppel, Vaneet Aggarwal, Brian M.
  Sadler, Amrit Singh Bedi, Dinesh Manocha
Categories: cs.LG
Comments: 27 Pages, 2 Figures
\\
  In the context of average-reward reinforcement learning, the requirement for
oracle knowledge of the mixing time, a measure of the duration a Markov chain
under a fixed policy needs to achieve its stationary distribution-poses a
significant challenge for the global convergence of policy gradient methods.
This requirement is particularly problematic due to the difficulty and expense
of estimating mixing time in environments with large state spaces, leading to
the necessity of impractically long trajectories for effective gradient
estimation in practical applications. To address this limitation, we consider
the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level
Monte Carlo (MLMC) gradient estimator. With our approach, we effectively
alleviate the dependency on mixing time knowledge, a first for average-reward
MDPs global convergence. Furthermore, our approach exhibits the
tightest-available dependence of $\mathcal{O}\left( \sqrt{\tau_{mix}} \right)$
relative to prior work. With a 2D gridworld goal-reaching navigation
experiment, we demonstrate that MAC achieves higher reward than a previous
PG-based method for average reward, Parameterized Policy Gradient with
Advantage Estimation (PPGAE), especially in cases with relatively small
training sample budget restricting trajectory length.
\\ ( https://arxiv.org/abs/2403.11925 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11940
Date: Mon, 18 Mar 2024 16:36:01 GMT   (1152kb,D)

Title: Multistep Inverse Is Not All You Need
Authors: Alexander Levine, Peter Stone, Amy Zhang
Categories: cs.LG cs.SY eess.SY
\\
  In real-world control settings, the observation space is often unnecessarily
high-dimensional and subject to time-correlated noise. However, the
controllable dynamics of the system are often far simpler than the dynamics of
the raw observations. It is therefore desirable to learn an encoder to map the
observation space to a simpler space of control-relevant variables. In this
work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022),
which formalizes control problems where observations can be factorized into an
action-dependent latent state which evolves deterministically, and
action-independent time-correlated noise. Lamb et al. (2022) proposes the
"AC-State" method for learning an encoder to extract a complete
action-dependent latent state representation from the observations in such
problems. AC-State is a multistep-inverse method, in that it uses the encoding
of the the first and last state in a path to predict the first action in the
path. However, we identify cases where AC-State will fail to learn a correct
latent representation of the agent-controllable factor of the state. We
therefore propose a new algorithm, ACDF, which combines multistep-inverse
prediction with a latent forward model. ACDF is guaranteed to correctly infer
an action-dependent latent state encoder for a large class of Ex-BMDP models.
We demonstrate the effectiveness of ACDF on tabular Ex-BMDPs through numerical
simulations; as well as high-dimensional environments using
neural-network-based encoders. Code is available at
https://github.com/midi-lab/acdf.
\\ ( https://arxiv.org/abs/2403.11940 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11960
Date: Mon, 18 Mar 2024 16:57:16 GMT   (4499kb,D)

Title: CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for
  Spatiotemporal Time Series Imputation
Authors: Baoyu Jing, Dawei Zhou, Kan Ren, Carl Yang
Categories: cs.LG stat.ML
Comments: Preprint. Work in progress
\\
  Spatiotemporal time series is the foundation of understanding human
activities and their impacts, which is usually collected via monitoring sensors
placed at different locations. The collected data usually contains missing
values due to various failures, which have significant impact on data analysis.
To impute the missing values, a lot of methods have been introduced. When
recovering a specific data point, most existing methods tend to take into
consideration all the information relevant to that point regardless of whether
they have a cause-and-effect relationship. During data collection, it is
inevitable that some unknown confounders are included, e.g., background noise
in time series and non-causal shortcut edges in the constructed sensor network.
These confounders could open backdoor paths between the input and output, in
other words, they establish non-causal correlations between the input and
output. Over-exploiting these non-causal correlations could result in
overfitting and make the model vulnerable to noises. In this paper, we first
revisit spatiotemporal time series imputation from a causal perspective, which
shows the causal relationships among the input, output, embeddings and
confounders. Next, we show how to block the confounders via the frontdoor
adjustment. Based on the results of the frontdoor adjustment, we introduce a
novel Causality-Aware SPatiotEmpoRal graph neural network (CASPER), which
contains a novel Spatiotemporal Causal Attention (SCA) and a Prompt Based
Decoder (PBD). PBD could reduce the impact of confounders and SCA could
discover the sparse causal relationships among embeddings. Theoretical analysis
reveals that SCA discovers causal relationships based on the values of
gradients. We evaluate Casper on three real-world datasets, and the
experimental results show that Casper outperforms the baselines and effectively
discovers causal relationships.
\\ ( https://arxiv.org/abs/2403.11960 ,  4499kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11963
Date: Mon, 18 Mar 2024 17:02:41 GMT   (6397kb,D)

Title: Transfer Learning Beyond Bounded Density Ratios
Authors: Alkis Kalavasis, Ilias Zadik, Manolis Zampetakis
Categories: cs.LG cs.DS math.ST stat.ML stat.TH
Comments: Abstract shortened to fit ArXiv requirements
\\
  We study the fundamental problem of transfer learning where a learning
algorithm collects data from some source distribution $P$ but needs to perform
well with respect to a different target distribution $Q$. A standard change of
measure argument implies that transfer learning happens when the density ratio
$dQ/dP$ is bounded. Yet, prior thought-provoking works by Kpotufe and Martinet
(COLT, 2018) and Hanneke and Kpotufe (NeurIPS, 2019) demonstrate cases where
the ratio $dQ/dP$ is unbounded, but transfer learning is possible.
  In this work, we focus on transfer learning over the class of low-degree
polynomial estimators. Our main result is a general transfer inequality over
the domain $\mathbb{R}^n$, proving that non-trivial transfer learning for
low-degree polynomials is possible under very mild assumptions, going well
beyond the classical assumption that $dQ/dP$ is bounded. For instance, it
always applies if $Q$ is a log-concave measure and the inverse ratio $dP/dQ$ is
bounded. To demonstrate the applicability of our inequality, we obtain new
results in the settings of: (1) the classical truncated regression setting,
where $dQ/dP$ equals infinity, and (2) the more recent out-of-distribution
generalization setting for in-context learning linear functions with
transformers. We also provide a discrete analogue of our transfer inequality on
the Boolean Hypercube $\{-1,1\}^n$, and study its connections with the recent
problem of Generalization on the Unseen of Abbe, Bengio, Lotfi and Rizk (ICML,
2023). Our main conceptual contribution is that the maximum influence of the
error of the estimator $\widehat{f}-f^*$ under $Q$,
$\mathrm{I}_{\max}(\widehat{f}-f^*)$, acts as a sufficient condition for
transferability; when $\mathrm{I}_{\max}(\widehat{f}-f^*)$ is appropriately
bounded, transfer is possible over the Boolean domain.
\\ ( https://arxiv.org/abs/2403.11963 ,  6397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11964
Date: Mon, 18 Mar 2024 17:04:33 GMT   (11980kb,D)

Title: Probabilistic Calibration by Design for Neural Network Regression
Authors: Victor Dheur, Souhaib Ben Taieb
Categories: cs.LG stat.ML
Comments: Accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2024)
\\
  Generating calibrated and sharp neural network predictive distributions for
regression problems is essential for optimal decision-making in many real-world
applications. To address the miscalibration issue of neural networks, various
methods have been proposed to improve calibration, including post-hoc methods
that adjust predictions after training and regularization methods that act
during training. While post-hoc methods have shown better improvement in
calibration compared to regularization methods, the post-hoc step is completely
independent of model training. We introduce a novel end-to-end model training
procedure called Quantile Recalibration Training, integrating post-hoc
calibration directly into the training process without additional parameters.
We also present a unified algorithm that includes our method and other post-hoc
and regularization methods, as particular cases. We demonstrate the performance
of our method in a large-scale experiment involving 57 tabular regression
datasets, showcasing improved predictive accuracy while maintaining
calibration. We also conduct an ablation study to evaluate the significance of
different components within our proposed method, as well as an in-depth
analysis of the impact of the base model and different hyperparameters on
predictive accuracy.
\\ ( https://arxiv.org/abs/2403.11964 ,  11980kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11966
Date: Mon, 18 Mar 2024 17:05:24 GMT   (301kb,D)

Title: Informed Spectral Normalized Gaussian Processes for Trajectory
  Prediction
Authors: Christian Schlauch, Christian Wirth, Nadja Klein
Categories: cs.LG cs.AI
\\
  Prior parameter distributions provide an elegant way to represent prior
expert and world knowledge for informed learning. Previous work has shown that
using such informative priors to regularize probabilistic deep learning (DL)
models increases their performance and data-efficiency. However, commonly used
sampling-based approximations for probabilistic DL models can be
computationally expensive, requiring multiple inference passes and longer
training times. Promising alternatives are compute-efficient last layer kernel
approximations like spectral normalized Gaussian processes (SNGPs). We propose
a novel regularization-based continual learning method for SNGPs, which enables
the use of informative priors that represent prior knowledge learned from
previous tasks. Our proposal builds upon well-established methods and requires
no rehearsal memory or parameter expansion. We apply our informed SNGP model to
the trajectory prediction problem in autonomous driving by integrating prior
drivability knowledge. On two public datasets, we investigate its performance
under diminishing training data and across locations, and thereby demonstrate
an increase in data-efficiency and robustness to location-transfers over
non-informed and informed baselines.
\\ ( https://arxiv.org/abs/2403.11966 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11968
Date: Mon, 18 Mar 2024 17:08:24 GMT   (1464kb,D)

Title: Unveil Conditional Diffusion Models with Classifier-free Guidance: A
  Sharp Statistical Theory
Authors: Hengyu Fu, Zhuoran Yang, Mengdi Wang, Minshuo Chen
Categories: cs.LG math.ST stat.ML stat.TH
Comments: 92 pages, 5 figures
\\
  Conditional diffusion models serve as the foundation of modern image
synthesis and find extensive application in fields like computational biology
and reinforcement learning. In these applications, conditional diffusion models
incorporate various conditional information, such as prompt input, to guide the
sample generation towards desired properties. Despite the empirical success,
theory of conditional diffusion models is largely missing. This paper bridges
this gap by presenting a sharp statistical theory of distribution estimation
using conditional diffusion models. Our analysis yields a sample complexity
bound that adapts to the smoothness of the data distribution and matches the
minimax lower bound. The key to our theoretical development lies in an
approximation result for the conditional score function, which relies on a
novel diffused Taylor approximation technique. Moreover, we demonstrate the
utility of our statistical theory in elucidating the performance of conditional
diffusion models across diverse applications, including model-based transition
kernel estimation in reinforcement learning, solving inverse problems, and
reward conditioned sample generation.
\\ ( https://arxiv.org/abs/2403.11968 ,  1464kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11996
Date: Mon, 18 Mar 2024 17:30:27 GMT   (38266kb,D)

Title: Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
Authors: Markus J. Buehler
Categories: cs.LG cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.soft cs.AI cs.CL
\\
  Using generative Artificial Intelligence (AI), we transformed a set of 1,000
scientific papers in the area of biological materials into detailed ontological
knowledge graphs, revealing their inherently scale-free nature. Using graph
traversal path detection between dissimilar concepts based on combinatorial
ranking of node similarity and betweenness centrality, we reveal deep insights
into unprecedented interdisciplinary relationships that can be used to answer
queries, identify gaps in knowledge, and propose never-before-seen material
designs and their behaviors. One comparison revealed detailed structural
parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. The
algorithm further created an innovative hierarchical mycelium-based composite
that incorporates joint synthesis of graph sampling with principles extracted
from Kandinsky's Composition VII painting, where the resulting composite
reflects a balance of chaos and order, with features like adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across physical, biological, and artistic spheres,
revealing a nuanced ontology of immanence and material flux that resonates with
postmodern philosophy, and positions these interconnections within a
heterarchical framework. Our findings reveal the dynamic, context-dependent
interplay of entities beyond traditional hierarchical paradigms, emphasizing
the significant role of individual components and their fluctuative
relationships within the system. Our predictions achieve a far higher degree of
novelty, technical detail and explorative capacity than conventional generative
AI methods. The approach establishes a widely useful framework for innovation
by revealing hidden connections that facilitate discovery.
\\ ( https://arxiv.org/abs/2403.11996 ,  38266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11998
Date: Mon, 18 Mar 2024 17:32:23 GMT   (6426kb,D)

Title: Learning Useful Representations of Recurrent Neural Network Weight
  Matrices
Authors: Vincent Herrmann, Francesco Faccio, J\"urgen Schmidhuber
Categories: cs.LG
ACM-class: I.2.6
\\
  Recurrent Neural Networks (RNNs) are general-purpose parallel-sequential
computers. The program of an RNN is its weight matrix. How to learn useful
representations of RNN weights that facilitate RNN analysis as well as
downstream tasks? While the mechanistic approach directly looks at some RNN's
weights to predict its behavior, the functionalist approach analyzes its
overall functionality -- specifically, its input-output mapping. We consider
several mechanistic approaches for RNN weights and adapt the permutation
equivariant Deep Weight Space layer for RNNs. Our two novel functionalist
approaches extract information from RNN weights by 'interrogating' the RNN
through probing inputs. We develop a theoretical framework that demonstrates
conditions under which the functionalist approach can generate rich
representations that help determine RNN behavior. We create and release the
first two 'model zoo' datasets for RNN weight representation learning. One
consists of generative models of a class of formal languages, and the other one
of classifiers of sequentially processed MNIST digits. With the help of an
emulation-based self-supervised learning technique we compare and evaluate the
different RNN weight encoding techniques on multiple downstream applications.
On the most challenging one, namely predicting which exact task the RNN was
trained on, functionalist approaches show clear superiority.
\\ ( https://arxiv.org/abs/2403.11998 ,  6426kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12017
Date: Mon, 18 Mar 2024 17:52:57 GMT   (30kb)

Title: Supervised Fine-Tuning as Inverse Reinforcement Learning
Authors: Hao Sun
Categories: cs.LG cs.AI cs.CL
\\
  The prevailing approach to aligning Large Language Models (LLMs) typically
relies on human or AI feedback and assumes access to specific types of
preference datasets. In our work, we question the efficacy of such datasets and
explore various scenarios where alignment with expert demonstrations proves
more realistic. We build a sequential decision-making framework to formulate
the problem of aligning LLMs using demonstration datasets. Drawing insights
from inverse reinforcement learning and imitation learning, we introduce
various approaches for divergence minimization in the LLM alignment tasks. Our
analysis highlights the mass-covering and mode-seeking behaviors of these
different approaches. Inclusively, we examine the pros and cons of the
classical supervised fine-tuning method, elaborating on scenarios where
different methods shine.
\\ ( https://arxiv.org/abs/2403.12017 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12031
Date: Mon, 18 Mar 2024 17:59:04 GMT   (1677kb,D)

Title: ROUTERBENCH: A Benchmark for Multi-LLM Routing System
Authors: Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin,
  Gaurav Ranganath, Kurt Keutzer, Shriyash Kaustubh Upadhyay
Categories: cs.LG cs.AI
\\
  As the range of applications for Large Language Models (LLMs) continues to
grow, the demand for effective serving solutions becomes increasingly critical.
Despite the versatility of LLMs, no single model can optimally address all
tasks and applications, particularly when balancing performance with cost. This
limitation has led to the development of LLM routing systems, which combine the
strengths of various models to overcome the constraints of individual LLMs.
Yet, the absence of a standardized benchmark for evaluating the performance of
LLM routers hinders progress in this area. To bridge this gap, we present
ROUTERBENCH, a novel evaluation framework designed to systematically assess the
efficacy of LLM routing systems, along with a comprehensive dataset comprising
over 405k inference outcomes from representative LLMs to support the
development of routing strategies. We further propose a theoretical framework
for LLM routing, and deliver a comparative analysis of various routing
approaches through ROUTERBENCH, highlighting their potentials and limitations
within our evaluation framework. This work not only formalizes and advances the
development of LLM routing systems but also sets a standard for their
assessment, paving the way for more accessible and economically viable LLM
deployments. The code and data are available at
https://github.com/withmartian/routerbench.
\\ ( https://arxiv.org/abs/2403.12031 ,  1677kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2211.08559 (*cross-listing*)
Date: Tue, 15 Nov 2022 23:04:15 GMT   (5167kb,I)
Date (revised v2): Tue, 10 Oct 2023 15:49:45 GMT   (10964kb,D)

Title: Robust Alzheimer's Progression Modeling using Cross-Domain
  Self-Supervised Deep Learning
Authors: Saba Dadsetan, Mohsen Hejrati, Shandong Wu, Somaye Hashemifar
Categories: cs.CV cs.AI cs.LG
Comments: This work has been published at the Transactions on Machine Learning
  Research (TMLR) journal
\\
  Developing successful artificial intelligence systems in practice depends on
both robust deep learning models and large, high-quality data. However,
acquiring and labeling data can be prohibitively expensive and time-consuming
in many real-world applications, such as clinical disease models.
Self-supervised learning has demonstrated great potential in increasing model
accuracy and robustness in small data regimes. In addition, many clinical
imaging and disease modeling applications rely heavily on regression of
continuous quantities. However, the applicability of self-supervised learning
for these medical-imaging regression tasks has not been extensively studied. In
this study, we develop a cross-domain self-supervised learning approach for
disease prognostic modeling as a regression problem using medical images as
input. We demonstrate that self-supervised pretraining can improve the
prediction of Alzheimer's Disease progression from brain MRI. We also show that
pretraining on extended (but not labeled) brain MRI data outperforms
pretraining on natural images. We further observe that the highest performance
is achieved when both natural images and extended brain-MRI data are used for
pretraining.
\\ ( https://arxiv.org/abs/2211.08559 ,  10964kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10534 (*cross-listing*)
Date: Fri, 23 Feb 2024 00:12:10 GMT   (21584kb,D)

Title: VISREAS: Complex Visual Reasoning with Unanswerable Questions
Authors: Syeda Nahida Akter, Sangwu Lee, Yingshan Chang, Yonatan Bisk, Eric
  Nyberg
Categories: cs.CV cs.AI
Comments: 18 pages, 14 figures, 5 tables
\\
  Verifying a question's validity before answering is crucial in real-world
applications, where users may provide imperfect instructions. In this scenario,
an ideal model should address the discrepancies in the query and convey them to
the users rather than generating the best possible answer. Addressing this
requirement, we introduce a new compositional visual question-answering
dataset, VISREAS, that consists of answerable and unanswerable visual queries
formulated by traversing and perturbing commonalities and differences among
objects, attributes, and relations. VISREAS contains 2.07M semantically diverse
queries generated automatically using Visual Genome scene graphs. The unique
feature of this task, validating question answerability with respect to an
image before answering, and the poor performance of state-of-the-art models
inspired the design of a new modular baseline, LOGIC2VISION that reasons by
producing and executing pseudocode without any external modules to generate the
answer. LOGIC2VISION outperforms generative models in VISREAS (+4.82% over
LLaVA-1.5; +12.23% over InstructBLIP) and achieves a significant gain in
performance against the classification models.
\\ ( https://arxiv.org/abs/2403.10534 ,  21584kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10538 (*cross-listing*)
Date: Sun, 3 Mar 2024 10:31:46 GMT   (31192kb,D)

Title: MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for
  Edge Applications
Authors: Tousif Rahman, Gang Mao, Sidharth Maheshwari, Rishad Shafik, Alex
  Yakovlev
Categories: cs.AR cs.AI cs.LG
\\
  System-on-Chip Field-Programmable Gate Arrays (SoC-FPGAs) offer significant
throughput gains for machine learning (ML) edge inference applications via the
design of co-processor accelerator systems. However, the design effort for
training and translating ML models into SoC-FPGA solutions can be substantial
and requires specialist knowledge aware trade-offs between model performance,
power consumption, latency and resource utilization. Contrary to other ML
algorithms, Tsetlin Machine (TM) performs classification by forming logic
proposition between boolean actions from the Tsetlin Automata (the learning
elements) and boolean input features. A trained TM model, usually, exhibits
high sparsity and considerable overlapping of these logic propositions both
within and among the classes. The model, thus, can be translated to RTL-level
design using a miniscule number of AND and NOT gates. This paper presents
MATADOR, an automated boolean-to-silicon tool with GUI interface capable of
implementing optimized accelerator design of the TM model onto SoC-FPGA for
inference at the edge. It offers automation of the full development pipeline:
model training, system level design generation, design verification and
deployment. It makes use of the logic sharing that ensues from propositional
overlap and creates a compact design by effectively utilizing the TM model's
sparsity. MATADOR accelerator designs are shown to be up to 13.4x faster, up to
7x more resource frugal and up to 2x more power efficient when compared to the
state-of-the-art Quantized and Binary Deep Neural Network implementations.
\\ ( https://arxiv.org/abs/2403.10538 ,  31192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10544 (*cross-listing*)
Date: Mon, 11 Mar 2024 09:33:21 GMT   (534kb,D)

Title: Process-Aware Analysis of Treatment Paths in Heart Failure Patients: A
  Case Study
Authors: Harry H. Beyel and Marlo Verket and Viki Peeva and Christian Rennert
  and Marco Pegoraro and Katharina Sch\"utt and Wil M.P. van der Aalst and
  Nikolaus Marx
Categories: stat.AP cs.AI
Comments: 10 pages, 3 figures, 9 tables, 31 references
DOI: 10.5220/0012392600003657
\\
  Process mining in healthcare presents a range of challenges when working with
different types of data within the healthcare domain. There is high diversity
considering the variety of data collected from healthcare processes:
operational processes given by claims data, a collection of events during
surgery, data related to pre-operative and post-operative care, and high-level
data collections based on regular ambulant visits with no apparent events. In
this case study, a data set from the last category is analyzed. We apply
process-mining techniques on sparse patient heart failure data and investigate
whether an information gain towards several research questions is achievable.
Here, available data are transformed into an event log format, and process
discovery and conformance checking are applied. Additionally, patients are
split into different cohorts based on comorbidities, such as diabetes and
chronic kidney disease, and multiple statistics are compared between the
cohorts. Conclusively, we apply decision mining to determine whether a patient
will have a cardiovascular outcome and whether a patient will die.
\\ ( https://arxiv.org/abs/2403.10544 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10547 (*cross-listing*)
Date: Tue, 12 Mar 2024 01:27:44 GMT   (80kb)

Title: Robust Second-Order Nonconvex Optimization and Its Application to Low
  Rank Matrix Sensing
Authors: Shuyao Li, Yu Cheng, Ilias Diakonikolas, Jelena Diakonikolas, Rong Ge,
  Stephen J. Wright
Categories: math.OC cs.AI cs.DS cs.LG
\\
  Finding an approximate second-order stationary point (SOSP) is a well-studied
and fundamental problem in stochastic nonconvex optimization with many
applications in machine learning. However, this problem is poorly understood in
the presence of outliers, limiting the use of existing nonconvex algorithms in
adversarial settings.
  In this paper, we study the problem of finding SOSPs in the strong
contamination model, where a constant fraction of datapoints are arbitrarily
corrupted. We introduce a general framework for efficiently finding an
approximate SOSP with \emph{dimension-independent} accuracy guarantees, using
$\widetilde{O}({D^2}/{\epsilon})$ samples where $D$ is the ambient dimension
and $\epsilon$ is the fraction of corrupted datapoints.
  As a concrete application of our framework, we apply it to the problem of low
rank matrix sensing, developing efficient and provably robust algorithms that
can tolerate corruptions in both the sensing matrices and the measurements. In
addition, we establish a Statistical Query lower bound providing evidence that
the quadratic dependence on $D$ in the sample complexity is necessary for
computationally efficient algorithms.
\\ ( https://arxiv.org/abs/2403.10547 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10562 (*cross-listing*)
Date: Thu, 14 Mar 2024 10:59:54 GMT   (1921kb,D)

Title: Counter-Samples: A Stateless Strategy to Neutralize Black Box
  Adversarial Attacks
Authors: Roey Bokobza, Yisroel Mirsky
Categories: cs.CR cs.AI cs.LG
\\
  Our paper presents a novel defence against black box attacks, where attackers
use the victim model as an oracle to craft their adversarial examples. Unlike
traditional preprocessing defences that rely on sanitizing input samples, our
stateless strategy counters the attack process itself. For every query we
evaluate a counter-sample instead, where the counter-sample is the original
sample optimized against the attacker's objective. By countering every black
box query with a targeted white box optimization, our strategy effectively
introduces an asymmetry to the game to the defender's advantage. This defence
not only effectively misleads the attacker's search for an adversarial example,
it also preserves the model's accuracy on legitimate inputs and is generic to
multiple types of attacks.
  We demonstrate that our approach is remarkably effective against
state-of-the-art black box attacks and outperforms existing defences for both
the CIFAR-10 and ImageNet datasets. Additionally, we also show that the
proposed defence is robust against strong adversaries as well.
\\ ( https://arxiv.org/abs/2403.10562 ,  1921kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10565 (*cross-listing*)
Date: Thu, 14 Mar 2024 14:57:16 GMT   (738kb,D)

Title: PTSD-MDNN : Fusion tardive de r\'eseaux de neurones profonds multimodaux
  pour la d\'etection du trouble de stress post-traumatique
Authors: Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro
Categories: eess.AS cs.AI cs.CV cs.SD eess.IV q-bio.NC
Comments: in French language. GRETSI 2023
\\
  In order to provide a more objective and quicker way to diagnose
post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two
unimodal convolutional neural networks and which gives low detection error
rate. By taking only videos and audios as inputs, the model could be used in
the configuration of teleconsultation sessions, in the optimization of patient
journeys or for human-robot interaction.
\\ ( https://arxiv.org/abs/2403.10565 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10570 (*cross-listing*)
Date: Thu, 14 Mar 2024 20:17:57 GMT   (908kb,D)

Title: Symbiotic Game and Foundation Models for Cyber Deception Operations in
  Strategic Cyber Warfare
Authors: Tao Li and Quanyan Zhu
Categories: cs.CR cs.AI cs.GT
\\
  We are currently facing unprecedented cyber warfare with the rapid evolution
of tactics, increasing asymmetry of intelligence, and the growing accessibility
of hacking tools. In this landscape, cyber deception emerges as a critical
component of our defense strategy against increasingly sophisticated attacks.
This chapter aims to highlight the pivotal role of game-theoretic models and
foundation models (FMs) in analyzing, designing, and implementing cyber
deception tactics. Game models (GMs) serve as a foundational framework for
modeling diverse adversarial interactions, allowing us to encapsulate both
adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the
building blocks for creating tailored machine learning models suited to given
applications. By leveraging the synergy between GMs and FMs, we can advance
proactive and automated cyber defense mechanisms by not only securing our
networks against attacks but also enhancing their resilience against
well-planned operations. This chapter discusses the games at the tactical,
operational, and strategic levels of warfare, delves into the symbiotic
relationship between these methodologies, and explores relevant applications
where such a framework can make a substantial impact in cybersecurity. The
chapter discusses the promising direction of the multi-agent neurosymbolic
conjectural learning (MANSCOL), which allows the defender to predict
adversarial behaviors, design adaptive defensive deception tactics, and
synthesize knowledge for the operational level synthesis and adaptation. FMs
serve as pivotal tools across various functions for MANSCOL, including
reinforcement learning, knowledge assimilation, formation of conjectures, and
contextual representation. This chapter concludes with a discussion of the
challenges associated with FMs and their application in the domain of
cybersecurity.
\\ ( https://arxiv.org/abs/2403.10570 ,  908kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10575 (*cross-listing*)
Date: Fri, 15 Mar 2024 04:41:50 GMT   (8499kb,D)

Title: Exploring Language Model's Code Generation Ability with Auxiliary
  Functions
Authors: Seonghyeon Lee, Sanghwan Jang, Seongbo Jang, Dongha Lee, Hwanjo Yu
Categories: cs.SE cs.AI cs.CL
Comments: NAACL2024 Findings
\\
  Auxiliary function is a helpful component to improve language model's code
generation ability. However, a systematic exploration of how they affect has
yet to be done. In this work, we comprehensively evaluate the ability to
utilize auxiliary functions encoded in recent code-pretrained language models.
First, we construct a human-crafted evaluation set, called HumanExtension,
which contains examples of two functions where one function assists the other.
With HumanExtension, we design several experiments to examine their ability in
a multifaceted way. Our evaluation processes enable a comprehensive
understanding of including auxiliary functions in the prompt in terms of
effectiveness and robustness. An additional implementation style analysis
captures the models' various implementation patterns when they access the
auxiliary function. Through this analysis, we discover the models' promising
ability to utilize auxiliary functions including their self-improving behavior
by implementing the two functions step-by-step. However, our analysis also
reveals the model's underutilized behavior to call the auxiliary function,
suggesting the future direction to enhance their implementation by eliciting
the auxiliary function call ability encoded in the models. We release our code
and dataset to facilitate this research direction.
\\ ( https://arxiv.org/abs/2403.10575 ,  8499kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10581 (*cross-listing*)
Date: Fri, 15 Mar 2024 13:25:09 GMT   (17090kb,D)

Title: Large Language Model-informed ECG Dual Attention Network for Heart
  Failure Risk Prediction
Authors: Chen Chen, Lei Li, Marcel Beetz, Abhirup Banerjee, Ramneek Gupta,
  Vicente Grau
Categories: q-bio.QM cs.AI cs.CL cs.LG eess.SP
Comments: Under journal revision
\\
  Heart failure (HF) poses a significant public health challenge due to its
rising global mortality rate. Addressing this issue through early diagnosis and
prevention could significantly reduce the disease's impact. This work
introduces a methodology for HF risk prediction using clinically acquired
12-lead electrocardiograms (ECGs). We present a novel, lightweight
dual-attention ECG network designed to capture complex ECG features essential
for early HF prediction, despite the notable imbalance between low and
high-risk groups. The network features a cross-lead attention module and twelve
lead-specific temporal attention modules to capture cross-lead interactions and
local temporal dynamics within each lead. To prevent model overfitting from
limited training data, we leverage a large language model (LLM) with a public
ECG-Report dataset for pretraining on an ECG-report alignment task. The network
is then fine-tuned for HF risk prediction using two specific cohorts from the
UK Biobank study, focusing on patients with hypertension (UKB-HYP) and those
who have had a myocardial infarction (UKB-MI). Our findings show that
LLM-informed pretraining significantly improves the network's HF risk
prediction capability in these cohorts. Moreover, the dual-attention mechanism
enhances interpretability and predictive performance, ensuring a transparent
and reliable prediction process. The method outperforms existing models,
achieving average C-index scores of 0.6349 and 0.5805 on the UKB-HYP and UKB-MI
test sets, respectively. This performance demonstrates our approach's
effectiveness in managing complex clinical ECG data and its potential to
improve HF risk assessment across various populations.
\\ ( https://arxiv.org/abs/2403.10581 ,  17090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10585 (*cross-listing*)
Date: Fri, 15 Mar 2024 16:38:47 GMT   (47629kb,D)

Title: Solving General Noisy Inverse Problem via Posterior Sampling: A Policy
  Gradient Viewpoint
Authors: Haoyue Tang, Tian Xie, Aosong Feng, Hanyu Wang, Chenyang Zhang, Yang
  Bai
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Accepted and to Appear, AISTATS 2024
\\
  Solving image inverse problems (e.g., super-resolution and inpainting)
requires generating a high fidelity image that matches the given input (the
low-resolution image or the masked image). By using the input image as
guidance, we can leverage a pretrained diffusion generative model to solve a
wide range of image inverse tasks without task specific model fine-tuning. To
precisely estimate the guidance score function of the input image, we propose
Diffusion Policy Gradient (DPG), a tractable computation method by viewing the
intermediate noisy images as policies and the target image as the states
selected by the policy. Experiments show that our method is robust to both
Gaussian and Poisson noise degradation on multiple linear and non-linear
inverse tasks, resulting into a higher image restoration quality on FFHQ,
ImageNet and LSUN datasets.
\\ ( https://arxiv.org/abs/2403.10585 ,  47629kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10588 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:04:27 GMT   (402kb,D)

Title: S3LLM: Large-Scale Scientific Software Understanding with LLMs using
  Source, Metadata, and Document
Authors: Kareem Shaik, Dali Wang, Weijian Zheng, Qinglei Cao, Heng Fan, Peter
  Schwartz, Yunhe Feng
Categories: cs.SE cs.AI
\\
  The understanding of large-scale scientific software poses significant
challenges due to its diverse codebase, extensive code length, and target
computing architectures. The emergence of generative AI, specifically large
language models (LLMs), provides novel pathways for understanding such complex
scientific codes. This paper presents S3LLM, an LLM-based framework designed to
enable the examination of source code, code metadata, and summarized
information in conjunction with textual technical reports in an interactive,
conversational manner through a user-friendly interface. S3LLM leverages
open-source LLaMA-2 models to enhance code analysis through the automatic
transformation of natural language queries into domain-specific language (DSL)
queries. Specifically, it translates these queries into Feature Query Language
(FQL), enabling efficient scanning and parsing of entire code repositories. In
addition, S3LLM is equipped to handle diverse metadata types, including DOT,
SQL, and customized formats. Furthermore, S3LLM incorporates retrieval
augmented generation (RAG) and LangChain technologies to directly query
extensive documents. S3LLM demonstrates the potential of using locally deployed
open-source LLMs for the rapid understanding of large-scale scientific
computing software, eliminating the need for extensive coding expertise, and
thereby making the process more efficient and effective. S3LLM is available at
https://github.com/ResponsibleAILab/s3llm.
\\ ( https://arxiv.org/abs/2403.10588 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10603 (*cross-listing*)
Date: Fri, 15 Mar 2024 18:00:11 GMT   (35039kb,D)

Title: SurvRNC: Learning Ordered Representations for Survival Prediction using
  Rank-N-Contrast
Authors: Numan Saeed, Muhammad Ridzuan, Fadillah Adamsyah Maani, Hussain
  Alasmawi, Karthik Nandakumar, Mohammad Yaqub
Categories: cs.CV cs.AI cs.LG
\\
  Predicting the likelihood of survival is of paramount importance for
individuals diagnosed with cancer as it provides invaluable information
regarding prognosis at an early stage. This knowledge enables the formulation
of effective treatment plans that lead to improved patient outcomes. In the
past few years, deep learning models have provided a feasible solution for
assessing medical images, electronic health records, and genomic data to
estimate cancer risk scores. However, these models often fall short of their
potential because they struggle to learn regression-aware feature
representations. In this study, we propose Survival Rank-N Contrast (SurvRNC)
method, which introduces a loss function as a regularizer to obtain an ordered
representation based on the survival times. This function can handle censored
data and can be incorporated into any survival model to ensure that the learned
representation is ordinal. The model was extensively evaluated on a HEad \&
NeCK TumOR (HECKTOR) segmentation and the outcome-prediction task dataset. We
demonstrate that using the SurvRNC method for training can achieve higher
performance on different deep survival models. Additionally, it outperforms
state-of-the-art methods by 3.6% on the concordance index. The code is publicly
available on https://github.com/numanai/SurvRNC
\\ ( https://arxiv.org/abs/2403.10603 ,  35039kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10667 (*cross-listing*)
Date: Fri, 15 Mar 2024 20:21:31 GMT   (1840kb,D)

Title: Towards Unified Multi-Modal Personalization: Large Vision-Language
  Models for Generative Recommendation and Beyond
Authors: Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui
  Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, Xianfeng Tang
Categories: cs.IR cs.AI cs.CL cs.MM
Comments: ICLR 2024
\\
  Developing a universal model that can effectively harness heterogeneous
resources and respond to a wide range of personalized needs has been a
longstanding community aspiration. Our daily choices, especially in domains
like fashion and retail, are substantially shaped by multi-modal data, such as
pictures and textual descriptions. These modalities not only offer intuitive
guidance but also cater to personalized user preferences. However, the
predominant personalization approaches mainly focus on the ID or text-based
recommendation problem, failing to comprehend the information spanning various
tasks or modalities. In this paper, our goal is to establish a Unified paradigm
for Multi-modal Personalization systems (UniMP), which effectively leverages
multi-modal data while eliminating the complexities associated with task- and
modality-specific customization. We argue that the advancements in foundational
generative modeling have provided the flexibility and effectiveness necessary
to achieve the objective. In light of this, we develop a generic and extensible
personalization generative framework, that can handle a wide range of
personalized needs including item recommendation, product search, preference
prediction, explanation generation, and further user-guided image generation.
Our methodology enhances the capabilities of foundational language models for
personalized tasks by seamlessly ingesting interleaved cross-modal user history
information, ensuring a more precise and customized experience for users. To
train and evaluate the proposed multi-modal personalized tasks, we also
introduce a novel and comprehensive benchmark covering a variety of user
requirements. Our experiments on the real-world benchmark showcase the model's
potential, outperforming competitive methods specialized for each task.
\\ ( https://arxiv.org/abs/2403.10667 ,  1840kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10684 (*cross-listing*)
Date: Fri, 15 Mar 2024 21:08:37 GMT   (1309kb)

Title: Improved discrete particle swarm optimization using Bee Algorithm and
  multi-parent crossover method (Case study: Allocation problem and benchmark
  functions)
Authors: Hamed Zibaei and Mohammad Saadi Mesgari
Categories: cs.NE cs.AI
Comments: 34 pages, 8 figures, 15 tables
\\
  Compared to other techniques, particle swarm optimization is more frequently
utilized because of its ease of use and low variability. However, it is
complicated to find the best possible solution in the search space in
large-scale optimization problems. Moreover, changing algorithm variables does
not influence algorithm convergence much. The PSO algorithm can be combined
with other algorithms. It can use their advantages and operators to solve this
problem. Therefore, this paper proposes the onlooker multi-parent crossover
discrete particle swarm optimization (OMPCDPSO). To improve the efficiency of
the DPSO algorithm, we utilized multi-parent crossover on the best solutions.
We performed an independent and intensive neighborhood search using the
onlooker bees of the bee algorithm. The algorithm uses onlooker bees and
crossover. They do local search (exploitation) and global search (exploration).
Each of these searches is among the best solutions (employed bees). The
proposed algorithm was tested on the allocation problem, which is an NP-hard
optimization problem. Also, we used two types of simulated data. They were used
to test the scalability and complexity of the better algorithm. Also, fourteen
2D test functions and thirteen 30D test functions were used. They also used
twenty IEEE CEC2005 benchmark functions to test the efficiency of OMPCDPSO.
Also, to test OMPCDPSO's performance, we compared it to four new binary
optimization algorithms and three classic ones. The results show that the
OMPCDPSO version had high capability. It performed better than other
algorithms. The developed algorithm in this research (OMCDPSO) in 36 test
functions out of 47 (76.60%) is better than other algorithms. The Onlooker bees
and multi-parent operators significantly impact the algorithm's performance.
\\ ( https://arxiv.org/abs/2403.10684 ,  1309kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10686 (*cross-listing*)
Date: Fri, 15 Mar 2024 21:14:44 GMT   (2463kb,D)

Title: AutoHLS: Learning to Accelerate Design Space Exploration for HLS Designs
Authors: Md Rubel Ahmed, Toshiaki Koike-Akino, Kieran Parsons, Ye Wang
Categories: cs.AR cs.AI cs.LG
Comments: 5 pages, 6 figures, MWSCAS 2023
\\
  High-level synthesis (HLS) is a design flow that leverages modern language
features and flexibility, such as complex data structures, inheritance,
templates, etc., to prototype hardware designs rapidly. However, exploring
various design space parameters can take much time and effort for hardware
engineers to meet specific design specifications. This paper proposes a novel
framework called AutoHLS, which integrates a deep neural network (DNN) with
Bayesian optimization (BO) to accelerate HLS hardware design optimization. Our
tool focuses on HLS pragma exploration and operation transformation. It
utilizes integrated DNNs to predict synthesizability within a given FPGA
resource budget. We also investigate the potential of emerging quantum neural
networks (QNNs) instead of classical DNNs for the AutoHLS pipeline. Our
experimental results demonstrate up to a 70-fold speedup in exploration time.
\\ ( https://arxiv.org/abs/2403.10686 ,  2463kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10698 (*cross-listing*)
Date: Fri, 15 Mar 2024 21:30:25 GMT   (5316kb,D)

Title: Robust Influence-based Training Methods for Noisy Brain MRI
Authors: Minh-Hao Van, Alycia N. Carey, Xintao Wu
Categories: cs.CV cs.AI
\\
  Correctly classifying brain tumors is imperative to the prompt and accurate
treatment of a patient. While several classification algorithms based on
classical image processing or deep learning methods have been proposed to
rapidly classify tumors in MR images, most assume the unrealistic setting of
noise-free training data. In this work, we study a difficult but realistic
setting of training a deep learning model on noisy MR images to classify brain
tumors. We propose two training methods that are robust to noisy MRI training
data, Influence-based Sample Reweighing (ISR) and Influence-based Sample
Perturbation (ISP), which are based on influence functions from robust
statistics. Using the influence functions, in ISR, we adaptively reweigh
training examples according to how helpful/harmful they are to the training
process, while in ISP, we craft and inject helpful perturbation proportional to
the influence score. Both ISR and ISP harden the classification model against
noisy training data without significantly affecting the generalization ability
of the model on test data. We conduct empirical evaluations over a common brain
tumor dataset and compare ISR and ISP to three baselines. Our empirical results
show that ISR and ISP can efficiently train deep learning models robust against
noisy training data.
\\ ( https://arxiv.org/abs/2403.10698 ,  5316kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10700 (*cross-listing*)
Date: Fri, 15 Mar 2024 21:36:15 GMT   (2406kb,D)

Title: Mind the Error! Detection and Localization of Instruction Errors in
  Vision-and-Language Navigation
Authors: Francesco Taioli, Stefano Rosa, Alberto Castellini, Lorenzo Natale,
  Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Yiming Wang
Categories: cs.RO cs.AI cs.CL
Comments: 3 figures, 8 pages
\\
  Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of
the most intuitive yet challenging embodied AI tasks. Agents are tasked to
navigate towards a target goal by executing a set of low-level actions,
following a series of natural language instructions. All VLN-CE methods in the
literature assume that language instructions are exact. However, in practice,
instructions given by humans can contain errors when describing a spatial
environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do
not address this scenario, making the state-of-the-art methods in VLN-CE
fragile in the presence of erroneous instructions from human users. For the
first time, we propose a novel benchmark dataset that introduces various types
of instruction errors considering potential human causes. This benchmark
provides valuable insight into the robustness of VLN systems in continuous
environments. We observe a noticeable performance drop (up to -25%) in Success
Rate when evaluating the state-of-the-art VLN-CE methods on our benchmark.
Moreover, we formally define the task of Instruction Error Detection and
Localization, and establish an evaluation protocol on top of our benchmark
dataset. We also propose an effective method, based on a cross-modal
transformer architecture, that achieves the best performance in error detection
and localization, compared to baselines. Surprisingly, our proposed method has
revealed errors in the validation set of the two commonly used datasets for
VLN-CE, i.e., R2R-CE and RxR-CE, demonstrating the utility of our technique in
other tasks. Code and dataset will be made available upon acceptance at
https://intelligolabs.github.io/R2RIE-CE
\\ ( https://arxiv.org/abs/2403.10700 ,  2406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10726 (*cross-listing*)
Date: Fri, 15 Mar 2024 23:17:24 GMT   (143kb,D)

Title: Strict Partitioning for Sporadic Rigid Gang Tasks
Authors: Binqi Sun, Tomasz Kloda, Marco Caccamo
Categories: cs.DC cs.AI cs.AR
Comments: to be published in IEEE Real-Time and Embedded Technology and
  Applications Symposium (RTAS 2024)
\\
  The rigid gang task model is based on the idea of executing multiple threads
simultaneously on a fixed number of processors to increase efficiency and
performance. Although there is extensive literature on global rigid gang
scheduling, partitioned approaches have several practical advantages (e.g.,
task isolation and reduced scheduling overheads). In this paper, we propose a
new partitioned scheduling strategy for rigid gang tasks, named strict
partitioning. The method creates disjoint partitions of tasks and processors to
avoid inter-partition interference. Moreover, it tries to assign tasks with
similar volumes (i.e., parallelisms) to the same partition so that the
intra-partition interference can be reduced. Within each partition, the tasks
can be scheduled using any type of scheduler, which allows the use of a less
pessimistic schedulability test. Extensive synthetic experiments and a case
study based on Edge TPU benchmarks show that strict partitioning achieves
better schedulability performance than state-of-the-art global gang
schedulability analyses for both preemptive and non-preemptive rigid gang task
sets.
\\ ( https://arxiv.org/abs/2403.10726 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10751 (*cross-listing*)
Date: Sat, 16 Mar 2024 01:04:34 GMT   (2583kb,D)

Title: LIGHTCODE: Light Analytical and Neural Codes for Channels with Feedback
Authors: Sravan Kumar Ankireddy, Krishna Narayanan, Hyeji Kim
Categories: cs.IT cs.AI math.IT
Comments: 13 pages, 11 figures
\\
  The design of reliable and efficient codes for channels with feedback remains
a longstanding challenge in communication theory. While significant
improvements have been achieved by leveraging deep learning techniques, neural
codes often suffer from high computational costs, a lack of interpretability,
and limited practicality in resource-constrained settings. We focus on
designing low-complexity coding schemes that are interpretable and more
suitable for communication systems. We advance both analytical and neural
codes. First, we demonstrate that POWERBLAST, an analytical coding scheme
inspired by Schalkwijk-Kailath (SK) and Gallager-Nakiboglu (GN) schemes,
achieves notable reliability improvements over both SK and GN schemes,
outperforming neural codes in high signal-to-noise ratio (SNR) regions. Next,
to enhance reliability in low-SNR regions, we propose LIGHTCODE, a lightweight
neural code that achieves state-of-the-art reliability while using a fraction
of memory and compute compared to existing deep-learning-based codes. Finally,
we systematically analyze the learned codes, establishing connections between
LIGHTCODE and POWERBLAST, identifying components crucial for performance, and
providing interpretation aided by linear regression analysis.
\\ ( https://arxiv.org/abs/2403.10751 ,  2583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10776 (*cross-listing*)
Date: Sat, 16 Mar 2024 02:29:42 GMT   (4148kb,D)

Title: From Melting Pots to Misrepresentations: Exploring Harms in Generative
  AI
Authors: Sanjana Gautam, Pranav Narayanan Venkit, Sourojit Ghosh
Categories: cs.HC cs.AI cs.CY cs.LG
Comments: In CHI 2024: Generative AI and HCI workshop (GenAICHI 24)
\\
  With the widespread adoption of advanced generative models such as Gemini and
GPT, there has been a notable increase in the incorporation of such models into
sociotechnical systems, categorized under AI-as-a-Service (AIaaS). Despite
their versatility across diverse sectors, concerns persist regarding
discriminatory tendencies within these models, particularly favoring selected
`majority' demographics across various sociodemographic dimensions. Despite
widespread calls for diversification of media representations, marginalized
racial and ethnic groups continue to face persistent distortion, stereotyping,
and neglect within the AIaaS context. In this work, we provide a critical
summary of the state of research in the context of social harms to lead the
conversation to focus on their implications. We also present open-ended
research questions, guided by our discussion, to help define future research
pathways.
\\ ( https://arxiv.org/abs/2403.10776 ,  4148kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10780 (*cross-listing*)
Date: Sat, 16 Mar 2024 02:54:49 GMT   (8672kb,D)

Title: Segment Any Object Model (SAOM): Real-to-Simulation Fine-Tuning Strategy
  for Multi-Class Multi-Instance Segmentation
Authors: Mariia Khan, Yue Qiu, Yuren Cong, Jumana Abu-Khalaf, David Suter, Bodo
  Rosenhahn
Categories: cs.CV cs.AI
\\
  Multi-class multi-instance segmentation is the task of identifying masks for
multiple object classes and multiple instances of the same class within an
image. The foundational Segment Anything Model (SAM) is designed for promptable
multi-class multi-instance segmentation but tends to output part or sub-part
masks in the "everything" mode for various real-world applications. Whole
object segmentation masks play a crucial role for indoor scene understanding,
especially in robotics applications. We propose a new domain invariant
Real-to-Simulation (Real-Sim) fine-tuning strategy for SAM. We use object
images and ground truth data collected from Ai2Thor simulator during
fine-tuning (real-to-sim). To allow our Segment Any Object Model (SAOM) to work
in the "everything" mode, we propose the novel nearest neighbour assignment
method, updating point embeddings for each ground-truth mask. SAOM is evaluated
on our own dataset collected from Ai2Thor simulator. SAOM significantly
improves on SAM, with a 28% increase in mIoU and a 25% increase in mAcc for 54
frequently-seen indoor object classes. Moreover, our Real-to-Simulation
fine-tuning strategy demonstrates promising generalization performance in real
environments without being trained on the real-world data (sim-to-real). The
dataset and the code will be released after publication.
\\ ( https://arxiv.org/abs/2403.10780 ,  8672kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10805 (*cross-listing*)
Date: Sat, 16 Mar 2024 04:40:10 GMT   (20316kb,D)

Title: Speech-driven Personalized Gesture Synthetics: Harnessing Automatic
  Fuzzy Feature Inference
Authors: Fan Zhang, Zhaohan Wang, Xin Lyu, Siyuan Zhao, Mengjian Li, Weidong
  Geng, Naye Ji, Hui Du, Fuxing Gao, Hao Wu, Shunman Li
Categories: cs.SD cs.AI cs.CV cs.GR cs.HC eess.AS
Comments: 12 pages,
\\
  Speech-driven gesture generation is an emerging field within virtual human
creation. However, a significant challenge lies in accurately determining and
processing the multitude of input features (such as acoustic, semantic,
emotional, personality, and even subtle unknown features). Traditional
approaches, reliant on various explicit feature inputs and complex multimodal
processing, constrain the expressiveness of resulting gestures and limit their
applicability. To address these challenges, we present Persona-Gestor, a novel
end-to-end generative model designed to generate highly personalized 3D
full-body gestures solely relying on raw speech audio. The model combines a
fuzzy feature extractor and a non-autoregressive Adaptive Layer Normalization
(AdaLN) transformer diffusion architecture. The fuzzy feature extractor
harnesses a fuzzy inference strategy that automatically infers implicit,
continuous fuzzy features. These fuzzy features, represented as a unified
latent feature, are fed into the AdaLN transformer. The AdaLN transformer
introduces a conditional mechanism that applies a uniform function across all
tokens, thereby effectively modeling the correlation between the fuzzy features
and the gesture sequence. This module ensures a high level of gesture-speech
synchronization while preserving naturalness. Finally, we employ the diffusion
model to train and infer various gestures. Extensive subjective and objective
evaluations on the Trinity, ZEGGS, and BEAT datasets confirm our model's
superior performance to the current state-of-the-art approaches. Persona-Gestor
improves the system's usability and generalization capabilities, setting a new
benchmark in speech-driven gesture synthesis and broadening the horizon for
virtual human technology. Supplementary videos and code can be accessed at
https://zf223669.github.io/Diffmotion-v2-website/
\\ ( https://arxiv.org/abs/2403.10805 ,  20316kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10823 (*cross-listing*)
Date: Sat, 16 Mar 2024 06:21:19 GMT   (1513kb,D)

Title: VisionCLIP: An Med-AIGC based Ethical Language-Image Foundation Model
  for Generalizable Retina Image Analysis
Authors: Hao Wei, Bowen Liu, Minqing Zhang, Peilun Shi, Wu Yuan
Categories: cs.CV cs.AI
\\
  Generalist foundation model has ushered in newfound capabilities in medical
domain. However, the contradiction between the growing demand for high-quality
annotated data with patient privacy continues to intensify. The utilization of
medical artificial intelligence generated content (Med-AIGC) as an
inexhaustible resource repository arises as a potential solution to address the
aforementioned challenge. Here we harness 1 million open-source synthetic
fundus images paired with natural language descriptions, to curate an ethical
language-image foundation model for retina image analysis named VisionCLIP.
VisionCLIP achieves competitive performance on three external datasets compared
with the existing method pre-trained on real-world data in a zero-shot fashion.
The employment of artificially synthetic images alongside corresponding textual
data for training enables the medical foundation model to successfully
assimilate knowledge of disease symptomatology, thereby circumventing potential
breaches of patient confidentiality.
\\ ( https://arxiv.org/abs/2403.10823 ,  1513kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10834 (*cross-listing*)
Date: Sat, 16 Mar 2024 07:05:47 GMT   (3966kb,D)

Title: SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data
  Augmentation
Authors: Uiwon Hwang, Jonghyun Lee, Juhyeon Shin, Sungroh Yoon
Categories: cs.CV cs.AI cs.LG
Comments: ICLR 2024. Code: https://github.com/shinyflight/SFDA2
\\
  In the face of the deep learning model's vulnerability to domain shift,
source-free domain adaptation (SFDA) methods have been proposed to adapt models
to new, unseen target domains without requiring access to source domain data.
Although the potential benefits of applying data augmentation to SFDA are
attractive, several challenges arise such as the dependence on prior knowledge
of class-preserving transformations and the increase in memory and
computational requirements. In this paper, we propose Source-free Domain
Adaptation Through the Lens of Data Augmentation (SF(DA)$^2$), a novel approach
that leverages the benefits of data augmentation without suffering from these
challenges. We construct an augmentation graph in the feature space of the
pretrained model using the neighbor relationships between target features and
propose spectral neighborhood clustering to identify partitions in the
prediction space. Furthermore, we propose implicit feature augmentation and
feature disentanglement as regularization loss functions that effectively
utilize class semantic information within the feature space. These regularizers
simulate the inclusion of an unlimited number of augmented target features into
the augmentation graph while minimizing computational and memory demands. Our
method shows superior adaptation performance in SFDA scenarios, including 2D
image and 3D point cloud datasets and a highly imbalanced dataset.
\\ ( https://arxiv.org/abs/2403.10834 ,  3966kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10850 (*cross-listing*)
Date: Sat, 16 Mar 2024 08:10:23 GMT   (7011kb,D)

Title: GAgent: An Adaptive Rigid-Soft Gripping Agent with Vision Language
  Models for Complex Lighting Environments
Authors: Zhuowei Li and Miao Zhang and Xiaotian Lin and Meng Yin and Shuai Lu
  and Xueqian Wang
Categories: cs.RO cs.AI
\\
  This paper introduces GAgent: an Gripping Agent designed for open-world
environments that provides advanced cognitive abilities via VLM agents and
flexible grasping abilities with variable stiffness soft grippers. GAgent
comprises three primary components - Prompt Engineer module, Visual-Language
Model (VLM) core and Workflow module. These three modules enhance gripper
success rates by recognizing objects and materials and accurately estimating
grasp area even under challenging lighting conditions. As part of creativity,
researchers also created a bionic hybrid soft gripper with variable stiffness
capable of gripping heavy loads while still gently engaging objects. This
intelligent agent, featuring VLM-based cognitive processing with bionic design,
shows promise as it could potentially benefit UAVs in various scenarios.
\\ ( https://arxiv.org/abs/2403.10850 ,  7011kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10860 (*cross-listing*)
Date: Sat, 16 Mar 2024 08:57:00 GMT   (6036kb,D)

Title: Efficient Domain Adaptation for Endoscopic Visual Odometry
Authors: Junyang Wu, Yun Gu, Guang-Zhong Yang
Categories: cs.CV cs.AI
\\
  Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity
of realistic images with ground truth poses poses a significant challenge.
Therefore, domain adaptation offers a promising approach to bridge the
pre-operative planning domain with the intra-operative real domain for learning
odometry information. However, existing methodologies suffer from
inefficiencies in the training time. In this work, an efficient neural style
transfer framework for endoscopic visual odometry is proposed, which compresses
the time from pre-operative planning to testing phase to less than five
minutes. For efficient traing, this work focuses on training modules with only
a limited number of real images and we exploit pre-operative prior information
to dramatically reduce training duration. Moreover, during the testing phase,
we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in
lighting conditions between training and testing datasets. Experimental
evaluations conducted on two public endoscope datasets showcase that our method
achieves state-of-the-art accuracy in visual odometry tasks while boasting the
fastest training speeds. These results demonstrate significant promise for
intra-operative surgery applications.
\\ ( https://arxiv.org/abs/2403.10860 ,  6036kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10863 (*cross-listing*)
Date: Sat, 16 Mar 2024 09:06:38 GMT   (4767kb,D)

Title: stMCDI: Masked Conditional Diffusion Model with Graph Neural Network for
  Spatial Transcriptomics Data Imputation
Authors: Xiaoyu Li, Wenwen Min, Shunfang Wang, Changmiao Wang, Taosheng Xu
Categories: q-bio.GN cs.AI cs.LG
Comments: Submitted to IJCAI2024
\\
  Spatially resolved transcriptomics represents a significant advancement in
single-cell analysis by offering both gene expression data and their
corresponding physical locations. However, this high degree of spatial
resolution entails a drawback, as the resulting spatial transcriptomic data at
the cellular level is notably plagued by a high incidence of missing values.
Furthermore, most existing imputation methods either overlook the spatial
information between spots or compromise the overall gene expression data
distribution. To address these challenges, our primary focus is on effectively
utilizing the spatial location information within spatial transcriptomic data
to impute missing values, while preserving the overall data distribution. We
introduce \textbf{stMCDI}, a novel conditional diffusion model for spatial
transcriptomics data imputation, which employs a denoising network trained
using randomly masked data portions as guidance, with the unmasked data serving
as conditions. Additionally, it utilizes a GNN encoder to integrate the spatial
position information, thereby enhancing model performance. The results obtained
from spatial transcriptomics datasets elucidate the performance of our methods
relative to existing approaches.
\\ ( https://arxiv.org/abs/2403.10863 ,  4767kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10944 (*cross-listing*)
Date: Sat, 16 Mar 2024 15:17:13 GMT   (4410kb,D)

Title: Human Centered AI for Indian Legal Text Analytics
Authors: Sudipto Ghosh, Devanshu Verma, Balaji Ganesan, Purnima Bindal, Vikas
  Kumar and Vasudha Bhatnagar
Categories: cs.HC cs.AI
Comments: 7 pages, 7 figures
\\
  Legal research is a crucial task in the practice of law. It requires intense
human effort and intellectual prudence to research a legal case and prepare
arguments. Recent boom in generative AI has not translated to proportionate
rise in impactful legal applications, because of low trustworthiness and and
the scarcity of specialized datasets for training Large Language Models (LLMs).
This position paper explores the potential of LLMs within Legal Text Analytics
(LTA), highlighting specific areas where the integration of human expertise can
significantly enhance their performance to match that of experts. We introduce
a novel dataset and describe a human centered, compound AI system that
principally incorporates human inputs for performing LTA tasks with LLMs.
\\ ( https://arxiv.org/abs/2403.10944 ,  4410kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10968 (*cross-listing*)
Date: Sat, 16 Mar 2024 16:45:28 GMT   (925kb)

Title: Enhancing IoT Security Against DDoS Attacks through Federated Learning
Authors: Ghazaleh Shirvani, Saeid Ghasemshirazi, Mohammad Ali Alipour
Categories: cs.CR cs.AI cs.LG
\\
  The rapid proliferation of the Internet of Things (IoT) has ushered in
transformative connectivity between physical devices and the digital realm.
Nonetheless, the escalating threat of Distributed Denial of Service (DDoS)
attacks jeopardizes the integrity and reliability of IoT networks. Conventional
DDoS mitigation approaches are ill-equipped to handle the intricacies of IoT
ecosystems, potentially compromising data privacy. This paper introduces an
innovative strategy to bolster the security of IoT networks against DDoS
attacks by harnessing the power of Federated Learning that allows multiple IoT
devices or edge nodes to collaboratively build a global model while preserving
data privacy and minimizing communication overhead. The research aims to
investigate Federated Learning's effectiveness in detecting and mitigating DDoS
attacks in IoT. Our proposed framework leverages IoT devices' collective
intelligence for real-time attack detection without compromising sensitive
data. This study proposes innovative deep autoencoder approaches for data
dimensionality reduction, retraining, and partial selection to enhance the
performance and stability of the proposed model. Additionally, two renowned
aggregation algorithms, FedAvg and FedAvgM, are employed in this research.
Various metrics, including true positive rate, false positive rate, and
F1-score, are employed to evaluate the model. The dataset utilized in this
research, N-BaIoT, exhibits non-IID data distribution, where data categories
are distributed quite differently. The negative impact of these distribution
disparities is managed by employing retraining and partial selection
techniques, enhancing the final model's stability. Furthermore, evaluation
results demonstrate that the FedAvgM aggregation algorithm outperforms FedAvg,
indicating that in non-IID datasets, FedAvgM provides better stability and
performance.
\\ ( https://arxiv.org/abs/2403.10968 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10988 (*cross-listing*)
Date: Sat, 16 Mar 2024 18:04:12 GMT   (40379kb,D)

Title: Boosting Flow-based Generative Super-Resolution Models via Learned Prior
Authors: Li-Yuan Tsao, Yi-Chen Lo, Chia-Che Chang, Hao-Wei Chen, Roy Tseng,
  Chien Feng, Chun-Yi Lee
Categories: cs.CV cs.AI
Comments: Accepted to CVPR2024
\\
  Flow-based super-resolution (SR) models have demonstrated astonishing
capabilities in generating high-quality images. However, these methods
encounter several challenges during image generation, such as grid artifacts,
exploding inverses, and suboptimal results due to a fixed sampling temperature.
To overcome these issues, this work introduces a conditional learned prior to
the inference phase of a flow-based SR model. This prior is a latent code
predicted by our proposed latent module conditioned on the low-resolution
image, which is then transformed by the flow model into an SR image. Our
framework is designed to seamlessly integrate with any contemporary flow-based
SR model without modifying its architecture or pre-trained weights. We evaluate
the effectiveness of our proposed framework through extensive experiments and
ablation analyses. The proposed framework successfully addresses all the
inherent issues in flow-based SR models and enhances their performance in
various SR scenarios. Our code is available at:
https://github.com/liyuantsao/FlowSR-LP
\\ ( https://arxiv.org/abs/2403.10988 ,  40379kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10997 (*cross-listing*)
Date: Sat, 16 Mar 2024 18:50:44 GMT   (18958kb,D)

Title: N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields
Authors: Yash Bhalgat, Iro Laina, Jo\~ao F. Henriques, Andrew Zisserman, Andrea
  Vedaldi
Categories: cs.CV cs.AI cs.GR cs.LG
\\
  Understanding complex scenes at multiple levels of abstraction remains a
formidable challenge in computer vision. To address this, we introduce Nested
Neural Feature Fields (N2F2), a novel approach that employs hierarchical
supervision to learn a single feature field, wherein different dimensions
within the same high-dimensional feature encode scene properties at varying
granularities. Our method allows for a flexible definition of hierarchies,
tailored to either the physical dimensions or semantics or both, thereby
enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D
class-agnostic segmentation model to provide semantically meaningful pixel
groupings at arbitrary scales in the image space, and query the CLIP
vision-encoder to obtain language-aligned embeddings for each of these
segments. Our proposed hierarchical supervision method then assigns different
nested dimensions of the feature field to distill the CLIP embeddings using
deferred volumetric rendering at varying physical scales, creating a
coarse-to-fine representation. Extensive experiments show that our approach
outperforms the state-of-the-art feature field distillation methods on tasks
such as open-vocabulary 3D segmentation and localization, demonstrating the
effectiveness of the learned nested feature field.
\\ ( https://arxiv.org/abs/2403.10997 ,  18958kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11015 (*cross-listing*)
Date: Sat, 16 Mar 2024 20:56:22 GMT   (3875kb)

Title: Identifying the Attractors of Gene Regulatory Networks from Expression
  Data under Uncertainty: An Interpretable Approach
Authors: Alireza Rowhanimanesh
Categories: q-bio.MN cs.AI cs.SY eess.SY
\\
  In systems biology, attractor landscape analysis of gene regulatory networks
is recognized as a powerful computational tool for studying various cellular
states from proliferation and differentiation to senescence and apoptosis.
Therefore, accurate identification of attractors plays a critical role in
determination of the cell fates. On the other hand, in a real biological
circuit, genetic/epigenetic alterations as well as varying environmental
factors drastically take effect on the location, characteristics, and even the
number of attractors. The central question is: Given a temporal gene expression
profile of a real gene regulatory network, how can the attractors be robustly
identified in the presence of huge amount of uncertainty? This paper addresses
this question using a novel approach based on Zadeh Computing with Words. The
proposed scheme could effectively identify the attractors from temporal gene
expression data in terms of both fuzzy logic-based and linguistic descriptions
which are simply interpretable by human experts. Therefore, this method can be
considered as an effective step towards interpretable artificial intelligence.
Without loss of generality, genetic toggle switch is considered as the case
study. The nonlinear dynamics of this benchmark gene regulatory network is
computationally modeled by the notion of uncertain stochastic differential
equations. The results of in-silico study demonstrate the efficiency and
robustness of the proposed method.
\\ ( https://arxiv.org/abs/2403.11015 ,  3875kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11021 (*cross-listing*)
Date: Sat, 16 Mar 2024 21:40:27 GMT   (14886kb,D)

Title: Neuro-Symbolic Video Search
Authors: Minkyu Choi, Harsh Goel, Mohammad Omama, Yunhao Yang, Sahil Shah,
  Sandeep Chinchali
Categories: cs.CV cs.AI
\\
  The unprecedented surge in video data production in recent years necessitates
efficient tools to extract meaningful frames from videos for downstream tasks.
Long-term temporal reasoning is a key desideratum for frame retrieval systems.
While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are
proficient in short-term semantic understanding, they surprisingly fail at
long-term reasoning across frames. A key reason for this failure is that they
intertwine per-frame perception and temporal reasoning into a single deep
network. Hence, decoupling but co-designing semantic understanding and temporal
reasoning is essential for efficient scene identification. We propose a system
that leverages vision-language models for semantic understanding of individual
frames but effectively reasons about the long-term evolution of events using
state machines and temporal logic (TL) formulae that inherently capture memory.
Our TL-based reasoning improves the F1 score of complex event identification by
9-15% compared to benchmarks that use GPT4 for reasoning on state-of-the-art
self-driving datasets such as Waymo and NuScenes.
\\ ( https://arxiv.org/abs/2403.11021 ,  14886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11027 (*cross-listing*)
Date: Sat, 16 Mar 2024 22:14:56 GMT   (48648kb,D)

Title: Reward Guided Latent Consistency Distillation
Authors: Jiachen Li, Weixi Feng, Wenhu Chen, William Yang Wang
Categories: cs.CV cs.AI
Comments: Project page: https://rg-lcd.github.io/
\\
  Latent Consistency Distillation (LCD) has emerged as a promising paradigm for
efficient text-to-image synthesis. By distilling a latent consistency model
(LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates
the generation of high-fidelity images within merely 2 to 4 inference steps.
However, the LCM's efficient inference is obtained at the cost of the sample
quality. In this paper, we propose compensating the quality loss by aligning
LCM's output with human preference during training. Specifically, we introduce
Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM)
into the LCD process by augmenting the original LCD loss with the objective of
maximizing the reward associated with LCM's single-step generation. As
validated through human evaluation, when trained with the feedback of a good
RM, the 2-step generations from our RG-LCM are favored by humans over the
50-step DDIM samples from the teacher LDM, representing a 25 times inference
acceleration without quality loss.
  As directly optimizing towards differentiable RMs can suffer from
over-optimization, we overcome this difficulty by proposing the use of a latent
proxy RM (LRM). This novel component serves as an intermediary, connecting our
LCM with the RM. Empirically, we demonstrate that incorporating the LRM into
our RG-LCD successfully avoids high-frequency noise in the generated images,
contributing to both improved FID on MS-COCO and a higher HPSv2.1 score on
HPSv2's test set, surpassing those achieved by the baseline LCM.
\\ ( https://arxiv.org/abs/2403.11027 ,  48648kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11046 (*cross-listing*)
Date: Sun, 17 Mar 2024 00:11:15 GMT   (550kb)

Title: Regulating Chatbot Output via Inter-Informational Competition
Authors: Jiawei Zhang
Categories: cs.CY cs.AI cs.ET cs.IT cs.LG math.IT
Comments: 20,000-word legal Article, forthcoming in Northwestern Journal of
  Technology and Intellectual Property
\\
  The advent of ChatGPT has sparked over a year of regulatory frenzy. However,
few existing studies have rigorously questioned the assumption that, if left
unregulated, AI chatbot's output would inflict tangible, severe real harm on
human affairs. Most researchers have overlooked the critical possibility that
the information market itself can effectively mitigate these risks and, as a
result, they tend to use regulatory tools to address the issue directly. This
Article develops a yardstick for reevaluating both AI-related content risks and
corresponding regulatory proposals by focusing on inter-informational
competition among various outlets. The decades-long history of regulating
information and communications technologies indicates that regulators tend to
err too much on the side of caution and to put forward excessive regulatory
measures when encountering the uncertainties brought about by new technologies.
In fact, a trove of empirical evidence has demonstrated that market competition
among information outlets can effectively mitigate most risks and that
overreliance on regulation is not only unnecessary but detrimental, as well.
This Article argues that sufficient competition among chatbots and other
information outlets in the information marketplace can sufficiently mitigate
and even resolve most content risks posed by generative AI technologies. This
renders certain loudly advocated regulatory strategies, like mandatory
prohibitions, licensure, curation of datasets, and notice-and-response regimes,
truly unnecessary and even toxic to desirable competition and innovation
throughout the AI industry. Ultimately, the ideas that I advance in this
Article should pour some much-needed cold water on the regulatory frenzy over
generative AI and steer the issue back to a rational track.
\\ ( https://arxiv.org/abs/2403.11046 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11047 (*cross-listing*)
Date: Sun, 17 Mar 2024 00:14:29 GMT   (4240kb,D)

Title: From Pixels to Predictions: Spectrogram and Vision Transformer for
  Better Time Series Forecasting
Authors: Zhen Zeng, Rachneet Kaur, Suchetha Siddagangappa, Tucker Balch,
  Manuela Veloso
Categories: cs.CV cs.AI cs.CE
Comments: Published at ACM ICAIF 2023
\\
  Time series forecasting plays a crucial role in decision-making across
various domains, but it presents significant challenges. Recent studies have
explored image-driven approaches using computer vision models to address these
challenges, often employing lineplots as the visual representation of time
series data. In this paper, we propose a novel approach that uses
time-frequency spectrograms as the visual representation of time series data.
We introduce the use of a vision transformer for multimodal learning,
showcasing the advantages of our approach across diverse datasets from
different domains. To evaluate its effectiveness, we compare our method against
statistical baselines (EMA and ARIMA), a state-of-the-art deep learning-based
approach (DeepAR), other visual representations of time series data (lineplot
images), and an ablation study on using only the time series as input. Our
experiments demonstrate the benefits of utilizing spectrograms as a visual
representation for time series data, along with the advantages of employing a
vision transformer for simultaneous learning in both the time and frequency
domains.
\\ ( https://arxiv.org/abs/2403.11047 ,  4240kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11073 (*cross-listing*)
Date: Sun, 17 Mar 2024 03:38:50 GMT   (3387kb)

Title: Tokensome: Towards a Genetic Vision-Language GPT for Explainable and
  Cognitive Karyotyping
Authors: Haoxi Zhang, Xinxu Zhang, Yuanxin Lin, Maiqi Wang, Yi Lai, Yu Wang,
  Linfeng Yu, Yufeng Xu, Ran Cheng, Edward Szczerbicki
Categories: cs.CV cs.AI
Comments: Preprint. Work in progress
\\
  Automatic karyotype analysis is often defined as a visual perception task
focused solely on chromosomal object-level modeling. This definition has led
most existing methods to overlook componential and holistic information,
significantly constraining model performance. Moreover, the lack of
interpretability in current technologies hinders clinical adoption. In this
paper, we introduce Tokensome, a novel vision-language model based on
chromosome tokenization for explainable and cognitive karyotyping. Tokensome
elevates the method from the conventional visual perception layer to the
cognitive decision-making layer. This elevation enables the integration of
domain knowledge and cognitive reasoning via knowledge graphs and LLMs,
markedly enhancing model's explainability and facilitating abnormality
detection.
\\ ( https://arxiv.org/abs/2403.11073 ,  3387kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11074 (*cross-listing*)
Date: Sun, 17 Mar 2024 03:45:14 GMT   (5714kb,D)

Title: Audio-Visual Segmentation via Unlabeled Frame Exploitation
Authors: Jinxiang Liu, Yikun Liu, Fei Zhang, Chen Ju, Ya Zhang, Yanfeng Wang
Categories: cs.CV cs.AI cs.MM cs.SD eess.AS
Comments: Accepted by CVPR 2024
\\
  Audio-visual segmentation (AVS) aims to segment the sounding objects in video
frames. Although great progress has been witnessed, we experimentally reveal
that current methods reach marginal performance gain within the use of the
unlabeled frames, leading to the underutilization issue. To fully explore the
potential of the unlabeled frames for AVS, we explicitly divide them into two
categories based on their temporal characteristics, i.e., neighboring frame
(NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame,
often contain rich motion information that assists in the accurate localization
of sounding objects. Contrary to NFs, DFs have long temporal distances from the
labeled frame, which share semantic-similar objects with appearance variations.
Considering their unique characteristics, we propose a versatile framework that
effectively leverages them to tackle AVS. Specifically, for NFs, we exploit the
motion cues as the dynamic guidance to improve the objectness localization.
Besides, we exploit the semantic cues in DFs by treating them as valid
augmentations to the labeled frames, which are then used to enrich data
diversity in a self-training manner. Extensive experimental results demonstrate
the versatility and superiority of our method, unleashing the power of the
abundant unlabeled frames.
\\ ( https://arxiv.org/abs/2403.11074 ,  5714kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11075 (*cross-listing*)
Date: Sun, 17 Mar 2024 03:52:52 GMT   (2320kb,D)

Title: GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented
  Mental Alignment
Authors: Lance Ying, Kunal Jha, Shivam Aarya, Joshua B. Tenenbaum, Antonio
  Torralba, Tianmin Shu
Categories: cs.HC cs.AI cs.MA
Comments: 8 pages, 5 figures
\\
  Verbal communication plays a crucial role in human cooperation, particularly
when the partners only have incomplete information about the task, environment,
and each other's mental state. In this paper, we propose a novel cooperative
communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates
verbal communication as a planning problem that minimizes the misalignment
between the parts of agents' mental states that are relevant to the goals. This
approach enables an embodied assistant to reason about when and how to
proactively initialize communication with humans verbally using natural
language to help achieve better cooperation. We evaluate our approach against
strong baselines in two challenging environments, Overcooked (a multiplayer
game) and VirtualHome (a household simulator). Our experimental results
demonstrate that large language models struggle with generating meaningful
communication that is grounded in the social and physical context. In contrast,
our approach can successfully generate concise verbal communication for the
embodied assistant to effectively boost the performance of the cooperation as
well as human users' perception of the assistant.
\\ ( https://arxiv.org/abs/2403.11075 ,  2320kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11116 (*cross-listing*)
Date: Sun, 17 Mar 2024 06:53:44 GMT   (12531kb,D)

Title: PhD: A Prompted Visual Hallucination Evaluation Dataset
Authors: Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong
  Lian, Zhanhui Kang, and Xirong Li
Categories: cs.CV cs.AI
\\
  The rapid growth of Large Language Models (LLMs) has driven the development
of Large Vision-Language Models (LVLMs). The challenge of hallucination,
prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly
focus on object hallucination in LVLM, ignoring diverse types of LVLM
hallucinations. In this study, we delve into the Intrinsic Vision-Language
Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of
IVL-Hallu on their causes and reflections. Specifically, we propose several
novel IVL-Hallu tasks and categorize them into four types: (a) object
hallucination, which arises from the misidentification of objects, (b)
attribute hallucination, which is caused by the misidentification of
attributes, (c) multi-modal conflicting hallucination, which derives from the
contradictions between textual and visual information, and (d)
counter-common-sense hallucination, which owes to the contradictions between
the LVLM knowledge and actual images. Based on these taxonomies, we propose a
more challenging benchmark named PhD to evaluate and explore IVL-Hallu. An
automated pipeline is proposed for generating different types of IVL-Hallu
data. Extensive experiments on five SOTA LVLMs reveal their inability to
effectively tackle our proposed IVL-Hallu tasks, with detailed analyses and
insights on the origins and possible solutions of these new challenging
IVL-Hallu tasks, facilitating future researches on IVL-Hallu and LVLM. The
benchmark can be accessed at
\href{https://github.com/jiazhen-code/IntrinsicHallu}{this https URL}.
\\ ( https://arxiv.org/abs/2403.11116 ,  12531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11162 (*cross-listing*)
Date: Sun, 17 Mar 2024 10:06:38 GMT   (35352kb,D)

Title: CGI-DM: Digital Copyright Authentication for Diffusion Models via
  Contrasting Gradient Inversion
Authors: Xiaoyu Wu, Yang Hua, Chumeng Liang, Jiaru Zhang, Hao Wang, Tao Song,
  Haibing Guan
Categories: cs.CV cs.AI cs.CR cs.CY cs.LG
Comments: Accepted by CVPR 2024
\\
  Diffusion Models (DMs) have evolved into advanced image generation tools,
especially for few-shot generation where a pretrained model is fine-tuned on a
small set of images to capture a specific style or object. Despite their
success, concerns exist about potential copyright violations stemming from the
use of unauthorized data in this process. In response, we present Contrasting
Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring
vivid visual representations for digital copyright authentication. Our approach
involves removing partial information of an image and recovering missing
details by exploiting conceptual differences between the pretrained and
fine-tuned models. We formulate the differences as KL divergence between latent
variables of the two models when given the same input image, which can be
maximized through Monte Carlo sampling and Projected Gradient Descent (PGD).
The similarity between original and recovered images serves as a strong
indicator of potential infringements. Extensive experiments on the WikiArt and
Dreambooth datasets demonstrate the high accuracy of CGI-DM in digital
copyright authentication, surpassing alternative validation techniques. Code
implementation is available at https://github.com/Nicholas0228/Revelio.
\\ ( https://arxiv.org/abs/2403.11162 ,  35352kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11175 (*cross-listing*)
Date: Sun, 17 Mar 2024 11:23:51 GMT   (46kb)

Title: Prior-dependent analysis of posterior sampling reinforcement learning
  with function approximation
Authors: Yingru Li and Zhi-Quan Luo
Categories: stat.ML cs.AI cs.IT cs.LG math.IT math.ST stat.TH
Comments: Published in the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS)
\\
  This work advances randomized exploration in reinforcement learning (RL) with
function approximation modeled by linear mixture MDPs. We establish the first
prior-dependent Bayesian regret bound for RL with function approximation; and
refine the Bayesian regret analysis for posterior sampling reinforcement
learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log
T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the
planning horizon, and $T$ the total number of interactions. This signifies a
methodological enhancement by optimizing the $\mathcal{O}(\sqrt{\log T})$
factor over the previous benchmark (Osband and Van Roy, 2014) specified to
linear mixture MDPs. Our approach, leveraging a value-targeted model learning
perspective, introduces a decoupling argument and a variance reduction
technique, moving beyond traditional analyses reliant on confidence sets and
concentration inequalities to formalize Bayesian regret bounds more
effectively.
\\ ( https://arxiv.org/abs/2403.11175 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11202 (*cross-listing*)
Date: Sun, 17 Mar 2024 13:01:03 GMT   (1341kb,D)

Title: Data is all you need: Finetuning LLMs for Chip Design via an Automated
  design-data augmentation framework
Authors: Kaiyan Chang and Kun Wang and Nan Yang and Ying Wang and Dantong Jin
  and Wenlong Zhu and Zhirong Chen and Cangyuan Li and Hao Yan and Yunhao Zhou
  and Zhuoliang Zhao and Yuan Cheng and Yudong Pan and Yiqi Liu and Mengdi Wang
  and Shengwen Liang and yinhe han and Huawei Li and Xiaowei Li
Categories: cs.AR cs.AI cs.PL
Comments: Accepted by DAC 2024; please note that this is not the final
  camera-ready version
\\
  Recent advances in large language models have demonstrated their potential
for automated generation of hardware description language (HDL) code from
high-level prompts. Researchers have utilized fine-tuning to enhance the
ability of these large language models (LLMs) in the field of Chip Design.
However, the lack of Verilog data hinders further improvement in the quality of
Verilog generation by LLMs. Additionally, the absence of a Verilog and
Electronic Design Automation (EDA) script data augmentation framework
significantly increases the time required to prepare the training dataset for
LLM trainers. This paper proposes an automated design-data augmentation
framework, which generates high-volume and high-quality natural language
aligned with Verilog and EDA scripts. For Verilog generation, it translates
Verilog files to an abstract syntax tree and then maps nodes to natural
language with a predefined template. For Verilog repair, it uses predefined
rules to generate the wrong verilog file and then pairs EDA Tool feedback with
the right and wrong verilog file. For EDA Script generation, it uses existing
LLM(GPT-3.5) to obtain the description of the Script. To evaluate the
effectiveness of our data augmentation method, we finetune Llama2-13B and
Llama2-7B models using the dataset generated by our augmentation framework. The
results demonstrate a significant improvement in the Verilog generation tasks
with LLMs. Moreover, the accuracy of Verilog generation surpasses that of the
current state-of-the-art open-source Verilog generation model, increasing from
58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass
rate improvement compared with GPT-3.5 in Verilog generation and outperforms in
EDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.
\\ ( https://arxiv.org/abs/2403.11202 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11207 (*cross-listing*)
Date: Sun, 17 Mar 2024 13:15:22 GMT   (9981kb,D)

Title: MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data
Authors: Paul S. Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese
  Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu,
  Thomas Naselaris, Kenneth A. Norman, Tanishq Mathew Abraham
Categories: cs.CV cs.AI q-bio.NC
Comments: Code at https://github.com/MedARC-AI/MindEyeV2/tree/main
\\
  Reconstructions of visual perception from brain activity have improved
tremendously, but the practical utility of such methods has been limited. This
is because such models are trained independently per subject where each subject
requires dozens of hours of expensive fMRI training data to attain high-quality
results. The present work showcases high-quality reconstructions using only 1
hour of fMRI training data. We pretrain our model across 7 subjects and then
fine-tune on minimal data from a new subject. Our novel functional alignment
procedure linearly maps all brain data to a shared-subject latent space,
followed by a shared non-linear mapping to CLIP image space. We then map from
CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP
latents as inputs instead of text. This approach improves out-of-subject
generalization with limited training data and also attains state-of-the-art
image retrieval and reconstruction metrics compared to single-subject
approaches. MindEye2 demonstrates how accurate reconstructions of perception
are possible from a single visit to the MRI facility. All code is available on
GitHub.
\\ ( https://arxiv.org/abs/2403.11207 ,  9981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11220 (*cross-listing*)
Date: Sun, 17 Mar 2024 13:43:10 GMT   (17938kb,D)

Title: CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object
  Detection under Unknown Degradations
Authors: Yuwei Zhang, Yan Wu, Yanming Liu, Xinyue Peng
Categories: cs.CV cs.AI cs.LG
\\
  Object detection methods under known single degradations have been
extensively investigated. However, existing approaches require prior knowledge
of the degradation type and train a separate model for each, limiting their
practical applications in unpredictable environments. To address this
challenge, we propose a chain-of-thought (CoT) prompted adaptive enhancer,
CPA-Enhancer, for object detection under unknown degradations. Specifically,
CPA-Enhancer progressively adapts its enhancement strategy under the
step-by-step guidance of CoT prompts, that encode degradation-related
information. To the best of our knowledge, it's the first work that exploits
CoT prompting for object detection tasks. Overall, CPA-Enhancer is a
plug-and-play enhancement model that can be integrated into any generic
detectors to achieve substantial gains on degraded images, without knowing the
degradation type priorly. Experimental results demonstrate that CPA-Enhancer
not only sets the new state of the art for object detection but also boosts the
performance of other downstream vision tasks under unknown degradations.
\\ ( https://arxiv.org/abs/2403.11220 ,  17938kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11299 (*cross-listing*)
Date: Sun, 17 Mar 2024 18:42:38 GMT   (3265kb,D)

Title: SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
Authors: Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, Zhiqiang Tao
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Recent advancements in the vision-language model have shown notable
generalization in vision-language tasks after visual instruction tuning.
However, bridging the gap between the pre-trained vision encoder and the large
language models becomes the whole network's bottleneck. To improve
cross-modality alignment, existing works usually consider more visual
instruction data covering a broader range of vision tasks to fine-tune the
model for question-answering, which are costly to obtain. However, the image
contains rich contextual information that has been largely under-explored. This
paper first attempts to harness this overlooked context within visual
instruction data, training the model to self-supervised `learning' how to ask
high-quality questions. In this way, we introduce a novel framework named
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA
exhibits proficiency in generating flexible and meaningful image-related
questions while analyzing the visual clue and prior language knowledge,
signifying an advanced level of generalized visual understanding. Moreover,
fine-tuning SQ-LLaVA on higher-quality instruction data shows a consistent
performance improvement compared with traditional visual-instruction tuning
methods. This improvement highlights the efficacy of self-questioning
techniques in achieving a deeper and more nuanced comprehension of visual
content across various contexts.
\\ ( https://arxiv.org/abs/2403.11299 ,  3265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11304 (*cross-listing*)
Date: Sun, 17 Mar 2024 18:53:46 GMT   (178kb,D)

Title: Pioneering SE(2)-Equivariant Trajectory Planning for Automated Driving
Authors: Steffen Hagedorn, Marcel Milich, Alexandru P. Condurache
Categories: cs.RO cs.AI cs.MA
\\
  Planning the trajectory of the controlled ego vehicle is a key challenge in
automated driving. As for human drivers, predicting the motions of surrounding
vehicles is important to plan the own actions. Recent motion prediction methods
utilize equivariant neural networks to exploit geometric symmetries in the
scene. However, no existing method combines motion prediction and trajectory
planning in a joint step while guaranteeing equivariance under
roto-translations of the input space. We address this gap by proposing a
lightweight equivariant planning model that generates multi-modal joint
predictions for all vehicles and selects one mode as the ego plan. The
equivariant network design improves sample efficiency, guarantees output
stability, and reduces model parameters. We further propose equivariant route
attraction to guide the ego vehicle along a high-level route provided by an
off-the-shelf GPS navigation system. This module creates a momentum from
embedded vehicle positions toward the route in latent space while keeping the
equivariance property. Route attraction enables goal-oriented behavior without
forcing the vehicle to stick to the exact route. We conduct experiments on the
challenging nuScenes dataset to investigate the capability of our planner. The
results show that the planned trajectory is stable under roto-translations of
the input scene which demonstrates the equivariance of our model. Despite using
only a small split of the dataset for training, our method improves L2 distance
at 3 s by 20.6 % and surpasses the state of the art.
\\ ( https://arxiv.org/abs/2403.11304 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11328 (*cross-listing*)
Date: Sun, 17 Mar 2024 20:14:57 GMT   (6866kb,D)

Title: Domain-Guided Masked Autoencoders for Unique Player Identification
Authors: Bavesh Balaji, Jerrin Bright, Sirisha Rambhatla, Yuhao Chen, Alexander
  Wong, John Zelek and David A Clausi
Categories: cs.CV cs.AI
Comments: Submitted to 21st International Conference on Robots and Vision
  (CRV'24), Guelph, Ontario, Canada
\\
  Unique player identification is a fundamental module in vision-driven sports
analytics. Identifying players from broadcast videos can aid with various
downstream tasks such as player assessment, in-game analysis, and broadcast
production. However, automatic detection of jersey numbers using deep features
is challenging primarily due to: a) motion blur, b) low resolution video feed,
and c) occlusions. With their recent success in various vision tasks, masked
autoencoders (MAEs) have emerged as a superior alternative to conventional
feature extractors. However, most MAEs simply zero-out image patches either
randomly or focus on where to mask rather than how to mask. Motivated by human
vision, we devise a novel domain-guided masking policy for MAEs termed d-MAE to
facilitate robust feature extraction in the presence of motion blur for player
identification. We further introduce a new spatio-temporal network leveraging
our novel d-MAE for unique player identification. We conduct experiments on
three large-scale sports datasets, including a curated baseball dataset, the
SoccerNet dataset, and an in-house ice hockey dataset. We preprocess the
datasets using an upgraded keyframe identification (KfID) module by focusing on
frames containing jersey numbers. Additionally, we propose a keyframe-fusion
technique to augment keyframes, preserving spatial and temporal context. Our
spatio-temporal network showcases significant improvements, surpassing the
current state-of-the-art by 8.58%, 4.29%, and 1.20% in the test set accuracies,
respectively. Rigorous ablations highlight the effectiveness of our
domain-guided masking approach and the refined KfID module, resulting in
performance enhancements of 1.48% and 1.84% respectively, compared to original
architectures.
\\ ( https://arxiv.org/abs/2403.11328 ,  6866kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11337 (*cross-listing*)
Date: Sun, 17 Mar 2024 20:36:43 GMT   (7669kb,D)

Title: Enhancing Bandwidth Efficiency for Video Motion Transfer Applications
  using Deep Learning Based Keypoint Prediction
Authors: Xue Bai, Tasmiah Haque, Sumit Mohan, Yuliang Cai, Byungheon Jeong,
  Adam Halasz, Srinjoy Das
Categories: cs.CV cs.AI
\\
  We propose a deep learning based novel prediction framework for enhanced
bandwidth reduction in motion transfer enabled video applications such as video
conferencing, virtual reality gaming and privacy preservation for patient
health monitoring. To model complex motion, we use the First Order Motion Model
(FOMM) that represents dynamic objects using learned keypoints along with their
local affine transformations. Keypoints are extracted by a self-supervised
keypoint detector and organized in a time series corresponding to the video
frames. Prediction of keypoints, to enable transmission using lower frames per
second on the source device, is performed using a Variational Recurrent Neural
Network (VRNN). The predicted keypoints are then synthesized to video frames
using an optical flow estimator and a generator network. This efficacy of
leveraging keypoint based representations in conjunction with VRNN based
prediction for both video animation and reconstruction is demonstrated on three
diverse datasets. For real-time applications, our results show the
effectiveness of our proposed architecture by enabling up to 2x additional
bandwidth reduction over existing keypoint based video motion transfer
frameworks without significantly compromising video quality.
\\ ( https://arxiv.org/abs/2403.11337 ,  7669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11368 (*cross-listing*)
Date: Sun, 17 Mar 2024 23:07:13 GMT   (3914kb,D)

Title: Driving Style Alignment for LLM-powered Driver Agent
Authors: Ruoxuan Yang, Xinyue Zhang, Anais Fernandez-Laaksonen, Xin Ding and
  Jiangtao Gong
Categories: cs.RO cs.AI
MSC-class: 68T42
\\
  Recently, LLM-powered driver agents have demonstrated considerable potential
in the field of autonomous driving, showcasing human-like reasoning and
decision-making abilities.However, current research on aligning driver agent
behaviors with human driving styles remains limited, partly due to the scarcity
of high-quality natural language data from human driving behaviors.To address
this research gap, we propose a multi-alignment framework designed to align
driver agents with human driving styles through demonstrations and feedback.
Notably, we construct a natural language dataset of human driver behaviors
through naturalistic driving experiments and post-driving interviews, offering
high-quality human demonstrations for LLM alignment. The framework's
effectiveness is validated through simulation experiments in the CARLA urban
traffic simulator and further corroborated by human evaluations. Our research
offers valuable insights into designing driving agents with diverse driving
styles.The implementation of the framework and details of the dataset can be
found at the link.
\\ ( https://arxiv.org/abs/2403.11368 ,  3914kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11401 (*cross-listing*)
Date: Mon, 18 Mar 2024 01:18:48 GMT   (1461kb,D)

Title: Scene-LLM: Extending Language Model for 3D Visual Understanding and
  Reasoning
Authors: Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, Wenhan Xiong
Categories: cs.CV cs.AI
\\
  This paper introduces Scene-LLM, a 3D-visual-language model that enhances
embodied agents' abilities in interactive 3D indoor environments by integrating
the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a
hybrid 3D visual feature representation, that incorporates dense spatial
information and supports scene state updates. The model employs a projection
layer to efficiently project these features in the pre-trained textual
embedding space, enabling effective interpretation of 3D visual information.
Unique to our approach is the integration of both scene-level and ego-centric
3D information. This combination is pivotal for interactive planning, where
scene-level data supports global planning and ego-centric data is important for
localization. Notably, we use ego-centric 3D frame features for feature
alignment, an efficient technique that enhances the model's ability to align
features of small objects within the scene. Our experiments with Scene-LLM
demonstrate its strong capabilities in dense captioning, question answering,
and interactive planning. We believe Scene-LLM advances the field of 3D visual
understanding and reasoning, offering new possibilities for sophisticated agent
interactions in indoor settings.
\\ ( https://arxiv.org/abs/2403.11401 ,  1461kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11402 (*cross-listing*)
Date: Mon, 18 Mar 2024 01:20:38 GMT   (192kb,D)

Title: Embracing the Generative AI Revolution: Advancing Tertiary Education in
  Cybersecurity with GPT
Authors: Raza Nowrozy and David Jam
Categories: cs.CY cs.AI
\\
  The rapid advancement of generative Artificial Intelligence (AI)
technologies, particularly Generative Pre-trained Transformer (GPT) models such
as ChatGPT, has the potential to significantly impact cybersecurity. In this
study, we investigated the impact of GPTs, specifically ChatGPT, on tertiary
education in cybersecurity, and provided recommendations for universities to
adapt their curricula to meet the evolving needs of the industry. Our research
highlighted the importance of understanding the alignment between GPT's
``mental model'' and human cognition, as well as the enhancement of GPT
capabilities to human skills based on Bloom's taxonomy. By analyzing current
educational practices and the alignment of curricula with industry
requirements, we concluded that universities providing practical degrees like
cybersecurity should align closely with industry demand and embrace the
inevitable generative AI revolution, while applying stringent ethics oversight
to safeguard responsible GPT usage. We proposed a set of recommendations
focused on updating university curricula, promoting agility within
universities, fostering collaboration between academia, industry, and
policymakers, and evaluating and assessing educational outcomes.
\\ ( https://arxiv.org/abs/2403.11402 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11415 (*cross-listing*)
Date: Mon, 18 Mar 2024 02:08:58 GMT   (5748kb,D)

Title: DreamSampler: Unifying Diffusion Sampling and Score Distillation for
  Image Manipulation
Authors: Jeongsol Kim, Geon Yeong Park, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG
\\
  Reverse sampling and score-distillation have emerged as main workhorses in
recent years for image manipulation using latent diffusion models (LDMs). While
reverse diffusion sampling often requires adjustments of LDM architecture or
feature engineering, score distillation offers a simple yet powerful
model-agnostic approach, but it is often prone to mode-collapsing. To address
these limitations and leverage the strengths of both approaches, here we
introduce a novel framework called {\em DreamSampler}, which seamlessly
integrates these two distinct approaches through the lens of regularized latent
optimization. Similar to score-distillation, DreamSampler is a model-agnostic
approach applicable to any LDM architecture, but it allows both distillation
and reverse sampling with additional guidance for image editing and
reconstruction. Through experiments involving image editing, SVG reconstruction
and etc, we demonstrate the competitive performance of DreamSampler compared to
existing approaches, while providing new applications.
\\ ( https://arxiv.org/abs/2403.11415 ,  5748kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11420 (*cross-listing*)
Date: Mon, 18 Mar 2024 02:20:22 GMT   (886kb,D)

Title: Neural network representation of quantum systems
Authors: Koji Hashimoto, Yuji Hirono, Jun Maeda, Jojiro Totsuka-Yoshinaka
Categories: hep-th cond-mat.dis-nn cs.AI cs.LG quant-ph
Comments: 24 pages, 6 figures
Report-no: KUNS-2996
\\
  It has been proposed that random wide neural networks near Gaussian process
are quantum field theories around Gaussian fixed points. In this paper, we
provide a novel map with which a wide class of quantum mechanical systems can
be cast into the form of a neural network with a statistical summation over
network parameters. Our simple idea is to use the universal approximation
theorem of neural networks to generate arbitrary paths in the Feynman's path
integral. The map can be applied to interacting quantum systems / field
theories, even away from the Gaussian limit. Our findings bring machine
learning closer to the quantum world.
\\ ( https://arxiv.org/abs/2403.11420 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11432 (*cross-listing*)
Date: Mon, 18 Mar 2024 02:59:13 GMT   (8659kb,D)

Title: Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle
  Decision-Making
Authors: Hanxi Wan, Pei Li, Arpan Kusari
Categories: cs.RO cs.AI cs.LG
Comments: Submitted for peer-review
\\
  With the advent of universal function approximators in the domain of
reinforcement learning, the number of practical applications leveraging deep
reinforcement learning (DRL) has exploded. Decision-making in automated driving
tasks has emerged as a chief application among them, taking the sensor data or
the higher-order kinematic variables as the input and providing a discrete
choice or continuous control output. However, the black-box nature of the
models presents an overwhelming limitation that restricts the real-world
deployment of DRL in autonomous vehicles (AVs). Therefore, in this research
work, we focus on the interpretability of an attention-based DRL framework. We
use a continuous proximal policy optimization-based DRL algorithm as the
baseline model and add a multi-head attention framework in an open-source AV
simulation environment. We provide some analytical techniques for discussing
the interpretability of the trained models in terms of explainability and
causality for spatial and temporal correlations. We show that the weights in
the first head encode the positions of the neighboring vehicles while the
second head focuses on the leader vehicle exclusively. Also, the ego vehicle's
action is causally dependent on the vehicles in the target lane spatially and
temporally. Through these findings, we reliably show that these techniques can
help practitioners decipher the results of the DRL algorithms.
\\ ( https://arxiv.org/abs/2403.11432 ,  8659kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11468 (*cross-listing*)
Date: Mon, 18 Mar 2024 04:41:38 GMT   (4332kb,D)

Title: Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V
Authors: Siyu Xu, Yunke Wang, Daochang Liu and Chang Xu
Categories: cs.CV cs.AI
\\
  Recent advancements in generative AI have suggested that by taking visual
prompt, GPT-4V can demonstrate significant proficiency in image recognition
task. Despite its impressive capabilities, the financial cost associated with
GPT-4V's inference presents a substantial barrier for its wide use. To address
this challenge, our work introduces Collage Prompting, a budget-friendly
prompting approach that concatenates multiple images into a single visual
input. With collage prompt, GPT-4V is able to perform image recognition on
several images simultaneously. Based on the observation that the accuracy of
GPT-4V's image recognition varies significantly with the order of images within
the collage prompt, our method further learns to optimize the arrangement of
images for maximum recognition accuracy. A graph predictor is trained to
indicate the accuracy of each collage prompt, then we propose an optimization
method to navigate the search space of possible image arrangements. Experiment
results across various datasets demonstrate the cost-efficiency score of
collage prompt is much larger than standard prompt. Additionally, collage
prompt with learned arrangement achieves clearly better accuracy than collage
prompt with random arrangement in GPT-4V's visual recognition.
\\ ( https://arxiv.org/abs/2403.11468 ,  4332kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11487 (*cross-listing*)
Date: Mon, 18 Mar 2024 05:38:07 GMT   (9664kb,D)

Title: Can LLMs Generate Human-Like Wayfinding Instructions? Towards
  Platform-Agnostic Embodied Instruction Synthesis
Authors: Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha
Categories: cs.RO cs.AI
Comments: 13 Pages
\\
  We present a novel approach to automatically synthesize "wayfinding
instructions" for an embodied robot agent. In contrast to prior approaches that
are heavily reliant on human-annotated datasets designed exclusively for
specific simulation platforms, our algorithm uses in-context learning to
condition an LLM to generate instructions using just a few references. Using an
LLM-based Visual Question Answering strategy, we gather detailed information
about the environment which is used by the LLM for instruction synthesis. We
implement our approach on multiple simulation platforms including Matterport3D,
AI Habitat and ThreeDWorld, thereby demonstrating its platform-agnostic nature.
We subjectively evaluate our approach via a user study and observe that 83.3%
of users find the synthesized instructions accurately capture the details of
the environment and show characteristics similar to those of human-generated
instructions. Further, we conduct zero-shot navigation with multiple approaches
on the REVERIE dataset using the generated instructions, and observe very close
correlation with the baseline on standard success metrics (< 1% change in SR),
quantifying the viability of generated instructions in replacing
human-annotated data. To the best of our knowledge, ours is the first
LLM-driven approach capable of generating "human-like" instructions in a
platform-agnostic manner, without requiring any form of training.
\\ ( https://arxiv.org/abs/2403.11487 ,  9664kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11492 (*cross-listing*)
Date: Mon, 18 Mar 2024 05:53:20 GMT   (1486kb,D)

Title: SmartRefine: An Scenario-Adaptive Refinement Framework for Efficient
  Motion Prediction
Authors: Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li,
  Yu Liu
Categories: cs.CV cs.AI cs.RO
Comments: Camera-ready version for CVPR 2024
\\
  Predicting the future motion of surrounding agents is essential for
autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed
environments. Context information, such as road maps and surrounding agents'
states, provides crucial geometric and semantic information for motion behavior
prediction. To this end, recent works explore two-stage prediction frameworks
where coarse trajectories are first proposed, and then used to select critical
context information for trajectory refinement. However, they either incur a
large amount of computation or bring limited improvement, if not both. In this
paper, we introduce a novel scenario-adaptive refinement strategy, named
SmartRefine, to refine prediction with minimal additional computation.
Specifically, SmartRefine can comprehensively adapt refinement configurations
based on each scenario's properties, and smartly chooses the number of
refinement iterations by introducing a quality score to measure the prediction
quality and remaining refinement potential of each scenario. SmartRefine is
designed as a generic and flexible approach that can be seamlessly integrated
into most state-of-the-art motion prediction models. Experiments on Argoverse
(1 & 2) show that our method consistently improves the prediction accuracy of
multiple state-of-the-art prediction models. Specifically, by adding
SmartRefine to QCNet, we outperform all published ensemble-free works on the
Argoverse 2 leaderboard (single agent track) at submission. Comprehensive
studies are also conducted to ablate design choices and explore the mechanism
behind multi-iteration refinement. Codes are available at
https://github.com/opendilab/SmartRefine/
\\ ( https://arxiv.org/abs/2403.11492 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11496 (*cross-listing*)
Date: Mon, 18 Mar 2024 06:00:38 GMT   (5094kb,D)

Title: MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception
Authors: Thien-Minh Nguyen, Shenghai Yuan, Thien Hoang Nguyen, Pengyu Yin,
  Haozhi Cao, Lihua Xie, Maciej Wozniak, Patric Jensfelt, Marko Thiel, Justin
  Ziegenbein, Noel Blunder
Categories: cs.RO cs.AI
Comments: Accepted by The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2024
\\
  Perception plays a crucial role in various robot applications. However,
existing well-annotated datasets are biased towards autonomous driving
scenarios, while unlabelled SLAM datasets are quickly over-fitted, and often
lack environment and domain variations. To expand the frontier of these fields,
we introduce a comprehensive dataset named MCD (Multi-Campus Dataset),
featuring a wide range of sensing modalities, high-accuracy ground truth, and
diverse challenging environments across three Eurasian university campuses. MCD
comprises both CCS (Classical Cylindrical Spinning) and NRE (Non-Repetitive
Epicyclic) lidars, high-quality IMUs (Inertial Measurement Units), cameras, and
UWB (Ultra-WideBand) sensors. Furthermore, in a pioneering effort, we introduce
semantic annotations of 29 classes over 59k sparse NRE lidar scans across three
domains, thus providing a novel challenge to existing semantic segmentation
research upon this largely unexplored lidar modality. Finally, we propose, for
the first time to the best of our knowledge, continuous-time ground truth based
on optimization-based registration of lidar-inertial data on large survey-grade
prior maps, which are also publicly released, each several times the size of
existing ones. We conduct a rigorous evaluation of numerous state-of-the-art
algorithms on MCD, report their performance, and highlight the challenges
awaiting solutions from the research community.
\\ ( https://arxiv.org/abs/2403.11496 ,  5094kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11504 (*cross-listing*)
Date: Mon, 18 Mar 2024 06:19:37 GMT   (7353kb,D)

Title: MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray
  Self-Supervised Representation Learning
Authors: Azad Singh, Vandan Gorade and Deepak Mishra
Categories: eess.IV cs.AI cs.CV cs.LG
\\
  Self-supervised learning (SSL) is potentially useful in reducing the need for
manual annotation and making deep learning models accessible for medical image
analysis tasks. By leveraging the representations learned from unlabeled data,
self-supervised models perform well on tasks that require little to no
fine-tuning. However, for medical images, like chest X-rays, which are
characterized by complex anatomical structures and diverse clinical conditions,
there arises a need for representation learning techniques that can encode
fine-grained details while preserving the broader contextual information. In
this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration
for Chest X-ray Self-Supervised Representation Learning), an approach to
capture rich representations in the form of embeddings from chest X-ray images.
Central to our approach is a novel multi-level variance and covariance
exploration strategy that empowers the model to detect diagnostically
meaningful patterns while reducing redundancy effectively. By enhancing the
variance and covariance of the learned embeddings, MLVICX promotes the
retention of critical medical insights by adapting both global and local
contextual details. We demonstrate the performance of MLVICX in advancing
self-supervised chest X-ray representation learning through comprehensive
experiments. The performance enhancements we observe across various downstream
tasks highlight the significance of the proposed approach in enhancing the
utility of chest X-ray embeddings for precision medical diagnosis and
comprehensive image analysis. For pertaining, we used the NIH-Chest X-ray
dataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR,
RSNA pneumonia, and SIIM-ACR Pneumothorax datasets. Overall, we observe more
than 3% performance gains over SOTA SSL approaches in various downstream tasks.
\\ ( https://arxiv.org/abs/2403.11504 ,  7353kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11506 (*cross-listing*)
Date: Mon, 18 Mar 2024 06:24:46 GMT   (3663kb,D)

Title: End-To-End Underwater Video Enhancement: Dataset and Model
Authors: Dazhao Du, Enhan Li, Lingyu Si, Fanjiang Xu, Jianwei Niu
Categories: cs.CV cs.AI
\\
  Underwater video enhancement (UVE) aims to improve the visibility and frame
quality of underwater videos, which has significant implications for marine
research and exploration. However, existing methods primarily focus on
developing image enhancement algorithms to enhance each frame independently.
There is a lack of supervised datasets and models specifically tailored for UVE
tasks. To fill this gap, we construct the Synthetic Underwater Video
Enhancement (SUVE) dataset, comprising 840 diverse underwater-style videos
paired with ground-truth reference videos. Based on this dataset, we train a
novel underwater video enhancement model, UVENet, which utilizes inter-frame
relationships to achieve better enhancement performance. Through extensive
experiments on both synthetic and real underwater videos, we demonstrate the
effectiveness of our approach. This study represents the first comprehensive
exploration of UVE to our knowledge. The code is available at
https://anonymous.4open.science/r/UVENet.
\\ ( https://arxiv.org/abs/2403.11506 ,  3663kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11536 (*cross-listing*)
Date: Mon, 18 Mar 2024 07:41:39 GMT   (1695kb,D)

Title: OCR is All you need: Importing Multi-Modality into Image-based Defect
  Detection System
Authors: Chih-Chung Hsu and Chia-Ming Lee and Chun-Hung Sun and Kuang-Ming Wu
Categories: cs.CV cs.AI cs.LG
\\
  Automatic optical inspection (AOI) plays a pivotal role in the manufacturing
process, predominantly leveraging high-resolution imaging instruments for
scanning purposes. It detects anomalies by analyzing image textures or
patterns, making it an essential tool in industrial manufacturing and quality
control. Despite its importance, the deployment of models for AOI often faces
challenges. These include limited sample sizes, which hinder effective feature
learning, variations among source domains, and sensitivities to changes in
lighting and camera positions during imaging. These factors collectively
compromise the accuracy of model predictions. Traditional AOI often fails to
capitalize on the rich mechanism-parameter information from machines or inside
images, including statistical parameters, which typically benefit AOI
classification. To address this, we introduce an external modality-guided data
mining framework, primarily rooted in optical character recognition (OCR), to
extract statistical features from images as a second modality to enhance
performance, termed OANet (Ocr-Aoi-Net). A key aspect of our approach is the
alignment of external modality features, extracted using a single
modality-aware model, with image features encoded by a convolutional neural
network. This synergy enables a more refined fusion of semantic representations
from different modalities. We further introduce feature refinement and a gating
function in our OANet to optimize the combination of these features, enhancing
inference and decision-making capabilities. Experimental outcomes show that our
methodology considerably boosts the recall rate of the defect detection model
and maintains high robustness even in challenging scenarios.
\\ ( https://arxiv.org/abs/2403.11536 ,  1695kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11552 (*cross-listing*)
Date: Mon, 18 Mar 2024 08:03:47 GMT   (17822kb,D)

Title: LLM^3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning
Authors: Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun
  Zhu, Hangxin Liu
Categories: cs.RO cs.AI
Comments: Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP
\\
  Conventional Task and Motion Planning (TAMP) approaches rely on manually
crafted interfaces connecting symbolic task planning with continuous motion
generation. These domain-specific and labor-intensive modules are limited in
addressing emerging tasks in real-world settings. Here, we present LLM^3, a
novel Large Language Model (LLM)-based TAMP framework featuring a
domain-independent interface. Specifically, we leverage the powerful reasoning
and planning capabilities of pre-trained LLMs to propose symbolic action
sequences and select continuous action parameters for motion planning.
Crucially, LLM^3 incorporates motion planning feed- back through prompting,
allowing the LLM to iteratively refine its proposals by reasoning about motion
failure. Consequently, LLM^3 interfaces between task planning and motion
planning, alleviating the intricate design process of handling domain- specific
messages between them. Through a series of simulations in a box-packing domain,
we quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP
problems and the efficiency in selecting action parameters. Ablation studies
un- derscore the significant contribution of motion failure reasoning to the
success of LLM^3. Furthermore, we conduct qualitative experiments on a physical
manipulator, demonstrating the practical applicability of our approach in
real-world settings.
\\ ( https://arxiv.org/abs/2403.11552 ,  17822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11598 (*cross-listing*)
Date: Mon, 18 Mar 2024 09:19:01 GMT   (340kb,D)

Title: Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors
  with 100+ Qubits
Authors: Irfansha Shaik and Jaco van de Pol
Categories: quant-ph cs.AI
Comments: 7 Figures, 4 Tables, 1 Listing
\\
  Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP
gate insertions are needed for scheduling 2-qubit gates only on connected
physical qubits. With the ever-increasing number of qubits in NISQ processors,
scalable layout synthesis is of utmost importance. With large optimality gaps
observed in heuristic approaches, scalable exact methods are needed. While
recent exact and near-optimal approaches scale to moderate circuits, large deep
circuits are still out of scope.
  In this work, we propose a SAT encoding based on parallel plans that apply 1
SWAP and a group of CNOTs at each time step. Using domain-specific information,
we maintain optimality in parallel plans while scaling to large and deep
circuits. From our results, we show the scalability of our approach which
significantly outperforms leading exact and near-optimal approaches (up to
100x). For the first time, we can optimally map several 8, 14, and 16 qubit
circuits onto 54, 80, and 127 qubit platforms with up to 17 SWAPs. While adding
optimal SWAPs, we also report near-optimal depth in our mapped circuits.
\\ ( https://arxiv.org/abs/2403.11598 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11626 (*cross-listing*)
Date: Mon, 18 Mar 2024 09:58:43 GMT   (1743kb,D)

Title: QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation
Authors: Zhizhen Zhou, Yejing Huo, Guoheng Huang, An Zeng, Xuhang Chen, Lian
  Huang, Zinuo Li
Categories: cs.GR cs.AI cs.CV cs.MM cs.SD eess.AS
Comments: Accepted by The Visual Computer Journal
\\
  The study of music-generated dance is a novel and challenging Image
generation task. It aims to input a piece of music and seed motions, then
generate natural dance movements for the subsequent music. Transformer-based
methods face challenges in time series prediction tasks related to human
movements and music due to their struggle in capturing the nonlinear
relationship and temporal aspects. This can lead to issues like joint
deformation, role deviation, floating, and inconsistencies in dance movements
generated in response to the music. In this paper, we propose a
Quaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a
quaternion perspective, which consists of a Spin Position Embedding (SPE)
module and a Quaternion Rotary Attention (QRA) module. First, SPE embeds
position information into self-attention in a rotational manner, leading to
better learning of features of movement sequences and audio sequences, and
improved understanding of the connection between music and dance. Second, QRA
represents and fuses 3D motion features and audio features in the form of a
series of quaternions, enabling the model to better learn the temporal
coordination of music and dance under the complex temporal cycle conditions of
dance generation. Finally, we conducted experiments on the dataset AIST++, and
the results show that our approach achieves better and more robust performance
in generating accurate, high-quality dance movements. Our source code and
dataset can be available from https://github.com/MarasyZZ/QEAN and
https://google.github.io/aistplusplus_dataset respectively.
\\ ( https://arxiv.org/abs/2403.11626 ,  1743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11671 (*cross-listing*)
Date: Mon, 18 Mar 2024 11:19:37 GMT   (3377kb,D)

Title: HDLdebugger: Streamlining HDL debugging with Large Language Models
Authors: Xufeng Yao, Haoyang Li, Tsz Ho Chan, Wenyi Xiao, Mingxuan Yuan, Yu
  Huang, Lei Chen, Bei Yu
Categories: cs.AR cs.AI cs.CE cs.LG cs.SE
Comments: 13 pages,5 figures
\\
  In the domain of chip design, Hardware Description Languages (HDLs) play a
pivotal role. However, due to the complex syntax of HDLs and the limited
availability of online resources, debugging HDL codes remains a difficult and
time-intensive task, even for seasoned engineers. Consequently, there is a
pressing need to develop automated HDL code debugging models, which can
alleviate the burden on hardware engineers. Despite the strong capabilities of
Large Language Models (LLMs) in generating, completing, and debugging software
code, their utilization in the specialized field of HDL debugging has been
limited and, to date, has not yielded satisfactory results. In this paper, we
propose an LLM-assisted HDL debugging framework, namely HDLdebugger, which
consists of HDL debugging data generation via a reverse engineering approach, a
search engine for retrieval-augmented generation, and a retrieval-augmented LLM
fine-tuning approach. Through the integration of these components, HDLdebugger
can automate and streamline HDL debugging for chip design. Our comprehensive
experiments, conducted on an HDL code dataset sourced from Huawei, reveal that
HDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional
effectiveness in HDL code debugging.
\\ ( https://arxiv.org/abs/2403.11671 ,  3377kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11703 (*cross-listing*)
Date: Mon, 18 Mar 2024 12:04:11 GMT   (2221kb,D)

Title: LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images
Authors: Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge,
  Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, Gao Huang
Categories: cs.CV cs.AI
Comments: Preprint
\\
  Visual encoding constitutes the basis of large multimodal models (LMMs) in
understanding the visual world. Conventional LMMs process images in fixed sizes
and limited resolutions, while recent explorations in this direction are
limited in adaptivity, efficiency, and even correctness. In this work, we first
take GPT-4V and LLaVA-1.5 as representative examples and expose systematic
flaws rooted in their visual encoding strategy. To address the challenges, we
present LLaVA-UHD, a large multimodal model that can efficiently perceive
images in any aspect ratio and high resolution. LLaVA-UHD includes three key
components: (1) An image modularization strategy that divides native-resolution
images into smaller variable-sized slices for efficient and extensible
encoding, (2) a compression module that further condenses image tokens from
visual encoders, and (3) a spatial schema to organize slice tokens for LLMs.
Comprehensive experiments show that LLaVA-UHD outperforms established LMMs
trained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our
model built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)
resolution images using only 94% inference computation, and achieves 6.4
accuracy improvement on TextVQA. Moreover, the model can be efficiently trained
in academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of
LLaVA-1.5). We make the data and code publicly available at
https://github.com/thunlp/LLaVA-UHD.
\\ ( https://arxiv.org/abs/2403.11703 ,  2221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11755 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:03:24 GMT   (1166kb,D)

Title: Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
Authors: M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub
  Micorek, Mateusz Kozinski, Hilde Kuhene, Horst Possegger
Categories: cs.CV cs.AI cs.LG
Comments: Project Page (Code and Data):
  https://jmiemirza.github.io/Meta-Prompting/
\\
  Prompt ensembling of Large Language Model (LLM) generated category-specific
prompts has emerged as an effective method to enhance zero-shot recognition
ability of Vision-Language Models (VLMs). To obtain these category-specific
prompts, the present methods rely on hand-crafting the prompts to the LLMs for
generating VLM prompts for the downstream tasks. However, this requires
manually composing these task-specific prompts and still, they might not cover
the diverse set of visual concepts and task-specific styles associated with the
categories of interest. To effectively take humans out of the loop and
completely automate the prompt generation process for zero-shot recognition, we
propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only
minimal information about the target task, in the form of its short natural
language description, and a list of associated class labels, MPVR automatically
produces a diverse set of category-specific prompts resulting in a strong
zero-shot classifier. MPVR generalizes effectively across various popular
zero-shot image recognition benchmarks belonging to widely different domains
when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot
recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on
average over 20 datasets) leveraging GPT and Mixtral LLMs, respectively
\\ ( https://arxiv.org/abs/2403.11755 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11780 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:39:05 GMT   (8007kb,D)

Title: Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural
  Language Prompt
Authors: Yongqi Wang, Ruofan Hu, Rongjie Huang, Zhiqing Hong, Ruiqi Li, Wenrui
  Liu, Fuming You, Tao Jin, Zhou Zhao
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: Accepted by NAACL 2024 (main conference)
\\
  Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio
quality and naturalness, yet they lack the capability to control the style
attributes of the synthesized singing explicitly. We propose Prompt-Singer, the
first SVS method that enables attribute controlling on singer gender, vocal
range and volume with natural language. We adopt a model architecture based on
a decoder-only transformer with a multi-scale hierarchy, and design a
range-melody decoupled pitch representation that enables text-conditioned vocal
range control while keeping melodic accuracy. Furthermore, we explore various
experiment settings, including different types of text representations, text
encoder fine-tuning, and introducing speech data to alleviate data scarcity,
aiming to facilitate further research. Experiments show that our model achieves
favorable controlling ability and audio quality. Audio samples are available at
http://prompt-singer.github.io .
\\ ( https://arxiv.org/abs/2403.11780 ,  8007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11790 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:47:18 GMT   (43111kb,D)

Title: Deep Medial Voxels: Learned Medial Axis Approximations for Anatomical
  Shape Modeling
Authors: Antonio Pepe, Richard Schussnig, Jianning Li, Christina Gsaxner,
  Dieter Schmalstieg, Jan Egger
Categories: cs.CV cs.AI
Comments: 10 pages
\\
  Shape reconstruction from imaging volumes is a recurring need in medical
image analysis. Common workflows start with a segmentation step, followed by
careful post-processing and,finally, ad hoc meshing algorithms. As this
sequence can be timeconsuming, neural networks are trained to reconstruct
shapes through template deformation. These networks deliver state-ofthe-art
results without manual intervention, but, so far, they have primarily been
evaluated on anatomical shapes with little topological variety between
individuals. In contrast, other works favor learning implicit shape models,
which have multiple benefits for meshing and visualization. Our work follows
this direction by introducing deep medial voxels, a semi-implicit
representation that faithfully approximates the topological skeleton from
imaging volumes and eventually leads to shape reconstruction via convolution
surfaces. Our reconstruction technique shows potential for both visualization
and computer simulations.
\\ ( https://arxiv.org/abs/2403.11790 ,  43111kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11821 (*cross-listing*)
Date: Mon, 18 Mar 2024 14:24:20 GMT   (12212kb,D)

Title: Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality
  Metrics
Authors: Sebastian Hartwig, Dominik Engel, Leon Sick, Hannah Kniesel, Tristan
  Payer, Poonam, Timo Ropinski
Categories: cs.CV cs.AI cs.GR
Comments: preprint, 18 pages, 2 figures, 2 tables
\\
  Recent advances in text-to-image synthesis have been enabled by exploiting a
combination of language and vision through foundation models. These models are
pre-trained on tremendous amounts of text-image pairs sourced from the World
Wide Web or other large-scale databases. As the demand for high-quality image
generation shifts towards ensuring content alignment between text and image,
novel evaluation metrics have been developed with the aim of mimicking human
judgments. Thus, researchers have started to collect datasets with increasingly
complex annotations to study the compositionality of vision-language models and
their incorporation as a quality measure of compositional alignment between
text and image contents. In this work, we provide a comprehensive overview of
existing text-to-image evaluation metrics and propose a new taxonomy for
categorizing these metrics. We also review frequently adopted text-image
benchmark datasets before discussing techniques to optimize text-to-image
synthesis models towards quality and human preferences. Ultimately, we derive
guidelines for improving text-to-image evaluation and discuss the open
challenges and current limitations.
\\ ( https://arxiv.org/abs/2403.11821 ,  12212kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11830 (*cross-listing*)
Date: Mon, 18 Mar 2024 14:40:33 GMT   (306kb,D)

Title: Problem space structural adversarial attacks for Network Intrusion
  Detection Systems based on Graph Neural Networks
Authors: Andrea Venturi, Dario Stabili, Mirco Marchetti
Categories: cs.CR cs.AI
Comments: preprint submitted to IEEE TIFS, under review
\\
  Machine Learning (ML) algorithms have become increasingly popular for
supporting Network Intrusion Detection Systems (NIDS). Nevertheless, extensive
research has shown their vulnerability to adversarial attacks, which involve
subtle perturbations to the inputs of the models aimed at compromising their
performance. Recent proposals have effectively leveraged Graph Neural Networks
(GNN) to produce predictions based also on the structural patterns exhibited by
intrusions to enhance the detection robustness. However, the adoption of
GNN-based NIDS introduces new types of risks. In this paper, we propose the
first formalization of adversarial attacks specifically tailored for GNN in
network intrusion detection. Moreover, we outline and model the problem space
constraints that attackers need to consider to carry out feasible structural
attacks in real-world scenarios. As a final contribution, we conduct an
extensive experimental campaign in which we launch the proposed attacks against
state-of-the-art GNN-based NIDS. Our findings demonstrate the increased
robustness of the models against classical feature-based adversarial attacks,
while highlighting their susceptibility to structure-based attacks.
\\ ( https://arxiv.org/abs/2403.11830 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11841 (*cross-listing*)
Date: Mon, 18 Mar 2024 14:51:19 GMT   (1636kb,D)

Title: Pessimistic Causal Reinforcement Learning with Mediators for Confounded
  Offline Data
Authors: Danyang Wang, Chengchun Shi, Shikai Luo, Will Wei Sun
Categories: stat.ML cs.AI cs.LG
\\
  In real-world scenarios, datasets collected from randomized experiments are
often constrained by size, due to limitations in time and budget. As a result,
leveraging large observational datasets becomes a more attractive option for
achieving high-quality policy learning. However, most existing offline
reinforcement learning (RL) methods depend on two key
assumptions--unconfoundedness and positivity--which frequently do not hold in
observational data contexts. Recognizing these challenges, we propose a novel
policy learning algorithm, PESsimistic CAusal Learning (PESCAL). We utilize the
mediator variable based on front-door criterion to remove the confounding bias;
additionally, we adopt the pessimistic principle to address the distributional
shift between the action distributions induced by candidate policies, and the
behavior policy that generates the observational data. Our key observation is
that, by incorporating auxiliary variables that mediate the effect of actions
on system dynamics, it is sufficient to learn a lower bound of the mediator
distribution function, instead of the Q-function, to partially mitigate the
issue of distributional shift. This insight significantly simplifies our
algorithm, by circumventing the challenging task of sequential uncertainty
quantification for the estimated Q-function. Moreover, we provide theoretical
guarantees for the algorithms we propose, and demonstrate their efficacy
through simulations, as well as real-world experiments utilizing offline
datasets from a leading ride-hailing platform.
\\ ( https://arxiv.org/abs/2403.11841 ,  1636kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11852 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:02:46 GMT   (559kb,D)

Title: Reinforcement Learning with Latent State Inference for Autonomous
  On-ramp Merging under Observation Delay
Authors: Amin Tabrizian, Zhitong Huang, Peng Wei
Categories: cs.RO cs.AI
\\
  This paper presents a novel approach to address the challenging problem of
autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly
integrate into a flow of vehicles on a multi-lane highway. We introduce the
Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller
(L3IS) agent, designed to perform the on-ramp merging task safely without
comprehensive knowledge about surrounding vehicles' intents or driving styles.
We also present an augmentation of this agent called AL3IS that accounts for
observation delays, allowing the agent to make more robust decisions in
real-world environments with vehicle-to-vehicle (V2V) communication delays. By
modeling the unobservable aspects of the environment through latent states,
such as other drivers' intents, our approach enhances the agent's ability to
adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure
safe interactions with other vehicles. We demonstrate the effectiveness of our
method through extensive simulations generated from real traffic data and
compare its performance with existing approaches. L3IS shows a 99.90\% success
rate in a challenging on-ramp merging case generated from the real US Highway
101 data. We further perform a sensitivity analysis on AL3IS to evaluate its
robustness against varying observation delays, which demonstrates an acceptable
performance of 93.84\% success rate in 1-second V2V communication delay.
\\ ( https://arxiv.org/abs/2403.11852 ,  559kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11865 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:18:55 GMT   (37318kb,D)

Title: Exploring Multi-modal Neural Scene Representations With Applications on
  Thermal Imaging
Authors: Mert \"Ozer, Maximilian Weiherer, Martin Hundhausen, Bernhard Egger
Categories: cs.CV cs.AI cs.GR
Comments: 24 pages, 14 figures
\\
  Neural Radiance Fields (NeRFs) quickly evolved as the new de-facto standard
for the task of novel view synthesis when trained on a set of RGB images. In
this paper, we conduct a comprehensive evaluation of neural scene
representations, such as NeRFs, in the context of multi-modal learning.
Specifically, we present four different strategies of how to incorporate a
second modality, other than RGB, into NeRFs: (1) training from scratch
independently on both modalities; (2) pre-training on RGB and fine-tuning on
the second modality; (3) adding a second branch; and (4) adding a separate
component to predict (color) values of the additional modality. We chose
thermal imaging as second modality since it strongly differs from RGB in terms
of radiosity, making it challenging to integrate into neural scene
representations. For the evaluation of the proposed strategies, we captured a
new publicly available multi-view dataset, ThermalMix, consisting of six common
objects and about 360 RGB and thermal images in total. We employ cross-modality
calibration prior to data capturing, leading to high-quality alignments between
RGB and thermal images. Our findings reveal that adding a second branch to NeRF
performs best for novel view synthesis on thermal images while also yielding
compelling results on RGB. Finally, we also show that our analysis generalizes
to other modalities, including near-infrared images and depth maps. Project
page: https://mert-o.github.io/ThermalNeRF/.
\\ ( https://arxiv.org/abs/2403.11865 ,  37318kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11879 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:32:02 GMT   (104kb,D)

Title: Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton
Authors: Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth Andr\'e
Categories: cs.SD cs.AI eess.AS
\\
  In this study, we propose a methodology for the Emotional Mimicry Intensity
(EMI) Estimation task within the context of the 6th Workshop and Competition on
Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0
framework, pre-trained on a comprehensive podcast dataset, to extract a broad
range of audio features encompassing both linguistic and paralinguistic
elements. We enhance feature representation through a fusion technique that
integrates individual features with a global mean vector, introducing global
contextual insights into our analysis. Additionally, we incorporate a
pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model.
Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient
temporal analysis of audio data. Utilizing only the provided audio data, our
approach demonstrates significant improvements over the established baseline.
\\ ( https://arxiv.org/abs/2403.11879 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11882 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:33:06 GMT   (7286kb,D)

Title: ReGenNet: Towards Human Action-Reaction Synthesis
Authors: Liang Xu, Yizhou Zhou, Yichao Yan, Xin Jin, Wenhan Zhu, Fengyun Rao,
  Xiaokang Yang, Wenjun Zeng
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024, Project Page:
  https://liangxuy.github.io/ReGenNet/
\\
  Humans constantly interact with their surrounding environments. Current
human-centric generative models mainly focus on synthesizing humans plausibly
interacting with static scenes and objects, while the dynamic human
action-reaction synthesis for ubiquitous causal human-human interactions is
less explored. Human-human interactions can be regarded as asymmetric with
actors and reactors in atomic interaction periods. In this paper, we
comprehensively analyze the asymmetric, dynamic, synchronous, and detailed
nature of human-human interactions and propose the first multi-setting human
action-reaction synthesis benchmark to generate human reactions conditioned on
given human actions. To begin with, we propose to annotate the actor-reactor
order of the interaction sequences for the NTU120, InterHuman, and Chi3D
datasets. Based on them, a diffusion-based generative model with a Transformer
decoder architecture called ReGenNet together with an explicit distance-based
interaction loss is proposed to predict human reactions in an online manner,
where the future states of actors are unavailable to reactors. Quantitative and
qualitative results show that our method can generate instant and plausible
human reactions compared to the baselines, and can generalize to unseen actor
motions and viewpoint changes.
\\ ( https://arxiv.org/abs/2403.11882 ,  7286kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11887 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:40:36 GMT   (2689kb,D)

Title: SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer
  Attention Modules
Authors: Xiangyu Chen, Jing Liu, Ye Wang, Pu (Perry) Wang, Matthew Brand,
  Guanghui Wang, Toshiaki Koike-Akino
Categories: cs.CV cs.AI cs.LG
Comments: 33 pages, 29 figures, Submitted to ECCV 2024
\\
  Low-rank adaptation (LoRA) and its variants are widely employed in
fine-tuning large models, including large language models for natural language
processing and diffusion models for computer vision. This paper proposes a
generalized framework called SuperLoRA that unifies and extends different LoRA
variants, which can be realized under different hyper-parameter settings.
Introducing grouping, folding, shuffling, projecting, and tensor factoring,
SuperLoRA offers high flexibility compared with other LoRA variants and
demonstrates superior performance for transfer learning tasks especially in the
extremely few-parameter regimes.
\\ ( https://arxiv.org/abs/2403.11887 ,  2689kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11942 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:36:54 GMT   (552kb,D)

Title: Exploring Facial Expression Recognition through Semi-Supervised
  Pretraining and Temporal Modeling
Authors: Jun Yu, Zhihong Wei, Zhongpeng Cai
Categories: cs.CV cs.AI
\\
  Facial Expression Recognition (FER) plays a crucial role in computer vision
and finds extensive applications across various fields. This paper aims to
present our approach for the upcoming 6th Affective Behavior Analysis
in-the-Wild (ABAW) competition, scheduled to be held at CVPR2024.. In the
facial expression recognition task, The limited size of the FER dataset poses a
challenge to the expression recognition model's generalization ability,
resulting in subpar recognition performance. To address this problem, we employ
a semi-supervised learning technique to generate expression category
pseudo-labels for unlabeled face data. At the same time, we uniformly sampled
the labeled facial expression samples and implemented a debiased feedback
learning strategy to address the problem of category imbalance in the dataset
and the possible data bias in semi-supervised learning. Moreover, , to further
compensate for the limitation and bias of features obtained only from static
images, we introduced a Temporal Encoder to learn and capture temporal
relationships between neighboring expression image features. In the 6th ABAW
competition, our method achieved outstanding results on the official validation
set, a result that fully confirms the effectiveness and competitiveness of our
proposed method.
\\ ( https://arxiv.org/abs/2403.11942 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11959 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:56:47 GMT   (661kb,D)

Title: IVAC-P2L: Enhancing Video Action Counting through Irregular Repetition
  Priors
Authors: Hang Wang, Zhi-Qi Cheng, Youtian Du, and Lei Zhang
Categories: cs.CV cs.AI cs.MM
Comments: Under continuous updates. Modified for arXiv
\\
  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and
everyday activities by quantifying repetitive actions in videos. However,
traditional VAC methods have overlooked the complexity of action repetitions,
such as interruptions and the variability in cycle duration. Our research
addresses the shortfall by introducing a novel approach to VAC, called
Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular
repetition patterns in videos, which we define through two primary aspects:
Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle
Consistency ensures homogeneity in the spatial-temporal representations of
cycle segments, signifying action uniformity within cycles. Cycle-interval
inconsistency highlights the importance of distinguishing between cycle
segments and intervals based on their inherent content differences. To
encapsulate these principles, we propose a new methodology that includes
consistency and inconsistency modules, supported by a unique pull-push loss
(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence
among cycle segment features and a push loss to clearly distinguish features of
cycle segments from interval segments. Empirical evaluations conducted on the
RepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in
VAC task performance. Furthermore, the model demonstrates exceptional
adaptability and generalization across various video contents, outperforming
existing models on two additional datasets, UCFRep and Countix, without the
need for dataset-specific optimization. These results confirm the efficacy of
our approach in addressing irregular repetitions in videos and pave the way for
further advancements in video analysis and understanding.
\\ ( https://arxiv.org/abs/2403.11959 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11961 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:58:23 GMT   (25193kb,D)

Title: Enhanced Event-Based Video Reconstruction with Motion Compensation
Authors: Siying Liu, Pier Luigi Dragotti
Categories: cs.CV cs.AI cs.LG
Comments: 22 pages, 8 figures (supplementary material included)
\\
  Deep neural networks for event-based video reconstruction often suffer from a
lack of interpretability and have high memory demands. A lightweight network
called CISTA-LSTC has recently been introduced showing that high-quality
reconstruction can be achieved through the systematic design of its
architecture. However, its modelling assumption that input signals and output
reconstructed frame share the same sparse representation neglects the
displacement caused by motion. To address this, we propose warping the input
intensity frames and sparse codes to enhance reconstruction quality. A
CISTA-Flow network is constructed by integrating a flow network with CISTA-LSTC
for motion compensation. The system relies solely on events, in which predicted
flow aids in reconstruction and then reconstructed frames are used to
facilitate flow estimation. We also introduce an iterative training framework
for this combined system. Results demonstrate that our approach achieves
state-of-the-art reconstruction accuracy and simultaneously provides reliable
dense flow estimation. Furthermore, our model exhibits flexibility in that it
can integrate different flow networks, suggesting its potential for further
performance enhancement.
\\ ( https://arxiv.org/abs/2403.11961 ,  25193kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12000 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:35:02 GMT   (7755kb,D)

Title: Notochord: a Flexible Probabilistic Model for Real-Time MIDI Performance
Authors: Victor Shepardson, Jack Armitage and Thor Magnusson
Categories: cs.SD cs.AI eess.AS
Comments: 12 pages, 6 figures. Published in AI Music Creativity 2022
Journal-ref: Shepardson, V., Armitage, J., & Magnusson, T. (2022, September
  17). Proceedings of the 3rd Conference on AI Music Creativity
DOI: 10.5281/zenodo.7088404
\\
  Deep learning-based probabilistic models of musical data are producing
increasingly realistic results and promise to enter creative workflows of many
kinds. Yet they have been little-studied in a performance setting, where the
results of user actions typically ought to feel instantaneous. To enable such
study, we designed Notochord, a deep probabilistic model for sequences of
structured events, and trained an instance of it on the Lakh MIDI dataset. Our
probabilistic formulation allows interpretable interventions at a sub-event
level, which enables one model to act as a backbone for diverse interactive
musical functions including steerable generation, harmonization, machine
improvisation, and likelihood-based interfaces. Notochord can generate
polyphonic and multi-track MIDI, and respond to inputs with latency below ten
milliseconds. Training code, model checkpoints and interactive examples are
provided as open source software.
\\ ( https://arxiv.org/abs/2403.12000 ,  7755kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12002 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:38:53 GMT   (44772kb,D)

Title: DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot
  Video Editing
Authors: Hyeonho Jeong, Jinho Chang, Geon Yeong Park, Jong Chul Ye
Categories: cs.CV cs.AI
Comments: Project page: https://hyeonho99.github.io/dreammotion/
\\
  Text-driven diffusion-based video editing presents a unique challenge not
encountered in image editing literature: establishing real-world motion. Unlike
existing video editing approaches, here we focus on score distillation sampling
to circumvent the standard reverse diffusion process and initiate optimization
from videos that already exhibit natural motion. Our analysis reveals that
while video score distillation can effectively introduce new content indicated
by target text, it can also cause significant structure and motion deviation.
To counteract this, we propose to match space-time self-similarities of the
original video and the edited video during the score distillation. Thanks to
the use of score distillation, our approach is model-agnostic, which can be
applied for both cascaded and non-cascaded video diffusion frameworks. Through
extensive comparisons with leading methods, our approach demonstrates its
superiority in altering appearances while accurately preserving the original
structure and motion.
\\ ( https://arxiv.org/abs/2403.12002 ,  44772kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12009 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:47:39 GMT   (1416kb)

Title: Leveraging Spatial and Semantic Feature Extraction for Skin Cancer
  Diagnosis with Capsule Networks and Graph Neural Networks
Authors: K. P. Santoso, R. V. H. Ginardi, R. A. Sastrowardoyo, F. A. Madany
Categories: cs.CV cs.AI
\\
  In the realm of skin lesion image classification, the intricate spatial and
semantic features pose significant challenges for conventional Convolutional
Neural Network (CNN)-based methodologies. These challenges are compounded by
the imbalanced nature of skin lesion datasets, which hampers the ability of
models to learn minority class features effectively. Despite augmentation
strategies, such as those using Generative Adversarial Networks (GANs),
previous attempts have not fully addressed these complexities. This study
introduces an innovative approach by integrating Graph Neural Networks (GNNs)
with Capsule Networks to enhance classification performance. GNNs, known for
their proficiency in handling graph-structured data, offer an advanced
mechanism for capturing complex patterns and relationships beyond the
capabilities of traditional CNNs. Capsule Networks further contribute by
providing superior recognition of spatial hierarchies within images. Our
research focuses on evaluating and enhancing the Tiny Pyramid Vision GNN (Tiny
Pyramid ViG) architecture by incorporating it with a Capsule Network. This
hybrid model was applied to the MNIST:HAM10000 dataset, a comprehensive skin
lesion dataset designed for benchmarking classification models. After 75 epochs
of training, our model achieved a significant accuracy improvement, reaching
89.23% and 95.52%, surpassing established benchmarks such as GoogLeNet
(83.94%), InceptionV3 (86.82%), MobileNet V3 (89.87%), EfficientNet-7B
(92.07%), ResNet18 (92.22%), ResNet34 (91.90%), ViT-Base (73.70%), and IRv2-SA
(93.47%) on the same dataset. This outcome underscores the potential of our
approach in overcoming the inherent challenges of skin lesion classification,
contributing to the advancement of image-based diagnosis in dermatology.
\\ ( https://arxiv.org/abs/2403.12009 ,  1416kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12010 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:48:15 GMT   (43671kb,D)

Title: VideoMV: Consistent Multi-View Generation Based on Large Video
  Generative Model
Authors: Qi Zuo, Xiaodong Gu, Lingteng Qiu, Yuan Dong, Zhengyi Zhao, Weihao
  Yuan, Rui Peng, Siyu Zhu, Zilong Dong, Liefeng Bo, and Qixing Huang
Categories: cs.CV cs.AI cs.GR
Comments: Project page: aigc3d.github.io/VideoMV/
\\
  Generating multi-view images based on text or single-image prompts is a
critical capability for the creation of 3D content. Two fundamental questions
on this topic are what data we use for training and how to ensure multi-view
consistency. This paper introduces a novel framework that makes fundamental
contributions to both questions. Unlike leveraging images from 2D diffusion
models for training, we propose a dense consistent multi-view generation model
that is fine-tuned from off-the-shelf video generative models. Images from
video generative models are more suitable for multi-view generation because the
underlying network architecture that generates them employs a temporal module
to enforce frame consistency. Moreover, the video data sets used to train these
models are abundant and diverse, leading to a reduced train-finetuning domain
gap. To enhance multi-view consistency, we introduce a 3D-Aware Denoising
Sampling, which first employs a feed-forward reconstruction module to get an
explicit global 3D model, and then adopts a sampling strategy that effectively
involves images rendered from the global 3D model into the denoising sampling
loop to improve the multi-view consistency of the final images. As a
by-product, this module also provides a fast way to create 3D assets
represented by 3D Gaussians within a few seconds. Our approach can generate 24
dense views and converges much faster in training than state-of-the-art
approaches (4 GPU hours versus many thousand GPU hours) with comparable visual
quality and consistency. By further fine-tuning, our approach outperforms
existing state-of-the-art methods in both quantitative metrics and visual
effects. Our project page is aigc3d.github.io/VideoMV.
\\ ( https://arxiv.org/abs/2403.12010 ,  43671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12026 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:57:02 GMT   (4628kb,D)

Title: FlexCap: Generating Rich, Localized, and Flexible Captions in Images
Authors: Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman,
  Yusuf Aytar
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  We introduce a versatile $\textit{flexible-captioning}$ vision-language model
(VLM) capable of generating region-specific descriptions of varying lengths.
The model, FlexCap, is trained to produce length-conditioned captions for input
bounding boxes, and this allows control over the information density of its
output, with descriptions ranging from concise object labels to detailed
captions. To achieve this we create large-scale training datasets of image
region descriptions of varying length, starting from captioned images. This
flexible-captioning capability has several valuable applications.
  First, FlexCap demonstrates superior performance in dense captioning tasks on
the Visual Genome dataset. Second, a visual question answering (VQA) system can
be built by employing FlexCap to generate localized descriptions as inputs to a
large language model. The resulting system achieves state-of-the-art zero-shot
performance on a number of VQA datasets. We also demonstrate a
$\textit{localize-then-describe}$ approach with FlexCap can be better at
open-ended object detection than a $\textit{describe-then-localize}$ approach
with other VLMs. We highlight a novel characteristic of FlexCap, which is its
ability to extract diverse visual information through prefix conditioning.
Finally, we qualitatively demonstrate FlexCap's broad applicability in tasks
such as image labeling, object attribute recognition, and visual dialog.
Project webpage: https://flex-cap.github.io .
\\ ( https://arxiv.org/abs/2403.12026 ,  4628kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12028 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:57:30 GMT   (7235kb,D)

Title: Ultraman: Single Image 3D Human Reconstruction with Ultra Speed and
  Detail
Authors: Mingjin Chen, Junhao Chen, Xiaojun Ye, Huan-ang Gao, Xiaoxue Chen,
  Zhaoxin Fan, Hao Zhao
Categories: cs.CV cs.AI eess.IV
Comments: Project Page: https://air-discover.github.io/Ultraman/
\\
  3D human body reconstruction has been a challenge in the field of computer
vision. Previous methods are often time-consuming and difficult to capture the
detailed appearance of the human body. In this paper, we propose a new method
called \emph{Ultraman} for fast reconstruction of textured 3D human models from
a single image. Compared to existing techniques, \emph{Ultraman} greatly
improves the reconstruction speed and accuracy while preserving high-quality
texture details. We present a set of new frameworks for human reconstruction
consisting of three parts, geometric reconstruction, texture generation and
texture mapping. Firstly, a mesh reconstruction framework is used, which
accurately extracts 3D human shapes from a single image. At the same time, we
propose a method to generate a multi-view consistent image of the human body
based on a single image. This is finally combined with a novel texture mapping
method to optimize texture details and ensure color consistency during
reconstruction. Through extensive experiments and evaluations, we demonstrate
the superior performance of \emph{Ultraman} on various standard datasets. In
addition, \emph{Ultraman} outperforms state-of-the-art methods in terms of
human rendering quality and speed. Upon acceptance of the article, we will make
the code and data publicly available.
\\ ( https://arxiv.org/abs/2403.12028 ,  7235kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12029 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:58:02 GMT   (36026kb,D)

Title: Align and Distill: Unifying and Improving Domain Adaptive Object
  Detection
Authors: Justin Kay, Timm Haucke, Suzanne Stathatos, Siqi Deng, Erik Young,
  Pietro Perona, Sara Beery, Grant Van Horn
Categories: cs.CV cs.AI cs.LG
Comments: 30 pages, 10 figures
\\
  Object detectors often perform poorly on data that differs from their
training set. Domain adaptive object detection (DAOD) methods have recently
demonstrated strong results on addressing this challenge. Unfortunately, we
identify systemic benchmarking pitfalls that call past results into question
and hamper further progress: (a) Overestimation of performance due to
underpowered baselines, (b) Inconsistent implementation practices preventing
transparent comparisons of methods, and (c) Lack of generality due to outdated
backbones and lack of diversity in benchmarks. We address these problems by
introducing: (1) A unified benchmarking and implementation framework, Align and
Distill (ALDI), enabling comparison of DAOD methods and supporting future
development, (2) A fair and modern training and evaluation protocol for DAOD
that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset,
CFC-DAOD, enabling evaluation on diverse real-world data, and (4) A new method,
ALDI++, that achieves state-of-the-art results by a large margin. ALDI++
outperforms the previous state-of-the-art by +3.5 AP50 on Cityscapes to Foggy
Cityscapes, +5.7 AP50 on Sim10k to Cityscapes (where ours is the only method to
outperform a fair baseline), and +2.0 AP50 on CFC Kenai to Channel. Our
framework, dataset, and state-of-the-art method offer a critical reset for DAOD
and provide a strong foundation for future research. Code and data are
available: https://github.com/justinkay/aldi and
https://github.com/visipedia/caltech-fish-counting.
\\ ( https://arxiv.org/abs/2403.12029 ,  36026kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10576 (*cross-listing*)
Date: Fri, 15 Mar 2024 05:35:02 GMT   (1606kb,D)

Title: Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for
  Pretraining on the Cybersecurity Domain
Authors: Eugene Jang, Jian Cui, Dayeon Yim, Youngjin Jin, Jin-Woo Chung,
  Seungwon Shin, Yongjae Lee
Categories: cs.CR cs.CL cs.LG
Comments: To appear in NAACL Findings 2024
\\
  Cybersecurity information is often technically complex and relayed through
unstructured text, making automation of cyber threat intelligence highly
challenging. For such text domains that involve high levels of expertise,
pretraining on in-domain corpora has been a popular method for language models
to obtain domain expertise. However, cybersecurity texts often contain
non-linguistic elements (such as URLs and hash values) that could be unsuitable
with the established pretraining methodologies. Previous work in other domains
have removed or filtered such text as noise, but the effectiveness of these
methods have not been investigated, especially in the cybersecurity domain. We
propose different pretraining methodologies and evaluate their effectiveness
through downstream tasks and probing tasks. Our proposed strategy (selective
MLM and jointly training NLE token classification) outperforms the commonly
taken approach of replacing non-linguistic elements (NLEs). We use our
domain-customized methodology to train CyBERTuned, a cybersecurity domain
language model that outperforms other cybersecurity PLMs on most tasks.
\\ ( https://arxiv.org/abs/2403.10576 ,  1606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10937 (*cross-listing*)
Date: Sat, 16 Mar 2024 14:34:31 GMT   (1481kb,D)

Title: Initial Decoding with Minimally Augmented Language Model for Improved
  Lattice Rescoring in Low Resource ASR
Authors: Savitha Murthy, Dinkar Sitaram
Categories: eess.AS cs.CL cs.LG
Comments: 14 pages, 7 figures, Accepted in Sadhana Journal
\\
  This paper addresses the problem of improving speech recognition accuracy
with lattice rescoring in low-resource languages where the baseline language
model is insufficient for generating inclusive lattices. We minimally augment
the baseline language model with word unigram counts that are present in a
larger text corpus of the target language but absent in the baseline. The
lattices generated after decoding with such an augmented baseline language
model are more comprehensive. We obtain 21.8% (Telugu) and 41.8% (Kannada)
relative word error reduction with our proposed method. This reduction in word
error rate is comparable to 21.5% (Telugu) and 45.9% (Kannada) relative word
error reduction obtained by decoding with full Wikipedia text augmented
language mode while our approach consumes only 1/8th the memory. We demonstrate
that our method is comparable with various text selection-based language model
augmentation and also consistent for data sets of different sizes. Our approach
is applicable for training speech recognition systems under low resource
conditions where speech data and compute resources are insufficient, while
there is a large text corpus that is available in the target language. Our
research involves addressing the issue of out-of-vocabulary words of the
baseline in general and does not focus on resolving the absence of named
entities. Our proposed method is simple and yet computationally less expensive.
\\ ( https://arxiv.org/abs/2403.10937 ,  1481kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10943 (*cross-listing*)
Date: Sat, 16 Mar 2024 15:14:15 GMT   (3509kb,D)

Title: MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent
  Recognition and Out-of-scope Detection in Conversations
Authors: Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su,
  jinyue Zhao, Wenrui Li, Yanting Chen
Categories: cs.MM cs.CL
Comments: Published in ICLR 2024; The abstract is slightly modified due to the
  length limitation
\\
  Multimodal intent recognition poses significant challenges, requiring the
incorporation of non-verbal modalities from real-world contexts to enhance the
comprehension of human intentions. Existing benchmark datasets are limited in
scale and suffer from difficulties in handling out-of-scope samples that arise
in multi-turn conversational interactions. We introduce MIntRec2.0, a
large-scale benchmark dataset for multimodal intent recognition in multi-party
conversations. It contains 1,245 dialogues with 15,040 samples, each annotated
within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope
samples, it also includes 5,736 out-of-scope samples appearing in multi-turn
contexts, which naturally occur in real-world scenarios. Furthermore, we
provide comprehensive information on the speakers in each utterance, enriching
its utility for multi-party conversational research. We establish a general
framework supporting the organization of single-turn and multi-turn dialogue
data, modality feature extraction, multimodal fusion, as well as in-scope
classification and out-of-scope detection. Evaluation benchmarks are built
using classic multimodal fusion methods, ChatGPT, and human evaluators. While
existing methods incorporating nonverbal information yield improvements,
effectively leveraging context information and detecting out-of-scope samples
remains a substantial challenge. Notably, large language models exhibit a
significant performance gap compared to humans, highlighting the limitations of
machine learning methods in the cognitive intent understanding task. We believe
that MIntRec2.0 will serve as a valuable resource, providing a pioneering
foundation for research in human-machine conversational interactions, and
significantly facilitating related applications. The full dataset and codes are
available at https://github.com/thuiar/MIntRec2.0.
\\ ( https://arxiv.org/abs/2403.10943 ,  3509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11083 (*cross-listing*)
Date: Sun, 17 Mar 2024 04:30:57 GMT   (637kb,D)

Title: Customizing Visual-Language Foundation Models for Multi-modal Anomaly
  Detection and Reasoning
Authors: Xiaohao Xu, Yunkang Cao, Yongqi Chen, Weiming Shen, Xiaonan Huang
Categories: cs.CV cs.CL
\\
  Anomaly detection is vital in various industrial scenarios, including the
identification of unusual patterns in production lines and the detection of
manufacturing defects for quality control. Existing techniques tend to be
specialized in individual scenarios and lack generalization capacities. In this
study, we aim to develop a generic anomaly detection model applicable across
multiple scenarios. To achieve this, we customize generic visual-language
foundation models that possess extensive knowledge and robust reasoning
abilities into anomaly detectors and reasoners. Specifically, we introduce a
multi-modal prompting strategy that incorporates domain knowledge from experts
as conditions to guide the models. Our approach considers multi-modal prompt
types, including task descriptions, class context, normality rules, and
reference images. In addition, we unify the input representation of
multi-modality into a 2D image format, enabling multi-modal anomaly detection
and reasoning. Our preliminary studies demonstrate that combining visual and
language prompts as conditions for customizing the models enhances anomaly
detection performance. The customized models showcase the ability to detect
anomalies across different data modalities such as images and point clouds.
Qualitative case studies further highlight the anomaly detection and reasoning
capabilities, particularly for multi-object scenes and temporal data. Our code
is available at https://github.com/Xiaohao-Xu/Customizable-VLM.
\\ ( https://arxiv.org/abs/2403.11083 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11085 (*cross-listing*)
Date: Sun, 17 Mar 2024 04:36:18 GMT   (33696kb,D)

Title: m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
Authors: Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna
Categories: cs.CV cs.CL
\\
  Real-world multi-modal problems are rarely solved by a single machine
learning model, and often require multi-step computational plans that involve
stitching several models. Tool-augmented LLMs hold tremendous promise for
automating the generation of such computational plans. However, the lack of
standardized benchmarks for evaluating LLMs as planners for multi-step
multi-modal tasks has prevented a systematic study of planner design decisions.
Should LLMs generate a full plan in a single shot or step-by-step? Should they
invoke tools directly with Python code or through structured data formats like
JSON? Does feedback improve planning? To answer these questions and more, we
introduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks
involving 33 tools that include multi-modal models, (free) public APIs, and
image processing modules. For each of these task queries, we provide
automatically generated plans using this realistic toolset. We further provide
a high-quality subset of 1,565 task plans that are human-verified and correctly
executable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies
(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3
types of feedback (parsing/verification/execution). Finally, we summarize
takeaways from our extensive experiments. Our dataset and code are available on
HuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github
(https://github.com/RAIVNLab/mnms).
\\ ( https://arxiv.org/abs/2403.11085 ,  33696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11176 (*cross-listing*)
Date: Sun, 17 Mar 2024 11:32:18 GMT   (27135kb,D)

Title: Quality-Aware Image-Text Alignment for Real-World Image Quality
  Assessment
Authors: Lorenzo Agnolucci, Leonardo Galteri, Marco Bertini
Categories: cs.CV cs.CL
\\
  No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods
to measure image quality in alignment with human perception when a high-quality
reference image is unavailable. The reliance on annotated Mean Opinion Scores
(MOS) in the majority of state-of-the-art NR-IQA approaches limits their
scalability and broader applicability to real-world scenarios. To overcome this
limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based
self-supervised opinion-unaware method that does not require labeled MOS. In
particular, we introduce a quality-aware image-text alignment strategy to make
CLIP generate representations that correlate with the inherent quality of the
images. Starting from pristine images, we synthetically degrade them with
increasing levels of intensity. Then, we train CLIP to rank these degraded
images based on their similarity to quality-related antonym text prompts, while
guaranteeing consistent representations for images with comparable quality. Our
method achieves state-of-the-art performance on several datasets with authentic
distortions. Moreover, despite not requiring MOS, QualiCLIP outperforms
supervised methods when their training dataset differs from the testing one,
thus proving to be more suitable for real-world scenarios. Furthermore, our
approach demonstrates greater robustness and improved explainability than
competing methods. The code and the model are publicly available at
https://github.com/miccunifi/QualiCLIP.
\\ ( https://arxiv.org/abs/2403.11176 ,  27135kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11335 (*cross-listing*)
Date: Sun, 17 Mar 2024 20:34:40 GMT   (359kb,D)

Title: ConvSDG: Session Data Generation for Conversational Search
Authors: Fengran Mo, Bole Yi, Kelong Mao, Chen Qu, Kaiyu Huang, Jian-Yun Nie
Categories: cs.IR cs.CL
Comments: Accepted by WWW 2024 Workshop
\\
  Conversational search provides a more convenient interface for users to
search by allowing multi-turn interaction with the search engine. However, the
effectiveness of the conversational dense retrieval methods is limited by the
scarcity of training data required for their fine-tuning. Thus, generating more
training conversational sessions with relevant labels could potentially improve
search performance. Based on the promising capabilities of large language
models (LLMs) on text generation, we propose ConvSDG, a simple yet effective
framework to explore the feasibility of boosting conversational search by using
LLM for session data generation. Within this framework, we design
dialogue/session-level and query-level data generation with unsupervised and
semi-supervised learning, according to the availability of relevance judgments.
The generated data are used to fine-tune the conversational dense retriever.
Extensive experiments on four widely used datasets demonstrate the
effectiveness and broad applicability of our ConvSDG framework compared with
several strong baselines.
\\ ( https://arxiv.org/abs/2403.11335 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11771 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:30:03 GMT   (4409kb,D)

Title: Modality-Agnostic fMRI Decoding of Vision and Language
Authors: Mitja Nikolaus, Milad Mozafari, Nicholas Asher, Leila Reddy, Rufin
  VanRullen
Categories: cs.CV cs.CL
Comments: To appear at ICLR 2024 workshop on Representational Alignment
  (Re-Align)
\\
  Previous studies have shown that it is possible to map brain activation data
of subjects viewing images onto the feature representation space of not only
vision models (modality-specific decoding) but also language models
(cross-modal decoding). In this work, we introduce and use a new large-scale
fMRI dataset (~8,500 trials per subject) of people watching both images and
text descriptions of such images. This novel dataset enables the development of
modality-agnostic decoders: a single decoder that can predict which stimulus a
subject is seeing, irrespective of the modality (image or text) in which the
stimulus is presented. We train and evaluate such decoders to map brain signals
onto stimulus representations from a large range of publicly available vision,
language and multimodal (vision+language) models. Our findings reveal that (1)
modality-agnostic decoders perform as well as (and sometimes even better than)
modality-specific decoders (2) modality-agnostic decoders mapping brain data
onto representations from unimodal models perform as well as decoders relying
on multimodal representations (3) while language and low-level visual
(occipital) brain regions are best at decoding text and image stimuli,
respectively, high-level visual (temporal) regions perform well on both
stimulus types.
\\ ( https://arxiv.org/abs/2403.11771 ,  4409kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12025 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:56:37 GMT   (2397kb,AD)

Title: A Toolbox for Surfacing Health Equity Harms and Biases in Large Language
  Models
Authors: Stephen R. Pfohl, Heather Cole-Lewis, Rory Sayres, Darlene Neal, Mercy
  Asiedu, Awa Dieng, Nenad Tomasev, Qazi Mamunur Rashid, Shekoofeh Azizi, Negar
  Rostamzadeh, Liam G. McCoy, Leo Anthony Celi, Yun Liu, Mike Schaekermann,
  Alanna Walton, Alicia Parrish, Chirag Nagpal, Preeti Singh, Akeiylah Dewitt,
  Philip Mansfield, Sushant Prakash, Katherine Heller, Alan Karthikesalingam,
  Christopher Semturs, Joelle Barral, Greg Corrado, Yossi Matias, Jamila
  Smith-Loud, Ivor Horn, Karan Singhal
Categories: cs.CY cs.CL cs.LG
\\
  Large language models (LLMs) hold immense promise to serve complex health
information needs but also have the potential to introduce harm and exacerbate
health disparities. Reliably evaluating equity-related model failures is a
critical step toward developing systems that promote health equity. In this
work, we present resources and methodologies for surfacing biases with
potential to precipitate equity-related harms in long-form, LLM-generated
answers to medical questions and then conduct an empirical case study with
Med-PaLM 2, resulting in the largest human evaluation study in this area to
date. Our contributions include a multifactorial framework for human assessment
of LLM-generated answers for biases, and EquityMedQA, a collection of seven
newly-released datasets comprising both manually-curated and LLM-generated
questions enriched for adversarial queries. Both our human assessment framework
and dataset design process are grounded in an iterative participatory approach
and review of possible biases in Med-PaLM 2 answers to adversarial queries.
Through our empirical study, we find that the use of a collection of datasets
curated through a variety of methodologies, coupled with a thorough evaluation
protocol that leverages multiple assessment rubric designs and diverse rater
groups, surfaces biases that may be missed via narrower evaluation approaches.
Our experience underscores the importance of using diverse assessment
methodologies and involving raters of varying backgrounds and expertise. We
emphasize that while our framework can identify specific forms of bias, it is
not sufficient to holistically assess whether the deployment of an AI system
promotes equitable health outcomes. We hope the broader community leverages and
builds on these tools and methods towards realizing a shared goal of LLMs that
promote accessible and equitable healthcare for all.
\\ ( https://arxiv.org/abs/2403.12025 ,  2397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10543 (*cross-listing*)
Date: Mon, 11 Mar 2024 08:48:54 GMT   (1154kb,D)

Title: Distinguishing Neighborhood Representations Through Reverse Process of
  GNNs for Heterophilic Graphs
Authors: MoonJeong Park, Jaeseung Heo, Dongwoo Kim
Categories: cs.SI cs.LG
\\
  Graph Neural Network (GNN) resembles the diffusion process, leading to the
over-smoothing of learned representations when stacking many layers. Hence, the
reverse process of message passing can sharpen the node representations by
inverting the forward message propagation. The sharpened representations can
help us to better distinguish neighboring nodes with different labels, such as
in heterophilic graphs. In this work, we apply the design principle of the
reverse process to the three variants of the GNNs. Through the experiments on
heterophilic graph data, where adjacent nodes need to have different
representations for successful classification, we show that the reverse process
significantly improves the prediction performance in many cases. Additional
analysis reveals that the reverse mechanism can mitigate the over-smoothing
over hundreds of layers.
\\ ( https://arxiv.org/abs/2403.10543 ,  1154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10549 (*cross-listing*)
Date: Tue, 12 Mar 2024 19:54:35 GMT   (556kb,D)

Title: On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge
  Embedded Systems
Authors: Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel de Prado,
  Luca Benini
Categories: cs.SD cs.LG eess.AS
Comments: 5 pages, 2 tables, 2 figures. Accepted at IEEE AICAS 2024
\\
  Keyword spotting accuracy degrades when neural networks are exposed to noisy
environments. On-site adaptation to previously unseen noise is crucial to
recovering accuracy loss, and on-device learning is required to ensure that the
adaptation process happens entirely on the edge device. In this work, we
propose a fully on-device domain adaptation system achieving up to 14% accuracy
gains over already-robust keyword spotting models. We enable on-device learning
with less than 10 kB of memory, using only 100 labeled utterances to recover 5%
accuracy after adapting to the complex speech noise. We demonstrate that domain
adaptation can be achieved on ultra-low-power microcontrollers with as little
as 806 mJ in only 14 s on always-on, battery-operated devices.
\\ ( https://arxiv.org/abs/2403.10549 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10558 (*cross-listing*)
Date: Thu, 14 Mar 2024 02:17:57 GMT   (4567kb,D)

Title: Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition
  Against Model Inversion Attack
Authors: Yuanqing Huang, Yinggui Wang, Jianshu Li, Le Yang, Kai Song, Lei Wang
Categories: cs.CV cs.CR cs.LG
\\
  The utilization of personal sensitive data in training face recognition (FR)
models poses significant privacy concerns, as adversaries can employ model
inversion attacks (MIA) to infer the original training data. Existing defense
methods, such as data augmentation and differential privacy, have been employed
to mitigate this issue. However, these methods often fail to strike an optimal
balance between privacy and accuracy. To address this limitation, this paper
introduces an adaptive hybrid masking algorithm against MIA. Specifically, face
images are masked in the frequency domain using an adaptive MixUp strategy.
Unlike the traditional MixUp algorithm, which is predominantly used for data
augmentation, our modified approach incorporates frequency domain mixing.
Previous studies have shown that increasing the number of images mixed in MixUp
can enhance privacy preservation but at the expense of reduced face recognition
accuracy. To overcome this trade-off, we develop an enhanced adaptive MixUp
strategy based on reinforcement learning, which enables us to mix a larger
number of images while maintaining satisfactory recognition accuracy. To
optimize privacy protection, we propose maximizing the reward function (i.e.,
the loss function of the FR system) during the training of the strategy
network. While the loss function of the FR network is minimized in the phase of
training the FR network. The strategy network and the face recognition network
can be viewed as antagonistic entities in the training process, ultimately
reaching a more balanced trade-off. Experimental results demonstrate that our
proposed hybrid masking scheme outperforms existing defense algorithms in terms
of privacy preservation and recognition accuracy against MIA.
\\ ( https://arxiv.org/abs/2403.10558 ,  4567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10571 (*cross-listing*)
Date: Thu, 14 Mar 2024 20:32:31 GMT   (201kb,D)

Title: JaxDecompiler: Redefining Gradient-Informed Software Design
Authors: Pierrick Pochelu
Categories: cs.PL cs.LG cs.SE
\\
  Among numerical libraries capable of computing gradient descent optimization,
JAX stands out by offering more features, accelerated by an intermediate
representation known as Jaxpr language. However, editing the Jaxpr code is not
directly possible. This article introduces JaxDecompiler, a tool that
transforms any JAX function into an editable Python code, especially useful for
editing the JAX function generated by the gradient function. JaxDecompiler
simplifies the processes of reverse engineering, understanding, customizing,
and interoperability of software developed by JAX. We highlight its
capabilities, emphasize its practical applications especially in deep learning
and more generally gradient-informed software, and demonstrate that the
decompiled code speed performance is similar to the original.
\\ ( https://arxiv.org/abs/2403.10571 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10573 (*cross-listing*)
Date: Fri, 15 Mar 2024 02:35:36 GMT   (10346kb,D)

Title: Medical Unlearnable Examples: Securing Medical Data from Unauthorized
  Traning via Sparsity-Aware Local Masking
Authors: Weixiang Sun, Yixin Liu, Zhiling Yan, Kaidi Xu, Lichao Sun
Categories: eess.IV cs.CR cs.CV cs.LG
\\
  With the rapid growth of artificial intelligence (AI) in healthcare, there
has been a significant increase in the generation and storage of sensitive
medical data. This abundance of data, in turn, has propelled the advancement of
medical AI technologies. However, concerns about unauthorized data
exploitation, such as training commercial AI models, often deter researchers
from making their invaluable datasets publicly available. In response to the
need to protect this hard-to-collect data while still encouraging medical
institutions to share it, one promising solution is to introduce imperceptible
noise into the data. This method aims to safeguard the data against
unauthorized training by inducing degradation in model generalization. Although
existing methods have shown commendable data protection capabilities in general
domains, they tend to fall short when applied to biomedical data, mainly due to
their failure to account for the sparse nature of medical images. To address
this problem, we propose the Sparsity-Aware Local Masking (SALM) method, a
novel approach that selectively perturbs significant pixel regions rather than
the entire image as previous strategies have done. This simple-yet-effective
approach significantly reduces the perturbation search space by concentrating
on local regions, thereby improving both the efficiency and effectiveness of
data protection for biomedical datasets characterized by sparse features.
Besides, we have demonstrated that SALM maintains the essential characteristics
of the data, ensuring its clinical utility remains uncompromised. Our extensive
experiments across various datasets and model architectures demonstrate that
SALM effectively prevents unauthorized training of deep-learning models and
outperforms previous state-of-the-art data protection methods.
\\ ( https://arxiv.org/abs/2403.10573 ,  10346kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10578 (*cross-listing*)
Date: Fri, 15 Mar 2024 09:30:29 GMT   (2617kb,D)

Title: Generative Modelling of Stochastic Rotating Shallow Water Noise
Authors: Dan Crisan, Oana Lang, Alexander Lobbe
Categories: stat.ML cs.LG cs.NA math.DS math.NA physics.flu-dyn
MSC-class: 68T05, 76M35
\\
  In recent work, the authors have developed a generic methodology for
calibrating the noise in fluid dynamics stochastic partial differential
equations where the stochasticity was introduced to parametrize subgrid-scale
processes. The stochastic parameterization of sub-grid scale processes is
required in the estimation of uncertainty in weather and climate predictions,
to represent systematic model errors arising from subgrid-scale fluctuations.
The previous methodology used a principal component analysis (PCA) technique
based on the ansatz that the increments of the stochastic parametrization are
normally distributed.
  In this paper, the PCA technique is replaced by a generative model technique.
This enables us to avoid imposing additional constraints on the increments. The
methodology is tested on a stochastic rotating shallow water model with the
elevation variable of the model used as input data. The numerical simulations
show that the noise is indeed non-Gaussian. The generative modelling technology
gives good RMSE, CRPS score and forecast rank histogram results.
\\ ( https://arxiv.org/abs/2403.10578 ,  2617kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10582 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:20:21 GMT   (4175kb,D)

Title: How Suboptimal is Training rPPG Models with Videos and Targets from
  Different Body Sites?
Authors: Bj\"orn Braun, Daniel McDuff, Christian Holz
Categories: eess.IV cs.LG
\\
  Remote camera measurement of the blood volume pulse via photoplethysmography
(rPPG) is a compelling technology for scalable, low-cost, and accessible
assessment of cardiovascular information. Neural networks currently provide the
state-of-the-art for this task and supervised training or fine-tuning is an
important step in creating these models. However, most current models are
trained on facial videos using contact PPG measurements from the fingertip as
targets/ labels. One of the reasons for this is that few public datasets to
date have incorporated contact PPG measurements from the face. Yet there is
copious evidence that the PPG signals at different sites on the body have very
different morphological features. Is training a facial video rPPG model using
contact measurements from another site on the body suboptimal? Using a recently
released unique dataset with synchronized contact PPG and video measurements
from both the hand and face, we can provide precise and quantitative answers to
this question. We obtain up to 40 % lower mean squared errors between the
waveforms of the predicted and the ground truth PPG signals using
state-of-the-art neural models when using PPG signals from the forehead
compared to using PPG signals from the fingertip. We also show qualitatively
that the neural models learn to predict the morphology of the ground truth PPG
signal better when trained on the forehead PPG signals. However, while models
trained from the forehead PPG produce a more faithful waveform, models trained
from a finger PPG do still learn the dominant frequency (i.e., the heart rate)
well.
\\ ( https://arxiv.org/abs/2403.10582 ,  4175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10615 (*cross-listing*)
Date: Fri, 15 Mar 2024 18:26:33 GMT   (15191kb,D)

Title: LightIt: Illumination Modeling and Control for Diffusion Models
Authors: Peter Kocsis (1), Julien Philip (2), Kalyan Sunkavalli (2), Matthias
  Nie{\ss}ner (1), Yannick Hold-Geoffroy (2) ((1) Technical University of
  Munich, (2) Adobe Research)
Categories: cs.CV cs.GR cs.LG
Comments: Project page: https://peter-kocsis.github.io/LightIt/ Video:
  https://youtu.be/cCfSBD5aPLI
ACM-class: I.4.8; I.2.10
\\
  We introduce LightIt, a method for explicit illumination control for image
generation. Recent generative methods lack lighting control, which is crucial
to numerous artistic aspects of image generation such as setting the overall
mood or cinematic appearance. To overcome these limitations, we propose to
condition the generation on shading and normal maps. We model the lighting with
single bounce shading, which includes cast shadows. We first train a shading
estimation module to generate a dataset of real-world images and shading pairs.
Then, we train a control network using the estimated shading and normals as
input. Our method demonstrates high-quality image generation and lighting
control in numerous scenes. Additionally, we use our generated dataset to train
an identity-preserving relighting model, conditioned on an image and a target
shading. Our method is the first that enables the generation of images with
controllable, consistent lighting and performs on par with specialized
relighting state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.10615 ,  15191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10650 (*cross-listing*)
Date: Fri, 15 Mar 2024 19:35:10 GMT   (2685kb,D)

Title: PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time
  Adaptation
Authors: Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo
Categories: cs.CV cs.LG
\\
  Real-world vision models in dynamic environments face rapid shifts in domain
distributions, leading to decreased recognition performance. Continual
test-time adaptation (CTTA) directly adjusts a pre-trained source
discriminative model to these changing domains using test data. A highly
effective CTTA method involves applying layer-wise adaptive learning rates, and
selectively adapting pre-trained layers. However, it suffers from the poor
estimation of domain shift and the inaccuracies arising from the pseudo-labels.
In this work, we aim to overcome these limitations by identifying layers
through the quantification of model prediction uncertainty without relying on
pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by
backpropagating the KL divergence between the softmax output and a uniform
distribution, to select layers for further adaptation. Subsequently, for the
parameters exclusively belonging to these selected layers, with the remaining
ones frozen, we evaluate their sensitivity in order to approximate the domain
shift, followed by adjusting their learning rates accordingly. Overall, this
approach leads to a more robust and stable optimization than prior approaches.
We conduct extensive image classification experiments on CIFAR-10C, CIFAR-100C,
and ImageNet-C and demonstrate the efficacy of our method against standard
benchmarks and prior methods.
\\ ( https://arxiv.org/abs/2403.10650 ,  2685kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10658 (*cross-listing*)
Date: Fri, 15 Mar 2024 19:54:10 GMT   (4606kb,D)

Title: InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance
  Semi-Supervised Learning
Authors: Zhe Huang, Xiaowei Yu, Dajiang Zhu, Michael C. Hughes
Categories: cs.CV cs.LG
Comments: Semi-supervised Learning; Vision Transformers
\\
  Semi-supervised learning (SSL) seeks to enhance task performance by training
on both labeled and unlabeled data. Mainstream SSL image classification methods
mostly optimize a loss that additively combines a supervised classification
objective with a regularization term derived solely from unlabeled data. This
formulation neglects the potential for interaction between labeled and
unlabeled images. In this paper, we introduce InterLUDE, a new approach to
enhance SSL made of two parts that each benefit from labeled-unlabeled
interaction. The first part, embedding fusion, interpolates between labeled and
unlabeled embeddings to improve representation learning. The second part is a
new loss, grounded in the principle of consistency regularization, that aims to
minimize discrepancies in the model's predictions between labeled versus
unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a
medical SSL task with an uncurated unlabeled set show clear benefits to our
approach. On the STL-10 dataset with only 40 labels, InterLUDE achieves 3.2%
error rate, while the best previous method reports 14.9%.
\\ ( https://arxiv.org/abs/2403.10658 ,  4606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10663 (*cross-listing*)
Date: Fri, 15 Mar 2024 20:12:41 GMT   (3768kb,D)

Title: Not Just Change the Labels, Learn the Features: Watermarking Deep Neural
  Networks with Multi-View Data
Authors: Yuxuan Li, Sarthak Kumar Maharana, Yunhui Guo
Categories: cs.CR cs.CV cs.LG
\\
  With the increasing prevalence of Machine Learning as a Service (MLaaS)
platforms, there is a growing focus on deep neural network (DNN) watermarking
techniques. These methods are used to facilitate the verification of ownership
for a target DNN model to protect intellectual property. One of the most widely
employed watermarking techniques involves embedding a trigger set into the
source model. Unfortunately, existing methodologies based on trigger sets are
still susceptible to functionality-stealing attacks, potentially enabling
adversaries to steal the functionality of the source model without a reliable
means of verifying ownership. In this paper, we first introduce a novel
perspective on trigger set-based watermarking methods from a feature learning
perspective. Specifically, we demonstrate that by selecting data exhibiting
multiple features, also referred to as $\textit{multi-view data}$, it becomes
feasible to effectively defend functionality stealing attacks. Based on this
perspective, we introduce a novel watermarking technique based on Multi-view
dATa, called MAT, for efficiently embedding watermarks within DNNs. This
approach involves constructing a trigger set with multi-view data and
incorporating a simple feature-based regularization method for training the
source model. We validate our method across various benchmarks and demonstrate
its efficacy in defending against model extraction attacks, surpassing relevant
baselines by a significant margin.
\\ ( https://arxiv.org/abs/2403.10663 ,  3768kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10671 (*cross-listing*)
Date: Fri, 15 Mar 2024 20:47:39 GMT   (267kb,D)

Title: Hessian-Free Laplace in Bayesian Deep Learning
Authors: James McInerney, Nathan Kallus
Categories: stat.ML cs.LG
Comments: 10 pages, 5 figures
\\
  The Laplace approximation (LA) of the Bayesian posterior is a Gaussian
distribution centered at the maximum a posteriori estimate. Its appeal in
Bayesian deep learning stems from the ability to quantify uncertainty post-hoc
(i.e., after standard network parameter optimization), the ease of sampling
from the approximate posterior, and the analytic form of model evidence.
However, an important computational bottleneck of LA is the necessary step of
calculating and inverting the Hessian matrix of the log posterior. The Hessian
may be approximated in a variety of ways, with quality varying with a number of
factors including the network, dataset, and inference task. In this paper, we
propose an alternative framework that sidesteps Hessian calculation and
inversion. The Hessian-free Laplace (HFL) approximation uses curvature of both
the log posterior and network prediction to estimate its variance. Only two
point estimates are needed: the standard maximum a posteriori parameter and the
optimal parameter under a loss regularized by the network prediction. We show
that, under standard assumptions of LA in Bayesian deep learning, HFL targets
the same variance as LA, and can be efficiently amortized in a pre-trained
network. Experiments demonstrate comparable performance to that of exact and
approximate Hessians, with excellent coverage for in-between uncertainty.
\\ ( https://arxiv.org/abs/2403.10671 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10672 (*cross-listing*)
Date: Fri, 15 Mar 2024 20:48:41 GMT   (7023kb,D)

Title: Riemannian Flow Matching Policy for Robot Motion Learning
Authors: Max Braun, No\'emie Jaquier, Leonel Rozo, Tamim Asfour
Categories: cs.RO cs.LG
Comments: 8 pages, 5 figures, 4 tables
\\
  We introduce Riemannian Flow Matching Policies (RFMP), a novel model for
learning and synthesizing robot visuomotor policies. RFMP leverages the
efficient training and inference capabilities of flow matching methods. By
design, RFMP inherits the strengths of flow matching: the ability to encode
high-dimensional multimodal distributions, commonly encountered in robotic
tasks, and a very simple and fast inference process. We demonstrate the
applicability of RFMP to both state-based and vision-conditioned robot motion
policies. Notably, as the robot state resides on a Riemannian manifold, RFMP
inherently incorporates geometric awareness, which is crucial for realistic
robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments,
comparing its performance against Diffusion Policies. Although both approaches
successfully learn the considered tasks, our results show that RFMP provides
smoother action trajectories with significantly lower inference times.
\\ ( https://arxiv.org/abs/2403.10672 ,  7023kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10682 (*cross-listing*)
Date: Fri, 15 Mar 2024 21:03:34 GMT   (2757kb)

Title: Evaluation of GlassNet for physics-informed machine learning of glass
  stability and glass-forming ability
Authors: Sarah I. Allec, Xiaonan Lu, Daniel R. Cassar, Xuan T. Nguyen, Vinay I.
  Hegde, Thiruvillamalai Mahadevan, Miroslava Peterson, Jincheng Du, Brian J.
  Riley, John D. Vienna, James E. Saal
Categories: cond-mat.mtrl-sci cs.LG
\\
  Glasses form the basis of many modern applications and also hold great
potential for future medical and environmental applications. However, their
structural complexity and large composition space make design and optimization
challenging for certain applications. Of particular importance for glass
processing is an estimate of a given composition's glass-forming ability (GFA).
However, there remain many open questions regarding the physical mechanisms of
glass formation, especially in oxide glasses. It is apparent that a proxy for
GFA would be highly useful in glass processing and design, but identifying such
a surrogate property has proven itself to be difficult. Here, we explore the
application of an open-source pre-trained NN model, GlassNet, that can predict
the characteristic temperatures necessary to compute glass stability (GS) and
assess the feasibility of using these physics-informed ML (PIML)-predicted GS
parameters to estimate GFA. In doing so, we track the uncertainties at each
step of the computation - from the original ML prediction errors, to the
compounding of errors during GS estimation, and finally to the final estimation
of GFA. While GlassNet exhibits reasonable accuracy on all individual
properties, we observe a large compounding of error in the combination of these
individual predictions for the prediction of GS, finding that random forest
models offer similar accuracy to GlassNet. We also breakdown the ML performance
on different glass families and find that the error in GS prediction is
correlated with the error in crystallization peak temperature prediction.
Lastly, we utilize this finding to assess the relationship between
top-performing GS parameters and GFA for two ternary glass systems: sodium
borosilicate and sodium iron phosphate glasses. We conclude that to obtain true
ML predictive capability of GFA, significantly more data needs to be collected.
\\ ( https://arxiv.org/abs/2403.10682 ,  2757kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10689 (*cross-listing*)
Date: Fri, 15 Mar 2024 21:18:14 GMT   (4269kb,D)

Title: Latent Object Characteristics Recognition with Visual to Haptic-Audio
  Cross-modal Transfer Learning
Authors: Namiko Saito, Joao Moura, Hiroki Uchida and Sethu Vijayakumar
Categories: cs.RO cs.CV cs.LG
Comments: 8 pages
\\
  Recognising the characteristics of objects while a robot handles them is
crucial for adjusting motions that ensure stable and efficient interactions
with containers. Ahead of realising stable and efficient robot motions for
handling/transferring the containers, this work aims to recognise the latent
unobservable object characteristics. While vision is commonly used for object
recognition by robots, it is ineffective for detecting hidden objects. However,
recognising objects indirectly using other sensors is a challenging task. To
address this challenge, we propose a cross-modal transfer learning approach
from vision to haptic-audio. We initially train the model with vision, directly
observing the target object. Subsequently, we transfer the latent space learned
from vision to a second module, trained only with haptic-audio and motor data.
This transfer learning framework facilitates the representation of object
characteristics using indirect sensor data, thereby improving recognition
accuracy. For evaluating the recognition accuracy of our proposed learning
framework we selected shape, position, and orientation as the object
characteristics. Finally, we demonstrate online recognition of both trained and
untrained objects using the humanoid robot Nextage Open.
\\ ( https://arxiv.org/abs/2403.10689 ,  4269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10696 (*cross-listing*)
Date: Fri, 15 Mar 2024 21:29:33 GMT   (2854kb,D)

Title: On the low-shot transferability of [V]-Mamba
Authors: Diganta Misra, Jay Gala, Antonio Orvieto
Categories: cs.CV cs.LG
Comments: Preprint (Work in progress)
\\
  The strength of modern large-scale neural networks lies in their ability to
efficiently adapt to new tasks with few examples. Although extensive research
has investigated the transferability of Vision Transformers (ViTs) to various
downstream tasks under diverse constraints, this study shifts focus to explore
the transfer learning potential of [V]-Mamba. We compare its performance with
ViTs across different few-shot data budgets and efficient transfer methods. Our
analysis yields three key insights into [V]-Mamba's few-shot transfer
performance: (a) [V]-Mamba demonstrates superior or equivalent few-shot
learning capabilities compared to ViTs when utilizing linear probing (LP) for
transfer, (b) Conversely, [V]-Mamba exhibits weaker or similar few-shot
learning performance compared to ViTs when employing visual prompting (VP) as
the transfer method, and (c) We observe a weak positive correlation between the
performance gap in transfer via LP and VP and the scale of the [V]-Mamba model.
This preliminary analysis lays the foundation for more comprehensive studies
aimed at furthering our understanding of the capabilities of [V]-Mamba variants
and their distinctions from ViTs.
\\ ( https://arxiv.org/abs/2403.10696 ,  2854kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10731 (*cross-listing*)
Date: Fri, 15 Mar 2024 23:31:41 GMT   (22034kb,D)

Title: Giving a Hand to Diffusion Models: a Two-Stage Approach to Improving
  Conditional Human Image Generation
Authors: Anton Pelykh, Ozge Mercanoglu Sincan, Richard Bowden
Categories: cs.CV cs.LG
\\
  Recent years have seen significant progress in human image generation,
particularly with the advancements in diffusion models. However, existing
diffusion methods encounter challenges when producing consistent hand anatomy
and the generated images often lack precise control over the hand pose. To
address this limitation, we introduce a novel approach to pose-conditioned
human image generation, dividing the process into two stages: hand generation
and subsequent body out-painting around the hands. We propose training the hand
generator in a multi-task setting to produce both hand images and their
corresponding segmentation masks, and employ the trained model in the first
stage of generation. An adapted ControlNet model is then used in the second
stage to outpaint the body around the generated hands, producing the final
result. A novel blending technique is introduced to preserve the hand details
during the second stage that combines the results of both stages in a coherent
way. This involves sequential expansion of the out-painted region while fusing
the latent representations, to ensure a seamless and cohesive synthesis of the
final image. Experimental evaluations demonstrate the superiority of our
proposed method over state-of-the-art techniques, in both pose accuracy and
image quality, as validated on the HaGRID dataset. Our approach not only
enhances the quality of the generated hands but also offers improved control
over hand pose, advancing the capabilities of pose-conditioned human image
generation. The source code of the proposed approach is available at
https://github.com/apelykh/hand-to-diffusion.
\\ ( https://arxiv.org/abs/2403.10731 ,  22034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10748 (*cross-listing*)
Date: Sat, 16 Mar 2024 00:45:06 GMT   (8333kb,D)

Title: A Comprehensive Review of Latent Space Dynamics Identification
  Algorithms for Intrusive and Non-Intrusive Reduced-Order-Modeling
Authors: Christophe Bonneville and Xiaolong He and April Tran and Jun Sur Park
  and William Fries and Daniel A. Messenger and Siu Wun Cheung and Yeonjong
  Shin and David M. Bortz and Debojyoti Ghosh and Jiun-Shyan Chen and Jonathan
  Belof and Youngsoo Choi
Categories: cs.CE cs.LG cs.MS cs.NA math.NA
\\
  Numerical solvers of partial differential equations (PDEs) have been widely
employed for simulating physical systems. However, the computational cost
remains a major bottleneck in various scientific and engineering applications,
which has motivated the development of reduced-order models (ROMs). Recently,
machine-learning-based ROMs have gained significant popularity and are
promising for addressing some limitations of traditional ROM methods,
especially for advection dominated systems. In this chapter, we focus on a
particular framework known as Latent Space Dynamics Identification (LaSDI),
which transforms the high-fidelity data, governed by a PDE, to simpler and
low-dimensional latent-space data, governed by ordinary differential equations
(ODEs). These ODEs can be learned and subsequently interpolated to make ROM
predictions. Each building block of LaSDI can be easily modulated depending on
the application, which makes the LaSDI framework highly flexible. In
particular, we present strategies to enforce the laws of thermodynamics into
LaSDI models (tLaSDI), enhance robustness in the presence of noise through the
weak form (WLaSDI), select high-fidelity training data efficiently through
active learning (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty
through Gaussian processes (GPLaSDI). We demonstrate the performance of
different LaSDI approaches on Burgers equation, a non-linear heat conduction
problem, and a plasma physics problem, showing that LaSDI algorithms can
achieve relative errors of less than a few percent and up to thousands of times
speed-ups.
\\ ( https://arxiv.org/abs/2403.10748 ,  8333kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10763 (*cross-listing*)
Date: Sat, 16 Mar 2024 02:06:14 GMT   (5583kb,D)

Title: A Primal-Dual Algorithm for Faster Distributionally Robust Optimization
Authors: Ronak Mehta, Jelena Diakonikolas, Zaid Harchaoui
Categories: stat.ML cs.LG math.OC
\\
  We consider the penalized distributionally robust optimization (DRO) problem
with a closed, convex uncertainty set, a setting that encompasses the $f$-DRO,
Wasserstein-DRO, and spectral/$L$-risk formulations used in practice. We
present Drago, a stochastic primal-dual algorithm that achieves a
state-of-the-art linear convergence rate on strongly convex-strongly concave
DRO problems. The method combines both randomized and cyclic components with
mini-batching, which effectively handles the unique asymmetric nature of the
primal and dual problems in DRO. We support our theoretical results with
numerical benchmarks in classification and regression.
\\ ( https://arxiv.org/abs/2403.10763 ,  5583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10786 (*cross-listing*)
Date: Sat, 16 Mar 2024 03:33:52 GMT   (4618kb,D)

Title: ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion
  Models
Authors: Yuwen Chen, Nicholas Konz, Hanxue Gu, Haoyu Dong, Yaqian Chen, Lin Li,
  Jisoo Lee, Maciej A. Mazurowski
Categories: eess.IV cs.CV cs.LG
Comments: Code will be released on GitHub
\\
  Accurately translating medical images across different modalities (e.g., CT
to MRI) has numerous downstream clinical and machine learning applications.
While several methods have been proposed to achieve this, they often prioritize
perceptual quality with respect to output domain features over preserving
anatomical fidelity. However, maintaining anatomy during translation is
essential for many tasks, e.g., when leveraging masks from the input domain to
develop a segmentation model with images translated to the output domain. To
address these challenges, we propose ContourDiff, a novel framework that
leverages domain-invariant anatomical contour representations of images. These
representations are simple to extract from images, yet form precise spatial
constraints on their anatomical content. We introduce a diffusion model that
converts contour representations of images from arbitrary input domains into
images in the output domain of interest. By applying the contour as a
constraint at every diffusion sampling step, we ensure the preservation of
anatomical content. We evaluate our method by training a segmentation model on
images translated from CT to MRI with their original CT masks and testing its
performance on real MRIs. Our method outperforms other unpaired image
translation methods by a significant margin, furthermore without the need to
access any input domain information during training.
\\ ( https://arxiv.org/abs/2403.10786 ,  4618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10790 (*cross-listing*)
Date: Sat, 16 Mar 2024 03:42:29 GMT   (1028kb,D)

Title: QuantumLeak: Stealing Quantum Neural Networks from Cloud-based NISQ
  Machines
Authors: Zhenxiao Fu, Min Yang, Cheng Chu, Yilun Xu, Gang Huang, Fan Chen
Categories: quant-ph cs.CR cs.LG
Journal-ref: published in IJCNN 2024
\\
  Variational quantum circuits (VQCs) have become a powerful tool for
implementing Quantum Neural Networks (QNNs), addressing a wide range of complex
problems. Well-trained VQCs serve as valuable intellectual assets hosted on
cloud-based Noisy Intermediate Scale Quantum (NISQ) computers, making them
susceptible to malicious VQC stealing attacks. However, traditional model
extraction techniques designed for classical machine learning models encounter
challenges when applied to NISQ computers due to significant noise in current
devices. In this paper, we introduce QuantumLeak, an effective and accurate QNN
model extraction technique from cloud-based NISQ machines. Compared to existing
classical model stealing techniques, QuantumLeak improves local VQC accuracy by
4.99\%$\sim$7.35\% across diverse datasets and VQC architectures.
\\ ( https://arxiv.org/abs/2403.10790 ,  1028kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10794 (*cross-listing*)
Date: Sat, 16 Mar 2024 03:53:55 GMT   (21754kb,D)

Title: Diffusion-Reinforcement Learning Hierarchical Motion Planning in
  Adversarial Multi-agent Games
Authors: Zixuan Wu, Sean Ye, Manisha Natarajan and Matthew C. Gombolay
Categories: cs.RO cs.LG cs.MA
Comments: This work has been submitted to the IEEE Robotics and Automation
  Letters (RA-L) for possible publication. Copyright may be transferred without
  notice, after which this version may no longer be accessible
\\
  Reinforcement Learning- (RL-)based motion planning has recently shown the
potential to outperform traditional approaches from autonomous navigation to
robot manipulation. In this work, we focus on a motion planning task for an
evasive target in a partially observable multi-agent adversarial
pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to
various applications, such as search and rescue operations and surveillance
robots, where robots must effectively plan their actions to gather intelligence
or accomplish mission tasks while avoiding detection or capture themselves. We
propose a hierarchical architecture that integrates a high-level diffusion
model to plan global paths responsive to environment data while a low-level RL
algorithm reasons about evasive versus global path-following behavior. Our
approach outperforms baselines by 51.2% by leveraging the diffusion model to
guide the RL algorithm for more efficient exploration and improves the
explanability and predictability.
\\ ( https://arxiv.org/abs/2403.10794 ,  21754kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10859 (*cross-listing*)
Date: Sat, 16 Mar 2024 08:51:02 GMT   (295kb,D)

Title: Neural-Kernel Conditional Mean Embeddings
Authors: Eiki Shimizu, Kenji Fukumizu, Dino Sejdinovic
Categories: stat.ML cs.LG
\\
  Kernel conditional mean embeddings (CMEs) offer a powerful framework for
representing conditional distribution, but they often face scalability and
expressiveness challenges. In this work, we propose a new method that
effectively combines the strengths of deep learning with CMEs in order to
address these challenges. Specifically, our approach leverages the end-to-end
neural network (NN) optimization framework using a kernel-based objective. This
design circumvents the computationally expensive Gram matrix inversion required
by current CME methods. To further enhance performance, we provide efficient
strategies to optimize the remaining kernel hyperparameters. In conditional
density estimation tasks, our NN-CME hybrid achieves competitive performance
and often surpasses existing deep learning-based methods. Lastly, we showcase
its remarkable versatility by seamlessly integrating it into reinforcement
learning (RL) contexts. Building on Q-learning, our approach naturally leads to
a new variant of distributional RL methods, which demonstrates consistent
effectiveness across different environments.
\\ ( https://arxiv.org/abs/2403.10859 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10861 (*cross-listing*)
Date: Sat, 16 Mar 2024 08:58:03 GMT   (1359kb,D)

Title: FedQNN: Federated Learning using Quantum Neural Networks
Authors: Nouhaila Innan, Muhammad Al-Zafar Khan, Alberto Marchisio, Muhammad
  Shafique, and Mohamed Bennai
Categories: quant-ph cs.ET cs.LG
Comments: Accepted for presentation at IJCNN 2024
\\
  In this study, we explore the innovative domain of Quantum Federated Learning
(QFL) as a framework for training Quantum Machine Learning (QML) models via
distributed networks. Conventional machine learning models frequently grapple
with issues about data privacy and the exposure of sensitive information. Our
proposed Federated Quantum Neural Network (FedQNN) framework emerges as a
cutting-edge solution, integrating the singular characteristics of QML with the
principles of classical federated learning. This work thoroughly investigates
QFL, underscoring its capability to secure data handling in a distributed
environment and facilitate cooperative learning without direct data sharing.
Our research corroborates the concept through experiments across varied
datasets, including genomics and healthcare, thereby validating the versatility
and efficacy of our FedQNN framework. The results consistently exceed 86%
accuracy across three distinct datasets, proving its suitability for conducting
various QML tasks. Our research not only identifies the limitations of
classical paradigms but also presents a novel framework to propel the field of
QML into a new era of secure and collaborative innovation.
\\ ( https://arxiv.org/abs/2403.10861 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10912 (*cross-listing*)
Date: Sat, 16 Mar 2024 12:25:30 GMT   (427kb)

Title: Automatic location detection based on deep learning
Authors: Anjali Karangiya, Anirudh Sharma, Divax Shah, Kartavya Badgujar, Dr.
  Chintan Thacker, Dainik Dave
Categories: cs.CV cs.LG
\\
  The proliferation of digital images and the advancements in deep learning
have paved the way for innovative solutions in various domains, especially in
the field of image classification. Our project presents an in- depth study and
implementation of an image classification system specifically tailored to
identify and classify images of Indian cities. Drawing from an extensive
dataset, our model classifies images into five major Indian cities: Ahmedabad,
Delhi, Kerala, Kolkata, and Mumbai to recognize the distinct features and
characteristics of each city/state. To achieve high precision and recall rates,
we adopted two approaches. The first, a vanilla Convolutional Neural Network
(CNN) and then we explored the power of transfer learning by leveraging the
VGG16 model. The vanilla CNN achieved commendable accuracy and the VGG16 model
achieved a test accuracy of 63.6%. Evaluations highlighted the strengths and
potential areas of improvement, positioning our model as not only competitive
but also scalable for broader applications. With an emphasis on open-source
ethos, our work aims to contribute to the community, encouraging further
development and diverse applications. Our findings demonstrate the potential
applications in tourism, urban planning, and even real-time location
identification systems, among others.
\\ ( https://arxiv.org/abs/2403.10912 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10927 (*cross-listing*)
Date: Sat, 16 Mar 2024 13:50:31 GMT   (422kb)

Title: Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground
  Cooperative MEC
Authors: Yang Huang, Miaomiao Dong, Yijie Mao, Wenqiang Liu, and Zhen Gao
Categories: cs.IT cs.LG math.IT
Comments: This paper has been accepted for publication in the IEEE Transactions
  on Vehicular Technology
\\
  Utilizing unmanned aerial vehicles (UAVs) with edge server to assist
terrestrial mobile edge computing (MEC) has attracted tremendous attention.
Nevertheless, state-of-the-art schemes based on deterministic optimizations or
single-objective reinforcement learning (RL) cannot reduce the backlog of task
bits and simultaneously improve energy efficiency in highly dynamic network
environments, where the design problem amounts to a sequential decision-making
problem. In order to address the aforementioned problems, as well as the curses
of dimensionality introduced by the growing number of terrestrial terrestrial
users, this paper proposes a distributed multi-objective (MO) dynamic
trajectory planning and offloading scheduling scheme, integrated with MORL and
the kernel method. The design of n-step return is also applied to average
fluctuations in the backlog. Numerical results reveal that the n-step return
can benefit the proposed kernel-based approach, achieving significant
improvement in the long-term average backlog performance, compared to the
conventional 1-step return design. Due to such design and the kernel-based
neural network, to which decision-making features can be continuously added,
the kernel-based approach can outperform the approach based on fully-connected
deep neural network, yielding improvement in energy consumption and the backlog
performance, as well as a significant reduction in decision-making and online
learning time.
\\ ( https://arxiv.org/abs/2403.10927 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10929 (*cross-listing*)
Date: Sat, 16 Mar 2024 14:00:04 GMT   (1616kb,D)

Title: Function-space Parameterization of Neural Networks for Sequential
  Learning
Authors: Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni
  Pajarinen, Arno Solin
Categories: stat.ML cs.LG
Comments: 29 pages, 8 figures, Published in The Twelfth International
  Conference on Learning Representations
\\
  Sequential learning paradigms pose challenges for gradient-based deep
learning due to difficulties incorporating new data and retaining prior
knowledge. While Gaussian processes elegantly tackle these problems, they
struggle with scalability and handling rich inputs, such as images. To address
these issues, we introduce a technique that converts neural networks from
weight space to function space, through a dual parameterization. Our
parameterization offers: (i) a way to scale function-space methods to large
data sets via sparsification, (ii) retention of prior knowledge when access to
past data is limited, and (iii) a mechanism to incorporate new data without
retraining. Our experiments demonstrate that we can retain knowledge in
continual learning and incorporate new data efficiently. We further show its
strengths in uncertainty quantification and guiding exploration in model-based
RL. Further information and code is available on the project website.
\\ ( https://arxiv.org/abs/2403.10929 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10940 (*cross-listing*)
Date: Sat, 16 Mar 2024 14:52:26 GMT   (14412kb,D)

Title: ViSaRL: Visual Reinforcement Learning Guided by Human Saliency
Authors: Anthony Liang, Jesse Thomason, Erdem B{\i}y{\i}k
Categories: cs.RO cs.LG
\\
  Training robots to perform complex control tasks from high-dimensional pixel
input using reinforcement learning (RL) is sample-inefficient, because image
observations are comprised primarily of task-irrelevant information. By
contrast, humans are able to visually attend to task-relevant objects and
areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement
Learning (ViSaRL). Using ViSaRL to learn visual representations significantly
improves the success rate, sample efficiency, and generalization of an RL agent
on diverse tasks including DeepMind Control benchmark, robot manipulation in
simulation and on a real robot. We present approaches for incorporating
saliency into both CNN and Transformer-based encoders. We show that visual
representations learned using ViSaRL are robust to various sources of visual
perturbations including perceptual noise and scene variations. ViSaRL nearly
doubles success rate on the real-robot tasks compared to the baseline which
does not use saliency.
\\ ( https://arxiv.org/abs/2403.10940 ,  14412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10946 (*cross-listing*)
Date: Sat, 16 Mar 2024 15:29:22 GMT   (685kb,D)

Title: The Fallacy of Minimizing Local Regret in the Sequential Task Setting
Authors: Ziping Xu, Kelly W. Zhang, Susan A. Murphy
Categories: stat.ML cs.LG
\\
  In the realm of Reinforcement Learning (RL), online RL is often
conceptualized as an optimization problem, where an algorithm interacts with an
unknown environment to minimize cumulative regret. In a stationary setting,
strong theoretical guarantees, like a sublinear ($\sqrt{T}$) regret bound, can
be obtained, which typically implies the convergence to an optimal policy and
the cessation of exploration. However, these theoretical setups often
oversimplify the complexities encountered in real-world RL implementations,
where tasks arrive sequentially with substantial changes between tasks and the
algorithm may not be allowed to adaptively learn within certain tasks. We study
the changes beyond the outcome distributions, encompassing changes in the
reward designs (mappings from outcomes to rewards) and the permissible policy
spaces. Our results reveal the fallacy of myopically minimizing regret within
each task: obtaining optimal regret rates in the early tasks may lead to worse
rates in the subsequent ones, even when the outcome distributions stay the
same. To realize the optimal cumulative regret bound across all the tasks, the
algorithm has to overly explore in the earlier tasks. This theoretical insight
is practically significant, suggesting that due to unanticipated changes (e.g.,
rapid technological development or human-in-the-loop involvement) between
tasks, the algorithm needs to explore more than it would in the usual
stationary setting within each task. Such implication resonates with the common
practice of using clipped policies in mobile health clinical trials and
maintaining a fixed rate of $\epsilon$-greedy exploration in robotic learning.
\\ ( https://arxiv.org/abs/2403.10946 ,  685kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10996 (*cross-listing*)
Date: Sat, 16 Mar 2024 18:47:04 GMT   (4403kb,D)

Title: A Scalable and Parallelizable Digital Twin Framework for Sustainable
  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
Authors: Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Krovi
Categories: cs.RO cs.LG cs.MA
Comments: arXiv admin note: substantial text overlap with arXiv:2309.10007
\\
  This work presents a sustainable multi-agent deep reinforcement learning
framework capable of selectively scaling parallelized training workloads
on-demand, and transferring the trained policies from simulation to reality
using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an
enabling digital twin framework to train, deploy, and transfer cooperative as
well as competitive multi-agent reinforcement learning policies from simulation
to reality. Particularly, we first investigate an intersection traversal
problem of 4 cooperative vehicles (Nigel) that share limited state information
in single as well as multi-agent learning settings using a common policy
approach. We then investigate an adversarial autonomous racing problem of 2
vehicles (F1TENTH) using an individual policy approach. In either set of
experiments, a decentralized learning architecture was adopted, which allowed
robust training and testing of the policies in stochastic environments. The
agents were provided with realistically sparse observation spaces, and were
restricted to sample control actions that implicitly satisfied the imposed
kinodynamic and safety constraints. The experimental results for both problem
statements are reported in terms of quantitative metrics and qualitative
remarks for training as well as deployment phases. We also discuss agent and
environment parallelization techniques adopted to efficiently accelerate MARL
training, while analyzing their computational performance. Finally, we
demonstrate a resource-aware transition of the trained policies from simulation
to reality using the proposed digital twin framework.
\\ ( https://arxiv.org/abs/2403.10996 ,  4403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11001 (*cross-listing*)
Date: Sat, 16 Mar 2024 19:11:57 GMT   (3762kb,D)

Title: Topologically faithful multi-class segmentation in medical images
Authors: Alexander H. Berger, Nico Stucki, Laurin Lux, Vincent Buergin,
  Suprosanna Shit, Anna Banaszak, Daniel Rueckert, Ulrich Bauer, Johannes C.
  Paetzold
Categories: eess.IV cs.CV cs.LG
\\
  Topological accuracy in medical image segmentation is a highly important
property for downstream applications such as network analysis and flow modeling
in vessels or cell counting. Recently, significant methodological advancements
have brought well-founded concepts from algebraic topology to binary
segmentation. However, these approaches have been underexplored in multi-class
segmentation scenarios, where topological errors are common. We propose a
general loss function for topologically faithful multi-class segmentation
extending the recent Betti matching concept, which is based on induced
matchings of persistence barcodes. We project the N-class segmentation problem
to N single-class segmentation tasks, which allows us to use 1-parameter
persistent homology making training of neural networks computationally
feasible. We validate our method on a comprehensive set of four medical
datasets with highly variant topological characteristics. Our loss formulation
significantly enhances topological correctness in cardiac, cell, artery-vein,
and Circle of Willis segmentation.
\\ ( https://arxiv.org/abs/2403.11001 ,  3762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11048 (*cross-listing*)
Date: Sun, 17 Mar 2024 00:29:42 GMT   (2788kb,D)

Title: JustQ: Automated Deployment of Fair and Accurate Quantum Neural Networks
Authors: Ruhan Wang, Fahiz Baba-Yara, Fan Chen
Categories: quant-ph cs.CY cs.LG
Journal-ref: published at ASP-DAC 2024
\\
  Despite the success of Quantum Neural Networks (QNNs) in decision-making
systems, their fairness remains unexplored, as the focus primarily lies on
accuracy. This work conducts a design space exploration, unveiling QNN
unfairness, and highlighting the significant influence of QNN deployment and
quantum noise on accuracy and fairness. To effectively navigate the vast QNN
deployment design space, we propose JustQ, a framework for deploying fair and
accurate QNNs on NISQ computers. It includes a complete NISQ error model,
reinforcement learning-based deployment, and a flexible optimization objective
incorporating both fairness and accuracy. Experimental results show JustQ
outperforms previous methods, achieving superior accuracy and fairness. This
work pioneers fair QNN design on NISQ computers, paving the way for future
investigations.
\\ ( https://arxiv.org/abs/2403.11048 ,  2788kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11079 (*cross-listing*)
Date: Sun, 17 Mar 2024 04:20:38 GMT   (1531kb,D)

Title: Bridging Expert Knowledge with Deep Learning Techniques for Just-In-Time
  Defect Prediction
Authors: Xin Zhou, DongGyun Han, David Lo
Categories: cs.SE cs.LG
Comments: 48 pages
\\
  Just-In-Time (JIT) defect prediction aims to automatically predict whether a
commit is defective or not, and has been widely studied in recent years. In
general, most studies can be classified into two categories: 1) simple models
using traditional machine learning classifiers with hand-crafted features, and
2) complex models using deep learning techniques to automatically extract
features from commit contents. Hand-crafted features used by simple models are
based on expert knowledge but may not fully represent the semantic meaning of
the commits. On the other hand, deep learning-based features used by complex
models represent the semantic meaning of commits but may not reflect useful
expert knowledge. Simple models and complex models seem complementary to each
other to some extent. To utilize the advantages of both simple and complex
models, we propose a model fusion framework that adopts both early fusions on
the feature level and late fusions on the decision level. We propose SimCom++
by adopting the best early and late fusion strategies. The experimental results
show that SimCom++ can significantly outperform the baselines by 5.7--26.9\%.
In addition, our experimental results confirm that the simple model and complex
model are complementary to each other.
\\ ( https://arxiv.org/abs/2403.11079 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11090 (*cross-listing*)
Date: Sun, 17 Mar 2024 04:59:30 GMT   (5337kb,D)

Title: Brain-on-Switch: Towards Advanced Intelligent Network Data Plane via
  NN-Driven Traffic Analysis at Line-Speed
Authors: Jinzhu Yan, Haotian Xu, Zhuotao Liu, Qi Li, Ke Xu, Mingwei Xu,
  Jianping Wu
Categories: cs.NI cs.LG
Comments: 12 pages body, 22 pages total, 14 figures, accepted by the 21st
  USENIX Symposium on Networked Systems Design and Implementation (NSDI'24)
\\
  The emerging programmable networks sparked significant research on
Intelligent Network Data Plane (INDP), which achieves learning-based traffic
analysis at line-speed. Prior art in INDP focus on deploying tree/forest models
on the data plane. We observe a fundamental limitation in tree-based INDP
approaches: although it is possible to represent even larger tree/forest tables
on the data plane, the flow features that are computable on the data plane are
fundamentally limited by hardware constraints. In this paper, we present BoS to
push the boundaries of INDP by enabling Neural Network (NN) driven traffic
analysis at line-speed. Many types of NNs (such as Recurrent Neural Network
(RNN), and transformers) that are designed to work with sequential data have
advantages over tree-based models, because they can take raw network data as
input without complex feature computations on the fly. However, the challenge
is significant: the recurrent computation scheme used in RNN inference is
fundamentally different from the match-action paradigm used on the network data
plane. BoS addresses this challenge by (i) designing a novel data plane
friendly RNN architecture that can execute unlimited RNN time steps with
limited data plane stages, effectively achieving line-speed RNN inference; and
(ii) complementing the on-switch RNN model with an off-switch transformer-based
traffic analysis module to further boost the overall performance. We implement
a prototype of BoS using a P4 programmable switch as our data plane, and
extensively evaluate it over multiple traffic analysis tasks. The results show
that BoS outperforms state-of-the-art in both analysis accuracy and
scalability.
\\ ( https://arxiv.org/abs/2403.11090 ,  5337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11093 (*cross-listing*)
Date: Sun, 17 Mar 2024 05:07:04 GMT   (641kb,D)

Title: Learning-Based Pricing and Matching for Two-Sided Queues
Authors: Zixian Yang, Lei Ying
Categories: math.OC cs.LG math.PR
Comments: 48 pages, 8 figures
\\
  We consider a dynamic system with multiple types of customers and servers.
Each type of waiting customer or server joins a separate queue, forming a
bipartite graph with customer-side queues and server-side queues. The platform
can match the servers and customers if their types are compatible. The matched
pairs then leave the system. The platform will charge a customer a price
according to their type when they arrive and will pay a server a price
according to their type. The arrival rate of each queue is determined by the
price according to some unknown demand or supply functions. Our goal is to
design pricing and matching algorithms to maximize the profit of the platform
with unknown demand and supply functions, while keeping queue lengths of both
customers and servers below a predetermined threshold. This system can be used
to model two-sided markets such as ride-sharing markets with passengers and
drivers. The difficulties of the problem include simultaneous learning and
decision making, and the tradeoff between maximizing profit and minimizing
queue length. We use a longest-queue-first matching algorithm and propose a
learning-based pricing algorithm, which combines gradient-free stochastic
projected gradient ascent with bisection search. We prove that our proposed
algorithm yields a sublinear regret $\tilde{O}(T^{5/6})$ and queue-length bound
$\tilde{O}(T^{2/3})$, where $T$ is the time horizon. We further establish a
tradeoff between the regret bound and the queue-length bound:
$\tilde{O}(T^{1-\gamma/4})$ versus $\tilde{O}(T^{\gamma})$ for $\gamma \in (0,
2/3].$
\\ ( https://arxiv.org/abs/2403.11093 ,  641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11125 (*cross-listing*)
Date: Sun, 17 Mar 2024 07:17:07 GMT   (8253kb)

Title: Machine learning-based system reliability analysis with Gaussian Process
  Regression
Authors: Lisang Zhou, Ziqian Luo, Xueting Pan
Categories: stat.ML cs.LG math.PR
\\
  Machine learning-based reliability analysis methods have shown great
advancements for their computational efficiency and accuracy. Recently, many
efficient learning strategies have been proposed to enhance the computational
performance. However, few of them explores the theoretical optimal learning
strategy. In this article, we propose several theorems that facilitates such
exploration. Specifically, cases that considering and neglecting the
correlations among the candidate design samples are well elaborated. Moreover,
we prove that the well-known U learning function can be reformulated to the
optimal learning function for the case neglecting the Kriging correlation. In
addition, the theoretical optimal learning strategy for sequential multiple
training samples enrichment is also mathematically explored through the
Bayesian estimate with the corresponding lost functions. Simulation results
show that the optimal learning strategy considering the Kriging correlation
works better than that neglecting the Kriging correlation and other
state-of-the art learning functions from the literatures in terms of the
reduction of number of evaluations of performance function. However, the
implementation needs to investigate very large computational resource.
\\ ( https://arxiv.org/abs/2403.11125 ,  8253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11163 (*cross-listing*)
Date: Sun, 17 Mar 2024 10:09:54 GMT   (46kb)

Title: A Selective Review on Statistical Methods for Massive Data Computation:
  Distributed Computing, Subsampling, and Minibatch Techniques
Authors: Xuetong Li, Yuan Gao, Hong Chang, Danyang Huang, Yingying Ma, Rui Pan,
  Haobo Qi, Feifei Wang, Shuyuan Wu, Ke Xu, Jing Zhou, Xuening Zhu, Yingqiu
  Zhu, Hansheng Wang
Categories: stat.ME cs.LG math.ST stat.CO stat.TH
\\
  This paper presents a selective review of statistical computation methods for
massive data analysis. A huge amount of statistical methods for massive data
computation have been rapidly developed in the past decades. In this work, we
focus on three categories of statistical computation methods: (1) distributed
computing, (2) subsampling methods, and (3) minibatch gradient techniques. The
first class of literature is about distributed computing and focuses on the
situation, where the dataset size is too huge to be comfortably handled by one
single computer. In this case, a distributed computation system with multiple
computers has to be utilized. The second class of literature is about
subsampling methods and concerns about the situation, where the sample size of
dataset is small enough to be placed on one single computer but too large to be
easily processed by its memory as a whole. The last class of literature studies
those minibatch gradient related optimization techniques, which have been
extensively used for optimizing various deep learning models.
\\ ( https://arxiv.org/abs/2403.11163 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11166 (*cross-listing*)
Date: Sun, 17 Mar 2024 10:26:41 GMT   (798kb,D)

Title: Pencil: Private and Extensible Collaborative Learning without the
  Non-Colluding Assumption
Authors: Xuanqi Liu, Zhuotao Liu, Qi Li, Ke Xu, Mingwei Xu
Categories: cs.CR cs.LG
Comments: Network and Distributed System Security Symposium (NDSS) 2024
Journal-ref: Proceedings 2024 Network and Distributed System Security Symposium
  (2024)
\\
  The escalating focus on data privacy poses significant challenges for
collaborative neural network training, where data ownership and model
training/deployment responsibilities reside with distinct entities. Our
community has made substantial contributions to addressing this challenge,
proposing various approaches such as federated learning (FL) and
privacy-preserving machine learning based on cryptographic constructs like
homomorphic encryption (HE) and secure multiparty computation (MPC). However,
FL completely overlooks model privacy, and HE has limited extensibility
(confined to only one data provider). While the state-of-the-art MPC frameworks
provide reasonable throughput and simultaneously ensure model/data privacy,
they rely on a critical non-colluding assumption on the computing servers, and
relaxing this assumption is still an open problem.
  In this paper, we present Pencil, the first private training framework for
collaborative learning that simultaneously offers data privacy, model privacy,
and extensibility to multiple data providers, without relying on the
non-colluding assumption. Our fundamental design principle is to construct the
n-party collaborative training protocol based on an efficient two-party
protocol, and meanwhile ensuring that switching to different data providers
during model training introduces no extra cost. We introduce several novel
cryptographic protocols to realize this design principle and conduct a rigorous
security and privacy analysis. Our comprehensive evaluations of Pencil
demonstrate that (i) models trained in plaintext and models trained privately
using Pencil exhibit nearly identical test accuracies; (ii) The training
overhead of Pencil is greatly reduced: Pencil achieves 10 ~ 260x higher
throughput and 2 orders of magnitude less communication than prior art; (iii)
Pencil is resilient against both existing and adaptive (white-box) attacks.
\\ ( https://arxiv.org/abs/2403.11166 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11173 (*cross-listing*)
Date: Sun, 17 Mar 2024 11:19:45 GMT   (460kb,D)

Title: Multi-Objective Evolutionary Neural Architecture Search for Recurrent
  Neural Networks
Authors: Reinhard Booysen, Anna Sergeevna Bosman
Categories: cs.NE cs.LG
\\
  Artificial neural network (NN) architecture design is a nontrivial and
time-consuming task that often requires a high level of human expertise. Neural
architecture search (NAS) serves to automate the design of NN architectures and
has proven to be successful in automatically finding NN architectures that
outperform those manually designed by human experts. NN architecture
performance can be quantified based on multiple objectives, which include model
accuracy and some NN architecture complexity objectives, among others. The
majority of modern NAS methods that consider multiple objectives for NN
architecture performance evaluation are concerned with automated feed forward
NN architecture design, which leaves multi-objective automated recurrent neural
network (RNN) architecture design unexplored. RNNs are important for modeling
sequential datasets, and prominent within the natural language processing
domain. It is often the case in real world implementations of machine learning
and NNs that a reasonable trade-off is accepted for marginally reduced model
accuracy in favour of lower computational resources demanded by the model. This
paper proposes a multi-objective evolutionary algorithm-based RNN architecture
search method. The proposed method relies on approximate network morphisms for
RNN architecture complexity optimisation during evolution. The results show
that the proposed method is capable of finding novel RNN architectures with
comparable performance to state-of-the-art manually designed RNN architectures,
but with reduced computational demand.
\\ ( https://arxiv.org/abs/2403.11173 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11180 (*cross-listing*)
Date: Sun, 17 Mar 2024 11:49:57 GMT   (12201kb,D)

Title: usfAD Based Effective Unknown Attack Detection Focused IDS Framework
Authors: Md. Ashraf Uddin, Sunil Aryal, Mohamed Reda Bouadjenek, Muna
  Al-Hawawreh, Md. Alamin Talukder
Categories: cs.CR cs.LG
Comments: Deakin University, Australia | This material is based upon work
  supported by the Air Force Office of Scientific Research under award number
  FA2386-23-1-4003
\\
  The rapid expansion of varied network systems, including the Internet of
Things (IoT) and Industrial Internet of Things (IIoT), has led to an increasing
range of cyber threats. Ensuring robust protection against these threats
necessitates the implementation of an effective Intrusion Detection System
(IDS). For more than a decade, researchers have delved into supervised machine
learning techniques to develop IDS to classify normal and attack traffic.
However, building effective IDS models using supervised learning requires a
substantial number of benign and attack samples. To collect a sufficient number
of attack samples from real-life scenarios is not possible since cyber attacks
occur occasionally. Further, IDS trained and tested on known datasets fails in
detecting zero-day or unknown attacks due to the swift evolution of attack
patterns. To address this challenge, we put forth two strategies for
semi-supervised learning based IDS where training samples of attacks are not
required: 1) training a supervised machine learning model using randomly and
uniformly dispersed synthetic attack samples; 2) building a One Class
Classification (OCC) model that is trained exclusively on benign network
traffic. We have implemented both approaches and compared their performances
using 10 recent benchmark IDS datasets. Our findings demonstrate that the OCC
model based on the state-of-art anomaly detection technique called usfAD
significantly outperforms conventional supervised classification and other OCC
based techniques when trained and tested considering real-life scenarios,
particularly to detect previously unseen attacks.
\\ ( https://arxiv.org/abs/2403.11180 ,  12201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11230 (*cross-listing*)
Date: Sun, 17 Mar 2024 14:34:51 GMT   (10438kb,D)

Title: Simple 2D Convolutional Neural Network-based Approach for COVID-19
  Detection
Authors: Chih-Chung Hsu, Chia-Ming Lee, Yang Fan Chiang, Yi-Shiuan Chou,
  Chih-Yu Jiang, Shen-Chieh Tai, Chi-Han Tsai
Categories: eess.IV cs.CV cs.LG
\\
  This study explores the use of deep learning techniques for analyzing lung
Computed Tomography (CT) images. Classic deep learning approaches face
challenges with varying slice counts and resolutions in CT images, a diversity
arising from the utilization of assorted scanning equipment. Typically,
predictions are made on single slices which are then combined for a
comprehensive outcome. Yet, this method does not incorporate learning features
specific to each slice, leading to a compromise in effectiveness. To address
these challenges, we propose an advanced Spatial-Slice Feature Learning
(SSFL++) framework specifically tailored for CT scans. It aims to filter out
out-of-distribution (OOD) data within the entire CT scan, allowing us to select
essential spatial-slice features for analysis by reducing data redundancy by
70\%. Additionally, we introduce a Kernel-Density-based slice Sampling (KDS)
method to enhance stability during training and inference phases, thereby
accelerating convergence and enhancing overall performance. Remarkably, our
experiments reveal that our model achieves promising results with a simple
EfficientNet-2D (E2D) model. The effectiveness of our approach is confirmed on
the COVID-19-CT-DB datasets provided by the DEF-AI-MIA workshop.
\\ ( https://arxiv.org/abs/2403.11230 ,  10438kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11291 (*cross-listing*)
Date: Sun, 17 Mar 2024 18:06:06 GMT   (1120kb)

Title: Advanced Knowledge Extraction of Physical Design Drawings, Translation
  and conversion to CAD formats using Deep Learning
Authors: Jesher Joshua M, Ragav V, Syed Ibrahim S P
Categories: cs.CV cs.LG
\\
  The maintenance, archiving and usage of the design drawings is cumbersome in
physical form in different industries for longer period. It is hard to extract
information by simple scanning of drawing sheets. Converting them to their
digital formats such as Computer-Aided Design (CAD), with needed knowledge
extraction can solve this problem. The conversion of these machine drawings to
its digital form is a crucial challenge which requires advanced techniques.
This research proposes an innovative methodology utilizing Deep Learning
methods. The approach employs object detection model, such as Yolov7, Faster
R-CNN, to detect physical drawing objects present in the images followed by,
edge detection algorithms such as canny filter to extract and refine the
identified lines from the drawing region and curve detection techniques to
detect circle. Also ornaments (complex shapes) within the drawings are
extracted. To ensure comprehensive conversion, an Optical Character Recognition
(OCR) tool is integrated to identify and extract the text elements from the
drawings. The extracted data which includes the lines, shapes and text is
consolidated and stored in a structured comma separated values(.csv) file
format. The accuracy and the efficiency of conversion is evaluated. Through
this, conversion can be automated to help organizations enhance their
productivity, facilitate seamless collaborations and preserve valuable design
information in a digital format easily accessible. Overall, this study
contributes to the advancement of CAD conversions, providing accurate results
from the translating process. Future research can focus on handling diverse
drawing types, enhanced accuracy in shape and line detection and extraction.
\\ ( https://arxiv.org/abs/2403.11291 ,  1120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11331 (*cross-listing*)
Date: Sun, 17 Mar 2024 20:23:06 GMT   (327kb)

Title: Potential of Domain Adaptation in Machine Learning in Ecology and
  Hydrology to Improve Model Extrapolability
Authors: Haiyang Shi
Categories: physics.geo-ph cs.LG physics.data-an
\\
  Due to the heterogeneity of the global distribution of ecological and
hydrological ground-truth observations, machine learning models can have
limited adaptability when applied to unknown locations, which is referred to as
weak extrapolability. Domain adaptation techniques have been widely used in
machine learning domains such as image classification, which can improve the
model generalization ability by adjusting the difference or inconsistency of
the domain distribution between the training and test sets. However, this
approach has rarely been used explicitly in machine learning models in ecology
and hydrology at the global scale, although these models have often been
questioned due to geographic extrapolability issues. This paper briefly
describes the shortcomings of current machine learning models of ecology and
hydrology in terms of the global representativeness of the distribution of
observations and the resulting limitations of the lack of extrapolability and
suggests that future related modelling efforts should consider the use of
domain adaptation techniques to improve extrapolability.
\\ ( https://arxiv.org/abs/2403.11331 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11338 (*cross-listing*)
Date: Sun, 17 Mar 2024 20:44:38 GMT   (10566kb,D)

Title: Ensembling and Test Augmentation for Covid-19 Detection and Covid-19
  Domain Adaptation from 3D CT-Scans
Authors: Fares Bougourzi, Feryal Windal Moula, Halim Benhabiles, Fadi Dornaika,
  Abdelmalik Taleb-Ahmed
Categories: eess.IV cs.CV cs.LG
\\
  Since the emergence of Covid-19 in late 2019, medical image analysis using
artificial intelligence (AI) has emerged as a crucial research area,
particularly with the utility of CT-scan imaging for disease diagnosis. This
paper contributes to the 4th COV19D competition, focusing on Covid-19 Detection
and Covid-19 Domain Adaptation Challenges. Our approach centers on lung
segmentation and Covid-19 infection segmentation employing the recent CNN-based
segmentation architecture PDAtt-Unet, which simultaneously segments lung
regions and infections. Departing from traditional methods, we concatenate the
input slice (grayscale) with segmented lung and infection, generating three
input channels akin to color channels. Additionally, we employ three 3D CNN
backbones Customized Hybrid-DeCoVNet, along with pretrained 3D-Resnet-18 and
3D-Resnet-50 models to train Covid-19 recognition for both challenges.
Furthermore, we explore ensemble approaches and testing augmentation to enhance
performance. Comparison with baseline results underscores the substantial
efficiency of our approach, with a significant margin in terms of F1-score (14
%). This study advances the field by presenting a comprehensive methodology for
accurate Covid-19 detection and adaptation, leveraging cutting-edge AI
techniques in medical image analysis.
\\ ( https://arxiv.org/abs/2403.11338 ,  10566kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11350 (*cross-listing*)
Date: Sun, 17 Mar 2024 21:35:45 GMT   (81kb,D)

Title: Robustness of the data-driven approach in limited angle tomography
Authors: Yiran Wang, Yimin Zhong
Categories: math.NA cs.LG cs.NA
MSC-class: 35R30
\\
  The limited angle Radon transform is notoriously difficult to invert due to
the ill-posedness. In this work, we give a mathematical explanation that the
data-driven approach based on deep neural networks can reconstruct more
information in a stable way compared to traditional methods.
\\ ( https://arxiv.org/abs/2403.11350 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11351 (*cross-listing*)
Date: Sun, 17 Mar 2024 21:43:19 GMT   (490kb,D)

Title: An SDP-based Branch-and-Cut Algorithm for Biclustering
Authors: Antonio M. Sudoso
Categories: math.OC cs.LG stat.ML
\\
  Biclustering, also called co-clustering, block clustering, or two-way
clustering, involves the simultaneous clustering of both the rows and columns
of a data matrix into distinct groups, such that the rows and columns within a
group display similar patterns. As a model problem for biclustering, we
consider the $k$-densest-disjoint biclique problem, whose goal is to identify
$k$ disjoint complete bipartite subgraphs (called bicliques) of a given
weighted complete bipartite graph such that the sum of their densities is
maximized. To address this problem, we present a tailored branch-and-cut
algorithm. For the upper bound routine, we consider a semidefinite programming
relaxation and propose valid inequalities to strengthen the bound. We solve
this relaxation in a cutting-plane fashion using a first-order method. For the
lower bound, we design a maximum weight matching rounding procedure that
exploits the solution of the relaxation solved at each node. Computational
results on both synthetic and real-world instances show that the proposed
algorithm can solve instances approximately 20 times larger than those handled
by general-purpose solvers.
\\ ( https://arxiv.org/abs/2403.11351 ,  490kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11375 (*cross-listing*)
Date: Mon, 18 Mar 2024 00:02:48 GMT   (47kb,D)

Title: Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival
  Outcome Prediction
Authors: Hongxiao Wang, Yang Yang, Zhuo Zhao, Pengfei Gu, Nishchal Sapkota,
  Danny Z. Chen
Categories: cs.CV cs.LG q-bio.GN
Comments: Accepted by IEEE International Symposium on Biomedical Imaging (ISBI
  2024)
\\
  For predicting cancer survival outcomes, standard approaches in clinical
research are often based on two main modalities: pathology images for observing
cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene
expressions. However, existing pathology-genomic multi-modal algorithms face
significant challenges: (1) Valuable biological insights regarding genes and
gene-gene interactions are frequently overlooked; (2) one modality often
dominates the optimization process, causing inadequate training for the other
modality. In this paper, we introduce a new multi-modal ``Path-GPTOmic"
framework for cancer survival outcome prediction. First, to extract valuable
biological insights, we regulate the embedding space of a foundation model,
scGPT, initially trained on single-cell RNA-seq data, making it adaptable for
bulk RNA-seq data. Second, to address the imbalance-between-modalities problem,
we propose a gradient modulation mechanism tailored to the Cox partial
likelihood loss for survival prediction. The contributions of the modalities
are dynamically monitored and adjusted during the training process, encouraging
that both modalities are sufficiently trained. Evaluated on two TCGA(The Cancer
Genome Atlas) datasets, our model achieves substantially improved survival
prediction accuracy.
\\ ( https://arxiv.org/abs/2403.11375 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11385 (*cross-listing*)
Date: Mon, 18 Mar 2024 00:22:33 GMT   (1150kb,D)

Title: Stochastic approach for elliptic problems in perforated domains
Authors: Jihun Han, Yoonsang Lee
Categories: math.NA cs.CE cs.LG cs.NA math.PR
Comments: 18 pages, 6 figures
MSC-class: 65N99, 65C05, 68T07
\\
  A wide range of applications in science and engineering involve a PDE model
in a domain with perforations, such as perforated metals or air filters.
Solving such perforated domain problems suffers from computational challenges
related to resolving the scale imposed by the geometries of perforations. We
propose a neural network-based mesh-free approach for perforated domain
problems. The method is robust and efficient in capturing various configuration
scales, including the averaged macroscopic behavior of the solution that
involves a multiscale nature induced by small perforations. The new approach
incorporates the derivative-free loss method that uses a stochastic
representation or the Feynman-Kac formulation. In particular, we implement the
Neumann boundary condition for the derivative-free loss method to handle the
interface between the domain and perforations. A suite of stringent numerical
tests is provided to support the proposed method's efficacy in handling various
perforation scales.
\\ ( https://arxiv.org/abs/2403.11385 ,  1150kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11407 (*cross-listing*)
Date: Mon, 18 Mar 2024 01:47:24 GMT   (26695kb,D)

Title: Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors
Authors: Yazid Janati, Alain Durmus, Eric Moulines, Jimmy Olsson
Categories: eess.IV cs.LG stat.ML
Comments: preprint
\\
  Interest in the use of Denoising Diffusion Models (DDM) as priors for solving
inverse Bayesian problems has recently increased significantly. However,
sampling from the resulting posterior distribution poses a challenge. To solve
this problem, previous works have proposed approximations to bias the drift
term of the diffusion. In this work, we take a different approach and utilize
the specific structure of the DDM prior to define a set of intermediate and
simpler posterior sampling problems, resulting in a lower approximation error
compared to previous methods. We empirically demonstrate the reconstruction
capability of our method for general linear inverse problems using synthetic
examples and various image restoration tasks.
\\ ( https://arxiv.org/abs/2403.11407 ,  26695kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11446 (*cross-listing*)
Date: Mon, 18 Mar 2024 03:44:55 GMT   (3644kb,D)

Title: LLM Guided Evolution - The Automation of Models Advancing Models
Authors: Clint Morris, Michael Jurado, and Jason Zutty
Categories: cs.NE cs.LG
\\
  In the realm of machine learning, traditional model development and automated
approaches like AutoML typically rely on layers of abstraction, such as
tree-based or Cartesian genetic programming. Our study introduces "Guided
Evolution" (GE), a novel framework that diverges from these methods by
utilizing Large Language Models (LLMs) to directly modify code. GE leverages
LLMs for a more intelligent, supervised evolutionary process, guiding mutations
and crossovers. Our unique "Evolution of Thought" (EoT) technique further
enhances GE by enabling LLMs to reflect on and learn from the outcomes of
previous mutations. This results in a self-sustaining feedback loop that
augments decision-making in model evolution. GE maintains genetic diversity,
crucial for evolutionary algorithms, by leveraging LLMs' capability to generate
diverse responses from expertly crafted prompts and modulate model temperature.
This not only accelerates the evolution process but also injects expert like
creativity and insight into the process. Our application of GE in evolving the
ExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously
produced variants with improved accuracy, increasing from 92.52% to 93.34%,
without compromising model compactness. This underscores the potential of LLMs
to accelerate the traditional model design pipeline, enabling models to
autonomously evolve and enhance their own designs.
\\ ( https://arxiv.org/abs/2403.11446 ,  3644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11497 (*cross-listing*)
Date: Mon, 18 Mar 2024 06:04:02 GMT   (1777kb,D)

Title: Do CLIPs Always Generalize Better than ImageNet Models?
Authors: Qizhou Wang, Yong Lin, Yongqiang Chen, Ludwig Schmidt, Bo Han, Tong
  Zhang
Categories: cs.CV cs.LG stat.ML
Comments: Qizhou Wang, Yong Lin, and Yongqiang Chen contributed equally.
  Project page: https://counteranimal.github.io
\\
  Large vision language models, such as CLIPs, have revolutionized modern
machine learning. CLIPs have demonstrated great generalizability under
distribution shifts, supported by an increasing body of literature. However,
the evaluation datasets for CLIPs are variations primarily designed for
ImageNet benchmarks, which may not fully reflect the extent to which CLIPs,
e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap,
we collect a real-world dataset called CounterAnimal that contains realistic
spurious features found in animal photos. CounterAnimal consists of a) the
common group: comprising animals on common backgrounds, and b) the counter
group: including animals on unusual backgrounds. The performance drops from the
common to counter groups quantify the reliance of models on spurious features
(i.e., backgrounds) to predict the animals. We find that CLIPs trained on
either LAION or the OpenAI data exhibit notable performance drops on the
counter group. Surprisingly, we observe that single-modal models trained on
ImageNet are more robust than CLIPs. We provide both theoretical and empirical
explanations for why CLIPs still learn spurious features. Our findings suggest
that distribution shifts remain an open problem for CLIPs, and one needs to be
cautious about test setups when evaluating foundation models pre-trained on a
significantly different scale and distribution.
\\ ( https://arxiv.org/abs/2403.11497 ,  1777kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11505 (*cross-listing*)
Date: Mon, 18 Mar 2024 06:20:49 GMT   (1800kb,D)

Title: Covid-19 detection from CT scans using EfficientNet and Attention
  mechanism
Authors: Ramy Farag, Parth Upadhyay, Guilhermen DeSouza
Categories: eess.IV cs.CV cs.LG
\\
  Manual diagnosis and analysis of COVID-19 through the examination of lung
Computed Tomography (CT) scan images by physicians tends to result in
inefficiency, especially with high patient volumes and numerous images per
patient. We address the need for automation by developing a deep learning
model-based pipeline for COVID-19 detection from CT scan images of the lungs.
The Domain adaptation, Explainability, and Fairness in AI for Medical Image
Analysis Workshop and COVID-19 Diagnosis Competition (DEF-AI-MIA COV19D)
provides an opportunity to assess our designed pipeline for COVID-19 detection
from CT scan images. The proposed pipeline incorporates EfficientNet with an
Attention mechanism with a pre-processing step. Our pipeline outperforms last
year's teams on the validation set of the competition dataset.
\\ ( https://arxiv.org/abs/2403.11505 ,  1800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11521 (*cross-listing*)
Date: Mon, 18 Mar 2024 07:15:01 GMT   (1422kb)

Title: A Data-driven Approach for Rapid Detection of Aeroelastic Modes from
  Flutter Flight Test Based on Limited Sensor Measurements
Authors: Arpan Das, Pier Marzocca, Giuliano Coppotelli, Oleg Levinski, and Paul
  Taylor
Categories: eess.SP cs.LG cs.NA math.DS math.NA
Comments: 31 pages, 12 figures, submitted in 'Mechanical Systems and Signal
  processing' journal
\\
  Flutter flight test involves the evaluation of the airframes aeroelastic
stability by applying artificial excitation on the aircraft lifting surfaces.
The subsequent responses are captured and analyzed to extract the frequencies
and damping characteristics of the system. However, noise contamination,
turbulence, non-optimal excitation of modes, and sensor malfunction in one or
more sensors make it time-consuming and corrupt the extraction process. In
order to expedite the process of identifying and analyzing aeroelastic modes,
this study implements a time-delay embedded Dynamic Mode Decomposition
technique. This approach is complemented by Robust Principal Component Analysis
methodology, and a sparsity promoting criterion which enables the automatic and
optimal selection of sparse modes. The anonymized flutter flight test data,
provided by the fifth author of this research paper, is utilized in this
implementation. The methodology assumes no knowledge of the input excitation,
only deals with the responses captured by accelerometer channels, and rapidly
identifies the aeroelastic modes. By incorporating a compressed sensing
algorithm, the methodology gains the ability to identify aeroelastic modes,
even when the number of available sensors is limited. This augmentation greatly
enhances the methodology's robustness and effectiveness, making it an excellent
choice for real-time implementation during flutter test campaigns.
\\ ( https://arxiv.org/abs/2403.11521 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11522 (*cross-listing*)
Date: Mon, 18 Mar 2024 07:22:31 GMT   (1073kb)

Title: LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers
Authors: Massinissa Merouani, Khaled Afif Boudaoud, Iheb Nassim Aouadj, Nassim
  Tchoulak, Islam Kara Bernou, Hamza Benyamina, Fatima Benbouzid-Si Tayeb,
  Karima Benatchba, Hugh Leather, Riyadh Baghdadi
Categories: cs.PL cs.DC cs.LG
\\
  While polyhedral compilers have shown success in implementing advanced code
transformations, they still have challenges in selecting the most profitable
transformations that lead to the best speedups. This has motivated the use of
machine learning to build cost models to guide the search for polyhedral
optimizations. State-of-the-art polyhedral compilers have demonstrated a viable
proof-of-concept of this approach. While such a proof-of-concept has shown
promise, it still has significant limitations. State-of-the-art polyhedral
compilers that use a deep-learning cost model only support a small subset of
affine transformations, limiting their ability to apply complex code
transformations. They also only support simple programs that have a single loop
nest and a rectangular iteration domain, limiting their applicability to many
programs. These limitations significantly impact the generality of such
compilers and autoschedulers and put into question the whole approach. In this
paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a
deep-learning based cost model and covers a large set of affine transformations
and programs. It supports the exploration of a large set of affine
transformations, allowing the application of complex sequences of polyhedral
transformations. It also supports the optimization of programs with multiple
loop nests and with rectangular and non-rectangular iteration domains, allowing
the optimization of an extensive set of programs. We implement and evaluate
LOOPer and show that it achieves speedups over the state-of-the-art. On the
Polybench benchmark, LOOPer achieves a geometric mean speedup of 1.59x over
Tiramisu. LOOPer also achieves competitive speedups with a geometric mean
speedup of 1.34x over Pluto, a state-of-the-art polyhedral compiler that does
not use a machine-learning based cost model.
\\ ( https://arxiv.org/abs/2403.11522 ,  1073kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11532 (*cross-listing*)
Date: Mon, 18 Mar 2024 07:35:25 GMT   (1016kb,D)

Title: Out-of-Distribution Detection Should Use Conformal Prediction (and
  Vice-versa?)
Authors: Paul Novello, Joseba Dalmau, L\'eo Andeol
Categories: stat.ML cs.CV cs.LG
\\
  Research on Out-Of-Distribution (OOD) detection focuses mainly on building
scores that efficiently distinguish OOD data from In Distribution (ID) data. On
the other hand, Conformal Prediction (CP) uses non-conformity scores to
construct prediction sets with probabilistic coverage guarantees. In this work,
we propose to use CP to better assess the efficiency of OOD scores.
Specifically, we emphasize that in standard OOD benchmark settings, evaluation
metrics can be overly optimistic due to the finite sample size of the test
dataset. Based on the work of (Bates et al., 2022), we define new conformal
AUROC and conformal FRP@TPR95 metrics, which are corrections that provide
probabilistic conservativeness guarantees on the variability of these metrics.
We show the effect of these corrections on two reference OOD and anomaly
detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al.,
2022). We also show that the benefits of using OOD together with CP apply the
other way around by using OOD scores as non-conformity scores, which results in
improving upon current CP methods. One of the key messages of these
contributions is that since OOD is concerned with designing scores and CP with
interpreting these scores, the two fields may be inherently intertwined.
\\ ( https://arxiv.org/abs/2403.11532 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11537 (*cross-listing*)
Date: Mon, 18 Mar 2024 07:43:14 GMT   (3174kb,D)

Title: Semantic Prompting with Image-Token for Continual Learning
Authors: Jisu Han, Jaemin Na, Wonjun Hwang
Categories: cs.CV cs.LG
\\
  Continual learning aims to refine model parameters for new tasks while
retaining knowledge from previous tasks. Recently, prompt-based learning has
emerged to leverage pre-trained models to be prompted to learn subsequent tasks
without the reliance on the rehearsal buffer. Although this approach has
demonstrated outstanding results, existing methods depend on preceding
task-selection process to choose appropriate prompts. However, imperfectness in
task-selection may lead to negative impacts on the performance particularly in
the scenarios where the number of tasks is large or task distributions are
imbalanced. To address this issue, we introduce I-Prompt, a task-agnostic
approach focuses on the visual semantic information of image tokens to
eliminate task prediction. Our method consists of semantic prompt matching,
which determines prompts based on similarities between tokens, and image
token-level prompting, which applies prompts directly to image tokens in the
intermediate layers. Consequently, our method achieves competitive performance
on four benchmarks while significantly reducing training time compared to
state-of-the-art methods. Moreover, we demonstrate the superiority of our
method across various scenarios through extensive experiments.
\\ ( https://arxiv.org/abs/2403.11537 ,  3174kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11563 (*cross-listing*)
Date: Mon, 18 Mar 2024 08:33:56 GMT   (23563kb,D)

Title: Advancing Neuromorphic Computing: Mixed-Signal Design Techniques
  Leveraging Brain Code Units and Fundamental Code Units
Authors: Murat Isik, Sols Miziev, Wiktoria Pawlak, Newton Howard
Categories: cs.AR cs.LG cs.NE
Comments: Accepted at 2024 International Joint Conference on Neural Networks
\\
  This paper introduces a groundbreaking digital neuromorphic architecture that
innovatively integrates Brain Code Unit (BCU) and Fundamental Code Unit (FCU)
using mixedsignal design methodologies. Leveraging open-source datasets and the
latest advances in materials science, our research focuses on enhancing the
computational efficiency, accuracy, and adaptability of neuromorphic systems.
The core of our approach lies in harmonizing the precision and scalability of
digital systems with the robustness and energy efficiency of analog processing.
Through experimentation, we demonstrate the effectiveness of our system across
various metrics. The BCU achieved an accuracy of 88.0% and a power efficiency
of 20.0 GOP/s/W, while the FCU recorded an accuracy of 86.5% and a power
efficiency of 18.5 GOP/s/W. Our mixed-signal design approach significantly
improved latency and throughput, achieving a latency as low as 0.75 ms and
throughput up to 213 TOP/s. These results firmly establish the potential of our
architecture in neuromorphic computing, providing a solid foundation for future
developments in this domain. Our study underscores the feasibility of
mixedsignal neuromorphic systems and their promise in advancing the field,
particularly in applications requiring high efficiency and adaptability
\\ ( https://arxiv.org/abs/2403.11563 ,  23563kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11565 (*cross-listing*)
Date: Mon, 18 Mar 2024 08:35:17 GMT   (217kb,D)

Title: Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex
  Optimization
Authors: Siyuan Zhang, Nachuan Xiao, Xin Liu
Categories: math.OC cs.LG
Comments: 23 pages
\\
  In this paper, we concentrate on decentralized optimization problems with
nonconvex and nonsmooth objective functions, especially on the decentralized
training of nonsmooth neural networks. We introduce a unified framework, named
DSM, to analyze the global convergence of decentralized stochastic subgradient
methods. We prove the global convergence of our proposed framework under mild
conditions, by establishing that the generated sequence asymptotically
approximates the trajectories of its associated differential inclusion.
Furthermore, we establish that our proposed framework encompasses a wide range
of existing efficient decentralized subgradient methods, including
decentralized stochastic subgradient descent (DSGD), DSGD with
gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In
addition, we introduce SignSGD employing the sign map to regularize the update
directions in DSGDm, and show it is enclosed in our proposed framework.
Consequently, our convergence results establish, for the first time, global
convergence of these methods when applied to nonsmooth nonconvex objectives.
Preliminary numerical experiments demonstrate that our proposed framework
yields highly efficient decentralized subgradient methods with convergence
guarantees in the training of nonsmooth neural networks.
\\ ( https://arxiv.org/abs/2403.11565 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11591 (*cross-listing*)
Date: Mon, 18 Mar 2024 09:10:39 GMT   (18280kb,D)

Title: A physics-informed neural network method for the approximation of slow
  invariant manifolds for the general class of stiff systems of ODEs
Authors: Dimitrios G. Patsatzis, Lucia Russo, Constantinos Siettos
Categories: math.NA cs.LG cs.NA math.DS
MSC-class: 65P99, 65L04, 37M21, 68T07
\\
  We present a physics-informed neural network (PINN) approach for the
discovery of slow invariant manifolds (SIMs), for the most general class of
fast/slow dynamical systems of ODEs. In contrast to other machine learning (ML)
approaches that construct reduced order black box surrogate models using simple
regression, and/or require a priori knowledge of the fast and slow variables,
our approach, simultaneously decomposes the vector field into fast and slow
components and provides a functional of the underlying SIM in a closed form.
The decomposition is achieved by finding a transformation of the state
variables to the fast and slow ones, which enables the derivation of an
explicit, in terms of fast variables, SIM functional. The latter is obtained by
solving a PDE corresponding to the invariance equation within the Geometric
Singular Perturbation Theory (GSPT) using a single-layer feedforward neural
network with symbolic differentiation. The performance of the proposed
physics-informed ML framework is assessed via three benchmark problems: the
Michaelis-Menten, the target mediated drug disposition (TMDD) reaction model
and a fully competitive substrate-inhibitor(fCSI) mechanism. We also provide a
comparison with other GPST methods, namely the quasi steady state approximation
(QSSA), the partial equilibrium approximation (PEA) and CSP with one and two
iterations. We show that the proposed PINN scheme provides SIM approximations,
of equivalent or even higher accuracy, than those provided by QSSA, PEA and
CSP, especially close to the boundaries of the underlying SIMs.
\\ ( https://arxiv.org/abs/2403.11591 ,  18280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11603 (*cross-listing*)
Date: Mon, 18 Mar 2024 09:25:59 GMT   (3300kb,D)

Title: Fair Distributed Cooperative Bandit Learning on Networks for Intelligent
  Internet of Things Systems (Technical Report)
Authors: Ziqun Chen, Kechao Cai, Jinbei Zhang, Zhigang Yu
Categories: cs.DC cs.LG
Comments: 10 pages, 8 figures, conference technical report
\\
  In intelligent Internet of Things (IoT) systems, edge servers within a
network exchange information with their neighbors and collect data from sensors
to complete delivered tasks. In this paper, we propose a multiplayer
multi-armed bandit model for intelligent IoT systems to facilitate data
collection and incorporate fairness considerations. In our model, we establish
an effective communication protocol that helps servers cooperate with their
neighbors. Then we design a distributed cooperative bandit algorithm, DC-ULCB,
enabling servers to collaboratively select sensors to maximize data rates while
maintaining fairness in their choices. We conduct an analysis of the reward
regret and fairness regret of DC-ULCB, and prove that both regrets have
logarithmic instance-dependent upper bounds. Additionally, through extensive
simulations, we validate that DC-ULCB outperforms existing algorithms in
maximizing reward and ensuring fairness.
\\ ( https://arxiv.org/abs/2403.11603 ,  3300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11624 (*cross-listing*)
Date: Mon, 18 Mar 2024 09:56:00 GMT   (1946kb,D)

Title: Dual-Channel Multiplex Graph Neural Networks for Recommendation
Authors: Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu
  Dong, Yanwei Yu
Categories: cs.IR cs.LG
\\
  Efficient recommender systems play a crucial role in accurately capturing
user and item attributes that mirror individual preferences. Some existing
recommendation techniques have started to shift their focus towards modeling
various types of interaction relations between users and items in real-world
recommendation scenarios, such as clicks, marking favorites, and purchases on
online shopping platforms. Nevertheless, these approaches still grapple with
two significant shortcomings: (1) Insufficient modeling and exploitation of the
impact of various behavior patterns formed by multiplex relations between users
and items on representation learning, and (2) ignoring the effect of different
relations in the behavior patterns on the target relation in recommender system
scenarios. In this study, we introduce a novel recommendation framework,
Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the
aforementioned challenges. It incorporates an explicit behavior pattern
representation learner to capture the behavior patterns composed of multiplex
user-item interaction relations, and includes a relation chain representation
learning and a relation chain-aware encoder to discover the impact of various
auxiliary relations on the target relation, the dependencies between different
relations, and mine the appropriate order of relations in a behavior pattern.
Extensive experiments on three real-world datasets demonstrate that our \model
surpasses various state-of-the-art recommendation methods. It outperforms the
best baselines by 10.06\% and 12.15\% on average across all datasets in terms
of R@10 and N@10 respectively.
\\ ( https://arxiv.org/abs/2403.11624 ,  1946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11643 (*cross-listing*)
Date: Mon, 18 Mar 2024 10:35:15 GMT   (2472kb,D)

Title: Diffusion-Based Environment-Aware Trajectory Prediction
Authors: Theodor Westny and Bj\"orn Olofsson and Erik Frisk
Categories: cs.CV cs.LG cs.RO
\\
  The ability to predict the future trajectories of traffic participants is
crucial for the safe and efficient operation of autonomous vehicles. In this
paper, a diffusion-based generative model for multi-agent trajectory prediction
is proposed. The model is capable of capturing the complex interactions between
traffic participants and the environment, accurately learning the multimodal
nature of the data. The effectiveness of the approach is assessed on
large-scale datasets of real-world traffic scenarios, showing that our model
outperforms several well-established methods in terms of prediction accuracy.
By the incorporation of differential motion constraints on the model output, we
illustrate that our model is capable of generating a diverse set of realistic
future trajectories. Through the use of an interaction-aware guidance signal,
we further demonstrate that the model can be adapted to predict the behavior of
less cooperative agents, emphasizing its practical applicability under
uncertain traffic conditions.
\\ ( https://arxiv.org/abs/2403.11643 ,  2472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11678 (*cross-listing*)
Date: Mon, 18 Mar 2024 11:29:43 GMT   (1151kb,D)

Title: Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous
  Scenes
Authors: Antoine Schnepf, Karim Kassab, Jean-Yves Franceschi, Laurent Caraffa,
  Flavian Vasile, Jeremie Mary, Andrew Comport, Val\'erie Gouet-Brunet
Categories: cs.CV cs.LG
\\
  We present a method enabling the scaling of NeRFs to learn a large number of
semantically-similar scenes. We combine two techniques to improve the required
training time and memory cost per scene. First, we learn a 3D-aware latent
space in which we train Tri-Plane scene representations, hence reducing the
resolution at which scenes are learned. Moreover, we present a way to share
common information across scenes, hence allowing for a reduction of model
complexity to learn a particular scene. Our method reduces effective per-scene
memory costs by 44% and per-scene time costs by 86% when training 1000 scenes.
Our project page can be found at https://3da-ae.github.io .
\\ ( https://arxiv.org/abs/2403.11678 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11687 (*cross-listing*)
Date: Mon, 18 Mar 2024 11:37:53 GMT   (72kb,D)

Title: Nonsmooth Implicit Differentiation: Deterministic and Stochastic
  Convergence Rates
Authors: Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo
Categories: stat.ML cs.LG math.OC
\\
  We study the problem of efficiently computing the derivative of the
fixed-point of a parametric non-differentiable contraction map. This problem
has wide applications in machine learning, including hyperparameter
optimization, meta-learning and data poisoning attacks. We analyze two popular
approaches: iterative differentiation (ITD) and approximate implicit
differentiation (AID). A key challenge behind the nonsmooth setting is that the
chain rule does not hold anymore. Building upon the recent work by Bolte et al.
(2022), who proved the linear convergence of non-differentiable ITD, we provide
refined linear convergence rates for both ITD and AID in the deterministic
case. We further introduce NSID, a new method to compute the implicit
derivative when the fixed point is defined as the composition of an outer map
and an inner map which is accessible only through a stochastic unbiased
estimator. We establish rates for the convergence of NSID to the true
derivative, encompassing the best available rates in the smooth setting. We
present illustrative experiments confirming our analysis.
\\ ( https://arxiv.org/abs/2403.11687 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11705 (*cross-listing*)
Date: Mon, 18 Mar 2024 12:07:46 GMT   (6970kb,D)

Title: Coarsening of chiral domains in itinerant electron magnets: A machine
  learning force field approach
Authors: Yunhao Fan, Sheng Zhang, Gia-Wei Chern
Categories: cond-mat.str-el cs.LG
Comments: 16 pages, 8 figures
\\
  Frustrated itinerant magnets often exhibit complex noncollinear or
noncoplanar magnetic orders which support topological electronic structures. A
canonical example is the anomalous quantum Hall state with a chiral spin order
stabilized by electron-spin interactions on a triangular lattice. While a
long-range magnetic order cannot survive thermal fluctuations in two
dimensions, the chiral order which results from the breaking of a discrete
Ising symmetry persists even at finite temperatures. We present a scalable
machine learning (ML) framework to model the complex electron-mediated
spin-spin interactions that stabilize the chiral magnetic domains in a
triangular lattice. Large-scale dynamical simulations, enabled by the ML
force-field models, are performed to investigate the coarsening of chiral
domains after a thermal quench. While the chiral phase is described by a broken
$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral
domains increases linearly with time, in stark contrast to the expected
Allen-Cahn domain growth law for a non-conserved Ising order parameter field.
The linear growth of the chiral domains is attributed to the orientational
anisotropy of domain boundaries. Our work also demonstrates the promising
potential of ML models for large-scale spin dynamics of itinerant magnets.
\\ ( https://arxiv.org/abs/2403.11705 ,  6970kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11706 (*cross-listing*)
Date: Mon, 18 Mar 2024 12:08:01 GMT   (104kb,D)

Title: Generalized Multi-Source Inference for Text Conditioned Music Diffusion
  Models
Authors: Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos,
  Emanuele Rodol\`a
Categories: cs.SD cs.LG eess.AS
Comments: Accepted at ICASSP 2024
\\
  Multi-Source Diffusion Models (MSDM) allow for compositional musical
generation tasks: generating a set of coherent sources, creating
accompaniments, and performing source separation. Despite their versatility,
they require estimating the joint distribution over the sources, necessitating
pre-separated musical data, which is rarely available, and fixing the number
and type of sources at training time. This paper generalizes MSDM to arbitrary
time-domain diffusion models conditioned on text embeddings. These models do
not require separated data as they are trained on mixtures, can parameterize an
arbitrary number of sources, and allow for rich semantic control. We propose an
inference procedure enabling the coherent generation of sources and
accompaniments. Additionally, we adapt the Dirac separator of MSDM to perform
source separation. We experiment with diffusion models trained on Slakh2100 and
MTG-Jamendo, showcasing competitive generation and separation results in a
relaxed data setting.
\\ ( https://arxiv.org/abs/2403.11706 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11735 (*cross-listing*)
Date: Mon, 18 Mar 2024 12:43:38 GMT   (2853kb,D)

Title: LSKNet: A Foundation Lightweight Backbone for Remote Sensing
Authors: Yuxuan Li, Xiang Li, Yimain Dai, Qibin Hou, Li Liu, Yongxiang Liu,
  Ming-Ming Cheng, Jian Yang
Categories: cs.CV cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2303.09030
\\
  Remote sensing images pose distinct challenges for downstream tasks due to
their inherent complexity. While a considerable amount of research has been
dedicated to remote sensing classification, object detection and semantic
segmentation, most of these studies have overlooked the valuable prior
knowledge embedded within remote sensing scenarios. Such prior knowledge can be
useful because remote sensing objects may be mistakenly recognized without
referencing a sufficiently long-range context, which can vary for different
objects. This paper considers these priors and proposes a lightweight Large
Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its
large spatial receptive field to better model the ranging context of various
objects in remote sensing scenarios. To our knowledge, large and selective
kernel mechanisms have not been previously explored in remote sensing images.
Without bells and whistles, our lightweight LSKNet sets new state-of-the-art
scores on standard remote sensing classification, object detection and semantic
segmentation benchmarks. Our comprehensive analysis further validated the
significance of the identified priors and the effectiveness of LSKNet. The code
is available at https://github.com/zcablii/LSKNet.
\\ ( https://arxiv.org/abs/2403.11735 ,  2853kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11757 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:11:10 GMT   (8611kb,D)

Title: Efficient Feature Extraction and Late Fusion Strategy for Audiovisual
  Emotional Mimicry Intensity Estimation
Authors: Jun Yu, Wangyuan Zhu, Jichao Zhu
Categories: cs.MM cs.LG cs.SD eess.AS
\\
  In this paper, we present the solution to the Emotional Mimicry Intensity
(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis
in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to
evaluate the emotional intensity of seed videos by assessing them from a set of
predefined emotion categories (i.e., "Admiration," "Amusement,"
"Determination," "Empathic Pain," "Excitement," and "Joy").
\\ ( https://arxiv.org/abs/2403.11757 ,  8611kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11778 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:35:10 GMT   (6794kb,D)

Title: Towards the Development of a Real-Time Deepfake Audio Detection System
  in Communication Platforms
Authors: Jonat John Mathew, Rakin Ahsan, Sae Furukawa, Jagdish Gautham Krishna
  Kumar, Huzaifa Pallan, Agamjeet Singh Padda, Sara Adamski, Madhu Reddiboina,
  Arjun Pankajakshan
Categories: cs.SD cs.CR cs.LG eess.AS
\\
  Deepfake audio poses a rising threat in communication platforms,
necessitating real-time detection for audio stream integrity. Unlike
traditional non-real-time approaches, this study assesses the viability of
employing static deepfake audio detection models in real-time communication
platforms. An executable software is developed for cross-platform
compatibility, enabling real-time execution. Two deepfake audio detection
models based on Resnet and LCNN architectures are implemented using the
ASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof
2019 challenge baselines. The study proposes strategies and frameworks for
enhancing these models, paving the way for real-time deepfake audio detection
in communication platforms. This work contributes to the advancement of audio
stream security, ensuring robust detection capabilities in dynamic, real-time
communication scenarios.
\\ ( https://arxiv.org/abs/2403.11778 ,  6794kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11826 (*cross-listing*)
Date: Mon, 18 Mar 2024 14:31:09 GMT   (2503kb,D)

Title: CapsLorentzNet: Integrating Physics Inspired Features with Graph
  Convolution
Authors: Rameswar Sahu
Categories: hep-ph cs.LG hep-ex
Comments: 16 pages, 3 figures
\\
  With the advent of advanced machine learning techniques, boosted object
tagging has witnessed significant progress. In this article, we take this field
further by introducing novel architectural modifications compatible with a wide
array of Graph Neural Network (GNN) architectures. Our approach advocates for
integrating capsule layers, replacing the conventional decoding blocks in
standard GNNs. These capsules are a group of neurons with vector activations.
The orientation of these vectors represents important properties of the objects
under study, with their magnitude characterizing whether the object under study
belongs to the class represented by the capsule. Moreover, capsule networks
incorporate a regularization by reconstruction mechanism, facilitating the
seamless integration of expert-designed high-level features into the analysis.
We have studied the usefulness of our architecture with the LorentzNet
architecture for quark-gluon tagging. Here, we have replaced the decoding block
of LorentzNet with a capsulated decoding block and have called the resulting
architecture CapsLorentzNet. Our new architecture can enhance the performance
of LorentzNet by 20 \% for the quark-gluon tagging task.
\\ ( https://arxiv.org/abs/2403.11826 ,  2503kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11827 (*cross-listing*)
Date: Mon, 18 Mar 2024 14:34:16 GMT   (142kb,D)

Title: Sound Event Detection and Localization with Distance Estimation
Authors: Daniel Aleksander Krause, Archontis Politis, Annamaria Mesaros
Categories: cs.SD cs.LG eess.AS
Comments: This paper has been submitted for the 32nd European Signal Processing
  Conference EUSIPCO 2024 in Lyon
\\
  Sound Event Detection and Localization (SELD) is a combined task of
identifying sound events and their corresponding direction-of-arrival (DOA).
While this task has numerous applications and has been extensively researched
in recent years, it fails to provide full information about the sound source
position. In this paper, we overcome this problem by extending the task to
Sound Event Detection, Localization with Distance Estimation (3D SELD). We
study two ways of integrating distance estimation within the SELD core - a
multi-task approach, in which the problem is tackled by a separate model
output, and a single-task approach obtained by extending the multi-ACCDOA
method to include distance information. We investigate both methods for the
Ambisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial
Soundscapes 2023. Moreover, our study involves experiments on the loss function
related to the distance estimation part. Our results show that it is possible
to perform 3D SELD without any degradation of performance in sound event
detection and DOA estimation.
\\ ( https://arxiv.org/abs/2403.11827 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11871 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:24:47 GMT   (64kb,D)

Title: The Real Tropical Geometry of Neural Networks
Authors: Marie-Charlotte Brandenburg, Georg Loho, and Guido Mont\'ufar
Categories: math.CO cs.LG
Comments: 43 pages, 6 figures; comments welcome!
MSC-class: 14T90, 52C45, 68T07 (Primary), 14P10, 52C35 (Secondary)
\\
  We consider a binary classifier defined as the sign of a tropical rational
function, that is, as the difference of two convex piecewise linear functions.
The parameter space of ReLU neural networks is contained as a semialgebraic set
inside the parameter space of tropical rational functions. We initiate the
study of two different subdivisions of this parameter space: a subdivision into
semialgebraic sets, on which the combinatorial type of the decision boundary is
fixed, and a subdivision into a polyhedral fan, capturing the combinatorics of
the partitions of the dataset. The sublevel sets of the 0/1-loss function arise
as subfans of this classification fan, and we show that the level-sets are not
necessarily connected. We describe the classification fan i) geometrically, as
normal fan of the activation polytope, and ii) combinatorially through a list
of properties of associated bipartite graphs, in analogy to covector axioms of
oriented matroids and tropical oriented matroids. Our findings extend and
refine the connection between neural networks and tropical geometry by
observing structures established in real tropical geometry, such as positive
tropicalizations of hypersurfaces and tropical semialgebraic sets.
\\ ( https://arxiv.org/abs/2403.11871 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11872 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:26:05 GMT   (2134kb,D)

Title: NuGraph2: A Graph Neural Network for Neutrino Physics Event
  Reconstruction
Authors: V Hewes and Adam Aurisano and Giuseppe Cerati and Jim Kowalkowski and
  Claire Lee and Wei-keng Liao and Daniel Grzenda and Kaushal Gumpula and
  Xiaohe Zhang
Categories: physics.data-an cs.LG hep-ex
Comments: 18 pages, 14 figures, submitted to Physical Review D
\\
  Liquid Argon Time Projection Chamber (LArTPC) detector technology offers a
wealth of high-resolution information on particle interactions, and leveraging
that information to its full potential requires sophisticated automated
reconstruction techniques. This article describes NuGraph2, a Graph Neural
Network (GNN) for low-level reconstruction of simulated neutrino interactions
in a LArTPC detector. Simulated neutrino interactions in the MicroBooNE
detector geometry are described as heterogeneous graphs, with energy
depositions on each detector plane forming nodes on planar subgraphs. The
network utilizes a multi-head attention message-passing mechanism to perform
background filtering and semantic labelling on these graph nodes, identifying
those associated with the primary physics interaction with 98.0\% efficiency
and labelling them according to particle type with 94.9\% efficiency. The
network operates directly on detector observables across multiple 2D
representations, but utilizes a 3D-context-aware mechanism to encourage
consistency between these representations. Model inference takes 0.12 s/event
on a CPU, and 0.005 s/event batched on a GPU. This architecture is designed to
be a general-purpose solution for particle reconstruction in neutrino physics,
with the potential for deployment across a broad range of detector
technologies, and offers a core convolution engine that can be leveraged for a
variety of tasks beyond the two described in this article.
\\ ( https://arxiv.org/abs/2403.11872 ,  2134kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11876 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:28:35 GMT   (5057kb,D)

Title: Deep Bayesian Future Fusion for Self-Supervised, High-Resolution,
  Off-Road Mapping
Authors: Shubhra Aich, Wenshan Wang, Parv Maheshwari, Matthew Sivaprakasam,
  Samuel Triest, Cherie Ho, Jason M. Gregory, John G. Rogers III, Sebastian
  Scherer
Categories: cs.RO cs.CV cs.LG
\\
  The limited sensing resolution of resource-constrained off-road vehicles
poses significant challenges towards reliable off-road autonomy. To overcome
this limitation, we propose a general framework based on fusing the future
information (i.e. future fusion) for self-supervision. Recent approaches
exploit this future information alongside the hand-crafted heuristics to
directly supervise the targeted downstream tasks (e.g. traversability
estimation). However, in this paper, we opt for a more general line of
development - time-efficient completion of the highest resolution (i.e. 2cm per
pixel) BEV map in a self-supervised manner via future fusion, which can be used
for any downstream tasks for better longer range prediction. To this end,
first, we create a high-resolution future-fusion dataset containing pairs of
(RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to
accommodate the noise and sparsity of the sensory information, especially in
the distal regions, we design an efficient realization of the Bayes filter onto
the vanilla convolutional network via the recurrent mechanism. Equipped with
the ideas from SOTA generative models, our Bayesian structure effectively
predicts high-quality BEV maps in the distal regions. Extensive evaluation on
both the quality of completion and downstream task on our future-fusion dataset
demonstrates the potential of our approach.
\\ ( https://arxiv.org/abs/2403.11876 ,  5057kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11898 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:56:44 GMT   (14000kb,D)

Title: Visuo-Tactile Pretraining for Cable Plugging
Authors: Abraham George, Selam Gano, Pranav Katragadda, Amir Barati Farimani
Categories: cs.RO cs.LG
Comments: 8 pages, 6 figures, submitted to IROS 2024
\\
  Tactile information is a critical tool for fine-grain manipulation. As
humans, we rely heavily on tactile information to understand objects in our
environments and how to interact with them. We use touch not only to perform
manipulation tasks but also to learn how to perform these tasks. Therefore, to
create robotic agents that can learn to complete manipulation tasks at a human
or super-human level of performance, we need to properly incorporate tactile
information into both skill execution and skill learning. In this paper, we
investigate how we can incorporate tactile information into imitation learning
platforms to improve performance on complex tasks. To do this, we tackle the
challenge of plugging in a USB cable, a dexterous manipulation task that relies
on fine-grain visuo-tactile serving. By incorporating tactile information into
imitation learning frameworks, we are able to train a robotic agent to plug in
a USB cable - a first for imitation learning. Additionally, we explore how
tactile information can be used to train non-tactile agents through a
contrastive-loss pretraining process. Our results show that by pretraining with
tactile information, the performance of a non-tactile agent can be
significantly improved, reaching a level on par with visuo-tactile agents.
  For demonstration videos and access to our codebase, see the project website:
https://sites.google.com/andrew.cmu.edu/visuo-tactile-cable-plugging/home
\\ ( https://arxiv.org/abs/2403.11898 ,  14000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11907 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:09:49 GMT   (696kb,D)

Title: Distill2Explain: Differentiable decision trees for explainable
  reinforcement learning in energy application controllers
Authors: Gargya Gokhale, Seyed Soroush Karimi Madahi, Bert Claessens, Chris
  Develder
Categories: eess.SY cs.LG cs.SY
Comments: 14 pages, 6 figures, to be published in e-Energy 2024,
\\
  Demand-side flexibility is gaining importance as a crucial element in the
energy transition process. Accounting for about 25% of final energy consumption
globally, the residential sector is an important (potential) source of energy
flexibility. However, unlocking this flexibility requires developing a control
framework that (1) easily scales across different houses, (2) is easy to
maintain, and (3) is simple to understand for end-users. A potential control
framework for such a task is data-driven control, specifically model-free
reinforcement learning (RL). Such RL-based controllers learn a good control
policy by interacting with their environment, learning purely based on data and
with minimal human intervention. Yet, they lack explainability, which hampers
user acceptance. Moreover, limited hardware capabilities of residential assets
forms a hurdle (e.g., using deep neural networks). To overcome both those
challenges, we propose a novel method to obtain explainable RL policies by
using differentiable decision trees. Using a policy distillation approach, we
train these differentiable decision trees to mimic standard RL-based
controllers, leading to a decision tree-based control policy that is
data-driven and easy to explain. As a proof-of-concept, we examine the
performance and explainability of our proposed approach in a battery-based home
energy management system to reduce energy costs. For this use case, we show
that our proposed approach can outperform baseline rule-based policies by about
20-25%, while providing simple, explainable control policies. We further
compare these explainable policies with standard RL policies and examine the
performance trade-offs associated with this increased explainability.
\\ ( https://arxiv.org/abs/2403.11907 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11938 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:35:13 GMT   (97kb,D)

Title: State space representations of the Roesser type for convolutional layers
Authors: Patricia Pauli, Dennis Gramlich, Fran Allg\"ower
Categories: eess.SY cs.LG cs.SY eess.IV eess.SP
\\
  From the perspective of control theory, convolutional layers (of neural
networks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual
representation of convolutional layers by the convolution kernel corresponds to
the representation of a dynamical system by its impulse response. However, many
analysis tools from control theory, e.g., involving linear matrix inequalities,
require a state space representation. For this reason, we explicitly provide a
state space representation of the Roesser type for 2-D convolutional layers
with $c_\mathrm{in}r_1 + c_\mathrm{out}r_2$ states, where
$c_\mathrm{in}$/$c_\mathrm{out}$ is the number of input/output channels of the
layer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel.
This representation is shown to be minimal for $c_\mathrm{in} =
c_\mathrm{out}$. We further construct state space representations for dilated,
strided, and N-D convolutions.
\\ ( https://arxiv.org/abs/2403.11938 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11947 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:40:41 GMT   (476kb,D)

Title: Explainable Reinforcement Learning-based Home Energy Management Systems
  using Differentiable Decision Trees
Authors: Gargya Gokhale, Bert Claessens, Chris Develder
Categories: eess.SY cs.LG cs.SY
Comments: 9 pages, 5 figures
\\
  With the ongoing energy transition, demand-side flexibility has become an
important aspect of the modern power grid for providing grid support and
allowing further integration of sustainable energy sources. Besides traditional
sources, the residential sector is another major and largely untapped source of
flexibility, driven by the increased adoption of solar PV, home batteries, and
EVs. However, unlocking this residential flexibility is challenging as it
requires a control framework that can effectively manage household energy
consumption, and maintain user comfort while being readily scalable across
different, diverse houses. We aim to address this challenging problem and
introduce a reinforcement learning-based approach using differentiable decision
trees. This approach integrates the scalability of data-driven reinforcement
learning with the explainability of (differentiable) decision trees. This leads
to a controller that can be easily adapted across different houses and provides
a simple control policy that can be explained to end-users, further improving
user acceptance. As a proof-of-concept, we analyze our method using a home
energy management problem, comparing its performance with commercially
available rule-based baseline and standard neural network-based RL controllers.
Through this preliminary study, we show that the performance of our proposed
method is comparable to standard RL-based controllers, outperforming baseline
controllers by ~20% in terms of daily cost savings while being straightforward
to explain.
\\ ( https://arxiv.org/abs/2403.11947 ,  476kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11948 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:42:39 GMT   (9504kb,D)

Title: Learning Dynamical Systems Encoding Non-Linearity within Space Curvature
Authors: Bernardo Fichera and Aude Billard
Categories: cs.RO cs.LG cs.SY eess.SY
\\
  Dynamical Systems (DS) are an effective and powerful means of shaping
high-level policies for robotics control. They provide robust and reactive
control while ensuring the stability of the driving vector field. The
increasing complexity of real-world scenarios necessitates DS with a higher
degree of non-linearity, along with the ability to adapt to potential changes
in environmental conditions, such as obstacles. Current learning strategies for
DSs often involve a trade-off, sacrificing either stability guarantees or
offline computational efficiency in order to enhance the capabilities of the
learned DS. Online local adaptation to environmental changes is either not
taken into consideration or treated as a separate problem. In this paper, our
objective is to introduce a method that enhances the complexity of the learned
DS without compromising efficiency during training or stability guarantees.
Furthermore, we aim to provide a unified approach for seamlessly integrating
the initially learned DS's non-linearity with any local non-linearities that
may arise due to changes in the environment. We propose a geometrical approach
to learn asymptotically stable non-linear DS for robotics control. Each DS is
modeled as a harmonic damped oscillator on a latent manifold. By learning the
manifold's Euclidean embedded representation, our approach encodes the
non-linearity of the DS within the curvature of the space. Having an explicit
embedded representation of the manifold allows us to showcase obstacle
avoidance by directly inducing local deformations of the space. We demonstrate
the effectiveness of our methodology through two scenarios: first, the 2D
learning of synthetic vector fields, and second, the learning of 3D robotic
end-effector motions in real-world settings.
\\ ( https://arxiv.org/abs/2403.11948 ,  9504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11981 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:17:07 GMT   (146kb,D)

Title: Diffusion Denoising as a Certified Defense against Clean-label Poisoning
Authors: Sanghyun Hong, Nicholas Carlini, Alexey Kurakin
Categories: cs.CR cs.CV cs.LG
\\
  We present a certified defense to clean-label poisoning attacks. These
attacks work by injecting a small number of poisoning samples (e.g., 1%) that
contain $p$-norm bounded adversarial perturbations into the training data to
induce a targeted misclassification of a test-time input. Inspired by the
adversarial robustness achieved by $denoised$ $smoothing$, we show how an
off-the-shelf diffusion model can sanitize the tampered training data. We
extensively test our defense against seven clean-label poisoning attacks and
reduce their attack success to 0-16% with only a negligible drop in the test
time accuracy. We compare our defense with existing countermeasures against
clean-label poisoning, showing that the defense reduces the attack success the
most and offers the best model utility. Our results highlight the need for
future work on developing stronger clean-label attacks and using our certified
yet practical defense as a strong baseline to evaluate these attacks.
\\ ( https://arxiv.org/abs/2403.11981 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12005 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:42:27 GMT   (11438kb,D)

Title: Visualization for Trust in Machine Learning Revisited: The State of the
  Field in 2023
Authors: Angelos Chatzimparmpas, Kostiantyn Kucher, Andreas Kerren
Categories: cs.HC cs.LG stat.ML
Comments: This manuscript is accepted for publication in the IEEE Computer
  Graphics and Applications Journal (IEEE CG&A)
DOI: 10.1109/MCG.2024.3360881
\\
  Visualization for explainable and trustworthy machine learning remains one of
the most important and heavily researched fields within information
visualization and visual analytics with various application domains, such as
medicine, finance, and bioinformatics. After our 2020 state-of-the-art report
comprising 200 techniques, we have persistently collected peer-reviewed
articles describing visualization techniques, categorized them based on the
previously established categorization schema consisting of 119 categories, and
provided the resulting collection of 542 techniques in an online survey
browser. In this survey article, we present the updated findings of new
analyses of this dataset as of fall 2023 and discuss trends, insights, and
eight open challenges for using visualizations in machine learning. Our results
corroborate the rapidly growing trend of visualization techniques for
increasing trust in machine learning models in the past three years, with
visualization found to help improve popular model explainability methods and
check new deep learning architectures, for instance.
\\ ( https://arxiv.org/abs/2403.12005 ,  11438kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12007 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:43:40 GMT   (1823kb,D)

Title: Defining Effective Engagement For Enhancing Cancer Patients' Well-being
  with Mobile Digital Behavior Change Interventions
Authors: Aneta Lisowska, Szymon Wilk, Laura Locati, Mimma Rizzo, Lucia Sacchi,
  Silvana Quaglini, Matteo Terzaghi, Valentina Tibollo, Mor Peleg
Categories: cs.HC cs.LG
\\
  Digital Behavior Change Interventions (DBCIs) are supporting development of
new health behaviors. Evaluating their effectiveness is crucial for their
improvement and understanding of success factors. However, comprehensive
guidance for developers, particularly in small-scale studies with ethical
constraints, is limited. Building on the CAPABLE project, this study aims to
define effective engagement with DBCIs for supporting cancer patients in
enhancing their quality of life. We identify metrics for measuring engagement,
explore the interest of both patients and clinicians in DBCIs, and propose
hypotheses for assessing the impact of DBCIs in such contexts. Our findings
suggest that clinician prescriptions significantly increase sustained
engagement with mobile DBCIs. In addition, while one weekly engagement with a
DBCI is sufficient to maintain well-being, transitioning from extrinsic to
intrinsic motivation may require a higher level of engagement.
\\ ( https://arxiv.org/abs/2403.12007 ,  1823kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12012 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:50:20 GMT   (2672kb,D)

Title: Convergence of Kinetic Langevin Monte Carlo on Lie groups
Authors: Lingkai Kong, Molei Tao
Categories: math.ST cs.LG cs.NA math.NA math.PR stat.ML stat.TH
\\
  Explicit, momentum-based dynamics for optimizing functions defined on Lie
groups was recently constructed, based on techniques such as variational
optimization and left trivialization. We appropriately add tractable noise to
the optimization dynamics to turn it into a sampling dynamics, leveraging the
advantageous feature that the momentum variable is Euclidean despite that the
potential function lives on a manifold. We then propose a Lie-group MCMC
sampler, by delicately discretizing the resulting kinetic-Langevin-type
sampling dynamics. The Lie group structure is exactly preserved by this
discretization. Exponential convergence with explicit convergence rate for both
the continuous dynamics and the discrete sampler are then proved under W2
distance. Only compactness of the Lie group and geodesically L-smoothness of
the potential function are needed. To the best of our knowledge, this is the
first convergence result for kinetic Langevin on curved spaces, and also the
first quantitative result that requires no convexity or, at least not
explicitly, any common relaxation such as isoperimetry.
\\ ( https://arxiv.org/abs/2403.12012 ,  2672kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12030 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:58:13 GMT   (1625kb,D)

Title: Expandable Subspace Ensemble for Pre-Trained Model-Based
  Class-Incremental Learning
Authors: Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, De-Chuan Zhan
Categories: cs.CV cs.LG
Comments: Accepted to CVPR 2024. Code is available at:
  https://github.com/sun-hailong/CVPR24-Ease
\\
  Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Despite the strong performance of
Pre-Trained Models (PTMs) in CIL, a critical issue persists: learning new
classes often results in the overwriting of old ones. Excessive modification of
the network causes forgetting, while minimal adjustments lead to an inadequate
fit for new classes. As a result, it is desired to figure out a way of
efficient model updating without harming former knowledge. In this paper, we
propose ExpAndable Subspace Ensemble (EASE) for PTM-based CIL. To enable model
updating without conflict, we train a distinct lightweight adapter module for
each new task, aiming to create task-specific subspaces. These adapters span a
high-dimensional feature space, enabling joint decision-making across multiple
subspaces. As data evolves, the expanding subspaces render the old class
classifiers incompatible with new-stage spaces. Correspondingly, we design a
semantic-guided prototype complement strategy that synthesizes old classes' new
features without using any old class instance. Extensive experiments on seven
benchmark datasets verify EASE's state-of-the-art performance. Code is
available at: https://github.com/sun-hailong/CVPR24-Ease
\\ ( https://arxiv.org/abs/2403.12030 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12034 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:59:12 GMT   (6108kb,D)

Title: VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion
  Models
Authors: Junlin Han, Filippos Kokkinos, Philip Torr
Categories: cs.CV cs.GR cs.LG
Comments: Project page: https://junlinhan.github.io/projects/vfusion3d.html
\\
  This paper presents a novel paradigm for building scalable 3D generative
models utilizing pre-trained video diffusion models. The primary obstacle in
developing foundation 3D generative models is the limited availability of 3D
data. Unlike images, texts, or videos, 3D data are not readily accessible and
are difficult to acquire. This results in a significant disparity in scale
compared to the vast quantities of other types of data. To address this issue,
we propose using a video diffusion model, trained with extensive volumes of
text, images, and videos, as a knowledge source for 3D data. By unlocking its
multi-view generative capabilities through fine-tuning, we generate a
large-scale synthetic multi-view dataset to train a feed-forward 3D generative
model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view
data, can generate a 3D asset from a single image in seconds and achieves
superior performance when compared to current SOTA feed-forward 3D generative
models, with users preferring our results over 70% of the time.
\\ ( https://arxiv.org/abs/2403.12034 ,  6108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12036 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:59:40 GMT   (28648kb,D)

Title: One-Step Image Translation with Text-to-Image Models
Authors: Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, Jun-Yan Zhu
Categories: cs.CV cs.GR cs.LG
Comments: Github: https://github.com/GaParmar/img2img-turbo
\\
  In this work, we address two limitations of existing conditional diffusion
models: their slow inference speed due to the iterative denoising process and
their reliance on paired data for model fine-tuning. To tackle these issues, we
introduce a general method for adapting a single-step diffusion model to new
tasks and domains through adversarial learning objectives. Specifically, we
consolidate various modules of the vanilla latent diffusion model into a single
end-to-end generator network with small trainable weights, enhancing its
ability to preserve the input image structure while reducing overfitting. We
demonstrate that, for unpaired settings, our model CycleGAN-Turbo outperforms
existing GAN-based and diffusion-based methods for various scene translation
tasks, such as day-to-night conversion and adding/removing weather effects like
fog, snow, and rain. We extend our method to paired settings, where our model
pix2pix-Turbo is on par with recent works like Control-Net for Sketch2Photo and
Edge2Image, but with a single-step inference. This work suggests that
single-step diffusion models can serve as strong backbones for a range of GAN
learning objectives. Our code and models are available at
https://github.com/GaParmar/img2img-turbo.
\\ ( https://arxiv.org/abs/2403.12036 ,  28648kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2208.00726
replaced with revised version Mon, 18 Mar 2024 09:51:27 GMT   (40kb)

Title: Fair Division of Multi-layered Cakes
Authors: Mohammad Azharuddin Sanpui
Categories: cs.AI cs.GT
\\ ( https://arxiv.org/abs/2208.00726 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14769
replaced with revised version Sat, 16 Mar 2024 18:35:52 GMT   (25849kb,D)

Title: Navigation as Attackers Wish? Towards Building Robust Embodied Agents
  under Federated Learning
Authors: Yunchao Zhang, Zonglin Di, Kaiwen Zhou, Cihang Xie, Xin Eric Wang
Categories: cs.AI cs.CL cs.CR cs.CV
\\ ( https://arxiv.org/abs/2211.14769 ,  25849kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17879
replaced with revised version Fri, 15 Mar 2024 22:49:33 GMT   (668kb,D)

Title: CoSMo: a Framework to Instantiate Conditioned Process Simulation Models
Authors: Rafael S. Oyamada and Gabriel M. Tavares and Sylvio Barbon Junior and
  Paolo Ceravolo
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2303.17879 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13138
replaced with revised version Sun, 17 Mar 2024 20:58:59 GMT   (626kb,D)

Title: The Update-Equivalence Framework for Decision-Time Planning
Authors: Samuel Sokota, Gabriele Farina, David J. Wu, Hengyuan Hu, Kevin A.
  Wang, J. Zico Kolter, Noam Brown
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2304.13138 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11074
replaced with revised version Mon, 18 Mar 2024 02:04:56 GMT   (7818kb,D)

Title: Tram: A Token-level Retrieval-augmented Mechanism for Source Code
  Summarization
Authors: Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu,
  Shouling Ji, Wenhai Wang
Categories: cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2305.11074 ,  7818kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07919
replaced with revised version Mon, 18 Mar 2024 01:13:32 GMT   (9206kb,D)

Title: Neural Architecture Retrieval
Authors: Xiaohuan Pei, Yanxi Li, Minjing Dong, Chang Xu
Categories: cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2307.07919 ,  9206kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06378
replaced with revised version Sun, 17 Mar 2024 14:10:15 GMT   (4039kb,D)

Title: DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System
Authors: Mojtaba Yeganejou, Kimia Honari, Ryan Kluzinski, Scott Dick, Michael
  Lipsett, James Miller
Categories: cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2308.06378 ,  4039kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11737
replaced with revised version Mon, 18 Mar 2024 01:35:48 GMT   (7383kb,D)

Title: Choice-75: A Dataset on Decision Branching in Script Learning
Authors: Zhaoyi Joey Hou, Li Zhang, Chris Callison-Burch
Categories: cs.AI
Comments: To be published in LREC-COLING-2024
\\ ( https://arxiv.org/abs/2309.11737 ,  7383kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00013
replaced with revised version Sat, 16 Mar 2024 15:20:43 GMT   (8184kb,D)

Title: Adaptive Communications in Collaborative Perception with Domain
  Alignment for Autonomous Driving
Authors: Senkang Hu, Zhengru Fang, Haonan An, Guowen Xu, Yuan Zhou, Xianhao
  Chen, Yuguang Fang
Categories: cs.AI
Comments: 6 pages, 6 figures
\\ ( https://arxiv.org/abs/2310.00013 ,  8184kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00229
replaced with revised version Sat, 16 Mar 2024 18:59:23 GMT   (4402kb,D)

Title: Consciousness-Inspired Spatio-Temporal Abstractions for Better
  Generalization in Reinforcement Learning
Authors: Mingde Zhao, Safa Alver, Harm van Seijen, Romain Laroche, Doina
  Precup, Yoshua Bengio
Categories: cs.AI cs.LG
Comments: ICLR 2024 Camera-Ready
\\ ( https://arxiv.org/abs/2310.00229 ,  4402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16580
replaced with revised version Mon, 18 Mar 2024 17:57:22 GMT   (247kb,D)

Title: Attention-based Reinforcement Learning for Combinatorial Optimization:
  Application to Job Shop Scheduling Problem
Authors: Jaejin Lee, Seho Kee, Mani Janakiram and George Runger
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.16580 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04578
replaced with revised version Mon, 18 Mar 2024 05:56:42 GMT   (4977kb,D)

Title: S-Agents: Self-organizing Agents in Open-ended Environments
Authors: Jiaqi Chen and Yuxian Jiang and Jiachen Lu and Li Zhang
Categories: cs.AI cs.MA
Comments: Preview, 15 pages, 12 figure
\\ ( https://arxiv.org/abs/2402.04578 ,  4977kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05808
replaced with revised version Sun, 17 Mar 2024 09:02:02 GMT   (668kb,D)

Title: Training Large Language Models for Reasoning through Reverse Curriculum
  Reinforcement Learning
Authors: Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He,
  Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran
  Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi
  Zhang, Xuanjing Huang
Categories: cs.AI cs.CL cs.LG
Comments: Preprint. Codes released:
  https://github.com/WooooDyy/LLM-Reverse-Curriculum-RL
\\ ( https://arxiv.org/abs/2402.05808 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14424
replaced with revised version Sun, 17 Mar 2024 04:14:27 GMT   (14097kb,D)

Title: Automating Psychological Hypothesis Generation with AI: Large Language
  Models Meet Causal Graph
Authors: Song Tong, Kai Mao, Zhen Huang, Yukun Zhao, Kaiping Peng
Categories: cs.AI cs.CY
DOI: 10.31234/osf.io/7ck9m
\\ ( https://arxiv.org/abs/2402.14424 ,  14097kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03768
replaced with revised version Mon, 18 Mar 2024 15:05:55 GMT   (19590kb,D)

Title: DeepCRE: Transforming Drug R&D via AI-Driven Cross-drug Response
  Evaluation
Authors: Yushuai Wu, Ting Zhang, Hao Zhou, Hainan Wu, Hanwen Sunchu, Lei Hu,
  Xiaofang Chen, Suyuan Zhao, Gaochao Liu, Chao Sun, Jiahuan Zhang, Yizhen Luo,
  Peng Liu, Zaiqing Nie and Yushuai Wu
Categories: cs.AI cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2403.03768 ,  19590kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07363
replaced with revised version Sun, 17 Mar 2024 11:08:15 GMT   (1060kb)

Title: A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees
Authors: Yingtao Ren, Xiaomin Zhu, Kaiyuan Bai, Runtong Zhang
Categories: cs.AI
Journal-ref: IEEE Transactions on Fuzzy Systems 31.5 (2023): 1729-1741
DOI: 10.1109/TFUZZ.2022.3215725
\\ ( https://arxiv.org/abs/2403.07363 ,  1060kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09404
replaced with revised version Mon, 18 Mar 2024 12:45:01 GMT   (436kb,D)

Title: Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption
Authors: Anirban Mukherjee, Hannah Hanwen Chang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.09404 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2103.11072
replaced with revised version Mon, 18 Mar 2024 08:29:49 GMT   (482kb,D)

Title: Local Interpretations for Explainable Natural Language Processing: A
  Survey
Authors: Siwen Luo and Hamish Ivison and Caren Han and Josiah Poon
Categories: cs.CL cs.AI
Comments: Accepted by ACM Computing Surveys
ACM-class: A.1; I.2.7
DOI: 10.1145/3649450
\\ ( https://arxiv.org/abs/2103.11072 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2105.00815
replaced with revised version Sat, 16 Mar 2024 15:06:58 GMT   (3250kb,D)

Title: Representation Learning for Weakly Supervised Relation Extraction
Authors: Zhuang Li
Categories: cs.CL
Comments: Master's Research Thesis for the Australian National University, 60
  pages, submitted in October 2015
\\ ( https://arxiv.org/abs/2105.00815 ,  3250kb)
------------------------------------------------------------------------------
\\
arXiv:2212.09563
replaced with revised version Sun, 17 Mar 2024 19:06:38 GMT   (8845kb,D)

Title: Source-Free Domain Adaptation for Question Answering with Masked
  Self-training
Authors: M. Yin, B. Wang, Y. Dong, C. Ling
Categories: cs.CL
\\ ( https://arxiv.org/abs/2212.09563 ,  8845kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02060
replaced with revised version Sat, 16 Mar 2024 04:28:35 GMT   (1028kb,D)

Title: Representation Deficiency in Masked Language Modeling
Authors: Yu Meng, Jitin Krishnan, Sinong Wang, Qifan Wang, Yuning Mao, Han
  Fang, Marjan Ghazvininejad, Jiawei Han, Luke Zettlemoyer
Categories: cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2302.02060 ,  1028kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06626
replaced with revised version Sun, 17 Mar 2024 23:39:58 GMT   (7804kb,D)

Title: When the Majority is Wrong: Modeling Annotator Disagreement for
  Subjective Tasks
Authors: Eve Fleisig, Rediet Abebe, Dan Klein
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.06626 ,  7804kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08339
replaced with revised version Mon, 18 Mar 2024 09:56:09 GMT   (884kb)

Title: Assessing the potential of AI-assisted pragmatic annotation: The case of
  apologies
Authors: Danni Yu, Luyang Li, Hang Su, Matteo Fuoli
Categories: cs.CL cs.AI
Comments: 24 pages, 2 figures, 3 tablels
\\ ( https://arxiv.org/abs/2305.08339 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12544
replaced with revised version Fri, 15 Mar 2024 20:10:51 GMT   (2386kb,D)

Title: Has It All Been Solved? Open NLP Research Questions Not Solved by Large
  Language Models
Authors: Oana Ignat, Zhijing Jin, Artem Abzaliev, Laura Biester, Santiago
  Castro, Naihao Deng, Xinyi Gao, Aylin Gunal, Jacky He, Ashkan Kazemi,
  Muhammad Khalifa, Namho Koh, Andrew Lee, Siyang Liu, Do June Min, Shinka
  Mori, Joan Nwatu, Veronica Perez-Rosas, Siqi Shen, Zekun Wang, Winston Wu,
  Rada Mihalcea
Categories: cs.CL cs.AI
Comments: Accepted at COLING 2024
\\ ( https://arxiv.org/abs/2305.12544 ,  2386kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14534
replaced with revised version Fri, 15 Mar 2024 18:26:48 GMT   (687kb,D)

Title: Detecting Propaganda Techniques in Code-Switched Social Media Text
Authors: Muhammad Umar Salman, Asif Hanif, Shady Shehata, Preslav Nakov
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.14534 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15852
replaced with revised version Fri, 15 Mar 2024 21:04:34 GMT   (993kb,D)

Title: Self-contradictory Hallucinations of Large Language Models: Evaluation,
  Detection and Mitigation
Authors: Niels M\"undler, Jingxuan He, Slobodan Jenko, Martin Vechev
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2305.15852 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05524
replaced with revised version Mon, 18 Mar 2024 03:14:54 GMT   (10180kb,D)

Title: On the Detectability of ChatGPT Content: Benchmarking, Methodology, and
  Evaluation through the Lens of Academic Writing
Authors: Zeyan Liu, Zijun Yao, Fengjun Li, Bo Luo
Categories: cs.CL cs.CR cs.LG
\\ ( https://arxiv.org/abs/2306.05524 ,  10180kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06693
replaced with revised version Mon, 18 Mar 2024 08:37:51 GMT   (250kb)

Title: Open Brain AI. Automatic Language Assessment
Authors: Charalambos Themistocleous
Categories: cs.CL
Comments: 1 figure, associated web application: openbrainai.com
\\ ( https://arxiv.org/abs/2306.06693 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11044
replaced with revised version Mon, 18 Mar 2024 10:36:05 GMT   (10730kb,D)

Title: Frequency effects in Linear Discriminative Learning
Authors: Maria Heitmeier, Yu-Ying Chuang, Seth D. Axen, R. Harald Baayen
Categories: cs.CL
Comments: 32 pages, 12 figures, 3 tables; revised version
Journal-ref: Frontiers in Human Neuroscience 17 (2024)
DOI: 10.3389/fnhum.2023.1242720
\\ ( https://arxiv.org/abs/2306.11044 ,  10730kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13063
replaced with revised version Sun, 17 Mar 2024 04:38:48 GMT   (675kb,D)

Title: Can LLMs Express Their Uncertainty? An Empirical Evaluation of
  Confidence Elicitation in LLMs
Authors: Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He,
  Bryan Hooi
Categories: cs.CL
Comments: The paper is accepted by ICLR 2024. The code is publicly available at
  https://github.com/MiaoXiong2320/llm-uncertainty
\\ ( https://arxiv.org/abs/2306.13063 ,  675kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16001
replaced with revised version Mon, 18 Mar 2024 16:22:16 GMT   (808kb)

Title: Streamlining Social Media Information Retrieval for COVID-19 Research
  with Deep Learning
Authors: Yining Hua, Jiageng Wu, Shixu Lin, Minghui Li, Yujie Zhang, Dinah
  Foer, Siwen Wang, Peilin Zhou, Jie Yang, Li Zhou
Categories: cs.CL cs.AI cs.IR
Comments: Updated full paper. Abstract presented at IEEE ICHI 2023 and AMIA
  Annual Symposium 2023
\\ ( https://arxiv.org/abs/2306.16001 ,  808kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06029
replaced with revised version Mon, 18 Mar 2024 08:07:52 GMT   (1236kb,D)

Title: Pluggable Neural Machine Translation Models via Memory-augmented
  Adapters
Authors: Yuzhuang Xu, Shuo Wang, Peng Li, Xuebo Liu, Xiaolong Wang, Weidong
  Liu, Yang Liu
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2307.06029 ,  1236kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06945
replaced with revised version Mon, 18 Mar 2024 00:45:48 GMT   (546kb,D)

Title: In-context Autoencoder for Context Compression in a Large Language Model
Authors: Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, Furu Wei
Categories: cs.CL cs.AI cs.LG
Comments: v3: camera ready for ICLR'24
\\ ( https://arxiv.org/abs/2307.06945 ,  546kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12950
replaced with revised version Sat, 16 Mar 2024 04:22:09 GMT   (627kb,D)

Title: RLCD: Reinforcement Learning from Contrastive Distillation for Language
  Model Alignment
Authors: Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, Yuandong Tian
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2307.12950 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10855
replaced with revised version Sun, 17 Mar 2024 13:11:08 GMT   (1005kb,D)

Title: LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete
  Information from Lateral Thinking Puzzles
Authors: Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong
  Zhang, Hai-Tao Zheng
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2308.10855 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00359
replaced with revised version Sat, 16 Mar 2024 14:02:45 GMT   (3962kb,D)

Title: Large Content And Behavior Models To Understand, Simulate, And Optimize
  Content And Behavior
Authors: Ashmit Khandelwal, Aditya Agrawal, Aanisha Bhattacharyya, Yaman K
  Singla, Somesh Singh, Uttaran Bhattacharya, Ishita Dasgupta, Stefano
  Petrangeli, Rajiv Ratn Shah, Changyou Chen, Balaji Krishnamurthy
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2309.00359 ,  3962kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05472
replaced with revised version Mon, 18 Mar 2024 10:54:15 GMT   (417kb,D)

Title: LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for
  Self-supervised Representations of French Speech
Authors: Titouan Parcollet, Ha Nguyen, Solene Evain, Marcely Zanon Boito,
  Adrien Pupier, Salima Mdhaffar, Hang Le, Sina Alisamir, Natalia Tomashenko,
  Marco Dinarelli, Shucong Zhang, Alexandre Allauzen, Maximin Coavoux, Yannick
  Esteve, Mickael Rouvier, Jerome Goulian, Benjamin Lecouteux, Francois Portet,
  Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier
Categories: cs.CL cs.AI cs.SD eess.AS
Comments: Published in Computer Science and Language. Preprint allowed
\\ ( https://arxiv.org/abs/2309.05472 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11235
replaced with revised version Sat, 16 Mar 2024 04:32:25 GMT   (1550kb,D)

Title: OpenChat: Advancing Open-source Language Models with Mixed-Quality Data
Authors: Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.11235 ,  1550kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13202
replaced with revised version Sat, 16 Mar 2024 12:15:24 GMT   (1831kb,D)

Title: Investigating Large Language Models and Control Mechanisms to Improve
  Text Readability of Biomedical Abstracts
Authors: Zihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew
  Shardlow, Goran Nenadic
Categories: cs.CL cs.AI
Comments: Accepted by IEEE-ICHI 2024 https://ieeeichi2024.github.io/
\\ ( https://arxiv.org/abs/2309.13202 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13345
replaced with revised version Sun, 17 Mar 2024 04:22:54 GMT   (1019kb,D)

Title: BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling
  Capacities of Large Language Models
Authors: Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
Categories: cs.CL
Comments: Accepted for the Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING) 2024
\\ ( https://arxiv.org/abs/2309.13345 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13788
replaced with revised version Sat, 16 Mar 2024 12:30:31 GMT   (1363kb,D)

Title: Can LLM-Generated Misinformation Be Detected?
Authors: Canyu Chen, Kai Shu
Categories: cs.CL cs.AI cs.CR cs.HC cs.LG
Comments: Accepted to Proceedings of ICLR 2024. 9 pages for main paper, 38
  pages including appendix. The code, results, dataset for this paper and more
  resources on "LLMs Meet Misinformation" have been released on the project
  website: https://llm-misinformation.github.io/
\\ ( https://arxiv.org/abs/2309.13788 ,  1363kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01382
replaced with revised version Sun, 17 Mar 2024 00:38:59 GMT   (8925kb,D)

Title: Compressing LLMs: The Truth is Rarely Pure and Never Simple
Authors: Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei
  Yang
Categories: cs.CL cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.01382 ,  8925kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05155
replaced with revised version Mon, 18 Mar 2024 03:19:33 GMT   (9114kb,D)

Title: Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on
  Open-Source Model
Authors: Cheng Qian, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu
Categories: cs.CL cs.AI
Comments: NAACL 2024 Main
\\ ( https://arxiv.org/abs/2310.05155 ,  9114kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06202
replaced with revised version Fri, 15 Mar 2024 20:21:28 GMT   (2591kb,D)

Title: GPT-who: An Information Density-based Machine-Generated Text Detector
Authors: Saranya Venkatraman, Adaku Uchendu, Dongwon Lee
Categories: cs.CL
Comments: To appear in Findings of the Association for Computational
  Linguistics: NAACL 2024
\\ ( https://arxiv.org/abs/2310.06202 ,  2591kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10631
replaced with revised version Fri, 15 Mar 2024 19:14:39 GMT   (116kb,D)

Title: Llemma: An Open Language Model For Mathematics
Authors: Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco
  Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella
  Biderman and Sean Welleck
Categories: cs.CL cs.AI cs.LO
Comments: Updated references; corrected description of COPRA search budget
\\ ( https://arxiv.org/abs/2310.10631 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16450
replaced with revised version Sun, 17 Mar 2024 12:53:28 GMT   (928kb,D)

Title: CLEX: Continuous Length Extrapolation for Large Language Models
Authors: Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.16450 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07914
replaced with revised version Sat, 16 Mar 2024 03:19:25 GMT   (8657kb,D)

Title: Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
Authors: Garima Agrawal, Tharindu Kumarage, Zeyad Alghamdi, Huan Liu
Categories: cs.CL cs.LG
Comments: Accepted Paper in NAACL 2024
\\ ( https://arxiv.org/abs/2311.07914 ,  8657kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08921
replaced with revised version Sun, 17 Mar 2024 06:58:01 GMT   (464kb,D)

Title: Self-Improving for Zero-Shot Named Entity Recognition with Large
  Language
Authors: Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang
Categories: cs.CL
Comments: Accepted to NAACL 2024 (Main Conference)
\\ ( https://arxiv.org/abs/2311.08921 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03724
replaced with revised version Sun, 17 Mar 2024 23:16:41 GMT   (208kb,D)

Title: DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt
  Engineer
Authors: Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li,
  Zhangyang Wang
Categories: cs.CL cs.AI
Comments: Accepted to ICLR'24 Splotlight (updated version)
\\ ( https://arxiv.org/abs/2312.03724 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04193
replaced with revised version Sat, 16 Mar 2024 17:44:27 GMT   (40kb)

Title: Language Model Knowledge Distillation for Efficient Question Answering
  in Spanish
Authors: Adri\'an Bazaga, Pietro Li\`o, Gos Micklem
Categories: cs.CL cs.LG stat.ML
Comments: ICLR 2024 Tiny Paper (6 pages, 2 tables)
\\ ( https://arxiv.org/abs/2312.04193 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03411
replaced with revised version Mon, 18 Mar 2024 09:47:24 GMT   (26580kb,D)

Title: GRAM: Global Reasoning for Multi-Page VQA
Authors: Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben
  Avraham, Aviad Aberdam, Shahar Tsiper and Ron Litman
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2401.03411 ,  26580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04536
replaced with revised version Sat, 16 Mar 2024 16:41:48 GMT   (987kb,D)

Title: Evaluating Language Model Agency through Negotiations
Authors: Tim R. Davidson, Veniamin Veselovsky, Martin Josifoski, Maxime
  Peyrard, Antoine Bosselut, Michal Kosinski, Robert West
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ICLR 2024, code and link to project data are made
  available at https://github.com/epfl-dlab/LAMEN
\\ ( https://arxiv.org/abs/2401.04536 ,  987kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05561
replaced with revised version Mon, 18 Mar 2024 02:49:05 GMT   (1509kb,D)

Title: TrustLLM: Trustworthiness in Large Language Models
Authors: Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li,
  Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu,
  Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming
  Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji,
  Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng
  Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han,
  Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell,
  Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil
  Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying,
  Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William
  Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, et al. (8 additional authors not
  shown)
Categories: cs.CL
Comments: This work is still under work and we welcome your contribution
\\ ( https://arxiv.org/abs/2401.05561 ,  1509kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11431
replaced with revised version Sat, 16 Mar 2024 13:03:01 GMT   (116kb,D)

Title: Majority or Minority: Data Imbalance Learning Method for Named Entity
  Recognition
Authors: Sota Nemoto and Shunsuke Kitada and Hitoshi Iyatomi
Categories: cs.CL
Comments: 5 pages, 1 figures, 3 tables. Accepted at Practical ML for Low
  Resource Settings (PML4LRS) Workshop @ ICLR 2024
\\ ( https://arxiv.org/abs/2401.11431 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11944
replaced with revised version Mon, 18 Mar 2024 09:02:03 GMT   (21779kb,D)

Title: CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding
  Benchmark
Authors: Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng,
  Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu,
  Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai,
  Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu
Categories: cs.CL cs.AI cs.CV
\\ ( https://arxiv.org/abs/2401.11944 ,  21779kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11969
replaced with revised version Mon, 18 Mar 2024 16:49:59 GMT   (938kb,D)

Title: Claim Detection for Automated Fact-checking: A Survey on Monolingual,
  Multilingual and Cross-Lingual Research
Authors: Rrubaa Panchendrarajan and Arkaitz Zubiaga
Categories: cs.CL
Comments: Accepted revision
\\ ( https://arxiv.org/abs/2401.11969 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11972
replaced with revised version Mon, 18 Mar 2024 17:05:30 GMT   (8259kb,D)

Title: Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid
  Approaches to Natural Language Processing
Authors: Rrubaa Panchendrarajan and Arkaitz Zubiaga
Categories: cs.CL
Comments: Revised according to review comments
\\ ( https://arxiv.org/abs/2401.11972 ,  8259kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12873
replaced with revised version Mon, 18 Mar 2024 15:16:16 GMT   (8356kb,D)

Title: Improving Machine Translation with Human Feedback: An Exploration of
  Quality Estimation as a Reward Model
Authors: Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang,
  Shuming Shi, Zhaopeng Tu
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2401.12873 ,  8356kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13463
replaced with revised version Mon, 18 Mar 2024 06:08:31 GMT   (750kb,D)

Title: SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken
  Question Answering
Authors: Chyi-Jiunn Lin, Guan-Ting Lin, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen
  Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee
Categories: cs.CL cs.IR cs.SD eess.AS
Comments: Accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2401.13463 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01030
replaced with revised version Mon, 18 Mar 2024 15:18:45 GMT   (8780kb,D)

Title: Executable Code Actions Elicit Better LLM Agents
Authors: Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao
  Peng, Heng Ji
Categories: cs.CL cs.AI
Comments: Code, data, model, and demo are available at
  https://github.com/xingyaoww/code-act
\\ ( https://arxiv.org/abs/2402.01030 ,  8780kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05131
replaced with revised version Sat, 16 Mar 2024 09:08:26 GMT   (256kb)

Title: Financial Report Chunking for Effective Retrieval Augmented Generation
Authors: Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, and
  Renyu Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.05131 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11453
replaced with revised version Mon, 18 Mar 2024 15:59:37 GMT   (3682kb,D)

Title: MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
  Data Visualization
Authors: Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan,
  Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi,
  Maosong Sun
Categories: cs.CL
Comments: Work in Progress
\\ ( https://arxiv.org/abs/2402.11453 ,  3682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12261
replaced with revised version Sat, 16 Mar 2024 20:29:34 GMT   (2047kb,D)

Title: NEO-BENCH: Evaluating Robustness of Large Language Models with
  Neologisms
Authors: Jonathan Zheng, Alan Ritter, Wei Xu
Categories: cs.CL
Comments: pre-print, 9 pages
\\ ( https://arxiv.org/abs/2402.12261 ,  2047kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12566
replaced with revised version Sat, 16 Mar 2024 21:14:16 GMT   (9263kb,D)

Title: GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
Authors: Kundan Krishna, Sanjana Ramprasad, Prakhar Gupta, Byron C. Wallace,
  Zachary C. Lipton, Jeffrey P. Bigham
Categories: cs.CL cs.LG
Comments: Code and models available at https://genaudit.org
\\ ( https://arxiv.org/abs/2402.12566 ,  9263kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13605
replaced with revised version Mon, 18 Mar 2024 04:12:17 GMT   (9560kb,D)

Title: KorNAT: LLM Alignment Benchmark for Korean Social Values and Common
  Knowledge
Authors: Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won,
  Hwaran Lee, Edward Choi
Categories: cs.CL
Comments: 35 pages, 7 figures, 16 tables
\\ ( https://arxiv.org/abs/2402.13605 ,  9560kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16159
replaced with revised version Fri, 15 Mar 2024 18:29:52 GMT   (2142kb,D)

Title: DistALANER: Distantly Supervised Active Learning Augmented Named Entity
  Recognition in the Open Source Software Ecosystem
Authors: Somnath Banerjee, Avik Dutta, Aaditya Agrawal, Rima Hazra, Animesh
  Mukherjee
Categories: cs.CL
Comments: Under review
\\ ( https://arxiv.org/abs/2402.16159 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17527
replaced with revised version Mon, 18 Mar 2024 16:21:24 GMT   (1199kb,D)

Title: Predict the Next Word: Humans exhibit uncertainty in this task and
  language models _____
Authors: Evgenia Ilia and Wilker Aziz
Categories: cs.CL cs.AI
Comments: 22 pages, EACL 2024
\\ ( https://arxiv.org/abs/2402.17527 ,  1199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17532
replaced with revised version Sat, 16 Mar 2024 04:31:47 GMT   (6916kb,D)

Title: Retrieval is Accurate Generation
Authors: Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou,
  Shuming Shi
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2402.17532 ,  6916kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17946
replaced with revised version Sat, 16 Mar 2024 15:29:06 GMT   (1096kb,D)

Title: Gradient-Free Adaptive Global Pruning for Pre-trained Language Models
Authors: Guangji Bai, Yijiang Li, Chen Ling, Kibaek Kim, Liang Zhao
Categories: cs.CL
Comments: Preprint. Under review
\\ ( https://arxiv.org/abs/2402.17946 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01139
replaced with revised version Mon, 18 Mar 2024 14:55:46 GMT   (4489kb,D)

Title: ParallelPARC: A Scalable Pipeline for Generating Natural-Language
  Analogies
Authors: Oren Sultan, Yonatan Bitton, Ron Yosef, Dafna Shahaf
Categories: cs.CL cs.AI
Comments: NAACL 2024 main
\\ ( https://arxiv.org/abs/2403.01139 ,  4489kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01222
replaced with revised version Mon, 18 Mar 2024 10:51:14 GMT   (8259kb,D)

Title: Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions
Authors: Flor Miriam Plaza-del-Arco, Alba Curry, Amanda Cercas Curry, Dirk Hovy
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.01222 ,  8259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01570
replaced with revised version Sat, 16 Mar 2024 04:07:01 GMT   (1381kb,D)

Title: SERVAL: Synergy Learning between Vertical Models and LLMs towards
  Oracle-Level Zero-shot Medical Prediction
Authors: Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun,
  Jian Wu
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.01570 ,  1381kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03121
replaced with revised version Mon, 18 Mar 2024 11:04:44 GMT   (8278kb,D)

Title: Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes
  in Emotion Attribution
Authors: Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Curry, Gavin
  Abercrombie, Dirk Hovy
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.03121 ,  8278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04325
replaced with revised version Mon, 18 Mar 2024 11:17:48 GMT   (1360kb,D)

Title: Measuring Meaning Composition in the Human Brain with Composition Scores
  from Large Language Models
Authors: Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.04325 ,  1360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06754
replaced with revised version Sat, 16 Mar 2024 12:43:33 GMT   (760kb,D)

Title: ALaRM: Align Language Models via Hierarchical Rewards Modeling
Authors: Yuhang Lai, Siyuan Wang, Shujun Liu, Xuanjing Huang, Zhongyu Wei
Categories: cs.CL cs.AI cs.LG
Comments: 15 pages, 6 figures
\\ ( https://arxiv.org/abs/2403.06754 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07311
replaced with revised version Mon, 18 Mar 2024 16:35:29 GMT   (2262kb,D)

Title: Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
Authors: Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, Yongfeng
  Zhang
Categories: cs.CL cs.LG
Comments: 23 pages, 2 figures
\\ ( https://arxiv.org/abs/2403.07311 ,  2262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07440
replaced with revised version Mon, 18 Mar 2024 10:13:05 GMT   (1385kb,D)

Title: Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning
Authors: Yao Liang, Yuwei Wang, Yang Li, Yi Zeng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.07440 ,  1385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08281
replaced with revised version Mon, 18 Mar 2024 07:21:28 GMT   (7386kb,D)

Title: Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized Language Models
Authors: Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing
  Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.08281 ,  7386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08293
replaced with revised version Mon, 18 Mar 2024 08:48:30 GMT   (3542kb,D)

Title: Generative Pretrained Structured Transformers: Unsupervised Syntactic
  Language Models at Scale
Authors: Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu
Categories: cs.CL cs.AI
Comments: preprint
\\ ( https://arxiv.org/abs/2403.08293 ,  3542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09017
replaced with revised version Fri, 15 Mar 2024 23:52:18 GMT   (5635kb,D)

Title: AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic
Authors: Emad A. Alghamdi, Reem I. Masoud, Deema Alnuhait, Afnan Y. Alomairi,
  Ahmed Ashraf, Mohamed Zaytoon
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.09017 ,  5635kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09113
replaced with revised version Sun, 17 Mar 2024 17:55:47 GMT   (129kb,D)

Title: AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based
  on Meta Learning
Authors: Ruiyi Zhang, Rushi Qiang, Sai Ashish Somayajula, Pengtao Xie
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.09113 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09629
replaced with revised version Mon, 18 Mar 2024 07:56:48 GMT   (525kb,D)

Title: Quiet-STaR: Language Models Can Teach Themselves to Think Before
  Speaking
Authors: Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber,
  Noah D. Goodman
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.09629 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09732
replaced with revised version Mon, 18 Mar 2024 12:45:41 GMT   (159kb,D)

Title: PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with
  Cross-consistency
Authors: Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru
  Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.09732 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:1703.05537
replaced with revised version Mon, 18 Mar 2024 11:37:21 GMT   (3303kb)

Title: Shift Aggregate Extract Networks
Authors: Francesco Orsini, Daniele Baracchi and Paolo Frasconi
Categories: cs.LG stat.ML
Journal-ref: Frontiers Robotics AI, 2018, 5(APR), 42
DOI: 10.3389/frobt.2018.00042
\\ ( https://arxiv.org/abs/1703.05537 ,  3303kb)
------------------------------------------------------------------------------
\\
arXiv:2102.09407
replaced with revised version Sat, 16 Mar 2024 12:40:45 GMT   (4857kb,D)

Title: Adaptive Rational Activations to Boost Deep Reinforcement Learning
Authors: Quentin Delfosse, Patrick Schramowski, Martin Mundt, Alejandro Molina
  and Kristian Kersting
Categories: cs.LG
Comments: Main paper: 9 pages, References: 4 pages, Appendix: 11 pages. Main
  paper: 5 figures, Appendix: 6 figures, 6 tables. Rational Activation
  Functions repository: https://github.com/k4ntz/activation-functions Rational
  Reinforcement Learning: https://github.com/ml-research/rational_rl
\\ ( https://arxiv.org/abs/2102.09407 ,  4857kb)
------------------------------------------------------------------------------
\\
arXiv:2204.12095
replaced with revised version Sun, 17 Mar 2024 19:37:05 GMT   (390kb,D)

Title: PyGOD: A Python Library for Graph Outlier Detection
Authors: Kay Liu, Yingtong Dou, Xueying Ding, Xiyang Hu, Ruitong Zhang, Hao
  Peng, Lichao Sun, Philip S. Yu
Categories: cs.LG cs.SI
Comments: Accepted by JMLR. Library available at https://pygod.org
\\ ( https://arxiv.org/abs/2204.12095 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2205.12532
replaced with revised version Sat, 16 Mar 2024 10:46:06 GMT   (10871kb,D)

Title: Skill Machines: Temporal Logic Skill Composition in Reinforcement
  Learning
Authors: Geraud Nangue Tasse, Devon Jarvis, Steven James, Benjamin Rosman
Categories: cs.LG cs.LO
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2205.12532 ,  10871kb)
------------------------------------------------------------------------------
\\
arXiv:2206.04742
replaced with revised version Sun, 17 Mar 2024 16:38:47 GMT   (2381kb,D)

Title: Accelerating Asynchronous Federated Learning Convergence via
  Opportunistic Mobile Relaying
Authors: Jieming Bian, Jie Xu
Categories: cs.LG cs.AI
Comments: IEEE Transactions on Vehicular Technology (accepted)
\\ ( https://arxiv.org/abs/2206.04742 ,  2381kb)
------------------------------------------------------------------------------
\\
arXiv:2209.05208
replaced with revised version Mon, 18 Mar 2024 09:28:39 GMT   (25470kb,D)

Title: Graph Neural Modeling of Network Flows
Authors: Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi
Categories: cs.LG cs.AI cs.NI
\\ ( https://arxiv.org/abs/2209.05208 ,  25470kb)
------------------------------------------------------------------------------
\\
arXiv:2209.12605
replaced with revised version Mon, 18 Mar 2024 01:32:52 GMT   (42771kb,D)

Title: MechProNet: Machine Learning Prediction of Mechanical Properties in
  Metal Additive Manufacturing
Authors: Parand Akbari and Masoud Zamani and Amir Mostafaei
Categories: cs.LG cond-mat.mtrl-sci cs.AI
\\ ( https://arxiv.org/abs/2209.12605 ,  42771kb)
------------------------------------------------------------------------------
\\
arXiv:2210.05279
replaced with revised version Mon, 18 Mar 2024 08:57:55 GMT   (1348kb,D)

Title: Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity
Authors: William de Vazelhes, Hualin Zhang, Huimin Wu, Xiao-Tong Yuan, Bin Gu
Categories: cs.LG math.OC
Comments: Accepted for publication at NeurIPS 2022
\\ ( https://arxiv.org/abs/2210.05279 ,  1348kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06891
replaced with revised version Sun, 17 Mar 2024 11:45:52 GMT   (2097kb,D)

Title: Experimental Design for Multi-Channel Imaging via Task-Driven Feature
  Selection
Authors: Stefano B. Blumberg, Paddy J. Slator, Daniel C. Alexander
Categories: cs.LG cs.AI q-bio.NC
Comments: Accepted In: International Conference on Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2210.06891 ,  2097kb)
------------------------------------------------------------------------------
\\
arXiv:2211.11656
replaced with revised version Sat, 16 Mar 2024 00:47:57 GMT   (2065kb,D)

Title: SIFU: Sequential Informed Federated Unlearning for Efficient and
  Provable Client Unlearning in Federated Optimization
Authors: Yann Fraboni, Martin Van Waerebeke, Kevin Scaman, Richard Vidal,
  Laetitia Kameni, Marco Lorenzi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2211.11656 ,  2065kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11476
replaced with revised version Mon, 18 Mar 2024 15:53:34 GMT   (28731kb,D)

Title: Generalized Munchausen Reinforcement Learning using Tsallis KL
  Divergence
Authors: Lingwei Zhu, Zheng Chen, Matthew Schlegel, Martha White
Categories: cs.LG cs.AI
Comments: Accepted by NeurIPS 2023
\\ ( https://arxiv.org/abs/2301.11476 ,  28731kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00569
replaced with revised version Sat, 16 Mar 2024 19:11:06 GMT   (5100kb)

Title: Agnostic Visual Recommendation Systems: Open Challenges and Future
  Directions
Authors: Luca Podo, Bardh Prenkaj, Paola Velardi
Categories: cs.LG
Comments: 16 pages, 4 figures
Journal-ref: TVCG (2024)
DOI: 10.1109/TVCG.2024.3374571
\\ ( https://arxiv.org/abs/2302.00569 ,  5100kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01404
replaced with revised version Sun, 17 Mar 2024 17:36:02 GMT   (3267kb,D)

Title: Provably Bounding Neural Network Preimages
Authors: Suhas Kotha, Christopher Brix, Zico Kolter, Krishnamurthy Dvijotham,
  Huan Zhang
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: NeurIPS 2023 (Spotlight)
\\ ( https://arxiv.org/abs/2302.01404 ,  3267kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02609
replaced with revised version Sat, 16 Mar 2024 19:50:05 GMT   (3905kb,D)

Title: Improving Domain Generalization with Domain Relations
Authors: Huaxiu Yao, Xinyu Yang, Xinyi Pan, Shengchao Liu, Pang Wei Koh,
  Chelsea Finn
Categories: cs.LG
Comments: Accepted by ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2302.02609 ,  3905kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04552
replaced with revised version Sat, 16 Mar 2024 15:36:02 GMT   (58kb)

Title: Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial
  Online Convex Optimization
Authors: Sijia Chen, Yu-Jie Zhang, Wei-Wei Tu, Peng Zhao, Lijun Zhang
Categories: cs.LG stat.ML
Comments: v3 substantially improves the presentation and has a few
  improvements, including the regret bound for strongly convex functions; v2 is
  an extended version that enriches the content with improved regret bounds for
  strongly convex functions, discussions on the optimism design for dynamic
  regret minimization, and extensions to non-smooth scenarios; v1 is the ICML
  2023 conference version
\\ ( https://arxiv.org/abs/2302.04552 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05825
replaced with revised version Sat, 16 Mar 2024 09:12:03 GMT   (527kb,D)

Title: Koopman-based generalization bound: New aspect for full-rank weights
Authors: Yuka Hashimoto, Sho Sonoda, Isao Ishikawa, Atsushi Nitanda, Taiji
  Suzuki
Categories: cs.LG math.FA stat.ML
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2302.05825 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06839
replaced with revised version Sun, 17 Mar 2024 22:19:40 GMT   (6689kb,D)

Title: Predicting the long-term collective behaviour of fish pairs with deep
  learning
Authors: Vaios Papaspyros, Ram\'on Escobedo, Alexandre Alahi, Guy Theraulaz,
  Cl\'ement Sire, Francesco Mondada
Categories: cs.LG
DOI: 10.1098/rsif.2023.0630
\\ ( https://arxiv.org/abs/2302.06839 ,  6689kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04488
replaced with revised version Mon, 18 Mar 2024 14:16:22 GMT   (1871kb,D)

Title: Magnushammer: A Transformer-Based Approach to Premise Selection
Authors: Maciej Miku{\l}a, Szymon Tworkowski, Szymon Antoniak, Bartosz
  Piotrowski, Albert Qiaochu Jiang, Jin Peng Zhou, Christian Szegedy, {\L}ukasz
  Kuci\'nski, Piotr Mi{\l}o\'s, Yuhuai Wu
Categories: cs.LG cs.AI cs.LO
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2303.04488 ,  1871kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05656
replaced with revised version Mon, 18 Mar 2024 13:34:44 GMT   (2673kb,D)

Title: EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models
Authors: Hongyi Yuan, Songchi Zhou, Sheng Yu
Categories: cs.LG cs.CV
Comments: Accepted by TMLR, preprint of camera-ready version
\\ ( https://arxiv.org/abs/2303.05656 ,  2673kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00133
replaced with revised version Mon, 18 Mar 2024 17:35:16 GMT   (13087kb,D)

Title: DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate
  Decision Stumps
Authors: Angelos Chatzimparmpas, Rafael M. Martins, Alexandru C. Telea, Andreas
  Kerren
Categories: cs.LG cs.HC
Comments: This manuscript is accepted for publication in Computer Graphics
  Forum (CGF)
DOI: 10.1111/cgf.15004
\\ ( https://arxiv.org/abs/2304.00133 ,  13087kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03376
replaced with revised version Mon, 18 Mar 2024 13:02:17 GMT   (10404kb,D)

Title: Interpretable statistical representations of neural population dynamics
  and geometry
Authors: Adam Gosztolai, Robert L. Peach, Alexis Arnaudon, Mauricio Barahona,
  Pierre Vandergheynst
Categories: cs.LG math.DS q-bio.NC q-bio.QM
Comments: Version before peer review
\\ ( https://arxiv.org/abs/2304.03376 ,  10404kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06094
replaced with revised version Mon, 18 Mar 2024 08:11:08 GMT   (28756kb,D)

Title: Energy-guided Entropic Neural Optimal Transport
Authors: Petr Mokrov and Alexander Korotin and Alexander Kolesov and Nikita
  Gushchin and Evgeny Burnaev
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2304.06094 ,  28756kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13124
replaced with revised version Mon, 18 Mar 2024 10:27:10 GMT   (8092kb,D)

Title: Hang-Time HAR: A Benchmark Dataset for Basketball Activity Recognition
  using Wrist-Worn Inertial Sensors
Authors: Alexander Hoelzemann, Julia Lee Romero, Marius Bock, Kristof Van
  Laerhoven, Qin Lv
Categories: cs.LG cs.HC
Journal-ref: MDPI Sensors, 25 June 2023, Special Issue Inertial Measurement
  Units in Sport
DOI: 10.3390/s23135879
\\ ( https://arxiv.org/abs/2305.13124 ,  8092kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13991
replaced with revised version Mon, 18 Mar 2024 14:35:21 GMT   (125kb,D)

Title: Expressive Losses for Verified Robustness via Convex Combinations
Authors: Alessandro De Palma, Rudy Bunel, Krishnamurthy Dvijotham, M. Pawan
  Kumar, Robert Stanforth, Alessio Lomuscio
Categories: cs.LG cs.CR stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.13991 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16877
replaced with revised version Mon, 18 Mar 2024 14:27:21 GMT   (1363kb,D)

Title: Distributional Reinforcement Learning with Dual Expectile-Quantile
  Regression
Authors: Sami Jullien, Romain Deffayet, Jean-Michel Renders, Paul Groth,
  Maarten de Rijke
Categories: cs.LG cs.AI
Comments: 16 pages, 3 figures, 1 algorithm
ACM-class: I.2.8; G.3
\\ ( https://arxiv.org/abs/2305.16877 ,  1363kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18246
replaced with revised version Mon, 18 Mar 2024 00:37:12 GMT   (5502kb,D)

Title: Provable and Practical: Efficient Exploration in Reinforcement Learning
  via Langevin Monte Carlo
Authors: Haque Ishfaq, Qingfeng Lan, Pan Xu, A. Rupam Mahmood, Doina Precup,
  Anima Anandkumar, Kamyar Azizzadenesheli
Categories: cs.LG
Comments: Published in The Twelfth International Conference on Learning
  Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2305.18246 ,  5502kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00321
replaced with revised version Sat, 16 Mar 2024 01:29:08 GMT   (266kb,D)

Title: Improving Offline RL by Blending Heuristics
Authors: Sinong Geng, Aldo Pacchiano, Andrey Kolobov, Ching-An Cheng
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.00321 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04590
replaced with revised version Sun, 17 Mar 2024 06:43:38 GMT   (2942kb,D)

Title: Proximity-Informed Calibration for Deep Neural Networks
Authors: Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing
  Xu, Bryan Hooi
Categories: cs.LG cs.AI
Comments: The paper is accepted by NeurIPS 2023. The code is available at:
  https://github.com/MiaoXiong2320/ProximityBias-Calibration
\\ ( https://arxiv.org/abs/2306.04590 ,  2942kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09884
replaced with revised version Sat, 16 Mar 2024 00:02:49 GMT   (5523kb,D)

Title: Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments
  in JAX
Authors: Cl\'ement Bonnet, Daniel Luo, Donal Byrne, Shikha Surana, Sasha
  Abramowitz, Paul Duckworth, Vincent Coyette, Laurence I. Midgley, Elshadai
  Tegegn, Tristan Kalloniatis, Omayma Mahjoub, Matthew Macfarlane, Andries P.
  Smit, Nathan Grinsztajn, Raphael Boige, Cemlyn N. Waters, Mohamed A. Mimouni,
  Ulrich A. Mbou Sob, Ruan de Kock, Siddarth Singh, Daniel Furelos-Blanco,
  Victor Le, Arnu Pretorius, Alexandre Laterre
Categories: cs.LG cs.AI
Comments: 9 pages + 21 pages of appendices and references. Published at ICLR
  2024
\\ ( https://arxiv.org/abs/2306.09884 ,  5523kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06104
replaced with revised version Mon, 18 Mar 2024 11:01:30 GMT   (737kb,D)

Title: Deep learning for dynamic graphs: models and benchmarks
Authors: Alessio Gravina and Davide Bacciu
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2307.06104 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11714
replaced with revised version Mon, 18 Mar 2024 09:55:08 GMT   (43kb)

Title: Convergence of SGD for Training Neural Networks with Sliced Wasserstein
  Losses
Authors: Eloi Tanguy
Categories: cs.LG math.OC math.PR
Journal-ref: Transactions on Machine Learning Research, 2023 2835-8856
\\ ( https://arxiv.org/abs/2307.11714 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06668
replaced with revised version Sun, 17 Mar 2024 18:29:53 GMT   (9542kb,D)

Title: Large Language Models and Foundation Models in Smart Agriculture:
  Basics, Opportunities, and Challenges
Authors: Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan
  Yin and Zhaojian Li
Categories: cs.LG cs.CV
Comments: 18 pages, 3 figures
\\ ( https://arxiv.org/abs/2308.06668 ,  9542kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07233
replaced with revised version Mon, 18 Mar 2024 02:03:52 GMT   (1780kb,D)

Title: A Unifying Generator Loss Function for Generative Adversarial Networks
Authors: Justin Veiner, Fady Alajaji, Bahman Gharesifard
Categories: cs.LG
Comments: 33 pages, 4 figures, 12 tables
\\ ( https://arxiv.org/abs/2308.07233 ,  1780kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07553
replaced with revised version Mon, 18 Mar 2024 15:05:36 GMT   (528kb,D)

Title: Enhancing the Antidote: Improved Pointwise Certifications against
  Poisoning Attacks
Authors: Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah M. Erfani, Benjamin
  I. P. Rubinstein
Categories: cs.LG cs.CR
Journal-ref: Proceedings of the 2023 AAAI Conference on Artificial
  Intelligence, 37(7), 8861-8869
DOI: 10.1609/aaai.v37i7.26065
\\ ( https://arxiv.org/abs/2308.07553 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13137
replaced with revised version Mon, 18 Mar 2024 05:33:22 GMT   (590kb,D)

Title: OmniQuant: Omnidirectionally Calibrated Quantization for Large Language
  Models
Authors: Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao,
  Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo
Categories: cs.LG cs.CL
Comments: ICLR 2024 Camera Ready
\\ ( https://arxiv.org/abs/2308.13137 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14969
replaced with revised version Fri, 15 Mar 2024 21:04:31 GMT   (45651kb,D)

Title: Uncovering the Hidden Cost of Model Compression
Authors: Diganta Misra, Muawiz Chaudhary, Agam Goyal, Bharat Runwal, Pin Yu
  Chen
Categories: cs.LG cs.CV
Comments: Preprint
\\ ( https://arxiv.org/abs/2308.14969 ,  45651kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08249
replaced with revised version Mon, 18 Mar 2024 13:48:37 GMT   (3435kb,D)

Title: Deep Nonnegative Matrix Factorization with Beta Divergences
Authors: Valentin Leplat, Le Thi Khanh Hien, Akwum Onwunta, Nicolas Gillis
Categories: cs.LG cs.NA eess.SP math.NA stat.ML
Comments: 34 pages. We have improved the presentation of the paper, corrected a
  few typoes, and added the MU for beta=1/2. Accepted in Neural Computation
\\ ( https://arxiv.org/abs/2309.08249 ,  3435kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09258
replaced with revised version Sun, 17 Mar 2024 21:22:05 GMT   (207kb,D)

Title: Global Convergence of SGD For Logistic Loss on Two Layer Neural Nets
Authors: Pulkit Gopalani, Samyak Jha, Anirbit Mukherjee
Categories: cs.LG math.OC stat.ML
Comments: 18 Pages, 1 figure. Published in the Transactions on Machine Learning
  Research (TMLR) in Feb, 2024. arXiv admin note: substantial text overlap with
  arXiv:2210.11452
\\ ( https://arxiv.org/abs/2309.09258 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10370
replaced with revised version Sun, 17 Mar 2024 08:09:45 GMT   (26kb)

Title: Geometric structure of shallow neural networks and constructive
  ${\mathcal L}^2$ cost minimization
Authors: Thomas Chen, Patricia Mu\~noz Ewald
Categories: cs.LG cs.AI math-ph math.MP math.OC stat.ML
Comments: AMS Latex, 28 pages. Exposition has been streamlined
MSC-class: 57R70, 62M45
\\ ( https://arxiv.org/abs/2309.10370 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12128
replaced with revised version Fri, 15 Mar 2024 18:59:11 GMT   (358kb,D)

Title: Convergence and Recovery Guarantees of Unsupervised Neural Networks for
  Inverse Problems
Authors: Nathan Buskulic, Jalal Fadili, Yvain Qu\'eau
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.12128 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16512
replaced with revised version Sat, 16 Mar 2024 05:48:39 GMT   (2594kb,D)

Title: From Complexity to Clarity: Analytical Expressions of Deep Neural
  Network Weights via Clifford's Geometric Algebra and Convexity
Authors: Mert Pilanci
Categories: cs.LG cs.AI cs.NE math.OC stat.ML
\\ ( https://arxiv.org/abs/2309.16512 ,  2594kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16883
replaced with revised version Mon, 18 Mar 2024 08:43:46 GMT   (1201kb,D)

Title: The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing
Authors: Blaise Delattre, Alexandre Araujo, Quentin Barth\'elemy and Alexandre
  Allauzen
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2309.16883 ,  1201kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00754
replaced with revised version Sat, 16 Mar 2024 19:28:08 GMT   (2357kb,D)

Title: Analyzing and Mitigating Object Hallucination in Large Vision-Language
  Models
Authors: Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng,
  Chelsea Finn, Mohit Bansal, Huaxiu Yao
Categories: cs.LG cs.CL cs.CV
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.00754 ,  2357kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00873
replaced with revised version Fri, 15 Mar 2024 21:35:51 GMT   (4217kb,D)

Title: Deep Neural Networks Tend To Extrapolate Predictably
Authors: Katie Kang, Amrith Setlur, Claire Tomlin, Sergey Levine
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.00873 ,  4217kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01012
replaced with revised version Mon, 18 Mar 2024 10:32:59 GMT   (1054kb,D)

Title: Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised
  Learning
Authors: James Chapman, Lennie Wells, Ana Lawry Aguila
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2310.01012 ,  1054kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01557
replaced with revised version Sun, 17 Mar 2024 23:23:31 GMT   (12218kb,D)

Title: SmartPlay: A Benchmark for LLMs as Intelligent Agents
Authors: Yue Wu, Xuan Tang, Tom M. Mitchell, Yuanzhi Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.01557 ,  12218kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01655
replaced with revised version Sun, 17 Mar 2024 23:35:24 GMT   (413kb,D)

Title: PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels
Authors: Praneeth Kacham, Vahab Mirrokni, Peilin Zhong
Categories: cs.LG
Comments: Added results of more experiments. Added a link to our JAX
  implementation of models
\\ ( https://arxiv.org/abs/2310.01655 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01820
replaced with revised version Sun, 17 Mar 2024 17:35:59 GMT   (683kb,D)

Title: Towards Robust Fidelity for Evaluating Explainability of Graph Neural
  Networks
Authors: Xu Zheng, Farhad Shirani, Tianchun Wang, Wei Cheng, Zhuomin Chen,
  Haifeng Chen, Hua Wei, Dongsheng Luo
Categories: cs.LG
Comments: Accepted by International Conference on Learning Representations
  (ICLR 2024); 26 Pages, 12 figures
\\ ( https://arxiv.org/abs/2310.01820 ,  683kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04971
replaced with revised version Sun, 17 Mar 2024 23:47:33 GMT   (586kb,D)

Title: Understanding the Robustness of Multi-modal Contrastive Learning to
  Distribution Shift
Authors: Yihao Xue, Siddharth Joshi, Dang Nguyen, Baharan Mirzasoleiman
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.04971 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05067
replaced with revised version Sat, 16 Mar 2024 01:17:11 GMT   (147kb,D)

Title: Robust-GBDT: GBDT with Nonconvex Loss for Tabular Classification in the
  Presence of Label Noise and Class Imbalance
Authors: Jiaqi Luo, Yuedong Quan, Shixin Xu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.05067 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06171
replaced with revised version Sat, 16 Mar 2024 15:06:07 GMT   (18368kb,D)

Title: Memory-Consistent Neural Networks for Imitation Learning
Authors: Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer,
  Insup Lee
Categories: cs.LG cs.AI cs.RO
Comments: ICLR 2024. 26 pages (9 main pages)
\\ ( https://arxiv.org/abs/2310.06171 ,  18368kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06790
replaced with revised version Sun, 17 Mar 2024 20:38:27 GMT   (14028kb,D)

Title: Enhancing Predictive Capabilities in Data-Driven Dynamical Modeling with
  Automatic Differentiation: Koopman and Neural ODE Approaches
Authors: C. Ricardo Constante-Amores and Alec J. Linot and Michael D. Graham
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.06790 ,  14028kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08754
replaced with revised version Sun, 17 Mar 2024 14:17:22 GMT   (146kb,D)

Title: Tokenizer Choice For LLM Training: Negligible or Crucial?
Authors: Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max
  L\"ubbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper
  Schulze Buschhoff, Charvi Jain, Alexander Arno Weber, Lena Jurkschat, Hammam
  Abdelwahab, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Samuel
  Weinbach, Rafet Sifa, Stefan Kesselheim, Nicolas Flores-Herr
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.08754 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10207
replaced with revised version Mon, 18 Mar 2024 09:05:12 GMT   (35980kb,D)

Title: Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in
  the Real World
Authors: Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun
  Zhu, Yizhou Wang
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.10207 ,  35980kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10636
replaced with revised version Sun, 17 Mar 2024 22:22:08 GMT   (3567kb,D)

Title: Dual-Encoders for Extreme Multi-Label Classification
Authors: Nilesh Gupta, Devvrit Khatri, Ankit S Rawat, Srinadh Bhojanapalli,
  Prateek Jain, Inderjit Dhillon
Categories: cs.LG
Comments: 27 pages, 8 figures
Journal-ref: ICLR 2024 camera-ready publication
\\ ( https://arxiv.org/abs/2310.10636 ,  3567kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17405
replaced with revised version Sat, 16 Mar 2024 17:58:33 GMT   (3773kb,D)

Title: Causal Modeling with Stationary Diffusions
Authors: Lars Lorch, Andreas Krause, Bernhard Sch\"olkopf
Categories: cs.LG
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2310.17405 ,  3773kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17513
replaced with revised version Mon, 18 Mar 2024 02:13:24 GMT   (4145kb,D)

Title: The Expressive Power of Low-Rank Adaptation
Authors: Yuchen Zeng, Kangwook Lee
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: 40 pages, 5 figures
\\ ( https://arxiv.org/abs/2310.17513 ,  4145kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17645
replaced with revised version Sun, 17 Mar 2024 04:40:48 GMT   (7908kb,D)

Title: PubDef: Defending Against Transfer Attacks From Public Models
Authors: Chawin Sitawarin, Jaewon Chang, David Huang, Wesson Altoyan, David
  Wagner
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: ICLR 2024. Code available at https://github.com/wagner-group/pubdef
\\ ( https://arxiv.org/abs/2310.17645 ,  7908kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17786
replaced with revised version Sat, 16 Mar 2024 21:41:34 GMT   (33346kb,D)

Title: Understanding when Dynamics-Invariant Data Augmentations Benefit
  Model-Free Reinforcement Learning Updates
Authors: Nicholas E. Corrado, Josiah P. Hanna
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.17786 ,  33346kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18247
replaced with revised version Sat, 16 Mar 2024 21:21:18 GMT   (20503kb,D)

Title: Guided Data Augmentation for Offline Reinforcement Learning and
  Imitation Learning
Authors: Nicholas E. Corrado, Yuxiao Qu, John U. Balis, Adam Labiosa, Josiah P.
  Hanna
Categories: cs.LG cs.RO
\\ ( https://arxiv.org/abs/2310.18247 ,  20503kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18515
replaced with revised version Sat, 16 Mar 2024 16:49:26 GMT   (18344kb,D)

Title: Learning to design protein-protein interactions with enhanced
  generalization
Authors: Anton Bushuiev, Roman Bushuiev, Petr Kouba, Anatolii Filkin, Marketa
  Gabrielova, Michal Gabriel, Jiri Sedlar, Tomas Pluskal, Jiri Damborsky,
  Stanislav Mazurenko, Josef Sivic
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.18515 ,  18344kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00318
replaced with revised version Mon, 18 Mar 2024 07:21:27 GMT   (17538kb,D)

Title: Flooding Regularization for Stable Training of Generative Adversarial
  Networks
Authors: Iu Yahiro, Takashi Ishida, Naoto Yokoya
Categories: cs.LG cs.CV
Comments: 25 pages, 9 figures, 18 tables
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/2311.00318 ,  17538kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00519
replaced with revised version Sat, 16 Mar 2024 15:41:28 GMT   (14258kb,D)

Title: REBAR: Retrieval-Based Reconstruction for Time-series Contrastive
  Learning
Authors: Maxwell A. Xu, Alexander Moreno, Hui Wei, Benjamin M. Marlin, James M.
  Rehg
Categories: cs.LG
Comments: ICLR 2024 | Code available at: https://github.com/maxxu05/rebar
Journal-ref: The Eleventh International Conference on Learning Representations
  (2024)
\\ ( https://arxiv.org/abs/2311.00519 ,  14258kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03351
replaced with revised version Sun, 17 Mar 2024 04:40:06 GMT   (13886kb,D)

Title: Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with
  Multi-Step On-Policy Optimization
Authors: Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, Huazhe Xu
Categories: cs.LG cs.RO
Comments: Our website: https://lei-kun.github.io/uni-o4/
\\ ( https://arxiv.org/abs/2311.03351 ,  13886kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10049
replaced with revised version Sat, 16 Mar 2024 10:25:41 GMT   (9546kb,D)

Title: Inherently Interpretable Time Series Classification via Multiple
  Instance Learning
Authors: Joseph Early, Gavin KC Cheung, Kurt Cutajar, Hanting Xie, Jas Kandola,
  Niall Twomey
Categories: cs.LG cs.AI
Comments: Published at ICLR 2024. 29 pages (9 main, 3 ref, 17 appendix)
\\ ( https://arxiv.org/abs/2311.10049 ,  9546kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12424
replaced with revised version Sat, 16 Mar 2024 21:10:38 GMT   (2328kb,D)

Title: Looped Transformers are Better at Learning Learning Algorithms
Authors: Liu Yang, Kangwook Lee, Robert Nowak, Dimitris Papailiopoulos
Categories: cs.LG cs.NE
Comments: Accepted for publication at ICLR 2024
\\ ( https://arxiv.org/abs/2311.12424 ,  2328kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13817
replaced with revised version Fri, 15 Mar 2024 19:31:41 GMT   (17448kb,D)

Title: Molecular Identification and Peak Assignment: Leveraging Multi-Level
  Multimodal Alignment on NMR
Authors: Hao Xu, Zhengyang Zhou, Pengyu Hong
Categories: cs.LG physics.chem-ph q-bio.QM
\\ ( https://arxiv.org/abs/2311.13817 ,  17448kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15487
replaced with revised version Mon, 18 Mar 2024 07:51:52 GMT   (15kb)

Title: Global $\mathcal{L}^2$ minimization at uniform exponential rate via
  geometrically adapted gradient descent in Deep Learning
Authors: Thomas Chen
Categories: cs.LG cs.AI math-ph math.MP math.OC stat.ML
Comments: AMS Latex, 16 pages. Section 2.1 on rank condition, and Section 2.4
  on the trapping of orbits in the standard gradient descent flow added. Title
  changed
MSC-class: 57R70, 62M45
\\ ( https://arxiv.org/abs/2311.15487 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16199
replaced with revised version Sat, 16 Mar 2024 21:22:11 GMT   (31996kb,D)

Title: Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for
  Molecule Generation
Authors: Ameya Daigavane, Song Kim, Mario Geiger, Tess Smidt
Categories: cs.LG q-bio.BM
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2311.16199 ,  31996kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16834
replaced with revised version Mon, 18 Mar 2024 17:39:11 GMT   (2299kb,D)

Title: Modular Neural Networks for Time Series Forecasting: Interpretability
  and Feature Selection using Attention
Authors: Qiqi Su, Christos Kloukinas, Artur d'Avila Garcez
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.16834 ,  2299kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00502
replaced with revised version Mon, 18 Mar 2024 10:32:01 GMT   (4664kb,D)

Title: A Comprehensive Evaluation of Augmentations for Robust OOD
  Self-Supervised Contrastive Phonocardiogram Representation Learning
Authors: Aristotelis Ballas, Vasileios Papapanagiotou and Christos Diou
Categories: cs.LG cs.SD q-bio.QM
Comments: PREPRINT Manuscript under review
\\ ( https://arxiv.org/abs/2312.00502 ,  4664kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05720
replaced with revised version Fri, 15 Mar 2024 18:23:05 GMT   (723kb,D)

Title: Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer
  Inputs of Language Models in Federated Learning
Authors: Jianwei Li, Sheng Liu, Qi Lei
Categories: cs.LG cs.AI cs.CL cs.CR
\\ ( https://arxiv.org/abs/2312.05720 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07718
replaced with revised version Fri, 15 Mar 2024 21:45:39 GMT   (649kb,D)

Title: CaVE: A Cone-Aligned Approach for Fast Predict-then-optimize with Binary
  Linear Programs
Authors: Bo Tang, Elias B. Khalil
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2312.07718 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10194
replaced with revised version Fri, 15 Mar 2024 18:25:34 GMT   (5636kb,D)

Title: Multi-Objective Reinforcement Learning-based Approach for Pressurized
  Water Reactor Optimization
Authors: Paul Seurin and Koroush Shirvan
Categories: cs.LG cs.CE
\\ ( https://arxiv.org/abs/2312.10194 ,  5636kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12741
replaced with revised version Sun, 17 Mar 2024 06:00:10 GMT   (4585kb,D)

Title: Locally Optimal Fixed-Budget Best Arm Identification in Two-Armed
  Gaussian Bandits with Unknown Variances
Authors: Masahiro Kato
Categories: cs.LG econ.EM math.ST stat.ME stat.ML stat.TH
\\ ( https://arxiv.org/abs/2312.12741 ,  4585kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14134
replaced with revised version Mon, 18 Mar 2024 04:08:54 GMT   (18824kb,D)

Title: Diffusion Reward: Learning Rewards via Conditional Video Diffusion
Authors: Tao Huang and Guangqi Jiang and Yanjie Ze and Huazhe Xu
Categories: cs.LG cs.CV cs.RO
Comments: Project page and code: https://diffusion-reward.github.io/
\\ ( https://arxiv.org/abs/2312.14134 ,  18824kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14567
replaced with revised version Sun, 17 Mar 2024 05:54:11 GMT   (2594kb,D)

Title: Accelerated Convergence of Stochastic Heavy Ball Method under
  Anisotropic Gradient Noise
Authors: Rui Pan, Yuxing Liu, Xiaoyu Wang, Tong Zhang
Categories: cs.LG math.OC
Comments: Published at ICLR 2024
\\ ( https://arxiv.org/abs/2312.14567 ,  2594kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01519
replaced with revised version Sat, 16 Mar 2024 13:37:48 GMT   (570kb)

Title: Exploring the Frontiers of LLMs in Psychological Applications: A
  Comprehensive Review
Authors: Luoma Ke (1), Song Tong (1), Peng Cheng (2), Kaiping Peng (1) ((1)
  Department of Psychology, Tsinghua University, (2) School of Social Science,
  Tsinghua University)
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.01519 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01851
replaced with revised version Fri, 15 Mar 2024 21:43:10 GMT   (2507kb,D)

Title: The Power of Training: How Different Neural Network Setups Influence the
  Energy Demand
Authors: Daniel Gei{\ss}ler, Bo Zhou, Mengxi Liu, Sungho Suh, Paul Lukowicz
Categories: cs.LG cs.AI cs.PF
\\ ( https://arxiv.org/abs/2401.01851 ,  2507kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06604
replaced with revised version Mon, 18 Mar 2024 09:51:00 GMT   (2208kb,D)

Title: Identifying Policy Gradient Subspaces
Authors: Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel
  H\"aufle, Bernhard Sch\"olkopf, Dieter B\"uchler
Categories: cs.LG
Comments: Published as conference paper at ICLR 2024
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2401.06604 ,  2208kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10383
replaced with revised version Sun, 17 Mar 2024 17:04:03 GMT   (1053kb,D)

Title: Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis
Authors: Phevos Paschalidis, Runyu Zhang, and Na Li
Categories: cs.LG cs.MA stat.ML
\\ ( https://arxiv.org/abs/2401.10383 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13652
replaced with revised version Sat, 16 Mar 2024 18:28:30 GMT   (2952kb,D)

Title: Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity
  Detectors
Authors: Francesco Della Santa and Sandra Pieraccini
Categories: cs.LG cs.AI cs.NA math.NA
MSC-class: 68T07, 03D32, 65D40
\\ ( https://arxiv.org/abs/2401.13652 ,  2952kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13923
replaced with revised version Sun, 17 Mar 2024 08:51:45 GMT   (3425kb,D)

Title: Towards 3D Molecule-Text Interpretation in Language Models
Authors: Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji
  Kawaguchi, Tat-Seng Chua, Qi Tian
Categories: cs.LG cs.IR q-bio.BM
\\ ( https://arxiv.org/abs/2401.13923 ,  3425kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14591
replaced with revised version Mon, 18 Mar 2024 14:12:48 GMT   (10283kb,D)

Title: Ricci flow-guided autoencoders in learning time-dependent dynamics
Authors: Andrew Gracyk
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.14591 ,  10283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14846
replaced with revised version Mon, 18 Mar 2024 03:09:57 GMT   (512kb,D)

Title: Understanding Domain Generalization: A Noise Robustness Perspective
Authors: Rui Qiao, Bryan Kian Hsiang Low
Categories: cs.LG cs.CV
Comments: Accepted to the 12th International Conference on Learning
  Representations (ICLR 2024). Code is available at
  https://github.com/qiaoruiyt/NoiseRobustDG
\\ ( https://arxiv.org/abs/2401.14846 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15603
replaced with revised version Mon, 18 Mar 2024 09:00:41 GMT   (1963kb,D)

Title: Improving Expressive Power of Spectral Graph Neural Networks with
  Eigenvalue Correction
Authors: Kangkang Lu, Yanhua Yu, Hao Fei, Xuan Li, Zixuan Yang, Zirui Guo,
  Meiyu Liang, Mengran Yin and Tat-Seng Chua
Categories: cs.LG cs.SI
Comments: Accepted by AAAI-24
\\ ( https://arxiv.org/abs/2401.15603 ,  1963kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16497
replaced with revised version Fri, 15 Mar 2024 21:42:09 GMT   (4384kb)

Title: A Bayesian Gaussian Process-Based Latent Discriminative Generative
  Decoder (LDGD) Model for High-Dimensional Data
Authors: Navid Ziaei, Behzad Nazari, Uri T. Eden, Alik Widge, Ali Yousefi
Categories: cs.LG
Comments: 40 pages, 6 figures
ACM-class: I.5.1; G.3
\\ ( https://arxiv.org/abs/2401.16497 ,  4384kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16694
replaced with revised version Sat, 16 Mar 2024 03:55:50 GMT   (2823kb,D)

Title: EdgeOL: Efficient in-situ Online Learning on Edge Devices
Authors: Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Chao Wu, Alex K. Jones,
  Jingtong Hu, Yanzhi Wang, Xulong Tang
Categories: cs.LG cs.CV cs.DC
\\ ( https://arxiv.org/abs/2401.16694 ,  2823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03126
replaced with revised version Mon, 18 Mar 2024 13:32:00 GMT   (30kb,D)

Title: How Free is Parameter-Free Stochastic Optimization?
Authors: Amit Attia, Tomer Koren
Categories: cs.LG math.OC stat.ML
Comments: 28 pages
\\ ( https://arxiv.org/abs/2402.03126 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05713
replaced with revised version Mon, 18 Mar 2024 13:19:33 GMT   (15276kb,D)

Title: Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on
  Vulnerable Patient Populations
Authors: Pranav Kulkarni, Andrew Chan, Nithya Navarathna, Skylar Chan, Paul H.
  Yi, Vishwa S. Parekh
Categories: cs.LG cs.AI cs.CV
Comments: 29 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.05713 ,  15276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07356
replaced with revised version Sat, 16 Mar 2024 04:02:00 GMT   (38kb)

Title: A Novel Gaussian Min-Max Theorem and its Applications
Authors: Danil Akhtiamov, David Bosch, Reza Ghane, K Nithin Varma, Babak
  Hassibi
Categories: cs.LG stat.ML
Comments: A discussion section was added
\\ ( https://arxiv.org/abs/2402.07356 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09288
replaced with revised version Mon, 18 Mar 2024 09:47:08 GMT   (7915kb,D)

Title: EcoVal: An Efficient Data Valuation Framework for Machine Learning
Authors: Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Hong Ming Tan, Bowei
  Chen, Mohan Kankanhalli
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.09288 ,  7915kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09900
replaced with revised version Sun, 17 Mar 2024 15:16:28 GMT   (30466kb,D)

Title: Revisiting Recurrent Reinforcement Learning with Memory Monoids
Authors: Steven Morad, Chris Lu, Ryan Kortvelesy, Stephan Liwicki, Jakob
  Foerster, Amanda Prorok
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.09900 ,  30466kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10228
replaced with revised version Mon, 18 Mar 2024 04:22:17 GMT   (3644kb,D)

Title: HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement
  Learning Framework for Complex Environments
Authors: Yingru Li, Jiawei Xu, Lei Han, Zhi-Quan Luo
Categories: cs.LG cs.AI stat.ML
Comments: Bridging the theory and practice! Invited talk in Informs
  Optimization Conference 2024 and International Symposium on Mathematical
  Programming 2024!
\\ ( https://arxiv.org/abs/2402.10228 ,  3644kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10359
replaced with revised version Sat, 16 Mar 2024 19:56:04 GMT   (190kb,D)

Title: Can we Soft Prompt LLMs for Graph Learning Tasks?
Authors: Zheyuan Liu, Xiaoxin He, Yijun Tian, Nitesh V. Chawla
Categories: cs.LG cs.CL
Comments: Accepted by The Web Conference (WWW) 2024 Short Paper Track
DOI: 10.1145/3589335.3651476
\\ ( https://arxiv.org/abs/2402.10359 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11887
replaced with revised version Sun, 17 Mar 2024 12:08:33 GMT   (3821kb,D)

Title: Generative Semi-supervised Graph Anomaly Detection
Authors: Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, Guansong Pang
Categories: cs.LG
Comments: 13 pages, 10 figures
\\ ( https://arxiv.org/abs/2402.11887 ,  3821kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14169
replaced with revised version Mon, 18 Mar 2024 12:17:50 GMT   (4111kb,D)

Title: A Temporal Bias Correction using a Machine Learning Attention model
Authors: Omer Nivron, Damon J. Wischik, Mathieu Vrac, Emily Shuckburgh
Categories: cs.LG physics.ao-ph
Comments: 19 pages, 15 figures
\\ ( https://arxiv.org/abs/2402.14169 ,  4111kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15097
replaced with revised version Sun, 17 Mar 2024 01:45:49 GMT   (4807kb,D)

Title: Learning solution operators of PDEs defined on varying domains via
  MIONet
Authors: Shanshan Xiao, Pengzhan Jin, Yifa Tang
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2402.15097 ,  4807kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15776
replaced with revised version Mon, 18 Mar 2024 11:51:18 GMT   (1283kb,D)

Title: Truly No-Regret Learning in Constrained MDPs
Authors: Adrian M\"uller, Pragnya Alatur, Volkan Cevher, Giorgia Ramponi, Niao
  He
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.15776 ,  1283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16681
replaced with revised version Mon, 18 Mar 2024 07:10:46 GMT   (2724kb,D)

Title: Enhancing Continuous Domain Adaptation with Multi-Path Transfer
  Curriculum
Authors: Hanbing Liu, Jingge Wang, Xuan Zhang, Ye Guo, Yang Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.16681 ,  2724kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01232
replaced with revised version Sat, 16 Mar 2024 20:21:27 GMT   (469kb,D)

Title: Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
Authors: Chenhui Deng, Zichao Yue, Zhiru Zhang
Categories: cs.LG cs.AI
Comments: Published as a conference paper at International Conference on
  Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2403.01232 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02221
replaced with revised version Mon, 18 Mar 2024 16:01:26 GMT   (805kb,D)

Title: TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language
  Models
Authors: Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong
  Cui
Categories: cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2403.02221 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02292
replaced with revised version Fri, 15 Mar 2024 18:59:55 GMT   (4099kb,D)

Title: A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends
Authors: Omer Akgul, Sai Teja Peddinti, Nina Taft, Michelle L. Mazurek, Hamza
  Harkous, Animesh Srivastava, Benoit Seguin
Categories: cs.LG cs.HC
Comments: This is the extended version of the paper accepted to USENIX Security
  2024
\\ ( https://arxiv.org/abs/2403.02292 ,  4099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03835
replaced with revised version Mon, 18 Mar 2024 05:09:15 GMT   (3063kb,D)

Title: Cobweb: An Incremental and Hierarchical Model of Human-Like Category
  Learning
Authors: Xin Lian, Sashank Varma, Christopher J. MacLellan
Categories: cs.LG cs.AI cs.IR
\\ ( https://arxiv.org/abs/2403.03835 ,  3063kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05751
replaced with revised version Sat, 16 Mar 2024 01:16:19 GMT   (1365kb,D)

Title: MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided
  Learning Process
Authors: Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, Jiang Bian
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2403.05751 ,  1365kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05822
replaced with revised version Mon, 18 Mar 2024 05:11:22 GMT   (736kb,D)

Title: TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic
  Analysis and Generation
Authors: Jian Qu, Xiaobo Ma, Jianfeng Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.05822 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06880
replaced with revised version Mon, 18 Mar 2024 09:43:20 GMT   (13830kb,D)

Title: Unveiling the Significance of Toddler-Inspired Reward Transition in
  Goal-Oriented Reinforcement Learning
Authors: Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim,
  Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang
Categories: cs.LG cs.AI
Comments: Accepted as a full paper at AAAI 2024 (Oral presentation): 7 pages
  (main paper), 2 pages (references), 17 pages (appendix) each
\\ ( https://arxiv.org/abs/2403.06880 ,  13830kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07030
replaced with revised version Mon, 18 Mar 2024 02:45:04 GMT   (1092kb,D)

Title: AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge
  Distillation
Authors: Zihao Tang, Zheqi Lv, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu,
  Kun Kuang
Categories: cs.LG cs.CV
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2403.07030 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08199
replaced with revised version Sat, 16 Mar 2024 01:02:35 GMT   (48599kb,D)

Title: Deep Submodular Peripteral Networks
Authors: Gantavya Bhatt, Arnav Das, Jeff Bilmes
Categories: cs.LG cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2403.08199 ,  48599kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08333
replaced with revised version Sat, 16 Mar 2024 22:06:04 GMT   (1641kb,D)

Title: Fast Inference of Removal-Based Node Influence
Authors: Weikai Li, Zhiping Xiao, Xiao Luo, Yizhou Sun
Categories: cs.LG cs.AI
Comments: To be published in the Web Conference 2024
DOI: 10.1145/3589334.3645389
\\ ( https://arxiv.org/abs/2403.08333 ,  1641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09701
replaced with revised version Mon, 18 Mar 2024 02:18:16 GMT   (2009kb,D)

Title: A Natural Extension To Online Algorithms For Hybrid RL With Limited
  Coverage
Authors: Kevin Tan, Ziping Xu
Categories: cs.LG stat.ML
Comments: Submitted to the reinforcement learning conference
\\ ( https://arxiv.org/abs/2403.09701 ,  2009kb)
------------------------------------------------------------------------------
\\
arXiv:2209.04589
replaced with revised version Mon, 18 Mar 2024 14:36:35 GMT   (4028kb,D)

Title: Causal Intervention for Fairness in Multi-behavior Recommendation
Authors: Xi Wang, Wenjie Wang, Wenge Rong, Fuli Feng, Chuantao Yin and Zhang
  Xiong
Categories: cs.IR cs.AI
Comments: This paper is accepted by IEEE Transactions on Computational Social
  Systems
\\ ( https://arxiv.org/abs/2209.04589 ,  4028kb)
------------------------------------------------------------------------------
\\
arXiv:2209.14875
replaced with revised version Sun, 17 Mar 2024 20:10:16 GMT   (19209kb,D)

Title: Accelerating Laboratory Automation Through Robot Skill Learning For
  Sample Scraping
Authors: Gabriella Pizzuto, Hetong Wang, Hatem Fakhruldeen, Bei Peng, Kevin S.
  Luck and Andrew I. Cooper
Categories: cs.RO cs.AI
Comments: 8 pages, 7 figures
\\ ( https://arxiv.org/abs/2209.14875 ,  19209kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01622 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 12:52:18 GMT   (13271kb,D)

Title: Private, fair and accurate: Training large-scale, privacy-preserving AI
  models in medical imaging
Authors: Soroosh Tayebi Arasteh, Alexander Ziller, Christiane Kuhl, Marcus
  Makowski, Sven Nebelung, Rickmer Braren, Daniel Rueckert, Daniel Truhn,
  Georgios Kaissis
Categories: eess.IV cs.AI cs.CR cs.CV cs.LG
Comments: Published in Communications Medicine. Nature Portfolio
Journal-ref: Commun Med 4(1), 46 (2024)
DOI: 10.1038/s43856-024-00462-6
\\ ( https://arxiv.org/abs/2302.01622 ,  13271kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00436
replaced with revised version Sun, 17 Mar 2024 00:39:40 GMT   (3318kb,D)

Title: Instance-Level Trojan Attacks on Visual Question Answering via
  Adversarial Learning in Neuron Activation Space
Authors: Yuwei Sun, Hideya Ochiai, Jun Sakuma
Categories: cs.CV cs.AI
Comments: Accepted for IJCNN 2024
\\ ( https://arxiv.org/abs/2304.00436 ,  3318kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09444
replaced with revised version Sat, 16 Mar 2024 02:36:37 GMT   (3480kb)

Title: Rank-Based Learning and Local Model Based Evolutionary Algorithm for
  High-Dimensional Expensive Multi-Objective Problems
Authors: Guodong Chen, Jiu Jimmy Jiao, Xiaoming Xue and Zhongzheng Wang
Categories: cs.NE cs.AI
\\ ( https://arxiv.org/abs/2304.09444 ,  3480kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12576
replaced with revised version Fri, 15 Mar 2024 21:30:14 GMT   (1171kb,D)

Title: Harnessing Deep Learning and HPC Kernels via High-Level Loop and Tensor
  Abstractions on CPU Architectures
Authors: Evangelos Georganas, Dhiraj Kalamkar, Kirill Voronin, Abhisek Kundu,
  Antonio Noack, Hans Pabst, Alexander Breuer, Alexander Heinecke
Categories: cs.DC cs.AI
\\ ( https://arxiv.org/abs/2304.12576 ,  1171kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11490
replaced with revised version Mon, 18 Mar 2024 03:41:09 GMT   (38650kb,D)

Title: LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and
  Generation
Authors: Suhyeon Lee, Won Jun Kim, Jinho Chang, Jong Chul Ye
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 21 pages, 8 figures; ICLR 2024 (poster)
\\ ( https://arxiv.org/abs/2305.11490 ,  38650kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11997 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 05:41:32 GMT   (1606kb,D)

Title: Robust Counterfactual Explanations for Neural Networks With
  Probabilistic Guarantees
Authors: Faisal Hamman, Erfaun Noorani, Saumitra Mishra, Daniele Magazzeni,
  Sanghamitra Dutta
Categories: stat.ML cs.AI cs.CY cs.IT cs.LG math.IT
Comments: International Conference on Machine Learning (ICML), 2023
\\ ( https://arxiv.org/abs/2305.11997 ,  1606kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05716
replaced with revised version Mon, 18 Mar 2024 11:57:09 GMT   (1439kb,D)

Title: Transferring Foundation Models for Generalizable Robotic Manipulation
Authors: Jiange Yang, Wenhui Tan, Chuhao Jin, Keling Yao, Bei Liu, Jianlong Fu,
  Ruihua Song, Gangshan Wu, Limin Wang
Categories: cs.RO cs.AI
Comments: 9 pages, 5 figures
\\ ( https://arxiv.org/abs/2306.05716 ,  1439kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10900
replaced with revised version Mon, 18 Mar 2024 04:14:50 GMT   (39536kb,D)

Title: MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators
Authors: Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei
  Bai, Qi Chu, Nenghai Yu, Wanli Ouyang
Categories: cs.CV cs.AI
Comments: 18 pages, 8 figures, accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2306.10900 ,  39536kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16922
replaced with revised version Sun, 17 Mar 2024 15:36:53 GMT   (9822kb,D)

Title: The Expressive Leaky Memory Neuron: an Efficient and Expressive
  Phenomenological Neuron Model Can Solve Long-Horizon Tasks
Authors: Aaron Spieler, Nasim Rahaman, Georg Martius, Bernhard Sch\"olkopf,
  Anna Levina
Categories: cs.NE cs.AI cs.LG q-bio.NC
Comments: 25 pages, 14 figures, 13 tables, additional experiments and
  clarifications, accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2306.16922 ,  9822kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01187
replaced with revised version Mon, 18 Mar 2024 15:21:47 GMT   (3437kb,D)

Title: SAMAug: Point Prompt Augmentation for Segment Anything Model
Authors: Haixing Dai, Chong Ma, Zhiling Yan, Zhengliang Liu, Enze Shi, Yiwei
  Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Fang Zeng, Dajiang Zhu, Wei
  Liu, Quanzheng Li, Lichao Sun, Shu Zhang Tianming Liu, and Xiang Li
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.01187 ,  3437kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09985
replaced with revised version Sun, 17 Mar 2024 11:34:29 GMT   (5411kb,D)

Title: Our Model Achieves Excellent Performance on MovieLens: What Does it
  Mean?
Authors: Yu-chen Fan, Yitong Ji, Jie Zhang, Aixin Sun
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2307.09985 ,  5411kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01313
replaced with revised version Mon, 18 Mar 2024 16:02:10 GMT   (10630kb,D)

Title: PerceptionCLIP: Visual Classification by Inferring and Conditioning on
  Contexts
Authors: Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya
  Kumar Mummadi, Furong Huang
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2308.01313 ,  10630kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00464
replaced with revised version Mon, 18 Mar 2024 14:24:34 GMT   (336kb,D)

Title: A Theoretical and Practical Framework for Evaluating Uncertainty
  Calibration in Object Detection
Authors: Pedro Conde, Rui L. Lopes, Cristiano Premebida
Categories: cs.CV cs.AI
Comments: Pre-print
\\ ( https://arxiv.org/abs/2309.00464 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15723
replaced with revised version Mon, 18 Mar 2024 13:00:17 GMT   (9916kb,D)

Title: Where Are We So Far? Understanding Data Storytelling Tools from the
  Perspective of Human-AI Collaboration
Authors: Haotian Li, Yun Wang, Huamin Qu
Categories: cs.HC cs.AI
Comments: Accepted by CHI 2024
\\ ( https://arxiv.org/abs/2309.15723 ,  9916kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16701
replaced with revised version Mon, 18 Mar 2024 08:55:36 GMT   (11259kb,D)

Title: Is it Really Negative? Evaluating Natural Language Video Localization
  Performance on Multiple Reliable Videos Pool
Authors: Nakyeong Yang, Minsung Kim, Seunghyun Yoon, Joongbo Shin, Kyomin Jung
Categories: cs.CV cs.AI cs.CL
Comments: 15 pages, 10 figures
\\ ( https://arxiv.org/abs/2309.16701 ,  11259kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01015
replaced with revised version Sun, 17 Mar 2024 04:15:13 GMT   (1324kb,D)

Title: EX-Graph: A Pioneering Dataset Bridging Ethereum and X
Authors: Qian Wang, Zhen Zhang, Zemin Liu, Shengliang Lu, Bingqiao Luo,
  Bingsheng He
Categories: cs.SI cs.AI
\\ ( https://arxiv.org/abs/2310.01015 ,  1324kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02003
replaced with revised version Sat, 16 Mar 2024 01:42:40 GMT   (7823kb,D)

Title: L2MAC: Large Language Model Automatic Computer for Extensive Code
  Generation
Authors: Samuel Holt, Max Ruiz Luyten, Mihaela van der Schaar
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: Published in The Twelfth International Conference on Learning
  Representations (ICLR), 2024. Copyright 2023 by the author(s)
ACM-class: I.2.7; I.2.6; I.2.5; D.2.2; D.2.3; D.3.4
\\ ( https://arxiv.org/abs/2310.02003 ,  7823kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02239
replaced with revised version Fri, 15 Mar 2024 21:54:08 GMT   (44876kb,D)

Title: MiniGPT-5: Interleaved Vision-and-Language Generation via Generative
  Vokens
Authors: Kaizhi Zheng, Xuehai He, Xin Eric Wang
Categories: cs.CV cs.AI
Comments: 23 pages, 10 figures
\\ ( https://arxiv.org/abs/2310.02239 ,  44876kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04566
replaced with revised version Fri, 15 Mar 2024 18:37:34 GMT   (5139kb,D)

Title: Knolling Bot: Learning Robotic Object Arrangement from Tidy
  Demonstrations
Authors: Yuhang Hu, Zhizhuo Zhang, Xinyue Zhu, Ruibo Liu, Philippe Wyder, Hod
  Lipson
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2310.04566 ,  5139kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05556
replaced with revised version Sun, 17 Mar 2024 05:53:44 GMT   (744kb,D)

Title: WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth
  Estimation under Adverse Weather Conditions
Authors: Jiyuan Wang, Chunyu Lin, Lang Nie, Shujun Huang, Yao Zhao, Xing Pan
  and Rui Ai
Categories: cs.CV cs.AI
Comments: 6 pages, accept by ICRA 2024
Journal-ref: ICRA 2024
\\ ( https://arxiv.org/abs/2310.05556 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09692
replaced with revised version Sat, 16 Mar 2024 05:45:59 GMT   (4789kb,D)

Title: Spike-based Neuromorphic Computing for Next-Generation Computer Vision
Authors: Md Sakib Hasan, Catherine D. Schuman, Zhongyang Zhang, Tauhidur
  Rahman, and Garrett S. Rose
Categories: cs.NE cs.AI cs.ET cs.LG eess.IV
Comments: Pending to be published as a book chapter in the book 'Computer
  Vision: Challenges, Trends, and Opportunities' from CRC Press
\\ ( https://arxiv.org/abs/2310.09692 ,  4789kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14402
replaced with revised version Mon, 18 Mar 2024 03:35:28 GMT   (21252kb,D)

Title: Value of Assistance for Grasping
Authors: Mohammad Masarwy, Yuval Goshen, David Dovrat and Sarah Keren
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2310.14402 ,  21252kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15778
replaced with revised version Mon, 18 Mar 2024 13:27:01 GMT   (3087kb,D)

Title: Privacy Protection in MRI Scans Using 3D Masked Autoencoders
Authors: Lennart Alexander Van der Goten and Kevin Smith
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.15778 ,  3087kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10319
replaced with revised version Mon, 18 Mar 2024 04:01:53 GMT   (32878kb,D)

Title: Shifting to Machine Supervision: Annotation-Efficient Semi and
  Self-Supervised Learning for Automatic Medical Image Segmentation and
  Classification
Authors: Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen,
  Mei Chen, Jinqian Pan, Craig Smuda and Jacopo Cirrone
Categories: cs.CV cs.AI
Comments: Seventeen pages (incl. references), five figures, and one table.
  (Under Review)
\\ ( https://arxiv.org/abs/2311.10319 ,  32878kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15599
replaced with revised version Mon, 18 Mar 2024 08:37:24 GMT   (269kb,D)

Title: UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio,
  Video, Point Cloud, Time-Series and Image Recognition
Authors: Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu
  Yue, Ying Shan
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024. Code, all the models, reproducible training scripts at
  https://github.com/AILab-CVC/UniRepLKNet
\\ ( https://arxiv.org/abs/2311.15599 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17248 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 15:35:52 GMT   (2733kb,D)

Title: Deep Regularized Compound Gaussian Network for Solving Linear Inverse
  Problems
Authors: Carter Lyons, Raghu G. Raj, and Margaret Cheney
Categories: eess.SP cs.AI cs.NA math.NA
Comments: Supplementary material appears after the main article in the PDF.
  Main article has 16 pages, 7 figures, 3 tables, and 1 algorithm.
  Supplementary material has 4 pages and 5 figures
Journal-ref: in IEEE Transactions on Computational Imaging, vol. 10, pp.
  399-414, 2024
DOI: 10.1109/TCI.2024.336939
\\ ( https://arxiv.org/abs/2311.17248 ,  2733kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17950
replaced with revised version Sun, 17 Mar 2024 03:56:13 GMT   (12849kb,D)

Title: Generalized Large-Scale Data Condensation via Various Backbone and
  Statistical Matching
Authors: Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang and Zhiqiang Shen
Categories: cs.CV cs.AI
Comments: Accepted by CVPR2024
\\ ( https://arxiv.org/abs/2311.17950 ,  12849kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18531
replaced with revised version Fri, 15 Mar 2024 22:14:40 GMT   (10658kb,D)

Title: Dataset Distillation via the Wasserstein Metric
Authors: Haoyang Liu, Yijiang Li, Tiancheng Xing, Vibhu Dalal, Luwei Li,
  Jingrui He, Haohan Wang
Categories: cs.CV cs.AI cs.LG
Comments: 21 pages, 8 figures
\\ ( https://arxiv.org/abs/2311.18531 ,  10658kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00029
replaced with revised version Fri, 15 Mar 2024 20:13:06 GMT   (196kb,D)

Title: Bergeron: Combating Adversarial Attacks through a Conscience-Based
  Alignment Framework
Authors: Matthew Pisano, Peter Ly, Abraham Sanders, Bingsheng Yao, Dakuo Wang,
  Tomek Strzalkowski, Mei Si
Categories: cs.CR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.00029 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03003
replaced with revised version Sat, 16 Mar 2024 06:17:52 GMT   (3747kb,D)

Title: Explore, Select, Derive, and Recall: Augmenting LLM with Human-like
  Memory for Mobile Task Automation
Authors: Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi,
  Steven Y. Ko, Sangeun Oh, Insik Shin
Categories: cs.HC cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.03003 ,  3747kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03297
replaced with revised version Sat, 16 Mar 2024 05:01:26 GMT   (16892kb,D)

Title: SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact
  Model and Two-way Coupling with Articulated Rigid Bodies and Clothes
Authors: Min Liu, Gang Yang, Siyuan Luo, Lin Shao
Categories: cs.RO cs.AI cs.GR
\\ ( https://arxiv.org/abs/2312.03297 ,  16892kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05626
replaced with revised version Fri, 15 Mar 2024 18:23:12 GMT   (1093kb,D)

Title: Redefining Developer Assistance: Through Large Language Models in
  Software Ecosystem
Authors: Somnath Banerjee, Avik Dutta, Sayan Layek, Amruit Sahoo, Sam Conrad
  Joyce, Rima Hazra
Categories: cs.SE cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2312.05626 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08459
replaced with revised version Sun, 17 Mar 2024 23:45:01 GMT   (12726kb,D)

Title: FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head
  Models
Authors: Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nie{\ss}ner
Categories: cs.CV cs.AI cs.GR cs.SD eess.AS
Comments: Paper Video: https://youtu.be/7Jf0kawrA3Q Project Page:
  https://shivangi-aneja.github.io/projects/facetalk/
Journal-ref: CVPR 2024
\\ ( https://arxiv.org/abs/2312.08459 ,  12726kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16476
replaced with revised version Sun, 17 Mar 2024 09:12:58 GMT   (48269kb,D)

Title: SVGDreamer: Text Guided SVG Generation with Diffusion Model
Authors: Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
Categories: cs.CV cs.AI
Comments: 19 pages, 16 figures, project link:
  https://ximinng.github.io/SVGDreamer-project/
\\ ( https://arxiv.org/abs/2312.16476 ,  48269kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14405
replaced with revised version Mon, 18 Mar 2024 08:45:52 GMT   (401kb,D)

Title: Multimodal Pathway: Improve Transformers with Irrelevant Data from Other
  Modalities
Authors: Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan,
  Xiangyu Yue
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024. Code and models are available at
  https://github.com/AILab-CVC/M2PT
\\ ( https://arxiv.org/abs/2401.14405 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17500
replaced with revised version Mon, 18 Mar 2024 07:10:02 GMT   (28986kb,D)

Title: LeTO: Learning Constrained Visuomotor Policy with Differentiable
  Trajectory Optimization
Authors: Zhengtong Xu, Yu She
Categories: cs.RO cs.AI
Comments: 8 pages, 5 figures
\\ ( https://arxiv.org/abs/2401.17500 ,  28986kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02544
replaced with revised version Mon, 18 Mar 2024 14:16:29 GMT   (25893kb,D)

Title: LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal
  Language Model
Authors: Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao
Categories: cs.CV cs.AI cs.LG
Comments: 36 pages, 10 figures. Github https://github.com/NJU-LHRS/LHRS-Bot
\\ ( https://arxiv.org/abs/2402.02544 ,  25893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11702
replaced with revised version Sat, 16 Mar 2024 22:16:40 GMT   (2225kb,D)

Title: Can ChatGPT Support Developers? An Empirical Evaluation of Large
  Language Models for Code Generation
Authors: Kailun Jin, Chung-Yu Wang, Hung Viet Pham, Hadi Hemmati
Categories: cs.SE cs.AI cs.LG
Comments: 4 pages, 3 figures, 21st International Conference on Mining Software
  Repositories (MSR '24), April 15-16, 2024, Lisbon, Portugal
ACM-class: I.2.2
DOI: 10.1145/3643991.3645074
\\ ( https://arxiv.org/abs/2402.11702 ,  2225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11780
replaced with revised version Mon, 18 Mar 2024 15:25:30 GMT   (1072kb,D)

Title: CiMNet: Towards Joint Optimization for DNN Architecture and
  Configuration for Compute-In-Memory Hardware
Authors: Souvik Kundu, Anthony Sarah, Vinay Joshi, Om J Omer, Sreenivas
  Subramoney
Categories: cs.AR cs.AI
Comments: 6 pages, 4 figures, 5 tables; Accepted as a full paper by the tinyML
  Research Symposium 2024
\\ ( https://arxiv.org/abs/2402.11780 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13602
replaced with revised version Mon, 18 Mar 2024 09:50:00 GMT   (4222kb,D)

Title: Hybrid Reasoning Based on Large Language Models for Autonomous Car
  Driving
Authors: Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab,
  Achim Rettberg
Categories: cs.CV cs.AI
Comments: 12 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.13602 ,  4222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14505
replaced with revised version Mon, 18 Mar 2024 12:28:31 GMT   (8453kb,D)

Title: Towards Seamless Adaptation of Pre-trained Models for Visual Place
  Recognition
Authors: Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun
  Yuan
Categories: cs.CV cs.AI
Comments: ICLR2024
\\ ( https://arxiv.org/abs/2402.14505 ,  8453kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14899
replaced with revised version Mon, 18 Mar 2024 10:55:36 GMT   (1178kb,D)

Title: Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning
  Meets Adversarial Images
Authors: Zefeng Wang, Zhen Han, Shuo Chen, Fan Xue, Zifeng Ding, Xun Xiao,
  Volker Tresp, Philip Torr, Jindong Gu
Categories: cs.CV cs.AI cs.CR cs.LG
\\ ( https://arxiv.org/abs/2402.14899 ,  1178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15321
replaced with revised version Sun, 17 Mar 2024 08:41:49 GMT   (1419kb,D)

Title: OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene
  Understanding
Authors: Francis Engelmann, Ayca Takmaz, Jonas Schult, Elisabetta Fedele,
  Johanna Wald, Songyou Peng, Xi Wang, Or Litany, Siyu Tang, Federico Tombari,
  Marc Pollefeys, Leonidas Guibas, Hongbo Tian, Chunjie Wang, Xiaosheng Yan,
  Bingwen Wang, Xuanyang Zhang, Xiao Liu, Phuc Nguyen, Khoi Nguyen, Anh Tran,
  Cuong Pham, Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu,
  Joan Lasenby
Categories: cs.CV cs.AI cs.LG
Comments: Our OpenSUN3D workshop website for ICCV 2023:
  https://opensun3d.github.io/index_iccv23.html
\\ ( https://arxiv.org/abs/2402.15321 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16086
replaced with revised version Mon, 18 Mar 2024 09:33:47 GMT   (7402kb,D)

Title: Deep Homography Estimation for Visual Place Recognition
Authors: Feng Lu, Shuting Dong, Lijun Zhang, Bingxi Liu, Xiangyuan Lan, Dongmei
  Jiang, Chun Yuan
Categories: cs.CV cs.AI
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2402.16086 ,  7402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17736
replaced with revised version Sat, 16 Mar 2024 21:56:58 GMT   (978kb,D)

Title: Learning-Based Algorithms for Graph Searching Problems
Authors: Adela Frances DePavia, Erasmo Tani, Ali Vakilian
Categories: cs.DS cs.AI cs.LG
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2402.17736 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18152 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 13:23:58 GMT   (46607kb,D)

Title: Boosting Neural Representations for Videos with a Conditional Decoder
Authors: Xinjie Zhang, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang,
  Hongwei Qin, Jun Zhang
Categories: eess.IV cs.AI cs.CV
Comments: Accept by CVPR 2024
\\ ( https://arxiv.org/abs/2402.18152 ,  46607kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00781
replaced with revised version Sat, 16 Mar 2024 17:31:11 GMT   (2333kb,D)

Title: ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender
  Chatbots through an LLM-Augmented Framework
Authors: Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman
  Azimi, Ramesh Jain, Amir M. Rahmani
Categories: cs.IR cs.AI cs.LG cs.MM
Comments: Accepted by The IEEE/ACM international conference on Connected
  Health: Applications, Systems and Engineering Technologies (CHASE) 2024
\\ ( https://arxiv.org/abs/2403.00781 ,  2333kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01281
replaced with revised version Sat, 9 Mar 2024 15:37:59 GMT   (130147kb,D)

Title: Fast Low-parameter Video Activity Localization in Collaborative Learning
  Environments
Authors: Venkatesh Jatla, Sravani Teeparthi, Ugesh Egala, Sylvia Celedon
  Pattichis, Marios S. Patticis
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.01281 ,  130147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01827
replaced with revised version Sat, 16 Mar 2024 15:43:04 GMT   (10917kb)

Title: Analysis and Fully Memristor-based Reservoir Computing for Temporal Data
  Classification
Authors: Ankur Singh, Sanghyeon Choi, Gunuk Wang, Maryaradhiya Daimari, and
  Byung-Geun Lee
Categories: cs.NE cs.AI
Comments: 22 pages, 20 figures, Journal, Typo corrected and updated reference
\\ ( https://arxiv.org/abs/2403.01827 ,  10917kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04164
replaced with revised version Mon, 18 Mar 2024 08:40:48 GMT   (4370kb)

Title: ProMISe: Promptable Medical Image Segmentation using SAM
Authors: Jinfeng Wang, Sifan Song, Xinkun Wang, Yiyi Wang, Yiyi Miao, Jionglong
  Su, S. Kevin Zhou
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.04164 ,  4370kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04306
replaced with revised version Mon, 18 Mar 2024 07:21:01 GMT   (1677kb,D)

Title: Effectiveness Assessment of Recent Large Vision-Language Models
Authors: Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong,
  Deng-Ping Fan, Fahad Shahbaz Khan
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.04306 ,  1677kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05050
replaced with revised version Mon, 18 Mar 2024 17:39:34 GMT   (10747kb,D)

Title: DyRoNet: Dynamic Routing and Low-Rank Adapters for Autonomous Driving
  Streaming Perception
Authors: Xiang Huang, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Wangmeng Xiang,
  Baigui Sun, Xiao Wu
Categories: cs.CV cs.AI cs.MM
Comments: Project: https://tastevision.github.io/DyRoNet/
\\ ( https://arxiv.org/abs/2403.05050 ,  10747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05828 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 08:54:10 GMT   (2017kb,D)

Title: Multi-GPU-Enabled Hybrid Quantum-Classical Workflow in Quantum-HPC
  Middleware: Applications in Quantum Simulations
Authors: Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang and Chen-Yu
  Liu
Categories: quant-ph cs.AI cs.AR cs.DC
Comments: 8 pages, 8 figures
\\ ( https://arxiv.org/abs/2403.05828 ,  2017kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06095
replaced with revised version Sat, 16 Mar 2024 09:07:12 GMT   (12835kb,D)

Title: RepoHyper: Better Context Retrieval Is All You Need for Repository-Level
  Code Completion
Authors: Huy N. Phan, Hoang N. Phan, Tien N. Nguyen, Nghi D. Q. Bui
Categories: cs.SE cs.AI
Comments: Under Review
\\ ( https://arxiv.org/abs/2403.06095 ,  12835kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08937
replaced with revised version Mon, 18 Mar 2024 14:34:13 GMT   (1323kb,D)

Title: Bugs in Large Language Models Generated Code: An Empirical Study
Authors: Florian Tambon, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh,
  Michel C. Desmarais, Giuliano Antoniol
Categories: cs.SE cs.AI
Comments: 47 pages, 7 figures
\\ ( https://arxiv.org/abs/2403.08937 ,  1323kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09209
replaced with revised version Sun, 17 Mar 2024 14:05:51 GMT   (4110kb,D)

Title: LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection
Authors: Xiangrui Cai, Yang Wang, Sihan Xu, Hao Li, Ying Zhang, Zheli Liu,
  Xiaojie Yuan
Categories: cs.CR cs.AI cs.LG
Comments: 13 pages
\\ ( https://arxiv.org/abs/2403.09209 ,  4110kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09603
replaced with revised version Sat, 16 Mar 2024 08:51:56 GMT   (1511kb,D)

Title: Optimistic Verifiable Training by Controlling Hardware Nondeterminism
Authors: Megha Srivastava, Simran Arora, Dan Boneh
Categories: cs.CR cs.AI cs.LG
Comments: 11 pages, 5 figures, preprint
\\ ( https://arxiv.org/abs/2403.09603 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2208.03886 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 02:15:20 GMT   (45kb)

Title: What can we know about that which we cannot even imagine?
Authors: David H. Wolpert
Categories: physics.hist-ph cs.CL
Comments: 39 pages, 10 pages of which are references
\\ ( https://arxiv.org/abs/2208.03886 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13451
replaced with revised version Mon, 18 Mar 2024 01:09:44 GMT   (2529kb,D)

Title: A low latency attention module for streaming self-supervised speech
  representation learning
Authors: Jianbo Ma, Siqi Pan, Deepak Chandran, Andrea Fanelli, Richard
  Cartwright
Categories: cs.SD cs.CL cs.LG eess.AS
Comments: 19 pages, 4 figures
\\ ( https://arxiv.org/abs/2302.13451 ,  2529kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07952
replaced with revised version Sun, 17 Mar 2024 06:49:19 GMT   (5051kb,D)

Title: MOFI: Learning Image Representations from Noisy Entity Annotated Images
Authors: Wentao Wu, Aleksei Timofeev, Chen Chen, Bowen Zhang, Kun Duan,
  Shuangning Liu, Yantao Zheng, Jonathon Shlens, Xianzhi Du, Zhe Gan, Yinfei
  Yang
Categories: cs.CV cs.CL cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2306.07952 ,  5051kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05961
replaced with revised version Fri, 15 Mar 2024 18:07:46 GMT   (306kb,D)

Title: Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering
  Trends across Diverse Platforms
Authors: Rima Hazra, Agnik Saha, Somnath Banerjee and Animesh Mukherjee
Categories: cs.SI cs.CL cs.IR cs.LG
Comments: Accepted as POSTER
\\ ( https://arxiv.org/abs/2309.05961 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11499
replaced with revised version Fri, 15 Mar 2024 19:19:28 GMT   (14990kb,D)

Title: DreamLLM: Synergistic Multimodal Comprehension and Creation
Authors: Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong
  Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong,
  Xiangyu Zhang, Kaisheng Ma, Li Yi
Categories: cs.CV cs.CL cs.LG
Comments: ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2309.11499 ,  14990kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03018 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 07:57:58 GMT   (2401kb,D)

Title: Zero Resource Code-switched Speech Benchmark Using Speech Utterance
  Pairs For Multiple Spoken Languages
Authors: Kuan-Po Huang, Chih-Kai Yang, Yu-Kuan Fu, Ewan Dunbar, Hung-yi Lee
Categories: eess.AS cs.CL cs.SD
Comments: Accepted by ICASSP 2024 (v2)
\\ ( https://arxiv.org/abs/2310.03018 ,  2401kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14566
replaced with revised version Mon, 18 Mar 2024 02:42:10 GMT   (23051kb,D)

Title: HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-Language Models
Authors: Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu
  Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha,
  Tianyi Zhou
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2310.14566 ,  23051kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03253
replaced with revised version Sat, 16 Mar 2024 16:39:07 GMT   (3101kb,D)

Title: VLLaVO: Mitigating Visual Gap through LLMs
Authors: Shuhao Chen, Yulong Zhang, Weisen Jiang, Jiangang Lu, and Yu Zhang
Categories: cs.CV cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.03253 ,  3101kb)
------------------------------------------------------------------------------
\\
arXiv:2202.04912 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 10:39:18 GMT   (3368kb,D)

Title: Random Forest Weighted Local Fr\'echet Regression with Random Objects
Authors: Rui Qiu, Zhou Yu, Ruoqing Zhu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2202.04912 ,  3368kb)
------------------------------------------------------------------------------
\\
arXiv:2206.08659
replaced with revised version Mon, 18 Mar 2024 12:24:55 GMT   (0kb,I)

Title: Digital Twin Data Modelling by Randomized Orthogonal Decomposition and
  Deep Learning
Authors: Diana Alina Bistrian and Omer San and Ionel Michael Navon
Categories: math.NA cs.LG cs.NA
Comments: The material will be revised
\\ ( https://arxiv.org/abs/2206.08659 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2208.04564 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 21:58:47 GMT   (550kb,D)

Title: Statistical Properties of the log-cosh Loss Function Used in Machine
  Learning
Authors: Resve A. Saleh and A.K.Md. Ehsanes Saleh
Categories: stat.ML cs.LG
Comments: 10 pages, 17 figures
\\ ( https://arxiv.org/abs/2208.04564 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2209.08307 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 11:22:36 GMT   (1826kb)

Title: A review of predictive uncertainty estimation with machine learning
Authors: Hristos Tyralis, Georgia Papacharalampous
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 89 pages, 5 figures
Journal-ref: Artificial Intelligence Review 57(94) (2024)
DOI: 10.1007/s10462-023-10698-8
\\ ( https://arxiv.org/abs/2209.08307 ,  1826kb)
------------------------------------------------------------------------------
\\
arXiv:2209.14568 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 09:39:00 GMT   (599kb,D)

Title: Local and Regional Counterfactual Rules: Summarized and Robust Recourses
Authors: Salim I. Amoukou, Nicolas J.B Brunel
Categories: stat.ML cs.LG
Comments: ICML (International Conference on Machine Learning) 2023 Workshop on
  Counterfactuals in Minds and Machines
Journal-ref: ICML (International Conference on Machine Learning) 2023 Workshop
  on Counterfactuals in Minds and Machines
\\ ( https://arxiv.org/abs/2209.14568 ,  599kb)
------------------------------------------------------------------------------
\\
arXiv:2211.16059 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 20:02:39 GMT   (53kb)

Title: On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach
Authors: Mehrdad Pournaderi and Yu Xiang
Categories: stat.ME cs.LG cs.SY eess.SP eess.SY
Comments: Published in the IEEE Transactions on Signal and Information
  Processing over Networks
\\ ( https://arxiv.org/abs/2211.16059 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01089
replaced with revised version Sun, 17 Mar 2024 15:53:18 GMT   (3763kb,D)

Title: Curriculum Learning for ab initio Deep Learned Refractive Optics
Authors: Xinge Yang, Qiang Fu, Wolfgang Heidrich
Categories: cs.CV cs.LG eess.IV physics.optics
Comments: Automatically design computational lenses from scratch with
  differentiable ray tracing
\\ ( https://arxiv.org/abs/2302.01089 ,  3763kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02257
replaced with revised version Mon, 18 Mar 2024 11:39:29 GMT   (461kb,D)

Title: Multi-Source Diffusion Models for Simultaneous Music Generation and
  Separation
Authors: Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi,
  Luca Cosmo, Emanuele Rodol\`a
Categories: cs.SD cs.LG eess.AS
Comments: ICLR 2024 oral presentation. Demo page:
  https://gladia-research-group.github.io/multi-source-diffusion-models/
\\ ( https://arxiv.org/abs/2302.02257 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09193 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 15:48:44 GMT   (285kb,D)

Title: Copula-based transferable models for synthetic population generation
Authors: Pascal Jutras-Dub\'e, Mohammad B. Al-Khasawneh, Zhichao Yang, Javier
  Bas, Fabian Bastin and Cinzia Cirillo
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2302.09193 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11323 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 22:00:45 GMT   (3343kb,D)

Title: Tangent Bundle Convolutional Learning: from Manifolds to Cellular
  Sheaves and Back
Authors: Claudio Battiloro, Zhiyang Wang, Hans Riess, Paolo Di Lorenzo,
  Alejandro Ribeiro
Categories: eess.SP cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2210.15058
\\ ( https://arxiv.org/abs/2303.11323 ,  3343kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09097
replaced with revised version Sat, 16 Mar 2024 06:59:28 GMT   (2972kb,D)

Title: Sheaf4Rec: Sheaf Neural Networks for Graph-based Recommender Systems
Authors: Antonio Purificato, Giulia Cassar\`a, Federico Siciliano, Pietro
  Li\`o, Fabrizio Silvestri
Categories: cs.IR cs.LG
Comments: 21 pages, 8 figures
MSC-class: 55
ACM-class: I.2.6; H.3.3
\\ ( https://arxiv.org/abs/2304.09097 ,  2972kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00905 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 12:02:00 GMT   (1410kb,D)

Title: BCQQ: Batch-Constraint Quantum Q-Learning with Cyclic Data Re-uploading
Authors: Maniraman Periyasamy and Marc H\"olle and Marco Wiedmann and Daniel D.
  Scherer and Axel Plinge and Christopher Mutschler
Categories: quant-ph cs.LG
\\ ( https://arxiv.org/abs/2305.00905 ,  1410kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11927
replaced with revised version Fri, 15 Mar 2024 18:23:16 GMT   (3111kb,D)

Title: Evaluating how interactive visualizations can assist in finding samples
  where and how computer vision models make mistakes
Authors: Hayeong Song, Gonzalo Ramos, and Peter Bodik
Categories: cs.HC cs.CV cs.LG
Comments: Hayeong Song, Gonzalo Ramos, and Peter Bodik. "Evaluating how
  interactive visualizations can assist in finding samples where and how
  computer vision models make mistakes" 2024 IEEE Pacific Visualization
  Symposium (PacificVis). Ieee, 2024
\\ ( https://arxiv.org/abs/2305.11927 ,  3111kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12681
replaced with revised version Mon, 18 Mar 2024 06:34:01 GMT   (5446kb)

Title: Phased Data Augmentation for Training a Likelihood-Based Generative
  Model with Limited Data
Authors: Yuta Mimura
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2305.12681 ,  5446kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14392
replaced with revised version Fri, 15 Mar 2024 21:28:59 GMT   (8974kb,D)

Title: FEDORA: Flying Event Dataset fOr Reactive behAvior
Authors: Amogh Joshi, Adarsh Kosta, Wachirawit Ponghiran, Manish Nagaraj,
  Kaushik Roy
Categories: cs.CV cs.ET cs.LG cs.NE cs.RO
\\ ( https://arxiv.org/abs/2305.14392 ,  8974kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15759 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 03:01:19 GMT   (12332kb,D)

Title: Differentially Private Latent Diffusion Models
Authors: Saiyue Lyu, Michael F. Liu, Margarita Vinaroz, Mijung Park
Categories: stat.ML cs.CR cs.LG
\\ ( https://arxiv.org/abs/2305.15759 ,  12332kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02574
replaced with revised version Sat, 16 Mar 2024 18:17:05 GMT   (999kb,D)

Title: Bayesian Learning of Optimal Policies in Markov Decision Processes with
  Countably Infinite State-Space
Authors: Saghar Adler, Vijay Subramanian
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2306.02574 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10060 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 15:57:19 GMT   (1636kb,D)

Title: MUBen: Benchmarking the Uncertainty of Molecular Representation Models
Authors: Yinghao Li, Lingkai Kong, Yuanqi Du, Yue Yu, Yuchen Zhuang, Wenhao Mu,
  Chao Zhang
Categories: physics.chem-ph cs.LG
\\ ( https://arxiv.org/abs/2306.10060 ,  1636kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12360 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 19:16:01 GMT   (1689kb,D)

Title: Protein Discovery with Discrete Walk-Jump Sampling
Authors: Nathan C. Frey, Daniel Berenberg, Karina Zadorozhny, Joseph Kleinhenz,
  Julien Lafrance-Vanasse, Isidro Hotzel, Yan Wu, Stephen Ra, Richard Bonneau,
  Kyunghyun Cho, Andreas Loukas, Vladimir Gligorijevic, Saeed Saremi
Categories: q-bio.BM cs.LG
Comments: ICLR 2024 oral presentation, top 1.2% of submissions; {ICLR 2023
  Physics for Machine Learning, NeurIPS 2023 GenBio, MLCB 2023} Spotlight
\\ ( https://arxiv.org/abs/2306.12360 ,  1689kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13769 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 17:29:42 GMT   (18510kb,D)

Title: Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation
  and Elaboration
Authors: Haitao Lin, Yufei Huang, Odin Zhang, Lirong Wu, Siyuan Li, Zhiyuan
  Chen, Stan Z. Li
Categories: q-bio.BM cs.LG
Comments: 9 pages
\\ ( https://arxiv.org/abs/2306.13769 ,  18510kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10352 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 10:07:40 GMT   (9989kb,D)

Title: Properties of Discrete Sliced Wasserstein Losses
Authors: Eloi Tanguy, R\'emi Flamary and Julie Delon
Categories: stat.ML cs.LG math.OC math.PR
\\ ( https://arxiv.org/abs/2307.10352 ,  9989kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13916 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 03:08:41 GMT   (5191kb,D)

Title: Online learning in bandits with predicted context
Authors: Yongyi Guo, Ziping Xu, Susan Murphy
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2307.13916 ,  5191kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01938 (*cross-listing*)
replaced with revised version Sun, 17 Mar 2024 16:22:39 GMT   (484kb,D)

Title: Online Multi-Task Learning with Recursive Least Squares and Recursive
  Kernel Methods
Authors: Gabriel R. Lencione, Fernando J. Von Zuben
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2308.01938 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08487
replaced with revised version Mon, 18 Mar 2024 11:24:24 GMT   (23530kb,D)

Title: Temporal Interest Network for User Response Prediction
Authors: Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng
  Gao, Guihai Chen
Categories: cs.IR cs.LG
Journal-ref: The Web Conference, 2024
DOI: 10.1145/3589335.3648340
\\ ( https://arxiv.org/abs/2308.08487 ,  23530kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08978
replaced with revised version Sun, 17 Mar 2024 22:17:41 GMT   (9982kb,D)

Title: Quantifying the biomimicry gap in biohybrid robot-fish pairs
Authors: Vaios Papaspyros, Guy Theraulaz, Cl\'ement Sire, Francesco Mondada
Categories: cs.RO cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2308.08978 ,  9982kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04664
replaced with revised version Sun, 17 Mar 2024 07:23:10 GMT   (1146kb,D)

Title: Compact: Approximating Complex Activation Functions for Secure
  Computation
Authors: Mazharul Islam, Sunpreet S. Arora, Rahul Chatterjee, Peter Rindal,
  Maliheh Shirvanian
Categories: cs.CR cs.LG
Comments: Accepted to Proceedings on Privacy Enhancing Technologies (PoPETs)
\\ ( https://arxiv.org/abs/2309.04664 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07183 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 19:00:10 GMT   (6137kb,D)

Title: Respiratory Disease Classification and Biometric Analysis Using
  Biosignals from Digital Stethoscopes
Authors: Constantino \'Alvarez Casado, Manuel Lage Ca\~nellas, Matteo Pedone,
  Xiaoting Wu, Le Nguyen, Miguel Bordallo L\'opez
Categories: eess.SP cs.LG cs.SD
Comments: 5 pages, 2 figures, 3 tables, Conference paper
\\ ( https://arxiv.org/abs/2309.07183 ,  6137kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08731
replaced with revised version Fri, 15 Mar 2024 20:21:26 GMT   (8336kb,D)

Title: Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP
  Weights
Authors: Daniil Lisus, Johann Laconte, Keenan Burnett, Timothy D. Barfoot
Categories: cs.RO cs.LG
Comments: 8 pages (6 content, 2 references). 4 figures
\\ ( https://arxiv.org/abs/2309.08731 ,  8336kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10152 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 07:03:00 GMT   (349kb)

Title: Sparse Index Tracking: Simultaneous Asset Selection and Capital
  Allocation via $\ell_0$-Constrained Portfolio
Authors: Eisuke Yamagata and Shunsuke Ono
Categories: q-fin.PM cs.CE cs.LG
Comments: Submitted to IEEE Open Journal of Signal Processing
\\ ( https://arxiv.org/abs/2309.10152 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00817 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 06:12:09 GMT   (1539kb,D)

Title: Learning to Make Adherence-Aware Advice
Authors: Guanting Chen, Xiaocheng Li, Chunlin Sun, Hanzhao Wang
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.00817 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02557
replaced with revised version Fri, 15 Mar 2024 18:21:48 GMT   (6790kb,D)

Title: Generalization in diffusion models arises from geometry-adaptive
  harmonic representations
Authors: Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, St\'ephane
  Mallat
Categories: cs.CV cs.LG
Comments: Accepted for oral presentation at ICLR, Vienna, May 2024
\\ ( https://arxiv.org/abs/2310.02557 ,  6790kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08601 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 13:20:37 GMT   (20971kb,D)

Title: Unit Commitment Predictor With a Performance Guarantee: A Support Vector
  Machine Classifier
Authors: Farzaneh Pourahmadi, Jalal Kazempour
Categories: math.OC cs.LG stat.AP
\\ ( https://arxiv.org/abs/2310.08601 ,  20971kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09362
replaced with revised version Mon, 18 Mar 2024 07:08:31 GMT   (1982kb,D)

Title: From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment
  Technique
Authors: Sina Elahimanesh, Shayan Salehi, Sara Zahedi Movahed, Lisa Alazraki,
  Ruoyu Hu, Abbas Edalat
Categories: cs.HC cs.LG
\\ ( https://arxiv.org/abs/2310.09362 ,  1982kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12359
replaced with revised version Mon, 18 Mar 2024 01:04:18 GMT   (7596kb,D)

Title: MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable
  Speed Limits
Authors: Yuhang Zhang, Marcos Quinones-Grueiro, Zhiyao Zhang, Yanbing Wang,
  William Barbour, Gautam Biswas and Daniel Work
Categories: cs.MA cs.LG
\\ ( https://arxiv.org/abs/2310.12359 ,  7596kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16597 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 16:28:44 GMT   (8769kb,D)

Title: Beyond IID weights: sparse and low-rank deep Neural Networks are also
  Gaussian Processes
Authors: Thiziri Nait-Saada, Alireza Naderi, Jared Tanner
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.16597 ,  8769kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00136 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 22:07:06 GMT   (11468kb,D)

Title: Neuroformer: Multimodal and Multitask Generative Pretraining for Brain
  Data
Authors: Antonis Antoniades, Yiyi Yu, Joseph Canzano, William Wang, Spencer
  LaVere Smith
Categories: q-bio.NC cs.LG cs.NE
Comments: 9 pages for main paper. 22 pages in total. 13 figures, 1 table
\\ ( https://arxiv.org/abs/2311.00136 ,  11468kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11749 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 16:04:36 GMT   (13851kb,D)

Title: Revealing behavioral impact on mobility prediction networks through
  causal interventions
Authors: Ye Hong, Yanan Xin, Simon Dirmeier, Fernando Perez-Cruz, Martin Raubal
Categories: physics.soc-ph cs.LG cs.SI
Comments: 31 pages, 6 figures
\\ ( https://arxiv.org/abs/2311.11749 ,  13851kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12528
replaced with revised version Mon, 18 Mar 2024 15:28:00 GMT   (6279kb,D)

Title: Inverse Problems with Learned Forward Operators
Authors: Simon Arridge, Andreas Hauptmann, Yury Korolev
Categories: math.NA cs.LG cs.NA
MSC-class: 65J22, 47A52, 35R30, 74J25
\\ ( https://arxiv.org/abs/2311.12528 ,  6279kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14272
replaced with revised version Mon, 18 Mar 2024 08:15:48 GMT   (11734kb,D)

Title: CRISP: Hybrid Structured Sparsity for Class-aware Model Pruning
Authors: Shivam Aggarwal, Kuluhan Binici, Tulika Mitra
Categories: cs.CV cs.AR cs.LG
Comments: 6 pages, accepted in Design, Automation & Test in Europe Conference &
  Exhibition (DATE) 2024
\\ ( https://arxiv.org/abs/2311.14272 ,  11734kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16909 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 11:53:00 GMT   (2594kb)

Title: Multinomial belief networks
Authors: H. C. Donker, D. Neijzen, J. de Jong, G. A. Lunter
Categories: stat.ML cs.LG stat.AP
Comments: 15 pages, 4 figs; supplement: 16 pages
\\ ( https://arxiv.org/abs/2311.16909 ,  2594kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04135
replaced with revised version Fri, 15 Mar 2024 20:41:03 GMT   (645kb,D)

Title: A Novel Federated Learning-Based IDS for Enhancing UAVs Privacy and
  Security
Authors: Ozlem Ceviz (1), Pinar Sadioglu (1), Sevil Sen (1) and Vassilios G.
  Vassilakis (2) ((1) WISE Lab., Deparment of Computer Engineering, Hacettepe
  University, Ankara, Turkey (2) Department of Computer Science, University of
  York, York, United Kingdom)
Categories: cs.CR cs.LG
Comments: 15
\\ ( https://arxiv.org/abs/2312.04135 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05320 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 18:16:23 GMT   (19537kb,D)

Title: Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with
  Denoising Diffusion Probabilistic Models
Authors: Qiang Liu, Nils Thuerey
Categories: physics.flu-dyn cs.LG
Comments: Published on AIAA Journal. Code and dataset can be found at
  https://github.com/tum-pbs/Diffusion-based-Flow-Prediction
MSC-class: 76G25 (Primary) 68T37 (Secondary)
\\ ( https://arxiv.org/abs/2312.05320 ,  19537kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07961 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 14:43:48 GMT   (5137kb,D)

Title: Solution of the Probabilistic Lambert Problem: Connections with Optimal
  Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs
Authors: Alexis M.H. Teter, Iman Nodozi, Abhishek Halder
Categories: math.OC cs.LG cs.SY eess.SY math-ph math.MP stat.ML
\\ ( https://arxiv.org/abs/2401.07961 ,  5137kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09493 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 10:40:00 GMT   (19544kb,D)

Title: Identifying Three-Dimensional Radiative Patterns Associated with Early
  Tropical Cyclone Intensification
Authors: Frederick Iat-Hin Tam, Tom Beucler, James H. Ruppert Jr
Categories: physics.ao-ph cs.LG
Comments: 15 pages, 6 figures (main text)
\\ ( https://arxiv.org/abs/2401.09493 ,  19544kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10253
replaced with revised version Mon, 18 Mar 2024 03:01:08 GMT   (1417kb,D)

Title: Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable
  and Transferable Bandwidth Allocation
Authors: Xin Hao, Changyang She, Phee Lep Yeoh, Yuhong Liu, Branka Vucetic, and
  Yonghui Li
Categories: cs.NI cs.LG
\\ ( https://arxiv.org/abs/2401.10253 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12046
replaced with revised version Fri, 15 Mar 2024 22:39:18 GMT   (11617kb,D)

Title: Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D
Authors: Haojie Huang, Owen Howell, Dian Wang, Xupeng Zhu, Robin Walters,
  Robert Platt
Categories: cs.RO cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2401.12046 ,  11617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02198 (*cross-listing*)
replaced with revised version Sun, 17 Mar 2024 08:41:27 GMT   (2856kb)

Title: Co-orchestration of Multiple Instruments to Uncover Structure-Property
  Relationships in Combinatorial Libraries
Authors: Boris N. Slautin, Utkarsh Pratiush, Ilia N. Ivanov, Yongtao Liu, Rohit
  Pant, Xiaohang Zhang, Ichiro Takeuchi, Maxim A. Ziatdinov and Sergei V.
  Kalinin
Categories: cond-mat.mtrl-sci cs.LG
Comments: 22 pages, 9 figures
\\ ( https://arxiv.org/abs/2402.02198 ,  2856kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03220 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 17:23:35 GMT   (2992kb,D)

Title: The Benefits of Reusing Batches for Gradient Descent in Two-Layer
  Networks: Breaking the Curse of Information and Leap Exponents
Authors: Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka
  Zdeborov\'a, and Florent Krzakala
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.03220 ,  2992kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07472 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 03:05:15 GMT   (1274kb,D)

Title: Cartesian atomic cluster expansion for machine learning interatomic
  potentials
Authors: Bingqing Cheng
Categories: physics.comp-ph cs.LG physics.chem-ph
\\ ( https://arxiv.org/abs/2402.07472 ,  1274kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00033 (*cross-listing*)
replaced with revised version Sun, 17 Mar 2024 03:59:44 GMT   (6422kb,D)

Title: Identification of Craving Maps among Marijuana Users via the Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks
Authors: Jun-En Ding, Shihao Yang, Anna Zilverstand, and Feng Liu
Categories: q-bio.NC cs.LG eess.SP
\\ ( https://arxiv.org/abs/2403.00033 ,  6422kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02967 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 13:41:55 GMT   (696kb,D)

Title: Non-Convex Stochastic Composite Optimization with Polyak Momentum
Authors: Yuan Gao and Anton Rodomanov and Sebastian U. Stich
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2403.02967 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06645 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 09:22:01 GMT   (11851kb,D)

Title: Ricci flow-based brain surface covariance descriptors for diagnosing
  Alzheimer's disease
Authors: Fatemeh Ahmadi, Mohamad Ebrahim Shiri, Behroz Bidabad, Maral Sedaghat,
  Pooran Memari
Categories: eess.IV cs.CV cs.LG
Comments: Accepted for publication in Biomedical Signal Processing and Control
  journal
\\ ( https://arxiv.org/abs/2403.06645 ,  11851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07454 (*cross-listing*)
replaced with revised version Sat, 16 Mar 2024 09:16:11 GMT   (24725kb,D)

Title: Fast, accurate and lightweight sequential simulation-based inference
  using Gaussian locally linear mappings
Authors: Henrik H\"aggstr\"om, Pedro L. C. Rodrigues, Geoffroy Oudoumanessah,
  Florence Forbes, Umberto Picchini
Categories: stat.ML cs.LG
Comments: 60 pages, 55 figures: the only difference with v1 is a different
  formatting/LaTeX stylefile
\\ ( https://arxiv.org/abs/2403.07454 ,  24725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08164
replaced with revised version Sun, 17 Mar 2024 10:06:12 GMT   (2478kb,D)

Title: EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight
  Text-to-Speech
Authors: Ziqi Liang, Haoxiang Shi, Jiawei Wang, Keda Lu
Categories: cs.SD cs.LG eess.AS
Comments: Accepted by the 27th IEEE International Conference on Computer
  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:
  substantial text overlap with arXiv:2211.01948
\\ ( https://arxiv.org/abs/2403.08164 ,  2478kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10075
replaced with revised version Mon, 18 Mar 2024 01:16:04 GMT   (2214kb)

Title: A survey of synthetic data augmentation methods in computer vision
Authors: Alhassan Mumuni, Fuseini Mumuni and Nana Kobina Gerrar
Categories: cs.CV cs.GR cs.LG
\\ ( https://arxiv.org/abs/2403.10075 ,  2214kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
